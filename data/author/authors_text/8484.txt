Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 929?936
Manchester, August 2008
Using Three Way Data for Word Sense Discrimination
Tim Van de Cruys
Humanities Computing
University of Groningen
t.van.de.cruys@rug.nl
Abstract
In this paper, an extension of a dimen-
sionality reduction algorithm called NON-
NEGATIVE MATRIX FACTORIZATION is
presented that combines both ?bag of
words? data and syntactic data, in order
to find semantic dimensions according to
which both words and syntactic relations
can be classified. The use of three way
data allows one to determine which dimen-
sion(s) are responsible for a certain sense
of a word, and adapt the corresponding
feature vector accordingly, ?subtracting?
one sense to discover another one. The
intuition in this is that the syntactic fea-
tures of the syntax-based approach can be
disambiguated by the semantic dimensions
found by the bag of words approach. The
novel approach is embedded into cluster-
ing algorithms, to make it fully automatic.
The approach is carried out for Dutch, and
evaluated against EuroWordNet.
1 Introduction
Automatically acquiring semantics from text is a
subject that has gathered a lot of attention for quite
some time now. As Manning and Schu?tze (Man-
ning and Schu?tze, 2000) point out, most work
on acquiring semantic properties of words has fo-
cused on semantic similarity. ?Automatically ac-
quiring a relative measure of how similar a word
is to known words [...] is much easier than deter-
mining what the actual meaning is.? (Manning and
Schu?tze, 2000, ?8.5)
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
Most work on semantic similarity relies on the
distributional hypothesis (Harris, 1985). This hy-
pothesis states that words that occur in similar con-
texts tend to be similar. With regard to the context
used, two basic approaches exist. One approach
makes use of ?bag of words? co-occurrence data; in
this approach, a certain window around a word is
used for gathering co-occurrence information. The
window may either be a fixed number of words,
or the paragraph or document that a word appears
in. Thus, words are considered similar if they ap-
pear in similar windows (documents). One of the
dominant methods using this method is LATENT
SEMANTIC ANALYSIS (LSA).
The second approach uses a more fine grained
distributional model, focusing on the syntactic re-
lations that words appear with. Typically, a large
text corpus is parsed, and dependency triples are
extracted.1 Words are considered similar if they
appear with similar syntactic relations. Note that
the former approach does not need any kind of
linguistic annotation, whereas for the latter, some
form of syntactic annotation is needed.
The results yielded by both approaches are typ-
ically quite different in nature: the former ap-
proach typically puts its finger on a broad, the-
matic kind of similarity, while the latter approach
typically grasps a tighter, synonym-like similarity.
Example (1) shows the difference between both
approaches; for each approach, the top ten most
similar nouns to the Dutch noun muziek ?music?
are given. In (a), the window-based approach is
used, while (b) uses the syntax-based approach. (a)
shows indeed more thematic similarity, whereas
(b) shows tighter similarity.
1e.g. dependency relations that qualify apple might be
?object of eat? and ?adjective red?. This gives us dependency
triples like < apple, obj, eat >.
929
(1) a. muziek ?music?: gitaar ?guitar?, jazz ?jazz?,
cd ?cd?, rock ?rock?, bas ?bass?, song ?song?,
muzikant ?musician?, musicus ?musician?, drum
?drum?, slagwerker ?drummer?
b. muziek ?music?: dans ?dance?, kunst ?art?,
klank ?sound?, liedje ?song?, geluid ?sound?,
poe?zie ?poetry?, literatuur ?literature?, pop-
muziek ?pop music?, lied ?song?, melodie
?melody?
Especially the syntax-based method has been
adopted by many researchers, in order to find se-
mantically similar words. There is, however, one
important problem with this kind of approach: the
method is not able to cope with ambiguous words.
Take the examples:
(2) een
a
oneven
odd
nummer
number
an odd number
(3) een
a
steengoed
great
nummer
number
?a great song?
The word nummer does not have the same
meaning in these examples. In example (2), num-
mer is used in the sense of ?designator of quantity?.
In example (3), it is used in the sense of ?musi-
cal performance?. Accordingly, we would like the
word nummer to be disambiguated into two senses,
the first sense being similar to words like getal
?number?, cijfer ?digit? and the second to words
like liedje ?song?, song ?song?.
While it is relatively easy for a human language
user to distinguish between the two senses, this
is a difficult task for a computer. Even worse:
the results get blurred because the attributes of
both senses (in this example oneven and steen-
goed) are grouped together into one sense. This
is the main drawback of the syntax-based method.
On the other hand, methods that capture seman-
tic dimensions are known to be useful in disam-
biguating different senses of a word. Particu-
larly, PROBABILISTIC LATENT SEMANTIC ANAL-
YSIS (PLSA) is known to simultaneously encode
various senses of words according to latent seman-
tic dimensions (Hofmann, 1999). In this paper, we
want to explore an approach that tries to remedy
the shortcomings of the former, syntax-based ap-
proach with the benefits of the latter. The intuition
in this is that the syntactic features of the syntax-
based approach can be disambiguated by the ?la-
tent semantic dimensions? found by the window-
based approach.
2 Previous Work
2.1 Distributional Similarity
There have been numerous approaches for com-
puting the similarity between words from distribu-
tional data. We mention some of the most impor-
tant ones.
With regard to the first approach ? using a con-
text window ? we already mentioned LSA (Lan-
dauer and Dumais, 1997). In LSA, a term-
document matrix is created, containing the fre-
quency of each word in a specific document. This
matrix is then decomposed into three other matri-
ces with a mathematical technique called SINGU-
LAR VALUE DECOMPOSITION. The most impor-
tant dimensions that come out of the SVD allegedly
represent ?latent semantic dimensions?, according
to which nouns and documents can be presented
more efficiently.
LSA has been criticized for not being the most
appropriate data reduction method for textual ap-
plications. The SVD underlying the method as-
sumes normally-distributed data, whereas textual
count data (such as the term-document matrix)
can be more appropriately modeled by other dis-
tributional models such as Poisson (Manning and
Schu?tze, 2000, ?15.4.3). Successive methods such
as PROBABILISTIC LATENT SEMANTIC ANALY-
SIS (PLSA) (Hofmann, 1999), try to remedy this
shortcoming by imposing a proper latent variable
model, according to which the values can be es-
timated. The method we adopt in our research
? NON-NEGATIVE MATRIX FACTORIZATION ? is
similar to PLSA, and adequately remedies this
problem as well.
The second approach ? using syntactic relations
? has been adopted by many researchers, in order
to acquire semantically similar words. One of the
most important is Lin?s (1998). For Dutch, the ap-
proach has been applied by Van der Plas & Bouma
(2005).
2.2 Discriminating senses
Schu?tze (1998) uses a disambiguation algorithm ?
called context-group discrimination ? based on the
clustering of the context of ambiguous words. The
clustering is based on second-order co-occurrence:
the contexts of the ambiguous word are similar if
the words they in turn co-occur with are similar.
Pantel and Lin (2002) present a clustering al-
gorithm ? coined CLUSTERING BY COMMITTEE
(CBC) ? that automatically discovers word senses
930
from text. The key idea is to first discover a set
of tight, unambiguous clusters, to which possibly
ambiguous words can be assigned. Once a word
has been assigned to a cluster, the features associ-
ated with that particular cluster are stripped off the
word?s vector. This way, less frequent senses of
the word can be discovered.
The former approach uses a window-based
method; the latter uses syntactic data. But none
of the algorithms developed so far have combined
both sources in order to discriminate among differ-
ent senses of a word.
3 Methodology
3.1 Non-negative Matrix Factorization
3.1.1 Theory
Non-negative matrix factorization (NMF) (Lee
and Seung, 2000) is a group of algorithms in which
a matrix V is factorized into two other matrices, W
and H .
V
n?m
?W
n?r
H
r?m
(1)
Typically r is much smaller than n,m so that
both instances and features are expressed in terms
of a few components.
Non-negative matrix factorization enforces the
constraint that all three matrices must be non-
negative, so all elements must be greater than or
equal to zero. The factorization turns out to be
particularly useful when one wants to find additive
properties.
Formally, the non-negative matrix factorization
is carried out by minimizing an objective function.
Two kinds of objective function exist: one that
minimizes the Euclidean distance, and one that
minimizes the Kullback-Leibler divergence. In
this framework, we will adopt the latter, as ? from
our experience ? entropy-based measures tend to
work well for natural language. Thus, we want to
find the matrices W and H for which the Kullback-
Leibler divergence between V and WH (the mul-
tiplication of W and H) is the smallest.
Practically, the factorization is carried out
through the iterative application of update rules.
Matrices W and H are randomly initialized, and
the rules in 2 and 3 are iteratively applied ? alter-
nating between them. In each iteration, each vec-
tor is adequately normalized, so that all dimension
values sum to 1.
H
a?
? H
a?
?
i
W
ia
V
i?
(WH)
i?
?
k
W
ka
(2)
W
ia
?W
ia
?
?
H
a?
V
i?
(WH)
i?
?
v
H
av
(3)
3.1.2 Example
We can now straightforwardly apply NMF to
create semantic word models. NMF is applied to
a frequency matrix, containing bag of words co-
occurrence data. The additive property of NMF en-
sures that semantic dimensions emerge, according
to which the various words can be classified. Two
sample dimensions are shown in example (4). For
each dimension, the words with the largest value
on that dimension are given. Dimension (a) can
be qualified as a ?transport? dimension, and dimen-
sion (b) as a ?cooking? dimension.
(4) a. bus ?bus?, taxi ?taxi?, trein ?train?, halte ?stop?,
reiziger ?traveler?, perron ?platform?, tram
?tram?, station ?station?, chauffeur ?driver?,
passagier ?passenger?
b. bouillon ?broth?, slagroom ?cream?, ui ?onion?,
eierdooier ?egg yolk?, laurierblad ?bay leaf?,
zout ?salt?, deciliter ?decilitre?, boter ?butter?,
bleekselderij ?celery?, saus ?sauce?
3.2 Extending Non-negative Matrix
Factorization
We now propose an extension of NMF that com-
bines both the bag of words approach and the syn-
tactic approach. The algorithm finds again latent
semantic dimensions, according to which nouns,
contexts and syntactic relations are classified.
Since we are interested in the classification of
nouns according to both ?bag-of-words? context
and syntactic context, we first construct three ma-
trices that capture the co-occurrence frequency in-
formation for each mode. The first matrix con-
tains co-occurrence frequencies of nouns cross-
classified by dependency relations, the second ma-
trix contains co-occurrence frequencies of nouns
cross-classified by words that appear in the noun?s
context window, and the third matrix contains co-
occurrence frequencies of dependency relations
cross-classified by co-occurring context words.
We then apply NMF to the three matrices, but we
interleave the separate factorizations: the results of
the former factorization are used to initialize the
factorization of the next matrix. This implies that
we need to initialize only three matrices at random;
the other three are initialized by calculations of the
931
previous step. The process is represented graphi-
cally in figure 1.
Figure 1: A graphical representation of the ex-
tended NMF
In the example in figure 1, matrix H is initial-
ized at random, and the update of matrix W is cal-
culated. The result of update W is then used to
initialize matrix V , and the update of matrix G is
calculated. This matrix is used again to initialize
matrix U , and the update of matrix F is calculated.
This matrix can be used to initialize matrix H , and
the process is repeated until convergence.
In (5), an example is given of the kind of se-
mantic dimensions found. This dimension may be
coined the ?transport? dimension, as is shown by
the top 10 nouns (a), context words (b) and syntac-
tic relations (c).
(5) a. auto ?car?, wagen ?car?, tram ?tram?, motor
?motorbike?, bus ?bus?, metro ?subway?, auto-
mobilist ?driver?, trein ?trein?, stuur ?steering
wheel?, chauffeur ?driver?
b. auto ?car?, trein ?train?, motor ?motorbike?, bus
?bus?, rij ?drive?, chauffeur ?driver?, fiets ?bike?,
reiziger ?reiziger?, passagier ?passenger?, ver-
voer ?transport?
c. viertraps
adj
?four pedal?, verplaats met
obj
?move with?, toeter
adj
?honk?, tank in houd
obj
[parsing error], tank
subj
?refuel?, tank
obj
?re-
fuel?, rij voorbij
subj
?pass by?, rij voorbij
adj
?pass by?, rij af
subj
?drive off?, peperduur
adj
?very expensive?
3.3 Sense Subtraction
Next, we want to use the factorization that has been
created in the former step for word sense discrim-
ination. The intuition is that we ?switch off? one
dimension of an ambiguous word, to reveal pos-
sible other senses of the word. From matrix H,
we know the importance of each syntactic relation
given a dimension. With this knowledge, we can
?subtract? the syntactic relations that are responsi-
ble for a certain dimension from the original noun
vector:
??
v
new
=
??
v
orig
(
??
1 ?
??
h
dim
) (4)
Equation 4 multiplies each feature (syntactic re-
lation) of the original noun vector (??v
orig
) with a
scaling factor, according to the load of the feature
on the subtracted dimension (??h
dim
? the vector
of matrix H containing the dimension we want to
subtract). ??1 is a vector of ones, the size of ??h
dim
.
3.4 A Clustering Framework
The last step is to determine which dimension(s)
are responsible for a certain sense of the word. In
order to do so, we embed our method in a cluster-
ing approach. First, a specific word is assigned to
its predominant sense (i.e. the most similar clus-
ter). Next, the dominant semantic dimension(s)
for this cluster are subtracted from the word vec-
tor (equation 4), and the resulting vector is fed to
the clustering algorithm again, to see if other word
senses emerge. The dominant semantic dimen-
sion(s) can be identified by ?folding in? the cluster
centroid into our factorization (so we get a vec-
tor ??w of dimension size r), and applying a thresh-
old to the result (in our experiments a threshold of
? = .05 ? so dimensions responsible for > 5% of
the centroid are subtracted).
We used two kinds of clustering algorithms to
determine our initial centroids. The first algorithm
is a standard K-means algorithm. The second one
is the CBC algorithm by Pantel and Lin (2002).
The initial vectors to be clustered are adapted with
pointwise mutual information (Church and Hanks,
1990).
3.4.1 K-means
First, a standard K-means algorithm is applied
to the nouns we want to cluster. This yields a hard
clustering, in which each noun is assigned to ex-
actly one (dominant) cluster. In the second step,
we try to determine for each noun whether it can
be assigned to other, less dominant clusters. First,
the salient dimension(s) of the centroid to which
the noun is assigned are determined. We com-
pute the centroid of the cluster by averaging the
frequencies of all cluster elements except for the
target element we want to reassign, and adapt the
centroid with pointwise mutual information. After
932
subtracting the salient dimensions from the noun
vector, we check whether the vector is reassigned
to another cluster centroid (i.e. whether it is more
similar to a different centroid). If this is the case,
(another instance of) the noun is assigned to the
cluster, and we repeat the second step. If there is
no reassignment, we continue with the next word.
The target element is removed from the centroid
to make sure that we only subtract the dimensions
associated with the sense of the cluster.
Note that K-means requires to set the number of
clusters beforehand, so k is a parameter to be set.
3.4.2 CBC
The second clustering algorithm operates in a
similar vein, but instead of using simple K-means,
we use Pantel and Lin?s CBC algorithm to find the
initial centroids (coined COMMITTEES).
In order to find committees, the top k nouns
for each noun in the database are clustered with
average-link clustering. The clusters are scored
and sorted in such a way that preference is given
to tight, representative clusters. If the committees
do not cover all elements sufficiently, the algorithm
recursively tries to find more committees. An elab-
orate description of the algorithm can be found in
(Pantel and Lin, 2002).
In the second step, we start assigning elements
to committees. Once an element is assigned, the
salient dimensions are subtracted from the noun
vector in the same way as in 3.4.1 (only do we not
have to remove any target word from the centroid;
committees are supposed to represent tight, unam-
biguous clusters).
CBC attempts to find the number of committees
automatically from the data, so k does not have to
be set.
4 Examples
4.1 Sense Subtraction
In what follows, we will talk about semantic di-
mensions as, e.g., the ?music? dimension or the
?city? dimension. In the vast majority of the cases,
the dimensions are indeed as clear-cut as the trans-
port dimension shown above, so that the dimen-
sions can be rightfully labeled this way.
Two examples are given of how the semantic
dimensions that have been found can be used for
word sense discrimination. We will consider two
ambiguous nouns: pop, which can mean ?pop mu-
sic? as well as ?doll?, and Barcelona, which can
designate either the Spanish city or the Spanish
football club.
First, we look up the top dimensions for each
noun. Next, we successively subtract the dimen-
sions dealing with a particular sense of the noun,
as described in 3.3. This gives us three vectors
for each noun: the original vector, and two vectors
with one of the dimensions eliminated. For each of
these vectors, the top ten similar nouns are given,
in order to compare the changes brought about.
(6) a. pop, rock, jazz, meubilair ?furniture?, pop-
muziek ?pop music?, heks ?witch?, speelgoed
?toy?, kast ?cupboard?, servies ?[tea] service?,
vraagteken ?question mark?
b. pop, meubilair ?furniture?, speelgoed ?toy?,
kast ?cupboard?, servies ?[tea] service?, heks
?witch?, vraagteken ?question mark? sieraad
?jewel?, sculptuur ?sculpture?, schoen ?shoe?
c. pop, rock, jazz, popmuziek ?pop music?, heks
?witch?, danseres ?dancer?, servies ?[tea] ser-
vice?, kopje ?cup?, house ?house music?, aap
?monkey?
Example (6) shows the top similar words for the
three vectors of pop. In (a), the most similar words
to the original vector are shown. In (b), the top
dimension (the ?music dimension?) has been sub-
tracted from (a), and in (c), the second highest di-
mension (a ?domestic items? dimension) has been
subtracted from (a).
The differences between the three vectors are
clear: in vector (a), both senses are mixed together,
with ?pop music? and ?doll? items interleaved. In
(b), no more music items are present. Only items
related to the doll sense are among the top similar
words. In (c), the music sense emerges much more
clearly, with rock, jazz and popmuziek being the
most similar, and a new music term (house) show-
ing up among the top ten.
Admittedly, in vector (c), not all items related to
the ?doll? sense are filtered out. We believe this
is due to the fact that this sense cannot be ade-
quately filtered out by one dimension (in this case,
a dimension of ?domestic items? alone), whereas it
is much easier to filter out the ?music? sense with
only one ?music? dimension. We will try to rem-
edy this in our clustering framework, in which it is
possible to subtract multiple dimensions related to
one sense.
A second example, the ambiguous proper name
Barcelona, is given in (7).
(7) a. Barcelona, Arsenal, Inter, Juventus, Vitesse,
Milaan ?Milan?, Madrid, Parijs ?Paris?, Wenen
?Vienna?, Mu?nchen ?Munich?
b. Barcelona, Milaan ?Milan?, Mu?nchen ?Mu-
933
nich?, Wenen ?Vienna?, Madrid, Parijs ?Paris?,
Bonn, Praag ?Prague?, Berlijn ?Berlin?, Londen
?London?
c. Barcelona, Arsenal, Inter, Juventus, Vitesse,
Parma, Anderlecht, PSV, Feyenoord, Ajax
In (a), the two senses of Barcelona are clearly
mixed up, showing cities as well as football clubs
among the most similar nouns. In (b), where
the ?football dimension? has been subtracted, only
cities show up. In (c), where the ?city dimension?
has been subtracted, only football clubs remain.
4.2 Clustering Output
In (8), an example of our clustering algorithm with
initial K-means clusters is given.
(8) a. werk ?work? beeld ?image? foto ?photo?
schilderij ?painting? tekening ?drawing? doek
?canvas? installatie ?installation? afbeelding
?picture? sculptuur ?sculpture? prent ?pic-
ture? illustratie ?illustration? handschrift
?manuscript? grafiek ?print? aquarel ?aquarelle?
maquette ?scale-model? collage ?collage? ets
?etching?
b. werk ?work? boek ?book? titel ?title? roman
?novel? boekje ?booklet? debuut ?debut? bi-
ografie ?biography? bundel ?collection? toneel-
stuk ?play? bestseller ?bestseller? kinderboek
?child book? autobiografie ?autobiography?
novelle ?short story?
c. werk ?work? voorziening ?service? arbeid
?labour? opvoeding ?education? kinderopvang
?child care? scholing ?education? huisvest-
ing ?housing? faciliteit ?facility? accommodatie
?acommodation? arbeidsomstandigheid ?work-
ing condition?
The example shows three different clusters to
which the noun werk ?work? is assigned. In (a),
werk refers to a work of art. In (b), it refers to a
written work. In (c), the ?labour? sense of werk
emerges.
5 Evaluation
5.1 Methodology
The clustering results are evaluated according to
Dutch EuroWordNet (Vossen and others, 1999).
Precision and recall are calculated by comparing
the results to EuroWordNet synsets. The precision
is the number of clusters found that correspond to
an actual sense of the word. Recall is the number
of word senses in EuroWordNet that are found by
the algorithm. Our evaluation method is largely the
same as the one used by Pantel and Lin (2002).
Both precision and recall are based on wordnet
similarity. A number of similarity measures have
been developed to calculate semantic similarity in
a hierarchical wordnet. Among these measures,
the most important are Wu & Palmer?s (Wu and
Palmer, 1994), Resnik?s (Resnik, 1995) and Lin?s
(Lin, 1998). In this evaluation, Wu & Palmer?s
(1994) measure will be adopted. The similarity is
calculated according to the formula in (5), in which
N
1
and N
2
are the number of is-a links from A and
B to their most specific common superclass C; N
3
is the number of is-a links from C to the root of
the taxonomy.
sim
Wu&Palmer
(A,B) =
2N
3
N
1
+N
2
+ 2N
3
(5)
iets
object
wezen
organisme
dier
zoogdier vis
hond zalm
Figure 2: Extract from the Dutch EuroWordNet
Hierarchy
For example, the most common superclass of
hond ?dog? en zalm ?salmon? is dier ?animal? (as
can be seen on the extract from Dutch EuroWord-
Net in figure 2). Consequently, N
1
= 2, N
2
= 2,
N
3
= 4 and sim
WP
(hond, zalm) = 0.67.
To calculate precision, we apply the same
methodology as Pantel and Lin (2002).2 Let S(w)
be the set of EuroWordNet senses. sim
W
(s, u),
the similarity between a synset s and a word u is
then defined as the maximum similarity between s
and a sense of u:
sim
W
(s, u) = max
t?S(u)
sim(s, t) (6)
Let c
k
be the top k-members of a cluster c,
where these are the k most similar members to the
centroid of c. simC(c, s), the similarity between
s and c, is then defined as the average similarity
between s and the top-k members of c:
sim
C
(s, c) =
?
u?c
k
simW (s, u)
k
(7)
2Note, however, that our similarity measure is different.
Where Pantel and Lin use Lin?s (1998) measure, we use Wu
and Palmer?s (1994) measure.
934
An assigment of a word w to a cluster c can now
be classified as correct if
max
s?S(w)
simC(s, c) > ? (8)
and the EuroWordNet sense of w that corre-
sponds to c is
argmax
s?S(w)
simC(s, c) (9)
When multiple clusters correspond to the same
EuroWordNet sense, only one of them is counted
as correct.
Precision of a word w is the percentage of cor-
rect clusters to which it is assigned. Recall of
a word w is the percentage of senses from Eu-
roWordnet that have a corresponding cluster.3 Pre-
cision and recall of a clustering algorithm is the
average precision and recall of all test words.
5.2 Experimental Design
We have applied the interleaved NMF presented in
section 3.2 to Dutch, using the TWENTE NIEUWS
CORPUS (Ordelman, 2002), containing > 500M
words of Dutch newspaper text. The corpus is con-
sistently divided into paragraphs, which have been
used as the context window for the bag-of-words
mode. The corpus has been parsed by the Dutch
dependency parser Alpino (van Noord, 2006), and
dependency triples have been extracted. Next, the
three matrices needed for our method have been
constructed: one containing nouns by dependency
relations (5K ? 80K), one containing nouns by
context words (5K ? 2K) and one containing de-
pendency relations by context words (80K ? 2K).
We did 200 iterations of the algorithm, factorizing
the matrices into 50 dimensions. The NMF algo-
rithm has been implemented in Matlab.
For the evaluation, we use all the words that ap-
pear in our original clustering input as well as in
EuroWordNet. This yields a test set of 3683 words.
5.3 Results
Table 1 shows precision and recall figures for four
different algorithms, according to two similarity
thresholds ? (equation 8). kmeans
nmf
describes
the results of our algorithm with K-means clus-
ters, as described in section 3.4.1. CBC describes
3Our notion of recall is slightly different from the one used
by Pantel and Lin, as they use ?the number of senses in which
w was used in the corpus? as gold standard. This information,
as they acknowledge, is difficult to get at, so we prefer to use
the sense information in EuroWordNet.
the results of our algorithm with the CBC commit-
tees, as described in section 3.4.2. For comparison,
we have also included the results of a standard K-
means clustering (kmeans
orig
, k = 600), and the
original CBC algorithm (CBC
orig
) as described by
Pantel and Lin (2002).
threshold ?
.40 (%) .60 (%)
kmeans
nmf
prec. 78.97 55.16
rec. 63.90 44.77
CBC
nmf
prec. 82.70 54.87
rec. 60.27 40.51
kmeans
orig
prec. 86.13 58.97
rec. 60.23 41.80
CBC
orig
prec. 44.94 29.74
rec. 69.61 48.00
Table 1: Precision and recall for four different al-
gorithms according to two similarity thresholds
The results show the same tendency across all
similarity thresholds: kmeans
nmf
has a high pre-
cision, but lower recall compared to CBC
orig
. Still
the recall is higher compared to standard K-means,
which indicates that the algorithm is able to find
multiple senses of nouns, with high precision. The
results of CBC
nmf
are similar to the results of
kmeans
orig
, indicating that few words are reas-
signed to multiple clusters when using CBC com-
mittees with our method.
Obviously, kmeans
orig
scores best with regard
to precision, but worse with regard to recall.
CBC
orig
finds most senses (highest recall), but pre-
cision is considerably worse.
The fact that recall is already quite high with
standard K-means clustering indicates that the
evaluation is skewed towards nouns with only one
sense, possibly due to a lack of coverage in Eu-
roWordNet. In future work, we specifically want
to evaluate the discrimination of ambiguous words.
Also, we want to make use of the new Cornetto
Database4, a successor of EuroWordNet for Dutch
which is currently under development.
Still, the evaluation shows that our method pro-
vides a genuine way of finding multiple senses of
words, while retaining high precision. Especially
the method using a simple K-means clustering per-
4http://www.let.vu.nl/onderzoek/
projectsites/cornetto/index.html
935
forms particularly well. The three way data al-
lows the algorithm to put its finger on the particular
sense of a centroid, and adapt the feature vector of
a possibly ambiguous noun accordingly.
6 Conclusion & Future Work
In this paper, an extension of NMF has been pre-
sented that combines both bag of words data and
syntactic data in order to find latent semantic di-
mensions according to which both words and syn-
tactic relations can be classified. The use of three
way data allows one to determine which dimen-
sion(s) are responsible for a certain sense of a
word, and adapt the corresponding feature vec-
tor accordingly, ?subtracting? one sense to dis-
cover another one. When embedded in a clustering
framework, the method provides a fully automatic
way to discriminate the various senses of words.
The evaluation against EuroWordNet shows that
the algorithm is genuinely able to disambiguate the
features of a given word, and accordingly its word
senses.
We conclude with some issues for future work.
First of all, we would like to test the method that
has been explored in this paper with other evalua-
tion frameworks. We already mentioned the focus
on ambiguous nouns, and the use of the new Cor-
netto database for Dutch. Next, we would like to
work out a proper probabilistic framework for the
?subtraction? of dimensions. At this moment, the
subtraction (using a cut-off) is somewhat ad hoc. A
probabilistic modeling of this intuition might lead
to an improvement.
And finally, we would like to use the results of
our method to learn selectional preferences. Our
method is able to discriminate the syntactic fea-
tures that are linked to a particular word sense. If
we can use the results to improve a parser?s perfor-
mance, this will also provide an external evaluation
of the algorithm.
References
Church, Kenneth Ward and Patrick Hanks. 1990. Word
association norms, mutual information & lexicogra-
phy. Computational Linguistics, 16(1):22?29.
Harris, Z. 1985. Distributional structure. In Katz, Jer-
rold J., editor, The Philosophy of Linguistics, pages
26?47. Oxford University Press.
Hofmann, Thomas. 1999. Probabilistic latent semantic
analysis. In Proc. of Uncertainty in Artificial Intelli-
gence, UAI?99, Stockholm.
Landauer, Thomas and Se Dumais. 1997. A solution to
Plato?s problem: The Latent Semantic Analysis the-
ory of the acquisition, induction, and representation
of knowledge. Psychology Review, 104:211?240.
Lee, Daniel D. and H. Sebastian Seung. 2000. Al-
gorithms for non-negative matrix factorization. In
NIPS, pages 556?562.
Lin, D. 1998. Automatic retrieval and clustering of
similar words. In Proceedings of COLING/ACL 98,
Montreal, Canada.
Manning, Christopher and Hinrich Schu?tze. 2000.
Foundations of Statistical Natural Language Pro-
cessing. MIT Press, Cambridge, Massachussets.
Ordelman, R.J.F. 2002. Twente Nieuws Corpus
(TwNC), August. Parlevink Language Techonology
Group. University of Twente.
Pantel, Patrick and Dekang Lin. 2002. Discovering
word senses from text. In Proceedings of the eighth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 613?619,
New York, NY, USA. ACM Special Interest Group
on Knowledge Discovery in Data, ACM Press.
Resnik, Philip. 1995. Using information content to
evaluate semantic similarity in a taxonomy. In IJ-
CAI, pages 448?453.
Schu?tze, Hinrich. 1998. Automatic word sense dis-
crimination. Computational Linguistics, 24(1):97?
123.
van der Plas, Lonneke and Gosse Bouma. 2005.
Syntactic contexts for finding semantically similar
words. In van der Wouden, Ton et al, editors,
Computational Linguistics in the Netherlands 2004.
Selected Papers from the Fifteenth CLIN Meeting,
pages 173?184, Utrecht. LOT.
van Noord, Gertjan. 2006. At Last Parsing Is Now
Operational. In Mertens, Piet, Cedrick Fairon, Anne
Dister, and Patrick Watrin, editors, TALN06. Verbum
Ex Machina. Actes de la 13e conference sur le traite-
ment automatique des langues naturelles, pages 20?
42, Leuven.
Vossen, Piek et al 1999. The Dutch Wordnet, July.
University of Amsterdam.
Wu, Zhibiao and Martha Palmer. 1994. Verb semantics
and lexical selection. In 32nd. Annual Meeting of
the Association for Computational Linguistics, pages
133?138, New Mexico State University, Las Cruces,
New Mexico.
936
Proceedings of the Workshop on A Broader Perspective on Multiword Expressions, pages 25?32,
Prague, June 2007. c?2007 Association for Computational Linguistics
Semantics-based Multiword Expression Extraction
Tim Van de Cruys and Begon?a Villada Moiro?n
Alfa Informatica, University of Groningen
Oude Kijk in ?t Jatstraat 26
9712 EK Groningen, The Netherlands
{T.Van.de.Cruys|M.B.Villada.Moiron}@rug.nl
Abstract
This paper describes a fully unsupervised
and automated method for large-scale ex-
traction of multiword expressions (MWEs)
from large corpora. The method aims at cap-
turing the non-compositionality of MWEs;
the intuition is that a noun within a MWE
cannot easily be replaced by a semanti-
cally similar noun. To implement this intu-
ition, a noun clustering is automatically ex-
tracted (using distributional similarity mea-
sures), which gives us clusters of semanti-
cally related nouns. Next, a number of statis-
tical measures ? based on selectional prefer-
ences ? is developed that formalize the intu-
ition of non-compositionality. Our approach
has been tested on Dutch, and automatically
evaluated using Dutch lexical resources.
1 Introduction
MWEs are expressions whose linguistic behaviour is
not predictable from the linguistic behaviour of their
component words. Baldwin (2006) characterizes the
idiosyncratic behavior of MWEs as ?a lack of com-
positionality manifest at different levels of analysis,
namely, lexical, morphological, syntactic, seman-
tic, pragmatic and statistical?. Some MWEs show
productive morphology and/or syntactic flexibility.
Therefore, these two aspects are not sufficient con-
ditions to discriminate actual MWEs from productive
expressions. Nonetheless, the mentioned character-
istics are useful indicators to distinguish literal and
idiomatic expressions (Fazly and Stevenson, 2006).
One property that seems to affect MWEs the most
is semantic non-compositionality. MWEs are typi-
cally non-compositional. As a consequence, it is not
possible to replace the noun of a MWE by semanti-
cally related nouns. Take for example the expres-
sions in (1) and (2):
(1) a. break the vase
b. break the cup
c. break the dish
(2) a. break the ice
b. *break the snow
c. *break the hail
Expression (1-a) is fully compositional. Therefore,
vase can easily be replaced with semantically re-
lated nouns such as cup and dish. Expression (2-a),
on the contrary, is non-compositional; ice cannot be
replaced with semantically related words, such as
snow and hail without loss of the original meaning.
Due to the idiosyncratic behavior, current propos-
als argue that MWEs need to be described in the lexi-
con (Sag et al, 2002). In most languages, electronic
lexical resources (such as dictionaries, thesauri, on-
tologies) suffer from a limited coverage of MWEs.
To facilitate the update and expansion of language
resources, the NLP community would clearly bene-
fit from automated methods that extract MWEs from
large text collections. This is the main motivation to
pursue an automated and fully unsupervised MWE
extraction method.
25
2 Previous Work
Recent proposals that attempt to capture seman-
tic compositionality (or lack thereof) employ vari-
ous strategies. Approaches evaluated so far make
use of dictionaries with semantic annotation (Piao
et al, 2006), WordNet (Pearce, 2001), automati-
cally generated thesauri (Lin, 1999; McCarthy et
al., 2003; Fazly and Stevenson, 2006), vector-based
methods that measure semantic distance (Baldwin et
al., 2003; Katz and Giesbrecht, 2006), translations
extracted from parallel corpora (Villada Moiro?n
and Tiedemann, 2006) or hybrid methods that use
machine learning techniques informed by features
coded using some of the above methods (Venkata-
pathy and Joshi, 2005).
Pearce (2001) describes a method to extract collo-
cations from corpora by measuring semantic compo-
sitionality. The underlying assumption is that a fully
compositional expression allows synonym replace-
ment of its component words, whereas a collocation
does not. Pearce measures to what degree a collo-
cation candidate allows synonym replacement. The
measurement is used to rank candidates relative to
their compositionality.
Building on Lin (1998), McCarthy et al (2003)
measure the semantic similarity between expres-
sions (verb particles) as a whole and their compo-
nent words (verb). They exploit contextual features
and frequency information in order to assess mean-
ing overlap. They established that human composi-
tionality judgements correlate well with those mea-
sures that take into account the semantics of the par-
ticle. Contrary to these measures, standard associ-
ation measures poorly correlate with human judge-
ments.
A different approach proposed by Villada Moiro?n
and Tiedemann (2006) measures translational en-
tropy as a sign of meaning predictability, and there-
fore non-compositionality. The entropy observed
among word alignments of a potential MWE varies:
highly predictable alignments show less entropy and
probably correspond to compositional expressions.
Data sparseness and polysemy pose problems be-
cause the entropy cannot be accurately calculated.
Fazly and Stevenson (2006) use lexical and
syntactic fixedness as partial indicators of non-
compositionality. Their method uses Lin?s (1998)
automatically generated thesaurus to compute a met-
ric of lexical fixedness. Lexical fixedness mea-
sures the deviation between the pointwise mutual
information of a verb-object phrase and the aver-
age pointwise mutual information of the expres-
sions resulting from substituting the noun by its
synonyms in the original phrase. This measure is
similar to Lin?s (1999) proposal for finding non-
compositional phrases. Separately, a syntactic flexi-
bility score measures the probability of seeing a can-
didate in a set of pre-selected syntactic patterns. The
assumption is that non-compositional expressions
score high in idiomaticity, that is, a score resulting
from the combination of lexical fixedness and syn-
tactic flexibility. The authors report an 80% accu-
racy in distinguishing literal from idiomatic expres-
sions in a test set of 200 expressions. The perfor-
mance of both metrics is stable across all frequency
ranges.
In this study, we are interested in establishing
whether a fully unsupervised method can capture
the (partial or) non-compositionality of MWEs. The
method should not depend on the existence of large
(open domain) parallel corpora or sense tagged cor-
pora. Also, the method should not require numer-
ous adjustments when applied to new subclasses
of MWEs, for instance, when coding empirical at-
tributes of the candidates. Similar to Lin (1999),
McCarthy et al (2003) and Fazly and Stevenson
(2006), our method makes use of automatically gen-
erated thesauri; the technique used to compile the
thesauri differs from previous work. Aiming at find-
ing a method of general applicability, the measures
to capture non-compositionality differ from those
employed in earlier work.
3 Methodology
In the description and evaluation of our algorithm,
we focus on the extraction of verbal MWEs that con-
tain prepositional complements, although we believe
the method can be easily generalized to other kinds
of MWEs.
In our semantics-based approach, we want to for-
malize the intuition of non-compositionality, so that
MWE extraction can be done in a fully automated
way. A number of statistical measures are developed
that try to capture the MWE?s non-compositional
26
bond between a verb-preposition combination and
its noun by comparing the particular noun of a MWE
candidate to other semantically related nouns.
3.1 Data extraction
The MWE candidates (verb + prepositional phrase)
are automatically extracted from the Twente Nieuws
Corpus (Ordelman, 2002), a large corpus of Dutch
newspaper texts (500 million words), which has
been automatically parsed by the Dutch dependency
parser Alpino (van Noord, 2006). Next, a matrix is
created of the 5,000 most frequent verb-preposition
combinations by the 10,000 most frequent nouns,
containing the frequency of each MWE candidate.1
To this matrix, a number of statistical measures are
applied to determine the non-compositionality of the
candidate MWEs. These statistical measures are ex-
plained in 3.3.
3.2 Clustering
In order to compare a noun to its semantically re-
lated nouns, a noun clustering is created. These
clusters are automatically extracted using standard
distributional similarity techniques (Weeds, 2003;
van der Plas and Bouma, 2005). First, depen-
dency triples are extracted from the Twente Nieuws
Corpus. Next, feature vectors are created for each
noun, containing the frequency of the dependency
relations in which the noun occurs.2 This way, a
frequency matrix of 10K nouns by 100K depen-
dency relations is constructed. The cell frequencies
are replaced by pointwise mutual information scores
(Church et al, 1991), so that more informative fea-
tures get a higher weight. The noun vectors are then
clustered into 1,000 clusters using a simple K-means
clustering algorithm (MacQueen, 1967) with cosine
similarity. During development, several other clus-
tering algorithms and parameters have been tested,
but the settings described above gave us the best
EuroWordNet similarity score (using Wu and Palmer
(1994)).
Note that our clustering algorithm is a hard clus-
tering algorithm, which means that a certain noun
1The lowest frequency verb-preposition combination (with
regard to the 10,000 nouns) appears 3 times.
2e.g. dependency relations that qualify apple might be ?ob-
ject of eat? and ?adjective red?. This gives us dependency triples
like < apple, obj, eat >.
can only be assigned to one cluster. This may pose a
problem for polysemous nouns. On the other hand,
this makes the computation of our metrics straight-
forward, since we do not have to decide among var-
ious senses of a word.
3.3 Measures
The measures used to find MWEs are inspired by
Resnik?s method to find selectional preferences
(Resnik, 1993; Resnik, 1996). Resnik uses a number
of measures based on the Kullback-Leibler diver-
gence, to measure the difference between the prior
probability of a noun class p(c) and the probabil-
ity of the class given a verb p(c|v). We adopt the
method for particular nouns, and add a measure for
determining the ?unique preference? of a noun given
other nouns in the cluster, which, we claim, yields
a measure of non-compositionality. In total, 4 mea-
sures are used, the latter two being the symmetric
counterpart of the former two.
The first two measures, Av?n (equation 2) and
Rv?n (equation 3), formalize the unique prefer-
ence of the verb3 for the noun. Equation 1 gives
the Kullback-Leibler divergence between the overall
probability distribution of the nouns and the proba-
bility distribution of the nouns given a verb; it is used
as a normalization constant in equation 2. Equa-
tion 2 models the actual preference of the verb for
the noun.
Sv =
?
n
p(n | v) logp(n | v)p(n) (1)
Av?n =
p(n | v) log p(n|v)p(n)
Sv
(2)
When p(n|v) is 0, Av?n is undefined. In this
case, we assign a score of 0.
Equation 3 gives the ratio of the verb preference
for a particular noun, compared to the other nouns
that are present in the cluster.
Rv?n =
Av?n
?
n??C Av?n?
(3)
When Rv?n is more or less equally divided
among the different nouns in the cluster, there is no
3We will use ?verb? to designate a prepositional verb, i.e. a
combination of a verb and a preposition.
27
preference of the verb for a particular noun in the
cluster, whereas scores close to 1 indicate a ?unique?
preference of the verb for a particular noun in the
cluster. Candidates whose Rv?n value approaches
1 are likely to be non-compositional expressions.
In the latter two measures, An?v and Rn?v, the
direction of preference is changed: equations 4 and 5
are the symmetric counterparts of equations 2 and 3.
Instead of the preference of the verb for the noun,
the preference of the noun for the verb is modelled.
Except for the change of preference direction, the
characteristics of the former and the latter two mea-
sures are the same.
An?v =
p(v | n) log p(v|n)p(v)
Sn
(4)
Rn?v =
An?v
?
n??C An??v
(5)
Note that, despite their symmetry, the measures
for verb preference and the measures for noun pref-
erence are different in nature. It is possible that
a certain verb only selects a restricted number of
nouns, while the nouns themselves can co-occur
with many different verbs. This brings about differ-
ent probability distributions. In our evaluation, we
want to investigate the impact of both preferences.
3.4 Example
In this section, an elaborated example is presented,
to show how our method works. Take for example
the two MWE candidates in (3):
(3) a. in
in
de
the
smaak
taste
vallen
fall
to be appreciated
b. in
in
de
the
put
well
vallen
fall
to fall down the well
In the first expression, smaak cannot be replaced
with other semantically similar nouns, such as geur
?smell? and zicht ?sight?, whereas in the second ex-
pression, put can easily be replaced with other se-
mantically similar words, such as kuil ?hole? and
krater ?crater?.
The first step in the formalization of this intuition,
is the extraction of the clusters in which the words
smaak and put appear from our clustering database.
This gives us the clusters in (4).
(4) a. smaak: aroma ?aroma?, gehoor ?hear-
ing?, geur ?smell?, gezichtsvermogen
?sight?, reuk ?smell?, spraak ?speech?,
zicht ?sight?
b. put: afgrond ?abyss?, bouwput ?build-
ing excavation?, gaatje ?hole?, gat
?hole?, hiaat ?gap?, hol ?cave?, kloof
?gap?, krater ?crater?, kuil ?hole?, lacune
?lacuna?, leemte ?gap?, valkuil ?pitfall?
Next, the various measures described in section 3.3
are applied. Resulting scores are given in tables 1
and 2.
MWE candidate Av?n Rv?n An?v Rn?v
val#in smaak .12 1.00 .04 1.00
val#in geur .00 .00 .00 .00
val#in zicht .00 .00 .00 .00
Table 1: Scores for MWE candidate in de smaak
vallen and other nouns in the same cluster.
Table 1 gives the scores for the MWE in de smaak
vallen, together with some other nouns that are
present in the same cluster. Av?n shows that there
is a clear preference (.12) of the verb val in for the
noun smaak. Rv?n shows that there is a unique
preference of the verb for the particular noun smaak.
For the other nouns (geur, zicht, . . . ), the verb has no
preference whatsoever. Therefore, the ratio of verb
preference for smaak compared to the other nouns
in the cluster is 1.00.
An?v and Rn?v show similar behaviour. There
is a preference (.04) of the noun smaak for the verb
val in, and this preference is unique (1.00).
MWE candidate Av?n Rv?n An?v Rn?v
val#in put .00 .05 .00 .05
val#in kuil .01 .11 .02 .37
val#in kloof .00 .02 .00 .03
val#in gat .04 .71 .01 .24
Table 2: Scores for MWE candidate in de put vallen
and other nouns in same cluster.
28
Table 2 gives the scores for the instance in de put
vallen ? which is not a MWE ? together with other
nouns from the same cluster. The results are quite
different from the ones in table 1. Av?n ? the pref-
erence of the verb for the noun ? is quite low in most
cases, the highest score being a score of .04 for gat.
Furthermore, Rv?n does not show a unique pref-
erence of val in for put (a low ratio score of .05).
Instead, the preference mass is divided among the
various nouns in the cluster, the highest preference
of val in being assigned to the noun gat (.71).4
The other two scores show again a similar ten-
dency; An?v ? the preference of the noun for the
verb ? is low in all cases, and when all nouns in the
cluster are considered (Rn?v), there is no ?unique?
preference of one noun for the verb val in. Instead,
the preference mass is divided among all nouns in
the cluster.
4 Results & Evaluation
4.1 Quantitative evaluation
In this section, we quantitatively evaluate our
method, and compare it to the lexical and syntactic
fixedness measures proposed by Fazly and Steven-
son (2006). More information about Fazly and
Stevenson?s measures can be found in their paper.
The potential MWEs that are extracted with the
fully unsupervised method described above and with
Fazly and Stevenson?s (2006) method (FS from here
onwards) are automatically evaluated by compar-
ing the extracted list to handcrafted MWE databases.
Since we have extracted Dutch MWEs, we are us-
ing the two Dutch resources available: the Refer-
entie Bestand Nederlands (RBN, Martin and Maks
(2005)) and the Van Dale Lexicographical Informa-
tion System (VLIS) database. Evaluation scores are
calculated with regard to the MWEs that are present
in our evaluation resources. Among the MWEs in our
reference data, we consider only those expressions
that are present in our frequency matrix: if the verb
is not among the 5,000 most frequent verbs, or the
noun is not among the 10,000 most frequent nouns,
the frequency information is not present in our input
4The expression is ambiguous: it can be used in a lit-
eral sense (in een gat vallen, ?to fall down a hole?) and in a
metaphorical sense (in een zwart gat vallen, ?to get depressed
after a joyful or busy period?).
data. Consequently, our algorithm would never be
able to find those MWEs.
The first six rows of table 3 show precision, re-
call and f-measure for various parameter thresholds
with regard to the measures Av?n, Rv?n, An?v
and Rn?v, together with the number of candidates
found (n). The last 3 rows show the highest val-
ues we were able to reach by using FS?s fixedness
scores.
Using only two parameters ? Av?n and Rv?n ?
gives the highest f-measure (? 14%), with a pre-
cision and recall of about 17% and about 12% re-
spectively. Adding parameter Rn?v increases preci-
sion but degrades recall, and this tendency continues
when adding both parameters An?v and Rn?v. In
all cases, a higher threshold increases precision but
degrades recall. When using a high threshold for all
parameters, the algorithm is able to reach a precision
of ? 38%, but recall is low (? 4%).
Lexical fixedness reaches an f-measure of ? 12%
(threshold of 3.00). These scores show the best per-
formance that we reached using lexical fixedness.
Following FS, we evaluated the syntactic fixedness
scores of expressions falling above a frequency cut-
off. Since our corpus is much larger than that used
by FS, a frequency cutoff of 50 was chosen. The pre-
cision, recall and f-measure of the syntactic fixed-
ness measure (shown on table 3) are ? 10%, 41%
and 16% respectively, showing worse precision than
our method but much better recall and f-measure.
As shown by FS, syntactic fixedness performs better
than lexical fixedness; Fixednessoverall improves
on the syntactic fixedness results and also reaches
better overall performance than our method.
The compared methods show a different behav-
ior. FS?s method favours high recall whereas our
method prefers the best trade-off between precision
and recall. We wish to highlight that our method
reaches better precision than FS?s method while han-
dling many low frequency candidates (minimum fre-
quency is 3); this makes our method preferable in
some NLP tasks. It is possible that the two methods
are capturing different properties of MWEs; in future
work, we want to analyse whether the expressions
extracted by the two methods differ.
29
parameters precision recall f-measure
Av?n Rv?n An?v Rn?v n (%) (%) (%)
.10 .80 ? ? 3175 16.09 13.11 14.45
.10 .90 ? ? 2655 17.59 11.98 14.25
.10 .80 ? .80 2225 19.19 10.95 13.95
.10 .90 ? .90 1870 20.70 9.93 13.42
.10 .80 .01 .80 1859 20.33 9.69 13.13
.20 .99 .05 .99 404 38.12 3.95 7.16
Fixednesslex(v, n) 3.00 3899 15.14 9.92 11.99
Fixednesssyn(v, n) 50 15,630 10.20 40.90 16.33
Fixednessoverall(v, n) 50 7819 13.73 27.54 18.33
Table 3: Evaluation results compared to RBN & VLIS
4.2 Qualitative evaluation
Next, we elaborate upon advantages and disadvan-
tages of our semantics-based MWE extraction algo-
rithm by examining the output of the procedure, and
looking at the characteristics of the MWEs found and
the errors made by the algorithm.
First of all, our algorithm is able to filter out gram-
matical collocations that cause problems in tradi-
tional MWE extraction paradigms. An example is
given in (5).
(5) voldoen
meet
aan
to
eisen,
demands,
voorwaarden
conditions
meet the {demands, conditions}
In traditional MWE extraction algorithms, based on
collocations, highly frequent expressions like the
ones in (5) often get classified as a MWE, even
though they are fully compositional. Such algo-
rithms correctly identify a strong lexical affinity be-
tween two component words (voldoen, aan), which
make up a grammatical collocation; however, they
fail to capture the fact that the noun may be filled in
by a semantic class of nouns. Our algorithm filters
out those expressions, because semantic similarity
between nouns that fill in the object slot is taken into
account.
Our quantitative evaluation shows that the algo-
rithm reaches the best results (i.e. the highest f-
measures) when using only two parameters (Av?n
and Rv?n). Upon closer inspection of the output,
we noticed that An?v and Rn?v are often able to
filter out non-MWEs like the expressions b in (6)
and (7).
(6) a. verschijnen
appear
op
on
toneel
stage
to appear
b. zingen
sing
op
on
toneel
stage
to sing on the stage
(7) a. lig
lie
in
in
geheugen
memory
be in memory
b. lig
lie
in
in
ziekenhuis
hospital
lie in the hospital
It is only when the two other measures (a unique
preference of the noun for the verb) are taken into
account that the b expressions are filtered out ? ei-
ther because the noun preference for the verb is very
low, or because it is more evenly distributed among
the cluster. The b expressions, which are non-MWEs,
result from the combination of a verb with a highly
frequent PP. These PPs are typically locative, direc-
tional or predicative PPs, that may combine with nu-
merous verbs.
Also, expressions like the ones in (8), where the
fixedness of the expression lies not so much in the
verb-noun combination, but more in the noun part
(naar school, naar huis) are filtered out by the lat-
ter two measures. These preposition-noun combina-
tions seem to be institutionalized PPs, so-called de-
terminerless PPs.
30
(8) a. naar
to
school
school
willen
want
want to go to school
b. naar
to
huis
home
willen
want
want to go home
We will now look at some errors made by our algo-
rithm. First of all, our algorithm highly depends on
the quality of the noun clustering. If a noun appears
in a cluster with unrelated words, the measures will
overrate the semantic uniqueness of the expressions
in which the noun appears.
Secondly, syntax might play an important role.
Sometimes, there are syntactic restrictions between
the preposition and the noun. A noun like pagina
?page? can only appear with the preposition op ?on?,
as in lees op pagina ?read on page?. Other, semanti-
cally related nouns, such as hoofdstuk ?chapter?, pre-
fer in ?in?. Due to these restrictions, the measures
will again overrate the semantic uniqueness of the
noun (pagina in the example).
Finally, our hard clustering method does not take
polysemous nouns into account. A noun may only
occur in one cluster, ignoring other possible mean-
ings. Schaal, for example, means ?dish? as well as
?scale?. In our clustering, it only appears in a cluster
of dish-related nouns. Therefore, expressions like
maak gebruik op [grote] schaal ?make use of [sth.]
on a [large] scale?, receive again overrated measures
of semantic uniqueness, because the ?scale? sense of
the noun is compared to nouns related to the ?dish?
sense.
5 Conclusions and further work
Our algorithm based on non-compositionality ex-
plores a new approach aimed at large-scale MWE
extraction. Using only two parameters, Av?n and
Rv?n, yields the highest f-measure. Using the two
other parameters, An?v and Rn?v, increases preci-
sion but degrades recall. Due to the formalization of
the intuition of non-compositionality (using an auto-
matic noun clustering), our algorithm is able to rule
out various expressions that are coined MWEs by tra-
ditional algorithms.
Note that our algorithm has taken on a purely
semantics-based approach. ?Syntactic fixedness? of
the expressions is not taken into account. Combin-
ing our semantics-based approach with other extrac-
tion techniques such as the syntactic fixedness mea-
sure proposed by Fazly and Stevenson (2006) might
improve the results significantly.
We conclude with some issues saved for future
work. First of all, we would like to combine our
semantics-based method with other methods that are
used to find MWEs (especially syntax-based meth-
ods), and implement the method in general classifi-
cation models (decision tree classifier and maximum
entropy model). This includes the use of a more
principled (machine learning) framework in order to
establish the optimal threshold values.
Next, we would like to investigate a number of
topics to improve on our semantics-based method.
First of all, using the top k similar nouns for a certain
noun ? instead of the cluster in which a noun appears
? might be more beneficial to get a grasp of the com-
positionality of MWE candidates. Also, making use
of a verb clustering in addition to the noun clustering
might help in determining the non-compositionality
of expressions. Disambiguating among the various
senses of nouns should also be a useful improve-
ment. Furthermore, we would like to generalize our
method to other syntactic patterns (e.g. verb object
combinations), and test the approach for English.
One final issue is the realization of a manual eval-
uation of our semantics-based algorithm, by hav-
ing human judges decide whether a MWE candidate
found by our algorithm is an actual MWE. Our au-
tomated evaluation framework is error-prone due to
mistakes and incompleteness of our resources. Dur-
ing qualitative evaluation, we found many actual
MWEs found by our algorithm, that were not con-
sidered correct by our resources (e.g. [iemand] in
de gordijnen jagen ?to drive s.o. mad?, op het [ver-
keerde] paard gokken ?back the wrong horse?, [de
kat] uit de boom kijken ?wait to see which way the
wind blows?, uit het [goede] hout gesneden ?be a
trustworthy person?). Conversely, there were also
questionable MWE candidates that were described
as actual MWEs in our evaluation resources (val op
woensdag ?fall on a wednesday?, neem als voorzitter
?take as chairperson?, ruik naar haring ?smell like
herring?, ben voor [. . . ] procent ?to be . . . percent?).
A manual evaluation could overcome these difficul-
ties.
We believe that our method provides a genuine
31
and successful approach to get a grasp of the non-
compositionality of MWEs in a fully automated way.
We also believe that it is one of the first methods
able to extract MWEs based on non-compositionality
on a large scale, and that traditional MWE extrac-
tion algorithms will benefit from taking this non-
compositionality into account.
Acknowledgements
This research was carried out as part of the research
program IRME STEVIN project. We would also like
to thank Gertjan van Noord and the two anonymous
reviewers for their helpful comments on an earlier
version of this paper.
References
T. Baldwin, C. Bannard, T. Tanaka, and D. Widdows. 2003. An
Empirical Model of Multiword Expressions Decomposabil-
ity. In Proc. of the ACL-2003 Workshop on Multiword Ex-
pressions: Analysis, Acquisition and Treatment, pages 89?
96, Sapporo, Japan.
T. Baldwin. 2006. Compositionality and Multiword Expres-
sions: Six of One, Half a Dozen of the Other? Invited talk
given at the COLING/ACL?06 Workshop on Multiword Ex-
pressions: Identifying and Exploiting Underlying Properties,
July.
K. Church, W. Gale, P. Hanks, and D. Hindle. 1991. Using
statistics in lexical analysis. In Uri Zernik, editor, Lexical
Acquisition: Exploiting On-line resources to build a lexicon,
pages 115?164. Lawrence Erlbaum Associates, New Jersey.
A. Fazly and S. Stevenson. 2006. Automatically constructing
a lexicon of verb phrase idiomatic combinations. In Pro-
ceedings of the 11th Conference of the European Chapter of
the Association for Computational Linguistics (EACL-2006),
Trento, Italy.
G. Katz and E. Giesbrecht. 2006. Automatic identification of
non-compositional multi-word expressions using Latent Se-
mantic Analysis. In Proc. of the COLING/ACL?06 Work-
shop on Multiword Expressions: Identifying and Exploiting
Underlying Properties, pages 12?19, Sydney, Australia.
D. Lin. 1998. Automatic retrieval and clustering of simi-
lar words. In Proceedings of COLING/ACL 98, Montreal,
Canada.
D. Lin. 1999. Automatic identification of non-compositional
phrases. In Proceedings of ACL-99, pages 317?324. Univer-
sity of Maryland.
J. B. MacQueen. 1967. Some methods for classification and
analysis of multivariate observations. In Proceedings of 5-th
Berkeley Symposium on Mathematical Statistics and Prob-
ability, volume 1, pages 281?297, Berkeley. University of
California Press.
W. Martin and I. Maks, 2005. Referentie Bestand Nederlands.
Documentatie, April.
D. McCarthy, B. Keller, and J. Carroll. 2003. Detecting a Con-
tinuum of Compositionality in Phrasal Verbs. In Proc. of
the ACL-2003 Workshop on Multiword Expressions: Analy-
sis, Acquisition and Treatment, Sapporo, Japan.
R.J.F. Ordelman. 2002. Twente Nieuws Corpus (TwNC), Au-
gust. Parlevink Language Techonology Group. University of
Twente.
D. Pearce. 2001. Synonymy in collocation extraction. In Word-
Net and Other lexical resources: applications, extensions
& customizations (NAACL 2001), pages 41?46, Pittsburgh.
Carnegie Mellon University.
S. Piao, P. Rayson, O. Mudraya, A. Wilson, and R. Garside.
2006. Measuring mwe compositionality using semantic an-
notation. In Proceedings of the Workshop on Multiword
Expressions: Identifying and Exploiting Underlying Prop-
erties, pages 2?11, Sydney, Australia. Association for Com-
putational Linguistics.
P. Resnik. 1993. Selection and Information: A Class-Based
Approach to Lexical Relationships. PhD Thesis, University
of Pennsylvania.
P. Resnik. 1996. Selectional constraints: An information-
theoretic model and its computational realization. Cogni-
tion, 61:127?159.
I. Sag, T. Baldwin, F. Bond, A. Copestake, and D. Flickinger.
2002. Multiword Expressions: a pain in the neck for NLP.
In Proceedings of the Third International Conference on
Intelligent Text Processing and Computational Linguistics,
pages 1?15, Mexico City, Mexico.
L. van der Plas and G. Bouma. 2005. Syntactic contexts for
finding semantically similar words. Computational Linguis-
tics in the Netherlands 2004. Selected Papers from the Fif-
teenth CLIN Meeting, pages 173?184.
G. van Noord. 2006. At Last Parsing Is Now Operational.
In P. Mertens, C. Fairon, A. Dister, and P. Watrin, editors,
TALN06. Verbum Ex Machina. Actes de la 13e conference
sur le traitement automatique des langues naturelles, pages
20?42, Leuven.
S. Venkatapathy and A. Joshi. 2005. Measuring the relative
compositionality of verb-noun collocations by integrating
features. In Proceedings of the Human Language Technol-
ogy Conference and Conference on Empirical Methods in
Natural Language Processing, pages 899?906, Vancouver.
B. Villada Moiro?n and J. Tiedemann. 2006. Identifying id-
iomatic expressions using automatic word-alignment. In
Proceedings of the EACL 2006 Workshop on Multi-word-
expressions in a multilingual context?, pages 33?40, Trento,
Italy.
J. Weeds. 2003. Measures and Applications of Lexical Distri-
butional Similarity. PhD Thesis, University of Sussex.
Z. Wu and M. Palmer. 1994. Verb semantics and lexical selec-
tion. In 32nd. Annual Meeting of the Association for Com-
putational Linguistics, pages 133?138, New Mexico State
University, Las Cruces, New Mexico.
32
Proceedings of the EACL 2009 Workshop on GEMS: GEometical Models of Natural Language Semantics, pages 83?90,
Athens, Greece, 31 March 2009. c?2009 Association for Computational Linguistics
A Non-negative Tensor Factorization Model for
Selectional Preference Induction
Tim Van de Cruys
University of Groningen
The Netherlands
t.van.de.cruys@rug.nl
Abstract
Distributional similarity methods have
proven to be a valuable tool for the in-
duction of semantic similarity. Up till
now, most algorithms use two-way co-
occurrence data to compute the mean-
ing of words. Co-occurrence frequencies,
however, need not be pairwise. One can
easily imagine situations where it is desir-
able to investigate co-occurrence frequen-
cies of three modes and beyond. This pa-
per will investigate a tensor factorization
method called non-negative tensor factor-
ization to build a model of three-way co-
occurrences. The approach is applied to
the problem of selectional preference in-
duction, and automatically evaluated in a
pseudo-disambiguation task. The results
show that non-negative tensor factoriza-
tion is a promising tool for NLP.
1 Introduction
Distributional similarity methods have proven to
be a valuable tool for the induction of semantic
similarity. The aggregate of a word?s contexts gen-
erally provides enough information to compute its
meaning, viz. its semantic similarity or related-
ness to other words.
Up till now, most algorithms use two-way co-
occurrence data to compute the meaning of words.
A word?s meaning might for example be computed
by looking at:
? the various documents that the word appears
in (words ? documents);
? a bag of words context window around the
word (words ? context words);
? the dependency relations that the word ap-
pears with (words ? dependency relations).
The extracted data ? representing the co-
occurrence frequencies of two different entities
? is encoded in a matrix. Co-occurrence fre-
quencies, however, need not be pairwise. One
can easily imagine situations where it is desirable
to investigate co-occurrence frequencies of three
modes and beyond. In an information retrieval
context, one such situation might be the investiga-
tion of words ? documents ? authors. In an NLP
context, one might want to investigatewords? de-
pendency relations ? bag of word context words,
or verbs ? subjects ? direct objects.
Note that it is not possible to investigate the
three-way co-occurrences in a matrix represen-
tation form. It is possible to capture the co-
occurrence frequencies of a verb with its sub-
jects and its direct objects, but one cannot cap-
ture the co-occurrence frequencies of the verb ap-
pearing with the subject and the direct object at
the same time. When the actual three-way co-
occurrence data is ?matricized?, valuable informa-
tion is thrown-away. To be able to capture the mu-
tual dependencies among the three modes, we will
make use of a generalized tensor representation.
Two-way co-occurrence models (such as la-
tent semantic analysis) have often been augmented
with some form of dimensionality reduction in or-
der to counter noise and overcome data sparseness.
We will also make use of a dimensionality reduc-
tion algorithm appropriate for tensor representa-
tions.
2 Previous Work
2.1 Selectional Preferences & Verb
Clustering
Selectional preferences have been a popular re-
search subject in the NLP community. One of
the first to automatically induce selectional pref-
erences from corpora was Resnik (1996). Resnik
generalizes among nouns by using WordNet noun
83
synsets as clusters. He then calculates the se-
lectional preference strength of a specific verb in
a particular relation by computing the Kullback-
Leibler divergence between the cluster distribu-
tion of the verb and the aggregate cluster distri-
bution. The selectional association is then the
contribution of the cluster to the verb?s prefer-
ence strength. The model?s generalization relies
entirely on WordNet; there is no generalization
among the verbs.
The research in this paper is related to previous
work on clustering. Pereira et al (1993) use an
information-theoretic based clustering approach,
clustering nouns according to their distribution as
direct objects among verbs. Their model is a one-
sided clustering model: only the direct objects are
clustered, there is no clustering among the verbs.
Rooth et al (1999) use an EM-based cluster-
ing technique to induce a clustering based on the
co-occurrence frequencies of verbs with their sub-
jects and direct objects. As opposed to the method
of Pereira et al (1993), their model is two-sided:
the verbs as well as the subjects/direct objects are
clustered. We will use a similar model for evalua-
tion purposes.
Recent approaches using distributional similar-
ity methods for the induction of selectional pref-
erences are the ones by Erk (2007), Bhagat et al
(2007) and Basili et al (2007).
This research differs from the approaches men-
tioned above by its use of multi-way data: where
the approaches above limit themselves to two-way
co-occurrences, this research will focus on co-
occurrences for multi-way data.
2.2 Factorization Algorithms
2.2.1 Two-way Factorizations
One of the best known factorization algorithms
is principal component analysis (PCA, Pearson
(1901)). PCA transforms the data into a new co-
ordinate system, yielding the best possible fit in a
least square sense given a limited number of di-
mensions. Singular value decomposition (SVD)
is the generalization of the eigenvalue decompo-
sition used in PCA (Wall et al, 2003).
In information retrieval, singular value decom-
position has been applied in latent semantic analy-
sis (LSA, Landauer and Dumais (1997), Landauer
et al (1998)). In LSA, a term-document matrix
is created, containing the frequency of each word
in a specific document. This matrix is then de-
composed into three other matrices with SVD. The
most important dimensions that come out of the
SVD allegedly represent ?latent semantic dimen-
sions?, according to which nouns and documents
can be represented more efficiently.
LSA has been criticized for a number of rea-
sons, one of them being the fact that the factor-
ization contains negative numbers. It is not clear
what negativity on a semantic scale should des-
ignate. Subsequent methods such as probabilistic
latent semantic analysis (PLSA, Hofmann (1999))
and non-negative matrix factorization (NMF, Lee
and Seung (2000)) remedy these problems, and
indeed get much more clear-cut semantic dimen-
sions.
2.2.2 Three-way Factorizations
To be able to cope with three-way data, sev-
eral algorithms have been developed as multilin-
ear generalizations of the SVD. In statistics, three-
way component analysis has been extensively in-
vestigated (for an overview, see Kiers and van
Mechelen (2001)). The two most popular methods
are parallel factor analysis (PARAFAC, Harshman
(1970), Carroll and Chang (1970)) and three-mode
principal component analysis (3MPCA, Tucker
(1966)), also called higher order singular value
decomposition (HOSVD, De Lathauwer et al
(2000)). Three-way factorizations have been ap-
plied in various domains, such as psychometry
and image recognition (Vasilescu and Terzopou-
los, 2002). In information retrieval, three-way fac-
torizations have been applied to the problem of
link analysis (Kolda and Bader, 2006).
One last important method dealing with multi-
way data is non-negative tensor factorization
(NTF, Shashua and Hazan (2005)). NTF is a gener-
alization of non-negative matrix factorization, and
can be considered an extension of the PARAFAC
model with the constraint of non-negativity (cfr.
infra).
One of the few papers that has investigated the
application of tensor factorization for NLP is Tur-
ney (2007), in which a three-mode tensor is used
to compute the semantic similarity of words. The
method achieves 83.75% accuracy on the TOEFL
synonym questions.
84
3 Methodology
3.1 Tensors
Distributional similarity methods usually repre-
sent co-occurrence data in the form of a matrix.
This form is perfectly suited to represent two-way
co-occurrence data, but for co-occurrence data be-
yond two modes, we need a more general repre-
sentation. The generalization of a matrix is called
a tensor. A tensor is able to encode co-occurrence
data of any n modes. Figure 1 shows a graphi-
cal comparison of a matrix and a tensor with three
modes ? although a tensor can easily be general-
ized to more than three modes.
Figure 1: Matrix representation vs. tensor repre-
sentation
3.2 Non-negative Tensor Factorization
In order to create a succinct and generalized model
of the extracted data, a statistical dimensional-
ity reduction technique called non-negative tensor
factorization (NTF) is applied to the data. The NTF
model is similar to the PARAFAC analysis ? popu-
lar in areas such as psychology and bio-chemistry
? with the constraint that all data needs to be non-
negative (i.e. ? 0).
Parallel factor analysis (PARAFAC) is a multi-
linear analogue of the singular value decomposi-
tion (SVD) used in latent semantic analysis. The
key idea is to minimize the sum of squares be-
tween the original tensor and the factorized model
of the tensor. For the three mode case of a tensor
T ? RD1?D2?D3 this gives equation 1, where k is
the number of dimensions in the factorized model
and ? denotes the outer product.
min
xi?RD1,yi?RD2,zi?RD3
? T ?
k
?
i=1
xi ? yi ? zi ?
2
F (1)
With non-negative tensor factorization, the non-
negativity constraint is enforced, yielding a model
like the one in equation 2:
min
xi?RD1?0,yi?R
D2
?0,zi?R
D3
?0
? T ?
k
?
i=1
xi ? yi ? zi ?
2
F (2)
The algorithm results in three matrices, indicat-
ing the loadings of each mode on the factorized
dimensions. The model is represented graphically
in figure 2, visualizing the fact that the PARAFAC
decomposition consists of the summation over the
outer products of n (in this case three) vectors.
Figure 2: Graphical representation of the NTF as
the sum of outer products
Computationally, the non-negative tensor fac-
torization model is fitted by applying an alternat-
ing least-squares algorithm. In each iteration, two
of the modes are fixed and the third one is fitted
in a least squares sense. This process is repeated
until convergence.1
3.3 Applied to Language Data
The model can straightforwardly be applied to lan-
guage data. In this part, we describe the fac-
torization of verbs ? subjects ? direct objects
co-occurrences, but the example can easily be
substituted with other co-occurrence information.
Moreover, the model need not be restricted to 3
modes; it is very well possible to go to 4 modes
and beyond ? as long as the computations remain
feasible.
The NTF decomposition for the verbs ? sub-
jects? direct objects co-occurrences into the three
loadings matrices is represented graphically in fig-
ure 3. By applying the NTF model to three-way
(s,v,o) co-occurrences, we want to extract a gen-
eralized selectional preference model, and eventu-
ally even induce some kind of frame semantics (in
the broad sense of the word).
In the resulting factorization, each verb, subject
and direct object gets a loading value for each fac-
tor dimension in the corresponding loadings ma-
trix. The original value for a particular (s,v,o)
1The algorithm has been implemented in MATLAB, using
the Tensor Toolbox for sparse tensor calculations (Bader and
Kolda, 2007).
85
Figure 3: Graphical representation of the NTF for
language data
triple xsvo can then be reconstructed with equa-
tion 3.
xsvo =
k
?
i=1
ssivviooi (3)
To reconstruct the selectional preference value
for the triple (man,bite,dog), for example, we
look up the subject vector for man, the verb vector
for bite and the direct object vector for dog. Then,
for each dimension i in the model, we multiply the
ith value of the three vectors. The sum of these
values is the final preference value.
4 Results
4.1 Setup
The approach described in the previous section has
been applied to Dutch, using the Twente Nieuws
Corpus (Ordelman, 2002), a 500M words corpus
of Dutch newspaper texts. The corpus has been
parsed with the Dutch dependency parser Alpino
(van Noord, 2006), and three-way co-occurrences
of verbs with their respective subject and direct
object relations have been extracted. As dimen-
sion sizes, the 1K most frequent verbs were used,
together with the 10K most frequent subjects and
10K most frequent direct objects, yielding a ten-
sor of 1K ? 10K ? 10K. The resulting tensor is
very sparse, with only 0.0002% of the values be-
ing non-zero.
The tensor has been adapted with a straight-
forward extension of pointwise mutual informa-
tion (Church and Hanks, 1990) for three-way co-
occurrences, following equation 4. Negative val-
ues are set to zero.2
2This is not just an ad hoc conversion to enforce non-
negativity. Negative values indicate a smaller co-occurrence
probability than the expected number of co-occurrences. Set-
ting those values to zero proves beneficial for similarity cal-
culations (see e.g. Bullinaria and Levy (2007)).
MI3(x,y,z) = log
p(x,y,z)
p(x)p(y)p(z)
(4)
The resulting matrix has been factorized into k
dimensions (varying between 50 and 300) with the
NTF algorithm described in section 3.2.
4.2 Examples
Table 1, 2 and 3 show example dimensions that
have been found by the algorithm with k = 100.
Each example gives the top 10 subjects, verbs
and direct objects for a particular dimension, to-
gether with the score for that particular dimension.
Table 1 shows the induction of a ?police action?
frame, with police authorities as subjects, police
actions as verbs and patients of the police actions
as direct objects.
In table 2, a legislation dimension is induced,
with legislative bodies as subjects3, legislative ac-
tions as verbs, and mostly law (proposals) as direct
objects. Note that some direct objects (e.g. ?min-
ister?) also designate persons that can be the object
of a legislative act.
Table 3, finally, is clearly an exhibition dimen-
sion, with verbs describing actions of display and
trade that art institutions (subjects) can do with
works of art (objects).
These are not the only sensible dimensions that
have been found by the algorithm. A quick qual-
itative evaluation indicates that about 44 dimen-
sions contain similar, framelike semantics. In an-
other 43 dimensions, the semantics are less clear-
cut (single verbs account for one dimension, or
different senses of a verb get mixed up). 13 dimen-
sions are not so much based on semantic character-
istics, but rather on syntax (e.g. fixed expressions
and pronomina).
4.3 Evaluation
The results of the NTF model have been quantita-
tively evaluated in a pseudo-disambiguation task,
similar to the one used by Rooth et al (1999). It is
used to evaluate the generalization capabilities of
the algorithm. The task is to judge which subject
(s or s?) and direct object (o or o?) is more likely
for a particular verb v, where (s,v,o) is a combi-
nation drawn from the corpus, and s? and o? are a
subject and direct object randomly drawn from the
corpus. A triple is considered correct if the algo-
rithm prefers both s and o over their counterparts
3Note that VVD, D66, PvdA and CDA are Dutch political
parties.
86
subjects sus verbs vs objects ob js
politie ?police? .99 houd aan ?arrest? .64 verdachte ?suspect? .16
agent ?policeman? .07 arresteer ?arrest? .63 man ?man? .16
autoriteit ?authority? .05 pak op ?run in? .41 betoger ?demonstrator? .14
Justitie ?Justice? .05 schiet dood ?shoot? .08 relschopper ?rioter? .13
recherche ?detective force? .04 verdenk ?suspect? .07 raddraaiers ?instigator? .13
marechaussee ?military police? .04 tref aan ?find? .06 overvaller ?raider? .13
justitie ?justice? .04 achterhaal ?overtake? .05 Roemeen ?Romanian? .13
arrestatieteam ?special squad? .03 verwijder ?remove? .05 actievoerder ?campaigner? .13
leger ?army? .03 zoek ?search? .04 hooligan ?hooligan? .13
douane ?customs? .02 spoor op ?track? .03 Algerijn ?Algerian? .13
Table 1: Top 10 subjects, verbs and direct objects for the ?police action? dimension
subjects sus verbs vs objects ob js
meerderheid ?majority? .33 steun ?support? .83 motie ?motion? .63
VVD .28 dien in ?submit? .44 voorstel ?proposal? .53
D66 .25 neem aan ?pass? .23 plan ?plan? .28
Kamermeerderheid ?Chamber majority? .25 wijs af ?reject? .17 wetsvoorstel ?bill? .19
fractie ?party? .24 verwerp ?reject? .14 hem ?him? .18
PvdA .23 vind ?think? .08 kabinet ?cabinet? .16
CDA .23 aanvaard ?accepts? .05 minister ?minister? .16
Tweede Kamer ?Second Chamber? .21 behandel ?treat? .05 beleid ?policy? .13
partij ?party? .20 doe ?do? .04 kandidatuur ?candidature? .11
Kamer ?Chamber? .20 keur goed ?pass? .03 amendement ?amendment? .09
Table 2: Top 10 subjects, verbs and direct objects for the ?legislation? dimension
s? and o? (so the (s,v,o) triple ? that appears in the
test corpus ? is preferred over the triples (s?,v,o?),
(s?,v,o) and (s,v,o?)). Table 4 shows three exam-
ples from the pseudo-disambiguation task.
s v o s? o?
jongere drink bier coalitie aandeel
?youngster? ?drink? ?beer? ?coalition? ?share?
werkgever riskeer boete doel kopzorg
?employer? ?risk? ?fine? ?goal? ?worry?
directeur zwaai scepter informateur vodka
?manager? ?sway? ?sceptre? ?informer? ?wodka?
Table 4: Three examples from the pseudo-
disambiguation evaluation task?s test set
Four different models have been evaluated. The
first two models are tensor factorization models.
The first model is the NTF model, as described
in section 3.2. The second model is the original
PARAFAC model, without the non-negativity con-
straints.
The other two models are matrix factorization
models. The third model is the non-negative ma-
trix factorization (NMF) model, and the fourth
model is the singular value decomposition (SVD).
For these models, a matrix has been constructed
that contains the pairwise co-occurrence frequen-
cies of verbs by subjects as well as direct objects.
This gives a matrix of 1K verbs by 10K subjects
+ 10K direct objects (1K ? 20K). The matrix has
been adapted with pointwise mutual information.
The models have been evaluated with 10-fold
cross-validation. The corpus contains 298,540 dif-
ferent (s,v,o) co-occurrences. Those have been
randomly divided into 10 equal parts. So in each
fold, 268,686 co-occurrences have been used for
training, and 29,854 have been used for testing.
The accuracy results of the evaluation are given in
table 5.
The results clearly indicate that the NTF model
outperforms all the other models. The model
achieves the best result with 300 dimensions, but
the differences between the different NTF models
are not very large ? all attaining scores around
90%.
87
subjects sus verbs vs objects ob js
tentoonstelling ?exhibition? .50 toon ?display? .72 schilderij ?painting? .47
expositie ?exposition? .49 omvat ?cover? .63 werk ?work? .46
galerie ?gallery? .36 bevat ?contain? .18 tekening ?drawing? .36
collectie ?collection? .29 presenteer ?present? .17 foto ?picture? .33
museum ?museum? .27 laat ?let? .07 sculptuur ?sculpture? .25
oeuvre ?oeuvre? .22 koop ?buy? .07 aquarel ?aquarelle? .20
Kunsthal .19 bezit ?own? .06 object ?object? .19
kunstenaar ?artist? .15 zie ?see? .05 beeld ?statue? .12
dat ?that? .12 koop aan ?acquire? .05 overzicht ?overview? .12
hij ?he? .10 in huis heb ?own? .04 portret ?portrait? .11
Table 3: Top 10 subjects, verbs and direct objects for the ?exhibition? dimension
dimensions
50 (%) 100 (%) 300 (%)
NTF 89.52 ? 0.18 90.43 ? 0.14 90.89 ? 0.16
PARAFAC 85.57 ? 0.25 83.58 ? 0.59 80.12 ? 0.76
NMF 81.79 ? 0.15 78.83 ? 0.40 75.74 ? 0.63
SVD 69.60 ? 0.41 62.84 ? 1.30 45.22 ? 1.01
Table 5: Results of the 10-fold cross-validation for
the NTF, PARAFAC, NMF and SVD model for 50,
100 and 300 dimensions (averages and standard
deviation)
The PARAFAC results indicate the fitness of ten-
sor factorization for the induction of three-way se-
lectional preferences. Even without the constraint
of non-negativity, the model outperforms the ma-
trix factorization models, reaching a score of about
85%. The model deteriorates when more dimen-
sions are used.
Both matrix factorization models perform
worse than their tensor factorization counterparts.
The NMF still scores reasonably well, indicating
the positive effect of the non-negativity constraint.
The simple SVD model performs worst, reaching a
score of about 70% with 50 dimensions.
5 Conclusion and Future Work
This paper has presented a novel method that
is able to investigate three-way co-occurrences.
Other distributional methods deal almost exclu-
sively with pairwise co-occurrences. The ability
to keep track of multi-way co-occurrences opens
up new possibilities and brings about interesting
results. The method uses a factorization model ?
non-negative tensor factorization ? that is suitable
for three way data. The model is able to generalize
among the data and overcome data sparseness.
The method has been applied to the problem
of selectional preference induction. The results
indicate that the algorithm is able to induce se-
lectional preferences, leading to a broad kind
of frame semantics. The quantitative evaluation
shows that use of three-way data is clearly benefi-
cial for the induction of three-way selectional pref-
erences. The tensor models outperform the sim-
ple matrix models in the pseudo-disambiguation
task. The results also indicate the positive ef-
fect of the non-negativity constraint: both mod-
els with non-negative constraints outperform their
non-constrained counterparts.
The results as well as the evaluation indicate
that the method presented here is a promising tool
for the investigation of NLP topics, although more
research and thorough evaluation are desirable.
There is quite some room for future work. First
of all, we want to further investigate the useful-
ness of the method for selectional preference in-
duction. This includes a deeper quantitative eval-
uation and a comparison to other methods for se-
lectional preference induction. We also want to
include other dependency relations in our model,
apart from subjects and direct objects.
Secondly, there is room for improvement and
further research with regard to the tensor factor-
ization model. The model presented here min-
imizes the sum of squared distance. This is,
however, not the only objective function possi-
ble. Another possibility is the minimization of the
Kullback-Leibler divergence. Minimizing the sum
of squared distance assumes normally distributed
data, and language phenomena are rarely normally
distributed. Other objective functions ? such as the
minimization of the Kullback-Leibler divergence
? might be able to capture the language structures
88
much more adequately. We specifically want to
stress this second line of future research as one of
the most promising and exciting ones.
Finally, the model presented here is not
only suitable for selectional preference induction.
There are many problems in NLP that involve
three-way co-occurrences. In future work, we
want to apply the NTF model presented here to
other problems in NLP, the most important one be-
ing word sense discrimination.
Acknowledgements
Brett Bader kindly provided his implementation of
non-negative tensor factorization for sparse ma-
trices, from which this research has substantially
benefited. The three anonymous reviewers pro-
vided fruitful comments and remarks, which con-
siderably improved the quality of this paper.
References
Brett W. Bader and Tamara G. Kolda. 2006. Efficient
MATLAB computations with sparse and factored
tensors. Technical Report SAND2006-7592, Sandia
National Laboratories, Albuquerque, NM and Liver-
more, CA, December.
Brett W. Bader and Tamara G. Kolda. 2007. Mat-
lab tensor toolbox version 2.2. http://csmr.ca.
sandia.gov/?tgkolda/TensorToolbox/, Jan-
uary.
Roberto Basili, Diego De Cao, Paolo Marocco, and
Marco Pennacchiotti. 2007. Learning selectional
preferences for entailment or paraphrasing rules. In
Proceedings of RANLP 2007, Borovets, Bulgaria.
Rahul Bhagat, Patrick Pantel, and Eduard Hovy. 2007.
Ledir: An unsupervised algorithm for learning di-
rectionality of inference rules. In Proceedings of
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP-07), pages 161?170,
Prague, Czech Republic.
John A. Bullinaria and Joseph P. Levy. 2007. Ex-
tracting semantic representations from word co-
occurrence statistics: A computational study. Be-
havior Research Methods, 39:510?526.
J. D. Carroll and J.-J. Chang. 1970. Analysis of in-
dividual differences in multidimensional scaling via
an n-way generalization of ?eckart-young? decom-
position. Psychometrika, 35:283?319.
Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information & lexicogra-
phy. Computational Linguistics, 16(1):22?29.
Lieven De Lathauwer, Bart De Moor, and Joos Vande-
walle. 2000. A multilinear singular value decompo-
sition. SIAM Journal on Matrix Analysis and Appli-
cations, 21(4):1253?1278.
Katrin Erk. 2007. A simple, similarity-based model
for selectional preferences. In Proceedings of ACL
2007, Prague, Czech Republic.
R.A. Harshman. 1970. Foundations of the parafac pro-
cedure: models and conditions for an ?explanatory?
multi-mode factor analysis. In UCLA Working Pa-
pers in Phonetics, volume 16, pages 1?84, Los An-
geles. University of California.
Thomas Hofmann. 1999. Probabilistic latent semantic
analysis. In Proc. of Uncertainty in Artificial Intelli-
gence, UAI?99, Stockholm.
H.A.L Kiers and I. van Mechelen. 2001. Three-way
component analysis: Principles and illustrative ap-
plication. Psychological Methods, 6:84?110.
Tamara Kolda and Brett Bader. 2006. The TOPHITS
model for higher-order web link analysis. In Work-
shop on Link Analysis, Counterterrorism and Secu-
rity.
Thomas Landauer and Susan Dumais. 1997. A so-
lution to Plato?s problem: The Latent Semantic
Analysis theory of the acquisition, induction, and
representation of knowledge. Psychology Review,
104:211?240.
Thomas Landauer, Peter Foltz, and Darrell Laham.
1998. An Introduction to Latent Semantic Analysis.
Discourse Processes, 25:295?284.
Daniel D. Lee and H. Sebastian Seung. 2000. Al-
gorithms for non-negative matrix factorization. In
NIPS, pages 556?562.
R.J.F. Ordelman. 2002. Twente Nieuws Corpus
(TwNC), August. Parlevink Language Technology
Group. University of Twente.
K. Pearson. 1901. On lines and planes of closest fit to
systems of points in space. Philosophical Magazine,
2(6):559?572.
Fernando Pereira, Naftali Tishby, and Lillian Lee.
1993. Distributional clustering of English words. In
31st Annual Meeting of the ACL, pages 183?190.
Philip Resnik. 1996. Selectional Constraints: An
Information-Theoretic Model and its Computational
Realization. Cognition, 61:127?159, November.
Mats Rooth, Stefan Riezler, Detlef Prescher, Glenn
Carroll, and Franz Beil. 1999. Inducing a semanti-
cally annotated lexicon via em-based clustering. In
37th Annual Meeting of the ACL.
Amnon Shashua and Tamir Hazan. 2005. Non-
negative tensor factorization with applications to
statistics and computer vision. In ICML ?05: Pro-
ceedings of the 22nd international conference on
89
Machine learning, pages 792?799, New York, NY,
USA. ACM.
L.R. Tucker. 1966. Some mathematical notes on three-
mode factor analysis. Psychometrika, 31:279?311.
Peter D. Turney. 2007. Empirical evaluation of four
tensor decomposition algorithms. Technical Report
ERB-1152, National Research Council, Institute for
Information Technology.
Gertjan van Noord. 2006. At Last Parsing Is Now
Operational. In Piet Mertens, Cedrick Fairon, Anne
Dister, and Patrick Watrin, editors, TALN06. Verbum
Ex Machina. Actes de la 13e conference sur le traite-
ment automatique des langues naturelles, pages 20?
42, Leuven.
M. Alex O. Vasilescu and Demetri Terzopoulos. 2002.
Multilinear analysis of image ensembles: Tensor-
faces. In ECCV, pages 447?460.
Michael E. Wall, Andreas Rechtsteiner, and Luis M.
Rocha, 2003. Singular Value Decomposition and
Principal Component Analysis, chapter 5, pages 91?
109. Kluwel, Norwell, MA, Mar.
90
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1012?1022,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Latent Vector Weighting for Word Meaning in Context
Tim Van de Cruys
RCEAL
University of Cambridge
tv234@cam.ac.uk
Thierry Poibeau
LaTTiCe, UMR8094
CNRS & ENS
thierry.poibeau@ens.fr
Anna Korhonen
Computer Laboratory & RCEAL
University of Cambridge
alk23@cam.ac.uk
Abstract
This paper presents a novel method for the com-
putation of word meaning in context. We make
use of a factorization model in which words, to-
gether with their window-based context words
and their dependency relations, are linked to
latent dimensions. The factorization model al-
lows us to determine which dimensions are im-
portant for a particular context, and adapt the
dependency-based feature vector of the word
accordingly. The evaluation on a lexical substi-
tution task ? carried out for both English and
French ? indicates that our approach is able to
reach better results than state-of-the-art meth-
ods in lexical substitution, while at the same
time providing more accurate meaning repre-
sentations.
1 Introduction
According to the distributional hypothesis of meaning
(Harris, 1954), words that occur in similar contexts
tend to be semantically similar. In the spirit of this by
now well-known adage, numerous algorithms have
sprouted up that try to capture the semantics of words
by looking at their distribution in texts, and compar-
ing those distributions in a vector space model.
Up till now, the majority of computational ap-
proaches to semantic similarity represent the mean-
ing of a word as the aggregate of the word?s contexts,
and hence do not differentiate between the different
senses of a word. The meaning of a word, however, is
largely dependent on the particular context in which
it appears. Take for example the word work in sen-
tences (1) and (2).
(1) The painter?s recent work is a classic example
of art brut.
(2) Equal pay for equal work!
The meaning of work is quite different in both sen-
tences. In sentence (1), work refers to the product of a
creative act, viz. a painting. In sentence (2), it refers
to labour carried out as a source of income. The
NLP community?s standard answer to the ambiguity
problem has always been some flavour of word sense
disambiguation (WSD), which in its standard form
boils down to choosing the best-possible fit from a
pre-defined sense inventory. In recent years, it has
become clear that this is in fact a very hard task to
solve for computers and humans alike (Ide and Wilks,
2006; Erk et al, 2009; Erk, 2010).
With these findings in mind, researchers have
started looking at different methods to tackle lan-
guage?s ambiguity, ranging from coarser-grained
sense inventories (Hovy et al, 2006) and graded
sense assignment (Erk and McCarthy, 2009), over
word sense induction (Schu?tze, 1998; Pantel and Lin,
2002; Agirre et al, 2006), to the computation of indi-
vidual word meaning in context (Erk and Pado?, 2008;
Thater et al, 2010; Dinu and Lapata, 2010). This
research inscribes itself in the same line of thought,
in which the meaning disambiguation of a word is
not just the assignment of a pre-defined sense; in-
stead, the original meaning representation of a word
is adapted ?on the fly?, according to ? and specifi-
cally tailored for ? the particular context in which
it appears. To be able to do so, we build a factor-
ization model in which words, together with their
window-based context words and their dependency
1012
relations, are linked to latent dimensions. The factor-
ization model allows us to determine which dimen-
sions are important for a particular context, and adapt
the dependency-based feature vector of the word ac-
cordingly. The evaluation on a lexical substitution
task ? carried out for both English and French ? indi-
cates that our method is able to reach better results
than state-of-the-art methods in lexical substitution,
while at the same time providing more accurate mean-
ing representations.
The remainder of this paper is organized as follows.
In section 2, we present some earlier work that is
related to the research presented here. Section 3
describes the methodology of our method, focusing
on the factorization model, and the computation of
meaning in context. Section 4 presents a thorough
evaluation on a lexical substitution task, both for
English and French. The last section then draws
conclusions, and presents a number of topics that
deserve further exploration.
2 Related work
One of the best known computational models of se-
mantic similarity is latent semantic analysis ? LSA
(Landauer and Dumais, 1997; Landauer et al, 1998).
In LSA, a term-document matrix is created, that con-
tains the frequency of each word in a particular doc-
ument. This matrix is then decomposed into three
other matrices with a mathematical factorization tech-
nique called singular value decomposition (SVD).
The most important dimensions that come out of the
SVD are said to represent latent semantic dimensions,
according to which nouns and documents can be rep-
resented more efficiently. Our model also applies
a factorization technique (albeit a different one) in
order to find a reduced semantic space.
The nature of a word?s context is a determining
factor in the kind of the semantic similarity that is in-
duced. A broad context window (e.g. a paragraph or
document) yields broad, topical similarity, whereas
a small context window yields tight, synonym-like
similarity. This has lead a number of researchers
(e.g. Lin (1998)) to use the dependency relations that
a particular word takes part in as context features.
An overview of dependency-based semantic space
models is given in Pado? and Lapata (2007).
A number of researchers have exploited the no-
tion of context to differentiate between the different
senses of a word in an unsupervised way (a task la-
beled word sense induction or WSI). Schu?tze (1998)
proposed a context-clustering approach, in which
context vectors are created for the different instances
of a particular word, and those contexts are grouped
into a number of clusters, representing the different
senses of the word. The context vectors are rep-
resented as second-order co-occurrences (i.e. the
contexts of the target word are similar if the words
they in turn co-occur with are similar). Van de Cruys
(2008) proposed a model for sense induction based
on latent semantic dimensions. Using a factorization
technique based on non-negative matrix factorization,
the model induces a latent semantic space according
to which both dependency features and broad con-
textual features are classified. Using the latent space,
the model is able to discriminate between different
word senses. Our approach makes use of a simi-
lar factorization model, but we extend the approach
with a probabilistic framework that is able to adapt
the original vector according to the context of the
instance.
Recently, a number of models emerged that aim
to model the individual meaning of words in context.
Erk and Pado? (2008, 2009) make use of selectional
preferences to express the meaning of a word in con-
text; the meaning of a word in the presence of an
argument is computed by multiplying the word?s vec-
tor with a vector that captures the inverse selectional
preferences of the argument. Thater et al (2009) and
Thater et al (2010) extend the approach based on se-
lectional preferences by incorporating second-order
co-occurrences in their model; their model allows
first-order co-occurrences to act as a filter upon the
second-order vector space, which allows for the com-
putation of meaning in context.
Erk and Pado? (2010) propose an exemplar-based
approach, in which the meaning of a word in context
is represented by the activated exemplars that are
most similar to it. And Mitchell and Lapata (2008)
propose a model for vector composition, focusing on
the different functions that might be used to combine
the constituent vectors. Their results indicate that
a model based on pointwise multiplication achieves
better results than models based on vector addition.
Finally, Dinu and Lapata (2010) propose a proba-
bilistic framework that models the meaning of words
1013
as a probability distribution over latent dimensions
(?senses?). Contextualized meaning is then mod-
eled as a change in the original sense distribution.
The model presented in this paper bears some resem-
blances to their approach; however, while their ap-
proach computes the contextualized meaning directly
within the latent space, our model exploits the latent
space to determine the features that are important
for a particular context, and adapt the original (out-
of-context) dependency-based feature vector of the
target word accordingly. This allows for a more pre-
cise and more distinct computation of word meaning
in context. Secondly, Dinu and Lapata use window-
based context features to build their latent model,
while our approach combines both window-based
and dependency-based features.
3 Methodology
3.1 Non-negative Matrix Factorization
Our model uses non-negative matrix factorization
(Lee and Seung, 2000) in order to find latent dimen-
sions. There are a number of reasons to prefer NMF
over the better known singular value decomposition
used in LSA. First of all, NMF allows us to mini-
mize the Kullback-Leibler divergence as an objec-
tive function, whereas SVD minimizes the Euclidean
distance. The Kullback-Leibler divergence is better
suited for language phenomena. Minimizing the Eu-
clidean distance requires normally distributed data,
and language phenomena are typically not normally
distributed. Secondly, the non-negative nature of
the factorization ensures that only additive and no
subtractive relations are allowed. This proves partic-
ularly useful for the extraction of semantic dimen-
sions, so that the NMF model is able to extract much
more clear-cut dimensions than an SVD model. And
thirdly, the non-negative property allows the resulting
model to be interpreted probabilistically, which is not
straightforward with an SVD factorization.
The key idea is that a non-negative matrix A is
factorized into two other non-negative matrices, W
and H
Ai?j ?Wi?kHk?j (1)
where k is much smaller than i, j so that both in-
stances and features are expressed in terms of a few
components. Non-negative matrix factorization en-
forces the constraint that all three matrices must be
non-negative, so all elements must be greater than or
equal to zero.
Using the minimization of the Kullback-Leibler di-
vergence as an objective function, we want to find the
matrices W and H for which the Kullback-Leibler
divergence between A and WH (the multiplication
of W and H) is the smallest. This factorization is
carried out through the iterative application of update
rules. Matrices W and H are randomly initialized,
and the rules in 2 and 3 are iteratively applied ? alter-
nating between them. In each iteration, each vector is
adequately normalized, so that all dimension values
sum to 1.
Ha? ? Ha?
?
iWia
Ai?
(WH)i??
kWka
(2)
Wia ?Wia
?
?Ha?
Ai?
(WH)i??
vHav
(3)
3.2 Combining syntax and context words
Using an extension of non-negative matrix factor-
ization (Van de Cruys, 2008), it is possible to
jointly induce latent factors for three different modes:
words, their window-based context words, and their
dependency-based context features. As input to
the algorithm, three matrices are constructed that
capture the pairwise co-occurrence frequencies for
the different modes. The first matrix contains co-
occurrence frequencies of words cross-classified
by dependency-based features, the second matrix
contains co-occurrence frequencies of words cross-
classified by words that appear in the word?s context
window, and the third matrix contains co-occurrence
frequencies of dependency-based features cross-
classified by co-occurring context words. NMF is
then applied to the three matrices, and the separate
factorizations are interleaved (i.e. the results of the
former factorization are used to initialize the factor-
ization of the next matrix). A graphical represen-
tation of the interleaved factorization algorithm is
given in figure 1.
When the factorization is finished, the three dif-
ferent modes (words, window-based context words
and dependency-based features) are all represented
according to a limited number of latent factors.
1014
= xW H
= xV G
= xU F
j
i
s
k
i
j
kAwords xdependency relations
B
words xcontext words
Ccontext words xdependency relations
k k
k
k
i
j
i
s
j
s s
Figure 1: A graphical representation of the interleaved
NMF
The factorization that comes out of the NMF model
can be interpreted probabilistically (Gaussier and
Goutte, 2005; Ding et al, 2008). More specifically,
we can transform the factorization into a standard
latent variable model of the form
p(wi, dj) =
K?
z=1
p(z)p(wi|z)p(dj |z) (4)
by introducing two K ?K diagonal scaling matrices
X and Y, such that Xkk = ?iWik and Ykk =?
jHkj . The factorization WH can then be rewritten
as
WH = (WX?1X)(YY?1H)
= (WX?1)(XY)(Y?1H)
(5)
such that WX?1 represents p(wi|z), (Y?1H)T rep-
resents p(dj |z), and XY represents p(z). Using
Bayes? theorem, it is now straightforward to deter-
mine p(z|wi) and p(z|dj).
p(z|wi) =
p(wi|z)p(z)
p(wi)
(6)
p(z|dj) =
p(dj |z)p(z)
p(dj)
(7)
3.3 Meaning in Context
3.3.1 Overview
Using the results of the factorization model de-
scribed above, we can now adapt a word?s feature vec-
tor according to the context in which it appears. Intu-
itively, the contextual features of the word (i.e. the
window-based context words or dependency-based
context features) pinpoint the important semantic di-
mensions of the particular instance, creating a proba-
bility distribution over latent factors. For a number of
context words of a particular instance, we determine
the probability distribution over latent factors given
the context, p(z|C), as the average of the context
words? probability distributions over latent factors
(equation 8).
p(z|C) =
?
wi?C p(z|wi)
|C| (8)
The probability distribution over latent factors
given a number of dependency-based context features
can be computed in a similar fashion, replacing wi
with dj . Additionally, this step allows us to combine
both windows-based context words and dependency-
based context features in order to determine the latent
probability distribution (e.g. by taking a linear com-
bination).
The resulting probability distribution over latent
factors can be interpreted as a semantic fingerprint of
the passage in which the target word appears. Using
this fingerprint, we can now determine a new prob-
ability distribution over dependency features given
the context.
p(d|C) = p(z|C)p(d|z) (9)
The last step is to weight the original probability
vector of the word according to the probability vector
of the dependency features given the word?s context,
by taking the pointwise multiplication of probability
vectors p(d|wi) and p(d|C).
p(d|wi, C) = p(d|wi) ? p(d|C) (10)
Note that this final step is a crucial one in our ap-
proach. We do not just build a model based on latent
factors, but we use the latent factors to determine
which of the features in the original word vector are
the salient ones given a particular context. This al-
lows us to compute an accurate adaptation of the
original word vector in context.
Also note the resemblance to Mitchell and Lap-
ata?s best scoring vector composition model which,
likewise, uses pointwise multiplication. However,
1015
the model presented here has two advantages. First
of all, it allows to take multiple context features into
account, each of which contributes to the probability
distribution over latent factors. Secondly, the target
word and its features do not need to live in the same
vector space (i.e. they do not need to be defined ac-
cording to the same features), as the connections and
the appropriate weightings are determined through
the latent model.
3.3.2 Example
Let us exemplify the procedure with an example.
Say we want to compute the distributionally similar
words to the noun record in the context of example
sentences (3) and (4).
(3) Jack is listening to a record.
(4) Jill updated the record.
First, we extract the context features for both in-
stances, in this case C1 = {listen?1prep(to)} for sen-
tence (3), and C2 = {update?1obj} for sentence (4).1Next, we compute p(z|C1) and p(z|C2) ? the proba-
bility distributions over latent factors given the con-
text ? by averaging over the latent probability dis-
tributions of the individual context features.2 Using
these probability distributions over latent factors, we
can now determine the probability of each depen-
dency feature given the different contexts ? p(d|C1)
and p(d|C2).
The former step yields a general probability dis-
tribution over dependency features that tells us how
likely a particular dependency feature is given the
context that our target word appears in. Our last step
is now to weight the original probability vector of
the target word (the aggregate of dependency-based
context features over all contexts of the target word)
according to the new distribution given the context
in which the target word appears. For the first sen-
tence, features associated with the music sense of
record (or more specifically, the dependency features
associated with latent factors that are related to the
feature {listen?1prep(to)}) will be emphasized, while
1In this example we use dependency features, but the compu-
tations are similar for window-based context words.
2In this case, the sets of context features contain only one
item, so the average probability distribution of the sets is just the
latent probability distribution of their respective item.
features associated with unrelated latent factors are
leveled out. For the second sentence, features that
are associated with the administrative sense of record
(dependency features associated with latent factors
that are related to the feature {update?1obj}) are em-phasized, while unrelated features are played down.
We can now return to our original matrix A and
compute the top similar words for the two adapted
vectors of record given the different contexts, which
yields the results presented below.
1. recordN , C1: album, song, recording, track, cd
2. recordN , C2: file, datum, document, database,
list
4 Evaluation
In this section, we present a thorough evaluation of
the method described above, and compare it with
related methods for meaning computation in context.
In order to test the applicability of the method to
multiple languages, we present evaluation results for
both English and French.
4.1 Datasets
For English, we make use of the SEMEVAL 2007 En-
glish Lexical Substitution task (McCarthy and Nav-
igli, 2007; McCarthy and Navigli, 2009). The task?s
goal is to find suitable substitutes for a target word in
a particular context. The complete data set contains
200 target words (about 50 for each part of speech,
viz. nouns, verbs, adjectives, and adverbs). Each
target word occurs in 10 different sentences, which
yields a total of 2000 sentences. Five annotators pro-
vided suitable substitutes for each target word in the
different contexts.
For French, we developed a small-scale lexical sub-
stitution task ourselves, closely following the guide-
lines of the original English task. We manually se-
lected 10 ambiguous French nouns, and for each noun
we selected 10 different sentences from the FRWaC
corpus (Baroni et al, 2009). Four different native
French speakers were then asked to provide suitable
substitutes for the nouns in context.3
3The task is provided as supplementary material to this paper;
it is also available from the first author?s website.
1016
4.2 Implementational details
The model for English has been trained on part of the
UKWaC corpus (Baroni et al, 2009), covering about
500M words. The corpus has been part of speech
tagged and lemmatized with Stanford Part-Of-Speech
Tagger (Toutanova and Manning, 2000; Toutanova
et al, 2003), and parsed with MaltParser (Nivre et
al., 2006) trained on sections 2-21 of the Wall Street
Journal section of the Penn Treebank extended with
about 4000 questions from the QuestionBank4, so
that dependency triples could be extracted. The sen-
tences of the English lexical substitution task have
been tagged, lemmatized and parsed in the same way.
The model for French has been trained on the French
version of Wikipedia (? 100M words), parsed with
the FRMG parser (Villemonte de La Clergerie, 2010)
for French.
For English, we built different models for each
part of speech (nouns, verbs, adjectives and adverbs),
which yields four models in total. For each model, the
matrices needed for our interleaved NMF factoriza-
tion are extracted from the corpus. The noun model,
for example, was built using 5K nouns, 80K depen-
dency relations, and 2K context words5 (excluding
stop words) with highest frequency in the training
set, which yields matrices of 5K nouns ? 80K de-
pendency relations, 5K nouns ? 2K context words,
and 80K dependency relations ? 2K context words.
The models for the three other parts of speech were
constructed in a similar vein. For French, we only
constructed a model for nouns, as our lexical substi-
tution task for French is limited to this part of speech.
The interleaved NMF model was carried out using
K = 600 (the number of factorized dimensions in
the model), and applying 100 iterations.6 The inter-
leaved NMF algorithm was implemented in Matlab;
the preprocessing scripts and scripts for vector com-
putation in context were written in Python. Cosine
was used as a similarity measure.
4http://maltparser.org/mco/english_
parser/engmalt.html
5We used a fairly large, paragraph-like window of four sen-
tences.
6We experimented with different values (in the range 300?
1500) for K, but the models did not seem to improve much
beyond K = 600; hence, we stuck with 600 factors, due to
speed and memory advantages of a lower number of factors.
4.3 Measures
Up till now, most researchers have interpreted the
lexical substitution task as a ranking problem, in
which the possible substitutes are given beforehand
and the goal is to rank the substitutes according to
their suitability in a particular context, so that sound
substitutes are given a higher rank than their non-
suitable counterparts. This means that all possible
substitutes for a given target word (extracted from the
gold standard) are lumped together, and the system
then has to produce a ranking for the complete set of
substitutes.
We also adopt this approach in our evaluation
framework, but we complement it with the original
evaluation measures of the lexical substitution task,
in which the system is not given a list of possible sub-
stitutes beforehand, but has to come up with the suit-
able candidates itself. This is a much harder task, but
we believe that such an approach is more compelling
in assessing the system?s ability to induce a proper
meaning representation for word usage in context.
We coin the former approach paraphrase ranking,
and the latter one paraphrase induction. In the next
paragraphs, we will describe the actual evaluation
measures that have been used for both approaches.
Paraphrase ranking Following Dinu and Lapata
(2010), we compare the ranking produced by our
model with the gold standard ranking using Kendall?s
?b (which is adjusted for ties). For reasons of com-
parison, we also compute general average precision
(GAP, Kishida (2005)), which was used by Erk and
Pado? (2010) and Thater et al (2010) to evaluate their
rankings. Differences between models are tested for
significance using stratified shuffling (Yeh, 2000),
using a standard number of 10000 iterations.
We compare the results for paraphrase ranking to
two different baselines. The first baseline is a ran-
dom one, in which the gold standard is compared
to an arbitrary ranking. The second baseline is a
dependency-based vector space model that does not
take the context of the particular instance into ac-
count (and thus returns the same ranking for each
instance of the target word). This is a fairly competi-
tive baseline, as noted by other researchers (Erk and
Pado?, 2008; Thater et al, 2009; Dinu and Lapata,
2010).
1017
Paraphrase induction To evaluate the system?s
ability to come up with suitable substitutes from
scratch, we use the measures designed to evaluate
systems that took part in the original English lexical
substitution task (McCarthy and Navigli, 2007). Two
different measures were used, which were coined
best and out-of-ten (oot). The strict best measure
allows the system to give as many candidate substi-
tutes as it considers appropriate, but the credit for
each correct substitute is divided by the total number
of guesses. Recall is then calculated as the average
annotator response frequency of substitutes found by
the system over all items T.
Rbest =
?
s?M?G f(s)
|M | ??s?G f(s)
(11)
whereM is the system?s candidate list7,G is the gold-
standard data, and f(s) is the annotator response
frequency of the candidate.
The out-of-ten measure is more liberal; it allows
the system to give up to ten substitutes, and the credit
for each correct substitute is not divided by the total
number of guesses. The more liberal measure was
introduced to account for the fact that the lexical
substitution task?s gold standard is susceptible to a
considerate amount of variation, and there is only a
limited number of annotators.
P10 =
?
s?M?G f(s)?
s?G f(s)
(12)
where M is the system?s list of 10 candidates, and
G and f(s) are the same as above. Because we only
use the best guess with Rbest, the two measures are
exactly the same except for the number of candidates
M .
4.4 Results
4.4.1 English
Table 1 presents the paraphrase ranking results of
our approach, comparing them to the two baselines
and to a number of previous approaches to meaning
computation in context.
The first two models represent our baselines. The
first baseline is the random baseline, where the can-
didate substitutes are ranked randomly (?b close to
7In our evaluations, we calculate best using the system?s best
guess only, so the candidate list contains only one item.
model ?b GAP
random -0.61 29.98
vectordep 16.57 45.08
EP09 ? 32.2 H
EP10 ? 39.9 H
TFP ? 45.94H
DL 16.56 41.68
NMFcontext 20.64?? 47.60??
NMFdep 22.49?? 48.97??
NMFc+d 22.59?? 49.02??
Table 1: Kendall?s ?b and GAP paraphrase ranking scores
for the English lexical substitution task. Scores marked
with ?H? are copied from the authors? respective papers.
Scores marked with ???? are statistically significant with
p < 0.01 compared to the second baseline.
zero indicates that there is no correlation). The sec-
ond baseline is a standard dependency-based vector
space model, which yields the same ranking for all
instances of a target word. Note that the second base-
line is a rather competitive one.
The next four models represent previous ap-
proaches to meaning computation in context. EP09
is Erk and Pado?s (2009) selectional preference ap-
proach; EP10 is Erk and Pado?s (2010) exemplar-
based approach; TFP stands for Thater et al?s (2010)
approach; and DL is Dinu and Lapata?s (2010) latent
modeling approach. The results are reproduced from
their respective papers, except for Dinu and Lapata?s
approach, which we reimplemented ourselves.8 Note
that the reproduced results (EP09, EP10 and TFP) are
not entirely comparable, because the authors only use
a subset of the lexical substitution task.
The last three models are instantiations of our ap-
proach: NMFcontext is a model that uses window-
based context features, NMFdep is a model that uses
dependency-based context features, and NMFc+d is
a model that uses a linear combination of window-
based and dependency-based context features, giving
equal weight to both.
The three instantiations of our approach reach bet-
ter results than all previous approaches. Moreover,
our approach is the only one able to significantly
8The original paper reports a slightly lower ?b of 16.01 for
their best scoring model.
1018
beat our second (competitive) baseline of a stan-
dard dependency-based vector model. Comparing
our three instantiations, the model that combines
window-based context and dependency-based con-
text scores best, closely followed by the dependency-
based model. The model that only uses window-
based context gets the lowest score of the three, but
is still fairly competitive compared to the previous
approaches. The differences between the models are
statistically significant (p < 0.01), except for the
difference between NMFdep and NMFc+d.
model n v a r
vectordep 15.85 11.68 16.71 25.29
NMFcontext 20.58 16.24 21.00 27.22
NMFdep 21.96 17.33 24.57 28.16
NMFc+d 22.68 17.47 23.84 28.66
Table 2: Kendall?s ?b paraphrase ranking scores for the
English lexical substitution task across different parts of
speech
Table 2 shows the performance of the three model
instantiations on paraphrase ranking across different
parts of speech. The results largely confirm tenden-
cies reported by other researchers (cfr. Dinu and
Lapata (2010)), viz. that verbs are the most difficult,
followed by nouns and adjectives. These parts of
speech also benefit the most from the use of a contex-
tualized model. Adverbs are easier, but there is less
to be gained from using contextualized models.
model Rbest P10
vectordep 8.78 30.21
DL 1.06 7.59
KU 20.65 46.15
IRST2 20.33 68.90
NMFcontext 8.81 30.49
NMFdep 7.73 26.92
NMFc+d 8.96 29.26
Table 3: Rbest and P10 paraphrase induction scores for
the English lexical substitution task
Table 3 shows the performance of the different
models on the paraphrase induction task. Note
once again that our baseline vectordep ? a simple
dependency-based vector space model ? is a highly
competitive one. NMFcontext and NMFc+d are able to
reach marginally better results, but the differences are
not statistically significant. However, all of our mod-
els are able to reach much better results than Dinu
and Lapata?s approach. The results indicate that our
approach, after vector adaptation in context, is still
able to provide accurate similarity calculations across
the complete word space. While other algorithms are
able to rank candidate substitutes at the expense of
accurate similarity calculations, our approach is able
to do both. This is one of the important advantages
of our approach.
For reasons of comparison, we also included the
scores of the best performing models that partici-
pated in the SEMEVAL 2007 lexical substitution task
(KU (Yuret, 2007) and IRST2 (Giuliano et al, 2007),
which got the best scores for Rbest and P10, respec-
tively). These models reach better scores compared
to our models. Note, however, that all participants
of the SEMEVAL 2007 lexical substitution task relied
on a predefined sense inventory (i.e. WordNet, or
a machine readable thesaurus). Our system, on the
other hand, induces paraphrases in a fully unsuper-
vised way. To our knowledge, this is the first time a
fully unsupervised system is tested on the paraphrase
induction task.
model n v a r
vectordep 31.66 23.53 29.91 38.43
NMFcontext 33.73?? 25.21? 28.58 36.45
NMFdep 31.40 25.97?? 20.56 31.48
NMFc+d 33.37? 25.99?? 24.20 35.81
Table 4: P10 paraphrase induction scores for the English
lexical substitution task across different parts of speech.
Scores marked with ???? and ??? are statistically significant
with respectively p < 0.01 and p < 0.05 compared to the
baseline.
Table 4 presents the results for paraphrase induc-
tion (oot) across the different parts of speech. The
results indicate that paraphrase induction works best
for nouns and verbs, with statistically significant im-
provements over the baseline. The differences among
the models themselves are not significant. Adjectives
and adverbs yield lower scores, indicating that their
1019
contextualization yields less precise vectors for mean-
ing computation. Note, however, that the NMFcontext
model is still quite apt for meaning computation,
yielding results that are only slightly lower than the
dependency-based vector space model.
4.4.2 French
This section presents the results on the French lex-
ical substitution task. Table 5 presents the results for
paraphrase ranking, while table 6 shows the models?
performance on the paraphrase induction task.
model Kendall?s ?b GAP
vectordep 7.79 36.46
DL 17.99 41.73
NMFcontext 18.63 44.96
NMFdep 17.15 44.66
NMFc+d 18.40 43.14
Table 5: Kendall?s ?b and GAP paraphrase ranking scores
for the French lexical substitution task
The results for paraphrase ranking in French (ta-
ble 5) show similar tendencies as the results for En-
glish: all of our models are able to improve signifi-
cantly over the dependency-based vector space base-
line. Note, however, thar our models generally score
a bit lower compared to the English results. This drop
in performance is not present for Dinu and Lapata?s
model. The difference might be due to the differ-
ence in corpora size: for the method to operate at full
power, we need to make a good estimate of the co-
occurrences of three modes (words, window-based
context words and dependency-based features), and
thus our methods requires a significant amount of
data. Nevertheless, our approach still yields the best
results, with NMFcontext as the best scoring model.
Finally, the results for paraphrase induction in
French (table 6) interestingly show a significant and
large improvement over the baseline. The improve-
ments indicate once again that the models are able
to carry out precise similarity computations over the
whole word space, while at the same time providing
an adequately adapted contextualized meaning vector.
Dinu and Lapata?s model, which performs similarity
calculations in the latent space, is not able to provide
accurate word vectors, and thus perform worse at the
paraphrase induction task.
model Rbest P10
vectordep 6.38 24.43
DL 0.50 5.34
NMFcontext 10.71 31.42
NMFdep 9.65 28.52
NMFc+d 10.64 35.32
Table 6: Rbest and P10 paraphrase induction scores for
the French lexical substitution task
5 Conclusion
In this paper, we presented a novel method for the
modeling of word meaning in context. We make use
of a factorization model based on non-negative ma-
trix factorization, in which words, together with their
window-based context words and their dependency
relations, are linked to latent dimensions. The factor-
ization model allows us to determine which particular
dimensions are important for a target word in a partic-
ular context. A key feature of the algorithm is that we
adapt the original dependency-based feature vector
of the target word through the latent semantic space.
By doing so, our model is able to make accurate simi-
larity calculations for word meaning in context across
the whole word space. Our evaluation shows that the
approach presented here is able to improve upon the
state-of-the art performance on paraphrase ranking.
Moreover, our approach scores well for both para-
phrase ranking and paraphrase induction, whereas
previous approaches only seem capable of improving
performance on the former task at the expense of the
latter.
During our research, a number of topics surfaced
that we consider worth exploring in the future. First
of all, we would like to further investigate the opti-
mal configuration for combining window-based and
dependency-based contexts. At the moment, the per-
formance of the combined model does not yield a
uniform picture. The results might improve further
if window-based context and dependency-based con-
text are combined in an optimal way. Secondly, we
would like to subject our approach to further evalu-
ation, in particular on a number of different evalua-
tion tasks, such as semantic compositionality. And
thirdly, we would like to transfer the general idea
of the approach presented in this paper to a tensor-
1020
based framework (which is able to capture the multi-
way co-occurrences of words, together with their
window-based and dependency-based context fea-
tures, in a natural way) and investigate whether such
a framework proves beneficial for the modeling of
word meaning in context.
Acknowledgements
The work reported in this paper was funded by
the Isaac Newton Trust (Cambridge, UK), the
EU FP7 project ?PANACEA?, the EPSRC grant
EP/G051070/1 and the Royal Society (UK).
References
Eneko Agirre, David Mart??nez, Oier Lo?pez de Lacalle, and
Aitor Soroa. 2006. Two graph-based algorithms for
state-of-the-art wsd. In Proceedings of the Empirical
Methods in Natural Language Processing (EMNLP)
Conference, pages 585?593, Sydney, Australia.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and
Eros Zanchetta. 2009. The wacky wide web: A
collection of very large linguistically processed web-
crawled corpora. Language Resources and Evaluation,
43(3):209?226.
Chris Ding, Tao Li, and Wei Peng. 2008. On the equiv-
alence between non-negative matrix factorization and
probabilistic latent semantic indexing. Computational
Statistics & Data Analysis, 52(8):3913?3927.
Georgiana Dinu and Mirella Lapata. 2010. Measuring
distributional similarity in context. In Proceedings of
the 2010 Conference on Empirical Methods in Natural
Language Processing, pages 1162?1172, Cambridge,
MA, October.
Katrin Erk and Diana McCarthy. 2009. Graded word
sense assignment. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
pages 440?449, Suntec, Singapore.
Katrin Erk and Sebastian Pado?. 2008. A structured vector
space model for word meaning in context. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, pages 897?906, Waikiki,
Hawaii, USA.
Katrin Erk and Sebastian Pado?. 2009. Paraphrase assess-
ment in structured vector space: Exploring parameters
and datasets. In Proceedings of the Workshop on Geo-
metrical Models of Natural Language Semantics, pages
57?65, Athens, Greece.
Katrin Erk and Sebastian Pado?. 2010. Exemplar-based
models for word meaning in context. In Proceedings of
the ACL 2010 Conference Short Papers, pages 92?97,
Uppsala, Sweden.
Katrin Erk, Diana McCarthy, and Nicholas Gaylord. 2009.
Investigations on word senses and word usages. In
Proceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP, pages 10?18.
Katrin Erk. 2010. What is word meaning, really? (and
how can distributional models help us describe it?). In
Proceedings of the 2010 Workshop on GEometrical
Models of Natural Language Semantics, pages 17?26.
Eric Gaussier and Cyril Goutte. 2005. Relation between
PLSA and NMF and implications. In Proceedings of
the 28th annual international ACM SIGIR conference
on Research and development in information retrieval,
pages 601?602, Salvador, Brazil.
Claudio Giuliano, Alfio Gliozzo, and Carlo Strapparava.
2007. Fbk-irst: Lexical substitution task exploiting
domain and syntagmatic coherence. In Proceedings of
the Fourth International Workshop on Semantic Evalu-
ations, pages 145?148.
Zellig S. Harris. 1954. Distributional structure. Word,
10(23):146?162.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
the 90% solution. In Proceedings of the Human Lan-
guage Technology Conference of the NAACL, pages
57?60, New York, New York, USA.
Nancy Ide and Yorick Wilks. 2006. Making Sense About
Sense. In Word Sense Disambiguation: Algorithms
And Applications, chapter 3. Springer, Dordrecht.
Kazuaki Kishida. 2005. Property of average precision and
its generalization: An examination of evaluation indi-
cator for information retrieval experiments. Technical
report, National Institute of Informatics.
Thomas Landauer and Susan Dumais. 1997. A solution
to Plato?s problem: The Latent Semantic Analysis the-
ory of the acquisition, induction, and representation of
knowledge. Psychology Review, 104:211?240.
Thomas Landauer, Peter Foltz, and Darrell Laham. 1998.
An Introduction to Latent Semantic Analysis. Dis-
course Processes, 25:295?284.
Daniel D. Lee and H. Sebastian Seung. 2000. Algorithms
for non-negative matrix factorization. In Advances in
Neural Information Processing Systems 13, pages 556?
562.
Dekang Lin. 1998. Automatic retrieval and clustering of
similar words. In Proceedings of the 36th Annual Meet-
ing of the Association for Computational Linguistics
and 17th International Conference on Computational
Linguistics (COLING-ACL98), Volume 2, pages 768?
774, Montreal, Quebec, Canada.
Diana McCarthy and Roberto Navigli. 2007. SemEval-
2007 task 10: English lexical substitution task. In
1021
Proceedings of the 4th International Workshop on Se-
mantic Evaluations (SemEval-2007), pages 48?53.
Diana McCarthy and Roberto Navigli. 2009. The En-
glish lexical substitution task. Language resources and
evaluation, 43(2):139?159.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. proceedings of ACL-
08: HLT, pages 236?244.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006. Malt-
parser: A data-driven parser-generator for dependency
parsing. In Proceedings of LREC-2006, pages 2216?
2219.
Sebastian Pado? and Mirella Lapata. 2007. Dependency-
based construction of semantic space models. Compu-
tational Linguistics, 33(2):161?199.
Patrick Pantel and Dekang Lin. 2002. Discovering word
senses from text. In ACM SIGKDD International Con-
ference on Knowledge Discovery and Data Mining,
pages 613?619, Edmonton, Alberta, Canada.
Hinrich Schu?tze. 1998. Automatic word sense discrimi-
nation. Computational Linguistics, 24(1):97?123.
Stefan Thater, Georgiana Dinu, and Manfred Pinkal. 2009.
Ranking paraphrases in context. In Proceedings of the
2009 Workshop on Applied Textual Inference, pages
44?47, Suntec, Singapore.
Stefan Thater, Hagen Fu?rstenau, and Manfred Pinkal.
2010. Contextualizing semantic representations using
syntactically enriched vector models. In Proceedings of
the 48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 948?957, Uppsala, Sweden.
Kristina Toutanova and Christopher D. Manning. 2000.
Enriching the knowledge sources used in a maximum
entropy part-of-speech tagger. In Proceedings of the
Joint SIGDAT Conference on Empirical Methods in
Natural Language Processing and Very Large Corpora
(EMNLP/VLC-2000), pages 63?70.
Kristina Toutanova, Dan Klein, Christopher Manning, and
Yoram Singer. 2003. Feature-rich part-of-speech tag-
ging with a cyclic dependency network. In Proceedings
of HLT-NAACL 2003, pages 252?259.
Tim Van de Cruys. 2008. Using three way data for word
sense discrimination. In Proceedings of the 22nd In-
ternational Conference on Computational Linguistics
(Coling 2008), pages 929?936, Manchester.
Eric Villemonte de La Clergerie. 2010. Building factor-
ized TAGs with meta-grammars. In Proceedings of
the 10th International Conference on Tree Adjoining
Grammars and Related Formalisms (TAG+10), pages
111?118, New Haven, Connecticut, USA.
Alexander Yeh. 2000. More accurate tests for the statis-
tical significance of result differences. In Proceedings
of the 18th conference on Computational linguistics,
pages 947?953, Saarbru?cken, Germany.
Deniz Yuret. 2007. Ku: Word sense disambiguation by
substitution. In Proceedings of the Fourth International
Workshop on Semantic Evaluations, pages 207?213.
1022
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 26?35,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
A Neural Network Approach to Selectional Preference Acquisition
Tim Van de Cruys
IRIT & CNRS
Toulouse, France
tim.vandecruys@irit.fr
Abstract
This paper investigates the use of neural
networks for the acquisition of selectional
preferences. Inspired by recent advances
of neural network models for NLP applica-
tions, we propose a neural network model
that learns to discriminate between felici-
tous and infelicitous arguments for a par-
ticular predicate. The model is entirely un-
supervised ? preferences are learned from
unannotated corpus data. We propose two
neural network architectures: one that han-
dles standard two-way selectional prefer-
ences and one that is able to deal with
multi-way selectional preferences. The
model?s performance is evaluated on a
pseudo-disambiguation task, on which it
is shown to achieve state of the art perfor-
mance.
1 Introduction
Predicates often have a semantically motivated pref-
erence for particular arguments. Compare for ex-
ample the sentences in (1) and (2).
(1) The vocalist sings a ballad.
(2) The exception sings a tomato.
Most language users would have no problems ac-
cepting the first sentence as well-formed: a vocalist
can be expected to sing, and a ballad is something
that can be sung. The same language users, how-
ever, would likely consider the second sentence to
be ill-formed: an exception is not supposed to sing,
nor is a tomato something that is typically sung.
Within the field of natural language processing,
this inclination of predicates to select for particular
arguments is known as selectional preference.
The automatic acquisition of selectional prefer-
ences has been a popular research subject within
the field of natural language processing. An auto-
matically acquired selectional preference resource
is a versatile tool for numerous NLP applications,
such as semantic role labeling (Gildea and Jurafsky,
2002), word sense disambiguation (McCarthy and
Carroll, 2003), and metaphor processing (Shutova
et al., 2013).
Models for selectional preference need to ade-
quately deal with the consequences of Zipf?s law:
language is inherently sparse, and the majority of
language utterances occur very infrequently. As
a consequence, models that are based on corpus
data need to properly generalize beyond the mere
co-occurrence frequencies of sparse corpus data,
taking into account the semantic similarity of both
predicates and arguments. Researchers have come
up with various approaches to this generalization
step. Earlier approaches to selectional preference
acquisition mostly rely on hand-crafted resources
such as WordNet (Resnik, 1996; Li and Abe, 1998;
Clark and Weir, 2001), while later approaches tend
to take advantage of unsupervised learning machin-
ery, such as latent variable models (Rooth et al.,
1999;
?
O S?eaghdha, 2010) and distributional simi-
larity metrics (Erk, 2007; Pad?o et al., 2007).
This paper investigates the use of neural net-
works for the acquisition of selectional preferences.
Inspired by recent advances of neural network mod-
els for NLP applications (Collobert and Weston,
2008; Mikolov et al., 2013), we propose a neural
network model that learns to discriminate between
felicitous and infelicitous arguments for a particu-
lar predicate. The model is entirely unsupervised ?
preferences are learned from unannotated corpus
data. Positive training instances are constructed
from attested corpus data, while negative instances
are constructed from randomly corrupted instances.
We propose two neural network architectures: one
that handles standard two-way selectional prefer-
ences and one that is able to deal with multi-way
selectional preferences, where the interaction be-
26
tween multiple verb arguments is taken into ac-
count. The model?s performance is evaluated on a
pseudo-disambiguation task, on which it is shown
to achieve state of the art performance.
The contributions of this paper are twofold. First
of all, we apply and evaluate a neural network ap-
proach to the problem of standard (two-way) se-
lectional preference acquisition. Selectional pref-
erence acquisition using neural networks has not
yet been explored in the literature. Secondly, we
propose a novel network architecture and training
objective for the acquisition of multi-way selec-
tional preferences, where the interaction between
a verb and its various arguments is captured at the
same time.
The remainder of this paper is as follows. Sec-
tion 2 first discusses related work with respect to se-
lectional preference acquisition and neural network
modeling. Section 3 describes our neural network
architecture and its training procedure. Section 4
evaluates the model?s performance, comparing it
to other existing models for selectional preference
acquisition. Finally, section 5 concludes and indi-
cates a number of avenues for future work.
2 Related Work
2.1 Selectional preferences
One of the first approaches to the automatic induc-
tion of selectional preferences from corpora was
the one by Resnik (1996). Resnik (1996) relies
on WordNet synsets in order to generate gener-
alized noun clusters. The selectional preference
strength of a specific verb v in a particular relation
is calculated by computing the Kullback-Leibler
divergence between the cluster distribution of the
verb and the prior cluster distribution.
S
R(v)
=
?
c
p(c|v) log
p(c|v)
p(c)
(1)
where c stands for a noun cluster, and R stands for a
given predicate-argument relation. The selectional
association of a particular noun cluster is then the
contribution of that cluster to the verb?s preference
strength.
A
R(v,c)
=
p(c|v) log
p(c|v)
p(c)
S
R(v)
(2)
The model?s generalization relies entirely on Word-
Net, and there is no generalization among the verbs.
Other researchers have equally relied on Word-
Net in order to generalize over arguments. Li and
Abe (1998) use the principle of Minimum Descrip-
tion Length in order to find a suitable generalization
level within the lexical WordNet hierarchy. A same
intuition is used by Clark and Weir (2001), but they
use hypothesis testing instead to find the appro-
priate level of generalization. A recent approach
that makes use of WordNet (in combination with
Bayesian modeling) is the one by
?
O S?eaghdha and
Korhonen (2012).
Most researchers, however, acknowledge the
shortcomings of hand-crafted resources, and fo-
cus on the acquisition of selectional preferences
from corpus data. Rooth et al. (1999) propose an
Expectation-Maximization (EM) clustering algo-
rithm for selectional preference acquisition based
on a probabilistic latent variable model. The idea
is that both predicate v and argument o are gen-
erated from a latent variable c, where the latent
variables represent clusters of tight verb-argument
interactions.
p(v,o) =
?
c?C
p(c,v,o) =
?
c?C
p(c)p(v|c)p(o|c) (3)
The use of latent variables allows the model to
generalize to predicate-argument tuples that have
not been seen during training. The latent variable
distribution ? and the probabilities of predicates
and argument given the latent variables ? are au-
tomatically induced from data using EM. We will
compare against their model for evaluation pur-
poses.
Erk (2007) and Erk et al. (2010) describe a
method that uses corpus-driven distributional simi-
larity metrics for the induction of selectional pref-
erences. The key idea is that a predicate-argument
tuple (v,o) is felicitous if the predicate v appears
in the training corpus with arguments o
?
that are
similar to o, i.e.
S(v,o) =
?
o
?
?O
v
wt(v,o
?
)
Z(v)
? sim(o,o
?
) (4)
where O
v
represents the set of arguments that have
been attested with predicate v, wt(?) represents an
appropriate weighting function (such as the fre-
quency of the (v,o
?
) tuple), and Z is a normaliza-
tion factor. We equally compare to their model for
evaluation purposes.
Bergsma et al. (2008) present a discriminative
approach to selectional preference acquisition. Pos-
itive examples are taken from observed predicate-
27
argument pairs, while negative examples are con-
structed from unobserved combinations. An SVM
classifier is used to distinguish the positive from the
negative instances. The training procedure used in
their model is based on an intuition that is similar
to ours, although it is implemented using different
techniques.
A number of researchers presented models that
are based on the framework of topic modeling.
?
O
S?eaghdha (2010) describes three models for selec-
tional preference induction based on Latent Dirich-
let Allocation, which model the selectional pref-
erence of a predicate and a single argument. Rit-
ter et al. (2010) equally present a selectional pref-
erence model based on topic modeling, but they
tackle multi-way selectional preferences (of transi-
tive predicates, which take two arguments) instead.
Finally, in previous work (Van de Cruys, 2009)
we presented a model for multi-way selectional
preference induction based on tensor factorization.
Three-way co-occurrences of subjects, verbs, and
objects are represented as a three-way tensor (the
generalization of a matrix), and a latent factoriza-
tion model is applied in order to generalize to
unseen instances. We will compare our neural
network based-approach for multi-way selectional
preference acquisition to this tensor-based factor-
ization model.
2.2 Neural networks
In the last few years, neural networks have become
increasingly popular in NLP applications. In partic-
ular, neural language models (Bengio et al., 2003;
Mnih and Hinton, 2007; Collobert and Weston,
2008) have demonstrated impressive performance
at the task of language modeling. By incorporating
distributed representations for words that model
their similarity, neural language models are able
to overcome the problem of data sparseness that
standard n-gram models are confronted with. Also
related to our work is the approach by Tsubaki et
al. (2013), who successfully use a neural network
to model co-compositionality.
Our model for selectional preference acquisition
uses a network architecture that is similar to the
abovementioned models. Its training objective is
also similar to the ranking-loss training objective
proposed by Collobert and Weston (2008), but we
present a novel, modified version in order to deal
with multi-way selectional preferences.
3 Methodology
3.1 Neural network architecture
Our model computes the score for a predicate i
and an argument j as follows. First, the selectional
preference tuple (i, j) is represented as the concate-
nation of the vectors v
i
and o
j
, i.e.
x= [v
i
,o
j
] (5)
Vectors v
i
and o
j
are extracted from two embedding
matrices, V ? R
N?I
(the predicate matrix, where I
represents the number of elements in the predicate
vocabulary) and O ? R
N?J
(the argument matrix,
where J represents the number of elements in the
argument vocabulary). N is a parameter setting of
the model, representing the vector size of the em-
beddings. Matrices V and O will be automatically
learned during training.
Vector x then serves as input vector to our neural
network. We use a feed-forward neural network
architecture with one hidden layer:
a
1
= f (W
1
x+b
1
) (6)
y = W
2
a
1
(7)
where x ? R
2N
is our input vector, a
1
? R
H
repre-
sents the activation of the hidden layer with H hid-
den nodes, W
1
? R
H?2N
and W
2
? R
1?H
respec-
tively represent the first and second layer weights,
b
1
represents the first layer?s bias, f (?) represents
the element-wise activation function tanh, and y is
our final selectional preference score. The left-hand
picture of figure 1 gives a graphical representation
of our standard neural network architecture.
3.2 Training the network
A proper estimation of a neural network?s param-
eters requires a large amount of training data. To
be able to use non-annotated corpus data for train-
ing, we use the method proposed by Collobert and
Weston (2008). The authors present a method for
training a neural network language model from un-
labeled data by corrupting actual attested n-grams
with a random word. They then define a ranking-
type cost function, which allows the network to
learn to discriminate between good and bad word
sequences. We adopt the same method for our se-
lectional preference model as follows.
Let (i, j) be our proper, attested predicate-
argument tuple. The goal of our model is to dis-
criminate the correct tuple (i, j) from other, non-
attested tuples (i, j
?
), in which the correct predicate
28
V i
O jj W2W1
a1x y
V i
O jk W2W1
a1
x
yS jj
Figure 1: Neural network architectures for selectional preference acquisition. The left-hand picture shows
the architecture for two-way selectional preferences, the right-hand picture shows the architecture for
three-way selectional preferences. In both cases, vector x is constructed from the appropriate predicate
and argument vectors from the embedding matrices, and fed forward through the network to yield a
preference score y.
j has been replaced with a random predicate j
?
. We
require the score for the correct tuple to be larger
than the score for the corrupt tuple by a margin
of one. For one tuple (i, j), this corresponds to
minimizing the objective function in (8)
?
j
?
?J
max(0,1?g[(i, j)]+g[(i, j
?
)]) (8)
where J represents the predicate vocabulary, and
g[?] represents our neural network scoring function
presented in the previous section.
In line with Collobert and Weston (2008), the
gradient of the objective function is sampled by
randomly picking one corrupt argument j
?
from the
argument vocabulary for each attested predicate-
argument tuple (i, j). The derivative of the cost
with respect to the model?s parameters (weight ma-
trices W
1
and W
2
, bias vector b
1
, and embedding
matrices V and O) is computed, and the appropriate
parameters are updated through backpropagation.
3.3 Multi-way selectional preferences
The model presented in the previous section is
only able to deal with two-way selectional pref-
erences. In this section, we present an extension of
the model that is able to handle multi-way selec-
tional preferences.
1
1
We exemplify the model using three-way selectional pref-
erences for transitive predicates, but the model can be straight-
forwardly generalized to other multi-way selectional prefer-
ences.
In order to model the selectional preference of a
transitive verb for its subject and direct object, we
start out in a similar fashion to the two-way case.
Instead of having only one embedding matrix, we
now have two embedding matrices S ? R
N?J
and
O?R
N?K
, representing the two different argument
slots of a transitive predicate. Our input vector can
now be represented as
x= (v
i
,s
j
,o
k
) (9)
Note that x ? R
3N
and W
1
? R
H?3N
. The rest of
our neural network architecture stays exactly the
same. The right-hand picture of figure 1 presents a
graphical representation.
For the multi-way case, we present an adapted
version of the training objective. Given an attested
subject-verb-object tuple (i, j,k), the goal of our
network is now to discriminate this correct tuple
from other, corrupted tuples (i, j,k
?
), (i, j
?
,k) and
(i, j
?
,k
?
), where the correct arguments have been
replaced by random subjects j
?
and random objects
k
?
. Note that we do not only want the network
to learn the infelicity of tuples in which both the
subject and object slot are corrupted; we also want
our network to learn the infelicity of tuples in which
either the subject or object slot is corrupt, while the
other slot contains the correct, attested argument.
This leads us to the objective function represented
in (10).
29
?k
?
?K
max(0,1?g[(i, j,k)]+g[(i, j,k
?
)])
+
?
j
?
?J
max(0,1?g[(i, j,k)]+g[(i, j
?
,k)])
+
?
j
?
?J
k
?
?K
max(0,1?g[(i, j,k)]+g[(i, j
?
,k
?
)]) (10)
As in the two-way case, the gradient of the objec-
tive function is sampled by randomly picking one
corrupted subject j
?
and one corrupted object k
?
for
each tuple (i, j,k). All of the model?s parameters
are again updated through backpropagation.
4 Evaluation
4.1 Implementational details
We evaluate our neural network approach to se-
lectional preference acquisition using verb-object
tuples for the two-way model, and subject-verb-
object tuples for the multi-way model.
Our model has been applied to English, using the
UKWaC corpus (Baroni et al., 2009), which covers
about 2 billion words of web text. The corpus
has been part of speech tagged and lemmatized
with Stanford Part-Of-Speech Tagger (Toutanova
et al., 2003), and parsed with MaltParser (Nivre
et al., 2006), so that dependency tuples could be
extracted.
For the two-way model, we select all verbs and
objects that appear within a predicate-argument re-
lation with a frequency of at least 50. This gives
us a total of about 7K verbs and 30K objects. For
the multi-way model, we select the 2K most fre-
quent verbs, together with the 10K most frequent
subjects and the 10K most frequent objects (that
appear within a transitive frame).
All words are converted to lowercase. We use
the lemmatized forms, and only keep those forms
that contain alphabetic characters. Furthermore,
we require each tuple to appear at least three times
in the corpus.
We set N, the size of our embedding matrices, to
50, and H, the number of units in the hidden layer,
to 100. Following Huang et al. (2012), we use
mini-batch L-BFGS (Liu and Nocedal, 1989) with
1000 pairs of good and corrupt tuples per batch for
training, and train for 10 epochs.
4.2 Evaluation Setup
4.2.1 Task
Our models are quantitatively evaluated using a
pseudo-disambiguation task (Rooth et al., 1999),
which bears some resemblance to our training pro-
cedure. The task provides an adequate test of the
generalization capabilities of our models. For the
two-way case, the task is to judge which object (o
or o
?
) is more likely for a particular verb v, where
(v,o) is a tuple attested in the corpus, and o
?
is a di-
rect object randomly drawn from the object vocab-
ulary. The tuple is considered correct if the model
prefers the attested tuple (v,o) over (v,o
?
). For the
three-way case, the task is to judge which subject
(s or s
?
) and direct object (o or o
?
) are more likely
for a particular verb v, where (v,s,o) is the attested
tuple, and s
?
and o
?
are a random subject and object
drawn from their respective vocabularies. The tu-
ple is considered correct if the model prefers the
attested tuple (v,s,o) over the alternatives (v,s,o
?
),
(v,s
?
,o), and (v,s
?
,o
?
). Tables 1 and 2 respectively
show a number of examples from the two-way and
three-way pseudo-disambiguation task.
v o o
?
perform play geometry
buy wine renaissance
read introduction peanut
Table 1: Pseudo-disambiguation examples for two-
way verb-object tuples
v s o s
?
o
?
win team game diversity egg
publish government document grid priest
develop company software breakfast landlord
Table 2: Pseudo-disambiguation examples for
three-way subject-verb-object tuples
The models are evaluated using 10-fold cross
validation. All tuples from our corpus are randomly
divided into 10 equal parts. Next, for each fold, 9
parts are used for training, and the remaining part
is used for testing. In order to properly test the
generalization capability of our models, we make
sure that all instances of a particular tuple appear in
one part only. This way, we make sure that tuples
used for testing are never seen during training.
For the two-way model, our corpus consists of
about 70M tuple instances (1.9M types), so in each
30
fold, about 63M tuple instances are used for train-
ing and about 7M (190K types) are used for testing.
For the three-way model, our corpus consists of
about 5,5M tuple instances (750K types), so in
each fold, about 5M tuples are used for training
and about 500K (75K types) are used for testing.
Note that our training procedure is instance-based,
while our evaluation is type-based: during training,
the neural network sees a tuple as many times as it
appears in the training set, while for testing each
individual tuple is only evaluated once.
4.2.2 Comparison models
We compare our neural network model to a number
of other models for selectional preference acquisi-
tion.
For the two-way case, we compare our model
to the EM-based clustering technique presented
by Rooth et al. (1999),
2
and to Erk et al.?s (2010)
similarity-based model. For Rooth et al.?s model,
we set the number of latent factors to 50. Us-
ing a larger number of latent factors does not in-
crease performance. For Erk et al.?s model, we
create a dependency-based similarity model from
the UKWaC corpus using our 30K direct objects
as instances and 100K dependency relations as
features. The resulting matrix is weighted using
pointwise mutual information (Church and Hanks,
1990). Similarity values are computed using cosine.
Furthermore, we use a sampling procedure in the
testing phase: we sample 5000 predicate-argument
pairs for each fold, as testing Erk et al.?s model on
the complete test sets proved prohibitively expen-
sive.
For the three-way case, we compare our model
to the tensor factorization model we developed in
previous work (Van de Cruys, 2009). We set the
number of latent factors to 300.
3
4.3 Results
4.3.1 Two-way model
Table 3 compares the results of our neural network
architecture for two-way selectional preferences to
the results of Rooth et al.?s (1999) model and Erk
et al.?s (2010) model.
2
Our own implementation of Rooth et al.?s (1999) al-
gorithm is based on non-negative matrix factorization (Lee
and Seung, 2000). Non-negative matrix factorization with
Kullback-Leibler divergence has been shown to minimize the
same objective function as EM (Li and Ding, 2006).
3
The best scoring model presented by Van de Cruys (2009)
also uses 300 latent factors; using more factors does not im-
prove the results.
model accuracy (??? )
Rooth et al. (1999) .720 ? .002
Erk et al. (2010) .887 ? .004
2-way neural network .880 ? .001
Table 3: Comparison of model results for two-way
selectional preference acquisition ? mean accuracy
and standard deviations of 10-fold cross-validation
results
The results indicate that our neural network ap-
proach outperforms Rooth et al.?s (1999) method
by a large margin (16%). Clearly, the neural net-
work architecture is able to model selectional pref-
erences more profoundly than Rooth et al.?s latent
variable approach. The difference between the
models is highly statistically significant (paired
t-test, p < .01), as the standard deviations already
indicate.
Erk et al.?s model reaches a slightly better score
than our model, and this result is also statistically
significant (paired t-test, p < .01). However, Erk et
al.?s model does not provide full coverage, whereas
the other two models are able to compute scores
for all pairs in the test set. In addition, Erk et al.?s
model is much more expensive to compute. Our
model computes selectional preference scores for
the test set in a matter of seconds, whereas for
Erk et al.?s model, we ended up sampling from
the test set, as computing preference values for the
complete test set proved prohibitively expensive.
4.3.2 Three-way model
Table 4 compares the results of our neural network
architecture for three-way selectional preference
acquisition to the results of the tensor-based factor-
ization method (Van de Cruys, 2009).
model accuracy (??? )
Van de Cruys (2009) .874 ? .001
3-way neural network .889 ? .001
Table 4: Comparison of model results for three-way
selectional preference acquisition ? mean accuracy
and standard deviations of 10-fold cross-validation
results
The results indicate that the neural network ap-
proach slightly outperforms the tensor-based factor-
ization method. Again the model difference is sta-
31
tistically significant (paired t-test, p< 0.01). Using
our adapted training objective, the neural network
is clearly able to learn a rich model of three-way
selectional preferences, reaching state of the art
performance.
4.4 Examples
We conclude our results section by briefly present-
ing a number of examples that illustrate the kind
of semantics present in our models. Similar to neu-
ral language models, the predicate and argument
embedding matrices of our neural network con-
tain distributed word representations, that capture
the similarity of predicates and arguments to other
words.
Tables 5 and 6 contain a number of nearest neigh-
bour similarity examples for predicate and argu-
ments from our two-way neural network model.
The nearest neighbours were calculated using stan-
dard cosine similarity.
DRINK PROGRAM INTERVIEW FLOOD
SIP RECOMPILE RECRUIT INUNDATE
BREW UNDELETE PERSUADE RAVAGE
MINCE CODE INSTRUCT SUBMERGE
FRY IMPORT PESTER COLONIZE
Table 5: Nearest neighbours of 4 verbs, calculated
using the distributed word representations of em-
bedding matrix V from our two-way neural net-
work model
Table 5 indicates that the network is effectively
able to capture a semantics for verbs. The first
column ? verbs similar to DRINK ? all have to do
with food consumption. The second column con-
tains verbs related to computer programming. The
third column is related to human communication;
and the fourth column seems to illustrate the net-
work?s comprehension of FLOOD having to do with
invasion and water.
PAPER RASPBERRY SECRETARY DESIGNER
BOOK COURGETTE PRESIDENT PLANNER
JOURNAL LATTE MANAGER PAINTER
ARTICLE LEMONADE POLICE SPECIALIST
CODE OATMEAL EDITOR SPEAKER
Table 6: Nearest neighbours of 4 direct objects, cal-
culated using the distributed word representations
of embedding matrix O from our two way neural
network model
Similarly, table 6 shows the network?s ability to
capture the meaning of nouns that appear as direct
objects to the verbs. Column one contains things
that can be read. Column two contains things that
can be consumed. Column three seems to hint at
supervising professions, while column four seems
to capture creative professions.
A similar kind of semantics is present in the em-
bedding matrices of the three-way neural network
model. Tables 7, 8, and 9 again illustrate this using
word similarity calculations.
SEARCH DIMINISH CONFIGURE PROSECUTE
CLICK LESSEN AUTOMATE CRITICISE
BROWSE DISTORT SCROLL URGE
SCROLL HEIGHTEN PROGRAM DEPLORE
UPLOAD DEGRADE INSTALL CONDEMN
Table 7: Nearest neighbours of 4 verbs, calculated
using the distributed word representations of em-
bedding matrix V from our three-way neural net-
work model
Table 7 shows the network?s verb semantics for
the three-way case. The first column is related to
internet usage, the second column contains verbs
of scalar change, column three is again related to
computer usage, and column four seems to capture
?mending? verbs.
FLOWER COLLEGE PRESIDENT SONG
FISH UNIVERSITY BUSH FILM
BIRD INSTITUTE BLAIR ALBUM
SUN DEPARTMENT MP PLAY
TREE CENTRE CHAIRMAN MUSIC
Table 8: Nearest neighbours of 4 subjects, calcu-
lated using the distributed word representations of
embedding matrix S from our three way neural
network model
Table 8 illustrates the semantics for the subject
slot of our three-way model. The first column cap-
tures nature terms, the second column contains
university-related terms, the third column contains
politicians/government terms, and the fourth col-
umn contains art expressions.
Finally, table 9 demonstrates the semantics of
our three-way model?s object slot. Column one
generally contains housing terms, column two con-
tains various locations, column three contains din-
ing occasions, and column four contains textual
expressions.
32
WALL PARK LUNCH THESIS
FLOOR STUDIO DINNER QUESTIONNAIRE
CEILING VILLAGE MEAL DISSERTATION
ROOF HALL BUFFET PERIODICAL
METRE MUSEUM BREAKFAST DISCOURSE
Table 9: Nearest neighbours of 4 direct objects, cal-
culated using the distributed word representations
of embedding matrix O from our three way neural
network model
Note that the embeddings for the subject and
the object slot is different, although they mostly
contain the same words. This allows the model to
capture specific semantic characteristics for words
given their argument position. Virus, for example,
is in subject position more similar to active words
like animal, whereas in object position, it is more
similar to passive words like cell, device. Similarly,
mouse in subject position tends to be similar to
words like animal, rat whereas in object position it
is similar to words like web, browser.
These examples, although anecdotal, illustrate
that our neural network model is able to capture a
rich semantics for predicates and arguments, which
subsequently allows the network to make accurate
predictions with regard to selectional preference.
5 Conclusion and future work
In this paper, we presented a neural network ap-
proach to the acquisition of selectional preferences.
Inspired by recent work on neural language models,
we proposed a neural network model that learns
to discriminate between felicitous and infelicitous
arguments for a particular predicate. The model is
entirely unsupervised, as preferences are learned
from unannotated corpus data. Positive training
instances are constructed from attested corpus data,
while negative instances are constructed from ran-
domly corrupted instances. Using designated net-
work architectures, we are able to handle stan-
dard two-way selectional preferences as well as
multi-way selectional preferences. A quantitative
evaluation on a pseudo-disambiguation task shows
that our models achieve state of the art perfor-
mance. The results for our two-way neural network
are on a par with Erk et al.?s (2010) similarity-
based approach, while our three-way neural net-
work slightly outperforms the tensor-based factor-
ization model (Van de Cruys, 2009) for multi-way
selectional preference induction.
We conclude with a number of issues for future
work. First of all, we would like to investigate how
our neural network approach might be improved by
incorporating information from other sources. In
particular, we think of initializing our embedding
matrices with distributed representations that come
from a large-scale neural language model (Mikolov
et al., 2013). We also want to further investigate
the advantages and disadvantages of having dif-
ferent embedding matrices for different argument
positions in our multi-way neural network. In our
results section, we demonstrated that such an ap-
proach allows for more flexibility, but it also adds
a certain level of redundancy. We want to inves-
tigate the benefit of our approach, compared to a
model that shares the distributed word representa-
tion among different argument positions. Finally,
we want to investigate more advanced neural net-
work architectures for the acquisition of selectional
preferences. In particular, neural tensor networks
(Yu et al., 2013) have recently demonstrated im-
pressive results in related fields like speech recogni-
tion, and might provide the necessary machinery to
model multi-way selectional preferences in a more
profound way.
References
Marco Baroni, Silvia Bernardini, Adriano Ferraresi,
and Eros Zanchetta. 2009. The wacky wide web: A
collection of very large linguistically processed web-
crawled corpora. Language Resources and Evalua-
tion, 43(3):209?226.
Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, 3:1137?1155.
Shane Bergsma, Dekang Lin, and Randy Goebel. 2008.
Discriminative learning of selectional preference
from unlabeled text. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 59?68. Association for Computa-
tional Linguistics.
Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information & lexicogra-
phy. Computational Linguistics, 16(1):22?29.
Stephen Clark and David Weir. 2001. Class-based
probability estimation using a semantic hierarchy.
In Proceedings of the second meeting of the North
American Chapter of the Association for Computa-
tional Linguistics on Language technologies, pages
95?102. Association for Computational Linguistics.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
33
neural networks with multitask learning. In Pro-
ceedings of the 25th international conference on Ma-
chine learning, pages 160?167. ACM.
Katrin Erk, Sebastian Pad?o, and Ulrike Pad?o. 2010. A
flexible, corpus-driven model of regular and inverse
selectional preferences. Computational Linguistics,
36(4):723?763.
Katrin Erk. 2007. A simple, similarity-based model
for selectional preferences. In Proceedings of the
45th Annual Meeting of the Association of Compu-
tational Linguistics, pages 216?223, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational linguis-
tics, 28(3):245?288.
Eric H. Huang, Richard Socher, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Improving word
representations via global context and multiple word
prototypes. In Annual Meeting of the Association
for Computational Linguistics (ACL).
Daniel D. Lee and H. Sebastian Seung. 2000. Al-
gorithms for non-negative matrix factorization. In
Advances in Neural Information Processing Systems
13, pages 556?562.
Hang Li and Naoki Abe. 1998. Generalizing case
frames using a thesaurus and the MDL principle.
Computational linguistics, 24(2):217?244.
Tao Li and Chris Ding. 2006. The relationships among
various nonnegative matrix factorization methods
for clustering. In Data Mining, 2006. ICDM?06.
Sixth International Conference on, pages 362?371.
IEEE.
Dong C. Liu and Jorge Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical programming, 45(1-3):503?528.
Diana McCarthy and John Carroll. 2003. Disam-
biguating nouns, verbs, and adjectives using auto-
matically acquired selectional preferences. Compu-
tational Linguistics, 29(4):639?654.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. In ICLR 2013.
Andriy Mnih and Geoffrey Hinton. 2007. Three new
graphical models for statistical language modelling.
In Proceedings of the 24th international conference
on Machine learning, pages 641?648. ACM.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006.
Maltparser: A data-driven parser-generator for de-
pendency parsing. In Proceedings of LREC-2006,
pages 2216?2219.
Diarmuid
?
O S?eaghdha and Anna Korhonen. 2012.
Modelling selectional preferences in a lexical hier-
archy. In Proceedings of the First Joint Conference
on Lexical and Computational Semantics-Volume 1:
Proceedings of the main conference and the shared
task, and Volume 2: Proceedings of the Sixth Inter-
national Workshop on Semantic Evaluation, pages
170?179. Association for Computational Linguis-
tics.
Diarmuid
?
O S?eaghdha. 2010. Latent variable mod-
els of selectional preference. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 435?444. Association for
Computational Linguistics.
Sebastian Pad?o, Ulrike Pad?o, and Katrin Erk. 2007.
Flexible, corpus-based modelling of human plausi-
bility judgements. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 400?409,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Philip Resnik. 1996. Selectional constraints: An
information-theoretic model and its computational
realization. Cognition, 61:127?159, November.
Alan Ritter, Mausam, and Oren Etzioni. 2010. A la-
tent dirichlet allocation method for selectional pref-
erences. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguistics,
pages 424?434, Uppsala, Sweden, July. Association
for Computational Linguistics.
Mats Rooth, Stefan Riezler, Detlef Prescher, Glenn Car-
roll, and Franz Beil. 1999. Inducing a semanti-
cally annotated lexicon via em-based clustering. In
Proceedings of the 37th annual meeting of the As-
sociation for Computational Linguistics on Compu-
tational Linguistics, pages 104?111. Association for
Computational Linguistics.
Ekaterina Shutova, Simone Teufel, and Anna Korho-
nen. 2013. Statistical metaphor processing. Compu-
tational Linguistics, 39(2):301?353.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of HLT-NAACL 2003, pages 252?
259.
Masashi Tsubaki, Kevin Duh, Masashi Shimbo, and
Yuji Matsumoto. 2013. Modeling and learning se-
mantic co-compositionality through prototype pro-
jections and neural networks. In Proceedings of
the 2013 Conference on Empirical Methods in Nat-
ural Language Processing, pages 130?140, Seattle,
Washington, USA, October. Association for Compu-
tational Linguistics.
Tim Van de Cruys. 2009. A non-negative tensor fac-
torization model for selectional preference induction.
34
In Proceedings of the Workshop on Geometrical
Models of Natural Language Semantics, pages 83?
90, Athens, Greece, March. Association for Compu-
tational Linguistics.
Dong Yu, Li Deng, and Frank Seide. 2013. The
deep tensor neural network with applications to
large vocabulary speech recognition. IEEE Transac-
tions on Audio, Speech, and Language Processing,
21(2):388?396.
35
Proceedings of NAACL-HLT 2013, pages 1142?1151,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
A Tensor-based Factorization Model of Semantic Compositionality
Tim Van de Cruys
IRIT ? UMR 5505
CNRS
Toulouse, France
tim.vandecruys@irit.fr
Thierry Poibeau?
LaTTiCe ? UMR 8094
CNRS & ENS
Paris, France
thierry.poibeau@ens.fr
Anna Korhonen
Computer Laboratory & DTAL?
University of Cambridge
United Kingdom
anna.korhonen@cl.cam.ac.uk
Abstract
In this paper, we present a novel method for the
computation of compositionality within a distri-
butional framework. The key idea is that com-
positionality is modeled as a multi-way interac-
tion between latent factors, which are automat-
ically constructed from corpus data. We use
our method to model the composition of sub-
ject verb object triples. The method consists
of two steps. First, we compute a latent factor
model for nouns from standard co-occurrence
data. Next, the latent factors are used to induce
a latent model of three-way subject verb object
interactions. Our model has been evaluated on
a similarity task for transitive phrases, in which
it exceeds the state of the art.
1 Introduction
In the course of the last two decades, significant
progress has been made with regard to the automatic
extraction of lexical semantic knowledge from large-
scale text corpora. Most work relies on the distribu-
tional hypothesis of meaning (Harris, 1954), which
states that words that appear within the same contexts
tend to be semantically similar. A large number of
researchers have taken this dictum to heart, giving
rise to a plethora of algorithms that try to capture
the semantics of words by looking at their distribu-
tion in text. Up till now, however, most work on the
automatic acquisition of semantics only deals with
individual words. The modeling of meaning beyond
the level of individual words ? i.e. the combination
of words into larger units ? is to a large degree left
unexplored.
The principle of compositionality, often attributed
to Frege, is the principle that states that the meaning
of a complex expression is a function of the meaning
of its parts and the way those parts are (syntactically)
combined (Frege, 1892). It is the fundamental prin-
ciple that allows language users to understand the
meaning of sentences they have never heard before,
by constructing the meaning of the complex expres-
sion from the meanings of the individual words. Re-
cently, a number of researchers have tried to reconcile
the framework of distributional semantics with the
principle of compositionality (Mitchell and Lapata,
2008; Baroni and Zamparelli, 2010; Coecke et al,
2010; Socher et al, 2012). However, the absolute
gains of the systems remain a bit unclear, and a sim-
ple method of composition ? vector multiplication ?
often seems to produce the best results (Blacoe and
Lapata, 2012).
In this paper, we present a novel method for the
joint composition of a verb with its subject and di-
rect object. The key idea is that compositionality is
modeled as a multi-way interaction between latent
factors, which are automatically constructed from
corpus data. In order to adequately model the multi-
way interaction between a verb and its subject and
objects, a significant part of our method relies on
tensor algebra. Additionally, our method makes use
of a factorization model appropriate for tensors.
The remainder of the paper is structured as follows.
In section 2, we give an overview of previous work
that is relevant to the task of computing composition-
ality within a distributional framework. Section 3
presents a detailed description of our method, in-
cluding an overview of the necessary mathematical
1142
machinery. Section 4 illustrates our method with a
number of detailed examples. Section 5 presents a
quantitative evaluation, and compares our method
to other models of distributional compositionality.
Section 6, then, concludes and lays out a number of
directions for future work.
2 Previous Work
In recent years, a number of methods have been de-
veloped that try to capture compositional phenomena
within a distributional framework. One of the first
approaches to tackle compositional phenomena in a
systematic way is Mitchell and Lapata?s (2008) ap-
proach. They explore a number of different models
for vector composition, of which vector addition (the
sum of each feature) and vector multiplication (the
elementwise multiplication of each feature) are the
most important. They evaluate their models on a
noun-verb phrase similarity task, and find that the
multiplicative model yields the best results, along
with a weighted combination of the additive and mul-
tiplicative model.
Baroni and Zamparelli (2010) present a method
for the composition of adjectives and nouns. In their
model, an adjective is a linear function of one vector
(the noun vector) to another vector (the vector for the
adjective-noun pair). The linear transformation for a
particular adjective is represented by a matrix, and
is learned automatically from a corpus, using partial
least-squares regression.
Coecke et al (2010) present an abstract theoreti-
cal framework in which a sentence vector is a func-
tion of the Kronecker product of its word vectors,
which allows for greater interaction between the dif-
ferent word features. A number of instantiations of
the framework are tested experimentally in Grefen-
stette and Sadrzadeh (2011a) and Grefenstette and
Sadrzadeh (2011b). The key idea is that relational
words (e.g. adjectives or verbs) have a rich (multi-
dimensional) structure that acts as a filter on their
arguments. Our model uses an intuition similar to
theirs.
Socher et al (2012) present a model for composi-
tionality based on recursive neural networks. Each
node in a parse tree is assigned both a vector and
a matrix; the vector captures the actual meaning of
the constituent, while the matrix models the way
it changes the meaning of neighbouring words and
phrases.
Closely related to the work on compositionality
is research on the computation of word meaning in
context. Erk and Pado? (2008, 2009) make use of
selectional preferences to express the meaning of a
word in context; the meaning of a word in the pres-
ence of an argument is computed by multiplying the
word?s vector with a vector that captures the inverse
selectional preferences of the argument. Thater et
al. (2009, 2010) extend the approach based on se-
lectional preferences by incorporating second-order
co-occurrences in their model. And Dinu and La-
pata (2010) propose a probabilistic framework that
models the meaning of words as a probability distri-
bution over latent factors. This allows them to model
contextualized meaning as a change in the original
sense distribution. Dinu and Lapata use non-negative
matrix factorization (NMF) to induce latent factors.
Similar to their work, our model uses NMF ? albeit
in a slightly different configuration ? as a first step
towards our final factorization model.
In general, latent models have proven to be useful
for the modeling of word meaning. One of the best
known latent models of semantics is Latent Seman-
tic Analysis (Landauer and Dumais, 1997), which
uses singular value decomposition in order to auto-
matically induce latent factors from term-document
matrices. Another well known latent model of mean-
ing, which takes a generative approach, is Latent
Dirichlet Allocation (Blei et al, 2003).
Tensor factorization has been used before for the
modeling of natural language. Giesbrecht (2010)
describes a tensor factorization model for the con-
struction of a distributional model that is sensitive to
word order. And Van de Cruys (2010) uses a tensor
factorization model in order to construct a three-way
selectional preference model of verbs, subjects, and
objects. Our underlying tensor factorization ? Tucker
decomposition ? is the same as Giesbrecht?s; and
similar to Van de Cruys (2010), we construct a la-
tent model of verb, subject, and object interactions.
The way our model is constructed, however, is sig-
nificantly different. The former research does not
use any syntactic information for the construction
of the tensor, while the latter makes use of a more
restricted tensor factorization model, viz. parallel
factor analysis (Harshman and Lundy, 1994).
1143
The idea of modeling compositionality by means
of tensor (Kronecker) product has been proposed
in the literature before (Clark and Pulman, 2007;
Coecke et al, 2010). However, the method presented
here is the first that tries to capture compositional
phenomena by exploiting the multi-way interactions
between latent factors, induced by a suitable tensor
factorization model.
3 Methodology
3.1 Mathematical preliminaries
The methodology presented in this paper requires
a number of concepts and mathematical operations
from tensor algebra, which are briefly reviewed in
this section. The interested reader is referred to Kolda
and Bader (2009) for a more thorough introduction
to tensor algebra (including an overview of various
factorization methods).
A tensor is a multidimensional array; it is the gen-
eralization of a matrix to more than two dimensions,
or modes. Whereas matrices are only able to cap-
ture two-way co-occurrences, tensors are able to cap-
ture multi-way co-occurrences.1 Following prevail-
ing convention, tensors are represented by boldface
Euler script notation (X), matrices by boldface capi-
tal letters (X), vectors by boldface lower case letters
(x), and scalars by italic letters (x).
The n-mode product of a tensor X ? RI1?I2?...?IN
with a matrix U ? RJ?In is denoted by X?n U, and
is defined elementwise as
(X?n U)i1...in?1 jin+1...iN =
In
?
in=1
xi1i2...iN u jin (1)
The Kronecker product of matrices A ? RI?J and
B?RK?L is denoted by A?B. The result is a matrix
of size (IK)? (JL), and is defined by
A?B =
?
?
?
?
?
a11B a12B ? ? ? a1JB
a21B a22B ? ? ? a2JB
...
...
. . .
...
aI1B aI2B . . . aIJB
?
?
?
?
?
(2)
1In this research, we limit ourselves to three-way co-
occurrences of verbs, subject, and objects, modelled using a
three-mode tensor.
A special case of the Kronecker product is the
outer product of two vectors a ? RI and b ? RJ , de-
noted a?b. The result is a matrix A ? RI?J obtained
by multiplying each element of a with each element
of b.
Finally, the Hadamard product, denoted A ?B,
is the elementwise multiplication of two matrices
A ? RI?J and B ? RI?J , which produces a matrix
that is equally of size I? J.
3.2 The construction of latent noun factors
The first step of our method consists in the construc-
tion of a latent factor model for nouns, based on their
context words. For this purpose, we make use of non-
negative matrix factorization (Lee and Seung, 2000).
Non-negative matrix factorization (NMF) minimizes
an objective function ? in our case the Kullback-
Leibler (KL) divergence ? between an original matrix
VI?J and WI?KHK?J (the matrix multiplication of
matrices W and H) subject to the constraint that all
values in the three matrices be non-negative. Param-
eter K is set  I,J so that a reduction is obtained
over the original data. The factorization model is
represented graphically in figure 1.
= xV W H
k
k
noun
s
context words
noun
s
context words
Figure 1: Graphical representation of NMF
NMF can be computed fairly straightforwardly,
alternating between the two iterative update rules
represented in equations 3 and 4. The update rules
are guaranteed to converge to a local minimum in the
KL divergence.
Ha? ?Ha?
?i Wia
Vi?
(WH)i?
?k Wka
(3)
Wia?Wia
?? Ha?
Vi?
(WH)i?
?v Hav
(4)
3.3 Modeling multi-way interactions
In our second step, we construct a multi-way interac-
tion model for subject verb object (svo) triples, based
1144
on the latent factors induced in the first step. Our
latent interaction model is inspired by a tensor factor-
ization model called Tucker decomposition (Tucker,
1966), although our own model instantiation differs
significantly. In order to explain our method, we
first revisit Tucker decomposition, and subsequently
explain how our model is constructed.
3.3.1 Tucker decomposition
Tucker decomposition is a multilinear generaliza-
tion of the well-known singular value decomposition,
used in Latent Semantic Analysis. It is also known as
higher order singular value decomposition (HOSVD,
De Lathauwer et al (2000)). In Tucker decomposi-
tion, a tensor is decomposed into a core tensor, multi-
plied by a matrix along each mode. For a three-mode
tensor X ? RI?J?L, the model is defined as
X = G?1 A?2 B?3 C (5)
=
P
?
p=1
Q
?
q=1
R
?
r=1
gpqrap ?bq ? cr (6)
Setting P,Q,R I,J,L, the core tensor G repre-
sents a compressed, latent version of the original ten-
sor X; matrices A ?RI?P, B ?RJ?Q, and C ?RL?R
represent the latent factors for each mode, while
G ? RP?Q?R indicates the level of interaction be-
tween the different latent factors. Figure 2 shows a
graphical representation of Tucker decomposition.2
subjects
verb
s
object
s
=
object
s
k
k
k
verb
s
subjects
k
k
k
Figure 2: A graphical representation of Tucker decompo-
sition
2where P = Q = R = K, i.e. the same number of latent factors
K is used for each mode
3.3.2 Reconstructing a Tucker model from
two-way factors
Computing the Tucker decomposition of a tensor
is rather costly in terms of time and memory require-
ments. Moreover, the decomposition is not unique:
the core tensor G can be modified without affecting
the model?s fit by applying the inverse modification
to the factor matrices. These two drawbacks led us
to consider an alternative method for the construc-
tion of the Tucker model. Specifically, we consider
the factor matrices as given (as the output from our
first step), and proceed to compute the core tensor G.
Additionally, we do not use a latent representation
for the first mode, which means that the first mode is
represented by its original instances.
Our model can be straightforwardly applied to lan-
guage data. The core tensor G models the latent
interactions between verbs, subject, and objects. G
is computed by applying the n-mode product to the
appropriate mode of the original tensor (equation 7),
G=X?2 WT ?3 WT (7)
where XV?N?N is our original data tensor, consisting
of the weighted co-occurrence frequencies of svo
triples (extracted from corpus data), and WN?K is
our latent factor matrix for nouns. Note that we do
not use a latent representation for the verb mode. To
be able to efficiently compute the similarity of verbs
(both within and outside of compositional phrases),
only the subject and object mode are represented by
latent factors, while the verb mode is represented
by its original instances. This means that our core
tensor G will be of size V ?K?K.3 A graphical
representation is given in figure 3.
Note that both tensor X and factor matrices W are
non-negative, which means our core tensor G will
also be non-negative.
3.4 The composition of svo triples
In order to compute the composition of a particular
subject verb object triple ?s,v,o?, we first extract the
appropriate subject vector ws and object vector wo
(both of length K) from our factor matrix W, and
3It is straightforward to also construct a latent factor model
for verbs using NMF, and include it in the construction of our
core tensor; we believe such a model might have interesting
applications, but we save this as an exploration for future work.
1145
subjects
verb
s
object
s
=
object
s
k
k
verb
s
subjectskk
Figure 3: A graphical representation of our model instan-
tiation without the latent verb mode
compute the outer product of both vectors, resulting
in a matrix Y of size K?K.
Y = ws ?wo (8)
Our second and final step is then to weight the
original verb matrix Gv of latent interactions (the
appropriate verb slice of tensor G) with matrix Y,
containing the latent interactions of the specific sub-
ject and object. This is carried out by taking the
Hadamard product of Gv and Y.
Z = Gv ?Y (9)
4 Example
In this section, we present a number of example com-
putations that clarify how our model is able to capture
compositionality. All examples come from actual cor-
pus data, and are computed in a fully automatic and
unsupervised way.
Consider the following two sentences:
(1) The athlete runs a race.
(2) The user runs a command.
Both sentences contain the verb run, but they rep-
resent clearly different actions. When we compute
the composition of both instances of run with their
respective subject and object, we want our model to
show this difference.
To compute the compositional representation of
sentences (1) and (2), we proceed as follows. First,
we extract the latent vectors for subject and object
(wathlete and wrace for the first sentence, wuser and
wcommand for the second sentence) from matrix W.
Next, we compute the outer product of subject and
object ? wathlete ?wrace and wuser ?wcommand ? which
yields matrices Y?athlete,race? and Y?user,command?. By
virtue of the outer product, the matrices Y ? of size
K?K ? represent the level of interaction between the
latent factors of the subject and the latent factors of
the object. We can inspect these interactions by look-
ing up the factor pairs (i.e. matrix cells) with the high-
est values in the matrices Y. Table 1 presents the fac-
tor pairs with highest value for matrix Y?athlete,race?;
table 2 represents the factor pairs with highest value
for matrix Y?user,command?. In order to render the fac-
tors interpretable, we include the three most salient
words for the various factors (i.e. the words with the
highest value for a particular factor).
The examples in tables 1 and 2 give an impression
of the effect of the outer product: semantic features
of the subject combine with semantic features of the
object, indicating the extent to which these features
interact within the expression. In table 1, we notice
that animacy features (28, 195) and a sport feature
(25) combine with a ?sport event? feature (119). In
table 2, we see that similar animacy features (40,
195) and technological features (7, 45) combine with
another technological feature (89).
Similarly, we can inspect the latent interactions of
the verb run, which are represented in the tensor slice
Grun. Note that this matrix contains the verb seman-
tics computed over the complete corpus. The most
salient factor interactions for Grun are represented in
table 3.
Table 3 illustrates that different senses of the verb
run are represented within the matrix Grun. The first
two factor pairs hint at the ?organize? sense of the
verb (run a seminar). The third factor pair repre-
sents the ?transport? sense of the verb (the bus runs
every hour).4 And the fourth factor pair represents
the ?execute? or ?deploy? sense of run (run Linux,
run a computer program). Note that we only show
the factor pairs with the highest value; matrix G con-
tains a value for each pairwise combination of the
latent factors, effectively representing a rich latent
semantics for the verb in question.
The last step is to take the Hadamard product of
matrices Y with verb matrix G, which yields our final
4Obviously, hour is not an object of the verb, but due to
parsing errors it is thus represented.
1146
factors subject object value
?195,119? people (.008), child (.008), adolescent (.007) cup (.007), championship (.006), final (.005) .007
?25,119? hockey (.007), poker (.007), tennis (.006) cup (.007), championship (.006), final (.005) .004
?90,119? professionalism (.007), teamwork (.007), confi-
dence (.006)
cup (.007), championship (.006), final (.005) .003
?28,119? they (.004), pupil (.003), participant (.003) cup (.007), championship (.006), final (.005) .003
Table 1: Factor pairs with highest value for matrix Y?athlete,race?
factors subject object value
?7,89? password (.009), login (.007), username (.007) filename (.007), null (.006), integer (.006) .010
?40,89? anyone (.004), reader (.004), anybody (.003) filename (.007), null (.006), integer (.006) .007
?195,89? people (.008), child (.008), adolescent (.007) filename (.007), null (.006), integer (.006) .006
?45,89? website (.004), Click (.003), site (.003) filename (.007), null (.006), integer (.006) .006
Table 2: Factor pairs with highest value for matrix Y?user,command?
matrices, Zrun,?athlete,race? and Zrun,?user,command?. The
Hadamard product will act as a bidirectional filter
on the semantics of both the verb and its subject
and object: interactions of semantic features that are
present in both matrix Y and G will be highlighted,
while the other interactions are played down. The
result is a representation of the verb?s semantics tuned
to its particular subject-object combination. Note that
this final step can be viewed as an instance of function
application (Baroni and Zamparelli, 2010). Also
note the similarity to Grefenstette and Sadrzadeh?s
(2011a,2011b) approach, who equally make use of
the elementwise matrix product in order to weight
the semantics of the verb.
We can now go back to our original tensor G, and
compute the most similar verbs (i.e. the most similar
tensor slices) for our newly computed matrices Z.5
If we do this for matrix Zrun,?athlete,race?, our model
comes up with verbs finish (.29), attend (.27), and
win (.25). If, instead, we compute the most similar
verbs for Zrun,?user,command?, our model yields execute
(.42), modify (.40), invoke (.39).
Finally, note that the design of our model natu-
rally takes into account word order. Consider the
following two sentences:
(3) man damages car
(4) car damages man
5Similarity is calculated by measuring the cosine of the vec-
torized and normalized representation of the verb matrices.
Both sentences contain the exact same words, but the
process of damaging described in sentences (3) and
(4) is of a rather different nature. Our model is able
to take this difference into account: if we compute
Zdamage,?man,car? following sentence (3), our model
yields crash (.43), drive (.35), ride (.35) as most sim-
ilar verbs. If we do the same for Zdamage,?car,man? fol-
lowing sentence (4), our model instead yields scare
(.26), kill (.23), hurt (.23).
5 Evaluation
5.1 Methodology
In order to evaluate the performance of our tensor-
based factorization model of compositionality, we
make use of the sentence similarity task for transi-
tive sentences, defined in Grefenstette and Sadrzadeh
(2011a). This is an extension of the similarity task
for compositional models developed by Mitchell and
Lapata (2008), and constructed according to the same
guidelines. The dataset contains 2500 similarity
judgements, provided by 25 participants, and is pub-
licly available.6
The data consists of transitive verbs, each paired
with both a subject and an object noun ? thus form-
ing a small transitive sentence. Additionally, a ?land-
mark? verb is provided. The idea is to compose both
the target verb and the landmark verb with subject
and noun, in order to form two small compositional
6http://www.cs.ox.ac.uk/activities/
CompDistMeaning/GS2011data.txt
1147
factors subject object value
?128,181? Mathematics (.004), Science (.004), Economics
(.004)
course (.005), tutorial (.005), seminar (.005) .058
?293,181? organization (.007), association (.007), federa-
tion (.006)
course (.005), tutorial (.005), seminar (.005) .053
?60,140? rail (.011), bus (.009), ferry (.008) third (.004), decade (.004), hour (.004) .038
?268,268? API (.008), Apache (.007), Unix (.007) API (.008), Apache (.007), Unix (.007) .038
Table 3: Factor combinations for Grun
phrases. The system is then required to come up with
a suitable similarity score for these phrases. The cor-
relation of the model?s judgements with human judge-
ments (scored 1?7) is then calculated using Spear-
man?s ? . Two examples of the task are provided in
table 4.
p target subject object landmark sim
19 meet system criterion visit 1
21 write student name spell 6
Table 4: Two example judgements from the phrase simi-
larity task defined by Grefenstette and Sadrzadeh (2011a)
Grefenstette and Sadrzadeh (2011a) seem to cal-
culate the similarity score contextualizing both the
target verb and the landmark verb. Another possibil-
ity is to contextualize only the target verb, and com-
pute the similarity score with the non-contextualized
landmark verb. In our view, the latter option pro-
vides a better assessment of the model?s similar-
ity judgements, since contextualizing low-similarity
landmarks often yields non-sensical phrases (e.g. sys-
tem visits criterion). We provide scores for both
contextualized and non-contextualized landmarks.
We compare our results to a number of different
models. The first is Mitchell and Lapata?s (2008)
model, which computes the elementwise vector mul-
tiplication of verb, subject and object. The second
is Grefenstette and Sadrzadeh?s (2011b) best scoring
model instantiation of the categorical distributional
compositional model (Coecke et al, 2010). This
model computes the outer product of the subject and
object vector, the outer product of the verb vector
with itself, and finally the elementwise product of
both results. It yields the best score on the transitive
sentence similarity task reported to date.
As a baseline, we compute the non-contextualized
similarity score for target verb and landmark. The up-
per bound is provided by Grefenstette and Sadrzadeh
(2011a), based on interannotator agreement.
5.2 Implementational details
All models have been constructed using the UKWAC
corpus (Baroni et al, 2009), a 2 billion word corpus
automatically harvested from the web. From this data,
we accumulate the input matrix V for our first NMF
step. We use the 10K most frequent nouns, cross-
classified by the 2K most frequent context words.7
Matrix V is weighted using pointwise mutual infor-
mation (PMI, Church and Hanks (1990)).
A parsed version of the corpus is available, which
has been parsed with MaltParser (Nivre et al, 2006).
We use this version in order to extract our svo triples.
From these triples, we construct our tensor X, using
1K verbs ? 10K subjects ? 10K objects. Note once
again that the subject and object instances in the sec-
ond step are exactly the same as the noun instances
in the first step. Tensor X has been weighted using a
three-way extension of PMI, following equation 10
(Van de Cruys, 2011).
pmi3(x,y,z) = log
p(x,y,z)
p(x)p(y)p(z)
(10)
We set K = 300 as our number of latent factors.
The value was chosen as a trade-off between a model
that is both rich enough, and does not require an
excessive amount of memory (for the modeling of
the core tensor). The algorithm runs fairly effi-
ciently. Each NMF step is computed in a matter of
seconds, with convergence after 50?100 iterations.
The construction of the core tensor is somewhat more
7We use a context window of 5 words, both before and after
the target word; a stop list was used to filter out grammatical
function words.
1148
evolved, but does not exceed a wall time of 30 min-
utes. Results have been computed on a machine with
Intel Xeon 2.93Ghz CPU and 32GB of RAM.
5.3 Results
The results of the various models are presented in ta-
ble 5; multiplicative represents Mitchell and Lapata?s
(2008) multiplicative model, categorical represents
Grefenstette and Sadrzadeh?s (2011b) model, and
latent represents the model presented in this paper.
model contextualized non-contextualized
baseline .23
multiplicative .32 .34
categorical .32 .35
latent .32 .37
upper bound .62
Table 5: Results of the different compositionality models
on the phrase similarity task
In the contextualized version of the similarity task
(in which the landmark is combined with subject
and object), all three models obtain the same result
(.32). However, in the non-contextualized version
(in which only the target verb is combined with sub-
ject and object), the models differ in performance.
These differences are statistically significant.8 As
mentioned before, we believe the non-contextualized
version of the task gives a better impression of the
systems? ability to capture compositionality. The
contextualization of the landmark verb often yields
non-sensical combinations, such as system visits crite-
rion. We therefore deem it preferable to compute the
similarity of the target verb in composition (system
meets criterion) to the non-contextualized semantics
of the landmark verb (visit).
Note that the scores presented in this evalua-
tion (including the baseline score) are significantly
higher than the scores presented in Grefenstette and
Sadrzadeh (2011b). This is not surprising, since the
corpus we use ? UKWAC ? is an order of magni-
tude larger than the corpus used in their research ?
the British National Corpus (BNC). Presumably, the
scores are also favoured by our weighting measure.
8 p < 0.01; model differences have been tested using stratified
shuffling (Yeh, 2000).
In our experience, PMI performs better than weight-
ing with conditional probabilities.9
6 Conclusion
In this paper, we presented a novel method for the
computation of compositionality within a distribu-
tional framework. The key idea is that composition-
ality is modeled as a multi-way interaction between
latent factors, which are automatically constructed
from corpus data. We used our method to model
the composition of subject verb object combinations.
The method consists of two steps. First, we com-
pute a latent factor model for nouns from standard
co-occurrence data. Next, the latent factors are used
to induce a latent model of three-way subject verb
object interactions, represented by a core tensor. Our
model has been evaluated on a similarity task for tran-
sitive phrases, in which it matches and even exceeds
the state of the art.
We conclude with a number of future work issues.
First of all, we would like to extend our framework in
order to incorporate more compositional phenomena.
Our current model is designed to deal with the latent
modeling of subject verb object combinations. We
would like to investigate how other compositional
phenomena might fit within our latent interaction
framework, and how our model is able to tackle the
computation of compositionality across a differing
number of modes.
Secondly, we would like to further explore the
possibilities of our model in which all three modes
are represented by latent factors. The instantiation
of our model presented in this paper has two latent
modes, using the original instances of the verb mode
in order to efficiently compute verb similarity. We
think a full-blown latent interaction model might
prove to have interesting applications in a number of
NLP tasks, such as the paraphrasing of compositional
expressions.
Finally, we would like to test our method using a
number of different evaluation frameworks. We think
tasks of similarity judgement have their merits, but in
a way are also somewhat limited. In our opinion, re-
search on the modeling of compositional phenomena
within a distributional framework would substantially
9Contrary to the findings of Mitchell and Lapata (2008), who
report a high correlation with human similarity judgements.
1149
benefit from new evaluation frameworks. In particu-
lar, we think of a lexical substitution or paraphrasing
task along the lines of McCarthy and Navigli (2009),
but specifically aimed at the assessment of composi-
tional phenomena.
Acknowledgements
Tim Van de Cruys and Thierry Poibeau are supported
by the Centre National de la Recherche Scientifique
(CNRS, France), Anna Korhonen is supported by the
Royal Society (UK).
References
Brett W. Bader, Tamara G. Kolda, et al 2012. Matlab ten-
sor toolbox version 2.5. http://www.sandia.gov/
~tgkolda/TensorToolbox/.
Marco Baroni and Roberto Zamparelli. 2010. Nouns are
vectors, adjectives are matrices: Representing adjective-
noun constructions in semantic space. In Proceedings
of the 2010 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1183?1193, Cam-
bridge, MA, October. Association for Computational
Linguistics.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and
Eros Zanchetta. 2009. The wacky wide web: A
collection of very large linguistically processed web-
crawled corpora. Language Resources and Evaluation,
43(3):209?226.
William Blacoe and Mirella Lapata. 2012. A comparison
of vector-based representations for semantic compo-
sition. In Proceedings of the 2012 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 546?556, Jeju Island, Korea, July. Association
for Computational Linguistics.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. The Journal of Ma-
chine Learning Research, 3:993?1022.
Kenneth W. Church and Patrick Hanks. 1990. Word
association norms, mutual information & lexicography.
Computational Linguistics, 16(1):22?29.
Stephen Clark and Stephen Pulman. 2007. Combining
symbolic and distributional models of meaning. In Pro-
ceedings of the AAAI Spring Symposium on Quantum
Interaction, pages 52?55.
Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen Clark.
2010. Mathematical foundations for a compositional
distributed model of meaning. Lambek Festschrift, Lin-
guistic Analysis, vol. 36, 36.
Lieven De Lathauwer, Bart De Moor, and Joseph Vande-
walle. 2000. A multilinear singular value decomposi-
tion. SIAM Journal on Matrix Analysis and Applica-
tions, 21(4):1253?1278.
Georgiana Dinu and Mirella Lapata. 2010. Measuring
distributional similarity in context. In Proceedings of
the 2010 Conference on Empirical Methods in Natural
Language Processing, pages 1162?1172, Cambridge,
MA, October.
Katrin Erk and Sebastian Pado?. 2008. A structured vector
space model for word meaning in context. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, pages 897?906, Waikiki,
Hawaii, USA.
Katrin Erk and Sebastian Pado?. 2009. Paraphrase assess-
ment in structured vector space: Exploring parameters
and datasets. In Proceedings of the Workshop on Geo-
metrical Models of Natural Language Semantics, pages
57?65, Athens, Greece.
Gottlob Frege. 1892. U?ber Sinn und Bedeutung.
Zeitschrift fu?r Philosophie und philosophische Kritik,
100:25?50.
Eugenie Giesbrecht. 2010. Towards a matrix-based dis-
tributional model of meaning. In Proceedings of the
NAACL HLT 2010 Student Research Workshop, pages
23?28. Association for Computational Linguistics.
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011a.
Experimental support for a categorical compositional
distributional model of meaning. In Proceedings of
the 2011 Conference on Empirical Methods in Natural
Language Processing, pages 1394?1404, Edinburgh,
Scotland, UK., July. Association for Computational
Linguistics.
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011b.
Experimenting with transitive verbs in a discocat. In
Proceedings of the GEMS 2011 Workshop on GEomet-
rical Models of Natural Language Semantics, pages
62?66, Edinburgh, UK, July. Association for Computa-
tional Linguistics.
Zellig S. Harris. 1954. Distributional structure. Word,
10(23):146?162.
Richard A Harshman and Margaret E Lundy. 1994.
Parafac: Parallel factor analysis. Computational Statis-
tics & Data Analysis, 18(1):39?72.
Tamara G. Kolda and Brett W. Bader. 2009. Ten-
sor decompositions and applications. SIAM Review,
51(3):455?500, September.
Tamara G. Kolda and Jimeng Sun. 2008. Scalable tensor
decompositions for multi-aspect data mining. In ICDM
2008: Proceedings of the 8th IEEE International Con-
ference on Data Mining, pages 363?372, December.
Thomas Landauer and Susan Dumais. 1997. A solution
to Plato?s problem: The Latent Semantic Analysis the-
1150
ory of the acquisition, induction, and representation of
knowledge. Psychology Review, 104:211?240.
Daniel D. Lee and H. Sebastian Seung. 2000. Algorithms
for non-negative matrix factorization. In Advances in
Neural Information Processing Systems 13, pages 556?
562.
Diana McCarthy and Roberto Navigli. 2009. The En-
glish lexical substitution task. Language resources and
evaluation, 43(2):139?159.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. proceedings of ACL-
08: HLT, pages 236?244.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006. Malt-
parser: A data-driven parser-generator for dependency
parsing. In Proceedings of LREC-2006, pages 2216?
2219.
Richard Socher, Brody Huval, Christopher D. Manning,
and Andrew Y. Ng. 2012. Semantic compositionality
through recursive matrix-vector spaces. In Proceedings
of the 2012 Joint Conference on Empirical Methods in
Natural Language Processing and Computational Nat-
ural Language Learning, pages 1201?1211, Jeju Island,
Korea, July. Association for Computational Linguistics.
Stefan Thater, Georgiana Dinu, and Manfred Pinkal. 2009.
Ranking paraphrases in context. In Proceedings of the
2009 Workshop on Applied Textual Inference, pages
44?47, Suntec, Singapore.
Stefan Thater, Hagen Fu?rstenau, and Manfred Pinkal.
2010. Contextualizing semantic representations using
syntactically enriched vector models. In Proceedings of
the 48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 948?957, Uppsala, Sweden.
Ledyard R. Tucker. 1966. Some mathematical notes on
three-mode factor analysis. Psychometrika, 31(3):279?
311.
Tim Van de Cruys. 2010. A non-negative tensor fac-
torization model for selectional preference induction.
Natural Language Engineering, 16(4):417?437.
Tim Van de Cruys. 2011. Two multivariate generaliza-
tions of pointwise mutual information. In Proceedings
of the Workshop on Distributional Semantics and Com-
positionality, pages 16?20, Portland, Oregon, USA,
June. Association for Computational Linguistics.
Alexander Yeh. 2000. More accurate tests for the statis-
tical significance of result differences. In Proceedings
of the 18th conference on Computational linguistics,
pages 947?953, Saarbru?cken, Germany.
1151
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1476?1485,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Latent Semantic Word Sense Induction and Disambiguation
Tim Van de Cruys
RCEAL
University of Cambridge
United Kingdom
tv234@cam.ac.uk
Marianna Apidianaki
Alpage, INRIA & Univ Paris Diderot
Sorbonne Paris Cite?, UMRI-001
75013 Paris, France
marianna.apidianaki@inria.fr
Abstract
In this paper, we present a unified model for
the automatic induction of word senses from
text, and the subsequent disambiguation of
particular word instances using the automati-
cally extracted sense inventory. The induction
step and the disambiguation step are based on
the same principle: words and contexts are
mapped to a limited number of topical dimen-
sions in a latent semantic word space. The in-
tuition is that a particular sense is associated
with a particular topic, so that different senses
can be discriminated through their association
with particular topical dimensions; in a similar
vein, a particular instance of a word can be dis-
ambiguated by determining its most important
topical dimensions. The model is evaluated on
the SEMEVAL-2010 word sense induction and
disambiguation task, on which it reaches state-
of-the-art results.
1 Introduction
Word sense induction (WSI) is the task of automati-
cally identifying the senses of words in texts, with-
out the need for handcrafted resources or manually
annotated data. The manual construction of a sense
inventory is a tedious and time-consuming job, and
the result is highly dependent on the annotators and
the domain at hand. By applying an automatic proce-
dure, we are able to only extract the senses that are
objectively present in a particular corpus, and it al-
lows for the sense inventory to be straightforwardly
adapted to a new domain.
Word sense disambiguation (WSD), on the other
hand, is the closely related task of assigning a sense
label to a particular instance of a word in context,
using an existing sense inventory. The bulk of WSD
algorithms up till now use pre-defined sense inven-
tories (such as WordNet) that often contain fine-
grained sense distinctions, which poses serious prob-
lems for computational semantic processing (Ide
and Wilks, 2007). Moreover, most WSD algorithms
take a supervised approach, which requires a signifi-
cant amount of manually annotated training data.
The model presented here induces the senses of
words in a fully unsupervised way, and subsequently
uses the induced sense inventory for the unsuper-
vised disambiguation of particular occurrences of
words. The induction step and the disambiguation
step are based on the same principle: words and
contexts are mapped to a limited number of topical
dimensions in a latent semantic word space. The
key idea is that the model combines tight, synonym-
like similarity (based on dependency relations) with
broad, topical similarity (based on a large ?bag of
words? context window). The intuition in this is that
the dependency features can be disambiguated by
the topical dimensions identified by the broad con-
textual features; in a similar vein, a particular in-
stance of a word can be disambiguated by determin-
ing its most important topical dimensions (based on
the instance?s context words).
The paper is organized as follows. Section 2
presents some previous research on distributional
similarity and word sense induction. Section 3 gives
an overview of our method for word sense induction
and disambiguation. Section 4 provides a quantita-
tive evaluation and comparison to other algorithms
in the framework of the SEMEVAL-2010 word sense
1476
induction and disambiguation (WSI/WSD) task. The
last section draws conclusions, and lays out a num-
ber of future research directions.
2 Previous Work
2.1 Distributional similarity
According to the distributional hypothesis of mean-
ing (Harris, 1954), words that occur in similar con-
texts tend to be semantically similar. In the spirit
of this by now well-known adage, numerous algo-
rithms have sprouted up that try to capture the se-
mantics of words by looking at their distribution in
texts, and comparing those distributions in a vector
space model.
One of the best known models in this respect is
latent semantic analysis ? LSA (Landauer and Du-
mais, 1997; Landauer et al, 1998). In LSA, a term-
document matrix is created, that contains the fre-
quency of each word in a particular document. This
matrix is then decomposed into three other matrices
with a mathematical factorization technique called
singular value decomposition (SVD). The most im-
portant dimensions that come out of the SVD are said
to represent latent semantic dimensions, according
to which nouns and documents can be represented
more efficiently. Our model also applies a factoriza-
tion technique (albeit a different one) in order to find
a reduced semantic space.
Context is a determining factor in the nature of
the semantic similarity that is induced. A broad con-
text window (e.g. a paragraph or document) yields
broad, topical similarity, whereas a small context
yields tight, synonym-like similarity. This has lead
a number of researchers to use the dependency rela-
tions that a particular word takes part in as contex-
tual features. One of the most important approaches
is Lin (1998). An overview of dependency-based
semantic space models is given in Pado? and Lapata
(2007).
2.2 Word sense induction
The following paragraphs provide a succinct
overview of word sense induction research. A thor-
ough survey on word sense disambiguation (includ-
ing unsupervised induction algorithms) is presented
in Navigli (2009).
Algorithms for word sense induction can roughly
be divided into local and global ones. Local WSI
algorithms extract the different senses of a word on
a per-word basis, i.e. the different senses for each
word are determined separately. They can be further
subdivided into context-clustering algorithms and
graph-based algorithms. In the context-clustering
approach, context vectors are created for the differ-
ent instances of a particular word, and those con-
texts are grouped into a number of clusters, repre-
senting the different senses of the word. The con-
text vectors may be represented as first or second-
order co-occurrences (i.e. the contexts of the target
word are similar if the words they in turn co-occur
with are similar). The first one to propose this idea
of context-group discrimination was Schu?tze (1998),
and many researchers followed a similar approach
to sense induction (Purandare and Pedersen, 2004).
In the graph-based approach, on the other hand, a
co-occurrence graph is created, in which nodes rep-
resent words, and edges connect words that appear
in the same context (dependency relation or context
window). The senses of a word may then be discov-
ered using graph clustering techniques (Widdows
and Dorow, 2002), or algorithms such as HyperLex
(Ve?ronis, 2004) or Pagerank (Agirre et al, 2006). Fi-
nally, Bordag (2006) recently proposed an approach
that uses word triplets to perform word sense induc-
tion. The underlying idea is the ?one sense per col-
location? assumption, and co-occurrence triplets are
clustered based on the words they have in common.
Global algorithms take an approach in which the
different senses of a particular word are determined
by comparing them to, and demarcating them from,
the senses of other words in a full-blown word space
model. The best known global approach is the one
by Pantel and Lin (2002). They present a global
clustering algorithm ? coined clustering by commit-
tee (CBC) ? that automatically discovers word senses
from text. The key idea is to first discover a set of
tight, unambiguous clusters, to which possibly am-
biguous words can be assigned. Once a word has
been assigned to a cluster, the features associated
with that particular cluster are stripped off the word?s
vector. This way, less frequent senses of the word
may be discovered.
Van de Cruys (2008) proposes a model for sense
induction based on latent semantic dimensions. Us-
ing an extension of non-negative matrix factoriza-
1477
tion, the model induces a latent semantic space
according to which both dependency features and
broad contextual features are classified. Using the
latent space, the model is able to discriminate be-
tween different word senses. The model presented
below is an extension of this approach: whereas the
model described in Van de Cruys (2008) is only able
to perform word sense induction, our model is ca-
pable of performing both word sense induction and
disambiguation.
3 Methodology
3.1 Non-negative Matrix Factorization
Our model uses non-negative matrix factorization ?
NMF (Lee and Seung, 2000) in order to find latent
dimensions. There are a number of reasons to prefer
NMF over the better known singular value decompo-
sition used in LSA. First of all, NMF allows us to min-
imize the Kullback-Leibler divergence as an objec-
tive function, whereas SVD minimizes the Euclidean
distance. The Kullback-Leibler divergence is better
suited for language phenomena. Minimizing the Eu-
clidean distance requires normally distributed data,
and language phenomena are typically not normally
distributed. Secondly, the non-negative nature of the
factorization ensures that only additive and no sub-
tractive relations are allowed. This proves partic-
ularly useful for the extraction of semantic dimen-
sions, so that the NMF model is able to extract much
more clear-cut dimensions than an SVD model. And
thirdly, the non-negative property allows the result-
ing model to be interpreted probabilistically, which
is not straightforward with an SVD factorization.
The key idea is that a non-negative matrix A is
factorized into two other non-negative matrices, W
and H
Ai?j ?Wi?kHk?j (1)
where k is much smaller than i, j so that both in-
stances and features are expressed in terms of a few
components. Non-negative matrix factorization en-
forces the constraint that all three matrices must be
non-negative, so all elements must be greater than or
equal to zero.
Using the minimization of the Kullback-Leibler
divergence as an objective function, we want to
find the matrices W and H for which the Kullback-
Leibler divergence between A and WH (the multipli-
cation of W and H) is the smallest. This factoriza-
tion is carried out through the iterative application
of update rules. Matrices W and H are randomly
initialized, and the rules in 2 and 3 are iteratively ap-
plied ? alternating between them. In each iteration,
each vector is adequately normalized, so that all di-
mension values sum to 1.
Ha? ? Ha?
?
iWia
Ai?
(WH)i?
?
kWka
(2)
Wia ?Wia
?
?Ha?
Ai?
(WH)i?
?
vHav
(3)
3.2 Word sense induction
Using an extension of non-negative matrix factoriza-
tion, we are able to jointly induce latent factors for
three different modes: words, their window-based
(?bag of words?) context words, and their depen-
dency relations. Three matrices are constructed that
capture the pairwise co-occurrence frequencies for
the different modes. The first matrix contains co-
occurrence frequencies of words cross-classified by
dependency relations, the second matrix contains
co-occurrence frequencies of words cross-classified
by words that appear in the noun?s context window,
and the third matrix contains co-occurrence frequen-
cies of dependency relations cross-classified by co-
occurring context words. NMF is then applied to the
three matrices and the separate factorizations are in-
terleaved (i.e. the results of the former factorization
are used to initialize the factorization of the next ma-
trix). A graphical representation of the interleaved
factorization algorithm is given in figure 1.
The procedure of the algorithm goes as follows.
First, matrices W, H, G, and F are randomly initial-
ized. We then start our first iteration, and compute
the update of matrix W (using equation 3). Matrix
W is then copied to matrix V, and the update of
matrix G is computed (using equation 2). The trans-
pose of matrix G is again copied to matrix U, and
the update ofF is computed (again using equation 2).
As a last step, matrix F is copied to matrix H, and
we restart the iteration loop until a stopping criterion
(e.g. a maximum number of iterations, or no more
significant change in objective function; we used the
1478
= xW H
= xV G
= xU F
j
i
s
k
i
j
kAwords xdependency relations
B
words xcontext words
Ccontext words xdependency relations
k k
k
k
i
j
i
s
j
s s
Figure 1: A graphical representation of the interleaved
NMF algorithm
former one) is reached.1 When the factorization is
finished, the three different modes (words, window-
based context words and dependency relations) are
all represented according to a limited number of la-
tent factors.
Next, the factorization that is thus created is used
for word sense induction. The intuition is that a par-
ticular, dominant dimension of an ambiguous word
is ?switched off?, in order to reveal other possible
senses of the word. Formally, we proceed as follows.
Matrix H indicates the importance of each depen-
dency relation given a topical dimension. With this
knowledge, the dependency relations that are respon-
sible for a certain dimension can be subtracted from
the original noun vector. This is done by scaling
down each feature of the original vector according
to the load of the feature on the subtracted dimen-
sion, using equation 4.
t = v(u1 ? hk) (4)
Equation 4 multiplies each dependency feature of
the original noun vector v with a scaling factor, ac-
cording to the load of the feature on the subtracted
dimension (hk ? the vector of matrix H that corre-
sponds to the dimension we want to subtract). u1 is
a vector of ones with the same length as hk. The re-
sult is vector t, in which the dependency features rel-
1Note that this is not the only possibly way of interleaving
the different factorizations, but in our experiments we found that
different constellations lead to similar results.
evant to the particular topical dimension have been
scaled down.
In order to determine which dimension(s) are re-
sponsible for a particular sense of the word, the
method is embedded in a clustering approach. First,
a specific word is assigned to its predominant sense
(i.e. the most similar cluster). Next, the dominant
semantic dimension(s) for this cluster are subtracted
from the word vector, and the resulting vector is
fed to the clustering algorithm again, to see if other
word senses emerge. The dominant semantic dimen-
sion(s) can be identified by folding vector c ? repre-
senting the cluster centroid ? into the factorization
(equation 5). This yields a probability vector b over
latent factors for the particular centroid.
b = cHT (5)
A simple k-means algorithm is used to com-
pute the initial clustering, using the non-factorized
dependency-based feature vectors (matrix A). k-
means yields a hard clustering, in which each noun
is assigned to exactly one (dominant) cluster. In the
second step, we determine for each noun whether
it can be assigned to other, less dominant clusters.
First, the salient dimension(s) of the centroid to
which the noun is assigned are determined. The cen-
troid of the cluster is computed by averaging the fre-
quencies of all cluster elements except for the tar-
get word we want to reassign. After subtracting the
salient dimensions from the noun vector, we check
whether the vector is reassigned to another cluster
centroid. If this is the case, (another instance of) the
noun is assigned to the cluster, and the second step
is repeated. If there is no reassignment, we continue
with the next word. The target element is removed
from the centroid to make sure that only the dimen-
sions associated with the sense of the cluster are sub-
tracted. When the algorithm is finished, each noun
is assigned to a number of clusters, representing its
different senses.
We use two different methods for selecting the fi-
nal number of candidate senses. The first method,
NMFcon , takes a conservative approach, and only
selects candidate senses if ? after the subtraction of
salient dimensions ? another sense is found that is
more similar2 to the adapted noun vector than the
2We use the cosine measure for our similarity calculations.
1479
dominant sense. The second method, NMFlib , is
more liberal, and also selects the next best cluster
centroid as candidate sense until a certain similarity
threshold ? is reached.3
3.3 Word sense disambiguation
The sense inventory that results from the induction
step can now be used for the disambiguation of in-
dividual instances as follows. For each instance of
the target noun, we extract its context words, i.e. the
words that co-occur in the same paragraph, and rep-
resent them as a probability vector f . Using matrix
G from our factorization model (which represents
context words by semantic dimensions), this vector
can be folded into the semantic space, thus represent-
ing a probability vector over latent factors for the
particular instance of the target noun (equation 6).
d = fGT (6)
Likewise, the candidate senses of the noun (repre-
sented as centroids) can be folded into our seman-
tic space using matrix H (equation 5). This yields
a probability distribution over the semantic dimen-
sions for each centroid. As a last step, we com-
pute the Kullback-Leibler divergence between the
context vector and the candidate centroids, and se-
lect the candidate centroid that yields the lowest di-
vergence as the correct sense. The disambiguation
process is represented graphically in figure 2.
3.4 Example
Let us clarify the process with an example for the
noun chip. The sense induction algorithm finds the
following candidate senses:4
1. cache, CPU, memory, microprocessor, proces-
sor, RAM, register
2. bread, cake, chocolate, cookie, recipe, sand-
wich
3. accessory, equipment, goods, item, machinery,
material, product, supplies
3Experimentally (examining the cluster output), we set ? =
0.2
4Note that we do not use the word sense to hint at a lexico-
graphic meaning distinction; rather, sense in this case should be
regarded as a more coarse-grained and topic-related entity.
G'
k
sscontext vector
k
cluster centroid
j
cluster centroid
j
cluster centroid
j
H'
k
j
k
k
k
Figure 2: Graphical representation of the disambiguation
process
Each candidate sense is associated with a centroid
(the average frequency vector of the cluster?s mem-
bers), that is folded into the semantic space, which
yields a ?semantic fingerprint?, i.e. a distribution
over the semantic dimensions. For the first sense,
the ?computer? dimension will be the most impor-
tant. Likewise, for the second and the third sense the
?food? dimension and the ?manufacturing? dimension
will be the most important.5
Let us now take a particular instance of the noun
chip, such as the one in (1).
(1) An N.V. Philips unit has created a com-
puter system that processes video images
3,000 times faster than conventional systems.
Using reduced instruction - set comput-
ing, or RISC, chips made by Intergraph of
Huntsville, Ala., the system splits the im-
age it ?sees? into 20 digital representations,
each processed by one chip.
Looking at the context of the particular instance of
chip, a context vector is created which represents
the semantic content words that appear in the same
paragraph (the extracted content words are printed
in boldface). This context vector is again folded
into the semantic space, yielding a distribution over
the semantic dimensions. By selecting the lowest
5In the majority of cases, the induced dimensions indeed
contain such clear-cut semantics, so that the dimensions can be
rightfully labeled as above.
1480
Kullback-Leibler divergence between the semantic
probability distribution of the target instance and the
semantic probability distributions of the candidate
senses, the algorithm is able to assign the ?computer?
sense of the target noun chip.
4 Evaluation
4.1 Dataset
Our word sense induction and disambiguation
model is trained and tested on the dataset of the
SEMEVAL-2010 WSI/WSD task (Manandhar et al,
2010). The SEMEVAL-2010 WSI/WSD task is based
on a dataset of 100 target words, 50 nouns and 50
verbs. For each target word, a training set is pro-
vided from which the senses of the word have to
be induced without using any other resources. The
training set for a target word consists of a set of
target word instances in context (sentences or para-
graphs). The complete training set contains 879,807
instances, viz. 716,945 noun and 162,862 verb in-
stances.
The senses induced during training are used for
disambiguation in the testing phase. In this phase,
the system is provided with a test set that consists
of unseen instances of the target words. The test
set contains 8,915 instances in total, of which 5,285
nouns and 3,630 verbs. The instances in the test
set are tagged with OntoNotes senses (Hovy et al,
2006). The system needs to disambiguate these in-
stances using the senses acquired during training.
4.2 Implementational details
The SEMEVAL training set has been part of speech
tagged and lemmatized with the Stanford Part-Of-
Speech Tagger (Toutanova and Manning, 2000;
Toutanova et al, 2003) and parsed with Malt-
Parser (Nivre et al, 2006), trained on sections 2-
21 of the Wall Street Journal section of the Penn
Treebank extended with about 4000 questions from
the QuestionBank6 in order to extract dependency
triples. The SEMEVAL test set has only been tagged
and lemmatized, as our disambiguation model does
not use dependency triples as features (contrary to
the induction model).
6http://maltparser.org/mco/english_
parser/engmalt.html
We constructed two different models ? one for
nouns and one for verbs. For each model, the matri-
ces needed for our interleaved NMF factorization are
extracted from the corpus. The noun model was built
using 5K nouns, 80K dependency relations, and 2K
context words (excluding stop words) with highest
frequency in the training set, which yields matrices
of 5K nouns ? 80K dependency relations, 5K nouns
? 2K context words, and 80K dependency relations
? 2K context words. The model for verbs was con-
structed analogously, using 3K verbs, and the same
number of dependency relations and context words.
For our initial k-means clustering, we set k = 600
for nouns, and k = 400 for verbs. For the under-
lying interleaved NMF model, we used 50 iterations,
and factored the model to 50 dimensions.
4.3 Evaluation measures
The results of the systems participating in the
SEMEVAL-2010 WSI/WSD task are evaluated both
in a supervised and in an unsupervised manner.
The supervised evaluation in the SEMEVAL-2010
WSI/WSD task follows the scheme of the SEMEVAL-
2007 WSI task (Agirre and Soroa, 2007), with some
modifications. One part of the test set is used as a
mapping corpus, which maps the automatically in-
duced clusters to gold standard senses; the other part
acts as an evaluation corpus. The mapping between
clusters and gold standard senses is used to tag the
evaluation corpus with gold standard tags. The sys-
tems are then evaluated as in a standard WSD task,
using recall.
In the unsupervised evaluation, the induced
senses are evaluated as clusters of instances which
are compared to the sets of instances tagged with
the gold standard senses (corresponding to classes).
Two partitions are thus created over the test set of
a target word: a set of automatically generated clus-
ters and a set of gold standard classes. A number of
these instances will be members of both one gold
standard class and one cluster. Consequently, the
quality of the proposed clustering solution is evalu-
ated by comparing the two groupings and measuring
their similarity.
Two evaluation metrics are used during the unsu-
pervised evaluation in order to estimate the quality
of the clustering solutions, the V-Measure (Rosen-
berg and Hirschberg, 2007) and the paired F-
1481
Score (Artiles et al, 2009). V-Measure assesses the
quality of a clustering by measuring its homogeneity
(h) and its completeness (c). Homogeneity refers to
the degree that each cluster consists of data points
primarily belonging to a single gold standard class,
while completeness refers to the degree that each
gold standard class consists of data points primarily
assigned to a single cluster. V-Measure is the har-
monic mean of h and c.
VM =
2 ? h ? c
h+ c
(7)
In the paired F-Score (Artiles et al, 2009) eval-
uation, the clustering problem is transformed into a
classification problem (Manandhar et al, 2010). A
set of instance pairs is generated from the automati-
cally induced clusters, which comprises pairs of the
instances found in each cluster. Similarly, a set of in-
stance pairs is created from the gold standard classes,
containing pairs of the instances found in each class.
Precision is then defined as the number of common
instance pairs between the two sets to the total num-
ber of pairs in the clustering solution (cf. formula 8).
Recall is defined as the number of common instance
pairs between the two sets to the total number of
pairs in the gold standard (cf. formula 9). Preci-
sion and recall are finally combined to produce the
harmonic mean (cf. formula 10).
P =
|F (K) ? F (S)|
|F (K)|
(8)
R =
|F (K) ? F (S)|
|F (S)|
(9)
FS =
2 ? P ?R
P +R
(10)
The obtained results are also compared to two
baselines. The most frequent sense (MFS) baseline
groups all testing instances of a target word into one
cluster. The Random baseline randomly assigns an
instance to one of the clusters.7 This baseline is exe-
cuted five times and the results are averaged.
7The number of clusters in Random was chosen to be
roughly equal to the average number of senses in the gold stan-
dard.
4.4 Results
4.4.1 Unsupervised evaluation
In table 1, we present the performance of a number
of algorithms on the V-measure. We compare our
V-measure scores with the scores of the best-ranked
systems in the SEMEVAL 2010 WSI/WSD task, both
for the complete data set and for nouns and verbs
separately. The fourth column shows the average
number of clusters induced in the test set by each
algorithm. The MFS baseline has a V-Measure equal
to 0, since by definition its completeness is 1 and its
homogeneity is 0.
NMFcon ? our model that takes a conservative ap-
proach in the induction of candidate senses ? does
not beat the random baseline. NMFlib ? our model
that is more liberal in inducing senses ? reaches bet-
ter results. With 11.8%, it scores similar to other
algorithms that induce a similar average number of
clusters, such as Duluth-WSI (Pedersen, 2010).
Pedersen (2010) has shown that the V-Measure
tends to favour systems producing a higher number
of clusters than the number of gold standard senses.
This is reflected in the scores of our models as well.
VM (%) all noun verb #cl
Hermit 16.2 16.7 15.6 10.78
UoY 15.7 20.6 8.5 11.54
KSU KDD 15.7 18.0 12.4 17.50
NMFlib 11.8 13.5 9.4 4.80
Duluth-WSI 9.0 11.4 5.7 4.15
Random 4.4 4.2 4.6 4.00
NMFcon 3.9 3.9 3.9 1.58
MFS 0.0 0.0 0.0 1.00
Table 1: Unsupervised V-measure evaluation on SE-
MEVAL test set
Motivated by the large divergences in the sys-
tem rankings on the different metrics used in the
SEMEVAL-2010 WSI/WSD task, Pedersen evaluated
the metrics themselves. His evaluation relied on
the assumption that a good measure should assign
low scores to random baselines. Pedersen showed
that the V-Measure continued to improve as random-
ness increased. We agree with Pedersen?s conclu-
sion that the V-Measure results should be interpreted
with caution, but we still report the results in order
1482
to perform a global comparison, on all metrics, of
our system?s performance to the systems that partic-
ipated to the SEMEVAL task.
Contrary to V-Measure, paired F-score is a fairly
reliable measure and the only one that managed to
identify and expose random baselines in the above
mentioned metric evaluation. This means that the
random systems used for testing were ranked low
when a high number of random senses was used.
In table 2, the paired F-Score of a number of al-
gorithms is given. The paired F-Score penalizes sys-
tems when they produce a higher number of clusters
(low recall) or a lower number of clusters (low pre-
cision) than the gold standard number of senses. We
again compare our results with the scores of the best-
ranked systems in the SEMEVAL-2010 WSI/WSD
TASK.
FS (%) all noun verb #cl
MFS 63.5 57.0 72.7 1.00
Duluth-WSI-SVD-Gap 63.3 57.0 72.4 1.02
NMFcon 60.2 54.6 68.4 1.58
NMFlib 45.3 42.2 49.8 5.42
Duluth-WSI 41.1 37.1 46.7 4.15
Random 31.9 30.4 34.1 4.00
Table 2: Unsupervised paired F-score evaluation on SE-
MEVAL testset
NMFcon reaches a score of 60.2%, which is again
similar to other algorithms that induce the same av-
erage number of clusters. NMFlib scores 45.3%, in-
dicating that the algorithm is able to retain a rea-
sonable F-Score while at the same time inducing a
significant number of clusters. This especially be-
comes clear when comparing its score to the other
algorithms.
4.4.2 Supervised evaluation
In the supervised evaluation, the automatically in-
duced clusters are mapped to gold standard senses,
using the mapping corpus (i.e. one part of the test
set). The obtained mapping is used to tag the evalu-
ation corpus (i.e. the other part of the test set) with
gold standard tags, which means that the methods
are evaluated in a standard WSD task.
Table 3 shows the recall of our algorithms in the
supervised evaluation, again compared to other algo-
rithms evaluated in the SEMEVAL-2010 WSI/WSD
task.
SR (%) all noun verb #S
NMFlib 62.6 57.3 70.2 1.82
UoY 62.4 59.4 66.8 1.51
Duluth-WSI 60.5 54.7 68.9 1.66
NMFcon 60.3 54.5 68.8 1.21
MFS 58.7 53.2 66.6 1.00
Random 57.3 51.5 65.7 1.53
Table 3: Supervised recall for SEMEVAL testset, 80%
mapping, 20% evaluation
NMFlib gets 62.6%, which makes it the best scor-
ing algorithm on the supervised evaluation. NMFcon
reaches 60.3%, which again indicates that it is in the
same ballpark as other algorithms that induce a sim-
ilar average number of senses.
Some doubts have been cast on the representative-
ness of the supervised recall results as well. Accord-
ing to Pedersen (2010), the supervised learning al-
gorithm that underlies this evaluation method tends
to converge to the Most Frequent Sense (MFS) base-
line, because the number of senses that the classi-
fier assigns to the test instances is rather low. We
think these shortcomings indicate the need for the
development of new evaluation metrics, capable of
providing a more accurate evaluation of the perfor-
mance of WSI systems. Nevertheless, these metrics
still constitute a useful testbed for comparing the per-
formance of different systems.
5 Conclusion and future work
In this paper, we presented a model based on latent
semantics that is able to perform word sense induc-
tion as well as disambiguation. Using latent topi-
cal dimensions, the model is able to discriminate be-
tween different senses of a word, and subsequently
disambiguate particular instances of a word. The
evaluation results indicate that our model reaches
state-of-the-art performance compared to other sys-
tems that participated in the SEMEVAL-2010 word
sense induction and disambiguation task. Moreover,
our global approach is able to reach similar perfor-
mance on an evaluation set that is tuned to fit the
needs of local approaches. The evaluation set con-
1483
tains an enormous amount of contexts for only a
small number of target words, favouring methods
that induce senses on a per-word basis. A global
approach like ours is likely to induce a more bal-
anced sense inventory using an unbiased corpus, and
is likely to outperform local methods when such an
unbiased corpus is used as input. We therefore think
that the global, unified approach to word sense in-
duction and disambiguation presented here provides
a genuine and powerful solution to the problem at
hand.
We conclude with some issues for future work.
First of all, we would like to evaluate the approach
presented here using a more balanced and unbiased
corpus, and compare its performance on such a cor-
pus to local approaches. Secondly, we would also
like to include grammatical dependency information
in the disambiguation step of the algorithm. For now,
the disambiguation step only uses a word?s context
words; enriching the feature set with dependency in-
formation is likely to improve the performance of
the disambiguation.
Acknowledgments
This work is supported by the Scribo project, funded
by the French ?po?le de compe?titivite?? System@tic,
and by the French national grant EDyLex (ANR-09-
CORD-008).
References
Eneko Agirre and Aitor Soroa. 2007. SemEval-2007
Task 02: Evaluating word sense induction and discrim-
ination systems. In Proceedings of the fourth Interna-
tional Workshop on Semantic Evaluations (SemEval),
ACL, pages 7?12, Prague, Czech Republic.
Eneko Agirre, David Mart??nez, Ojer Lo?pez de Lacalle,
and Aitor Soroa. 2006. Two graph-based algo-
rithms for state-of-the-art WSD. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP-06), pages 585?593, Syd-
ney, Australia.
Marianna Apidianaki and Tim Van de Cruys. 2011. A
Quantitative Evaluation of Global Word Sense Induc-
tion. In Proceedings of the 12th International Con-
ference on Intelligent Text Processing and Computa-
tional Linguistics (CICLing), published in Springer
Lecture Notes in Computer Science (LNCS), volume
6608, pages 253?264, Tokyo, Japan.
Javier Artiles, Enrique Amigo?, and Julio Gonzalo. 2009.
The role of named entities in web people search. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing (EMNLP-09), pages
534?542, Singapore.
Stefan Bordag. 2006. Word sense induction: Triplet-
based clustering and automatic evaluation. In Proceed-
ings of the 11th Conference of the European Chap-
ter of the Association for Computational Linguistics
(EACL-06), pages 137?144, Trento, Italy.
Zellig S. Harris. 1954. Distributional structure. Word,
10(23):146?162.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
the 90% solution. In Proceedings of the Human Lan-
guage Technology / North American Association of
Computational Linguistics conference (HLT-NAACL-
06), pages 57?60, New York, NY.
Nancy Ide and Yorick Wilks. 2007. Making Sense About
Sense. In Eneko Agirre and Philip Edmonds, editors,
Word Sense Disambiguation, Algorithms and Applica-
tions, pages 47?73. Springer.
Thomas Landauer and Susan Dumais. 1997. A solution
to Plato?s problem: The Latent Semantic Analysis the-
ory of the acquisition, induction, and representation of
knowledge. Psychology Review, 104:211?240.
Thomas Landauer, Peter Foltz, and Darrell Laham. 1998.
An Introduction to Latent Semantic Analysis. Dis-
course Processes, 25:295?284.
Daniel D. Lee and H. Sebastian Seung. 2000. Algo-
rithms for non-negative matrix factorization. In Ad-
vances in Neural Information Processing Systems, vol-
ume 13, pages 556?562.
Dekang Lin. 1998. Automatic Retrieval and Clustering
of Similar Words. In Proceedings of the 36th Annual
Meeting of the Association for Computational Linguis-
tics and 17th International Conference on Computa-
tional Linguistics (COLING-ACL98), volume 2, pages
768?774, Montreal, Quebec, Canada.
Suresh Manandhar, Ioannis P. Klapaftis, Dmitriy Dligach,
and Sameer S. Pradhan. 2010. SemEval-2010 Task
14: Word Sense Induction & Disambiguation. In Pro-
ceedings of the fifth International Workshop on Seman-
tic Evaluation (SemEval), ACL-10, pages 63?68, Upp-
sala, Sweden.
Roberto Navigli. 2009. Word Sense Disambiguation: a
Survey. ACM Computing Surveys, 41(2):1?69.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006. Malt-
parser: A data-driven parser-generator for dependency
parsing. In Proceedings of the fifth International
Conference on Language Resources and Evaluation
(LREC-06), pages 2216?2219, Genoa, Italy.
1484
Sebastian Pado? and Mirella Lapata. 2007. Dependency-
based construction of semantic space models. Compu-
tational Linguistics, 33(2):161?199.
Patrick Pantel and Dekang Lin. 2002. Discovering word
senses from text. In ACM SIGKDD International Con-
ference on Knowledge Discovery and Data Mining,
pages 613?619, Edmonton, Alberta, Canada.
Ted Pedersen. 2010. Duluth-WSI: SenseClusters Ap-
plied to the Sense Induction Task of SemEval-2. In
Proceedings of the fifth International Workshop on Se-
mantic Evaluations (SemEval-2010), pages 363?366,
Uppsala, Sweden.
Amruta Purandare and Ted Pedersen. 2004. Word
Sense Discrimination by Clustering Contexts in Vec-
tor and Similarity Spaces. In Proceedings of the Con-
ference on Computational Natural Language Learning
(CoNLL), pages 41?48, Boston, MA.
Andrew Rosenberg and Julia Hirschberg. 2007. V-
measure: A conditional entropy-based external clus-
ter evaluation measure. In Proceedings of the Joint
2007 Conference on Empirical Methods in Natural
Language Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 410?420,
Prague, Czech Republic.
Hinrich Schu?tze. 1998. Automatic Word Sense Discrim-
ination. Computational Linguistics, 24(1):97?123.
Kristina Toutanova and Christopher D. Manning. 2000.
Enriching the Knowledge Sources Used in a Maxi-
mum Entropy Part-of-Speech Tagger. In Proceedings
of the Joint SIGDAT Conference on Empirical Meth-
ods in Natural Language Processing and Very Large
Corpora (EMNLP/VLC-2000), pages 63?70.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-Rich Part-of-
Speech Tagging with a Cyclic Dependency Network.
In Proceedings of the Human Language Technology
/ North American Association of Computational Lin-
guistics conference (HLT-NAACL-03, pages 252?259,
Edmonton, Canada.
Tim Van de Cruys. 2008. Using Three Way Data for
Word Sense Discrimination. In Proceedings of the
22nd International Conference on Computational Lin-
guistics (COLING-08), pages 929?936, Manchester,
UK.
Jean Ve?ronis. 2004. Hyperlex: lexical cartography for
information retrieval. Computer Speech & Language,
18(3):223?252.
Dominic Widdows and Beate Dorow. 2002. A Graph
Model for Unsupervised Lexical Acquisition. In Pro-
ceedings of the 19th International Conference on Com-
putational Linguistics (COLING-02), pages 1093?
1099, Taipei, Taiwan.
1485
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 98?102, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
MELODI: Semantic Similarity of Words and Compositional Phrases
using Latent Vector Weighting
Tim Van de Cruys
IRIT, CNRS
tim.vandecruys@irit.fr
Stergos Afantenos
IRIT, Toulouse University
stergos.afantenos@irit.fr
Philippe Muller
IRIT, Toulouse University
philippe.muller@irit.fr
Abstract
In this paper we present our system for the
SemEval 2013 Task 5a on semantic similar-
ity of words and compositional phrases. Our
system uses a dependency-based vector space
model, in combination with a technique called
latent vector weighting. The system computes
the similarity between a particular noun in-
stance and the head noun of a particular noun
phrase, which was weighted according to the
semantics of the modifier. The system is en-
tirely unsupervised; one single parameter, the
similarity threshold, was tuned using the train-
ing data.
1 Introduction
In the course of the last two decades, vector space
models have gained considerable momentum for se-
mantic processing. Initially, these models only dealt
with individual words, ignoring the context in which
these words appear. More recently, two different but
related approaches emerged that take into account
the interaction between different words within a par-
ticular context. The first approach aims at building a
joint, compositional representation for larger units
beyond the individual word level (e.g., the com-
posed, semantic representation of the noun phrase
crispy chips). The second approach, different but re-
lated to the first one, computes the specific meaning
of a word within a particular context (e.g. the mean-
ing of the noun bank in the context of the adjective
bankrupt).
In this paper, we describe our system for the Sem-
Eval 2013 Task 5a: semantic similarity of words and
compositional phrases ? which follows the latter ap-
proach. Our system uses a dependency-based vector
space model, in combination with a technique called
latent vector weighting (Van de Cruys et al, 2011).
The system computes the similarity between a par-
ticular noun instance and the head noun of a par-
ticular noun phrase, which was weighted according
to the semantics of the modifier. The system is en-
tirely unsupervised; one single parameter, the simi-
larity threshold, was tuned using the training data.
2 Related work
In recent years, a number of methods have been de-
veloped that try to capture the compositional mean-
ing of units beyond the individual word level within
a distributional framework. One of the first ap-
proaches to tackle compositional phenomena in a
systematic way is Mitchell and Lapata?s (2008) ap-
proach. They explore a number of different mod-
els for vector composition, of which vector addition
(the sum of each feature) and vector multiplication
(the elementwise multiplication of each feature) are
the most important. Baroni and Zamparelli (2010)
present a method for the composition of adjectives
and nouns. In their model, an adjective is a linear
function of one vector (the noun vector) to another
vector (the vector for the adjective-noun pair). The
linear transformation for a particular adjective is rep-
resented by a matrix, and is learned automatically
from a corpus, using partial least-squares regression.
Coecke et al (2010) present an abstract theoretical
framework in which a sentence vector is a function
of the Kronecker product of its word vectors, which
allows for greater interaction between the different
98
word features. And Socher et al (2012) present a
model for compositionality based on recursive neu-
ral networks.
Closely related to the work on compositionality
is research on the computation of word meaning in
context. Erk and Pado? (2008, 2009) make use of
selectional preferences to express the meaning of
a word in context. And Dinu and Lapata (2010)
propose a probabilistic framework that models the
meaning of words as a probability distribution over
latent factors. This allows them to model contex-
tualized meaning as a change in the original sense
distribution.
Our work takes the latter approach of computing
word meaning in context, and is described in detail
below.
3 Methodology
Our method uses latent vector weighting (Van de
Cruys et al, 2011) in order to compute a se-
mantic representation for the meaning of a word
within a particular context. The method relies
upon a factorization model in which words, together
with their window-based context features and their
dependency-based context features, are linked to la-
tent dimensions. The factorization model allows us
to determine which dimensions are important for a
particular context, and adapt the dependency-based
feature vector of the word accordingly. The mod-
ified feature vector is then compared to the target
noun feature vector with the cosine similarity func-
tion.
This following sections describe our model in
more detail. In section 3.1, we describe non-
negative matrix factorization ? the factorization
technique that our model uses. Section 3.2 describes
our way of combining dependency-based context
features and window-based context features within
the same factorization model. Section 3.3, then, de-
scribes our method of computing the meaning of a
word within a particular context.
3.1 Non-negative Matrix Factorization
Our latent model uses a factorization technique
called non-negative matrix factorization (Lee and
Seung, 2000) in order to find latent dimensions. The
key idea is that a non-negative matrix A is factorized
into two other non-negative matrices, W and H
Ai? j ?Wi?kHk? j (1)
where k is much smaller than i, j so that both in-
stances and features are expressed in terms of a few
components. Non-negative matrix factorization en-
forces the constraint that all three matrices must be
non-negative, so all elements must be greater than or
equal to zero.
Using the minimization of the Kullback-Leibler
divergence as an objective function, we want to find
the matrices W and H for which the divergence
between A and WH (the multiplication of W and
H) is the smallest. This factorization is carried
out through the iterative application of update rules.
Matrices W and H are randomly initialized, and the
rules in 2 and 3 are iteratively applied ? alternating
between them. In each iteration, each vector is ade-
quately normalized, so that all dimension values sum
to 1.
Ha? ?Ha?
?i Wia
Ai?
(WH)i?
?k Wka
(2)
Wia?Wia
?? Ha?
Ai?
(WH)i?
?v Hav
(3)
3.2 Combining syntax and context words
Using an extension of non-negative matrix fac-
torization (Van de Cruys, 2008), it is possible
to jointly induce latent factors for three different
modes: nouns, their window-based context words,
and their dependency-based context features. The
intuition is that the window-based context words
inform us about broad, topical similarity, whereas
the dependency-based features get at a tighter,
synonym-like similarity. As input to the algo-
rithm, two matrices are constructed that capture the
pairwise co-occurrence frequencies for the different
modes. The first matrix contains co-occurrence fre-
quencies of words cross-classified by dependency-
based features, and the second matrix contains co-
occurrence frequencies of words cross-classified by
words that appear in the word?s context window.
NMF is then applied to the two matrices, and the
separate factorizations are interleaved (i.e. matrix
W, which contains the nouns by latent dimensions,
99
is shared between both factorizations). A graphical
representation of the interleaved factorization algo-
rithm is given in figure 1. The numbered arrows in-
dicate the sequence of the updates.
= W=
U
I
V
K
I
Anouns xdependencies
Bnouns xcontext wordsI
HK U
3
21
4 GK V
Figure 1: A graphical representation of the interleaved
NMF
When the factorization is finished, the three dif-
ferent modes (words, window-based context words
and dependency-based context features) are all rep-
resented according to a limited number of latent fac-
tors.
The factorization that comes out of the NMF
model can be interpreted probabilistically (Gaussier
and Goutte, 2005; Ding et al, 2008). More specifi-
cally, we can transform the factorization into a stan-
dard latent variable model of the form
p(wi,d j) =
K
?
z=1
p(z)p(wi|z)p(d j|z) (4)
by introducing two K?K diagonal scaling matrices
X and Y, such that Xkk = ?i Wik and Ykk = ? j Hk j.
The factorization WH can then be rewritten as
WH = (WX?1X)(YY?1H)
= (WX?1)(XY)(Y?1H)
(5)
such that WX?1 represents p(wi|z), (Y?1H)T rep-
resents p(d j|z), and XY represents p(z). Using
Bayes? theorem, it is now straightforward to deter-
mine p(z|d j).
p(z|d j) =
p(d j|z)p(z)
p(d j)
(6)
3.3 Meaning in Context
3.3.1 Overview
Using the results of the factorization model de-
scribed above, we can now adapt a word?s feature
vector according to the context in which it appears.
Intuitively, the context of the word (in our case,
the dependency-based context feature that acts as an
adjectival modifier to the head noun) pinpoint the
important semantic dimensions of the particular in-
stance, creating a probability distribution over latent
factors. The required probability vector, p(z|d j), is
yielded by our factorization model. This probabil-
ity distribution over latent factors can be interpreted
as a semantic fingerprint of the passage in which the
target word appears. Using this fingerprint, we can
now determine a new probability distribution over
dependency features given the context.
p(d|d j) = p(z|d j)p(d|z) (7)
The last step is to weight the original probability
vector of the word according to the probability vec-
tor of the dependency features given the word?s con-
text, by taking the pointwise multiplication of prob-
ability vectors p(d|wi) and p(d|d j).
p(d|wi,d j) = p(d|wi) ? p(d|d j) (8)
Note that this final step is a crucial one in our
approach. We do not just build a model based on
latent factors, but we use the latent factors to de-
termine which of the features in the original word
vector are the salient ones given a particular context.
This allows us to compute an accurate adaptation of
the original word vector in context.
3.3.2 Example
Let us exemplify the procedure with an example.
Say we want to compute the distributionally similar
words to the noun instrument within the phrases (1)
and (2), taken from the task?s test set:
(1) musical instrument
(2) optical instrument
First, we extract the context feature for both in-
stances, in this case C1 = {musicalad j} for phrase
(1), and C2 = {opticalad j} for phrase (2). Next, we
100
look up p(z|C1) and p(z|C2) ? the probability distri-
butions over latent factors given the context ? which
are yielded by our factorization model. Using these
probability distributions over latent factors, we can
now determine the probability of each dependency
feature given the different contexts ? p(d|C1) and
p(d|C2) (equation 7).
The former step yields a general probability dis-
tribution over dependency features that tells us how
likely a particular dependency feature is given the
context that our target word appears in. Our last step
is now to weight the original probability vector of
the target word (the aggregate of dependency-based
context features over all contexts of the target word)
according to the new distribution given the context
in which the target word appears (equation 8).
We can now return to our original matrix A and
compute the top similar words for the two adapted
vectors of instrument given the different contexts,
which yields the results presented below.
1. instrumentN , C1: percussion, flute, violin,
melody, harp
2. instrumentN , C2: sensor, detector, amplifier,
device, microscope
3.4 Implementational details
Our model has been trained on the UKWaC cor-
pus (Baroni et al, 2009). The corpus has been
part of speech tagged and lemmatized with Stan-
ford Part-Of-Speech Tagger (Toutanova and Man-
ning, 2000; Toutanova et al, 2003), and parsed with
MaltParser (Nivre et al, 2006) trained on sections
2-21 of the Wall Street Journal section of the Penn
Treebank extended with about 4000 questions from
the QuestionBank1, so that dependency triples could
be extracted.
The matrices needed for our interleaved NMF fac-
torization are extracted from the corpus. Our model
was built using 5K nouns, 80K dependency relations,
and 2K context words2 (excluding stop words) with
highest frequency in the training set, which yields
matrices of 5K nouns ? 80K dependency relations,
and 5K nouns ? 2K context words.
1http://maltparser.org/mco/english_parser/
engmalt.html
2We used a fairly large, paragraph-like window of four sen-
tences.
model accuracy precision recall F1
dist .69 .83 .48 .61
lvw .75 .84 .61 .71
Table 1: Results of the distributional model (dist) and la-
tent vector weighting model (lvw) on the SemEval task
5a
The interleaved NMF model was carried out using
K = 600 (the number of factorized dimensions in the
model), and applying 100 iterations. The interleaved
NMF algorithm was implemented in Matlab; the pre-
processing scripts and scripts for vector computation
in context were written in Python.
The model is entirely unsupervised. The only pa-
rameter to set, the cosine similarity threshold ? , is
induced from the training set. We set ? = .049.
4 Results
Table 1 shows the evaluation results of the simple
distributional model (which only takes into account
the head noun) and our model that uses latent vector
weighting. The results indicate that our model based
on latent vector weighting performs quite a bit bet-
ter than a standard dependency-based distributional
model. The lvw model attains an accuracy of .75 ?
a 6% improvement over the distributional model ?
and an F-measure of .71 ? a 10% improvement over
the distributional model.
5 Conclusion
In this paper we presented an entirely unsuper-
vised system for the assessment of the similarity of
words and compositional phrases. Our system uses a
dependency-based vector space model, in combina-
tion with latent vector weighting. The system com-
putes the similarity between a particular noun in-
stance and the head noun of a particular noun phrase,
which was weighted according to the semantics of
the modifier. Using our system yields a substantial
improvement over a simple dependency-based dis-
tributional model, which only takes the head noun
into account.
101
References
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of the 2010 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1183?1193, Cambridge, MA, October. Association for
Computational Linguistics.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and
Eros Zanchetta. 2009. The wacky wide web: A
collection of very large linguistically processed web-
crawled corpora. Language Resources and Evalua-
tion, 43(3):209?226.
Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen Clark.
2010. Mathematical foundations for a compositional
distributed model of meaning. Lambek Festschrift,
Linguistic Analysis, vol. 36, 36.
Chris Ding, Tao Li, and Wei Peng. 2008. On the equiv-
alence between non-negative matrix factorization and
probabilistic latent semantic indexing. Computational
Statistics & Data Analysis, 52(8):3913?3927.
Georgiana Dinu and Mirella Lapata. 2010. Measuring
distributional similarity in context. In Proceedings of
the 2010 Conference on Empirical Methods in Natural
Language Processing, pages 1162?1172, Cambridge,
MA, October.
Katrin Erk and Sebastian Pado?. 2008. A structured
vector space model for word meaning in context. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages 897?906,
Waikiki, Hawaii, USA.
Katrin Erk and Sebastian Pado?. 2009. Paraphrase as-
sessment in structured vector space: Exploring param-
eters and datasets. In Proceedings of the Workshop on
Geometrical Models of Natural Language Semantics,
pages 57?65, Athens, Greece.
Eric Gaussier and Cyril Goutte. 2005. Relation between
PLSA and NMF and implications. In Proceedings of
the 28th annual international ACM SIGIR conference
on Research and development in information retrieval,
pages 601?602, Salvador, Brazil.
Daniel D. Lee and H. Sebastian Seung. 2000. Algo-
rithms for non-negative matrix factorization. In Ad-
vances in Neural Information Processing Systems 13,
pages 556?562.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. proceedings of ACL-
08: HLT, pages 236?244.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006. Malt-
parser: A data-driven parser-generator for dependency
parsing. In Proceedings of LREC-2006, pages 2216?
2219.
Richard Socher, Brody Huval, Christopher D. Manning,
and Andrew Y. Ng. 2012. Semantic compositional-
ity through recursive matrix-vector spaces. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 1201?
1211, Jeju Island, Korea, July. Association for Com-
putational Linguistics.
Kristina Toutanova and Christopher D. Manning. 2000.
Enriching the knowledge sources used in a maximum
entropy part-of-speech tagger. In Proceedings of the
Joint SIGDAT Conference on Empirical Methods in
Natural Language Processing and Very Large Corpora
(EMNLP/VLC-2000), pages 63?70.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Pro-
ceedings of HLT-NAACL 2003, pages 252?259.
Tim Van de Cruys, Thierry Poibeau, and Anna Korho-
nen. 2011. Latent vector weighting for word meaning
in context. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Processing,
pages 1012?1022, Edinburgh, Scotland, UK., July. As-
sociation for Computational Linguistics.
Tim Van de Cruys. 2008. Using three way data for word
sense discrimination. In Proceedings of the 22nd In-
ternational Conference on Computational Linguistics
(Coling 2008), pages 929?936, Manchester.
102
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 144?147, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
MELODI: A Supervised Distributional Approach
for Free Paraphrasing of Noun Compounds
Tim Van de Cruys
IRIT, CNRS
tim.vandecruys@irit.fr
Stergos Afantenos
IRIT, Toulouse University
stergos.afantenos@irit.fr
Philippe Muller
IRIT, Toulouse University
philippe.muller@irit.fr
Abstract
This paper describes the system submitted
by the MELODI team for the SemEval-2013
Task 4: Free Paraphrases of Noun Compounds
(Hendrickx et al, 2013). Our approach com-
bines the strength of an unsupervised distri-
butional word space model with a supervised
maximum-entropy classification model; the
distributional model yields a feature represen-
tation for a particular compound noun, which
is subsequently used by the classifier to induce
a number of appropriate paraphrases.
1 Introduction
Interpretation of noun compounds is making explicit
the relation between the component nouns, for in-
stance that running shoes are shoes used in running
activities, while leather shoes are made from leather.
The relations can have very different meanings, and
existing work either postulates a fixed set of rela-
tions (Tratz and Hovy, 2010) or relies on appropri-
ate descriptions of the relations, through constrained
verbal paraphrases (Butnariu et al, 2010) or uncon-
strained paraphrases as in the present campaign. The
latter is much simpler for annotation purposes, but
raises difficult challenges involving not only com-
pound interpretation but also paraphrase evaluation
and ranking.
In terms of constrained verbal paraphrases
Wubben (2010), for example, uses a supervised
memory-based ranker using features from the
Google n-gram corpus as well as WordNet. Nulty
and Costello (2010) rank paraphrases of compounds
according to the number of times they co-occurred
with other paraphrases for other compounds. They
use these co-occurrences to compute conditional
probabilities estimating is-a relations between para-
phrases. Li et al (2010) provide a hybrid sys-
tem which combines a Bayesian algorithm exploit-
ing Google n-grams, a score which captures human
preferences at the tail distribution of the training
data, as well as a metric that captures pairwise para-
phrase preferences.
Our methodology consists of two steps. First,
an unsupervised distributional word space model is
constructed, which yields a feature representation
for a particular compound. The feature representa-
tion is then used by a maximum entropy classifier to
induce a number of appropriate paraphrases.
2 Methodology
2.1 Distributional word space model
In order to induce appropriate feature representa-
tions for the various noun compounds, we start by
constructing a standard distributional word space
model for nouns. We construct a co-occurrence
matrix of the 5K most frequent nouns1 by the 2K
most frequent context words2, which occur in a win-
dow of 5 words to the left and right of the target
word. The bare frequencies of the word-context ma-
trix are weighted using pointwise mutual informa-
tion (Church and Hanks, 1990).
Next, we compute a joint, compositional repre-
sentation of the noun compound, combining the se-
1making sure all nouns that appear in the training and test
set are included
2excluding the 50 most frequent context words as stop words
144
mantics of the head noun with the modifier noun. To
do so, we make use of a simple vector-based multi-
plicative model of compositionality, as proposed by
Mitchell and Lapata (2008). In order to compute the
compositional representation of a compound noun,
this model takes the elementwise multiplication of
the vectors for the head noun and the modifier noun,
i.e.
pi = uivi
for each feature i. The resulting features are used as
input to our next classification step.
We compare the performance of the abovemen-
tioned compositional model with a simpler model
that only takes into account the semantics of the
head noun. This model only uses the context fea-
tures for the head noun as input to our second clas-
sification step. This means that the model only takes
into account the semantics of the head noun, and ig-
nores the semantics of the modifier noun.
2.2 Maximum entropy classification
The second step of our paraphrasing system consists
of a supervised maximum entropy classification ap-
proach. Training vectors for each noun compound
from the training set are constructed according to
the approach described in the previous section. The
(non-zero) context features yielded by the first step
are used as input for the maximum entropy classi-
fier, together with the appropriate paraphrase labels
and the label counts (used to weight the instances),
which are extracted from the training set.
We then deploy the model in order to induce a
probability distribution over the various paraphrase
labels. Every paraphrase label above a threshold ? is
considered an appropriate paraphrase. Using a por-
tion of held-out training data (20%), we set ? = 0.01
for our official submission. In this paper, we show a
number of results using different thresholds.
2.3 Set of paraphrases labels
For our classification approach to work, we need to
extract an appropriate set of paraphrase labels from
the training data. In order to create this set, we
substitute the nouns that appear in the training set?s
paraphrases by dummy variables. Table 1 gives an
example of three different paraphrases and the re-
sulting paraphrase labels after substitution. Note
that we did not apply any NLP techniques to prop-
erly deal with inflected words.
We apply a frequency threshold of 2 (counted over
all the instances), so we discard paraphrase labels
that appear only once in the training set. This gives
us a total of 285 possible paraphrase labels.
One possible disadvantage of this supervised ap-
proach is a loss of recall on unseen paraphrases. A
rough estimation shows that our set of training labels
accounts for only 25% of the similarly constructed
labels extracted from the test set. However, the most
frequently used paraphrase labels are present in both
training and test set, so this does not prevent our
system to come up with a number of suitable para-
phrases for the test set.
2.4 Implementational details
All frequency co-occurrence information has been
extracted from the ukWaC corpus (Baroni et al,
2009). The corpus has been part of speech tagged
and lemmatized with Stanford Part-Of-Speech Tag-
ger (Toutanova and Manning, 2000; Toutanova et
al., 2003). Distributional word space algorithms
have been implemented in Python. The maximum
entropy classifier was implemented using the Maxi-
mum Entropy Modeling Toolkit for Python and C++
(Le, 2004).
3 Results
Table 2 shows the results of the different systems in
terms of the isomorphic and non-isomorphic evalu-
ation measures defined by the task organizers (Hen-
drickx et al, 2013). For comparison, we include a
number of baselines. The first baseline assigns the
two most frequent paraphrase labels (Y of X, Y for
X) to each test instance; the second baseline assigns
the four most frequent paraphrase labels (Y of X, Y
for X, Y on X, Y in X); and the third baseline assigns
all of the possible 285 paraphrase labels as correct
answer for each test instance.
For both our primary system (the multiplicative
model) and our contrastive system (the head noun
model), we vary the threshold used to select the final
set of paraphrases. A threshold ? = 0.01 results in
a smaller set of paraphrases, whereas a threshold of
? = 0.001 results in a broad set of paraphrases. Our
official submission uses the former threshold.
145
compound paraphrase paraphrase label
textile company company that makes textiles Y that makes Xs
textile company company that produces textiles Y that produces Xs
textile company company in textile industry Y in X industry
Table 1: Example of induced paraphrase labels
model ? isomorphic non-isomorphic
baseline (2) ? .058 .808
baseline (4) ? .090 .633
baseline (all) ? .332 .200
multiplicative .01 .130 .548
.001 .270 .259
head noun .01 .136 .536
.001 .277 .302
Table 2: Results
First of all, we note that the different baseline
models are able to obtain substantial scores for the
different evaluation measures. The first two base-
lines, which use a limited number of paraphrase
labels, perform very well in terms of the non-
isomorphic evaluation measure. The third baseline,
which uses a very large number of candidate para-
phrase labels, gets more balanced results in terms of
both the isomorphic and non-isomorphic measure.
Considering our different thresholds, the results
of our models are in line with the baseline re-
sults. A larger threshold, which results in a smaller
number of paraphrase labels, reaches a higher non-
isomorphic score. A smaller threshold, which re-
sults in a larger number of paraphrase labels, gives
more balanced results for the isomorphic and non-
isomorphic measure.
There does not seem to be a significant difference
between our primary system (multiplicative) and our
contrastive system (head noun). For ? = 0.01, the
results of both models are very similar; for ? =
0.001, the head noun model reaches slightly better
results, in particular for the non-isomorphic score.
Finally, we note that our models do not seem to
improve significantly on the baseline scores. For
? = 0.001, the results of our models seem somewhat
more balanced compared to the all baseline, but the
differences are not very large. In general, our sys-
tems (in line with the other systems participating in
the task) seem to have a hard time beating a num-
ber of simple baselines, in terms of the evaluation
measures defined by the task.
4 Conclusion
We have presented a system for producing free para-
phrases of noun compounds. Our methodology con-
sists of two steps. First, an unsupervised distribu-
tional word space model is constructed, which is
used to compute a feature representation for a par-
ticular compound. The feature representation is then
used by a maximum entropy classifier to induce a
number of appropriate paraphrases.
Although our models do seem to yield slightly
more balanced scores than the baseline models, the
differences are not very large. Moreover, there is
no substantial difference between our primary mul-
tiplicative model, which takes into account the se-
mantics of both head and modifier noun, and our
contrastive model, which only uses the semantics of
the head noun.
References
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and
Eros Zanchetta. 2009. The wacky wide web: A
collection of very large linguistically processed web-
crawled corpora. Language Resources and Evalua-
tion, 43(3):209?226.
Cristina Butnariu, Su Nam Kim, Preslav Nakov, Diar-
muid O? Se?aghdha, Stan Szpakowicz, and Tony Veale.
2010. Semeval-2 task 9: The interpretation of noun
compounds using paraphrasing verbs and prepositions.
In Proceedings of the 5th International Workshop on
Semantic Evaluation, pages 39?44, Uppsala, Sweden,
July. Association for Computational Linguistics.
Kenneth W. Church and Patrick Hanks. 1990. Word as-
sociation norms, mutual information & lexicography.
Computational Linguistics, 16(1):22?29.
146
Iris Hendrickx, Zornitsa Kozareva, Preslav Nakov, Diar-
muid O? Se?aghdha, Stan Szpakowicz, and Tony Veale.
2013. SemEval-2013 task 4: Free paraphrases of noun
compounds. In Proceedings of the International Work-
shop on Semantic Evaluation, SemEval ?13, June.
Zhang Le. 2004. Maximum entropy modeling toolkit for
python and c++. http://homepages.inf.ed.ac.
uk/lzhang10/maxent_toolkit.html.
Guofu Li, Alejandra Lopez-Fernandez, and Tony Veale.
2010. Ucd-goggle: A hybrid system for noun com-
pound paraphrasing. In Proceedings of the 5th In-
ternational Workshop on Semantic Evaluation, pages
230?233, Uppsala, Sweden, July. Association for
Computational Linguistics.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. proceedings of ACL-
08: HLT, pages 236?244.
Paul Nulty and Fintan Costello. 2010. Ucd-pn: Select-
ing general paraphrases using conditional probability.
In Proceedings of the 5th International Workshop on
Semantic Evaluation, pages 234?237, Uppsala, Swe-
den, July. Association for Computational Linguistics.
Kristina Toutanova and Christopher D. Manning. 2000.
Enriching the knowledge sources used in a maximum
entropy part-of-speech tagger. In Proceedings of the
Joint SIGDAT Conference on Empirical Methods in
Natural Language Processing and Very Large Corpora
(EMNLP/VLC-2000), pages 63?70.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Pro-
ceedings of HLT-NAACL 2003, pages 252?259.
Stephen Tratz and Eduard Hovy. 2010. A taxonomy,
dataset, and classifier for automatic noun compound
interpretation. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 678?687, Uppsala, Sweden, July. Associa-
tion for Computational Linguistics.
Sander Wubben. 2010. Uvt: Memory-based pairwise
ranking of paraphrasing verbs. In Proceedings of the
5th International Workshop on Semantic Evaluation,
pages 260?263, Uppsala, Sweden, July. Association
for Computational Linguistics.
147
Proceedings of the 11th Meeting of the ACL-SIGMORPHON, ACL 2010, pages 46?53,
Uppsala, Sweden, 15 July 2010.
c?2010 Association for Computational Linguistics
Exploring dialect phonetic variation using PARAFAC
Jelena Proki
?
c
University of Groningen
The Netherlands
j.prokic@rug.nl
Tim Van de Cruys
University of Groningen
The Netherlands
t.van.de.cruys@rug.nl
Abstract
In this paper we apply the multi-way de-
composition method PARAFAC in order to
detect the most prominent sound changes
in dialect variation. We investigate various
phonetic patterns, both in stressed and un-
stressed syllables. We proceed from regu-
lar sound correspondences which are auto-
matically extracted from the aligned tran-
scriptions and analyzed using PARAFAC.
This enables us to analyze simultaneously
the co-occurrence patterns of all sound
correspondences found in the data set and
determine the most important factors of
the variation. The first ten dimensions are
examined in more detail by recovering the
geographical distribution of the extracted
correspondences. We also compare dia-
lect divisions based on the extracted cor-
respondences to the divisions based on the
whole data set and to the traditional schol-
arship as well. The results show that PAR-
AFAC can be successfully used to detect
the linguistic basis of the automatically
obtained dialect divisions.
1 Introduction
Dialectometry is a multidisciplinary field that uses
quantitative methods in the analysis of dialect
data. From the very beginning, most of the re-
search in dialectometry has been focused on the
identification of dialect groups and development
of methods that would tell us how similar (or dif-
ferent) one variety is when compared to the neigh-
boring varieties. Dialect data is usually analyzed
on the aggregate level by summing up the differ-
ences between various language varieties into a
single number. The main drawback of aggregate
analyses is that it does not expose the underlying
linguistic structure, i.e. the specific linguistic ele-
ments that contributed to the differences between
the dialects. In recent years there have been sev-
eral attempts to automatically extract linguistic
basis from the aggregate analysis, i.e. to determine
which linguistic features are responsible for which
dialect divisions. Although interesting for dialect-
ology itself, this kind of research is very important
in the investigation of sound variation and change,
both on the synchronic and diachronic level.
The paper is structured as follows. In the next
section, we discuss a number of earlier approaches
to the problem of identifying underlying linguistic
structure in dialect divisions. In section 3, we give
a description of the dialect data used in this re-
search. Section 4 then describes the methodology
of our method, explaining our data representation
using tensors, our three-way factorization method,
and the design of our data set. In section 5, the res-
ults of our method are discussed, examining the
values that come out of our factorization method
in a number of ways. Section 6, then, draws con-
clusions and gives some pointers for future work.
2 Previous work
In order to detect the linguistic basis of dialect
variation Nerbonne (2006) applied factor analysis
to the results of the dialectometric analysis of
southern American dialects. The analysis is based
on 1132 different vowels found in the data. 204
vowel positions are investigated, where a vowel
position is, e.g., the first vowel in the word ?Wash-
ington? or the second vowel in the word ?thirty?.
Factor analysis has shown that 3 factors are most
important, explaining 35% of the total amount of
variation. However, this approach is based only on
vowel positions in specific words.
Proki?c (2007) extracted the 10 most frequent
non-identical sound correspondences from the
aligned word transcriptions. Based on the relative
frequency of each of these correspondences each
site in the data set was assigned a correspondence
index. Higher value of this index indicates sites
46
where the presence of a certain sound is domin-
ant with respect to some sound alternation. Al-
though successful in describing some important
sound alternations in the dialect variation, it ex-
amines only the 10 most frequent sound alterna-
tions without testing patterns of variation between
different sound correspondences.
Shackleton (2007) applies principal compon-
ent analysis (PCA) to a group of self constructed
articulation-based features. All segments found in
the data are translated into vectors of numerical
features and analyzed using PCA. Based on the
component scores for features, different groups
of varieties (in which a certain group of features
is present) are identified. We note that the main
drawback of this approach is the subjectivity of the
feature selection and segment quantification.
Wieling and Nerbonne (2009) used a bipart-
ite spectral graph partitioning method to simul-
taneously cluster dialect varieties and sound cor-
respondences. Although promising, this method
compares the pronunciation of every site only to
the reference site, rather than comparing it to all
other sites. Another drawback of this method is
that it does not use any information on the frequen-
cies of sound correspondences, but instead em-
ploys binary features to represent whether a cer-
tain correspondence is present at a certain site or
not.
In this paper we present an approach that tries
to overcome some of the problems described in
the previous approaches. It proceeds from auto-
matically aligned phonetic transcriptions, where
pronunciations of every site are compared to the
corresponding pronunciations for all other sites.
Extracted sound correspondences are analyzed us-
ing the multi-way decomposition method PARA-
FAC. The method allows us to make generaliza-
tions over multi-way co-occurrence data, and to
look simultaneously at the co-occurrence patterns
of all sound correspondences found in the data set.
3 Data description
The data set used in this paper consists of phon-
etic transcriptions of 152 words collected at 197
sites evenly distributed all over Bulgaria. It is part
of the project Buldialect ? Measuring Linguistic
unity and diversity in Europe. Phonetic transcrip-
tions include various diacritics and suprasegment-
als, making the total number of unique phones in
the data set 95: 43 vowels and 52 consonants.
1
The sign for primary stress is moved to a cor-
responding vowel, so that there is a distinction
between stressed and unstressed vowels. Vowels
are also marked for their length. Sonorants /r/ and
/l/ have a mark for syllabicity and for stress in case
they are syllabic. Here we list all phones present
in the data set:
"A, e, i, "e, @, "E, 7, "6, A, I, o, "o, u, "A:, U, "u:, "7, "@,
"a, "i, "I, "e:, E, "O, "2, "i:, "u, e:, 1, "1, "o:, "E:, "7:, u:, A:,
y, "a:, a, o:, 7:, "U, "y, "I:, j, g, n, n
j
, ?, r, w, x, r
j
, h,
C, f, s, v, c?, F, p,
>
?, m, k,
>
?C, p
j
, c, l, l
j
, t, t
j
, S, d, d
j
,
"r
"
, v
j
,
>
dz, Z, ?,
>
?, r
"
, c
j
, z, s
j
, b, g
j
, m
j
, l
"
, z
j
, "l
"
, k
j
, b
j
,
>
??,
>
dz, f
j
, ?
Each of the 152 words in the data set shows
phonetic variation, with some words displaying
more than one change. There are in total 39 dif-
ferent dialectal features that are represented in the
data set, with each of the features being present in
a similar number of words. For example, the re-
flexes of Old Bulgarian vowels that show dialect
variation are represented with the same or nearly
the same number of words. A more detailed de-
scription of all features can be found in Proki?c et
al. (2009). For all villages only one speaker was
recorded. In the data set, for some villages there
were multiple pronunciations of the same word. In
this reasearch we have randomly picked only one
per every village.
4 Methodology
4.1 Tensors
Co-occurrence data (such as the sound corres-
pondences used in this research) are usually rep-
resented in the form of a matrix. This form is per-
fectly suited to represent two-way co-occurrence
data, but for co-occurrence data beyond two
modes, we need a more general representation.
The generalization of a matrix is called a tensor.
A tensor is able to encode co-occurrence data of
any n modes. Figure 1 shows a graphical com-
parison of a matrix and a tensor with three modes
? although a tensor can easily be generalized to
more than three modes.
Tensor operations come with their own algeb-
raic machinery. We refer the interested reader to
Kolda and Bader (2009) for a thorough and in-
sightful introduction to the subject.
1
The data is publicly available and can be down-
loaded from http://www.bultreebank.org/
BulDialects/index.html
47




	
	





Figure 1: Matrix representation vs. tensor representation.
4.2 PARAFAC
In order to create a succinct and generalized
model, the co-occurrence data are often ana-
lyzed with dimensionality reduction techniques.
One of the best known dimensionality reduction
techniques is principal component analysis (PCA,
Pearson (1901)). PCA transforms the data into a
new coordinate system, yielding the best possible
fit in a least squares sense given a limited num-
ber of dimensions. Singular value decomposition
(SVD) is the generalization of the eigenvalue de-
composition used in PCA (Wall et al, 2003).
To be able to make generalizations among the
three-way co-occurrence data, we apply a statist-
ical dimensionality reduction technique called par-
allel factor analysis (PARAFAC, Harshman (1970);
Carroll and Chang (1970)), a technique that has
been sucessfully applied in areas such as psycho-
logy and bio-chemistry. PARAFAC is a multilinear
analogue of SVD. The key idea is to minimize the
sum of squares between the original tensor and the
factorized model of the tensor. For the three mode
case of a tensor T ? R
D
1
?D
2
?D
3
this gives the
objective function in 1, where k is the number of
dimensions in the factorized model and ? denotes
the outer product.
min
x
i
?R
D1
,y
i
?R
D2
,z
i
?R
D3
? T ?
k
?
i=1
x
i
? y
i
? z
i
?
2
F
(1)
The algorithm results in three matrices, indic-
ating the loadings of each mode on the factorized
dimensions. The model is represented graphically
in Figures 2 and 3. Figure 2 visualizes the fact
that the PARAFAC decomposition consists of the
summation over the outer products of n (in this
case three) vectors. Figure 3 represents the three
resulting matrices that come out of the factoriza-
tion, indicating the loadings of each mode on the
factorized dimensions. We will be using the latter
representation in our research.
Computationally, the PARAFAC model is fitted
by applying an alternating least-squares algorithm.
In each iteration, two of the modes are fixed and
the third one is fitted in a least squares sense. This
process is repeated until convergence.
2
4.3 Sound correspondences
In order to detect the most important sound vari-
ation within Bulgarian dialects, we proceed from
extracting all sound correspondences from the
automatically aligned word transcriptions. All
transcriptions were pairwise aligned using the
Levenshtein algorithm (Levenshtein, 1965) as im-
plemented in the program L04.
3
The Leven-
shtein algorithm is a dynamic programming al-
gorithm used to measure the differences between
two strings. The distance between two strings is
the smallest number of insertions, deletions, and
substitutions needed to transform one string to the
other. In this work all three operations were as-
signed the same value, namely 1. The algorithm is
also directly used to align two sequences. An ex-
ample showing two aligned pronunciations of the
word v?lna /v7lna/ ?wool? is given in Figure 4.
4
v "7 - n A
v "A l n @
Figure 4: Example of two pairwise aligned word
transcriptions.
From the aligned transcriptions for all words
and all villages in the data set we first extracted
2
The algorithm has been implemented in MATLAB, using
the Tensor Toolbox for sparse tensor calculations (Bader and
Kolda, 2009).
3
http://www.let.rug.nl/kleiweg/L04
4
For some pairs of transcriptions there are two or more
possible alignments, i.e. alignments that have the same cost.
In these cases we have randomly picked only one of them.
48
Figure 2: Graphical representation of PARAFAC as the sum of outer products.

	



	



	




	





Figure 3: Graphical representation of the PARAFAC as three loadings matrices.
all corresponding non-identical sounds. For ex-
ample, from the aligned transcriptions in Figure 4
we would extract the following sound pairs: ["7]-
["A], [-]-[l], [A]-[@]. The hyphen (?-?) stands for a
missing (i.e. inserted or deleted) sound, and in fur-
ther analyses it is treated the same as any sound
in the data set. For each pair of corresponding
sounds from the data set we counted how often it
appeared in the aligned transcriptions for each pair
of villages separately. In total we extracted 907
sound correspondences and stored the information
on each of them in a separate matrix. Every matrix
records the distances between each two villages
in the data set, measured as the number of times
a certain phonetic alternation is recorded while
comparing pronunciations from these sites.
Since we are interested in analyzing all sound
correspondences simultaneously, we merged the
information from all 907 two-mode matrices into
a three-mode tensor n?n?v, where n represents
the sites in the data set, and v represents the sound
alternations. By arranging our data in a cube in-
stead of a matrix, we are able to look into several
sets of variables simultaneously. We are especially
interested in the loadings for the third mode, that
contains the values for the sound correspondences.
5 Results
In order to detect the most prominent sound cor-
respondences we analyzed the three-mode tensor
described in the previous section using a PARAFAC
factorization with k = 10 dimensions. In Table 5
we present only the first five dimensions extracted
by the algorithm. The final model fits 44% of the
original data. The contribution of the first extrac-
ted dimension (dim1) to the final fit of the model
is the largest ? 23.81 per cent ? while the next four
dimensions contribute to the final fit with similar
percentages: dim2 with 10.63 per cent, dim3 with
9.50 per cent, dim4 with 9.26 per cent, and dim5
with 9.09 per cent. Dimensions six to ten contrib-
ute in the range from 8.66 per cent to 6.98 per cent.
For every dimension we extracted the twenty
sound correspondences with the highest scores. In
the first dimension we find 11 pairs involving vow-
els and 9 involving consonant variation. The three
sound correspondences with the highest scores
are the [A]-[@], [o]-[u], and [e]-[i] alternations.
This finding corresponds well with the traditional
scholarly views on Bulgarian phonetics (Wood
and Pettersson, 1988; Barnes, 2006) where we find
that in unstressed syllables mid vowels [e] and [o]
raise to neutralize with the high vowels [i] and [u].
The low vowel [a] raises to merge with [@].
For every sound alternation we also check their
geographical distribution. We do so by applying
the following procedure. From the aligned pairs
of transcriptions we extract corresponding pairs of
sounds for every alternation. We count how many
times each of the two sounds appears in the tran-
scriptions for every village. Thus, for every pair of
sound correspondences, we can create two maps
that show the distribution of each of the sounds
separately. On the map of Bulgaria these values
49
Table 1: First five dimensions for the sound cor-
respondences.
dim1 dim2 dim3 dim4 dim5
[A]-[@] [@]-[7] [u]-[o] [A]-[@] [e]-[i]
[u]-[o] [e]-[i] [A]-[7] [@]-[7] [i]-["e]
[e]-[i] ["e]-["E] [A]-[@] [U]-[o] [e]-[@]
[-]-[j] [-]-[j] [7]-[e] [e]-[@] [r]-[r
j
]
[e]-["e] [S]-[C] [e]-["e] [d]-[d
j
] [d]-[d
j
]
[S]-[C] [
>
?]-[
>
?C] ["e]-["E] [v]-[v
j
] ["e]-["A]
[
>
?]-[
>
?C] ["A]-["E] [-]-[j] [n]-[n
j
] [-]-[j]
["e]-["E] [r]-[r
j
] ["e]-["A] [-]-[j] ["o]-[u]
[n]-[n
j
] [l]-[l
j
] [e]-[i] ["e]-["E] [l]-[l
j
]
[A]-[7] [e]-[@] [n]-[n
j
] [l]-[l
j
] [v]-[v
j
]
[e]-[@] [d]-[d
j
] [r]-[r
j
] [t]-[t
j
] [u]-[o]
["A]-["E] [n]-[n
j
] [
>
?]-[
>
?C] ["e]-["A] [n]-[n
j
]
["e]-["A] [u]-[U] ["7]-["A] [e]-["e] [-]-[v]
[d]-[d
j
] ["7]-["O] [-]-[r] [S]-[C] ["7]-[@]
[7]-[e] [@]-["A] [S]-[C] [
>
?]-[
>
?C] [u]-[U]
[l]-[l
j
] [7]-[e] [l]-[l
j
] [r]-[r
j
] [
>
?]-[
>
?C]
[v]-[v
j
] ["o]-[u] [u]-[e] [p]-[p
j
] ["A]-["E]
[r]-[r
j
] [Z]-[?] [-]-["7] [Z]-[?] [A]-["7]
[Z]-[?] [i]-[@] [v]-[-] [@]-["A] [@]-["A]
["7]-["O] [v]-[v
j
] [A]-["7] [e]-[i] [b]-[b
j
]
are represented using a gradual color, which en-
ables us to see not only the geographic distribution
of a certain sound but also how regular it is in a
given sound alternation. The highest scoring sites
are coloured black and the lowest scoring sites are
coloured white.
In Figure 5 we see the geographical distribu-
tion of the first three extracted correspondences.
The first two alternations [A]-[@] and [o]-[u] have
almost the same geographical distribution and di-
vide the country into west and east. While in the
west there is a clear presence of vowels [A] and [o],
in the east those vowels would be pronounced as
[@] and [u]. The division into east and west corres-
ponds well with the so-called jat line, which is,
according to traditional dialectologists (Stojkov,
2002) the main dialect border in Bulgaria. On the
maps in Figure 5 we represent it with the black line
that roughly divides Bulgaria into east and west.
The third correspondence follows a slightly dif-
ferent pattern: mid vowel [e] is present not only
west of the jat line, but also in the southern part
of the country, in the region of Rodopi mountains.
In the central and northeastern areas this sound is
pronounced as high vowel [i]. For all three sound
correspondences we see a clear two-way division
of the country, with almost all sites being charac-
terized by one of the two pronunciations, which,
as we shall see later, is not always the case due
to multiple reflections of some sounds at certain
positions.
We also note that the distribution of the sound
correspondences that involve soft consonants and
their counterparts have the same east-west dis-
tribution (see Figure 6). In the first dimension
we find the following consonants and their pal-
atal counterparts [n], [d], [l], [v] and [r], but be-
cause of space limitations we show maps only
for three correspondences. The east-west division
also emerges with respect to the distribution of the
[A]-[7] and ["e]-["A] sounds.
Unlike the correspondences mentioned before,
the [S]-[C], [
>
?]-[
>
?C], and [Z]-[?] pairs are defining
the south part of the country as a separate zone.
As shown on the maps in Figure 7, the southern
part of the country (the region of Rodopi moun-
tains) is characterized by a soft pronunciation of
[S], [
>
?] and [Z]. In traditonal literature on Bul-
garain dialectology (Stojkov, 2002), we also find
that soft pronunciation of [S], [
>
?] and [Z] is one of
the most important phonetic features of the variet-
ies in the Rodopi zone. Based on the correspond-
ences extracted in the first dimension, this area
is also defined by the presence of the vowel ["E]
in stressed syllables (["e]-["E] and ["A]-["E] corres-
pondences).
In some extracted correspondences, only one of
the sounds has a geographically coherent distri-
bution, like in the case of the [7]-[e] pair where
[e] is found in the west and south, while the [7]
sound is only sporadically present in the central
region. This kind of asymmetrical distribution is
also found with respect to the pair [A]-[7].
Most of the sound correspondences in the first
dimension either divide the country along the jat
line or separate the Rodopi area from the rest of the
varieties. The only two exceptions are the [-]-[j]
and ["7]-["O] pairs. They both define the southwest
area as a separate zone, while the northwest shares
its pronunciation of the sound in question with the
eastern part of the country.
We use the first 20 correspondences from the
first dimension and perform k-means clustering in
order to check which dialect areas would emerge
based on this limited set of sound correspond-
50
Figure 5: [A]-[@] (left), [o]-[u] (middle), [e]-[i] (right) sound correspondences.
Figure 6: [d]-[d
j
] (left), [v]-[v
j
] (middle), [r]-[r
j
] (right) sound correspondences.
ences. The results of the 2-way, 3-way and 4-way
clustering are given in Figure 8.
In two-way clustering the algorithm detects an
east-west split approximately along the jat line,
slightly moved to the east. This fully corres-
ponds to the traditional dialectology but also to
the results obtained using Levenshtein algorithm
on the whole data set where only east, west and
south varieties could be asserted with great con-
fidence (Proki?c and Nerbonne, 2008). In Figure 9
we present the dialect divisions that we get if the
distances between the sites are calculated using
whole word transcriptions instead of only the 20
most prominent sound correspondences. We no-
tice a high correspondence between the two ana-
lyses at the two- and three-level division. On the
level of four and more groups, the two analyses
start detecting different groups. In the analysis
based on 20 sound correspondences, southern dia-
lects are divided into smaller and smaller groups,
while in the analysis based on the whole data set,
the area in the west ? near the Serbian border ?
emerges as the fourth group. This is no surprise, as
the first 20 extracted correspondences do not con-
tain any sounds typical only for this western area.
In order to compare two divisions of sites, we
calculated the adjusted Rand index (Hubert and
Arabie, 1985). The adjusted Rand index (ARI) is
used in classification for comparing two different
partitions of a finite set of objects. It is based on
the Rand index (Rand, 1971), one of the most pop-
ular measures for comparing the degree to which
partitions agree (in classification). Value 1 of the
ARI indicates that two classifications match per-
fectly, while value 0 means that two partitions do
not agree on any pair of points. For both two-
level and three-level divisions of the sites the ARI
for two classifications is 0.84. We also compared
51
Figure 7: [S]-[C] (left), [
>
?]-[
>
?C] (middle), [Z]-[?] (right) sound correspondences.
Figure 8: Dialect varieties detected by k-means clustering algorithm based on the first 20 sound corres-
pondences in the first dimension.
Figure 9: Dialect varieties detected by k-means clustering algorithm based on all word transcriptions.
both of the classifications to the classification of
the sites done by Stojkov (2002). For the classi-
fication based on the first dimension extracted by
PARAFAC, ARI is 0.73 for two-way and 0.64 for
the three-way division. ARI score for the clas-
sification based on whole word transcriptions is
0.69 for two-way and 0.62 for three-way. As in-
dicated by ARI the two classifications correspond
with a high degree to each other, but to the tra-
ditional classification as well. We note that two-
way classification based on the extracted sound
correspondences corresponds higher to the tradi-
tional classification than classification that takes
all sounds into account.
We conclude that the sound correspondences
detected by PARAFAC form the linguistic basis
of the two-way and three-way divisions of Bul-
garian dialect area. Using the PARAFAC method
we are able to detect that the most important sound
changes on which two-way division is based are
[o]-[u], [A]-[@] and palatal pronunciation of con-
sonants. In the three-way division of sites done
by k-means, the area in the south of the country
appears as the third most important dialect zone.
In the twenty investigated sound correspondences
we find that the soft pronunciation of [S],[
>
?] and
[Z] sounds is typical only for the varieties in this
area. Apart from divisions that divide the country
into west and east, including the southern variet-
ies, we also detect sound correspondences whose
distribution groups together western and southern
areas.
We also analyzed in more depth sound corres-
pondences extracted in other dimensions by the
PARAFAC algorithm. Most of the correspondences
found in the first dimension, also reappear in the
following nine dimensions. Closer inspection of
the language groups obtained using information
52
from these dimensions show that eastern, western
and southern varieties are the only three that are
identified. No other dialect areas were detected
based on the sound correspondences from these
nine dimensions.
6 Conclusion
In this paper we have applied PARAFAC in the task
of detecting the linguistic basis of dialect phonetic
variation. The distances between varieties were
expressed as a numerical vector that records in-
formation on all sound correspondences found in
the data set. Using PARAFAC we were able to ex-
tract the most important sound correspondences.
Based on the 20 most important sound correspond-
ences we performed clustering of all sites in the
data set and were able to detect three groups of
sites. As found in traditional literature on Bul-
garian dialects, these three dialects are the main
dialect groups in Bulgaria. Using the aggregate
approach on the same data set, the same three dia-
lects were the only groups in the data that could be
asserted with high confidence. We conclude that
this approach is successful in extracting underly-
ing linguistic structure in dialect variation, while
at the same time overcoming some of the problems
found in the earlier approaches to this problem.
In future work sounds in the data set could be
defined in a more sophisticated way, using some
kind of feature representation. Also, the role of
stress should be examined in more depth, since
there are different patterns of change in stressed
in unstressed syllables. We would also like to ex-
tend the method and examine more than just two
sound correspondences at a time.
References
Brett W. Bader and Tamara G. Kolda. 2009. Matlab
tensor toolbox version 2.3. http://csmr.ca.
sandia.gov/
?
tgkolda/TensorToolbox/,
July.
Jonathan Barnes. 2006. Strength and Weakness at
the Interface: Positional Neutralization in Phonetics
and Phonology. Walter de Gruyter GmbH, Berlin.
J. Douglas Carroll and Jih-Jie Chang. 1970. Analysis
of individual differences in multidimensional scal-
ing via an n-way generalization of ?eckart-young?
decomposition. Psychometrika, 35:283?319.
Richard A. Harshman. 1970. Foundations of the par-
afac procedure: models and conditions for an ?ex-
planatory? multi-mode factor analysis. In UCLA
Working Papers in Phonetics, volume 16, pages 1?
84, Los Angeles. University of California.
Lawrence Hubert and Phipps Arabie. 1985. Compar-
ing partitions. Journal of Classification, 2:193?218.
Tamara G. Kolda and Brett W. Bader. 2009. Tensor de-
compositions and applications. SIAM Review, 51(3),
September.
Vladimir Levenshtein. 1965. Binary codes capable of
correcting deletions, insertions and reversals. Dok-
lady Akademii Nauk SSSR, 163:845?848.
John Nerbonne. 2006. Identifying linguistic structure
in aggregate comparison. Literary and Linguistic
Computing, 21(4):463?476.
Karl Pearson. 1901. On lines and planes of closest
fit to systems of points in space. Philosophical
Magazine, 2(6):559?572.
Jelena Proki?c and John Nerbonne. 2008. Recogniz-
ing groups among dialects. International Journal of
Humanities and Arts Computing, Special Issue on
Language Variation ed. by John Nerbonne, Char-
lotte Gooskens, Sebastian K?urschner, and Ren?ee van
Bezooijen:153?172.
Jelena Proki?c, John Nerbonne, Vladimir Zhobov, Petya
Osenova, Krili Simov, Thomas Zastrow, and Erhard
Hinrichs. 2009. The Computational Analysis of
Bulgarian Dialect Pronunciation. Serdica Journal of
Computing, 3:269?298.
Jelena Proki?c. 2007. Identifying linguistic structure
in a quantitative analysis of dialect pronunciation.
In Proceedings of the ACL 2007 Student Research
Workshop, pages 61?66.
WilliamM. Rand. 1971. Objective criteria for the eval-
uation of clustering methods. Journal of American
Statistical Association, 66(336):846?850, Decem-
ber.
Robert G. Shackleton. 2007. Phonetic variation in the
traditional English dialects. Journal of English Lin-
guistics, 35(1):30?102.
Stojko Stojkov. 2002. Bulgarska dialektologiya.
Sofia, 4th ed.
Michael E. Wall, Andreas Rechtsteiner, and Luis M.
Rocha, 2003. Singular Value Decomposition and
Principal Component Analysis, chapter 5, pages 91?
109. Kluwer, Norwell, MA, Mar.
Martijn Wieling and John Nerbonne. 2009. Bipart-
ite spectral graph partitioning to co-cluster varieties
and sound correspondences in dialectology. In Text
Graphs 4, Workshop at the 47th Meeting of the Asso-
ciation for Computational Linguistics, pages 14?22.
Sidney A. J. Wood and Thore Pettersson. 1988. Vowel
reduction in Bulgarian: the phonetic data and model
experiments. Folia Linguistica, 22(3-4):239?262.
53
Proceedings of the Workshop on Distributional Semantics and Compositionality (DiSCo?2011), pages 16?20,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
Two Multivariate Generalizations of Pointwise Mutual Information
Tim Van de Cruys
RCEAL
University of Cambridge
United Kingdom
tv234@cam.ac.uk
Abstract
Since its introduction into the NLP community,
pointwise mutual information has proven to be
a useful association measure in numerous nat-
ural language processing applications such as
collocation extraction and word space models.
In its original form, it is restricted to the anal-
ysis of two-way co-occurrences. NLP prob-
lems, however, need not be restricted to two-
way co-occurrences; often, a particular prob-
lem can be more naturally tackled when for-
mulated as a multi-way problem. In this pa-
per, we explore two multivariate generaliza-
tions of pointwise mutual information, and ex-
plore their usefulness and nature in the extrac-
tion of subject verb object triples.
1 Introduction
Mutual information (Shannon and Weaver, 1949) is
a measure of mutual dependence between two ran-
dom variables. The measure ? and more specifically
its instantiation for specific outcomes called point-
wise mutual information (PMI) ? has proven to be a
useful association measure in numerous natural lan-
guage processing applications. Since its introduc-
tion into the NLP community (Church and Hanks,
1990), it has been used in order to tackle or im-
prove upon several NLP problems, including col-
location extraction (ibid.) and word space mod-
els (Pantel and Lin, 2002). In its original form, it is
restricted to the analysis of two-way co-occurrences.
NLP problems, however, need not be restricted to
two-way co-occurrences; often, a particular prob-
lem can be more naturally tackled when formulated
as a multi-way problem. Notably, the framework
of tensor decomposition, that has recently perme-
ated into the NLP community (Turney, 2007; Ba-
roni and Lenci, 2010; Giesbrecht, 2010; Van de
Cruys, 2010), analyzes language issues as multi-
way co-occurrences. Up till now, little attention has
been devoted to the weighting of such multi-way co-
occurrences (which, for the research cited above, re-
sults either in using no weighting at all, or in apply-
ing an ad-hoc weighting solution without any theo-
retical underpinnings).
In this paper, we explore two possible generaliza-
tions of pointwise mutual information for multi-way
co-occurrences from a theoretical point of view. In
section 2, we discuss some relevant related work,
mainly in the field of information theory. In sec-
tion 3 the two generalizations of PMI are laid out in
more detail, based on their global multivariate coun-
terparts. Section 4 then discusses some applications
in the light of NLP, while section 5 concludes and
hints at some directions for future research.
2 Previous work
Research into the generalization of mutual informa-
tion was pioneered in two seminal papers. The first
one to explore the interaction of multiple random
variables in the scope of information theory was
McGill (1954). McGill described a first generaliza-
tion of mutual information based on the notion of
conditional entropy. This first generalization, called
interaction information, is described in section 3.2.1
below. A second generalization, solely based on
the commonalities of the random variables, was de-
scribed by Watanabe (1960). This generalization,
16
called total correlation is presented in section 3.2.2.
3 Theory
3.1 Mutual information
Mutual information is a measure of the amount of
information that one random variable contains about
another random variable. It is the reduction in
the uncertainty of one random variable due to the
knowledge of the other.
I(X;Y ) =
?
x?X
?
y?Y
p(x, y) log
p(x, y)
p(x)p(y)
(1)
Pointwise mutual information is a measure of as-
sociation that looks at particular instances of the two
random variablesX and Y . More specifically, point-
wise mutual information measures the difference be-
tween the probability of their co-occurrence given
their joint distribution and the probability of their
co-occurrence given the marginal distributions of X
and Y (thus assuming the two random variables are
independent).
pmi(x, y) = log
p(x, y)
p(x)p(y)
(2)
Note that mutual information (equation 1) yields
the expected PMI value over all possible instances of
random variables X and Y .
Ep(X,Y )[pmi(X,Y )] (3)
Furthermore, note that PMI may be positive or
negative, but its expected outcome over all events
(i.e. the global mutual information) is always non-
negative.
3.2 Multivariate mutual information
In this section, the two generalizations for multivari-
ate distributions are presented. For both generaliza-
tions, we examine their standard form (which looks
at the interaction between the random variables as a
whole) and their specific instantiation (that looks at
particular outcomes of the random variables). Anal-
ogously to PMI, it is these specific instantiations
of the measures that are able to weigh specific co-
occurrences according to their importance in the cor-
pus. As with PMI, the value for the global case ought
to be the expected value for all the instantiations of
the specific measure.
3.2.1 Interaction information
Interaction information (McGill, 1954) ? also
called co-information (Bell, 2003) ? is based on the
notion of conditional mutual information. Condi-
tional mutual information is the mutual information
of two random variables conditioned on a third one.
I(X;Y |Z)
=
?
x?X
?
y?Y
?
z?Z
p(x, y, z) log
p(x, y|z)
p(x|z)p(y|z)
(4)
which can be rewritten as
?
x?X
?
y?Y
?
z?Z
p(x, y, z) log
p(z)p(x, y, z)
p(x, z)p(y, z)
(5)
For the case of three variables, the interaction in-
formation is then defined as the conditional mutual
information subtracted by the standard mutual infor-
mation.
I1(X;Y ;Z) = I(X;Y |Z)? I(X;Y )
= I(X;Z|Y )? I(X;Z)
= I(Y ;Z|X)? I(Y ;Z) (6)
Expanded, this gives the following equation:
I1(X;Y ;Z)
=
?
x?X
?
y?Y
?
z?Z
p(x, y, z) log
p(z)p(x, y, z)
p(x, z)p(y, z)
?
?
x?X
?
y?Y
p(x, y) log
p(x, y)
p(x)p(y)
(7)
We can now define specific interaction informa-
tion as follows1:
1Note that ? compared to equation 7 ? the two subparts in
the right-hand side of the equation have been swapped. For the
three-variable case, this gives exactly the same outcome except
for a change in sign. The swap is necessary in order to ensure a
proper set-theoretic measure (Fano, 1961; Reza, 1994).
17
SI1(x, y, z) = log
p(x, y)
p(x)p(y)
? log
p(z)p(x, y, z)
p(x, z)p(y, z)
= log
p(x, y)p(y, z)p(x, z)
p(x)p(y)p(z)p(x, y, z)
(8)
Interaction information ? as well as specific in-
teraction information ? can equally be defined for
n > 3 variables.
3.2.2 Total correlation
Total correlation (Watanabe, 1960) ? also called
multi-information (Studeny? and Vejnarova?, 1998)
quantifies the amount of information that is shared
among the different random variables, and thus ex-
presses how related a particular group of random
variables are.
I2(X1, X2, . . . , Xn)
=
?
x1?X1,
x2?X2,
...
xn?Xn
p(x1, x2, . . . , xn) log
p(x1, x2, . . . , xn)
?ni=1p(xi)
(9)
Analogously to the definition of pointwise mu-
tual information, we can straightforwardly define the
correlation for specific instances of the random vari-
ables, which we coin specific correlation.
SI2(x1, x2, . . . , xn) = log
p(x1, x2, . . . , xn)
?ni=1p(xi)
(10)
For the case of three variables, this gives the follow-
ing equation:
SI2(x, y, z) = log
p(x, y, z)
p(x)p(y)p(z)
(11)
Note that this measure has been used in NLP tasks
before, notably for collocation extraction (Villada
Moiro?n, 2005).
4 Application
In this section, we explore the performance of the
measures defined above in an NLP context, viz. the
extraction of salient subject verb object triples. This
research has been carried out for Dutch. The Twente
Nieuws Corpus (Ordelman, 2002), a 500M Dutch
word corpus, has been automatically parsed with
the Dutch dependency parser ALPINO (van Noord,
2006), and all subject verb object triples with fre-
quency f ? 3 have been extracted. Next, a ten-
sor T of size I ? J ? K has been constructed,
containing the three-way co-occurrence frequencies
of the I most frequent subjects by the J most fre-
quent verbs by the K most frequent objects, with
I = 10000, J = 1000,K = 10000. Finally, two
new tensors U and V have been constructed, such
that Uijk = SI1(Tijk) and Vijk = SI2(Tijk), i.e.
tensor U has been weighted using specific interac-
tion information (equation 8) and tensor V has been
weighted using specific correlation (equation 11).
Table 1 shows the top five subject verb object
triples that received the highest specific interaction
information score, while table 2 gives the top five
subject verb object triples that gained the highest
specific correlation score (both with f > 30).
Note that both methods are able to extract salient
subject verb object triples, such as prototypical svo
combinations (peiling geeft opinie weer ?poll repre-
sents opinion?, helikopter vuurt raket af ?helicopter
fires rocket?) and fixed expressions (Dutch proverbs
such as de wal keert het schip ?the circumstances
change the course? and de vlag dekt de lading ?the
content corresponds to the title?).
subject verb object SI1
peiling geef weer opinie 18.20
?poll? ?represent? ?opinion?
helikopter vuur af raket 17.57
?helicopter? ?fire? ?rocket?
Man bijt hond 17.15
?man? ?bite? ?dog?
verwijt snijd hout 17.10
?reproach? ?cut? ?wood?
wal keer schip 17.01
?quay? ?turn? ?ship?
Table 1: Top five subject verb object triples with highest
specific interaction information score
Comparing both methods, the results seem to in-
dicate that the extracted triples are similar for both
weightings. This, however, is not consistently the
case: the results can differ significantly for partic-
18
subject verb object SI2
verwijt snijd hout 8.05
?reproach? ?cut? ?wood?
helikopter vuur af raket 7.75
?helicopter? ?fire? ?rocket?
peiling geef weer opinie 7.64
?poll? ?represent? ?opinion?
vlag dek lading 7.21
?flag? ?cover? ?load?
argument snijd hout 7.17
?argument? ?cut? ?wood?
Table 2: Top five subject verb object triples with highest
specific correlation score
ular instances. This becomes apparent when com-
paring table 3 and table 4, which for each method
contain the top five combinations for the Dutch verb
speel ?play?.
Table 3 indicates that specific interaction informa-
tion picks up on prototypical svo combinations (ork-
est speelt symfonie ?orchestra plays symphony?; also
note the 4 other triples that come from bridge game
descriptions). Specific correlation (table 4), on the
other hand, picks up on the expression een rol spe-
len ?play a role?, and extracts salient subjects that go
with the expression.
subject verb object SI1
orkest speel symfonie 11.65
?orchestra? ?play? ?symphony?
leider speel ruiten 10.29
?leader? ?play? ?diamonds?
leider speel harten 10.20
?leader? ?play? ?hearts?
leider speel schoppen 10.01
?leader? ?play? ?spades?
leider speel klaveren 9.89
?leader? ?play? ?clubs?
Table 3: Top five combinations with highest specific in-
teraction information scores for verb speel
In order to quantitatively assess the aptness of the
two methods for the extraction of salient svo triples,
we performed a small-scale manual evaluation of the
100 triples that scored the highest for each measure.
subject verb object SI2
nationaliteit speel rol 4.12
?nationality? ?play? ?role?
afkomst speel rol 4.06
?descent? ?play? ?role?
toeval speel rol 4.04
?coincidence? ?play? ?role?
motief speel rol 4.04
?motive? ?play? ?role?
afstand speel rol 4.02
?distance? ?play? ?role?
Table 4: Top five combinations with highest specific cor-
relation scores for verb speel
A triple is considered salient when it is made up of
a fixed (multi-word) expression, or when it consists
of a fixed expression combined with a salient sub-
ject or object (e.g. argument snijd hout ?argument
cut wood?). The bare frequency tensor (without any
weighting) was used as a baseline. The results are
presented in table 5.
measure precision
baseline .00
SI1 .24
SI2 .31
Table 5: Manual evaluation results for the extraction of
salient svo triples
The results indicate that both measures are able to
extract a significant number of salient triples com-
pared to the frequency baseline, which is not able
to extract any salient triples at all. Comparing both
measures, specific correlation clearly performs best
(.31 versus .24 for specific interaction information).
Additionally, we computed Kendall?s ?b to com-
pare the rankings yielded by the two different meth-
ods (over all triples). The correlation between both
rankings is ?b = 0.21, indicating that the results
yielded by both methods ? though correlated ? differ
to a significant extent.
These are, of course, preliminary results, and a
more thorough evaluation is necessary to confirm the
tendencies that emerge.
19
5 Conclusion
In this paper, we presented two multivariate gen-
eralizations of mutual information, as well as their
instantiated counterparts specific interaction infor-
mation and specific correlation, that are useful for
weighting multi-way co-occurrences in NLP tasks.
The main goal of this paper is to show that there is
not just one straightforward generalization of point-
wise mutual information for the multivariate case,
and NLP researchers that want to exploit multi-way
co-occurrences in an information-theoretic frame-
work should take this fact into account.
Moreover, we have applied the two different mea-
sures to the extraction of subject verb object triples,
and demonstrated that the results may differ signif-
icantly. It goes without saying that these are just
exploratory and rudimentary observations; more re-
search into the exact nature of both generalizations
and their repercussions for NLP ? as well as a proper
quantitative evaluation ? are imperative.
This brings us to some avenues for future work.
More research needs to be carried with regard to the
exact nature of the dependencies that both measures
capture. Preliminary results show that they extract
different information, but it is not clear what the
exact nature of that information is. Secondly, we
want to carry out a proper quantitative evaluation
on different multi-way co-occurrence (factorization)
tasks, in order to indicate which measure works best,
and which measure might be more suitable for a par-
ticular task.
Acknowledgements
A number of anonymous reviewers provided fruitful
remarks and comments on an earlier draft of this pa-
per, from which the current version has significantly
benefited.
References
Marco Baroni and Alessandro Lenci. 2010. Distribu-
tional memory: A general framework for corpus-based
semantics. Computational Linguistics, 36(4):1?48.
Anthony J. Bell. 2003. The co-information lattice. In
Proceedings of the Fifth International Workshop on In-
dependent Component Analysis and Blind Signal Sep-
aration: ICA 2003.
Kenneth W. Church and Patrick Hanks. 1990. Word as-
sociation norms, mutual information & lexicography.
Computational Linguistics, 16(1):22?29.
Robert Fano. 1961. Transmission of information. MIT
Press, Cambridge, MA.
Eugenie Giesbrecht. 2010. Towards a matrix-based dis-
tributional model of meaning. In Proceedings of the
NAACL HLT 2010 Student Research Workshop, pages
23?28. Association for Computational Linguistics.
William J. McGill. 1954. Multivariate information trans-
mission. Psychometrika, 19(2):97?116.
R.J.F. Ordelman. 2002. Twente Nieuws Corpus (TwNC),
August. Parlevink Language Techonology Group.
University of Twente.
Patrick Pantel and Dekang Lin. 2002. Discovering word
senses from text. In Proceedings of ACM Confer-
ence on Knowledge Discovery and Data Mining, pages
613?619, Edmonton, Canada.
Fazlollah M. Reza. 1994. An introduction to information
theory. Dover Publications.
Claude Shannon and Warren Weaver. 1949. The math-
ematical theory of communication. University of Illi-
nois Press, Urbana, Illinois.
M. Studeny? and J. Vejnarova?. 1998. The multiinforma-
tion function as a tool for measuring stochastic depen-
dence. In Proceedings of the NATO Advanced Study
Institute on Learning in graphical models, pages 261?
297, Norwell, MA, USA. Kluwer Academic Publish-
ers.
Peter D. Turney. 2007. Empirical evaluation of four ten-
sor decomposition algorithms. Technical Report ERB-
1152, National Research Council, Institute for Infor-
mation Technology.
Tim Van de Cruys. 2010. A non-negative tensor fac-
torization model for selectional preference induction.
Natural Language Engineering, 16(4):417?437.
Gertjan van Noord. 2006. At Last Parsing Is Now Op-
erational. In Piet Mertens, Cedrick Fairon, Anne Dis-
ter, and Patrick Watrin, editors, TALN06. Verbum Ex
Machina. Actes de la 13e conference sur le traite-
ment automatique des langues naturelles, pages 20?
42, Leuven.
Begon?a Villada Moiro?n. 2005. Data-driven identifica-
tion of fixed expressions and their modifiability. Ph.D.
thesis, University of Groningen, The Netherlands.
Satosi Watanabe. 1960. Information theoretical analysis
of multivariate correlation. IBM Journal of Research
and Development, 4:66?82.
20
