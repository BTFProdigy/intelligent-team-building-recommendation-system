Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1545?1556,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Scaling Semantic Parsers with On-the-fly Ontology Matching
Tom Kwiatkowski Eunsol Choi Yoav Artzi Luke Zettlemoyer
Computer Science & Engineering
University of Washington
Seattle, WA 98195
{tomk,eunsol,yoav,lsz}@cs.washington.edu
Abstract
We consider the challenge of learning seman-
tic parsers that scale to large, open-domain
problems, such as question answering with
Freebase. In such settings, the sentences cover
a wide variety of topics and include many
phrases whose meaning is difficult to rep-
resent in a fixed target ontology. For ex-
ample, even simple phrases such as ?daugh-
ter? and ?number of people living in? can-
not be directly represented in Freebase, whose
ontology instead encodes facts about gen-
der, parenthood, and population. In this pa-
per, we introduce a new semantic parsing ap-
proach that learns to resolve such ontologi-
cal mismatches. The parser is learned from
question-answer pairs, uses a probabilistic
CCG to build linguistically motivated logical-
form meaning representations, and includes
an ontology matching model that adapts the
output logical forms for each target ontology.
Experiments demonstrate state-of-the-art per-
formance on two benchmark semantic parsing
datasets, including a nine point accuracy im-
provement on a recent Freebase QA corpus.
1 Introduction
Semantic parsers map sentences to formal represen-
tations of their underlying meaning. Recently, al-
gorithms have been developed to learn such parsers
for many applications, including question answering
(QA) (Kwiatkowski et al, 2011; Liang et al, 2011),
relation extraction (Krishnamurthy and Mitchell,
2012), robot control (Matuszek et al, 2012; Kr-
ishnamurthy and Kollar, 2013), interpreting instruc-
tions (Chen and Mooney, 2011; Artzi and Zettle-
moyer, 2013), and generating programs (Kushman
and Barzilay, 2013).
In each case, the parser uses a predefined set
of logical constants, or an ontology, to construct
meaning representations. In practice, the choice
of ontology significantly impacts learning. For
example, consider the following questions (Q) and
candidate meaning representations (MR):
Q1: What is the population of Seattle?
Q2: How many people live in Seattle?
MR1: ?x.population(Seattle, x)
MR2: count(?x.person(x) ? live(x, Seattle))
A semantic parser might aim to construct MR1 for
Q1 and MR2 for Q2; these pairings align constants
(count, person, etc.) directly to phrases (?How
many,? ?people,? etc.). Unfortunately, few ontologies
have sufficient coverage to support both meaning
representations, for example many QA databases
would only include the population relation required
for MR1. Most existing approaches would, given
this deficiency, simply aim to produce MR1 for Q2,
thereby introducing significant lexical ambiguity
that complicates learning. Such ontological mis-
matches become increasingly common as domain
and language complexity increases.
In this paper, we introduce a semantic parsing ap-
proach that supports scalable, open-domain ontolog-
ical reasoning. The parser first constructs a linguis-
tically motivated domain-independent meaning rep-
resentation. For example, possibly producing MR1
for Q1 and MR2 for Q2 above. It then uses a learned
ontology matching model to transform this represen-
1545
x : How many people visit the public library of New York annually
l0 : ?x.eq(x, count(?y.people(y) ? ?e.visit(y, ?z.public(z) ? library(z) ? of(z, new york), e) ? annually(e)))
y : ?x.library.public library system.annual visits(x, new york public library)
a : 13,554,002
x : What works did Mozart dedicate to Joseph Haydn
l0 : ?x.works(x) ? ?e.dedicate(mozart, x, e) ? to(haydn, e)))
y : ?x.dedicated work(x) ? ?e.dedicated by(mozart, e) ? dedication(x, e) ? dedicated to(haydn, e)))
a : { String Quartet No. 19, Haydn Quartets, String Quartet No. 16, String Quartet No. 18, String Quartet No. 17 }
Figure 1: Examples of sentences x, domain-independent underspecified logical forms l0, fully specified
logical forms y, and answers a drawn from the Freebase domain.
tation for the target domain. In our example, pro-
ducing either MR1, MR2 or another more appropri-
ate option, depending on the QA database schema.
This two stage approach enables parsing without
any domain-dependent lexicon that pairs words with
logical constants. Instead, word meaning is filled
in on-the-fly through ontology matching, enabling
the parser to infer the meaning of previously un-
seen words and more easily transfer across domains.
Figure 1 shows the desired outputs for two example
Freebase sentences.
The first parsing stage uses a probabilistic combi-
natory categorial grammar (CCG) (Steedman, 2000;
Clark and Curran, 2007) to map sentences to
new, underspecified logical-form meaning represen-
tations containing generic logical constants that are
not tied to any specific ontology. This approach en-
ables us to share grammar structure across domains,
instead of repeatedly re-learning different grammars
for each target ontology. The ontology-matching
step considers a large number of type-equivalent
domain-specific meanings. It enables us to incorpo-
rate a number of cues, including the target ontology
structure and lexical similarity between the names of
the domain-independent and dependent constants, to
construct the final logical forms.
During learning, we estimate a linear model over
derivations that include all of the CCG parsing de-
cisions and the choices for ontology matching. Fol-
lowing a number of recent approaches (Clarke et al,
2010; Liang et al, 2011), we treat all intermediate
decisions as latent and learn from data containing
only easily gathered question answer pairs. This ap-
proach aligns naturally with our two-stage parsing
setup, where the final logical expression can be di-
rectly used to provide answers.
We report performance on two benchmark
datasets: GeoQuery (Zelle and Mooney, 1996) and
Freebase QA (FQ) (Cai and Yates, 2013a). Geo-
Query includes a geography database with a small
ontology and questions with relatively complex,
compositional structure. FQ includes questions to
Freebase, a large community-authored database that
spans many sub-domains. Experiments demonstrate
state-of-the-art performance in both cases, including
a nine point improvement in recall for the FQ test.
2 Formal Overview
Task Let an ontology O be a set of logical con-
stants and a knowledge base K be a collection of
logical statements constructed with constants from
O. For example, K could be facts in Freebase (Bol-
lacker et al, 2008) and O would define the set
of entities and relation types used to encode those
facts. Also, let y be a logical expression that can
be executed against K to return an answer a =
EXEC(y,K). Figure 1 shows example queries and
answers for Freebase. Our goal is to build a function
y = PARSE(x,O) for mapping a natural language
sentence x to a domain-dependent logical form y.
Parsing We use a two-stage approach to define
the space of possible parses GEN(x,O) (Section 5).
First, we use a CCG and word-class information
from Wiktionary1 to build domain-independent un-
derspecified logical forms, which closely mirror the
linguistic structure of the sentence but do not use
constants from O. For example, in Figure 1, l0 de-
notes the underspecified logical forms paired with
each sentence x. The parser then maps this interme-
diate representation to a logical form that uses con-
stants from O, such as the y seen in Figure 1.
1www.wiktionary.com
1546
Learning We assume access to data containing
question-answer pairs {(xi, ai) : i = 1 . . . n} and
a corresponding knowledge base K. The learn-
ing algorithm (Section 7.1) estimates the parame-
ters of a linear model for ranking the possible en-
tires in GEN(x,O). Unlike much previous work
(e.g., Zettlemoyer and Collins (2005)), we do not
induce a CCG lexicon. The lexicon is open domain,
using no symbols from the ontology O for K. This
allows us to write a single set of lexical templates
that are reused in every domain (Section 5.1). The
burden of learning word meaning is shifted to the
second, ontology matching, stage of parsing (Sec-
tion 5.2), and modeled with a number of new fea-
tures (Section 7.2) as part of the joint model.
Evaluation We evaluate on held out question-
answer pairs in two benchmark domains, Freebase
and GeoQuery. Following Cai and Yates (2013a),
we also report a cross-domain evaluation where the
Freebase data is divided by topics such as sports,
film, and business. This condition ensures that the
test data has a large percentage of previously unseen
words, allowing us to measure the effectiveness of
the real time ontology matching.
3 Related Work
Supervised approaches for learning semantic parsers
have received significant attention, e.g. (Kate and
Mooney, 2006; Wong and Mooney, 2007; Muresan,
2011; Kwiatkowski et al, 2010, 2011, 2012; Jones
et al, 2012). However, these techniques require
training data with hand-labeled domain-specific log-
ical expressions. Recently, alternative forms of su-
pervision were introduced, including learning from
question-answer pairs (Clarke et al, 2010; Liang
et al, 2011), from conversational logs (Artzi and
Zettlemoyer, 2011), with distant supervision (Kr-
ishnamurthy and Mitchell, 2012; Cai and Yates,
2013b), and from sentences paired with system
behavior (Goldwasser and Roth, 2011; Chen and
Mooney, 2011; Artzi and Zettlemoyer, 2013). Our
work adds to these efforts by demonstrating a new
approach for learning with latent meaning represen-
tations that scales to large databases like Freebase.
Cai and Yates (2013a) present the most closely
related work. They applied schema matching tech-
niques to expand a CCG lexicon learned with the
UBL algorithm (Kwiatkowski et al, 2010). This ap-
proach was one of the first to scale to Freebase, but
required labeled logical forms and did not jointly
model semantic parsing and ontological reasoning.
This method serves as the state of the art for our
comparison in Section 9.
We build on a number of existing algorithmic
ideas, including using CCGs to build meaning rep-
resentations (Zettlemoyer and Collins, 2005, 2007;
Kwiatkowski et al, 2010, 2011), building deriva-
tions to transform the output of the CCG parser
based on context (Zettlemoyer and Collins, 2009),
and using weakly supervised margin-sensitive pa-
rameter updates (Artzi and Zettlemoyer, 2011,
2013). However, we introduce the idea of learning
an open-domain CCG semantic parser; all previous
methods suffered, to various degrees, from the onto-
logical mismatch problem that motivates our work.
The challenge of ontological mismatch has been
previously recognized in many settings. Hobbs
(1985) describes the need for ontological promiscu-
ity in general language understanding. Many pre-
vious hand-engineered natural language understand-
ing systems (Grosz et al, 1987; Alshawi, 1992; Bos,
2008) are designed to build general meaning rep-
resentations that are adapted for different domains.
Recent efforts to build natural language interfaces to
large databases, for example DBpedia (Yahya et al,
2012; Unger et al, 2012), have also used hand-
engineered ontology matching techniques. Fader
et al (2013) recently presented a scalable approach
to learning an open domain QA system, where onto-
logical mismatches are resolved with learned para-
phrases. Finally, the databases research commu-
nity has a long history of developing schema match-
ing techniques (Doan et al, 2004; Euzenat et al,
2007), which has inspired more recent work on dis-
tant supervision for relation extraction with Free-
base (Zhang et al, 2012).
4 Background
Semantic Modeling We use the typed lambda cal-
culus to build logical forms that represent the mean-
ings of words, phrases and sentences. Logical forms
contain constants, variables, lambda abstractions,
and literals. In this paper, we use the term literal to
refer to the application of a constant to a sequence of
1547
library of new york
N N\N/NP NP
?x.library(x) ?y?f?x.f(x) ? loc(x, y) NY C
>
N\N
?f.?x.f(x) ? loc(x,NY C)
<
N
?x.library(x) ? loc(x,NY C)
Figure 2: A sample CCG parse.
arguments. We include types for entities e, truth val-
ues t, numbers i, events ev, and higher-order func-
tions, such as ?e, t? and ??e, t?, e?. We use David-
sonian event semantics (Davidson, 1967) to explic-
itly represent events using event-typed variables and
conjunctive modifiers to capture thematic roles.
Combinatory Categorial Grammars (CCG)
CCGs are a linguistically-motivated formalism
for modeling a wide range of language phenom-
ena (Steedman, 1996, 2000). A CCG is defined by
a lexicon and a set of combinators. The lexicon
contains entries that pair words or phrases with
CCG categories. For example, the lexical entry
library ` N : ?x.library(x) in Figure 2 pairs
the word ?library? with the CCG category that has
syntactic category N and meaning ?x.library(x).
A CCG parse starts from assigning lexical entries to
words and phrases. These are then combined using
the set of CCG combinators to build a logical form
that captures the meaning of the entire sentence. We
use the application, composition, and coordination
combinators. Figure 2 shows an example parse.
5 Parsing Sentences to Meanings
The function GEN(x,O) defines the set of possible
derivations for an input sentence x. Each derivation
d = ??,M? builds a logical form y using constants
from the ontology O. ? is a CCG parse tree that
maps x to an underspecified logical form l0. M is an
ontological match that maps l0 onto the fully spec-
ified logical form y. This section describes, with
reference to the example in Figure 3, the operations
used by ? and M .
5.1 Domain Independent Parsing
Domain-independent CCG parse trees ? are built
using a predefined set of 56 underspecified lexi-
cal categories, 49 domain-independent lexical items,
and the combinatory rules introduced in Section 4.
An underspecified CCG lexical category has a
syntactic category and a logical form containing no
constants from the domain ontology O. Instead, the
logical form includes underspecified constants that
are typed placeholders which will later be replaced
during ontology matching. For example, a noun
might be assigned the lexical category N : ?x.p(x),
where p is an underspecified ?e, t?-type constant.
During parsing, lexical categories are created dy-
namically. We manually define a set of POS tags for
each underspecified lexical category, and use Wik-
tionary as a tag dictionary to define the possible POS
tags for words and phrases. Each phrase is assigned
every matching lexical category. For example, the
word ?visit? can be either a verb or a noun in Wik-
tionary. We accordingly assign it all underspecified
categories for the classes, including:
N :?x.p(x) , S\NP/NP :?x?y?ev.p(y, x, ev)
for nouns and transitive verbs respectively.
We also define domain-independent lexical items
for function words such as ?what,? ?when,? and
?how many,? ?and,? and ?is.? These lexi-
cal items pair a word with a lexical cate-
gory containing only domain-independent con-
stants. For example, how many ` S/(S\NP)/N :
?f.?g.?x.eq(x, count(?y.f(y) ? g(y))) contains
the function count and the predicate eq.
Figure 3a shows the lexical categories and combi-
nator applications used to construct the underspeci-
fied logical form l0. Underspecified constants in this
figure have been labeled with the words that they are
associated with for readability.
5.2 Ontological Matching
The second, domain specific, step M maps the un-
derspecified logical form l0 onto the fully specified
logical form y. The mapping from constants in l0
to constants in y is not one-to-one. For example, in
Figure 3, l0 contains 11 constants but y contains only
2. The ontological match is a sequence of matching
operations M = ?o1 . . . , on? that can transform the
structure of the logical form or replace underspeci-
fied constants with constants from O.
1548
(a) Underspecified CCG parse ?: Map words onto underspecified lexical categories as described in Section 5.1. Use
the CCG combinators to combine lexical categories to give the full underpecified logical form l0.
how many people visit the public library of new york annually
S/(S\NP )/N N S\NP/NP NP/N N/N N N\N/NP NP AP
?f.?g.?x.eq(x, count( ?x.People(x) ?x.?y.?ev. ?f.?x.f(x) ?f.?x.f(x)? ?x.Library(x) ?y.?f.?x.Of NewY ork ?ev.Annually(ev)
?y.f(y) ? g(y))) V isit(y, x, ev) Public(x) (x, y) ? f(x)
> >
<
>
>
> <
>
S
l0 : ?x.eq(x, count(?y.People(y) ? ?e.V isit(y, ?z.Public(z) ? Library(z) ? Of(z,NewY ork)) ? Annually(e)))
(b) Structure Matching Steps in M : Use the operators described in Section 5.2.1 and Figure 4 to transform l0. In
each step one of the operators is applied to a subexpression of the existing logical form to generate a modified logical
form with a new underspecified constant marked in bold.
l0 : ?x.eq(x, count(?y.People(y) ? ?e.V isit(y, ?z.Public(z) ? Library(z) ?Of(z,NewY ork), e) ?Annually(e)))
l1 : ?x.eq(x, count(?y.People(y) ? ?e.V isit(y,PublicLibraryOfNewYork, e) ?Annually(e)))
l2 : ?x.HowManyPeopleVisitAnnually(x, PublicLibraryOfNewY ork)))
(c) Constant Matching Steps in M : Replace all underspecified constants in the transformed logical form with a
similarly typed constant from O, as described in Section 5.2.2. The underspecified constant to be replaced is marked
in bold and constants from O are written in typeset.
?x.HowManyPeopleV isitAnnually(x,PublicLibraryOfNewYork)
l3 : 7? ?x.HowManyPeopleV isitAnnually(x, new york public library)
?x.HowManyPeopleVisitAnnually(x, new york public library)
y : 7? ?x.public library system.annual visits(x, new york public library)
Figure 3: Example derivation for the query ?how many people visit the public library of new york annu-
ally.? Underspecified constants are labelled with the words from the query that they are associated with for
readability. Constants from O, written in typeset, are introduced in step (c).
Operator Definition and Conditions Example
a.
Collapse
Literal
to
Constant
P (a1, . . . , an) 7? c
?z.Public(z) ? Library(z) ?Of(z,NewY ork))
7? PublicLibraryOfNewY ork
s.t. type(P (a1, . . . , an)) = type(c) Input and output have type e.
type(c) ? {e, i} e is allowed in O.
freev(P (a1, . . . , an)) = ? Input contains no free variables.
b.
Collapse
Literal
to
Literal
P (a1, . . . , an) 7? Q(b1, . . . , bm)
eq(x, count(?y.People(y) ? ?e.V isit(y,
PublicLibraryOfNewY ork) ?Annually(e)))
7? CountPeopleV isitAnnually(x,
PublicLibraryOfNewY ork)
s.t. type(P (a1, . . . , an)) = type(Q(b1, . . . , bm)) Input and output have type t.
type(Q) ? {type(c) : c ? O} New constant has type ?i, ?e, t??, allowed in O.
freev(P (a1, . . . , an)) = freev(Q(b1, . . . , bm)) Input and output contain single free variable x.
{b1, . . . , bm} ? subexps(P (a1, . . . , an)) Arguments of output literal are subexpressions of input.
c. Split
Literal
P (a1, . . . , ak, x, ak+1, . . . , an)
7? Q(b1, . . . , x, . . . bn) ?Q??(c1, . . . , x, . . . cm)
Dedicate(Mozart,Haydn, ev)
7? Dedicate(Mozart, ev) ?Dedicate??(Haydn, ev)
s.t. type(P (. . . )) = t Input has type t. This matches output type by definition.
{type(Q), type(Q??)} ? {type(c) : c ? O} New constants have allowed type ?e, ?ev, t??.
{b1, . . . , bn, c1, . . . , cm} = {a1, . . . , an} All arguments of input literal are preserved in output.
Figure 4: Definition of the operations used to transform the structure of the underspecified logical form l0 to
match the ontology O. The function type(c) calculates a constant c?s type. The function freev(lf) returns
the set of variables that are free in lf (not bound by a lambda term or quantifier). The function subexps(lf)
generates the set of all subexpressions of the lambda calculus expression lf .
1549
5.2.1 Structure Matching
Three structure matching operators, illustrated in
Figure 4, are used to collapse or expand literals in
l0. Collapses merge a subexpression from l0 to cre-
ate a new underspecified constant, generating a log-
ical form with fewer constants. Expansions split a
subexpression from l0 to generate a new logical form
containing one extra constant.
Collapsing Operators The collapsing operator
defined in Figure 4a merges all constants in a
literal to generate a single constant of the same
type. This operator is used to map ?z.Public(z)?
Library(z)?Of(z,NewY ork) to PublicLibraryOfNewY ork
in Figure 3b. Its operation is limited to entity typed
expressions that do not contain free variables.
The operator in Figure 4b, in contrast, can be used
to collapse the expression eq(x,count(?y.People(y)?
?e.V isit(y,PublicLibraryOfNewY ork,e))?Annually(e))),
which contains free variable x onto a new expression
CountPeopleV isitAnnually(x,PublicLibraryOfNewY ork).
This is only possible when the type of the newly
created constant is allowed in O and the variable x
is free in the output expression. Subsets of conjuncts
can be collapsed using the operator in Figure 4b by
creating ad-hoc conjunctions that encapsulate them.
Disjunctions are treated similarly.
Performing collapses on the underspecified logi-
cal form allows non-contiguous phrases to be rep-
resented in the collapsed form. In this exam-
ple, the logical form representing the phrase ?how
many people visit? has been merged with the logi-
cal form representing the non-adjacent adverb ?an-
nually.? This generates a new underspecified con-
stant that can be mapped onto the Freebase relation
public library system annual visits that re-
lates to both phrases.
The collapsing operations preserve semantic type,
ensuring that all logical forms generated by the
derivation sequence are well typed. The full set of
allowed collapses of l0 is given by the transitive clo-
sure of the collapsing operations. The size of this
set is limited by the number of constants in l0, since
each collapse removes at least one constant. At each
step, the number of possible collapses is polynomial
in the number of constants in l0 and exponential in
the arity of the most complex type in O. For do-
mains of interest this arity is unlikely to be high and
for triple stores such as Freebase it is 2.
Expansion Operators The fully specified logical
form y can contain constants relating to multiple
words in x. It can also use multiple constants to rep-
resent the meaning of a single word. For example,
Freebase does not contain a relation representing the
concept ?daughter?, instead using two relations rep-
resenting ?female? and ?child?. The expansion oper-
ator in Figure 4c allows a single predicate to be split
into a pair of conjoined predicates sharing an argu-
ment variable. For example, in Figure 1, the constant
for ?dedicate? is split in two to match its represen-
tation in Freebase. Underspecified constants from
l0 can be split once. For the experiments in Sec-
tion 8, we constrain the expansion operator to work
on event modifiers but the procedure generalizes to
all predicates.
5.2.2 Constant Matching
To build an executable logical form y, all under-
specified constants must be replaced with constants
from O. This is done through a sequence of con-
stant replacement operations, each of which replaces
a single underspecified constant with a constant of
the same type from O. Two example replacements
are shown in Figure 3c. The output from the last re-
placement operation is a fully specified logical form.
6 Building and Scoring Derivations
This section introduces a dynamic program used to
construct derivations and a linear scoring model.
6.1 Building Derivations
The space of derivations is too large to explicitly
enumerate. However, each logical form (both final
and interim) can be constructed with many differ-
ent derivations, and we only need to find the highest
scoring one. This allows us to develop a simple dy-
namic program for our two-stage semantic parser.
We use a CKY style chart parser to calculate the
k-best logical forms output by parses of x. We then
store each interim logical form generated by an op-
erator in M once in a hyper-graph chart structure.
The branching factor of this hypergraph is polyno-
mial in the number of constants in l0 and linear in
the size of O. Subsequently, there are too many
possible logical forms to enumerate explicitly; we
1550
prune as follows. We allow the top N scoring on-
tological matches for each original subexpression in
l0 and remove matches that differ from score from
the maximum scoring match by more than a thresh-
old ? . When building derivations, we apply constant
matching operators as soon as they are applicable to
new underspecified constants created by collapses
and expansions. This allows the scoring function
used by the pruning strategy to take advantage of all
features defined in Section 7.2.
6.2 Ranking Derivations
Given feature vector ? and weight vector ?, the score
of a derivation d = ??,M? is a linear function that
decomposes over the parse tree ? and the individual
ontology-matching steps o.
SCORE(d) = ?(d)? (1)
= ?(?)? +
?
o?M
?(o)?
The function PARSE(x,O) introduced as our goal in
Section 2 returns the logical form associated with
the highest scoring derivation of x:
PARSE(x,O) = arg max
d?GEN(x,O)
(SCORE(d))
The features and learning algorithm used to estimate
? are defined in the next section.
7 Learning
This section describes an online learning algorithm
for question-answering data, along with the domain-
independent feature set.
7.1 Learning Model Parameters
Our learning algorithm estimates the parameters ?
from a set {(xi, ai) : i = 1 . . . n} of questions xi
paired with answers ai from the knowledge base
K. Each derivation d generated by the parser is
associated with a fully specified logical form y =
YIELD(d) that can be executed in K. A derivation d
of xi is correct if EXEC(YIELD(d),K) = ai. We use
a perceptron to estimate a weight vector ? that sup-
port a separation of ? between correct and incorrect
answers. Figure 5 presents the learning algorithm.
Input: Q/A pairs {(xi, ai) : i = 1 . . . n}; Knowledge base
K; Ontology O; Function GEN(x,O) that computes deriva-
tions of x; Function YIELD(d)that returns logical form yield
of derivation d; Function EXEC(y,K) that calculates execu-
tion of y in K; Margin ?; Number of iterations T .
Output: Linear model parameters ?.
Algorithm:
For t = 1 . . . T, i = 1 . . . n :
C = {d : d ? GEN(xi,O); EXEC(YIELD(d),K) = ai}
W = {d : d ? GEN(xi,O); EXEC(YIELD(d),K) 6= ai}
C? = argmaxd?C(?(d)?)
W ? = {d : d ?W ; ?c ? C? s.t. ?(c)? ? ?(d)? < ?)}
If |C?| > 0 ? |W ?| > 0 :
? = ? + 1|C?|
?
c?C? ?(c)?
1
|W?|
?
e?W? ?(e)
Figure 5: Parameter estimation from Q/A pairs.
7.2 Features
The feature vector ?(d) introduced in Section 6.2
decomposes over each of the derivation steps in d.
CCG Parse Features Each lexical item in ? has
three indicator features. The first indicates the num-
ber of times each underspecified category is used.
For example, the parse in Figure 3a uses the under-
specified category N : ?x.p(x) twice. The second
feature indicates (word, category) pairings ? e.g.
that N : ?x.p(x) is paired with ?library? and ?pub-
lic? once each in Figure 3a. The final lexical feature
indicates (part-of-speech, category) pairings for all
parts of speech associated with the word.
Structural Features The structure matching op-
erators (Section 5.2.1) in M generate new under-
specified constants that define the types of constants
in the output logical form y. These operators are
scored using features that indicate the type of each
complex-typed constant present in y and the iden-
tity of domain-independent functional constants in
y. The logical form y generated in Figure 3 contains
one complex typed constant with type ?i, ?e, t?? and
no domain-independent functional constants. Struc-
tural features allow the model to adapt to different
knowledge bases K. They allow it to determine, for
example, whether a numeric quantity such as ?pop-
ulation? is likely to be explicitly listed in K or if it
should be computed with the count function.
Lexical Features Each constant replacement op-
erator (Section 5.2.2) in M replaces an underspec-
1551
ified constant cu with a constant cO from O. The
underspecified constant cu is associated with the se-
quence of words ~wu used in the CCG lexical en-
tries that introduced it in ?. We assume that each
of the constants cO in O is associated with a string
label ~wO. This allows us to introduce five domain-
independent features that measure the similarity of
~wu and ~wO.
The feature ?np(cu, cO) signals the replacement
of an entity-typed constant cu with entity cO that has
label ~wu. For the second example in Figure 1 this
feature indicates the replacement of the underspeci-
fied constant associated with the word ?mozart? with
the Freebase entity mozart. Stem and synonymy
features ?stem(cu, cO) and ?syn(cu, cO) signal the
existence of words wu ? ~wu and wu ? ~wO that
share a stem or synonym respectively. Stems are
computed with the Porter stemmer and synonyms
are extracted from Wiktionary. A single Freebase
specific feature ?fp:stem(cu, cO) indicates a word
stem match between wu ? ~wu and the word filling
the most specific position in ~wu under Freebase?s hi-
erarchical naming schema.
A final feature ?gl(cu, cO) calculates the overlap
between Wiktionary definitions for ~wu and ~wO. Let
gl(w) be the Wiktionary definition for w. Then:
?gl(cu, cO) =
?
wu? ~wu;wO? ~wO
2?|gl(wO)?gl(wc)|
| ~wO |?| ~wu|?|gl(wO)|+|gl(wc)|
Domain-indepedent lexical features allow the
model to reason about the meaning of unseen words.
In small domains, however, the majority of word us-
ages may be covered by training data. We make use
of this fact in the GeoQuery domain with features
?m(cu, cO) that indicate the pairing of ~wu with cO.
Knowledge Base Features Guided by the obser-
vation that we generally want to create queries y
which have answers in knowledge base K, we de-
fine features to signal whether each operation could
build a logical form y with an answer in K.
If a predicate-argument relation in y does not
exist in K, then the execution of y against K
will not return an answer. Two features indicate
whether predicate-argument relations in y exist inK.
?direct(y,K) indicates predicate-argument applica-
tions in y that exists in K. For example, if the appli-
cation of dedicated by to mozart in Figure 1 ex-
ists in Freebase, ?direct(y,K) will fire. ?join(y,K)
indicates entities separated from a predicate by one
join in y, such as mozart and dedicated to in Fig-
ure 1, that exist in the same relationship in K.
If two predicates that share a variable in y
do not share an argument in that position in K
then the execution of y against K will fail. The
predicate-predicate ?pp(y,K) feature indicates pairs
of predicates that share a variable in y but can-
not occur in this relationship in K. For ex-
ample, since the subject of the Freebase prop-
erty date of birth does not take arguments of
type location, ?pp(y,K) will fire if y con-
tains the logical form ?x?y.date of birth(x, y)?
location(x).
Both the predicate-argument and predicate-
predicate features operate on subexpressions of y.
We also define the execution features: ?emp(y,K) to
signal an empty answer for y in K; ?0(y,K) to sig-
nal a zero-valued answer created by counting over
an empty set; and ?1(y,K) to signal a one-valued
answer created by counting over a singleton set.
As with the lexical cues, we use knowledge base
features as soft constraints since it is possible for
natural language queries to refer to concepts that do
not exist in K.
8 Experimental Setup
Data We evaluate performance on the benchmark
GeoQuery dataset (Zelle and Mooney, 1996), and a
newly introduced Freebase Query (FQ) dataset (Cai
and Yates, 2013a). FQ contains 917 questions la-
beled with logical form meaning representations for
querying Freebase. We gathered question answer la-
bels by executing the logical forms against Freebase,
and manually correcting any inconsistencies.
Freebase (Bollacker et al, 2008) is a large, col-
laboratively authored database containing almost 40
million entities and two billion facts, covering more
than 100 domains. We filter Freebase to cover the
domains contained in the FQ dataset resulting in a
database containing 18 million entities, 2072 rela-
tions, 635 types, 135 million facts and 81 domains,
including for example film, sports, and business. We
use this schema to define our target domain, allow-
ing for a wider variety of queries than could be en-
coded with the 635 collapsed relations previously
used to label the FQ data.
1552
We report two different experiments on the FQ
data: test results on the existing 642/275 train/test
split and domain adaptation results where the data is
split three ways, partitioning the topics so that the
logical meaning expressions do not share any sym-
bols across folds. We report on the standard 600/280
training/test split for GeoQuery.
Parameter Initialization and Training We ini-
tialize weights for ?np and ?direct to 10, and weights
for ?stem and ?join to 5. This promotes the use of
entities and relations named in sentences. We ini-
tialize weights for ?pp and ?emp to -1 to favour log-
ical forms that have an interpretation in the knowl-
edge base K. All other feature weights are initial-
ized to 0. We run the training algorithm for one it-
eration on the Freebase data, at which point perfor-
mance on the development set had converged. This
fast convergence is due to the very small number of
matching parameters used (5 lexical features and 8
K features). For GeoQuery, we include the larger
domain specific feature set introduced in Section 7.2
and train for 10 iterations. We set the pruning pa-
rameters from Section 6.1 as follows: k = 5 for
Freebase, k = 30 for GeoQuery, N = 50, ? = 10.
Comparison Systems We compare performance
to state-of-the-art systems in both domains. On
GeoQuery, we report results from DCS (Liang
et al, 2011) without special initialization (DCS) and
with an small hand-engineered lexicon (DCS with
L+). We also include results for the FUBL algo-
rithm (Kwiatkowski et al, 2011), the CCG learning
approach that is most closely related to our work. On
FQ, we compare to Cai and Yates (2013a) (CY13).
Evaluation We evaluate by comparing the pro-
duced question answers to the labeled ones, with no
partial credit. Because the parser can fail to pro-
duce a complete query, we report recall, the percent
of total questions answered correctly, and precision,
the percentage of produced queries with correct an-
swers. CY13 and FUBL report fully correct logical
forms, which is a close proxy to our numbers.
9 Results
Quantitative Analysis For FQ, we report results
on the test set and in the cross-domain setting, as de-
fined in Section 8. Figure 6 shows both results. Our
Setting System R P F1
Test Our Approach 68.0 76.7 72.1
CY13 59 67 63
Cross Our Approach 67.9 73.5 71.5
Domain CY13 60 69 65
Figure 6: Results on the FQ dataset.
R P F1
All Features 68.6 72.0 70.3
Without Wiktionary 67.2 70.7 68.9
Without K Features 61.8 62.5 62.1
Figure 7: Ablation Results
approach outperforms the previous state of the art,
achieving a nine point improvement in test recall,
while not requiring labeled logical forms in train-
ing. We also see consistent improvements on both
scenarios, indicating that our approach is generaliz-
ing well across topic domains. The learned ontology
matching model is able to reason about previously
unseen ontological subdomains as well as if it was
provided explicit, in-domain training data.
We also performed feature ablations with 5-fold
cross validation on the training set, as seen in Fig-
ure 7. Both the Wiktionary features and knowledge
base features were helpful. Without the Wiktionary
features, the model must rely on word stem matches
which, in combination with graph constraints, can
still recover many of the correct queries. However,
without the knowledge base constraints, the model
produces many queries that return empty answers,
and significantly impacts overall performance.
For GeoQuery, we report test results in Figure 8.
Our approach outperforms the most closely related
CCG model (FUBL) and DCS without initialization,
but falls short of DCS with a small hand-built initial
lexicon. Given the small size of the test set, it is fair
to say that all algorithms are performing at state-of-
the-art levels. This result demonstrates that our ap-
Recall
FUBL 88.6
DCS 87.9
DCS with L+ 91.1
Our Approach 89.0
Figure 8: GeoQuery Results
1553
Parse Failures (20%)
1. Query in what year did motorola have the most revenue
2 Query on how many projects was james walker a design engineer
Structural Matching Failure (30%)
Query how many children does jerry seinfeld have
3. Labeled ?x.eq(x, count(?y.people.person.children(jerry seinfeld, y)))
Predicted ?x.eq(x, count(?y.people.person.children(y, jerry seinfeld)))
Incomplete Database (10%)
Query how many countries participated in the 2006 winter olympics
4. Labeled ?y.olympics.olympic games.number of countries(2006 winter olympics, y)
Predicted ?y.eq(y, count(?y.olympic participation country.olympics participated in(x, 2006 winter olympics)))
Query what programming languages were used for aol instant messenger
5. Labeled ?y.computer.software.languages used(aol instant messenger, y)
Predicted ?y.computer.software.languages used(aol instant messenger, y) ? computer.programming language(y)
Lexical Ambiguity (35%)
Query when was the frida kahlo exhibit at the philadelphia art museum
Labeled ?y.?x.exhibition run.exhibition(x, frida kahlo)?
6. exhibition venue.exhibitions at(philadelphia art museum, x) ? exhibition run.opened on(x, y)
Predicted ?y.?x.exhibition run.exhibition(x, frida kahlo)?
exhibition venue.exhibitions at(philadelphia art museum, x) ? exhibition run.closed on(x, y)
Figure 9: Example error cases, with associated frequencies, illustrating system output and gold standard
references. 5% of the cases were miscellaneous or otherwise difficult to categorize.
proach can handle the high degree of lexical ambi-
guity in the FQ data, without sacrificing the ability
to understanding the rich, compositional phenomena
that are common in the GeoQuery data.
Qualitative Analysis We also did a qualitative
analysis of errors in the FQ domain. The model
learns to correctly produce complex forms that join
multiple relations. However, there are a number of
systematic error cases, grouped into four categories
as seen in Figure 9.
The first and second examples show parse fail-
ures, where the underspecified CCG grammar did
not have sufficient coverage. The third shows a
failed structural match, where all of the correct logi-
cal constants are selected, but the argument order is
reversed for one of the literals. The fourth and fifth
examples demonstrate a failures due to database in-
completeness. In both cases, the predicted queries
would have returned the same answers as the gold-
truth ones if Freebase contained all of the required
facts. Developing models that are robust to database
incompleteness is a challenging problem for future
work. Finally, the last example demonstrates a lex-
ical ambiguity, where the system was unable to de-
termine if the query should include the opening date
or the closing date for the exhibit.
10 Conclusion
We considered the problem of learning domain-
independent semantic parsers, with application to
QA against large knowledge bases. We introduced
a new approach for learning a two-stage semantic
parser that enables scalable, on-the-fly ontological
matching. Experiments demonstrated state-of-the-
art performance on benchmark datasets, including
effective generalization to previously unseen words.
We would like to investigate more nuanced no-
tions of semantic correctness, for example to support
many of the essentially equivalent meaning repre-
sentations we found in the error analysis. Although
we focused exclusively on QA applications, the gen-
eral two-stage analysis approach should allow for
the reuse of learned grammars across a number of
different domains, including robotics or dialog ap-
plications, where data is more challenging to gather.
11 Acknowledgements
This research was supported in part by DARPA un-
der the DEFT program through the AFRL (FA8750-
13-2-0019) and the CSSG (N11AP20020), the ARO
(W911NF-12-1-0197), the NSF (IIS-1115966), and
by a gift from Google. The authors thank Anthony
Fader, Nicholas FitzGerald, Adrienne Wang, Daniel
Weld, and the anonymous reviewers for their helpful
comments and feedback.
1554
References
Alshawi, H. (1992). The core language engine. The
MIT Press.
Artzi, Y. and Zettlemoyer, L. (2011). Bootstrapping
semantic parsers from conversations. In Proceed-
ings of the Conference on Empirical Methods in
Natural Language Processing.
Artzi, Y. and Zettlemoyer, L. (2013). Weakly super-
vised learning of semantic parsers for mapping in-
structions to actions. Transactions of the Associ-
ation for Computational Linguistics, 1(1):49?62.
Bollacker, K., Evans, C., Paritosh, P., Sturge, T., and
Taylor, J. (2008). Freebase: a collaboratively cre-
ated graph database for structuring human knowl-
edge. In Proceedings of the ACM SIGMOD Inter-
national Conference on Management of Data.
Bos, J. (2008). Wide-coverage semantic analysis
with boxer. In Proceedings of the Conference on
Semantics in Text Processing.
Cai, Q. and Yates, A. (2013a). Large-scale semantic
parsing via schema matching and lexicon exten-
sion. In Proceedings of the Annual Meeting of the
Association for Computational Linguistics.
Cai, Q. and Yates, A. (2013b). Semantic parsing
freebase: Towards open-domain semantic pars-
ing. In Proceedings of the Joint Conference on
Lexical and Computational Semantics.
Chen, D. and Mooney, R. (2011). Learning to inter-
pret natural language navigation instructions from
observations. In Proceedings of the National Con-
ference on Artificial Intelligence.
Clark, S. and Curran, J. (2007). Wide-coverage ef-
ficient statistical parsing with CCG and log-linear
models. Computational Linguistics, 33(4):493?
552.
Clarke, J., Goldwasser, D., Chang, M., and Roth,
D. (2010). Driving semantic parsing from the
world?s response. In Proceedings of the Confer-
ence on Computational Natural Language Learn-
ing.
Davidson, D. (1967). The logical form of action sen-
tences. Essays on actions and events, pages 105?
148.
Doan, A., Madhavan, J., Domingos, P., and Halevy,
A. (2004). Ontology matching: A machine
learning approach. In Handbook on ontologies.
Springer.
Euzenat, J., Euzenat, J., Shvaiko, P., et al (2007).
Ontology matching. Springer.
Fader, A., Zettlemoyer, L., and Etzioni, O. (2013).
Paraphrase-driven learning for open question an-
swering. In Proceedings of the Annual Meeting of
the Association for Computational Linguistics.
Goldwasser, D. and Roth, D. (2011). Learning from
natural instructions. In Proceedings of the In-
ternational Joint Conference on Artificial Intelli-
gence.
Grosz, B. J., Appelt, D. E., Martin, P. A., and
Pereira, F. (1987). TEAM: An experiment in
the design of transportable natural language inter-
faces. Artificial Intelligence, 32(2):173?243.
Hobbs, J. R. (1985). Ontological promiscuity. In
Proceedings of the Annual Meeting on Associa-
tion for Computational Linguistics.
Jones, B. K., Johnson, M., and Goldwater, S. (2012).
Semantic parsing with bayesian tree transducers.
In Proceedings of the 50th Annual Meeting of the
Association of Computational Linguistics.
Kate, R. and Mooney, R. (2006). Using string-
kernels for learning semantic parsers. In Pro-
ceedings of the Conference of the Association for
Computational Linguistics.
Krishnamurthy, J. and Kollar, T. (2013). Jointly
learning to parse and perceive: Connecting nat-
ural language to the physical world. Transactions
of the Association for Computational Linguistics,
1(2).
Krishnamurthy, J. and Mitchell, T. (2012). Weakly
supervised training of semantic parsers. In Pro-
ceedings of the Joint Conference on Empirical
Methods in Natural Language Processing and
Computational Natural Language Learning.
Kushman, N. and Barzilay, R. (2013). Using se-
mantic unification to generate regular expressions
from natural language. In Proceedings of the Con-
ference of the North American Chapter of the As-
sociation for Computational Linguistics.
1555
Kwiatkowski, T., Goldwater, S., Zettlemoyer, L.,
and Steedman, M. (2012). A probabilistic model
of syntactic and semantic acquisition from child-
directed utterances and their meanings. Proceed-
ings of the Conference of the European Chapter
of the Association of Computational Linguistics.
Kwiatkowski, T., Zettlemoyer, L., Goldwater, S.,
and Steedman, M. (2010). Inducing probabilis-
tic CCG grammars from logical form with higher-
order unification. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language
Processing.
Kwiatkowski, T., Zettlemoyer, L., Goldwater, S.,
and Steedman, M. (2011). Lexical generalization
in CCG grammar induction for semantic parsing.
In Proceedings of the Conference on Empirical
Methods in Natural Language Processing.
Liang, P., Jordan, M., and Klein, D. (2011). Learn-
ing dependency-based compositional semantics.
In Proceedings of the Conference of the Associ-
ation for Computational Linguistics.
Matuszek, C., FitzGerald, N., Zettlemoyer, L., Bo,
L., and Fox, D. (2012). A joint model of language
and perception for grounded attribute learning. In
Proceedings of the International Conference on
Machine Learning.
Muresan, S. (2011). Learning for deep language un-
derstanding. In Proceedings of the International
Joint Conference on Artificial Intelligence.
Steedman, M. (1996). Surface Structure and Inter-
pretation. The MIT Press.
Steedman, M. (2000). The Syntactic Process. The
MIT Press.
Unger, C., Bu?hmann, L., Lehmann, J.,
Ngonga Ngomo, A., Gerber, D., and Cimiano, P.
(2012). Template-based question answering over
RDF data. In Proceedings of the International
Conference on World Wide Web.
Wong, Y. and Mooney, R. (2007). Learning syn-
chronous grammars for semantic parsing with
lambda calculus. In Proceedings of the Confer-
ence of the Association for Computational Lin-
guistics.
Yahya, M., Berberich, K., Elbassuoni, S., Ramanath,
M., Tresp, V., and Weikum, G. (2012). Natural
language questions for the web of data. In Pro-
ceedings of the Conference on Empirical Methods
in Natural Language Processing.
Zelle, J. and Mooney, R. (1996). Learning to parse
database queries using inductive logic program-
ming. In Proceedings of the National Conference
on Artificial Intelligence.
Zettlemoyer, L. and Collins, M. (2005). Learning
to map sentences to logical form: Structured clas-
sification with probabilistic categorial grammars.
In Proceedings of the Conference on Uncertainty
in Artificial Intelligence.
Zettlemoyer, L. and Collins, M. (2007). Online
learning of relaxed CCG grammars for parsing to
logical form. In Proceedings of the Joint Confer-
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning.
Zettlemoyer, L. and Collins, M. (2009). Learn-
ing context-dependent mappings from sentences
to logical form. In Proceedings of the Joint Con-
ference of the Association for Computational Lin-
guistics and International Joint Conference on
Natural Language Processing.
Zhang, C., Hoffmann, R., and Weld, D. S. (2012).
Ontological smoothing for relation extraction
with minimal supervision. In Proceeds of the
Conference on Artificial Intelligence.
1556
Proceedings of the ACL-2012 Workshop on Extra-Propositional Aspects of Meaning in Computational Linguistics (ExProM-2012),
pages 70?79, Jeju, Republic of Korea, 13 July 2012. c?2012 Association for Computational Linguistics
Hedge Detection as a Lens on Framing in the GMO Debates:
A Position Paper
Eunsol Choi?, Chenhao Tan?, Lillian Lee?, Cristian Danescu-Niculescu-Mizil? and Jennifer Spindel?
?Department of Computer Science, ?Department of Plant Breeding and Genetics
Cornell University
ec472@cornell.edu, chenhao|llee|cristian@cs.cornell.edu, jes462@cornell.edu
Abstract
Understanding the ways in which participants
in public discussions frame their arguments is
important in understanding how public opin-
ion is formed. In this paper, we adopt the po-
sition that it is time for more computationally-
oriented research on problems involving fram-
ing. In the interests of furthering that goal,
we propose the following specific, interesting
and, we believe, relatively accessible ques-
tion: In the controversy regarding the use
of genetically-modified organisms (GMOs) in
agriculture, do pro- and anti-GMO articles dif-
fer in whether they choose to adopt a more
?scientific? tone?
Prior work on the rhetoric and sociology of
science suggests that hedging may distin-
guish popular-science text from text written
by professional scientists for their colleagues.
We propose a detailed approach to studying
whether hedge detection can be used to un-
derstanding scientific framing in the GMO de-
bates, and provide corpora to facilitate this
study. Some of our preliminary analyses sug-
gest that hedges occur less frequently in scien-
tific discourse than in popular text, a finding
that contradicts prior assertions in the litera-
ture. We hope that our initial work and data
will encourage others to pursue this promising
line of inquiry.
1 Introduction
1.1 Framing, ?scientific discourse?, and GMOs
in the media
The issue of framing (Goffman, 1974; Scheufele,
1999; Benford and Snow, 2000) is of great im-
portance in understanding how public opinion is
formed. In their Annual Review of Political Science
survey, Chong and Druckman (2007) describe fram-
ing effects as occurring ?when (often small) changes
in the presentation of an issue or an event produce
(sometimes large) changes of opinion? (p. 104);
as an example, they cite a study wherein respon-
dents answered differently, when asked whether a
hate group should be allowed to hold a rally, depend-
ing on whether the question was phrased as one of
?free speech? or one of ?risk of violence?.
The genesis of our work is in a framing question
motivated by a relatively current political issue. In
media coverage of transgenic crops and the use of
genetically modified organisms (GMOs) in food, do
pro-GMO vs. anti-GMO articles differ not just with
respect to word choice, but in adopting a more ?sci-
entific? discourse, meaning the inclusion of more
uncertainty and fewer emotionally-laden words? We
view this as an interesting question from a text anal-
ysis perspective (with potential applications and im-
plications that lie outside the scope of this article).
1.2 Hedging as a sign of scientific discourse
To obtain a computationally manageable character-
ization of ?scientific discourse?, we turned to stud-
ies of the culture and language of science, a body
of work spanning fields ranging from sociology to
applied linguistics to rhetoric and communication
(Gilbert and Mulkay, 1984; Latour, 1987; Latour
and Woolgar, 1979; Halliday and Martin, 1993; Baz-
erman, 1988; Fahnestock, 2004; Gross, 1990).
One characteristic that has drawn quite a bit of
attention in such studies is hedging (Myers, 1989;
70
Hyland, 1998; Lewin, 1998; Salager-Meyer, 2011).1
Hyland (1998, pg. 1) defines hedging as the ex-
pression of ?tentativeness and possibility? in com-
munication, or, to put it another way, language cor-
responding to ?the writer withholding full commit-
ment to statements? (pg. 3). He supplies many
real-life examples from scientific research articles,
including the following:
1. ?It seems that this group plays a critical role in
orienting the carboxyl function? (emphasis Hy-
land?s)
2. ?...implies that phytochrome A is also not nec-
essary for normal photomorphogenesis, at least
under these irradiation conditions? (emphasis
Hyland?s)
3. ?We wish to suggest a structure for the salt of
deoxyribose nucleic acid (D.N.A.)? (emphasis
added)2
Several scholars have asserted the centrality of hedg-
ing in scientific and academic discourse, which cor-
responds nicely to the notion of ?more uncertainty?
mentioned above. Hyland (1998, p. 6) writes, ?De-
spite a widely held belief that professional scientific
writing is a series of impersonal statements of fact
which add up to the truth, hedges are abundant in
science and play a critical role in academic writing?.
Indeed, Myers (1989, p. 13) claims that in scien-
tific research articles, ?The hedging of claims is so
common that a sentence that looks like a claim but
has no hedging is probably not a statement of new
knowledge?.3
Not only is understanding hedges important to un-
derstanding the rhetoric and sociology of science,
but hedge detection and analysis ? in the sense of
identifying uncertain or uncertainly-sourced infor-
mation (Farkas et al, 2010) ? has important appli-
cations to information extraction, broadly construed,
and has thus become an active sub-area of natural-
language processing. For example, the CoNLL 2010
1In linguistics, hedging has been studied since the 1970s
(Lakoff, 1973).
2This example originates from Watson and Crick?s land-
mark 1953 paper. Although the sentence is overtly tentative,
did Watson and Crick truly intend to be polite and modest in
their claims? See Varttala (2001) for a review of arguments re-
garding this question.
3Note the inclusion of the hedge ?probably?.
Shared Task was devoted to this problem (Farkas
et al, 2010).
Putting these two lines of research together, we
see before us what appears to be an interesting in-
terdisciplinary and, at least in principle, straightfor-
ward research program: relying on the aforemen-
tioned rhetoric analyses to presume that hedging is
a key characteristic of scientific discourse, build a
hedge-detection system to computationally ascertain
which proponents in the GMO debate tend to use
more hedges and thus, by presumption, tend to adopt
a more ?scientific? frame.4
1.3 Contributions
Our overarching goal in this paper is to convince
more researchers in NLP and computational linguis-
tics to work on problems involving framing. We
try to do so by proposing a specific problem that
may be relatively accessible. Despite the apparent
difficulty in addressing such questions, we believe
that progress can be made by drawing on observa-
tions drawn from previous literature across many
fields, and integrating such work with movements
in the computational community toward considera-
tion of extra-propositional and pragmatic concerns.
We have thus intentionally tried to ?cover a lot of
ground?, as one referee put it, in the introductory
material just discussed.
Since framing problems are indeed difficult, we
elected to narrow our scope in the hope of making
some partial progress. Our technical goal here, at
this workshop, where hedge detection is one of the
most relevant topics to the broad questions we have
raised, is not to learn to classify texts as being pro-
vs. anti-GMO, or as being scientific or not, per se.5
Our focus is on whether hedging specifically, con-
sidered as a single feature, is correlated with these
different document classes, because of the previous
research attention that has been devoted to hedging
in particular and because of hedging being one of the
topics of this workshop. The point of this paper is
4However, this presumption that more hedges characterize a
more scientific discourse has been contested. See section 2 for
discussion and section 4.2 for our empirical investigation.
5Several other groups have addressed the problem of try-
ing to identify different sides or perspectives (Lin et al, 2006;
Hardisty et al, 2010; Beigman Klebanov et al, 2010; Ahmed
and Xing, 2010).
71
thus not to compare the efficacy of hedging features
with other types, such as bag-of-words features. Of
course, to do so is an important and interesting di-
rection for future work.
In the end, we were not able to achieve satisfac-
tory results even with respect to our narrowed goal.
However, we believe that other researchers may be
able to follow the plan of attack we outline below,
and perhaps use the data we are releasing, in order
to achieve our goal. We would welcome hearing the
results of other people?s efforts.
2 How should we test whether hedging
distinguishes scientific text?
One very important point that we have not yet ad-
dressed is: While the literature agrees on the impor-
tance of hedging in scientific text, the relative de-
gree of hedging in scientific vs. non-scientific text is
a matter of debate.
On the one side, we have assertions like those of
Fahnestock (1986), who shows in a clever, albeit
small-scale, study involving parallel texts that when
scientific observations pass into popular accounts,
changes include ?removing hedges ... thus con-
ferring greater certainty on the reported facts? (pg.
275). Similarly, Juanillo, Jr. (2001) refers to a shift
from a forensic style to a ?celebratory? style when
scientific research becomes publicized, and credits
Brown (1998) with noting that ?celebratory scien-
tific discourses tend to pay less attention to caveats,
contradictory evidence, and qualifications that are
highlighted in forensic or empiricist discourses. By
downplaying scientific uncertainty, it [sic] alludes to
greater certainty of scientific results for public con-
sumption? (Juanillo, Jr., 2001, p. 42).
However, others have contested claims that the
popularization process involves simplification, dis-
tortion, hype, and dumbing down, as Myers (2003)
colorfully puts it; he provides a critique of the rel-
evant literature. Varttala (1999) ran a corpus anal-
ysis in which hedging was found not just in pro-
fessional medical articles, but was also ?typical of
popular scientific articles dealing with similar top-
ics? (p. 195). Moreover, significant variation in use
of hedging has been found across disciplines and au-
thors? native language; see Salager-Meyer (2011) or
Varttala (2001) for a review.
To the best of our knowledge, there have been no
large-scale empirical studies validating the hypoth-
esis that hedges appear more or less frequently in
scientific discourse.
Proposed procedure Given the above, our first
step must be to determine whether hedges are more
or less prominent in ?professional scientific? (hence-
forth ?prof-science??) vs. ?public science? (hence-
forth ?pop-science?) discussions of GMOs. Of
course, for a large-scale study, finding hedges re-
quires developing and training an effective hedge de-
tection algorithm.
If the first step shows that hedges can indeed be
used to effectively distinguish prof-science vs. pop-
science discourse on GMOs, then the second step is
to examine whether the use of hedging in pro-GMO
articles follows our inferred ?scientific? occurrence
patterns to a greater extent than the hedging in anti-
GMO articles.
However, as our hedge classifier trained on the
CoNLL dataset did not perform reliably on the dif-
ferent domain of prof-science vs. pop-science dis-
cussions of GMOs, we focus the main content of this
paper on the first step. We describe data collection
for the second step in the appendix.
3 Data
To accomplish the first step of our proposed pro-
cedure outlined above, we first constructed a prof-
science/pop-science corpus by pulling text from
Web of Science for prof-science examples and from
LexisNexis for pop-science examples, as described
in Section 3.1. Our corpus will be posted online
at https://confluence.cornell.edu/display/llresearch/
HedgingFramingGMOs.
As noted above, computing the degree of hedg-
ing in the aforementioned corpus requires access to
a hedge-detection algorithm. We took a supervised
approach, taking advantage of the availability of the
CoNLL 2010 hedge-detection training and evalua-
tion corpora, described in Section 3.2
3.1 Prof-science/pop-science data: LEXIS and
WOS
As mentioned previously, a corpus of prof-science
and pop-science articles is required to ascertain
whether hedges are more prevalent in one or the
72
Dataset Doc type # docs # sentences Avg sentence length Flesch reading ease
Prof-science/pop-science corpus
WOS abstracts 648 5596 22.35 23.39
LEXIS (short) articles 928 36795 24.92 45.78
Hedge-detection corpora
Bio (train) abstracts, articles 1273, 9 14541 (18% uncertain) 29.97 20.77
Bio (eval) articles 15 5003 (16% uncertain) 31.30 30.49
Wiki (train) paragraphs 2186 11111 (22% uncertain) 23.07 35.23
Wiki (eval) paragraphs 2346 9634 (23% uncertain) 20.82 31.71
Table 1: Basic descriptive statistics for the main corpora we worked with. We created the first two. Higher Flesch
scores indicate text that is easier to read.
other of these two writing styles. Since our ultimate
goal is to look at discourse related to GMOs, we re-
strict our attention to documents on this topic.
Thomson Reuter?s Web of Science (WOS), a
database of scientific journal and conference arti-
cles, was used as a source of prof-science samples.
We chose to collect abstracts, rather than full scien-
tific articles, because intuition suggests that the lan-
guage in abstracts is more high-level than that in the
bodies of papers, and thus more similar to the lan-
guage one would see in a public debate on GMOs.
To select for on-topic abstracts, we used the phrase
?transgenic foods? as a search keyword and dis-
carded results containing any of a hand-selected list
of off-topic filtering terms (e.g., ?mice? or ?rats?).
We then made use of domain expertise to manually
remove off-topic texts. The process yielded 648 doc-
uments for a total of 5596 sentences.
Our source of pop-science articles was Lexis-
Nexis (LEXIS). On-topic documents were collected
from US newspapers using the search keywords ?ge-
netically modified foods? or ?transgenic crops? and
then imposing the additional requirement that at
least two terms on a hand-selected list7 be present
in each document. After the removal of duplicates
and texts containing more than 2000 words to delete
excessively long articles, our final pop-science sub-
corpus was composed of 928 documents.
7The term list: GMO, GM, GE, genetically modified, ge-
netic modification, modified, modification, genetic engineer-
ing, engineered, bioengineered, franken, transgenic, spliced,
G.M.O., tweaked, manipulated, engineering, pharming, aqua-
culture.
3.2 CoNLL hedge-detection training data 8
As described in Farkas et al (2010), the motivation
behind the CoNLL 2010 shared task is that ?distin-
guishing factual and uncertain information in texts is
of essential importance in information extraction?.
As ?uncertainty detection is extremely important for
biomedical information extraction?, one component
of the dataset is biological abstracts and full arti-
cles from the BioScope corpus (Bio). Meanwhile,
the chief editors of Wikipedia have drawn the at-
tention of the public to specific markers of uncer-
tainty known as weasel words9: they are words or
phrases ?aimed at creating an impression that some-
thing specific and meaningful has been said?, when,
in fact, ?only a vague or ambiguous claim, or even
a refutation, has been communicated?. An example
is ?It has been claimed that ...?: the claimant has not
been identified, so the source of the claim cannot be
verified. Thus, another part of the dataset is a set
of Wikipedia articles (Wiki) annotated with weasel-
word information. We view the combined Bio+Wiki
corpus (henceforth the CoNLL dataset) as valuable
for developing hedge detectors, and we attempt to
study whether classifiers trained on this data can be
generalized to other datasets.
3.3 Comparison
Table 1 gives the basic statistics on the main datasets
we worked with. Though WOS and LEXIS differ in
the total number of sentences, the average sentence
length is similar. The average sentence length in Bio
is longer than that in Wiki. The articles in WOS
are markedly more difficult to read than the articles
8http://www.inf.u-szeged.hu/rgai/conll2010st/
9http://en.wikipedia.org/wiki/Weasel word
73
in LEXIS according to Flesch reading ease (Kincaid
et al, 1975).
4 Hedging to distinguish scientific text:
Initial annotation
As noted in Section 1, it is not a priori clear whether
hedging distinguishes scientific text or that more
hedges correspond to a more ?scientific? discourse.
To get an initial feeling for how frequently hedges
occur in WOS and LEXIS, we hand-annotated a
sample of sentences from each. In Section 4.1, we
explain the annotation policy of the CoNLL 2010
Shared Task and our own annotation method for
WOS and LEXIS. After that, we move forward in
Section 4.2 to compare the percentage of uncertain
sentences in prof-science vs. pop-science text on
this small hand-labeled sample, and gain some ev-
idence that there is indeed a difference in hedge oc-
currence rates, although, perhaps surprisingly, there
seem to be more hedges in the pop-science texts.
As a side benefit, we subsequently use the
hand-labeled sample we produce to investigate the
accuracy of an automatic hedge detector in the
WOS+LEXIS domain; more on this in Section 5.
4.1 Uncertainty annotation
CoNLL 2010 Shared Task annotation policy As
described in Farkas et al (2010, pg. 4), the data an-
notation polices for the CoNLL 2010 Shared Task
were that ?sentences containing at least one cue
were considered as uncertain, while sentences with
no cues were considered as factual?, where a cue
is a linguistic marker that in context indicates un-
certainty. A straightforward example of a sentence
marked ?uncertain? in the Shared Task is ?Mild blad-
der wall thickening raises the question of cystitis.?
The annotated cues are not necessarily general, par-
ticularly in Wiki, where some of the marked cues
are as specific as ?some of schumann?s best choral
writing?, ?people of the jewish tradition?, or ?certain
leisure or cultural activities?.
Note that ?uncertainty? in the Shared Task def-
inition also encompassed phrasing that ?creates an
impression that something important has been said,
but what is really communicated is vague, mislead-
ing, evasive or ambiguous ... [offering] an opinion
without any backup or source?. An example of such
Dataset % of uncertain sentences
WOS (estimated from 75-sentence sample) 20
LEXIS (estimated from 78-sentence sample) 28
Bio 17
Wiki 23
Table 2: Percentages of uncertain sentences.
a sentence, drawn from Wikipedia and marked ?un-
certain? in the Shared Task, is ?Some people claim
that this results in a better taste than that of other diet
colas (most of which are sweetened with aspartame
alone).?; Farkas et al (2010) write, ?The ... sentence
does not specify the source of the information, it is
just the vague term ?some people? that refers to the
holder of this opinion?.
Our annotation policy We hand-annotated 200
randomly-sampled sentences, half from WOS and
half from LEXIS10, to gauge the frequency with
which hedges occur in each corpus. Two annota-
tors each followed the rules of the CoNLL 2010
Shared Task to label sentences as certain, uncertain,
or not a proper sentence.11 The annotators agreed on
153 proper sentences of the 200 sentences (75 from
WOS and 78 from LEXIS). Cohen?s Kappa (Fleiss,
1981) was 0.67 on the annotation, which means that
the consistency between the two annotators was fair
or good. However, there were some interesting cases
where the two annotators could not agree. For ex-
ample, in the sentence ?Cassava is the staple food of
tropical Africa and its production, averaged over 24
countries, has increased more than threefold from
1980 to 2005 ... ?, one of the annotators believed
that ?more than? made the sentence uncertain. These
borderline cases indicate that the definition of hedg-
ing should be carefully delineated in future studies.
4.2 Percentages of uncertain sentences
To validate the hypothesis that prof-science articles
contain more hedges, we computed the percentage
10We took steps to attempt to hide from the annotators any
explicit clues as to the source of individual sentences: the sub-
set of authors who did the annotation were not those that col-
lected the data, and the annotators were presented the sentences
in random order.
11The last label was added because of a few errors in scraping
the data.
74
of uncertain sentences in our labeled data. As shown
in Table 2, we observed a trend contradicting ear-
lier studies. Uncertain sentences were more frequent
in LEXIS than in WOS, though the difference was
not statistically significant12 (perhaps not surprising
given the small sample size). The same trend was
seen in the CoNLL dataset: there, too, the percent-
age of uncertain sentences was significantly smaller
in Bio (prof-science articles) than in Wiki. In order
to make a stronger argument about prof-science vs
pop-science, however, more annotation on the WOS
and LEXIS datasets is needed.
5 Experiments
As stated in Section 1, our proposal requires devel-
oping an effective hedge detection algorithm. Our
approach for the preliminary work described in this
paper is to re-implement Georgescul?s (2010) algo-
rithm; the experimental results on the Bio+Wiki do-
main, given in Section 5.1, are encouraging. Then
we use this method to attempt to validate (at a larger
scale than in our manual pilot annotation) whether
hedges can be used to distinguish between prof-
science and pop-science discourse on GMOs. Un-
fortunately, our results, given in Section 5.2, are
inconclusive, since our trained model could not
achieve satisfactory automatic hedge-detection ac-
curacy on the WOS+LEXIS domain.
5.1 Method
We adopted the method of Georgescul (2010): Sup-
port Vector Machine classification based on a Gaus-
sian Radial Basis kernel function (Vapnik, 1998; Fan
et al, 2005), employing n-grams from annotated cue
phrases as features, as described in more detail be-
low. This method achieved the top performance in
the CoNLL 2010 Wikipedia hedge-detection task
(Farkas et al, 2010), and SVMs have been proven
effective for many different applications. We used
the LIBSVM toolkit in our experiments13.
As described in Section 3.2, there are two separate
datasets in the CoNLL dataset. We experimented on
them separately (Bio, Wiki). Also, to make our clas-
sifier more generalizable to different datasets, we
12Throughout, ?statistical significance? refers to the student
t-test with p < .05.
13http://www.csie.ntu.edu.tw/?cjlin/libsvm/
also trained models based on the two datasets com-
bined (Bio+Wiki). As for features, we took advan-
tage of the observation in Georgescul (2010) that the
bag-of-words model does not work well for this task.
We used different sets of features based on hedge
cue words that have been annotated as part of the
CoNLL dataset distribution14. The basic feature set
was the frequency of each hedge cue word from the
training corpus after removing stop words and punc-
tuation and transforming words to lowercase. Then,
we extracted unigrams, bigrams and trigrams from
each hedge cue phrase. Table 3 shows the number
of features in different settings. Notice that there are
many more features in Wiki. As mentioned above,
in Wiki, some cues are as specific as ?some of schu-
mann?s best choral writing?, ?people of the jewish
tradition?, or ? certain leisure or cultural activities?.
Taking n-grams from such specific cues can cause
some sentences to be classified incorrectly.
Feature source #features
Bio 220
Bio (cues + bigram + trigram) 340
Wiki 3740
Wiki (cues + bigram + trigram) 10603
Table 3: Number of features.
Best cross-validation performance
Dataset (C, ?) P R F
Bio (40, 2?3) 84.0 92.0 87.8
Wiki (30, 2?6) 64.0 76.3 69.6
Bio+Wiki (10, 2?4) 66.7 78.3 72.0
Table 4: Best 5-fold cross-validation performance for Bio
and/or Wiki after parameter tuning. As a reminder, we
repeat that our intended final test set is the WOS+LEXIS
corpus, which is disjoint from Bio+Wiki.
We adopted several techniques from Georgescul
(2010) to optimize performance through cross vali-
dation. Specifically, we tried different combinations
of feature sets (the cue phrases themselves, cues +
14For the Bio model, we used cues extracted from Bio. Like-
wise, the Wiki model used cues from Wiki, and the Bio+Wiki
model used cues from Bio+Wiki.
75
Evaluation set Model P R F
WOS+LEXIS Bio 54 68 60
WOS+LEXIS Wiki 38 54 45
WOS+LEXIS Bio+Wiki 21 93 34
Sub-corpus performance of the model based on Bio
WOS Bio 58 73 65
LEXIS Bio 52 64 57
Table 5: The upper part shows the performance on WOS
and LEXIS based on models trained on the CoNLL
dataset. The lower part gives the sub-corpus results for
Bio, which provided the best performance on the full
WOS+LEXIS corpus.
unigram, cues + bigram, cues + trigram, cues + uni-
gram + bigram + trigram, cues + bigram + trigram).
We tuned the width of the RBF kernel (?) and the
regularization parameter (C) via grid search over the
following range of values: {2?9, 2?8, 2?7, . . . , 24}
for ?, {1, 10, 20, 30, . . . , 150} for C. We also tried
different weighting strategies for negative and pos-
itive classes (i.e., either proportional to the number
of positive instances, or uniform). We performed 5-
fold cross validation for each possible combination
of experimental settings on the three datasets (Bio,
Wiki, Bio+Wiki).
Table 4 shows the best performance on all three
datasets and the corresponding parameters. In the
three datasets, cue+bigram+trigram provided the
best performance, and the weighted model con-
sistently produced superior results to the uniform
model. The F1 measure for Bio was 87.8, which
was satisfactory, while the F1 results for Wiki were
69.6, which were the worst of all the datasets.
This resonates with our observation that the task on
Wikipedia is more subtly defined and thus requires
a more sophisticated approach than counting the oc-
currences of bigrams and trigrams.
5.2 Results on WOS+LEXIS
Next, we evaluated whether our best classifier
trained on the CoNLL dataset can be generalized to
other datasets, in particular, the WOS and LEXIS
corpus. Performance was measured on the 153 sen-
tences on which our annotators agreed, a dataset
that was introduced in Section 4.1. Table 5 shows
how the best models trained on Bio, Wiki, and
Evaluation set (C, ?) P R F
WOS + LEXIS (50, 2?9) 68 62 65
WOS (50, 2?9) 85 73 79
LEXIS (50, 2?9) 57 54 56
Table 6: Best performance after parameter tuning
based on the 153 labeled WOS+LEXIS sentences; this
gives some idea of the upper-bound potential of our
Georgescul-based method. The training set is Bio, which
gave the best performance in Table 5.
Bio+Wiki, respectively, performed on the 153 la-
beled sentences. First, we can see that the perfor-
mance degraded significantly compared to the per-
formance for in-domain cross validation. Second, of
the three different models, Bio showed the best per-
formance. Bio+Wiki gave the worst performance,
which hints that combining two datasets and cue
words may not be a promising strategy: although
Bio+Wiki shows very good recall, this can be at-
tributed to its larger feature set, which contains all
available cues and perhaps as a result has a very high
false-positive rate. We further investigated and com-
pared performance on LEXIS and WOS for the best
model (Bio). Not surprisingly, our classifier works
better in WOS than in LEXIS.
It is clear that there exist domain differences be-
tween the CoNLL dataset and WOS+LEXIS. To bet-
ter understand the poor cross-domain performance
of the classifier, we tuned another model based on
the performance on the 153 labeled sentences us-
ing Bio as training data. As we can see in Table
6, the performance on WOS improved significantly,
while the performance on LEXIS decreased. This
is probably caused by the fact that WOS is a col-
lection of scientific paper abstracts, which is more
similar to the training corpus than LEXIS, which is
a collection of news media articles15. Also, LEXIS
articles are hard to classify even with the tuned
model, which challenges the effectiveness of a cue-
words frequency approach beyond professional sci-
entific texts. Indeed, the simplicity of our reim-
plementation of Georgescul?s algorithm seems to
cause longer sentences to be classified as uncer-
tain, because cue phrases (or n-grams extracted from
15The Wiki model performed better on LEXIS than on WOS.
Though the performance was not good, this result further rein-
forces the possibility of a domain-dependence problem.
76
cue phrases) are more likely to appear in lengthier
sentences. Analysis of the best performing model
shows that the false-positive sentences are signifi-
cantly longer than the false-negative ones.16
Dataset Model % classified uncertain
WOS Bio 16
LEXIS Bio 19
WOS Tuned 15
LEXIS Tuned 14
Table 7: For completeness, we report here the percentage
of uncertain sentences in WOS and LEXIS according to
our trained classifiers, although we regard these results as
unreliable since those classifiers have low accuracy. Bio
refers to the best model trained on Bio only in Section 5.1,
while Tuned refers to the model in Table 6 that is tuned
based on the 153 labeled sentences in WOS+LEXIS.
While the cross-domain results were not reliable,
we produced preliminary results on whether there
exist fewer hedges in scientific text. We can see that
the relative difference in certain/uncertain ratios pre-
dicted by the two different models (Bio, Tuned) are
different in Table 7. In the tuned model, the differ-
ence between LEXIS and WOS in terms of the per-
centage of uncertain sentences was not statistically
significant, while in the Bio model, their difference
was statistically significant. Since the performance
of our hedge classifier on the 153 hand-annotated
WOS+LEXIS sentences was not reliable, though,
we must abstain from making conclusive statements
here.
6 Conclusion and future work
In this position paper, we advocated that researchers
apply hedge detection not only to the classic moti-
vation of information-extraction problems, but also
to questions of how public opinion forms. We pro-
posed a particular problem in how participants in de-
bates frame their arguments. Specifically, we asked
whether pro-GMO and anti-GMO articles differ in
adopting a more ?scientific? discourse. Inspired by
earlier studies in social sciences relating hedging to
texts aimed at professional scientists, we proposed
16Average length of true positive sentences : 28.6, false pos-
itive sentences 35.09, false negative sentences: 22.0.
addressing the question with automatic hedge de-
tection as a first step. To develop a hedge clas-
sifier, we took advantage of the CoNLL dataset
and a small annotated WOS and LEXIS dataset.
Our preliminary results show there may exist a gap
which indicates that hedging may, in fact, distin-
guish prof-science and pop-science documents. In
fact, this computational analysis suggests the possi-
bility that hedges occur less frequently in scientific
prose, which contradicts several prior assertions in
the literature.
To confirm the argument that pop-science tends
to use more hedging than prof-science, we need
a hedge classifier that performs more reliably in
the WOS and LEXIS dataset than ours does. An
interesting research direction would be to develop
transfer-learning techniques to generalize hedge
classifiers for different datasets, or to develop a gen-
eral hedge classifier relatively robust to domain dif-
ferences. In either case, more annotated data on
WOS and LEXIS is needed for better evaluation or
training.
Another strategy would be to bypass the first step,
in which we determine whether hedges are more
or less prominent in scientific discourse, and pro-
ceed directly to labeling and hedge-detection in pro-
GMO and anti-GMO texts. However, this will not
answer the question of whether advocates in debates
other than on GMO-related topics employ a more
scientific discourse. Nonetheless, to aid those who
wish to pursue this alternate strategy, we have col-
lected two sets of opinionated articles on GMO (pro-
and anti-); see appendix for more details.
Acknowledgments We thank Daniel Hopkins and
Bonnie Webber for reference suggestions, and the
anonymous reviewers for helpful and thoughtful
comments. This paper is based upon work sup-
ported in part by US NSF grants IIS-0910664 and
IIS-1016099, a US NSF graduate fellowship to JS,
Google, and Yahoo!
References
Amr Ahmed and Eric P Xing. Staying informed: su-
pervised and semi-supervised multi-view topical
analysis of ideological perspective. In EMNLP,
pages 1140?1150, 2010.
Charles Bazerman. Shaping Written Knowledge:
77
The Genre and Activity of the Experimental Ar-
ticle in Science. University of Wisconsin Press,
Madison, Wis., 1988.
Beata Beigman Klebanov, Eyal Beigman, and
Daniel Diermeier. Vocabulary choice as an indi-
cator of perspective. In ACL Short Papers, pages
253?257, Stroudsburg, PA, USA, 2010. Associa-
tion for Computational Linguistics.
Robert D. Benford and David A. Snow. Framing
processes and social movements: An overview
and assessment. Annual Review of Sociology, 26:
611?639, 2000.
Richard Harvey Brown. Toward a democratic sci-
ence: Scientific narration and civic communica-
tion. Yale University Press, New Haven, 1998.
Dennis Chong and James N. Druckman. Framing
theory. Annual Review of Political Science, 10:
103?126, 2007.
Jeanne Fahnestock. Accommodating Science. Writ-
ten Communication, 3(3):275?296, 1986.
Jeanne Fahnestock. Preserving the figure: Consis-
tency in the presentation of scientific arguments.
Written Communication, 21(1):6?31, 2004.
Rong-En Fan, Pai-Hsuen Chen, and Chih-Jen Lin.
Working set selection using second order in-
formation for training support vector machines.
JMLR, 6:1889?1918, December 2005. ISSN
1532-4435.
Richa?rd Farkas, Veronika Vincze, Gyo?rgy Mo?ra,
Ja?nos Csirik, and Gyo?rgy Szarvas. The CoNLL-
2010 shared task: Learning to detect hedges and
their scope in natural language text. In CoNLL?
Shared Task, pages 1?12, 2010.
Joseph L. Fleiss. Statistical Methods for Rates
and Proportions. Wiley series in probability and
mathematical statistics. John Wiley & Sons, New
York, second edition, 1981.
Maria Georgescul. A hedgehop over a max-margin
framework using hedge cues. In CONLL?
Shared-Task, pages 26?31, 2010.
G. Nigel Gilbert and Michael Joseph Mulkay. Open-
ing Pandora?s box: A sociological analysis of sci-
entists? discourse. CUP Archive, 1984.
Erving Goffman. Frame analysis: An essay on the
organization of experience. Harvard University
Press, 1974.
Alan G. Gross. The rhetoric of science. Harvard
University Press, Cambridge, Mass., 1990.
Michael Alexander Kirkwood Halliday and
James R. Martin. Writing science: Literacy and
discursive power. Psychology Press, London
[u.a.], 1993.
Eric A Hardisty, Jordan Boyd-Graber, and Philip
Resnik. Modeling perspective using adaptor
grammars. In EMNLP, pages 284?292, 2010.
Ken Hyland. Hedging in scientific research articles.
John Benjamins Pub. Co., Amsterdam; Philadel-
phia, 1998.
Napoleon K. Juanillo, Jr. Frames for Public Dis-
course on Biotechnology. In Genetically Modified
Food and the Consumer: Proceedings of the 13th
meeting of the National Agricultural Biotechnol-
ogy Council, pages 39?50, 2001.
J. Peter Kincaid, Robert P. Fishburne, Richard L.
Rogers, and Brad S. Chissom. Derivation of new
readability formulas for navy enlisted personnel.
Technical report, National Technical Information
Service, Springfield, Virginia, February 1975.
George Lakoff. Hedges: A study in meaning cri-
teria and the logic of fuzzy concepts. Journal of
Philosophical Logic, 2(4):458?508, 1973.
Bruno Latour. Science in action: How to follow sci-
entists and engineers through society. Harvard
University Press, Cambridge, Mass., 1987.
Bruno Latour and Steve Woolgar. Laboratory life:
The social construction of scientific facts. Sage
Publications, Beverly Hills, 1979.
Beverly A. Lewin. Hedging: Form and function
in scientific research texts. In Genre Studies in
English for Academic Purposes, volume 9, pages
89?108. Universitat Jaume I, 1998.
Wei-Hao Lin, Theresa Wilson, Janyce Wiebe, and
Alexander Hauptmann. Which side are you on?
identifying perspectives at the document and sen-
tence levels. In CoNLL, 2006.
Greg Myers. The pragmatics of politeness in sci-
entific articles. Applied Linguistics, 10(1):1?35,
1989.
78
Greg Myers. Discourse studies of scientific popular-
ization: Questioning the boundaries. Discourse
Studies, 5(2):265?279, 2003.
Franc?oise Salager-Meyer. Scientific discourse and
contrastive linguistics: hedging. European Sci-
ence Editing, 37(2):35?37, 2011.
Dietram A. Scheufele. Framing as a theory of media
effects. Journal of Communication, 49(1):103?
122, 1999.
Vladimir N. Vapnik. Statistical Learning Theory.
Wiley-Interscience, 1998.
Teppo Varttala. Remarks on the communicative
functions of hedging in popular scientific and spe-
cialist research articles on medicine. English for
Specific Purposes, 18(2):177?200, 1999.
Teppo Varttala. Hedging in scientifically oriented
discourse: Exploring variation according to dis-
cipline and intended audience. PhD thesis, Uni-
versity of Tampere, 2001.
7 Appendix: pro- vs. anti-GMO dataset
Here, we describe the pro- vs. anti-GMO dataset we
collected, in the hopes that this dataset may prove
helpful in future research regarding the GMO de-
bates, even though we did not use the corpus in the
project described in this paper.
The second step of our overall procedure out-
lined in the introduction ? that step being to ex-
amine whether the use of hedging in pro-GMO arti-
cles corresponds with our inferred ?scientific? oc-
currence patterns more than that in anti-GMO ar-
ticles ? requires a collection of opinionated arti-
cles on GMOs. Our first attempt to use news me-
dia articles (LEXIS) was unsatisfying, as we found
many articles attempt to maintain a neutral position.
This led us to collect documents from more strongly
opinionated organizational websites such as Green-
peace (anti-GMO), Non GMO Project (anti-GMO),
or Why Biotechnology (pro-GMO). Articles were
collected from 20 pro-GMO and 20 anti-GMO or-
ganizational web sites.
After the initial collection of data, near-duplicates
and irrelevant articles were filtered through cluster-
ing, keyword searches and distance between word
vectors at the document level. We have collected
762 ?anti? documents and 671 ?pro? documents.
We reduced this to a 404 ?pro? and 404 ?con?
set as follows. Each retained ?document? con-
sists of only the first 200 words after excluding the
first 50 words of documents containing over 280
words. This was done to avoid irrelevant sections
such as Educators have permission to reprint arti-
cles for classroom use; other users, please contact
editor@actionbioscience.org for reprint permission.
See reprint policy.
The data will be posted online at
https://confluence.cornell.edu/display/llresearch/
HedgingFramingGMOs.
79
