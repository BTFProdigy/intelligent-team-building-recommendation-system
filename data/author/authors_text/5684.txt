Multiword Lexical Acquisition and Dictionary Formalization 
Cristina MOTA 
LabEL, CAUTL, IST 
Av. Rovisco Pais 
Lisboa, Portugal, 1049-001 
cristina@label.ist.utl.pt 
Paula CARVALHO 
University of Lisbon and 
LabEL, CAUTL, IST  
Av. Rovisco Pais 
Lisboa, Portugal, 1049-001 
paula@label.ist.utl.pt 
Elisabete RANCHHOD 
University of Lisbon and 
LabEL, CAUTL, IST  
Av. Rovisco Pais 
Lisboa, Portugal, 1049-001 
elisabet@label.ist.utl.pt 
 
Abstract 
In this paper, we present the current state of 
development of a large-scale lexicon built at 
LabEL1 for Portuguese. We will concentrate on 
multiword expressions (MWE), particularly on 
multiword nouns, (i) illustrating their most relevant 
morphological features, and (ii) pointing out the 
methods and techniques adopted to generate  the 
inflected forms from lemmas. Moreover, we 
describe a corpus-based aproach for the acquisition 
of new multiword nouns, which  led to a 
significant enlargement of the existing lexicon. 
Evaluation results concerning lexical coverage in 
the corpus are also discussed. 
1 Introduction 
MWEs have been viewed, for long time, as 
marginal idiosyncratic combinations of words. In 
recent years, however, there has been a growing 
awareness in the NLP community of the problems 
that MWEs pose and the need for their robust 
handling. Several major conferences and satellite 
workshops have been dedicated to the subject 
(ACL, EACL, LREC, for instance); major 
publications devote thematic issues to MWEs. 
Anticipating that growing interest, over the last 
years, a significant part of LabEL?s research has 
been devoted to the development of large-scale, 
linguistically precise language resources, namely 
to the construction of computational lexicons for 
simple  and multiword units (Eleut?rio et al, 1995; 
Ranchhod et al, 1999; Ranchhod et al, 2004). 
In fact, we have observed that MWEs are used 
frequently in both everyday language and technical 
and scientific texts to express ideas and concepts 
that in general cannot be stated by ?free? linguistic 
structures. They include a large range of different 
linguistic phenomena, such as: (i) lexical 
compounds (nouns: cellular phone, rush hour, New 
Jersey; adjectives: well-known; adverbs: for the 
time being, in short; prepositions and conjunctions: 
                                                     
1
 LabEL (Laborat?rio de Engenharia da Linguagem) 
http://label.ist.utl.pt 
in spite of, in order to) (ii) phrasal verbs (give up); 
(iii) light verbs (give a lecture); (iv) fixed 
(proverbs and maxims) and semi-fixed sentences 
(to see the light at the end of the tunnel; to take the 
Lord? name in vain). From a linguistic point of 
view, all these expressions exhibit distributional 
and selectional constraints, i.e. they lack 
compositionality, and frequently have idiomatic 
interpretations. 
In this paper, we focus on multiword nouns. 
Special attention will be given to their 
formalization and generation, using INTEX, a 
public FST (Finite-State Transducer) based NLP 
system [Silberztein, 1993]. In this context, we 
present the main characteristics of a new 
inflectional module, conceived at LabEL, fully 
compatible with this system. Next, we describe the 
acquisition methodology used to gather new 
dictionary entries in a fragment (extracts 1,520,001 
to 1,567,625) of the non-annotated version of a 
public Portuguese corpus, CETEMPublico2. 
Finally, based on this experiment, we assess, on 
the one hand, the  dictionary increase, and, on the 
other hand, the lexical coverage improvement in 
the referred corpus. 
2 Characterization of Multiword Nouns 
Multiword (or compound) nouns are composed of 
non-capitalized simple words. Superficially, they 
seem to result from general rules of word 
combinations but they present constraints 
(morphological, combinatorial, etc.) concerning 
the properties they were supposed to have. 
Regarding inflection, general rules presented by 
grammarians do apply to some cases, but most 
compounds exhibit inflectional restrictions on 
gender or number that cannot be described by the 
morphological properties of their constituents. 
Table 1 presents a few examples of the most 
representative classes of compound nouns in 
Portuguese. 
                                                     
2
 CETEMPublico is a journalistic corpus containing 
about 180 million words (see Santos and Rocha, 2001 
for techical information). 
    	 
 	         

 

 
  
 

 
 	 
  
 	 
  	 
 
  	 
 
 	 
     	 
 
 	 
       
    	 
 
 	 
  	 
 
 	 
   
  	 
 
                  
  
   Event Detection and Summarization in Weblogs with Temporal Collocations 
Chun-Yuan Teng and Hsin-Hsi Chen 
Department of Computer Science and Information Engineering 
National Taiwan University 
Taipei, Taiwan 
{r93019, hhchen}@csie.ntu.edu.tw 
Abstract 
 
This paper deals with the relationship between weblog content and time. With the proposed temporal mutual information, we analyze 
the collocations in time dimension, and the interesting collocations related to special events. The temporal mutual information is 
employed to observe the strength of term-to-term associations over time. An event detection algorithm identifies the collocations that 
may cause an event in a specific timestamp. An event summarization algorithm retrieves a set of collocations which describe an event. 
We compare our approach with the approach without considering the time interval. The experimental results demonstrate that the 
temporal collocations capture the real world semantics and real world events over time. 
 
1. 
2. 
Introduction 
Compared with traditional media such as online news 
and enterprise websites, weblogs have several unique 
characteristics, e.g., containing abundant life experiences 
and public opinions toward different topics, highly 
sensitive to the events occurring in the real world, and 
associated with the personal information of bloggers. 
Some works have been proposed to leverage these 
characteristics, e.g., the study of the relationship between 
the content and bloggers? profiles (Adamic & Glance, 
2005; Burger & Henderson, 2006; Teng & Chen, 2006), 
and content and real events (Glance, Hurst & Tornkiyo, 
2004; Kim, 2005; Thelwall, 2006; Thompson, 2003). 
In this paper, we will use temporal collocation to 
model the term-to-term association over time.  In the past, 
some useful collocation models (Manning & Sch?tze, 
1999) have been proposed such as mean and variance, 
hypothesis test, mutual information, etc. Some works 
analyze the weblogs from the aspect of time like the 
dynamics of weblogs in time and location (Mei, et al, 
2006), the weblog posting behavior (Doran, Griffith & 
Henderson, 2006; Hurst, 2006), the topic extraction (Oka, 
Abe & Kato, 2006), etc. The impacts of events on social 
media are also discussed, e.g., the change of weblogs after 
London attack (Thelwall, 2006), the relationship between 
the warblog and weblogs (Kim, 2005; Thompson, 2003), 
etc. 
This paper is organized as follows. Section 2 defines 
temporal collocation to model the strength of term-to-term 
associations over time.  Section 3 introduces an event 
detection algorithm to detect the events in weblogs, and 
an event summarization algorithm to extract the 
description of an event in a specific time with temporal 
collocations. Section 4 shows and discusses the 
experimental results.  Section 5 concludes the remarks. 
Temporal Collocations 
We derive the temporal collocations from Shannon?s 
mutual information (Manning & Sch?tze, 1999) which is 
defined as follows (Definition 1). 
Definition 1 (Mutual Information) The mutual 
information of two terms x and y is defined as: 
)()(
),(log),(),(
yPxP
yxPyxPyxI =  
where P(x,y) is the co-occurrence probability of x and y, 
and P(x) and P(y) denote the occurrence probability of x 
and y, respectively. 
Following the definition of mutual information, we 
derive the temporal mutual information modeling the 
term-to-term association over time, and the definition is 
given as follows.  
 Definition 2 (Temporal Mutual Information) Given 
a timestamp t and a pair of terms x and y, the temporal 
mutual information of x and y in t is defined as: 
)|()|(
)|,(log)|,()|,(
tyPtxP
tyxPtyxPtyxI =
where P(x,y|t) is the probability of co-occurrence of terms 
x and y in timestamp t, P(x|t) and P(y|t) denote the 
probability of occurrences of x and y in timestamp t, 
respectively. 
To measure the change of mutual information in time 
dimension, we define the change of temporal mutual 
information as follows. 
Definition 3 (Change of Temporal Mutual 
Information) Given time interval [t1, t2], the change of 
temporal mutual information is defined as: 
12
12
21
)|,()|,(),,,(
tt
tyxItyxIttyxC ?
?=  
where C(x,y,t1,t2) is the change of temporal mutual 
information of terms x and y in time interval [t1, t2], I(x,y| 
t1) and I(x,y| t2) are the temporal mutual information in 
time t1 and t2, respectively. 
3. Event Detection 
Event detection aims to identify the collocations 
resulting in events and then retrieve the description of 
events. Figure 1 sketches an example of event detection. 
The weblog is parsed into a set of collocations. All 
collocations are processed and monitored to identify the 
plausible events.  Here, a regular event ?Mother?s day? 
and an irregular event ?Typhoon Chanchu? are detected.  
The event ?Typhoon Chanchu? is described by the words  
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1: An Example of Event Detection
?Typhoon?, ?Chanchu?, ?2k?, ?Eye?, ?Path? and 
?chinaphillippine?.  
The architecture of an event detection system includes 
a preprocessing phase for parsing the weblogs and 
retrieving the collocations; an event detection phase 
detecting the unusual peak of the change of temporal 
mutual information and identifying the set of collocations 
which may result in an event in a specific time duration; 
and an event summarization phase extracting the 
collocations related to the seed collocations found in a 
specific time duration. 
The most important part in the preprocessing phase is 
collocation extraction. We retrieve the collocations from 
the sentences in blog posts. The candidates are two terms 
within a window size. Due to the size of candidates, we 
have to identify the set of tracking terms for further 
analysis. In this paper, those candidates containing 
stopwords or with low change of temporal mutual 
information are removed. 
In the event detection phase, we detect events by 
using the peak of temporal mutual information in time 
dimension.  However, the regular pattern of temporal 
mutual information may cause problems to our detection. 
Therefore, we remove the regular pattern by seasonal 
index, and then detect the plausible events by measuring 
the unusual peak of temporal mutual information. 
If a topic is suddenly discussed, the relationship 
between the related terms will become higher. Two 
alternatives including change of temporal mutual 
information and relative change of temporal mutual 
information are employed to detect unusual events. Given 
timestamps t1 and t2 with temporal mutual information 
MI1 and MI2, the change of temporal mutual information 
is calculated by (MI2-MI1). The relative change of 
temporal mutual information is calculated by (MI2-
MI1)/MI1. 
For each plausible event, there is a seed collocation, 
e.g., ?Typhoon Chanchu?. In the event description 
retrieval phase, we try to select the collocations with the 
highest mutual information with the word w in a seed 
collocation. They will form a collocation network for the 
event.  Initially, the seed collocation is placed into the 
network.  When a new collocation is added, we compute 
the mutual information of the multiword collocations by 
the following formula, where n is the number of 
collocations in the network up to now. 
?= n iMInInformatioMutualMultiwo  
If the multiword mutual information is lower than a 
threshold, the algorithm stops and returns the words in the 
collocation network as a description of the event.  Figure 
2 sketches an example.  The collocations ?Chanchu?s 
path?, ?Typhoon eye?, and ?Chanchu affects? are added 
into the network in sequence based on their MI. 
We have two alternatives to add the collocations to 
the event description. The first method adds the 
collocations which have the highest mutual information 
as discussed above. In contrast, the second method adds 
the collocations which have the highest product of mutual 
information and change of temporal mutual information. 
 
 
 
 
 
 
Figure 2: An Example of Collocation network 
4. 
4.1. 
Experiments and Discussions 
Temporal Mutual Information versus 
Mutual Information 
In the experiments, we adopt the ICWSM weblog data 
set (Teng & Chen, 2007; ICWSM, 2007). This data set 
collected from May 1, 2006 through May 20, 2006 is 
about 20 GB. Without loss of generality, we use the 
English weblog of 2,734,518 articles for analysis. 
To evaluate the effectiveness of time information, we 
made the experiments based on mutual information 
(Definition 1) and temporal mutual information 
(Definition 2). The former called the incremental 
approach measures the mutual information at each time 
point based on all available temporal information at that 
time. The latter called the interval-based approach 
considers the temporal mutual information in different 
time stamps.  Figures 3 and 4 show the comparisons 
between interval-based approach and incremental 
approach, respectively, in the event of Da Vinci Code.   
We find that ?Tom Hanks? has higher change of 
temporal mutual information compared to ?Da Vinci 
Code?. Compared to the incremental approach in Figure 4, 
the interval-based approach can reflect the exact release 
date of ?Da Vinci Code.? 
 rd
=i 1 4.2. Evaluation of Event Detection 
We consider the events of May 2006 listed in 
wikipedia1 as gold standard. On the one hand, the events 
posted in wikipedia are not always complete, so that we 
adopt recall rate as our evaluation metric.  On the other 
hand, the events specified in wikipedia are not always 
discussed in weblogs.  Thus, we search the contents of 
blog post to verify if the events were touched on in our 
blog corpus. Before evaluation, we remove the events 
listed in wikipedia, but not referenced in the weblogs. 
 
 
 
 
 
 
 
 
 
 
 
Figure 3: Interval-based Approach in Da Vinci Code  
 
 
 
 
 
 
 
 
Figure 4: Incremental Approach in Da Vinci Code 
gure 5 sketches the idea of evaluation.  The left side 
of t s figure shows the collocations detected by our event 
dete tion system, and the right side shows the events 
liste  in wikipedia.  After matching these two lists, we 
can find that the first three listed events were correctly 
identified by our system.  Only the event ?Nepal Civil 
War? was listed, but not found. Thus, the recall rate is 
75% in this case. 
 
 
 
 
 
 
 
Figure 5: Evaluation of Event Detection Phase 
As discussed in Section 3, we adopt change of 
temporal mutual information, and relative change of 
temporal mutual information to detect the peak. In Figure 
6, we compare the two methods to detect the events in 
weblogs. The relative change of temporal mutual 
information achieves better performance than the change 
of temporal mutual information. 
                                                     
1 http://en.wikipedia.org/wiki/May_2006 
Table 1 and Table 2 list the top 20 collocations based 
on these two approaches, respectively. The results of the 
first approach show that some collocations are related to 
the feelings such as ?fell left? and time such as ?Saturday 
night?. In contrast, the results of the second approach 
show more interesting collocations related to the news 
events at that time, such as terrorists ?zacarias 
moussaoui? and ?paramod mahajan.? These two persons 
were killed in May 3. Besides, ?Geena Davis? got the 
golden award in May 3. That explains why the 
collocations detected by relative change of temporal 
mutual information are better than those detected by 
change of temporal mutual information. 
-20
-15
-10
-5
0
5
10
1 3 5 7 9 11 13 15 17 19
Time (day)
M
ut
ua
l i
nf
or
m
at
io
n
Da-Vinci Tom Hanks
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 6: Performance of Event Detection Phase 
-15
-10
-5
0
5
10
1 3 5 7 9 11 13 15 17 19
Time (day)
M
ut
ua
l i
nf
or
m
at
io
n
Da-Vinci Tom Hanks
Collocations CMI Collocations CMI 
May 03 9276.08 Current music 1842.67
Illegal immigrants 5833.17 Hate studying 1722.32
Feel left 5411.57 Stephen Colbert 1709.59
Saturday night 4155.29 Thursday night 1678.78
Past weekend 2405.32 Can?t believe 1533.33
White house 2208.89 Feel asleep 1428.18
Red sox 2208.43 Ice cream 1373.23
Album tool 2120.30 Oh god 1369.52
Sunday morning 2006.78 Illegalimmigration 1368.12
16.56
f 
CMI
32.50
31.63
29.09
28.45
28.34
28.13Sunday night 1992.37 Pretty cool 13
Table 1: Top 20 collocations with highest change o
temporal mutual information 
Collocations CMI Collocations 
casinos online 618.36 Diet sodas 
zacarias moussaoui 154.68 Ving rhames 
Tsunami warning 107.93 Stock picks 
Conspirator zacarias 71.62 Happy hump 
Artist formerly 57.04 Wong kan 
Federal  
Jury 
41.78 Sixapartcom 
movabletype Wed 3 39.20 Aaron echolls 27.48
Pramod mahajan 35.41 Phnom penh 25.78
BBC  
Version 
35.21 Livejournal 
sixapartcom 
23.83  Fi
hi
c
dGeena davis 33.64 George yeo 20.34
Table 2: Top 20 collocations with highest relative change 
of mutual information 
4.3. Evaluation of Event Summarization 
As discussed in Section 3, we have two methods to 
include collocations to the event description. Method 1 
employs the highest mutual information, and Method 2 
utilizes the highest product of mutual information and 
change of temporal mutual information. Figure 7 shows 
the performance of Method 1 and Method 2. We can see 
that the performance of Method 2 is better than that of 
Method 1 in most cases. 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 7: Overall Performance of Event Summarization 
The results of event summarization by Method 2 are 
shown in Figure 8. Typhoon Chanchu appeared in the 
Pacific Ocean on May 10, 2006, passed through 
Philippine and China and resulted in disasters in these 
areas on May 13 and 18, 2006.  The appearance of the 
typhoon Chanchu cannot be found from the events listed 
in wikipedia on May 10.  However, we can identify the 
appearance of typhoon Chanchu from the description of 
the typhoon appearance such as ?typhoon named? and 
?Typhoon eye.  In addition, the typhoon Chanchu?s path 
can also be inferred from the retrieved collocations such 
as ?Philippine China? and ?near China?. The response of 
bloggers such as ?unexpected typhoon? and ?8 typhoons? 
is also extracted.   
 
 
 
 
 
 
 
 
 
 
Figure 8: Event Summarization for Typhoon Chanchu 
5. Concluding Remarks 
This paper introduces temporal mutual information to 
capture term-term association over time in weblogs. The 
extracted collocation with unusual peak which is in terms 
of relative change of temporal mutual information is 
selected to represent an event.  We collect those 
collocations with the highest product of mutual 
information and change of temporal mutual information 
to summarize the specific event.  The experiments on 
ICWSM weblog data set and evaluation with wikipedia 
event lists at the same period as weblogs demonstrate the 
feasibility of the proposed temporal collocation model 
and event detection algorithms. 
Currently, we do not consider user groups and 
locations. This methodology will be extended to model 
the collocations over time and location, and the 
relationship between the user-preferred usage of 
collocations and the profile of users. 
Acknowledgments 
Research of this paper was partially supported by 
National Science Council, Taiwan (NSC96-2628-E-002-
240-MY3) and Excellent Research Projects of National 
Taiwan University (96R0062-AE00-02). 
References 
Adamic, L.A., Glance, N. (2005). The Political 
Blogosphere and the 2004 U.S. Election: Divided 
They Blog. In: Proceedings of the 3rd International 
Workshop on Link Discovery, pp. 36--43. 
Burger, J.D., Henderson J.C. (2006). An Exploration of 
Observable Features Related to Blogger Age. In: 
Proceedings of AAAI 2006 Spring Symposium on 
Computational Approaches to Analysing Weblogs, pp. 
15--20. 
Doran, C., Griffith, J., Henderson, J. (2006). Highlights 
from 12 Months of Blogs. In: Proceedings of AAAI 
2006 Spring Symposium on Computational 
Approaches to Analysing Weblogs, pp. 30--33. 
Glance, N., Hurst, M., Tornkiyo, T. (2004). Blogpulse: 
Automated Trend Discovery for Weblogs. In: 
Proceedings of WWW 2004 Workshop on the 
Weblogging Ecosystem: Aggregation, Analysis, and 
Dynamics. 
Hurst, M. (2006). 24 Hours in the Blogosphere. In: 
Proceedings of AAAI 2006 Spring Symposium on 
Computational Approaches to Analysing Weblogs, pp. 
73--77. 
ICWSM (2007). http://www.icwsm.org/data.html 
Kim, J.H. (2005). Blog as an Oppositional Medium? A 
Semantic Network Analysis on the Iraq War Blogs. In: 
Internet Research 6.0: Internet Generations. 
 
Manning, C.D., Sch?tze, H. (1999). Foundations of 
Statistical Natural Language Processing, The MIT 
Press, London England. 
Mei, Q., Liu, C., Su, H., Zhai, C. (2006). A Probabilistic 
Approach to Spatiotemporal Theme Pattern Mining on 
Weblogs. In: Proceedings of the 15th International 
Conference on World Wide Web, Edinburgh, Scotland, 
pp. 533--542. 
Oka, M., Abe, H., Kato, K. (2006). Extracting Topics 
from Weblogs Through Frequency Segments. In: 
Proceedings of WWW 2006 Annual Workshop on the 
Weblogging Ecosystem: Aggregation, Analysis, and 
Dynamics. 
Teng, C.Y., Chen, H.H. (2006). Detection of Bloggers? 
Interest: Using Textual, Temporal, and Interactive 
Features. In: Proceeding of IEEE/WIC/ACM 
International Conference on Web Intelligence, pp. 
366--369. 
Teng, C.Y., Chen, H.H. (2007). Analyzing Temporal 
Collocations in Weblogs. In: Proceeding of 
International Conference on Weblogs and Social 
Media, 303--304. 
Thelwall, M. (2006). Blogs During the London Attacks: 
Top Information Sources and Topics. In: Proceedings 
of 3rd Annual Workshop on the Weblogging 
Ecosystem: Aggregation, Analysis and Dynamics. 
Thompson, G. (2003). Weblogs, Warblogs, the Public 
Sphere, and Bubbles. Transformations, 7(2). 
Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 129?137,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Relation detection between named entities: report of a shared task
Cla?udia Freitas, Diana Santos
Cristina Mota
SINTEF ICT
claudiafreitas@puc-rio.br
Diana.Santos@sintef.no
cmota@ist.utl.pt
Hugo Gonc?alo Oliveira
CISUC, DEI - FCTUC
hroliv@dei.uc.pt
Paula Carvalho
Univ. Lisbon, FCUL, XLDB
pcc@di.fc.ul.pt
Abstract
In this paper we describe the first evalu-
ation contest (track) for Portuguese whose
goal was to detect and classify relations be-
tween named entities in running text, called
ReRelEM. Given a collection annotated with
named entities belonging to ten different se-
mantic categories, we marked all relationships
between them within each document. We used
the following fourfold relationship classifi-
cation: identity, included-in, located-in, and
other (which was later on explicitly detailed
into twenty different relations). We provide a
quantitative description of this evaluation re-
source, as well as describe the evaluation ar-
chitecture and summarize the results of the
participating systems in the track.
1 Motivation
Named entity recognition can be considered the first
step towards semantic analysis of texts and a crucial
subtask of information extraction systems. Proper
names, besides their high frequency in language, do
more than just refer ? they convey additional infor-
mation as instances of general semantic categories.
But NE recognition is, as just mentioned, only the
first step for full language processing. If we want to
go beyond the detection of entities, a natural step is
establishing semantic relations between these enti-
ties, and this is what this paper is about.
There are two fairly independent communities
that focus on the task of detecting relations between
named entities: the work on anaphora resolution, il-
lustrated by (Mitkov, 2000; Collovini et al, 2007;
de Souza et al, 2008) and the work on relation de-
tection in information extraction, see e.g. (Agichtein
and Gravano, 2000; Zhao and Grishman, 2005; Cu-
lotta and Sorensen, 2004). Although both commu-
nities are doing computational semantics, the two
fields are largely non-overlapping, and one of the
merits of our work is that we tried to merge the two.
Let us briefly describe both traditions: as (Mitkov,
2000) explains, anaphora resolution is concerned
with studying the linguistic phenomenon of pointing
back to another expression in the text. The seman-
tic relations between the referents of these expres-
sions can be of different types, being co-reference a
special case when the relation is identity. The focus
of anaphora resolution is determining the antecedent
chains, although it implicitly also allows to elicit se-
mantic relations between referents. This task has a
long tradition in natural language processing (NLP)
since the early days of artificial intelligence (Web-
ber, 1978), and has from the start been considered a
key ingredient in text understanding.
A different tradition, within information extrac-
tion and ontology building, is devoted to fact ex-
traction. The detection of relations involving named
entities is seen as a step towards a more structured
model of the meaning of a text. The main concerns
here (see e.g. (Zhao and Grishman, 2005)) are the
extraction of large quantities of facts, generally cou-
pled with machine learning approaches.1
Although mentions of named entities may ex-
1Other authors use the term relation detection in still other
ways: for example, (Roth and tau Yih, 2004) use it for the trans-
lation of any natural language sentences into ?logical form?, as
in kill (x,y). This task does not concern us here.
129
Relations Works
orgBased-in, Headquarters, Org-Location, Based-in RY, AG, DI, Sn, CS, ACE07, ACE04, ZG
live-in, Citizen-or-Resident RY, ACE04, ZG, ACE07,CS
Employment, Membership, Subsidiary ZG, CS, ACE04, ACE07
located(in), residence, near ACE04, ACE07,CS, ZG
work-for, Affiliate, Founder, Management,
Client, Member, Staff CS, ACE04, ACE07, RY, ZG
Associate, Grandparent, Parent, Sibling,
Spouse, Other-professional, Other-relative, Other-personal CS, ACE04, ACE07
User, Owner,Inventor, Manufacturer ACE04,ACE07, ZG, CS
DiseaseOutbreaks AG
Metonymy ACE07
identity ARE
synonym ARE
generalisation ARE
specialisation ARE
Table 1: Relations used in other works or evaluation contests.
press semantic relations other than identity or de-
pendency, the main focus of the first school has
been limited to co-reference. Yet, relations such
as part-of have been considered under the label
of indirect anaphora, also known as associative or
bridging anaphora.
Contrarywise, the list of relations of interest for
the second school is defined simply by world knowl-
edge (not linguistic clues), and typical are the rela-
tions between an event and its location, or an orga-
nization and its headquarters. Obviously, these rela-
tions do occur between entities that do not involve
(direct or indirect) anaphora in whatever broad un-
derstanding of the term.
Also, relation detection in the second school does
not usually cover identity (cf. ACE?s seven relation
types): identity or co-reference is often considered
an intermediate step before relation extraction (Cu-
lotta and Sorensen, 2004).
Table 1 displays a non-exhaustive overview of the
different relations found in the literature.2
In devising the ReRelEM3 pilot track, our goal
was twofold: to investigate which relations could
2There is overlap between ACE 2007 and 2004 types of re-
lations. In order to ease the comparison, we used the names of
subtypes for ACE relations.
3ReRelEM stands for Reconhecimento de Relac?o?es entre
Entidades Mencionadas, Portuguese for ?recognition of rela-
tions between named entities?, see (Freitas et al, 2008).
be found between named entities in Portuguese text,
and how could a pilot task be devised that compared
the performance of different automatic systems sup-
posed to identify them. It should be emphasized that
both MUC and ACE were key inspiration sources for
ReRelEM, which stems from Linguateca?s emphasis
on evaluation.
In fact, we were conversant with MUC co-
reference track and the way it was scored, as well
as aware of two other related evaluation contests:
ACE (Doddington et al, 2004; NIST and ACE,
2007), which extended MUC by dropping the re-
quirement that entities had to be named, and ARE
(Ora?san et al, 2008), which requested the identifi-
cation of an anaphoric relation in certain types of
pre-defined relations (identity, synonymy, general-
ization and specification), but which ignored indirect
anaphora (that may convey meronymy, or inclusion,
in a broad sense).
ReRelEM, although maintaining (or adding) the
restriction to named entitites, is, from our point of
view, an advance in the field of relation detection,
since we proposed the detection (and classification)
of all (relevant) kinds of relations between NEs in a
document, providing thus both a merge and an ex-
tension of the previous evaluation campaigns.
130
Category/gloss #
PESSOA/person 196
LOCAL/place 145
ORGANIZACAO/org 102
TEMPO/time 84
OBRA/title 33
VALOR/value 33
ACONTECIMENTO/event 21
ABSTRACCAO/abstraction 17
OUTRO/other 6
COISA/thing 5
Table 2: Category distribution in the golden collection
2 Track description
The purpose of ReRelEM is to assess systems that
try to recognize the most relevant relations between
named entities, even if those relations do not involve
coreference or anaphora.
2.1 Context
In order for it to be feasible in the short time we
had, the track definition required that both referring
expression and their semantic referent were named
entities. Pronouns and definite descriptions were
hence excluded. Note also that ReRelEM was de-
fined in the context of the second edition of a larger
evaluation contest dealing with NE detection and
classification in Portuguese, HAREM (Santos et al,
2008) (for a detailed description of HAREM, in Por-
tuguese, see also (Santos and Cardoso, 2007; Mota
and Santos, 2008)). HAREM required systems to
choose among ten categories (see Table 2), 43 types
and 21 subtypes, the later concerning the categories
TEMPO (time) and LOCAL (place).
So, it should be emphasized that ReRelEM fo-
cuses only on the classification and detection of
the relations, not limiting in any way the kinds of
(named) entities that can be related (as usualy done
in other detection tasks). It only enforces the kinds
of relations that must be identified.
2.2 Relation inventory
The establishment of an inventory of the most rele-
vant relations between NEs is ultimately subjective,
depending on the kind of information that each par-
ticipant aims to extract. We have nevertheless done
an exploratory study and annotated exhaustively a
few texts to assess the most frequent and less con-
troversial (or easier to assign) relations, and came
up with just the following relation types for the task
proposal:
? identity (ident);
? inclusion (inclui (includes) or incluido
(included));
? placement (ocorre-em (occurs-in) or
sede-de (place-of));
? other (outra)
For further description and examples see section 3.
However, during the process of building
ReRelEM?s golden collection (a subset of the
HAREM collection used as gold standard), human
annotation was felt to be more reliable ? and also
more understandable ? if one specified what ?other?
actually meant, and so a further level of detail
(twenty new relations) was selected and marked,
see Table 3. (In any case, since this new refinement
did not belong to the initial task description, all
were mapped back to the coarser outra relation
for evaluation purposes.)
2.3 ReRelEM features and HAREM
requirements
The annotation process began after the annotation
of HAREM?s golden collection, that is, the relations
started to be annotated after all NE had been tagged
and totally revised. For ReRelEM, we had therefore
no say in that process ? again, ReRelEM was only
concerned with the relations between the classified
NEs. However, our detailed consideration of rela-
tions helped to uncover ? and correct some mistakes
in the original classification.
In order to explain the task(s) at hand, let us de-
scribe shortly ReRelEM?s syntax: In ReRelEM?s
golden collection, each NE has a unique ID. A re-
lation between NE is indicated by the additional at-
tributes COREL (filled with the ID of the related en-
tity) and TIPOREL (filled with the name of the re-
lation) present in the NE that corresponds to one of
the arguments of the relation. (Actually, there?s no
difference if the relation is marked in the first or in
the second argument.)
131
One referring expression can be associated with
one or more NEs through several semantic relations.
In such cases, all possible relations must be assigned
to the referring expression, in the form of a list, as
illustrated by UTAD in Figure 1.
In this example, the NE with name UTAD (and
id ex1-42) corresponds to an acronym of Univer-
sidade de Tra?s-os-Montes e Alto Douro (a univer-
sity in Portugal), maintaining with this entity an
identity relation (ident). The TIPOREL field of
ex1-42 contains another relation, inclui, which
stands for the relation of inclusion, this time with
the previously mentioned Servic?os Administrativos
(ex1-40), a specific department of the university.
In order to minimize human labour and also to
let systems mark relations the way it would better
suit them, we have postulated from the start that, for
all purposes, it would be equivalent to annotate ev-
erything or just enough relations so that all others
can be automatically computed. So, the evaluation
programs, in an obvious extension of what was pro-
posed in (Vilain et al, 1995) for identity,
1. add/expand all relations with their inverses
(e.g., ?A includes B? entails ?B is included in
A?), and
2. apply a set of expansion rules (see examples in
Table 4) to compute the closure
As a consequence, different systems may tag the
same text in different ways, but encoding the same
knowledge.
2.4 What is a relevant relation?
An important difference as to what we expect as rel-
evant relations should be pointed out: instead of re-
quiring explicit (linguistic) clues, as in traditional
research on anaphor, we look for all relations that
may make sense in the specific context of the whole
document. Let us provide two arguments supporting
this decision:
? the first one is philosophical: the borders be-
tween world knowledge and contextual infer-
ence can be unclear in many cases, so it is not
easy to distinguish them, even if we did believe
in that separation in the first place;
? the second is practical: marking all possible re-
lations is a way to also deal with unpredictable
informational needs, for example for text min-
ing applications. Take a sentence like ?When I
lived in Peru, I attended a horse show and was
able to admire breeds I had known only from
pictures before, like Falabella and Paso.?. From
this sentence, few people would infer that Paso
is a Peruvian breed, but a horse specialist might
at once see the connection. The question is:
should one identify a relation between Peru and
Paso in this document? We took the affirmative
decision, assuming the existence of users inter-
ested in the topic: ?relation of breeds to horse
shows: are local breeds predominant??.
However, and since this was the first time such eval-
uation contest was run, we took the following mea-
sure: we added the attribute INDEP to the cases
where the relation was not possible to be inferred
by the text. In this way, it is possible to assess
the frequency of these cases in the texts, and one
may even filter them out before scoring the system
runs to check their weight in the ranking of the sys-
tems. Interestingly, there were very few cases (only
6) marked INDEP in the annotated collection.
3 Qualitative relation description
Identity (or co-reference) does not need to be ex-
plained, although we should insist that this is not
identity of expression, but of meaning. So the same
string does not necessarily imply identity, cf.:
Os adeptos do Porto invadiram a cidade
do Porto em ju?bilo.4
Interestingly, even though organization is only the
third most frequent category, Figure 2 shows that we
found more co-reference among organizations than
among any other category.
As to inclusion (see Figure 3), it was defined be-
tween NEs of the same sort, as the folowing ex-
amples, respectively illustrating LOCAL, PESSOA,
OBRA and ORGANIZACAO, show:
Centenas de pessoas recepcionaram no
Aeroporto da Portela num clima de
enorme entusiasmo e euforia, a selecc?a?o
4The (FC) Porto fans invaded the (city of) Porto, very happy
132
<EM ID="ex1-39" CATEG="PESSOA" TIPO="INDIVIDUAL"> Miguel Rodrigues</EM>
, chefe dos <EM ID="ex1-40" CATEG="ORGANIZACAO" TIPO="INSTITUICAO"
COREL="ex1-39" TIPOREL="outra">Servic?os Administrativos</EM> da <EM
ID="ex1-41" CATEG="ORGANIZACAO" TIPO="INSTITUICAO" COREL="ex1-40"
TIPOREL="inclui"> Universidade de Tra?s-os-Montes e Alto Douro</EM> <EM
ID="ex1-42" CATEG="ORGANIZACAO" TIPO="INSTITUICAO" COREL="ex1-41 ex1-40"
TIPOREL="ident inclui">UTAD</EM>
Figure 1: Full example of ReRelEM syntax.
Figure 2: Distribution of NE categories for identity.
portuguesa de ra?guebi. A boa prestac?a?o
global da equipa (...) na?o passou desperce-
bida em Portugal.5
Lewis Hamilton, colega de Alonso na
McLaren6
da assinatura do Tratado de Lisboa (...)
de ver reconhecido o valor juridicamente
vinculativo da Carta um passo ?essencial
no quadro de reforma dos Tratados7
por participar na cerimo?nia de
proclamac?a?o da Carta dos Direitos
Fundamentais da UE (...) salientou
ainda o compromisso assumido pelas tre?s
instituic?o?es - PE8
5Hundreds of people waited with enthusiasm and eupho-
ria at the Portela Airport for the Portuguese national rugby
team.(...) The team?s good performance did not go unnoticed in
Portugal
6Lewis Hamilton, Alonso?s team-mate in McLaren ? Note
that, in HAREM, teams are considered groups of people, there-
fore an individual and a team have the same category PESSOA
(person), but differ in the type.
7the signing of the Lisbon Treaty (...) juridically vincula-
tive value of the Charter, a crucial step for the Treaties reform
policy
8to participate in the proclamation ceremony of the Charter
Figure 3: NE categories related by inclusion.
Placement is clearly skewed towards placement of
organizations (518 cases) as opposed to occurrence
of events (just 98 instances). However, if we con-
sider the relative distribution of organizations and
events (see Table 2), we can state that, relative to
their places, events have 4.8 relations in average and
organizations 5.0, which is a far more interesting re-
sult, not favouring any of the NE classes.
Examples of this relation are:
GP Brasil ? Na?o faltou emoc?a?o em Inter-
lagos no Circuito Jose? Carlos Pace9
As to the refinement of outra, Table 3 presents the
relations found in the material.
3.1 Vague categories
It is important to stress that the basic tenets of
HAREM had to be followed or reckoned with, not
only the classification grid (see Table 2) but par-
ticularly the fact that some named entities are con-
sidered to be vague among different categories in
of Fundamental Rights of the EU (...) stressed the commitment
assumed by the three institutions - EP
9GP Brasil ? There was no lack of excitement in Interlagos
at the Jose? Carlos Pace Circuit.
133
Relation / gloss Number
vinculo-inst / inst-commitment 936
obra-de / work-of 300
participante-em / participant-in 202
ter-participacao-de / has-participant 202
relacao-familiar / family-tie 90
residencia-de / home-of 75
natural-de / born-in 47
relacao-profissional / professional-tie 46
povo-de / people-of 30
representante-de / representative-of 19
residente-de / living-in 15
personagem-de / character-of 12
periodo-vida / life-period 11
propriedade-de / owned-by 10
proprietario-de / owner-of 10
representado-por / represented-by 7
praticado-em / practised-in 7
outra-rel/other 6
nome-de-ident / name-of 4
outra-edicao / other-edition 2
Table 3: Frequency of other relations.
HAREM.10
This last property, namely that named entities
could belong to more than one category, posed some
problems, since it was not straightforward whether
different relations would involve all or just some (or
one) category. So, in order to specify clearly the
relations between vague NEs, we decided to spec-
ify separate relations between facets of vague named
entities. Cf. the following example, in which vague-
ness is conveyed by a slash:
(...) a ideia de uma Europa (LO-
CAL/PESSOA) unida. (...) um dia feliz
para as cidada?s e os cidada?os da Unia?o
Europeia (LOCAL). (...) Somos essen-
cialmente uma comunidade de valores ?
sa?o estes valores comuns que constituem
o fundamento da Unia?o Europeia (AB-
STRACCAO/ORG/LOCAL).11
10This is different from considering metonymy classes, in
that no classifications are considered more basic than others, see
(Santos, 2006) for vagueness as an intrinsic property of natural
language.
11the idea of a united Europe (...) a happy day for the citizens
The several relations between the three bold-faced
NEs have been found to be as follows: The LO-
CAL facet of the first NE is identical with the LO-
CAL facets of the second and third NEs, while the
ORG(ANIZACAO) facet of the third NE is located
in the LOCAL facet of the second and first NEs.
(Two kinds of relations are therefore involved here:
ident and inclui.)
4 Evaluation: architecture and measures
Our first concern in this pilot track was to make a
clear separation between the evaluation of relations
and the evaluation of NE detection, which was the
goal of HAREM. So, ReRelEM ?s evaluation uses as
a starting point the set of alignments that correspond
to a mapping of the NE in the golden collection (GC)
to a (candidate) NE in the participation.
Evaluation has the following stages:
? Maximization: the sets of relations annotated in
both the GC and in the participation are maxi-
mized, applying the rules in Table 4;
? Selection: the alignments where the NE in the
GC is different from the corresponding one in
the participation are removed, and so are all re-
lations held between removed NEs;
? Normalization: The identifiers of the NE in the
participation are normalized in order to make it
possible to compare the relations in both sides,
given that each system uses its own identifiers.
? Translation: The alignments are translated to
triples: arg1 relation arg2, where the
arguments consist of the identifiers of the
NE together with the facet, for example x67
LOCAL sede-de ty45 ORGANIZACAO.
? Filtering: removing relations of types not be-
ing evaluated (because HAREM, and therefore
ReRelEM, allows for partial participation ? and
evaluation ? scenarios12).
? Individual evaluation: the triples in the GC are
compared to the triples in the participation.
of the European Union (...) We are mainly a community of
values and these common values constitute the foundation of
the European Union.
12In other words, it is possible to select a subset of the classi-
fication hierarchy.
134
A ident B ? B ident C ? A ident C
A inclui B ? B inclui C ? A inclui C
A inclui B ? B sede de C ? A sede de C
A ident B ? B any rel C ? A any rel C
Table 4: Maximization rules
System NE task Relations
Rembr. all all
SeRelEP only identification all but outra
SeiGeo only LOCAL detection inclusion
Table 5: Participant systems
? Global evaluation: measures (precision, recall
and F-measure) are calculated based on the
score of each triple.
Each triple is scored as correct, missing or incor-
rect. We only considered as correct triples (and cor-
rect relations) those which linked the correct NEs
and whose relation was well classified. So, a system
doesn?t score if it correctly matches the NEs to be re-
lated, but fails to recognize the kind of relation. We
assign one point to each correct relation and none to
incorrect or missing relations, and then we compute
precision, recall and F-measure.
ReRelEM?s golden collection includes 12 texts
with 4,417 words and 573 NEs (corresponding to
642 different facets). In all we annotated 6,790 re-
lations (1436 identity; 1612 inclusion; 1232 place-
ment; 2510 other).
5 Participation and results
For this first edition, only three systems (totalling
nine runs) participated, namely REMBRANDT
(Cardoso, 2008), SEI-Geo (Chaves, 2008), and
SeRelEP (Bruckschen et al, 2008), whose results
are found in Figure 4. However, they did not com-
pare well: they selected different NER tasks and dif-
ferent relation types, as shown in Table 5. So, given
the little and diverse participation in ReRelEM, we
cannot do a useful state of the art, but we were def-
initely able to provide an interesting and important
resource for empirical studies and for training of fu-
ture systems, as well as a set of publicly available
programs to manipulate, evaluate and display this
Figure 4: ReRelEM results: F-measure, all relations
kind of semantic data13.
6 Discussion and further work
Although this was just a pilot, a lot of knowledge
about the task and the problems to be dealt with were
gathered for the future, and important resources
were offered to the community.
We intend to annotate further sections of the
HAREM golden collection (as well as other kinds
of texts and materials) with more relations in order
to have more quantitative empirical data for studying
the semantic fabric of Portuguese.
Although from an organization point of view it
made sense to couple ReRelEM with HAREM, one
should reflect over the consequences of inheriting a
lot of decisions taken in HAREM, somehow going
counter the intuitive and easier task of just annotat-
ing relations in a first round. However, despite initial
fears to the contrary, we found out that the consid-
erably fine-grained HAREM grid was in fact benefi-
cial to the task of specifying relations: it is, after all,
much more informative to have a relation of inclu-
sion between a COISA-MEMBROCLASSE (concrete
instance of a class of objects) and a COISA-CLASSE
(a class of objects), than just a relation of inclusion
13http://www.linguateca.pt/HAREM/
135
tout court. In fact, in the next sentence, a kind of
specialization relation can be uncovered.
Astro?nomos brasileiros esperam fotogra-
far os primeiros planetas fora do Sistema
Solar com a ajuda do maior telesco?pio
do mundo, o Gemini (...) os telesco?pios
Gemini te?m capacidade cient??fica...14
Likewise, an inclusion relation held between
PESSOA-GRUPOCARGO (a group of roles performed
by people) and PESSOA-INDIVIDUAL (an individ-
ual person) , as in the following example, is more
informative than a simple relation of inclusion be-
tween NEs, or even inclusion between PESSOA enti-
ties without further discrimination.
Po?ttering, So?crates e Barroso assinam
Carta dos Direitos Fundamentais da UE.
Depois de a Carta ser assinada pelos
Presidentes das tre?s instituic?o?es, ouviu-se
o hino europeu...15
Furthermore, this relation is also different from
an inclusion relation held between PESSOA-
INDIVIDUAL (an individual) and PESSOA-
GRUPOMEMBRO (a group of people):
Lobos recebidos em apoteose. (...) o
capita?o Vasco Uva explicou por que houve
uma empatia ta?o grande entre... 16
Conversely, the specification of relations between
different NEs in a text may help in detecting and jus-
tifying different facets of a particular NE, i.e., mul-
tiple semantic categories that should be assigned to
it.
This illustrates the often observed case that it may
be easier for a human annotator to decide and choose
a specific issue than a too general one, and that there-
fore categories or choices should be more dependent
on ease of human interpretation than quantitative
factors (such as few categories or balanced ones).
14Brazilian astronomers expect to take the first pictures of
planets beyond the solar system with the help of the largest
telescope in the world, Gemini (...) Gemini telescopes have
a capacity...
15Po?ttering, So?crates e Barroso sign the declaration... After
being signed by the Presidents of the three institutions, ...
16Lobos received apoteothically. (...) Captain Vasco Uva
explained why ...
For future work, we obviously intend to increase
the size of the annotated collection (to the whole
HAREM collection and even beyond), and investi-
gate a couple of issues that interest us: which strate-
gies are used to avoid repetition of proper names
and establish textual cohesion? How do relations
between noun phrases in general compare with re-
lations between entities?
We would also like to investigate closer rela-
tionships between different relations: for exam-
ple, is it more appropriate to also develop a hi-
erarchy of relations, reconsidering, for example,
affiliation (currently one of the other) as a
kind of inclusion?
In order to understand better what this task is
about, we would also like to investigate whether
there are interesting correlations between NE cate-
gories and relations, as well as text genre and this
sort of connectivity. Even though we only studied
and annotated in depth 12 different texts, it was at
once obvious that they had quite different properties
as far as the number and kinds of relations was con-
cerned.
From an evaluation point of view, we would like
to improve our inconsistency detection programs
and be able to reason about possible contradictions
(of the annotation or of the interpretation) as well
as experiment with different weights and evaluation
measures, taking into account criteria such as pre-
dictability of relationships between NEs.
In any case, we believe this was an important first
step to understand a number of issues and to reflect
about what computational systems should be doing
to harvest semantic knowledge. We would like to
receive feedback on whether the task design seems
sound to the rest of the community, and whether sys-
tems which would perform well in such task could
be put to good use in real world applications.
Acknowledgments
This work was done in the scope of the Linguateca
project, jointly funded by the Portuguese Govern-
ment and the European Union (FEDER and FSE)
under contract ref. POSC/339/1.3/C/NAC.
136
References
Eugene Agichtein and Luis Gravano. 2000. Snowball:
Extracting Relations from Large Plain-Text Collec-
tions. In Proc. of the 5th ACM International Con-
ference on Digital Libraries (ACM DL), pages 85?94,
San Antonio, Texas, USA, June, 2-7.
M??rian Bruckschen, Jose? Guilherme Camargo de Souza,
Renata Vieira, and Sandro Rigo. 2008. Sistema
SeRELeP para o reconhecimento de relac?o?es entre en-
tidades mencionadas. In Mota and Santos (Mota and
Santos, 2008).
Nuno Cardoso. 2008. REMBRANDT - Reconhecimento
de Entidades Mencionadas Baseado em Relac?o?es e
ANa?lise Detalhada do Texto. In Mota and Santos
(Mota and Santos, 2008).
Marc??rio Chaves. 2008. Geo-ontologias e padro?es para
reconhecimento de locais e de suas relac?o?es em textos:
o SEI-Geo no Segundo HAREM. In Mota and Santos
(Mota and Santos, 2008).
Sandra Collovini, Thiago Ianez Carbonel, Ju-
liana Thiesen Fuchs, Jorge Ce?sar Coelho, Lucia
Helena Machado Rino, and Renata Vieira. 2007.
Summ-it: Um corpus anotado com informac?o?es
discursivas visando a` sumarizac?a?o automa?tica. In
Anais do XXVII Congresso da SBC: V Workshop em
Tecnologia da Informac?a?o e da Linguagem Humana
? TIL, pages 1605?1614, Rio de Janeiro, RJ, Brazil,
junho/julho.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
tree kernels for relation extraction. In Proceedings of
the 42rd Annual Meeting of the Association for Com-
putational Linguistics (ACL?04), pages 423?429. As-
sociation for Computational Linguistics, July.
Jose? Guilherme Camargo de Souza, Patr??cia Nunes
Gonc?alves, and Renata Vieira. 2008. Learning coref-
erence resolution for portuguese texts. In Anto?nio
Teixeira, Vera Lu?cia Strube de Lima, Lu??s Caldas
de Oliveira, and Paulo Quaresma, editors, PROPOR,
volume 5190 of Lecture Notes in Computer Science,
pages 153?162. Springer.
Georde Doddington, Alexis Mitchell, Mark Przybocki,
Lance Ramshaw, Stephanie Strassel, and Ralph
Weischedel. 2004. The automatic content extraction
(ace) programm. tasks, data and evaluation. In Pro-
ceedings of the Fourth International Conference on
Language Resources and Evaluation, pages 837?840,
Lisbon, Portugal.
Cla?udia Freitas, Diana Santos, Hugo Gonc?alo Oliveira,
Paula Carvalho, and Cristina Mota. 2008. Relac?o?es
sema?nticas do ReRelEM: ale?m das entidades no Se-
gundo HAREM. In Mota and Santos (Mota and San-
tos, 2008).
Ruslan Mitkov. 2000. Towards a more consistent and
comprehensive evaluation of anaphora resolution al-
gorithms and systems. In Proceedings of the Dis-
course Anaphora and Anaphora Resolution Collo-
quium (DAARC-2000), pages 96?107, Lancaster, UK.
Cristina Mota and Diana Santos, editors. 2008. Desafios
no reconhecimento de entidades mencionadas: O Se-
gundo HAREM. Linguateca.
NIST and ACE. 2007. Automatic Content Extrac-
tion 2008 Evaluation Plan (ACE08) ? Assessment of
Detection and Recognition of Entities and Relations
within and across Documents. Technical report, NIST.
Constantin Ora?san, Dan Cristea, Ruslan Mitkov, and An-
tonio Branco. 2008. Anaphora resolution exercise:
An overview. In Proceedings of the Sixth International
Language Resources and Evaluation (LREC?08), Mar-
rakech, Morocco, May, 28 - 30.
Dan Roth and Wen tau Yih. 2004. A linear programming
formulation for global inference in natural language
tasks. In Proceedings of CoNLL-2004, pages 1?8.
Diana Santos and Nuno Cardoso, editors. 2007. Re-
conhecimento de entidades mencionadas em por-
tugue?s: Documentac?a?o e actas do HAREM, a primeira
avaliac?a?o conjunta na a?rea. Linguateca, Portugal.
Diana Santos, Cla?udia Freitas, Hugo Gonc?alo Oliveira,
and Paula Carvalho. 2008. Second HAREM: new
challenges and old wisdom. In Anto?nio Teixeira, Vera
Lu?cia Strube de Lima, Lu??s Caldas de Oliveira, and
Paulo Quaresma, editors, Computational Processing
of the Portuguese Language, 8th International Con-
ference, Proceedings (PROPOR 2008), volume LNAI
5190, pages 212?215. Springer Verlag.
Diana Santos. 2006. What is natural language? Dif-
ferences compared to artificial languages, and conse-
quences for natural language processing, 15 May. In-
vited lecture, SBLP2006 and PROPOR?2006.
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In Proceed-
ings of the Sixth Message Understanding Conference
(MUC-6), pages 45?52. Morgan Kaufmann.
Bonnie Lynn Webber. 1978. A formal approach to dis-
course anaphora. Outstanding dissertations in linguis-
tics. Garland Publishing, New York, NY, USA.
Shubin Zhao and Ralph Grishman. 2005. Extracting re-
lations with integrated information using kernel meth-
ods. In Proceedings of the 43rd Annual Meeting on As-
sociation for Computational Linguistics (ACL 2005),
pages 419?426, Morristown, NJ, USA. Association for
Computational Linguistics.
137
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 353?356,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
Updating a Name Tagger Using Contemporary Unlabeled Data
Cristina Mota
L2F (INESC-ID) & IST & NYU
Rua Alves Redol 9
1000-029 Lisboa Portugal
cmota@ist.utl.pt
Ralph Grishman
New York University
Computer Science Department
New York NY 10003 USA
grishman@cs.nyu.edu
Abstract
For many NLP tasks, including named en-
tity tagging, semi-supervised learning has
been proposed as a reasonable alternative
to methods that require annotating large
amounts of training data. In this paper,
we address the problem of analyzing new
data given a semi-supervised NE tagger
trained on data from an earlier time pe-
riod. We will show that updating the unla-
beled data is sufficient to maintain quality
over time, and outperforms updating the
labeled data. Furthermore, we will also
show that augmenting the unlabeled data
with older data in most cases does not re-
sult in better performance than simply us-
ing a smaller amount of current unlabeled
data.
1 Introduction
Brill (2003) observed large gains in performance
for different NLP tasks solely by increasing the
size of unlabeled data, but stressed that for other
NLP tasks, such as named entity recognition
(NER), we still need to focus on developing tools
that help to increase the size of annotated data.
This problem is particularly crucial when pro-
cessing languages, such as Portuguese, for which
the labeled data is scarce. For instance, in the first
NER evaluation for Portuguese, HAREM (San-
tos and Cardoso, 2007), only two out of the nine
participants presented systems based on machine
learning, and they both argued they could have
achieved significantly better results if they had
larger training sets.
Semi-supervised methods are commonly cho-
sen as an alternative to overcome the lack of an-
notated resources, because they present a good
trade-off between amount of labeled data needed
and performance achieved. Co-training is one of
those methods, and has been extensively studied in
NLP (Nigam and Ghani, 2000; Pierce and Cardie,
2001; Ng and Cardie, 2003; Mota and Grishman,
2008). In particular, we showed that the perfor-
mance of a name tagger based on co-training de-
cays as the time gap between training data (seeds
and unlabeled data) and test data increases (Mota
and Grishman, 2008). Compared to the original
classifier of Collins and Singer (1999) that uses
seven seeds, we used substantially larger seed sets
(more than 1000), which raises the question of
which of the parameters (seeds or unlabeled data)
are causing the performance deterioration.
In the present study, we investigated two main
questions, from the point of view of a developer
who wants to analyze a new data set, given an NE
tagger trained with older data. First, we studied
whether it was better to update the seeds or the
unlabeled data; then, we analyzed whether using
a smaller amount of current unlabeled data could
be better than increasing the amount of unlabeled
data drawn from older sources. The experiments
show that using contemporary unlabeled data is
the best choice, outperforming most experiments
with larger amounts of older unlabeled data and
all experiments with contemporary seeds.
2 Contemporary labeled data in NLP
The speech community has been defending for
some time now the idea of having similar tem-
poral data for training and testing automatic
speech recognition systems for broadcast news.
Most works focus on improving out-of-vocabulary
(OOV) rates, to which new names contribute
significantly. For instance, Palmer and Osten-
dorf (2005) aiming at reducing the error rate due
to OOV names propose to generate offline name
lists from diverse sources, including temporally
relevant news texts; Federico and Bertoldi (2004),
and Martins et al (2006) propose to daily adapt
the statistical language model of a broadcast
353
news transcription system, exploiting contempo-
rary newswire texts available on the web; Auzanne
et al (2000) proposed a time-adaptive language
model, studying its impact over a period of five
months on the reduction of OOV rate, word error
rate and retrieval accuracy on a spoken document
retrieval system.
Concerning variations over longer periods of
time, we observed that the performance of a semi-
supervised name tagger decays over a period of
eight years, which seems to be directly related
with the fact that the texts used to train and test the
tagger also show a tendency to become less simi-
lar over time (Mota and Grishman, 2008); Batista
et al (2008) also observed a decaying tendency in
the performance of a system for recovering capi-
talization over a period of six years, proposing to
retrain a MaxEnt model using additional contem-
porary written texts.
3 Name tagger overview
We assessed the name tagger described in Mota
and Grishman (2008) to recognize names of peo-
ple, organizations and locations. The tagger is
based on the co-training NE classifier proposed
by Collins and Singer (1999), and is comprised
of several components organized sequentially (cf.
Figure 1).
!"#$%$"&$
'()*#%+,-./01$"&$2
3(4"5"6%'()*#
%!"&$%7)$8%/5(##)9"6%,-
!"&$%7)$8%:1/5(##)9"6%,-;6"1$)9/($)01
<5(##)9/($)01
'*0=(>($)01
?"($:*"%"&$*(/$)01
'()*#%+#="55)1>%@"($:*"#.%
/01$"&$:(5%@"($:*"#2
<0A$*()1)1>
B="55)1>%C%
/01$"&$:(5%*:5"#
B""6#%
D15(4"5"6%$"&$
!"#$%&'!()%&%&'
Figure 1: NE tagger architecture
4 Data sets
CETEMPu?blico (Rocha and Santos, 2000) is a
Portuguese journalistic corpus with 180 million
words that spans eight years of news, from 1991
to 1998. The minimum size of epoch (time span
of data set) available for analysis is a six-month
period, corresponding either to the first half of the
year or the second.
The data sets were created using the first 8256
extracts1 within each six-month period of the pol-
itics section of the corpus: the first 192 are used to
collect seeds, the next 208 extracts are used as test
sets and the remaining 7856 are used to collect the
unlabeled examples. The seeds correspond to the
first 1150 names occurring in those extracts. From
the list of unlabeled examples obtained after the
NE identification stage, only the first 41226 exam-
ples of each epoch were used to bootstrap in the
classification stage.
5 Experiments
We denote by S, U and T , respectively, the seed,
unlabeled and test texts, and by (S
i
, U
j
, T
k
) a
training-test configuration, where 91a ? i, j, k ?
98b, i.e., epochs i, j and k vary between the first
half of 1991 (91a) and the second half of 1998
(98b). For instance, the training-test configuration
(S
i=91a...98b
, U
i=91a...98b
, T
j=98b
) represents the
training-test configuration where the test set was
drawn from epoch 98b, and the tagger was trained
in turn with seeds and unlabeled data drawn from
the same epoch i that varied from 91a to 98b.
5.1 Do we need contemporary labeled data?
In order to understand whether it is better to label
examples falling within the epoch of the test set
or to keep using old labeled data while bootstrap-
ping with contemporary unlabeled data, we fixed
the test set to be within the last epoch of the inter-
val (98b), and performed backward experiments,
i.e., we varied the epoch of either the seeds or the
unlabeled data backwards. The choice of fixing
the test within the last epoch of the interval is the
one that most approximates a real situation where
one has a tagger trained with old data and wants to
process a more recent text.
Figure 2 shows the results for both experiments,
where (S
j=98b
, U
i=91a...98b
, T
j=98b
) represents the
experiment where the test was within the same
epoch as the seeds and the unlabeled data were
drawn from a single, variable, epoch in turn, and
(S
i=91a...98b
, U
j=98b
, T
j=98b
) represents the exper-
iment where the test was within the epoch of the
1Extracts are typically two paragraphs.
354
unlabeled data and the seeds were drawn in turn
from each of the epochs; the graphic also shows
the baseline backward training (varying the epoch
of both the seeds and the unlabeled data together).
0.
74
0.
76
0.
78
0.
80
0.
82
Training epoch
F?
m
ea
su
re
(i,i,98b)
(98b,i,98b)
(i,98b,98b)
91
a
91
b
92
a
92
b
93
a
93
b
94
a
94
b
95
a
95
b
96
a
96
b
97
a
97
b
98
a
98
b
Figure 2: F-measure over time for test set 98b with
configurations: (S
i=91a...98b
, U
i=91a...98b
, T
j=98b
),
(S
j=98b
, U
i=91a...98b
, T
j=98b
), and (S
i=91a...98b
,
U
j=98b
, T
j=98b
)
As can be seen, there is a small gain in perfor-
mance by using seeds within the epoch of the test
set, but the decay is still observable as we increase
the time gap between the unlabeled data and the
test set. On the contrary, if we use unlabeled data
within the epoch of the test set, we hardly see
a degradation trend as the time gap between the
epochs of seeds and test set is increased.
An examination of the results shows that, for
instance, Sendero Luminoso received the correct
classification of organization when the tagger is
trained with unlabeled data drawn from the same
epoch, but is incorretly classified as person when
trained with data that is not contemporary with the
test set. Even though that name is not a seed in any
of the cases, it occurs twice in good contexts for
organization in unlabeled data contemporary with
the test set (l??der do Sendero Luminoso/leader of
the Shining Path and acc?o?es do Sendero Lumi-
noso/actions of the Shining Path), while it does
not occur in the unlabeled data that is not contem-
porary. Given that both the name spelling and the
context in the test set, o messianismo do peruano
Sendero Luminoso/the messianism of the Peruvian
Shining Path, are insufficient to assign a correct la-
bel, the occurrence of the name in the contempo-
rary unlabeled data contributes to its correct clas-
sification in the test set.
5.2 Is more older unlabeled data better?
The second question we addressed was whether
having more older unlabeled data could result in
better performance than less data but within the
epoch of the test set. In this case, we conducted
two backward experiments, augmenting the un-
labeled data backwards with older data than the
test set (98b), starting in the previous epoch (98a):
in the first experiment, the seeds were within the
same epoch as the test set, and in the second ex-
periment the seeds were within the same epoch as
the unlabeled set being added. This corresponds to
configurations (S
j=98b
, U ?
i=91a...98a
, T
j=98b
) and
(S
i=91a...98a
, U ?
i=91a...98a
, T
j=98b
), respectively,
where U ?
i
=
?
98a
k=i
U
k
.
In Figure 3, we show the result of these con-
figurations together with the result of the back-
ward experiment corresponding to configuration
(S
i=91a...98b
, U
j=98b
, T
j=98b
), also represented in
Figure 2. We note that, in the case of the former
experiments, the size of the unlabeled examples is
increasing in the direction 98a to 91a.
0.
77
0.
78
0.
79
0.
80
0.
81
0.
82
0.
83
Training epoch
F?
m
ea
su
re
(i,98b,98b)
(i,u[i,...,98a],98b)
(98b,u[i,...,98a],98b)
91
a
91
b
92
a
92
b
93
a
93
b
94
a
94
b
95
a
95
b
96
a
96
b
97
a
97
b
98
a
98
b
Figure 3: F-measure for test set 98b with
configurations (S
i=91a...98b
, U
j=98b
, T
j=98b
),
(S
j=98b
, U ?
i=91a...98a
, T
j=98b
) and (S
i=91a...98a
,
U
?
i=91a...98a
, T
j=98b
), where U ?
i
=
?
98a
k=i
U
k
As can be observed, increasing the size of the
unlabeled data does not necessarily result in bet-
ter performance: for both choices of seeds, perfor-
mance sometimes improves, sometimes worsens,
as the unlabeled data grows (following the curves
355
from right to left).
Furthermore, the tagger trained with more unla-
beled data in most cases did not outperform the
tagger trained with less unlabeled data selected
from the epoch of the test set.
6 Discussion and future directions
We conducted experiments varying the epoch of
seeds and unlabeled data of a named entity tagger
based on co-training. We observed that the per-
formance decay resulting from increasing the time
gap between training data (seeds and unlabeled ex-
amples) and the test set can be slightly attenuated
by using the seeds contemporary with the test set.
The gain is larger if one uses older seeds and con-
temporary unlabeled data, a strategy that, in most
of the experiments, results in better performance
than using increasing sizes of older unlabeled data.
These results suggest that we may not need to
label new data nor train our tagger with increasing
sizes of data, as long as we are able to train it with
unlabeled data time compatible with the test set.
In the future, one issue that needs clarification is
why bootstraping from contemporary labeled data
had so little influence on the performance of co-
training, and if other semi-supervised approches
are also sensitive to this question.
Acknowledgment
The first author?s research work was funded by
Fundac?a?o para a Cie?ncia e a Tecnologia through a
doctoral scholarship (ref.: SFRH/BD/3237/2000).
References
Ce?dric Auzanne, John S. Garofolo, Jonathan G. Fiscus,
and William M. Fisher. 2000. Automatic language
model adaptation for spoken document retrieval. In
Proceedings of RIAO 2000 Conference on Content-
Based Multimedia Information Access.
Fernando Batista, Nuno Mamede, and Isabel Trancoso.
2008. Language dynamics and capitalization using
maximum entropy. In Proceedings of ACL-08: HLT,
Short Papers, pages 1?4, Columbus, Ohio, June. As-
sociation for Computational Linguistics.
Eric Brill. 2003. Processing natural language with-
out natural language processing. In CICLing, pages
360?369.
Michael Collins and Yoram Singer. 1999. Unsuper-
vised models for named entity classification. In
Proceedings of the Joint SIGDAT Conference on
EMNLP.
Marcello Federico and Nicola Bertoldi. 2004. Broad-
cast news lm adaptation over time. Computer
Speech & Language, 18(4):417?435.
Ciro Martins, Anto?nio Teixeira, and Joa?o Neto. 2006.
Dynamic vocabulary adaptation for a daily and
real-time broadcast news transcription system. In
IEEE/ACL Workshop on Spoken Language Technol-
ogy, Aruba.
Cristina Mota and Ralph Grishman. 2008. Is this NE
tagger getting old? In Proceedings of the Sixth
International Language Resources and Evaluation
(LREC?08), Marrakech, Morocco, may.
Vincente Ng and Claire Cardie. 2003. Weakly super-
vised natural language learning without redundant
views. In NAACL?03: Proceedings of the 2003 Con-
ference of the North American Chapter of the As-
sociation for Computational Linguistics on Human
Language Technology, pages 94?101, Morristown,
NJ, USA. ACL.
Kamal Nigam and Rayid Ghani. 2000. Analyzing
the effectiveness and applicability of co-training. In
Proceedings of CIKM, pages 86?93.
David D. Palmer and Mari Ostendorf. 2005. Improv-
ing out-of-vocabulary name resolution. Computer
Speech & Language, 19(1):107?128.
David Pierce and Claire Cardie. 2001. Limitations of
co-training for natural language learning from large
datasets. In Proceedings of the 2001 Conference on
Empirical Methods in Natural Language Processing
(EMNLP-2001).
Paulo Rocha and Diana Santos. 2000. Cetempu?blico:
Um corpus de grandes dimenso?es de linguagem
jornal??stica portuguesa. In Maria das Grac?as
Volpe Nunes, editor, Actas do V Encontro para o
processamento computacional da l??ngua portuguesa
escrita e falada PROPOR 2000, pages 131?140, At-
ibaia, Sa?o Paulo, Brasil.
Diana Santos and Nuno Cardoso, editors. 2007. Re-
conhecimento de entidades mencionadas em por-
tugue?s: Documentac?a?o e actas do HAREM, a
primeira avaliac?a?o conjunta na a?rea. Linguateca,
12 de Novembro.
356
