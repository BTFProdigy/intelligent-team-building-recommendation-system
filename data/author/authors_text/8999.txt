Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 157?166,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Stacking Dependency Parsers
Andre? F. T. Martins?? Dipanjan Das? Noah A. Smith? Eric P. Xing?
?School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, USA
?Instituto de Telecomunicac?o?es, Instituto Superior Te?cnico, Lisboa, Portugal
{afm,dipanjan,nasmith,epxing}@cs.cmu.edu
Abstract
We explore a stacked framework for learn-
ing to predict dependency structures for natu-
ral language sentences. A typical approach in
graph-based dependency parsing has been to
assume a factorized model, where local fea-
tures are used but a global function is opti-
mized (McDonald et al, 2005b). Recently
Nivre and McDonald (2008) used the output
of one dependency parser to provide features
for another. We show that this is an example
of stacked learning, in which a second pre-
dictor is trained to improve the performance
of the first. Further, we argue that this tech-
nique is a novel way of approximating rich
non-local features in the second parser, with-
out sacrificing efficient, model-optimal pre-
diction. Experiments on twelve languages
show that stacking transition-based and graph-
based parsers improves performance over ex-
isting state-of-the-art dependency parsers.
1 Introduction
In this paper we address a representation-efficiency
tradeoff in statistical natural language processing
through the use of stacked learning (Wolpert,
1992). This tradeoff is exemplified in dependency
parsing, illustrated in Fig. 1, on which we focus in
this paper:
? Exact algorithms for dependency parsing (Eis-
ner and Satta, 1999; McDonald et al, 2005b)
are tractable only when the model makes very
strong, linguistically unsupportable independence
assumptions, such as ?arc factorization? for non-
projective dependency parsing (McDonald and
Satta, 2007).
? Feature-rich parsers must resort to search or
greediness, (Ratnaparkhi et al, 1994; Sagae and
Lavie, 2005; Hall et al, 2006), so that parsing
solutions are inexact and learned models may be
subject to certain kinds of bias (Lafferty et al,
2001).
A solution that leverages the complementary
strengths of these two approaches?described in de-
tail by McDonald and Nivre (2007)?was recently
and successfully explored by Nivre and McDonald
(2008). Our contribution begins by reinterpreting
and generalizing their parser combination scheme as
a stacking of parsers.
We give a new theoretical motivation for stacking
parsers, in terms of extending a parsing model?s fea-
ture space. Specifically, we view stacked learning as
a way of approximating non-local features in a lin-
ear model, rather than making empirically dubious
independence (McDonald et al, 2005b) or structural
assumptions (e.g., projectivity, Eisner, 1996), using
search approximations (Sagae and Lavie, 2005; Hall
et al, 2006; McDonald and Pereira, 2006), solving a
(generally NP-hard) integer linear program (Riedel
and Clarke, 2006), or adding latent variables (Titov
and Henderson, 2007). Notably, we introduce the
use of very rich non-local approximate features in
one parser, through the output of another parser.
Related approaches are the belief propagation algo-
rithm of Smith and Eisner (2008), and the ?trading
of structure for features? explored by Liang et al
157
Figure 1: A projective dependency graph.
Figure 2: Non-projective dependency graph.
those that assume each dependency decision is in-
dependent modulo the global structural constraint
that dependency graphs must be trees. Such mod-
els are commonly referred to as edge-factored since
their parameters factor relative to individual edges
of the graph (Paskin, 2001; McDonald et al,
2005a). Edge-factored models have many computa-
tional benefits, most notably that inference for non-
projective dependency graphs can be achieved in
polynomial time (McDonald et al, 2005b). The pri-
mary problem in treating each dependency as in-
dependent is that it is not a realistic assumption.
Non-local information, such as arity (or valency)
and neighbouring dependencies, can be crucial to
obtaining high parsing accuracies (Klein and Man-
ning, 2002; McDonald and Pereira, 2006). How-
ever, in the data-driven parsing setting this can be
partially adverted by incorporating rich feature rep-
resentations over the input (McDonald et al, 2005a).
The goal of this work is to further our current
understanding of the computational nature of non-
projective parsing algorithms for both learning and
inference within the data-driven setting. We start by
investigating and extending the edge-factored model
of McDonald et al (2005b). In particular, we ap-
peal to the Matrix Tree Theorem for multi-digraphs
to design polynomial-time algorithms for calculat-
ing both the partition function and edge expecta-
tions over all possible dependency graphs for a given
sentence. To motivate these algorithms, we show
that they can be used in many important learning
and inference problems including min-risk decod-
ing, training globally normalized log-linear mod-
els, syntactic language modeling, and unsupervised
learning via the EM algorithm ? none of which have
previously been known to have exact non-projective
implementations.
We then switch focus to models that account for
non-local information, in particular arity and neigh-
bouring parse decisions. For systems that model ar-
ity constraints we give a reduction from the Hamilto-
nian graph problem suggesting that the parsing prob-
lem is intractable in this case. For neighbouring
parse decisions, we extend the work of McDonald
and Pereira (2006) and show that modeling vertical
neighbourhoods makes parsing intractable in addi-
tion to modeling horizontal neighbourhoods. A con-
sequence of these results is that it is unlikely that
exact non-projective dependency parsing is tractable
for any model assumptions weaker than those made
by the edge-factored models.
1.1 Related Work
There has been extensive work on data-driven de-
pendency parsing for both projective parsing (Eis-
ner, 1996; Paskin, 2001; Yamada and Matsumoto,
2003; Nivre and Scholz, 2004; McDonald et al,
2005a) and non-projective parsing systems (Nivre
and Nilsson, 2005; Hall and No?va?k, 2005; McDon-
ald et al, 2005b). These approaches can often be
classified into two broad categories. In the first cat-
egory are those methods that employ approximate
inference, typically through the use of linear time
shift-reduce parsing algorithms (Yamada and Mat-
sumoto, 2003; Nivre and Scholz, 2004; Nivre and
Nilsson, 2005). In the second category are those
that employ exhaustive inference algorithms, usu-
ally by making strong independence assumptions, as
is the case for edge-factored models (Paskin, 2001;
McDonald et al, 2005a; McDonald et al, 2005b).
Recently there have also been proposals for exhaus-
tive methods that weaken the edge-factored assump-
tion, including both approximate methods (McDon-
ald and Pereira, 2006) and exact methods through in-
teger linear programming (Riedel and Clarke, 2006)
or branch-and-bound algorithms (Hirakawa, 2006).
For grammar based models there has been limited
work on empirical systems for non-projective pars-
ing systems, notable exceptions include the work
of Wang and Harper (2004). Theoretical studies of
note include the work of Neuhaus and Bo?ker (1997)
showing that the recognition problem for a mini-
$ Figure 1: A projective dependency graph.
Figure 2: Non-projective dependency graph.
those that assume each dependency decision is in-
dependent modulo the global structural constraint
that dependency graphs must be trees. Such mod-
els are commonly referred to as edge-factored since
their parameters factor relative to individual edges
of the graph (Paskin, 2001; McDonald et al,
2005a). Edge-factored models have many computa-
tional benefits, most notably that inference for non-
projective dependency graphs can be achieved in
polynomial time (McDonald et al, 2005b). The pri-
mary problem in treating each dependency as in-
dependent is that it is not a realistic assumption.
Non-local information, such as arity (or valency)
and eighbouring dependencies, can be crucial to
obtaining high parsing accuracies (Klein and Man-
ning, 2002; McDonald and P reira, 2006). How-
ever, i the data-dr ven parsing etting this can be
partially adverted by incorporating rich feature rep-
resentations over the input (McDonald et al, 2005a).
The goal of this work is to further our curr nt
understanding of the computational nature of non-
projective parsing algorithms for both learning and
inference within the data-driven setting. We start by
investigating and extending the edge-factored model
of McDonald et al (2005b). In particular, we ap-
peal to the Matrix Tree Theorem for multi-digraphs
to design polynomial-time algorithms for calculat-
ing both the partition function and edge expecta-
tions over all possible dependency graphs for a given
sentence. To motivate these algorithms, we show
that they can be used in many important learning
and inference problems including min-risk decod-
ing, training globally normalized log-linear mod-
els, syntactic language modeling, and unsupervised
learning via the EM algorithm ? none of which have
previously been known t have exact n n-projective
implementations.
We th switch focus to models that account for
non-local information, in particular arity and neigh-
bouring parse decisions. For systems that model ar-
ity constraints we give a reduction from the Hamilto-
nian graph problem suggesting that the parsing prob-
lem is intractable in this case. For neighbouring
parse decisions, we extend the work of McDonald
and Pereira (2006) and show that modeling vertical
neighbourhoods makes parsing intractable in addi-
tion to modeling horizontal neighbourhoods. A con-
sequence of these results is that it is unlikely that
exact non-projective dependency parsing is tractable
for any model assumptions weaker than those made
by the edge-factored models.
1.1 Related W rk
There has been extensive work on data-driven de-
pendency parsing for both projective parsing (Eis-
ner, 1996; Paskin, 2001; Yamada and Matsumoto,
2003; Nivre and Scholz, 2004; McDonald et al,
2005a) and non-projective parsing systems (Nivre
and Nilsson, 2005; Hall and No?va?k, 2005; McDon-
ald et al, 2005b). These approaches can often be
classified into two broad categories. In the first cat-
egory are those methods that employ approximate
inference, typically through the use of linear time
shift-reduce parsing algorit ms (Yamad and Mat-
sumoto, 2003; Nivre and Scholz, 2004; Nivre and
Nilsson, 2005). I the second category are those
that employ exhaustive inference algorithms, usu-
ally by making strong independence assumptions, a
is the case for edge-factored mod ls (Paskin, 2001;
McDonald et al, 2005a; McDonald et al, 2005b).
Recently there have also been proposals for exhaus-
tive methods that weaken the edge-factored assump-
tion, including both approximate methods (McDon-
ald and Pereira, 2006) and exact methods through in-
teger linear programming (Riedel and Clarke, 2006)
or branch-and-bound algorithms (Hirakawa, 2006).
For grammar based models there has been limited
work on empirical systems for non-projective pars-
ing systems, notable exceptions include the work
of Wang and Harper (2004). Theoretical studies of
note include the work of Neuhaus and Bo?ker (1997)
showing that the recognition problem for a mini-
$
Figure 1: A projective dependency parse (top), and a non-
projective dependency parse (bottom) for two English
sentences; examples from McDonald and Satta (2007).
(2008).
This paper focuses on dependency parsing, which
has become widely used in relation extraction (Cu-
lotta and Sorensen, 2004), machine translation
(Ding and Palmer, 2005), question answering (Wang
et al, 2007), and many other NLP applications.
We show that stacking methods outperform the ap-
proximate ?second-order? parser of McDonald and
Pereira (2006) on twelve languages and can be used
within that approximatio to chieve even better re-
sults. These results are similar in sp rit t (Nivre and
McDonald, 008), but with the following novel con-
tributio s:
? a stacking interpretation,
? a ric r fe tu e set that includes non-l c l f atures
(shown here to improve erform nc ), and
? a variety of stacking architectures.
Using stacking with rich features, we obtain results
comp titive ith Nivre an McDonald (2008) while
preserving the fast qua ratic parsing ime of arc-
factored spanning tree algorithms.
The paper is organiz d as follows. We discuss re-
lated prior work on dependency parsing and stacking
in ?2. Our model is given in ?3. A novel analysis of
stacking in linear models is given in ?4. Experiments
are presented in ?5.
2 Background and Related Work
We briefly review work on the NLP task of depen-
dency parsing and the machine learning framework
known as stacked learning.
2.1 Dep dency Parsing
Dependency syntax is a lightweight syntactic rep-
resentation that models a sentence as a graph where
the words are vertices and syntactic relationships are
directed edges (arcs) connecting heads to their argu-
ments and modifiers.
Dependency parsing is often viewed computa-
tionally as a structured prediction problem: for each
input sentence x, with n words, exponentially many
candidate depend ncy trees y ? Y(x) are possible in
principle. We den te each tree by its set of vertices
and directed arcs, y = (Vy, Ay). A legal depen-
dency tree has n+ 1 verti es, each corresponding to
one word plus a ?wall? symbol, $, assumed to be the
hidden root of the sentence. In a valid dependency
tree, each vertex except the root has exactly one par-
ent. In the projective case, arcs cannot cross when
depicted on one side of the sentence; in the non-
projective cas , this constraint is not impos d (see
Fig. 1).
2.1.1 Graph-based vs. transition-based models
Most recent work on dependency parsing can be
categorized as graph-based or transition-based. In
graph-based parsing, dependency trees are scored
by factoring the tree into its arcs, and parsing is
perfo med by searching for the highest scoring tree
(Eis er 1996; McD nald et al, 2005b). Transition-
based parsers model the sequ nce of ecisions of
shift-reduce parser, given previous deci ions and
current state, and par ing is perfor ed by greedily
choosing the highest scori g transition out of each
successive parsing state or by searching for the best
sequence of transitions (Ratnaparkhi et al, 1994;
Yamada and Matsumoto, 2003; Nivre et al, 2004;
Sagae and Lavie, 2005; Hall et al, 2006).
Both approaches most commonly use linear mod-
els to assign scores to arcs or decisions, so that a
score is a dot-product of a feature vector f and a
learned weight vector w.
In sum, these two lines of researc use different
approximations to achieve trac ability. Transition-
based approaches solve sequence of local prob-
lems in sequ nce, sacrificing global opti ality guar-
antees and possibly expressive power (Abney et al,
1999). Graph-based methods perform global in-
ference using score factorizations that correspond
to strong independence assumptions (discussed in
158
?2.1.2). Recently, Nivre and McDonald (2008) pro-
posed combining a graph-based and a transition-
based parser and have shown a significant improve-
ment for several languages by letting one of the
parsers ?guide? the other. Our stacked formalism
(to be described in ?3) generalizes this approach.
2.1.2 Arc factorization
In the successful graph-based method of McDon-
ald et al (2005b), an arc factorization independence
assumption is used to ensure tractability. This as-
sumption forbids any feature that depends on two
or more arcs, permitting only ?arc-factored? features
(i.e. features that depend only on a single candidate
arc a ? Ay and on the input sequence x). This in-
duces a decomposition of the feature vector f(x, y)
as:
f(x, y) =
?
a?Ay
fa(x).
Parsing amounts to solving arg maxy?Y(x)
w>f(x, y), where w is a weight vector. With
a projectivity constraint and arc factorization, the
parsing problem can be solved in cubic time by
dynamic programming (Eisner, 1996), and with a
weaker ?tree? constraint (permitting nonprojective
parses) and arc factorization, a quadratic-time
algorithm exists (Chu and Liu, 1965; Edmonds,
1967), as shown by McDonald et al (2005b). In
the projective case, the arc-factored assumption can
be weakened in certain ways while maintaining
polynomial parser runtime (Eisner and Satta, 1999),
but not in the nonprojective case (McDonald and
Satta, 2007), where finding the highest-scoring tree
becomes NP-hard.
McDonald and Pereira (2006) adopted an approx-
imation based on O(n3) projective parsing followed
by rearrangement to permit crossing arcs, achieving
higher performance. In ?3 we adopt a framework
that maintains O(n2) runtime (still exploiting the
Chu-Liu-Edmonds algorithm) while approximating
non arc-factored features.
2.2 Stacked Learning
Stacked generalization was first proposed by
Wolpert (1992) and Breiman (1996) for regression.
The idea is to include two ?levels? of predictors. The
first level, ?level 0,? includes one or more predictors
g1, . . . , gK : Rd ? R; each receives input x ? Rd
and outputs a prediction gk(x). The second level,
?level 1,? consists of a single function h : Rd+K ?
R that takes as input ?x, g1(x), . . . gK(x)? and out-
puts a final prediction y? = h(x, g1(x), . . . gK(x)).
The predictor, then, combines an ensemble (the gk)
with a meta-predictor (h).
Training is done as follows: the training data are
split into L partitions, and L instances of the level
0 predictor are trained in a ?leave-one-out? basis.
Then, an augmented dataset is formed by letting
each instance output predictions for the partition that
was left out. Finally, each level 0 predictor is trained
using the original dataset, and the level 1 predictor
is trained on the augmented dataset, simulating the
test-time setting when h is applied to a new instance
x concatenated with ?gk(x)?k.
This framework has also been applied to classifi-
cation, for example with structured data. Some ap-
plications (including here) use only one classifier at
level 0; recent work includes sequence labeling (Co-
hen and de Carvalho, 2005) and inference in condi-
tional random fields (Kou and Cohen, 2007). Stack-
ing is also intuitively related to transformation-based
learning (Brill, 1993).
3 Stacked Dependency Parsing
We next describe how to use stacked learning for
efficient, rich-featured dependency parsing.
3.1 Architecture
The architecture consists of two levels. At level 0
we include a single dependency parser. At runtime,
this ?level 0 parser? g processes an input sentence x
and outputs the set of predicted edges that make up
its estimation of the dependency tree, y?0 = g(x). At
level 1, we apply a dependency parser?in this work,
always a graph-based dependency parser?that uses
basic factored features plus new ones from the edges
predicted by the level 0 parser. The final parser pre-
dicts parse trees as h(x, g(x)), so that the total run-
time is additive in calculating h(?) and g(?).
The stacking framework is agnostic about the
form of g and h and the methods used to learn them
from data. In this work we use two well-known,
publicly available dependency parsers, MSTParser
(McDonald et al, 2005b),1 which implements ex-
1http://sourceforge.net/projects/mstparser
159
act first-order arc-factored nonprojective parsing
(?2.1.2) and approximate second-order nonprojec-
tive parsing, and MaltParser (Nivre et al, 2006),
which is a state-of-the-art transition-based parser.2
We do not alter the training algorithms used in prior
work for learning these two parsers from data. Us-
ing the existing parsers as starting points, we will
combine them in a variety of ways.
3.2 Training
Regardless of our choices for the specific parsers and
learning algorithms at level 0 and level 1, training is
done as sketched in ?2.2. Let D be a set of training
examples {?xi, yi?}i.
1. Split training data D into L partitions
D1, . . . ,DL.
2. Train L instances of the level 0 parser in the fol-
lowing way: the l-th instance, gl, is trained on
D?l = D \ Dl. Then use gl to output predic-
tions for the (unseen) partition Dl. At the end,
an augmented dataset D? =
?L
l=1 D?
l is built, so
that D? = {?xi, g(xi), yi?}i.
3. Train the level 0 parser g on the original training
data D.
4. Train the level 1 parser h on the augmented train-
ing data D?.
The runtime of this algorithm is O(LT0+T1), where
T0 and T1 are the individual runtimes required for
training level 0 and level 1 alone, respectively.
4 Two Views of Stacked Parsing
We next describe two motivations for stacking
parsers: as a way of augmenting the features of a
graph-based dependency parser or as a way to ap-
proximate higher-order models.
4.1 Adding Input Features
Suppose that the level 1 classifier is an arc-factored
graph-based parser. The feature vectors will take the
form3
f(x, y) = f1(x, y) ^ f2(x, y?0, y)
=
?
a?Ay
f1,a(x) ^ f2,a(x, g(x)),
2http://w3.msi.vxu.se/?jha/maltparser
3We use^ to denote vector concatenation.
where f1(x, y) =
?
a?Ay f1,a(x) are regu-
lar arc-factored features, and f2(x, y?0, y) =?
a?Ay f2,a(x, g(x)) are the stacked features. An
example of a stacked feature is a binary feature
f2,a(x, g(x)) that fires if and only if the arc a was
predicted by g, i.e., if a ? Ag(x); such a feature was
used by Nivre and McDonald (2008).
It is difficult in general to decide whether the in-
clusion of such a feature yields a better parser, since
features strongly correlate with each other. How-
ever, a popular heuristic for feature selection con-
sists of measuring the information gain provided by
each individual feature. In this case, we may obtain
a closed-form expression for the information gain
that f2,a(x, g(x)) provides about the existence or not
of the arc a in the actual dependency tree y. Let A
and A? be binary random variables associated with
the events a ? Ay and a? ? Ag(x), respectively. We
have:
I(A;A?) =
?
a,a??{0,1}
p(a, a?) log2
p(a, a?)
p(a)p(a?)
= H(A?)?
?
a?{0,1}
p(a)H(A?|A = a).
Assuming, for simplicity, that at level 0 the prob-
ability of false positives equals the probability of
false negatives (i.e., Perr , p(a? = 0|a = 1) =
p(a? = 1|a = 0)), and that the probability of
true positives equals the probability of true negatives
(1 ? Perr = p(a? = 0|a = 0) = p(a? = 1|a = 1)),
the expression above reduces to:
I(A;A?) = H(A?) + Perr log2 Perr
+ (1? Perr) log2(1? Perr)
= H(A?)?Herr,
where Herr denotes the entropy of the probability of
error on each arc?s prediction by the level 0 classi-
fier. If Perr ? 0.5 (i.e. if the level 0 classifier is
better than random), then the information gain pro-
vided by this simple stacked feature increases with
(a) the accuracy of the level 0 classifier, and (b) the
entropy H(A?) of the distribution associated with its
arc predictions.
4.2 Approximating Non-factored Features
Another way of interpreting the stacking framework
is as a means to approximate a higher order model,
160
such as one that is not arc-factored, by using stacked
features that make use of the predicted structure
around a candidate arc. Consider a second-order
model where the features decompose by arc and by
arc pair:
f(x, y) =
?
a1?Ay
?
?fa1(x) ^
?
a2?Ay
fa1,a2(x)
?
? .
Exact parsing under such model, with arbitrary
second-order features, is intractable (McDonald and
Satta, 2007). Let us now consider a stacked model
in which the level 0 predictor outputs a parse y?. At
level 1, we use arc-factored features that may be
written as
f?(x, y) =
?
a1?Ay
?
?fa1(x) ^
?
a2?Ay?
fa1,a2(x)
?
? ;
this model differs from the previous one only by re-
placing Ay by Ay? in the index set of the second sum-
mation. Since y? is given, this makes the latter model
arc-factored, and therefore, tractable. We can now
view f?(x, y) as an approximation of f(x, y); indeed,
we can bound the score approximation error,
?s(x, y) =
?
?
?w?>f?(x, y)?w>f(x, y)
?
?
? ,
where w? and w stand respectively for the parameters
learned for the stacked model and those that would
be learned for the (intractable) exact second order
model. We can bound ?s(x, y) by spliting it into
two terms: ?s(x, y) =
?
?
?(w? ?w)>f?(x, y) + w>(f?(x, y)? f(x, y))
?
?
?
?
?
?
?(w? ?w)>f?(x, y)
?
?
?
? ?? ?
,?str(x,y)
+
?
?
?w>(f?(x, y)? f(x, y))
?
?
?
? ?? ?
,?sdec(x,y)
;
where we introduced the terms ?str and ?sdec that
reflect the portion of the score approximation error
that are due to training error (i.e., different parame-
terizations of the exact and approximate models) and
decoding error (same parameterizations, but differ-
ent feature vectors). Using Ho?lder?s inequality, the
former term can be bounded as:
?str(x, y) =
?
?
?(w? ?w)>f?(x, y)
?
?
?
? ?w? ?w?1 ? ?f?(x, y)??
? ?w? ?w?1 ;
where ?.?1 and ?.?? denote the `1-norm and sup-
norm, respectively, and the last inequality holds
when the features are binary (so that ?f?(x, y)?? ?
1). The proper way to bound the term ?w? ?w?1
depends on the training algorithm. As for the de-
coding error term, it can bounded for a given weight
vector w, sentence x, candidate tree y, and level 0
prediction y?. Decomposing the weighted vector as
w = w1 ^ w2, w2 being the sub-vector associ-
ated with the second-order features, we have respec-
tively: ?sdec(x, y) =
?
?
?w>(f?(x, y)? f(x, y))
?
?
?
=
?
?
?
?
?
?
?
a1?Ay
w>2
?
?
?
a2?Ay?
fa1,a2(x)?
?
a2?Ay
fa1,a2(x)
?
?
?
?
?
?
?
?
?
?
a1?Ay
?
a2?Ay??Ay
?
?
?w>2 fa1,a2(x)
?
?
?
?
?
a1?Ay
|Ay??Ay| ? max
a2?Ay??Ay
?
?
?w>2 fa1,a2(x)
?
?
?
=
?
a1?Ay
2L(y, y?) ? max
a2?Ay??Ay
?
?
?w>2 fa1,a2(x)
?
?
? ,
where Ay??Ay , (Ay? ?Ay) ? (Ay ?Ay?) denotes
the symmetric difference of the sets Ay? and Ay,
which has cardinality 2L(y, y?), i.e., twice the Ham-
ming distance between the sequences of heads that
characterize y and the predicted parse y?. Using
Ho?lder?s inequality, we have both
?
?
?w>2 fa1,a2(x)
?
?
? ? ?w2?1 ? ?fa1,a2(x)??
and
?
?
?w>2 fa1,a2(x)
?
?
? ? ?w2?? ? ?fa1,a2(x)?1.
Assuming that all features are binary valued, we
have that ?fa1,a2(x)?? ? 1 and that ?fa1,a2(x)?1 ?
Nf,2, where Nf,2 denotes the maximum number of
active second order features for any possible pair of
arcs (a1, a2). Therefore:
?sdec(x, y) ? 2nL(y, y?) min{?w2?1, Nf,2??w2??},
where n is the sentence length. Although this bound
can be loose, it suggests (intuitively) that the score
approximation degrades as the predicted tree y? gets
farther away from the true tree y (in Hamming dis-
tance). It also degrades with the magnitude of
weights associated with the second-order features,
161
Name Description
PredEdge Indicates whether the candidate edge
was present, and what was its label.
Sibling Lemma, POS, link label, distance and
direction of attachment of the previous
and and next predicted siblings
GrandParents Lemma, POS, link label, distance and
direction of attachment of the grandpar-
ent of the current modifier
PredHead Predicted head of the candidate modifier
(if PredEdge=0)
AllChildren Sequence of POS and link labels of all
the predicted children of the candidate
head
Table 1: Feature sets derived from the level 0 parser.
Subset Description
A PredEdge
B PredEdge+Sibling
C PredEdge+Sibling+GrandParents
D PredEdge+Sibling+GrandParents+PredHead
E PredEdge+Sibling+GrandParents+PredHead+
AllChildren
Table 2: Combinations of features enumerated in Table 1
used for stacking. A is a replication of (Nivre and Mc-
Donald, 2008), except for the modifications described in
footnote 4.
which suggests that a separate regularization of the
first-order and stacked features might be beneficial
in a stacking framework.
As a side note, if we set each component of
the weight vector to one, we obtain a bound
on the `1-norm of the feature vector difference,?
?
?f?(x, y)? f(x, y)
?
?
?
1
? 2nL(y, y?)Nf,2.
5 Experiments
In the following experiments we demonstrate the ef-
fectiveness of stacking parsers. As noted in ?3.1, we
make use of two component parsers, the graph-based
MSTParser and the transition-based MaltParser.
5.1 Implementation and Experimental Details
The publicly available version of MSTParser per-
forms parsing and labeling jointly. We adapted this
system to first perform unlabeled parsing, then la-
bel the arcs using a log-linear classifier with access
to the full unlabeled parse (McDonald et al, 2005a;
McDonald et al, 2005b; McDonald and Pereira,
2006). In stacking experiments, the arc labels from
the level 0 parser are also used as a feature.4
In the following subsections, we refer to our mod-
ification of the MSTParser as MST 1O (the arc-
factored version) and MST 2O (the second-order
arc-pair-factored version). All our experiments use
the non-projective version of this parser. We refer to
the MaltParser as Malt .
We report experiments on twelve languages from
the CoNLL-X shared task (Buchholz and Marsi,
2006).5 All experiments are evaluated using the
labeled attachment score (LAS), using the default
settings.6 Statistical significance is measured us-
ing Dan Bikel?s randomized parsing evaluation com-
parator with 10,000 iterations.7 The additional fea-
tures used in the level 1 parser are enumerated in
Table 1 and their various subsets are depicted in Ta-
ble 2. The PredEdge features are exactly the six fea-
tures used by Nivre and McDonald (2008) in their
MSTMalt parser; therefore, feature set A is a repli-
cation of this parser except for modifications noted
in footnote 4. In all our experiments, the number of
partitions used to create D? is L = 2.
5.2 Experiment: MST 2O + MST 2O
Our first experiment stacks the highly accurate
MST 2O parser with itself. At level 0, the parser
uses only the standard features (?5.1), and at level 1,
these are augmented by various subsets of features
of x along with the output of the level 0 parser, g(x)
(Table 2). The results are shown in Table 3. While
we see improvements over the single-parser baseline
4We made other modifications to MSTParser, implement-
ing many of the successes described by (McDonald et al,
2006). Our version of the code is publicly available at http:
//www.ark.cs.cmu.edu/MSTParserStacked. The
modifications included an approximation to lemmas for datasets
without lemmas (three-character prefixes), and replacing mor-
phology/word and morphology/lemma features with morphol-
ogy/POS features.
5The CoNLL-X shared task actually involves thirteen lan-
guages; our experiments do not include Czech (the largest
dataset), due to time constraints. Therefore, the average results
plotted in the last rows of Tables 3, 4, and 5 are not directly
comparable with previously published averages over thirteen
languages.
6http://nextens.uvt.nl/?conll/software.html
7http://www.cis.upenn.edu/?dbikel/software.
html
162
MST
2O
+MS
T 2O
, A
+MS
T 2O
, B
+MS
T 2O
, C
+MS
T 2O
, D
+MS
T 2O
, E
Arabic 67.88 66.91 67.41 67.68 67.37 68.02
Bulgarian 87.31 87.39 87.03 87.61 87.57 87.55
Chinese 87.57 87.16 87.24 87.48 87.42 87.48
Danish 85.27 85.39 85.61 85.57 85.43 85.57
Dutch 79.99 79.79 79.79 79.83 80.17 80.13
German 87.44 86.92 87.32 87.32 87.26 87.04
Japanese 90.93 91.41 91.21 91.35 91.11 91.19
Portuguese 87.12 87.26 86.88 87.02 87.04 86.98
Slovene 74.02 74.30 74.30 74.00 74.14 73.94
Spanish 82.43 82.17 82.35 82.81 82.53 82.75
Swedish 82.87 82.99 82.95 82.51 83.01 82.69
Turkish 60.11 59.47 59.25 59.47 59.45 59.31
Average 81.08 80.93 80.94 81.05 81.04 81.05
Table 3: Results of stacking MST 2O with itself at both level 0 and level 1. Column 2 enumerates LAS for MST 2O.
Columns 3?6 enumerate results for four different stacked feature subsets. Bold indicates best results for a particular
language.
for nine languages, the improvements are small (less
than 0.5%). One of the biggest concerns about this
model is the fact that it stacks two predictors that
are very similar in nature: both are graph-based and
share the features f1,a(x). It has been pointed out by
Breiman (1996), among others, that the success of
ensemble methods like stacked learning strongly de-
pends on how uncorrelated the individual decisions
made by each predictor are from the others? deci-
sions.8 This experiment provides further evidence
for the claim.
5.3 Experiment: Malt + MST 2O
We next use MaltParser at level 0 and the second-
order arc-pair-factored MST 2O at level 1. This
extends the experiments of Nivre and McDonald
(2008), replicated in our feature subset A.
Table 4 enumerates the results. Note that the
best-performing stacked configuration for each and
every language outperforms MST 2O, corroborat-
ing results reported by Nivre and McDonald (2008).
The best performing stacked configuration outper-
forms Malt as well, except for Japanese and Turk-
ish. Further, our non-arc-factored features largely
outperform subset A, except on Bulgarian, Chinese,
8This claim has a parallel in the cotraining method (Blum
and Mitchell, 1998), whose performance is bounded by the de-
gree of independence between the two feature sets.
and Japanese. On average, the best feature config-
uration is E, which is statistically significant over
Malt and MST 2O with p < 0.0001, and over fea-
ture subset A with p < 0.01.
5.4 Experiment: Malt + MST 1O
Finally, we consider stacking MaltParser with the
first-order, arc-factored MSTParser. We view this
approach as perhaps the most promising, since it is
an exact parsing method with the quadratic runtime
complexity of MST 1O.
Table 5 enumerates the results. For all twelve
languages, some stacked configuration outperforms
MST 1O and also, surprisingly, MST 2O, the sec-
ond order model. This provides empirical evi-
dence that using rich features from MaltParser at
level 0, a stacked level 1 first-order MSTParser can
outperform the second-order MSTParser.9 In only
two cases (Japanese and Turkish), the MaltParser
slightly outperforms the stacked parser.
On average, feature configuration D performs
the best, and is statistically significant over Malt ,
MST 1O, and MST 2O with p < 0.0001, and over
feature subset A with p < 0.05. Encouragingly, this
configuration is barely outperformed by configura-
9Recall that MST 2O uses approximate search, as opposed
to stacking, which uses approximate features.
163
Mal
t
MST
2O
Mal
t +
MST
2O
, A
Mal
t +
MST
2O
, B
Mal
t +
MST
2O
, C
Mal
t +
MST
2O
, D
Mal
t +
MST
2O
, E
Arabic 66.71 67.88 68.56 69.12 68.64 68.34 68.92
Bulgarian 87.41 87.31 88.99 88.89 88.89 88.93 88.91
Chinese 86.92 87.57 88.41 88.31 88.29 88.13 88.41
Danish 84.77 85.27 86.45 86.67 86.79 86.13 86.71
Dutch 78.59 79.99 80.75 81.47 81.47 81.51 81.29
German 85.82 87.44 88.16 88.50 88.56 88.68 88.38
Japanese 91.65 90.93 91.63 91.43 91.59 91.61 91.49
Portuguese 87.60 87.12 88.00 88.24 88.30 88.18 88.22
Slovene 70.30 74.02 76.62 76.00 76.60 76.18 76.72
Spanish 81.29 82.43 83.09 83.73 83.47 83.21 83.43
Swedish 84.58 82.87 84.92 84.60 84.80 85.16 84.88
Turkish 65.68 60.11 64.35 64.51 64.51 65.07 65.21
Average 80.94 81.08 82.52 82.58 82.65 82.59 82.71
Table 4: Results of stacking Malt and MST 2O at level 0 and level 1, respectively. Columns 2?4 enumerate LAS for
Malt , MST 2O and Malt + MST 2O as in Nivre and McDonald (2008). Columns 5?8 enumerate results for four other
stacked feature configurations. Bold indicates best result for a language.
tion A of Malt + MST 2O (see Table 4), the dif-
ference being statistically insignificant (p > 0.05).
This shows that stacking Malt with the exact, arc-
factored MST 1O bridges the difference between the
individual MST 1O and MST 2O models, by approx-
imating higher order features, but maintaining an
O(n2) runtime and finding the model-optimal parse.
5.5 Disagreement as a Confidence Measure
In pipelines or semisupervised settings, it is use-
ful when a parser can provide a confidence measure
alongside its predicted parse tree. Because stacked
predictors use ensembles with observable outputs,
differences among those outputs may be used to es-
timate confidence in the final output. In stacked de-
pendency parsing, this can be done (for example) by
measuring the Hamming distance between the out-
puts of the level 0 and 1 parsers, L(g(x), h(x)). In-
deed, the bound derived in ?4.2 suggests that the
second-order approximation degrades for candidate
parses y that are Hamming-far from g(x); therefore,
if L(g(x), h(x)) is large, the best score s(x, h(x))
may well be ?biased? due to misleading neighbor-
ing information provided by the level 0 parser.
We illustrate this point with an empirical analysis
of the level 0/1 disagreement for the set of exper-
iments described in ?5.3; namely, we compare the
0 2 4 6 8 10
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
L(g(x),h(x))
Sent
ence
 Ave
rage
d Ac
cura
cy
 
 Level 0Level 1Level 0 (Overall)Level 1 (Overall)
Figure 2: Accuracy as a function of token disagreement
between level 0 and level 1. The x-axis is the Hamming
distance L(g(x), h(x)), i.e., the number of tokens where
level 0 and level 1 disagree. The y-axis is the accuracy
averaged over sentences that have the specified Hamming
distance, both for level 0 and level 1.
164
Mal
t
MST
1O
MST
2O
Mal
t +
MST
1O
, A
Mal
t +
MST
1O
, B
Mal
t +
MST
1O
, C
Mal
t +
MST
1O
, D
Mal
t +
MST
1O
, E
Arabic 66.71 66.81 67.88 68.40 68.50 68.20 68.42 68.68
Bulgarian 87.41 86.65 87.31 88.55 88.67 88.75 88.71 88.79
Chinese 86.92 86.60 87.57 87.67 87.73 87.83 87.67 87.61
Danish 84.77 84.87 85.27 86.59 86.27 86.21 86.35 86.15
Dutch 78.59 78.95 79.99 80.53 81.51 80.71 81.61 81.37
German 85.82 86.26 87.44 88.18 88.30 88.20 88.36 88.42
Japanese 91.65 91.01 90.93 91.55 91.53 91.51 91.43 91.57
Portuguese 87.60 86.28 87.12 88.16 88.26 88.46 88.26 88.36
Slovene 70.30 73.96 74.02 75.84 75.64 75.42 75.96 75.64
Spanish 81.29 81.07 82.43 82.61 83.13 83.13 83.09 82.99
Swedish 84.58 81.88 82.87 84.86 84.62 84.64 84.82 84.76
Turkish 65.68 59.63 60.11 64.49 64.97 64.47 64.63 64.61
Average 80.94 80.33 81.08 82.28 82.42 82.29 82.44 82.41
Table 5: Results of stacking Malt and MST 1O at level 0 and level 1, respectively. Columns 2?4 enumerate LAS for
Malt , MST 1O and MST 2O. Columns 5?9 enumerate results for five different stacked feature configurations. Bold
indicates the best result for a language.
level 0 and level 1 predictions under the best overall
configuration (configuration E of Malt+MST2O).
Figure 2 depicts accuracy as a function of level 0-
level 1 disagreement (in number of tokens), aver-
aged over all datasets.
We can see that performance degrades steeply
when the disagreement between levels 0 and 1 in-
creases in the range 0?4, and then behaves more ir-
regularly but keeping the same trend. This suggests
that the Hamming distance L(g(x), h(x)) is infor-
mative about parser performance and may be used
as a confidence measure.
6 Conclusion
In this work, we made use of stacked learning to im-
prove dependency parsing. We considered an archi-
tecture with two layers, where the output of a stan-
dard parser in the first level provides new features
for a parser in the subsequent level. During learning,
the second parser learns to correct mistakes made by
the first one. The novelty of our approach is in the
exploitation of higher-order predicted edges to simu-
late non-local features in the second parser. We pro-
vided a novel interpretation of stacking as feature
approximation, and our experimental results show
rich-featured stacked parsers outperforming state-
of-the-art single-layer and ensemble parsers. No-
tably, using a simple arc-factored parser at level 1,
we obtain an exact O(n2) stacked parser that outper-
forms earlier approximate methods (McDonald and
Pereira, 2006).
Acknowledgments
The authors thank the anonymous reviewers for
helpful comments, Vitor Carvalho, William Cohen,
and David Smith for interesting discussions, and
Ryan McDonald and Joakim Nivre for providing
us their code and preprocessed datasets. A.M. was
supported by a grant from FCT through the CMU-
Portugal Program and the Information and Com-
munications Technologies Institute (ICTI) at CMU.
N.S. was supported by NSF IIS-0713265 and an
IBM faculty award. E.X. was supported by NSF
DBI-0546594, DBI-0640543, and IIS-0713379.
References
S. P. Abney, D. A. McAllester, and F. Pereira. 1999. Re-
lating probabilistic grammars and automata. In Pro-
ceedings of ACL.
A. Blum and T. Mitchell. 1998. Combining labeled
and unlabeled data with co-training. In Proceedings
of COLT.
L. Breiman. 1996. Stacked regressions. Machine Learn-
ing, 24:49.
165
E. Brill. 1993. A Corpus-Based Approach to Language
Learning. Ph.D. thesis, University of Pennsylvania.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared task
on multilingual dependency parsing. In Proceedings
of CoNLL.
Y. J. Chu and T. H. Liu. 1965. On the shortest arbores-
cence of a directed graph. Science Sinica, 14:1396?
1400.
W. W. Cohen and V. Rocha de Carvalho. 2005. Stacked
sequential learning. In Proceedings of IJCAI.
A. Culotta and J. Sorensen. 2004. Dependency tree ker-
nels for relation extraction. In Proceedings of ACL.
Y. Ding and M. Palmer. 2005. Machine translation using
probabilistic synchronous dependency insertion gram-
mar. In Proceedings of ACL.
J. Edmonds. 1967. Optimum branchings. Journal of Re-
search of the National Bureau of Standards, 71B:233?
240.
J. Eisner and G. Satta. 1999. Efficient parsing for bilex-
ical context-free grammars and head automaton gram-
mars. In Proceedings of ACL.
J. Eisner. 1996. Three new probabilistic models for de-
pendency parsing: An exploration. In Proceedings of
COLING.
J. Hall, J. Nivre, and J. Nilsson. 2006. Discriminative
classifiers for deterministic dependency parsing. In
Proceedings of ACL.
Z. Kou and W. W. Cohen. 2007. Stacked graphical mod-
els for efficient inference in Markov random fields. In
Proceedings of SDM.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings
of ICML.
P. Liang, H. Daume?, and D. Klein. 2008. Structure com-
pilation: trading structure for features. In Proceedings
of ICML.
R. McDonald and J. Nivre. 2007. Characterizing the
errors of data-driven dependency parsing models. In
Proceedings of EMNLP-CoNLL.
R. T. McDonald and F. C. N. Pereira. 2006. Online learn-
ing of approximate dependency parsing algorithms. In
Proceedings of EACL.
R. McDonald and G. Satta. 2007. On the complexity
of non-projective data-driven dependency parsing. In
Proceedings of IWPT.
R. McDonald, K. Crammer, and F. Pereira. 2005a. On-
line large-margin training of dependency parsers. In
Proceedings of ACL.
R. T. McDonald, F. Pereira, K. Ribarov, and J. Ha-
jic. 2005b. Non-projective dependency parsing us-
ing spanning tree algorithms. In Proceedings of HLT-
EMNLP.
R. McDonald, K. Lerman, and F. Pereira. 2006. Multi-
lingual dependency analysis with a two-stage discrim-
inative parser. In Proceedings CoNLL.
J. Nivre and R. McDonald. 2008. Integrating graph-
based and transition-based dependency parsers. In
Proceedings of ACL-HLT.
J. Nivre, J. Hall, and J. Nilsson. 2004. Memory-based
dependency parsing. In Proceedings of CoNLL.
J. Nivre, J. Hall, J. Nilsson, G. Eryig?it, and S. Marinov.
2006. Labeled pseudo-projective dependency pars-
ing with support vector machines. In Proceedings of
CoNLL.
A. Ratnaparkhi, S. Roukos, and R. T. Ward. 1994. A
maximum entropy model for parsing. In Proceedings
of ICSLP.
S. Riedel and J. Clarke. 2006. Incremental integer linear
programming for non-projective dependency parsing.
In Proceedings of EMNLP.
K. Sagae and A. Lavie. 2005. A classifier-based parser
with linear run-time complexity. In Proceedings of
IWPT.
D. A. Smith and J. Eisner. 2008. Dependency parsing by
belief propagation. In Proceedings of EMNLP.
I. Titov and J. Henderson. 2007. A latent variable model
for generative dependency parsing. In Proceedings of
IWPT.
M. Wang, N. A. Smith, and T. Mitamura. 2007. What is
the Jeopardy model? A quasi-synchronous grammar
for QA. In Proceedings of EMNLP-CoNLL.
D. Wolpert. 1992. Stacked generalization. Neural Net-
works, 5(2):241?260.
H. Yamada and Y. Matsumoto. 2003. Statistical depen-
dency analysis with support vector machines. In Pro-
ceedings of IWPT.
166
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 969?976,
Sydney, July 2006. c?2006 Association for Computational Linguistics
BiTAM: Bilingual Topic AdMixture Models for Word Alignment
Bing Zhao? and Eric P. Xing??
{bzhao,epxing}@cs.cmu.edu
Language Technologies Institute? and Machine Learning Department?
School of Computer Science, Carnegie Mellon University
Abstract
We propose a novel bilingual topical ad-
mixture (BiTAM) formalism for word
alignment in statistical machine transla-
tion. Under this formalism, the paral-
lel sentence-pairs within a document-pair
are assumed to constitute a mixture of
hidden topics; each word-pair follows a
topic-specific bilingual translation model.
Three BiTAM models are proposed to cap-
ture topic sharing at different levels of lin-
guistic granularity (i.e., at the sentence or
word levels). These models enable word-
alignment process to leverage topical con-
tents of document-pairs. Efficient vari-
ational approximation algorithms are de-
signed for inference and parameter esti-
mation. With the inferred latent topics,
BiTAM models facilitate coherent pairing
of bilingual linguistic entities that share
common topical aspects. Our preliminary
experiments show that the proposed mod-
els improve word alignment accuracy, and
lead to better translation quality.
1 Introduction
Parallel data has been treated as sets of unre-
lated sentence-pairs in state-of-the-art statistical
machine translation (SMT) models. Most current
approaches emphasize within-sentence dependen-
cies such as the distortion in (Brown et al, 1993),
the dependency of alignment in HMM (Vogel et
al., 1996), and syntax mappings in (Yamada and
Knight, 2001). Beyond the sentence-level, corpus-
level word-correlation and contextual-level topical
information may help to disambiguate translation
candidates and word-alignment choices. For ex-
ample, the most frequent source words (e.g., func-
tional words) are likely to be translated into words
which are also frequent on the target side; words of
the same topic generally bear correlations and sim-
ilar translations. Extended contextual information
is especially useful when translation models are
vague due to their reliance solely on word-pair co-
occurrence statistics. For example, the word shot
in ?It was a nice shot.? should be translated dif-
ferently depending on the context of the sentence:
a goal in the context of sports, or a photo within
the context of sightseeing. Nida (1964) stated
that sentence-pairs are tied by the logic-flow in a
document-pair; in other words, the document-pair
should be word-aligned as one entity instead of be-
ing uncorrelated instances. In this paper, we pro-
pose a probabilistic admixture model to capture
latent topics underlying the context of document-
pairs. With such topical information, the trans-
lation models are expected to be sharper and the
word-alignment process less ambiguous.
Previous works on topical translation models
concern mainly explicit logical representations of
semantics for machine translation. This include
knowledge-based (Nyberg and Mitamura, 1992)
and interlingua-based (Dorr and Habash, 2002)
approaches. These approaches can be expen-
sive, and they do not emphasize stochastic trans-
lation aspects. Recent investigations along this
line includes using word-disambiguation schemes
(Carpua and Wu, 2005) and non-overlapping bilin-
gual word-clusters (Wang et al, 1996; Och, 1999;
Zhao et al, 2005) with particular translation mod-
els, which showed various degrees of success. We
propose a new statistical formalism: Bilingual
Topic AdMixture model, or BiTAM, to facilitate
topic-based word alignment in SMT.
Variants of admixture models have appeared in
population genetics (Pritchard et al, 2000) and
text modeling (Blei et al, 2003). Statistically, an
object is said to be derived from an admixture if it
consists of a bag of elements, each sampled inde-
pendently or coupled in some way, from a mixture
model. In a typical SMT setting, each document-
pair corresponds to an object; depending on a
chosen modeling granularity, all sentence-pairs or
word-pairs in the document-pair correspond to the
elements constituting the object. Correspondingly,
a latent topic is sampled for each pair from a prior
topic distribution to induce topic-specific transla-
tions; and the resulting sentence-pairs and word-
pairs are marginally dependent. Generatively, this
admixture formalism enables word translations to
be instantiated by topic-specific bilingual models
969
and/or monolingual models, depending on their
contexts. In this paper we investigate three in-
stances of the BiTAM model, They are data-driven
and do not need hand-crafted knowledge engineer-
ing.
The remainder of the paper is as follows: in sec-
tion 2, we introduce notations and baselines; in
section 3, we propose the topic admixture models;
in section 4, we present the learning and inference
algorithms; and in section 5 we show experiments
of our models. We conclude with a brief discus-
sion in section 6.
2 Notations and Baseline
In statistical machine translation, one typically
uses parallel data to identify entities such as
?word-pair?, ?sentence-pair?, and ?document-
pair?. Formally, we define the following terms1:
? A word-pair (fj , ei) is the basic unit for word
alignment, where fj is a French word and ei
is an English word; j and i are the position
indices in the corresponding French sentence
f and English sentence e.
? A sentence-pair (f , e) contains the source
sentence f of a sentence length of J ; a target
sentence e of length I . The two sentences f
and e are translations of each other.
? A document-pair (F,E) refers to two doc-
uments which are translations of each other.
Assuming sentences are one-to-one corre-
spondent, a document-pair has a sequence of
N parallel sentence-pairs {(fn, en)}, where
(fn, en) is the n?th parallel sentence-pair.
? A parallel corpus C is a collection of M par-
allel document-pairs: {(Fd,Ed)}.
2.1 Baseline: IBM Model-1
The translation process can be viewed as opera-
tions of word substitutions, permutations, and in-
sertions/deletions (Brown et al, 1993) in noisy-
channel modeling scheme at parallel sentence-pair
level. The translation lexicon p(f |e) is the key
component in this generative process. An efficient
way to learn p(f |e) is IBM-1:
p(f |e) =
J?
j=1
I?
i=1
p(fj |ei) ? p(ei|e). (1)
1We follow the notations in (Brown et al, 1993) for
English-French, i.e., e ? f , although our models are tested,
in this paper, for English-Chinese. We use the end-user ter-
minology for source and target languages.
IBM-1 has global optimum; it is efficient and eas-
ily scalable to large training data; it is one of the
most informative components for re-ranking trans-
lations (Och et al, 2004). We start from IBM-1 as
our baseline model, while higher-order alignment
models can be embedded similarly within the pro-
posed framework.
3 Bilingual Topic AdMixture Model
Now we describe the BiTAM formalism that
captures the latent topical structure and gener-
alizes word alignments and translations beyond
sentence-level via topic sharing across sentence-
pairs:
E?=argmax
{E}
p(F|E)p(E), (2)
where p(F|E) is a document-level translation
model, generating the document F as one entity.
In a BiTAM model, a document-pair (F,E) is
treated as an admixture of topics, which is induced
by random draws of a topic, from a pool of topics,
for each sentence-pair. A unique normalized and
real-valued vector ?, referred to as a topic-weight
vector, which captures contributions of different
topics, are instantiated for each document-pair, so
that the sentence-pairs with their alignments are
generated from topics mixed according to these
common proportions. Marginally, a sentence-
pair is word-aligned according to a unique bilin-
gual model governed by the hidden topical assign-
ments. Therefore, the sentence-level translations
are coupled, rather than being independent as as-
sumed in the IBM models and their extensions.
Because of this coupling of sentence-pairs (via
topic sharing across sentence-pairs according to
a common topic-weight vector), BiTAM is likely
to improve the coherency of translations by treat-
ing the document as a whole entity, instead of un-
correlated segments that have to be independently
aligned and then assembled. There are at least
two levels at which the hidden topics can be sam-
pled for a document-pair, namely: the sentence-
pair and the word-pair levels. We propose three
variants of the BiTAM model to capture the latent
topics of bilingual documents at different levels.
3.1 BiTAM-1: The Frameworks
In the first BiTAM model, we assume that topics
are sampled at the sentence-level. Each document-
pair is represented as a random mixture of la-
tent topics. Each topic, topic-k, is presented by a
topic-specific word-translation table: Bk, which is
970
f
a
J
I
N
M
e
B? z?
?
J B
I
f
e
a
? ? z
M
N
a
J
I
N
M
e
B? z? f
(a) (b) (c)
Figure 1: BiTAM models for Bilingual document- and sentence-pairs. A node in the graph represents a random variable, and
a hexagon denotes a parameter. Un-shaded nodes are hidden variables. All the plates represent replicates. The outmost plate
(M -plate) represents M bilingual document-pairs, while the inner N -plate represents the N repeated choice of topics for each
sentence-pairs in the document; the inner J-plate represents J word-pairs within each sentence-pair. (a) BiTAM-1 samples
one topic (denoted by z) per sentence-pair; (b) BiTAM-2 utilizes the sentence-level topics for both the translation model (i.e.,
p(f |e, z)) and the monolingual word distribution (i.e., p(e|z)); (c) BiTAM-3 samples one topic per word-pair.
a translation lexicon: Bi,j,k=p(f=fj |e=ei, z=k),
where z is an indicator variable to denote the
choice of a topic. Given a specific topic-weight
vector ?d for a document-pair, each sentence-pair
draws its conditionally independent topics from a
mixture of topics. This generative process, for a
document-pair (Fd,Ed), is summarized as below:
1. Sample sentence-number N from a Poisson(?).
2. Sample topic-weight vector ?d from a Dirichlet(?).
3. For each sentence-pair (fn, en) in the d?th doc-pair ,
(a) Sample sentence-length Jn from Poisson(?);
(b) Sample a topic zdn from a Multinomial(?d);
(c) Sample ej from a monolingual model p(ej);
(d) Sample each word alignment link aj from a uni-
form model p(aj) (or an HMM);
(e) Sample each fj according to a topic-specific
translation lexicon p(fj |e, aj , zn,B).
We assume that, in our model, there are K pos-
sible topics that a document-pair can bear. For
each document-pair, a K-dimensional Dirichlet
random variable ?d, referred to as the topic-weight
vector of the document, can take values in the
(K?1)-simplex following a probability density:
p(?|?) = ?(
?K
k=1 ?k)?K
k=1 ?(?k)
??1?11 ? ? ? ??K?1K , (3)
where the hyperparameter ? is a K-dimension
vector with each component ?k>0, and ?(x)
is the Gamma function. The alignment is
represented by a J-dimension vector a =
{a1, a2, ? ? ? , aJ}; for each French word fj at the
position j, an position variable aj maps it to an
English word eaj at the position aj in English sen-
tence. The word level translation lexicon probabil-
ities are topic-specific, and they are parameterized
by the matrix B = {Bk}.
For simplicity, in our current models we omit
the modelings of the sentence-number N and the
sentence-length Jn, and focus only on the bilin-
gual translation model. Figure 1 (a) shows the
graphical model representation for the BiTAM
generative scheme discussed so far. Note that, the
sentence-pairs are now connected by the node ?d.
Therefore, marginally, the sentence-pairs are not
independent of each other as in traditional SMT
models, instead they are conditionally indepen-
dent given the topic-weight vector ?d. Specifi-
cally, BiTAM-1 assumes that each sentence-pair
has one single topic. Thus, the word-pairs within
this sentence-pair are conditionally independent of
each other given the hidden topic index z of the
sentence-pair.
The last two sub-steps (3.d and 3.e) in the
BiTam sampling scheme define a translation
model, in which an alignment link aj is proposed
and an observation of fj is generated according
to the proposed distributions. We simplify align-
ment model of a, as in IBM-1, by assuming that
aj is sampled uniformly at random. Given the pa-
rameters ?, B, and the English part E, the joint
conditional distribution of the topic-weight vector
?, the topic indicators z, the alignment vectors A,
and the document F can be written as:
p(F,A, ?, z|E, ?,B)=
p(? |?)
N?
n=1
p(zn|?)p(fn,an|en, ?, Bzn),
(4)
where N is the number of the sentence-pair.
Marginalizing out ? and z, we can obtain the
marginal conditional probability of generating F
from E for each document-pair:
p(F,A|E, ?,Bzn) =
?
p(?|?)
( N?
n=1
?
zn
p(zn|?)p(fn,an|en, Bzn)
)
d?, (5)
where p(fn,an|en, Bzn) is a topic-specific
sentence-level translation model. For simplicity,
we assume that the French words fj?s are condi-
tionally independent of each other; the alignment
971
variables aj?s are independent of other variables
and are uniformly distributed a priori. Therefore,
the distribution for each sentence-pair is:
p(fn,an|en, Bzn) = p(fn|en,an, Bzn)p(an|en, Bzn)
= 1IJnn
Jn?
j=1
p(fnj |eanj , Bzn). (6)
Thus, the conditional likelihood for the entire
parallel corpus is given by taking the product
of the marginal probabilities of each individual
document-pair in Eqn. 5.
3.2 BiTAM-2: Monolingual Admixture
In general, the monolingual model for English
can also be a rich topic-mixture. This is real-
ized by using the same topic-weight vector ?d and
the same topic indicator zdn sampled according
to ?d, as described in ?3.1, to introduce not only
topic-dependent translation lexicon, but also topic-
dependent monolingual model of the source lan-
guage, English in this case, for generating each
sentence-pair (Figure 1 (b)). Now e is generated
from a topic-based language model ?, instead of a
uniform distribution in BiTAM-1. We refer to this
model as BiTAM-2.
Unlike BiTAM-1, where the information ob-
served in ei is indirectly passed to z via the node
of fj and the hidden variable aj , in BiTAM-2, the
topics of corresponding English and French sen-
tences are also strictly aligned so that the informa-
tion observed in ei can be directly passed to z, in
the hope of finding more accurate topics. The top-
ics are inferred more directly from the observed
bilingual data, and as a result, improve alignment.
3.3 BiTAM-3: Word-level Admixture
It is straightforward to extend the sentence-level
BiTAM-1 to a word-level admixture model, by
sampling topic indicator zn,j for each word-pair
(fj , eaj ) in the n?th sentence-pair, rather than
once for all (words) in the sentence (Figure 1 (c)).
This gives rise to our BiTAM-3. The conditional
likelihood functions can be obtained by extending
the formulas in ?3.1 to move the variable zn,j in-
side the same loop over each of the fn,j .
3.4 Incorporation of Word ?Null?
Similar to IBM models, ?Null? word is used for
the source words which have no translation coun-
terparts in the target language. For example, Chi-
nese words ?de? () , ?ba? (r) and ?bei?
(Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 342?350,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Concise Integer Linear Programming Formulations
for Dependency Parsing
Andre? F. T. Martins?? Noah A. Smith? Eric P. Xing?
?School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, USA
?Instituto de Telecomunicac?o?es, Instituto Superior Te?cnico, Lisboa, Portugal
{afm,nasmith,epxing}@cs.cmu.edu
Abstract
We formulate the problem of non-
projective dependency parsing as a
polynomial-sized integer linear pro-
gram. Our formulation is able to handle
non-local output features in an efficient
manner; not only is it compatible with
prior knowledge encoded as hard con-
straints, it can also learn soft constraints
from data. In particular, our model is able
to learn correlations among neighboring
arcs (siblings and grandparents), word
valency, and tendencies toward nearly-
projective parses. The model parameters
are learned in a max-margin framework
by employing a linear programming
relaxation. We evaluate the performance
of our parser on data in several natural
languages, achieving improvements over
existing state-of-the-art methods.
1 Introduction
Much attention has recently been devoted to in-
teger linear programming (ILP) formulations of
NLP problems, with interesting results in appli-
cations like semantic role labeling (Roth and Yih,
2005; Punyakanok et al, 2004), dependency pars-
ing (Riedel and Clarke, 2006), word alignment
for machine translation (Lacoste-Julien et al,
2006), summarization (Clarke and Lapata, 2008),
and coreference resolution (Denis and Baldridge,
2007), among others. In general, the rationale for
the development of ILP formulations is to incorpo-
rate non-local features or global constraints, which
are often difficult to handle with traditional algo-
rithms. ILP formulations focus more on the mod-
eling of problems, rather than algorithm design.
While solving an ILP is NP-hard in general, fast
solvers are available today that make it a practical
solution for many NLP problems.
This paper presents new, concise ILP formu-
lations for projective and non-projective depen-
dency parsing. We believe that our formula-
tions can pave the way for efficient exploitation of
global features and constraints in parsing applica-
tions, leading to more powerful models. Riedel
and Clarke (2006) cast dependency parsing as
an ILP, but efficient formulations remain an open
problem. Our formulations offer the following
comparative advantages:
? The numbers of variables and constraints are
polynomial in the sentence length, as opposed to
requiring exponentially many constraints, elim-
inating the need for incremental procedures like
the cutting-plane algorithm;
? LP relaxations permit fast online discriminative
training of the constrained model;
? Soft constraints may be automatically learned
from data. In particular, our formulations han-
dle higher-order arc interactions (like siblings
and grandparents), model word valency, and can
learn to favor nearly-projective parses.
We evaluate the performance of the new parsers
on standard parsing tasks in seven languages. The
techniques that we present are also compatible
with scenarios where expert knowledge is avail-
able, for example in the form of hard or soft first-
order logic constraints (Richardson and Domin-
gos, 2006; Chang et al, 2008).
2 Dependency Parsing
2.1 Preliminaries
A dependency tree is a lightweight syntactic repre-
sentation that attempts to capture functional rela-
tionships between words. Lately, this formalism
has been used as an alternative to phrase-based
parsing for a variety of tasks, ranging from ma-
chine translation (Ding and Palmer, 2005) to rela-
tion extraction (Culotta and Sorensen, 2004) and
question answering (Wang et al, 2007).
Let us first describe formally the set of legal de-
pendency parse trees. Consider a sentence x =
342
?w0, . . . , wn?, where wi denotes the word at the i-
th position, and w0 = $ is a wall symbol. We form
the (complete1) directed graph D = ?V,A?, with
vertices in V = {0, . . . , n} (the i-th vertex corre-
sponding to the i-th word) and arcs in A = V 2.
Using terminology from graph theory, we say that
B ? A is an r-arborescence2 of the directed
graph D if ?V,B? is a (directed) tree rooted at r.
We define the set of legal dependency parse trees
of x (denoted Y(x)) as the set of 0-arborescences
of D, i.e., we admit each arborescence as a poten-
tial dependency tree.
Let y ? Y(x) be a legal dependency tree for
x; if the arc a = ?i, j? ? y, we refer to i as the
parent of j (denoted i = pi(j)) and j as a child of
i. We also say that a is projective (in the sense of
Kahane et al, 1998) if any vertex k in the span of
a is reachable from i (in other words, if for any k
satisfying min(i, j) < k < max(i, j), there is a
directed path in y from i to k). A dependency tree
is called projective if it only contains projective
arcs. Fig. 1 illustrates this concept.3
The formulation to be introduced in ?3 makes
use of the notion of the incidence vector associ-
ated with a dependency tree y ? Y(x). This is
the binary vector z , ?za?a?A with each compo-
nent defined as za = I(a ? y) (here, I(.) denotes
the indicator function). Considering simultane-
ously all incidence vectors of legal dependency
trees and taking the convex hull, we obtain a poly-
hedron that we call the arborescence polytope,
denoted by Z(x). Each vertex of Z(x) can be
identified with a dependency tree in Y(x). The
Minkowski-Weyl theorem (Rockafellar, 1970) en-
sures that Z(x) has a representation of the form
Z(x) = {z ? R|A| | Az ? b}, for some p-by-|A|
matrix A and some vector b in Rp. However, it is
not easy to obtain a compact representation (where
p grows polynomially with the number of words
n). In ?3, we will provide a compact represen-
tation of an outer polytope Z?(x) ? Z(x) whose
integer vertices correspond to dependency trees.
Hence, the problem of finding the dependency tree
that maximizes some linear function of the inci-
1The general case where A ? V 2 is also of interest; it
arises whenever a constraint or a lexicon forbids some arcs
from appearing in dependency tree. It may also arise as a
consequence of a first-stage pruning step where some candi-
date arcs are eliminated; this will be further discussed in ?4.
2Or ?directed spanning tree with designated root r.?
3In this paper, we consider unlabeled dependency parsing,
where only the backbone structure (i.e., the arcs without the
labels depicted in Fig. 1) is to be predicted.
Figure 1: A projective dependency graph.
Figure 2: Non-projective dependency graph.
those that assume each dependency decision is in-
dependent modulo the global structural constraint
that dependency graphs must be trees. Such mod-
els are commonly referred to as edge-factored since
their parameters factor relative to individual edges
of the graph (Paskin, 2001; McDonald et al,
2005a). Edge-factored models have many computa-
tional benefits, most notably that inference for non-
projective dependency graphs can be achieved in
polynomial time (McDonald et al, 2005b). The pri-
mary problem in treating each dependency as in-
dependent is that it is not a realistic assumption.
Non-local information, such as arity (or valency)
and neighbouring dependencies, can be crucial to
obtaining high parsing accuracies (Klein and Man-
ning, 2002; McDonald and Pereira, 2006). How-
ever, in the data-driven parsing setting this can be
partially adverted by incorporating rich feature rep-
resentations over the input (McDonald et al, 2005a).
The goal of this work is to further our current
understanding of the computational nature of non-
projective parsing algorithms for both learning and
inference within the data-driven setting. We start by
investigating and extending the edge-factored model
of McDonald et al (2005b). In particular, we ap-
peal to the Matrix Tree Theorem for multi-digraphs
to design polynomial-time algorithms for calculat-
ing both the partition function and edge expecta-
tions over all possible dependency graphs for a given
sentence. To motivate these algorithms, we show
that they can be used in many important learning
and inference problems including min-risk decod-
ing, training globally normalized log-linear mod-
els, syntactic language modeling, and unsupervised
learning via the EM algorithm ? none of which have
previously been known to have exact non-projective
implementations.
We then switch focus to models that account for
non-local information, in particular arity and neigh-
bouring parse decisions. For systems that model ar-
ity constraints we give a reduction from the Hamilto-
nian graph problem suggesting that the parsing prob-
lem is intractable in this case. For neighbouring
parse decisions, we extend the work of McDonald
and Pereira (2006) and show that modeling vertical
neighbourhoods makes parsing intractable in addi-
tion to modeling horizontal neighbourhoods. A con-
sequence of these results is that it is unlikely that
exact non-projective dependency parsing is tractable
for any model assumptions weaker than those made
by the edge-factored models.
1.1 Related Work
There has been extensive work on data-driven de-
pendency parsing for both projective parsing (Eis-
ner, 1996; Paskin, 2001; Yamada and Matsumoto,
2003; Nivre and Scholz, 2004; McDonald et al,
2005a) and non-projective parsing systems (Nivre
and Nilsson, 2005; Hall and No?va?k, 2005; McDon-
ald et al, 2005b). These approaches can often be
classified into two broad categories. In the first cat-
egory are those methods that employ approximate
inference, typically through the use of linear time
shift-reduce parsing algorithms (Yamada and Mat-
sumoto, 2003; Nivre and Scholz, 2004; Nivre and
Nilsson, 2005). In the second category are those
that employ exhaustive inference algorithms, usu-
ally by making strong independence assumptions, as
is the case for edge-factored models (Paskin, 2001;
McDonald et al, 2005a; McDonald et al, 2005b).
Recently there have also been proposals for exhaus-
tive methods that weaken the edge-factored assump-
tion, including both approximate methods (McDon-
ald and Pereira, 2006) and exact methods through in-
teger linear programming (Riedel and Clarke, 2006)
or branch-and-bound algorithms (Hirakawa, 2006).
For grammar based models there has been limited
work on empirical systems for non-projective pars-
ing systems, notable exceptions include the work
of Wang and Harper (2004). Theoretical studies of
note include the work of Neuhaus and Bo?ker (1997)
showing that the recognition problem for a mini-
$ Figure 1: A projective dependency graph.
Figure 2: Non-projective dependency graph.
those that assume each dependency decision is in-
dependent modulo the global structural constraint
that dependency graphs must be trees. Such mod-
els are commonly referred to as edge-factored since
their parameters factor relative to individual edges
of the graph (Paskin, 2001; McDonald et al,
2005a). Edge-factored models have many computa-
tional benefits, most notably that inference for non-
projective dependency graphs can be achieved in
polynomial time (McDonald et al, 2005b). The pri-
mary problem in treating each dependency as in-
dependent is that it is not a realistic assumption.
Non-local information, such as arity (or valency)
and eighbouring dependencies, can be crucial to
obtaining high parsing accuracies (Klein and Man-
ning, 2002; McDonald and P reira, 2006). How-
ever, i the data-dr ven parsing etting this can be
partially adverted by incorporating rich feature rep-
resentations over the input (McDonald et al, 2005a).
The goal of this work is to further our curr nt
understanding of the computational nature of non-
projective parsing algorithms for both learning and
inference within the data-driven setting. We start by
investigating and extending the edge-factored model
of McDonald et al (2005b). In particular, we ap-
peal to the Matrix Tree Theorem for multi-digraphs
to design polynomial-time algorithms for calculat-
ing both the partition function and edge expecta-
tions over all possible dependency graphs for a given
sentence. To motivate these algorithms, we show
that they can be used in many important learning
and inference problems including min-risk decod-
ing, training globally normalized log-linear mod-
els, syntactic language modeling, and unsupervised
learning via the EM algorithm ? none of which have
previously been known t have exact n n-projective
implementations.
We th switch focus to models that account for
non-local information, in particular arity and neigh-
bouring parse decisions. For systems that model ar-
ity constraints we give a reduction from the Hamilto-
nian graph problem suggesting that the parsing prob-
lem is intractable in this case. For neighbouring
parse decisions, we extend the work of McDonald
and Pereira (2006) and show that modeling vertical
neighbourhoods makes parsing intractable in addi-
tion to modeling horizontal neighbourhoods. A con-
sequence of these results is that it is unlikely that
exact non-projective dependency parsing is tractable
for any model assumptions weaker than those made
by the edge-factored models.
1.1 Related W rk
There has been extensive work on data-driven de-
pendency parsing for both projective parsing (Eis-
ner, 1996; Paskin, 2001; Yamada and Matsumoto,
2003; Nivre and Scholz, 2004; McDonald et al,
2005a) and non-projective parsing systems (Nivre
and Nilsson, 2005; Hall and No?va?k, 2005; McDon-
ald et al, 2005b). These approaches can often be
classified into two broad categories. In the first cat-
egory are those methods that employ approximate
inference, typically through the use of linear time
shift-reduce parsing algorit ms (Yamad and Mat-
sumoto, 2003; Nivre and Scholz, 2004; Nivre and
Nilsson, 2005). I the second category are those
that employ exhaustive inference algorithms, usu-
ally by making strong independence assumptions, a
is the case for edge-factored mod ls (Paskin, 2001;
McDonald et al, 2005a; McDonald et al, 2005b).
Recently there have also been proposals for exhaus-
tive methods that weaken the edge-factored assump-
tion, including both approximate methods (McDon-
ald and Pereira, 2006) and exact methods through in-
teger linear programming (Riedel and Clarke, 2006)
or branch-and-bound algorithms (Hirakawa, 2006).
For grammar based models there has been limited
work on empirical systems for non-projective pars-
ing systems, notable exceptions include the work
of Wang and Harper (2004). Theoretical studies of
note include the work of Neuhaus and Bo?ker (1997)
showing that the recognition problem for a mini-
$
Figure 1: A projective dependency parse (top), and a non-
projective dependency parse (bottom) for two English sen-
tences; examples from McDonald and Satta (2007).
dence vectors can be cast as an ILP. A similar idea
was applied to word alignment by Lacoste-Julien
et al (2006), where permutations (rather than ar-
borescences) were the combinatorial structure be-
ing requiring representation.
Letting X denote the set of possible sentences,
define Y ,
?
x?X Y(x). Given a labeled dataset
L , ??x1, y1?, . . . , ?xm, ym?? ? (X ? Y)m, we
aim to learn a parser, i.e., a function h : X ? Y
that given x ? X ou puts a legal dependency parse
y ? Y(x). T e f ct that t er e xponentially
ma y candidates in Y(x) makes dependency pars-
ing a structured classification problem.
2.2 Arc Factorizat o and L c lity
There has been much recent work on dependency
parsing using graph-based, transition-based, and
hybrid methods; see Nivre and McDonald (2008)
for an overview. Typical graph-based methods
consider linear classifiers of the fo m
hw(x) = argmaxy?Y w
>f(x, y), (1)
where f(x, y) is a vector of features and w is the
corresponding weight vector. One wants hw to
have small expected loss; the ty ical loss func-
tion is th Hamming loss, `(y?; y) , |{?i, j? ?
y? : ?i, j? /? y}|. Tractability is usually ensured
by strong factorization assumptions, like the one
underlying the arc-factored model (Eisner, 1996;
McDonald et al, 2005), which forbids any feature
that depends on two or more arcs. This induces a
decomposition of the feature vector f(x, y) as:
f(x, y) =
?
a?y fa(x). (2)
Under this decomposition, each arc receives a
score; parsing amounts to choosing the configu-
ration that maximizes the overall score, which, as
343
shown by McDonald et al (2005), is an instance
of the maximal arborescence problem. Combi-
natorial algorithms (Chu and Liu, 1965; Edmonds,
1967) can solve this problem in cubic time.4 If
the dependency parse trees are restricted to be
projective, cubic-time algorithms are available via
dynamic programming (Eisner, 1996). While in
the projective case, the arc-factored assumption
can be weakened in certain ways while maintain-
ing polynomial parser runtime (Eisner and Satta,
1999), the same does not happen in the nonprojec-
tive case, where finding the highest-scoring tree
becomes NP-hard (McDonald and Satta, 2007).
Approximate algorithms have been employed to
handle models that are not arc-factored (although
features are still fairly local): McDonald and
Pereira (2006) adopted an approximation based
on O(n3) projective parsing followed by a hill-
climbing algorithm to rearrange arcs, and Smith
and Eisner (2008) proposed an algorithm based on
loopy belief propagation.
3 Dependency Parsing as an ILP
Our approach will build a graph-based parser
without the drawback of a restriction to local fea-
tures. By formulating inference as an ILP, non-
local features can be easily accommodated in our
model; furthermore, by using a relaxation tech-
nique we can still make learning tractable. The im-
pact of LP-relaxed inference in the learning prob-
lem was studied elsewhere (Martins et al, 2009).
A linear program (LP) is an optimization prob-
lem of the form
minx?Rd c
>x
s.t. Ax ? b.
(3)
If the problem is feasible, the optimum is attained
at a vertex of the polyhedron that defines the con-
straint space. If we add the constraint x ? Zd, then
the above is called an integer linear program
(ILP). For some special parameter settings?e.g.,
when b is an integer vector and A is totally uni-
modular5?all vertices of the constraining polyhe-
dron are integer points; in these cases, the integer
constraint may be suppressed and (3) is guaran-
teed to have integer solutions (Schrijver, 2003).
Of course, this need not happen: solving a gen-
eral ILP is an NP-complete problem. Despite this
4There is also a quadratic algorithm due to Tarjan (1977).
5A matrix is called totally unimodular if the determinants
of each square submatrix belong to {0, 1,?1}.
fact, fast solvers are available today that make this
a practical solution for many problems. Their per-
formance depends on the dimensions and degree
of sparsity of the constraint matrix A.
Riedel and Clarke (2006) proposed an ILP for-
mulation for dependency parsing which refines
the arc-factored model by imposing linguistically
motivated ?hard? constraints that forbid some arc
configurations. Their formulation includes an ex-
ponential number of constraints?one for each
possible cycle. Since it is intractable to throw
in all constraints at once, they propose a cutting-
plane algorithm, where the cycle constraints are
only invoked when violated by the current solu-
tion. The resulting algorithm is still slow, and an
arc-factored model is used as a surrogate during
training (i.e., the hard constraints are only used at
test time), which implies a discrepancy between
the model that is optimized and the one that is ac-
tually going to be used.
Here, we propose ILP formulations that elim-
inate the need for cycle constraints; in fact, they
require only a polynomial number of constraints.
Not only does our model allow expert knowledge
to be injected in the form of constraints, it is also
capable of learning soft versions of those con-
straints from data; indeed, it can handle features
that are not arc-factored (correlating, for exam-
ple, siblings and grandparents, modeling valency,
or preferring nearly projective parses). While, as
pointed out by McDonald and Satta (2007), the
inclusion of these features makes inference NP-
hard, by relaxing the integer constraints we obtain
approximate algorithms that are very efficient and
competitive with state-of-the-art methods. In this
paper, we focus on unlabeled dependency parsing,
for clarity of exposition. If it is extended to labeled
parsing (a straightforward extension), our formu-
lation fully subsumes that of Riedel and Clarke
(2006), since it allows using the same hard con-
straints and features while keeping the ILP poly-
nomial in size.
3.1 The Arborescence Polytope
We start by describing our constraint space. Our
formulations rely on a concise polyhedral repre-
sentation of the set of candidate dependency parse
trees, as sketched in ?2.1. This will be accom-
plished by drawing an analogy with a network
flow problem.
Let D = ?V,A? be the complete directed graph
344
associated with a sentence x ? X , as stated in
?2. A subgraph y = ?V,B? is a legal dependency
tree (i.e., y ? Y(x)) if and only if the following
conditions are met:
1. Each vertex in V \ {0} must have exactly one
incoming arc in B,
2. 0 has no incoming arcs in B,
3. B does not contain cycles.
For each vertex v ? V , let ??(v) , {?i, j? ?
A | j = v} denote its set of incoming arcs, and
?+(v) , {?i, j? ? A | i = v} denote its set of
outgoing arcs. The two first conditions can be eas-
ily expressed by linear constraints on the incidence
vector z:
?
a???(j) za = 1, j ? V \ {0} (4)
?
a???(0) za = 0 (5)
Condition 3 is somewhat harder to express. Rather
than adding exponentially many constraints, one
for each potential cycle (like Riedel and Clarke,
2006), we equivalently replace condition 3 by
3?. B is connected.
Note that conditions 1-2-3 are equivalent to 1-2-
3?, in the sense that both define the same set Y(x).
However, as we will see, the latter set of condi-
tions is more convenient. Connectedness of graphs
can be imposed via flow constraints (by requir-
ing that, for any v ? V \ {0}, there is a directed
path in B connecting 0 to v). We adapt the single
commodity flow formulation for the (undirected)
minimum spanning tree problem, due to Magnanti
and Wolsey (1994), that requires O(n2) variables
and constraints. Under this model, the root node
must send one unit of flow to every other node.
By making use of extra variables, ? , ??a?a?A,
to denote the flow of commodities through each
arc, we are led to the following constraints in ad-
dition to Eqs. 4?5 (we denote U , [0, 1], and
B , {0, 1} = U ? Z):
? Root sends flow n:
?
a??+(0) ?a = n (6)
? Each node consumes one unit of flow:
?
a???(j)
?a ?
?
a??+(j)
?a = 1, j ? V \ {0} (7)
? Flow is zero on disabled arcs:
?a ? nza, a ? A (8)
? Each arc indicator lies in the unit interval:
za ? U, a ? A. (9)
These constraints project an outer bound of the ar-
borescence polytope, i.e.,
Z?(x) , {z ? R|A| | (z,?) satisfy (4?9)}
? Z(x). (10)
Furthermore, the integer points of Z?(x) are pre-
cisely the incidence vectors of dependency trees
in Y(x); these are obtained by replacing Eq. 9 by
za ? B, a ? A. (11)
3.2 Arc-Factored Model
Given our polyhedral representation of (an outer
bound of) the arborescence polytope, we can
now formulate dependency parsing with an arc-
factored model as an ILP. By storing the arc-
local feature vectors into the columns of a matrix
F(x) , [fa(x)]a?A, and defining the score vec-
tor s , F(x)>w (each entry is an arc score) the
inference problem can be written as
max
y?Y(x)
w>f(x, y) = max
z?Z(x)
w>F(x)z
= max
z,?
s>z
s.t. A
[
z
?
]
? b
z ? B
(12)
whereA is a sparse constraint matrix (withO(|A|)
non-zero elements), and b is the constraint vec-
tor; A and b encode the constraints (4?9). This
is an ILP with O(|A|) variables and constraints
(hence, quadratic in n); if we drop the integer
constraint the problem becomes the LP relaxation.
As is, this formulation is no more attractive than
solving the problem with the existing combinato-
rial algorithms discussed in ?2.2; however, we can
now start adding non-local features to build a more
powerful model.
3.3 Sibling and Grandparent Features
To cope with higher-order features of the form
fa1,...,aK (x) (i.e., features whose values depend on
the simultaneous inclusion of arcs a1, . . . , aK on
345
a candidate dependency tree), we employ a lin-
earization trick (Boros and Hammer, 2002), defin-
ing extra variables za1...aK , za1 ? . . .?zaK . This
logical relation can be expressed by the following
O(K) agreement constraints:6
za1...aK ? zai , i = 1, . . . ,K
za1...aK ?
?K
i=1 zai ?K + 1. (13)
As shown by McDonald and Pereira (2006) and
Carreras (2007), the inclusion of features that
correlate sibling and grandparent arcs may be
highly beneficial, even if doing so requires resort-
ing to approximate algorithms.7 Define Rsibl ,
{?i, j, k? | ?i, j? ? A, ?i, k? ? A} and Rgrand ,
{?i, j, k? | ?i, j? ? A, ?j, k? ? A}. To include
such features in our formulation, we need to add
extra variables zsibl , ?zr?r?Rsibl and z
grand ,
?zr?r?Rgrand that indicate the presence of sibling
and grandparent arcs. Observe that these indica-
tor variables are conjunctions of arc indicator vari-
ables, i.e., zsiblijk = zij ? zik and z
grand
ijk = zij ? zjk.
Hence, these features can be handled in our formu-
lation by adding the following O(|A| ? |V |) vari-
ables and constraints:
zsiblijk ? zij , z
sibl
ijk ? zik, z
sibl
ijk ? zij + zik ? 1
(14)
for all triples ?i, j, k? ? Rsibl, and
zgrandijk ? zij , z
grand
ijk ? zjk, z
grand
ijk ? zij+zjk?1
(15)
for all triples ?i, j, k? ? Rgrand. Let R , A ?
Rsibl ? Rgrand; by redefining z , ?zr?r?R and
F(x) , [fr(x)]r?R, we may express our inference
problem as in Eq. 12, with O(|A| ? |V |) variables
and constraints.
Notice that the strategy just described to han-
dle sibling features is not fully compatible with
the features proposed by Eisner (1996) for pro-
jective parsing, as the latter correlate only con-
secutive siblings and are also able to place spe-
cial features on the first child of a given word.
The ability to handle such ?ordered? features is
intimately associated with Eisner?s dynamic pro-
gramming parsing algorithm and with the Marko-
vian assumptions made explicitly by his genera-
tive model. We next show how similar features
6Actually, any logical condition can be encoded with lin-
ear constraints involving binary variables; see e.g. Clarke and
Lapata (2008) for an overview.
7By sibling features we mean features that depend on
pairs of sibling arcs (i.e., of the form ?i, j? and ?i, k?); by
grandparent features we mean features that depend on pairs
of grandparent arcs (of the form ?i, j? and ?j, k?).
can be incorporated in our model by adding ?dy-
namic? constraints to our ILP. Define:
znext siblijk ,
?
??
??
1 if ?i, j? and ?i, k? are
consecutive siblings,
0 otherwise,
zfirst childij ,
{
1 if j is the first child of i,
0 otherwise.
Suppose (without loss of generality) that i < j <
k ? n. We could naively compose the constraints
(14) with additional linear constraints that encode
the logical relation
znext siblijk = z
sibl
ijk ?
?
j<l<k ?zil,
but this would yield a constraint matrix with
O(n4) non-zero elements. Instead, we define aux-
iliary variables ?jk and ?ij :
?jk =
{
1, if ?l s.t. pi(l) = pi(j) < j < l < k
0, otherwise,
?ij =
{
1, if ?k s.t. i < k < j and ?i, k? ? y
0, otherwise.
(16)
Then, we have that znext siblijk = z
sibl
ijk ? (??jk) and
zfirst childij = zij?(??ij), which can be encoded via
znext siblijk ? z
sibl
ijk z
first child
ij ? zij
znext siblijk ? 1 ? ?jk z
first child
ij ? 1 ? ?ij
znext siblijk ? z
sibl
ijk ? ?jk z
first child
ij ? zij ? ?ij
The following ?dynamic? constraints encode the
logical relations for the auxiliary variables (16):
?j(j+1) = 0 ?i(i+1) = 0
?j(k+1) ? ?jk ?i(j+1) ? ?ij
?j(k+1) ?
?
i<j
zsiblijk ?i(j+1) ? zij
?j(k+1) ? ?jk +
?
i<j
zsiblijk ?i(j+1) ? ?ij + zij
Auxiliary variables and constraints are defined
analogously for the case n ? i > j > k. This
results in a sparser constraint matrix, with only
O(n3) non-zero elements.
3.4 Valency Features
A crucial fact about dependency grammars is that
words have preferences about the number and ar-
rangement of arguments and modifiers they ac-
cept. Therefore, it is desirable to include features
346
that indicate, for a candidate arborescence, how
many outgoing arcs depart from each vertex; de-
note these quantities by vi ,
?
a??+(i) za, for
each i ? V . We call vi the valency of the ith ver-
tex. We add valency indicators zvalik , I(vi = k)
for i ? V and k = 0, . . . , n? 1. This way, we are
able to penalize candidate dependency trees that
assign unusual valencies to some of their vertices,
by specifying a individual cost for each possible
value of valency. The following O(|V |2) con-
straints encode the agreement between valency in-
dicators and the other variables:
?n?1
k=0 kz
val
ik =
?
a??+(i) za, i ? V (17)
?n?1
k=0 z
val
ik = 1, i ? V
zvalik ? 0, i ? V, k ? {0, . . . , n? 1}
3.5 Projectivity Features
For most languages, dependency parse trees tend
to be nearly projective (cf. Buchholz and Marsi,
2006). We wish to make our model capable of
learning to prefer ?nearly? projective parses when-
ever that behavior is observed in the data.
The multicommodity directed flow model of
Magnanti and Wolsey (1994) is a refinement of the
model described in ?3.1 which offers a compact
and elegant way to indicate nonprojective arcs, re-
quiring O(n3) variables and constraints. In this
model, every node k 6= 0 defines a commodity:
one unit of commodity k originates at the root
node and must be delivered to node k; the vari-
able ?kij denotes the flow of commodity k in arc
?i, j?. We first replace (4?9) by (18?22):
? The root sends one unit of commodity to each
node:
?
a???(0)
?ka ?
?
a??+(0)
?ka = ?1, k ? V \ {0} (18)
? Any node consumes its own commodity and no
other:
?
a???(j)
?ka ?
?
a??+(j)
?ka = ?
k
j , j, k ? V \ {0} (19)
where ?kj , I(j = k) is the Kronecker delta.
? Disabled arcs do not carry any flow:
?ka ? za, a ? A, k ? V (20)
? There are exactly n enabled arcs:
?
a?A za = n (21)
? All variables lie in the unit interval:
za ? U, ?ka ? U, a ? A, k ? V (22)
We next define auxiliary variables ?jk that indi-
cate if there is a path from j to k. Since each ver-
tex except the root has only one incoming arc, the
following linear equalities are enough to describe
these new variables:
?jk =
?
a???(j) ?
k
a, j, k ? V \ {0}
?0k = 1, k ? V \ {0}. (23)
Now, define indicators znp , ?znpa ?a?A, where
znpa , I(a ? y and a is nonprojective).
From the definition of projective arcs in ?2.1, we
have that znpa = 1 if and only if the arc is active
(za = 1) and there is some vertex k in the span of
a = ?i, j? such that ?ik = 0. We are led to the
following O(|A| ? |V |) constraints for ?i, j? ? A:
znpij ? zij
znpij ? zij ? ?ik, min(i, j) ? k ? max(i, j)
znpij ? ?
?max(i,j)?1
k=min(i,j)+1 ?ik + |j ? i| ? 1
There are other ways to introduce nonprojectiv-
ity indicators and alternative definitions of ?non-
projective arc.? For example, by using dynamic
constraints of the same kind as those in ?3.3,
we can indicate arcs that ?cross? other arcs with
O(n3) variables and constraints, and a cubic num-
ber of non-zero elements in the constraint matrix
(omitted for space).
3.6 Projective Parsing
It would be straightforward to adapt the con-
straints in ?3.5 to allow only projective parse trees:
simply force znpa = 0 for any a ? A. But there are
more efficient ways of accomplish this. While it is
difficult to impose projectivity constraints or cycle
constraints individually, there is a simpler way of
imposing both. Consider 3 (or 3?) from ?3.1.
Proposition 1 Replace condition 3 (or 3?) with
3??. If ?i, j? ? B, then, for any k = 1, . . . , n
such that k 6= j, the parent of k must satisfy
(defining i? , min(i, j) and j? , max(i, j)):
?
??
??
i? ? pi(k) ? j?, if i? < k < j?,
pi(k) < i? ? pi(k) > j?, if k < i? or k > j?
or k = i.
347
Then, Y(x) will be redefined as the set of projec-
tive dependency parse trees.
We omit the proof for space. Conditions 1, 2, and
3?? can be encoded with O(n2) constraints.
4 Experiments
We report experiments on seven languages, six
(Danish, Dutch, Portuguese, Slovene, Swedish
and Turkish) from the CoNLL-X shared task
(Buchholz and Marsi, 2006), and one (English)
from the CoNLL-2008 shared task (Surdeanu et
al., 2008).8 All experiments are evaluated using
the unlabeled attachment score (UAS), using the
default settings.9 We used the same arc-factored
features as McDonald et al (2005) (included in the
MSTParser toolkit10); for the higher-order models
described in ?3.3?3.5, we employed simple higher
order features that look at the word, part-of-speech
tag, and (if available) morphological information
of the words being correlated through the indica-
tor variables. For scalability (and noting that some
of the models require O(|V | ? |A|) constraints and
variables, which, when A = V 2, grows cubically
with the number of words), we first prune the base
graph by running a simple algorithm that ranks the
k-best candidate parents for each word in the sen-
tence (we set k = 10); this reduces the number of
candidate arcs to |A| = kn.11 This strategy is sim-
ilar to the one employed by Carreras et al (2008)
to prune the search space of the actual parser. The
ranker is a local model trained using a max-margin
criterion; it is arc-factored and not subject to any
structural constraints, so it is very fast.
The actual parser was trained via the online
structured passive-aggressive algorithm of Cram-
mer et al (2006); it differs from the 1-best MIRA
algorithm of McDonald et al (2005) by solv-
ing a sequence of loss-augmented inference prob-
lems.12 The number of iterations was set to 10.
The results are summarized in Table 1; for the
sake of comparison, we reproduced three strong
8We used the provided train/test splits except for English,
for which we tested on the development partition. For train-
ing, sentences longer than 80 words were discarded. For test-
ing, all sentences were kept (the longest one has length 118).
9
http://nextens.uvt.nl/?conll/software.html
10
http://sourceforge.net/projects/mstparser
11Note that, unlike reranking approaches, there are still ex-
ponentially many candidate parse trees after pruning. The
oracle constrained to pick parents from these lists achieves
> 98% in every case.
12The loss-augmented inference problem can also be ex-
pressed as an LP for Hamming loss functions that factor over
arcs; we refer to Martins et al (2009) for further details.
baselines, all of them state-of-the-art parsers based
on non-arc-factored models: the second order
model of McDonald and Pereira (2006), the hy-
brid model of Nivre and McDonald (2008), which
combines a (labeled) transition-based and a graph-
based parser, and a refinement of the latter, due
to Martins et al (2008), which attempts to ap-
proximate non-local features.13 We did not repro-
duce the model of Riedel and Clarke (2006) since
the latter is tailored for labeled dependency pars-
ing; however, experiments reported in that paper
for Dutch (and extended to other languages in the
CoNLL-X task) suggest that their model performs
worse than our three baselines.
By looking at the middle four columns, we can
see that adding non-arc-factored features makes
the models more accurate, for all languages. With
the exception of Portuguese, the best results are
achieved with the full set of features. We can
also observe that, for some languages, the valency
features do not seem to help. Merely modeling
the number of dependents of a word may not be
as valuable as knowing what kinds of dependents
they are (for example, distinguishing among argu-
ments and adjuncts).
Comparing with the baselines, we observe that
our full model outperforms that of McDonald and
Pereira (2006), and is in line with the most ac-
curate dependency parsers (Nivre and McDonald,
2008; Martins et al, 2008), obtained by com-
bining transition-based and graph-based parsers.14
Notice that our model, compared with these hy-
brid parsers, has the advantage of not requiring an
ensemble configuration (eliminating, for example,
the need to tune two parsers). Unlike the ensem-
bles, it directly handles non-local output features
by optimizing a single global objective. Perhaps
more importantly, it makes it possible to exploit
expert knowledge through the form of hard global
constraints. Although not pursued here, the same
kind of constraints employed by Riedel and Clarke
(2006) can straightforwardly fit into our model,
after extending it to perform labeled dependency
parsing. We believe that a careful design of fea-
13Unlike our model, the hybrid models used here as base-
lines make use of the dependency labels at training time; in-
deed, the transition-based parser is trained to predict a la-
beled dependency parse tree, and the graph-based parser use
these predicted labels as input features. Our model ignores
this information at training time; therefore, this comparison
is slightly unfair to us.
14See also Zhang and Clark (2008) for a different approach
that combines transition-based and graph-based methods.
348
[M
P0
6]
[N
M0
8]
[M
DS
X0
8]
AR
C-F
AC
TO
RE
D
+S
IB
L/G
RA
ND
P.
+V
AL
EN
CY
+P
RO
J. (
FU
LL
)
FU
LL
, R
EL
AX
ED
DANISH 90.60 91.30 91.54 89.80 91.06 90.98 91.18 91.04 (-0.14)
DUTCH 84.11 84.19 84.79 83.55 84.65 84.93 85.57 85.41 (-0.16)
PORTUGUESE 91.40 91.81 92.11 90.66 92.11 92.01 91.42 91.44 (+0.02)
SLOVENE 83.67 85.09 85.13 83.93 85.13 85.45 85.61 85.41 (-0.20)
SWEDISH 89.05 90.54 90.50 89.09 90.50 90.34 90.60 90.52 (-0.08)
TURKISH 75.30 75.68 76.36 75.16 76.20 76.08 76.34 76.32 (-0.02)
ENGLISH 90.85 ? ? 90.15 91.13 91.12 91.16 91.14 (-0.02)
Table 1: Results for nonprojective dependency parsing (unlabeled attachment scores). The three baselines are the second order
model of McDonald and Pereira (2006) and the hybrid models of Nivre and McDonald (2008) and Martins et al (2008). The
four middle columns show the performance of our model using exact (ILP) inference at test time, for increasing sets of features
(see ?3.2??3.5). The rightmost column shows the results obtained with the full set of features using relaxed LP inference
followed by projection onto the feasible set. Differences are with respect to exact inference for the same set of features. Bold
indicates the best result for a language. As for overall performance, both the exact and relaxed full model outperform the arc-
factored model and the second order model of McDonald and Pereira (2006) with statistical significance (p < 0.01) according
to Dan Bikel?s randomized method (http://www.cis.upenn.edu/?dbikel/software.html).
tures and constraints can lead to further improve-
ments on accuracy.
We now turn to a different issue: scalability. In
previous work (Martins et al, 2009), we showed
that training the model via LP-relaxed inference
(as we do here) makes it learn to avoid frac-
tional solutions; as a consequence, ILP solvers
will converge faster to the optimum (on average).
Yet, it is known from worst case complexity the-
ory that solving a general ILP is NP-hard; hence,
these solvers may not scale well with the sentence
length. Merely considering the LP-relaxed version
of the problem at test time is unsatisfactory, as it
may lead to a fractional solution (i.e., a solution
whose components indexed by arcs, z? = ?za?a?A,
are not all integer), which does not correspond to a
valid dependency tree. We propose the following
approximate algorithm to obtain an actual parse:
first, solve the LP relaxation (which can be done
in polynomial time with interior-point methods);
then, if the solution is fractional, project it onto the
feasible set Y(x). Fortunately, the Euclidean pro-
jection can be computed in a straightforward way
by finding a maximal arborescence in the directed
graph whose weights are defined by z? (we omit
the proof for space); as we saw in ?2.2, the Chu-
Liu-Edmonds algorithm can do this in polynomial
time. The overall parsing runtime becomes poly-
nomial with respect to the length of the sentence.
The last column of Table 1 compares the ac-
curacy of this approximate method with the ex-
act one. We observe that there is not a substantial
drop in accuracy; on the other hand, we observed
a considerable speed-up with respect to exact in-
ference, particularly for long sentences. The av-
erage runtime (across all languages) is 0.632 sec-
onds per sentence, which is in line with existing
higher-order parsers and is much faster than the
runtimes reported by Riedel and Clarke (2006).
5 Conclusions
We presented new dependency parsers based on
concise ILP formulations. We have shown how
non-local output features can be incorporated,
while keeping only a polynomial number of con-
straints. These features can act as soft constraints
whose penalty values are automatically learned
from data; in addition, our model is also compati-
ble with expert knowledge in the form of hard con-
straints. Learning through a max-margin frame-
work is made effective by the means of a LP-
relaxation. Experimental results on seven lan-
guages show that our rich-featured parsers outper-
form arc-factored and approximate higher-order
parsers, and are in line with stacked parsers, hav-
ing with respect to the latter the advantage of not
requiring an ensemble configuration.
Acknowledgments
The authors thank the reviewers for their com-
ments. Martins was supported by a grant from
FCT/ICTI through the CMU-Portugal Program,
and also by Priberam Informa?tica. Smith was
supported by NSF IIS-0836431 and an IBM Fac-
ulty Award. Xing was supported by NSF DBI-
0546594, DBI-0640543, IIS-0713379, and an Al-
fred Sloan Foundation Fellowship in Computer
Science.
349
References
E. Boros and P.L. Hammer. 2002. Pseudo-Boolean op-
timization. Discrete Applied Mathematics, 123(1?
3):155?225.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared
task on multilingual dependency parsing. In Proc.
of CoNLL.
X. Carreras, M. Collins, and T. Koo. 2008. TAG,
dynamic programming, and the perceptron for effi-
cient, feature-rich parsing. In Proc. of CoNLL.
X. Carreras. 2007. Experiments with a higher-order
projective dependency parser. In Proc. of CoNLL.
M. Chang, L. Ratinov, and D. Roth. 2008. Constraints
as prior knowledge. In ICML Workshop on Prior
Knowledge for Text and Language Processing.
Y. J. Chu and T. H. Liu. 1965. On the shortest arbores-
cence of a directed graph. Science Sinica, 14:1396?
1400.
J. Clarke and M. Lapata. 2008. Global inference
for sentence compression an integer linear program-
ming approach. JAIR, 31:399?429.
K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz,
and Y. Singer. 2006. Online passive-aggressive al-
gorithms. JMLR, 7:551?585.
A. Culotta and J. Sorensen. 2004. Dependency tree
kernels for relation extraction. In Proc. of ACL.
P. Denis and J. Baldridge. 2007. Joint determination
of anaphoricity and coreference resolution using in-
teger programming. In Proc. of HLT-NAACL.
Y. Ding and M. Palmer. 2005. Machine translation us-
ing probabilistic synchronous dependency insertion
grammar. In Proc. of ACL.
J. Edmonds. 1967. Optimum branchings. Journal
of Research of the National Bureau of Standards,
71B:233?240.
J. Eisner and G. Satta. 1999. Efficient parsing for
bilexical context-free grammars and head automaton
grammars. In Proc. of ACL.
J. Eisner. 1996. Three new probabilistic models for de-
pendency parsing: An exploration. In Proc. of COL-
ING.
S. Kahane, A. Nasr, and O. Rambow. 1998. Pseudo-
projectivity: a polynomially parsable non-projective
dependency grammar. In Proc. of COLING-ACL.
S. Lacoste-Julien, B. Taskar, D. Klein, and M. I. Jor-
dan. 2006. Word alignment via quadratic assign-
ment. In Proc. of HLT-NAACL.
T. L. Magnanti and L. A. Wolsey. 1994. Optimal
Trees. Technical Report 290-94, Massachusetts In-
stitute of Technology, Operations Research Center.
A. F. T. Martins, D. Das, N. A. Smith, and E. P. Xing.
2008. Stacking dependency parsers. In Proc. of
EMNLP.
A. F. T. Martins, N. A. Smith, and E. P. Xing. 2009.
Polyhedral outer approximations with application to
natural language parsing. In Proc. of ICML.
R. T. McDonald and F. C. N. Pereira. 2006. Online
learning of approximate dependency parsing algo-
rithms. In Proc. of EACL.
R. McDonald and G. Satta. 2007. On the complex-
ity of non-projective data-driven dependency pars-
ing. In Proc. of IWPT.
R. T. McDonald, F. Pereira, K. Ribarov, and J. Hajic?.
2005. Non-projective dependency parsing using
spanning tree algorithms. In Proc. of HLT-EMNLP.
J. Nivre and R. McDonald. 2008. Integrating graph-
based and transition-based dependency parsers. In
Proc. of ACL-HLT.
V. Punyakanok, D. Roth, W. Yih, and D. Zimak. 2004.
Semantic role labeling via integer linear program-
ming inference. In Proc. of COLING.
M. Richardson and P. Domingos. 2006. Markov logic
networks. Machine Learning, 62(1):107?136.
S. Riedel and J. Clarke. 2006. Incremental integer
linear programming for non-projective dependency
parsing. In Proc. of EMNLP.
R. T. Rockafellar. 1970. Convex Analysis. Princeton
University Press.
D. Roth and W. T. Yih. 2005. Integer linear program-
ming inference for conditional random fields. In
ICML.
A. Schrijver. 2003. Combinatorial Optimization:
Polyhedra and Efficiency, volume 24 of Algorithms
and Combinatorics. Springer.
D. A. Smith and J. Eisner. 2008. Dependency parsing
by belief propagation. In Proc. of EMNLP.
M. Surdeanu, R. Johansson, A. Meyers, L. Ma`rquez,
and J. Nivre. 2008. The conll-2008 shared task
on joint parsing of syntactic and semantic dependen-
cies. Proc. of CoNLL.
R. E. Tarjan. 1977. Finding optimum branchings. Net-
works, 7(1):25?36.
M. Wang, N. A. Smith, and T. Mitamura. 2007. What
is the Jeopardy model? A quasi-synchronous gram-
mar for QA. In Proceedings of EMNLP-CoNLL.
Y. Zhang and S. Clark. 2008. A tale of
two parsers: investigating and combining graph-
based and transition-based dependency parsing us-
ing beam-search. In Proc. of EMNLP.
350
Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 25?32,
Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005
Bilingual Word Spectral Clustering for Statistical Machine Translation
Bing Zhao? Eric P. Xing? ? Alex Waibel?
?Language Technologies Institute
?Center for Automated Learning and Discovery
Carnegie Mellon University
Pittsburgh, Pennsylvania 15213
{bzhao,epxing,ahw}@cs.cmu.edu
Abstract
In this paper, a variant of a spectral clus-
tering algorithm is proposed for bilingual
word clustering. The proposed algorithm
generates the two sets of clusters for both
languages efficiently with high seman-
tic correlation within monolingual clus-
ters, and high translation quality across
the clusters between two languages. Each
cluster level translation is considered as
a bilingual concept, which generalizes
words in bilingual clusters. This scheme
improves the robustness for statistical ma-
chine translation models. Two HMM-
based translation models are tested to use
these bilingual clusters. Improved per-
plexity, word alignment accuracy, and
translation quality are observed in our ex-
periments.
1 Introduction
Statistical natural language processing usually suf-
fers from the sparse data problem. Comparing to
the available monolingual data, we have much less
training data especially for statistical machine trans-
lation (SMT). For example, in language modelling,
there are more than 1.7 billion words corpora avail-
able: English Gigaword by (Graff, 2003). However,
for machine translation tasks, there are typically less
than 10 million words of training data.
Bilingual word clustering is a process of form-
ing corresponding word clusters suitable for ma-
chine translation. Previous work from (Wang et al,
1996) showed improvements in perplexity-oriented
measures using mixture-based translation lexicon
(Brown et al, 1993). A later study by (Och,
1999) showed improvements on perplexity of bilin-
gual corpus, and word translation accuracy using a
template-based translation model. Both approaches
are optimizing the maximum likelihood of parallel
corpus, in which a data point is a sentence pair: an
English sentence and its translation in another lan-
guage such as French. These algorithms are es-
sentially the same as monolingual word clusterings
(Kneser and Ney, 1993)?an iterative local search.
In each iteration, a two-level loop over every possi-
ble word-cluster assignment is tested for better like-
lihood change. This kind of approach has two draw-
backs: first it is easily to get stuck in local op-
tima; second, the clustering of English and the other
language are basically two separated optimization
processes, and cluster-level translation is modelled
loosely. These drawbacks make their approaches
generally not very effective in improving translation
models.
In this paper, we propose a variant of the spec-
tral clustering algorithm (Ng et al, 2001) for bilin-
gual word clustering. Given parallel corpus, first, the
word?s bilingual context is used directly as features
- for instance, each English word is represented by
its bilingual word translation candidates. Second,
latent eigenstructure analysis is carried out in this
bilingual feature space, which leads to clusters of
words with similar translations. Essentially an affin-
ity matrix is computed using these cross-lingual fea-
tures. It is then decomposed into two sub-spaces,
which are meaningful for translation tasks: the left
subspace corresponds to the representation of words
in English vocabulary, and the right sub-space cor-
responds to words in French. Each eigenvector is
considered as one bilingual concept, and the bilin-
gual clusters are considered to be its realizations in
two languages. Finally, a general K-means cluster-
25
ing algorithm is used to find out word clusters in the
two sub-spaces.
The remainder of the paper is structured as fol-
lows: in section 2, concepts of translation models
are introduced together with two extended HMMs;
in section 3, our proposed bilingual word cluster-
ing algorithm is explained in detail, and the related
works are analyzed; in section 4, evaluation metrics
are defined and the experimental results are given;
in section 5, the discussions and conclusions.
2 Statistical Machine Translation
The task of translation is to translate one sentence
in some source language F into a target language E.
For example, given a French sentence with J words
denoted as fJ1 = f1f2...fJ , an SMT system auto-
matically translates it into an English sentence with
I words denoted by eI1 = e1e2...eI . The SMT sys-
tem first proposes multiple English hypotheses in its
model space. Among all the hypotheses, the system
selects the one with the highest conditional proba-
bility according to Bayes?s decision rule:
e?I1 = argmax
{eI1}
P (eI1|fJ1 ) = argmax
{eI1}
P (fJ1 |eI1)P (eI1),
(1)
where P (fJ1 |eI1) is called translation model, and
P (eI1) is called language model. The translation
model is the key component, which is the focus in
this paper.
2.1 HMM-based Translation Model
HMM is one of the effective translation models (Vo-
gel et al, 1996), which is easily scalable to very
large training corpus.
To model word-to-word translation, we introduce
the mapping j ? aj , which assigns a French word
fj in position j to a English word ei in position
i = aj denoted as eaj . Each French word fj is
an observation, and it is generated by a HMM state
defined as [eaj , aj], where the alignment aj for po-
sition j is considered to have a dependency on the
previous alignment aj?1. Thus the first-order HMM
is defined as follows:
P (fJ1 |eI1) =
?
aJ1
J?
j=1
P (fj |eaj )P (aj |aj?1), (2)
where P (aj |aj?1) is the transition probability. This
model captures the assumption that words close in
the source sentence are aligned to words close in
the target sentence. An additional pseudo word of
?NULL? is used as the beginning of English sen-
tence for HMM to start with. The (Och and Ney,
2003) model includes other refinements such as spe-
cial treatment of a jump to a Null word, and a uni-
form smoothing prior. The HMM with these refine-
ments is used as our baseline. Motivated by the work
in both (Och and Ney, 2000) and (Toutanova et al,
2002), we propose the two following simplest ver-
sions of extended HMMs to utilize bilingual word
clusters.
2.2 Extensions to HMM with word clusters
Let F denote the cluster mapping fj ? F(fj), which
assigns French word fj to its cluster ID Fj = F(fj).
Similarly E maps English word ei to its cluster ID
of Ei = E(ei). In this paper, we assume each word
belongs to one cluster only.
With bilingual word clusters, we can extend the
HMM model in Eqn. 1 in the following two ways:
P (fJ1 |eI1) =
?
aJ1
?J
j=1 P (fj |eaj )?
P (aj |aj?1,E(eaj?1),F(fj?1)),
(3)
where E(eaj?1) and F(fj?1) are non overlapping
word clusters (Eaj?1 , Fj?1)for English and French
respectively.
Another explicit way of utilizing bilingual word
clusters can be considered as a two-stream HMM as
follows:
P (fJ1 , F J1 |eI1, EI1) =?
aJ1
?J
j=1 P (fj |eaj )P (Fj |Eaj )P (aj |aj?1).
(4)
This model introduces the translation of bilingual
word clusters directly as an extra factor to Eqn. 2.
Intuitively, the role of this factor is to boost the trans-
lation probabilities for words sharing the same con-
cept. This is a more expressive model because it
models both word and the cluster level translation
equivalence. Also, compared with the model in Eqn.
3, this model is easier to train, as it uses a two-
dimension table instead of a four-dimension table.
However, we do not want this P (Fj |Eaj ) to dom-
inate the HMM transition structure, and the obser-
26
vation probability of P (fj |eaj ) during the EM itera-
tions. Thus a uniform prior P (Fj) = 1/|F | is intro-
duced as a smoothing factor for P (Fj |Eaj ):
P (Fj |Eaj ) = ?P (Fj |Eaj ) + (1? ?)P (Fj), (5)
where |F | is the total number of word clusters in
French (we use the same number of clusters for both
languages). ? can be chosen to get optimal perfor-
mance on a development set. In our case, we fix it to
be 0.5 in all our experiments.
3 Bilingual Word Clustering
In bilingual word clustering, the task is to build word
clusters F and E to form partitions of the vocabular-
ies of the two languages respectively. The two par-
titions for the vocabularies of F and E are aimed to
be suitable for machine translation in the sense that
the cluster/partition level translation equivalence is
reliable and focused to handle data sparseness; the
translation model using these clusters explains the
parallel corpus {(fJ1 , eI1)} better in terms of perplex-
ity or joint likelihood.
3.1 From Monolingual to Bilingual
To infer bilingual word clusters of (F,E), one can
optimize the joint probability of the parallel corpus
{(fJ1 , eI1)} using the clusters as follows:
(F?, E?) = argmax
(F,E)
P (fJ1 , eI1|F,E)
= argmax
(F,E)
P (eI1|E)P (fJ1 |eI1, F, E).(6)
Eqn. 6 separates the optimization process into two
parts: the monolingual part for E, and the bilingual
part for F given fixed E. The monolingual part is
considered as a prior probability:P (eI1|E), and E can
be inferred using corpus bigram statistics in the fol-
lowing equation:
E? = argmax
{E}
P (eI1|E)
= argmax
{E}
I?
i=1
P (Ei|Ei?1)P (ei|Ei). (7)
We need to fix the number of clusters beforehand,
otherwise the optimum is reached when each word
is a class of its own. There exists efficient leave-one-
out style algorithm (Kneser and Ney, 1993), which
can automatically determine the number of clusters.
For the bilingual part P (fJ1 |eI1, F, E), we can
slightly modify the same algorithm as in (Kneser
and Ney, 1993). Given the word alignment {aJ1}
between fJ1 and eI1 collected from the Viterbi path
in HMM-based translation model, we can infer F? as
follows:
F? = argmax
{F}
P (fJ1 |eI1, F,E)
= argmax
{F}
J?
j=1
P (Fj |Eaj )P (fj |Fj). (8)
Overall, this bilingual word clustering algorithm is
essentially a two-step approach. In the first step, E
is inferred by optimizing the monolingual likelihood
of English data, and secondly F is inferred by op-
timizing the bilingual part without changing E. In
this way, the algorithm is easy to implement without
much change from the monolingual correspondent.
This approach was shown to give the best results
in (Och, 1999). We use it as our baseline to compare
with.
3.2 Bilingual Word Spectral Clustering
Instead of using word alignment to bridge the par-
allel sentence pair, and optimize the likelihood in
two separate steps, we develop an alignment-free al-
gorithm using a variant of spectral clustering algo-
rithm. The goal is to build high cluster-level trans-
lation quality suitable for translation modelling, and
at the same time maintain high intra-cluster similar-
ity , and low inter-cluster similarity for monolingual
clusters.
3.2.1 Notations
We define the vocabulary VF as the French vo-
cabulary with a size of |VF |; VE as the English vo-
cabulary with size of |VE |. A co-occurrence matrix
C{F,E} is built with |VF | rows and |VE | columns;
each element represents the co-occurrence counts of
the corresponding French word fj and English word
ei. In this way, each French word forms a row vec-
tor with a dimension of |VE |, and each dimensional-
ity is a co-occurring English word. The elements in
the vector are the co-occurrence counts. We can also
27
view each column as a vector for English word, and
we?ll have similar interpretations as above.
3.2.2 Algorithm
With C{F,E}, we can infer two affinity matrixes
as follows:
AE = CT{F,E}C{F,E}
AF = C{F,E}CT{F,E},
where AE is an |VE | ? |VE | affinity matrix for En-
glish words, with rows and columns representing
English words and each element the inner product
between two English words column vectors. Corre-
spondingly, AF is an affinity matrix of size |VF | ?
|VF | for French words with similar definitions. Both
AE and AF are symmetric and non-negative. Now
we can compute the eigenstructure for both AE and
AF . In fact, the eigen vectors of the two are corre-
spondingly the right and left sub-spaces of the orig-
inal co-occurrence matrix of C{F,E} respectively.
This can be computed using singular value decom-
position (SVD): C{F,E} = USV T , AE = V S2V T ,
and AF = US2UT , where U is the left sub-space,
and V the right sub-space of the co-occurrence ma-
trix C{F,E}. S is a diagonal matrix, with the singular
values ranked from large to small along the diagonal.
Obviously, the left sub-space U is the eigenstructure
for AF ; the right sub-space V is the eigenstructure
for AE .
By choosing the top K singular values (the square
root of the eigen values for both AE and AF ), the
sub-spaces will be reduced to: U|VF |?K and V|VE |?K
respectively. Based on these subspaces, we can carry
out K-means or other clustering algorithms to in-
fer word clusters for both languages. Our algorithm
goes as follows:
? Initialize bilingual co-occurrence matrix
C{F,E} with rows representing French words,
and columns English words. Cji is the co-
occurrence raw counts of French word fj and
English word ei;
? Form the affinity matrix AE = CT{F,E}C{F,E}
and AF = CT{F,E}C{F,E}. Kernels can also be
applied here such as AE = exp(
C{F,E}CT{F,E}
?2 )
for English words. Set AEii = 0 and AF ii = 0,
and normalize each row to be unit length;
? Compute the eigen structure of the normalized
matrix AE , and find the k largest eigen vectors:
v1, v2, ..., vk; Similarly, find the k largest eigen
vectors of AF : u1, u2, ..., uk;
? Stack the k eigenvectors of v1, v2, ..., vk in
the columns of YE , and stack the eigenvectors
u1, u2, ..., uk in the columns for YF ; Normalize
rows of both YE and YF to have unit length. YE
is size of |VE | ? k and YF is size of |VF | ? k;
? Treat each row of YE as a point in R|VE |?k, and
cluster them into K English word clusters us-
ing K-means. Treat each row of YF as a point in
R|VF |?k, and cluster them into K French word
clusters.
? Finally, assign original word ei to cluster Ek
if row i of the matrix YE is clustered as Ek;
similar assignments are for French words.
Here AE and AF are affinity matrixes of pair-wise
inner products between the monolingual words. The
more similar the two words, the larger the value.
In our implementations, we did not apply a kernel
function like the algorithm in (Ng et al, 2001). But
the kernel function such as the exponential func-
tion mentioned above can be applied here to control
how rapidly the similarity falls, using some carefully
chosen scaling parameter.
3.2.3 Related Clustering Algorithms
The above algorithm is very close to the variants
of a big family of the spectral clustering algorithms
introduced in (Meila and Shi, 2000) and studied in
(Ng et al, 2001). Spectral clustering refers to a class
of techniques which rely on the eigenstructure of
a similarity matrix to partition points into disjoint
clusters with high intra-cluster similarity and low
inter-cluster similarity. It?s shown to be computing
the k-way normalized cut: K ? trY TD? 12AD? 12Y
for any matrix Y ? RM?N . A is the affinity matrix,
and Y in our algorithm corresponds to the subspaces
of U and V .
Experimentally, it has been observed that using
more eigenvectors and directly computing a k-way
partitioning usually gives better performance. In our
implementations, we used the top 500 eigen vectors
to construct the subspaces of U and V for K-means
clustering.
28
3.2.4 K-means
The K-means here can be considered as a post-
processing step in our proposed bilingual word clus-
tering. For initial centroids, we first compute the
center of the whole data set. The farthest centroid
from the center is then chosen to be the first initial
centroid; and after that, the other K-1 centroids are
chosen one by one to well separate all the previous
chosen centroids.
The stopping criterion is: if the maximal change
of the clusters? centroids is less than the threshold of
1e-3 between two iterations, the clustering algorithm
then stops.
4 Experiments
To test our algorithm, we applied it to the TIDES
Chinese-English small data track evaluation test set.
After preprocessing, such as English tokenization,
Chinese word segmentation, and parallel sentence
splitting, there are in total 4172 parallel sentence
pairs for training. We manually labeled word align-
ments for 627 test sentence pairs randomly sampled
from the dry-run test data in 2001, which has four
human translations for each Chinese sentence. The
preprocessing for the test data is different from the
above, as it is designed for humans to label word
alignments correctly by removing ambiguities from
tokenization and word segmentation as much as pos-
sible. The data statistics are shown in Table 1.
English Chinese
Train
Sent. Pairs 4172
Words 133598 105331
Voc Size 8359 7984
Test
Sent. Pairs 627
Words 25500 19726
Voc Size 4084 4827
Unseen Voc Size 1278 1888
Alignment Links 14769
Table 1: Training and Test data statistics
4.1 Building Co-occurrence Matrix
Bilingual word co-occurrence counts are collected
from the training data for constructing the matrix
of C{F,E}. Raw counts are collected without word
alignment between the parallel sentences. Practi-
cally, we can use word alignment as used in (Och,
1999). Given an initial word alignment inferred by
HMM, the counts are collected from the aligned
word pair. If the counts are L-1 normalized, then
the co-occurrence matrix is essentially the bilingual
word-to-word translation lexicon such as P (fj |eaj ).
We can remove very small entries (P (f |e) ? 1e?7),
so that the matrix of C{F,E} is more sparse for eigen-
structure computation. The proposed algorithm is
then carried out to generate the bilingual word clus-
ters for both English and Chinese.
Figure 1 shows the ranked Eigen values for the
co-occurrence matrix of C{F,E}.
0 100 200 300 400 500 600 700 800 900 10000.5
1
1.5
2
2.5
3
3.5 Eigen values of affinity matrices
Top 1000 Eigen Values
Eige
n Va
lues
(a) co?occur counts from init word alignment(b) raw co?occur counts from data
Figure 1: Top-1000 Eigen Values of Co-occurrence
Matrix
It is clear, that using the initial HMM word align-
ment for co-occurrence matrix makes a difference.
The top Eigen value using word alignment in plot a.
(the deep blue curve) is 3.1946. The two plateaus
indicate how many top K eigen vectors to choose to
reduce the feature space. The first one indicates that
K is in the range of 50 to 120, and the second plateau
indicates K is in the range of 500 to 800. Plot b. is
inferred from the raw co-occurrence counts with the
top eigen value of 2.7148. There is no clear plateau,
which indicates that the feature space is less struc-
tured than the one built with initial word alignment.
We find 500 top eigen vectors are good enough
for bilingual clustering in terms of efficiency and ef-
fectiveness.
29
4.2 Clustering Results
Clusters built via the two described methods are
compared. The first method bil1 is the two-step op-
timization approach: first optimizing the monolin-
gual clusters for target language (English), and af-
terwards optimizing clusters for the source language
(Chinese). The second method bil2 is our proposed
algorithm to compute the eigenstructure of the co-
occurrence matrix, which builds the left and right
subspaces, and finds clusters in such spaces. Top
500 eigen vectors are used to construct these sub-
spaces. For both methods, 1000 clusters are inferred
for English and Chinese respectively. The number
of clusters is chosen in a way that the final word
alignment accuracy was optimal. Table 2 provides
the clustering examples using the two algorithms.
settings cluster examples
mono-E1 entirely,mainly,merely
mono-E2
10th,13th,14th,16th,17th,18th,19th
20th,21st,23rd,24th,26th
mono-E3 drink,anglophobia,carota,giant,gymnasium
bil1-C3 ?,d,?,?,??,yQ,y
bil2-E1 alcoholic cognac distilled drinkscotch spirits whiskey
bil2-C1 ??,?,,??,2,?y,
?h,7,??},6,?,,k
bil2-E2 evrec harmony luxury people sedan sedanstour tourism tourist toward travel
bil2-C2 ??,s?,?,?(,ff?,u?,
@q,@?,|,|?,-|
Table 2: Bilingual Cluster Examples
The monolingual word clusters often contain
words with similar syntax functions. This hap-
pens with esp. frequent words (eg. mono-E1 and
mono-E2). The algorithm tends to put rare words
such as ?carota, anglophobia? into a very big cluster
(eg. mono-E3). In addition, the words within these
monolingual clusters rarely share similar transla-
tions such as the typical cluster of ?week, month,
year?. This indicates that the corresponding Chi-
nese clusters inferred by optimizing Eqn. 7 are not
close in terms of translational similarity. Overall, the
method of bil1 does not give us a good translational
correspondence between clusters of two languages.
The English cluster of mono-E3 and its best aligned
candidate of bil1-C3 are not well correlated either.
Our proposed bilingual cluster algorithm bil2
generates the clusters with stronger semantic mean-
ing within a cluster. The cluster of bil2-E1 relates
to the concept of ?wine? in English. The mono-
lingual word clustering tends to scatter those words
into several big noisy clusters. This cluster also has a
good translational correspondent in bil2-C1 in Chi-
nese. The clusters of bil2-E2 and bil2-C2 are also
correlated very well. We noticed that the Chinese
clusters are slightly more noisy than their English
corresponding ones. This comes from the noise in
the parallel corpus, and sometimes from ambiguities
of the word segmentation in the preprocessing steps.
To measure the quality of the bilingual clusters,
we can use the following two kind of metrics:
? Average ?-mirror (Wang et al, 1996): The ?-
mirror of a class Ei is the set of clusters in
Chinese which have a translation probability
greater than ?. In our case, ? is 0.05, the same
value used in (Och, 1999).
? Perplexity: The perplexity is defined as pro-
portional to the negative log likelihood of the
HMM model Viterbi alignment path for each
sentence pair. We use the bilingual word clus-
ters in two extended HMM models, and mea-
sure the perplexities of the unseen test data af-
ter seven forward-backward training iterations.
The two perplexities are defined as PP1 =
exp(??Jj=1 log(P (fj |eaj )P (aj |aj?1, Eaj?1 ,
Fj?1))/J) and PP2 = exp(?J?1
?J
j=1 log(
P (fj |eaj )P (aj |aj?1)P (Fj?1|Eaj?1))) for the
two extended HMM models in Eqn 3 and 4.
Both metrics measure the extent to which the trans-
lation probability is spread out. The smaller the bet-
ter. The following table summarizes the results on
?-mirror and perplexity using different methods on
the unseen test data.
algorithms ?-mirror HMM-1 Perp HMM-2 Perp
baseline - 1717.82
bil1 3.97 1810.55 352.28
bil2 2.54 1610.86 343.64
The baseline uses no word clusters. bil1 and bil2
are defined as above. It is clear that our proposed
method gives overall lower perplexity: 1611 from
the baseline of 1717 using the extended HMM-1.
If we use HMM-2, the perplexity goes down even
more using bilingual clusters: 352.28 using bil1, and
343.64 using bil2. As stated, the four-dimensional
30
table of P (aj |aj?1, E(eaj?1), F (fj?1)) is easily
subject to overfitting, and usually gives worse per-
plexities.
Average ?-mirror for the two-step bilingual clus-
tering algorithm is 3.97, and for spectral cluster-
ing algorithm is 2.54. This means our proposed al-
gorithm generates more focused clusters of transla-
tional equivalence. Figure 2 shows the histogram for
the cluster pairs (Fj , Ei), of which the cluster level
translation probabilities P (Fj |Ei) ? [0.05, 1]. The
interval [0.05, 1] is divided into 10 bins, with first bin
[0.05, 0.1], and 9 bins divides[0.1, 1] equally. The
percentage for clusters pairs with P (Fj |Ei) falling
in each bin is drawn.
Histogram of (F,E) pairs with P(F|E) > 0.05 
0
0.1
0.2
0.3
0.4
0.5
0.6
0.05 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
Ten bins for P(F|E) ranging from [0.05, 1.0] 
spec-bi-clustering
two-step-bi-clustering
Figure 2: Histogram of cluster pairs (Fj , Ei)
Our algorithm generates much better aligned clus-
ter pairs than the two-step optimization algorithm.
There are 120 cluster pairs aligned with P (Fj |Ei) ?
0.9 using clusters from our algorithm, while there
are only 8 such cluster pairs using the two-step ap-
proach. Figure 3 compares the ?-mirror at different
numbers of clusters using the two approaches. Our
algorithm has a much better ?-mirror than the two-
step approach over different number of clusters.
Overall, the extended HMM-2 is better than
HMM-1 in terms of perplexity, and is easier to train.
4.3 Applications in Word Alignment
We also applied our bilingual word clustering in a
word alignment setting. The training data is the
TIDES small data track. The word alignments are
manually labeled for 627 sentences sampled from
the dryrun test data in 2001. In this manually
aligned data, we include one-to-one, one-to-many,
and many-to-many word alignments. Figure 4 sum-
marizes the word alignment accuracy for different
e-mirror over different settings
00.5
11.5
22.5
33.5
44.5
0 200 400 600 800 1000 1200 1400 1600 1800 2000
number of clusters
e-m
irro
r
BIL2: Co-occur raw countsBIL2: Co-occur counts from init word-alignBIL1: Two-step optimization
Figure 3: ?-mirror with different settings
methods. The baseline is the standard HMM trans-
lation model defined in Eqn. 2; the HMM1 is de-
fined in Eqn 3, and HMM2 is defined in Eqn 4. The
algorithm is applying our proposed bilingual word
clustering algorithm to infer 1000 clusters for both
languages. As expected, Figure 4 shows that using
F-measure of word alignment
38.00%
39.00%
40.00%
41.00%
42.00%
43.00%
44.00%
45.00%
1 2 3 4 5 6 7HMM Viterbi Iterations
F-m
ea
su
re
Baseline HMM
Extended HMM-1
Extended HMM-2
Figure 4: Word Alignment Over Iterations
word clusters is helpful for word alignment. HMM2
gives the best performance in terms of F-measure of
word alignment. One quarter of the words in the test
vocabulary are unseen as shown in Table 1. These
unseen words related alignment links (4778 out of
14769) will be left unaligned by translation models.
Thus the oracle (best possible) recall we could get
is 67.65%. Our standard t-test showed that signifi-
cant interval is 0.82% at the 95% confidence level.
The improvement at the last iteration of HMM is
marginally significant.
4.4 Applications in Phrase-based Translations
Our pilot word alignment on unseen data showed
improvements. However, we find it more effective
in our phrase extraction, in which three key scores
31
are computed: phrase level fertilities, distortions,
and lexicon scores. These scores are used in a lo-
cal greedy search to extract phrase pairs (Zhao and
Vogel, 2005). This phrase extraction is more sen-
sitive to the differences in P (fj |ei) than the HMM
Viterbi word aligner.
The evaluation conditions are defined in NIST
2003 Small track. Around 247K test set (919 Chi-
nese sentences) specific phrase pairs are extracted
with up to 7-gram in source phrase. A trigram
language model is trained using Gigaword XinHua
news part. With a monotone phrase-based decoder,
the translation results are reported in Table 3. The
Eval. Baseline Bil1 Bil2
NIST 6.417 6.507 6.582
BLEU 0.1558 0.1575 0.1644
Table 3: NIST?03 C-E Small Data Track Evaluation
baseline is using the lexicon P (fj |ei) trained from
standard HMM in Eqn. 2, which gives a BLEU
score of 0.1558 +/- 0.0113. Bil1 and Bil2 are using
P (fj |ei) from HMM in Eqn. 4 with 1000 bilingual
word clusters inferred from the two-step algorithm
and the proposed one respectively. Using the clus-
ters from the two-step algorithm gives a BLEU score
of 0.1575, which is close to the baseline. Using clus-
ters from our algorithm, we observe more improve-
ments with BLEU score of 0.1644 and a NIST score
of 6.582.
5 Discussions and Conclusions
In this paper, a new approach for bilingual word
clustering using eigenstructure in bilingual feature
space is proposed. Eigenvectors from this feature
space are considered as bilingual concepts. Bilin-
gual clusters from the subspaces expanded by these
concepts are inferred with high semantic correla-
tions within each cluster, and high translation quali-
ties across clusters from the two languages.
Our empirical study also showed effectiveness of
using bilingual word clusters in extended HMMs for
statistical machine translation. The K-means based
clustering algorithm can be easily extended to do hi-
erarchical clustering. However, extensions of trans-
lation models are needed to leverage the hierarchical
clusters appropriately.
References
P.F. Brown, Stephen A. Della Pietra, Vincent. J.
Della Pietra, and Robert L. Mercer. 1993. The mathe-
matics of statistical machine translation: Parameter es-
timation. In Computational Linguistics, volume 19(2),
pages 263?331.
David Graff. 2003. Ldc gigaword corpora: English gi-
gaword (ldc catalog no: Ldc2003t05). In LDC link:
http://www.ldc.upenn.edu/Catalog/index.jsp.
R. Kneser and Hermann Ney. 1993. Improved clus-
tering techniques for class-based statistical language
modelling. In European Conference on Speech Com-
munication and Technology, pages 973?976.
Marina Meila and Jianbo Shi. 2000. Learning segmenta-
tion by random walks. In Advances in Neural Informa-
tion Processing Systems. (NIPS2000), pages 873?879.
A. Ng, M. Jordan, and Y. Weiss. 2001. On spectral
clustering: Analysis and an algorithm. In Advances in
Neural Information Processing Systems 14: Proceed-
ings of the 2001.
Franz J. Och and Hermann Ney. 2000. A comparison of
alignment models for statistical machine translation.
In COLING?00: The 18th Int. Conf. on Computational
Linguistics, pages 1086?1090, Saarbrucken, Germany,
July.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models. In
Computational Linguistics, volume 29, pages 19?51.
Franz J. Och. 1999. An efficient method for determin-
ing bilingal word classes. In Ninth Conf. of the Europ.
Chapter of the Association for Computational Linguis-
tics (EACL?99), pages 71?76.
Kristina Toutanova, H. Tolga Ilhan, and Christopher D.
Manning. 2002. Extensions to hmm-based statistical
word alignment models. In Proc. of the Conference on
Empirical Methods in Natural Language Processing.
S. Vogel, Hermann Ney, and C. Tillmann. 1996. Hmm
based word alignment in statistical machine transla-
tion. In Proc. The 16th Int. Conf. on Computational
Lingustics, (Coling?96), pages 836?841.
Yeyi Wang, John Lafferty, and Alex Waibel. 1996.
Word clustering with parallel spoken language cor-
pora. In proceedings of the 4th International Con-
ference on Spoken Language Processing (ICSLP?96),
pages 2364?2367.
Bing Zhao and Stephan Vogel. 2005. A generalized
alignment-free phrase extraction algorithm. In ACL
2005 Workshop: Building and Using Parallel Cor-
pora: Data-driven Machine Translation and Beyond,
Ann Arbor, Michigan.
32
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 34?44,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Turbo Parsers: Dependency Parsing by Approximate Variational Inference
Andre? F. T. Martins?? Noah A. Smith? Eric P. Xing?
?School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{afm,nasmith,epxing}@cs.cmu.edu
Pedro M. Q. Aguiar?
?Instituto de Sistemas e Robo?tica
Instituto Superior Te?cnico
Lisboa, Portugal
aguiar@isr.ist.utl.pt
Ma?rio A. T. Figueiredo?
?Instituto de Telecomunicac?o?es
Instituto Superior Te?cnico
Lisboa, Portugal
mtf@lx.it.pt
Abstract
We present a unified view of two state-of-the-
art non-projective dependency parsers, both
approximate: the loopy belief propagation
parser of Smith and Eisner (2008) and the re-
laxed linear program of Martins et al (2009).
By representing the model assumptions with
a factor graph, we shed light on the optimiza-
tion problems tackled in each method. We also
propose a new aggressive online algorithm to
learn the model parameters, which makes use
of the underlying variational representation.
The algorithm does not require a learning rate
parameter and provides a single framework for
a wide family of convex loss functions, includ-
ing CRFs and structured SVMs. Experiments
show state-of-the-art performance for 14 lan-
guages.
1 Introduction
Feature-rich discriminative models that break local-
ity/independence assumptions can boost a parser?s
performance (McDonald et al, 2006; Huang, 2008;
Finkel et al, 2008; Smith and Eisner, 2008; Martins
et al, 2009; Koo and Collins, 2010). Often, infer-
ence with such models becomes computationally in-
tractable, causing a demand for understanding and
improving approximate parsing algorithms.
In this paper, we show a formal connection be-
tween two recently-proposed approximate inference
techniques for non-projective dependency parsing:
loopy belief propagation (Smith and Eisner, 2008)
and linear programming relaxation (Martins et al,
2009). While those two parsers are differently moti-
vated, we show that both correspond to inference in
a factor graph, and both optimize objective functions
over local approximations of the marginal polytope.
The connection is made clear by writing the explicit
declarative optimization problem underlying Smith
and Eisner (2008) and by showing the factor graph
underlying Martins et al (2009). The success of
both approaches parallels similar approximations in
other fields, such as statistical image processing and
error-correcting coding. Throughtout, we call these
turbo parsers.1
Our contributions are not limited to dependency
parsing: we present a general method for inference
in factor graphs with hard constraints (?2), which
extends some combinatorial factors considered by
Smith and Eisner (2008). After presenting a geo-
metric view of the variational approximations un-
derlying message-passing algorithms (?3), and clos-
ing the gap between the two aforementioned parsers
(?4), we consider the problem of learning the model
parameters (?5). To this end, we propose an ag-
gressive online algorithm that generalizes MIRA
(Crammer et al, 2006) to arbitrary loss functions.
We adopt a family of losses subsuming CRFs (Laf-
ferty et al, 2001) and structured SVMs (Taskar et
al., 2003; Tsochantaridis et al, 2004). Finally, we
present a technique for including features not at-
tested in the training data, allowing for richer mod-
els without substantial runtime costs. Our experi-
ments (?6) show state-of-the-art performance on de-
pendency parsing benchmarks.
1The name stems from ?turbo codes,? a class of high-
performance error-correcting codes introduced by Berrou et al
(1993) for which decoding algorithms are equivalent to running
belief propagation in a graph with loops (McEliece et al, 1998).
34
2 Structured Inference and Factor Graphs
Denote by X a set of input objects from which we
want to infer some hidden structure conveyed in an
output set Y. Each input x ? X (e.g., a sentence)
is associated with a set of candidate outputs Y(x) ?
Y (e.g., parse trees); we are interested in the case
where Y(x) is a large structured set.
Choices about the representation of elements of
Y(x) play a major role in algorithm design. In
many problems, the elements of Y(x) can be rep-
resented as discrete-valued vectors of the form y =
?y1, . . . , yI?, each yi taking values in a label set Yi.
For example, in unlabeled dependency parsing, I is
the number of candidate dependency arcs (quadratic
in the sentence length), and each Yi = {0, 1}. Of
course, the yi are highly interdependent.
Factor Graphs. Probabilistic models like CRFs
(Lafferty et al, 2001) assume a factorization of the
conditional distribution of Y ,
Pr(Y = y | X = x) ?
?
C?C ?C(x,yC), (1)
where each C ? {1, . . . , I} is a factor, C is the set
of factors, each yC , ?yi?i?C denotes a partial out-
put assignment, and each ?C is a nonnegative po-
tential function that depends on the output only via
its restriction to C. A factor graph (Kschischang
et al, 2001) is a convenient representation for the
factorization in Eq. 1: it is a bipartite graph Gx com-
prised of variable nodes {1, . . . , I} and factor nodes
C ? C, with an edge connecting the ith variable
node and a factor node C iff i ? C. Hence, the fac-
tor graph Gx makes explicit the direct dependencies
among the variables {y1, . . . , yI}.
Factor graphs have been used for several NLP
tasks, such as dependency parsing, segmentation,
and co-reference resolution (Sutton et al, 2007;
Smith and Eisner, 2008; McCallum et al, 2009).
Hard and Soft Constraint Factors. It may be
the case that valid outputs are a proper subset of
Y1 ? ? ? ? ? YI?for example, in dependency pars-
ing, the entries of the output vector y must jointly
define a spanning tree. This requires hard constraint
factors that rule out forbidden partial assignments
by mapping them to zero potential values. See Ta-
ble 1 for an inventory of hard constraint factors used
in this paper. Factors that are not of this special kind
are called soft factors, and have strictly positive po-
tentials. We thus have a partition C = Chard ? Csoft.
We let the soft factor potentials take the form
?C(x,yC) , exp(?>?C(x,yC)), where ? ? R
d
is a vector of parameters (shared across factors) and
?C(x,yC) is a local feature vector. The conditional
distribution of Y (Eq. 1) thus becomes log-linear:
Pr?(y|x) = Zx(?)
?1 exp(?>?(x,y)), (2)
where Zx(?) ,
?
y??Y(x) exp(?
>?(x,y?)) is the
partition function, and the features decompose as:
?(x,y) ,
?
C?Csoft
?C(x,yC). (3)
Dependency Parsing. Smith and Eisner (2008)
proposed a factor graph representation for depen-
dency parsing (Fig. 1). The graph has O(n2) vari-
able nodes (n is the sentence length), one per candi-
date arc a , ?h,m? linking a head h and modifier
m. Outputs are binary, with ya = 1 iff arc a belongs
to the dependency tree. There is a hard factor TREE
connected to all variables, that constrains the overall
arc configurations to form a spanning tree. There is a
unary soft factor per arc, whose log-potential reflects
the score of that arc. There are also O(n3) pair-
wise factors; their log-potentials reflect the scores
of sibling and grandparent arcs. These factors cre-
ate loops, thus calling for approximate inference.
Without them, the model is arc-factored, and ex-
act inference in it is well studied: finding the most
probable parse tree takes O(n3) time with the Chu-
Liu-Edmonds algorithm (McDonald et al, 2005),2
and computing posterior marginals for all arcs takes
O(n3) time via the matrix-tree theorem (Smith and
Smith, 2007; Koo et al, 2007).
Message-passing algorithms. In general
factor graphs, both inference problems?
obtaining the most probable output (the MAP)
argmaxy?Y(x) Pr?(y|x), and computing the
marginals Pr?(Yi = yi|x)?can be addressed
with the belief propagation (BP) algorithm (Pearl,
1988), which iteratively passes messages between
variables and factors reflecting their local ?beliefs.?
2There is a faster but more involvedO(n2) algorithm due to
Tarjan (1977).
35
A general binary factor: ?C(v1, . . . , vn) =
?
1 v1, . . . , vn ? SC
0 otherwise,
where SC ? {0, 1}n.
?Message-induced distribution: ? , ?mj?C?j=1,...,n ? Partition function: ZC(?) ,
P
?v1,...,vn??SC
Qn
i=1m
vi
i?C
?Marginals: MARGi(?) , Pr?{Vi = 1|?V1, . . . , Vn? ? SC} ?Max-marginals: MAX-MARGi,b(?) , maxv?SC Pr?(v|vi = b)
? Sum-prod.: mC?i = m
?1
i?C ? MARGi(?)/(1? MARGi(?)) ?Max-prod.: mC?i = m
?1
i?C ? MAX-MARGi,1(?)/MAX-MARGi,0(?)
? Local agreem. constr.: z ? conv SC , where z = ??i(1)?ni=1 ? Entropy: HC = logZC(?)?
Pn
i=1 MARGi(?) logmi?C
TREE ?TREE(?ya?a?A) =
?
1 y ? Ytree (i.e., {a ? A | ya = 1} is a directed spanning tree)
0 otherwise,
where A is the set of candidate arcs.
? Partition function Ztree(?) and marginals ?MARGa(?)?a?A computed via the matrix-tree theorem, with ? , ?ma?TREE?a?A
? Sum-prod.: mTREE?a = m
?1
a?TREE ? MARGa(?)/(1? MARGa(?))
?Max-prod.: mTREE?a = m
?1
a?TREE ? MAX-MARGa,1(?)/MAX-MARGa,0(?), where MAX-MARGa,b(?) , maxy?Ytree Pr?(y|ya = b)
? Local agreem. constr.: z ? Ztree, where Ztree , convYtree is the arborescence polytope
? Entropy: Htree = logZtree(?)?
P
a?A MARGa(?) logma?TREE
XOR (?one-hot?) ?XOR(v1, . . . , vn) =
?
1
Pn
i=1 vi = 1
0 otherwise.
? Sum-prod.: mXOR?i =
?P
j 6=imj?XOR
??1
?Max-prod.: mXOR?i =
`
maxj 6=imj?XOR
??1
? Local agreem. constr.:
P
i zi = 1, zi ? [0, 1],?i ?HXOR = ?
P
i(mi?XOR/
P
j mj?XOR) log(mi?XOR/
P
j mj?XOR)
OR ?OR(v1, . . . , vn) =
?
1
Pn
i=1 vi ? 1
0 otherwise.
? Sum-prod.: mOR?i =
?
1?
Q
j 6=i(1 +mj?OR)
?1
??1
?Max-prod.: mOR?i = max{1,minj 6=im
?1
j?OR}
? Local agreem. constr.:
P
i zi ? 1, zi ? [0, 1],?i
OR-WITH-OUTPUT ?OR-OUT(v1, . . . , vn) =
?
1 vn =
Wn?1
i=1 vi
0 otherwise.
? Sum-prod.: mOR-OUT?i =
( ?
1? (1?m?1n?OR-OUT)
Q
j 6=i,n(1 +mj?OR-OUT)
?1
??1
i < n
Q
j 6=n(1 +mj?OR-OUT)? 1 i = n.
?Max-prod.: mOR-OUT?i =
(
min mn?OR-OUT
Q
j 6=i,n max{1,mj?OR-OUT},max{1,minj 6=i,nm
?1
j?OR-OUT}
o
i < n
Q
j 6=n max{1,mj?OR-OUT}min{1,maxj 6=nmj?OR-OUT} i = n.
Table 1: Hard constraint factors, their potentials, messages, and entropies. The top row shows expressions for a
general binary factor: each outgoing message is computed from incoming marginals (in the sum-product case), or
max-marginals (in the max-product case); the entropy of the factor (see ?3) is computed from these marginals and the
partition function; the local agreement constraints (?4) involve the convex hull of the set SC of allowed configurations
(see footnote 5). The TREE, XOR, OR and OR-WITH-OUTPUT factors allow tractable computation of all these quantities
(rows 2?5). Two of these factors (TREE and XOR) had been proposed by Smith and Eisner (2008); we provide further
information (max-product messages, entropies, and local agreement constraints). Factors OR and OR-WITH-OUTPUT
are novel to the best of our knowledge. This inventory covers many cases, since the above formulae can be extended
to the case where some inputs are negated: just replace the corresponding messages by their reciprocal, vi by 1? vi,
etc. This allows building factors NAND (an OR factor with negated inputs), IMPLY (a 2-input OR with the first input
negated), and XOR-WITH-OUTPUT (an XOR factor with the last input negated).
In sum-product BP, the messages take the form:3
Mi?C(yi) ?
?
D 6=CMD?i(yi) (4)
MC?i(yi) ?
?
yC?yi
?C(yC)
?
j 6=iMj?C(yj). (5)
In max-product BP, the summation in Eq. 5 is re-
placed by a maximization. Upon convergence, vari-
able and factor beliefs are computed as:
?i(yi) ?
?
CMC?i(yi) (6)
?C(yC) ? ?C(yC)
?
iMi?C(yi). (7)
BP is exact when the factor graph is a tree: in the
sum-product case, the beliefs in Eqs. 6?7 correspond
3We employ the standard ? notation, where a summa-
tion/maximization indexed by yC ? yi means that it is over
all yC with the i-th component held fixed and set to yi.
to the true marginals, and in the max-product case,
maximizing each ?i(yi) yields the MAP output. In
graphs with loops, BP is an approximate method, not
guaranteed to converge, nicknamed loopy BP. We
highlight a variational perspective of loopy BP in ?3;
for now we consider algorithmic issues. Note that
computing the factor-to-variable messages for each
factorC (Eq. 5) requires a summation/maximization
over exponentially many configurations. Fortu-
nately, for all the hard constraint factors in rows 3?5
of Table 1, this computation can be done in linear
time (and polynomial for the TREE factor)?this ex-
tends results presented in Smith and Eisner (2008).4
4The insight behind these speed-ups is that messages on
binary-valued potentials can be expressed as MC?i(yi) ?
36
TREE
1
ARC
(T,RE)
SIB
(T1RE1RE)1 2
SIB
(T1RE1RE)1 3
SIB
(T1RE1RE)2 3GRAND
(A,T1RE)1
2
ARC
(T,RE) 3
ARC
(T,RE)ARC(A,T)
Figure 1: Factor graph corresponding to the dependency
parsing model of Smith and Eisner (2008) with sibling
and grandparent features. Circles denote variable nodes,
and squares denote factor nodes. Note the loops created
by the inclusion of pairwise factors (GRAND and SIB).
In Table 1 we present closed-form expressions
for the factor-to-variable message ratios mC?i ,
MC?i(1)/MC?i(0) in terms of their variable-to-
factor counterparts mi?C , Mi?C(1)/Mi?C(0);
these ratios are all that is necessary when the vari-
ables are binary. Detailed derivations are presented
in an extended version of this paper (Martins et al,
2010b).
3 Variational Representations
Let Px , {Pr?(.|x) | ? ? Rd} be the family of all
distributions of the form in Eq. 2. We next present
an alternative parametrization for the distributions in
Px in terms of factor marginals. We will see that
each distribution can be seen as a point in the so-
called marginal polytope (Wainwright and Jordan,
2008); this will pave the way for the variational rep-
resentations to be derived next.
Parts and Output Indicators. A part is a pair
?C,yC?, where C is a soft factor and yC a partial
output assignment. We let R = {?C,yC? | C ?
Csoft,yC ?
?
i?C Yi} be the set of all parts. Given
an output y? ? Y(x), a part ?C,yC? is said to be ac-
tive if it locally matches the output, i.e., if yC = y?C .
Any output y? ? Y(x) can be mapped to a |R|-
dimensional binary vector ?(y?) indicating which
parts are active, i.e., [?(y?)]?C,yC? = 1 if yC = y
?
C
Pr{?C(YC) = 1|Yi = yi} and MC?i(yi) ?
max?C(yC)=1 Pr{YC = yC |Yi = yi}, respectively for the
sum-product and max-product cases; these probabilities are in-
duced by the messages in Eq. 4: for an event A ?
Q
i?C Yi,
Pr{YC ? A} ,
P
yC
I(yC ? A)
Q
i?CMi?C(yi).
and 0 otherwise; ?(y?) is called the output indicator
vector. This mapping allows decoupling the feature
vector in Eq. 3 as the product of an input matrix and
an output vector:
?(x,y) =
?
C?Csoft
?C(x,yC) = F(x)?(y), (8)
where F(x) is a d-by-|R| matrix whose columns
contain the part-local feature vectors ?C(x,yC).
Observe, however, that not every vector in {0, 1}|R|
corresponds necessarily to a valid output in Y(x).
Marginal Polytope. Moving to vector representa-
tions of outputs leads naturally to a geometric view
of the problem. The marginal polytope is the convex
hull5 of all the ?valid? output indicator vectors:
M(Gx) , conv{?(y) | y ? Y(x)}.
Note that M(Gx) only depends on the factor graph
Gx and the hard constraints (i.e., it is independent of
the parameters ?). The importance of the marginal
polytope stems from two facts: (i) each vertex of
M(Gx) corresponds to an output in Y(x); (ii) each
point in M(Gx) corresponds to a vector of marginal
probabilities that is realizable by some distribution
(not necessarily in Px) that factors according to Gx.
Variational Representations. We now describe
formally how the points in M(Gx) are linked to the
distributions in Px. We extend the ?canonical over-
complete parametrization? case, studied by Wain-
wright and Jordan (2008), to our scenario (common
in NLP), where arbitrary features are allowed and
the parameters are tied (shared by all factors). Let
H(Pr?(.|x)) , ?
?
y?Y(x) Pr?(y|x) log Pr?(y|x)
denote the entropy of Pr?(.|x), and E?[.] the ex-
pectation under Pr?(.|x). The component of ? ?
M(Gx) indexed by part ?C,yC? is denoted ?C(yC).
Proposition 1. There is a map coupling each distri-
bution Pr?(.|x) ? Px to a unique ? ? M(Gx) such
that E?[?(Y )] = ?. Define H(?) , H(Pr?(.|x))
if some Pr?(.|x) is coupled to ?, and H(?) = ??
if no such Pr?(.|x) exists. Then:
1. The following variational representation for the
log-partition function (mentioned in Eq. 2) holds:
logZx(?) = max
??M(Gx)
?>F(x)? +H(?). (9)
5The convex hull of {z1, . . . , zk} is the set of points that can
be written as
Pk
i=1 ?izi, where
Pk
i=1 ?i = 1 and each ?i ? 0.
37
Parameter?space Factor?log-potentials?
space???????
Marginal?polytope?
Figure 2: Dual parametrization of the distributions in
Px. Our parameter space (left) is first linearly mapped to
the space of factor log-potentials (middle). The latter is
mapped to the marginal polytope M(Gx) (right). In gen-
eral only a subset of M(Gx) is reachable from our param-
eter space. Any distribution in Px can be parametrized by
a vector ? ? Rd or by a point ? ?M(Gx).
2. The problem in Eq. 9 is convex and its solution
is attained at the factor marginals, i.e., there is a
maximizer ?? s.t. ??C(yC) = Pr?(YC = yC |x)
for each C ? C. The gradient of the log-partition
function is? logZx(?) = F(x)??.
3. The MAP y? , argmaxy?Y(x) Pr?(y|x) can be
obtained by solving the linear program
?? , ?(y?) = argmax
??M(Gx)
?>F(x)?. (10)
A proof of this proposition can be found in Mar-
tins et al (2010a). Fig. 2 provides an illustration of
the dual parametrization implied by Prop. 1.
4 Approximate Inference & Turbo Parsing
We now show how the variational machinery just
described relates to message-passing algorithms and
provides a common framework for analyzing two re-
cent dependency parsers. Later (?5), Prop. 1 is used
constructively for learning the model parameters.
4.1 Loopy BP as a Variational Approximation
For general factor graphs with loops, the marginal
polytope M(Gx) cannot be compactly specified and
the entropy term H(?) lacks a closed form, render-
ing exact optimizations in Eqs. 9?10 intractable. A
popular approximate algorithm for marginal infer-
ence is sum-product loopy BP, which passes mes-
sages as described in ?2 and, upon convergence,
computes beliefs via Eqs. 6?7. Were loopy BP exact,
these beliefs would be the true marginals and hence
a point in the marginal polytope M(Gx). However,
this need not be the case, as elucidated by Yedidia et
al. (2001) and others, who first analyzed loopy BP
from a variational perspective. The following two
approximations underlie loopy BP:
? The marginal polytope M(Gx) is approximated by
the local polytope L(Gx). This is an outer bound;
its name derives from the fact that it only imposes
local agreement constraints ?i, yi ? Yi, C ? C:
?
yi
?i(yi) = 1,
?
yC?yi
?C(yC) = ?i(yi). (11)
Namely, it is characterized by L(Gx) , {? ?
R|R|+ | Eq. 11 holds ?i, yi ? Yi, C ? C}. The
elements of L(Gx) are called pseudo-marginals.
Clearly, the true marginals satisfy Eq. 11, and
therefore M(Gx) ? L(Gx).
? The entropy H is replaced by its Bethe approx-
imation HBethe(? ) ,
?I
i=1(1 ? di)H(? i) +?
C?CH(?C), where di = |{C | i ? C}| is the
number of factors connected to the ith variable,
H(? i) , ?
?
yi
?i(yi) log ?i(yi) and H(?C) ,
?
?
yC
?C(yC) log ?C(yC).
Any stationary point of sum-product BP is a lo-
cal optimum of the variational problem in Eq. 9
with M(Gx) replaced by L(Gx) and H replaced by
HBethe (Yedidia et al, 2001). Note however that
multiple optima may exist, since HBethe is not nec-
essarily concave, and that BP may not converge.
Table 1 shows closed form expressions for the
local agreement constraints and entropies of some
hard-constraint factors, obtained by invoking Eq. 7
and observing that ?C(yC) must be zero if configu-
ration yC is forbidden. See Martins et al (2010b).
4.2 Two Dependency Turbo Parsers
We next present our main contribution: a formal
connection between two recent approximate depen-
dency parsers, which at first sight appear unrelated.
Recall that (i) Smith and Eisner (2008) proposed a
factor graph (Fig. 1) in which they run loopy BP,
and that (ii) Martins et al (2009) approximate pars-
ing as the solution of a linear program. Here, we
fill the blanks in the two approaches: we derive ex-
plicitly the variational problem addressed in (i) and
we provide the underlying factor graph in (ii). This
puts the two approaches side-by-side as approximate
methods for marginal and MAP inference. Since
both rely on ?local? approximations (in the sense
38
of Eq. 11) that ignore the loops in their graphical
models, we dub them turbo parsers by analogy with
error-correcting turbo decoders (see footnote 1).
Turbo Parser #1: Sum-Product Loopy BP. The
factor graph depicted in Fig. 1?call it Gx?includes
pairwise soft factors connecting sibling and grand-
parent arcs.6 We next characterize the local polytope
L(Gx) and the Bethe approximationHBethe inherent
in Smith and Eisner?s loopy BP algorithm.
Let A be the set of candidate arcs, and P ?
A2 the set of pairs of arcs that have factors. Let
? = ??A, ?P ? with ?A = ??a?a?A and ?P =
??ab??a,b??P . Since all variables are binary, we may
write, for each a ? A, ?a(1) = za and ?a(0) =
1 ? za, where za is a variable constrained to [0, 1].
Let zA , ?za?a?A; the local agreement constraints
at the TREE factor (see Table 1) are written as zA ?
Ztree(x), where Ztree(x) is the arborescence poly-
tope, i.e., the convex hull of all incidence vectors
of dependency trees (Martins et al, 2009). It is
straightforward to write a contingency table and ob-
tain the following local agreement constraints at the
pairwise factors:
?ab(1, 1) = zab, ?ab(0, 0) = 1? za ? zb + zab
?ab(1, 0) = za ? zab, ?ab(0, 1) = zb ? zab.
Noting that all these pseudo-marginals are con-
strained to the unit interval, one can get rid of all
variables ?ab and write everything as
za ? [0, 1], zb ? [0, 1], zab ? [0, 1],
zab ? za, zab ? zb, zab ? za + zb ? 1,
(12)
inequalities which, along with zA ? Ztree(x), de-
fine the local polytope L(Gx). As for the factor en-
tropies, start by noting that the TREE-factor entropy
Htree can be obtained in closed form by computing
the marginals z?A and the partition function Zx(?)
(via the matrix-tree theorem) and recalling the vari-
ational representation in Eq. 9, yielding Htree =
logZx(?)? ?>F(x)z?A. Some algebra allows writ-
ing the overall Bethe entropy approximation as:
HBethe(? ) = Htree(zA)?
?
?a,b??P
Ia;b(za, zb, zab), (13)
where we introduced the mutual information asso-
ciated with each pairwise factor, Ia;b(za, zb, zab) =
6Smith and Eisner (2008) also proposed other variants with
more factors, which we omit for brevity.
TRE1
ACTRTE(
TRE1
A1TRTE(
,)SIARTE(
,)SIB23GRNDARTE(
TRE1AATTE( TRE1AAT1TE(
,)SI
AATE(
TRE1BNDRS)AATE(
)
AATR(TRE1AATRTE(
TRE1BG,RGDB)AATRTE(
)ACTR(
)A1TR(
GRDB,)DS
AR(
E
E
E
Figure 3: Details of the factor graph underlying the parser
of Martins et al (2009). Dashed circles represent auxil-
iary variables. See text and Table 1.
?
ya,yb
?ab(ya, yb) log
?ab(ya,yb)
?a(ya)?b(yb)
. The approximate
variational expression becomes logZx(?) ?
maxz ?
>F(x)z +Htree(zA)?
?
?a,b??P
Ia;b(za, zb, zab)
s.t. zab ? za, zab ? zb,
zab ? za + zb ? 1, ??a, b? ? P,
zA ? Ztree,
(14)
whose maximizer corresponds to the beliefs re-
turned by the Smith and Eisner?s loopy BP algorithm
(if it converges).
Turbo Parser #2: LP-Relaxed MAP. We now
turn to the concise integer LP formulation of Mar-
tins et al (2009). The formulation is exact but NP-
hard, and so an LP relaxation is made there by drop-
ping the integer constraints. We next construct a fac-
tor graph G?x and show that the LP relaxation corre-
sponds to an optimization of the form in Eq. 10, with
the marginal polytope M(G?x) replaced by L(G
?
x).
G?x includes the following auxiliary variable
nodes: path variables ?pij?i=0,...,n,j=1,...,n, which
indicate whether word j descends from i in the de-
pendency tree, and flow variables ?fka ?a?A,k=1,...,n,
which evaluate to 1 iff arc a ?carries flow? to k,
i.e., iff there is a path from the root to k that passes
through a. We need to seed these variables imposing
p0k = pkk = 1,?k, f
h
?h,m? = 0, ?h,m; (15)
i.e., any word descends from the root and from it-
self, and arcs leaving a word carry no flow to that
39
word. This can be done with unary hard constraint
factors. We then replace the TREE factor in Fig. 1 by
the factors shown in Fig. 3:
? O(n) XOR factors, each connecting all arc vari-
ables of the form {?h,m?}h=0,...,n. These ensure
that each word has exactly one parent. Each factor
yields a local agreement constraint (see Table 1):
?n
h=0 z?h,m? = 1, m ? {1, . . . , n} (16)
? O(n3) IMPLY factors, each expressing that if an
arc carries flow, then that arc must be active. Such
factors are OR factors with the first input negated,
hence, the local agreement constraints are:
fka ? za, a ? A, k ? {1, . . . , n}. (17)
? O(n2) XOR-WITH-OUTPUT factors, which im-
pose the constraint that each path variable pmk is
active if and only if exactly one incoming arc in
{?h,m?}h=0,...,n carries flow to k. Such factors
are XOR factors with the last input negated, and
hence their local constraints are:
pmk =
?n
h=0 f
k
?h,m?, m, k ? {1, . . . , n} (18)
? O(n2) XOR-WITH-OUTPUT factors to impose the
constraint that words don?t consume other words?
commodities; i.e., if h 6= k and k 6= 0, then there
is a path from h to k iff exactly one outgoing arc
in {?h,m?}m=1,...,n carries flow to k:
phk =
?n
m=1 f
k
?h,m?, h, k ? {0, . . . , n}, k /? {0, h}.
(19)
L(G?x) is thus defined by the constraints in Eq. 12
and 15?19. The approximate MAP problem, that
replaces M(G?x) by L(G
?
x) in Eq. 10, thus becomes:
maxz,f ,p ?
>F(x)z
s.t. Eqs. 12 and 15?19 are satisfied.
(20)
This is exactly the LP relaxation considered by Mar-
tins et al (2009) in their multi-commodity flow
model, for the configuration with siblings and grand-
parent features.7 They also considered a config-
uration with non-projectivity features?which fire
if an arc is non-projective.8 That configuration
can also be obtained here if variables {n?h,m?} are
7To be precise, the constraints of Martins et al (2009) are
recovered after eliminating the path variables, via Eqs. 18?19.
8An arc ?h,m? is non-projective if there is some word in its
span not descending from h (Kahane et al, 1998).
added to indicate non-projective arcs and OR-WITH-
OUTPUT hard constraint factors are inserted to en-
force n?h,m? = z?h,m??
?
min(h,m)<j<min(h,m) ?phj .
Details are omitted for space.
In sum, although the approaches of Smith and Eis-
ner (2008) and Martins et al (2009) look very dif-
ferent, in reality both are variational approximations
emanating from Prop. 1, respectively for marginal
and MAP inference. However, they operate on dis-
tinct factor graphs, respectively Figs. 1 and 3.9
5 Online Learning
Our learning algorithm is presented in Alg. 1. It is a
generalized online learner that tackles `2-regularized
empirical risk minimization of the form
min??Rd
?
2???
2 + 1m
?m
i=1 L(?;xi,yi), (21)
where each ?xi,yi? is a training example, ? ? 0 is
the regularization constant, and L(?;x,y) is a non-
negative convex loss. Examples include the logistic
loss used in CRFs (? log Pr?(y|x)) and the hinge
loss of structured SVMs (maxy??Y(x) ?
>(?(x,y?)?
?(x,y)) + `(y?,y) for some cost function `). These
are both special cases of the family defined in Fig. 4,
which also includes the structured perceptron?s loss
(? ? ?, ? = 0) and the softmax-margin loss of
Gimpel and Smith (2010; ? = ? = 1).
Alg. 1 is closely related to stochastic or online
gradient descent methods, but with the key advan-
tage of not needing a learning rate hyperparameter.
We sketch the derivation of Alg. 1; full details can
be found in Martins et al (2010a). On the tth round,
one example ?xt,yt? is considered. We seek to solve
min?,? ?m2 ?? ? ?t?
2 + ?
s.t. L(?;xt,yt) ? ?, ? ? 0,
(23)
9Given what was just exposed, it seems appealing to try
max-product loopy BP on the factor graph of Fig. 1, or sum-
product loopy BP on the one in Fig. 3. Both attempts present se-
rious challenges: the former requires computing messages sent
by the tree factor, which requires O(n2) calls to the Chu-Liu-
Edmonds algorithm and hence O(n5) time. No obvious strat-
egy seems to exist for simultaneous computation of all mes-
sages, unlike in the sum-product case. The latter is even more
challenging, as standard sum-product loopy BP has serious is-
sues in the factor graph of Fig. 3; we construct in Martins et al
(2010b) a simple example with a very poor Bethe approxima-
tion. This might be fixed by using other variants of sum-product
BP, e.g., ones in which the entropy approximation is concave.
40
L?,?(?;x,y) , 1? log
?
y??Y(x) exp
[
?
(
?>
(
?(x,y?)? ?(x,y)
)
+ ?`(y?,y)
)]
(22)
Figure 4: A family of loss functions including as particular cases the ones used in CRFs, structured SVMs, and the
structured perceptron. The hyperparameter ? is the analogue of the inverse temperature in a Gibbs distribution, while
? scales the cost. For any choice of ? > 0 and ? ? 0, the resulting loss function is convex in ?, since, up to a scale
factor, it is the composition of the (convex) log-sum-exp function with an affine map.
Algorithm 1 Aggressive Online Learning
1: Input: {?xi,yi?}mi=1, ?, number of epochs K
2: Initialize ?1 ? 0; set T = mK
3: for t = 1 to T do
4: Receive instance ?xt, yt? and set ?t = ?(yt)
5: Solve Eq. 24 to obtain ??t and L?,?(?t, xt,yt)
6: Compute?L?,?(?t, xt,yt)=F(xt)(??t??t)
7: Compute ?t = min
{
1
?m ,
L?,?(?t;xt,yt)
??L?,?(?t;xt,yt)?2
}
8: Return ?t+1 = ?t ? ?t?L?,?(?t;xt,yt)
9: end for
10: Return the averaged model ?? ? 1T
?T
t=1 ?t.
which trades off conservativeness (stay close to the
most recent solution ?t) and correctness (keep the
loss small). Alg. 1?s lines 7?8 are the result of tak-
ing the first-order Taylor approximation of L around
?t, which yields the lower bound L(?;xt,yt) ?
L(?t;xt,yt) + (? ? ?t)>?L(?t;xt,yt), and plug-
ging that linear approximation into the constraint of
Eq. 23, which gives a simple Euclidean projection
problem (with slack) with a closed-form solution.
The online updating requires evaluating the loss
and computing its gradient. Both quantities can
be computed using the variational expression in
Prop. 1, for any loss L?,?(?;x,y) in Fig. 4.10 Our
only assumption is that the cost function `(y?,y)
can be written as a sum over factor-local costs; let-
ting ? = ?(y) and ?? = ?(y?), this implies
`(y?,y) = p>?? + q for some p and q which are
constant with respect to ??.11 Under this assump-
tion, L?,?(?;x,y) becomes expressible in terms of
the log-partition function of a distribution whose
log-potentials are set to ?(F(x)>? + ?p). From
Eq. 9 and after some algebra, we finally obtain
L?,?(?;x,y) =
10Our description also applies to the (non-differentiable)
hinge loss case, when ? ? ?, if we replace all instances of
?the gradient? in the text by ?a subgradient.?
11For the Hamming cost, this holds with p = 1 ? 2? and
q = 1>?. See Taskar et al (2006) for other examples.
max
???M(Gx)
?>F(x)(????)+
1
?
H(??)+?(p>??+q).
(24)
Let ?? be a maximizer in Eq. 24; from the second
statement of Prop. 1 we obtain ?L?,?(?;x,y) =
F(x)(????). When the inference problem in Eq. 24
is intractable, approximate message-passing algo-
rithms like loopy BP still allow us to obtain approx-
imations of the loss L?,? and its gradient.
For the hinge loss, we arrive precisely at the max-
loss variant of 1-best MIRA (Crammer et al, 2006).
For the logistic loss, we arrive at a new online learn-
ing algorithm for CRFs that resembles stochastic
gradient descent but with an automatic step size that
follows from our variational representation.
Unsupported Features. As datasets grow, so do
the sets of features, creating further computational
challenges. Often only ?supported? features?those
observed in the training data?are included, and
even those are commonly eliminated when their fre-
quencies fall below a threshold. Important infor-
mation may be lost as a result of these expedi-
ent choices. Formally, the supported feature set
is Fsupp ,
?m
i=1 supp?(xi,yi), where suppu ,
{j |uj 6= 0} denotes the support of vector u. Fsupp
is a subset of the complete feature set, comprised of
those features that occur in some candidate output,
Fcomp ,
?m
i=1
?
y?i?Y(xi)
supp?(xi,y?i). Features
in Fcomp\Fsupp are called unsupported.
Sha and Pereira (2003) have shown that training a
CRF-based shallow parser with the complete feature
set may improve performance (over the supported
one), at the cost of 4.6 times more features. De-
pendency parsing has a much higher ratio (around
20 for bilexical word-word features, as estimated in
the Penn Treebank), due to the quadratic or faster
growth of the number of parts, of which only a few
are active in a legal output. We propose a simple
strategy for handling Fcomp efficiently, which can
be applied for those losses in Fig. 4 where ? = ?.
(e.g., the structured SVM and perceptron). Our pro-
cedure is the following: keep an active set F contain-
41
CRF (TURBO PARS. #1) SVM (TURBO PARS. #2) SVM (TURBO #2)
ARC-FACT. SEC. ORD. ARC-FACT. SEC. ORD. |F| |F||Fsupp| +NONPROJ., COMPL.
ARABIC 78.28 79.12 79.04 79.42 6,643,191 2.8 80.02 (-0.14)
BULGARIAN 91.02 91.78 90.84 92.30 13,018,431 2.1 92.88 (+0.34) (?)
CHINESE 90.58 90.87 91.09 91.77 28,271,086 2.1 91.89 (+0.26)
CZECH 86.18 87.72 86.78 88.52 83,264,645 2.3 88.78 (+0.44) (?)
DANISH 89.58 90.08 89.78 90.78 7,900,061 2.3 91.50 (+0.68)
DUTCH 82.91 84.31 82.73 84.17 15,652,800 2.1 84.91 (-0.08)
GERMAN 89.34 90.58 89.04 91.19 49,934,403 2.5 91.49 (+0.32) (?)
JAPANESE 92.90 93.22 93.18 93.38 4,256,857 2.2 93.42 (+0.32)
PORTUGUESE 90.64 91.00 90.56 91.50 16,067,150 2.1 91.87 (-0.04)
SLOVENE 83.03 83.17 83.49 84.35 4,603,295 2.7 85.53 (+0.80)
SPANISH 83.83 85.07 84.19 85.95 11,629,964 2.6 87.04 (+0.50) (?)
SWEDISH 87.81 89.01 88.55 88.99 18,374,160 2.8 89.80 (+0.42)
TURKISH 76.86 76.28 74.79 76.10 6,688,373 2.2 76.62 (+0.62)
ENGLISH NON-PROJ. 90.15 91.08 90.66 91.79 57,615,709 2.5 92.13 (+0.12)
ENGLISH PROJ. 91.23 91.94 91.65 92.91 55,247,093 2.4 93.26 (+0.41) (?)
Table 2: Unlabeled attachment scores, ignoring punctuation. The leftmost columns show the performance of arc-
factored and second-order models for the CRF and SVM losses, after 10 epochs with 1/(?m) = 0.001 (tuned on the
English Non-Proj. dev.-set). The rightmost columns refer to a model to which non-projectivity features were added,
trained under the SVM loss, that handles the complete feature set. Shown is the total number of features instantiated,
the multiplicative factor w.r.t. the number of supported features, and the accuracies (in parenthesis, we display the
difference w.r.t. a model trained with the supported features only). Entries marked with ? are the highest reported in
the literature, to the best of our knowledge, beating (sometimes slightly) McDonald et al (2006), Martins et al (2008),
Martins et al (2009), and, in the case of English Proj., also the third-order parser of Koo and Collins (2010), which
achieves 93.04% on that dataset (their experiments in Czech are not comparable, since the datasets are different).
ing all features that have been instantiated in Alg. 1.
At each round, run lines 4?5 as usual, using only
features in F. Since the other features have not been
used before, they have a zero weight, hence can be
ignored. When ? = ?, the variational problem in
Eq. 24 consists of a MAP computation and the solu-
tion corresponds to one output y?t ? Y(xt). Only the
parts that are active in y?t but not in yt, or vice-versa,
will have features that might receive a nonzero up-
date. Those parts are reexamined for new features
and the active set F is updated accordingly.
6 Experiments
We trained non-projective dependency parsers for
14 languages, using datasets from the CoNLL-X
shared task (Buchholz and Marsi, 2006) and two
datasets for English: one from the CoNLL-2008
shared task (Surdeanu et al, 2008), which contains
non-projective arcs, and another derived from the
Penn Treebank applying the standard head rules of
Yamada and Matsumoto (2003), in which all parse
trees are projective.12 We implemented Alg. 1,
12We used the provided train/test splits for all datasets. For
English, we used the standard test partitions (section 23 of the
Wall Street Journal). We did not exploit the fact that some
datasets only contain projective trees and have unique roots.
which handles any loss function L?,? .13 When ? <
?, Turbo Parser #1 and the loopy BP algorithm of
Smith and Eisner (2008) is used; otherwise, Turbo
Parser #2 is used and the LP relaxation is solved with
CPLEX. In both cases, we employed the same prun-
ing strategy as Martins et al (2009).
Two different feature configurations were first
tried: an arc-factored model and a model with
second-order features (siblings and grandparents).
We used the same arc-factored features as McDon-
ald et al (2005) and second-order features that con-
join words and lemmas (at most two), parts-of-
speech tags, and (if available) morphological infor-
mation; this was the same set of features as in Mar-
tins et al (2009). Table 2 shows the results obtained
in both configurations, for CRF and SVM loss func-
tions. While in the arc-factored case performance is
similar, in second-order models there seems to be a
consistent gain when the SVM loss is used. There
are two possible reasons: first, SVMs take the cost
function into consideration; second, Turbo Parser #2
is less approximate than Turbo Parser #1, since only
the marginal polytope is approximated (the entropy
function is not involved).
13The code is available at http://www.ark.cs.cmu.edu/
TurboParser.
42
? 1 1 1 1 3 5 ?
? 0 (CRF) 1 3 5 1 1 1 (SVM)
ARC-F. 90.15 90.41 90.38 90.53 90.80 90.83 90.66
2 ORD. 91.08 91.85 91.89 91.51 92.04 91.98 91.79
Table 3: Varying ? and ?: neither the CRF nor the
SVM is optimal. Results are UAS on the English Non-
Projective dataset, with ? tuned with dev.-set validation.
The loopy BP algorithm managed to converge for
nearly all sentences (with message damping). The
last three columns show the beneficial effect of un-
supported features for the SVM case (with a more
powerful model with non-projectivity features). For
most languages, unsupported features convey help-
ful information, which can be used with little extra
cost (on average, 2.5 times more features are instan-
tiated). A combination of the techniques discussed
here yields parsers that are in line with very strong
competitors?for example, the parser of Koo and
Collins (2010), which is exact, third-order, and con-
strains the outputs to be projective, does not outper-
form ours on the projective English dataset.14
Finally, Table 3 shows results obtained for differ-
ent settings of ? and ?. Interestingly, we observe
that higher scores are obtained for loss functions that
are ?between? SVMs and CRFs.
7 Related Work
There has been recent work studying efficient com-
putation of messages in combinatorial factors: bi-
partite matchings (Duchi et al, 2007), projective
and non-projective arborescences (Smith and Eis-
ner, 2008), as well as high order factors with count-
based potentials (Tarlow et al, 2010), among others.
Some of our combinatorial factors (OR, OR-WITH-
OUTPUT) and the analogous entropy computations
were never considered, to the best of our knowledge.
Prop. 1 appears in Wainwright and Jordan (2008)
for canonical overcomplete models; we adapt it here
for models with shared features. We rely on the vari-
ational interpretation of loopy BP, due to Yedidia et
al. (2001), to derive the objective being optimized
by Smith and Eisner?s loopy BP parser.
Independently of our work, Koo et al (2010)
14This might be due to the fact that Koo and Collins (2010)
trained with the perceptron algorithm and did not use unsup-
ported features. Experiments plugging the perceptron loss
(? ? ?, ? ? 0) into Alg. 1 yielded worse performance than
with the hinge loss.
recently proposed an efficient dual decomposition
method to solve an LP problem similar (but not
equal) to the one in Eq. 20,15 with excellent pars-
ing performance. Their parser is also an instance
of a turbo parser since it relies on a local approxi-
mation of a marginal polytope. While one can also
use dual decomposition to address our MAP prob-
lem, the fact that our model does not decompose as
nicely as the one in Koo et al (2010) would likely
result in slower convergence.
8 Conclusion
We presented a unified view of two recent approxi-
mate dependency parsers, by stating their underlying
factor graphs and by deriving the variational prob-
lems that they address. We introduced new hard con-
straint factors, along with formulae for their mes-
sages, local belief constraints, and entropies. We
provided an aggressive online algorithm for training
the models with a broad family of losses.
There are several possible directions for future
work. Recent progress in message-passing algo-
rithms yield ?convexified? Bethe approximations
that can be used for marginal inference (Wainwright
et al, 2005), and provably convergent max-product
variants that solve the relaxed LP (Globerson and
Jaakkola, 2008). Other parsing formalisms can be
handled with the inventory of factors shown here?
among them, phrase-structure parsing.
Acknowledgments
The authors would like to thank the reviewers for their
comments, and Kevin Gimpel, David Smith, David Son-
tag, and Terry Koo for helpful discussions. A. M. was
supported by a grant from FCT/ICTI through the CMU-
Portugal Program, and also by Priberam Informa?tica.
N. S. was supported in part by Qatar NRF NPRP-08-485-
1-083. E. X. was supported by AFOSR FA9550010247,
ONR N000140910758, NSF CAREER DBI-0546594,
NSF IIS-0713379, and an Alfred P. Sloan Fellowship.
M. F. and P. A. were supported by the FET programme
(EU FP7), under the SIMBAD project (contract 213250).
15The difference is that the model of Koo et al (2010)
includes features that depend on consecutive siblings?
making it decompose into subproblems amenable to dynamic
programming?while we have factors for all pairs of siblings.
43
References
C. Berrou, A. Glavieux, and P. Thitimajshima. 1993.
Near Shannon limit error-correcting coding and decod-
ing. In Proc. of ICC, volume 93, pages 1064?1070.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared task
on multilingual dependency parsing. In CoNLL.
K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz,
and Y. Singer. 2006. Online passive-aggressive al-
gorithms. JMLR, 7:551?585.
J. Duchi, D. Tarlow, G. Elidan, and D. Koller. 2007.
Using combinatorial optimization within max-product
belief propagation. NIPS, 19.
J. R. Finkel, A. Kleeman, and C. D. Manning. 2008. Effi-
cient, feature-based, conditional random field parsing.
Proc. of ACL.
K. Gimpel and N. A. Smith. 2010. Softmax-margin
crfs: Training log-linear models with loss functions.
In Proc. of NAACL.
A. Globerson and T. Jaakkola. 2008. Fixing max-
product: Convergent message passing algorithms for
MAP LP-relaxations. NIPS, 20.
L. Huang. 2008. Forest reranking: Discriminative pars-
ing with non-local features. In Proc. of ACL.
S. Kahane, A. Nasr, and O. Rambow. 1998. Pseudo-
projectivity: a polynomially parsable non-projective
dependency grammar. In Proc. of COLING.
T. Koo and M. Collins. 2010. Efficient third-order de-
pendency parsers. In Proc. of ACL.
T. Koo, A. Globerson, X. Carreras, and M. Collins. 2007.
Structured prediction models via the matrix-tree theo-
rem. In Proc. of EMNLP.
T. Koo, A. M. Rush, M. Collins, T. Jaakkola, and D. Son-
tag. 2010. Dual decomposition for parsing with non-
projective head automata. In Proc. of EMNLP.
F. R. Kschischang, B. J. Frey, and H. A. Loeliger. 2001.
Factor graphs and the sum-product algorithm. IEEE
Trans. Inf. Theory, 47(2):498?519.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. of ICML.
A. F. T. Martins, D. Das, N. A. Smith, and E. P. Xing.
2008. Stacking dependency parsers. In EMNLP.
A. F. T. Martins, N. A. Smith, and E. P. Xing. 2009.
Concise integer linear programming formulations for
dependency parsing. In Proc. of ACL-IJCNLP.
A. F. T. Martins, K. Gimpel, N. A. Smith, E. P. Xing,
P. M. Q. Aguiar, and M. A. T. Figueiredo. 2010a.
Learning structured classifiers with dual coordinate
descent. Technical Report CMU-ML-10-109.
A. F. T. Martins, N. A. Smith, E. P. Xing, P. M. Q. Aguiar,
and M. A. T. Figueiredo. 2010b. Turbo parsers:
Dependency parsing by approximate variational infer-
ence (extended version).
A. McCallum, K. Schultz, and S. Singh. 2009. Fac-
torie: Probabilistic programming via imperatively de-
fined factor graphs. In NIPS.
R. T. McDonald, F. Pereira, K. Ribarov, and J. Hajic.
2005. Non-projective dependency parsing using span-
ning tree algorithms. In Proc. of HLT-EMNLP.
R. McDonald, K. Lerman, and F. Pereira. 2006. Multi-
lingual dependency analysis with a two-stage discrim-
inative parser. In Proc. of CoNLL.
R. J. McEliece, D. J. C. MacKay, and J. F. Cheng. 1998.
Turbo decoding as an instance of Pearl?s ?belief prop-
agation? algorithm. IEEE Journal on Selected Areas
in Communications, 16(2).
J. Pearl. 1988. Probabilistic Reasoning in Intelligent
Systems: Networks of Plausible Inference. Morgan
Kaufmann.
F. Sha and F. Pereira. 2003. Shallow parsing with condi-
tional random fields. In Proc. of HLT-NAACL.
D. A. Smith and J. Eisner. 2008. Dependency parsing by
belief propagation. In Proc. of EMNLP.
D. A. Smith and N. A. Smith. 2007. Probabilistic models
of nonprojective dependency trees. In EMNLP.
M. Surdeanu, R. Johansson, A. Meyers, L. Ma`rquez, and
J. Nivre. 2008. The CoNLL-2008 shared task on
joint parsing of syntactic and semantic dependencies.
CoNLL.
C. Sutton, A. McCallum, and K. Rohanimanesh. 2007.
Dynamic conditional random fields: Factorized prob-
abilistic models for labeling and segmenting sequence
data. JMLR, 8:693?723.
R. E. Tarjan. 1977. Finding optimum branchings. Net-
works, 7(1):25?36.
D. Tarlow, I. E. Givoni, and R. S. Zemel. 2010. HOP-
MAP: Efficient message passing with high order po-
tentials. In Proc. of AISTATS.
B. Taskar, C. Guestrin, and D. Koller. 2003. Max-margin
Markov networks. In NIPS.
B. Taskar, S. Lacoste-Julien, and M. I. Jordan. 2006.
Structured prediction, dual extragradient and Bregman
projections. JMLR, 7:1627?1653.
I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun.
2004. Support vector machine learning for interdepen-
dent and structured output spaces. In Proc. of ICML.
M. J. Wainwright and M. I. Jordan. 2008. Graphical
Models, Exponential Families, and Variational Infer-
ence. Now Publishers.
M. J. Wainwright, T.S. Jaakkola, and A.S. Willsky. 2005.
A new class of upper bounds on the log partition func-
tion. IEEE Trans. Inf. Theory, 51(7):2313?2335.
H. Yamada and Y. Matsumoto. 2003. Statistical depen-
dency analysis with support vector machines. In Proc.
of IWPT.
J. S. Yedidia, W. T. Freeman, and Y. Weiss. 2001. Gen-
eralized belief propagation. In NIPS.
44
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1140?1150,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Staying Informed: Supervised and Semi-Supervised Multi-view
Topical Analysis of Ideological Perspective
Amr Ahmed
School of Computer Science
Carnegie Mellon University
amahmed@cs.cmu.edu
Eric P. Xing
School of Computer Science
Carnegie Mellon University
epxing@cs.cmu.edu
Abstract
With the proliferation of user-generated arti-
cles over the web, it becomes imperative to de-
velop automated methods that are aware of the
ideological-bias implicit in a document col-
lection. While there exist methods that can
classify the ideological bias of a given docu-
ment, little has been done toward understand-
ing the nature of this bias on a topical-level. In
this paper we address the problem of modeling
ideological perspective on a topical level using
a factored topic model. We develop efficient
inference algorithms using Collapsed Gibbs
sampling for posterior inference, and give var-
ious evaluations and illustrations of the util-
ity of our model on various document collec-
tions with promising results. Finally we give a
Metropolis-Hasting inference algorithm for a
semi-supervised extension with decent results.
1 Introduction
With the avalanche of user-generated articles over
the web, it is quite important to develop models that
can recognize the ideological bias behind a given
document, summarize where this bias is manifested
on a topical level, and provide the user with alter-
nate views that would help him/her staying informed
about different perspectives. In this paper, we fol-
low the notion of ideology as defines by Van Dijk
in (Dijk, 1998) as ?a set of general abstract beliefs
commonly shared by a group of people.? In other
words, an ideology is a set of ideas that directs one?s
goals, expectations, and actions. For instance, free-
dom of choice is a general aim that directs the ac-
tions of?liberals?, whereas conservation of values is
the parallel for ?conservatives?.
We can attribute the lexical variations of the word
content of a document to three factors:
? Writer Ideological Belief. A liberal writer
might use words like freedom and choice re-
gardless of the topical content of the document.
These words define the abstract notion of be-
lief held by the writer and its frequency in the
document largely depends on the writer?s style.
? Topical Content. This constitutes the main
source of the lexical variations in a given docu-
ment. For instance, a document about abortion
is more likely to have facts related to abortion,
health, marriage and relationships.
? Topic-Ideology Interaction. When a liberal
thinker writes about abortion, his/her abstract
beliefs are materialized into a set of concrete
opinions and stances, therefore, we might find
words like: pro-choice and feminism. On the
contrary, a conservative writer might stress is-
sues like pro-life, God and faith.
Given a collection of ideologically-labeled docu-
ments, our goal is to develop a computer model that
factors the document collection into a representation
that reflects the aforementioned three sources of lex-
ical variations. This representation can then be used
for:
? Visualization. By visualizing the abstract no-
tion of belief in each ideology, and the way
each ideology approaches and views main-
stream topics, the user can view and contrast
each ideology side-by-side and build the right
mental landscape that acts as the basis for
his/her future decision making.
1140
? Classification or Ideology Identification.
Given a document, we would like to tell the
user from which side it was written, and ex-
plain the ideological bias in the document at a
topical level.
? Staying Informed: Getting alternative
views1. Given a document written from per-
spective A, we would like the model to provide
the user with other documents that represent al-
ternative views about the same topic addressed
in the original document.
In this paper, we approach this problem using
Topic Models (Blei et al, 2003). We introduce a
factored topic model that we call multi-view Latent
Dirichlet Allocation or mview-LDA for short. Our
model views the word content of each document as
the result of the interaction between the document?s
idealogical and topical dimensions. The rest of this
paper is organized as follows. First, in Section 2,
we review related work, and then present our model
in Section 3. Then in Section 4, we detail a col-
lapsed Gibbs sampling algorithm for posterior infer-
ence. Sections 5 and 6 give details about the dataset
used in the evaluation and illustrate the capabilities
of our model using both qualitative and quantitative
measures. Section 7 describes and evaluates the ef-
ficacy of a semi-supervised extension, and finally in
Section 8 we conclude and list several directions for
future research.
2 Related Work
Ideological text is inherently subjective, thus our
work is related to the growing area of subjectiv-
ity analysis(Wiebe et al, 2004; Riloff et al, 2003).
The goal of this area of research is to learn to dis-
criminate between subjective and objective text. In
contrast,in modeling ideology, we aim toward con-
trasting two or more ideological perspectives each of
which is subjective in nature. Further more, subjec-
tive text can be classified into sentiments which gave
rise to a surge of work in automatic opinion min-
ing (Wiebe et al, 2004; Yu and Hatzivassiloglou,
2003; Pang et al, 2002; Turney and Littman, 2003;
Popescu and Etzioni, 2005) as well as sentiment
1In this paper, we use the words ideology, view, perspective
interchangeably to denote the same concept
analysis and product review mining (Nasukawa and
Yi, 2003; Hu and Liu, 2004; Pang and Lee, 2008;
Branavan et al, 2008; Titov and McDonald, 2008;
Titov and McDonald, 2008; Mei et al, 2007; Ling
et al, 2008). The research goal of sentiment anal-
ysis and classification is to identify language used
to convey positive and negative opinions, which dif-
fers from contrasting two ideological perspectives.
While ideology can be expressed in the form of a
sentiment toward a given topic,like abortion, ideo-
logical perspectives are reflected in many ways other
than sentiments as we will illustrate later in the pa-
per. Perhaps more related to this paper is the work
of (Fortuna et al, 2008; Lin et al, 2008) whose
goal is to detect bias in news articles via discrimina-
tive and generative approaches, respectively. How-
ever, this work still addresses ideology at an abstract
level as opposed to our approach of modeling ideol-
ogy at a topical level. Finally, independently, (Paul
and Girju, 2009) gives a construction similar to ours
however for a different task 2.
3 Multi-View Topic Models
In this section we introduce multi-view topic mod-
els, or mview-LDA for short. Our model, mview-
LDA, views each document as the result of the in-
teraction between its topical and idealogical dimen-
sions. The model seeks to explain lexical variabili-
ties in the document by attributing this variabilities
to one of those dimensions or to their interactions.
Topic models, like LDA, define a generative process
for a document collection based on a set of parame-
ters. LDA employs a semantic entity known as topic
to drive the generation of the document in question.
Each topic is represented by a topic-specific word
distribution which is modeled as a multinomial dis-
tribution over words, denoted by Multi(?). The
generative process of LDA proceeds as follows:
1. Draw topic proportions ?d|? ? Dir(?).
2. For each word
(a) Draw a topic zn|?d ? Mult(?d).
(b) Draw a word wn|zn, ? ? Multi(?zn).
In step 1 each document d samples a topic-mixing
vector ?d from a Dirichlet prior. The component ?d,k
2In fact, we only get to know about this related work after
our paper was accepted
1141
DN
v
z
w
V
a2 b2
KV
a1 b1
x2
x1
Variable Meaning
w word
v document?s ideology
z topic
x1, x2 word switches, one per word (see text)
? document-specific distribution over topics
? document?s expected usage of the ideology?s
background topic
? ideology?s background-topic
? ideology-independent topic distribution
? ideology-specific topic distribution
? topic bias across ideology
Figure 1: A plate diagram of the graphical model.
of this vector defines how likely topic k will appear
in document d. For each word in the document wn,
a topic indicator zn is sampled from ?d, and then
the word itself is sampled from a topic-specific word
distribution specified by this indicator. Thus LDA
can capture and represent lexical variabilities via the
components of ?d which represents the topical con-
tent of the document. In the next section we will ex-
plain how our new model mview-LDA can capture
other sources of lexical variabilities beyond topical
content.
3.1 Multi-View LDA
As we noted earlier, LDA captures lexical variabili-
ties due to topical content via ?d and the set of top-
ics ?1:K . In mview-LDA each document d is tagged
with the ideological view it represents via the ob-
served variable vd which takes values in the discrete
range: {1, 2, ? ? ? , V } as shown in Fig. 1. For sim-
plicity, lets first assume that V = 2. The topics ?1:K
retain the same meaning: a set of K multinomial
distributions each of which represents a given theme
or factual topic. In addition, we utilize an ideology-
specific topic ?v which is again a multinomial dis-
tribution over the same vocabulary. ?v models the
abstract belief shared by all the documents written
from view v. In other words, if v denotes the liberal
perspective, then ?v gives high probability to words
like progressive, choice, etc. Moreover, we defined
a set of K ? V topics that we refer to as ideology-
specific topics. For example, topic ?v,k represents
how ideology v addresses topic k. The generative
process of a document d with ideological view vd
proceeds as follows:
1. Draw ?d ? Beta(a1, b1)
2. Draw topic proportions ?d|? ? Dir(?2).
3. For each word wn
(a) Draw xn,1 ? Bernoulli(?d)
(b) If(xn,1 = 1)
i. Draw wn|xn,1 = 1 ? Multi(?vd)
(c) If(xn,1 = 0)
i. Draw zn|?d ? Mult(?d).
ii. Draw xn,2|vd, zn ? Bernoulli(?zn)
iii. If(xn,2 = 1)
A. Draw wn|zn, ? ? Multi(?zn).
iv. If(xn,2 = 0)
A. Draw wn|vd, zn ? Multi(?vd,zn).
In step 1, we draw a document-specific biased
coin,?d. The bias of this coin determines the pro-
portions of words in the document that are gener-
ated from its ideology background topic ?vd . As in
LDA, we draw the document-specific topic propor-
tion ?d from a Dirichlet prior. ?d thus controls the
lexical variabilities due to topical content inside the
document.
To generate a word wn, we first generate a coin
flip xn,1 from the coin ?d. If it turns head, then
we proceed to generate this word from the ideology-
specific topic associated with the document?s ideo-
logical view vd. In this case, the word is drawn in-
dependently of the topical content of the document,
and thus accounts for the lexical variation due to the
ideology associated with the document. The propor-
tion of such words is document-specific by design
1142
and depends on the writer?s style to a large degree.
If xn,1 turns to be tail,we proceed to the next step
and draw a topic-indicator zn. Now, we have two
choices: either to generate this word directly from
the ideology-independent portion of the topic ?zn ,
or to draw the word from the ideology-specific por-
tion ?vd,zn . The choice here is not document spe-
cific, but rather depends on the interaction between
the ideology and the specific topic in question. If
the ideology associated with the document holds a
strong opinion or view with regard to this topic,
then we expect that most of the time we will take
the second choice, and generate wn from ?vd,zn ;
and vice versa. This decision is controlled by the
Bernoulli variable ?zn . Therefore, in step c.ii, we
first generate a coin flip xn,2 from ?zn . Based on
xn,2 we either generate the word from the ideology-
independent portion of the topic ?zn , and this con-
stitutes how the model accounts for lexical variation
due to the topical content of the document, or gen-
erate the word from the ideology-specific portion of
the topic ?vd,zn , and this specifies how the model
accounts for lexical variation due to the interaction
between the topical and ideological dimensions of
the document.
Finally, it is worth mentioning that the decision to
model ?zn
3 at the topic-ideology level rather than at
the document level, as we have done with ?d, stems
from our goal to capture ideology-specific behavior
on a corpus level rather than capturing document-
specific writing style. However, it is worth mention-
ing that if one truly seeks to measure the degree of
bias associated with a given document,then one can
compute the frequency of the event xn,2 = 0 from
posterior samples. In this case, ?zn acts as the prior
bias only. Moreover, computing the frequency of
the event xn,2 = 0 and zn = k gives the document?s
bias toward topic k per se.
Finally, it is worth mentioning that all multino-
mial topics in the model: ?,?, ? are generated once
for the whole collection from a symmetric Dirichlet
prior, similarly, all bias variables, ?1:K are sampled
from a Beta distribution also once at the beginning
of the generative process.
3In an earlier version of the work we modeled ? on a per-
ideology basis, however, we found that using a single shared ?
results in more robust results
4 Posterior Inference Via Collapsed Gibbs
Sampling
The main tasks can be summarized as follows:
? Learning: Given a collection of documents,
find a point estimate of the model parameters
(i.e. ?,?, ?, ?,etc.).
? Inference: Given a new document, and a point
estimate of the model parameters, find the pos-
terior distribution of the latent variables associ-
ated with the document at hand:
(?d, {xn,1}, {zn}, {xn,2}).
Under a hierarchical Bayesian setting, like the ap-
proach we took in this paper, both of these tasks can
be handled via posterior inference. Under the gener-
ative process, and hyperparmaters choices, outlined
in section 3, we seek to compute:
P (d1:D, ?1:K ,?1:V , ?1:V,1:K, ?1:K |?, a, b,w,v),
where d is a shorthand for the hidden variables
(?d, ?d, z,x1,x2) in document d. The above poste-
rior probability is unfortunately intractable,and we
approximate it via a collapsed Gibbs sampling pro-
cedure (Griffiths and Steyvers, 2004; Gelman et al,
2003) by integrating out, i.e. collapsing, the fol-
lowing hidden variables: the topic-mixing vectors
?d and the ideology bias ?d for each document, as
well as all the multinomial topic distributions: (?,?
and ?) in addition to the ideology-topic biases given
by the set of ? random variables.
Therefore, the state of the sampler at each itera-
tion contains only the following topic indicators and
coin flips for each document:(z,x1,x2). We alter-
nate sampling each of these variables conditioned on
its Markov blanket until convergence. At conver-
gence, we can calculate expected values for all the
parameters that were integrated out, especially for
the topic distributions, for each document?s latent
representation (mixing-vector) and for all coin bi-
ases. To ease the calculation of the Gibbs sampling
update equations we keep a set of sufficient statistics
(SS) in the form of co-occurrence counts and sum
matrices of the form CEQeq to denote the number of
times instance e appeared with instance q. For ex-
ample, CWKwk gives the number of times word w was
sampled from the ideology-independent portion of
1143
topic k. Moreover, we follow the standard practice
of using the subscript ?i to denote the same quan-
tity it is added to without the contribution of item
i. For example,CWKwk,?i is the same as C
WK
wk with-
out the contribution of word wi. For simplicity, we
might drop dependencies on the document whenever
the meaning is implicit form the context.
For word wn in document d, instead of sampling
zn, xn,1, xn,2 independently, we sample them as a
block as follows:
P (xn,1 = 1|wn = w, vd = v) ?
(CDX1d1,?n + a1)?
CVWvw,?n + ?1
?
w?(C
VW
vw?,?n + ?1)
P (xn,1 = 0, x2,n = 1, zn = k|wn = w, vd = v)
? (CDX1d0,?n + b1)?
CKX2k1,?n + a2
CKX2k1,?n + C
KX2
k0,?n + a2 + b2
?
CKWkw,?n + ?1
?
w?(C
KW
kw?,?n + ?1)
?
CDKdk,?n + ?2
?
k?(C
DK
dk?,?n + ?2)
P (xn,1 = 0, x2,n = 0, zn = k|wn = w, vd = v)
? (CDX1d0,?n + b1)?
CKX2k0,?n + b2
CKX2k1,?n + C
KX2
k0,?n + a2 + b2
?
CV KWvkw,?n + ?1
?
w?(C
V KW
vkw?,?n + ?1)
?
CDKdk,?n + ?2
?
k?(C
DK
dk?,?n + ?2)
The above three equations can be normalized to
form a 2 ? K + 1 multinomial distribution: one
component for generating a word from the ideol-
ogy topic, K components for generating the word
from the ideology-independent portion of topic k =
1, ? ? ? ,K, and finally K components for generat-
ing the word from the ideology-specific portion of
topic k = 1, ? ? ? ,K. Each of these 2 ? K + 1
cases corresponds to a unique assignment of the
variables zn, xn,1, xn,2. Therefore, our Gibbs sam-
pler just repeatedly draws sample from this 2?K+1-
components multinomial distribution until conver-
gence. Upon convergence, we compute point es-
timates for all the collapsed variables by a simple
marginalization of the appropriate count matrices.
During inference, we hold the corpus-level count
matrices fixed, and keep sampling from the above
2?K+1-component multinomial while only chang-
ing the document-level count matrices: CDK , CDX1
until convergence. Upon convergence, we compute
estimates for ?d and ?d by normalizing CDK and
CDX1 (or possibly averaging this quantity across
posterior samples). As we mentioned in Section 3,
to compute the ideology-bias in addressing a given
topic say k in a given document, say d, we can sim-
ply compute the expected value of the event xn,2 =
0 and zn = k across posterior samples.
5 Data Sets
We evaluated our model over three datasets: the bit-
terlemons croups and a two political blog-data set.
Below we give details of each dataset.
5.1 The Bitterlemons dataset
The bitterlemons corpus consists of
the articles published on the website
http://bitterlemons.org/. The website
is set up to contribute to mutual understanding
between Palestinians and Israelis through the
open exchange of ideas. Every week, an issue
about the Israeli-Palestinian conflict is selected for
discussion, and a Palestinian editor and an Israeli
editor contribute one article each addressing the
issue. In addition, the Israeli and Palestinian editors
invite one Israeli and one Palestinian to express
their views on the issue. The data was collected
and pre-proceed as describes in (Lin et al, 2008).
Overall, the dataset contains 297 documents written
from the Israeli?s point of view, and 297 documents
written from the Palestinian?s point of view. On
average each document contains around 740 words.
After trimming words appearing less than 5 times,
we ended up with a vocabulary size of 4100 words.
We split the dataset randomly and used 80% of the
documents for training and the rest for testing.
5.2 The Political Blog Datasets
The first dataset refereed to as Blog-1 is a subset
of the data collected and processed in (Yano et al,
2009). The authors in (Yano et al, 2009) collected
blog posts from blog sites focusing on American
politics during the period November 2007 to Oc-
tober 2008. We selected three blog sites from this
dataset: the Right Wing News (right-ideology) ;
the Carpetbagger, and Daily Kos as representatives
1144
palest inian
is raeli
peace
year 
polit ical 
proces s  
state 
end 
rig ht  
g overnment 
need 
conflict
way
s ecurit y
palest inian
is raeli
Peace
polit ical 
occupation 
proces s
end 
s ecurit y  
conflict  
way 
g overnment  
people
t ime year
force 
neg ot iation
bush US pres ident  american
sharon administ ration prime 
s ett lem ent  pres s ure policy  
washingt on ariel new middle
unit  state american georg e
powell minister colin visit  
internal policy  statement  
expres s  pro previous  package 
work t ransfer european
administ ration receive
arafat state leader roadmap 
g eorg e elect ion mont h iraq
week peace june realist ic 
yasir s enior involvement  
clint on november post  
mandate terroris m
US  ro leIsraeli View
roadmap phase violence 
s ecurit y  ceasefire state plan 
international step implement  
authorit y  final quartet  is s ue 
map effort
roadmap end s ett lem ent  
implem entation obligation 
st op expansion commit ment 
cons olidate fulfill unit  illegal 
pres ent  previou assassination 
mee t  forward negative calm
proces s  force terroris m unit  
road demand provide 
confidence elem ent  interim 
dis cus s ion want union s uccee
point  build pos it ive recog nize 
pres ent  t imetable
Roadmap pro c ess
is rael sy ria syrian neg ot iate 
lebanon deal conference 
conces s ion asad agreement  
reg ional oct ober init iative 
relations hip
t rack neg ot iation official 
leaders hip pos it ion 
withdrawal time victory  
pres ent  s econd stand 
circumstance repres ent  s ens e 
talk st rateg y  is s ue participant 
parti neg ot iator
peace st rateg ic plo hizballah
is lamic neig hbor territ orial 
radical iran relation t hink  
obviou count ri mandate 
g reater conventional int ifada 
affect  jihad time
Arab Involvement
Palestinian  View
Israeli 
Backgro und
to pic
Palestinian
Backgro und
to pic
Figure 2: Illustrating the big picture overview over the bitterlemons dataset using few topics. Each box lists the top
words in the corresponding multinomial topic distribution. See text for more details
of the liberal view (left-ideology). After trimming
short posts of less than 20 words, we ended up with
2040 posts distributed as 1400 from the left-wing
and the rest from the right-wing. On average, each
post contains around 100 words and the total size of
the vocabulary is 14276 words. For this dataset, we
followed the train-test split in (Yano et al, 2009).
In this split each blog is represented in both train-
ing and test sets. Thus this dataset does not measure
the model?s ability to generalize to a totally different
writing style.
The second dataset refereed to as Blog-2 is sim-
ilar to Blog-1 in its topical content and time frame
but larger in its blog coverage (Eisenstein and Xing,
2010). Blog-2 spans 6 blogs: three from the left-
wing and three from the right-wing. The dataset
contains 13246 posts. After removing words that
appear less then 20 times, the total vocabulary be-
comes 13236 with an average of 200 words per post.
We used 4 blogs (2 from each view) for training
and held two blogs (one from each view) for test-
ing. Thus this dataset measures the model?s ability
to generalize to a totally new blog.
6 Experimental Results
In this section we gave various qualitative and quan-
titative evaluations of our model over the datasets
listed in Section 5. For all experiments, we set
?1 = .01, ?2 = .1, a = 1 and b = 1. We run Gibbs
sampling during training for 1000 iterations. During
inference, we ran Gibbs sampling for 300 iterations,
and took 10 samples, with 50-iterations lag, for eval-
uations.
6.1 Visualization and Browsing
One advantage of our approach is its ability to create
a ?big-picture? overview of the interaction between
ideology and topics. In figure 2 we show a portion of
that diagram over the bitterlemons dataset. First note
how the ideology-specific topics in both ideology
share the top-three words, which highlights that the
two ideologies seek peace even though they still both
disagree on other issues. The figure gives example
of three topics: the US role, the Roadmap peace
process, and the Arab involvement in the conflict
1145
(the name of these topics were hand-crafted). For
each topic, we display the top words in the ideology-
independent part of the topic (?), along with top
words in each ideology?s view of the topic (?).
For example, when discussing the roadmap pro-
cess, the Palestinian view brings the following is-
sues: [the Israeli side should] implement the oblig-
atory points in this agreement, stop expansion of
settlements, and move forward to the commitments
brought by this process. On the other hand, the Is-
raeli side brings the following points: [Israelis] need
to build confidence [with Palestinian], address the
role of terrorism on the implementation of the pro-
cess, and ask for a positive recognition of Israel
from the different Palestinian political parties. As
we can see, the ideology-specific portion of the topic
needn?t always represent a sentiment shared by its
members toward a given topic, but it might rather
includes extra important dimensions that need to be
taken into consideration when addressing this topic.
Another interesting topic addresses the involve-
ment of the neighboring Arab countries in the con-
flict. From the Israeli point of view, Israel is worried
about the existence of hizballah [in lebanon] and its
relationship with radical Iran, and how this might
affect the Palestinian-uprising (Intifada) and Jihad.
From the other side, the Palestinians think that the
Arab neighbors need to be involved in the peace pro-
cess and negotiations as some of these countries like
Syria and Lebanon are involved in the conflict.
The user can use the above chart as an entry point
to retrieve various documents pertinent to a given
topic or to a given view over a specific topic. For
instance, if the user asks for a representative sam-
ple of the Israeli(Palestinian) view with regard to the
roadmap process, the system can first retrieve docu-
ments tagged with the Israeli(Palestinian) view and
having a high topical value in their latent representa-
tion ? over this topic. Finally, the system then sorts
these documents by how much bias they show over
this topic. As we discussed in Section 4, this can be
done by computing the expected value of the event
xn,2 = 0 and zn = k where k is the topic under
consideration.
6.2 Classification
We have also performed a classification task over
all the datasets. The Scenario proceeded as follows.
We train a model over the training data with various
number of topics. Then given a test document, we
predict its ideology using the following equation:
vd = argmaxv?V P (wd|v); (1)
We use three baselines. The first baseline
is an SVM classifier trained on the normalized
word frequency of each document. We trained
SVM using a regularization parameter in the range
{1, 10, 20, ? ? ? , 100} and report the best result (i.e.
no cross-validation was performed). The other
two are supervised LDA models: supervised LDA
(sLDA) (Wang et. al., 2009; Blei and McCauliffe,
2007) and discLDA (Lacoste-Julien et al, 2008).
discLDA is a conditional model that divides the
available number of topics into class-specific top-
ics and shared-topics. Since the code is not publicly
available, we followed the same strategy in the orig-
inal paper and share 0.1K topics across ideologies
and then divide the rest of the topics between ide-
ologies4. However, unlike our model, there are no
internal relationships between these two sets of top-
ics. The decision rule employed by discLDA is very
similar to the one we used for mview-LDA in Eq
(1). For sLDA, we used the publicly available code
by the authors.
As shown in Figure 3, our model performs better
than the baselines over the three datasets. We should
note from this figure that mview-LDA peaks at a
small number of topics, however, each topic is repre-
sented by three multinomials. Moreover, it is evident
from the figure that the experiment over the blog-
2 dataset which measures each model?s ability to
generalizes to a totally unseen new blog is a harder
task than generalizing to unseen posts form the same
blog. However, our model still performs competi-
tively with the SVM baseline. We believe that sep-
arating each topic into an ideology-independent part
and ideology-specific part is the key behind this per-
formance, as it is expected that the new blogs would
still share much of the ideology-independent parts
of the topics and hopefully would use similar (but
4(Lacoste-Julien et al, 2008) gave an optimization algo-
rithm for learning the topic structure (transformation matrix),
however since the code is not available, we resorted to one of
the fixed splitting strategies mentioned in the paper. We tried
other splits but this one gives the best results
1146
(a) (b) (c)
Figure 3: Classification accuracy over the Bitterlemons dataset in (a) and over the two blog datasets in (b) and (c). For SVM we
give the best result obtained across a wide range of the SVM?s regularization parameter(not the cross-validation result).
no necessarily all) words from the ideology-specific
parts of each topic when addressing this topic.
Finally, it should be noted that the bitterlemons
dataset is a multi-author dataset and thus the models
were tested on some authors that were not seen dur-
ing training, however, two factors contributed to the
good performance by all models over this dataset.
The first being the larger size of each document (740
words per document as compared to 200 words per
post in blog-2) and the second being the more formal
writing style in the bitterlemons dataset.
6.3 An Ablation Study
To understand the contribution of each component of
our model, we conducted an ablation study over the
bitterlemons dataset. In this experiment we turned-
off one feature of our model at a time and mea-
sured the classification performance. The results are
shown in Figure 4. Full, refers to the full model; No-
? refers to a model in which the ideology-specific
background topic ? is turned-off; and No-? refers
to a model in which the ideology-specific portions of
the topics are turned-off. As evident from the figure,
? is more important to the model than ? and the dif-
ference in performance between the full model and
the No-? model is rather significant. In fact without
? the model has little power to discriminate between
ideologies beyond the ideology-specific background
topic ?.
6.4 Retrieval: Getting the Other Views
To evaluate the ability of our model in finding al-
ternative views toward a given topic, we conducted
the following experiment over the Bitterlemons cor-
pus. In this corpus each document is associated with
a meta-topic that highlights the issues addressed in
this document like: ?A possible Jordanian role?,
Figure 4: An Ablation study over the bitterlemons dataset.
?Demography and the conflict?,etc. There are a to-
tal of 148 meta-topics. These topics were not used
in fitting our model but we use them in the evalu-
ation as follows. We divided the dataset into 60%
for training and 40% for testing. We trained mview-
LDA over the training set, and then used the learned
model to infer the latent representation of the test
documents as well as their ideologies. We then used
each document in the training set as a query to re-
trieve documents from the test set that address the
same meta-topic in the query document but from the
other-side?s perspective. Note that we have access to
the view of the query document but not the view of
the test document. Moreover, the value of the meta-
topic is only used to construct the ground-truth result
of each query over the test set. In addition to mview-
LDA, we also implemented a strong baseline using
SVM+Dirichlet smoothing that we will refer to as
LM. In this baseline, we build an SVM classifier
over the training set, and use Dirichlet-smoothing
to represent each document (in test and training set)
as a multinomial-distribution over the vocabulary.
Given a query document d, we rank documents in
1147
Figure 5: Evaluating the performance of the view-Retrieval task. Figure compare performance between mview-LD vs. an SVM+a
smoothed language model approach using three measures: average rank, best rank and rank at full recall. ( Lower better)
.
the test set by each model as follows:
? mview-LDA: we computed the cosine-distance
between ?mv?LDA?sharedd and ?
mv?LDA?shared
d?
weighted by the probability that d? is written
from a different view than vd. The latter
quantity can be computed by normaliz-
ing P (v|d?). Moreover, ?mv?LDA?sharedd,k ??
n I
[
(xn,1 = 0) and (xn,2 = 1) and (zn = k)
]
,
and n ranges over words in document d. In-
tuitively, we would like ?mv?LDA?sharedd to
reflect variation due to the topical content, but
not ideological view of the document.
? LM: For a document d?, we apply the SVM
classifier to get P (v|d?), then we measure sim-
ilarity by computing the cosine-distance be-
tween the smoothed multinomial-distribution
of d and d?. We combine these two components
as in mview-LDA.
Finally we rank documents in the test set in a
descending-order and evaluate the resulting rank-
ing using three measures: the rank at full recall
(lowest rank), average rank, and best rank of the
ground-truth documents as they appear in the pre-
dicted ranking. Figure 5 shows the results across a
number of topics. From this figure, it is clear that
our model outperforms this baseline over all mea-
sures. It should be noted that this is a very hard
task since the meta-topics are very fine-grained like:
Settlements revisited, The status of the settlements,
Is the roadmap still relevant?,The ceasefire and the
roadmap: a progress report,etc. We did not attempt
to cluster these meta-topics since our goal is just to
compare our model against the baseline.
7 A Semi-Supervised Extension
In this section we present and assess the efficacy of
a semi-supervised extension of mview-LDA. In this
setting, the model is given a set of ideologically-
labeled documents and a set of unlabeled docu-
ments. One of the key advantages of using a prob-
abilistic graphical model is the ability to deal with
hidden variables in a principled way. Thus the only
change needed in this case is adding a single step in
the sampling algorithm to sample the ideology v of
an unlabeled document as follows:
P (vd = v|rest) ? P (wd|vd = v, zd,x1,d,x2,d)
Note that the probability of the indicators
(x1,d,x2,d, zd) do not depend on the view of the
document and thus got absorbed in the normaliza-
tion constant, and thus one only needs to measures
the likelihood of generating the words in the doc-
ument under the view v. We divide the words
into three groups: Ad = {wn|xn,1 = 1} is the
set of words generated from the view-background
topic, Bd,k = {wn|zn = k, xn,1 = 0, xn,2 =
1} is the set of words generated from ?k, and
Cd,k = {wn|zn = k, xn,1 = 0, xn,2 = 0} is the
set of words generated from ?k,v. The probabil-
ity of Bd,k does not depend on the value of v and
thus can be absorbed into the normalization factor.
Therefore, we only need to compute the following
probability:P (Ad, Cd,1:K |vd = v, rest)=
?
k
?
?k,v
P (Cd,k|?k,v, rest)p(?k,v|rest)d?k,v
?
?
?
P (Ad|?, rest)p(?|rest)d? (2)
1148
All the integrals in (2) reduce to the ratio of two
log partition functions. For example, the product of
integrals containing Cd,k reduce to:
?
k
?
w ?
(
CDKW,X2=0dkw + C
V KW
vkw,?d + ?1
)
?
(?
w
[
CDKW,X2=0dkw + C
V KW
vkw,?d + ?1
])
?
?
(?
w
[
CV KWvkw + ?1
])
?
w ?
(
CV KWvkw,?d + ?1
) (3)
Unfortunately, the above scheme does not mix
well because the value of the integrals in (2) are
very low for any view other than the view of the
document in the current state of the sampler. This
happens because of the tight coupling between vd
and the indicators (x1,x2, z). To remedy this prob-
lem we used a Metropolis-Hasting step to sample
(vd,x1,x2, z) jointly. We construct a set of V pro-
posals each of which is indexed by a possible view:
qv(x1,x2, z) = P (x1,x2, z|vd = v,wd). Since
we have a collection of proposal distributions, we
select one of them at random at each step. To
generate a sample from qv?(), we run a few it-
erations of a restricted Gibbs scan over the docu-
ment d conditioned on fixing vd = v? and then
take the last sample jointly with v? as our pro-
posed new state. With probability min(r,1), the new
state (v?,x1?,x2?, z?) is accepted, otherwise the
old state is retained. The acceptance ratio,r, is com-
puted as: r = p(wd|v?,x1?,x2?,z?)p(wd|v,x1,x2,z) , where the non-*
variables represent the current state of the sampler.
It is interesting to note that the above acceptance ra-
tio is equivalent to a likelihood ratio test. We com-
pute the marginal probability P (wd|..) using the
estimated-theta method (Wallach et al, 2009).
We evaluated the semi-supervised extension using
the blog-2 dataset as follows. We reveal R% of the
labels in the training set; then we train mview-LDA
only over the labeled portion and train the semi-
supervised version (ss-mview-LDA) on both the la-
beled and unlabeled documents. Finally we evaluate
the classification performance on the test set. We
used R = {20, 40, 80}. The results are given in Ta-
ble 1 which shows a decent improvement over the
supervised mview-LDA.
R mview-LDA ss-mview-LDA
80 65.60 66.41
60 62.31 65.43
20 60.87 63.25
Table 1: Classification performance of the semi-
supervised model. R is the ratio of labeled documents.
8 Discussion and Future Work
In this paper, we addressed the problem of model-
ing ideological perspective at a topical level. We
developed a factored topic model that we called
multiView-LDA or mview-LDA for short. mview-
LDA factors a document collection into three set
of topics: ideology-specific, topic-specific, and
ideology-topic ones. We showed that the resulting
representation can be used to give a bird-eyes? view
to where each ideology stands with regard to main-
stream topics. Moreover, we illustrated how the la-
tent structure induced by the model can by used to
perform bias-detection at the document and topic
level, and retrieve documents that represent alterna-
tive views.
It is important to mention that our model induces
a hierarchical structure over the topics, and thus it
is interesting to contrast it with hierarchical topic
models like hLDA (Blei et al, 2003) and PAM (Li
and McCallum, 2006; Mimno et al, 2007). First,
these models are unsupervised in nature, while our
model is supervised. Second, the semantic of the
hierarchical structure in our model is different than
the one induced by those models since documents in
our model are constrained to use a specific portion
of the topic structure while in those models docu-
ments can freely sample words from any topic. Fi-
nally,in the future we plan to extend our model to
perform joint modeling and summarization of ide-
alogical discourse.
9 Acknowledgment
We thank Jacob Eisenstein, John Lafferty, Tom
Mitchell, and the anonymous reviewers for their
helpful comments and suggestions. This work is
supported in part by grants NSF IIS- 0713379,
NSF DBI-0546594 career award to EPX, ONR
N000140910758, DARPA NBCH1080007, and
AFOSR FA9550010247. EPX is supported by an
Alfred P. Sloan Research Fellowship.
1149
References
J. Wiebe, T. Wilson, R.Bruce, M. Bell, and M. Martin.
Learning subjective language. Computational Linguis-
tics, 30(3), 2004.
H. Yu and V. Hatzivassiloglou. Towards answering opin-
ion questions: Separating facts from opinions and
identifying the polarity of opinion sentences. In Pro-
ceedings of EMNLP-2003
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs
up? Sentiment classification using machine learning
techniques. In Proceedings of EMNLP-2002.
P. Turney and M. Littman. Measuring praise and criti-
cism: Inference of semantic orientation from associa-
tion. ACM TOIS, 21(4):315346, 2003
A. Popescu and O. Etzioni. Extracting product fea-
tures and opinions from reviews. In Proceedings of
HLT/EMNLP-2005, pages 339346, 2005.
T. Nasukawa and J. Yi. Sentiment analysis: Capturing
favorability using natural language processing. In Pro-
ceedings of K-CAP, 2003.
M. Hu and B. Liu. Mining and summarizing customer
reviews. In Proceedings of KDD, 2004.
B. Pang and L. Lee. Opinion mining and sentiment anal-
ysis. Foundations and Trends in Information Retrieval,
2(12), 1135, 2008.
S. Branavan, H. Chen, J. Eisenstein and R. Barzilay.
Learning Document-Level Semantic Properties from
Free-text Annotations, Proceedings of ACL, 2008.
I. Titov and R. McDonald. Modeling Online Reviews
with Multi-Grain Topic Models International World
Wide Web Conference (WWW), 2008.
I. Titov and R. McDonald. A Joint Model of Text and
Aspect Ratings for Sentiment Summarization Associ-
ation for Computational Linguistics (ACL), 2008.
Q. Mei, X. Ling, M. Wondra, H. Su, ChengXiang Zhai.
Topic Sentiment Mixture: Modeling Facets and Opin-
ions in Weblogs, Proceedings of the 16th International
World Wide Web Conference (WWW), pages 171-
180, 2007.
X. Ling, Q. Mei, C. Zhai, B. Schatz. Mining Multi-
Faceted Overviews of Arbitrary Topics in a Text Col-
lection, Proceedings of the 15th ACM SIGKDD In-
ternational Conference on Knowledge Discovery and
Data Mining (KDD? 08), pages 497-505, 2008
B. Fortuna , C. Galleguillos, N. Cristianini. Detecting
the bias in media with statistical learning methods.
In: Text Mining: Theory and Applications. Taylor and
Francis Publisher,2008.
W. Lin, E.P. Xing, and A. Hauptmann. A Joint Topic and
Perspective Model for Ideological Discourse Euro-
pean Conference on Machine Learning and Principles
and Practice of Knowledge Discovery in Databases
(ECML/PKDD), 2008.
T. A. Van Dijk. Ideology: A multidisciplinary approach.
Sage Publications, 1998.
T. Griffiths, M. Steyvers Finding scientific topics.PNAS,
101:5228-5235, 2004.
A. Gelman, J. Carlin, Hal Stern, and Donald Ru-
bin. Bayesian Data Analysis, Chapman-Hall, 2 edi-
tion,2003.
D. Blei, A. Ng, and M. Jordan. Latent Dirichlet al
location. Journal of Machine Learning Research,
3:9931022, January 2003.
T. Yano, W. W. Cohen, and N. A. Smith. Predicting
Response to Political Blog Posts with Topic Models.
NAACL-HLT 2009, Boulder, CO, MayJune 2009
J. Eisenstein and E.P. Xing.The CMU-2008 Political
Blog Corpus. CMU-ML-10-101 Technical Report,
2010.
C. Wang, D. Blei and L. Fei-Fei. Simultaneous image
classification and annotation. CVPR, 2010.
D. Blei and J. McAuliffe. Supervised topic models. NIPS,
2007.
S. Lacoste-Julien, F. Sha, and M. Jordan. DiscLDA:
Discriminative Learning for Dimensionality Reduc-
tion and Classification. Neural Information Process-
ing Systems Conference (NIPS08), Vancouver, British
Columbia, December 2008.
E. Riloff, J. Wiebe, and T. Wilson. Learning subjective
nouns using extraction pattern bootstrapping. In Pro-
ceedings of CoNLL-2003.
D. Blei, T. Griffiths, M. Jordan, and J. Tenenbaum. Hier-
archical topic models and the nested Chinese restau-
rant process. In Neural Information Processing Sys-
tems (NIPS)16, 2003.
D. Mimno, W. Li and A. McCallum. Mixtures of Hier-
archical Topics with Pachinko Allocation. In Interna-
tional Conference of Machine Learning, ICML, 2007.
W. Li, and A. McCallum. Pachinko Allocation: DAG-
structured Mixture Models of Topic Correlations. In
International Conference of Machine Learning, ICML,
2006.
M. Paul and R. Girju. Cross-cultural Analysis of Blogs
and Forums with Mixed-Collection Topic Models.
EMNLP 2009.
H. Wallach, I. Murray, R. Salakhutdinov, and D. Mimno.
Evaluation Methods for Topic Models. ICML 2009.
1150
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1277?1287,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
A Latent Variable Model for Geographic Lexical Variation
Jacob Eisenstein Brendan O?Connor Noah A. Smith Eric P. Xing
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{jacobeis,brendano,nasmith,epxing}@cs.cmu.edu
Abstract
The rapid growth of geotagged social media
raises new computational possibilities for in-
vestigating geographic linguistic variation. In
this paper, we present a multi-level generative
model that reasons jointly about latent topics
and geographical regions. High-level topics
such as ?sports? or ?entertainment? are ren-
dered differently in each geographic region,
revealing topic-specific regional distinctions.
Applied to a new dataset of geotagged mi-
croblogs, our model recovers coherent top-
ics and their regional variants, while identi-
fying geographic areas of linguistic consis-
tency. The model also enables prediction of
an author?s geographic location from raw text,
outperforming both text regression and super-
vised topic models.
1 Introduction
Sociolinguistics and dialectology study how lan-
guage varies across social and regional contexts.
Quantitative research in these fields generally pro-
ceeds by counting the frequency of a handful of
previously-identified linguistic variables: pairs of
phonological, lexical, or morphosyntactic features
that are semantically equivalent, but whose fre-
quency depends on social, geographical, or other
factors (Paolillo, 2002; Chambers, 2009). It is left to
the experimenter to determine which variables will
be considered, and there is no obvious procedure for
drawing inferences from the distribution of multiple
variables. In this paper, we present a method for
identifying geographically-aligned lexical variation
directly from raw text. Our approach takes the form
of a probabilistic graphical model capable of iden-
tifying both geographically-salient terms and coher-
ent linguistic communities.
One challenge in the study of lexical variation is
that term frequencies are influenced by a variety of
factors, such as the topic of discourse. We address
this issue by adding latent variables that allow us to
model topical variation explicitly. We hypothesize
that geography and topic interact, as ?pure? topi-
cal lexical distributions are corrupted by geographi-
cal factors; for example, a sports-related topic will
be rendered differently in New York and Califor-
nia. Each author is imbued with a latent ?region?
indicator, which both selects the regional variant of
each topic, and generates the author?s observed ge-
ographical location. The regional corruption of top-
ics is modeled through a cascade of logistic normal
priors?a general modeling approach which we call
cascading topic models. The resulting system has
multiple capabilities, including: (i) analyzing lexi-
cal variation by both topic and geography; (ii) seg-
menting geographical space into coherent linguistic
communities; (iii) predicting author location based
on text alone.
This research is only possible due to the rapid
growth of social media. Our dataset is derived from
the microblogging website Twitter,1 which permits
users to post short messages to the public. Many
users of Twitter also supply exact geographical co-
ordinates from GPS-enabled devices (e.g., mobile
phones),2 yielding geotagged text data. Text in
computer-mediated communication is often more
vernacular (Tagliamonte and Denis, 2008), and as
such it is more likely to reveal the influence of ge-
ographic factors than text written in a more formal
genre, such as news text (Labov, 1966).
We evaluate our approach both qualitatively and
quantitatively. We investigate the topics and regions
1http://www.twitter.com
2User profiles also contain self-reported location names, but
we do not use that information in this work.
1277
that the model obtains, showing both common-sense
results (place names and sports teams are grouped
appropriately), as well as less-obvious insights about
slang. Quantitatively, we apply our model to predict
the location of unlabeled authors, using text alone.
On this task, our model outperforms several alterna-
tives, including both discriminative text regression
and related latent-variable approaches.
2 Data
The main dataset in this research is gathered from
the microblog website Twitter, via its official API.
We use an archive of messages collected over the
first week of March 2010 from the ?Gardenhose?
sample stream,3 which then consisted of 15% of
all public messages, totaling millions per day. We
aggressively filter this stream, using only messages
that are tagged with physical (latitude, longitude)
coordinate pairs from a mobile client, and whose au-
thors wrote at least 20 messages over this period. We
also filter to include only authors who follow fewer
than 1,000 other people, and have fewer than 1,000
followers. Kwak et al (2010) find dramatic shifts
in behavior among users with social graph connec-
tivity outside of that range; such users may be mar-
keters, celebrities with professional publicists, news
media sources, etc. We also remove messages con-
taining URLs to eliminate bots posting information
such as advertising or weather conditions. For inter-
pretability, we restrict our attention to authors inside
a bounding box around the contiguous U.S. states,
yielding a final sample of about 9,500 users and
380,000 messages, totaling 4.7 million word tokens.
We have made this dataset available online.4
Informal text from mobile phones is challeng-
ing to tokenize; we adapt a publicly available tok-
enizer5 originally developed for Twitter (O?Connor
et al, 2010), which preserves emoticons and blocks
of punctuation and other symbols as tokens. For
each user?s Twitter feed, we combine all messages
into a single ?document.? We remove word types
that appear in fewer than 40 feeds, yielding a vocab-
ulary of 5,216 words. Of these, 1,332 do not appear
in the English, French, or Spanish dictionaries of the
3http://dev.twitter.com/pages/streaming_api
4http://www.ark.cs.cmu.edu/GeoTwitter
5http://tweetmotif.com
spell-checking program aspell.
Every message is tagged with a location, but most
messages from a single individual tend to come from
nearby locations (as they go about their day); for
modeling purposes we use only a single geographic
location for each author, simply taking the location
of the first message in the sample.
The authors in our dataset are fairly heavy Twit-
ter users, posting an average of 40 messages per day
(although we see only 15% of this total). We have
little information about their demographics, though
from the text it seems likely that this user set skews
towards teens and young adults. The dataset cov-
ers each of the 48 contiguous United States and the
District of Columbia.
3 Model
We develop a model that incorporates two sources
of lexical variation: topic and geographical region.
We treat the text and geographic locations as out-
puts from a generative process that incorporates both
topics and regions as latent variables.6 During infer-
ence, we seek to recover the topics and regions that
best explain the observed data.
At the base level of model are ?pure? topics (such
as ?sports?, ?weather?, or ?slang?); these topics are
rendered differently in each region. We call this gen-
eral modeling approach a cascading topic model; we
describe it first in general terms before moving to the
specific application to geographical variation.
3.1 Cascading Topic Models
Cascading topic models generate text from a chain
of random variables. Each element in the chain de-
fines a distribution over words, and acts as the mean
of the distribution over the subsequent element in
the chain. Thus, each element in the chain can be
thought of as introducing some additional corrup-
tion. All words are drawn from the final distribution
in the chain.
At the beginning of the chain are the priors, fol-
lowed by unadulerated base topics, which may then
be corrupted by other factors (such as geography or
time). For example, consider a base ?food? topic
6The region could be observed by using a predefined geo-
graphical decomposition, e.g., political boundaries. However,
such regions may not correspond well to linguistic variation.
1278
that emphasizes words like dinner and delicious;
the corrupted ?food-California? topic would place
weight on these words, but might place extra em-
phasis on other words like sprouts.
The path through the cascade is determined by a
set of indexing variables, which may be hidden or
observed. As in standard latent Dirichlet alocation
(Blei et al, 2003), the base topics are selected by
a per-token hidden variable z. In the geographical
topic model, the next level corresponds to regions,
which are selected by a per-author latent variable r.
Formally, we draw each level of the cascade from
a normal distribution centered on the previous level;
the final multinomial distribution over words is ob-
tained by exponentiating and normalizing. To ensure
tractable inference, we assume that all covariance
matrices are uniform diagonal, i.e., aI with a > 0;
this means we do not model interactions between
words.
3.2 The Geographic Topic Model
The application of cascading topic models to ge-
ographical variation is straightforward. Each doc-
ument corresponds to the entire Twitter feed of a
given author during the time period covered by our
corpus. For each author, the latent variable r cor-
responds to the geographical region of the author,
which is not observed. As described above, r se-
lects a corrupted version of each topic: the kth basic
topic has mean ?k, with uniform diagonal covari-
ance ?2k; for region j, we can draw the regionally-
corrupted topic from the normal distribution, ?jk ?
N(?k, ?
2
kI).
Because ? is normally-distributed, it lies not in
the simplex but in RW . We deterministically com-
pute multinomial parameters ? by exponentiating
and normalizing: ?jk = exp(?jk)/
?
i exp(?
(i)
jk ).
This normalization could introduce identifiability
problems, as there are multiple settings for ? that
maximize P (w|?) (Blei and Lafferty, 2006a). How-
ever, this difficulty is obviated by the priors: given
? and ?2, there is only a single ? that maximizes
P (w|?)P (?|?, ?2); similarly, only a single ?max-
imizes P (?|?)P (?|a, b2).
The observed latitude and longitude, denoted y,
are normally distributed and conditioned on the re-
gion, with mean ?r and precision matrix ?r indexed
by the region r. The region index r is itself drawn
from a single shared multinomial ?. The model is
shown as a plate diagram in Figure 1.
Given a vocabulary size W , the generative story
is as follows:
? Generate base topics: for each topic k < K
? Draw the base topic from a normal distribu-
tion with uniform diagonal covariance: ?k ?
N(a, b2I),
? Draw the regional variance from a Gamma
distribution: ?2k ? G(c, d).
? Generate regional variants: for each region
j < J ,
? Draw the region-topic ?jk from a normal
distribution with uniform diagonal covari-
ance: ?jk ? N(?k, ?
2
kI).
? Convert ?jk into a multinomial
distribution over words by ex-
ponentiating and normalizing:
?jk = exp
(
?jk
)
/
?W
i exp(?
(i)
jk ),
where the denominator sums over the
vocabulary.
? Generate regions: for each region j < J ,
? Draw the spatial mean ?j from a normal dis-
tribution.
? Draw the precision matrix ?j from a Wishart
distribution.
? Draw the distribution over regions ? from a sym-
metric Dirichlet prior, ? ? Dir(??1).
? Generate text and locations: for each document d,
? Draw topic proportions from a symmetric
Dirichlet prior, ? ? Dir(?1).
? Draw the region r from the multinomial dis-
tribution ?.
? Draw the location y from the bivariate Gaus-
sian, y ? N(?r,?r).
? For each word token,
? Draw the topic indicator z ? ?.
? Draw the word token w ? ?rz .
4 Inference
We apply mean-field variational inference: a fully-
factored variational distribution Q is chosen to min-
imize the Kullback-Leibler divergence from the
true distribution. Mean-field variational inference
with conjugate priors is described in detail else-
where (Bishop, 2006; Wainwright and Jordan,
2008); we restrict our focus to the issues that are
unique to the geographic topic model.
1279
? ?
w
z
?
D
Nd
y
r K
?
J
?
?
?
??2
?
?k log of base topic k?s distribution over word types
?2k variance parameter for regional variants of topic k
?jk region j?s variant of base topic ?k
?d author d?s topic proportions
rd author d?s latent region
yd author d?s observed GPS location
?j region j?s spatial center
?j region j?s spatial precision
zn token n?s topic assignment
wn token n?s observed word type
? global prior over author-topic proportions
? global prior over region classes
Figure 1: Plate diagram for the geographic topic model, with a table of all random variables. Priors (besides ?) are
omitted for clarity, and the document indices on z and w are implicit.
We place variational distributions over all latent
variables of interest: ?, z, r,?,?,?, ?2,?, and ?,
updating each of these distributions in turn, until
convergence. The variational distributions over ?
and ? are Dirichlet, and have closed form updates:
each can be set to the sum of the expected counts,
plus a term from the prior (Blei et al, 2003). The
variational distributions q(z) and q(r) are categor-
ical, and can be set proportional to the expected
joint likelihood?to set q(z) we marginalize over r,
and vice versa.7 The updates for the multivariate
Gaussian spatial parameters ? and ? are described
by Penny (2001).
4.1 Regional Word Distributions
The variational region-topic distribution ?jk is nor-
mal, with uniform diagonal covariance for tractabil-
ity. Throughout we will write ?x? to indicate the ex-
pectation of x under the variational distribution Q.
Thus, the vector mean of the distribution q(?jk) is
written ??jk?, while the variance (uniform across i)
of q(?) is written V(?jk).
To update the mean parameter ??jk?, we max-
imize the contribution to the variational bound L
from the relevant terms:
L
[??(i)jk ?]
= ?log p(w|?, z, r)?+?log p(?(i)jk |?
(i)
k , ?
2
k)?,
(1)
7Thanks to the na??ve mean field assumption, we can
marginalize over z by first decomposing across all Nd words
and then summing over q(z).
with the first term representing the likelihood of the
observed words (recall that ? is computed determin-
istically from ?) and the second term corresponding
to the prior. The likelihood term requires the expec-
tation ?log??, but this is somewhat complicated by
the normalizer
?W
i exp(?
(i)), which sums over all
terms in the vocabulary. As in previous work on lo-
gistic normal topic models, we use a Taylor approx-
imation for this term (Blei and Lafferty, 2006a).
The prior on ? is normal, so the contribution from
the second term of the objective (Equation 1) is
? 1
2??2k?
?(?(i)jk ? ?
(i)
k )
2?. We introduce the following
notation for expected counts: N(i, j, k) indicates the
expected count of term i in region j and topic k, and
N(j, k) =
?
iN(i, j, k). After some calculus, we
can write the gradient ?L/???((i))jk ? as
N(i, j, k)?N(j, k)??(i)jk ? ? ??
?2
k ?(??
(i)
jk ? ? ??
(i)
k ?),
(2)
which has an intuitive interpretation. The first two
terms represent the difference in expected counts for
term i under the variational distributions q(z, r) and
q(z, r, ?): this difference goes to zero when ?(i)jk per-
fectly matches N(i, j, k)/N(j, k). The third term
penalizes ?(i)jk for deviating from its prior ?
(i)
k , but
this penalty is proportional to the expected inverse
variance ???2k ?. We apply gradient ascent to maxi-
mize the objective L. A similar set of calculations
gives the gradient for the variance of ?; these are
described in an forthcoming appendix.
1280
4.2 Base Topics
The base topic parameters are?k and ?
2
k; in the vari-
ational distribution, q(?k) is normally distributed
and q(?2k) is Gamma distributed. Note that ?k and
?2k affect only the regional word distributions ?jk.
An advantage of the logistic normal is that the vari-
ational parameters over ?k are available in closed
form,
??(i)k ? =
b2
?J
j ??
(i)
jk ?+ ??
2
k?a
(i)
b2J + ??2k?
V(?k) = (b
?2 + J???2k ?)
?1,
where J indicates the number of regions. The ex-
pectation of the base topic ? incorporates the prior
and the average of the generated region-topics?
these two components are weighted respectively by
the expected variance of the region-topics ??2k? and
the prior topical variance b2. The posterior variance
V(?) is a harmonic combination of the prior vari-
ance b2 and the expected variance of the region top-
ics.
The variational distribution over the region-topic
variance ?2k has Gamma parameters. These param-
eters cannot be updated in closed form, so gradi-
ent optimization is again required. The derivation
of these updates is more involved, and is left for a
forthcoming appendix.
5 Implementation
Variational scheduling and initialization are impor-
tant aspects of any hierarchical generative model,
and are often under-discussed. In our implementa-
tion, the variational updates are scheduled as fol-
lows: given expected counts, we iteratively update
the variational parameters on the region-topics ? and
the base topics?, until convergence. We then update
the geographical parameters ? and ?, as well as the
distribution over regions ?. Finally, for each doc-
ument we iteratively update the variational param-
eters over ?, z, and r until convergence, obtaining
expected counts that are used in the next iteration
of updates for the topics and their regional variants.
We iterate an outer loop over the entire set of updates
until convergence.
We initialize the model in a piecewise fashion.
First we train a Dirichlet process mixture model on
the locations y, using variational inference on the
truncated stick-breaking approximation (Blei and
Jordan, 2006). This automatically selects the num-
ber of regions J , and gives a distribution over each
region indicator rd from geographical information
alone. We then run standard latent Dirichlet aloca-
tion to obtain estimates of z for each token (ignoring
the locations). From this initialization we can com-
pute the first set of expected counts, which are used
to obtain initial estimates of all parameters needed
to begin variational inference in the full model.
The prior a is the expected mean of each topic
?; for each term i, we set a(i) = logN(i) ? logN ,
where N(i) is the total count of i in the corpus and
N =
?
iN(i). The variance prior b
2 is set to 1, and
the prior on ?2 is the Gamma distribution G(2, 200),
encouraging minimal deviation from the base topics.
The symmetric Dirichlet prior on ? is set to 12 , and
the symmetric Dirichlet parameter on ? is updated
from weak hyperpriors (Minka, 2003). Finally, the
geographical model takes priors that are linked to the
data: for each region, the mean is very weakly en-
couraged to be near the overall mean, and the covari-
ance prior is set by the average covariance of clusters
obtained by running K-means.
6 Evaluation
For a quantitative evaluation of the estimated rela-
tionship between text and geography, we assess our
model?s ability to predict the geographic location of
unlabeled authors based on their text alone.8 This
task may also be practically relevant as a step toward
applications for recommending local businesses or
social connections. A randomly-chosen 60% of au-
thors are used for training, 20% for development,
and the remaining 20% for final evaluation.
6.1 Systems
We compare several approaches for predicting au-
thor location; we divide these into latent variable
generative models and discriminative approaches.
8Alternatively, one might evaluate the attributed regional
memberships of the words themselves. While the Dictionary of
American Regional English (Cassidy and Hall, 1985) attempts
a comprehensive list of all regionally-affiliated terms, it is based
on interviews conducted from 1965-1970, and the final volume
(covering Si?Z) is not yet complete.
1281
6.1.1 Latent Variable Models
Geographic Topic Model This is the full version
of our system, as described in this paper. To pre-
dict the unseen location yd, we iterate until con-
vergence on the variational updates for the hidden
topics zd, the topic proportions ?d, and the region
rd. From rd, the location can be estimated as y?d =
arg maxy
?J
j p(y|?j ,?j)q(rd = j). The develop-
ment set is used to tune the number of topics and to
select the best of multiple random initializations.
Mixture of Unigrams A core premise of our ap-
proach is that modeling topical variation will im-
prove our ability to understand geographical varia-
tion. We test this idea by fixing K = 1, running our
system with only a single topic. This is equivalent
to a Bayesian mixture of unigrams in which each au-
thor is assigned a single, regional unigram language
model that generates all of his or her text. The de-
velopment set is used to select the best of multiple
random initializations.
Supervised Latent Dirichlet Allocation In a
more subtle version of the mixture-of-unigrams
model, we model each author as an admixture of re-
gions. Thus, the latent variable attached to each au-
thor is no longer an index, but rather a vector on the
simplex. This model is equivalent to supervised la-
tent Dirichlet alocation (Blei and McAuliffe, 2007):
each topic is associated with equivariant Gaussian
distributions over the latitude and longitude, and
these topics must explain both the text and the ob-
served geographical locations. For unlabeled au-
thors, we estimate latitude and longitude by esti-
mating the topic proportions and then applying the
learned geographical distributions. This is a linear
prediction
f(z?d;a) = (z?
T
da
lat, z?Tda
lon)
for an author?s topic proportions z?d and topic-
geography weights a ? R2K .
6.1.2 Baseline Approaches
Text Regression We perform linear regression
to discriminatively learn the relationship between
words and locations. Using term frequency features
xd for each author, we predict locations with word-
geography weights a ? R2W :
f(xd;a) = (x
T
da
lat, xTda
lon)
Weights are trained to minimize the sum of squared
Euclidean distances, subject to L1 regularization:
?
d
(xTda
lat ? ylatd )
2 + (xTda
lon ? ylond )
2
+ ?lat||a
lat||1 + ?lon||a
lon||1
The minimization problem decouples into two sep-
arate latitude and longitude models, which we fit
using the glmnet elastic net regularized regres-
sion package (Friedman et al, 2010), which ob-
tained good results on other text-based prediction
tasks (Joshi et al, 2010). Regularization parameters
were tuned on the development set. The L1 penalty
outperformed L2 and mixtures of L1 and L2.
Note that for both word-level linear regression
here, and the topic-level linear regression in SLDA,
the choice of squared Euclidean distance dovetails
with our use of spatial Gaussian likelihoods in the
geographic topic models, since optimizing a is
equivalent to maximum likelihood estimation un-
der the assumption that locations are drawn from
equivariant circular Gaussians centered around each
f(xd;a) linear prediction. We experimented with
decorrelating the location dimensions by projecting
yd into the principal component space, but this did
not help text regression.
K-Nearest Neighbors Linear regression is a poor
model for the multimodal density of human popula-
tions. As an alternative baseline, we applied super-
vised K-nearest neighbors to predict the location yd
as the average of the positions of the K most sim-
ilar authors in the training set. We computed term-
frequency inverse-document frequency features and
applied cosine similarity over their first 30 principal
components to find the neighbors. The choices of
principal components, IDF weighting, and neighbor-
hood size K = 20 were tuned on the development
set.
6.2 Metrics
Our principle error metrics are the mean and median
distance between the predicted and true location in
kilometers.9 Because the distance error may be dif-
ficult to interpret, we also report accuracy of classi-
9For convenience, model training and prediction use latitude
and longitude as an unprojected 2D Euclidean space. However,
properly measuring the physical distance between points on the
1282
Regression Classification accuracy (%)
System Mean Dist. (km) Median Dist. (km) Region (4-way) State (49-way)
Geographic topic model 900 494 58 24
Mixture of unigrams 947 644 53 19
Supervised LDA 1055 728 39 4
Text regression 948 712 41 4
K-nearest neighbors 1077 853 37 2
Mean location 1148 1018
Most common class 37 27
Table 1: Location prediction results; lower scores are better on the regression task, higher scores are better on the
classification task. Distances are in kilometers. Mean location and most common class are computed from the test set.
Both the geographic topic model and supervised LDA use the best number of topics from the development set (10 and
5, respectively).
fication by state and by region of the United States.
Our data includes the 48 contiguous states plus the
District of Columbia; the U.S. Census Bureau di-
vides these states into four regions: West, Midwest,
Northeast, and South.10 Note that while major pop-
ulation centers straddle several state lines, most re-
gion boundaries are far from the largest cities, re-
sulting in a clearer analysis.
6.3 Results
As shown in Table 1, the geographic topic model
achieves the strongest performance on all metrics.
All differences in performance between systems
are statistically significant (p < .01) using the
Wilcoxon-Mann-Whitney test for regression error
and the ?2 test for classification accuracy. Figure 2
shows how performance changes as the number of
topics varies.
Note that the geographic topic model and the mix-
ture of unigrams use identical code and parametriza-
tion ? the only difference is that the geographic topic
model accounts for topical variation, while the mix-
ture of unigrams sets K = 1. These results validate
our basic premise that it is important to model the
interaction between topical and geographical varia-
tion.
Text regression and supervised LDA perform es-
pecially poorly on the classification metric. Both
methods make predictions that are averaged across
Earth?s surface requires computing or approximating the great
circle distance ? we use the Haversine formula (Sinnott, 1984).
For the continental U.S., the relationship between degrees and
kilometers is nearly linear, but extending the model to a conti-
nental scale would require a more sophisticated approach.
10http://www.census.gov/geo/www/us_regdiv.pdf
0 5 10 15 20400
500
600
700
800
900
1000
1100
Number of topics
Me
dia
n e
rro
r (k
m)
 
 
Geographic Topic Model
Supervised LDA
Mean location
Figure 2: The effect of varying the number of topics on
the median regression error (lower is better).
each word in the document: in text regression, each
word is directly multiplied by a feature weight; in
supervised LDA the word is associated with a la-
tent topic first, and then multiplied by a weight. For
these models, all words exert an influence on the pre-
dicted location, so uninformative words will draw
the prediction towards the center of the map. This
yields reasonable distance errors but poor classifica-
tion accuracy. We had hoped that K-nearest neigh-
bors would be a better fit for this metric, but its per-
formance is poor at all values of K. Of course it is
always possible to optimize classification accuracy
directly, but such an approach would be incapable
of predicting the exact geographical location, which
is the focus of our evaluation (given that the desired
geographical partition is unknown). Note that the
geographic topic model is also not trained to opti-
mize classification accuracy.
1283
?basketball?
?popular
music?
?daily life? ?emoticons? ?chit chat?
PISTONS KOBE
LAKERS game
DUKE NBA
CAVS STUCKEY
JETS KNICKS
album music
beats artist video
#LAKERS
ITUNES tour
produced vol
tonight shop
weekend getting
going chilling
ready discount
waiting iam
:) haha :d :( ;) :p
xd :/ hahaha
hahah
lol smh jk yea
wyd coo ima
wassup
somethin jp
Boston
+ CELTICS victoryBOSTON
CHARLOTTE
playing daughter
PEARL alive war
comp
BOSTON ;p gna loveee
ese exam suttin
sippin
N. California+
THUNDER
KINGS GIANTS
pimp trees clap
SIMON dl
mountain seee 6am OAKLAND
pues hella koo
SAN fckn
hella flirt hut
iono OAKLAND
New York
+ NETS KNICKS BRONX iam cab oww wasssup nm
Los Angeles+
#KOBE
#LAKERS
AUSTIN
#LAKERS load
HOLLYWOOD
imm MICKEY
TUPAC
omw tacos hr
HOLLYWOOD
af papi raining
th bomb coo
HOLLYWOOD
wyd coo af nada
tacos messin
fasho bomb
Lake Erie
+
CAVS
CLEVELAND
OHIO BUCKS od
COLUMBUS
premiere prod
joint TORONTO
onto designer
CANADA village
burr
stink CHIPOTLE
tipsy
;d blvd BIEBER
hve OHIO
foul WIZ salty
excuses lames
officer lastnight
Table 2: Example base topics (top line) and regional variants. For the base topics, terms are ranked by log-odds
compared to the background distribution. The regional variants show words that are strong compared to both the base
topic and the background. Foreign-language words are shown in italics, while terms that are usually in proper nouns
are shown in SMALL CAPS. See Table 3 for definitions of slang terms; see Section 7 for more explanation and details
on the methodology.
Figure 3: Regional clustering of the training set obtained by one randomly-initialized run of the geographical topic
model. Each point represents one author, and each shape/color combination represents the most likely cluster as-
signment. Ellipses represent the regions? spatial means and covariances. The same model and coloring are shown in
Table 2.
1284
7 Analysis
Our model permits analysis of geographical vari-
ation in the context of topics that help to clarify
the significance of geographically-salient terms. Ta-
ble 2 shows a subset of the results of one randomly-
initialized run, including five hand-chosen topics (of
50 total) and five regions (of 13, as chosen automat-
ically during initialization). Terms were selected by
log-odds comparison. For the base topics we show
the ten strongest terms in each topic as compared to
the background word distribution. For the regional
variants, we show terms that are strong both region-
ally and topically: specifically, we select terms that
are in the top 100 compared to both the background
distribution and to the base topic. The names for the
topics and regions were chosen by the authors.
Nearly all of the terms in column 1 (?basketball?)
refer to sports teams, athletes, and place names?
encouragingly, terms tend to appear in the regions
where their referents reside. Column 2 contains sev-
eral proper nouns, mostly referring to popular mu-
sic figures (including PEARL from the band Pearl
Jam).11 Columns 3?5 are more conversational.
Spanish-language terms (papi, pues, nada, ese) tend
to appear in regions with large Spanish-speaking
populations?it is also telling that these terms ap-
pear in topics with emoticons and slang abbrevia-
tions, which may transcend linguistic barriers. Other
terms refer to people or subjects that may be espe-
cially relevant in certain regions: tacos appears in
the southern California region and cab in the New
York region; TUPAC refers to a rap musician from
Los Angeles, and WIZ refers to a rap musician from
Pittsburgh, not far from the center of the ?Lake Erie?
region.
A large number of slang terms are found to have
strong regional biases, suggesting that slang may
depend on geography more than standard English
does. The terms af and hella display especially
strong regional affinities, appearing in the regional
variants of multiple topics (see Table 3 for defini-
tions). Northern and Southern California use variant
spellings koo and coo to express the same meaning.
11This analysis is from an earlier version of our dataset that
contained some Twitterbots, including one from a Boston-area
radio station. The bots were purged for the evaluation in Sec-
tion 6, though the numerical results are nearly identical.
term definition
af as fuck (very)
coo cool
dl download
fasho for sure
gna going to
hella very
hr hour
iam I am
ima I?m going to
imm I?m
iono I don?t know
lames lame (not cool)
people
term definition
jk just kidding
jp just playing (kid-
ding)
koo cool
lol laugh out loud
nm nothing much
od overdone (very)
omw on my way
smh shake my head
suttin something
wassup what?s up
wyd what are you do-
ing?
Table 3: A glossary of non-standard terms from Ta-
ble 2. Definitions are obtained by manually inspecting
the context in which the terms appear, and by consulting
www.urbandictionary.com.
While research in perceptual dialectology does con-
firm the link of hella to Northern California (Bu-
choltz et al, 2007), we caution that our findings
are merely suggestive, and a more rigorous analysis
must be undertaken before making definitive state-
ments about the regional membership of individual
terms. We view the geographic topic model as an
exploratory tool that may be used to facilitate such
investigations.
Figure 3 shows the regional clustering on the
training set obtained by one run of the model. Each
point represents an author, and the ellipses represent
the bivariate Gaussians for each region. There are
nine compact regions for major metropolitan areas,
two slightly larger regions that encompass Florida
and the area around Lake Erie, and two large re-
gions that partition the country roughly into north
and south.
8 Related Work
The relationship between language and geography
has been a topic of interest to linguists since the
nineteenth century (Johnstone, 2010). An early
work of particular relevance is Kurath?s Word Geog-
raphy of the Eastern United States (1949), in which
he conducted interviews and then mapped the oc-
currence of equivalent word pairs such as stoop and
porch. The essence of this approach?identifying
variable pairs and measuring their frequencies?
remains a dominant methodology in both dialec-
1285
tology (Labov et al, 2006) and sociolinguis-
tics (Tagliamonte, 2006). Within this paradigm,
computational techniques are often applied to post
hoc analysis: logistic regression (Sankoff et al,
2005) and mixed-effects models (Johnson, 2009) are
used to measure the contribution of individual vari-
ables, while hierarchical clustering and multidimen-
sional scaling enable aggregated inference across
multiple variables (Nerbonne, 2009). However, in
all such work it is assumed that the relevant linguis-
tic variables have already been identified?a time-
consuming process involving considerable linguistic
expertise. We view our work as complementary to
this tradition: we work directly from raw text, iden-
tifying both the relevant features and coherent lin-
guistic communities.
An active recent literature concerns geotagged in-
formation on the web, such as search queries (Back-
strom et al, 2008) and tagged images (Crandall et
al., 2009). This research identifies the geographic
distribution of individual queries and tags, but does
not attempt to induce any structural organization of
either the text or geographical space, which is the
focus of our research. More relevant is the work
of Mei et al (2006), in which the distribution over
latent topics in blog posts is conditioned on the ge-
ographical location of the author. This is somewhat
similar to the supervised LDA model that we con-
sider, but their approach assumes that a partitioning
of geographical space into regions is already given.
Methodologically, our cascading topic model is
designed to capture multiple dimensions of variabil-
ity: topics and geography. Mei et al (2007) include
sentiment as a second dimension in a topic model,
using a switching variable so that individual word
tokens may be selected from either the topic or the
sentiment. However, our hypothesis is that individ-
ual word tokens reflect both the topic and the ge-
ographical aspect. Sharing this intuition, Paul and
Girju (2010) build topic-aspect models for the cross
product of topics and aspects. They do not impose
any regularity across multiple aspects of the same
topic, so this approach may not scale when the num-
ber of aspects is large (they consider only two as-
pects). We address this issue using cascading distri-
butions; when the observed data for a given region-
topic pair is low, the model falls back to the base
topic. The use of cascading logistic normal distri-
butions in topic models follows earlier work on dy-
namic topic models (Blei and Lafferty, 2006b; Xing,
2005).
9 Conclusion
This paper presents a model that jointly identifies
words with high regional affinity, geographically-
coherent linguistic regions, and the relationship be-
tween regional and topic variation. The key model-
ing assumption is that regions and topics interact to
shape observed lexical frequencies. We validate this
assumption on a prediction task in which our model
outperforms strong alternatives that do not distin-
guish regional and topical variation.
We see this work as a first step towards a unsuper-
vised methodology for modeling linguistic variation
using raw text. Indeed, in a study of morphosyn-
tactic variation, Szmrecsanyi (2010) finds that by
the most generous measure, geographical factors ac-
count for only 33% of the observed variation. Our
analysis might well improve if non-geographical
factors were considered, including age, race, gen-
der, income and whether a location is urban or ru-
ral. In some regions, estimates of many of these fac-
tors may be obtained by cross-referencing geogra-
phy with demographic data. We hope to explore this
possibility in future work.
Acknowledgments
We would like to thank Amr Ahmed, Jonathan Chang,
Shay Cohen, William Cohen, Ross Curtis, Miro Dud??k,
Scott Kiesling, Seyoung Kim, and the anonymous re-
viewers. This research was enabled by Google?s sup-
port of the Worldly Knowledge project at CMU, AFOSR
FA9550010247, ONR N0001140910758, NSF CAREER
DBI-0546594, NSF IIS-0713379, and an Alfred P. Sloan
Fellowship.
References
L. Backstrom, J. Kleinberg, R. Kumar, and J. Novak.
2008. Spatial variation in search engine queries. In
Proceedings of WWW.
C. M. Bishop. 2006. Pattern Recognition and Machine
Learning. Springer.
D. M. Blei and M. I. Jordan. 2006. Variational infer-
ence for Dirichlet process mixtures. Bayesian Analy-
sis, 1:121?144.
1286
D. M. Blei and J. Lafferty. 2006a. Correlated topic mod-
els. In NIPS.
D. M. Blei and J. Lafferty. 2006b. Dynamic topic mod-
els. In Proceedings of ICML.
D. M. Blei and J. D. McAuliffe. 2007. Supervised topic
models. In NIPS.
D. M. Blei, A. Y. Ng, and M. I. Jordan. 2003. Latent
Dirichlet alocation. JMLR, 3:993?1022.
M. Bucholtz, N. Bermudez, V. Fung, L. Edwards, and
R. Vargas. 2007. Hella Nor Cal or totally So Cal?
the perceptual dialectology of California. Journal of
English Linguistics, 35(4):325?352.
F. G. Cassidy and J. H. Hall. 1985. Dictionary of Amer-
ican Regional English, volume 1. Harvard University
Press.
J. Chambers. 2009. Sociolinguistic Theory: Linguistic
Variation and its Social Significance. Blackwell.
D. J Crandall, L. Backstrom, D. Huttenlocher, and
J. Kleinberg. 2009. Mapping the world?s photos. In
Proceedings of WWW, page 761770.
J. Friedman, T. Hastie, and R. Tibshirani. 2010. Regular-
ization paths for generalized linear models via coordi-
nate descent. Journal of Statistical Software, 33(1).
D. E. Johnson. 2009. Getting off the GoldVarb standard:
Introducing Rbrul for mixed-effects variable rule anal-
ysis. Language and Linguistics Compass, 3(1):359?
383.
B. Johnstone. 2010. Language and place. In R. Mesthrie
and W. Wolfram, editors, Cambridge Handbook of So-
ciolinguistics. Cambridge University Press.
M. Joshi, D. Das, K. Gimpel, and N. A. Smith. 2010.
Movie reviews and revenues: An experiment in text
regression. In Proceedings of NAACL-HLT.
H. Kurath. 1949. A Word Geography of the Eastern
United States. University of Michigan Press.
H. Kwak, C. Lee, H. Park, and S. Moon. 2010. What
is Twitter, a social network or a news media? In Pro-
ceedings of WWW.
W. Labov, S. Ash, and C. Boberg. 2006. The Atlas of
North American English: Phonetics, Phonology, and
Sound Change. Walter de Gruyter.
W. Labov. 1966. The Social Stratification of English in
New York City. Center for Applied Linguistics.
Q. Mei, C. Liu, H. Su, and C. X Zhai. 2006. A proba-
bilistic approach to spatiotemporal theme pattern min-
ing on weblogs. In Proceedings of WWW, page 542.
Q. Mei, X. Ling, M. Wondra, H. Su, and C. X. Zhai.
2007. Topic sentiment mixture: modeling facets and
opinions in weblogs. In Proceedings of WWW.
T. P. Minka. 2003. Estimating a Dirichlet distribution.
Technical report, Massachusetts Institute of Technol-
ogy.
J. Nerbonne. 2009. Data-driven dialectology. Language
and Linguistics Compass, 3(1).
B. O?Connor, M. Krieger, and D. Ahn. 2010. TweetMo-
tif: Exploratory search and topic summarization for
twitter. In Proceedings of ICWSM.
J. C. Paolillo. 2002. Analyzing Linguistic Variation: Sta-
tistical Models and Methods. CSLI Publications.
M. Paul and R. Girju. 2010. A two-dimensional topic-
aspect model for discovering multi-faceted topics. In
Proceedings of AAAI.
W. D. Penny. 2001. Variational Bayes for d-dimensional
Gaussian mixture models. Technical report, Univer-
sity College London.
D. Sankoff, S. A. Tagliamonte, and E. Smith. 2005.
Goldvarb X: A variable rule application for Macintosh
and Windows. Technical report, Department of Lin-
guistics, University of Toronto.
R. W. Sinnott. 1984. Virtues of the Haversine. Sky and
Telescope, 68(2).
B. Szmrecsanyi. 2010. Geography is overrated. In
S. Hansen, C. Schwarz, P. Stoeckle, and T. Streck, ed-
itors, Dialectological and Folk Dialectological Con-
cepts of Space. Walter de Gruyter.
S. A. Tagliamonte and D. Denis. 2008. Linguistic ruin?
LOL! Instant messanging and teen language. Ameri-
can Speech, 83.
S. A. Tagliamonte. 2006. Analysing Sociolinguistic Vari-
ation. Cambridge University Press.
M. J. Wainwright and M. I. Jordan. 2008. Graphical
Models, Exponential Families, and Variational Infer-
ence. Now Publishers.
E. P. Xing. 2005. On topic evolution. Technical Report
05-115, Center for Automated Learning and Discov-
ery, Carnegie Mellon University.
1287
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1487?1498,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Language Modeling with Power Low Rank Ensembles
Ankur P. Parikh
School of Computer Science
Carnegie Mellon University
apparikh@cs.cmu.edu
Avneesh Saluja
Electrical & Computer Engineering
Carnegie Mellon University
avneesh@cs.cmu.edu
Chris Dyer
School of Computer Science
Carnegie Mellon University
cdyer@cs.cmu.edu
Eric P. Xing
School of Computer Science
Carnegie Mellon University
epxing@cs.cmu.edu
Abstract
We present power low rank ensembles
(PLRE), a flexible framework for n-gram
language modeling where ensembles of
low rank matrices and tensors are used
to obtain smoothed probability estimates
of words in context. Our method can
be understood as a generalization of n-
gram modeling to non-integer n, and in-
cludes standard techniques such as abso-
lute discounting and Kneser-Ney smooth-
ing as special cases. PLRE training is effi-
cient and our approach outperforms state-
of-the-art modified Kneser Ney baselines
in terms of perplexity on large corpora as
well as on BLEU score in a downstream
machine translation task.
1 Introduction
Language modeling is the task of estimating the
probability of sequences of words in a language
and is an important component in, among other
applications, automatic speech recognition (Ra-
biner and Juang, 1993) and machine translation
(Koehn, 2010). The predominant approach to lan-
guage modeling is the n-gram model, wherein
the probability of a word sequence P (w
1
, . . . , w
`
)
is decomposed using the chain rule, and then a
Markov assumption is made: P (w
1
, . . . , w
`
) ?
?
`
i=1
P (w
i
|w
i?1
i?n+1
). While this assumption sub-
stantially reduces the modeling complexity, pa-
rameter estimation remains a major challenge.
Due to the power-law nature of language (Zipf,
1949), the maximum likelihood estimator mas-
sively overestimates the probability of rare events
and assigns zero probability to legitimate word se-
quences that happen not to have been observed in
the training data (Manning and Sch?utze, 1999).
Many smoothing techniques have been pro-
posed to address the estimation challenge. These
reassign probability mass (generally from over-
estimated events) to unseen word sequences,
whose probabilities are estimated by interpolating
with or backing off to lower order n-gram models
(Chen and Goodman, 1999).
Somewhat surprisingly, these widely used
smoothing techniques differ substantially from
techniques for coping with data sparsity in other
domains, such as collaborative filtering (Koren et
al., 2009; Su and Khoshgoftaar, 2009) or matrix
completion (Cand`es and Recht, 2009; Cai et al.,
2010). In these areas, low rank approaches based
on matrix factorization play a central role (Lee
and Seung, 2001; Salakhutdinov and Mnih, 2008;
Mackey et al., 2011). For example, in recom-
mender systems, a key challenge is dealing with
the sparsity of ratings from a single user, since
typical users will have rated only a few items. By
projecting the low rank representation of a user?s
(sparse) preferences into the original space, an es-
timate of ratings for new items is obtained. These
methods are attractive due to their computational
efficiency and mathematical well-foundedness.
In this paper, we introduce power low rank en-
sembles (PLRE), in which low rank tensors are
used to produce smoothed estimates for n-gram
probabilities. Ideally, we would like the low rank
structures to discover semantic and syntactic relat-
edness among words and n-grams, which are used
to produce smoothed estimates for word sequence
probabilities. In contrast to the few previous low
rank language modeling approaches, PLRE is not
orthogonal to n-gram models, but rather a gen-
eral framework where existing n-gram smoothing
methods such as Kneser-Ney smoothing are spe-
cial cases. A key insight is that PLRE does not
compute low rank approximations of the original
1487
joint count matrices (in the case of bigrams) or ten-
sors i.e. multi-way arrays (in the case of 3-grams
and above), but instead altered quantities of these
counts based on an element-wise power operation,
similar to how some smoothing methods modify
their lower order distributions.
Moreover, PLRE has two key aspects that lead
to easy scalability for large corpora and vocabu-
laries. First, since it utilizes the original n-grams,
the ranks required for the low rank matrices and
tensors tend to be remain tractable (e.g. around
100 for a vocabulary size V ? 1 ? 10
6
) leading
to fast training times. This differentiates our ap-
proach over other methods that leverage an under-
lying latent space such as neural networks (Bengio
et al., 2003; Mnih and Hinton, 2007; Mikolov et
al., 2010) or soft-class models (Saul and Pereira,
1997) where the underlying dimension is required
to be quite large to obtain good performance.
Moreover, at test time, the probability of a se-
quence can be queried in time O(?
max
) where
?
max
is the maximum rank of the low rank matri-
ces/tensors used. While this is larger than Kneser
Ney?s virtually constant query time, it is substan-
tially faster than conditional exponential family
models (Chen and Rosenfeld, 2000; Chen, 2009;
Nelakanti et al., 2013) and neural networks which
require O(V ) for exact computation of the nor-
malization constant. See Section 7 for a more de-
tailed discussion of related work.
Outline: We first review existing n-gram
smoothing methods (?2) and then present the in-
tuition behind the key components of our tech-
nique: rank (?3.1) and power (?3.2). We then
show how these can be interpolated into an ensem-
ble (?4). In the experimental evaluation on English
and Russian corpora (?5), we find that PLRE out-
performs Kneser-Ney smoothing and all its vari-
ants, as well as class-based language models. We
also include a comparison to the log-bilinear neu-
ral language model (Mnih and Hinton, 2007) and
evaluate performance on a downstream machine
translation task (?6) where our method achieves
consistent improvements in BLEU.
2 Discount-based Smoothing
We first provide background on absolute discount-
ing (Ney et al., 1994) and Kneser-Ney smooth-
ing (Kneser and Ney, 1995), two common n-gram
smoothing methods. Both methods can be formu-
lated as back-off or interpolated models; we de-
scribe the latter here since that is the basis of our
low rank approach.
2.1 Notation
Let c(w) be the count of word w, and similarly
c(w,w
i?1
) for the joint count of words w and
w
i?1
. For shorthand we will define w
j
i
to denote
the word sequence {w
i
, w
i+1
, ..., w
j?1
, w
j
}. Let
?
P (w
i
) refer to the maximum likelihood estimate
(MLE) of the probability of word w
i
, and simi-
larly
?
P (w
i
|w
i?1
) for the probability conditioned
on a history, or more generally,
?
P (w
i
|w
i?1
i?n+1
).
Let N
?
(w
i
) := |{w : c(w
i
, w) > 0}| be
the number of distinct words that appear be-
fore w
i
. More generally, let N
?
(w
i
i?n+1
) =
|{w : c(w
i
i?n+1
, w) > 0}|. Similarly, let
N
+
(w
i?1
i?n+1
) = |{w : c(w,w
i?1
i?n+1
) > 0}|. V
denotes the vocabulary size.
2.2 Absolute Discounting
Absolute discounting works on the idea of inter-
polating higher order n-gram models with lower-
order n-gram models. However, first some prob-
ability mass must be ?subtracted? from the higher
order n-grams so that the leftover probability can
be allocated to the lower order n-grams. More
specifically, define the following discounted con-
ditional probability:
?
P
D
(w
i
|w
i?1
i?n+1
) =
max{c(w
i
, w
i?1
i?n+1
)?D, 0}
c(w
i?1
i?n+1
)
Then absolute discounting P
abs
(?) uses the follow-
ing (recursive) equation:
P
abs
(w
i
|w
i?1
i?n+1
) =
?
P
D
(w
i
|w
i?1
i?n+1
)
+ ?(w
i?1
i?n+1
)P
abs
(w
i
|w
i?1
i?n+2
)
where ?(w
i?1
i?n+1
) is the leftover weight (due to
the discounting) that is chosen so that the con-
ditional distribution sums to one: ?(w
i?1
i?n+1
) =
D
c(w
i?1
i?n+1
)
N
+
(w
i?1
i?n+1
). For the base case, we set
P
abs
(w
i
) =
?
P (w
i
).
Discontinuity: Note that if c(w
i?1
i?n+1
) = 0, then
?(w
i?1
i?n+1
) =
0
0
, in which case ?(w
i?1
i?n+1
) is set
to 1. We will see that this discontinuity appears in
PLRE as well.
1488
2.3 Kneser Ney Smoothing
Ideally, the smoothed probability should preserve
the observed unigram distribution:
?
P (w
i
) =
?
w
i?1
i?n+1
P
sm
(w
i
|w
i?1
i?n+1
)
?
P (w
i?1
i?n+1
) (1)
where P
sm
(w
i
|w
i?1
i?n+1
) is the smoothed condi-
tional probability that a model outputs. Unfortu-
nately, absolute discounting does not satisfy this
property, since it exclusively uses the unaltered
MLE unigram model as its lower order model. In
practice, the lower order distribution is only uti-
lized when we are unsure about the higher order
distribution (i.e., when ?(?) is large). Therefore,
the unigram model should be altered to condition
on this fact.
This is the inspiration behind Kneser-Ney (KN)
smoothing, an elegant algorithm with robust per-
formance in n-gram language modeling. KN
smoothing defines alternate probabilities P
alt
(?):
P
alt
D
(w
i
|w
i?1
i?n
?
+1
) =
?
?
?
?
?
?
?
?
P
D
(w
i
|w
i?1
i?n
?
+1
), if n
?
= n
max{N
?
(w
i
i?n
?
+1
)?D,0}
?
w
i
N
?
(w
i
i?n
?
+1
)
, if n
?
< n
The base case for unigrams reduces to
P
alt
(w
i
) =
N
?
(w
i
)?
w
i
N
?
(w
i
)
. Intuitively P
alt
(w
i
) is
proportional to the number of unique words that
precede w
i
. Thus, words that appear in many dif-
ferent contexts will be given higher weight than
words that consistently appear after only a few
contexts. These alternate distributions are then
used with absolute discounting:
P
kn
(w
i
|w
i?1
i?n+1
) = P
alt
D
(w
i
|w
i?1
i?n+1
)
+ ?(w
i?1
i?n+1
)P
kn
(w
i
|w
i?1
i?n+2
) (2)
where we set P
kn
(w
i
) = P
alt
(w
i
). By definition,
KN smoothing satisfies the marginal constraint in
Eq. 1 (Kneser and Ney, 1995).
3 Power Low Rank Ensembles
In n-gram smoothing methods, if a bigram count
c(w
i
, w
i?1
) is zero, the unigram probabilities are
used, which is equivalent to assuming that w
i
and
w
i?1
are independent ( and similarly for general
n). However, in this situation, instead of back-
ing off to a 1-gram, we may like to back off to a
?1.5-gram? or more generally an order between 1
and 2 that captures a coarser level of dependence
between w
i
and w
i?1
and does not assume full in-
dependence.
Inspired by this intuition, our strategy is to con-
struct an ensemble of matrices and tensors that
not only consists of MLE-based count informa-
tion, but also contains quantities that represent lev-
els of dependence in-between the various orders in
the model. We call these combinations power low
rank ensembles (PLRE), and they can be thought
of as n-gram models with non-integer n. Our ap-
proach can be recursively formulated as:
P
plre
(w
i
|w
i?1
i?n+1
) = P
alt
D
0
(w
i
|w
i?1
i?n+1
)
+ ?
0
(w
i?1
i?n+1
)
(
ZD
1
(w
i
|w
i?1
i?n+1
) + .....
+ ?
??1
(w
i?1
i?n+1
)
(
ZD
?
(w
i
|w
i?1
i?n+1
)
+ ?
?
(w
i?1
i?n+1
)
(
P
plre
(w
i
|w
i?1
i?n+2
)
))
...
)
(3)
where Z
1
, ...,Z
?
are conditional probability ma-
trices that represent the intermediate n-gram or-
ders
1
and D is a discount function (specified in
?4).
This formulation begs answers to a few crit-
ical questions. How to construct matrices that
represent conditional probabilities for intermedi-
ate n? How to transform them in a way that
generalizes the altered lower order distributions
in KN smoothing? How to combine these matri-
ces such that the marginal constraint in Eq. 1 still
holds? The following propose solutions to these
three queries:
1. Rank (Section 3.1): Rank gives us a concrete
measurement of the dependence between w
i
and w
i?1
. By constructing low rank ap-
proximations of the bigram count matrix and
higher-order count tensors, we obtain matri-
ces that represent coarser dependencies, with
a rank one approximation implying that the
variables are independent.
2. Power (Section 3.2): In KN smoothing, the
lower order distributions are not the original
counts but rather altered estimates. We pro-
pose a continuous generalization of this alter-
ation by taking the element-wise power of the
counts.
1
with a slight abuse of notation, let ZD
j
be shorthand
for Z
j,D
j
1489
3. Creating the Ensemble (Section 4): Lastly,
PLRE also defines a way to interpolate the
specifically constructed intermediate n-gram
matrices. Unfortunately a constant discount,
as presented in Section 2, will not in general
preserve the lower order marginal constraint
(Eq. 1). We propose a generalized discount-
ing scheme to ensure the constraint holds.
3.1 Rank
We first show how rank can be utilized to construct
quantities between an n-gram and an n? 1-gram.
In general, we think of an n-gram as an n
th
or-
der tensor i.e. a multi-way array with n indices
{i
1
, ..., i
n
}. (A vector is a tensor of order 1, a ma-
trix is a tensor of order 2 etc.) Computing a spe-
cial rank one approximation of slices of this tensor
produces the n? 1-gram. Thus, taking rank ? ap-
proximations in this fashion allows us to represent
dependencies between an n-gram and n?1-gram.
Consider the bigram count matrix B with
N counts which has rank V . Note that
?
P (w
i
|w
i?1
) =
B(w
i
,w
i?1
)?
w
B(w,w
i?1
)
. Additionally, B
can be considered a random variable that is the re-
sult of sampling N tuples of (w
i
, w
i?1
) and ag-
glomerating them into a count matrix. Assum-
ing w
i
and w
i?1
are independent, the expected
value (with respect to the empirical distribution)
E[B] = NP (w
i
)P (w
i?1
), which can be rewrit-
ten as being proportional to the outer product of
the unigram probability vector with itself, and is
thus rank one.
This observation extends to higher order
n-grams as well. Let C
n
be the n
th
order tensor
where C
n
(w
i
, ...., w
i?n+1
) = c(w
i
, ..., w
i?n+1
).
Furthermore denote C
n
(:, w?
i?1
i?n+2
, :) to
be the V ? V matrix slice of C
n
where
w
i?n+2
, ..., w
i?1
are held fixed to a particular
sequence w?
i?n+2
, ..., w?
i?1
. Then if w
i
is con-
ditionally independent of w
i?n+1
given w
i?1
i?n+2
,
then E[C
n
(:, w?
i?1
i?n+2
, :)] is rank one ?w?
i?1
i?n+2
.
However, it is rare that these matrices are ac-
tually rank one, either due to sampling vari-
ance or the fact that w
i
and w
i?1
are not in-
dependent. What we would really like to say
is that the best rank one approximation B
(1)
(under some norm) of B is ?
?
P (w
i
)
?
P (w
i?1
).
While this statement is not true under the `
2
norm, it is true under generalized KL diver-
gence (Lee and Seung, 2001): gKL(A||B) =
?
ij
(
A
ij
log(
A
ij
B
ij
)?A
ij
+B
ij
)
)
.
In particular, generalized KL divergence pre-
serves row and column sums: if M
(?)
is the best
rank ? approximation of M under gKL then the
row sums and column sums of M
(?)
and M are
equal (Ho and Van Dooren, 2008). Leveraging
this property, it is straightforward to prove the fol-
lowing lemma:
Lemma 1. Let B
(?)
be the best rank ? ap-
proximation of B under gKL. Then B
(1)
?
?
P (w
i
)
?
P (w
i?1
) and ?w
i?1
s.t. c(w
i?1
) 6= 0:
?
P (w
i
) =
B
(1)
(w
i
, w
i?1
)
?
w
B
(1)
(w,w
i?1
)
For more general n, let C
n,(?)
i?1,...,i?n+2
be the
best rank ? approximation of C
n
(:, w?
i?1
i?n+2
, :
) under gKL. Then similarly, ?w
i?1
i?n+1
s.t.
c(w
i?1
i?n+1
) > 0:
?
P (w
i
|w
i?1
, ..., w
i?n+2
)
=
C
n,(1)
i?1,...,i?n+2
(w
i
, w
i?1
i?n+1
)
?
w
C
n,(1)
i?1,...,i?n+2
(w,w
i?1
i?n+1
)
(4)
Thus, by selecting 1 < ? < V , we obtain count
matrices and tensors between n and n ? 1-grams.
The condition that c(w
i?1
i?n+1
) > 0 corresponds to
the discontinuity discussed in ?2.2.
3.2 Power
Since KN smoothing alters the lower order distri-
butions instead of simply using the MLE, vary-
ing the rank is not sufficient in order to generalize
this suite of techniques. Thus, PLRE computes
low rank approximations of altered count matri-
ces. Consider taking the elementwise power ? of
the bigram count matrix, which is denoted by B
??
.
For example, the observed bigram count matrix
and associated row sum:
B
?1
=
(
1.0 2.0 1.0
0 5.0 0
2.0 0 0
)
row sum
?
(
4.0
5.0
2.0
)
As expected the row sum is equal to the uni-
gram counts (which we denote as u). Now con-
sider B
?0.5
:
B
?0.5
=
(
1.0 1.4 1.0
0 2.2 0
1.4 0 0
)
row sum
?
(
3.4
2.2
1.4
)
Note how the row sum vector has been altered.
In particular since w
1
(corresponding to the first
1490
row) has a more diverse history than w
2
, it has
a higher row sum (compared to in u where w
2
has the higher row sum). Lastly, consider the case
when p = 0:
B
?0
=
(
1.0 1.0 1.0
0 1.0 0
1.0 0 0
)
row sum
?
(
3.0
1.0
1.0
)
The row sum is now the number of unique words
that precede w
i
(since B
0
is binary) and is thus
equal to the (unnormalized) Kneser Ney unigram.
This idea also generalizes to higher order n-grams
and leads us to the following lemma:
Lemma 2. Let B
(?,?)
be the best rank ? ap-
proximation of B
??
under gKL. Then ?w
i?1
s.t.
c(w
i?1
) 6= 0:
P
alt
(w
i
) =
B
(0,1)
(w
i
, w
i?1
)
?
w
B
(0,1)
(w,w
i?1
)
For more general n, let C
n,(?,?)
i?1,...,i?n+2
be the best
rank ? approximation of C
n,(?)
(:, w?
i?1
i?n+2
, :) un-
der gKL. Similarly, ?w
i?1
i?n+1
s.t. c(w
i?1
i?n+1
) > 0:
P
alt
(w
i
|w
i?1
, ..., w
i?n+2
)
=
C
n,(0,1)
i?1,...,i?n+2
(w
i
, w
i?1
i?n+1
)
?
w
C
n,(0,1)
i?1,...,i?n+2
(w,w
i?1
i?n+1
)
(5)
4 Creating the Ensemble
Recall our overall formulation in Eq. 3; a naive
solution would be to set Z
1
, ...,Z
?
to low rank
approximations of the count matrices/tensors un-
der varying powers, and then interpolate through
constant absolute discounting. Unfortunately, the
marginal constraint in Eq. 1 will generally not hold
if this strategy is used. Therefore, we propose a
generalized discounting scheme where each non-
zero n-gram count is associated with a different
discount D
j
(w
i
, w
i?1
i?n
?
+1
). The low rank approxi-
mations are then computed on the discounted ma-
trices, leaving the marginal constraint intact.
For clarity of exposition, we focus on the spe-
cial case where n = 2 with only one low rank
matrix before stating our general algorithm:
P
plre
(w
i
|w
i?1
) =
?
PD
0
(w
i
|w
i?1
)
+ ?
0
(w
i?1
)
(
ZD
1
(w
i
|w
i?1
) + ?
1
(w
i?1
)P
alt
(w
i
)
)
(6)
Our goal is to compute D
0
,D
1
and Z
1
so
that the following lower order marginal constraint
holds:
?
P (w
i
) =
?
w
i?1
P
plre
(w
i
|w
i?1
)
?
P (w
i?1
) (7)
Our solution can be thought of as a two-
step procedure where we compute the discounts
D
0
,D
1
(and the ?(w
i?1
) weights as a by-
product), followed by the low rank quantity Z
1
.
First, we construct the following intermediate en-
semble of powered, but full rank terms. Let
Y
?
j
be the matrix such that Y
?
j
(w
i
, w
i?1
) :=
c(w
i
, w
i?1
)
?
j
. Then define
P
pwr
(w
i
|w
i?1
) := Y
(?
0
=1)
D
0
(w
i
|w
i?1
)
+ ?
0
(w
i?1
)
(
Y
(?
1
)
D
1
(w
i
|w
i?1
)
+ ?
1
(w
i?1
)Y
(?
2
=0)
(w
i
|w
i?1
)
)
(8)
where with a little abuse of notation:
Y
?
j
D
j
(w
i
|w
i?1
) =
c(w
i
, w
i?1
)
?
j
?D
j
(w
i
, w
i?1
)
?
w
i
c(w
i
, w
i?1
)
?
j
Note that P
alt
(w
i
) has been replaced with
Y
(?
2
=0)
(w
i
|w
i?1
), based on Lemma 2, and will
equal P
alt
(w
i
) once the low rank approximation is
taken as discussed in ? 4.2).
Since we have only combined terms of differ-
ent power (but all full rank), it is natural choose
the discounts so that the result remains unchanged
i.e., P
pwr
(w
i
|w
i?1
) =
?
P (w
i
|w
i?1
), since the low
rank approximation (not the power) will imple-
ment smoothing. Enforcing this constraint gives
rise to a set of linear equations that can be solved
(in closed form) to obtain the discounts as we now
show below.
4.1 Step 1: Computing the Discounts
To ensure the constraint that P
pwr
(w
i
|w
i?1
) =
?
P (w
i
|w
i?1
), it is sufficient to enforce the follow-
ing two local constraints:
Y
(?
j
)
(w
i
|w
i?1
) = Y
(?
j
)
D
j
(w
i
|w
i?1
)
+ ?
j
(w
i?1
)Y
(?
j+1
)
(w
i
|w
i?1
) for j = 0, 1
(9)
This allows each D
j
to be solved for indepen-
dently of the other {D
j
?
}
j
?
6=j
. Let c
i,i?1
=
c(w
i
, w
i?1
), c
j
i,i?1
= c(w
i
, w
i?1
)
?
j
, and d
j
i,i?1
=
1491
Dj
(w
i
, w
i?1
). Expanding Eq. 9 yields that
?w
i
, w
i?1
:
c
j
i,i?1
?
i
c
j
i,i?1
=
c
j
i,i?1
? d
j
i,i?1
?
i
c
j
i,i?1
+
(
?
i
d
j
i,i?1
?
i
c
j
i,i?1
)
c
j+1
i,i?1
?
i
c
j+1
i,i?1
(10)
which can be rewritten as:
?d
j
i,i?1
+
(
?
i
d
j
i,i?1
)
c
j+1
i,i?1
?
i
c
j+1
i,i?1
= 0 (11)
Note that Eq. 11 decouples across w
i?1
since the
only d
j
i,i?1
terms that are dependent are the ones
that share the preceding context w
i?1
.
It is straightforward to see that setting d
j
i,i?1
proportional to c
j+1
i,i?1
satisfies Eq. 11. Furthermore
it can be shown that all solutions are of this form
(i.e., the linear system has a null space of exactly
one). Moreover, we are interested in a particular
subset of solutions where a single parameter d
?
(independent of w
i?1
) controls the scaling as in-
dicated by the following lemma:
Lemma 3. Assume that ?
j
? ?
j+1
. Choose any
0 ? d
?
? 1. Set d
j
i,i?1
= d
?
c
j+1
i,i?1
?i, j. The
resulting discounts satisfy Eq. 11 as well as the
inequality constraints 0 ? d
j
i,i?1
? c
j
i,i?1
. Fur-
thermore, the leftover weight ?
j
takes the form:
?
j
(w
i?1
) =
?
i
d
j
i,i?1
?
i
c
j
i,i?1
=
d
?
?
i
c
j+1
i,i?1
?
i
c
j
i,i?1
Proof. Clearly this choice of d
j
i,i?1
satisfies
Eq. 11. The largest possible value of d
j
i,i?1
is
c
j+1
i,i?1
. ?
j
? ?
j+1
, implies c
j
i,i?1
? c
j+1
i,i?1
. Thus
the inequality constraints are met. It is then easy
to verify that ? takes the above form.
The above lemma generalizes to longer contexts
(i.e. n > 2) as shown in Algorithm 1. Note that if
?
j
= ?
j+1
then Algorithm 1 is equivalent to scal-
ing the counts e.g. deleted-interpolation/Jelinek
Mercer smoothing (Jelinek and Mercer, 1980). On
the other hand, when ?
j+1
= 0, Algorithm 1
is equal to the absolute discounting that is used
in Kneser-Ney. Thus, depending on ?
j+1
, our
method generalizes different types of interpola-
tion schemes to construct an ensemble so that the
marginal constraint is satisfied.
Algorithm 1 Compute D
In: Count tensor C
n
, powers ?
j
, ?
j+1
such that
?
j
? ?
j+1
, and parameter d
?
.
Out: Discount D
j
for powered counts C
n,(?
j
)
and associated leftover weight ?
j
1: Set D
j
(w
i
, w
i?1
i?n+1
) = d
?
c(w
i
, w
i?1
i?n+1
)
?
j+1
.
2:
?
j
(w
i
, w
i?1
i?n+1
) =
d
?
?
w
i
c(w
i
, w
i?1
i?n+1
)
?
j+1
?
w
i
c(w
i
, w
i?1
i?n+1
)
?
j
Algorithm 2 Compute Z
In: Count tensor C
n
, power ?, discounts D, rank
?
Out: Discounted low rank conditional probability
table Z
(?,?)
D (wi|w
i?1
i?n+1
) (represented implicitly)
1: Compute powered counts C
n,(??)
.
2: Compute denominators
?
w
i
c(w
i
, w
i?1
i?n+1
)
?
?w
i?1
i?n+1
s.t. c(w
i?1
i?n+1
) > 0.
3: Compute discounted powered counts
C
n,(??)
D = C
n,(??)
?D.
4: For each slice M
w?
i?1
i?n+2
:= C
n,(??)
D (:
, w?
i?1
i?n+2
, :) compute
M
(?)
:= min
A?0:rank(A)=?
?M
w?
i?1
i?n+2
?A?
KL
(stored implicitly as M
(?)
= LR)
Set Z
(?,?)
D (:, w?
i?1
i?n+2
, :) = M
(?)
5: Note that
Z
(?,?)
D (wi|w
i?1
i?n+1
) =
Z
(?,?)
D (wi, w
i?1
i?n+1
)
?
w
i
c(w
i
, w
i?1
i?n+1
)
?
4.2 Step 2: Computing Low Rank Quantities
The next step is to compute low rank approxi-
mations ofY
(?
j
)
D
j
to obtainZD
j
such that the inter-
mediate marginal constraint in Eq. 7 is preserved.
This constraint trivially holds for the intermediate
ensemble P
pwr
(w
i
|w
i?1
) due to how the discounts
were derived in ? 4.1. For our running bigram ex-
ample, define Z
(?
j
,?
j
)
D
j
to be the best rank ?
j
ap-
proximation to Y
(?
j
,?
j
)
D
j
according to gKL and let
Z
?
j
,?
j
D
j
(w
i
|w
i?1
) =
Z
?
j
,?
j
D
j
(w
i
, w
i?1
)
?
w
i
c(w
i
, w
i?1
)
?
j
Note that Z
?
j
,?
j
D
j
(w
i
|w
i?1
) is a valid (discounted)
conditional probability since gKL preserves
row/column sums so the denominator remains un-
changed under the low rank approximation. Then
1492
using the fact that Z
(0,1)
(w
i
|w
i?1
) = P
alt
(w
i
)
(Lemma 2) we can embellish Eq. 6 as
P
plre
(w
i
|w
i?1
) = PD
0
(w
i
|w
i?1
)+
?
0
(w
i?1
)
(
Z
(?
1
,?
1
)
D
1
(w
i
|w
i?1
) + ?
1
(w
i?1
)P
alt
(w
i
)
)
Leveraging the form of the discounts and
row/column sum preserving property of gKL, we
then have the following lemma (the proof is in the
supplementary material):
Lemma 4. Let P
plre
(w
i
|w
i?1
) indicate the PLRE
smoothed conditional probability as computed by
Eq. 6 and Algorithms 1 and 2. Then, the marginal
constraint in Eq. 7 holds.
4.3 More general algorithm
In general, the principles outlined in the previ-
ous sections hold for higher order n-grams. As-
sume that the discounts are computed according
to Algorithm 1 with parameter d
?
and Z
(?
j
,?
j
)
D
j
is
computed according to Algorithm 2. Note that, as
shown in Algorithm 2, for higher order n-grams,
theZ
(?
j
,?
j
)
D
j
are created by taking low rank approx-
imations of slices of the (powered) count tensors
(see Lemma 2 for intuition). Eq. 3 can now be
embellished:
P
plre
(w
i
|w
i?1
i?n+1
) = P
alt
D
0
(w
i
|w
i?1
i?n+1
)
+ ?
0
(w
i?1
i?n+1
)
(
Z
(?
1
,?
1
)
D
1
(w
i
|w
i?1
i?n+1
) + .....
+ ?
??1
(w
i?1
i?n+1
)
(
Z
(?
?
,?
?
)
D
?
(w
i
|w
i?1
i?n+1
)
+ ?
?
(w
i?1
i?n+1
)
(
P
plre
(w
i
|w
i?1
i?n+2
)
))
...
)
(12)
Lemma 4 also applies in this case and is given in
Theorem 1 in the supplementary material.
4.4 Links with KN Smoothing
In this section, we explicitly show the relation-
ship between PLRE and KN smoothing. Rewrit-
ing Eq. 12 in the following form:
P
plre
(w
i
|w
i?1
i?n+1
) = P
terms
plre
(w
i
|w
i?1
i?n+1
)
+?
0:?
(w
i?1
i?n+1
)P
plre
(w
i
|w
i?1
i?n+2
) (13)
where P
terms
plre
(w
i
|w
i?1
i?n+1
) contains the terms in
Eq. 12 except the last, and ?
0:?
(w
i?1
i?n+1
) =
?
?
h=0
?
h
(w
i?1
i?n+1
), we can leverage the form of
the discount, and using the fact that ?
?+1
= 0
2
:
?
0:?
(w
i?1
i?n?1
) =
d
?
?+1
N
+
(w
i?1
i?n+1
)
c(w
i?1
i?n+1
)
With this form of ?(?), Eq. 13 is remarkably sim-
ilar to KN smoothing (Eq. 2) if KN?s discount pa-
rameter D is chosen to equal (d
?
)
?+1
.
The difference is that P
alt
(?) has been replaced
with the alternate estimate P
terms
plre
(w
i
|w
i?1
i?n+1
),
which have been enriched via the low rank struc-
ture. Since these alternate estimates were con-
structed via our ensemble strategy they contain
both very fine-grained dependencies (the origi-
nal n-grams) as well as coarser dependencies (the
lower rank n-grams) and is thus fundamentally
different than simply taking a single matrix/tensor
decomposition of the trigram/bigram matrices.
Moreover, it provides a natural way of setting
d
?
based on the Good-Turing (GT) estimates em-
ployed by KN smoothing. In particular, we can set
d
?
to be the (? + 1)
th
root of the KN discount D
that can be estimated via the GT estimates.
4.5 Computational Considerations
PLRE scales well even as the order n increases.
To compute a low rank bigram, one low rank ap-
proximation of a V ? V matrix is required. For
the low rank trigram, we need to compute a low
rank approximation of each slice C
n,(?p)
D (:, w?i?1, :
) ?w?
i?1
. While this may seem daunting at first, in
practice the size of each slice (number of non-zero
rows/columns) is usually much, much smaller than
V , keeping the computation tractable.
Similarly, PLRE also evaluates conditional
probabilities at evaluation time efficiently. As
shown in Algorithm 2, the normalizer can be pre-
computed on the sparse powered matrix/tensor. As
a result our test complexity is O(
?
?
total
i=1
?
i
) where
?
total
is the total number of matrices/tensors in
the ensemble. While this is larger than Kneser
Ney?s practically constant complexity of O(n),
it is much faster than other recent methods for
language modeling such as neural networks and
conditional exponential family models where ex-
act computation of the normalizing constant costs
O(V ).
5 Experiments
To evaluate PLRE, we compared its performance
on English and Russian corpora with several vari-
2
for derivation see proof of Lemma 4 in the supplemen-
tary material
1493
ants of KN smoothing, class-based models, and
the log-bilinear neural language model (Mnih and
Hinton, 2007). We evaluated with perplexity in
most of our experiments, but also provide results
evaluated with BLEU (Papineni et al., 2002) on a
downstream machine translation (MT) task. We
have made the code for our approach publicly
available
3
.
To build the hard class-based LMs, we utilized
mkcls
4
, a tool to train word classes that uses
the maximum likelihood criterion (Och, 1995) for
classing. We subsequently trained trigram class
language models on these classes (correspond-
ing to 2
nd
-order HMMs) using SRILM (Stolcke,
2002), with KN-smoothing for the class transition
probabilities. SRILM was also used for the base-
line KN-smoothed models.
For our MT evaluation, we built a hierarchi-
cal phrase translation (Chiang, 2007) system us-
ing cdec (Dyer et al., 2010). The KN-smoothed
models in the MT experiments were compiled us-
ing KenLM (Heafield, 2011).
5.1 Datasets
For the perplexity experiments, we evaluated our
proposed approach on 4 datasets, 2 in English and
2 in Russian. In all cases, the singletons were re-
placed with ?<unk>? tokens in the training cor-
pus, and any word not in the vocabulary was re-
placed with this token during evaluation. There is
a general dearth of evaluation on large-scale cor-
pora in morphologically rich languages such as
Russian, and thus we have made the processed
Large-Russian corpus available for comparison
3
.
? Small-English: APNews corpus (Bengio et al.,
2003): Train - 14 million words, Dev - 963,000,
Test - 963,000. Vocabulary- 18,000 types.
? Small-Russian: Subset of Russian news com-
mentary data from 2013 WMT translation task
5
:
Train- 3.5 million words, Dev - 400,000 Test -
400,000. Vocabulary - 77,000 types.
? Large-English: English Gigaword, Training -
837 million words, Dev - 8.7 million, Test - 8.7
million. Vocabulary- 836,980 types.
? Large-Russian: Monolingual data from WMT
2013 task. Training - 521 million words, Vali-
dation - 50,000, Test - 50,000. Vocabulary- 1.3
million types.
3
http://www.cs.cmu.edu/?apparikh/plre.html
4
http://code.google.com/p/giza-pp/
5
http://www.statmt.org/wmt13/training-monolingual-
nc-v8.tgz
For the MT evaluation, we used the parallel data
from the WMT 2013 shared task, excluding the
Common Crawl corpus data. The newstest2012
and newstest2013 evaluation sets were used as the
development and test sets respectively.
5.2 Small Corpora
For the class-based baseline LMs, the
number of classes was selected from
{32, 64, 128, 256, 512, 1024} (Small-English)
and {512, 1024} (Small-Russian). We could not
go higher due to the computationally laborious
process of hard clustering. For Kneser-Ney, we
explore four different variants: back-off (BO-KN)
interpolated (int-KN), modified back-off (BO-
MKN), and modified interpolated (int-MKN).
Good-Turing estimates were used for discounts.
All models trained on the small corpora are of
order 3 (trigrams).
For PLRE, we used one low rank bigram and
one low rank trigram in addition to the MLE n-
gram estimates. The powers of the intermediate
matrices/tensors were fixed to be 0.5 and the dis-
counts were set to be square roots of the Good Tur-
ing estimates (as explained in ? 4.4). The ranks
were tuned on the development set. For Small-
English, the ranges were {1e ? 3, 5e ? 3} (as a
fraction of the vocabulary size) for both the low
rank bigram and low rank trigram models. For
Small-Russian the ranges were {5e ? 4, 1e ? 3}
for both the low rank bigram and the low rank tri-
gram models.
The results are shown in Table 1. The best class-
based LM is reported, but is not competitive with
the KN baselines. PLRE outperforms all of the
baselines comfortably. Moreover, PLRE?s perfor-
mance over the baselines is highlighted in Russian.
With larger vocabulary sizes, the low rank ap-
proach is more effective as it can capture linguistic
similarities between rare and common words.
Next we discuss how the maximum n-gram or-
der affects performance. Figure 1 shows the rela-
tive percentage improvement of our approach over
int-MKN as the order is increased from 2 to 4 for
both methods. The Small-English dataset has a
rather small vocabulary compared to the number
of tokens, leading to lower data sparsity in the bi-
gram. Thus the PLRE improvement is small for
order = 2, but more substantial for order = 3. On
the other hand, for the Small-Russian dataset, the
vocabulary size is much larger and consequently
the bigram counts are sparser. This leads to sim-
1494
Dataset class-1024(3) BO-KN(3) int-KN(3) BO-MKN(3) int-MKN(3) PLRE(3)
Small-English Dev 115.64 99.20 99.73 99.95 95.63 91.18
Small-English Test 119.70 103.86 104.56 104.55 100.07 95.15
Small-Russian Dev 286.38 281.29 265.71 287.19 263.25 241.66
Small-Russian Test 284.09 277.74 262.02 283.70 260.19 238.96
Table 1: Perplexity results on small corpora for all methods.
Small-Russian
Small-English
Figure 1: Relative percentage improvement of
PLRE over int-MKN as the maximum n-gram or-
der for both methods is increased.
ilar improvements for all orders (which are larger
than that for Small-English).
On both these datasets, we also experimented
with tuning the discounts for int-MKN to see if
the baseline could be improved with more careful
choices of discounts. However, this achieved only
marginal gains (reducing the perplexity to 98.94
on the Small-English test set and 259.0 on the
Small-Russian test set).
Comparison to LBL (Mnih and Hinton,
2007): Mnih and Hinton (2007) evaluate on the
Small-English dataset (but remove end markers
and concatenate the sentences). They obtain per-
plexities 117.0 and 107.8 using contexts of size 5
and 10 respectively. With this preprocessing, a 4-
gram (context 3) PLRE achieves 108.4 perplexity.
5.3 Large Corpora
Results on the larger corpora for the top 2 per-
forming methods ?PLRE? and ?int-MKN? are pre-
sented in Table 2. Due to the larger training size,
we use 4-gram models in these experiments. How-
ever, including the low rank 4-gram tensor pro-
vided little gain and therefore, the 4-gram PLRE
only has additional low rank bigram and low rank
trigram matrices/tensors. As above, ranks were
tuned on the development set. For Large-English,
the ranges were {1e?4, 5e?4, 1e?3} (as a frac-
tion of the vocabulary size) for both the low rank
Dataset int-MKN(4) PLRE(4)
Large-English Dev 73.21 71.21
Large-English Test 77.90 ? 0.203 75.66 ? 0.189
Large-Russian Dev 326.9 297.11
Large-Russian Test 289.63 ? 6.82 264.59 ? 5.839
Table 2: Mean perplexity results on large corpora,
with standard deviation.
Dataset PLRE Training Time
Small-English 3.96 min ( order 3) / 8.3 min (order 4)
Small-Russian 4.0 min (order 3) / 4.75 min (order 4)
Large-English 3.2 hrs (order 4)
Large-Russian 8.3 hrs (order 4)
Table 3: PLRE training times for a fixed parameter
setting
6
. 8 Intel Xeon CPUs were used.
Method BLEU
int-MKN(4) 17.63 ? 0.11
PLRE(4) 17.79 ? 0.07
Smallest Diff PLRE+0.05
Largest Diff PLRE+0.29
Table 4: Results on English-Russian translation
task (mean ? stdev). See text for details.
bigram and low rank trigram models. For Small-
Russian the ranges were {1e?5, 5e?5, 1e?4} for
both the low rank bigram and the low rank trigram
models. For statistical validity, 10 test sets of size
equal to the original test set were generated by ran-
domly sampling sentences with replacement from
the original test set. Our method outperforms ?int-
MKN? with gains similar to that on the smaller
datasets. As shown in Table 3, our method obtains
fast training times even for large datasets.
6 Machine Translation Task
Table 4 presents results for the MT task, trans-
lating from English to Russian
7
. We used
MIRA (Chiang et al., 2008) to learn the feature
weights. To control for the randomness in MIRA,
we avoid retuning when switching LMs - the set
of feature weights obtained using int-MKN is the
same, only the language model changes. The
6
As described earlier, only the ranks need to be tuned, so
only 2-3 low rank bigrams and 2-3 low rank trigrams need to
be computed (and combined depending on the setting).
7
the best score at WMT 2013 was 19.9 (Bojar et al.,
2013)
1495
procedure is repeated 10 times to control for op-
timizer instability (Clark et al., 2011). Unlike
other recent approaches where an additional fea-
ture weight is tuned for the proposed model and
used in conjunction with KN smoothing (Vaswani
et al., 2013), our aim is to show the improvements
that PLRE provides as a substitute for KN. On av-
erage, PLRE outperforms the KN baseline by 0.16
BLEU, and this improvement is consistent in that
PLRE never gets a worse BLEU score.
7 Related Work
Recent attempts to revisit the language model-
ing problem have largely come from two direc-
tions: Bayesian nonparametrics and neural net-
works. Teh (2006) and Goldwater et al. (2006)
discovered the connection between interpolated
Kneser Ney and the hierarchical Pitman-Yor pro-
cess. These have led to generalizations that ac-
count for domain effects (Wood and Teh, 2009)
and unbounded contexts (Wood et al., 2009).
The idea of using neural networks for language
modeling is not new (Miikkulainen and Dyer,
1991), but recent efforts (Mnih and Hinton, 2007;
Mikolov et al., 2010) have achieved impressive
performance. These methods can be quite expen-
sive to train and query (especially as the vocab-
ulary size increases). Techniques such as noise
contrastive estimation (Gutmann and Hyv?arinen,
2012; Mnih and Teh, 2012; Vaswani et al., 2013),
subsampling (Xu et al., 2011), or careful engi-
neering approaches for maximum entropy LMs
(which can also be applied to neural networks)
(Wu and Khudanpur, 2000) have improved train-
ing of these models, but querying the probabil-
ity of the next word given still requires explicitly
normalizing over the vocabulary, which is expen-
sive for big corpora or in languages with a large
number of word types. Mnih and Teh (2012) and
Vaswani et al. (2013) propose setting the normal-
ization constant to 1, but this is approximate and
thus can only be used for downstream evaluation,
not for perplexity computation. An alternate tech-
nique is to use word-classing (Goodman, 2001;
Mikolov et al., 2011), which can reduce the cost
of exact normalization to O(
?
V ). In contrast, our
approach is much more scalable, since it is triv-
ially parallelized in training and does not require
explicit normalization during evaluation.
There are a few low rank approaches (Saul and
Pereira, 1997; Bellegarda, 2000; Hutchinson et al.,
2011), but they are only effective in restricted set-
tings (e.g. small training sets, or corpora divided
into documents) and do not generally perform
comparably to state-of-the-art models. Roark et
al. (2013) also use the idea of marginal constraints
for re-estimating back-off parameters for heavily-
pruned language models, whereas we use this con-
cept to estimate n-gram specific discounts.
8 Conclusion
We presented power low rank ensembles, a tech-
nique that generalizes existing n-gram smoothing
techniques to non-integer n. By using ensembles
of sparse as well as low rank matrices and ten-
sors, our method captures both the fine-grained
and coarse structures in word sequences. Our
discounting strategy preserves the marginal con-
straint and thus generalizes Kneser Ney, and un-
der slight changes can also extend other smooth-
ing methods such as deleted-interpolation/Jelinek-
Mercer smoothing. Experimentally, PLRE con-
vincingly outperforms Kneser-Ney smoothing as
well as class-based baselines.
Acknowledgements
This work was supported by NSF IIS1218282,
NSF IIS1218749, NSF IIS1111142, NIH
R01GM093156, the U. S. Army Research Labo-
ratory and the U. S. Army Research Office under
contract/grant number W911NF-10-1-0533, the
NSF Graduate Research Fellowship Program
under Grant No. 0946825 (NSF Fellowship to
APP), and a grant from Ebay Inc. (to AS).
References
Jerome R. Bellegarda. 2000. Large vocabulary speech
recognition with multispan statistical language mod-
els. IEEE Transactions on Speech and Audio Pro-
cessing, 8(1):76?84.
Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. J. Mach. Learn. Res., 3:1137?1155,
March.
Ond?rej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut, and
Lucia Specia. 2013. Findings of the 2013 Work-
shop on Statistical Machine Translation. In Pro-
ceedings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 1?44, Sofia, Bulgaria, Au-
gust. Association for Computational Linguistics.
Jian-Feng Cai, Emmanuel J Cand`es, and Zuowei Shen.
2010. A singular value thresholding algorithm for
1496
matrix completion. SIAM Journal on Optimization,
20(4):1956?1982.
Emmanuel J Cand`es and Benjamin Recht. 2009. Exact
matrix completion via convex optimization. Foun-
dations of Computational mathematics, 9(6):717?
772.
Stanley F. Chen and Joshua Goodman. 1999. An
empirical study of smoothing techniques for lan-
guage modeling. Computer Speech & Language,
13(4):359?393.
Stanley F Chen and Ronald Rosenfeld. 2000. A survey
of smoothing techniques for me models. Speech and
Audio Processing, IEEE Transactions on, 8(1):37?
50.
Stanley F. Chen. 2009. Shrinking exponential lan-
guage models. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, NAACL ?09, pages
468?476, Stroudsburg, PA, USA. Association for
Computational Linguistics.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 224?233. Association for Com-
putational Linguistics.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Comput. Linguist., 33(2):201?228, June.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis testing for
statistical machine translation: Controlling for op-
timizer instability. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies: Short
Papers - Volume 2, HLT ?11, pages 176?181.
Chris Dyer, Jonathan Weese, Hendra Setiawan, Adam
Lopez, Ferhan Ture, Vladimir Eidelman, Juri Gan-
itkevitch, Phil Blunsom, and Philip Resnik. 2010.
cdec: A decoder, alignment, and learning framework
for finite-state and context-free translation models.
In Proceedings of the ACL 2010 System Demonstra-
tions, pages 7?12. Association for Computational
Linguistics.
Sharon Goldwater, Thomas Griffiths, and Mark John-
son. 2006. Interpolating between types and tokens
by estimating power-law generators. In Advances in
Neural Information Processing Systems, volume 18.
Joshua Goodman. 2001. Classes for fast maximum
entropy training. In Acoustics, Speech, and Signal
Processing, 2001. Proceedings.(ICASSP?01). 2001
IEEE International Conference on, volume 1, pages
561?564. IEEE.
Michael Gutmann and Aapo Hyv?arinen. 2012. Noise-
contrastive estimation of unnormalized statistical
models, with applications to natural image statistics.
Journal of Machine Learning Research, 13:307?
361.
Kenneth Heafield. 2011. KenLM: faster and smaller
language model queries. In Proceedings of the
EMNLP 2011 Sixth Workshop on Statistical Ma-
chine Translation, pages 187?197, Edinburgh, Scot-
land, United Kingdom, July.
Ngoc-Diep Ho and Paul Van Dooren. 2008. Non-
negative matrix factorization with fixed row and col-
umn sums. Linear Algebra and its Applications,
429(5):1020?1025.
Brian Hutchinson, Mari Ostendorf, and Maryam Fazel.
2011. Low rank language models for small training
sets. Signal Processing Letters, IEEE, 18(9):489?
492.
Frederick Jelinek and Robert Mercer. 1980. Interpo-
lated estimation of markov source parameters from
sparse data. Pattern recognition in practice.
Reinhard Kneser and Hermann Ney. 1995. Im-
proved backing-off for m-gram language modeling.
In Acoustics, Speech, and Signal Processing, 1995.
ICASSP-95., 1995 International Conference on, vol-
ume 1, pages 181?184. IEEE.
Philipp Koehn. 2010. Statistical Machine Translation.
Cambridge University Press, New York, NY, USA,
1st edition.
Yehuda Koren, Robert Bell, and Chris Volinsky. 2009.
Matrix factorization techniques for recommender
systems. Computer, 42(8):30?37.
Daniel D. Lee and H. Sebastian Seung. 2001. Algo-
rithms for non-negative matrix factorization. Ad-
vances in Neural Information Processing Systems,
13:556?562.
Lester Mackey, Ameet Talwalkar, and Michael I Jor-
dan. 2011. Divide-and-conquer matrix factoriza-
tion. arXiv preprint arXiv:1107.0789.
Christopher D Manning and Hinrich Sch?utze. 1999.
Foundations of statistical natural language process-
ing, volume 999. MIT Press.
Risto Miikkulainen and Michael G. Dyer. 1991. Natu-
ral language processing with modular pdp networks
and distributed lexicon. Cognitive Science, 15:343?
399.
Tom Mikolov, Martin Karafit, Luk Burget, Jan ernock,
and Sanjeev Khudanpur. 2010. Recurrent neu-
ral network based language model. In Proceed-
ings of the 11th Annual Conference of the Interna-
tional Speech Communication Association (INTER-
SPEECH 2010), volume 2010, pages 1045?1048.
International Speech Communication Association.
1497
Tomas Mikolov, Stefan Kombrink, Lukas Burget,
JH Cernocky, and Sanjeev Khudanpur. 2011.
Extensions of recurrent neural network language
model. In Acoustics, Speech and Signal Processing
(ICASSP), 2011 IEEE International Conference on,
pages 5528?5531. IEEE.
Andriy Mnih and Geoffrey Hinton. 2007. Three new
graphical models for statistical language modelling.
In Proceedings of the 24th international conference
on Machine learning, pages 641?648. ACM.
Andriy Mnih and Yee Whye Teh. 2012. A fast and
simple algorithm for training neural probabilistic
language models. In Proceedings of the Interna-
tional Conference on Machine Learning.
Anil Kumar Nelakanti, Cedric Archambeau, Julien
Mairal, Francis Bach, and Guillaume Bouchard.
2013. Structured penalties for log-linear language
models. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Process-
ing, pages 233?243, Seattle, Washington, USA, Oc-
tober. Association for Computational Linguistics.
Hermann Ney, Ute Essen, and Reinhard Kneser.
1994. On Structuring Probabilistic Dependencies in
Stochastic Language Modelling. Computer Speech
and Language, 8:1?38.
Franz Josef Och. 1995. Maximum-likelihood-
sch?atzung von wortkategorien mit verfahren der
kombinatorischen optimierung. Bachelor?s thesis
(Studienarbeit), University of Erlangen.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei
jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. pages 311?318.
Lawrence Rabiner and Biing-Hwang Juang. 1993.
Fundamentals of speech recognition.
Brian Roark, Cyril Allauzen, and Michael Riley. 2013.
Smoothed marginal distribution constraints for lan-
guage modeling. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics (ACL), pages 43?52.
Ruslan Salakhutdinov and Andriy Mnih. 2008.
Bayesian probabilistic matrix factorization using
Markov chain Monte Carlo. In Proceedings of the
25th international conference on Machine learning,
pages 880?887. ACM.
Lawrence Saul and Fernando Pereira. 1997. Aggre-
gate and mixed-order markov models for statistical
language processing. In Proceedings of the sec-
ond conference on empirical methods in natural lan-
guage processing, pages 81?89. Somerset, New Jer-
sey: Association for Computational Linguistics.
Andreas Stolcke. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. In Proceedings of the In-
ternational Conference in Spoken Language Pro-
cessing.
Xiaoyuan Su and Taghi M Khoshgoftaar. 2009. A sur-
vey of collaborative filtering techniques. Advances
in artificial intelligence, 2009:4.
Yee Whye Teh. 2006. A hierarchical bayesian lan-
guage model based on pitman-yor processes. In
Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics, pages 985?992. Association for Computa-
tional Linguistics.
Ashish Vaswani, Yinggong Zhao, Victoria Fossum,
and David Chiang. 2013. Decoding with large-
scale neural language models improves translation.
In Proceedings of the 2013 Conference on Em-
pirical Methods in Natural Language Processing,
pages 1387?1392, Seattle, Washington, USA, Oc-
tober. Association for Computational Linguistics.
Frank Wood and Yee Whye Teh. 2009. A hierarchical
nonparametric Bayesian approach to statistical lan-
guage model domain adaptation. In Artificial Intel-
ligence and Statistics, pages 607?614.
Frank Wood, C?edric Archambeau, Jan Gasthaus,
Lancelot James, and Yee Whye Teh. 2009. A
stochastic memoizer for sequence data. In Proceed-
ings of the 26th Annual International Conference on
Machine Learning, pages 1129?1136. ACM.
Jun Wu and Sanjeev Khudanpur. 2000. Efficient train-
ing methods for maximum entropy language model-
ing. In Interspeech, pages 114?118.
Puyang Xu, Asela Gunawardana, and Sanjeev Khu-
danpur. 2011. Efficient subsampling for training
complex language models. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP ?11, pages 1128?1136,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
George Zipf. 1949. Human behaviour and the prin-
ciple of least-effort. Addison-Wesley, Cambridge,
MA.
1498
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1365?1374,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Discovering Sociolinguistic Associations with Structured Sparsity
Jacob Eisenstein Noah A. Smith Eric P. Xing
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{jacobeis,nasmith,epxing}@cs.cmu.edu
Abstract
We present a method to discover robust and
interpretable sociolinguistic associations from
raw geotagged text data. Using aggregate de-
mographic statistics about the authors? geo-
graphic communities, we solve a multi-output
regression problem between demographics
and lexical frequencies. By imposing a com-
posite `1,? regularizer, we obtain structured
sparsity, driving entire rows of coefficients
to zero. We perform two regression studies.
First, we use term frequencies to predict de-
mographic attributes; our method identifies a
compact set of words that are strongly asso-
ciated with author demographics. Next, we
conjoin demographic attributes into features,
which we use to predict term frequencies. The
composite regularizer identifies a small num-
ber of features, which correspond to com-
munities of authors united by shared demo-
graphic and linguistic properties.
1 Introduction
How is language influenced by the speaker?s so-
ciocultural identity? Quantitative sociolinguistics
usually addresses this question through carefully
crafted studies that correlate individual demographic
attributes and linguistic variables?for example, the
interaction between income and the ?dropped r? fea-
ture of the New York accent (Labov, 1966). But
such studies require the knowledge to select the
?dropped r? and the speaker?s income, from thou-
sands of other possibilities. In this paper, we present
a method to acquire such patterns from raw data. Us-
ing multi-output regression with structured sparsity,
our method identifies a small subset of lexical items
that are most influenced by demographics, and dis-
covers conjunctions of demographic attributes that
are especially salient for lexical variation.
Sociolinguistic associations are difficult to model,
because the space of potentially relevant interactions
is large and complex. On the linguistic side there
are thousands of possible variables, even if we limit
ourselves to unigram lexical features. On the demo-
graphic side, the interaction between demographic
attributes is often non-linear: for example, gender
may negate or amplify class-based language differ-
ences (Zhang, 2005). Thus, additive models which
assume that each demographic attribute makes a lin-
ear contribution are inadequate.
In this paper, we explore the large space of po-
tential sociolinguistic associations using structured
sparsity. We treat the relationship between language
and demographics as a set of multi-input, multi-
output regression problems. The regression coeffi-
cients are arranged in a matrix, with rows indicating
predictors and columns indicating outputs. We ap-
ply a composite regularizer that drives entire rows
of the coefficient matrix to zero, yielding compact,
interpretable models that reuse features across dif-
ferent outputs. If we treat the lexical frequencies
as inputs and the author?s demographics as outputs,
the induced sparsity pattern reveals the set of lexi-
cal items that is most closely tied to demographics.
If we treat the demographic attributes as inputs and
build a model to predict the text, we can incremen-
tally construct a conjunctive feature space of demo-
graphic attributes, capturing key non-linear interac-
tions.
1365
The primary purpose of this research is ex-
ploratory data analysis to identify both the most
linguistic-salient demographic features, and the
most demographically-salient words. However, this
model also enables predictions about demographic
features by analyzing raw text, potentially support-
ing applications in targeted information extraction
or advertising. On the task of predicting demo-
graphics from text, we find that our sparse model
yields performance that is statistically indistinguish-
able from the full vocabulary, even with a reduction
in the model complexity an order of magnitude. On
the task of predicting text from author demograph-
ics, we find that our incrementally constructed fea-
ture set obtains significantly better perplexity than a
linear model of demographic attributes.
2 Data
Our dataset is derived from prior work in which
we gathered the text and geographical locations of
9,250 microbloggers on the website twitter.
com (Eisenstein et al, 2010). Bloggers were se-
lected from a pool of frequent posters whose mes-
sages include metadata indicating a geographical lo-
cation within a bounding box around the continen-
tal United States. We limit the vocabulary to the
5,418 terms which are used by at least 40 authors; no
stoplists are applied, as the use of standard or non-
standard orthography for stopwords (e.g., to vs. 2)
may convey important information about the author.
The dataset includes messages during the first week
of March 2010.
O?Connor et al (2010) obtained aggregate demo-
graphic statistics for these data by mapping geoloca-
tions to publicly-available data from the U. S. Cen-
sus ZIP Code Tabulation Areas (ZCTA).1 There
are 33,178 such areas in the USA (the 9,250 mi-
crobloggers in our dataset occupy 3,458 unique ZC-
TAs), and they are designed to contain roughly
equal numbers of inhabitants and demographically-
homogeneous populations. The demographic at-
tributes that we consider in this paper are shown
in Table 1. All attributes are based on self-reports.
The race and ethnicity attributes are not mutually
exclusive?individuals can indicate any number of
races or ethnicities. The ?other language? attribute
1http://www.census.gov/support/cen2000.
html
mean std. dev.
race & ethnicity
% white 52.1 29.0
% African American 32.2 29.1
% Hispanic 15.7 18.3
language
% English speakers 73.7 18.4
% Spanish speakers 14.6 15.6
% other language speakers 11.7 9.2
socioeconomic
% urban 95.1 14.3
% with family 64.1 14.4
% renters 48.9 23.4
median income ($) 42,500 18,100
Table 1: The demographic attributes used in this research.
aggregates all languages besides English and Span-
ish. ?Urban areas? refer to sets of census tracts or
census blocks which contain at least 2,500 residents;
our ?% urban? attribute is the percentage of individ-
uals in each ZCTA who are listed as living in an ur-
ban area. We also consider the percentage of indi-
viduals who live with their families, the percentage
who live in rented housing, and the median reported
income in each ZCTA.
While geographical aggregate statistics are fre-
quently used to proxy for individual socioeconomic
status in research areas such as public health (e.g.,
Rushton, 2008), it is clear that interpretation must
proceed with caution. Consider an author from a ZIP
code in which 60% of the residents are Hispanic:2
we do not know the likelihood that the author is His-
panic, because the set of Twitter users is not a rep-
resentative sample of the overall population. Polling
research suggests that users of both Twitter (Smith
and Rainie, 2010) and geolocation services (Zick-
uhr and Smith, 2010) are much more diverse with
respect to age, gender, race and ethnicity than the
general population of Internet users. Nonetheless,
at present we can only use aggregate statistics to
make inferences about the geographic communities
in which our authors live, and not the authors them-
selves.
2In the U.S. Census, the official ethnonym is Hispanic or
Latino; for brevity we will use Hispanic in the rest of this paper.
1366
3 Models
The selection of both words and demographic fea-
tures can be framed in terms of multi-output regres-
sion with structured sparsity. To select the lexical
indicators that best predict demographics, we con-
struct a regression problem in which term frequen-
cies are the predictors and demographic attributes
are the outputs; to select the demographic features
that predict word use, this arrangement is reversed.
Through structured sparsity, we learn models in
which entire sets of coefficients are driven to zero;
this tells us which words and demographic features
can safely be ignored.
This section describes the model and implemen-
tation for output-regression with structured sparsity;
in Section 4 and 5 we give the details of its applica-
tion to select terms and demographic features. For-
mally, we consider the linear equationY = XB+,
where,
? Y is the dependent variable matrix, with di-
mensions N ? T , where N is the number of
samples and T is the number of output dimen-
sions (or tasks);
? X is the independent variable matrix, with di-
mensions N ? P , where P is the number of
input dimensions (or predictors);
? B is the matrix of regression coefficients, with
dimensions P ? T ;
?  is a N ? T matrix in which each element is
noise from a zero-mean Gaussian distribution.
We would like to solve the unconstrained opti-
mization problem,
minimizeB ||Y ?XB||
2
F + ?R(B), (1)
where ||A||2F indicates the squared Frobenius norm?
i
?
j a
2
ij , and the function R(B) defines a norm
on the regression coefficients B. Ridge regres-
sion applies the `2 norm R(B) =
?T
t=1
??P
p b
2
pt,
and lasso regression applies the `1 norm R(B) =?T
t=1
?P
p |bpt|; in both cases, it is possible to de-
compose the multi-output regression problem, treat-
ing each output dimension separately. However, our
working hypothesis is that there will be substantial
correlations across both the vocabulary and the de-
mographic features?for example, a demographic
feature such as the percentage of Spanish speakers
will predict a large set of words. Our goal is to select
a small set of predictors yielding good performance
across all output dimensions. Thus, we desire struc-
tured sparsity, in which entire rows of the coefficient
matrix B are driven to zero.
Structured sparsity is not achieved by the lasso?s
`1 norm. The lasso gives element-wise sparsity, in
which many entries ofB are driven to zero, but each
predictor may have a non-zero value for some output
dimension. To drive entire rows of B to zero, we re-
quire a composite regularizer. We consider the `1,?
norm, which is the sum of `? norms across output
dimensions: R(B) =
?T
t maxp bpt (Turlach et al,
2005). This norm, which corresponds to a multi-
output lasso regression, has the desired property of
driving entire rows of B to zero.
3.1 Optimization
There are several techniques for solving the `1,?
normalized regression, including interior point
methods (Turlach et al, 2005) and projected gradi-
ent (Duchi et al, 2008; Quattoni et al, 2009). We
choose the blockwise coordinate descent approach
of Liu et al (2009) because it is easy to implement
and efficient: the time complexity of each iteration
is independent of the number of samples.3
Due to space limitations, we defer to Liu et al
(2009) for a complete description of the algorithm.
However, we note two aspects of our implementa-
tion which are important for natural language pro-
cessing applications. The algorithm?s efficiency is
accomplished by precomputing the matrices C =
X?TY? and D = X?TX?, where X? and Y? are the stan-
dardized versions ofX andY, obtained by subtract-
ing the mean and scaling by the variance. Explicit
mean correction would destroy the sparse term fre-
quency data representation and render us unable to
store the data in memory; however, we can achieve
the same effect by computing C = XTY ?N x?Ty?,
where x? and y? are row vectors indicating the means
3Our implementation is available at http://sailing.
cs.cmu.edu/sociolinguistic.html.
1367
ofX andY respectively.4 We can similarly compute
D = XTX?N x?Tx?.
If the number of predictors is too large, it may
not be possible to store the dense matrix D in mem-
ory. We have found that approximation based on the
truncated singular value decomposition provides an
effective trade-off of time for space. Specifically, we
compute XTX ?
USVT
(
USVT
)T
= U
(
SVTVSTUT
)
= UM.
Lower truncation levels are less accurate, but are
faster and require less space: for K singular val-
ues, the storage cost is O(KP ), instead of O(P 2);
the time cost increases by a factor of K. This ap-
proximation was not necessary in the experiments
presented here, although we have found that it per-
forms well as long as the regularizer is not too close
to zero.
3.2 Regularization
The regularization constant ? can be computed us-
ing cross-validation. As ? increases, we reuse the
previous solution of B for initialization; this ?warm
start? trick can greatly accelerate the computation
of the overall regularization path (Friedman et al,
2010). At each ?i, we solve the sparse multi-output
regression; the solution Bi defines a sparse set of
predictors for all tasks.
We then use this limited set of predictors to con-
struct a new input matrix X?i, which serves as the
input in a standard ridge regression, thus refitting
the model. The tuning set performance of this re-
gression is the score for ?i. Such post hoc refitting
is often used in tandem with the lasso and related
sparse methods; the effectiveness of this procedure
has been demonstrated in both theory (Wasserman
and Roeder, 2009) and practice (Wu et al, 2010).
The regularization parameter of the ridge regression
is determined by internal cross-validation.
4 Predicting Demographics from Text
Sparse multi-output regression can be used to select
a subset of vocabulary items that are especially in-
dicative of demographic and geographic differences.
4Assume without loss of generality that X and Y are scaled
to have variance 1, because this scaling does not affect the spar-
sity pattern.
Starting from the regression problem (1), the predic-
tors X are set to the term frequencies, with one col-
umn for each word type and one row for each author
in the dataset. The outputsY are set to the ten demo-
graphic attributes described in Table 1 (we consider
much larger demographic feature spaces in the next
section) The `1,? regularizer will drive entire rows
of the coefficient matrix B to zero, eliminating all
demographic effects for many words.
4.1 Quantitative Evaluation
We evaluate the ability of lexical features to predict
the demographic attributes of their authors (as prox-
ied by the census data from the author?s geograph-
ical area). The purpose of this evaluation is to as-
sess the predictive ability of the compact subset of
lexical items identified by the multi-output lasso, as
compared with the full vocabulary. In addition, this
evaluation establishes a baseline for performance on
the demographic prediction task.
We perform five-fold cross-validation, using the
multi-output lasso to identify a sparse feature set
in the training data. We compare against several
other dimensionality reduction techniques, match-
ing the number of features obtained by the multi-
output lasso at each fold. First, we compare against
a truncated singular value decomposition, with the
truncation level set to the number of terms selected
by the multi-output lasso; this is similar in spirit to
vector-based lexical semantic techniques (Schu?tze
and Pedersen, 1993). We also compare against sim-
ply selecting the N most frequent terms, and the N
terms with the greatest variance in frequency across
authors. Finally, we compare against the complete
set of all 5,418 terms. As before, we perform post
hoc refitting on the training data using a standard
ridge regression. The regularization constant for the
ridge regression is identified using nested five-fold
cross validation within the training set.
We evaluate on the refit models on the heldout
test folds. The scoring metric is Pearson?s correla-
tion coefficient between the predicted and true de-
mographics: ?(y, y?) = cov(y,y?)?y?y? , with cov(y, y?) in-
dicating the covariance and ?y indicating the stan-
dard deviation. On this metric, a perfect predictor
will score 1 and a random predictor will score 0. We
report the average correlation across all ten demo-
1368
102 103
0.16
0.18
0.2
0.22
0.24
0.26
0.28
number of features
av
er
age
 co
rre
lati
on
 
 
multi?output lasso
SVD
highest variance
most frequent
Figure 1: Average correlation plotted against the number
of active features (on a logarithmic scale).
graphic attributes, as well as the individual correla-
tions.
Results Table 2 shows the correlations obtained
by regressions performed on a range of different vo-
cabularies, averaged across all five folds. Linguistic
features are best at predicting race, ethnicity, lan-
guage, and the proportion of renters; the other de-
mographic attributes are more difficult to predict.
Among feature sets, the highest average correlation
is obtained by the full vocabulary, but the multi-
output lasso obtains nearly identical performance
using a feature set that is an order of magnitude
smaller. Applying the Fischer transformation, we
find that all correlations are statistically significant
at p < .001.
The Fischer transformation can also be used to
estimate 95% confidence intervals around the cor-
relations. The extent of the confidence intervals
varies slightly across attributes, but all are tighter
than ?0.02. We find that the multi-output lasso and
the full vocabulary regression are not significantly
different on any of the attributes. Thus, the multi-
output lasso achieves a 93% compression of the fea-
ture set without a significant decrease in predictive
performance. The multi-output lasso yields higher
correlations than the other dimensionality reduction
techniques on all of the attributes; these differences
are statistically significant in many?but not all?
cases. The correlations for each attribute are clearly
not independent, so we do not compare the average
across attributes.
Recall that the regularization coefficient was cho-
sen by nested cross-validation within the training
set; the average number of features selected is
394.6. Figure 1 shows the performance of each
dimensionality-reduction technique across the reg-
ularization path for the first of five cross-validation
folds. Computing the truncated SVD of a sparse ma-
trix at very large truncation levels is computationally
expensive, so we cannot draw the complete perfor-
mance curve for this method. The multi-output lasso
dominates the alternatives, obtaining a particularly
strong advantage with very small feature sets. This
demonstrates its utility for identifying interpretable
models which permit qualitative analysis.
4.2 Qualitative Analysis
For a qualitative analysis, we retrain the model on
the full dataset, and tune the regularization to iden-
tify a compact set of 69 features. For each identified
term, we apply a significance test on the relationship
between the presence of each term and the demo-
graphic indicators shown in the columns of the ta-
ble. Specifically, we apply the Wald test for compar-
ing the means of independent samples, while mak-
ing the Bonferroni correction for multiple compar-
isons (Wasserman, 2003). The use of sparse multi-
output regression for variable selection increases the
power of post hoc significance testing, because the
Bonferroni correction bases the threshold for sta-
tistical significance on the total number of compar-
isons. We find 275 associations at the p < .05 level;
at the higher threshold required by a Bonferroni cor-
rection for comparisons among all terms in the vo-
cabulary, 69 of these associations would have been
missed.
Table 3 shows the terms identified by our model
which have a significant correlation with at least one
of the demographic indicators. We divide words in
the list into categories, which order alphabetically
by the first word in each category: emoticons; stan-
dard English, defined as words with Wordnet entries;
proper names; abbreviations; non-English words;
non-standard words used with English. The cate-
gorization was based on the most frequent sense in
an informal analysis of our data. A glossary of non-
standard terms is given in Table 4.
Some patterns emerge from Table 3. Standard
English words tend to appear in areas with more
1369
vocabulary # features av
er
ag
e
w
hi
te
A
fr
.A
m
.
H
is
p.
E
ng
.l
an
g.
S
pa
n.
la
ng
.
ot
he
r
la
ng
.
ur
ba
n
fa
m
il
y
re
nt
er
m
ed
.i
nc
.
full 5418 0.260 0.337 0.318 0.296 0.384 0.296 0.256 0.155 0.113 0.295 0.152
multi-output lasso
394.6
0.260 0.326 0.308 0.304 0.383 0.303 0.249 0.153 0.113 0.302 0.156
SVD 0.237 0.321 0.299 0.269 0.352 0.272 0.226 0.138 0.081 0.278 0.136
highest variance 0.220 0.309 0.287 0.245 0.315 0.248 0.199 0.132 0.085 0.250 0.135
most frequent 0.204 0.294 0.264 0.222 0.293 0.229 0.178 0.129 0.073 0.228 0.126
Table 2: Correlations between predicted and observed demographic attributes, averaged across cross validation folds.
English speakers; predictably, Spanish words tend
to appear in areas with Spanish speakers and His-
panics. Emoticons tend to be used in areas with
many Hispanics and few African Americans. Ab-
breviations (e.g., lmaoo) have a nearly uniform
demographic profile, displaying negative correla-
tions with whites and English speakers, and posi-
tive correlations with African Americans, Hispanics,
renters, Spanish speakers, and areas classified as ur-
ban.
Many non-standard English words (e.g., dats)
appear in areas with high proportions of renters,
African Americans, and non-English speakers,
though a subset (haha, hahaha, and yep) display
the opposite demographic pattern. Many of these
non-standard words are phonetic transcriptions of
standard words or phrases: that?s?dats, what?s
up?wassup, I?m going to?ima. The relationship
between these transcriptions and the phonological
characteristics of dialects such as African-American
Vernacular English is a topic for future work.
5 Conjunctive Demographic Features
Next, we demonstrate how to select conjunctions of
demographic features that predict text. Again, we
apply multi-output regression, but now we reverse
the direction of inference: the predictors are demo-
graphic features, and the outputs are term frequen-
cies. The sparsity-inducing `1,? norm will select a
subset of demographic features that explain the term
frequencies.
We create an initial feature set f (0)(X) by bin-
ning each demographic attribute, using five equal-
frequency bins. We then constructive conjunctive
features by applying a procedure inspired by related
work in computational biology, called ?Screen and
Clean? (Wu et al, 2010). On iteration i:
? Solve the sparse multi-output regression prob-
lem Y = f (i)(X)B(i) + .
? Select a subset of features S(i) such that m ?
S(i) iff maxj |b
(i)
m,j | > 0. These are the row
indices of the predictors with non-zero coeffi-
cients.
? Create a new feature set f (i+1)(X), including
the conjunction of each feature (and its nega-
tion) in S(i) with each feature in the initial set
f (0)(X).
We iterate this process to create features that con-
join as many as three attributes. In addition to the
binned versions of the demographic attributes de-
scribed in Table 1, we include geographical infor-
mation. We built Gaussian mixture models over the
locations, with 3, 5, 8, 12, 17, and 23 components.
For each author we include the most likely cluster
assignment in each of the six mixture models. For
efficiency, the outputs Y are not set to the raw term
frequencies; instead we compute a truncated sin-
gular value decomposition of the term frequencies
W ? UVDT, and use the basis U. We set the trun-
cation level to 100.
5.1 Quantitative Evaluation
The ability of the induced demographic features to
predict text is evaluated using a traditional perplex-
ity metric. The same test and training split is used
from the vocabulary experiments. We construct a
language model from the induced demographic fea-
tures by training a multi-output ridge regression,
which gives a matrix B? that maps from demographic
features to term frequencies across the entire vocab-
ulary. For each document in the test set, the ?raw?
predicted language model is y?d = f(xd)B, which
is then normalized. The probability mass assigned
1370
w
hi
te
A
fr
.A
m
.
H
is
p.
E
ng
.l
an
g.
S
pa
n.
la
ng
.
ot
he
r
la
ng
.
ur
ba
n
fa
m
il
y
re
nt
er
m
ed
.i
nc
.
- - - + - + + +
;) - + - +
:( -
:) -
:d + - + - +
as - + -
awesome + - - - +
break - + - -
campus - + - -
dead - + - + + +
hell - + - -
shit - +
train - + +
will - + -
would + -
atlanta - + - -
famu + - + - - -
harlem - +
bbm - + - + + +
lls + - + - -
lmaoo - + + - + + + +
lmaooo - + + - + + + +
lmaoooo - + + - + + +
lmfaoo - + - + + +
lmfaooo - + - + + +
lml - + + - + + + + -
odee - + - + + +
omw - + + - + + + +
smfh - + + - + + + +
smh - + + +
w| - + - + + + +
con + - + +
la - + - +
si - + - +
dats - + - + -
deadass - + + - + + + +
haha + - -
hahah + -
hahaha + - - +
ima - + - + +
madd - - + +
nah - + - + + +
ova - + - +
sis - + +
skool - + - + + + -
wassup - + + - + + + + -
wat - + + - + + + + -
ya - + +
yall - +
yep - + - - - -
yoo - + + - + + + +
yooo - + - + +
Table 3: Demographically-indicative terms discovered by
multi-output sparse regression. Statistically significant
(p < .05) associations are marked with a + or -.
term definition
bbm Blackberry Messenger
dats that?s
dead(ass) very
famu Florida Agricultural
and Mechanical Univ.
ima I?m going to
lls laughing like shit
lm(f)ao+ laughing my (fucking)
ass off
lml love my life
madd very, lots
nah no
odee very
term definition
omw on my way
ova over
sis sister
skool school
sm(f)h shake my (fuck-
ing) head
w| with
wassup what?s up
wat what
ya your, you
yall you plural
yep yes
yoo+ you
Table 4: A glossary of non-standard terms from Ta-
ble 3. Definitions are obtained by manually inspecting
the context in which the terms appear, and by consulting
www.urbandictionary.com.
model perplexity
induced demographic features 333.9
raw demographic attributes 335.4
baseline (no demographics) 337.1
Table 5: Word perplexity on test documents, using
language models estimated from induced demographic
features, raw demographic attributes, and a relative-
frequency baseline. Lower scores are better.
to unseen words is determined through nested cross-
validation. We compare against a baseline language
model obtained from the training set, again using
nested cross-validation to set the probability of un-
seen terms.
Results are shown in Table 5. The language mod-
els induced from demographic data yield small but
statistically significant improvements over the base-
line (Wilcoxon signed-rank test, p < .001). More-
over, the model based on conjunctive features signif-
icantly outperforms the model constructed from raw
attributes (p < .001).
5.2 Features Discovered
Our approach discovers 37 conjunctive features,
yielding the results shown in Table 5. We sort all
features by frequency, and manually select a sub-
set to display in Table 6. Alongside each feature,
we show the words with the highest and lowest log-
odds ratios with respect to the feature. Many of these
terms are non-standard; while space does not permit
a complete glossary, some are defined in Table 4 or
in our earlier work (Eisenstein et al, 2010).
1371
feature positive terms negative terms
1 geo: Northeast m2 brib mangoville soho odeee fasho #ilovefamu foo coo fina
2 geo: NYC mangoville lolss m2 brib wordd bahaha fasho goofy #ilovefamu
tacos
4 geo: South+Midwest renter? 0.615 white? 0.823 hme muthafucka bae charlotte tx odeee m2 lolss diner mangoville
7 Afr. Am.> 0.101 renter> 0.615 Span. lang.> 0.063 dhat brib odeee lolss wassupp bahaha charlotte california ikr en-
ter
8 Afr. Am.? 0.207 Hispanic> 0.119 Span. lang.> 0.063 les ahah para san donde bmore ohio #lowkey #twitterjail
nahhh
9 geo: NYC Span. lang.? 0.213 mangoville thatt odeee lolss
buzzin
landed rodney jawn wiz golf
12 Afr. Am.> 0.442 geo: South+Midwest white? 0.823 #ilovefamu panama midtermswillies #lowkey knoe esta pero odeee hii
15 geo: West Coast other lang.> 0.110 ahah fasho san koo diego granted pride adore phat pressure
17 Afr. Am.> 0.442 geo: NYC other lang.? 0.110 lolss iim buzzin qonna qood foo tender celebs pages pandora
20 Afr. Am.? 0.207 Span. lang.> 0.063 white> 0.823 del bby cuando estoy muscle knicks becoming uncomfortablelarge granted
23 Afr. Am.? 0.050 geo: West Span. lang.? 0.106 leno it?d 15th hacked government knicks liquor uu hunn homee
33 Afr. Am.> 0.101 geo: SF Bay Span. lang.> 0.063 hella aha california bay o.o aj everywhere phones shift re-gardless
36 Afr. Am.? 0.050 geo: DC/Philadelphia Span. lang.? 0.106 deh opens stuffed yaa bmore hmmmmm dyin tea cousin hella
Table 6: Conjunctive features discovered by our method with a strong sparsity-inducing prior, ordered by frequency.
We also show the words with high log-odds for each feature (postive terms) and its negation (negative terms).
In general, geography was a strong predictor, ap-
pearing in 25 of the 37 conjunctions. Features 1
and 2 (F1 and F2) are purely geographical, captur-
ing the northeastern United States and the New York
City area. The geographical area of F2 is completely
contained by F1; the associated terms are thus very
similar, but by having both features, the model can
distinguish terms which are used in northeastern ar-
eas outside New York City, as well as terms which
are especially likely in New York.5
Several features conjoin geography with demo-
graphic attributes. For example, F9 further refines
the New York City area by focusing on communities
that have relatively low numbers of Spanish speak-
ers; F17 emphasizes New York neighborhoods that
have very high numbers of African Americans and
few speakers of languages other than English and
Spanish. The regression model can use these fea-
tures in combination to make fine-grained distinc-
tions about the differences between such neighbor-
hoods. Outside New York, we see that F4 combines
a broad geographic area with attributes that select at
least moderate levels of minorities and fewer renters
(a proxy for areas that are less urban), while F15
identifies West Coast communities with large num-
5Mangoville and M2 are clubs in New York; fasho and coo
were previously found to be strongly associated with the West
Coast (Eisenstein et al, 2010).
bers of speakers of languages other than English and
Spanish.
Race and ethnicity appear in 28 of the 37 con-
junctions. The attribute indicating the proportion of
African Americans appeared in 22 of these features,
strongly suggesting that African American Vernac-
ular English (Rickford, 1999) plays an important
role in social media text. Many of these features
conjoined the proportion of African Americans with
geographical features, identifying local linguistic
styles used predominantly in either African Amer-
ican or white communities. Among features which
focus on minority communities, F17 emphasizes the
New York area, F33 focuses on the San Francisco
Bay area, and F12 selects a broad area in the Mid-
west and South. Conversely, F23 selects areas with
very few African Americans and Spanish-speakers
in the western part of the United States, and F36 se-
lects for similar demographics in the area of Wash-
ington and Philadelphia.
Other features conjoined the proportion of
African Americans with the proportion of Hispan-
ics and/or Spanish speakers. In some cases, features
selected for high proportions of both African Amer-
icans and Hispanics; for example, F7 seems to iden-
tify a general ?urban minority? group, emphasizing
renters, African Americans, and Spanish speakers.
Other features differentiate between African Ameri-
1372
cans and Hispanics: F8 identifies regions with many
Spanish speakers and Hispanics, but few African
Americans; F20 identifies regions with both Span-
ish speakers and whites, but few African Americans.
F8 and F20 tend to emphasize more Spanish words
than features which select for both African Ameri-
cans and Hispanics.
While race, geography, and language predom-
inate, the socioeconomic attributes appear in far
fewer features. The most prevalent attribute is the
proportion of renters, which appears in F4 and F7,
and in three other features not shown here. This at-
tribute may be a better indicator of the urban/rural
divide than the ?% urban? attribute, which has a
very low threshold for what counts as urban (see
Table 1). It may also be a better proxy for wealth
than median income, which appears in only one of
the thirty-seven selected features. Overall, the se-
lected features tend to include attributes that are easy
to predict from text (compare with Table 2).
6 Related Work
Sociolinguistics has a long tradition of quantitative
and computational research. Logistic regression has
been used to identify relationships between demo-
graphic features and linguistic variables since the
1970s (Cedergren and Sankoff, 1974). More re-
cent developments include the use of mixed factor
models to account for idiosyncrasies of individual
speakers (Johnson, 2009), as well as clustering and
multidimensional scaling (Nerbonne, 2009) to en-
able aggregate inference across multiple linguistic
variables. However, all of these approaches assume
that both the linguistic indicators and demographic
attributes have already been identified by the re-
searcher. In contrast, our approach focuses on iden-
tifying these indicators automatically from data. We
view our approach as an exploratory complement to
more traditional analysis.
There is relatively little computational work on
identifying speaker demographics. Chang et al
(2010) use U.S. Census statistics about the ethnic
distribution of last names as an anchor in a latent-
variable model that infers the ethnicity of Facebook
users; however, their paper analyzes social behav-
ior rather than language use. In unpublished work,
David Bamman uses geotagged Twitter text and U.S.
Census statistics to estimate the age, gender, and
racial distributions of various lexical items.6 Eisen-
stein et al (2010) infer geographic clusters that are
coherent with respect to both location and lexical
distributions; follow-up work by O?Connor et al
(2010) applies a similar generative model to demo-
graphic data. The model presented here differs in
two key ways: first, we use sparsity-inducing regu-
larization to perform variable selection; second, we
eschew high-dimensional mixture models in favor of
a bottom-up approach of building conjunctions of
demographic and geographic attributes. In a mix-
ture model, each component must define a distribu-
tion over all demographic variables, which may be
difficult to estimate in a high-dimensional setting.
Early examples of the use of sparsity in natu-
ral language processing include maximum entropy
classification (Kazama and Tsujii, 2003), language
modeling (Goodman, 2004), and incremental pars-
ing (Riezler and Vasserman, 2004). These papers all
apply the standard lasso, obtaining sparsity for a sin-
gle output dimension. Structured sparsity has rarely
been applied to language tasks, but Duh et al (2010)
reformulated the problem of reranking N -best lists
as multi-task learning with structured sparsity.
7 Conclusion
This paper demonstrates how regression with struc-
tured sparsity can be applied to select words and
conjunctive demographic features that reveal soci-
olinguistic associations. The resulting models are
compact and interpretable, with little cost in accu-
racy. In the future we hope to consider richer lin-
guistic models capable of identifying multi-word ex-
pressions and syntactic variation.
Acknowledgments We received helpful feedback
from Moira Burke, Scott Kiesling, Seyoung Kim, Andre?
Martins, Kriti Puniyani, and the anonymous reviewers.
Brendan O?Connor provided the data for this research,
and Seunghak Lee shared a Matlab implementation of
the multi-output lasso, which was the basis for our C
implementation. This research was enabled by AFOSR
FA9550010247, ONR N0001140910758, NSF CAREER
DBI-0546594, NSF CAREER IIS-1054319, NSF IIS-
0713379, an Alfred P. Sloan Fellowship, and Google?s
support of the Worldly Knowledge project at CMU.
6http://www.lexicalist.com
1373
References
Henrietta J. Cedergren and David Sankoff. 1974. Vari-
able rules: Performance as a statistical reflection of
competence. Language, 50(2):333?355.
Jonathan Chang, Itamar Rosenn, Lars Backstrom, and
Cameron Marlow. 2010. ePluribus: Ethnicity on so-
cial networks. In Proceedings of ICWSM.
John Duchi, Shai Shalev-Shwartz, Yoram Singer, and
Tushar Chandra. 2008. Efficient projections onto the
`1-ball for learning in high dimensions. In Proceed-
ings of ICML.
Kevin Duh, Katsuhito Sudoh, Hajime Tsukada, Hideki
Isozaki, and Masaaki Nagata. 2010. n-best rerank-
ing by multitask learning. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation and
Metrics.
Jacob Eisenstein, Brendan O?Connor, Noah A. Smith,
and Eric P. Xing. 2010. A latent variable model of ge-
ographic lexical variation. In Proceedings of EMNLP.
Jerome Friedman, Trevor Hastie, and Rob Tibshirani.
2010. Regularization paths for generalized linear
models via coordinate descent. Journal of Statistical
Software, 33(1):1?22.
Joshua Goodman. 2004. Exponential priors for maxi-
mum entropy models. In Proceedings of NAACL-HLT.
Daniel E. Johnson. 2009. Getting off the GoldVarb
standard: Introducing Rbrul for mixed-effects variable
rule analysis. Language and Linguistics Compass,
3(1):359?383.
Jun?ichi Kazama and Jun?ichi Tsujii. 2003. Evaluation
and extension of maximum entropy models with in-
equality constraints. In Proceedings of EMNLP.
William Labov. 1966. The Social Stratification of En-
glish in New York City. Center for Applied Linguis-
tics.
Han Liu, Mark Palatucci, and Jian Zhang. 2009. Block-
wise coordinate descent procedures for the multi-task
lasso, with applications to neural semantic basis dis-
covery. In Proceedings of ICML.
John Nerbonne. 2009. Data-driven dialectology. Lan-
guage and Linguistics Compass, 3(1):175?198.
Brendan O?Connor, Jacob Eisenstein, Eric P. Xing, and
Noah A. Smith. 2010. A mixture model of de-
mographic lexical variation. In Proceedings of NIPS
Workshop on Machine Learning in Computational So-
cial Science.
Ariadna Quattoni, Xavier Carreras, Michael Collins, and
Trevor Darrell. 2009. An efficient projection for `1,?
regularization. In Proceedings of ICML.
John R. Rickford. 1999. African American Vernacular
English. Blackwell.
Stefan Riezler and Alexander Vasserman. 2004. Incre-
mental feature selection and `1 regularization for re-
laxed maximum-entropy modeling. In Proceedings of
EMNLP.
Gerard Rushton, Marc P. Armstrong, Josephine Gittler,
Barry R. Greene, Claire E. Pavlik, Michele M. West,
and Dale L. Zimmerman, editors. 2008. Geocoding
Health Data: The Use of Geographic Codes in Cancer
Prevention and Control, Research, and Practice. CRC
Press.
Hinrich Schu?tze and Jan Pedersen. 1993. A vector model
for syntagmatic and paradigmatic relatedness. In Pro-
ceedings of the 9th Annual Conference of the UW Cen-
tre for the New OED and Text Research.
Aaron Smith and Lee Rainie. 2010. Who tweets? Tech-
nical report, Pew Research Center, December.
Berwin A. Turlach, William N. Venables, and Stephen J.
Wright. 2005. Simultaneous variable selection. Tech-
nometrics, 47(3):349?363.
Larry Wasserman and Kathryn Roeder. 2009. High-
dimensional variable selection. Annals of Statistics,
37(5A):2178?2201.
Larry Wasserman. 2003. All of Statistics: A Concise
Course in Statistical Inference. Springer.
Jing Wu, Bernie Devlin, Steven Ringquist, Massimo
Trucco, and Kathryn Roeder. 2010. Screen and clean:
A tool for identifying interactions in genome-wide as-
sociation studies. Genetic Epidemiology, 34(3):275?
285.
Qing Zhang. 2005. A Chinese yuppie in Beijing: Phono-
logical variation and the construction of a new profes-
sional identity. Language in Society, 34:431?466.
Kathryn Zickuhr and Aaron Smith. 2010. 4% of online
Americans use location-based services. Technical re-
port, Pew Research Center, November.
1374
Tutorial Abstracts of ACL 2012, page 3,
Jeju, Republic of Korea, 8 July 2012. c?2012 Association for Computational Linguistics
Topic Models, Latent Space Models, Sparse Coding, and All That: A
systematic understanding of probabilistic semantic extraction in large
corpus
Eric Xing
School of Computer Science
Carnegie Mellon University
Abstract
Probabilistic topic models have recently
gained much popularity in informational re-
trieval and related areas. Via such mod-
els, one can project high-dimensional objects
such as text documents into a low dimen-
sional space where their latent semantics are
captured and modeled; can integrate multiple
sources of information?to ?share statistical
strength? among components of a hierarchical
probabilistic model; and can structurally dis-
play and classify the otherwise unstructured
object collections. However, to many practi-
tioners, how topic models work, what to and
not to expect from a topic model, how is it dif-
ferent from and related to classical matrix al-
gebraic techniques such as LSI, NMF in NLP,
how to empower topic models to deal with
complex scenarios such as multimodal data,
contractual text in social media, evolving cor-
pus, or presence of supervision such as la-
beling and rating, how to make topic mod-
eling computationally tractable even on web-
scale data, etc., in a principled way, remain un-
clear. In this tutorial, I will demystify the con-
ceptual, mathematical, and computational is-
sues behind all such problems surrounding the
topic models and their applications by present-
ing a systematic overview of the mathemati-
cal foundation of topic modeling, and its con-
nections to a number of related methods pop-
ular in other fields such as the LDA, admix-
ture model, mixed membership model, latent
space models, and sparse coding. I will offer
a simple and unifying view of all these tech-
niques under the framework multi-view latent
space embedding, and online the roadmap of
model extension and algorithmic design to-
ward different applications in IR and NLP. A
main theme of this tutorial that tie together a
wide range of issues and problems will build
on the ?probabilistic graphical model? formal-
ism, a formalism that exploits the conjoined
talents of graph theory and probability theory
to build complex models out of simpler pieces.
I will use this formalism as a main aid to dis-
cuss both the mathematical underpinnings for
the models and the related computational is-
sues in a unified, simplistic, transparent, and
actionable fashion.
3
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1062?1072,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Spectral Unsupervised Parsing with Additive Tree Metrics
Ankur P. Parikh
School of Computer Science
Carnegie Mellon University
apparikh@cs.cmu.edu
Shay B. Cohen
School of Informatics
University of Edinburgh
scohen@inf.ed.ac.uk
Eric P. Xing
School of Computer Science
Carnegie Mellon University
epxing@cs.cmu.edu
Abstract
We propose a spectral approach for un-
supervised constituent parsing that comes
with theoretical guarantees on latent struc-
ture recovery. Our approach is grammar-
less ? we directly learn the bracketing
structure of a given sentence without us-
ing a grammar model. The main algorithm
is based on lifting the concept of additive
tree metrics for structure learning of la-
tent trees in the phylogenetic and machine
learning communities to the case where
the tree structure varies across examples.
Although finding the ?minimal? latent tree
is NP-hard in general, for the case of pro-
jective trees we find that it can be found
using bilexical parsing algorithms. Empir-
ically, our algorithm performs favorably
compared to the constituent context model
of Klein and Manning (2002) without the
need for careful initialization.
1 Introduction
Solutions to the problem of grammar induction
have been long sought after since the early days of
computational linguistics and are interesting both
from cognitive and engineering perspectives. Cog-
nitively, it is more plausible to assume that chil-
dren obtain only terminal strings of parse trees and
not the actual parse trees. This means the unsu-
pervised setting is a better model for studying lan-
guage acquisition. From the engineering perspec-
tive, training data for unsupervised parsing exists
in abundance (i.e. sentences and part-of-speech
tags), and is much cheaper than the syntactically
annotated data required for supervised training.
Most existing solutions treat the problem of un-
supervised parsing by assuming a generative pro-
cess over parse trees e.g. probabilistic context
free grammars (Jelinek et al, 1992), and the con-
stituent context model (Klein and Manning, 2002).
Learning then reduces to finding a set of parame-
ters that are estimated by identifying a local max-
imum of an objective function such as the likeli-
hood (Klein and Manning, 2002) or a variant of it
(Smith and Eisner, 2005; Cohen and Smith, 2009;
Headden et al, 2009; Spitkovsky et al, 2010b;
Gillenwater et al, 2010; Golland et al, 2012). Un-
fortunately, finding the global maximum for these
objective functions is usually intractable (Cohen
and Smith, 2012) which often leads to severe lo-
cal optima problems (but see Gormley and Eisner,
2013). Thus, strong experimental results are often
achieved by initialization techniques (Klein and
Manning, 2002; Gimpel and Smith, 2012), incre-
mental dataset use (Spitkovsky et al, 2010a) and
other specialized techniques to avoid local optima
such as count transforms (Spitkovsky et al, 2013).
These approaches, while empirically promising,
generally lack theoretical justification.
On the other hand, recently proposed spectral
methods approach the problem via restriction of
the PCFG model (Hsu et al, 2012) or matrix com-
pletion (Bailly et al, 2013). These novel perspec-
tives offer strong theoretical guarantees but are not
designed to achieve competitive empirical results.
In this paper, we suggest a different approach,
to provide a first step to bridging this theory-
experiment gap. More specifically, we approach
unsupervised constituent parsing from the per-
spective of structure learning as opposed to pa-
rameter learning. We associate each sentence with
an undirected latent tree graphical model, which is
a tree consisting of both observed variables (corre-
sponding to the words in the sentence) and an ad-
ditional set of latent variables that are unobserved
in the data. This undirected latent tree is then di-
rected via a direction mapping to give the final
constituent parse.
In our framework, parsing reduces to finding the
best latent structure for a given sentence. How-
ever, due to the presence of latent variables, struc-
ture learning of latent trees is substantially more
complicated than in observed models. As before,
one solution would be local search heuristics.
Intuitively, however, latent tree models en-
code low rank dependencies among the observed
variables permitting the development of ?spec-
1062
tral? methods that can lead to provably correct
solutions. In particular we leverage the con-
cept of additive tree metrics (Buneman, 1971;
Buneman, 1974) in phylogenetics and machine
learning that can create a special distance met-
ric among the observed variables as a function
of the underlying spectral dependencies (Choi et
al., 2011; Song et al, 2011; Anandkumar et al,
2011; Ishteva et al, 2012). Additive tree met-
rics can be leveraged by ?meta-algorithms? such
as neighbor-joining (Saitou and Nei, 1987) and
recursive grouping (Choi et al, 2011) to provide
consistent learning algorithms for latent trees.
Moreover, we show that it is desirable to learn
the ?minimal? latent tree based on the tree metric
(?minimum evolution? in phylogenetics). While
this criterion is in general NP-hard (Desper and
Gascuel, 2005), for projective trees we find that a
bilexical parsing algorithm can be used to find an
exact solution efficiently (Eisner and Satta, 1999).
Unlike in phylogenetics and graphical models,
where a single latent tree is constructed for all the
data, in our case, each part of speech sequence is
associated with its own parse tree. This leads to a
severe data sparsity problem even for moderately
long sentences. To handle this issue, we present
a strategy that is inspired by ideas from kernel
smoothing in the statistics community (Zhou et al,
2010; Kolar et al, 2010b; Kolar et al, 2010a).
This allows principled sharing of samples from
different but similar underlying distributions.
We provide theoretical guarantees on the re-
covery of the correct underlying latent tree and
characterize the associated sample complexity un-
der our technique. Empirically we evaluate our
method on data in English, German and Chi-
nese. Our algorithm performs favorably to Klein
and Manning?s (2002) constituent-context model
(CCM), without the need for careful initialization.
In addition, we also analyze CCM?s sensitivity to
initialization, and compare our results to Seginer?s
algorithm (Seginer, 2007).
2 Learning Setting and Model
In this section, we detail the learning setting and a
conditional tree model we learn the structure for.
2.1 Learning Setting
Let w = (w
1
, ..., w
`
) be a vector of words corre-
sponding to a sentence of length `. Each w
i
is rep-
resented by a vector in R
p
for p ? N. The vector
is an embedding of the word in some space, cho-
VBD DT NNVBD DT NN
Figure 2: Candidate constituent parses for x = (VBD, DT, NN)
(left-correct, right -incorrect)
sen from a fixed dictionary that maps word types
to R
p
. In addition, let x = (x
1
, ..., x
`
) be the as-
sociated vector of part-of-speech (POS) tags (i.e.
x
i
is the POS tag of w
i
).
In our learning algorithm, we assume that ex-
amples of the form (w
(i)
,x
(i)
) for i ? [N ] =
{1, . . . , N} are given, and the goal is to predict
a bracketing parse tree for each of these examples.
The word embeddings are used during the learn-
ing process, but the final decoder that the learning
algorithm outputs maps a POS tag sequence x to
a parse tree. While ideally we would want to use
the word information in decoding as well, much of
the syntax of a sentence is determined by the POS
tags, and relatively high level of accuracy can be
achieved by learning, for example, a supervised
parser from POS tag sequences.
Just like our decoder, our model assumes that
the bracketing of a given sentence is a function
of its POS tags. The POS tags are generated
from some distribution, followed by a determin-
istic generation of the bracketing parse tree. Then,
latent states are generated for each bracket, and
finally, the latent states at the yield of the bracket-
ing parse tree generate the words of the sentence
(in the form of embeddings). The latent states are
represented by vectors z ? R
m
where m < p.
2.2 Intuition
For intuition, consider the simple tag sequence
x = (VBD, DT, NN). Two candidate constituent
parse structures are shown in Figure 2 and the cor-
rect one is boxed in green (the other in red). Re-
call that our training data contains word phrases
that have the tag sequence x e.g. w
(1)
=
(hit, the, ball), w
(2)
= (ate, an, apple).
Intuitively, the words in the above phrases ex-
hibit dependencies that can reveal the parse struc-
ture. The determiner (w
2
) and the direct object
(w
3
) are correlated in that the choice of deter-
miner depends on the plurality of w
3
. However,
the choice of verb (w
1
) is mostly independent of
the determiner. We could thus conclude that w
2
and w
3
should be closer in the parse tree than w
1
1063
The be ar ate the fish
?1 , ?2 , ?3 , ?4 , ?5 , ?1, ?2, ?3
? = (??,??, ???, ??,??)
?(?)
((DT NN) (VBD (DT NN)))
w 1 w 2 w 3
z 3
z 1
w 4 w 5
z 2
w 1 w 2 w 3
z 3z 1
w 4 w 5
z 2
Figure 1: Example for the tag
sequence (DT, NN, VBD, DT, NN)
showing the overview of our
approach. We first learn a undi-
rected latent tree for the se-
quence (left). We then ap-
ply a direction mapping h
dir
to
direct the latent tree (center).
This can then easily be con-
verted into a bracketing (right).
andw
2
, giving us the correct structure. Informally,
the latent state z corresponding to the (w
2
, w
3
)
bracket would store information about the plural-
ity of z, the key to the dependence betweenw
2
and
w
3
. It would then be reasonable to assume that w
2
and w
3
are independent given z.
2.3 A Conditional Latent Tree Model
Following this intuition, we propose to model the
distribution over the latent bracketing states and
words for each tag sequence x as a latent tree
graphical model, which encodes conditional inde-
pendences among the words given the latent states.
Let V := {w
1
, ..., w
`
, z
1
, ..., z
H
}, with w
i
rep-
resenting the word embeddings, and z
i
represent-
ing the latent states of the bracketings. Then, ac-
cording to our base model it holds that:
p(w, z|x) =
H
?
i=1
p(z
i
|pix(zi), ?(x))
?
`(x)
?
i=1
p(w
i
|pix(wi), ?(x)) (1)
where pix(?) returns the parent node index of the
argument in the latent tree corresponding to tag
sequence x.
1
If z is the root, then pix(z) = ?.
All the w
i
are assumed to be leaves while all the
z
i
are internal (i.e. non-leaf) nodes. The param-
eters ?(x) control the conditional probability ta-
bles. We do not commit to a certain parametric
family, but see more about the assumptions we
make about ? in ?3.2. The parameter space is de-
noted ?. The model assumes a factorization ac-
cording to a latent-variable tree. The latent vari-
ables can incorporate various linguistic properties,
such as head information, valence of dependency
being generated, and so on. This information is
expected to be learned automatically from data.
Our generative model deterministically maps a
POS sequence to a bracketing via an undirected
1
At this point, pi refers to an arbitrary direction of the
undirected tree u(x).
latent-variable tree. The orientation of the tree is
determined by a direction mapping h
dir
(u), which
is fixed during learning and decoding. This means
our decoder first identifies (given a POS sequence)
an undirected tree, and then orients it by applying
h
dir
on the resulting tree (see below).
Define U to be the set of undirected latent trees
where all internal nodes have degree exactly 3 (i.e.
they correspond to binary bracketing), and in addi-
tion h
dir
(u) for any u ? U is projective (explained
in the h
dir
section). In addition, let T be the set
of binary bracketings. The complete generative
model that we follow is then:
? Generate a tag sequence x = (x
1
, . . . , x
`
)
? Decide on u(x) ? U , the undirected latent tree
that x maps to.
? Set t ? T by computing t = h
dir
(u).
? Set ? ? ? by computing ? = ?(x).
? Generate a tuple v = (w
1
, . . . , w
`
, z
1
, ..., z
H
)
where w
i
? R
p
, z
j
? R
m
according to Eq. 1.
See Figure 1 (left) for an example.
The Direction Mapping h
dir
. Generating a
bracketing via an undirected tree enables us to
build on existing methods for structure learning
of latent-tree graphical models (Choi et al, 2011;
Anandkumar et al, 2011). Our learning algorithm
focuses on recovering the undirected tree based
for the generative model that was described above.
This undirected tree is converted into a directed
tree by applying h
dir
. The mapping h
dir
works in
three steps:
? It first chooses a top bracket ([1, R ? 1], [R, `])
where R is the mid-point of the bracket and ` is
the length of the sentence.
? It marks the edge e
i,j
that splits the tree accord-
ing to the top bracket as the ?root edge? (marked
in red in Figure 1(center))
? It then creates t from u by directing the tree out-
ward from e
i,j
as shown in Figure 1(center)
1064
The resulting t is a binary bracketing parse tree.
As implied by the above definition of h
dir
, se-
lecting which edge is the root can be interpreted
as determining the top bracket of the constituent
parse. For example, in Figure 1, the top bracket
is ([1, 2], [3, 5]) = ([DT, NN], [VBD, DT, NN]). Note
that the ?root? edge e
z
1
,z
2
partitions the leaves
into precisely this bracketing. As indicated in the
above section, we restrict the set of undirected
trees to be those such that after applying h
dir
the
resulting t is projective i.e. there are no crossing
brackets. In ?4.1, we discuss an effective heuristic
to find the top bracket without supervision.
3 Spectral Learning Algorithm based on
Additive Tree Metrics
Our goal is to recover t ? T for tag sequence x
using the data D = [(w
(i)
,x
(i)
)]
N
i=1
. To get an in-
tuition about the algorithm, consider a partition of
the set of examplesD intoD(x) = {(w
(i)
,x
(i)
) ?
D|x
(i)
= x}, i.e. each section in the partition has
an identical sequence of part of speech tags. As-
sume for this section |D(x)| is large (we address
the data sparsity issue in ?3.4).
We can then proceed by learning how to map a
POS sequence x to a tree t ? T (through u ? U)
by focusing only on examples in D(x).
Directly attempting to maximize the likelihood
unfortunately results in an intractable optimiza-
tion problem and greedy heuristics are often em-
ployed (Harmeling and Williams, 2011). Instead
we propose a method that is provably consistent
and returns a tree that can be mapped to a bracket-
ing using h
dir
.
If all the variables were observed, then the
Chow-Liu algorithm (Chow and Liu, 1968) could
be used to find the most likely tree structure u ?
U . The Chow-Liu algorithm essentially computes
the distances among all pairs of variables (the neg-
ative of the mutual information) and then finds the
minimum cost tree. However, the fact that the z
i
are latent variables makes this strategy substan-
tially more complicated. In particular, it becomes
challenging to compute the distances among pairs
of latent variables. What is needed is a ?special?
distance function that allows us to reverse engineer
the distances among the latent variables given the
distances among the observed variables. This is
the key idea behind additive tree metrics that are
the basis of our approach.
In the following sections, we describe the key
steps to our method. ?3.1 and ?3.2 largely describe
existing background on additive tree metrics and
latent tree structure learning, while ?3.3 and ?3.4
discuss novel aspects that are unique to our prob-
lem.
3.1 Additive Tree Metrics
Let u(x) be the true undirected tree of sentence x
and assume the nodes V to be indexed by [M ] =
{1, . . . ,M} such that M = |V| = H + `. Fur-
thermore, let v ? V refer to a node in the undi-
rected tree (either observed or latent). We assume
the existence of a distance function that allows us
to compute distances between pairs of nodes. For
example, as we see in ?3.2 we will define the dis-
tance d(i, j) to be a function of the covariance ma-
trix E[v
i
v
>
j
|u(x), ?(x)]. Thus if v
i
and v
j
are both
observed variables, the distance can be directly
computed from the data.
Moreover, the metrics we construct are such
that they are tree additive, defined below:
Definition 1 A function d
u(x) : [M ]?[M ]? R is
an additive tree metric (Erd?os et al, 1999) for the
undirected tree u(x) if it is a distance metric,
2
and
furthermore, ?i, j ? [M ] the following relation
holds:
d
u(x)(i, j) =
?
(a,b)?path
u(x)(i,j)
d
u(x)(a, b) (2)
where path
u(x)(i, j) is the set of all the edges in
the (undirected) path from i to j in the tree u(x).
As we describe below, given the tree structure,
the additive tree metric property allows us to com-
pute ?backwards? the distances among the latent
variables as a function of the distances among the
observed variables.
Define D to be the M ? M distance matrix
among the M variables, i.e. D
ij
= d
u(x)(i, j).
LetD
WW
, D
ZW
(equal toD
>
WZ
), andD
ZZ
indi-
cate the word-word, latent-word and latent-latent
sub-blocks of D respectively. In addition, since
u(x) is assumed to be known from context, we
denote d
u(x)(i, j) just by d(i, j).
Given the fact that the distance between a pair
of nodes is a function of the random variables
they represent (according to the true model), only
D
WW
can be empirically estimated from data.
However, if the underlying tree structure is known,
then Definition 1 can be leveraged to compute
D
ZZ
and D
ZW
as we show below.
2
This means that it satisfies d(i, j) = 0 if and only if
i = j, the triangle inequality and is also symmetric.
1065
v j v ie i , j
(a)
v ie i , jv j
(b)
Figure 3: Two types of edges in general undirected latent
trees. (a) leaf edge, (b) internal edge
We first show how to compute d(i, j) for all i, j
such that i and j are adjacent to each other in u(x),
based only on observed nodes. It then follows that
the other elements of the distance matrix can be
computed based on Definition 1. To show how to
compute distances between adjacent nodes, con-
sider the two cases: (1) (i, j) is a leaf edge; (2)
(i, j) is an internal edge.
Case 1 (leaf edge, figure 3(a)) Assume without
loss of generality that j is the leaf and i is an in-
ternal latent node. Then i must have exactly two
other neighbors a ? [M ] and b ? [M ]. Let A
denote the set of nodes that are closer to a than
i and similarly let B denote the set of nodes that
are closer to b than i. Let A
?
and B
?
denote all
the leaves (word nodes) in A and B respectively.
Then using path additivity (Definition 1), it can be
shown that for any a
?
? A
?
, b
?
? B
?
it holds that:
d(i, j) =
1
2
(d(j, a
?
) + d(j, b
?
)? d(a
?
, b
?
)) (3)
Note that the right-hand side only depends on
distances between observed random variables.
Case 2 (internal edge, figure 3(b)) Both i and
j are internal nodes. In this case, i has exactly
two other neighbors a ? [M ] and b ? [M ], and
similarly, j has exactly other two neighbors g ?
[M ] and h ? [M ]. Let A denote the set of nodes
closer to a than i, and analogously for B, G, and
H . Let A
?
, B
?
, G
?
, and H
?
refer to the leaves in
A,B,G, and H respectively. Then for any a
?
?
A
?
, b
?
? B
?
, g
?
? G
?
, and h
?
? H
?
it can be
shown that:
d(i, j) =
1
4
(
d(a
?
, g
?
) + d(a
?
, h
?
) + d(b
?
, g
?
)
+d(b
?
, h
?
)? 2d(a
?
, b
?
)? 2d(g
?
, h
?
)
)
(4)
Empirically, one can obtain a more robust em-
pirical estimate
?
d(i, j) by averaging over all valid
choices of a
?
, b
?
in Eq. 3 and all valid choices of
a
?
, b
?
, g
?
, h
?
in Eq. 4 (Desper and Gascuel, 2005).
3.2 Constructing a Spectral Additive Metric
In constructing our distance metric, we begin with
the following assumption on the distribution in
Eq. 1 (analogous to the assumptions made in
Anandkumar et al, 2011).
Assumption 1 (Linear, Rank m, Means)
E[z
i
|pix(zi),x] = A
(z
i
|z
pix(z
i
)
,x)pix(zi) ?i ? [H]
where A
(z
i
|pix(z
i
),x) ? R
m?m
has rank m.
E[w
i
|pix(wi),x] = C
(w
i
|pix(w
i
),x)pix(wi) ?i ? [`(x)]
where C
(w
i
|pix(w
i
),x) ? R
p?m
has rank m.
Also assume that E[z
i
z
>
i
|x] has rank m ?i ?
[H].
Note that the matrices A and C are a direct
function of ?(x), but we do not specify a model
family for ?(x). The only restriction is in the form
of the above assumption. If w
i
and z
i
were dis-
crete, represented as binary vectors, the above as-
sumption would correspond to requiring all con-
ditional probability tables in the latent tree to have
rankm. Assumption 1 allows for the w
i
to be high
dimensional features, as long as the expectation
requirement above is satisfied. Similar assump-
tions are made with spectral parameter learning
methods e.g. Hsu et al (2009), Bailly et al (2009),
Parikh et al (2011), and Cohen et al (2012).
Furthermore, Assumption 1 makes it explicit
that regardless of the size of p, the relationships
among the variables in the latent tree are restricted
to be of rank m, and are thus low rank since p >
m. To leverage this low rank structure, we propose
using the following additive metric, a normalized
variant of that in Anandkumar et al (2011):
d
spectral
(i, j) = ? log ?
m
(?x(i, j))
+
1
2
log ?
m
(?x(i, i)) +
1
2
log ?
m
(?x(j, j)) (5)
where ?
m
(A) denotes the product of the top m
singular values of A and ?x(i, j) := E[viv
>
j
|x],
i.e. the uncentered cross-covariance matrix.
We can then show that this metric is additive:
Lemma 1 If Assumption 1 holds then, d
spectral
is
an additive tree metric (Definition 1).
A proof is in the supplementary for completeness.
From here, we use d to denote d
spectral
, since that
is the metric we use for our learning algorithm.
1066
3.3 Recovering the Minimal Projective
Latent Tree
It has been shown (Rzhetsky and Nei, 1993) that
for any additive tree metric, u(x) can be recovered
by solving arg min
u?U
c(u) for c(u):
c(u) =
?
(i,j)?E
u
d(i, j). (6)
where E
u
is the set of pairs of nodes which are
adjacent to each other in u and d(i, j) is computed
using Eq. 3 and Eq. 4.
Note that the metric d we use in defining c(u)
is based on the expectations from the true distri-
bution. In practice, the true distribution is un-
known, and therefore we use an approximation for
the distance metric
?
d. As we discussed in ?3.1
all elements of the distance matrix are functions
of observable quantities if the underlying tree u is
known. However, only the word-word sub-block
D
WW
can be directly estimated from the data
without knowledge of the tree structure.
This subtlety makes solving the minimization
problem in Eq. 6 NP-hard (Desper and Gascuel,
2005) if u is allowed to be an arbitrary undirected
tree. However, if we restrict u to be in U , as we do
in the above, then maximizing c?(u) over U can be
solved using the bilexical parsing algorithm from
Eisner and Satta (1999). This is because the com-
putation of the other sub-blocks of the distance
matrix only depend on the partitions of the nodes
shown in Figure 3 into A, B, G, and H , and not
on the entire tree structure.
Therefore, the procedure to find a bracketing
for a given POS tag x is to first estimate the dis-
tance matrix sub-block
?
D
WW
from raw text data
(see ?3.4), and then solve the optimization prob-
lem arg min
u?U
c?(u) using a variant of the Eisner-
Satta algorithm where c?(u) is identical to c(u) in
Eq. 6, with d replaced with
?
d.
Summary. We first defined a generative model
that describes how a sentence, its sequence of POS
tags, and its bracketing is generated (?2.3). First
an undirected u ? U is generated (only as a func-
tion of the POS tags), and then u is mapped to
a bracketing using a direction mapping h
dir
. We
then showed that we can define a distance met-
ric between nodes in the undirected tree, such that
minimizing it leads to a recovery of u. This dis-
tance metric can be computed based only on the
text, without needing to identify the latent infor-
mation (?3.2). If the true distance metric is known,
Algorithm 1 The learning algorithm for find-
ing the latent structure from a set of examples
(w
(i)
,x
(i)
), i ? [N ].
Inputs: Set of examples (w
(i)
,x
(i)
) for i ? [N ],
a kernel K
?
(j, k, j
?
, k
?
|x,x
?
), an integer m
Data structures: For each i ? [N ], j, k ?
`(x
(i)
) there is a (uncentered) covariance matrix
?
?x(i)(j, k) ? R
p?p
, and a distance
?
d
spectral
(j, k).
Algorithm:
(Covariance estimation) ?i ? [N ], j, k ? `(x
(i)
)
? Let C
j
?
,k
?
|i
? = w
(i
?
)
j
?
(w
(i
?
)
k
?
)
>
, k
j,k,j
?
,k
?
,i,i
?
=
K
?
(j, k, j
?
, k
?
|x
(i)
,x
(i
?
)
) and `
i
?
= `(x
(i
?
)
),
and estimate each p? p covariance matrix as:
?
?x(j, k) =
?
N
i
?
=1
?
`
i
?
j
?
=1
?
`
i
?
k
?
=1
k
j,k,j
?
,k
?
,i,i
?
C
j
?
,k
?
|i
?
?
N
i
?
=1
?
`
i
?
j
?
=1
?
`
i
?
k
?
=1
k
j,k,j
?
,k
?
,i,i
?
? Compute
?
d
spectral
(j, k) ?j, k ? `(x
(i)
) using
Eq. 5.
(Uncover structure) ?i ? [N ]
? Find u?
(i)
= arg min
u?U
c?(u), and for the ith
example, return the structure h
dir
(u?
(i)
).
with respect to the true distribution that generates
the words in a sentence, then u can be fully recov-
ered by optimizing the cost function c(u). How-
ever, in practice the distance metric must be esti-
mated from data, as discussed below.
3.4 Estimation of d from Sparse Data
We now address the data sparsity problem, in par-
ticular that D(x) can be very small, and therefore
estimating d for each POS sequence separately can
be problematic.
3
In order to estimate d from data, we need to es-
timate the covariance matrices ?x(i, j) (for i, j ?
{1, . . . , `(x)}) from Eq. 5.
To give some motivation to our solu-
tion, consider estimating the covariance
matrix ?x(1, 2) for the tag sequence
x = (DT
1
, NN
2
, VBD
3
, DT
4
, NN
5
). D(x) may
be insufficient for an accurate empirical es-
3
This data sparsity problem is quite severe ? for example,
the Penn treebank (Marcus et al, 1993) has a total number
of 43,498 sentences, with 42,246 unique POS tag sequences,
averaging |D(x)| to be 1.04.
1067
timate. However, consider another sequence
x
?
= (RB
1
, DT
2
, NN
3
, VBD
4
, DT
5
, ADJ
6
, NN
7
).
Although x and x
?
are not identical, it is likely
that ?x?(2, 3) is similar to ?x(1, 2) because the
determiner and the noun appear in similar syn-
tactic context. ?x?(5, 7) also may be somewhat
similar, but ?x?(2, 7) should not be very similar
to ?x(1, 2) because the noun and the determiner
appear in a different syntactic context.
The observation that the covariance matrices
depend on local syntactic context is the main driv-
ing force behind our solution. The local syntactic
context acts as an ?anchor,? which enhances or re-
places a word index in a sentence with local syn-
tactic context. More formally, an anchor is a func-
tion G that maps a word index j and a sequence of
POS tags x to a local context G(j,x). The anchor
we use is G(j,x) = (j, x
j
). Then, the covariance
matrices ?x are estimated using kernel smooth-
ing (Hastie et al, 2009), where the smoother tests
similarity between the different anchors G(j,x).
The full learning algorithm is given in Figure 1.
The first step in the algorithm is to estimate the
covariance matrix block
?
?x(i)(j, k) for each train-
ing example x
(i)
and each pair of preterminal po-
sitions (j, k) in x
(i)
. Instead of computing this
block by computing the empirical covariance ma-
trix for positions (j, k) in the data D(x), the al-
gorithm uses all of the pairs (j
?
, k
?
) from all of
N training examples. It averages the empirical
covariance matrices from these contexts using a
kernel weight, which gives a similarity measure
for the position (j, k) in x
(i)
and (j
?
, k
?
) in an-
other example x
(i
?
)
. ? is the kernel ?bandwidth?,
a user-specified parameter that controls how in-
clusive the kernel will be with respect to exam-
ples in D (see ? 4.1 for a concrete example). Note
that the learning algorithm is such that it ensures
that
?
?x(i)(j, k) =
?
?x(i?)(j
?
, k
?
) if G(j,x
(i)
) =
G(j
?
,x
(i
?
)
) and G(k,x
(i)
) = G(k
?
,x
(i
?
)
).
Once the empirical estimates for the covariance
matrices are obtained, a variant of the Eisner-Satta
algorithm is used, as mentioned in ?3.3.
3.5 Theoretical Guarantees
Our main theoretical guarantee is that Algorithm 1
will recover the correct tree u ? U with high prob-
ability, if the given top bracket is correct and if
we obtain enough examples (w
(i)
,x
(i)
) from the
model in ?2. We give the theorem statement be-
low. The constants lurking in the O-notation and
the full proof are in the supplementary.
Denote ?x(j, k)
(r)
as the r
th
singu-
lar value of ?x(j, k). Let ?
?
(x) :=
min
j,k?`(x) min
(
?x(j, k)
(m)
)
.
Theorem 1 Define u? as the estimated tree for tag
sequence x and u(x) as the correct tree. Let
4(x) := min
u
?
?U :u
?
6=u(x)
(c(u(x))? c(u
?
))/(8|`(x)|)
Assume that
N ? O
?
?
m
2
log
(
p
2
`(x)2
?
)
min(?
?
(x)
2
4(x)
2
, ?
?
(x)
2
)?x(?)
2
?
?
Then with probability 1? ?, u? = u(x).
where ?x(?), defined in the supplementary, is a
function of the underlying distribution over the tag
sequences x and the kernel bandwidth ?.
Thus, the sample complexity of our approach
depends on the dimensionality of the latent and
observed states (m and p), the underlying singu-
lar values of the cross-covariance matrices (?
?
(x))
and the difference in the cost of the true tree com-
pared to the cost of the incorrect trees (4(x)).
4 Experiments
We report results on three different languages: En-
glish, German, and Chinese. For English we use
the Penn treebank (Marcus et al, 1993), with sec-
tions 2?21 for training and section 23 for final
testing. For German and Chinese we use the Ne-
gra treebank and the Chinese treebank respectively
and the first 80% of the sentences are used for
training and the last 20% for testing. All punc-
tuation from the data is removed.
4
We primarily compare our method to the
constituent-context model (CCM) of Klein and
Manning (2002). We also compare our method to
the algorithm of Seginer (2007).
4.1 Experimental Settings
Top bracket heuristic Our algorithm requires
the top bracket in order to direct the latent tree.
In practice, we employ the following heuristic to
find the bracket using the following three steps:
? If there exists a comma/semicolon/colon at in-
dex i that has at least a verb before i and both
a noun followed by a verb after i, then return
([0, i ? 1], [i, `(x)]) as the top bracket. (Pick
the rightmost comma/semicolon/colon if multi-
ple satisfy the criterion).
4
We make brief use of punctuation for our top bracket
heuristic detailed below before removing it.
1068
Length CCM CCM-U CCM-OB CCM-UB
? 10 72.5 57.1 58.2 62.9
? 15 54.1 36 24 23.7
? 20 50 34.7 19.3 19.1
? 25 47.2 30.7 16.8 16.6
? 30 44.8 29.6 15.3 15.2
? 40 26.3 13.5 13.9 13.8
Table 1: Comparison of different CCM variants on English
(training). U stands for universal POS tagset, OB stands for
conjoining original POS tags with Brown clusters and UB
stands for conjoining universal POS tags with Brown clusters.
The best setting is just the vanilla setting, CCM.
? Otherwise find the first non-participle verb (say
at index j) and return ([0, j ? 1], [j, `(x)]).
? If no verb exists, return ([0, 1], [1, `(x)]).
Word embeddings As mentioned earlier, each
w
i
can be an arbitrary feature vector. For all lan-
guages we use Brown clustering (Brown et al,
1992) to construct a log(C) + C feature vector
where the first log(C) elements indicate which
mergable cluster the word belongs to, and the last
C elements indicate the cluster identity. For En-
glish, more sophisticated word embeddings are
easily obtainable, and we experiment with neural
word embeddings Turian et al (2010) of length
50. We also explored two types of CCA embed-
dings: OSCCA and TSCCA, given in Dhillon et
al. (2012). The OSCCA embeddings behaved bet-
ter, so we only report its results.
Choice of kernel For our experiments, we use
the kernel
K
?
(j, k, j
?
, k
?
|x,x
?
)
= max
{
0, 1?
?(j, k, j
?
, k
?
|x,x
?
)
?
}
where ? denotes the user-specified bandwidth,
and ?(j, k, j
?
, k
?
|x,x
?
) =
|j ? k| ? |j
?
? k
?
|
|j ? k|+ |j
?
? k
?
|
if
x(j) = x(j
?
) and x(k
?
) = x(k), and sign(j ?
k) = sign(j
?
? k
?
) (and? otherwise).
The kernel is non-zero if and only if the tags at
position j and k in x are identical to the ones in
position j
?
and k
?
in x
?
, and if the direction be-
tween j and k is identical to the one between j
?
and k
?
. Note that the kernel is not binary, as op-
posed to the theoretical kernel in the supplemen-
tary material. Our experiments show that using a
non-zero value different than 1 that is a function
of the distance between j and k compared to the
distance between j
?
and k
?
does better in practice.
Choice of data For CCM, we found that if the
full dataset (all sentence lengths) is used in train-
ing, then performance degrades when evaluating
on sentences of length ? 10. We therefore restrict
the data used with CCM to sentences of length
? `, where ` is the maximal sentence length being
evaluated. This does not happen with our algo-
rithm, which manages to leverage lexical informa-
tion whenever more data is available. We therefore
use the full data for our method for all lengths.
We also experimented with the original POS
tags and the universal POS tags of Petrov et al
(2011). Here, we found out that our method
does better with the universal part of speech tags.
For CCM, we also experimented with the origi-
nal parts of speech, universal tags (CCM-U), the
cross-product of the original parts of speech with
the Brown clusters (CCM-OB), and the cross-
product of the universal tags with the Brown clus-
ters (CCM-UB). The results in Table 1 indicate
that the vanilla setting is the best for CCM.
Thus, for all results, we use universal tags for
our method and the original POS tags for CCM.
We believe that our approach substitutes the need
for fine-grained POS tags with the lexical informa-
tion. CCM, on the other hand, is fully unlexical-
ized.
Parameter Selection Our method requires two
parameters, the latent dimension m and the band-
width ?. CCM also has two parameters, the num-
ber of extra constituent/distituent counts used for
smoothing. For both methods we chose the best
parameters for sentences of length ` ? 10 on the
English Penn Treebank (training) and used this
set for all other experiments. This resulted in
m = 7, ? = 0.4 for our method and 2, 8 for
CCM?s extra constituent/distituent counts respec-
tively. We also tried letting CCM choose differ-
ent hyperparameters for different sentence lengths
based on dev-set likelihood, but this gave worse
results than holding them fixed.
4.2 Results
Test I: Accuracy Table 2 summarizes our re-
sults. CCM is used with the initializer proposed
in Klein and Manning (2002).
5
NN, CC, and BC
indicate the performance of our method for neural
embeddings, CCA embeddings, and Brown clus-
tering respectively, using the heuristic for h
dir
de-
5
We used the implementation available at
http://tinyurl.com/lhwk5n6.
1069
` English German Chinese
NN-O NN CC-O CC BC-O BC CCM BC-O BC CCM BC-O BC CCM
t
r
a
i
n
? 10 70.9 69.2 70.4 68.7 71.1 69.3 72.5 64.6 59.9 62.6 64.9 57.3 46.1
? 20 55.1 53.5 53.2 51.6 53.0 51.5 50 52.7 48.7 47.9 51.4 46 22.4
? 40 46.1 44.5 43.6 41.9 43.3 41.8 26.3 46.7 43.6 19.8 42.6 38.6 15
t
e
s
t
? 10 69.2 66.7 68.3 65.5 68.9 66.1 70.5 66.4 61.6 64.7 58.0 53.2 40.7
? 15 60.3 58.3 58.6 56.4 58.6 56.5 53.8 57.5 53.5 49.6 54.3 49.4 35.9
? 20 54.1 52.3 52.3 50.3 51.9 50.2 50.4 52.8 49.2 48.9 49.7 45.5 20.1
? 25 50.8 49.0 48.6 46.6 48.3 46.6 47.4 50.0 46.8 45.6 46.7 42.7 17.8
? 30 48.1 46.3 45.6 43.7 45.4 43.8 44.9 48.3 45.4 21.9 44.6 40.7 16.1
? 40 45.5 43.8 43.0 41.1 42.7 41.1 26.1 46.9 44.1 20.1 42.2 38.6 14.3
Table 2: F
1
bracketing measure for the test sets and train sets in three languages. NN, CC, and BC indicate the performance of
our method for neural embeddings, CCA embeddings, and Brown clustering respectively, using the heuristic for h
dir
described
in ? 4.1. NN-O, CC-O, and BC-O indicate that the oracle (i.e. true top bracket) was used for h
dir
.
0
5
1 0
1 5
2 0
2 5
3 0
3 5
20- 30 31- 40 41- 50 51- 60 61- 70 71- 80
Fre
que
ncy
 
Bracketing F1 
CCM Random Restarts (Length <= 10) 
Figure 4: Histogram showing performance of CCM across
100 random restarts for sentences of length ? 10.
scribed in ? 4.1. NN-O, CC-O, and BC-O indicate
that the oracle (i.e. true top bracket) was used for
h
dir
. For our method, test set results can be ob-
tained by using Algorithm 1 (except the distances
are computed using the training data).
For English, while CCM behaves better for
short sentences (` ? 10), our algorithm is more
robust with longer sentences. This is especially
noticeable for length ? 40, where CCM breaks
down and our algorithm is more stable. We find
that the neural embeddings modestly outperform
the CCA and Brown cluster embeddings.
The results for German are similar, except CCM
breaks down earlier at sentences of ` ? 30. For
Chinese, our method substantially outperforms
CCM for all lengths. Note that CCM performs
very poorly, obtaining only around 20% accu-
racy even for sentences of ` ? 20. We didn?t
have neural embeddings for German and Chinese
(which worked best for English) and thus only
used Brown cluster embeddings.
For English, the disparity between NN-O (ora-
cle top bracket) and NN (heuristic top bracket) is
rather low suggesting that our top bracket heuris-
tic is rather effective. However, for German and
Chinese note that the ?BC-O? performs substan-
tially better, suggesting that if we had a better top
bracket heuristic our performance would increase.
Test II: Sensitivity to initialization The EM al-
gorithm with the CCM requires very careful ini-
tialization, which is described in Klein and Man-
ning (2002). If, on the other hand, random ini-
tialization is used, the variance of the performance
of the CCM varies greatly. Figure 4 shows a his-
togram of the performance level for sentences of
length ? 10 for different random initializers. As
one can see, for some restarts, CCM obtains ac-
curacies lower than 30% due to local optima. Our
method does not suffer from local optima and thus
does not require careful initialization.
Test III: Comparison to Seginer?s algorithm
Our approach is not directly comparable to
Seginer?s because he uses punctuation, while we
use POS tags. Using Seginer?s parser we were
able to get results on the training sets. On English:
75.2% (` ? 10), 64.2% (` ? 20), 56.7% (` ? 40).
On German: 57.8% (` ? 10), 45.0% (` ? 20), and
39.9% (` ? 40). On Chinese: 56.6% (` ? 10),
45.1% (` ? 20), and 38.9% (` ? 40).
Thus, while Seginer?s method performs better
on English, our approach performs 2-3 points bet-
ter on German, and both methods give similar per-
formance on Chinese.
5 Conclusion
We described a spectral approach for unsu-
pervised constituent parsing that comes with
theoretical guarantees on latent structure recovery.
Empirically, our algorithm performs favorably to
the CCM of Klein and Manning (2002) without
the need for careful initialization.
Acknowledgements: This work is supported
by NSF IIS1218282, NSF IIS1111142, NIH
R01GM093156, and the NSF Graduate Research
Fellowship Program under Grant No. 0946825
(NSF Fellowship to APP).
1070
References
A. Anandkumar, K. Chaudhuri, D. Hsu, S. M. Kakade,
L. Song, and T. Zhang. 2011. Spectral methods
for learning multivariate latent tree structure. arXiv
preprint arXiv:1107.1283.
R. Bailly, F. Denis, and L. Ralaivola. 2009. Gram-
matical inference as a principal component analysis
problem. In Proceedings of ICML.
R. Bailly, X. Carreras, F. M. Luque, and A. Quattoni.
2013. Unsupervised spectral learning of WCFG
as low-rank matrix completion. In Proceedings of
EMNLP.
P. F. Brown, P.V. Desouza, R.L. Mercer, V.J.D. Pietra,
and J.C. Lai. 1992. Class-based n-gram mod-
els of natural language. Computational linguistics,
18(4):467?479.
O. P. Buneman. 1971. The recovery of trees from mea-
sures of dissimilarity. Mathematics in the archaeo-
logical and historical sciences.
P. Buneman. 1974. A note on the metric properties of
trees. Journal of Combinatorial Theory, Series B,
17(1):48?50.
M.J. Choi, V. YF Tan, A. Anandkumar, and A.S. Will-
sky. 2011. Learning latent tree graphical mod-
els. The Journal of Machine Learning Research,
12:1771?1812.
C. K. Chow and C. N. Liu. 1968. Approximating
Discrete Probability Distributions With Dependence
Trees. IEEE Transactions on Information Theory,
IT-14:462?467.
S. B. Cohen and N. A. Smith. 2009. Shared logistic
normal distributions for soft parameter tying in un-
supervised grammar induction. In Proceedings of
HLT-NAACL.
S. B. Cohen and N. A. Smith. 2012. Empirical risk
minimization for probabilistic grammars: Sample
complexity and hardness of learning. Computa-
tional Linguistics, 38(3):479?526.
S. B. Cohen, K. Stratos, M. Collins, D. P. Foster, and
L. Ungar. 2012. Spectral learning of latent-variable
PCFGs. In Proceedings of ACL.
R. Desper and O. Gascuel. 2005. The minimum evo-
lution distance-based approach to phylogenetic in-
ference. Mathematics of evolution and phylogeny,
pages 1?32.
P. S. Dhillon, J. Rodu, D. P. Foster, and L. H. Ungar.
2012. Two step cca: A new spectral method for es-
timating vector models of words. In Proceedings of
ICML.
J. Eisner and G. Satta. 1999. Efficient parsing for
bilexical context-free grammars and head automaton
grammars. In Proceedings of ACL.
P. Erd?os, M. Steel, L. Sz?ekely, and T. Warnow. 1999.
A few logs suffice to build (almost) all trees: Part ii.
Theoretical Computer Science, 221(1):77?118.
J. Gillenwater, K. Ganchev, J. Grac?a, F. Pereira, and
B. Taskar. 2010. Sparsity in dependency grammar
induction. In Proceedings of ACL.
K. Gimpel and N.A. Smith. 2012. Concavity and ini-
tialization for unsupervised dependency parsing. In
Proceedings of NAACL.
D. Golland, J. DeNero, and J. Uszkoreit. 2012. A
feature-rich constituent context model for grammar
induction. In Proceedings of ACL.
M. Gormley and J. Eisner. 2013. Nonconvex global
optimization for latent-variable models. In Proceed-
ings of ACL.
S. Harmeling and C. KI Williams. 2011. Greedy
learning of binary latent trees. Pattern Analysis
and Machine Intelligence, IEEE Transactions on,
33(6):1087?1097.
T. Hastie, R. Tibshirani, and J. Friedman. 2009. The
Elements of Statistical Learning: Data Mining, In-
ference, and Prediction. Springer Series in Statis-
tics. Springer Verlag.
W. P. Headden, M. Johnson, and D. McClosky. 2009.
Improving unsupervised dependency parsing with
richer contexts and smoothing. In Proceedings of
NAACL-HLT.
D. Hsu, S. Kakade, and T. Zhang. 2009. A spectral
algorithm for learning hidden Markov models. In
Proceedings of COLT.
D. Hsu, S. M. Kakade, and P. Liang. 2012. Identi-
fiability and unmixing of latent parse trees. arXiv
preprint arXiv:1206.3137.
M. Ishteva, H. Park, and L. Song. 2012. Unfolding
latent tree structures using 4th order tensors. arXiv
preprint arXiv:1210.1258.
F. Jelinek, J. D. Lafferty, and R. L. Mercer. 1992. Ba-
sic methods of probabilistic context free grammars.
Springer.
D. Klein and C. D. Manning. 2002. A generative
constituent-context model for improved grammar in-
duction. In Proceedings of ACL.
M. Kolar, A. P. Parikh, and E. P. Xing. 2010a. On
sparse nonparametric conditional covariance selec-
tion. In Proceedings of ICML.
M. Kolar, L. Song, A. Ahmed, and E. P. Xing. 2010b.
Estimating time-varying networks. The Annals of
Applied Statistics, 4(1):94?123.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn treebank. Computational Linguis-
tics, 19:313?330.
1071
A.P. Parikh, L. Song, and E.P. Xing. 2011. A spectral
algorithm for latent tree graphical models. In Pro-
ceedings of ICML.
S. Petrov, D. Das, and R. McDonald. 2011. A univer-
sal part-of-speech tagset. ArXiv:1104.2086.
A. Rzhetsky and M. Nei. 1993. Theoretical founda-
tion of the minimum-evolution method of phyloge-
netic inference. Molecular Biology and Evolution,
10(5):1073?1095.
N. Saitou and M. Nei. 1987. The neighbor-joining
method: a new method for reconstructing phylo-
genetic trees. Molecular biology and evolution,
4(4):406?425.
Y. Seginer. 2007. Fast unsupervised incremental pars-
ing. In Proceedings of ACL.
N. A. Smith and J. Eisner. 2005. Contrastive estima-
tion: Training log-linear models on unlabeled data.
In Proceedings of ACL.
L. Song, A.P. Parikh, and E.P. Xing. 2011. Kernel
embeddings of latent tree graphical models. In Pro-
ceedings of NIPS.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2010a.
From baby steps to leapfrog: how less is more in
unsupervised dependency parsing. In Proceedings
of NAACL.
V. I. Spitkovsky, H. Alshawi, D. Jurafsky, and C. D.
Manning. 2010b. Viterbi training improves un-
supervised dependency parsing. In Proceedings of
CoNLL.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2013.
Breaking out of local optima with count transforms
and model recombination: A study in grammar in-
duction. In Proceedings of EMNLP.
J. P. Turian, L.-A. Ratinov, and Y. Bengio. 2010. Word
representations: A simple and general method for
semi-supervised learning. In Proceedings of ACL.
S. Zhou, J. Lafferty, and L. Wasserman. 2010. Time
varying undirected graphs. Machine Learning,
80(2-3):295?319.
1072
Dynamic Language Models for Streaming Text
Dani Yogatama? Chong Wang? Bryan R. Routledge? Noah A. Smith? Eric P. Xing?
?School of Computer Science
?Tepper School of Business
Carnegie Mellon University
Pittsburgh, PA 15213, USA
?{dyogatama,chongw,nasmith,epxing}@cs.cmu.edu, ?routledge@cmu.edu
Abstract
We present a probabilistic language model that
captures temporal dynamics and conditions on
arbitrary non-linguistic context features. These
context features serve as important indicators
of language changes that are otherwise difficult
to capture using text data by itself. We learn
our model in an efficient online fashion that is
scalable for large, streaming data. With five
streaming datasets from two different genres?
economics news articles and social media?we
evaluate our model on the task of sequential
language modeling. Our model consistently
outperforms competing models.
1 Introduction
Language models are a key component in many NLP
applications, such as machine translation and ex-
ploratory corpus analysis. Language models are typi-
cally assumed to be static?the word-given-context
distributions do not change over time. Examples
include n-gram models (Jelinek, 1997) and proba-
bilistic topic models like latent Dirichlet allocation
(Blei et al., 2003); we use the term ?language model?
to refer broadly to probabilistic models of text.
Recently, streaming datasets (e.g., social media)
have attracted much interest in NLP. Since such data
evolve rapidly based on events in the real world, as-
suming a static language model becomes unrealistic.
In general, more data is seen as better, but treating all
past data equally runs the risk of distracting a model
with irrelevant evidence. On the other hand, cau-
tiously using only the most recent data risks overfit-
ting to short-term trends and missing important time-
insensitive effects (Blei and Lafferty, 2006; Wang
et al., 2008). Therefore, in this paper, we take steps
toward methods for capturing long-range temporal
dynamics in language use.
Our model also exploits observable context vari-
ables to capture temporal variation that is otherwise
difficult to capture using only text. Specifically for
the applications we consider, we use stock market
data as exogenous evidence on which the language
model depends. For example, when an important
company?s price moves suddenly, the language model
should be based not on the very recent history, but
should be similar to the language model for a day
when a similar change happened, since people are
likely to say similar things (either about that com-
pany, or about conditions relevant to the change).
Non-linguistic contexts such as stock price changes
provide useful auxiliary information that might indi-
cate the similarity of language models across differ-
ent timesteps.
We also turn to a fully online learning framework
(Cesa-Bianchi and Lugosi, 2006) to deal with non-
stationarity and dynamics in the data that necessitate
adaptation of the model to data in real time. In on-
line learning, streaming examples are processed only
when they arrive. Online learning also eliminates
the need to store large amounts of data in memory.
Strictly speaking, online learning is distinct from
stochastic learning, which for language models built
on massive datasets has been explored by Hoffman
et al. (2013) and Wang et al. (2011). Those tech-
niques are still for static modeling. Language model-
ing for streaming datasets in the context of machine
translation was considered by Levenberg and Os-
borne (2009) and Levenberg et al. (2010). Goyal
et al. (2009) introduced a streaming algorithm for
large scale language modeling by approximating n-
gram frequency counts. We propose a general online
learning algorithm for language modeling that draws
inspiration from regret minimization in sequential
predictions (Cesa-Bianchi and Lugosi, 2006) and on-
181
Transactions of the Association for Computational Linguistics, 2 (2014) 181?192. Action Editor: Eric Fosler-Lussier.
Submitted 10/2013; Revised 2/2014; Published 4/2014. c?2014 Association for Computational Linguistics.
line variational algorithms (Sato, 2001; Honkela and
Valpola, 2003).
To our knowledge, our model is the first to bring
together temporal dynamics, conditioning on non-
linguistic context, and scalable online learning suit-
able for streaming data and extensible to include
topics and n-gram histories. The main idea of our
model is independent of the choice of the base lan-
guage model (e.g., unigrams, bigrams, topic models,
etc.). In this paper, we focus on unigram and bi-
gram language models in order to evaluate the basic
idea on well understood models, and to show how it
can be extended to higher-order n-grams. We leave
extensions to topic models for future work.
We propose a novel task to evaluate our proposed
language model. The task is to predict economics-
related text at a given time, taking into account the
changes in stock prices up to the corresponding day.
This can be seen an inverse of the setup considered
by Lavrenko et al. (2000), where news is assumed
to influence stock prices. We evaluate our model
on economics news in various languages (English,
German, and French), as well as Twitter data.
2 Background
In this section, we first discuss the background for
sequential predictions then describe how to formulate
online language modeling as sequential predictions.
2.1 Sequential Predictions
Let w1, w2, . . . , wT be a sequence of response vari-
ables, revealed one at a time. The goal is to design
a good learner to predict the next response, given
previous responses and additional evidence which
we denote by xt ? RM (at time t). Throughout this
paper, we use the term features for x. Specifically, at
each round t, the learner receives xt and makes a pre-
diction w?t, by choosing a parameter vector?t ? RM .
In this paper, we refer to ? as feature coefficients.
There has been an enormous amount of work on
online learning for sequential predictions, much of it
building on convex optimization. For a sequence of
loss functions `1, `2, . . . , `T (parameterized by ?),
an online learning algorithm is a strategy to minimize
the regret, with respect to the best fixed ?? in hind-
sight.1 Regret guarantees assume a Lipschitz con-
1Formally, the regret is defined as RegretT (??) =
dition on the loss function ` that can be prohibitive
for complex models. See Cesa-Bianchi and Lugosi
(2006), Rakhlin (2009), Bubeck (2011), and Shalev-
Shwartz (2012) for in-depth discussion and review.
There has also been work on online and stochastic
learning for Bayesian models (Sato, 2001; Honkela
and Valpola, 2003; Hoffman et al., 2013), based on
variational inference. The goal is to approximate pos-
terior distributions of latent variables when examples
arrive one at a time.
In this paper, we will use both kinds of techniques
to learn language models for streaming datasets.
2.2 Problem Formulation
Consider an online language modeling problem, in
the spirit of sequential predictions. The task is to
build a language model that accurately predicts the
texts generated on day t, conditioned on observ-
able features up to day t, x1:t. Every day, after
the model makes a prediction, the actual texts wt
are revealed and we suffer a loss. The loss is de-
fined as the negative log likelihood of the model
`t = ? log p(wt | ?,?1:t?1,x1:t?1,n1:t?1), where
? and ?1:T are the model parameters andn is a back-
ground distribution (details are given in ?3.2). We
can then update the model and proceed to day t+ 1.
Notice the similarity to the sequential prediction de-
scribed above. Importantly, this is a realistic setup for
building evolving language models from large-scale
streaming datasets.
3 Model
3.1 Notation
We index timesteps by t ? {1, . . . , T} and word
types by v ? {1, . . . , V }, both are always given as
subscripts. We denote vectors in boldface and use
1 : T as a shorthand for {1, 2, . . . , T}. We assume
words of the form {wt}Tt=1 for wt ? RV , which is
the vector of word frequences at timetstep t. Non-
linguistic context features are {xt}Tt=1 for xt ? RM .
The goal is to learn parameters ? and ?1:T , which
will be described in detail next.
3.2 Generative Story
The main idea of our model is illustrated by the fol-
lowing generative story for the unigram language
PT
t=1 `t(xt,?t, wt)? inf??
PT
t=1 `t(xt,??, wt).
182
model. (We will discuss the extension to higher-order
language models later.) A graphical representation
of our proposed model is given in Figure 1.
1. Draw feature coefficients ? ? N(0, ?I).2 Here
? is a vector in RM , where M is the dimension-
ality of the feature vector.
2. For each timestep t:
(a) Observe non-linguistic context features xt.
(b) Draw ?t ?
N
(?t?1
k=1 ?k
exp(?>f(xt,xk))Pt?1
j=1 ?j exp(?>f(xt,xj))
?k, ?I
)
.
Here, ?t is a vector in RV , where V is
the size of the word vocabulary, ? is
the variance parameter and ?k is a fixed
hyperparameter; we discuss them below.
(c) For each word wt,v, draw wt,v ?
Categorical
(
exp(n1:t?1,v+?t,v)P
j?V exp(n1:t?1,j+?t,j)
)
.
In the last step, ?t and n are mapped to the V -
dimensional simplex, forming a distribution over
words. n1:t?1 ? RV is a background (log) distri-
bution, inspired by a similar idea in Eisenstein et al.
(2011). In this paper, we set n1:t?1,v to be the log-
frequency of v up to time t? 1. We can interpret ?
as a time-dependent deviation from the background
log-frequencies that incorporates world-context. This
deviation comes in the form of a weighted average of
earlier deviation vectors.
The intuition behind the model is that the probabil-
ity of a word appearing at day t depends on the back-
ground log-frequencies, the deviation coefficients of
the word at previous timesteps ?1:t?1, and the sim-
ilarity of current conditions of the world (based on
observable features x) to previous timesteps through
f(xt,xk). That is, f is a function that takes d-
dimensional feature vectors at two timesteps xt and
xk and returns a similarity vector f(xt,xk) ? RM
(see ?6.1.1 for an example of f that we use in our
experiments). The similarity is parameterized by ?,
and decays over time with rate ?k. In this work, we
assume a fixed window size c (i.e., we consider c
most recent timesteps), so that ?1:t?c?1 = 0 and
?t?c:t?1 = 1. This allows up to cth order depen-
dencies.3 Setting ? this way allows us to bound the
2Feature coefficients ? can be also drawn from other distri-
butions such as ? ? Laplace(0, ?).
3In online Bayesian learning, it is known that forgetting
inaccurate estimates from earlier timesteps is important (Sato,
 
xtxsxrxq
wq wr ws wt
 t s r q
?
NrNq Ns Nt
T
Figure 1: Graphical representation of the model. The
subscript indices q, r, s are shorthands for the previ-
ous timesteps t ? 3, t ? 2, t ? 1. Only four timesteps
are shown here. There are arrows from previous
?t?4,?t?5, . . . ,?t?c to ?t, where c is the window size
as described in ?3.2. They are not shown here, for read-
ability.
number of past vectors ? that need to be kept in
memory. We set ?0 to 0.
Although the generative story described above
is for unigram language models, extensions can be
made to more complex models (e.g., mixture of un-
igrams, topic models, etc.) and to longer n-gram
contexts. In the case of topic models, the model
will be related to dynamic topic models (Blei and
Lafferty, 2006) augmented by context features, and
the learning procedure in ?4 can be used to perform
online learning of dynamic topic models. However,
our model captures longer-range dependencies than
dynamic topic models, and can condition on non-
linguistic features or metadata. In the case of higher-
order n-grams, one simple way is to draw more ?,
one for each history. For example, for a bigram
model, ? is in RV 2 , rather than RV in the unigram
model. We consider both unigram and bigram lan-
guage models in our experiments in ?6. However, the
main idea presented in this paper is largely indepen-
dent of the base model.
Related work. Mimno and McCallum (2008) and
Eisenstein et al. (2010) similarly conditioned text on
2001; Honkela and Valpola, 2003). Since we set ?1:t?c?1 = 0,
at every timestep t, ?k leads to forgetting older examples.
183
observable features (e.g., author, publication venue,
geography, and other document-level metadata), but
conducted inference in a batch setting, thus their ap-
proaches are not suitable for streaming data. It is not
immediately clear how to generalize their approach to
dynamic settings. Algorithmically, our work comes
closest to the online dynamic topic model of Iwata
et al. (2010), except that we also incorporate context
features.
4 Learning and Inference
The goal of the learning procedure is to minimize the
overall negative log likelihood,
? logL(D) =
? log
?
d?1:T p(?1:T | ?,x1:T )p(w1:T | ?1:T ,n).
However, this quantity is intractable. Instead, we
derive an upper bound for this quantity and minimize
that upper bound. Using Jensen?s inequality, the vari-
ational upper bound on the negative log likelihood
is:
? logL(D) ? ?
?
d?1:T q(?1:T | ?1:T ) (4)
log p(?1:T | ?,x1:T )p(w1:T | ?1:T ,n)q(?1:T | ?1:T )
.
Specifically, we use mean-field variational inference
where the variables in the variational distribution q
are completely independent. We use Gaussian distri-
butions as our variational distributions for ?, denoted
by ? in the bound in Eq. 4. We denote the parameters
of the Gaussian variational distribution for ?t,v (word
v at timestep t) by ?t,v (mean) and ?t,v (variance).
Figure 2 shows the functional form of the varia-
tional bound that we seek to minimize, denoted by B?.
The two main steps in the optimization of the bound
are inferring ?t and updating feature coefficients ?.
We next describe each step in detail.
4.1 Learning
The goal of the learning procedure is to minimize the
upper bound in Figure 2 with respect to ?. However,
since the data arrives in an online fashion, and speed
is very important for processing streaming datasets,
the model needs to be updated at every timestep t (in
our experiments, daily).
Notice that at timestep t, we only have access
to x1:t and w1:t, and we perform learning at every
timestep after the text for the current timestep wt
is revealed. We do not know xt+1:T and wt+1:T .
Nonetheless, we want to update our model so that
it can make a better prediction at t + 1. Therefore,
we can only minimize the bound until timestep t.
Let Ck , exp(?>f(xt,xk))Pt?1
j=t?c exp(?>f(xt,xj))
. Our learning al-
gorithm is a variational Expectation-Maximization
algorithm (Wainwright and Jordan, 2008).
E-step Recall that we use variational inference and
the variational parameters for ? are ? and ?. As
shown in Figure 2, since the log-sum-exp in the last
term of B is problematic, we introduce additional
variational parameters ? to simplify B and obtain
B? (Eqs. 2?3). The E-step deals with all the local
variables ?, ?, and ?.
Fixing other variables and taking the derivative
of the bound B? w.r.t. ?t and setting it to zero,
we obtain the closed-form update for ?t: ?t =?
v?V exp (n1:t?1,v) exp
(
?t,v + ?t,v2
).
To minimize with respect to ?t and ?t, we apply
gradient-based methods since there are no closed-
form solutions. The derivative w.r.t. ?t,v is:
?B?
??t,v
=?t,v ? Ck?k,v?
? nt,v +
nt
?t
exp (n1:t?1,v) exp
(
?t,v +
?t,v
2
)
,
where nt =?v?V nt,v.
The derivative w.r.t. ?t,v is:
?B?
??t,v
= 12?t,v
+ 12? +
nt
2?t
exp (n1:t?1,v) exp
(
?t,v +
?t,v
2
)
.
Although we require iterative methods in the E-step,
we find it to be reasonably fast in practice.4 Specifi-
cally, we use the L-BFGS quasi-Newton algorithm
(Liu and Nocedal, 1989).
We can further improve the bound by updating
the variational parameters for timestep 1 : t? 1, i.e.,
?1:t?1 and?1:t?1, as well. However, this will require
storing the texts from previous timesteps. Addition-
ally, this will complicate the M-step update described
4Approximately 16.5 seconds/day (walltime) to learn the
model on the EN:NA dataset on a 2.40GHz CPU with 24GB
memory.
184
B =?
T?
t=1
Eq[log p(?t | ?k,?,xt)]?
T?
t=1
Eq[log p(wt | ?t,nt)]?H(q) (1)
=
T?
t=1
?
??
??
1
2
?
j?V
log ?t,j? ? Eq
?
???
(
?t ?
?t?1
k=t?c Ck?k
)2
2?
?
??? Eq
?
??
v?wt
n1:t?1,v + ?t,v ? log
?
j?V
exp(n1:t?1,j + ?t,j)
?
?
?
??
??
(2)
?
T?
t=1
?
??
??
1
2
?
j?V
log ?t,v? +
(
?t ?
?t?1
k=t?c Ck?k
)2
2? +
?t +
?t?1
k=t?c C2k?k
2?
?
?
v?wt
?
??t,v ? log ?t ?
1
?t
?
j?V
exp (n1:t?1,j) exp
(
?t,j +
?t,j
2
)
?
?
?
??
??
+ const (3)
Figure 2: The variational bound that we seek to minimize, B. H(q) is the entropy of the variational distribution q. The
derivation from line 1 to line 2 is done by replacing the probability distributions p(?t | ?k,?,xt) and p(wt | ?t,nt)
by their respective functional forms. Notice that in line 3 we compute the expectations under the variational distributions
and further bound B by introducing additional variational parameters ? using Jensen?s inequality on the log-sum-exp in
the last term. We denote the new bound B?.
below. Therefore, for each s < t, we choose to fix
?s and ?s once they are learned at timestep s.
M-step In the M-step, we update the global pa-
rameter ?, fixing ?1:t. Fixing other parameters and
taking the derivative of B? w.r.t. ?, we obtain:5
?B?
?? =
(?t ?
?t?1
k=t?cCk?k)(?
?t?1
k=t?c
?Ck
?? )
?
+
?t?1
k=t?cCk?k ?Ck??
? ,
where:
?Ck
?? =Ckf(xt,xk)
?Ck
?t?1
s=t?c f(xt,xs) exp(?>f(xt,xs))?t?1
s=t?c exp(?>f(xt,xs))
.
We follow the convex optimization strategy and sim-
ply perform a stochastic gradient update: ?t+1 =
?t + ?t ?B???t (Zinkevich, 2003). While the variational
bound B? is not convex, given the local variables ?1:t
5In our implementation, we augment ? with a squared L2
regularization term (i.e., we assume that ? is drawn from a
normal distribution with mean zero and variance ?) and use the
FOBOS algorithm (Duchi and Singer, 2009). The derivative
of the regularization term is simple and is not shown here. Of
course, other regularizers (e.g., the L1-norm, which we use for
other parameters, or the L1/?-norm) can also be explored.
and ?1:t, optimizing ? at timestep t without know-
ing the future becomes a convex problem.6 Since
we do not reestimate ?1:t?1 and ?1:t?1 in the E-step,
the choice to perform online gradient descent instead
of iteratively performing batch optimization at every
timestep is theoretically justified.
Notice that our overall learning procedure is still
to minimize the variational upper bound B?. All these
choices are made to make the model suitable for
learning in real time from large streaming datasets.
Preliminary experiments showed that performing
more than one EM iteration per day does not consid-
erably improve performance, so in our experiments
we perform one EM iteration per day.
To learn the parameters of the model, we rely on
approximations and optimize an upper bound B?. We
have opted for this approach over alternatives (such
as MCMC methods) because of our interest in the
online, large-data setting. Our experiments show that
we are still able to learn reasonable parameter esti-
mates by optimizing B?. Like online variational meth-
ods for other latent-variable models such as LDA
(Sato, 2001; Hoffman et al., 2013), open questions re-
main about the tightness of such approximations and
the identifiability of model parameters. We note, how-
6As a result, our algorithm is Hannan consistent w.r.t. the
best fixed ? (for B?) in hindsight; i.e., the average regret goes to
zero as T goes to?.
185
ever, that our model does not include latent mixtures
of topics and may be generally easier to estimate.
5 Prediction
As described in ?2.2, our model is evaluated by the
loss suffered at every timestep, where the loss is
defined as the negative log likelihood of the model
on text at timestep wt. Therefore, at each timestep t,
we need to predict (the distribution of)wt. In order
to do this, for each word v ? V , we simply compute
the deviation means ?t,v as weighted combinations
of previous means, where the weights are determined
by the world-context similarity encoded in x:
Eq[?t,v | ?t,v] =
t?1?
k=t?c
exp(?>f(xt,xk))?t?1
j=t?c exp(?>f(xt,xj))
?k,v.
Recall that the word distribution that we use for
prediction is obtained by applying the operator pi
that maps ?t and n to the V -dimensional simplex,
forming a distribution over words: pi(?t,n1:t?1)v =
exp(n1:t?1,v+?t,v)P
j?V exp(n1:t?1,j+?t,j)
, where n1:t?1,v ? RV is a
background distribution (the log-frequency of word
v observed up to time t? 1).
6 Experiments
In our experiments, we consider the problem of pre-
dicting economy-related text appearing in news and
microblogs, based on observable features that reflect
current economic conditions in the world at a given
time. In the following, we describe our dataset in de-
tail, then show experimental results on text prediction.
In all experiments, we set the window size c = 7 (one
week) or c = 14 (two weeks), ? = 12|V | (V is thesize of vocabulary of the dataset under consideration),
and ? = 1.
6.1 Dataset
Our data contains metadata and text corpora. The
metadata is used as our features, whereas the text
corpora are used for learning language models and
predictions. The dataset (excluding Twitter) can
be downloaded at http://www.ark.cs.cmu.
edu/DynamicLM.
6.1.1 Metadata
We use end-of-day stock prices gathered from
finance.yahoo.com for each stock included in
the Standard & Poor?s 500 index (S&P 500). The
index includes large (by market value) companies
listed on US stock exchanges.7 We calculate daily
(continuously compounded) returns for each stock, o:
ro,t = logPo,t? logPo,t?1, where Po,t is the closing
stock price.8 We make a simplifying assumption that
text for day t is generated after Po,t is observed.9
In general, stocks trade Monday to Friday (except
for federal holidays and natural disasters). For days
when stocks do not trade, we set ro,t = 0 for all
stocks since any price change is not observed.
We transform returns into similarity values as fol-
lows: f(xo,t, xo,k) = 1 iff sign(ro,t) = sign(ro,k)
and 0 otherwise. While this limits the model by ig-
noring the magnitude of price changes, it is still rea-
sonable to capture the similarity between two days.10
There are 500 stocks in the S&P 500, so xt ? R500
and f(xt,xk) ? R500.
6.1.2 Text data
We have five streams of text data. The first four
corpora are news streams tracked through Reuters.11
Two of them are written in English, North American
Business Report (EN:NA) and Japanese Investment
News (EN:JP). The remaining two are German Eco-
nomic News Service (DE, in German) and French
Economic News Service (FR, in French). For all four
of the Reuters streams, we collected news data over
a period of thirteen months (392 days), 2012-05-26
to 2013-06-21. See Table 1 for descriptive statistics
of these datasets. Numerical terms are mapped to a
single word, and all letters are downcased.
The last text stream comes from the Deca-
hose/Gardenhose stream from Twitter. We collected
public tweets that contain ticker symbols (i.e., sym-
bols that are used to denote stocks of a particular
company in a stock market), preceded by the dollar
7For a list of companies listed in the S&P 500 as of
2012, see http://en.wikipedia.org/wiki/List_
of_S\%26P_500_companies. This set was fixed during
the time periods of all our experiments.
8We use the ?adjusted close? on Yahoo that includes interim
dividend cash flows and also adjusts for ?splits? (changes in the
number of outstanding shares).
9This is done in order to avoid having to deal with hourly
timesteps. In addition, intraday price data is only available
through commercial data provided.
10Note that daily stock returns are equally likely to be positive
or negative and display little serial correlation.
11http://www.reuters.com
186
Dataset Total # Doc. Avg. # Doc. #Days Unigrams BigramsTotal # Tokens Size Vocab. Total # Tokens Size Vocab.
EN:NA 86,683 223 392 28,265,550 10,000 11,804,201 5,000
EN:JP 70.807 182 392 16,026,380 10,000 7,047,095 5,000
FR 62,355 160 392 11,942,271 10,000 3,773,517 5,000
DE 51,515 132 392 9,027,823 10,000 3,499,965 5,000
Twitter 214,794 336 639 1,660,874 10,000 551,768 5,000
Table 1: Statistics about the datasets. Average number of documents (third column) is per day.
sign $ (e.g., $GOOG, $MSFT, $AAPL, etc.). These
tags are generally used to indicate tweets about the
stock market. We look at tweets from the period
2011-01-01 to 2012-09-30 (639 days). As a result,
we have approximately 100?800 tweets per day. We
tokenized the tweets using the CMU ARK TweetNLP
tools,12 numerical terms are mapped to a single word,
and all letters are downcased.
We perform two experiments using unigram and
bigram language models as the base models. For
each dataset, we consider the top 10,000 unigrams
after removing corpus-specific stopwords (the top
100 words with highest frequencies). For the bigram
experiments, we only use 5,000 words to limit the
number of unique bigrams so that we can simulate
experiments for the entire time horizon in a reason-
able amount of time. In standard open-vocabulary
language modeling experiments, the treatment of un-
known words deserves care. We have opted for a
controlled, closed-vocabulary experiment, since stan-
dard smoothing techniques will almost surely interact
with temporal dynamics and context in interesting
ways that are out of scope in the present work.
6.2 Baselines
Since this is a forecasting task, at each timestep, we
only have access to data from previous timesteps.
Our model assumes that all words in all documents
in a corpus come from a single multinomial distri-
bution. Therefore, we compare our approach to the
corresponding base models (standard unigram and bi-
gram language models) over the same vocabulary (for
each stream). The first one maintains counts of every
word and updates the counts at each timestep. This
corresponds to a base model that uses all of the avail-
able data up to the current timestep (?base all?). The
second one replaces counts of every word with the
12https://www.ark.cs.cmu.edu/TweetNLP
counts from the previous timestep (?base one?). Ad-
ditionally, we also compare with a base model whose
counts decay exponentially (?base exp?). That is, the
counts from previous timesteps decay by exp(??s),
where s is the distance between previous timesteps
and the current timestep and ? is the decay constant.
We set the decay constant ? = 1. We put a symmetric
Dirichlet prior on the counts (?add-one? smoothing);
this is analogous to our treatment of the background
frequencies n in our model. Note that our model,
similar to ?base all,? uses all available data up to
timestep t? 1 when making predictions for timestep
t. The window size c only determines which previ-
ous timesteps? models can be chosen for making a
prediction today. The past models themselves are es-
timated from all available data up to their respective
timesteps.
We also compare with two strong baselines: a lin-
ear interpolation of ?base one? models for the past
week (?int. week?) and a linear interpolation of ?base
all? and ?base one? (?int one all?). The interpolation
weights are learned online using the normalized expo-
nentiated gradient algorithm (Kivinen and Warmuth,
1997), which has been shown to enjoy a stronger
regret guarantee compared to standard online gra-
dient descent for learning a convex combination of
weights.
6.3 Results
We evaluate the perplexity on unseen dataset to eval-
uate the performance of our model. Specifically, we
use per-word predictive perplexity:
perplexity = exp
(
?
?T
t=1 log p(wt | ?,x1:t,n1:t?1)?T
t=1
?
j?V wt,j
)
.
Note that the denominator is the number of tokens
up to timestep T . Lower perplexity is better.
Table 2 and Table 3 show the perplexity results for
187
Dataset base all base one base exp int. week int. one all c = 7 c = 14
EN:NA 3,341 3,677 3,486 3,403 3,271 3,262 3,285
EN:JP 2,802 3,212 2,750 2,949 2,708 2,656 2,689
FR 3,603 3,910 3,678 3,625 3,416 3,404 3,438
DE 3,789 4,199 3,979 3,926 3,634 3,649 3,687
Twitter 3,880 6,168 5,133 5,859 4,047 3,801 3,819
Table 2: Perplexity results for our five data streams in the unigram experiments. The base models in ?base all,? ?base
one,? and ?base exp? are unigram language models. ?int. week? is a linear interpolation of ?base one? from the past
week. ?int. one all? is a linear interpolation of ?base one? and ?base all?. The rightmost two columns are versions of
our model. Best results are highlighted in bold.
Dataset base all base one base exp int. week int. one all c = 7
EN:NA 242 2,229 1,880 2,200 244 223
EN:JP 185 2,101 1,726 2,050 189 167
FR 159 2,084 1,707 2,068 166 139
DE 268 2,634 2,267 2,644 282 243
Twitter 756 4,245 4,253 5,859 4,046 739
Table 3: Perplexity results for our five data streams in the bigram experiments. The base models in ?base all,? ?base
one,? and ?base exp? are bigram language models. ?int. week? is a linear interpolation of ?base one? from the past
week. ?int. one all? is a linear interpolation of ?base one? and ?base all?. The rightmost column is a version of our
model with c = 7. Best results are highlighted in bold.
each of the datasets for unigram and bigram experi-
ments respectively. Our model outperformed other
competing models in all cases but one. Recall that we
only define the similarity function of world context
as: f(xo,t, xo,k) = 1 iff sign(ro,t) = sign(ro,k) and
0 otherwise. A better similarity function (e.g., one
that takes into account market size of the company
and the magnitude of increase or decrease in the stock
price) might be able to improve the performance fur-
ther. We leave this for future work. Furthermore,
the variations can be captured using models from the
past week. We discuss why increasing c from 7 to 14
did not improve performance of the model in more
detail in ?6.4.
We can also see how the models performed over
time. Figure 4 traces perplexity for four Reuters news
stream datasets.13 We can see that in some cases the
performance of the ?base all? model degraded over
time, whereas our model is more robust to temporal
13In both experiments, in order to manage the time and space
complexities of updating ?, we apply a sparsity shrinkage tech-
nique by using OWL-QN (Andrew and Gao, 2007) when maxi-
mizing it, with regularization constant set to 1. Intuitively, this
is equivalent to encouraging the deviation vector to be sparse
(Eisenstein et al., 2011).
shifts.
In the bigram experiments, we only ran our model
with c = 7, since we need to maintain ? in RV 2 ,
instead of RV in the unigram model. The goal of
this experiment is to determine whether our method
still adds benefit to more expressive language mod-
els. Note that the weights of the linear interpolation
models are also learned in an online fashion since
there are no classical training, development, and test
sets in our setting. Since the ?base one? model per-
formed poorly in this experiment, the performance of
the interpolated models also suffered. For example,
the ?int. one all? model needed time to learn that the
?base one? model has to be downweighted (we started
with all interpolated models having uniform weights),
so it was not able to outperform even the ?base all?
model.
6.4 Analysis and Discussion
It should not be surprising that conditioning on
world-context reduces perplexity (Cover and Thomas,
1991). A key attraction of our model, we believe, lies
in the ability to inspect its parameters.
Deviation coefficients. Inspecting the model al-
lows us to gain insight into temporal trends. We
188
Twitter:Google
timestep
?
0 100 200 300 400 500 600
0.0
0.5
1.0
1.5
2.0
goog
@google
google+
#goog
r
GOOG
Twitter:Microsoft
timestep
?
0 100 200 300 400 500 600
0.0
0.5
1.0
1.5
microsoft
msft
#microsoft
r
MSFT
Figure 3: Deviation coefficients ? over time for Google- and Microsoft-related words on Twitter with unigram base
model (c = 7). Significant changes (increases or decreases) in the returns of Google and Microsoft stocks are usually
followed by increases in ? of related words.
investigate the deviations learned by our model on the
Twitter dataset. Examples are shown in Figure 3. The
left plot shows ? for four words related to Google:
goog, #goog, @google, google+. For compari-
son, we also show the return of Google stock for the
corresponding timestep (scaled by 50 and centered at
0.5 for readability, smoothed using loess (Cleveland,
1979), denoted by rGOOG in the plot). We can see
that significant changes of return of Google stocks
(e.g., the rGOOG spikes between timesteps 50?100,
150?200, 490?550 in the plot) occurred alongside
an increase in ? of Google-related words. Similar
trends can also be observed for Microsoft-related
words in the right plot. The most significant loss of
return of Microsoft stocks (the downward spike near
timestep 500 in the plot) is followed by a sudden
sharp increase in ? of the words #microsoft and
microsoft.
Feature coefficients. We can also inspect the
learned feature coefficients ? to investigate which
stocks have higher associations with the text that
is generated. Our feature coefficients are designed
to reflect which changes (or lack of changes) in
stock prices influence the word distribution more,
not which stocks are talked about more often. We
find that the feature coefficients do not correlate with
obvious company characteristics like market capi-
talization (firm size). For example, on the Twitter
dataset with bigram base models, the five stocks with
the highest weights are: ConAgra Foods Inc., Intel
Corp., Bristol-Myers Squibb, Frontier Communica-
tions Corp., and Amazon.com Inc. Strongly negative
weights tended to align with streams with less activ-
time lags
frequ
ency
0
20
40
60
80
1 2 3 4 5 6 7 8 9 10 11 12 13 14
Figure 5: Distributions of the selection probabilities of
models from the previous c = 14 timesteps, on the EN:NA
dataset with unigram base model. For simplicity, we show
E-step modes. The histogram shows that the model tends
to favor models from days closer to the current date.
ity, suggesting that these were being used to smooth
across all c days of history. A higher weight for stock
o implies an increase in probability of choosing mod-
els from previous timesteps s, when the state of the
world for the current timestep t and timestep s is the
same (as represented by our similarity function) with
respect to stock o (all other things being equal), and
a decrease in probability for a lower weight.
Selected models. Besides feature coefficients, our
model captures temporal shift by modeling similar-
ity across the most recent c days. During inference,
our model weights different word distributions from
the past. The similarity is encoded in the pairwise
features f(xt,xk) and the parameters ?. Figure 5
shows the distributions of the strongest-posterior
models from previous timesteps, based on how far
189
EN:NA
timestep
perpl
exity
0 50 100 150 200 250 300 350
200
400
600
base all
complete
int. one all
EN:JP
timestep
perpl
exity
0 50 100 150 200 250 300 350
200
400
600
base all
complete
int. one all
FR
timestep
perpl
exity
0 50 100 150 200 250 300 350
200
400
600
base all
complete
int. one all
DE
timestep
perpl
exity
0 50 100 150 200 250 300 350
300
500
700
base all
complete
int. one all
Figure 4: Perplexity over time for four Reuters news streams (c = 7) with bigram base models.
190
in the past they are at the time of use, aggregated
across rounds on the EN:NA dataset, for window size
c = 14. It shows that the model tends to favor models
from days closer to the current date, with the t ? 1
models selected the most, perhaps because the state
of the world today is more similar to dates closer to
today compare to more distant dates. The plot also
explains why increasing c from 7 to 14 did not im-
prove performance of the model, since most of the
variation in our datasets can be captured with models
from the past week.
Topics. Latent topic variables have often figured
heavily in approaches to dynamic language model-
ing. In preliminary experiments incorporating single-
membership topic variables (i.e., each document be-
longs to a single topic, as in a mixture of unigrams),
we saw no benefit to perplexity. Incorporating top-
ics also increases computational cost, since we must
maintain and estimate one language model per topic,
per timestep. It is straightforward to design mod-
els that incorporate topics with single- or mixed-
membership as in LDA (Blei et al., 2003), an in-
teresting future direction.
Potential applications. Dynamic language models
like ours can be potentially useful in many applica-
tions, either as a standalone language model, e.g.,
predictive text input, whose performance may de-
pend on the temporal dimension; or as a component
in applications like machine translation or speech
recognition. Additionally, the model can be seen as
a step towards enhancing text understanding with
numerical, contextual data.
7 Conclusion
We presented a dynamic language model for stream-
ing datasets that allows conditioning on observable
real-world context variables, exemplified in our ex-
periments by stock market data. We showed how to
perform learning and inference in an online fashion
for this model. Our experiments showed the predic-
tive benefit of such conditioning and online learning
by comparing to similar models that ignore temporal
dimensions and observable variables that influence
the text.
Acknowledgements
The authors thank several anonymous reviewers for help-
ful feedback on earlier drafts of this paper and Brendan
O?Connor for help with collecting Twitter data. This re-
search was supported in part by Google, by computing
resources at the Pittsburgh Supercomputing Center, by
National Science Foundation grant IIS-1111142, AFOSR
grant FA95501010247, ONR grant N000140910758, and
by the Intelligence Advanced Research Projects Activ-
ity via Department of Interior National Business Center
contract number D12PC00347. The U.S. Government is
authorized to reproduce and distribute reprints for Govern-
mental purposes notwithstanding any copyright annotation
thereon. The views and conclusions contained herein are
those of the authors and should not be interpreted as nec-
essarily representing the official policies or endorsements,
either expressed or implied, of IARPA, DoI/NBC, or the
U.S. Government.
References
Galen Andrew and Jianfeng Gao. 2007. Scalable training
of l1-regularized log-linear models. In Proc. of ICML.
David M. Blei and John D. Lafferty. 2006. Dynamic topic
models. In Proc. of ICML.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet allocation. Journal of Machine
Learning Research, 3:993?1022.
Se?bastien Bubeck. 2011. Introduction to online opti-
mization. Technical report, Department of Operations
Research and Financial Engineering, Princeton Univer-
sity.
Nicolo` Cesa-Bianchi and Ga?bor Lugosi. 2006. Prediction,
Learning, and Games. Cambridge University Press.
William S. Cleveland. 1979. Robust locally weighted
regression and smoothing scatterplots. Journal of the
American Statistical Association, 74(368):829?836.
Thomas M. Cover and Joy A. Thomas. 1991. Elements of
Information Theory. John Wiley & Sons.
John Duchi and Yoram Singer. 2009. Efficient online
and batch learning using forward backward splitting.
Journal of Machine Learning Research, 10(7):2899?
2934.
Jacob Eisenstein, Brendan O?Connor, Noah A. Smith,
and Eric P. Xing. 2010. A latent variable model for
geographic lexical variation. In Proc. of EMNLP.
Jacob Eisenstein, Amr Ahmed, and Eric P. Xing. 2011.
Sparse additive generative models of text. In Proc. of
ICML.
Amit Goyal, Hal Daume III, and Suresh Venkatasubrama-
nian. 2009. Streaming for large scale NLP: Language
modeling. In Proc. of HLT-NAACL.
191
Matt Hoffman, David M. Blei, Chong Wang, and John
Paisley. 2013. Stochastic variational inference. Jour-
nal of Machine Learning Research, 14:1303?1347.
Antti Honkela and Harri Valpola. 2003. On-line varia-
tional Bayesian learning. In Proc. of ICA.
Tomoharu Iwata, Takeshi Yamada, Yasushi Sakurai, and
Naonori Ueda. 2010. Online multiscale dynamic topic
models. In Proc. of KDD.
Frederick Jelinek. 1997. Statistical Methods for Speech
Recognition. MIT Press.
Jyrki Kivinen and Manfred K. Warmuth. 1997. Expo-
nentiated gradient versus gradient descent for linear
predictors. Information and Computation, 132:1?63.
Victor Lavrenko, Matt Schmill, Dawn Lawrie, Paul
Ogilvie, David Jensen, and James Allan. 2000. Mining
of concurrent text and time series. In Proc. of KDD
Workshop on Text Mining.
Abby Levenberg and Miles Osborne. 2009. Stream-based
randomised language models for SMT. In Proc. of
EMNLP.
Abby Levenberg, Chris Callison-Burch, and Miles Os-
borne. 2010. Stream-based translation models for sta-
tistical machine translation. In Proc. of HLT-NAACL.
Dong C. Liu and Jorge Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical Programming B, 45(3):503?528.
David Mimno and Andrew McCallum. 2008. Topic mod-
els conditioned on arbitrary features with Dirichlet-
multinomial regression. In Proc. of UAI.
Alexander Rakhlin. 2009. Lecture notes on online learn-
ing. Technical report, Department of Statistics, The
Wharton School, University of Pennsylvania.
Masaaki Sato. 2001. Online model selection based on the
variational bayes. Neural Computation, 13(7):1649?
1681.
Shai Shalev-Shwartz. 2012. Online learning and online
convex optimization. Foundations and Trends in Ma-
chine Learning, 4(2):107?194.
Martin J. Wainwright and Michael I. Jordan. 2008. Graph-
ical models, exponential families, and variational infer-
ence. Foundations and Trends in Machine Learning,
1(1?2):1?305.
Chong Wang, David M. Blei, and David Heckerman.
2008. Continuous time dynamic topic models. In Proc.
of UAI.
Chong Wang, John Paisley, and David M. Blei. 2011. On-
line variational inference for the hierarchical Dirichlet
process. In Proc. of AISTATS.
Martin Zinkevich. 2003. Online convex programming
and generalized infinitesimal gradient ascent. In Proc.
of ICML.
192
Proceedings of the NAACL HLT 2010 Workshop on Computational Linguistics in a World of Social Media, pages 19?20,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Social Links from Latent Topics in Microblogs?
Kriti Puniyani and Jacob Eisenstein and Shay Cohen and Eric P. Xing
School of Computer Science
Carnegie Mellon University
{kpuniyan,jacobeis,scohen,epxing}@cs.cmu.edu
1 Introduction
Language use is overlaid on a network of social con-
nections, which exerts an influence on both the topics
of discussion and the ways that these topics can be ex-
pressed (Halliday, 1978). In the past, efforts to under-
stand this relationship were stymied by a lack of data, but
social media offers exciting new opportunities. By com-
bining large linguistic corpora with explicit representa-
tions of social network structures, social media provides
a new window into the interaction between language and
society. Our long term goal is to develop joint sociolin-
guistic models that explain the social basis of linguistic
variation.
In this paper we focus on microblogs: internet jour-
nals in which each entry is constrained to a few words
in length. While this platform receives high-profile at-
tention when used in connection with major news events
such as natural disasters or political turmoil, less is
known about the themes that characterize microblogging
on a day-to-day basis. We perform an exploratory anal-
ysis of the content of a well-known microblogging plat-
form (Twitter), using topic models to uncover latent se-
mantic themes (Blei et al, 2003). We then show that these
latent topics are predictive of the network structure; with-
out any supervision, they predict which other microblogs
a user is likely to follow, and to whom microbloggers will
address messages. Indeed, our topical link predictor out-
performs a competitive supervised alternative from tra-
ditional social network analysis. Finally, we explore the
application of supervision to our topical link predictor,
using regression to learn weights that emphasize topics
of particular relevance to the social network structure.
2 Data
We acquired data from Twitter?s streaming ?Gardenhose?
API, which returned roughly 15% of all messages sent
over a period of two weeks in January 2010. This com-
?We thank the reviews for their helpful suggestions and Brendan
O?Connor for making the Twitter data available.
prised 15GB of compressed data; we aimed to extract a
representative subset by first sampling 500 people who
posted at least sixteen messages over this period, and
then ?crawled? at most 500 randomly-selected followers
of each of these original authors. The resulting data in-
cludes 21,306 users, 837,879 messages, and 10,578,934
word tokens.
Text Twitter contains highly non-standard orthography
that poses challenges for early-stage text processing.1 We
took a conservative approach to tokenization, splitting
only on whitespaces and apostrophes, and eliminating
only token-initial and token-final punctuation characters.
Two markers are used to indicate special tokens: #, indi-
cating a topic (e.g. #curling); and @, indicating that the
message is addressed to another user. Topic tokens were
included after stripping the leading #, but address tokens
were removed. All terms occurring less than 50 times
were removed, yielding a vocabulary of 11,425 terms.
Out-of-vocabulary items were classified as either words,
URLs, or numbers. To ensure a fair evaluation, we re-
moved ?retweets? ? when a user reposts verbatim the
message of another user ? if the original message author
is also part of the dataset.
Links We experiment with two social graphs extracted
from the data: a follower graph and a communication
graph. The follower graph places directed edges between
users who have chosen to follow each other?s updates;
the message graph places a directed edge between users
who have addressed messages to each other (using the @
symbol). Huberman et al (2009) argue that the commu-
nication graph captures direct interactions and is thus a
more accurate representation of the true underlying social
structure, while the follower graph contains more con-
nections than could possibly be maintained in a realistic
social network.
1For example, some tweets use punctuation for tokenization (You
look like a retired pornstar!lmao) while others
use punctuation inside the token (lOv!n d!s th!ng call3d
l!f3).
19
Figure 1: Mean rank of test links (lower is better), reported over 4-fold cross-validation. Common-neighbors is a network-based
method that ignores text; the LDA (Latent Dirichlet Allocation) methods are grouped by number of latent topics.
3 Method
We constructed a topic model over twitter messages,
identifying the latent themes that characterize the cor-
pus. In standard topic modeling methodology, topics de-
fine distributions over vocabulary items, and each docu-
ment contains a set of latent topic proportions (Blei et al,
2003). However, the average message on Twitter is only
sixteen word tokens, which is too sparse for traditional
topic modeling; instead, we gathered together all of the
messages from a given user into a single document. Thus
our model learns the latent topics that characterize au-
thors, rather than messages.
Authors with similar topic proportions are likely to
share interests or dialect, suggesting potential social con-
nections. Author similarity can be quantified without
supervision by taking the dot product of the topic pro-
portions. If labeled data is available (a partially ob-
served network), then regression can be applied to learn
weights for each topic. Chang and Blei (2009) describe
such a regression-based predictor, which takes the form
exp
(
??T (z?i ? z?j) ? (z?i ? z?j)? ?
)
, denoting the pre-
dicted strength of connection between authors i and j.
Here z?i (z?j) refers to the expected topic proportions for
user i (j), ? is a vector of learned regression weights, and
? is an intercept term which is only necessary if a the link
prediction function must return a probability. We used
the updates from Chang and Blei to learn ? in a post hoc
fashion, after training the topic model.
4 Results
We constructed topic models using an implemen-
tation of variational inference2 for Latent Dirich-
let Allocation (LDA). The results of the run with
the best variational bound on 50 topics can be
found at http://sailing.cs.cmu.edu/
socialmedia/naacl10ws/. While many of
the topics focus on content (for example, electronics
and sports), others capture distinct languages and even
dialect variation. Such dialects are particularly evident in
2http://www.cs.princeton.edu/?blei/lda-c
stopwords (you versus u). Structured topic models that
explicitly handle these two orthogonal axes of linguistic
variation are an intriguing possibility for future work.
We evaluate our topic-based approach for link predic-
tion on both the message and follower graphs, compar-
ing against an approach that only considers the network
structure. Liben-Nowell and Kleinberg (2003) perform
a quantitative comparison of such approaches, finding
that the relatively simple technique of counting the num-
ber of shared neighbors between two nodes is a surpris-
ingly competitive predictor of whether they are linked;
we call this approach common-neighbors. We evaluate
this method and our own supervised LDA+regression ap-
proach by hiding half of the edges in the graph, and pre-
dicting them from the other half.
For each author in the dataset, we apply each method
to rank all possible links; the evaluation computes the av-
erage rank of the true links that were held out (for our
data, a random baseline would score 10653 ? half the
number of authors in the network). As shown in Figure
1, topic-based link prediction outperforms the alternative
that considers only the graph structure. Interestingly, post
hoc regression on the topic proportions did not consis-
tently improve performance, though joint learning may
do better (e.g., Chang and Blei, 2009). The text-based ap-
proach is especially strong on the message graph, while
the link-based approach is more competitive on the fol-
lowers graph; a model that captures both features seems
a useful direction for future work.
References
D. Blei, A. Ng, and M. Jordan. 2003. Latent Dirichlet aloca-
tion. Journal of Machine Learning Research, 3:993?1022.
J. Chang and D. Blei. 2009. Hierarchical relational models for
document networks. Annals of Applied Statistics.
M.A.K. Halliday. 1978. Language as social semiotic: The
social interpretation of language and meaning. University
Park Press.
Bernardo Huberman, Daniel M. Romero, and Fang Wu. 2009.
Social networks that matter: Twitter under the microscope.
First Monday, 14(1?5), January.
D. Liben-Nowell and J. Kleinberg. 2003. The link prediction
problem for social networks. In Proc. of CIKM.
20
Proceedings of EMNLP 2011, Conference on Empirical Methods in Natural Language Processing, pages 2?12,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Structured Databases of Named Entities from Bayesian Nonparametrics
Jacob Eisenstein Tae Yano William W. Cohen Noah A. Smith Eric P. Xing
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{jacobeis,taey,wcohen,nasmith,epxing}@cs.cmu.edu
Abstract
We present a nonparametric Bayesian ap-
proach to extract a structured database of enti-
ties from text. Neither the number of entities
nor the fields that characterize each entity are
provided in advance; the only supervision is
a set of five prototype examples. Our method
jointly accomplishes three tasks: (i) identify-
ing a set of canonical entities, (ii) inferring a
schema for the fields that describe each entity,
and (iii) matching entities to their references in
raw text. Empirical evaluation shows that the
approach learns an accurate database of enti-
ties and a sensible model of name structure.
1 Introduction
Consider the task of building a set of structured
records from a collection of text: for example, ex-
tracting the names of people or businesses from
blog posts, where each full name decomposes into
fields corresponding to first-name, last-name, title,
etc. To instruct a person to perform this task, one
might begin with a few examples of the records to
be obtained; assuming that the mapping from text to
records is relatively straightforward, no additional
instruction would be necessary. In this paper, we
present a method for training information extraction
software in the same way: starting from a small table
of partially-complete ?prototype? records (Table 1),
our system learns to add new entries and fields to
the table, while simultaneously aligning the records
to text.
We assume that the dimensionality of the database
is unknown, so that neither the number of entries
John McCain Sen. Mr.
George Bush W. Mr.
Hillary Clinton Rodham Mrs.
Barack Obama Sen.
Sarah Palin
Table 1: A set of partially-complete prototype records,
which constitutes the only supervision for the system.
nor the number of fields is specified in advance. To
accommodate this uncertainty, we apply a Bayesian
model which is nonparametric along three dimen-
sions: the assignment of text mentions to entities
(making popular entries more likely while always al-
lowing new entries); the alignment of individual text
tokens to fields (encouraging the re-use of common
fields, but permitting the creation of new fields); and
the assignment of values to entries in the database
itself (encouraging the reuse of values across entries
in a given field). By adaptively updating the con-
centration parameter of stick-breaking distribution
controlling the assignment of values to entries in the
database, our model can learn domain-specific infor-
mation about each field: for example, that titles are
often repeated, while names are more varied.
Our system?s input consists of a very small proto-
type table and a corpus of text which has been au-
tomatically segmented to identify names. Our de-
sired output is a set of structured records in which
each field contains a single string ? not a distribu-
tion over strings, which would be more difficult to
interpret. This requirement induces a tight proba-
bilistic coupling between the assignment of text to
cells in the table, so special care is required to ob-
2
tain efficient inference. Our procedure alternates
between two phases. In the first phase, we per-
form collapsed Gibbs sampling on the assignments
of string mentions to rows and columns in the table,
while marginalizing the values of the table itself. In
the second phase, we apply Metropolis-Hastings to
swap the values of columns in the table, while simul-
taneously relabeling the affected strings in the text.
Our model performs three tasks: it constructs a
set of entities from raw text, matches mentions in
text with the entities to which they refer, and discov-
ers general categories of tokens that appear in names
(such as titles and first names). We are aware of
no existing system that performs all three of these
tasks jointly. We evaluate on a dataset of political
blogs, measuring our system?s ability to discover
a set of reference entities (recall) while maintain-
ing a compact number of rows and columns (pre-
cision). With as few as five partially-complete pro-
totype examples, our approach gives accurate tables
that match well against a manually-annotated refer-
ence list. Our method outperforms a baseline single-
link clustering approach inspired by one of the most
successful entries (Elmacioglu et al, 2007) in the
SEMEVAL ?Web People Search? shared task (Ar-
tiles et al, 2007).
2 Task Definition
In this work, we assume that a bag of M mentions
in text have been identified. The mth mention wm
is a sequence of contiguous word tokens (its length
is denoted Nm) understood to refer to a real-world
entity. The entities (and the mapping of mentions to
entities) are not known in advance. While our focus
in this paper is names of people, the task is defined
in a more generic way.
Formally, the task is to construct a table x where
rows correspond to entities and columns to func-
tional fields. The number of entities and the num-
ber of fields are not prespecified. x?,j denotes the
jth column of x, and xi,j is a single word type fill-
ing the cell in row i, column j. An example is Ta-
ble 1, where the fields are first-name, last-name, ti-
tle, middle-name, and so on. In addition to the table,
we require that each mention be mapped to an en-
tity (i.e., a row in the table). Success at this task
therefore requires (i) identifying entities, (ii) discov-
ering the internal structure of mentions (effectively
canonicalizing them), and (iii) mapping mentions
to entities (therefore resolving coreference relation-
ships among mentions). Note that this task differs
from previous work on knowledge base population
(e.g., McNamee, 2009) because the schema is not
formally defined in advance; rather, the number of
fields and their meaning must be induced from just
a few prototype examples.
To incorporate partial supervision, a subset of the
table x is specified manually by an annotator. We
denote this subset of ?prototypes? by x?; for entries
that are unspecified by the user, we write x?i,j = ?.
Prototypes are not assumed to provide complete in-
formation for any entity.
3 Model
We now craft a nonparametric generative story that
explains both the latent table and the observed men-
tions. The model incorporates three nonparamet-
ric components, allowing an unbounded number of
rows (entities) and columns (fields), as well as an un-
bounded number of values per column (field values).
A plate diagram for the graphical model is shown in
Figure 1.
A key point is that the column distributions ?
range over possible values at the entity level, not
over mentions in text. For example, ?2 might be
the distribution over possible last names and ?3 the
distribution over elected office titles. Note that ?2
would contain a low value for the last name Obama
? which indicates that few people have this last
name ? even though a very high proportion of men-
tions in our data include the string Obama.
The user-generated entries (x?) can still be treated
as the outcome of the generative process: using ex-
changeability, we treat these entries as the first sam-
ples drawn in each column. In this work, we treat
them as fully observed, but it is possible to treat
them as noisy and incorporate a stochastic depen-
dency between xi,j and x?i,j .
4 Inference
We now develop sampling-based inference for the
model described in the previous section. We be-
gin with a token-based collapsed Gibbs sampler, and
then add larger-scale Metropolis-Hastings moves.
3
? ?2
x ? ?
w r ?r
c ?r
?c ?c
Figure 1: A plate diagram for the
text-and-tables graphical model.
The upper plate is the table x, and
the lower plate is the set of textual
mentions. Notation is defined in the
generative model to the right.
? Generate the table entries. For each column j,
? Draw a concentration parameter ?j from a log-normal distribution,
log?j ? N (?, ?2).
? Draw a distribution over strings from a Dirichlet process ?j ?
DP(?j , G0), where the base distribution G0 is a uniform distribution
over strings in a fixed character alphabet, up to an arbitrary finite length.
? For each row i, draw the entry xi,j ? ?j .
? Generate the text mentions.
? Draw a prior distribution over rows from a stick-breaking distribution,
?r ? Stick(?r).
? Draw a prior distribution over columns from a stick-breaking distribu-
tion, ?c ? Stick(?c).
? For each mention wm,
? Draw a row in the table rm ? ?r.
? For each word token wm,n (n ? {1, . . . , Nm}),
? Draw a column in the table cm,n ? ?c.
? Set the text wm,n = xrm,cm,n .
4.1 Gibbs sampling
A key aspect of the generative process is that the
word token wm,n is completely determined by the
table x and the row and column indicators rm and
cm,n: given that a token was generated by row i
and column j of the table, it must be identical to
the value of xi,j . Using Bayes? rule, we can reverse
this deterministic dependence: given the values for
the row and column indices, the entries in the table
are restricted to exact matches with the text men-
tions that they generate. This allows us to marginal-
ize the unobserved entries in the table. We can also
marginalize the distributions ?r, ?c, and ?j , using
the standard collapsed Gibbs sampling equations for
Dirichlet processes. Thus, sampling the row and col-
umn indices is all that is required to explore the en-
tire space of model configurations.
4.1.1 Conditional probability for word tokens
The conditional sampling distributions for both
rows and columns will marginalize the table (be-
sides the prototypes x?). To do this, we must be
able to compute P (wm,n | rm = i, cm,n =
j, x?,w?(m,n), r?m, c?(m,n), ?j), which represents
the probability of generating word wm,n, given
rm = i and cm,n = j. The notation w?(m,n), r?m,
and c?m,n represent the words, row indices, and col-
umn indices for all mentions besides wm,n. For sim-
plicity, we will elide these variables in much of the
subsequent notation.
We first consider the case where we have a user-
specified entry for the row and column ?i, j?? that
is, if x?ij 6= ?. Then the probability is simply,
P (wm,n | rm = i, cm,n = j, x?, . . .) =
{
1, if x?ij = wm,n
0, if x?ij 6= wm,n.
(1)
Because the table cell xij is observed, we do not
marginalize over it; we have a generative probability
of one if the word matches, and zero otherwise. If
the table cell xij is not specified by the user, then we
marginalize over its possible values. For any given
xij , the probability P (wm,n | xij , rm = i, cm,n =
j) is still a delta function, so we have:
?
P (wm,n | xrm,cm,n)P (xrm,cm,n | . . .) dxrm,cm,n
= P (x = wm,n | w?(m,n), r?m, c?(m,n), x?, . . .)
The integral is equal to the probability of the value
of the cell xrm,cm,n being identical to the string
wm,n, given assignments to all other variables. To
compute this probability, we again must consider
two cases: if the cell xi,j has generated some other
string wm?,n? then its value must be identical to that
4
string; otherwise it is unknown. More formally, for
any cell ?i, j?, if ?wm?,n? : rm? = i ? cm?,n? =
j ? ?m?, n?? 6= ?m,n?, then P (xi,j = wm?,n?) = 1;
all other strings have zero probability. If xi,j has not
generated any other entry, then its probability is con-
ditioned on the other elements of the table x. The
known elements of this table are themselves deter-
mined by either the user entries x? or the observa-
tionsw?(m,n). We can define these known elements
as x?, where x?ij = ? if x?ij = ? ? @?m,n? : rm =
i ? cm,n = j. Then we can apply the standard Chi-
nese restaurant process marginalization to obtain:
P (xij | x??(i,j), ?) =
{ N(x??(i,j)=xij)
N(x??(i,j) 6=?)+?
, N(x??(i,j) = xij) > 0
?
N(x??(i,j) 6=?)+?
, N(x??(i,j) = xij) = 0
(2)
In our implementation, we maintain the table x?,
updating it as we resample the row and column as-
signments. To construct the conditional distribution
for any given entry, we first consult this table, and
then compute the probability in Equation 2 for en-
tries where x?ij = ?.
4.1.2 Sampling columns
We can now derive sampling equations for the
column indices cm,n. We first apply Bayes? rule
to obtain P (cm,n | wm,n, rm, . . .) ? P (cm,n |
c?(m,n), ?c)?P (wm,n | cm,n, rm, x?, . . .). The like-
lihood term P (wm,n | cm,n, . . .) is defined in the
previous section; we can compute the first factor us-
ing the standard Dirichlet process marginalization
over ?c. Writing N(c?(m,n) = j) for the count of
occurrences of column j in the set c?(m,n), we ob-
tain
P (cm,n = j | c?(m,n), ?c) =
{ N(c?(m,n)=j)
N(c?(m,n))+?c
, if N(c?(m,n) = j) > 0
?c
N(c?(m,n))+?c
, if N(c?(m,n) = j) = 0
(3)
4.1.3 Sampling rows
In principle the row indicators can be sampled
identically to the columns, with the caveat that the
generative probability P (wm | rm, . . .) is a product
across all Nm tokens in wm.1 However, because of
1This relies on the assumption that the values of {cm,n} are
mutually independent given c?m. Future work might apply
the tight probabilistic coupling between the row and
column indicators, straightforward Gibbs sampling
mixes slowly. Instead, we marginalize the column
indicators while sampling r. Only the likelihood
term is affected by this change:
P (wm | rm,w?m, r?m, . . .)
=
?
j
P (c = j | c?m, ?c)P (wm,n | cm,n = j, rm, x?, ?).
(4)
The tokens are conditionally independent given the
row, so we factor and then explicitly marginalize
over each cm,n. The chain rule gives the form in
Equation 4, which contains terms for the prior over
columns and the likelihood of the word; these are
defined in Equations 2 and 3. Note that neither the
inferred table x? nor the heldout column counts c?m
include counts from any of the cells in row m.
4.2 Column swaps
Suppose that during initialization, we encounter the
string Barry Obama before encountering Barack
Obama. We would then put Barry in the first-name
column, and put Barack in some other column for
nicknames. After making these initial decisions,
they would be very difficult to undo using Gibbs
sampling ? we would have to first shift all instances
of Barry to another column, then move an instance
of Barack to the first-name column, and then move
the instances of Barry to the nickname column. To
rectify this issue, we perform sampling on the table
itself, swapping the columns of entries in the table,
while simultaneously updating the relevant column
indices of the mentions.
In the proposal, we select at random a row t and
indices i and j. In the table, we will swap xt,i with
xt,j ; in the text we will swap the values of each cm,n
whenever rm = t and cm,n = i or j. This pro-
posal is symmetric, so no Hastings correction is re-
quired. Because we are simultaneously updating the
table and the column indices, the generative likeli-
hood of the words is unchanged; the only changes
a more structured model of the ways that fields are combined
when mentioning an entity. For example, a first-order Markov
model could learn that family names often follow given names,
but the reverse rarely occurs (in English).
5
in the overall likelihood come from the column in-
dices and the values of the cells in the table. Letting
x?, c? indicate the state of the table and column in-
dices after the proposed move, we will accept with
probability,
Paccept(x? x?) = min
(
1,
P (c?)P (x?)
P (c)P (x)
)
(5)
We first consider the ratio of the table probabili-
ties, P (x
?|?)
P (x|?) . Recall that each column of x is drawn
from a Dirichlet process; appealing to exchangeabil-
ity, we can treat the row t as the last element drawn,
and compute the probabilities P (xt,i | x?(t,i), ?i),
with x?(t,i) indicating the elements of the column i
excluding row t. This probability is given by Equa-
tion 2. For a swap of columns i and j, we compute
the ratio:
P (xt,i | x?(t,j), ?j)P (xt,j | x?(t,i), ?i)
P (xt,i | x?(t,i), ?i)P (xt,j | x?(t,j), ?j)
(6)
Next we consider the ratio of the column proba-
bilities, P (c
?)
P (c) . Again we can apply exchangeabil-
ity, P (c) = P ({cm : rm = t} | {cm? : rm? 6=
t})P ({cm? : rm? 6= t}). The second term P ({cm? :
rm? 6= t}) is unaffected by the move, and so is iden-
tical in both the numerator and denominator of the
likelihood ratio; probabilities from columns other
than i and j also cancel in this way. The remaining
ratio can be simplified to,
(
P (c = j | c?t, ?c)
P (c = i | c?t, ?c)
)N(r=t?c=i)?N(r=t?c=j)
(7)
where the counts N() are from the state of the sam-
pler before executing the proposed move. The prob-
ability P (c = i | c?t, ?c) is defined in Equation 3,
and the overall acceptance ratio for column swaps is
the product of (6) and (7).
4.3 Hyperparameters
The concentration parameters ?r and ?c help to con-
trol the number of rows and columns in the ta-
ble, respectively. These parameters are updated to
their maximum likelihood values using gradient-
based optimization, so our overall inference pro-
cedure is a form of Monte Carlo Expectation-
Maximization (Wei and Tanner, 1990).
The concentration parameters ?j control the di-
versity of each column in the table: if ?j is low then
we expect a high degree of repetition, as with titles;
if ?j is high then we expect a high degree of diver-
sity. When the sampling procedure adds a new col-
umn, there is very little information for how to set
its concentration parameter, as the conditional like-
lihood will be flat. Consequently, greater care must
be taken to handle these priors appropriately.
We place a log-normal hyperprior on the col-
umn concentration parameters, log?j ? N (?, ?2).
The parameters of the log-normal are shared across
columns, which provides additional information to
constrain the concentration parameters of newly-
created columns. We then use Metropolis-Hastings
to sample the values of each ?j , using the joint like-
lihood,
P (?j , x?(j) | ?, ?2) ?
exp(?(log?j ? ?)2)?
kj
j ?(?j)
2?2?(nj + ?j)
,
where x?(j) is column j of the inferred table, nj is
the number of specified entries in column j of the
table x? and kj is the number of unique entries in
the column; see Rasmussen (2000) for a derivation.
After repeatedly sampling several values of ?j for
each column in the table, we update ? and ?2 to their
maximum-likelihood estimates.
5 Temporal Prominence
Andy Warhol predicted, ?in the future, everyone will
be world-famous for fifteen minutes.? A model of
temporal dynamics that accounts for the fleeting and
fickle nature of fame might yield better performance
for transient entities, like Joe the Plumber. Among
several alternatives for modeling temporal dynamics
in latent variable models, we choose a simple non-
parametric approach: the recurrent Chinese restau-
rant process (RCRP; Ahmed and Xing, 2008). The
core idea of the RCRP is that time is partitioned into
epochs, with a unique Chinese restaurant process in
each epoch. Each CRP has a prior which takes the
form of pseudo-counts computed from the counts in
previous epochs. We employ the simplest version of
the RCRP, a first-order Markov model in which the
prior for epoch t is equal to the vector of counts for
epoch t? 1:
6
P (r(t)m = i|r
(t)
1...m?1, r
(t?1), ?r) ?
{
N(r(t)1...m?1 = i) + N(r
(t?1) = i), if > 0;
?r, otherwise.
(8)
The count of row i in epoch t ? 1 is written
N(r(t?1) = i); the count in epoch t for mentions
1 to m ? 1 is written N(r(t)1...m?1 = i). As before,
we can apply exchangeability to treat each mention
as the last in the epoch, so during inference we can
replace this with the count N(r(t)?m). Note that there
is zero probability of drawing an entity that has no
counts in epochs t or t ? 1 but exists in some other
epoch; the probability mass ?r is reserved for draw-
ing a new entity, and the chance of this matching
some existing entity from another epoch is vanish-
ingly small.
During Gibbs sampling, we also need to consider
the effect of r(t)m on the subsequent epoch t + 1.
While space does not permit a derivation, the result-
ing probability is proportional to
P (r(t+1)|r(t)?m, r
(t)
m = i, ?r) ?
?
???
???
1 if N(r(t+1) = i) = 0,
N(r(t+1)=i)
?r
if N(r(t)?m = i) = 0,
1 + N(r
(t+1)=i)
N(r(t)?m=i)
if N(r(t)?m = i) > 0.
(9)
This favors entities which are frequent in epoch
t+ 1 but infrequent in epoch t.
The move to a recurrent Chinese restaurant pro-
cess does not affect the sampling equations for the
columns c, nor the concentration parameters of the
table, ?. The only part of the inference procedure
that needs to be changed is the optimization of the
hyperparameter ?r; the log-likelihood is now the
sum across all epochs, and each epoch makes a con-
tribution to the gradient.
6 Evaluation Setup
Our model jointly performs three tasks: identifying
a set of entities, discovering the set of fields, and
matching mention strings with the entities and fields
to which they refer. We are aware of no prior work
that performs these tasks jointly, nor any dataset that
is annotated for all three tasks.2 Consequently, we
focus our quantitative evaluation on what we take to
be the most important subtask: identifying the enti-
ties which are mentioned in raw text. We annotate
a new dataset of blog text for this purpose, and de-
sign precision and recall metrics to reward systems
that recover as much of the reference set as possi-
ble, while avoiding spurious entities and fields. We
also perform a qualitative analysis, noting the areas
where our method outperforms string matching ap-
proaches, and where there is need for further im-
provement.
Data Evaluation was performed on a corpus
of blogs describing United States politics in
2008 (Eisenstein and Xing, 2010). We ran the Stan-
ford Named Entity Recognition system (Finkel et
al., 2005) to obtain a set of 25,000 candidate men-
tions which the system judged to be names of peo-
ple. We then pruned strings that appeared fewer than
four times and eliminated strings with more than
seven tokens (these were usually errors). The result-
ing dataset has 19,247 mentions comprising 45,466
word tokens, and 813 unique mention strings.
Gold standard We develop a reference set of 100
entities for evaluation. This set was created by sort-
ing the unique name strings in the training set by fre-
quency, and manually merging strings that reference
the same entity. We also manually discarded strings
from the reference set if they resulted from errors in
the preprocessing pipeline (tokenization and named
entity recognition). Each entity is represented by
the set of all word tokens that appear in its refer-
ences; there are a total of 231 tokens for the 100 en-
tities. Most entities only include first and last names,
though the most frequent entities have many more:
for example, the entity Barack Obama has known
names: {Barack, Obama, Sen., Mr.}.
Metrics We evaluate the recall and precision of
a system?s response set by matching against the
reference set. The first step is to create a bipar-
tite matching between response and reference enti-
ties.3 Using a cost function that quantifies the sim-
2Recent work exploiting Wikipedia disambiguation pages
for evaluating cross-document coreference suggests an appeal-
ing alternative for future work (Singh et al, 2011).
3Bipartite matchings are typical in information extraction
evaluation metrics (e.g., Doddington et al, 2004).
7
ilarity of response and reference entities, we opti-
mize the matching using the Kuhn-Munkres algo-
rithm (Kuhn, 1955). For recall, the cost function
counts the number of shared word tokens, divided
by the number of word tokens in the reference enti-
ties; the recall is one minus the average cost of the
best matching (with a cost of one for reference enti-
ties that are not matched, and no cost for unmatched
response entities). Precision is computed identically,
but we normalize by the number of word tokens in
the response entity. Precision assigns a penalty of
one to unmatched response entities and no penalty
for unmatched reference entities.
Note that this metric grossly underrates the preci-
sion of all systems: the reference set is limited to 100
entities, but it is clear that our text mentions many
other people. This is harsh but fair: all systems are
penalized equally for identifying entities that are not
present in the reference set, and the ideal system will
recover the fifty reference entities (thus maximizing
recall) while keeping the table as compact as possi-
ble (thus maximizing precision). However, the raw
precision values have little meaning outside the con-
text of a direct comparison under identical experi-
mental conditions.
Systems The initial seed set for our system con-
sists of a partial annotation of five entities (Table 1)
? larger seed sets did not improve performance. We
run the inference procedure described in the previ-
ous section for 20,000 iterations, and then obtain a
final database by taking the intersection of the in-
ferred tables x? obtained at every 100 iterations, start-
ing with iteration 15,000. To account for variance
across Markov chains, we perform three different
runs. We evaluate a non-temporal version of our
model (as described in Sections 3 and 4), and a tem-
poral version with 5 epochs. For the non-temporal
version, a non-parallel C implementation had a wall
clock sampling time of roughly 16 hours; the tem-
poral version required 24 hours.
We compare against a baseline that incrementally
clusters strings into entities using a string edit dis-
tance metric, based on the work of Elmacioglu et
al. (2007). Starting from a configuration in which
each unique string forms its own cluster, we incre-
mentally merge clusters using the single-link crite-
rion, based on the minimum Jaccard edit distance
0.2 0.3 0.4 0.5 0.6 0.70
0.1
0.2
0.3
recall
pre
cis
ion
 
 
baseline
atemporal model
temporal model
Figure 2: The precision and recall of our models, as com-
pared to the curve defined by the incremental clustering
baseline. Each point indicates a unique sampling run.
Bill Clinton Benazir Bhutto
Nancy Pelosi Speaker
John Kerry Sen. Roberts
Martin King Dr. Jr. Luther
Bill Nelson
Table 2: A subset of the entity database discovered by
our model, hand selected to show highlight interesting
success and failure cases.
between each pair of clusters. This yields a series of
outputs that move along the precision-recall curve,
with precision increasing as the clusters encompass
more strings. There is prior work on heuristics for
selecting a stopping point, but we compare our re-
sults against the entire precision-recall curve (Man-
ning et al, 2008).
7 Results
The results of our evaluation are shown in Figure 2.
All sampling runs from our models lie well beyond
the precision-recall curve defined by the baseline
system, demonstrating the ability to achieve reason-
able recall with a far more compact database. The
baseline system can achieve nearly perfect recall by
creating one entity per unique string, but as it merges
strings to improve precision, its recall suffers sig-
nificantly. As noted above, perfect precision is not
possible on this task, because the reference set cov-
ers only a subset of the entities that appear in the
data. However, the numbers do measure the ability
to recover the reference entities in the most compact
table possible, allowing a quantitative comparison of
our models and the baseline approach.
8
Table 2 shows a database identified by the atem-
poral version of our model. The most densely-
populated columns in the table correspond to well-
defined name parts: columns 1 and 2 are almost
exclusively populated with first and last names re-
spectively, and column 3 is mainly populated by ti-
tles. The remaining columns are more of a grab
bag. Column 4 correctly captures Jr. for Martin
Luther King; column 5 correctly captures Luther,
but mistakenly contains Roberts (thus merging the
John Kerry and John Roberts entities), and Bhutto
(thus helping to merge the Bill Clinton and Benazir
Bhutto entities).
The model successfully distinguishes some, but
not all, of the entities that share tokens. For example,
the model separates Bill Clinton from Bill Nelson;
it also separates John McCain from John Kerry
(whom it mistakenly merges with John Roberts).
The ability to distinguish individuals who share first
names is due in part to the model attributing a low
concentration parameter to first names, meaning that
some repetition in the first name column is expected.
The model correctly identifies several titles and al-
ternative names, including the rare title Speaker for
Nancy Pelosi; however, it misses others, such as the
Senator title for Bill Nelson. This may be due in
part to the sample merging procedure used to gener-
ate this table, which requires that a cell contain the
same value in at least 80% of the samples.
Many errors may be attributed to slow mixing.
After mistakenly merging Bhutto and Clinton at
an early stage, the Gibbs sampler ? which treats
each mention independently ? is unable to sep-
arate them. Given that several other mentions of
Bhutto are already in the row occupied by Clin-
ton, the overall likelihood would benefit little from
creating a new row for a single mention, though
moving all such mentions simultaneously would re-
sult in an improvement. Larger scale Metropolis-
Hastings moves, such as split-merge or type-based
sampling (Liang et al, 2010) may help.
8 Related Work
Information Extraction A tradition of research
in information extraction focuses on processing raw
text to fill in the fields of manually-defined tem-
plates, thus populating databases of events or re-
lations (McNamee and Dang, 2009). While early
approaches focused on surface-level methods such
as wrapper induction (Kushmerick et al, 1997),
more recent work in this area includes Bayesian
nonparametrics to select the number of rows in the
database (Haghighi and Klein, 2010a). However,
even in such nonparametric work, the form of the
template and the number of slots are fixed in ad-
vance. Our approach differs in that the number of
fields and their meaning is learned from data. Recent
work by Chambers and Jurafsky (2011) approaches
a related problem, applying agglomerative cluster-
ing over sentences to detect events, and then clus-
tering syntactic constituents to induce the relevant
fields of each event entity. As described in Section 6,
our method performs well against an agglomerative
clustering baseline, though a more comprehensive
comparison of the two approaches is an important
step for future work.
Name Segmentation and Structure A related
stream of research focuses specifically on names:
identifying them in raw text, discovering their struc-
ture, and matching names that refer to the same en-
tity. We do not undertake the problem of named en-
tity recognition (Tjong Kim Sang, 2002), but rather
apply an existing NER system as a preprocessing
step (Finkel et al, 2005). Typical NER systems
do not attempt to discover the internal structure of
names or a database of canonical names, although
they often use prefabricated ?gazetteers? of names
and name parts as features to improve performance
(Borthwick et al, 1998; Sarawagi and Cohen, 2005).
Charniak (2001) shows that it is possible to learn a
model of name structure, either by using coreference
information as labeled data, or by leveraging a small
set of hand-crafted constraints. Elsner et al (2009)
develop a nonparametric Bayesian model of name
structure using adaptor grammars, which they use to
distinguish types of names (e.g., people, places, and
organizations). Li et al (2004) use a set of manually-
crafted ?transformations? of name parts to build a
model of how a name might be rendered in multi-
ple different ways. While each of these approaches
bears on one or more facets of the problem that we
consider here, none provides a holistic treatment of
name disambiguation and structure.
9
Resolving Mentions to Entities The problem of
resolving mentions to entities has been approach
from a variety of different perspectives. There is
an extensive literature on probabilistic record link-
age, in which database records are compared to de-
termine if they are likely to have the same real-world
referents (e.g., Felligi and Sunter, 1969; Bilenko
et al, 2003). Most approaches focus on pairwise
assessments of whether two records are the same,
whereas our method attempts to infer a single coher-
ent model of the underlying relational data. Some
more recent work in record linkage has explicitly
formulated the task of inferring a latent relational
model of a set of observed datasets (e.g., Cohen
et al, 2000; Pasula et al, 2002; Bhattacharya and
Getoor, 2007); however, to our knowledge, these
prior models have all exploited some predefined
database schema (i.e., set of columns), which our
model does not require. Many of these prior mod-
els have been applied to bibliographic data, where
different conventions and abbreviations lead to im-
perfect matches in different references to the same
publication. In our task, we consider name mentions
in raw text; such mentions are short, and may not
offer as many redundant clues for linkage as biblio-
graphic references.
In natural language processing, coreference res-
olution is the task of grouping entity mentions
(strings), in one or more documents, based on their
common referents in the world. Although much of
coreference resolution has on the single document
setting, there has been some recent work on cross-
document coreference resolution (Li et al, 2004;
Haghighi and Klein, 2007; Poon and Domingos,
2008; Singh et al, 2011). The problem we consider
is related to cross-document coreference, although
we take on the additional challenge of providing
a canonicalized name for each referent (the corre-
sponding table row), and in inferring a structured
representation of entity names (the table columns).
For this reason, our evaluation focuses on the in-
duced table of entities, rather than the clustering of
mention strings. The best coreference systems de-
pend on carefully crafted, problem-specific linguis-
tic features (Bengtson and Roth, 2008) and exter-
nal knowledge (Haghighi and Klein, 2010b). Future
work might consider how to exploit such features for
the more holistic information extraction setting.
9 Conclusion
This paper presents a Bayesian nonparametric ap-
proach to recover structured records from text. Us-
ing only a small set of prototype records, we are able
to recover an accurate table that jointly identifies en-
tities and internal name structure. In our view, the
main advantage of a Bayesian approach compared
to more heuristic alternatives is that it facilitates in-
corporation of additional information sources when
available. In this paper, we have considered one
such additional source, incorporating temporal con-
text using the recurrent Chinese restaurant process.
We envision enhancing the model in several other
respects. One promising direction is the incorpo-
ration of name structure, which could be captured
using a first-order Markov model of the transitions
between name parts. In the nonparametric setting,
a transition matrix is unbounded along both dimen-
sions, and this can be handled by a hierarchical
Dirichlet process (HDP; Teh et al2006).4 We en-
vision other potential applications of the HDP: for
example, learning ?topics? of entities which tend to
appear together (i.e., given a mention of Mahmoud
Abbas in the American press, a mention of Ben-
jamin Netanyahu is likely), and handling document-
specific burstiness (i.e., given that an entity is men-
tioned once in a document, it is much more likely
to be mentioned again). Finally, we would like
to incorporate lexical context from the sentences in
which each entity is mentioned, which might help to
distinguish, say, computer science researchers who
share names with former defense secretaries or pro-
fessional basketball players.
Acknowledgments This research was enabled
by AFOSR FA95501010247, DARPA grant
N10AP20042, ONR N000140910758, NSF DBI-
0546594, IIS-0713379, IIS-0915187, IIS-0811562,
an Alfred P. Sloan Fellowship, and Google?s support
of the Worldly Knowledge project at CMU. We
thank the reviewers for their thoughtful feedback.
4One of the reviewers proposed to draw entire column se-
quences from a Dirichlet process. Given the relatively small
number of columns and canonical name forms, this may be a
straightforward and effective alternative to the HDP.
10
References
Amr Ahmed and Eric P. Xing. 2008. Dynamic non-
parametric mixture models and the recurrent Chinese
restaurant process with applications to evolutionary
clustering. In International Conference on Data Min-
ing.
Javier Artiles, Julio Gonzalo, and Satoshi Sekine. 2007.
The SemEval-2007 WePS evaluation: establishing a
benchmark for the web people search task. In Pro-
ceedings of the 4th International Workshop on Seman-
tic Evaluations, SemEval ?07, pages 64?69. Associa-
tion for Computational Linguistics.
Eric Bengtson and Dan Roth. 2008. Understanding
the value of features for coreference resolution. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ?08,
pages 294?303, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Indrajit Bhattacharya and Lise Getoor. 2007. Collec-
tive entity resolution in relational data. ACM Trans.
Knowl. Discov. Data, 1(1), March.
Mikhail Bilenko, William W. Cohen, Stephen Fien-
berg, Raymond J. Mooney, and Pradeep Ravikumar.
2003. Adaptive name-matching in information in-
tegration. IEEE Intelligent Systems, 18(5):16?23,
September/October.
A. Borthwick, J. Sterling, E. Agichtein, and R. Grishman.
1998. Exploiting diverse knowledge sources via max-
imum entropy in named entity recognition. In Sixth
Workshop on Very Large Corpora New Brunswick,
New Jersey. Association for Computational Linguis-
tics.
Nathanael Chambers and Dan Jurafsky. 2011. Template-
based information extraction without the templates. In
Proceedings of ACL.
Eugene Charniak. 2001. Unsupervised learning of name
structure from coreference data. In Proceedings of the
Second Meeting of the North American Chapter of the
Association for Computational Linguistics.
William W. Cohen, Henry Kautz, and David McAllester.
2000. Hardening soft information sources. In Pro-
ceedings of the Sixth International Conference on
Knowledge Discovery and Data Mining, pages 255?
259.
George Doddington, Alexis Mitchell, Mark Przybocki,
Lance Ramshaw, Stephanie Strassel, and Ralph
Weischedel. 2004. The automatic content extraction
(ace) program: Tasks, data, and evaluation. In 4th
international conference on language resources and
evaluation (LREC?04).
Jacob Eisenstein and Eric Xing. 2010. The CMU 2008
political blog corpus. Technical report, Carnegie Mel-
lon University.
Ergin Elmacioglu, Yee Fan Tan, Su Yan, Min-Yen Kan,
and Dongwon Lee. 2007. Psnus: Web people name
disambiguation by simple clustering with rich features.
In Proceedings of the Fourth International Workshop
on Semantic Evaluations (SemEval-2007), pages 268?
271, Prague, Czech Republic, June. Association for
Computational Linguistics.
Micha Elsner, Eugene Charniak, and Mark Johnson.
2009. Structured generative models for unsupervised
named-entity clustering. In Proceedings of Human
Language Technologies: The 2009 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, pages 164?172, Boulder,
Colorado, June. Association for Computational Lin-
guistics.
I. P. Felligi and A. B. Sunter. 1969. A theory for record
linkage. Journal of the American Statistical Society,
64:1183?1210.
Jenny R. Finkel, Trond Grenager, and Christopher Man-
ning. 2005. Incorporating non-local information
into information extraction systems by Gibbs sam-
pling. In Proceedings of the 43rd Annual Meeting on
Association for Computational Linguistics, ACL ?05,
pages 363?370, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Aria Haghighi and Dan Klein. 2007. Unsupervised
coreference resolution in a nonparametric bayesian
model. In Proceedings of the 45th Annual Meeting
of the Association of Computational Linguistics, pages
848?855, Prague, Czech Republic, June. Association
for Computational Linguistics.
Aria Haghighi and Dan Klein. 2010a. An entity-level
approach to information extraction. In Proceedings
of the ACL 2010 Conference Short Papers, ACLShort
?10, pages 291?295, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Aria Haghighi and Dan Klein. 2010b. Coreference reso-
lution in a modular, entity-centered model. In Human
Language Technologies: The 2010 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, pages 385?393, Los An-
geles, California, June. Association for Computational
Linguistics.
Harold W. Kuhn. 1955. The Hungarian method for the
assignment problem. Naval Research Logistic Quar-
terly, 2:83?97.
Nicholas Kushmerick, Daniel S. Weld, and Robert
Doorenbos. 1997. Wrapper induction for information
extraction. In Proceedings of IJCAI.
Xin Li, Paul Morie, and Dan Roth. 2004. Identification
and tracing of ambiguous names: Discriminative and
generative approaches. In Proceedings of AAAI, pages
419?424.
11
Percy Liang, Michael I. Jordan, and Dan Klein. 2010.
Type-Based MCMC. In Human Language Technolo-
gies: The 2010 Annual Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics, pages 573?581, Los Angeles, California,
June. Association for Computational Linguistics.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Schu?tze. 2008. Introduction to Information Re-
trieval. Cambridge University Press, 1 edition, July.
Paul McNamee and Hoa Trang Dang. 2009. Overview
of the TAC 2009 knowledge base population track. In
Proceedings of the Text Analysis Conference (TAC).
Hanna Pasula, Bhaskara Marthi, Brian Milch, Stuart Rus-
sell, and Ilya Shpitser. 2002. Identity uncertainty and
citation matching. In Advances in Neural Processing
Systems 15, Vancouver, British Columbia. MIT Press.
Hoifung Poon and Pedro Domingos. 2008. Joint un-
supervised coreference resolution with Markov Logic.
In Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, pages 650?
659, Honolulu, Hawaii, October. Association for Com-
putational Linguistics.
Carl E. Rasmussen. 2000. The Infinite Gaussian Mixture
Model. In In Advances in Neural Information Process-
ing Systems 12, volume 12, pages 554?560.
Sunita Sarawagi and William W. Cohen. 2005. Semi-
Markov conditional random fields for information ex-
traction. In Lawrence K. Saul, Yair Weiss, and Le?on
Bottou, editors, Advances in Neural Information Pro-
cessing Systems 17, pages 1185?1192. MIT Press,
Cambridge, MA.
Sameer Singh, Amarnag Subramanya, Fernando Pereira,
and Andrew McCallum. 2011. Large-scale cross-
document coreference using distributed inference and
hierarchical models. In Association for Computa-
tional Linguistics: Human Language Technologies
(ACL HLT).
Yee W. Teh, Michael I. Jordan, Matthew J. Beal, and
David M. Blei. 2006. Hierarchical dirichlet pro-
cesses. Journal of the American Statistical Associa-
tion, 101:1566?1581, December.
Erik F. Tjong Kim Sang. 2002. Introduction to
the CoNLL-2002 shared task: Language-independent
named entity recognition. In Proceedings of the Sixth
Conference on Natural Language Learning.
Greg C. G. Wei and Martin A. Tanner. 1990. A Monte
Carlo Implementation of the EM Algorithm and the
Poor Man?s Data Augmentation Algorithms. Journal
of the American Statistical Association, 85(411):699?
704.
12
