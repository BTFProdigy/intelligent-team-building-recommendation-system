Proceedings of the ACL 2007 Student Research Workshop, pages 55?60,
Prague, June 2007. c?2007 Association for Computational Linguistics
Adaptive String Distance Measures
for Bilingual Dialect Lexicon Induction
Yves Scherrer
Language Technology Laboratory (LATL)
University of Geneva
1211 Geneva 4, Switzerland
yves.scherrer@lettres.unige.ch
Abstract
This paper compares different measures of
graphemic similarity applied to the task
of bilingual lexicon induction between a
Swiss German dialect and Standard Ger-
man. The measures have been adapted
to this particular language pair by training
stochastic transducers with the Expectation-
Maximisation algorithm or by using hand-
made transduction rules. These adaptive
metrics show up to 11% F-measure improve-
ment over a static metric like Levenshtein
distance.
1 Introduction
Building lexical resources is a very important step in
the development of any natural language processing
system. However, it is a time-consuming and repeti-
tive task, which makes research on automatic induc-
tion of lexicons particularly appealing. In this pa-
per, we will discuss different ways of finding lexical
mappings for a translation lexicon between a Swiss
German dialect and Standard German. The choice
of this language pair has important consequences on
the methodology. On the one hand, given the so-
ciolinguistic conditions of dialect use (diglossia), it
is difficult to find written data of high quality; par-
allel corpora are virtually non-existent. These data
constraints place our work in the context of scarce-
resource language processing. On the other hand,
as the two languages are closely related, the lexical
relations to be induced are less complex. We argue
that this point alleviates the restrictions imposed by
the scarcity of the resources. In particular, we claim
that if two languages are close, even if one of them is
scarcely documented, we can successfully use tech-
niques that require training.
Finding lexical mappings amounts to finding
word pairs that are maximally similar, with respect
to a particular definition of similarity. Similarity
measures can be based on any level of linguistic
analysis: semantic similarity relies on context vec-
tors (Rapp, 1999), while syntactic similarity is based
on the alignment of parallel corpora (Brown et al,
1993). Our work is based on the assumption that
phonetic (or rather graphemic, as we use written
data) similarity measures are the most appropriate
in the given language context because they require
less sophisticated training data than semantic or syn-
tactic similarity models. However, phonetic simi-
larity measures can only be used for cognate lan-
guage pairs, i.e. language pairs that can be traced
back to a common historical origin and that possess
highly similar linguistic (in particular, phonologi-
cal and morphological) characteristics. Moreover,
we can only expect phonetic similarity measures to
induce cognate word pairs, i.e. word pairs whose
forms and significations are similar, as a result of a
historical relationship.
We will present different models of phonetic sim-
ilarity that are adapted to the given language pair. In
particular, attention has been paid to develop tech-
niques requiring little manually annotated data.
2 Related Work
Our work is inspired by Mann and Yarowsky
(2001). They induce translation lexicons between
a resource-rich language (typically English) and a
scarce resource language of another language fam-
ily (for example, Portuguese) by using a resource-
55
rich bridge language of the same family (for ex-
ample, Spanish). While they rely on existing
translation lexicons for the source-to-bridge step
(English-Spanish), they use string distance models
(called cognate models) for the bridge-to-target step
(Spanish-Portuguese). Mann and Yarowsky (2001)
distinguish between static metrics, which are suffi-
ciently general to be applied to any language pair,
and adaptive metrics, which are adapted to a spe-
cific language pair. The latter allow for much finer-
grained results, but require more work for the adap-
tation. Mann and Yarowsky (2001) use variants of
Levenshtein distance as a static metric, and a Hidden
Markov Model (HMM) and a stochastic transducer
trained with the Expectation-Maximisation (EM) al-
gorithm as adaptive metrics. We will also use Leven-
shtein distance as well as the stochastic transducer,
but not the HMM, which performed worst in Mann
and Yarowsky?s study.
The originality of their approach is that they ap-
ply models used for speech processing to cognate
word pair induction. In particular, they refer to a
previous study by Ristad and Yianilos (1998). Ris-
tad and Yianilos showed how a stochastic transducer
can be trained in a non-supervised manner using the
EM algorithm and successfully applied their model
to the problem of pronunciation recognition (sound-
to-letter conversion). Jansche (2003) reviews their
work in some detail, correcting thereby some errors
in the presentation of the algorithms.
Heeringa et al (2006) present several modifica-
tions of the Levenshtein distance that approximate
linguistic intuitions better. These models are pre-
sented in the framework of dialectometry, i.e. they
provide numerical measures for the classification of
dialects. However, some of their models can be
adapted to be used in a lexicon induction task. Kon-
drak and Sherif (2006) use phonetic similarity mod-
els for cognate word identification.
Other studies deal with lexicon induction for cog-
nate language pairs and for scarce resource lan-
guages. Rapp (1999) extends an existing bilin-
gual lexicon with the help of non-parallel cor-
pora, assuming that corresponding words share co-
occurrence patterns. His method has been used by
Hwa et al (2006) to induce a dictionary between
Modern Standard Arabic and the Levantine Arabic
dialect. Although this work involves two closely re-
lated language varieties, graphemic similarity mea-
sures are not used at all. Nevertheless, Schafer and
Yarowsky (2002) have shown that these two tech-
niques can be combined efficiently. They use Rapp?s
co-occurrence vectors in combination with Mann
and Yarowsky?s EM-trained transducer.
3 Two-Stage Models of Lexical Induction
Following the standard statistical machine transla-
tion architecture, we represent the lexicon induction
task as a two-stage model. In the first stage, we use
the source word to generate a fixed number of can-
didate translation strings, according to a transducer
which represents a particular similarity measure. In
the second stage, these candidate strings are filtered
through a lexicon of the target language. Candidates
that are not words of the target language are thus
eliminated.
This article is, like previous work, mostly con-
cerned with the comparison of different similarity
measures. However, we extend previous work by
introducing two original measures (3.3 and 3.4) and
by embedding the measures into the proposed two-
stage framework of lexicon induction.
3.1 Levenshtein Distance
One of the simplest string distance measures is the
Levenshtein distance. According to it, the distance
between two words is defined as the least-cost se-
quence of edit and identity operations. All edit oper-
ations (insertion of one character, substitution of one
character by another, and deletion of one character)
have a fixed cost of 1. The identity operation (keep-
ing one character from the source word in the target
word) has a fixed cost of 0. Levenshtein distance op-
erates on single letters without taking into account
contextual features. It can thus be implemented in
a memoryless (one-state) transducer. This distance
measure is static ? it remains the same for all lan-
guage pairs. We will use Levenshtein distance as a
baseline for our experiments.
3.2 Stochastic Transducers Trained with EM
The algorithm presented by Ristad and Yianilos
(1998) enables one to train a memoryless stochastic
transducer with the Expectation-Maximisation (EM)
algorithm. In a stochastic transducer, all transitions
represent probabilities (rather than costs or weights).
56
The transduction probability of a given word pair is
the sum of the probabilities of all paths that gen-
erate it. The goal of using the EM algorithm is to
find the transition probabilities of a stochastic trans-
ducer which maximise the likelihood of generating
the word pairs given in the training stage. This
goal is achieved iteratively by using a training lex-
icon consisting of correct word pairs. The initial
transducer contains uniform probabilities. It is used
to transduce the word pairs of the training lexicon,
thereby counting all transitions used in this process.
Then, the transition probabilities of the transducer
are reestimated according to the frequency of usage
of the transitions counted before. This new trans-
ducer is then used in the next iteration.
This adaptive model is likely to perform better
than the static Levenshtein model. For example, to
transduce Swiss German dialects to Standard Ger-
man, inserting n or e is much more likely than in-
serting m or i. Language-independent models can-
not predict such specific facts, but stochastic trans-
ducers learn them easily. However, these improve-
ments come at a cost: a training bilingual lexicon of
sufficient size must be available. For scarce resource
languages, such lexicons often need to be built man-
ually.
3.3 Training without a Bilingual Corpus
In order to further reduce the data requirements,
we developed another strategy that avoided using a
training bilingual lexicon altogether and used other
resources for the training step instead. The main
idea is to use a simple list of dialect words, and the
Standard German lexicon. In doing this, we assume
that the structure of the lexicon informs us about
which transitions are most frequent. For example,
the dialect word chue ?cow? does not appear in the
Standard German lexicon, but similar words like
Kuh ?cow?, Schuh ?shoe?, Schule ?school?, Sache
?thing?, K?he ?cows? do. Just by inspecting these
most similar existing words, we can conclude that c
may transform to k (Kuh, K?he), that s is likely to
be inserted (Schuh, Schule, Sache), and that e may
transform to h (Kuh, Schuh ). But we also conclude
that none of the letters c, h, u, e is likely to transform
to ? or f, just because such words do not exist in
the target lexicon. While such statements are coinci-
dental for one single word, they may be sufficiently
reliable when induced over a large corpus.
In this model, we use an iterative training algo-
rithm alternating two tasks. The first task is to build
a list of hypothesized word pairs by using the di-
alect word list, the Standard German lexicon, and a
transducer1: for each dialect word, candidate strings
are generated, filtered by the lexicon, and the best
candidate is selected. The second task is to train a
stochastic transducer with EM, as explained above,
on the previously constructed list of word pairs. In
the next iteration, this new transducer is used in the
first task to obtain a more accurate list of word pairs,
which in turn allows us to build a new transducer
in the second task. This process is iterated several
times to gradually eliminate erroneous word pairs.
The most crucial step is the selection of the best
candidate from the list returned by the lexicon filter.
We could simply use the word which obtained the
highest transduction probability. However, prelimi-
nary experiments have shown that the iterative algo-
rithm tends to prefer deletion operations, so that it
will converge to generating single-letter words only
(which turn out to be present in our lexicon). To
avoid this scenario, the length of the suggested can-
didate words must be taken into account. We there-
fore simply selected the longest candidate word.2
3.4 A Rule-based Model
This last model does not use learning algorithms.
It consists of a simple set of transformation rules
that are known to be important for the chosen lan-
guage pair. Marti (1985, 45-64) presents a precise
overview of the phonetic correspondences between
the Bern dialect and Standard German. Contrary
to the learning models, this model is implemented
in a weighted transducer with more than one state.
Therefore, it allows contextual rules too. For ex-
ample, we can state that the Swiss German sequence
?ech should be translated to euch. Each rule is given
a weight of 1, no matter how many characters it con-
cerns. The rule set contains about 50 rules. These
rules are then superposed with a Levenshtein trans-
ducer, i.e. with context-free edit and identity opera-
1In the initialization step, we use a Levenshtein transducer.
2In fact, we should select the word with the lowest abso-
lute value of the length difference. The suggested simplification
prevents us from being trapped in the single-letter problem and
reflects the linguistic reality that Standard German words tend
to be longer than dialect words.
57
tions for each letter. These additional transitions as-
sure that every word can be transduced to its target,
even if it does not use any of the language-specific
rules. The identity transformations of the Leven-
shtein part weigh 2, and its edit operations weigh
3. With these values, the rules are always preferred
to the Levenshtein edit operations. These weights
are set somewhat arbitrarily, and further adjustments
could slightly improve the results.
4 Experiments and Results
4.1 Data and Training
Written data is difficult to obtain for Swiss German
dialects. Most available data is in colloquial style
and does not reliably follow orthographic rules. In
order to avoid tackling these additional difficulties,
we chose a dialect literature book written in the Bern
dialect. From this text, a word list was extracted;
each word was manually translated to Standard Ger-
man. Ambiguities were resolved by looking at the
word context, and by preferring the alternatives per-
ceived as most frequent.3 No morphological analy-
sis was performed, so that different inflected forms
of the same lemma may occur in the word list. The
only preprocessing step concerned the elimination
of morpho-phonological variants (sandhi phenom-
ena). The whole list contains 5124 entries. For
the experiments, 393 entries were excluded because
they were foreign language words, proper nouns or
Standard German words.4 From the remaining word
pairs, about 92% were annotated as cognate pairs.5
One half of the corpus was reserved for training the
EM-based models, and the other half was used for
testing.
The Standard German lexicon is a word list con-
sisting of 202?000 word forms. While the lexicon
provides more morphological, syntactic and seman-
tic information, we do not use it in this work.
3Further quality improvements could be obtained by includ-
ing the results of a second annotator, and by allowing multiple
translations.
4This last category was introduced because the dialect text
contained some quotations in Standard German.
5This annotation was done by the author, a native speaker
of both German varieties. Mann and Yarowsky (2001) consider
a word pair as cognate if the Levenshtein distance between the
two words is less than 3. Their heuristics is very conservative:
it detects 84% of the manually annotated cognate pairs of our
corpus.
The test corpus contains 2366 word pairs. 407
pairs (17.2 %) consist of identical words (lower
bound). 1801 pairs (76.1%) contain a Standard Ger-
man word present in the lexicon, and 1687 pairs
(71.3%) are cognate pairs, with the Standard Ger-
man word present in the lexicon (upper bound). It
may surprise that many Standard German words of
the test corpus do not exist in the lexicon. This con-
cerns mostly ad-hoc compound nouns, which cannot
be expected to be found in a Standard German lex-
icon of a reasonable size. Additionally, some Bern
dialect words are expressed by two words in Stan-
dard German, such as the sequence ir ?in the (fem.)?
that corresponds to Standard German in der. For rea-
sons of computational complexity, our model only
looks for single words and will not find such corre-
spondences.
The basic EM model (3.2) was trained in 50 iter-
ations, using a training corpus of 200 word pairs.
Interestingly, training on 2000 word pairs did not
improve the results. The larger training corpus did
not even lead the algorithm to converge faster.6 The
monolingual EM model (3.3) was trained in 10 iter-
ations, each of which involved a basic EM training
with 50 iterations on a training corpus of 2000 di-
alect words.
4.2 Results
As explained above, the first stage of the model takes
the dialect words given in the test corpus and gen-
erates, for each dialect word, the 500 most similar
strings according to the transducer used. This list
is then filtered by the lexicon. Between 0 and 20
candidate words remain, depending on how effective
the lexicon filter has been. Thus, each source word
is associated to a candidate list, which is ordered
with respect to the costs or probabilities attributed to
the candidates by the transducer. Experiments with
1000 candidate strings yielded comparable results.
Table 1 shows some results for the four models.
The table reports the number of times the expected
Standard German words appeared anywhere in the
corresponding candidate lists (List), and the number
6This is probably due to the fact that the percentage of iden-
tical words is quite high, which facilitates the training. Another
reason could be that the orthographical conventions used in the
dialect text are quite close to the Standard German ones, so that
they conceal some phonetic differences.
58
N L P R F
Levenshtein List 840 3.1 18.5 35.5 24.3
Top 671 1.1 32.7 28.4 30.4
EM bilingual List 1210 4.5 21.4 51.1 30.2
Top 794 0.7 52.5 33.6 41.0
EM mono- List 1070 5.0 16.6 45.2 24.3
lingual Top 700 0.7 47.9 29.6 36.6
Rules List 987 3.2 22.8 41.7 29.5
Top 909 1.0 45.6 38.4 41.7
Table 1: Results. The table shows the absolute num-
bers of correct target words induced (N) and the av-
erage lengths of the candidate lists (L). The three
rightmost columns represent percentage values of
precision (P), recall (R), and F-measure (F).
of times they appeared at the best-ranked position of
the candidate lists (Top). Precision and recall mea-
sures are computed as follows:7
precision =
|correct target words|
|unique candidate words|
recall =
|correct target words|
|tested words|
As Table 1 shows, the three adaptive models
perform better than the static Levenshtein distance
model. This finding is consistent with the results
of Mann and Yarowsky (2001), although our experi-
ments showmore clear-cut differences. The stochas-
tic transducer trained on the bilingual corpus ob-
tained similar results to the rule-based system, while
the transducer trained on a monolingual corpus per-
formed only slightly better than the baseline. Never-
theless, its performance can be considered to be sat-
isfactory if we take into account that virtually no in-
formation on the exact graphemic correspondences
has been given. The structure of the lexicon and of
the source word list suffice to make some generali-
sations about graphemic correspondences between
two languages. However, it remains to be shown
if this method can be extended to more distant lan-
guage pairs.
In contrast to Levenshtein distance, the bilingual
EM model improves the List statistics a lot, at the
expense of longer candidate lists. However, when
comparing the Top statistics, the difference between
the models is less marked. The rule-based model
7The words that occur in several candidate lists (i.e., for
different source words) are counted only once, hence the term
unique candidate words.
generates rather short candidate lists, but it still out-
performs all other models with respect to the words
proposed in first position. The rule-based model ob-
tains high F-measure values, which means that its
precision and recall values are better balanced than
in the other models.
4.3 Discussion
All models require only a small amount of training
or development data. Such data should be available
for most language pairs that relate a scarce resource
language to a resource-rich language. However, the
performances of the rule-based model and the bilin-
gual EM model show that building a training corpus
with manually translated word pairs, or alternatively
implementing a small rule set, may be worthwhile.
The overall performances of the presented sys-
tems may seem poor. Looking at the recall values
of the Top statistics, our models only induce about
one third of the test corpus, or only about half of the
test words that can be induced by phonetic similar-
ity models ? we cannot expect our models to induce
non-cognate words or words that are not in the lex-
icon (see the upper bound values in 4.1). Using the
same models, Mann and Yarowsky (2001) induced
over 90% of the Spanish-Portuguese cognate vocab-
ulary. One reason for their excellent results lies in
their testing procedure. They use a small test corpus
of 100 word pairs. For each given word, they com-
pute the transduction costs to each of the 100 pos-
sible target words, and select the best-ranked candi-
date as hypothesized solution. The list of possible
target words can thus be explored exhaustively. We
tested our models withMann and Yarowsky?s testing
procedure and obtained very competitive results (see
Table 2). Interestingly, the monolingual EM model
performed much worse in this evaluation, a result
which could not be expected in light of the results in
Table 1.
While Mann and Yarowsky?s procedure is very
useful to evaluate the performance of different simi-
larity measures and the impact of different language
pairs, we believe that it is not representative for the
task of lexicon induction. Typically, the list of possi-
ble target words (the target lexicon) does not contain
100 words only, but is much larger (202?000 words
in our case). This difference has several implica-
tions. First, the lexicon is more likely to present very
59
Mann and Yarowsky Our work
cognate full cognate full
Levenshtein 92.3 67.9 90.5 85.2
EM bilingual 92.3 67.1 92.2 86.5
EM monolingual 81.9 76.7
Rules 94.1 88.7
Table 2: Comparison between Mann and
Yarowsky?s results on Spanish-Portuguese (68%
of the full vocabulary are cognate pairs), and our
results on Swiss German-Standard German (83%
cognate pairs). The tests were performed on 10
corpora of 100 word pairs each. The numbers
represent the percentage of correctly induced word
pairs.
similar words (for example, different inflected forms
of the same lexeme), increasing the probability of
?near misses?. Second, our lexicon is too large to be
searched exhaustively. Therefore, we introduced our
two-stage approach, whose first stage is completely
independent of the lexicon. The drawback of this
approach is that for many dialect words, it yields
no result at all, because the 500 generated candi-
dates were all non-words. The recall rates could
be increased by generating more candidates, but this
would lead to longer execution times and lower pre-
cision rates.
5 Conclusion and Perspectives
The experiments conducted with various adaptive
metrics of graphemic similarity show that in the
case of closely related language pairs, lexical in-
duction performances can be increased compared to
a static measure like Levenshtein distance. They
also show that requirements for training data can
be kept rather small. However, these models also
show their limits. They only use single word in-
formation for training and testing, which means that
the rich contextual information encoded in texts, as
well as the morphologic and syntactic information
available in the target lexicon, cannot be exploited.
Future research will focus on integrating contextual
information about the syntactic and semantic prop-
erties of the words into our models, still keeping
in mind the data restrictions for dialects and other
scarce resource languages. Such additional informa-
tion could be implemented by adding a third step to
our two-stage model.
Acknowledgements
We thank Paola Merlo for her precious and useful
comments on this work. We also thank Eric Wehrli
for allowing us to use the LATL Standard German
lexicon.
References
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: parameter esti-
mation. Computational Linguistics, 19(2):263?311.
Wilbert Heeringa, Peter Kleiweg, Charlotte Gooskens,
and John Nerbonne. 2006. Evaluation of string dis-
tance algorithms for dialectology. In Proceedings of
the ACL Workshop on Linguistic Distances, pages 51?
62, Sydney, Australia.
Rebecca Hwa, Carol Nichols, and Khalil Sima?an. 2006.
Corpus variations for translation lexicon induction. In
Proceedings of AMTA?06, pages 74?81, Cambridge,
MA, USA.
Martin Jansche. 2003. Inference of String Mappings for
Language Technology. Ph.D. thesis, Ohio State Uni-
versity.
Grzegorz Kondrak and Tarek Sherif. 2006. Evaluation
of several phonetic similarity algorithms on the task
of cognate identification. In Proceedings of the ACL
Workshop on Linguistic Distances, pages 43?50, Syd-
ney, Australia.
Gideon S. Mann and David Yarowsky. 2001. Multipath
translation lexicon induction via bridge languages. In
Proceedings of NAACL?01, Pittsburgh, PA, USA.
Werner Marti. 1985. Berndeutsch-Grammatik. Francke
Verlag, Bern, Switzerland.
Reinhard Rapp. 1999. Automatic identification of word
translations from unrelated English and German cor-
pora. In Proceedings of ACL?99, pages 519?526,
Maryland, USA.
Eric Sven Ristad and Peter N. Yianilos. 1998. Learn-
ing string-edit distance. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 20(5):522?532.
Charles Schafer and David Yarowsky. 2002. Induc-
ing translation lexicons via diverse similarity measures
and bridge languages. In Proceedings of CoNLL?02,
pages 146?152, Taipei, Taiwan.
60
Proceedings of the ACL-08: HLT Workshop on Parsing German (PaGe-08), pages 16?23,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Part-of-Speech Tagging with a Symbolic Full Parser:
Using the TIGER Treebank to Evaluate Fips
Yves Scherrer
Language Technology Laboratory (LATL)
University of Geneva
1211 Geneva 4, Switzerland
yves.scherrer@lettres.unige.ch
Abstract
In this paper, we introduce the German ver-
sion of the multilingual Fips parsing system.
We focus on the evaluation of its part-of-
speech tagging component with the help of the
TIGER treebank. We explain how Fips can be
adapted to the tagset used by TIGER and re-
port first results of this study: currently, 87%
of words are tagged correctly. We also discuss
some common errors and explore a possible
extension of this study to parsing.
1 Introduction
Fips is a parsing framework based on the main as-
sumptions of Chomsky?s generative linguistics. It
has been designed as a multilingual framework,
making it easy to add new languages. Currently, it
is available for six languages (English, French, Ger-
man, Italian, Spanish and Greek). While the French
version (providing the best coverage) has taken part
in evaluation campaigns (Adda et al, 1998; Gold-
man et al, 2005), the other language modules have
only been subject to internal qualitative evaluation.
However, the availability of gold standard treebanks
allows for quantitative evaluation of rule-based pars-
ing systems. In particular, we propose to use the
TIGER treebank for the evaluation of the German
version of Fips.
This paper reports on research in progress. As a
preliminary step towards a quantitative assessment
of parser performance, we focus on the task of Part-
of-Speech (POS) tag comparison here. This task is
intended to yield a first appreciation of the quality of
the German Fips component without having to deal
with the full parser output and its possible incom-
patibilities due to underlying theoretical differences.
Tag comparison operates on a word-by-word basis
and provides binary measures of accuracy (tag iden-
tity or difference).
We extend our work to the tasks of lemma identi-
fication and morphological analysis: Fips as well as
the TIGER treebank provide this information.
Fips has been developed independently of the
TIGER treebank. Therefore, a large part of this pa-
per deals with problems arising from mismatches
between the design decisions made for Fips and the
annotation guidelines of TIGER. In our view, a de-
tailed discussion of these mismatches is essential for
a fair assessment of the performances of Fips, but
may also be interesting for future research involving
evaluation.
This paper is organized as follows. In Section 2,
we present the Fips framework. In Section 3, we
recall the main characteristics of the TIGER tree-
bank, explain the adaptations we applied to the Fips
tagger and give some information about the evalu-
ation setup. We go on to report the results for the
three main tasks: Part-of-Speech tagging (Section
4), lemma identification (Section 5), and morpho-
logical analysis (Section 6). Section 7 compares our
work to statistical POS tagging and to parser eval-
uation. We conclude by giving an overview of the
benefits of quantitative evaluation.
2 The Fips framework
Fips (Wehrli, 2007) is a deep symbolic parser devel-
oped at the University of Geneva. It currently sup-
ports six languages, and others are under develop-
ment. The parser is based on an adaption of gener-
ative linguistics, borrowing concepts from the Min-
imalist model (Chomsky, 1995), from the Simpler
16
CP
TP
VP
Part
an
DP
NP
N
Obergrenze
D
eine
AdvP
PP
DP
NP
N
Kuren
P
bei
AdvP
PP
DP
D
Zuzahlungen
P
f?r
C
deutete
DP
NP
N
Minister
D
Der
Figure 1: Example output of the German Fips parser.
Syntax model (Culicover and Jackendoff, 2005), as
well as from Lexical Functional Grammar (Bresnan,
2001). Each syntactic constituent is represented as a
simplified X-bar structure without intermediate lev-
els, in the form [XPLXR]. X denotes a lexical cate-
gory, L and R stand for (possibly empty) lists of left
and right subconstituents, respectively.
The originality of Fips lies in its two-layer archi-
tecture. Fundamental properties and structures that
are common to all languages are defined in an ab-
stract, language-independent layer. On a theoreti-
cal level, this layer can be associated to the con-
cept of ?universal grammar?. On top of this layer,
a particular, language-dependent layer extends the
abstract structures and adds language-specific gram-
mar rules. The Fips lexicon contains detailed mor-
phosyntactic and semantic information such as se-
lectional properties, subcategorization information
and syntactico-semantic features. The parser is thus
based on a strong lexicalist framework. In order to
guide ambiguity resolution, numeric penalty values
can be assigned to rules and lexemes.
The German component of Fips contains around
100 language-specific grammar rules. The lexicon
contains 39 000 lexemes and 410 000 word forms.
The word forms are generated by a rule-based mor-
phological generator. The lexicon also contains 500
multi-word expressions and 1500 high-frequency
compound nouns. Unknown compound nouns are
chunked at runtime.
Fips operates in two modes: parser (see Figure 1)
and tagger (see Figure 2) output.1 The tagger out-
put allows us to benefit from the rich information of
the Fips lexicon, being at the same time more robust
than the parser.
3 Experimental setup
3.1 The TIGER treebank
The TIGER treebank contains about 50 000 sen-
tences of newspaper text, covering all domains
(Brants et al, 2002). The annotation has been per-
formed with the help of interactive tools. This
methodology allows the human annotator to easily
accept or reject proposals made by the computer.
Part-of-speech tags are proposed by a statistical tag-
ger trained on a manually annotated corpus. It uses
the Stuttgart-T?bingen-Tagset (STTS) (Thielen et
al., 1999). The parse trees were constructed inter-
actively with the help of a statistical parser. Figure 3
shows an example of the TIGER export file.
3.2 Adaptations
In order to compare the Fips output with the TIGER
tags, some adaptations had to be made. First of all,
the tagset had to be changed to match the STTS
tagset. While this procedure was straightforward
for most of the categories, it showed that the Ger-
man tagging module of Fips had never been subject
1The parser output is shown here for illustration ? we do not
use it in the present study.
Given the scope of this workshop, we forgo translating Ger-
man examples into English.
17
der ART SIN-MAS-NOM 311000336 0 der SUBJ
minister NN SIN-MAS-NOM 311019783 3 Minister
deutete VVFIN IND-KON-PRA-3-SIN 311021998 12 andeuten
f?r APPR 311050006 20 f?r
Zuzahlungen NE INN-ING-NOM-ACC-DAT 0 24 Zuzahlungen
bei APPR 311050009 36 bei
kuren NN PLU-FEM-NOM-ACC-DAT-GEN 311004912 40 Kur
eine ART SIN-FEM-NOM-ACC 311000346 46 ein OBJ
ober? NN SIN-MAS-NOM-ACC-DAT 311019956 51 Ober COMP-CHUNK
grenze NN SIN-FEM-NOM-ACC 311001176 55 Grenze COMP-HEAD
an PTKVZ 311050018 62 an
. $. 0 65 .
Figure 2: Example output of the German Fips tagger. The columns show: the word as found in the text; the POS
tag in the STTS tagset; morphological information in a proprietary tagset; the lexeme number of the internal database
(0 stands for unknown words); the character position at which the word begins; the lemma. The rightmost column
contains additional information like grammatical function and compound noun syntax.
Note that the compound noun Obergrenze was automatically chunked and that the word Zuzahlungen was not found
in the lexicon; the particle an is attached to the lemma of the main verb deutete.
to a rigorous evaluation. For example, there were
no particular tags for pronominal prepositions (e.g.,
dar?ber, deswegen), for prepositions with articles
(e.g., beim, ins), and for the infinitival particle zu.
Small adaptions concerned the replacement of ?
by ss (Fips uses the Swiss Standard German orthog-
raphy, lacking the letter ? ) and the different lemma-
tization of the particle verbs: in TIGER and in con-
trast to Fips, the particles are not attached to the
lemma (see the verb andeuten in Figures 2 and 3).
Finally, the Fips tagger contains a compound
noun chunker which is automatically used for un-
known words and which outputs one line for each
chunk. These lines had to be reassembled to fit with
the unchunked TIGER output (cf. the compound
noun Obergrenze in Figures 2 and 3).
3.3 Evaluation
From the TIGER export file, we extracted the orig-
inal sentences and submitted them to the Fips tag-
ger. Then, we compared its results with the informa-
tion given in TIGER. Overall, 792 885 words were
compared. This number does not correspond to the
888 578 tokens of the TIGER corpus, because the
concept of word is much more flexible in Fips than
in TIGER. For example, the token 62j?hriger is split
into two words 62 and j?hriger. By contrast, vor
allem is regarded as a single lexical item (adverb) by
Fips, but as two words by TIGER. Moreover, for a
TIGER Tag Fips Tag Number Percentage
NN NE 12592 1.59
KON ADV 8000 1.01
ADJD ADV 6737 0.85
ADV PTKA 4976 0.63
NE NN 4782 0.60
VAFIN VVFIN 3529 0.45
ART PRELS 2935 0.37
VVFIN VVIMP 1937 0.24
VVINF VVFIN 1859 0.23
VVPP VVFIN 1624 0.20
Correct tags 692 386 87.32
Tested words 792 885 100.00
Table 1: Results of the part-of-speech tag comparison.
The table shows the number of tags correctly predicted
by Fips (second last line), as well as the ten most fre-
quent erroneous predictions. The first column shows the
correct tag as given by TIGER, the second column shows
the erroneous tag assigned by Fips.
currently unknown reason, some words do not show
up in the output of the Fips tagger.
4 Part-of-speech tagging results
The most important part of this evaluation concerns
the part-of-speech tags. As explained above, we
have adapted Fips to generate STTS tags. Table 1
shows the number of correctly predicted tags, and
18
the ten most frequent tagging errors. In the follow-
ing sections, we discuss some of these errors.
4.1 Proper and common nouns
The most common error is related to the distinction
between proper (NE) and common nouns (NN). This
error affects 2.19% of words (see first and fifth line
in Table 1) and accounts for 17.29% of all tagging
errors. Currently, the distinction between proper and
common nouns is implemented in Fips as follows.
A noun is regarded as common noun if:
? it is present in the lexicon and not explic-
itly marked as proper noun: Chemie, Hirsch,
Konkurrenz, or
? it is a compound noun that can be analyzed into
chunks which are present in the lexicon: Bun-
des+bank, Finanz+markt, Sitz+platz.
A noun is regarded as proper noun if:
? it is explicitly marked as such in the lexicon:
Gregor, Berlin, Europa.
? it is not present in the lexicon and cannot
be fully analyzed as compound noun: Talk,
Gaullismus, Kibbuzarbeiter.
Tagging errors occur in two ways. Words that are
annotated as common nouns by TIGER are anno-
tated as proper nouns by Fips (see first line in Ta-
ble 1). This happens for all common nouns that are
not present in the lexicon (e.g., Primadonna, Port-
folio, Niedersachse, Gaullismus). There are also
compound nouns with a proper noun complement:
Vichy-Zeiten, Spreearm. While TIGER considers
these words as common nouns because the head is
a common noun, Fips still analyzes them as proper
nouns. For other words likeMarseillaise, the TIGER
annotation as common noun may be questioned.
In the other way, some TIGER proper nouns have
been tagged by Fips as common nouns (cf. fifth line
in Table 1). One common category of erroneous tag-
ging is the case of homonymous proper and common
nouns. For example, Kohl and Teufel are common
nouns, but also the names of German politicians and
therefore proper nouns. These misinterpretations are
due to the fact that Fips does not contain any spe-
cific Named Entity Recognition module. While Fips
successfully relies on letter case to identify proper
nouns in other languages, this approach obviously
does not work in German.
Some proper nouns exhibit a more subtle phe-
nomenon: words like Mannheim, Wendland or
Kantstrasse are analyzed by Fips as common
compound nouns (Mann+Heim, wenden+Land,
Kante+Strasse). Again, a Named Entity Recogni-
tion system would prevent such unfortunate analy-
ses. Furthermore, we do not find it compelling to an-
alyze Buddha, Bundesbank and Bundeskriminalamt
as proper nouns.
To sum up, the source of noun mistagging is three-
fold. First, the Fips lexicon contains some gaps.
Second, the lack of a Named Entity Recognition
module in Fips causes an overgeneration of homo-
graph common nouns where a proper noun would be
appropriate. Third, the distinction between proper
and common nouns is not clear-cut, and some diver-
gences can be considered as normal.
4.2 Conjunctions and adverbs
Conjunctions are frequently mistagged as adverbs.
Above all, this error affects the words und, aber,
denn, which can have an adverbial (ADV) or a con-
junction (KON) reading. In (1), the first occurrence
of und is erroneously tagged as adverb. However, if
we parse the first part of the sentence only (2), Fips
obtains the correct conjunction reading. This sug-
gests that the conjunction reading is available also
for (1), but that the ranking mechanism is flawed and
prefers the adverb reading.
(1) Automaten sind dort nur in Gesch?ften und
Restaurants erlaubt und nicht wie in der
Bundesrepublik auch im Freien.
(2) Automaten sind dort nur in Gesch?ften und
Restaurants erlaubt.
In general, it seems that Fips gets the conjunctions
right in short sentences, while it easily gets confused
with longer sentences. However, the preference for
the adverbial reading can be easily explained. In or-
der to propose a conjunction, the parser must iden-
tify two conjuncts of the same category, whereas an
adverb does not have that requirement. Thus, if the
parser fails to find two suitable conjuncts, it will pro-
pose the less constrained adverbial reading.
19
#BOS 47149 0 1088427994 0
Der der ART Nom.Sg.Masc NK 500
Minister Minister NN Nom.Sg.Masc NK 500
deutete deuten VVFIN 3.Sg.Past.Ind HD 504
f?r f?r APPR ? AC 503
Zuzahlungen Zuzahlung NN Acc.Pl.Fem NK 503
bei bei APPR ? AC 501
Kuren Kur NN Dat.Pl.Fem NK 501
eine ein ART Acc.Sg.Fem NK 502
Obergrenze Obergrenze NN Acc.Sg.Fem NK 502
an an PTKVZ ? SVP 504
. ? $. ? ? 0
#500 ? NP ? SB 504
#501 ? PP ? MNR 503
#502 ? NP ? OA 504
#503 ? PP ? MO 504
#504 ? S ? ? 0
#EOS 47149
Figure 3: An example sentence of the TIGER corpus. The #BOS and #EOS lines mark the beginning and the end of
a sentence. The columns show: the word (or word component) as found in the text; the lemma; the POS tag in the
STTS tagset; the morphological features. The fifth and sixth column, as well as the lines beginning with #50x, contain
information for the construction of the parse tree and are not relevant for our study.
4.3 Adjectives and adverbs
In contrast to English or French, there is no for-
mal difference in German between adjectives used
as predicates (e.g., Er ist schnell ) or as adverbs (e.g.,
Er f?hrt schnell ). This formal identity may have mo-
tivated the developers of the STTS tagset to use the
same tag (ADJD) in both cases. In contrast, the Ger-
man Fips tagger is based on earlier work on French
and English, where distinct tags for adverbials and
predicatives are needed. Therefore, it also uses dif-
ferent tags for German.
We tried to come up with a simple solution to this
problem by assigning the ADJD tag to all adverbs
whose base forms are homograph with an adjective.
However, in this case, we also assigned the ADJD
tag to words like ganz, nat?rlich, wirklich, which
are tagged as proper adverbs (ADV) in TIGER. In
short, we had the choice of either overgenerating
ADV tags (keeping the Fips output as-is) or over-
generating ADJD tags (with the homograph modi-
fication). Preliminary tests showed similar amounts
of overgeneration in both cases. We have thus cho-
sen to stick to the original Fips analyses.
4.4 Particles followed by adjectives
STTS introduces a special tag (PTKA) for parti-
cles ?followed by adjectives or adverbs?, for exam-
ple am [sch?nsten], zu [schnell]. In Fips, the class
of comparative adverbs also contains auch, so and
mehr. Of course, these words are not always fol-
lowed by adjectives, and should thus not always be
given the PTKA tag. While different readings are
indeed available in the Fips lexicon, the results sug-
gest that Fips overgeneralizes the comparative read-
ing and assigns the PTKA tag even in cases where
a normal ADV tag would be adequate. (3) shows a
sentence where Fips erroneously assigned the PTKA
tag to auch.
(3) Der Verkehrssenator, wie er k?nftig auch
hei?en m?ge, . . .
4.5 Pronouns
The seventh line refers to the homography of the def-
inite determiner and the relative pronoun (PRELS)
whenever Fips cannot find an agreement between
the determiner and the head of the noun phrase.
(4) Neue Debatte ?ber den Atomschild
20
In (4), the Fips lexicon only contains the neuter
lexeme Schild (which serves as a head of the com-
pound noun Atomschild ), but not the rarer mas-
culine homograph lexeme. This lexical gap pre-
vents the masculine determiner den to be attached
to Atomschild as a determiner, and Fips resorts to
the relative pronoun analysis instead.
4.6 Verb problems
Verb tagging seems to be a serious problem to Fips:
four of the ten most frequent tagging errors involve
verbs.
The first type of error is related to the distinction
between auxiliary and full verbs. The three auxiliary
verbs haben, sein, werden can also have full verb
readings, depending on the context. We recently ob-
served that Fips preferred the auxiliary reading even
in cases where a full verb reading is required, and
subsequently modified the constraints on the lexeme
selection. It now turns out that these constraints are
too strong and lead to a massive overgeneration of
the full verb reading.
Then, Fips tends to overgenerate imperatives:
third person singular forms are erroneously analyzed
as imperative plurals (e.g., kommt, schreit). Again,
this is due to agreement constraints: the third person
singular requires an overt subject, while an imper-
ative does not. If Fips fails to find a subject that
agrees with the verb (for example because of an un-
detected long distance dependency), it will resort to
an imperative reading. In the future development of
Fips, further restrictions should be imposed on the
use of imperative forms as these are extremely rare
in newspaper text.
The last two lines in Table 1 reveal that finite verb
forms are preferred to infinite forms: infinitives are
mistagged as finite plural forms, and past partici-
ples without ge- prefix are mistagged as third person
singular forms (for regular verbs) or as past plural
form (for irregular verbs with -en participle). These
phenomena depend on long distance relations and
should typically benefit from a full parsing approach
like the one used by Fips. Two factors may explain
why this is not the case. First, many sentences in
which such errors occur could not be parsed com-
pletely by Fips; long distance relations are not fully
detected in these cases. Second, the implementation
of passive and modal sentences is incomplete and
TIGER Base Form Fips Base Form
dieser diese
anderer ander
welche welcher
Beamte Beamter
Angestellte Angestellter
Figure 4: For some pronouns and nouns, TIGER and Fips
use different base forms.
lacks some essential constraints on verb form selec-
tion.
5 Lemmatizer results
On the whole TIGER corpus (792 885 words),
94.32% of the words (747 855) were correctly
lemmatized. Most errors were due to diverg-
ing base form choices. This especially holds for
pronouns and nominalized adjectives (cf. Fig-
ure 4), but also for pronouns. In TIGER, femi-
nine and neuter pronouns always refer to the mas-
culine lemma, whereas Fips separates the gen-
ders more strictly: der (Dat.Sg.Fem) refers to the
lemma der (Nom.Sg.Masc) in TIGER, but to die
(Nom.Sg.Fem) in Fips. Moreover, participles used
as adjectives keep the infinitive as base form in Fips,
but not in TIGER.
Some lemma errors are due to wrong POS tag-
ging. For instance, we found that Fips overgenerates
imperatives. For example, einig is not analyzed as
adjective, but as the imperative singular (with elision
of final e) of sich einigen; the adjective n?tige is an-
alyzed as the imperative singular of n?tigen. How-
ever, such awkward analyses should be easy to iron
out.
Globally, we find that very few errors are directly
due to the lemmatizer; most of them are either due
to different base forms or to POS tagging errors.
6 Morphology results
After the discussion of the part-of-speech tagger and
lemmatizer functionalities of Fips, we now turn to
the last functionality, the morphological analyzer.
We restricted our evaluation to the words that ob-
tained correct POS tags: if the POS tag is already
wrong, it is very likely that the morphology will be
wrong as well. Table 2 reports the results of the mor-
21
Type Number Percentage
Number mismatch 15617 2.26
Case mismatch 12420 1.79
Gender mismatch 8461 1.22
Degree mismatch 514 0.07
Person mismatch 108 0.02
Correct analysis
or no morphology 665 110 96.06
Tested words 692 386 100.00
Table 2: Results of the morphological analysis. The table
presents the numbers of words that have been correctly
analyzed by Fips, and the types of errors that occurred. A
word can present several mismatch types.
phology evaluation. Parts of speech without inflec-
tion were considered as correctly analyzed. We split
the errors into five categories, according to the in-
flection feature that Fips failed to predict correctly.
The different mismatch types do not sum up to 100%
because a word can show several mismatches (e.g., a
noun can show case and number mismatch), and be-
cause not all types of mismatch apply to all parts of
speech (for instance, degree mismatch only applies
to adjectives).
It is not easy to find recurrent patterns in the er-
rors. However, we found that most errors occurred
in noun phrases. Most inflected adjective and ar-
ticle forms admit several morphological analyses,
but the ambiguities can usually be reduced by the
syntactic context. If the ambiguities are reduced in
an incorrect way, this means that the syntactic con-
text has been analyzed badly. In other words, such
morphology errors often reflect bad parses. There-
fore, it might be useful to address these errors be-
fore evaluating the parsing performance of Fips. An-
other rather odd fact is that nouns with identical sin-
gular and plural forms (for example, Minister, Un-
ternehmen) prefer to be analyzed as plurals by Fips.
Here again, these cases hint at bad parses.
Degree mismatches result from a bug in Fips:
comparative forms in predicative positions as in (5)
are assigned the positive tag instead of the compara-
tive one.
(5) . . . um noch tiefer in den Kosmos blicken zu
k?nnen.
7 Related work
It may be interesting to compare Fips to a statistical
part-of-speech tagger for German. The TnT tagger
(Brants, 2000) is based on Hidden Markov Models,
and has been trained and tested on the NEGRA cor-
pus (Skut et al, 1997); NEGRA is the predecessor of
TIGER and uses the same tagset. Brants (2000) re-
ports an overall accuracy of 96.7%. However, TnT is
not directly comparable to Fips for several reasons.
First, we showed that Fips originally used a differ-
ent tagset, based on different linguistic assumptions
than STTS. Those conceptional differences make up
a large part of the errors, as has been shown for
the distinction between the ADJD and ADV tags.
By contrast, TnT has been trained directly over the
STTS tagset and should thus not present such errors.
Second, the recurrence of certain error patterns
with Fips illustrates the classical problem of manual
rule ranking and weighting in rule-based systems.
Third, Fips has been conceived as a parser in the
first place, and its tagger functionality should rather
be viewed as a by-product. Hence, its algorithms are
not optimized for POS tagging. While there may be
simpler approaches to obtain high tagging accuracy,
the method chosen for Fips seems theoretically more
plausible to us.
As we pointed out at the beginning, this tagger
evaluation has been started as a first step towards the
evaluation of the Fips parser. While POS tagging has
the advantage of operating word-by-word and of be-
ing rather theory-independent, these two properties
do not hold for parsing.
The phrase trees in TIGER are rather flat, while
the ones generated by Fips are deeper and closer
to recent generative grammar frameworks. We will
thus need to define the type of constituents that can
be compared. An even bigger issue is the allowance
of discontinuous phrases and crossing branches in
TIGER, whereas Fips resolves these phenomena by
resorting to projections and traces. Further research
has to show if these structural differences can be
overcome in order to lead to a meaningful compar-
ison. The exact evaluation metric will also have to
be chosen. While PARSEVAL (Black et al, 1991)
is still one of the most important metrics, other mea-
sures may be more adapted to our problem (Carroll
et al, 2002; Rehbein and van Genabith, 2007).
22
8 Conclusion
As we remarked above, this article reports on work
in progress. Until now, we have been able to show
that the general approach of evaluating Fips with the
help of the TIGER treebank is valid. With very little
adaptation work (see Section 3.2), we managed to
obtain 87.32% of POS-tagging accuracy. This is a
very promising beginning, and the discussion of the
errors has shown that there are many ?low hanging
fruits? to improve the performance.
In any way, we find that the quantitative evalu-
ation of NLP systems can be quite rewarding: de-
veloping rule-based systems is a complex task, of-
ten guided by vague intuitions about parsing qual-
ity. Quantitative evaluation allows us to measure
the progress of the development and guarantees us
that improvements on one parameter do not yield un-
wanted side effects on another.
Finally, the quantitative evaluation of the POS
tagging performances yields important feedback on
the forces and weaknesses of Fips. The result of the
evaluation can be viewed as a sort of priority list for
the developer. By working on the most common er-
rors in a target-oriented way, (s)he is guaranteed to
invest his/her time in a maximally effective manner.
Such guiding principles are very valuable for the fur-
ther development of any rule-based parsing system,
independently of the precise accuracy figures of the
evaluation. Even if the adaptation of two different
tagsets and tagging philosophies is not straightfor-
ward, we plan to extend our evaluation to other lan-
guages of the Fips project for which suitable gold
standard corpora exist.
Acknowledgements
We thank Eric Wehrli and for his precious support
for this work and for his valuable comments on pre-
vious versions of this paper.
References
G. Adda, J. Mariani, J. Lecomte, P. Paroubek, and M. Ra-
jman. 1998. The GRACE French part-of-speech tag-
ging evaluation task. In Proceedings of the First In-
ternational Conference on Language Resources and
Evaluation (LREC), Granada.
E. Black, S. Abney, S. Flickenger, C. Gdaniec, C. Gr-
ishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek,
J. Klavans, M. Liberman, M. Marcus, S. Roukos,
B. Santorini, and T. Strzalkowski. 1991. Procedure
for quantitatively comparing the syntactic coverage of
English grammars. In HLT ?91: Proceedings of the
Workshop on Speech and Natural Language, pages
306?311, Pacific Grove, California.
S. Brants, S. Dipper, S. Hansen, W. Lezius, and G. Smith.
2002. The TIGER Treebank. In Proceedings of the
Workshop on Treebanks and Linguistic Theories, So-
zopol.
T. Brants. 2000. TnT ? a statistical part-of-speech tagger.
In Proceedings of the Sixth Applied Natural Language
Processing (ANLP-2000), Seattle.
J. Bresnan. 2001. Lexical Functional Syntax. Blackwell,
Oxford.
J. Carroll, A. Frank, D. Lin, D. Prescher, and H. Uszko-
reit. 2002. Beyond PARSEVAL ? towards improved
evaluation measures for parsing systems. In Proceed-
ings of the LREC 2002 Workshop, Las Palmas, Gran
Canaria.
N. Chomsky. 1995. The Minimalist Program. MIT
Press, Cambridge, Mass.
P. W. Culicover and R. Jackendoff. 2005. Simpler Syn-
tax. Oxford University Press, Oxford.
J.-P. Goldman, C. Laenzlinger, G. Soare, and E. Wehrli.
2005. L?analyseur syntaxique multilingue Fips dans la
campagne EASy. In Proceedings of TALN XII, vol-
ume 2, pages 35?49, Dourdan.
I. Rehbein and J. van Genabith. 2007. Treebank
annotation schemes and parser evaluation for Ger-
man. In Proceedings of the 2007 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP/CoNLL 2007), pages 630?639, Prague.
W. Skut, B. Krenn, T. Brants, and H. Uszkoreit. 1997.
An annotation scheme for free word order languages.
In Proceedings of the Fifth Conference on Applied Nat-
ural Language Processing ANLP-97, Washington, DC.
C. Thielen, A. Schiller, S. Teufel, and C. St?ckert. 1999.
Guidelines f?r das Tagging deutscher Textkorpora mit
STTS. Technical report, University of Stuttgart and
University of T?bingen.
E. Wehrli. 2007. Fips, a ?deep? linguistic multilingual
parser. In Proceedings of the ACL 2007 Workshop on
Deep Linguistic Processing, pages 120?127, Prague.
23
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 90?94,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
Deep Linguistic Multilingual Translation and Bilingual Dictionaries
Eric Wehrli, Luka Nerima & Yves Scherrer
LATL-Department of Linguistics
University of Geneva
fEric.Wehrli, Luka.Nerima, Yves.Scherrerg@unige.ch
Abstract
This paper describes the MulTra project,
aiming at the development of an efficient
multilingual translation technology based
on an abstract and generic linguistic model
as well as on object-oriented software de-
sign. In particular, we will address the is-
sue of the rapid growth both of the trans-
fer modules and of the bilingual databases.
For the latter, we will show that a signifi-
cant part of bilingual lexical databases can
be derived automatically through transitiv-
ity, with corpus validation.
1 Introduction
The goal of the MulTra project is to develop a
grammar-based translation model capable of han-
dling not just a couple of languages, but poten-
tially a large number of languages. This is not
an original goal, but as 50 years of work and in-
vestment have shown, the task is by no means an
easy one, and although SMT has shown fast and
impressive results towards it (e.g. EuroMatrix),
we believe that a (principled) grammar-based ap-
proach is worth developing, taking advantage of
the remarkable similarities displayed by languages
at an abstract level of representation. In the first
phase of this project (2007-2009), our work has
focused on French, English, German, Italian and
Spanish, with preliminary steps towards Greek,
Romanian, Russian and Japanese.
To evaluate the quality of the (still under devel-
opment) system, we decided to join the WMT09
translation evaluation with prototypes for the fol-
lowing language pairs: English to French, French
to English and German to English. In this short
paper, we will first give a rough description of the
MulTra system architecture and then turn to the
difficult issue of the bilingual dictionaries.
The MulTra project relies to a large extent on
abstract linguistics, inspired from recent work in
generative grammar (Chomsky, 1995, Culicover &
Jackendoff, 2005, Bresnan, 2001). The grammar
formalism developed for this project is both rich
enough to express the structural diversity of all the
languages taken into account, and abstract enough
to capture the generalizations hidden behind ob-
vious surface diversity. At the software level, an
object-oriented design has been used, similar in
many ways to the one adopted for the multilingual
parser (cf. Wehrli, 2007).
The rapid growth of the number of transfer
modules has often been viewed as a major flaw
of the transfer model when applied to multilingual
translation (cf. Arnold, 2000, Kay, 1997). This ar-
gument, which relies on the fact that the number of
transfer modules and of the corresponding bilin-
gual dictionaries increases as a quadratic function
of the number of languages, is considerably weak-
ened if one can show that transfer modules can
be made relatively simple and light (cf. section 2),
compared to the analysis and generation modules
(whose numbers are a linear function of the num-
ber of languages). Likewise, section 3 will show
how one can drastically reduce the amount of work
by deriving bilingual dictionaries by transitivity.
2 The architecture of the MulTra system
To a large extent, this system can be viewed as an
extension of the Multilingual Fips parsing project.
For one thing, the availability of the ?deep linguis-
tic? Fips parser for the targeted languages is a cru-
cial element for the MulTra project; second, the
MulTra software design matches the one devel-
oped for the multilingual parser. In both cases, the
goal is to set up a generic system which can be re-
defined (through type extension and method rede-
finition) to suit the specific needs of, respectively,
a particular language or a particular language pair.
90
2.1 Methodology
The translation algorithm follows the traditional
pattern of a transfer system. First the input
sentence is parsed by the Fips parser, produc-
ing an information-rich phrase-structure repre-
sentation with associated predicate-argument rep-
resentations. The parser also identifies multi-
word expressions such as idioms and colloca-
tions ? crucial elements for a translation sys-
tem (cf. Seretan & Wehrli, 2006). The transfer
module maps the source-language abstract repre-
sentation into the target-language representation.
Given the abstract nature of this level of repre-
sentation, the mapping operation is relatively sim-
ple and can be sketched as follows: recursively
traverse the source-language phrase structure in
the order: head, right subconstituents, left sub-
constituents. Lexical transfer (the mapping of a
source-language lexical item with an equivalent
target-language item) occurs at the head-transfer
level (provided the head is not empty) and yields
a target-language equivalent term often, but by no
means always, of the same category. Following
the projection principle used in the Fips parser, the
target-language structure is projected on the ba-
sis of the lexical item which is its head. In other
words, we assume that the lexical head determines
a syntactic projection (or meta-projection).
Projections (ie. constituents) which have been
analyzed as arguments of a predicate undergo
a slightly different transfer process, since their
precise target-language properties may be in
part determined by the subcategorization fea-
tures of the target-language predicate. To take
a simple example, the direct object of the
French verb regarder in (1a) will be trans-
ferred into English as a prepositional phrase
headed by the preposition at, as illustrated in
(2a). This information comes from the lexical
database. More specifically, the French-English
bilingual lexicon specifies a correspondence be-
tween the French lexeme [
VP
regarder NP ]
and the English lexeme [
VP
look [
PP
at NP ] ].
For both sentences, we also illustrate the syntactic
structures as built, respectively, by the parser for
the source sentence and by the translator for the
target sentence.
(1)a. Paul a regarde? la voiture.
b. [
TP
[
DP
Paul ] a [
VP
regarde? [
DP
la [
NP
voiture
] ] ] ]
(2)a. Paul looked at the car.
b. [
TP
[
DP
Paul ] [
VP
looked [
PP
at [
DP
the [
NP
car ] ] ] ] ]
2.2 Adding a language to the system
Given the general model as sketched above, the
addition of a language to the system requires (i) a
parser and (ii) a generator. Then for each language
pair for which that language is concerned, the sys-
tem needs (iii) a (potentially empty) language-pair
specific transfer module, and (iv) a bilingual lex-
ical database. The first three components are de-
scribed below, while the fourth will be the topic of
section 3.
Parser The Fips multilingual parser is assumed.
Adding a new language requires the following
tasks: (i) grammar description in the Fips formal-
ism, (ii) redefinition of the language-specific pars-
ing methods to suit particular properties of the lan-
guage, and (iii) creation of an appropriate lexical
database for the language.
Generator Target-language generation is done
in a largely generic fashion (as described above
with the transfer and projection mechanisms).
What remains specific in the generation phase is
the selection of the proper morphological form of
a lexical item.
Language-pair-specific transfer Transfer from
language A to language B requires no language-
pair specification if the language structures of A
and B are isomorphic. Simplifying a little bit,
this happens among closely related languages,
such as Spanish and Italian for instance. For
languages which are typologically different, the
transfer module must indicate how the precise
mapping is to be done.
Consider, for instance, word-order differences
such as adjectives which are prenominal in Eng-
lish and postnominal in French ? a red car vs.
une voiture rouge. The specific English-French
transfer module specifies that French adjectives,
which do not bear the [+prenominal] lexical fea-
ture, correspond to right subconstituents (vs. left
subconstituents) of the head noun. Other cases are
more complicated, such as the V2 phenomenon
in German, pronominal cliticization in Romance
languages, or even the use of the do auxiliary in
English interrogative or negative sentences. Such
cases are handled by means of specific procedures,
91
which are in some ways reminiscent of transfor-
mation rules of the standard theory of generative
grammar, ie. rules that can insert, move or even
delete phrase-structure constituents (cf. Akmajian
& Heny, 1975).
So far, the languages taken into account in
the MulTra project are those for which the Fips
parser has been well developed, that is English,
French, German, Italian and Spanish. Of the 20
potential language pairs five are currently opera-
tional (English-French, French-English, German-
French, German-English, Italian-French), while 6
other pairs are at various stages of development.
3 Multilingual lexical database
3.1 Overview of the lexical database
The lexical database is composed for each lan-
guage of (i) a lexicon of words, containing all
the inflected forms of the words of the language,
(ii) a lexicon of lexemes, containing the syn-
tactic/semantic information of the words (corre-
sponding roughly to the entries of a classical dic-
tionary) and (iii) a lexicon of collocations (in fact
multi-word expressions including collocations and
idioms). We call the lexemes and the collocations
the lexical items of a language.
The bilingual lexical database contains the in-
formation necessary for the lexical transfer from
one language to another. For storage purposes, we
use a relational database management system. For
each language pair, the bilingual dictionary is im-
plemented as a relational table containing the asso-
ciations between lexical items of language A and
lexical items of language B. The bilingual dictio-
nary is bi-directional, i.e. it also associates lexi-
cal items of language B with lexical items of lan-
guage A. In addition to these links, the table con-
tains transfer information such as translation con-
text (eg. sport, finance, law, etc.), ranking of the
pairs in a one-to-many correspondence, seman-
tic descriptors (used for interactive disambigua-
tion), argument matching for predicates (mostly
for verbs). The table structures are identical for
all pairs of languages.
Although the bilingual lexicon is bidirectional,
it is not symmetrical. If a word v from lan-
guage A has only one translation w in language
B, it doesn?t necessarily mean that w has only one
translation v. For instance the word tongue cor-
responds to French langue, while in the opposite
direction the word langue has two translations,
tongue and language. In this case the descriptor
attribute from French to English will mention re-
spectively ?body part? and ?language?. Another
element of asymmetry is the ranking attribute used
to mark the preferred correspondences in a one-to-
many translation1. For instance the lexicographer
can mark his preference to translate lovely into the
French word charmant rather than agre?able. Of
course the opposite translation direction must be
considered independently.
What is challenging in this project is that it ne-
cessitates as many bilingual tables as the number
of language pairs considered, i.e. n(n   1)=2 ta-
bles. We consider that an appropriate bilingual
coverage (for general purpose translation) requires
well over 60?000 correspondences per language
pair.
In the framework of this project we consider
5 languages (French, English, German, Italian,
Spanish). Currently, our database contains 4 bilin-
gual dictionaries (out of the 10 needed) with the
number of entries given in figure 1:
language pair Number of entries
English - French 77?569
German - French 47?797
French - Italian 38?188
Spanish - French 23?696
Figure 1: Number of correspondences in bilingual
dictionaries
Note that these 4 bilingual dictionaries were
manually created by lexicographers and the qual-
ity of the entries can be considered as good.
3.2 Automatic generation
The importance of multilingual lexical resources
in MT and, unfortunately, the lack of available
multilingual lexical resources has motivated many
initiatives and research work to establish collabo-
ratively made multilingual lexicons, e.g. the Pa-
pillon project (Boitet & al. 2002) or automatically
generated multilingual lexicons (see for instance
Aymerish & Camelo, 2007, Gamallo, 2007).
We plan to use semi-automatic generation to
build the 6 remaining dictionaries. For this pur-
pose we will derive a bilingual lexicon by transi-
tivity, using two existing ones. For instance, if we
have bilingual correspondences for language pair
1This attribute takes the form of an integer between 6 (pre-
ferred) and 0 (lowest).
92
A! B and B! C, we can obtain A! C. We will
see below how the correspondences are validated.
The idea of using a pivot language for deriv-
ing bilingual lexicons from existing ones is not
new. The reader can find related approaches in
(Paik & al. 2004, Ahn & Frampton 2006, Zhang
& al. 2007) . The specificity of our approach is
that the initial resources are manually made, i.e.
non noisy, lexicons.
The derivation process goes as follows:
1. Take two bilingual tables for language pairs
(A, B) and (B, C) and perform a relational
equi-join. Perform a filtering based on the
preference attribute to avoid combinatory ex-
plosion of the number of generated corre-
spondences.
2. Consider as valid all the unambiguous cor-
respondences. We consider that a generated
correspondence a ! c is unambiguous if for
the lexical item a there exists only one corre-
spondence a! b in the bilingual lexicon (A,
B) and for b there exists only one correspon-
dence b ! c in (B, C). As the lexicon is non
symmetrical, this process is performed twice,
once for each translation direction.
3. Consider as valid all the correspondences ob-
tained by a pivot lexical item of type colloca-
tion. We consider as very improbable that a
collocation is ambiguous.
4. All other correspondences are checked in a
parallel corpus, i.e. only the correspondences
actually used as translations in the corpus
are kept. First, the parallel corpus is tagged
by the Fips tagger (Wehrli, 2007) in order
to lemmatize the words. This is especially
valuable for languages with rich inflection,
as well as for verbs with particles. In order
to check the validity of the correspondences,
we count the effective occurrences of a given
correspondence in a sentence-aligned paral-
lel corpus, as well as the occurrences of each
of the lexical items of the correspondence. At
the end of the process, we apply the log like-
lihood ratio test to decide whether to keep or
discard the correspondence.
3.3 Results of automatic generation
The English-German lexicon that we used in the
shared translation task was generated automati-
cally. We derived it on the basis of English-French
and German-French lexicons. For the checking of
the validity of the correspondences (point 4 of the
process) we used the parallel corpus of the debates
of the European Parliament during the period 1996
to 2001 (Koehn, 2005). Figure 2 summarizes the
results of the four steps of the derivation process:
Step Type Eng.-Ger.
1 Candidate corresp. 89?022
2 Unambiguous corresp. 67?012
3 Collocation pivot 2?642
4 Corpus checked 2?404
Total validated corresp. 72?058
Figure 2: Number of derived entries for English-
German
We obtained a number of entries compara-
ble to those of the manually built bilingual lex-
icons. The number of the correspondences for
which a validation is necessary is 19?368 (89?022-
(67?012+2?642)), of which 2?404 (approximately
12%) have been validated based on the the Eu-
roParl corpus, as explained above. The low figure,
well below our expectations, is due to the fact that
the corpus we used is not large enough and is prob-
ably not representative of the general language.
Up to now, the English-German dictionary re-
quired approximately 1?400 entries to be added
manually, which is less than 2% of the entire lexi-
con.
4 Conclusion
Based on a deep linguistic transfer approach and
an object-oriented design, the MulTra multilingual
translation system aims at developing a large num-
ber of language pairs while significantly reduc-
ing the development cost as the number of pairs
grows. We have argued that the use of an abstract
and relatively generic linguistic level of represen-
tation, as well as the use of an object-oriented soft-
ware design play a major role in the reduction of
the complexity of language-pair transfer modules.
With respect to the bilingual databases, (corpus-
checked) automatic derivation by transitivity has
been shown to drastically reduce the amount of
work.
Acknowledgments
The research described in this paper has been sup-
ported in part by a grant from the Swiss national
science foundation (no 100015-113864).
93
5 References
Ahn, K. and Frampton, M. 2006. ?Automatic Gen-
eration of Translation Dictionaries Using In-
termediary Languages?? in Cross-Language
knowledge Induction Workshop of the EACL
06, Trento, Italy, pp 41- 44.
Akmajian, A. and F. Heny, 1975. An Introduction
to the Principles of Generative Syntax, MIT
Press.
Arnold, D. 2000. ?Why translation is difficult for
computers? in H.L. Somers (ed.) Computers
and Translation : a handbook for translators,
John Benjamin.
Aymerich, J. and Camelo, H. 2007.? Automatic
extraction of entries for a machine translation
dictionary using bitexts?? in MT Summit XI,
Copenhagen, pp. 21-27
Boitet, Ch. 2001. ?Four technical and organi-
zational keys to handle more languages and
improve quality (on demand) in MT? in Pro-
ceedings of MT-Summit VIII, Santiago de
Compostela, 18-22.
Boitet, Ch., Mangeot, M. and Se?rasset, G.
2002. ?The PAPILLON project: coopera-
tively building a multilingual lexical data-
base to derive open source dictionaries & lex-
icons? in Proceedings of the 2nd workshop
on NLP and XML, COLING 2002, Taipei,
Taiwan.
Bresnan, J. 2001. Lexical Functional Syntax, Ox-
ford, Blackwell.
Chomsky, N. 1995. The Minimalist Program,
Cambridge, Mass., MIT Press.
Culicover, P. & R. Jackendoff, 2005. Simpler Syn-
tax, Oxford, Oxford University Press.
Gamallo, P. 2007. ?Learning Bilingual Lexi-
cons from Comparable English and Spanish
Corpora? in Proceedings of MT Summit XI,
Copenhagen.
Hutchins, J. 2003. ?Has machine translation im-
proved?? in Proceedings of MT-Summit IX,
New Orleans, 23-27.
Kay, M. 1997. ?Machine Translation : the Dis-
appointing Past and Present? in R.A. Cole, J.
Mariani, H. Uskoreit, G. Varile, A. Zaenen
and A. Zampoli Survey of the State of the
Art in Human Language Technology, Giar-
dini Editori.
Koehn, P. 2005. ?Europarl: A Parallel Corpus
for Statistical Machine Translation?? in MT
Summit 2005.
Ney, H. 2005. ?One Decade of Statistical Machine
Translation? in Proceedings of MT-Summit
X, Pukhet, Thailand.
Paik, K., Shirai, S. and Nakaiwa, H. 2004. ?Au-
tomatic Construction of a Transfer Dictio-
nary Considering Directionality?, in COL-
ING 2004 Multilingual Linguistic Resources
Workshop, Geneva, pp. 25-32.
Seretan, V. & E. Wehrli, 2006. ?Accurate Colloca-
tion Extraction Using a Multilingual Parser?
in Proceedings of the ACL, 953-960, Sydney,
Australia.
Wehrli, E. 2007. ?Fips, a ?deep? linguistic mul-
tilingual parse? in Proceedings of the ACL
2007 Workshop on Deep Linguistic process-
ing, 120-127, Prague, Czech Republic.
Zhang, Y., Ma, Q. and Isahara, H. 2007. ?Build-
ing Japanese-Chinese Translation Dictionary
Based on EDR Japanese-English Bilingual
Dictionary? inMT Summit XI, Copenhagen,
pp 551-557.
94
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1151?1161,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Word-based dialect identification with georeferenced rules
Yves Scherrer
LATL
Universit? de Gen?ve
Gen?ve, Switzerland
yves.scherrer@unige.ch
Owen Rambow
CCLS
Columbia University
New York, USA
rambow@ccls.columbia.edu
Abstract
We present a novel approach for (written) di-
alect identification based on the discrimina-
tive potential of entire words. We generate
Swiss German dialect words from a Standard
German lexicon with the help of hand-crafted
phonetic/graphemic rules that are associated
with occurrence maps extracted from a linguis-
tic atlas created through extensive empirical
fieldwork. In comparison with a character-
n-gram approach to dialect identification, our
model is more robust to individual spelling dif-
ferences, which are frequently encountered in
non-standardized dialect writing. Moreover, it
covers the whole Swiss German dialect contin-
uum, which trained models struggle to achieve
due to sparsity of training data.
1 Introduction
Dialect identification (dialect ID) can be viewed as
an instance of language identification (language ID)
where the different languages are very closely re-
lated. Written language ID has been a popular re-
search object in the last few decades, and relatively
simple algorithms have proved to be very successful.
The central question of language ID is the following:
given a segment of text, which one of a predefined
set of languages is this segment written in? Language
identification is thus a classification problem.
Dialect identification comes in two flavors: spoken
dialect ID and written dialect ID. These two tasks are
rather different. Spoken dialect ID relies on speech
recognition techniques which may not cope well with
dialectal diversity. However, the acoustic signal is
also available as input. Written dialect ID has to deal
with non-standardized spellings that may occult real
dialectal differences. Moreover, some phonetic dis-
tinctions cannot be expressed in orthographic writing
systems and limit the input cues in comparison with
spoken dialect ID.
This paper deals with written dialect ID, applied to
the Swiss German dialect area. An important aspect
of our model is its conception of the dialect area as a
continuum without clear-cut borders. Our dialect ID
model follows a bag-of-words approach based on the
assumption that every dialectal word form is defined
by a probability with which it may occur in each
geographic area. By combining the cues of all words
of a sentence, it should be possible to obtain a fairly
reliable geographic localization of that sentence.
The main challenge is to create a lexicon of dialect
word forms and their associated probability maps.
We start with a Standard German word list and use
a set of phonetic, morphological and lexical rules
to obtain the Swiss German forms. These rules are
manually extracted from a linguistic atlas. This lin-
guistic atlas of Swiss German dialects is the result of
decades-long empirical fieldwork.
This paper is organized as follows. We start with
an overview of relevant research (Section 2) and
present the characteristics of the Swiss German di-
alect area (Section 3). Section 4 deals with the im-
plementation of word transformation rules and the
corresponding extraction of probability maps from
the linguistic atlas of German-speaking Switzerland.
We present our dialect ID model in Section 5 and
discuss its performance in Section 6 by relating it to
a baseline n-gram model.
1151
2 Related work
Various language identification methods have been
proposed in the last three decades. Hughes et al
(2006) and R?ehu?r?ek and Kolkus (2009) provide re-
cent overviews of different approaches. One of
the simplest and most popular approaches is based
on character n-gram sequences (Cavnar and Tren-
kle, 1994). For each language, a character n-gram
language model is learned, and test segments are
scored by all available language models and labeled
with the best scoring language model. Related ap-
proaches involve more sophisticated learning tech-
niques (feature-based models, SVM and other kernel-
based methods).
A completely different approach relies on the iden-
tification of entire high-frequency words in the test
segment (Ingle, 1980). Other models have proposed
to use morpho-syntactic information.
Dialect ID has usually been studied from a speech
processing point of view. For instance, Biadsy et
al. (2009) classify speech material from four Arabic
dialects plus Modern Standard Arabic. They first run
a phone recognizer on the speech input and use the
resulting transcription to build a trigram language
model. Classification is done by minimizing the per-
plexity of the trigram models on the test segment.
An original approach to the identification of Swiss
German dialects has been taken by the Chochich?stli-
Orakel.1 By specifying the pronunciation of ten pre-
defined words, the web site creates a probability map
that shows the likelihood of these pronunciations in
the Swiss German dialect area. Our model is heavily
inspired by this work, but extends the set of cues to
the entire lexicon.
As mentioned, the ID model is based on a large
Swiss German lexicon. Its derivation from a Standard
German lexicon can be viewed as a case of lexicon
induction. Lexicon induction methods for closely
related languages using phonetic similarity have been
proposed by Mann and Yarowsky (2001) and Schafer
and Yarowsky (2002), and applied to Swiss German
data by Scherrer (2007).
The extraction of digital data from hand-drawn di-
alectological maps is a time-consuming task. There-
fore, the data should be made available for differ-
ent uses. Our Swiss German raw data is accessible
1http://dialects.from.ch
on an interactive web page (Scherrer, 2010), and
we have proposed ideas for reusing this data for
machine translation and dialect parsing (Scherrer
and Rambow, 2010). An overview of digital dialec-
tological maps for other languages is available on
http://www.ericwheeler.ca/atlaslist.
3 Swiss German dialects
The German-speaking area of Switzerland encom-
passes the Northeastern two thirds of the Swiss ter-
ritory, and about two thirds of the Swiss population
define (any variety of) German as their first language.
In German-speaking Switzerland, dialects are used
in speech, while Standard German is used nearly ex-
clusively in written contexts (diglossia). It follows
that all (adult) Swiss Germans are bidialectal: they
master their local dialect and Standard German. In
addition, they usually have no difficulties understand-
ing Swiss German dialects other than their own.
Despite the preference for spoken dialect use, writ-
ten dialect data has been produced in the form of
dialect literature and transcriptions of speech record-
ings made for scientific purposes. More recently,
written dialect has been used in electronic media like
blogs, SMS, e-mail and chatrooms. The Alemannic
Wikipedia contains about 6000 articles, among which
many are written in a Swiss German dialect.2 How-
ever, all this data is very heterogeneous in terms of
the dialects used, spelling conventions and genre.
4 Georeferenced word transformation
rules
The key component of the proposed dialect ID model
is an automatically generated list of Swiss German
word forms, each of which is associated with a
map that specifies its likelihood of occurrence over
German-speaking Switzerland. This word list is gen-
erated with the help of a set of transformation rules,
taking a list of Standard German words as a start-
ing point. In this section, we present the different
types of rules and how they can be extracted from a
dialectological atlas.
2http://als.wikipedia.org; besides Swiss German, the
Alemannic dialect group encompasses Alsatian, South-West Ger-
man Alemannic and Vorarlberg dialects of Austria.
1152
4.1 Orthography
Our system generates written dialect words according
to the Dieth spelling conventions without diacritics
(Dieth, 1986).3 These are characterized by a transpar-
ent grapheme-phone correspondence and are widely
used by dialect writers. However, they are by no
means enforced or even taught.
This lack of standardization is problematic for di-
alect ID. We have noted two major types of deviations
from the Dieth spelling conventions in our data. First,
Standard German orthography may unduly influence
dialect spelling. For example, spiele is modelled af-
ter Standard German spielen ?to play?, although the
vowel is a short monophthong in Swiss German and
should thus be written spile (ie represents a diph-
thong in Dieth spelling). Second, dialect writers do
not always distinguish short and long vowels, while
the Dieth conventions always use letter doubling to
indicate vowel lengthening. Future work will incor-
porate these fluctuations directly into the dialect ID
model.
Because of our focus on written dialect, the follow-
ing discussion will be based on written representa-
tions, but IPA equivalents are added for convenience.
4.2 Phonetic rules
Our work is based on the assumption that many words
show predictable phonetic differences between Stan-
dard German and the different Swiss German dialects.
Hence, in many cases, it is not necessary to explicitly
model word-to-word correspondences, but a set of
phonetic rules suffices to correctly transform words.
For example, the word-final sequence nd [nd
?
] (as
in Standard German Hund ?dog?4) is maintained in
most Swiss German dialects. However, it has to be
transformed to ng [N] in Berne dialect, to nn [n]
in Fribourg dialect, and to nt [nt] in Valais and Uri
dialects.
This phenomenon is captured in our system by four
transformation rules nd? nd, nd? ng, nd? nn and
nd? nt. Each rule is georeferenced, i.e. linked to
3Of course, these spelling conventions make use of umlauts
like in Standard German. There is another variant of the Di-
eth conventions that uses additional diacritics for finer-grained
phonetic distinctions.
4Standard German nd is always pronounced [nt] following a
general final devoicing rule; we neglect that artifact as we rely
only on graphemic representations.
a probability map that specifies its validity in every
geographic point. These four rules capture one single
linguistic phenomenon: their left-hand side is the
same, and they are geographically complementary.
Some rules apply uniformly to all Swiss Ger-
man dialects (e.g. the transformation st [st]? scht
[St]). These rules do not immediately contribute to
the dialect identification task, but they help to ob-
tain correct Swiss German forms that contain other
phonemes with better localization potential.
More information about the creation of the proba-
bility maps is given in Sections 4.5 and 4.6.
4.3 Lexical rules
Some differences at the word level cannot be ac-
counted for by pure phonetic alternations. One reason
are idiosyncrasies in the phonetic evolution of high
frequency words (e.g. Standard German und ?and?
is reduced to u in Bern dialect, where the phonetic
rules would rather suggest *ung). Another reason is
the use of different lexemes altogether (e.g. Standard
German immer ?always? corresponds to geng, immer,
or all, depending on the dialect). We currently use
lexical rules mainly for function words and irregular
verb stems.
4.4 Morphological rules
The transformation process from inflected Standard
German word forms to inflected Swiss German word
forms is done in two steps. First, the word stem is
adapted with phonetic or lexical rules, and then, the
affixes are generated according to the morphological
features of the word.
Inflection markers also provide dialect discrimina-
tion potential. For example, the verbal plural suffixes
offer a surprisingly rich (and diachronically stable)
interdialectal variation pattern.
4.5 The linguistic atlas SDS
One of the largest research projects in Swiss German
dialectology has been the elaboration of the Sprachat-
las der deutschen Schweiz (SDS), a linguistic atlas
that covers phonetic, morphological and lexical dif-
ferences of Swiss German dialects. Data collection
and publication were carried out between 1939 and
1997 (Hotzenk?cherle et al, 1962-1997). Linguis-
tic data were collected in about 600 villages (in-
quiry points) of German-speaking Switzerland, and
1153
resulted in about 1500 published maps (see Figure 1
for an example).
Each map represents a linguistic phenomenon that
potentially yields a set of transformation rules. For
our experiments, we selected a subset of the maps ac-
cording to the perceived importance of the described
phenomena. There is no one-to-one correspondence
between maps and implemented phenomena, for sev-
eral reasons. First, some SDS maps represent in-
formation that is best analyzed as several distinct
phenomena. Second, a set of maps may illustrate the
same phenomenon with different words and slightly
different geographic distributions. Third, some maps
describe (especially lexical) phenomena that are be-
coming obsolete and that we chose to omit.
As a result, our rule base contains about 300 pho-
netic rules covering 130 phenomena, 540 lexical
rules covering 250 phenomena and 130 morpholog-
ical rules covering 60 phenomena. We believe this
coverage to be sufficient for the dialect ID task.
4.6 Map digitization and interpolation
Recall the nd -example used to illustrate the phonetic
rules above. Figure 1 shows a reproduction of the
original, hand-drawn SDS map related to this phe-
nomenon. Different symbols represent different pho-
netic variants of the phenomenon.5 We will use this
example in this section to explain the preprocessing
steps involved in the creation of georeferenced rules.
In a first preprocessing step, the hand-drawn map
is digitized manually with the help of a geographical
information system. The result is shown in Figure 2.
To speed up this process, variants that are used in less
than ten inquiry points are omitted. (Many of these
small-scale variants likely have disappeared since the
data collection in the 1940s.) We also collapse minor
phonetic variants which cannot be distinguished in
the Dieth spelling system.
The SDS maps, hand-drawn or digitized, are point
maps. They only cover the inquiry points, but do not
provide information about the variants used in other
locations. Therefore, a further preprocessing step in-
terpolates the digitized point maps to obtain surface
maps. We follow Rumpf et al (2009) to create kernel
density estimators for each variant. This method is
5We define a variant simply as a string that may occur on the
right-hand side of a transformation rule.
Figure 1: Original SDS map for the transformation of
word-final -nd. The map contains four major linguistic
variants, symbolized by horizontal lines (-nd ), vertical
lines (-nt), circles (-ng), and triangles (-nn) respectively.
Minor linguistic variants are symbolized by different types
of circles and triangles.
Figure 2: Digitized equivalent of the map in Figure 1.
Figure 3: Interpolated surface maps for the variants -nn
(upper left), -ng (upper right), -nt (lower left) and -nd
(lower right). Black areas represent a probability of 1,
white areas a probability of 0.
1154
less sensitive to outliers than simpler linear interpola-
tion methods.6 The resulting surface maps are then
normalized such that at each point of the surface, the
weights of all variants sum up to 1. These normalized
weights can be interpreted as conditional probabili-
ties of the corresponding transfer rule: p(r | t), where
r is the rule and t is the geographic location (repre-
sented as a pair of longitude and latitude coordinates)
situated in German-speaking Switzerland. (We call
the set of all points in German-speaking Switzerland
GSS.) Figure 3 shows the resulting surface maps for
each variant. Surface maps are generated with a reso-
lution of one point per square kilometer.
As mentioned above, rules with a common left-
hand side are grouped into phenomena, such that at
any given point t ? GSS, the probabilities of all rules
r describing a phenomenon Ph sum up to 1:
?
t?GSS
?
r?Ph
p(r | t) = 1
5 The model
The dialect ID system consists of a Swiss German
lexicon that associates word forms with their geo-
graphical extension (Section 5.1), and of a testing
procedure that splits a sentence into words, looks
up their geographical extensions in the lexicon, and
condenses the word-level maps into a sentence-level
map (Sections 5.2 to 5.4).
5.1 Creating a Swiss German lexicon
The Swiss German word form lexicon is created
with the help of the georeferenced transfer rules pre-
sented above. These rules require a lemmatized, POS-
tagged and morphologically disambiguated Standard
German word as an input and generate a set of di-
alect word/map tuples: each resulting dialect word
is associated with a probability map that specifies its
likelihood in each geographic point.
To obtain a Standard German word list, we ex-
tracted all leaf nodes of the TIGER treebank (Brants
et al, 2002), which are lemmatized and morphologi-
cally annotated. These data also allowed us to obtain
word frequency counts. We discarded words with
one single occurrence in the TIGER treebank, as well
as forms that contained the genitive case or preterite
6A comparison of different interpolation methods will be the
object of future work.
tense attribute (the corresponding grammatical cate-
gories do not exist in Swiss German dialects).
The transfer rules are then applied sequentially on
each word of this list. The notation w0
?
? wn repre-
sents an iterative derivation leading from a Standard
German word w0 to a dialectal word form wn by the
application of n transfer rules of the type wi? wi+1.
The probability of a derivation corresponds to the
joint probability of the rules it consists of. Hence,
the probability map of a derivation is defined as the
pointwise product of all rule maps it consists of:
?
t?GSS
p(w0
?
? wn | t) =
n?1
?
k=0
p(wi? wi+1 | t)
Note that in dialectological transition zones, there
may be several valid outcomes for a given w0.
The Standard German word list extracted from
TIGER contains about 36,000 entries. The derived
Swiss German word list contains 560,000 word
forms, each of which is associated with a map that
specifies its regional distribution.7 Note that proper
nouns and words tagged as ?foreign material? were
not transformed. Derivations that did not obtain a
probability higher than 0.1 anywhere (because of
geographically incompatible transformations) were
discarded.
5.2 Word lookup and dialect identification
At test time, the goal is to compute a probability map
for a text segment of unknown origin.8 As a prepro-
cessing step, the segment is tokenized, punctuation
markers are removed and all words are converted to
lower case.
The identification process can be broken down in
three levels:
1. The probability map of a text segment depends
on the probability maps of the words contained
in the segment.
2. The probability map of a word depends on the
probability maps of the derivations that yield
the word.
7Technically, we do not store the probability map, but the
sequence of rule variants involved in the derivation. The proba-
bility map is restored from this rule sequence at test time.
8The model does not require the material to be syntactically
well-formed. Although we use complete sentences to test the
system, any sequence of words is accepted.
1155
3. The probability map of a derivation depends on
the probability maps of the rules it consists of.
In practice, every word of a given text segment is
looked up in the lexicon. If this lookup does not suc-
ceed (either because its Standard German equivalent
did not appear in the TIGER treebank, or because the
rule base lacked a relevant rule), the word is skipped.
Otherwise, the lookup yields m derivations from m
different Standard German words.9 The lexicon al-
ready contains the probability maps of the derivations
(see 5.1), so that the third level does not need to be
discussed here. Let us thus explain the first two levels
in more detail, in reverse order.
5.3 Computing the probability map for a word
A dialectal word form may originate in different Stan-
dard German words. For example, the three deriva-
tions sind [VAFIN]
?
? si (valid only in Western di-
alects), sein [PPOSAT]
?
? si (in Western and Central
dialects), and sie [PPER]
?
? si (in the majority of
Swiss German dialects) all lead to the same dialectal
form si.
Our system does not take the syntactic context
into account and therefore cannot determine which
derivation is the correct one. We approximate by
choosing the most probable one in each geographic
location. The probability map of a Swiss German
word w is thus defined as the pointwise maximum10
of all derivations leading to w, starting with different
Standard German words w( j)0 :
?
t?GSS
p(w | t) = max
j
p(w( j)0
?
? w | t)
This formula does not take into account the relative
frequency of the different derivations of a word. This
may lead to unintuitive results. Consider the two
derivations der [ART]
?
? dr (valid only in Western
dialects) and Dr. [NN]
?
? dr (valid in all dialects).
The occurrence of the article dr in a dialect text is a
good indicator for Western Swiss dialects, but it is
completely masked by the potential presence of the
9Theoretically, two derivations can originate at the same
Standard German word and yield the same Swiss German word,
but nevertheless use different rules. Our system handles such
cases as well, but we are not aware of such cases occurring with
the current rule base.
10Note that these derivations are alternatives and not joint
events. This is thus not a joint probability.
abreviation Dr. in all dialects. We can avoid this by
weighting the derivations by the word frequency of
w0: the article der is much more frequent than the
abreviation Dr. and is thus given more weight in the
identification task. This weighting can be justified
on dialectological grounds: frequently used words
tend to show higher interdialectal variation than rare
words.
Another assumption in the above formula is that
each derivation has the same discriminative poten-
tial. Again, this is not true: a derivation that is valid
in only 10% of the Swiss German dialect area is
much more informative than a derivation that is valid
in 95% of the dialect area. Therefore, we propose
to weight each derivation by the proportional size of
its validity area. The discriminative potential of a
derivation d is defined as follows:11
DP(d) = 1?
?t?GSS p(d | t)
|GSS|
The experiments in Section 6 will show the relative
impact of these two weighting techniques and of the
combination of both with respect to the unweighted
map computation.
5.4 Computing the probability map for a
segment
The probability of a text segment s can be defined as
the joint probability of all words w contained in the
segment. Again, we compute the pointwise product
of all word maps. In contrast to 5.1, we performed
some smoothing in order to prevent erroneous word
derivations from completely zeroing out the proba-
bilities. We assumed a minimum word probability of
? = 0.1 for all words in all geographic points:
?
t?GSS
p(s | t) =?
w?s
max(? , p(w | t))
Erroneous derivations were mainly due to non-
implemented lexical exceptions.
6 Experiments and results
6.1 Data
In order to evaluate our model, we need texts an-
notated with their gold dialect. We have chosen to
use the Alemannic Wikipedia as a main data source.
11d is a notational abreviation for w0
?
? wn.
1156
Wikipedia name Abbr. Pop. Surface
Baseldytsch BA 8% 1%
B?rnd?tsch BE 17% 13%
Seislert?tsch FR 2% 1%
Ostschwizert?tsch OS 14% 8%
Wallisertiitsch WS 2% 7%
Z?rit??tsch ZH 22% 4%
Table 1: The six dialect regions selected for our tests,
with their annotation on Wikipedia and our abreviation.
We also show the percentage of the German-speaking
population living in the regions, and the percentage of the
surface of the region relative to the entire country.
Figure 4: The localization of the six dialect regions used
in our study.
The Alemannic Wikipedia allows authors to write
articles in any dialect, and to annotate the articles
with their dialect. Eight dialect categories contained
more than 10 articles; we selected six dialects for our
experiments (see Table 1 and Figure 4).
We compiled a test set consisting of 291 sentences,
distributed across the six dialects according to their
population size. The sentences were taken from dif-
ferent articles. In addition, we created a development
set consisting of 550 sentences (100 per dialect, ex-
cept FR, where only 50 sentences were available).
This development set was also used to train the base-
line model discussed in section 6.2.
In order to test the robustness of our model, we
collected a second set of texts from various web sites
other than Wikipedia. The gold dialect of these texts
could be identified through metadata.12 This informa-
tion was checked for plausibility by the first author.
The Web data set contains 144 sentences (again dis-
12We mainly chose websites of local sports and music clubs,
whose localization allowed to determine the dialect of their con-
tent.
Wikipedia Web
Dialect P R F P R F
BA 34 61 44 27 61 37
BE 78 51 61 51 47 49
FR 28 71 40 10 33 15
OS 63 64 64 50 38 43
WS 58 100 74 14 33 20
ZH 77 62 69 77 41 53
W. Avg. 62 46
Table 2: Performances of the 5-gram model on Wikipedia
test data (left) and Web test data (right). The columns
refer to precision, recall and F-measure respectively. The
average is weighted by the relative population sizes of the
dialect regions.
tributed according to population size) and is thus
roughly half the size of the Wikipedia test set.
The Wikipedia data contains an average of 17.8
words per sentence, while the Web data shows 14.9
words per sentence on average.
6.2 Baseline: N-gram model
To compare our dialect ID model, we created a base-
line system that uses a character-n-gram approach.
This approach is fairly common for language ID and
has also been successfully applied to dialect ID (Bi-
adsy et al, 2009). However, it requires a certain
amount of training data that may not be available for
specific dialects, and it is uncertain how it performs
with very similar dialects.
We trained 2-gram to 6-gram models for each di-
alect with the SRILM toolkit (Stolcke, 2002), using
the Wikipedia development corpus. We scored each
sentence of the Wikipedia test set with each dialect
model. The predicted dialect was the one which ob-
tained the lowest perplexity.13
The 5-gram model obtained the best overall per-
formance, and results on the Wikipedia test set were
surprisingly good (see Table 2, leftmost columns).14
Note that in practice, 100% accuracy is not always
achievable; a sentence may not contain a sufficient
localization potential to assign it unambiguously to
one dialect.
13We assume that all test sentences are written in one of the
six dialects.
14All results represent percentage points. We omit decimal
places as all values are based on 100 or less data points. We did
not perform statistical significance tests on our data.
1157
However, we suspect that these results are due to
overfitting. It turns out that the number of Swiss
German Wikipedia authors is very low (typically,
one or two active writers per dialect), and that ev-
ery author uses distinctive spelling conventions and
writes about specific subjects. For instance, most
ZH articles are about Swiss politicians, while many
OS articles deal with religion and mysticism. Our
hypothesis is thus that the n-gram model learns to
recognize a specific author and/or topic rather than
a dialect. This hypothesis is confirmed on the Web
data set: the performances drop by 15 percentage
points or more (same table, rightmost columns; the
performance drops are similar for n = [2..6]).
In all our evaluations, the average F-measures for
the different dialects are weighted according to the
relative population sizes of the dialect regions be-
cause the size of the test corpus is proportional to
population size (see Section 6.1).15
We acknowledge that a training corpus of only 100
sentences per dialect provides limited insight into the
performance of the n-gram approach. We were able
to double the training corpus size with additional
Wikipedia sentences. With this extended corpus,
the 4-gram model performed better than the 5-gram
model. It yielded a weighted average F-measure
of 79% on Wikipedia test data, but only 43% on
Web data. The additional increase on Wikipedia data
(+17% absolute with respect to the small training
set), together with the decrease on Web data (?3%
absolute) confirms our hypothesis of overfitting. An
ideal training corpus should thus contain data from
several sources per dialect.
To sum up, n-gram models can yield good perfor-
mance even with similar dialects, but require large
amounts of training data from different sources to
achieve robust results. For many small-scale dialects,
such data may not be available.
6.3 Our model
The n-gram system presented above has no geo-
graphic knowledge whatsoever; it just consists of
six distinct language models that could be located
anywhere. In contrast, our model yields probability
15Roughly, this weighting can be viewed as a prior (the proba-
bility of the text being constant):
p(dialect | text) = p(text | dialect)? p(dialect)
maps of German-speaking Switzerland. In order to
evaluate its performance, we thus had to determine
the geographic localization of the six dialect regions
defined by the Wikipedia authors (see Table 1). We
defined the regions according to the respective can-
ton boundaries and to the German-French language
border in the case of bilingual cantons. The result of
this mapping is shown in Figure 4.
The predicted dialect region of a sentence s is de-
fined as the region in which the most probable point
has a higher value than the most probable point in
any other region:
Region(s) = arg max
Region
(
max
t?Region
p(s | t)
)
Experiments were carried out for the four combi-
nations of the two derivation-weighting techniques
presented in Section 5.3 and for the two test sets
(Wikipedia and Web). Results are displayed in Ta-
bles 3 to 6. The majority of FR sentences were mis-
classified as BE, which reflects the geographic and
linguistic proximity of these regions.
The tables show that frequency weighting helps
on both corpora: the discriminative potential only
slightly improves performance on the web corpus.
Crucially, the two techniques are additive, so in
combination, they yield the best overall results. In
comparison with the baseline model, there is a per-
formance drop of about 16 percent absolute on
Wikipedia data. In contrast, our model is very ro-
bust and outperforms the baseline model on the Web
test set by about 7 percent absolute.
These results seem to confirm what we suggested
above: that the n-gram model overfitted on the small
Wikipedia training corpus. Nevertheless, it is still
surprising that our model has a lower performance
on Wikipedia than on Web data. The reason for this
discrepancy probably lies in the spelling conventions
assumed in the transformation rules: it seems that
Web writers are closer to these (implicit) spelling
conventions than Wikipedia authors. This may be
explained by the fact that many Wikipedia articles
are translations of existing Standard German articles,
and that some words are not completely adapted to
their dialectal form. Another reason could be that
Wikipedia articles use a proportionally larger amount
of proper nouns and low-frequency words which can-
1158
Wikipedia Web
Dialect P R F P R F
BA 41 19 26 80 22 35
BE 42 62 50 48 76 59
FR 0 0 0 17 33 22
OS 36 41 38 45 41 43
WS 3 14 5 8 33 13
ZH 65 33 44 62 37 46
W. Avg. 40 46
Table 3: Performances of the word-based model using
unweighted derivation maps.
Wikipedia Web
Dialect P R F P R F
BA 50 33 40 57 22 32
BE 47 60 53 60 79 68
FR 0 0 0 0 0 0
OS 29 31 30 46 50 48
WS 11 29 15 17 33 22
ZH 60 47 53 65 53 58
W. Avg. 44 53
Table 4: Performances of the word-based model using
derivation maps weighted by word frequency.
not be found in the lexicon and which therefore re-
duce the localization potential of a sentence.
However, one should note that the word-based di-
alect ID model is not limited on the six dialect regions
used for evaluation here. It can be used with any size
and number of dialect regions of German-speaking
Switzerland. This contrasts with the n-gram model
which has to be trained specifically on every dialect
region; in this case, the Swiss German Wikipedia
only contains two additional dialect regions with an
equivalent amount of data.
6.4 Variations
In the previous section, we have defined the predicted
dialect region as the one in which the most probable
point (maximum) has a higher probability than the
most probable point of any other region. The results
suggest that this metric penalizes small regions (BA,
FR, ZH). In these cases, it is likely that the most
probable point is slightly outside the region, but that
the largest part of the probability mass is still inside
the correct region. Therefore, we tested another ap-
proach: we defined the predicted dialect region as the
one in which the average probability is higher than
Wikipedia Web
Dialect P R F P R F
BA 34 31 32 38 17 23
BE 46 47 47 54 76 63
FR 11 14 13 20 33 25
OS 34 50 40 53 59 56
WS 5 14 7 0 0 0
ZH 47 27 34 75 43 55
W. Avg. 37 51
Table 5: Performances of the word-based model using
derivation maps weighted by their discriminative potential.
Wikipedia Web
Dialect P R F P R F
BA 46 28 35 33 11 17
BE 47 62 54 58 84 69
FR 0 0 0 20 33 25
OS 35 31 33 47 47 47
WS 8 29 13 14 33 20
ZH 63 53 58 66 51 58
W. Avg. 46 52
Table 6: Performances using derivation maps weighted by
word frequency and discriminative potential.
the average probability in any other region:
Region(s) = arg max
Region
(
?t?Region p(s | t)
|Region|
)
This metric effectively boosts the performance on
the smaller regions, but comes at a cost for larger
regions (Table 7). We also combined the two metrics
by using the maximum metric for the three larger
regions and the average metric for the three smaller
ones (the cutoff lies at 5% of the Swiss territory).
This combined metric further improves the perfor-
mance of our system while relying on an objective
measure of region surface.
We believe that region surface as such is not so
crucial for the metrics discussed above, but rather
serves as a proxy for linguistic heterogeneity. Geo-
graphically large regions like BE tend to have internal
dialect variation, and averaging over all dialects in
the region leads to low figures. In contrast, small
regions show a quite homogeneous dialect landscape
that may protrude over adjacent regions. In this case,
the probability peak is less relevant than the average
probability in the entire region. Future work will at-
tempt to come up with more fine-grained measures of
1159
Wikipedia Web
Dialect Max Avg Cmb Max Avg Cmb
BA 35 32 32 17 43 43
BE 54 39 54 69 54 69
FR 0 7 7 25 11 11
OS 33 23 33 47 49 47
WS 13 13 13 20 31 20
ZH 58 60 60 58 68 68
W. Avg. 46 40 47 52 55 58
Table 7: Comparison of different evaluation metrics. All
values refer to F-measures obtained with frequency and
discriminative potential-weighted derivation maps. Max
refers to the Maximum metric as used in Table 6. Avg
refers to the average metric, and Cmb is the combination
of both metrics depending on region surfaces. The under-
lined values in the Avg and Max columns represent those
used for the Cmb metric.
linguistic heterogeneity in order to test these claims.
7 Future work
In our experiments, the word-based dialect identifi-
cation model skipped about one third of all words
(34% on the Wikipedia test set, 39% on the Web
test set) because they could not be found in the lex-
icon. While our model does not require complete
lexical coverage, this figure shows that the system
can be improved. We see two main possibilities of
improvement. First, the rule base can be extended
to better account for lexical exceptions, orthographic
variation and irregular morphology. Second, a mixed
approach could combine the benefits of the word-
based model with the n-gram model. This would
require a larger, more heterogeneous set of training
material for the latter in order to avoid overfitting.
Additional training data could be extracted from the
web and automatically annotated with the current
model in a semi-supervised approach.
In the evaluation presented above, the task con-
sisted of identifying the dialect of single sentences.
However, one often has access to longer text seg-
ments, which makes our evaluation setup harder
than necessary. This is especially important in situ-
ations where a single sentence may not always con-
tain enough discriminative material to assign it to a
unique dialect. Testing our dialect identification sys-
tem on the paragraph or document level could thus
provide more realistic results.
8 Conclusion
In this paper, we have compared two empirical meth-
ods for the task of dialect identification. The n-gram
method is based on the approach most commonly
used in NLP: it is a supervised machine learning ap-
proach where training data of the type we need to
process is annotated with the desired outcome of the
processing.
Our second approach ? the main contribution of
this paper ? is quite different. The empirical compo-
nent consists in a collection of data (the SDS atlas)
which is not of the type we want to process, but rather
embodies some features of the data we ultimately
want to process. We therefore analyze this data in
order to extract empirically grounded knowledge for
more general use (the creation of the georeferenced
rules), and then use this knowledge to perform the
dialect ID task in conjunction with an unrelated data
source (the Standard German corpus).
Our choice of method was of course related to the
fact that few corpora, annotated or not, were avail-
able for our task. But beyond this constraint, we
think it may be well worthwhile for NLP tasks in
general to move away from a narrow machine learn-
ing paradigm (supervised or not) and to consider
a broader set of empirical resources, sometimes re-
quiring methods which are quite different from the
prevalent ones.
Acknowledgements
Part of this work was carried out during the first au-
thor?s stay at Columbia University, New York, funded
by the Swiss National Science Foundation (grant
PBGEP1-125929).
References
Fadi Biadsy, Julia Hirschberg, and Nizar Habash. 2009.
Spoken Arabic dialect identification using phonotactic
modeling. In EACL 2009 Workshop on Computational
Approaches to Semitic Languages, Athens.
S. Brants, S. Dipper, S. Hansen, W. Lezius, and G. Smith.
2002. The TIGER Treebank. In Proceedings of the
Workshop on Treebanks and Linguistic Theories, So-
zopol.
W. B. Cavnar and J. M. Trenkle. 1994. N-gram based
text categorization. In Proceedings of SDAIR?94, Las
Vegas.
1160
Eugen Dieth. 1986. Schwyzert?tschi Dial?ktschrift.
Sauerl?nder, Aarau, 2nd edition.
Rudolf Hotzenk?cherle, Robert Schl?pfer, Rudolf Tr?b,
and Paul Zinsli, editors. 1962-1997. Sprachatlas der
deutschen Schweiz. Francke, Berne.
Baden Hughes, Timothy Baldwin, Steven Bird, Jeremy
Nicholson, and Andrew MacKinlay. 2006. Recon-
sidering language identification for written language
resources. In Proceedings of LREC?06, Genoa.
N. Ingle. 1980. A language identification table. Technical
Translation International.
Gideon S. Mann and David Yarowsky. 2001. Multipath
translation lexicon induction via bridge languages. In
Proceedings of NAACL?01, Pittsburgh.
Radim R?ehu?r?ek and Milan Kolkus. 2009. Language
identification on the web: Extending the dictionary
method. In Computational Linguistics and Intelligent
Text Processing ? Proceedings of CICLing 2009, pages
357?368, Mexico. Springer.
Jonas Rumpf, Simon Pickl, Stephan Elspa?, Werner
K?nig, and Volker Schmidt. 2009. Structural analysis
of dialect maps using methods from spatial statistics.
Zeitschrift f?r Dialektologie und Linguistik, 76(3).
Charles Schafer and David Yarowsky. 2002. Inducing
translation lexicons via diverse similarity measures and
bridge languages. In Proceedings of CoNLL?02, pages
146?152, Taipei.
Yves Scherrer and Owen Rambow. 2010. Natural lan-
guage processing for the Swiss German dialect area. In
Proceedings of KONVENS?10, Saarbr?cken.
Yves Scherrer. 2007. Adaptive string distance measures
for bilingual dialect lexicon induction. In Proceedings
of ACL?07, Student Research Workshop, pages 55?60,
Prague.
Yves Scherrer. 2010. Des cartes dialectologiques
num?ris?es pour le TALN. In Proceedings of TALN?10,
Montr?al.
Andreas Stolcke. 2002. SRILM ? an extensible language
modeling toolkit. In Proceedings of ICSLP?02, pages
901?904, Denver.
1161
Proceedings of EMNLP 2011, Conference on Empirical Methods in Natural Language Processing, pages 30?38,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Syntactic transformations for Swiss German dialects
Yves Scherrer
LATL
Universite? de Gene`ve
Geneva, Switzerland
yves.scherrer@unige.ch
Abstract
While most dialectological research so far fo-
cuses on phonetic and lexical phenomena, we
use recent fieldwork in the domain of dia-
lect syntax to guide the development of mul-
tidialectal natural language processing tools.
In particular, we develop a set of rules that
transform Standard German sentence struc-
tures into syntactically valid Swiss German
sentence structures. These rules are sensitive
to the dialect area, so that the dialects of more
than 300 towns are covered. We evaluate the
transformation rules on a Standard German
treebank and obtain accuracy figures of 85%
and above for most rules. We analyze the most
frequent errors and discuss the benefit of these
transformations for various natural language
processing tasks.
1 Introduction
For over a century, dialectological research has fo-
cused on phonetic, lexical and morphological phe-
nomena. It is only recently, since the 1990s, that
syntax has gained the attraction of dialectologists.
As a result, syntactic data from field studies are now
available for many dialect areas. This paper explores
how dialect syntax fieldwork can guide the develop-
ment of multidialectal natural language processing
tools. Our goal is to transform Standard German
sentence structures so that they become syntactically
valid in Swiss German dialects.1
1Here, we do not take into account the phonetic, morpholog-
ical and lexical changes involved in generating the actual Swiss
German word forms. For such a model, see for example Scher-
rer and Rambow (2010a).
These transformations are accomplished by a set
of hand-crafted rules, developed and evaluated on
the basis of the dependency version of the Standard
German TIGER treebank. Ultimately, the rule set
can be used either as a tool for treebank transduction
(i.e. deriving Swiss German treebanks from Stan-
dard German ones), or as the syntactic transfer mod-
ule of a transfer-based machine translation system.
After the discussion of related work (Section 2),
we present the major syntactic differences between
Standard German and Swiss German dialects (Sec-
tion 3). We then show how these differences can
be covered by a set of transformation rules that ap-
ply to syntactically annotated Standard German text,
such as found in treebanks (Section 4). In Section
5, we give some coverage figures and discuss the
most common errors that result from these transfor-
mations. We conclude in Section 6.
2 Related work
One line of research in natural language processing
deals with parsing methods for dialects. Chiang et
al. (2006) argue that it is often easier to manually
create resources that relate a dialect to a standard
language than it is to manually create syntactically
annotated resources for the dialect itself. They in-
vestigate three approaches for parsing the Levantine
dialect of Arabic, one of which consists of transduc-
ing a Standard Arabic treebank into Levantine with
the help of hand-crafted rules. We agree with this
point of view: we devise transformation rules that
relate Swiss German dialects to Standard German.
In the case of closely related languages,2 different
2In any case, it is difficult to establish strict linguistic criteria
30
types of annotation projection have been proposed
to facilitate the creation of treebanks. See Volk and
Samuelsson (2004) for an overview of the problem.
In a rather different approach, Vaillant (2008)
presents a hand-crafted multi-dialect grammar that
conceives of a dialect as some kind of ?agreement
feature?. This allows to share identical rules across
dialects and differentiate them only where neces-
sary. We follow a similar approach by linking the
transformation rules to geographical data from re-
cent dialectological fieldwork.
Another line of research is oriented towards ma-
chine translation models for closely related lan-
guages. It is common in this field that minor syn-
tactic differences are dealt with explicitly. Corb??-
Bellot et al (2005) present a shallow-transfer sys-
tem for the different Romance languages of Spain.
Structural transfer rules account for gender change
and word reorderings. Another system (Homola and
Kubon?, 2005) covers several Slavonic languages of
Eastern Europe and confirms the necessity of shal-
low parsing except for the most similar language
pair (Czech-Slovak).
In contrast, statistical machine translation systems
have been proposed to translate closely related lan-
guages on a letter-by-letter basis (Vilar et al, 2007;
Tiedemann, 2009). However, the word reordering
capabilities of a common phrase-based model are
still required to obtain reasonable performances.
3 The main syntactic features of Swiss
German dialects
A general description of the linguistic particularities
of Swiss German dialects, including syntax, can be
found, for example, in Lo?tscher (1983). Some syn-
tactic case studies within the framework of Genera-
tive Grammar are presented in Penner (1995). Cur-
rently, a dialectological survey, under the name of
SADS (Syntaktischer Atlas der deutschen Schweiz),
aims at producing a syntactic atlas of German-
speaking Switzerland (Bucheli and Glaser, 2002).
Some preliminary results of this project are de-
scribed in Klausmann (2006).3
to distinguish ?dialects? from ?closely related languages?.
3We thank Elvira Glaser and her team for providing us ac-
cess to the SADS database. This work could not have been
carried out without these precious data.
There are two main types of syntactic differences
between Swiss German dialects and Standard Ger-
man. Some of the differences are representative of
the mainly spoken use of Swiss German. They do
not show much interdialectal variation, and they are
also encountered in other spoken varieties of Ger-
man. Other differences are dialectological in nature,
in the sense that they are specific to some subgroups
of Swiss German dialects and usually do not occur
outside of the Alemannic dialect group. This second
type of differences constitutes the main research ob-
ject of the SADS project. In the following subsec-
tions, we will show some examples of both types of
phenomena.
3.1 Features of spoken language
No preterite tense Swiss German dialects do not
have synthetic preterite forms and use (analytic) per-
fect forms instead (1a).4 Transforming a Standard
German preterite form is not trivial: the correct aux-
iliary verb and participle forms have to be generated,
and they have to be inserted at the correct place (in
the right verb bracket).
Standard German pluperfect is handled in the
same way: the inflected preterite auxiliary verb is
transformed into an inflected present auxiliary verb
and an auxiliary participle, while the participle of
the main verb is retained (1b). The resulting con-
struction is called double perfect.
(1) a. Wir gingen ins Kino.
?Wir sind ins Kino gegangen.
?We went to the cinema.?
b. als er gegangen war
? als er gegangen gewesen ist
?when he had gone?
No genitive case Standard German genitive case
is replaced by different means in Swiss German.
Some prepositions (e.g. wegen, wa?hrend ?because,
during?) use dative case instead of genitive. Other
prepositions become complex through the addi-
tion of a second preposition von (e.g. innerhalb
?within?). Verbs requiring a genitive object in Stan-
dard German generally use a dative object in Swiss
4Throughout this paper, the examples are given with Stan-
dard German words, but Swiss German word order. We hope
that this simplifies the reading for Standard German speakers.
31
German unless they are lexically replaced. Geni-
tive appositions are converted to PPs with von ?of?
in the case of non-human NPs (2a), or to a dative-
possessive construction with human NPs (2b).
(2) a. der Schatzmeister der Partei
? der Schatzmeister von der Partei
?the treasurer of the party?
b. das Haus des Lehrers
? dem Lehrer sein Haus
?the teacher?s house?,
litt. ?to the teacher his house?
Determiners with person names A third differ-
ence is the prevalent use of person names with deter-
miners, whereas (written) Standard German avoids
determiners in this context:
(3) a. Hans? der Hans ?Hans?
b. Frau Mu?ller? die Frau Mu?ller ?Miss M.?
3.2 Dialect-specific features
Verb raising When two or more verbal forms ap-
pear in the right verb bracket, their order is often
reversed with respect to Standard German. Several
cases exist. In Western Swiss dialects, the auxil-
iary verb may precede the participle in subordinate
clauses (4a). In all but Southeastern dialects, the
modal verb precedes the infinitive (4b).
Verb raising also occurs for full verbs with infini-
tival complements, like lassen ?to let? (4c). In this
case, the dependencies between lassen and its com-
plements cross those between the main verb and its
complements:
mich einen Apfel la?sst essen
Verb projection raising In the same contexts as
above, the main verb extraposes to the right along
with its complements (4d), (4e).
(4) a. dass er gegangen ist
? dass er ist gegangen
?that he has gone?
b. dass du einen Apfel essen willst
? dass du einen Apfel willst essen
?that you want to eat an apple?
c. dass du mich einen Apfel essen la?sst
? dass du mich einen Apfel la?sst essen
?that you let me eat an apple?
d. dass du einen Apfel essen willst
? dass du willst einen Apfel essen
?that you want to eat an apple?
e. dass du mich einen Apfel essen la?sst
? dass du mich la?sst einen Apfel essen
?that you let me eat an apple?
Prepositional dative marking In Central Swiss
dialects, dative objects are introduced by a dummy
preposition i or a (5a). However, this preposition is
not added if the dative noun phrase is already part of
a prepositional phrase (5b).
(5) a. der Mutter? i/a der Mutter
?the mother (dative)?
b. mit der Mutter? mit (*i/a) der Mutter
?with the mother?
Article doubling In adjective phrases that contain
an intensity adverb like ganz, so ?very, such?, the de-
terminer occurs either before the adverb as in Stan-
dard German, or after the adverb, or in both posi-
tions, depending on the dialect:
(6) ein ganz lieber Mann
? ganz ein lieber Mann
? ein ganz ein lieber Mann
?a very dear man?
Complementizer in wh-phrases Interrogative
subordinate clauses introduced by verbs like fragen
?to ask? may see the complementizer dass attached
after the interrogative adverb or pronoun.
Relative pronouns Nominative and accusative
relative pronouns are substituted in most Swiss Ger-
man dialects by the uninflected particle wo. In da-
tive (7a) or prepositional (7b) contexts, the particle
wo appears together with an inflected personal pro-
noun:
(7) a. dem? wo . . . ihm
b. mit dem? wo . . . mit ihm, wo . . . damit
Final clauses Standard German allows non-finite
final clauses with the complementizer um . . . zu ?in
order to?. In Western dialects, this complementizer
32
is rendered as fu?r . . . z. In Eastern dialects, a single
particle zum is used. An intermediate form zum . . . z
also exists.
Pronoun sequences In a sequence of accusative
and dative pronouns, the accusative usually precedes
in Standard German, whereas the dative precedes in
many Swiss German dialects:
(8) es ihm? ihm es ?it to him?
Predicative adjectives In Southwestern dialects,
predicative adjectives agree in gender and number
with the subject:
(9) er / sie / es ist alt
? er / sie / es ist alter / alte / altes
?he / she / it is old?
Copredicative adjectives A slightly different
problem is the agreement of copredicative adjec-
tives. A copredicative adjective5 relates as an at-
tribute to a noun phrase, but also to the predicate
of the sentence (see example below). In Northeast-
ern dialects, there is an invariable er-ending6 for all
genders and numbers. In Southern dialects, the co-
predicative adjective agrees in gender and number.
Elsewhere, the uninflected adjective form is used, as
in Standard German.
(10) Sie sollten die Milch warm trinken.
? Sie sollten die Milch warme Fem.Sg /
warmer Invar trinken.
?You should drink the milk warm.?
3.3 The SADS data
The SADS survey consists of four written ques-
tionnaires, each of which comprises about 30 ques-
tions about syntactic phenomena like the ones cited
above. They were submitted to 3185 informants in
383 inquiry points.7 For each question, the infor-
mants were asked to write down the variant(s) that
they deemed acceptable in their dialect.
5This phenomenon is also known as depictive secondary
predicate construction.
6This (reconstructed) ending is thought to be a frozen mas-
culine inflection marker; in practice, it is pronounced [@] or [a]
in the corresponding dialects.
7http://www.ds.uzh.ch/dialektsyntax/
eckdaten.html, accessed 8.6.2011.
Figure 1: The three maps show the geographical distribu-
tion of prepositional dative marking with a (top) and with
i (center). The bottom map shows the inquiry points in
which no preposition is added to dative NPs. The maps
are based on SADS question I/7. Larger circles represent
larger proportions of informants considering the respec-
tive variant as the most natural one.
The SADS data give us an overview of the syn-
tactic phenomena and their variants occurring in the
different Swiss German dialects. It is on the basis of
these data that we compiled the list of phenomena
presented above. More importantly, the SADS data
provide us with a mapping from variants to inquiry
points. It suffices thus to implement a small num-
ber of variants (between 1 and 5 for a typical phe-
nomenon) to obtain full coverage of the 383 inquiry
points. Figure 1 shows the geographical distribution
of the three variants of prepositional dative marking.
For a subset of syntactic phenomena, two types of
questions were asked:
33
? Which variants are acceptable in your dialect?
? Which variant do you consider the most natural
one in your dialect?
In the first case, multiple mentions were allowed.
Usually, dialect speakers are very tolerant in accept-
ing also variants that they would not naturally utter
themselves. In this sense, the first set of questions
can be conceived as a geographical model of dialect
perception, while the second set of questions rather
yields a geographical model of dialect production.
According to the task at hand, the transformation
rules can be used with either one of the data sets.
4 Transformation rules
4.1 The Standard German corpus
The transformation rules require morphosyntacti-
cally annotated Standard German input data. There-
fore, we had to choose a specific annotation format
and a specific corpus to test the rules on. We selected
the Standard German TIGER treebank (Brants et
al., 2002), in the CoNLL-style dependency format
(Buchholz and Marsi, 2006; Ku?bler, 2008).8 This
format allows a compact representation of the syn-
tactic structure. Figure 2 shows a sample sentence,
annotated in this format.
While we use the TIGER corpus for test and eval-
uation purposes in this paper, the rules are aimed to
be sufficiently generic so that they apply correctly
to any other corpus annotated according to the same
guidelines.
4.2 Rule implementation
We have manually created transformation rules for a
dozen of syntactic and morphosyntactic phenomena.
These rules (i) detect a specific syntactic pattern in a
sentence and (ii) modify the position, content and/or
dependency link of the nodes in that pattern. The
rules are implemented in the form of Python scripts.
As an example, let us describe the transformation
rule for article doubling. This rule detects the fol-
lowing syntactic pattern:9
8Thanks to Yannick Versley for making this version avail-
able to us.
9X symbolizes any type of node that possesses an article and
an adjective as dependents. In practice, X usually is a noun.
ART ADV
{ganz, sehr, so. . .}
ADJA X
The rule then produces the three valid Swiss Ger-
man patterns ? as said above, the transformation
rules may yield different output structures for dif-
ferent dialects. One of the three variants is identical
to the Standard German structure produced above.
In a second variant, the positions of the article and
the adverb are exchanged without modifying the de-
pendency links:
ADV ART ADJA X
This transformation yields non-projective depen-
dencies (i.e. crossing arcs), which are problematic
for some parsing algorithms. However, the original
TIGER annotations already contain non-projective
dependencies. Thus, there is no additional complex-
ity involved in the resulting Swiss German struc-
tures.
The third variant contains two occurrences of the
determiner, before and after the intensity adverb. We
chose to make both occurrences dependents of the
same head node:
ART ADV ART ADJA X
As mentioned previously, the SADS data tell us
which of the three variants is accepted in which
of the 384 inquiry points. This mapping is non-
deterministic: more than one variant may be ac-
cepted at a given inquiry point.
5 Evaluation
5.1 Corpus frequencies
In order to get an idea of the frequency of the syntac-
tic constructions mentioned in Section 3, we started
by searching the TIGER treebank for the crucial
syntactic patterns. Table 1 shows frequency counts
34
ID FORM LEMMA CPOSTAG POSTAG FEATS HEAD DEPREL
1 fu?r fu?r APPR PREP ? 4 PP
2 eine eine ART ART Acc.Sg.Fem 3 DET
3 Statistik Statistik NN N Acc.Sg.Fem 1 PN
4 reicht reichen VVFIN V 3.Sg.Pres.Ind 0 ROOT
5 das das PDS PRO Nom.Sg.Neut 4 SUBJ
6 nicht nicht PTKNEG PTKNEG ? 4 ADV
7 . . $. $. ? 0 ROOT
Figure 2: Example of a CoNLL-style annotated sentence. Each word (FORM) is numbered (ID), lemmatized
(LEMMA), annotated with two levels of part-of-speech tags (CPOSTAG and POSTAG), annotated with morpho-
logical information (FEATS) and with dependency relations. HEAD indicates the ID of the head word, and DEPREL
indicates the type of dependency relation. For example, the word at position 1 (fu?r) depends on the word at position 4
(reicht) by a PP relation.
Construction Sentences
Preterite tense 13439
Genitive case 15351
Person name determiners 5410
Verb raising 3246
Verb projection raising 2597
Prep. dative marking 2708
Article doubling 61
Compl. in wh-phrases 478
Relative pronouns 4619
Final clauses 629
Pronoun sequences 6
Predicative adjectives 2784
Total TIGER sentences 40000
Table 1: Number of sentences in the TIGER corpus that
trigger the mentioned transformation rule.
of the respective phenomena.10
This preliminary study led us to exclude phe-
nomena that could not be detected reliably because
the morphosyntactic annotations in TIGER were not
precise enough. For example, TIGER does not dis-
tinguish between copredicative (11a) and adverbial
(11b) uses of adjectives. Therefore, it is impossible
to automatically count the number of copredicative
adjectives, let alne perform the necessary dialectal
transformations.
10These figures should be taken with a grain of salt. First, the
TIGER corpus consists of newspaper text, which is hardly rep-
resentative of everyday use of Swiss German dialects. Second,
it is difficult to obtain reliable recall figures without manually
inspecting the entire corpus.
(11) a. Blitzblank ha?ngen die To?pfe an der
Ku?chenwand.
?The pots are hanging sparkling clean on
the kitchen wall.?
b. Ha?ufig ha?ngen die To?pfe an der Ku?chen-
wand.
?The pots frequently hang on the kitchen
wall.?
5.2 Results
For each syntactic construction, a development set
and a test set were extracted from the TIGER tree-
bank, each of them comprising at most 100 sen-
tences showing that construction. After achieving
fair performance on the development sets, the held-
out test data was manually evaluated.
We did not evaluate the accusative-dative pro-
noun sequences because of their small number of
occurrences. Predicative adjective agreement was
not evaluated because the author did not have native
speaker?s intuitions about this phenomenon.
Table 2 shows the accuracy of the rules on the test
data. Recall that some rules cover different dialec-
tal variants, each of which may show different types
of errors. In consequence, the performance of some
rules is indicated as an interval. Moreover, some di-
alectal variants do not require any syntactic change
of the Standard German source, yielding figures of
100% accuracy.
The evaluation was performed on variants, not on
inquiry points. The mapping between the variants
and the inquiry points is supported by the SADS data
and is not the object of the present evaluation.
35
Construction Accuracy
Preterite tense 89%
Genitive case 85?93%
Person name determiners 80%
Verb raising 96?100%
Verb projection raising 85?100%
Prep. dative marking 93?100%
Article doubling 100%
Compl. in wh-phrases 69?100%
Relative pronouns 86?99%
Final clauses 92?100%
Table 2: This table shows the accuracy of the transforma-
tions, manually evaluated on the test set.
The overall performance of the transformation
rules lies at 85% accuracy and above for most rules.
Four major error types can be distinguished.
Annotation errors The annotation of the TIGER
treebank has been done semi-automatically and is
not exempt of errors, especially in the case of out-
of-vocabulary words. These problems degrade the
performance of rules dealing with proper nouns. In
(12), the first name Traute is wrongly analyzed as a
preterite verb form traute ?trusted, wedded?, leading
to an erroneous placement of the determiner.
(12) Traute Mu?ller
? *traute die Mu?ller / die Traute Mu?ller
Imperfect heuristics Some rules rely on a syntac-
tic distinction that is not explicitly encoded in the
TIGER annotation. Therefore, we had to resort to
heuristics, which do not work well in all cases. For
example, the genitive replacement rule needs to dis-
tinguish human from non-human NPs. Likewise,
adding a complementizer to wh-phrases overgener-
ates because the TIGER annotation does not reliably
distinguish between clause-adjoined relative clauses
and interrogative clauses introduced as complement
of the main verb.
Conjunctions Many rules rely on the dependency
relation type (the DEPREL field in Figure 2). Ac-
cording to the CoNLL guidelines, the dependency
type is only encoded in the first conjunct of a con-
junction, but not in the second. As a result, the trans-
formations are often only applied to the first con-
junct. However, it should not be too difficult to han-
dle the most frequent types of conjunctions.
Word order errors Appositions and quotation
marks sometimes interfere with transformation rules
and lead to typographically or syntactically unfor-
tunate sentences. In other cases, the linguistic de-
scription is not very explicit. For example, in the
verb projection raising rule, we found it difficult to
decide which constituents are moved and which are
not. Moving polarity items is sometimes blocked
due to scope effects. Different types of adverbs also
tend to behave differently.
5.3 An example
In the previous section, we evaluated each syntac-
tic transformation rule individually. It is also pos-
sible to apply all rules in cascade. The following
example shows an original Standard German sen-
tence (13a) along with three dialectal variants, ob-
tained by the cascaded application of our transfor-
mation rules. The Mo?rschwil dialect (Northeast-
ern Switzerland, Canton St. Gallen) shows geni-
tive replacement and relative pronoun replacement
(13b). The Central Swiss dialect of Sempach (Can-
ton Lucerne) additionally shows prepositional dative
marking (13c), while the Guttannen dialect (South-
western Switzerland, Canton Berne) shows an in-
stance of verb raising (13d). All transformations are
underlined. Note again that the transformation rules
only produce Swiss German morphosyntactic struc-
tures, but do not include word-level adaptations. For
illustration, the last example (13e) includes word-
level translations and corresponds thus to the ?real?
dialect spoken in Mo?rschwil.
(13) a. Original: Einen besonderen Stellen-
wert verdient dabei die alarmierende Zahl
junger Menschen, die der PDS ihre
Stimme gegeben haben.
?Special importance should be paid to the
alarming number of young people who
have given their vote to the PDS.?
b. Mo?rschwil: Einen besonderen Stellen-
wert verdient dabei die alarmierende Zahl
von jungen Menschen, wo der PDS ihre
Stimme gegeben haben.
c. Sempach: Einen besonderen Stellen-
36
wert verdient dabei die alarmierende Zahl
von jungen Menschen, wo i der PDS ihre
Stimme gegeben haben.
d. Guttannen: Einen besonderen Stellen-
wert verdient dabei die alarmierende Zahl
von jungen Menschen, wo der PDS ihre
Stimme haben gegeben.
e. Mo?rschwil (?real?): En bsondere Stelle-
wert vedient debii di alarmierend Zahl vo
junge Mensche, wo de PDS iri Stimm ggee
hend.
6 Conclusion and future work
We have shown that a small number of manually
written transformation rules can model the most im-
portant syntactic differences between Standard Ger-
man and Swiss German dialects with high levels of
accuracy. Data of recent dialectological fieldwork
provides us with a list of relevant phenomena and
their respective geographic distribution patterns, so
that we are able to devise the unique combination
of transformation rules for more than 300 inquiry
points.
A large part of current work in natural language
processing deals with inferring linguistic structures
from raw textual data. In our setting, this work
has already been done by the dialectologists: by de-
vising questionnaires of the most important syntac-
tic phenomena, collecting data from native dialect
speakers and synthesizing the results of the survey
in the form of a database. Relying on this work al-
lows us to obtain precise results for a great variety of
dialects, where machine learning techniques would
likely run into data sparseness issues.
The major limitation we found with our ap-
proach is the lacking precision (for our purposes) of
the Standard German treebank annotation. Indeed,
some of the syntactic distinctions that are made in
Swiss German dialects are not relevant from a purely
Standard German point of view, and have therefore
not been distinguished in the annotation. Additional
annotation could be added with the help of semantic
heuristics. For example, in the case of copredicative
adjectives (11), a semantic resource could easily tell
that pots can be sparkling clean but not frequent.
The purpose of our work is twofold. First, the
rule set can be viewed as part of a transfer-based
machine translation system from Standard German
to Swiss German dialects. In this case, one could
use a parser to analyze any Standard German sen-
tence before applying the transformation rules. Sec-
ond, the rules allow to transform the manually anno-
tated sentences of a Standard German treebank in or-
der to automatically derive Swiss German treebanks.
Such treebanks ? even if they are of lower quality
than manually annotated ones ? could then be used
to train statistical models for Swiss German part-of-
speech tagging or full parsing. Moreover, they could
be used to train statistical machine translation mod-
els to translate out of the dialects into Standard Ger-
man.11
Both lines of research will be tested in future
work. In addition, the rules presented here only deal
with syntactic transformations. Word-level transfor-
mations (phonetic, lexical and morphological adap-
tations) will have to be dealt with by other means.
Furthermore, we would like to test if syntactic
patterns can be used successfully for dialect identi-
fication, as this has been done with lexical and pho-
netic cues in previous work (Scherrer and Rambow,
2010b).
Another aspect of future research concerns the
type of treebank used. The TIGER corpus consists
of newspaper texts, which is hardly a genre fre-
quently used in Swiss German. Spoken language
texts would be more realistic to translate. The Tu?Ba-
D/S treebank (Hinrichs et al, 2000) provides syntac-
tically annotated speech data, but its lack of morpho-
logical annotation and its diverging annotation stan-
dard have prevented its use in our research for the
time being.
References
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang
Lezius, and George Smith. 2002. The TIGER Tree-
bank. In Proceedings of the Workshop on Treebanks
and Linguistic Theories, Sozopol.
Claudia Bucheli and Elvira Glaser. 2002. The syn-
tactic atlas of Swiss German dialects: empirical and
11While nearly all speakers of Swiss German also understand
Standard German, the inverse is not the case. Hence, a ma-
chine translation system would be most useful for the dialect-to-
standard direction. The lack of parallel training data and syntac-
tic resources for the dialect side prevented the creation of such
a system until now.
37
methodological problems. In Sjef Barbiers, Leonie
Cornips, and Susanne van der Kleij, editors, Syntac-
tic Microvariation, volume II. Meertens Institute Elec-
tronic Publications in Linguistics, Amsterdam.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of the 10th Conference on Computational
Natural Language Learning (CoNLL-X), pages 149?
164, New York City.
David Chiang, Mona Diab, Nizar Habash, Owen Ram-
bow, and Safiullah Shareef. 2006. Parsing Arabic di-
alects. In EACL?06: Proceedings of the Eleventh Con-
ference of the European Chapter of the Association for
Compuational Linguistics, pages 369?376, Trento.
Antonio M. Corb??-Bellot, Mikel L. Forcada, Sergio Ortiz-
Rojas, Juan Antonio Pe?rez-Ortiz, Gema Ram??rez-
Sa?nchez, Felipe Sa?nchez-Mart??nez, In?aki Alegria,
Aingeru Mayor, and Kepa Sarasola. 2005. An open-
source shallow-transfer machine translation engine for
the Romance languages of Spain. In Proceedings of
EAMT?05, pages 79?86, Budapest.
Erhard W. Hinrichs, Julia Bartels, Yasuhiro Kawata, Valia
Kordoni, and Heike Telljohann. 2000. The Tu?bingen
treebanks for spoken German, English, and Japanese.
In Wolfgang Wahlster, editor, Verbmobil: Foundations
of Speech-to-Speech Translation. Springer, Berlin.
Petr Homola and Vladislav Kubon?. 2005. A machine
translation system into a minority language. In Pro-
ceedings of RANLP?05, Borovets.
Hubert Klausmann, editor. 2006. Raumstrukturen im
Alemannischen. Neugebauer, Graz/Feldkirch.
Sandra Ku?bler. 2008. The PaGe 2008 shared task on
parsing German. In Proceedings of the Workshop on
Parsing German, pages 55?63, Columbus, Ohio.
Andreas Lo?tscher. 1983. Schweizerdeutsch. Geschichte,
Dialekte, Gebrauch. Huber, Frauenfeld.
Zvi Penner, editor. 1995. Topics in Swiss German Syn-
tax. Peter Lang, Bern.
Yves Scherrer and Owen Rambow. 2010a. Natural lan-
guage processing for the Swiss German dialect area.
In Proceedings of KONVENS?10, Saarbru?cken.
Yves Scherrer and Owen Rambow. 2010b. Word-based
dialect identification with georeferenced rules. In Pro-
ceedings of EMNLP 2010, Cambridge, MA.
Jo?rg Tiedemann. 2009. Character-based PSMT
for closely related languages. In Proceedings of
EAMT?09, pages 12 ? 19, Barcelona.
Pascal Vaillant. 2008. A layered grammar model: Using
tree-adjoining grammars to build a common syntactic
kernel for related dialects. In TAG+9 2008 ? The Ninth
International Workshop on Tree Adjoining Grammars
and Related Formalisms, pages 157?164, Tu?bingen.
David Vilar, Jan-Thorsten Peter, and Hermann Ney.
2007. Can we translate letters? In Proceedings of the
Second Workshop on Statistical Machine Translation,
pages 33?39, Prague.
Martin Volk and Yvonne Samuelsson. 2004. Bootstrap-
ping parallel treebanks. In COLING 2004 5th Inter-
national Workshop on Linguistically Interpreted Cor-
pora, pages 63?70, Geneva.
38
Proceedings of the EACL 2012 Joint Workshop of LINGVIS & UNCLH, pages 63?71,
Avignon, France, April 23 - 24 2012. c?2012 Association for Computational Linguistics
Recovering dialect geography from an unaligned comparable corpus
Yves Scherrer
LATL
Universite? de Gene`ve
Geneva, Switzerland
yves.scherrer@unige.ch
Abstract
This paper proposes a simple metric of di-
alect distance, based on the ratio between
identical word pairs and cognate word pairs
occurring in two texts. Different variations
of this metric are tested on a corpus contain-
ing comparable texts from different Swiss
German dialects and evaluated on the basis
of spatial autocorrelation measures. The vi-
sualization of the results as cluster dendro-
grams shows that closely related dialects
are reliably clustered together, while mul-
tidimensional scaling produces graphs that
show high agreement with the geographic
localization of the original texts.
1 Introduction
In the last few decades, dialectometry has
emerged as a field of linguistics that investigates
the application of statistical and mathematical
methods in dialect research. Also called quanti-
tative dialectology, one of its purposes is to dis-
cover the regional distribution of dialect similari-
ties from aggregated data, such as those collected
in dialectological surveys.
The work presented here aims to apply dialec-
tometric analysis and visualization techniques to
a different type of raw data. We argue that classi-
cal dialectological survey data are word-aligned
by design, whereas our data set, a comparable
multidialectal corpus, has to be word-aligned by
automatic algorithms.
We proceed in two steps. First, we present a
cognate identification algorithm that allows us to
extract cognate word pairs from the corpus. Then,
we measure how many of these cognate word
pairs are identical. This ratio gives us a measure
of dialectal distance between two texts that is then
shown to correlate well with geographic distance.
The visualization of the resulting data allows us to
recover certain characteristics of the Swiss Ger-
man dialect landscape.
The paper is structured as follows. In Section 2,
the multidialectal corpus is presented. We then
discuss how this corpus differs from classical di-
alectological data, and how we can use techniques
from machine translation to extract the relevant
data (Section 3). In Section 4, we define dialect
distance as a function of the number of cognate
word pairs and identical word pairs. Both types of
word pairs are in turn defined by different thresh-
olds of normalized Levenshtein distance. Sec-
tion 5 deals with the evaluation and visualization
of the resulting data, the latter in terms of clus-
tering and multi-dimensional scaling. We discuss
the results and conclude in Section 6.
2 Data: the Archimob corpus
The Archimob corpus used in our experiments is
a corpus of transcribed speech, containing texts
from multiple Swiss German dialects.
The Archimob project was started in 1998 as
an oral history project with the aim of gathering
and archiving the people?s memory of the Second
World War period in Switzerland.1 555 surviving
witnesses were interviewed in all Swiss language
regions. The interviews of the German-speaking
witnesses were conducted in their local dialect.
With the goal of obtaining spontaneous di-
alect data to complement ongoing work on di-
alect syntax (Bucheli and Glaser, 2002; Friedli,
2006; Steiner, 2006), researchers at the Univer-
1Archimob stands for ?Archives de la mobilisation?; see
www.archimob.ch.
63
BE1142: de vatter ?`sch lokomitiiffu?erer gs?`?` / de ?`sch dispensiert gs?`?` vom dienscht nattu?rlech / und
/ zwo schwo?schtere / hani ghaa / wobii ei gsch / eini gschtoorben ?`sch u di ander ?`sch ?`sch
ime autersheim / u soo bini ufgwachse ir lenggass / mit em / pruefsleer / mit wiiterbiudig
na?chheer / ( ? )
Translation: the father has been a train driver / he has been dispensed from military service of course /
and / two sisters / I have had / where one / one has died and the other is is in a home for the
elderly / this is how I have grown up in the Lenggass / with a / apprenticeship / with further
education afterwards / ( ? )
ZH1270: min vatter isch / eh eeh / schlosser ha?t er gleert / und und isch aber da?n schoffo?o?r woorde
dur en verwante wo bim S. z zu?ri / gschafft ha?t und de` ha?t gsait / chum tue doch umsattle
bim S. vediensch mee / und da?n ha?t de`e` schoffo?o?r gleert und das isch doozmaal ja na eener
en sa?ltene pruef gsii / da?n ha?t de` das gleert und ich bin schtolz gsii das min / vatter en / pruef
ghaa ha?t wo franzo?sischsch to?o?nt ha?t oder schoffo?o?r / ich han gfunde das seig en waansinige
pruef
Translation: my father has / eh eeh / been a locksmith apprentice / and and has then become a driver
through a relative who has worked at S. in Zurich and he said / come and switch jobs, at S.
you earn more / and then he was a driver apprentice and this was rather a rare job at that
time / so he learned that and I was proud that my / father / had a job which sounded French,
you know, chauffeur / I found that this was an extraordinary job
Figure 1: Excerpts of two informants? turns in the Archimob corpus. The excerpts contain identical cognate pairs
like ?vatter, vatter?, and non-identical cognate pairs like ??`sch, isch?.
sity of Zurich selected a subset of the Swiss Ger-
man Archimob interviews and transcribed them.2
The selection process ensured that only interviews
from non-mobile speakers (speakers that have not
spent long periods of their life outside of their na-
tive town) were retained, and that the most impor-
tant dialect areas of German-speaking Switzer-
land were represented.
As a result, 16 interviews were selected for
transcription, amounting to 26 hours of speech.
All texts were anonymized. In order to ensure
consistency, all texts were transcribed by the same
person.
The interviews were transcribed using the
spelling system of Dieth (1986). This is an ortho-
graphic transcription system which intends to be
as phonetically transparent as possible, while re-
maining readable for readers accustomed to Stan-
dard German orthography (see Figure 1 for two
examples). For instance, the Dieth guidelines dis-
tinguish ?` (IPA [I]) from i (IPA [i]), while Stan-
dard German spelling only uses i.
In our experiments, we discarded the inter-
viewer?s questions and only used the witnesses?
turns. The whole corpus contains 183 000 words,
with individual interviews ranging from 6 500 to
16 700 words. Excerpts of two interviews are
2The corpus is not yet publicly available, awaiting the
completion of further annotation layers.
shown in Figure 1. The place of residence of the
witness was given in the corpus metadata.
It should be stressed that our data set is very
small in comparison with other studies in the field:
it contains 16 data points (texts) from 15 different
locations. Moreover, some dialect areas are not
represented in the sample (e.g. Graubu?nden in the
South-East and Fribourg in the West).3 Therefore,
the goal of the present study cannot be to induce
a precise dialect landscape of German-speaking
Switzerland. Rather, we aim to find out if geo-
graphically close texts can be shown to be linguis-
tically close, and if the most important dialectal
divisions of German-speaking Switzerland are re-
flected in the classification of the texts.
3 Corpora and word alignment
3.1 Comparable corpora
The machine translation community generally
distinguishes between parallel and comparable
corpora (McEnery and Xiao, 2008). A parallel
corpus consists of a source text and its transla-
tions into other languages. Hence, the different
language versions share the same content and the
same order of paragraphs and sentences. On the
other hand, such corpora have been criticized for
containing ?translationese?, i.e., wording which
3For an overview of the geographic distribution of the
texts, see Figure 3.
64
is influenced by the grammatical and informa-
tional structure of the source text and which is not
necessarily representative of the target language.
In contrast, a comparable corpus is a collection
of original texts of different languages that share
similar form and content (typically, same genre,
same domain and same time period).
The Archimob corpus can be qualified as com-
parable: all texts deal with the same subject and
the same time period (life in Switzerland at the
outbreak of the Second World War), and they are
collected in the same way, in the form of oral in-
terviews guided by an interviewer.
3.2 Word alignment in dialectology
Dialectological analyses rely on word-aligned
data. Traditionally, dialectological data are col-
lected in surveys with the help of questionnaires.
A typical question usually intends to elicit the lo-
cal words or pronunciations of a given concept.
The mere fact that two responses are linked to the
same question number of the questionnaire suf-
fices to guarantee that they refer to the same con-
cept. This property leads us to consider dialecto-
logical survey data as word-aligned by design.
In contrast, the Archimob corpus is not aligned.
Again, algorithms for aligning words in parallel
and comparable corpora have been proposed in
the field of machine translation. For large par-
allel corpora, distributional alignment methods
based solely on cooccurrence statistics are suffi-
cient (Och and Ney, 2003; Koehn et al, 2007).
For comparable corpora, the order and frequency
of occurrence of the words cannot be used as
alignment cues. Instead, the phonetic and ortho-
graphic structures are used to match similar word
pairs (Simard et al, 1992; Koehn and Knight,
2002; Kondrak and Sherif, 2006). Obviously, this
approach only works for cognate word pairs ?
word pairs with a common etymology and simi-
lar surface forms. This task is known as cognate
identification.
In the next section, we detail how cognate iden-
tification is used to compute the distance between
different dialect versions of a comparable corpus.
4 Computing the linguistic similarity of
two comparable texts
The hypothesis put forward in this paper is that
the linguistic similarity of two comparable texts
can be approximated by the degree of similarity
of the cognate word pairs occurring in the texts.
Computing the similarity of two texts amounts to
the following two tasks:
1. Given two texts, extract the set of word pairs
that are considered cognates. This corre-
sponds to the cognate identification task pre-
sented above.
2. Given a set of cognate word pairs, determine
the proportion of word pairs that are consid-
ered identical.
The underlying intuition is that identically pro-
nounced cognate words account for evidence that
the two dialects are closely related, whereas dif-
ferently pronounced cognate words are evidence
that the two dialects are distant. Word pairs that
are not cognates are not relevant for our similarity
measure.
Let us illustrate the idea with an example:
(1) es schto`o`t n?`d
(2) wil si n?`d schtoot
Intuitively, two cognate word pairs can be found
in the texts (1) and (2): ?schto`o`t, schtoot? and
?n?`d, n?`d?.4 The words es, wil, si do not have cog-
nate equivalents in the other text. As a result, the
two texts have a similarity of 12 , one of the two
cognate pairs consisting of identical words.
In the example above, we have assumed infor-
mal meanings of cognate word pair and identical
word pair. In the following sections, we define
these concepts more precisely.
4.1 Identifying cognate word pairs
Most recently proposed cognate identification al-
gorithms are based on variants of Levenshtein dis-
tance, or string edit distance (Levenshtein, 1966;
Heeringa et al, 2006; Kondrak and Sherif, 2006).
Levenshtein distance is defined as the smallest
number of insertion, deletion and substitution op-
erations required to transform one string into an-
other.
(3)
b i i s c h p i i u
b i s c h p i l
0 0 1 0 0 0 0 0 1 1
4Accented and unaccented characters are considered as
different. See footnote 5.
65
Example (3) shows two words and the associated
operation costs. There are two deletion operations
and one substitution operation, hence Levenshtein
distance between biischpiiu and bischpil is 3.5
Among other proposals, Heeringa et al (2006)
suggest normalizing Levenshtein distance by the
length of the alignment. The underlying idea
is that a Levenshtein distance of 2 for two long
words does not mean the same as a Levenshtein
distance of 2 for two very short words. In exam-
ple (3), the length of the alignment is 10 (in this
case, it is equal to the length of the longer word).
Normalized Levenshtein distance is 310 = 0.3.
A cognate identification algorithm based on
normalized Levenshtein distance requires a
threshold such that only those word pairs whose
distance is below the threshold are considered
cognates. In order to identify sensible values for
this threshold, we classified all word pairs of the
corpus according to their distance. We evaluated
nine thresholds between 0.05 and 0.4 to see if they
effectively discriminate cognate pairs from non-
cognate pairs. The evaluation was done on the
basis of 100 randomly selected word pairs with
a normalized Levenshtein distance lower or equal
than the respective threshold.
In this evaluation, we distinguish between form
cognates ? words that represent the same inflected
forms of the same lemma ?, and lemma cognates
? words that represent different inflected forms of
the same lemma. Example (4) is a form cognate
pair: it shows two dialectally different realiza-
tions of the singular form of the Standard German
lemma Gemeinde ?municipality?. Example (5) is
only a lemma cognate pair: one of the word con-
tains the plural ending -e, while the other word is
a singular form.
(4) gmeind ? gmaind
(5) gmeind ? gmainde
Table 1 shows the results of this evaluation. As
the distance threshold increases, the proportion
of cognates drops while the proportion of non-
cognates rises. With thresholds higher than 0.25,
the number of non-cognates surpasses the number
5Note that we treat all characters in the same way: replac-
ing o by k yields the same cost as replacing it by u or by o`.
This simple approach may not be the optimal solution when
dealing with similar dialects. This issue will be addressed in
future work.
of cognates. We therefore expect the cognate de-
tection algorithm to work best below this thresh-
old.
Let us conclude this section by some additional
remarks about the evaluation:
? The distinction between form cognates and
lemma cognates cannot be easily opera-
tionalized with an automatic approach. For
instance, the correspondance u ? u? may be a
phonological one and distinguish two iden-
tical forms of different dialects. But it may
also be a morphological correspondence that
distinguishes singular from plural forms in-
dependently of the dialect. In the following
experiments, we treat both types of cognate
pairs in the same way.
? In practice, the reported figures are mea-
sures of precision. Recall may be estimated
by the number of cognates situated above a
given threshold. While we have not eval-
uated the entire distance interval, the given
figures suggest that many true cognates are
indeed found at high distance levels. This
issue may be addressed by improving the
string distance metric.
? Ambiguous words were not disambiguated
according to the syntactic context and the di-
alect. As a result, all identical word pairs
(threshold 0.00) are considered form cog-
nates, although some of them may be false
friends.
4.2 Identifying identical words
In common understanding, an identical word pair
is a pair of words whose Levenshtein distance is
0. In some of the following experiments, we adopt
this assumption.
However, we found it useful to relax this defi-
nition in order to avoid minor inconsistencies in
the transcription and to neglect the smallest di-
alect differences. Therefore, we also carried out
experiments where identical word pairs were de-
fined as having a normalized Levenshtein distance
of 0.10 or lower.
4.3 Experiments
Recall that we propose to measure the linguis-
tic similarity of two texts by the ratio of iden-
tical word pairs among the cognate word pairs.
66
Distance Word Form Lemma All Non- Non-
threshold pairs cognates cognates cognates cognates words
0.00 5230 100% 0% 100% 0% 0%
0.05 5244 98% 0% 98% 0% 2%
0.10 6611 94% 4% 98% 1% 1%
0.15 10674 79% 16% 95% 4% 1%
0.20 18582 55% 16% 71% 29% 0%
0.25 27383 48% 13% 61% 38% 1%
0.30 36002 40% 12% 52% 47% 1%
0.35 49011 29% 10% 39% 61% 0%
0.40 65955 20% 13% 33% 67% 0%
Table 1: Manual evaluation of the cognate identification task. Percentages are based on a random sample of 100
word pairs with a normalized Levenshtein distance below or equal to the given threshold. Form cognate and
lemma cognate counts are summed up in the ?All cognates? column. The interviewees sometimes made false
starts and stopped in the middle of the word; these incomplete words, together with obvious typing errors in the
transcription, are counted in the last column.
Cognate pairs as well as identical word pairs are
characterized by different thresholds of normal-
ized Levenshtein distance. We experiment with
thresholds of 0.20, 0.25, 0.30, 0.35 and 0.40 for
cognate word pairs, and with thresholds of 0 and
0.10 for identical word pairs.
4.4 Normalization by text length
A major issue of using comparable corpora is the
large variation in text length and vocabulary use.
This has to be accounted for in our experiments.
First, all counts refer to types of word pairs, not
tokens. We argue that the frequency of a word in
a given text depends too much on the content of
the text and is not truly representative of its di-
alect. Second, if few identical words are found,
this does not necessarily mean that the two texts
are dialectally distant, but may also be because
one text is much shorter than the other. Hence, the
proportion of identical words is normalized by the
number of cognate words contained in the shorter
of the two texts.
5 Evaluation and visualisation
By computing the linguistic distance for all
pairs of texts in our corpus, we obtain a two-
dimensional distance matrix. Recent dialectomet-
ric tradition provides several techniques to evalu-
ate and visualize the data encoded in this matrix.
First, one can measure how well the lin-
guistic distances correlate with geographic dis-
tances (Section 5.1). Second, one can group the
texts into maximally homogeneous clusters (Sec-
tion 5.2). Third, one can plot the texts as data
points on a two-dimensional graph and visually
compare this graph with the geographical loca-
tions of the texts (Section 5.3).
5.1 Numerical measures of spatial
autocorrelation
A general postulate of spatial analysis is that ?on
average, values at points close together in space
are more likely to be similar than points further
apart? (Burrough and McDonnell, 1998, 100).
This idea that the distance of attribute values cor-
relates with their geographical distance is known
as spatial autocorrelation. The same idea has
been coined the fundamental dialectological pos-
tulate by Nerbonne and Kleiweg (2005, 10): ?Ge-
ographically proximate varieties tend to be more
similar than distant ones.?
Here, we use this postulate to evaluate the dif-
ferent threshold combinations of our dialect sim-
ilarity measure: the higher a threshold combi-
nation correlates with geographic distance (i.e.,
places of residence of the interviewees), the better
it is able to discriminate the dialects. Here, the re-
sults obtained with two correlation measures are
reported.
Local incoherence has been proposed by Ner-
bonne and Kleiweg (2005). The idea of this mea-
sure is that the correlation between linguistic and
geographic distances is local and does not need to
hold over larger geographical distances. In prac-
tice, for every data point, the 8 linguistically most
67
similar points6 are inspected according to their
linguistic distance value. Then, the geographic
distance of these pairs of points is measured and
summed up. This means that high incoherence
values represent poor measurements, while lower
values stand for better results.
The Mantel-Test (Sokal and Rohlf, 1995, 813-
819) is a general statistical test which applies to
data expressed as dissimilarities. It is often used
in evolutionary biology and ecology, for example,
to correlate genetic distances of animal popula-
tions with the geographic distances of their range.
The Mantel coefficient Z is computed by com-
puting the Hadamard product of the two matri-
ces. The statistical significance of this coefficient
is obtained by a randomization test. A sample of
permutations is created, whereby the elements of
one matrix are randomly rearranged. The corre-
lation level depends on the proportion of samples
whose Z-value is higher than the Z-value of the
reference matrix. All experiments were carried
out with a sample size of 999 permutations, which
corresponds to a simulated p-value of 0.001.
Table 2 shows the results of both correlation
measures for all experiments. These results are
in line with the manual evaluation of Table 1. At
first, increasing the cognate pair threshold leads
to more data, and in consequence, to better re-
sults. Above 0.35 however, the added data is es-
sentially noise (i.e., non-cognate pairs), and the
results drop again.
According to local incoherence, the best thresh-
old combination is ?0.10, 0.35?. In terms of Man-
tel test correlation, the ?0.10, 0.25? threshold per-
forms slightly better. Adopting an identical pair
threshold of 0.00 results in slightly inferior corre-
lations.
5.2 Clustering
The distance matrix can also be used as input to a
clustering algorithm. Clustering has become one
of the major data analysis techniques in dialec-
tometry (Mucha and Haimerl, 2005), but has also
been used with plain text data in order to improve
information retrieval (Yoo and Hu, 2006).
Hierarchical clustering results in a dendrogram
which represents the distances between every two
data points as a tree. However, clustering is
6The restriction to 8 points is the key of the local compo-
nent of this measure. The exact value of this parameter has
been determined empirically by the authors of the measure.
Distance thresholds Local Mantel Test
Identical Cognate inc. r p
0.00 0.20 0.59 0.56 0.001
0.25 0.47 0.68 0.001
0.30 0.49 0.66 0.001
0.35 0.41 0.70 0.001
0.40 0.46 0.65 0.001
0.10 0.20 0.55 0.65 0.001
0.25 0.41 0.73 0.001
0.30 0.43 0.70 0.001
0.35 0.37 0.72 0.001
0.40 0.43 0.67 0.001
Table 2: Correlation values for the different experi-
ments. The first and second columns define each ex-
periment in terms of two Levenshtein distance thresh-
olds. For local incoherence, lower values are better.
For the Mantel test figures, we report the correlation
coefficient r as well as the significance level p.
known to be unreliable: small changes in the dis-
tance matrix may result in completely different
dendrograms. To counter this issue, noisy clus-
tering has been proposed (Nerbonne et al, 2008):
clustering is repeated 100 times, and at each run,
random amounts of noise are added to the differ-
ent cells of the distance matrix. This gives an
indication of the reliability of the resulting clus-
ters. Figure 2 shows a dendrogram obtained with
noisy clustering. We used both group average
and weighted average clustering algorithms, and
a noise level of 0.2.7 Figure 3 localizes the data
points on a geographical map. All clusters show a
reliability score of 92% or above.
Clustering allows us to recover certain charac-
teristics of the Swiss German dialect landscape.
First, texts from the same canton (whose IDs con-
tain the same two-letter abreviation) are grouped
together with high reliability. Second, the dendro-
gram shows ? albeit with lower reliability scores
? a three-fold East-West stratification with blue
regions in the West (BE), green regions in Cen-
tral Switzerland (AG, LU) and yellow areas in the
East (ZH, SZ, GL). The border between Western
and Central dialects roughly corresponds to the
so-called Bru?nig-Napf line. The border between
Central and Eastern varieties is also confirmed
by former dialectological research (Haas, 1982;
Hotzenko?cherle, 1984). Third, three dialects are
7These are the default settings of the Gabmap program
(Nerbonne et al, 2011).
68
AG1147
LU1195 100
AG1063
100
LU1261
100
BE1142
BE1170 100
BL1073
92
92
GL1048
GL1207 100
ZH1143
ZH1270 100
SZ1209
100
92
NW1007
100
BS1057
SG1198
100
VS1212
100
0.0 0.2 0.4 0.6
Figure 2: Dendrogram obtained with a threshold setting of ?0.10, 0.35?. The scale at the bottom of the graphics
represents the distance of the clusters, while the numbers on the vertical lines represent the reliability of the
clusters (i.e. in how many of the 100 runs a cluster has been found).
BS1057
BL1073
BE1142
BE1170
VS1212
SG1198
GL1207
GL1048
SZ1209
NW1007
LU1195
LU1261
ZH1143
ZH1270
AG1147
AG1063
Figure 3: Geographic localization of the Archimob
texts, according to the place of residence of the in-
terviewed persons. The colors represent the linguistic
distance between texts; they correspond to the colors
used in the dendrogram of Figure 2.
clearly considered as outliers: the Northwestern
dialect of Basel (BS1057), the Northeastern di-
alect of St. Gallen (SG1198), and most of all the
Southwestern Wallis dialect (VS1212). Again,
these observations are in line with common di-
alectological knowledge.
5.3 Multidimensional scaling
The Swiss German dialect landscape has been
known to feature major East-West divisions (see
above) as well as several levels of stratification
on the North-South axis. Our hypothesis is that
the linguistic distances represented in the distance
matrix should be able to recover this mainly two-
dimensional organization of Swiss German di-
alects. Since the distance matrix defines a multi-
dimensional space in which all data points (texts)
are placed, this space has to be reduced to two di-
mensions. For this purpose, we use multidimen-
sional scaling. If the linguistic distances are cor-
rectly defined and the multidimensional scaling
algorithm truly extracts the two main dimensions
of variation, the resulting two-dimensional graph
should be comparable with a geographic map.
Figure 4 shows the resulting graph for one ex-
periment. Figures 5 and 6 show the values of each
data point in grey levels for the two first dimen-
sions obtained by multi-dimensional scaling.
One observes that the localization of data
points in Figure 4 closely corresponds to their
geographic location (as illustrated in Figure 3):
the major North-South divisions as well as some
East-West divisions are clearly recovered.
More surprisingly, the two main dimensions of
multidimensional scaling correspond to diagonals
in geographic terms. The first dimension (Fig-
ure 5) allows to distinguish Northwestern from
Southeastern variants, while the second dimen-
sion (Figure 6) distinguishes Northeastern from
Southwestern variants. Instead of +-shaped di-
alect divisions put forward by traditional dialec-
tology, our approach rather finds X-shaped dialect
divisions.
6 Discussion and future work
We have proposed a simple measure that approx-
imates the linguistic distance between two texts
according to the ratio of identical words among
the cognate word pairs. The definitions of iden-
tical word pair and cognate word pair are op-
erationalized with fixed thresholds of normalized
69
BS1057
BL1073
BE1142
BE1170
VS1212
NW1007
GL1207
GL1048
ZH1143
SZ1209
SG1198
ZH1270LU1261
LU1195
AG1147
AG1063
Figure 4: Plot representing the first two dimensions
of multi-dimensional scaling applied to the experiment
with ?0.10, 0.35? thresholds.
Figure 5: Map representing the first dimension of
multi-dimensional scaling (same experiment as Fig. 4).
Figure 6: Map representing the second dimension of
multi-dimensional scaling (same experiment as Fig. 4).
Levenshtein distance. The resulting distance ma-
trix has been analyzed with correlation measures,
and visualized with clustering and multidimen-
sional scaling techniques. The visualizations rep-
resent the main characteristics of the Swiss Ger-
man dialect landscape in a surprisingly faithful
way.
The close relation obtained among texts from
the same canton may suggest that the distance
measure is biased towards proper nouns. For ex-
ample, two Zurich German texts are more likely
to use toponyms from the Zurich region than
a Bernese German text. If there are many of
these (likely identically pronounced) toponyms,
the similarity value will increase. However, man-
ual inspection of the relevant texts did not show
such an effect. Region-specific toponyms are rare.
The results suggest that a more fine-grained
variant of Levenshtein distance might be useful.
In the following paragraphs, we present several
improvements for future work.
The results suggest that a more fine-grained
variant of Levenshtein distance might improve the
precision and recall of the cognate detection al-
gorithm. Notably, it has been found that vowels
change more readily than consonant in closely re-
lated language varieties. In consequence, chang-
ing one vowel by another should be penalized less
than changing a vowel by a consonant (Mann and
Yarowsky, 2001). The same holds for accented
vs. non-accented characters. Complex graphemes
representing a single phoneme appear rather fre-
quently in the Dieth transcription system (e.g. for
long vowels) and should also be treated sepa-
rately.
We should also mention that the proposed
method likely faces a problem of scale. Indeed,
each word of each text has to be compared with
each word of each text. This is only manageable
with a small corpus like ours.
We conclude by pointing out a limitation of this
approach: the automatic alignment process based
on the concept of cognate pairs obviously only
works for phonetically related word pairs. This
contrasts with other dialectometric approaches
based on lexical differences, in whose data sets
different lemmas have been aligned. Future work
on the Archimob corpus shall add normalization
and lemmatization layers. This information could
be useful to improve word alignment beyond cog-
nate pairs.
70
Acknowledgments
The author wishes to thank Prof. Elvira Glaser,
Alexandra Bu?nzli, Anne Go?hring and Agnes
Kolmer (University of Zurich) for granting access
to the Archimob corpus and giving detailed infor-
mation about its constitution. Furthermore, the
anonymous reviewers are thanked for most help-
ful remarks.
References
Claudia Bucheli and Elvira Glaser. 2002. The syn-
tactic atlas of Swiss German dialects: empirical and
methodological problems. In Sjef Barbiers, Leonie
Cornips, and Susanne van der Kleij, editors, Syn-
tactic Microvariation, volume II. Meertens Institute
Electronic Publications in Linguistics, Amsterdam.
Peter A. Burrough and Rachael A. McDonnell. 1998.
Principles of Geographical Information Systems.
Oxford University Press, Oxford.
Eugen Dieth. 1986. Schwyzertu?tschi Diala?ktschrift.
Sauerla?nder, Aarau, 2nd edition.
Matthias Friedli. 2006. Der Komparativan-
schluss im Schweizerdeutschen ? ein raumbilden-
des Pha?nomen. In Hubert Klausmann, editor,
Raumstrukturen im Alemannischen, pages 103?108.
Neugebauer, Graz/Feldkirch.
Walter Haas, 1982. Die deutschsprachige Schweiz,
pages 71?160. Benziger, Zu?rich.
Wilbert Heeringa, Peter Kleiweg, Charlotte Gooskens,
and John Nerbonne. 2006. Evaluation of string dis-
tance algorithms for dialectology. In Proceedings
of the ACL 2006 Workshop on Linguistic Distances,
pages 51?62, Sydney, Australia.
Rudolf Hotzenko?cherle. 1984. Die Sprachlandschaf-
ten der deutschen Schweiz. Sauerla?nder, Aarau.
Philipp Koehn and Kevin Knight. 2002. Learning a
translation lexicon from monolingual corpora. In
Proceedings of the ACL 2002 Workshop on Unsu-
pervised Lexical Acquisition (SIGLEX 2002), pages
9?16, Philadelphia, PA.
Philipp Koehn, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Chris Dyer, Ondrej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical machine
translation. In Proceedings of the ACL 2007 demon-
stration session, Prague, Czech Republic.
Grzegorz Kondrak and Tarek Sherif. 2006. Evaluation
of several phonetic similarity algorithms on the task
of cognate identification. In Proceedings of the ACL
2006 Workshop on Linguistic Distances, pages 43?
50, Sydney, Australia.
Vladimir I. Levenshtein. 1966. Binary codes capa-
ble of correcting deletions, insertions, and reversals.
Soviet Physics Doklady, 10(8):707?710.
Gideon S. Mann and David Yarowsky. 2001. Mul-
tipath translation lexicon induction via bridge lan-
guages. In Proceedings of NAACL 2001, Pittsburgh,
PA, USA.
Tony McEnery and Richard Xiao. 2008. Parallel and
comparable corpora: What is happening? In Gu-
nilla Anderman and Margaret Rogers, editors, In-
corporating Corpora: The Linguist and the Trans-
lator, chapter 2, pages 18?31. Multilingual Matters,
Clevedon.
Hans-Joachim Mucha and Edgar Haimerl. 2005. Au-
tomatic validation of hierarchical cluster analysis
with application in dialectometry. In C. Weihs and
W. Gaul, editors, Classification ? the Ubiquitous
Challenge, pages 513?520. Springer, Berlin.
John Nerbonne and Peter Kleiweg. 2005. Toward a
dialectological yardstick. Journal of Quantitative
Linguistics, 5.
John Nerbonne, Peter Kleiweg, Wilbert Heeringa, and
Franz Manni. 2008. Projecting dialect differ-
ences to geography: Bootstrap clustering vs. noisy
clustering. In Christine Preisach, Lars Schmidt-
Thieme, Hans Burkhardt, and Reinhold Decker, ed-
itors, Data Analysis, Machine Learning, and Appli-
cations. Proceedings of the 31st Annual Meeting of
the German Classification Society, pages 647?654.
Springer, Berlin.
John Nerbonne, Rinke Colen, Charlotte Gooskens, Pe-
ter Kleiweg, and Therese Leinonen. 2011. Gabmap
? a web application for dialectology. Dialectologia,
Special Issue, II:65?89.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Michel Simard, George F. Foster, and Pierre Isabelle.
1992. Using cognates to align sentences in bilin-
gual corpora. In Proceedings of the Fourth Inter-
national Conference on Theoretical and Method-
ological Issues in Machine Translation (TMI 1992),
pages 67?81, Montre?al, Canada.
Robert R. Sokal and F. James Rohlf. 1995. Biometry:
the principles and practice of statistics in biological
research. W.H. Freeman, New York, 3rd edition.
Janine Steiner. 2006. Syntaktische Variation in der
Nominalphrase ? ein Fall fu?r die Dialektgeographin
oder den Soziolinguisten? In Hubert Klausmann,
editor, Raumstrukturen im Alemannischen, pages
109?115. Neugebauer, Graz/Feldkirch.
Illhoi Yoo and Xiaohua Hu. 2006. A comprehen-
sive comparison study of document clustering for a
biomedical digital library MEDLINE. In Proceed-
ings of the 6th ACM/IEEE-CS joint conference on
Digital libraries, JCDL ?06, pages 220?229, Chapel
Hill, NC, USA.
71
Proceedings of the 4th Biennial International Workshop on Balto-Slavic Natural Language Processing, pages 58?62,
Sofia, Bulgaria, 8-9 August 2013. c?2010 Association for Computational Linguistics
Modernizing Historical Slovene Words with Character-Based SMT
Yves Scherrer
ALPAGE
Universit? Paris 7 Diderot & INRIA
5 Rue Thomas Mann, Paris, France
yves.scherrer@inria.fr
Toma? Erjavec
Dept. of Knowledge Technologies
Jo?ef Stefan Institute
Jamova cesta 39, Ljubljana, Slovenia
tomaz.erjavec@ijs.si
Abstract
We propose a language-independent word
normalization method exemplified on
modernizing historical Slovene words.
Our method relies on character-based sta-
tistical machine translation and uses only
shallow knowledge. We present the rel-
evant lexicons and two experiments. In
one, we use a lexicon of historical word?
contemporary word pairs and a list of con-
temporary words; in the other, we only
use a list of historical words and one of
contemporary ones. We show that both
methods produce significantly better re-
sults than the baseline.
1 Introduction
A lot of recent work deals with detecting and
matching cognate words in corpora of closely re-
lated language varieties. This approach is also use-
ful for processing historical language (Piotrowski,
2012), where historical word forms are matched
against contemporary forms, thus normalizing the
varied and changing spelling of words over time.
Such normalization has a number of applications:
it enables better full-text search in cultural heritage
digital libraries, makes old texts more understand-
able to today?s readers and significantly improves
further text processing by allowing PoS tagging,
lemmatization and parsing models trained on con-
temporary language to be used on historical texts.
In this paper, we try to match word pairs of dif-
ferent historical stages of the Slovene language. In
one experiment we use character-based machine
translation to learn the character correspondences
from pairs of words. In the second experiment, we
start by extracting noisy word pairs from monolin-
gual1 lexicons; this experiment simulates a situa-
1For lack of a better term, we use ?monolingual? to refer
to a single diachronic state of the language, and ?bilingual?
to refer to two diachronic states of the language.
tion where bilingual data is not available.
The rest of this paper is structured as follows:
Section 2 presents related work, Section 3 details
the dataset used, Section 4 shows the experiments
and results, and Section 5 concludes.
2 Related Work
The most common approach to modernizing his-
torical words uses (semi-) hand-constructed tran-
scription rules, which are then applied to historical
words, and the results filtered against a contempo-
rary lexicon (Baron and Rayson, 2008; Scheible et
al., 2010; Scheible et al, 2011); such rules are of-
ten encoded and used as (extended) finite state au-
tomata (Reffle, 2011). An alternative to such de-
ductive approaches is the automatic induction of
mappings. For example, Kestemont et al (2010)
use machine learning to convert 12th century Mid-
dle Dutch word forms to contemporary lemmas.
Word modernization can be viewed as a special
case of transforming cognate words from one lan-
guage to a closely related one. This task has tradi-
tionally been performed with stochastic transduc-
ers or HMMs trained on a set of cognate word
pairs (Mann and Yarowsky, 2001). More re-
cently, character-based statistical machine trans-
lation (C-SMT) (Vilar et al, 2007; Tiedemann,
2009) has been proposed as an alternative ap-
proach to translating words between closely re-
lated languages and has been shown to outperform
stochastic transducers on the task of name translit-
eration (Tiedemann and Nabende, 2009).
For the related task of matching cognate pairs in
bilingual non-parallel corpora, various language-
independent similarity measures have been pro-
posed on the basis of string edit distance (Kon-
drak and Dorr, 2004). Cognate word matching has
been shown to facilitate the extraction of trans-
lation lexicons from comparable corpora (Koehn
and Knight, 2002; Kondrak et al, 2003; Fi?er and
Ljube?ic?, 2011).
58
For using SMT for modernizing historical
words, the only work so far is, to the best of our
knowledge, S?nchez-Mart?nez et al (2013).
3 The Dataset
In this section we detail the dataset that was used
in the subsequent experiments, which consists
of a frequency lexicon of contemporary Slovene
and training and testing lexicons of historical
Slovene.2
3.1 The Lexicon of Contemporary Slovene
Sloleks is a large inflectional lexicon of contem-
porary Slovene.3 The lexicon contains lemmas
with their full inflectional paradigms and with
the word forms annotated with frequency of oc-
currence in a large reference corpus of Slovene.
For the purposes of this experiment, we extracted
from Sloleks the list of its lower-cased word forms
(930,000) together with their frequency.
3.2 Corpora of Historical Slovene
The lexicons used in the experiments are con-
structed from two corpora of historical Slovene.4
The texts in the corpora are, inter alia marked up
with the year of publication and their IANA lan-
guage subtag (sl for contemporary Slovene al-
phabet and sl-bohoric for the old, pre-1850
Bohoric? alphabet). The word tokens are anno-
tated with the attributes nform, mform, lemma, tag,
gloss, where only the first two are used in the pre-
sented experiments.
The nform attribute contains the result of a sim-
ple normalization step, consisting of lower-casing,
removal of vowel diacritics (which are not used in
contemporary Slovene), and conversion of the Bo-
horic? alphabet to the contemporary one. Thus, we
do not rely on the C-SMT model presented below
to perform these pervasive, yet deterministic and
fairly trivial transformations.
The modernized form of the word, mform is the
word as it is (or would be, for extinct words) writ-
ten today: the task of the experiments is to predict
the correct mform given an nform.
2The dataset used in this paper is available under the
CC-BY-NC-SA license from http://nl.ijs.si/imp/
experiments/bsnlp-2013/.
3Sloleks is encoded in LMF and available under the CC-
BY-NC-SA license from http://www.slovenscina.
eu/.
4The data for historical Slovene comes from the IMP re-
sources, see http://nl.ijs.si/imp/.
Period Texts Words Verified
18B 8 21,129 21,129
19A 9 83,270 83,270
19B 59 146,100 146,100
? 75 250,499 250,499
Table 1: Size of goo300k corpus.
Period Texts Words Verified
18B 11 139,649 15,466
19A 13 457,291 17,616
19B 270 2,273,959 65,769
? 293 2,870,899 98,851
Table 2: Size of foo3M corpus.
The two corpora were constructed by sampling
individual pages from a collection of books and
editions of one newspaper, where the pages (but
not necessarily the publications) of the two cor-
pora are disjoint:5
? goo300k is the smaller, but fully manually
annotated corpus, in which the annotations of
each word have been verified;6
? foo3M is the larger, and only partially manu-
ally annotated corpus, in which only the more
frequent word forms that do not already ap-
pear in goo300k have verified annotations.
The texts have been marked up with the time
period in which they were published, e.g., 18B
meaning the second half of the 18th century. This
allows us to observe the changes to the vocabulary
in 50-year time slices. The sizes of the corpora are
given in Table 1 and Table 2.
3.3 Lexicons of Historical Slovene
From the two corpora we have extracted the
training and testing lexicons, keeping only words
(e.g., discarding digits) that have been manually
verified. The training lexicon, Lgoo is derived
from the goo300k corpus, while the test lexicon,
Lfoo is derived from the foo3M corpus and, as
5The corpora used in our experiments are slightly smaller
than the originals: the text from two books and one newspa-
per issue has been removed, as the former contain highly id-
iosyncratic ways of spelling words, not seen elsewhere, and
the latter contains a mixture of the Bohoric? and contempo-
rary alphabet, causing problems for word form normaliza-
tion. The texts older than 1750 have also been removed from
goo300k, as such texts do not occur in foo3M, which is used
for testing our approach.
6A previous version of this corpus is described in (Er-
javec, 2012).
59
Period Pairs Ident Diff OOV
18B 6,305 2,635 3,670 703
19A 18,733 12,223 6,510 2,117
19B 30,874 24,597 6,277 4,759
? 45,810 31,160 14,650 7,369
Table 3: Size of Lgoo lexicon.
Period OOV Pairs Ident Diff
18B 660 3,199 493 2,706
19A 886 3,638 1,708 1,930
19B 1,983 10,033 8,281 1,752
? 3,480 16,029 9,834 6,195
Table 4: Size of Lfoo lexicon.
mentioned, contains no ?nform, mform? pairs al-
ready appearing in Lgoo. This setting simulates
the task of an existing system receiving a new text
to modernize.
The lexicons used in the experiment contain en-
tries with nform, mform, and the per-slice frequen-
cies of the pair in the corpus from which the lexi-
con was derived, as illustrated in the example be-
low:
benetkah benetkah 19A:1 19B:1
aposteljnov apostolov 19A:1 19B:1
ar?ati ar?etu* 18B:2
The first example is a word that has not changed
its spelling (and was observed twice in the 19th
century texts), while the second and third have
changed their spelling. The asterisk on the third
example indicates that the mform is not present in
Sloleks. We exclude such pairs from the test lexi-
con (but not from the training lexicon) since they
will most likely not be correctly modernized by
our model, which relies on Sloleks. The sizes of
the two lexicons are given in Table 3 and Table 4.
For Lgoo we give the number of pairs including the
OOV words, while for Lfoo we exclude them; the
tables also show the numbers of pairs with iden-
tical and different words. Note that the summary
row has smaller numbers than the sum of the in-
dividual rows, as different slices can contain the
same pairs.
4 Experiments and Results
We conducted two experiments with the data de-
scribed above. In both cases, the goal is to cre-
ate C-SMT models for automatically modernizing
historical Slovene words. In each experiment, we
create three different models for the three time pe-
riods of old Slovene (18B, 19A, 19B).
The first experiment follows a supervised setup:
we train a C-SMT model on ?historical word,
contemporary word? pairs from Lgoo and test the
model on the word pairs of Lfoo. The second ex-
periment is unsupervised and relies on monolin-
gual data only: we match the old Slovene words
from Lgoo with modern Slovene word candidates
from Sloleks; this noisy list of word pairs then
serves to train the C-SMT model. We test again
on Lfoo.
4.1 Supervised Learning
SMT models consist of two main components: the
translation model, which is trained on bilingual
data, and the language model, which is trained
on monolingual data of the target language. We
use the word pairs from Lgoo to train the transla-
tion model, and the modern Slovene words from
Lgoo to train the language model.7 As said above,
we test the model on the word pairs of Lfoo.
The experiments have been carried out with the
tools of the standard SMT pipeline: GIZA++ (Och
and Ney, 2003) for alignment, Moses (Koehn et
al., 2007) for phrase extraction and decoding, and
IRSTLM (Federico et al, 2008) for language mod-
elling. After preliminary experimentation, we set-
tled on the following parameter settings:
? We have obtained the best results with a 5-
gram language model. The beginning and
the end of each word were marked by special
symbols.
? The alignments produced by GIZA++ are
combined with the grow-diag-final method.
? We chose to disable distortion, which ac-
counts for the possibility of swapping ele-
ments; there is not much evidence of this phe-
nomenon in the evolution of Slovene.
? We use Good Turing discounting to adjust the
weights of rare alignments.
? We set 20% of Lgoo aside for Minimum Error
Rate Training.
The candidates proposed by the C-SMT sys-
tem are not necessarily existing modern Slovene
words. Following Vilar et al (2007), we added a
7It is customary to use a larger dataset for the language
model than for the translation model. However, adding the
Sloleks data to the language model did not improve perfor-
mances.
60
Supervised Unsupervised
Period Total Baseline No lex filter With lex filter No lex filter With lex filter
18B 3199 493 (15.4%) 2024 (63.3%) 2316 (72.4%) 1289 (40.3%) 1563 (48.9%)
19A 3638 1708 (46.9%) 2611 (71.8%) 2941 (80.0%) 2327 (64.0%) 2644 (72.7%)
19B 10033 8281 (82.5%) 8707 (86.8%) 9298 (92.7%) 8384 (83.6%) 8766 (87.4%)
Table 5: Results of the supervised and the unsupervised experiments on Lfoo.
lexicon filter, which selects the first candidate pro-
posed by the C-SMT that also occurs in Sloleks.8
The results of these experiments, with and with-
out lexicon filter, are shown in Table 5. As a base-
line, we consider the words that are identical in
both language varieties. Without lexicon filter, we
obtain significant improvements over the baseline
for the first two time spans, but as the language va-
rieties become closer and the proportion of identi-
cal words increases, the SMT model becomes less
efficient. In contrast to Vilar et al (2007), we have
found the lexicon filter to be very useful: it im-
proves the results by nearly 10% absolute in 18B
and 19A, and by 5% in 19B.
4.2 Unsupervised Learning
The supervised approach requires a bilingual
training lexicon which associates old words with
modern words. Such lexicons may not be available
for a given language variety. In the second exper-
iment we investigate what can be achieved with
purely monolingual data. Concretely, we propose
a bootstrapping step to collect potential cognate
pairs from two monolingual word lists (the histor-
ical words of Lgoo, and Sloleks). We then train the
C-SMT system on these hypothesized pairs.
The bootstrapping step consists of searching,
for each historical word of Lgoo, its most similar
modern words in Sloleks.9 The similarity between
two words is computed with the BI-SIM measure
(Kondrak and Dorr, 2004). BI-SIM is a measure
of graphemic similarity which uses character bi-
grams as basic units. It does not allow crossing
alignments, and it is normalized by the length of
the longer string. As a result, this measure cap-
tures a certain degree of context sensitivity, avoids
8In practice, we generated 50-best candidate lists with
Moses, and applied the lexicon filter on that lists. In case
none of the 50 candidates occurs in Sloleks, the filter returns
the candidate with the best Moses score.
9In order to speed up the process and remove some noise,
we excluded hapaxes from Lgoo and all but the 20,000 most
frequent words from Sloleks. We also excluded words that
contain less than four characters from both corpora, since the
similarity measures proved unreliable on them.
counterintuitive alignments and favours associa-
tions between words of similar lengths. BI-SIM
is a language-independent measure and therefore
well-suited for this bootstrapping step.
For each old Slovene word, we keep the corre-
spondences that maximize the BI-SIM value, but
only if this value is greater than 0.8.10 For the
18B slice, this means that 812 out of 1333 histori-
cal words (60.9%) have been matched with at least
one modern word; 565 of the matches (69.6%, or
42.4% of the total) were correct.
These word correspondences are then used to
train a C-SMT model, analogously to the super-
vised approach. As for the language model, it is
trained on Sloleks, since the modernized forms
of Lgoo are not supposed to be known. Due to
the smaller training set size, MERT yielded un-
satisfactory results; we used the default weights of
Moses instead. The other settings are the same as
reported in Section 4.1. Again, we conducted ex-
periments for the three time slices. We tested the
system on the word pairs of the Lfoo lexicon, as
above. Results are shown in Table 5.
While the unsupervised approach performs sig-
nificantly less well on the 18B period, the differ-
ences gradually diminish for the subsequent time
slices; the model always performs better than the
baseline. Again, the lexicon filter proves useful in
all cases.
5 Conclusion
We have successfully applied the C-SMT ap-
proach to modernize historical words, obtaining
up to 57.0% (absolute) accuracy improvements
with the supervised approach and up to 33.5% (ab-
solute) with the unsupervised approach. In the fu-
ture, we plan to extend our model to modernize
entire texts in order to take into account possible
tokenization changes.
10This threshold has been chosen empirically on the basis
of earlier experiments, and allows us to eliminate correspon-
dences that are likely to be wrong. If several modern words
correspond to the same old word, we keep all of them.
61
Acknowledgements
The authors thank the anonymous reviewers for
their comments ? all errors, of course, remain
our own. This work has been partially funded
by the LabEx EFL (ANR/CGI), operation LR2.2,
by the EU IMPACT project ?Improving Access to
Text? and the Google Digital Humanities Research
Award ?Language models for historical Slove-
nian?.
References
Alistair Baron and Paul Rayson. 2008. VARD 2: A
tool for dealing with spelling variation in historical
corpora. In Proceedings of the Postgraduate Confer-
ence in Corpus Linguistics, Birmingham, UK. Aston
University.
Toma? Erjavec. 2012. The goo300k corpus of his-
torical Slovene. In Proceedings of the Eighth In-
ternational Conference on Language Resources and
Evaluation, LREC?12, Paris. ELRA.
Marcello Federico, Nicola Bertoldi, and Mauro Cet-
tolo. 2008. IRSTLM: an open source toolkit for
handling large scale language models. In Proceed-
ings of Interspeech 2008, Brisbane.
Darja Fi?er and Nikola Ljube?ic?. 2011. Bilingual lexi-
con extraction from comparable corpora for closely
related languages. In Proceedings of the Interna-
tional Conference on Recent Advances in Natural
Language Processing (RANLP?11), pages 125?131.
Mike Kestemont, Walter Daelemans, and Guy De
Pauw. 2010. Weigh your words ? memory-based
lemmatization for Middle Dutch. Literary and Lin-
guistic Computing, 25:287?301.
Philipp Koehn and Kevin Knight. 2002. Learning a
translation lexicon from monolingual corpora. In
Proceedings of the ACL 2002 Workshop on Unsu-
pervised Lexical Acquisition (SIGLEX 2002), pages
9?16, Philadelphia.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the As-
sociation for Computational Linguistics (ACL?07),
demonstration session, Prague.
Grzegorz Kondrak and Bonnie Dorr. 2004. Identifi-
cation of confusable drug names: A new approach
and evaluation methodology. In In Proceedings of
COLING 2004, pages 952?958.
Grzegorz Kondrak, Daniel Marcu, and Kevin Knight.
2003. Cognates can improve statistical translation
models. In Proceedings of NAACL-HLT 2003.
Gideon S. Mann and David Yarowsky. 2001. Mul-
tipath translation lexicon induction via bridge lan-
guages. In Proceedings of the Second Meeting
of the North American Chapter of the Association
for Computational Linguistics (NAACL 2001), pages
151?158, Pittsburgh.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Michael Piotrowski. 2012. Natural Language Pro-
cessing for Historical Texts. Synthesis Lectures on
Human Language Technologies. Morgan & Clay-
pool.
Ulrich Reffle. 2011. Efficiently generating correc-
tion suggestions for garbled tokens of historical lan-
guage. Natural Language Engineering, 17:265?
282.
Silke Scheible, Richard J. Whitt, Martin Durrell, and
Paul Bennett. 2010. Annotating a Historical Corpus
of German: A Case Study. In Proceedings of the
LREC 2010 Workshop on Language Resources and
Language Technology Standards, Paris. ELRA.
Silke Scheible, Richard J. Whitt, Martin Durrell, and
Paul Bennett. 2011. A Gold Standard Corpus of
Early Modern German. In Proceedings of the 5th
Linguistic Annotation Workshop, pages 124?128,
Portland, Oregon, USA, June. Association for Com-
putational Linguistics.
Felipe S?nchez-Mart?nez, Isabel Mart?nez-Sempere,
Xavier Ivars-Ribes, and Rafael C. Carrasco. 2013.
An open diachronic corpus of historical Span-
ish: annotation criteria and automatic modernisa-
tion of spelling. Research report, Departament
de Llenguatges i Sistemes Inform?tics, Universi-
tat d?Alacant, Alicante. http://arxiv.org/
abs/1306.3692.
J?rg Tiedemann and Peter Nabende. 2009. Translating
transliterations. International Journal of Computing
and ICT Research, 3(1):33?41. Special Issue of Se-
lected Papers from the fifth international conference
on computing and ICT Research (ICCIR 09), Kam-
pala, Uganda.
J?rg Tiedemann. 2009. Character-based PSMT for
closely related languages. In Proceedings of the
13th Conference of the European Association for
Machine Translation (EAMT 2009), pages 12 ? 19,
Barcelona.
David Vilar, Jan-Thorsten Peter, and Hermann Ney.
2007. Can we translate letters? In Proceedings of
the Second Workshop on Statistical Machine Trans-
lation, pages 33?39, Prague.
62
Proceedings of the First Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects, pages 30?38,
Dublin, Ireland, August 23 2014.
Unsupervised adaptation of supervised part-of-speech taggers
for closely related languages
Yves Scherrer
LATL-CUI
University of Geneva
Route de Drize 7, 1227 Carouge, Switzerland
yves.scherrer@unige.ch
Abstract
When developing NLP tools for low-resource languages, one is often confronted with the lack of
annotated data. We propose to circumvent this bottleneck by training a supervised HMM tagger
on a closely related language for which annotated data are available, and translating the words in
the tagger parameter files into the low-resource language. The translation dictionaries are created
with unsupervised lexicon induction techniques that rely only on raw textual data. We obtain a
tagging accuracy of up to 89.08% using a Spanish tagger adapted to Catalan, which is 30.66%
above the performance of an unadapted Spanish tagger, and 8.88% below the performance of
a supervised tagger trained on annotated Catalan data. Furthermore, we evaluate our model on
several Romance, Germanic and Slavic languages and obtain tagging accuracies of up to 92%.
1 Introduction
Recently, a lot of research has dealt with the task of creating part-of-speech taggers for languages which
lack manually annotated training corpora. This is usually done through some type of annotation pro-
jection from a language for which a tagger or an annotated corpus exists (henceforth called RL for
resourced language) towards another language that lacks such data (NRL for non-resourced language).
One possibility is to use word-aligned parallel corpora and transfer the tags from the RL to the NRL
along alignment links. Another possibility is to adapt the parameters of the RL tagger using bilingual
dictionaries or manually built transformation rules.
In this paper, we argue that neither parallel corpora nor hand-written resources are required if the RL
and the NRL are closely related. We propose a generic method for tagger adaptation that relies on three
assumptions which generally hold for closely related language varieties. First, we assume that the two
languages share a lot of cognates, i.e., word pairs that are formally similar and that are translations of
each other. Second, we suppose that the word order of both languages is similar. Third, we assume that
the set of POS tags is identical. Under these assumptions, we can avoid the requirements of parallel data
and of manual annotation.
Following Feldman et al. (2006), the reasoning behind our method is that a Hidden Markov Model
(HMM) tagger trained in a supervised way on RL data can be adapted to the NRL by translating the RL
words in its parameter files to the NRL. This requires a bilingual dictionary between RL words and NRL
words. In this paper, we create different HMM taggers using the bilingual dictionaries obtained with the
unsupervised lexicon induction methods presented in our earlier work (Scherrer and Sagot, 2014).
The paper is organized as follows. In Section 2, we present related work on tagger adaptation and
lexicon induction. In Section 3, we review Hidden Markov Models and their relevance for tagging and
for our method of tagger adaptation. Section 4 presents a set of different taggers in some detail and eval-
uates them on Catalan, using Spanish as RL. In Section 5, we demonstrate the validity of the proposed
approach by performing small-scale evaluations on a number of Romance, Germanic and Slavic lan-
guages: we transfer part-of-speech tags from Spanish to Aragonese, from Czech to Slovak and Sorbian,
from Standard German to Dutch and Palatine German. We conclude in Section 6.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
30
2 Related work
The task of creating part-of-speech taggers (and other NLP tools) for new languages without resorting to
manually annotated corpora has inspired a lot of recent research. The most popular line of work, initiated
by Yarowsky et al. (2001), draws on parallel corpora. They tag the source side of a parallel corpus with
an existing tagger, and then project the tags along the word alignment links onto the target side of the
parallel corpus. A new tagger is then trained on the target side, using aggressive smoothing to reduce the
noise caused by alignment errors.
In a similar setting, Das and Petrov (2011) use a more sophisticated graph-based projection algorithm
with label propagation to obtain high-precision tags for the target words. Follow-up work by Li et
al. (2012) uses tag dictionaries extracted from Wiktionary instead of parallel corpora, and T?ckstr?m
et al. (2013) attempt to combine these two data sources: the Wiktionary data provides constraints on
word types, whereas the parallel data is used to filter these constraints on the token level, depending on
the context of a given word occurrence. Duong et al. (2013) show that the original approach of Das
and Petrov (2011) can be simplified by focusing on high-confidence alignment links, thus achieving
equivalent performance without resorting to graph-based projection. The research based on parallel
corpora does not assume any particular etymological relationship between the two languages, but Duong
et al. (2013) note that their approach works best when the source and target languages are closely related.
Other approaches explicity model the case of two closely related languages, such as Feldman et al.
(2006). They train a tagger on the source language with standard tools and resources, and then adapt the
parameter files of that tagger to the target language using a hand-written morphological analyzer and a
list of cognate word pairs. Bernhard and Ligozat (2013) use a similar approach to adapt a German tagger
to Alsatian; they show that manually annotating a small list of closed-class words leads to considerable
gains in tagging accuracy. In a slightly different setting, Garrette and Baldridge (2013) show that taggers
for low-resource languages can be built from scratch with only two hours of manual annotation work.
Even though recent work on closely related and low-resource languages presupposes manually an-
notated data to some extent, we believe that it is possible to create a tagger for such languages fully
automatically. We adopt the general model proposed by Feldman et al. (2006), but use automatically
induced bilingual dictionaries to translate the source language words in the tagger parameter files. The
bilingual dictionaries are obtained with our unsupervised lexicon induction pipeline (Scherrer and Sagot,
2013; Scherrer and Sagot, 2014). This pipeline is inspired by early work by Koehn and Knight (2002),
who propose various methods for inferring translation lexicons using monolingual data.
Our lexicon induction pipeline is composed of three main steps. First, a list of formally similar word
pairs (cognate pairs) is extracted from monolingual corpora using the BI-SIM score (Kondrak and Dorr,
2004). Second, regularities occurring in these word pairs are learned by training and applying a character-
level statistical machine translation (CSMT) system (Vilar et al., 2007; Tiedemann, 2009). Third, cross-
lingual contextual similarity measures are used to induce additional word pairs. The main idea is to
extract word n-grams from comparable corpora of both languages and induce word pairs that co-occur
in the context of already known word pairs (Fung, 1998; Rapp, 1999; Fi?er and Ljube?i
?
c, 2011). In our
pipeline, the already known word pairs are those induced with CSMT.
In this paper, we extend our previous work (Scherrer and Sagot, 2014) in two aspects. First, we use
a more powerful HMM tagging model instead of the simple unigram tagger that insufficiently accounts
for the ambiguity in language. Second, we assess the impact of each lexicon induction step separately
rather than merely evaluating the final result of the pipeline.
3 HMM tagging
Hidden Markov Models (HMMs) are a simple yet powerful formal device frequently used for part-of-
speech tagging. A HMM describes a process that generates a joint sequence of tags and words by
decomposing the problem into so-called transitions and emissions. Transitions represent the probabilities
of a tag given the preceding tag(s), and emissions represent the probabilities of a word given the tag
assigned to it (Jurafsky and Martin, 2009).
31
The main advantage of HMM taggers for our work lies in the independence assumption between
transitions and emissions: crucially, the emission probability of a word only depends on its tag; it does
not depend on previous words or on previous tags. Assuming, as stated in the introduction, that the
word order is similar and the tag sets identical between the RL and the NRL, we argue that the transition
probabilities estimated on RL data are also valid for NRL. Only the emission probabilities have to be
adapted since RL words are formally different from NRL words.
Following earlier work (Feldman et al., 2006; Duong et al., 2013), we use the TnT tagger (Brants,
2000), an implementation of a trigram HMM tagger that includes smoothing and handling of unknown
words. In contrast to other implementations that use inaccessible binary files, TnT stores the estimated
parameters in easily modifiable plain text files.
3.1 Adapting emission counts
The goal of this work is to adapt an existing RL HMM tagger for a closely related NRL by replacing the
RL words in the emission parameters by the corresponding NRL words. Let us explain this process with
an example, using Spanish as RL and Catalan as NRL.
The TnT tagger creates an emission parameter file that contains, for each word, the tags and their
frequencies observed in the training corpus. For example, a tagger trained on Spanish data may contain
the following lines (word on the left, tag in the middle, frequency on the right):
(1)
intelectual AQ 11
intelectual NC 3
intelectuales AQ 3
intelectuales NC 7
Furthermore, suppose that we have a dictionary that associates Catalan words (left) with Spanish words
(center), where the weight (right) indicates the ambiguity level of the Catalan word, which is simply
defined as the inverse of the number of its Spanish translations:
(2)
intel?lectual intelectual 0.5
intel?lectual intelectuales 0.5
intel?lectuals intelectuales 1
A new Catalan emission file is then created by taking, for each Catalan word, the union of the tags of
its Spanish translations and by multiplying the tag weights with the dictionary weights. This yields the
following entries:
(3)
intel?lectual AQ (0.5 ?11)+(0.5 ?3) = 7
intel?lectual NC (0.5 ?3)+(0.5 ?7) = 5
intel?lectuals AQ 1 ?3 = 3
intel?lectuals NC 1 ?7 = 7
Or more formally: for each dictionary triple ?w
RL
,w
NRL
, f
d
? and each emission triple ?w
RL
, t, f
e
? with
matching w
RL
, add the new emission triple ?w
NRL
, t, f
d
? f
e
?. Merge emission triples with identical w
NRL
and t and sum their weights.
Finally, RL words occurring in the emission file that have not been translated to NRL (because no
appropriate word pair existed in the dictionary) are copied without modification to the new emission file.
In particular, this allows us to cover punctuation signs and numbers as well as named entities (which are
mostly spelled identically in both languages).
4 Tagger adaptation for Catalan
In this section, we present seven taggers for Catalan. Three of them (Sections 4.2 to 4.4) are supervised
taggers and serve as baseline taggers and as upper bounds. The four remaining taggers (Sections 4.6 to
4.9) are taggers created by adaptation from a Spanish tagger, using the method presented in Section 3.1;
32
they differ in the lexicons used to translate the emission counts. These four taggers represent the main
contribution of this paper. We start by listing the data used in our experiments.
4.1 Data
Most taggers presented below are initially trained on a part-of-speech annotated corpus of Spanish. We
use the Spanish part of the AnCora treebank (Taul? et al., 2008), which contains about 500 000 words.
The AnCora morphosyntactic annotation includes the main category (e.g. noun), the subcategory
(e.g. proper noun), and several morphological categories (e.g., gender, number, person, tense, mode),
yielding about 280 distinct labels. Since we are mainly interested in part-of-speech information, we
simplified these labels by taking into account the two first characters of each label, corresponding to
the main category and the subcategory. This simplified tagset contains 42 distinct labels, which is still
considerably more than the 12 tags of Petrov et al. (2012) commonly used in comparable settings.
All taggers need to be evaluated on a Catalan gold standard that shares the same tagset as Spanish. For
this purpose, we use the Catalan part of AnCora, which also contains about 500 000 words. We simplified
the tags in the same way as above. The Catalan part of AnCora is also used to train the supervised models
presented in Sections 4.3 and 4.4.
Finally, the lexicon induction algorithms require data on their own, which we present here for com-
pleteness. As in Scherrer and Sagot (2013), we use Wikipedia dumps consisting of 140M words for
Catalan and 430M words for Spanish.
1
4.2 Baseline: a Spanish tagger
Since Spanish and Catalan are closely related languages, one could presume that a lot of words are
identical, and that a tagger trained on Spanish data would yield acceptable performance on Catalan test
data without modifications. In order to test this hypothesis, we trained a TnT tagger on Spanish AnCora
and tested it on Catalan AnCora. We obtained a tagging accuracy of 58.42% only, which suggests that
this approach is clearly insufficient. (The results of all experiments are summed up in Table 1.) For
comparison, Feldman et al. (2006) obtain 64.5% accuracy on the same languages with a smaller training
corpus (100k instead of 500k words), but also with a smaller tagset (14 instead of 42).
We view this model as a baseline that we expect to beat with the adaptation methods.
4.3 Upper bound 1: a supervised Catalan tagger
The upper bound of the Catalan tagging experiments is represented by a tagger created under ideal data
conditions: a tagger trained in a supervised way on an annotated Catalan corpus. We train a TnT tagger
on Catalan AnCora and test it on the same corpus, using 10-fold cross-validation to avoid having the
same sentences in the training and the test set. This yields an averaged accuracy value of 97.96%.
For comparison, Feldman et al. (2006) obtain 97.5% accuracy on their dataset. More recently, Petrov
et al. (2012) report an accuracy of 98.5% by training on the CESS-ECE corpus, but do not mention the
tagging algorithm used. In any case, our result obtained with TnT can be considered close to state-of-
the-art performance on Catalan.
4.4 Upper bound 2: a tagger with Spanish transition counts and Catalan emission counts
We introduce a second upper bound that shares the assumption of structural similarity underlying the
adaptation-based models. Concretely, we combine the transition probabilities from the baseline Spanish
tagger (Section 4.2) with the emission probabilities of the supervised Catalan tagger (Section 4.3). The
resulting tagger is evaluated again on Catalan AnCora using 10-fold cross-validation. We get an accuracy
value of 97.66%, or just 0.3% absolute below the supervised tagger of Section 4.3.
2
This suggests that
the transition probabilities are indeed very similar between the two languages, and that they can safely
be kept constant in the adaptation-based models presented below.
1
This is not exactly a realistic setting for the intended use for low-resource languages. However, Section 5 will illustrate the
performance of the proposed models on smaller data sets. Note also that the lexicon induction methods do not require the two
corpora to be of similar size.
2
This difference is significant: ?
2
(1;N = 1064002) = 109.9747799; p < 0.01.
33
Cognate pair extraction
using BI-SIM score
Cognate pair extraction
using CSMT model
Word pair extraction
using contextual similarity
Tagger 1 (4.6)
Tagger 2 (4.7)
Tagger 3 (4.8) /
S&S unigram tagger
Figure 1: Flowchart of the lexicon induction pipeline and of the resulting taggers.
4.5 Lexicon induction methods for adaptation-based taggers
The adaptation-based taggers presented in Sections 4.6 to 4.8 differ in the bilingual dictionaries used to
adapt the emission counts. These dictionaries have been created using the pipeline of Scherrer and Sagot
(2014), which we summarize in this section (see Figure 1).
The pipeline starts with a cognate pair extraction step that uses the BI-SIM score to identify likely
cognate pairs. The result of this step is used as training data for the second step, in which a CSMT model
is trained to identify likely cognate pairs even more reliably. The result of the second step is in turn used
as seed data for the third step, in which additional word pairs are extracted on the basis of contextual
similarity. Scherrer and Sagot (2014) create a single unigram tagger (abreviated S&S in Figure 1) with
the union of the word pairs obtained in the second and third steps (plus additional clues like word identity
and suffix analysis, which are not required here).
The three steps are evaluated separately: Tagger 1 relies on the lexicon induced in the first step; Tagger
2 relies on the lexicon induced in the second step; Tagger 3 relies on the union of the lexicons induced
in the second and third steps.
4.6 Tagger 1: cognate pairs induced with BI-SIM score
As first step of the lexicon induction pipeline, word lists are extracted from both Wikipedia corpora,
and short words (words with less than 5 characters) as well as rare words (words accounting for the
lowest 10% of the frequency distribution) are removed. Then, the BI-SIM score is computed between
each Catalan word w
ca
and each Spanish word w
es
. For each w
ca
, we keep the ?w
ca
,w
es
? pair(s) that
maximize(s) the BI-SIM value, provided it is above the empirically chosen threshold of 0.8. When a w
ca
is associated with several w
es
, we keep all of them. This creates a list of cognate pairs, albeit a rather
noisy one since it does not take into account regular correspondences between languages, but merely
counts letter bigram differences.
Tagger 1, the first adaptation-based tagger, is created by replacing the Spanish emission counts with
their Catalan equivalents using the list of cognate pairs. Tagger 1 yields an accuracy of 68.32%, which is
a full 10% higher than the baseline. This improvement is surprisingly high, as the cognate list is not only
noisy, but also incomplete: only 17.91% of the words in the emission file could be translated with it.
4.7 Tagger 2: cognate pairs induced with CSMT
In this model, the Spanish emission counts are replaced using the list of cognate pairs obtained in the
second step of the lexicon induction pipeline.
We train a CSMT system on the list of potential cognate pairs of the first step. We then apply this
system to translate each Catalan word again into Spanish. We assume that the CSMT system learns
useful generalizations about the relationship between Catalan and Spanish words, which the generic BI-
SIM measure was not able to make. Moreover, the CSMT system is able to translate Catalan words even
34
Baseline Tagger 1 Tagger 2 Tagger 3 Tagger 4
Upper Upper
bound 2 bound 1
Tagging accuracy 58.42% 68.32% 72.32% 88.72% 89.08% 97.66% 97.96%
Translated words 17.91% 64.03% 65.62%
Table 1: Results of the Catalan tagging experiments. The first line reports tagging accuracies of the
different taggers. The second line shows ? where applicable ? how many words of the emission files
could be translated.
if their Spanish translations have not been seen, on the basis of the character correspondences observed
in other words.
This new dictionary allowed us to translate 64.03% of the words in the emission file. In consequence,
the resulting tagger shows improved performance compared with Tagger 1: its accuracy lies at 72.32%,
suggesting that the CSMT system yields a dictionary that is at the same time more precise and more
complete than the one obtained with BI-SIM in the previous step.
4.8 Tagger 3: word pairs induced with CSMT and context similarity
In previous work (Scherrer and Sagot, 2014), we have argued that lexicon induction methods based on
formal similarity alone are not sufficient, for the following reasons: (1) even in closely related languages,
not all word pairs are cognates; (2) high-frequency words are often related through irregular phonetic
correspondences; (3) pairs of short words may just be too hard to predict on the basis of formal criteria
alone; (4) formal similarity methods are prone to inducing false friends, i.e., words that are formally
similar but are not translations of each other. For these types of words, we have proposed a different
approach that relies on contextual similarity.
We extract 3-gram and 4-gram contexts from both languages and form context pairs whenever the first
and the last word pairs figure in the dictionary obtained with CSMT, allowing the word pair(s) in the
center to be newly inferred. Several filters are added in order to remove noise.
In order to create Tagger 3, we merge the dictionary induced with CSMT and the dictionary induced
with context similarity, giving preference to the latter. Again, the emission parameters of the baseline
Spanish tagger are adapted using this dictionary. 65.62% of the words in the emission file could be
translated, i.e. only 1.59% more than for Tagger 2. Nevertheless, the accuracy of Tagger 3 (88.72%) lies
about 18% absolute above Tagger 2. This large gain in accuracy is due to the fact that context similarity
mostly adds high-frequency words, which are few but crucial to obtain satisfactory tagging performance.
One goal of these experiments was to show whether the improved handling of ambiguity provided by
HMMs in comparison with the unigram model used by Scherrer and Sagot (2013) is reflected in better
overall tagging performance. This goal has been reached: the unigram model of Scherrer and Sagot
(2013) shows a tagging accuracy of 85.1%, which is 3% absolute below Tagger 3, the most directly
comparable HMM-based tagger.
3
4.9 Tagger 4: re-estimate transition probabilities
In this last model, we challenge the initial assumption that the Spanish transition probabilities are ?good
enough? for tagging Catalan. Concretely, we use Tagger 3 to tag the entire Catalan Wikipedia corpus
(the one also used for the lexicon induction tasks) and then train Tagger 4 in a supervised way on this
data. The idea behind this additional step is that the transition (and emission) counts estimated on the
large Catalan corpus are more reliable than those obtained by direct tagger adaptation.
Tagger 4 yields an accuracy value of 89.08%, outperforming Tagger 3 by only 0.36%.
4
This difference
is consistent with the one observed between Upper Bound 1 and Upper Bound 2, suggesting once more
3
The Catalan results reported in Scherrer and Sagot (2014) are based on a different test set, which is why we rather refer to
the directly comparable Scherrer and Sagot (2013) results in this section.
4
This difference is significant: ?
2
(1;N = 1064002) = 35.84835013; p < 0.01.
35
that transition counts only marginally influence the tagging performance if the former are estimated on a
language that is structurally similar.
5 Multilingual experiments
In addition to the Spanish?Catalan experiment, we have induced taggers for several closely related lan-
guages from Romance, Germanic and Slavic language families and tested them on the multilingual data
set used by Scherrer and Sagot (2014). Although the results of these additional experiments are less
reliable than the Spanish?Catalan data due to the small test corpus sizes, they allow us to generalize our
findings to other languages and language families. The experiments are set up as follows:
? The Aragonese taggers were adapted from a Spanish tagger trained on AnCora. They are tested
on a Wikipedia excerpt of 100 sentences that was manually annotated with the simplified AnCora
labels of Section 4.1. The Wikipedia corpora used for lexicon induction contained 5.4M words for
Aragonese, and 431M words for Spanish.
? The Dutch and Palatine German taggers were adapted from a Standard German tagger trained
on the TIGER treebank (900 000 tokens; 55 tags; Brants et al. (2002)). The gold standard cor-
pora are Wikipedia excerpts of 100 sentences each, manually annotated with TIGER labels. The
Wikipedia corpora used for lexicon induction contained 0.5M words for Dutch, 0.3M words for
Palatine German, and 612M words for Standard German.
? The Upper Sorbian, Slovak and Polish taggers were adapted from a Czech Tagger trained on the
Prague Dependency Treebank 2.5 (2M tokens; 57 simplified tags).
5
The gold standard corpora
are Wikipedia excerpts of 30 sentences each, manually annotated with simplified PDT labels. The
Wikipedia corpora used for lexicon induction contained 0.9M words for Upper Sorbian, 30M words
for Slovak, 206M words for Polish, and 85M words for Czech.
The tagging accuracies are reported in the left part of Table 2. The accuracy values vary widely across
languages, with baseline performances ranging from 24% to 81%. This variation essentially reflects the
linguistic distance between the RL and the NRL: German and Dutch seem to be particularly distant,
while Czech and Slovak are particularly closely related. In contrast, the overall tendency of the tagging
models is the same for all languages: there are consistent gradual improvements from the baseline tagger
to Tagger 3. These findings are in line with the Catalan experiments. The differences between Tagger 3
and Tagger 4 are not significant for any language, whereas the Catalan experiment showed a slight but
significant improvement. Finally, Taggers 3 and 4 slightly outperform the unigram tagger of Scherrer
and Sagot (2014) (S&S in Table 2) on most languages, although the difference is less marked than for
Catalan.
The right half of Table 2 shows what percentage of the emission files could be translated at each step,
analogously to the figures reported for Catalan in Table 1. The variation observed here mainly depends
on the language proximity and on the size of the corpora used for lexicon induction.
Globally, the Germanic languages obtain the lowest accuracy scores. This is due to a combination
of factors. First, as stated above, the baseline performance is already lower than in the other language
families, which essentially results from a lower number of identical NRL?RL word pairs than in other
language families. Second, the lexicon induction corpora are much smaller than for the other language
families.
6
Third, Germanic languages tend to have longer words due to compounding, so that the BI-
SIM threshold is more difficult to satisfy. The combination of the second and third factors lead to poor
performance of the first lexicon induction step: less than 4% of the German words could be translated
5
Similarly to AnCora, the morphosyntactic labels of the PDT consist of 15 positions that encode the main morphosyntactic
category, the subcategory as well as various morphological categories. We simplify the tagset analogously to AnCora, keeping
only the main category and the subcategory, which leads to 57 distinct labels.
The PDT is available at http://ufal.mff.cuni.cz/pdt2.5/.
6
As in our earlier work, we used all of the Palatine German Wikipedia, whereas we reduced the Dutch Wikipedia corpus on
purpose to better simulate the low-resource scenario.
36
Language Tagging accuracy Translated words
Baseline T1 T2 T3 T4 S&S T1 T2 T3
Aragonese 72% 74% 74% 87% 87% 85% 16.11% 42.65% 43.23%
Dutch 24% 30% 39% 60% 62% 59% 3.69% 6.73% 6.79%
Palatine German 50% 54% 57% 70% 70% 65% 3.86% 5.52% 5.58%
Upper Sorbian 70% 72% 77% 84% 84% 84% 5.70% 11.60% 11.69%
Slovak 81% 85% 88% 93% 93% 92% 29.39% 52.40% 54.41%
Polish 66% 69% 72% 78% 79% 78% 8.50% 42.27% 42.73%
Table 2: Results of the multilingual tagging experiments. The left half of the table reports tagging
accuracies and compares them with the results reported by Scherrer and Sagot (2014) (S&S column).
The right half of the table shows how many words of the emission files could be translated.
when building Tagger 1. This obviously reduces the potential for accuracy gains in Tagger 1, but it also
hampers the training of the CSMT system at the origin of Tagger 2. However, one should note that good
tagging results can be achieved even with relatively low translation coverage, as shown by the Upper
Sorbian experiment.
6 Conclusion
One goal of the experiments presented here was to validate the pipeline proposed earlier in Scherrer
and Sagot (2014). By showing that there are gradual improvements from the baseline tagger to Tagger
3 on a large number of languages, we demonstrate that the overall approach of inducing word pairs in
subsequent steps is sound, and that the order of these steps is reasonably chosen. Furthermore, we find
that re-estimating the tagger parameters on a large monolingual corpus (Tagger 4) does not improve its
performance substantially, as we have predicted in Section 4.4 on the basis of supervised Catalan taggers.
A second goal of these experiments was to show that the HMM taggers offer improved handling of
ambiguity compared with the unigram tagger of Scherrer and Sagot (2014). We have indeed noted an
accuracy gain of 3% on the Catalan data, and the multilingual data set shows similar (yet less marked)
tendencies.
However, the Catalan experiments show that there still is a gap of about 10% absolute accuracy be-
tween the adaptation taggers and fully supervised taggers. We see two main reasons for this gap. First,
the completely unsupervised lexicon induction algorithms obviously produce a number of erroneous
word pairs, which may then result in erroneous tagging. Second, the lexicon induction algorithms cur-
rently do not allow a given NRL word to relate to two different RL words. As a result, the taggers are
not able to model tagging ambiguities arising from translation ambiguities. Better ambiguity handling,
for instance on the basis of token-level constraints as suggested by T?ckstr?m et al. (2013), could thus
further improve tagging accuracy.
Finally, discriminative models using Maximum Entropy or Perceptron training have largely superseded
HMMs for part-of-speech tagging in the last few years.
7
Such models take into account a larger set of
features such as word suffixes, word structure (presence of punctuation signs, numerals, etc.) and external
lexicon information. Further research will be needed to investigate how our adaptation methods can be
applied to feature-based tagging models.
Acknowledgements
The author would like to thank Beno?t Sagot for his collaboration on earlier versions of this work. This
work was partially funded by the Labex EFL (ANR/CGI), Strand 6, operation LR2.2.
7
For an overview on recent English taggers, see for example http://aclweb.org/aclwiki/index.php?title=POS_
Tagging_(State_of_the_art).
37
References
Delphine Bernhard and Anne-Laure Ligozat. 2013. Hassle-free POS-tagging for the Alsatian dialects. In Marcos
Zampieri and Sascha Diwersy, editors, Non-Standard Data Sources in Corpus Based-Research, volume 5 of
ZSM Studien, pages 85?92. Shaker.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang Lezius, and George Smith. 2002. The TIGER Treebank.
In Proceedings of the First Workshop on Treebanks and Linguistic Theories (TLT 2002), pages 24?41.
Thorsten Brants. 2000. TnT ? a statistical part-of-speech tagger. In Proceedings of ANLP 2000, pages 224?231.
Dipanjan Das and Slav Petrov. 2011. Unsupervised part-of-speech tagging with bilingual graph-based projections.
In Proceedings of ACL-HLT 2011, pages 600?609.
Long Duong, Paul Cook, Steven Bird, and Pavel Pecina. 2013. Simpler unsupervised POS tagging with bilingual
projections. In Proceedings of ACL 2013, pages 634?639.
Anna Feldman, Jirka Hana, and Chris Brew. 2006. A cross-language approach to rapid creation of new morpho-
syntactically annotated resources. In Proceedings of LREC 2006, pages 549?554.
Darja Fi?er and Nikola Ljube?i?c. 2011. Bilingual lexicon extraction from comparable corpora for closely related
languages. In Proceedings of RANLP 2011, pages 125?131.
Pascale Fung. 1998. A statistical view on bilingual lexicon extraction: from parallel corpora to non-parallel
corpora. Machine Translation and the Information Soup, pages 1?17.
Dan Garrette and Jason Baldridge. 2013. Learning a part-of-speech tagger from two hours of annotation. In
Proceedings of NAACL-HLT 2013, pages 138?147.
Daniel Jurafsky and James H. Martin. 2009. Speech and language processing. Pearson, 2nd edition.
Philipp Koehn and Kevin Knight. 2002. Learning a translation lexicon from monolingual corpora. In Proceedings
of the ACL 2002 Workshop on Unsupervised Lexical Acquisition (SIGLEX 2002), pages 9?16.
Grzegorz Kondrak and Bonnie Dorr. 2004. Identification of confusable drug names: A new approach and evalua-
tion methodology. In Proceedings of COLING 2004, pages 952?958.
Shen Li, Jo?o Gra?a, and Ben Taskar. 2012. Wiki-ly supervised part-of-speech tagging. In Proceedings of
EMNLP-CoNLL 2012, pages 1389?1398.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012. A universal part-of-speech tagset. In Proceedings of
LREC 2012, pages 2089?2096.
Reinhard Rapp. 1999. Automatic identification of word translations from unrelated English and German corpora.
In Proceedings of ACL 1999, pages 519?526.
Yves Scherrer and Beno?t Sagot. 2013. Lexicon induction and part-of-speech tagging of non-resourced languages
without any bilingual resources. In Proceedings of the RANLP 2013 Workshop on Adaptation of language
resources and tools for closely related languages and language variants.
Yves Scherrer and Beno?t Sagot. 2014. A language-independent and fully unsupervised approach to lexicon
induction and part-of-speech tagging for closely related languages. In Proceedings of LREC 2014, pages 502?
508.
Oscar T?ckstr?m, Dipanjan Das, Slav Petrov, Ryan McDonald, and Joakim Nivre. 2013. Token and type con-
straints for cross-lingual part-of-speech tagging. Transactions of the Association for Computational Linguistics,
1:1?12.
Mariona Taul?, M. Ant?nia Mart?, and Marta Recasens. 2008. Ancora: Multilevel annotated corpora for Catalan
and Spanish. In Proceedings of LREC 2008, pages 96?101.
J?rg Tiedemann. 2009. Character-based PSMT for closely related languages. In Proceedings of EAMT 2009,
pages 12?19.
David Vilar, Jan-Thorsten Peter, and Hermann Ney. 2007. Can we translate letters? In Proceedings of WMT 2007,
pages 33?39.
David Yarowsky, Grace Ngai, and Richard Wicentowski. 2001. Inducing multilingual text analysis tools via robust
projection across aligned corpora. In Proceedings of HLT 2001.
38
