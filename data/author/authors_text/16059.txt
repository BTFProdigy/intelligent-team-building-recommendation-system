Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 420?429,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Learning Syntactic Verb Frames Using Graphical Models
Thomas Lippincott
University of Cambridge
Computer Laboratory
United Kingdom
tl318@cam.ac.uk
Diarmuid O? Se?aghdha
University of Cambridge
Computer Laboratory
United Kingdom
do242@cam.ac.uk
Anna Korhonen
University of Cambridge
Computer Laboratory
United Kingdom
alk23@cam.ac.uk
Abstract
We present a novel approach for building
verb subcategorization lexicons using a simple
graphical model. In contrast to previous meth-
ods, we show how the model can be trained
without parsed input or a predefined subcate-
gorization frame inventory. Our method out-
performs the state-of-the-art on a verb clus-
tering task, and is easily trained on arbitrary
domains. This quantitative evaluation is com-
plemented by a qualitative discussion of verbs
and their frames. We discuss the advantages of
graphical models for this task, in particular the
ease of integrating semantic information about
verbs and arguments in a principled fashion.
We conclude with future work to augment the
approach.
1 Introduction
Subcategorization frames (SCFs) give a compact de-
scription of a verb?s syntactic preferences. These
two sentences have the same sequence of lexical
syntactic categories (VP-NP-SCOMP), but the first
is a simple transitive (?X understood Y?), while the
second is a ditransitive with a sentential complement
(?X persuaded Y that Z?):
1. Kim (VP understood (NP the evidence
(SCOMP that Sandy was present)))
2. Kim (VP persuaded (NP the judge) (SCOMP
that Sandy was present))
An SCF lexicon would indicate that ?persuade?
is likely to take a direct object and sentential com-
plement (NP-SCOMP), while ?understand? is more
likely to take just a direct object (NP). A compre-
hensive lexicon would also include semantic infor-
mation about selectional preferences (or restrictions)
on argument heads of verbs, diathesis alternations
(i.e. semantically-motivated alternations between
pairs of SCFs) and a mapping from surface frames
to the underlying predicate-argument structure. In-
formation about verb subcategorization is useful for
tasks like information extraction (Cohen and Hunter,
2006; Rupp et al, 2010), verb clustering (Korho-
nen et al, 2006b; Merlo and Stevenson, 2001) and
parsing (Carroll et al, 1998). In general, tasks that
depend on predicate-argument structure can benefit
from a high-quality SCF lexicon (Surdeanu et al,
2003).
Large, manually-constructed SCF lexicons
mostly target general language (Boguraev and
Briscoe, 1987; Grishman et al, 1994). However,
in many domains verbs exhibit different syntactic
behavior (Roland and Jurafsky, 1998; Lippincott
et al, 2010). For example, the verb ?develop?
has specific usages in newswire, biomedicine and
engineering that dramatically change its probability
distribution over SCFs. In a few domains like
biomedicine, the need for focused SCF lexicons
has led to manually-built resources (Bodenreider,
2004). Such resources, however, are costly, prone to
human error, and in domains where new lexical and
syntactic constructs are frequently coined, quickly
become obsolete (Cohen and Hunter, 2006). Data-
driven methods for SCF acquisition can alleviate
420
these problems by building lexicons tailored to
new domains with less manual effort, and higher
coverage and scalability.
Unfortunately, high quality SCF lexicons are dif-
ficult to build automatically. The argument-adjunct
distinction is challenging even for humans, many
SCFs have no reliable cues in data, and some SCFs
(e.g. those involving control such as type raising)
rely on semantic distinctions. As SCFs follow a Zip-
fian distribution (Korhonen et al, 2000), many gen-
uine frames are also low in frequency. State-of-the-
art methods for building data-driven SCF lexicons
typically rely on parsed input (see section 2). How-
ever, the treebanks necessary for training a high-
accuracy parsing model are expensive to build for
new domains. Moreover, while parsing may aid the
detection of some frames, many experiments have
also reported SCF errors due to noise from parsing
(Korhonen et al, 2006a; Preiss et al, 2007).
Finally, many SCF acquisition methods operate
with predefined SCF inventories. This subscribes to
a single (often language or domain-specific) inter-
pretation of subcategorization a priori, and ignores
the ongoing debate on how this interpretation should
be tailored to new domains and applications, such as
the more prominent role of adjuncts in information
extraction (Cohen and Hunter, 2006).
In this paper, we describe and evaluate a novel
probabilistic data-driven method for SCF acquisi-
tion aimed at addressing some of the problems with
current approaches. In our model, a Bayesian net-
work describes how verbs choose their arguments
in terms of a small number of frames, which are
represented as distributions over syntactic relation-
ships. First, we show that by allowing the infer-
ence process to automatically define a probabilistic
SCF inventory, we outperform systems with hand-
crafted rules and inventories, using identical syntac-
tic features. Second, by replacing the syntactic fea-
tures with an approximation based on POS tags, we
achieve state-of-the-art performance without relying
on error-prone unlexicalized or domain-specific lex-
icalized parsers. Third, we highlight a key advantage
of our method compared to previous approaches: the
ease of integrating and performing joint inference of
additional syntactic and semantic information. We
describe how we plan to exploit this in our future
research.
2 Previous work
Many state-of-the-art SCF acquisition systems take
grammatical relations (GRs) as input. GRs ex-
press binary dependencies between lexical items,
and many parsers produce them as output, with
some variation in inventory (Briscoe et al, 2006;
De Marneffe et al, 2006). For example, a subject-
relation like ?ncsubj(HEAD, DEPENDENT)? ex-
presses the fact that the lexical item referred to by
HEAD (such as a present-tense verb) has the lexi-
cal item referred to by DEPENDENT as its subject
(such as a singular noun). GR inventories include
direct and indirect objects, complements, conjunc-
tions, among other relations. The dependency rela-
tionships included in GRs correspond closely to the
head-complement structure of SCFs, which is why
they are the natural choice for SCF acquisition.
There are several SCF lexicons for general lan-
guage, such as ANLT (Boguraev and Briscoe, 1987)
and COMLEX (Grishman et al, 1994), that depend
on manual work. VALEX (Preiss et al, 2007) pro-
vides SCF distributions for 6,397 verbs acquired
from a parsed general language corpus via a system
that relies on hand-crafted rules. There are also re-
sources which provide information about both syn-
tactic and semantic properties of verbs: VerbNet
(Kipper et al, 2008) draws on several hand-built
and semi-automatic sources to link the syntax and
semantics of 5,726 verbs. FrameNet (Baker et al,
1998) provides semantic frames and annotated ex-
ample sentences for 4,186 verbs. PropBank (Palmer
et al, 2005) is a corpus where each verb is annotated
for its arguments and their semantic roles, covering
a total of 4,592 verbs.
There are many language-specific SCF acquisi-
tion systems, e.g. for French (Messiant, 2008),
Italian (Lenci et al, 2008), Turkish (Han et al,
2008) and Chinese (Han et al, 2008). These typ-
ically rely on language-specific knowledge, either
directly through heuristics, or indirectly through
parsing models trained on treebanks. Furthermore,
some require labeled training instances for super-
vised (Uzun et al, 2008) or semi-supervised (Han
et al, 2008) learning algorithms.
Two state-of-the-art data-driven systems for En-
glish verbs are those that produced VALEX, Preiss et
al. (2007), and the BioLexicon (Venturi et al, 2009).
421
The Preiss system extracts a verb instance?s GRs us-
ing the Rasp general-language unlexicalized parser
(Briscoe et al, 2006) as input, and based on hand-
crafted rules, maps verb instances to a predefined
inventory of 168 SCFs. Filtering is then performed
to remove noisy frames, with methods ranging from
a simple single threshold to SCF-specific hypothesis
tests based on external verb classes and SCF inven-
tories. The BioLexicon system extracts each verb in-
stance?s GRs using the lexicalized Enju parser tuned
to the biomedical domain (Miyao, 2005). Each
unique GR-set considered a potential SCF, and an
experimentally-determined threshold is used to fil-
ter low-frequency SCFs.
Note that both methods require extensive man-
ual work: the Preiss system involves the a priori
definition of the SCF inventory, careful construc-
tion of matching rules, and an unlexicalized pars-
ing model. The BioLexicon system induces its SCF
inventory automatically, but requires a lexicalized
parsing model, rendering it more sensitive to domain
variation. Both rely on a filtering stage that depends
on external resources and/or gold standards to select
top-performing thresholds. Our method, by contrast,
does not use a predefined SCF inventory, and can
perform well without parsed input.
Graphical models have been increasingly popu-
lar for a variety of tasks such as distributional se-
mantics (Blei et al, 2003) and unsupervised POS
tagging (Finkel et al, 2007), and sampling methods
allow efficient estimation of full joint distributions
(Neal, 1993). The potential for joint inference of
complementary information, such as syntactic verb
and semantic argument classes, has a clear and in-
terpretable way forward, in contrast to the pipelined
methods described above. This was demonstrated in
Andrew et al (2004), where a Bayesian model was
used to jointly induce syntactic and semantic classes
for verbs, although that study relied on manually
annotated data and a predefined SCF inventory and
MLE. More recently, Abend and Rappoport (2010)
trained ensemble classifiers to perform argument-
adjunct disambiguation of PP complements, a task
closely related to SCF acquisition. Their study em-
ployed unsupervised POS tagging and parsing, and
measures of selectional preference and argument
structure as complementary features for the classi-
fier.
Finally, our task-based evaluation, verb clustering
with Levin (1993)?s alternation classes as the gold
standard, was previously conducted by Joanis and
Stevenson (2003), Korhonen et al (2008) and Sun
and Korhonen (2009).
3 Methodology
In this section we describe the basic components of
our study: feature sets, graphical model, inference,
and evaluation.
3.1 Input and feature sets
We tested several feature sets either based on, or
approximating, the concept of grammatical relation
described in section 2. Our method is agnostic re-
garding the exact definition of GR, and for example
could use the Stanford inventory (De Marneffe et al,
2006) or even an entirely different lexico-syntactic
formalism like CCG supertags (Curran et al, 2007).
In this paper, we distinguish ?true GRs? (tGRs), pro-
duced by a parser, and ?pseudo GRs? (pGRs), a
POS-based approximation, and employ subscripts to
further specify the variations described below. Our
input has been parsed into Rasp-style tGRs (Briscoe
et al, 2006), which facilitates comparison with pre-
vious work based on the same data set.
We?ll use a simple example sentence to illustrate
how our feature sets are extracted from CONLL-
formatted data (Nivre et al, 2007). The CONLL
format is a common language for comparing output
from dependency parsers: each lexical item has an
index, lemma, POS tag, tGR in which it is the de-
pendent, and index to the corresponding head. Table
1 shows the relevant fields for the sentence ?We run
training programmes in Romania and other coun-
tries?.
We define the feature set for a verb occurrence as
the counts of each GR the verb participates in. Table
2 shows the three variations we tested: the simple
tGR type, with parameterization for the POS tags
of head and dependent, and with closed-class POS
tags (determiners, pronouns and prepositions) lexi-
calized. In addition, we tested the effect of limiting
the features to subject, object and complement tGRs,
indicated by adding the subscript ?lim?, for a total of
six tGR-based feature sets.
While ideally tGRs would give full informa-
422
Index Lemma POS Head tGR
1 we PPIS2 2 ncsubj
2 run VV0 0
3 training NN1 4 ncmod
4 programme NN2 2 dobj
5 in II 4 ncmod
6 romania NP1 7 conj
7 and CC 5 dobj
8 other JB 9 ncmod
9 country NN2 7 conj
Table 1: Simplified CONLL format for example sen-
tence ?We run training programmes in Romania and
other countries?. Head=0 indicates the token is the
root.
Name Features
tGR ncsubj dobj
tGRparam ncsubj(VV0,PPIS2) dobj(VV0,NN2)
tGRparam,lex ncsubj(VV0,PPIS2-we) dobj(VV0,NN2)
Table 2: True-GR features for example sentence:
note there are also tGR?,lim versions of each that
only consider subjects, objects and complements
and are not shown.
tion about the verb?s syntactic relationship to other
words, in practice parsers make (possibly prema-
ture) decisions, such as deciding that ?in? modifies
?programme?, and not ?run? in our example sen-
tence. An unlexicalized parser cannot distinguish
these based just on POS tags, while a lexicalized
parser requires a large treebank. We therefore define
pseudo-GRs (pGRs), which consider each (distance,
POS) pair within a given window of the verb to be
a potential tGR. Table 3 shows the pGR features for
the test sentence using a window of three. As with
tGRs, the closed-class tags can be lexicalized, but
there are no corresponding feature sets for param
(since they are already built from POS tags) or lim
(since there is no similar rule-based approach).
Name Features
pGR -1(PPIS2) 1(NN1) 2(NN2) 3(II)
pGRlex -1(PPIS2-we) 1(NN1) 2(NN2) 3(II-in)
Table 3: Pseudo-GR features for example sentence
with window=3
Whichever feature set is used, an instance is sim-
ply the count of each GR?s occurrences. We extract
instances for the 385 verbs in the union of our two
gold standards from the VALEX lexicon?s data set,
which was used in previous studies (Sun and Korho-
nen, 2009; Preiss et al, 2007) and facilitates com-
parison with that resource. This data set is drawn
from five general-language corpora parsed by Rasp,
and provides, on average, 7,000 instances per verb.
3.2 SCF extraction
Our graphical modeling approach uses the Bayesian
network shown in Figure 1. Its generative story
is as follows: when a verb is instantiated, an SCF
is chosen according to a verb-specific multinomial.
Then, the number and type of syntactic arguments
(GRs) are chosen from two SCF-specific multino-
mials. These three multinomials are modeled with
uniform Dirichlet priors and corresponding hyper-
parameters ?, ? and ?. The model is trained via
collapsed Gibbs sampling, where the probability of
assigning a particular SCF s to an instance of verb v
with GRs (gr1 . . . grn) is the product
P (s|V erb = v,GRs = gr1 . . . grn) =
P (SCF = s|V erb = v)?
P (N = n|SCF = s)?
?
i=1:n
P (GR = gri|SCF = s)
The three terms, given the hyper-parameters and
conjugate-prior relationship between Dirichlet and
Multinomial distributions, can be expressed in terms
of current assignments of s to verb v ( csv ), s to
GR-count n ( csn ) and s to GR ( csg ), the corre-
sponding totals ( cv, cs ), the dimensionality of the
distributions ( |SCF |, |N | and |G| ) and the hyper-
parameters ?, ? and ?:
P (SCF = s|V erb = v) = (csv+?)/(cv+|SCF |?)
P (N = n|SCF = s) = (csn + ?)/(cs + |N |?)
P (GR = gri|SCF = s) = (csgri +?)/(cs + |G|?)
Note that N , the possible GR-count for an in-
stance, is usually constant for pGRs ( 2 ? window
), unless the verb is close to the start or end of the
sentence.
423
? // V erbxSCF
&&
V erbi

i ? I
SCFi //

Ni
||
SCFxNoo ?oo
GRi SCFxGRoo ?oo
Figure 1: Our simple graphical model reflecting subcategorization. Double-circles indicate an observed
value, arrows indicate conditional dependency. What constitutes a ?GR? depends on the feature set being
used.
We chose our hyper-parameters ? = ? = ? = .02
to reflect the characteristic sparseness of the phe-
nomena (i.e. verbs tend to take a small number of
SCFs, which in turn are limited to a small number
of realizations). For the pGRs we used a window
of 5 tokens: a verb?s arguments will fall within a
small window in the majority of cases, so there is
diminished return in expanding the window at the
cost of increased noise. Finally, we set our SCF
count to 40, about twice the size of the strictly syn-
tactic general-language gold standard we describe in
section 3.3. This overestimation allows some flex-
ibility for the model to define its inventory based
on the data; any supernumerary frames will act as
?junk frames? that are rarely assigned and hence
will have little influence. We run Gibbs sampling
for 1000 iterations, and average the final 100 sam-
ples to estimate the posteriors P (SCF |V erb) and
P (GR|SCF ). Variance between adjacent states?
estimates of P (SCF |V erb) indicates that the sam-
pling typically converges after about 100-200 itera-
tions.1
3.3 Evaluation
Quantitative: cluster gold standard
Evaluating the output of unsupervised methods is
not straightforward: discrete, expert-defined cate-
gories (like many SCF inventories) are unlikely to
line up perfectly with data-driven, probabilistic out-
put. Even if they do, finding a mapping between
them is a problem of its own (Meila, 2003).
1Full source code for this work is available at http://cl.
cam.ac.uk/?tl318/files/subcat.tgz
Our goal is to define a fair quantitative compari-
son between arbitrary SCF lexicons. An SCF lexi-
con makes two claims: first, that it defines a reason-
able SCF inventory. Second, that for each verb, it
has an accurate distribution over that inventory. We
therefore compare the lexicons based on their per-
formance on a task that a good SCF lexicon should
be useful for: clustering verbs into lexical-semantic
classes. Our gold standard is from (Sun and Korho-
nen, 2009), where 200 verbs were assigned to 17
classes based on their alternation patterns (Levin,
1993). Previous work (Schulte im Walde, 2009;
Sun and Korhonen, 2009) has demonstrated that the
quality of an SCF lexicon?s inventory and probabil-
ity estimates corresponds to its predictive power for
membership in such alternation classes.
To compare the performance of our feature sets,
we chose the simple and familiar K-Means cluster-
ing algorithm (Hartigan and Wong, 1979). The in-
stances are the verbs? SCF distributions, and we se-
lect the number of clusters by the Silhouette vali-
dation technique (Rousseeuw, 1987). The clusters
are then compared to the gold standard clusters with
the purity-based F-Score from Sun and Korhonen
(2009) and the more familiar Adjusted Rand Index
(Hubert and Arabie, 1985). Our main point of com-
parison is the VALEX lexicon of SCF distributions,
whose scores we report alongside ours.
Qualitative: manual gold standard
We also want to see how our results line up with
a traditional linguistic view of subcategorization,
but this requires digging into the unsupervised out-
424
put and associating anonymous probabilistic objects
with established categories. We therefore present
sample output in three ways: first, we show the
clustering output from our top-performing method.
Second, we plot the probability mass over GRs for
two anonymous SCFs that correspond to recogniz-
able traditional SCFs, and one that demonstrates un-
expected behavior. Third, we compared the out-
put for several verbs to a coarsened version of the
manually-annotated gold standard used to evaluate
VALEX (Preiss et al, 2007). We collapsed the orig-
inal inventory of 168 SCFs to 18 purely syntactic
SCFs based on their characteristic GRs and removed
frames that depend on semantic distinctions, leav-
ing the detection of finer-grained and semantically-
based frames for future work.
4 Results
4.1 Verb clustering
We evaluated SCF lexicons based on the eight fea-
ture sets described in section 3.1, as well as the
VALEX SCF lexicon described in section 2. Table 4
shows the performance of the lexicons in ascending
order.
Method Pur. F-score Adj. Rand
tGR .24 .02
tGRlim .27 .02
pGRlex .32 .09
tGRlim,param .35 .08
pGR .35 .10
VALEX .36 .10
tGRparam,lex .37 .10
tGRparam .39 .12
tGRlim,param,lex .44 .12
Table 4: Task-based evaluation of lexicons acquired
with each of the eight feature types, and the state-of-
the-art rule-based VALEX lexicon.
These results lead to several conclusions: first,
training our model on tGRs outperforms pGRs and
VALEX. Since the parser that produced them is
known to perform well on general language (Briscoe
et al, 2006), the tGRs are of high quality: it makes
sense that reverting to the pGRs is unnecessary in
this case. The interesting point is the major perfor-
mance gain over VALEX, which uses the same tGR
features along with expert-developed rules and in-
ventory.
Second, we achieve performance comparable to
VALEX using pGRs with a narrow window width.
Since POS tagging is more reliable and robust across
domains than parsing, retraining on new domains
will not suffer the effects of a mismatched parsing
model (Lippincott et al, 2010). It is therefore pos-
sible to use this method to build large-scale lexicons
for any new domain with sufficient data.
Third, lexicalizing the closed-class POS tags in-
troduces semantic information outside the scope
of the alternation-based definition of subcatego-
rization. For example, subdividing the indefinite
pronoun tag ?PN1? into ?PN1-anyone? and ?PN1-
anything? gives information about the animacy of
the verb?s arguments. Our results show this degrades
performance for both pGR and tGR features, unless
the latter are limited to tGRs traditionally thought to
be relevant for the task.
4.2 Qualitative analysis
Table 5 shows clusters produced by our top-scoring
method, GRparam,lex,lim. Some clusters are imme-
diately intelligible at the semantic level and corre-
spond closely to the lexical-semantic classes found
in Levin (1993). For example, clusters 1, 6, and 14
include member verbs of Levin?s SAY, PEER and
AMUSE classes, respectively. Some clusters are
based on broader semantic distinctions (e.g. cluster
2 which groups together verbs related to locations)
while others relate semantic classes purely based
on their syntactic similarity (e.g. the verbs in clus-
ter 17 share strong preference for ?to? preposition).
The syntactic-semantic nature of the clusters reflects
the multimodal nature of verbs and illustrates why a
comprehensive subcategorization lexicon should not
be limited to syntactic frames. This phenomenon is
also encouraging for future work to tease apart and
simultaneously exploit several verbal aspects via ad-
ditional latent structure in the model.
An SCF?s distribution over features can reveal its
place in the traditional definition of subcategoriza-
tion. Figure 2 shows the high-probability (>.02)
tGRs for one SCF: the large mass centered on di-
rect object tGRs indicates this approximates the no-
tion of ?transitive?. Looking at the verbs most likely
to take this SCF (?stimulate?, ?conserve?) confirms
425
1 exclaim, murmur, mutter, reply, retort, say,
sigh, whisper
2 bang, knock, snoop, swim, teeter
3 flicker, multiply, overlap, shine
4 batter, charter, compromise, overwhelm,
regard, sway, treat
5 abolish, broaden, conserve, deepen, eradi-
cate, remove, sharpen, shorten, stimulate,
strengthen, unify
6 gaze, glance, look, peer, sneer, squint, stare
7 coincide, commiserate, concur, flirt, inter-
act
8 grin, smile, wiggle
9 confuse, diagnose, march
10 mate, melt, swirl
11 frown, jog, stutter
12 chuckle, mumble, shout
13 announce, envisage, mention, report, state
14 frighten, intimidate, scare, shock, upset
15 bash, falter, snarl, wail, weaken
16 cooperate, eject, respond, transmit
17 affiliate, compare, contrast, correlate, for-
ward, mail, ship
Table 5: Clusters (of size >2 and <20) produced
using tGRparam,lex,lim
this. Figure 3 shows a complement-taking SCF,
which is far rarer than simple transitive but also
clearly induced by our model.
The induced SCF inventory also has some redun-
dancy, such as additional transitive frames beside
figure 2, and frames with poor probability estimates.
Most of these issues can be traced to our simplifying
assumption that each tGR is drawn independently
w.r.t. an instance?s other tGRs. For example, if an
SCF gives any weight to indirect objects, it gives
non-zero probability to an instance with only indi-
rect objects, an impossible case. This can lead to
skewed probability estimates: since some tGRs can
occur multiple times in a given instance (e.g. in-
direct objects and prepositional phrases) the model
may find it reasonable to create an SCF with all
probability focused on that tGR, ignoring all oth-
ers, such as in figure 4. We conclude that our inde-
pendence assumption was too strong, and the model
would benefit from defining more structure within
Figure 2: The SCF corresponding to transitive has
most probability centered on dobj (e.g. stimulate,
conserve, deepen, eradicate, broaden)
Figure 3: The SCF corresponding to verbs taking
complements has more probability on xcomp and
ccomp (e.g. believe, state, agree, understand, men-
tion)
instances.
The full tables necessary to compare verb SCF
distributions from our output with the manual gold
standard are prohibited by space, but a few exam-
ples reinforce the analysis above. The verbs ?load?
and ?fill? show particularly high usage of ditransi-
tive SCFs in the gold standard. In our inventory, this
is reflected in high usage of an SCF with probabil-
ity centered on indirect objects, but due to the inde-
pendence assumptions the frame has a correspond-
ing low probability on subjects and direct objects,
despite the fact that these necessarily occur along
with any indirect object. The verbs ?acquire? and
?buy? demonstrate both a strength of our approach
and a weakness of using parsed input: both verbs
426
Figure 4: This SCF is dominated by indirect objects
and complements, catering to verbs that may take
several such tGRs, at the expense of subjects
show high probability of simple transitive in our
output and the gold standard. However, the Rasp
parser often conflates indirect objects and preposi-
tional phrases due to its unlexicalized model. While
our system correctly gives high probability to ditran-
sitive for both verbs, it inherits this confusion and
over-estimates ?acquire??s probability mass for the
frame. This is an example of how bad decisions
made by the parser cannot be fixed by the graphi-
cal model, and an area where pGR features have an
advantage.
5 Conclusions and future work
Our study reached two important conclusions: first,
given the same data as input, an unsupervised prob-
abilistic model can outperform a hand-crafted rule-
based SCF extractor with a predefined inventory.
We achieve better results with far less effort than
previous approaches by allowing the data to gov-
ern the definition of frames while estimating the
verb-specific distributions in a fully Bayesian man-
ner. Second, simply treating POS tags within a
small window of the verb as pseudo-GRs produces
state-of-the-art results without the need for a pars-
ing model. This is particularly encouraging when
building resources for new domains, where com-
plex models fail to generalize. In fact, by integrat-
ing results from unsupervised POS tagging (Teichert
and Daume? III, 2009) we could render this approach
fully domain- and language-independent.
We did not dwell on issues related to choosing
our hyper-parameters or latent class count. Both of
these can be accomplished with additional sampling
methods: hyper-parameters of Dirichlet priors can
be estimated via slice sampling (Heinrich, 2009),
and their dimensionality via Dirichlet Process priors
(Heinrich, 2011). This could help address the redun-
dancy we find in the induced SCF inventory, with the
potential SCFs growing to accommodate the data.
Our initial attempt at applying graphical models
to subcategorization also suggested several ways to
extend and improve the method. First, the indepen-
dence assumptions between GRs in a given instance
turned out to be too strong. To address this, we could
give instances internal structure to capture condi-
tional probability between generated GRs. Second,
our results showed the conflation of several verbal
aspects, most notably the syntactic and semantic.
In a sense this is encouraging, as it motivates our
most exciting future work: augmenting this simple
model to explicitly capture complementary infor-
mation such as distributional semantics (Blei et al,
2003), diathesis alternations (McCarthy, 2000) and
selectional preferences (O? Se?aghdha, 2010). This
study targeted high-frequency verbs, but the use of
syntactic and semantic classes would also help with
data sparsity down the road. These extensions would
also call for a more comprehensive evaluation, aver-
aging over several tasks, such as clustering by se-
mantics, syntax, alternations and selectional prefer-
ences.
In concrete terms, we plan to introduce latent vari-
ables corresponding to syntactic, semantic and alter-
nation classes, that will determine a verb?s syntac-
tic arguments, their semantic realization (i.e. selec-
tional preferences), and possible predicate-argument
structures. By combining the syntactic classes with
unsupervised POS tagging (Teichert and Daume? III,
2009) and the selectional preferences with distribu-
tional semantics (O? Se?aghdha, 2010), we hope to
produce more accurate results on these complemen-
tary tasks while avoiding the use of any supervised
learning. Finally, a fundamental advantage of a data-
driven, parse-free method is that it can be easily
trained for new domains. We next plan to test our
method on a new domain, such as biomedical text,
where verbs are known to take on distinct syntactic
behavior (Lippincott et al, 2010).
427
6 Acknowledgements
The work in this paper was funded by the Royal So-
ciety, (UK), EPSRC (UK) grant EP/G051070/1 and
EU grant 7FP-ITC-248064. We are grateful to Lin
Sun and Laura Rimell for the use of their cluster-
ing and subcategorization gold standards, and the
ACL reviewers for their helpful comments and sug-
gestions.
References
Omri Abend and Ari Rappoport. 2010. Fully unsuper-
vised core-adjunct argument classification. In ACL
?10.
Galen Andrew, Trond Grenager, and Christopher Man-
ning. 2004. Verb sense and subcategorization: us-
ing joint inference to improve performance on com-
plementary tasks. EMNLP ?04.
Collin Baker, Charles Fillmore, and John Lowe. 1998.
The Berkeley FrameNet project. In COLING ACL ?98.
David Blei, Andrew Ng, Michael Jordan, and John Laf-
ferty. 2003. Latent dirichlet alocation. Journal of
Machine Learning Research.
Olivier Bodenreider. 2004. The Unified Medical Lan-
guage System (UMLS): integrating biomedical termi-
nology. Nucleic Acids Research, 32.
Bran Boguraev and Ted Briscoe. 1987. Large lexicons
for natural language processing. Computational Lin-
guistics, 13.
Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The second release of the RASP system. In Proceed-
ings of the COLING/ACL on Interactive presentation
sessions.
John Carroll, Guido Minnen, and Ted Briscoe. 1998.
Can subcategorisation probabilities help a statistical
parser? In The 6th ACL/SIGDAT Workshop on Very
Large Corpora.
K Bretonnel Cohen and Lawrence Hunter. 2006. A
critical review of PASBio?s argument structures for
biomedical verbs. BMC Bioinformatics, 7.
James Curran, Stephen Clark, and Johan Bos. 2007. Lin-
guistically motivated large-Scale NLP with C&C and
Boxer. In ACL ?07.
Marie-Catherine De Marneffe, Bill Maccartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
LREC ?06.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2007. The infinite tree. In ACL ?07.
Ralph Grishman, Catherine Macleod, and Adam Meyers.
1994. Comlex syntax: building a computational lexi-
con. In COLING ?94.
Xiwu Han, Chengguo Lv, and Tiejun Zhao. 2008.
Weakly supervised SVM for Chinese-English cross-
lingual subcategorization lexicon acquisition. In The
11th Joint Conference on Information Science.
J.A. Hartigan and M.A. Wong. 1979. Algorithm AS 136:
A K-Means clustering algorithm. Journal of the Royal
Statistical Society. Series C (Applied Statistics).
Gregor Heinrich. 2009. Parameter estimation for text
analysis. Technical report, Fraunhofer IGD.
428
Gregor Heinrich. 2011. Infinite LDA implementing the
HDP with minimum code complexity. Technical re-
port, arbylon.net.
Lawrence Hubert and Phipps Arabie. 1985. Comparing
partitions. Journal of Classification, 2.
Eric Joanis and Suzanne Stevenson. 2003. A general fea-
ture space for automatic verb classification. In EACL
?03.
Karin Kipper, Anna Korhonen, Neville Ryant, and
Martha Palmer. 2008. A large-scale classification of
English verbs. In LREC ?08.
Anna Korhonen, Genevieve Gorrell, and Diana Mc-
Carthy. 2000. Statistical filtering and subcategoriza-
tion frame acquisition. In Proceedings of the Joint
SIGDAT Conference on Empirical Methods in Natural
Language Processing and Very Large Corpora.
Anna Korhonen, Yuval Krymolowski, and Ted Briscoe.
2006a. A large subcategorization lexicon for natural
language processing applications. In LREC ?06.
Anna Korhonen, Yuval Krymolowski, and Nigel Collier.
2006b. Automatic classification of verbs in biomedi-
cal texts. In ACL ?06.
Anna Korhonen, Yuval Krymolowski, and Nigel Collier.
2008. The choice of features for classification of verbs
in biomedical texts. In COLING ?08.
Ro Lenci, Barbara Mcgillivray, Simonetta Montemagni,
and Vito Pirrelli. 2008. Unsupervised acquisition
of verb subcategorization frames from shallow-parsed
corpora. In LREC ?08.
Beth Levin. 1993. English Verb Classes and Alternation:
A Preliminary Investigation. University of Chicago
Press, Chicago, IL.
Thomas Lippincott, Anna Korhonen, and Diarmuid O?
Se?aghdha. 2010. Exploring subdomain variation in
biomedical language. BMC Bioinformatics.
Diana McCarthy. 2000. Using semantic preferences to
identify verbal participation in role switching alterna-
tions. In NAACL ?00.
Marina Meila. 2003. Comparing clusterings by the Vari-
ation of Information. In COLT.
Paola Merlo and Suzanne Stevenson. 2001. Automatic
verb classification based on statistical distributions of
argument structure. Computational Linguistics.
Ce?dric Messiant. 2008. A subcategorization acquisition
system for French verbs. In ACL HLT ?08 Student Re-
search Workshop.
Yusuke Miyao. 2005. Probabilistic disambiguation mod-
els for wide-coverage HPSG parsing. In ACL ?05.
Radford M. Neal. 1993. Probabilistic inference using
markov chain Monte Carlo methods. Technical report,
University of Toronto.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-
donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CoNLL 2007 shared task on de-
pendency parsing. In The CoNLL Shared Task Session
of EMNLP-CoNLL 2007.
Diarmuid O? Se?aghdha. 2010. Latent variable models of
selectional preference. In ACL ?10.
Martha Palmer, Paul Kingsbury, and Daniel Gildea.
2005. The Proposition Bank: an annotated corpus of
semantic roles. Computational Linguistics.
Judita Preiss, Ted Briscoe, and Anna Korhonen. 2007. A
system for large-scale acquisition of verbal, nominal
and adjectival subcategorization frames from corpora.
In ACL ?07.
Douglas Roland and Daniel Jurafsky. 1998. How verb
subcategorization frequencies are affected by corpus
choice. In ACL ?98.
Peter Rousseeuw. 1987. Silhouettes: a graphical aid
to the interpretation and validation of cluster analysis.
Journal of Computational and Applied Mathematics.
C.J. Rupp, Paul Thompson, William Black, and John Mc-
Naught. 2010. A specialised verb lexicon as the ba-
sis of fact extraction in the biomedical domain. In In-
terdisciplinary Workshop on Verbs: The Identification
and Representation of Verb Features.
Sabine Schulte im Walde. 2009. The induction of verb
frames and verb classes from corpora. In Corpus
Linguistics. An International Handbook. Mouton de
Gruyter.
Lin Sun and Anna Korhonen. 2009. Improving
verb clustering with automatically acquired selectional
preferences. In EMNLP?09.
Mihai Surdeanu, Sanda Harabagiu, John Williams, and
Paul Aarseth. 2003. Using predicate-argument struc-
tures for information extraction. In ACL ?03.
Adam R. Teichert and Hal Daume? III. 2009. Unsuper-
vised part of speech tagging without a lexicon. In
NIPS Workshop on Grammar Induction, Representa-
tion of Language and Language Learning.
E. Uzun, Y. Klaslan, H.V. Agun, and E. Uar. 2008.
Web-based acquisition of subcategorization frames for
Turkish. In The Eighth International Conference on
Artificial Intelligence and Soft Computing.
Giulia Venturi, Simonetta Montemagni, Simone Marchi,
Yutaka Sasaki, Paul Thompson, John McNaught, and
Sophia Ananiadou. 2009. Bootstrapping a verb lex-
icon for biomedical information extraction. In Com-
putational Linguistics and Intelligent Text Processing.
Springer Berlin / Heidelberg.
429
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1349?1359,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Unsupervised Morphology-Based Vocabulary Expansion
Mohammad Sadegh Rasooli, Thomas Lippincott, Nizar Habash and Owen Rambow
Center for Computational Learning Systems
Columbia University, New York, NY, USA
{rasooli,tom,habash,rambow}@ccls.columbia.edu
Abstract
We present a novel way of generating un-
seen words, which is useful for certain ap-
plications such as automatic speech recog-
nition or optical character recognition in
low-resource languages. We test our vo-
cabulary generator on seven low-resource
languages by measuring the decrease in
out-of-vocabulary word rate on a held-out
test set. The languages we study have
very different morphological properties;
we show how our results differ depend-
ing on the morphological complexity of
the language. In our best result (on As-
samese), our approach can predict 29% of
the token-based out-of-vocabulary with a
small amount of unlabeled training data.
1 Introduction
In many applications in human language technolo-
gies (HLT), the goal is to generate text in a target
language, using its standard orthography. Typical
examples include automatic speech recognition
(ASR, also known as STT or speech-to-text), opti-
cal character recognition (OCR), or machine trans-
lation (MT) into a target language. We will call
such HLT applications ?target-language genera-
tion technologies? (TLGT). The best-performing
systems for these applications today rely on train-
ing on large amounts of data: in the case of ASR,
the data is aligned audio and transcription, plus
large unannotated data for the language model-
ing; in the case of OCR, it is transcribed optical
data; in the case of MT, it is aligned bitexts. More
data provides for better results. For languages with
rich resources, such as English, more data is often
the best solution, since the required data is readily
available (including bitexts), and the cost of anno-
tating (e.g., transcribing) data is outweighed by the
potential significance of the systems that the data
will enable. Thus, in HLT, improvements in qual-
ity are often brought about by using larger data
sets (Banko and Brill, 2001).
When we move to low-resource languages, the
solution of simply using more data becomes less
appealing. Unannotated data is less readily avail-
able: for example, at the time of publishing this
paper, 55% of all websites are in English, the top
10 languages collectively account for 90% of web
presence, and the top 36 languages have a web
presence that covers at least 0.1% of web sites.
1
All other languages (and all languages considered
in this paper except Persian) have a web presence
of less than 0.1%. Considering Wikipedia, another
resource often used in HLT, English has 4.4 mil-
lion articles, while only 48 other languages have
more than 100,000.
2
As attention turns to de-
veloping HLT for more languages, including low-
resource languages, alternatives to ?more-data?
approaches become important.
At the same time, it is often not possible to use
knowledge-rich approaches. For low-resource lan-
guages, resources such as morphological analyz-
ers are not usually available, and even good schol-
arly descriptions of the morphology (from which
a tool could be built) are often not available. The
challenge is therefore to use data, but to make do
with a small amount of data, and thus to use data
better. This paper is a contribution to this goal.
Specifically, we address TLGTs, i.e., the types
of HLT mentioned above that generate target lan-
guage text. We propose a new approach to gener-
ating unseen words of the target language which
have not been seen in the training data. Our ap-
proach is entirely unsupervised. It assumes that
word-units are specified, typically by whitespace
and punctuation.
1
http://en.wikipedia.org/wiki/
Languages_used_on_the_Internet
2
http://meta.wikimedia.org/wiki/List_
of_Wikipedias
1349
Expanding the vocabulary of the target lan-
guage can be useful for TLGTs in different ways.
For ASR and OCR, which can compose words
from smaller units (phones or graphically recog-
nized letters), an expanded target language vocab-
ulary can be directly exploited without the need
for changing the technology at all: the new words
need to be inserted into the relevant resources (lex-
icon, language model) etc, with appropriately es-
timated probabilities. In the case of MT into mor-
phologically rich low-resource languages, mor-
phological segmentation is typically used in devel-
oping the translation models to reduce sparsity, but
this does not guarantee against generating wrong
word combinations. The expanded word combi-
nations can be used to extend the language models
used for MT to bias against incoherent hypothe-
sized new sequences of segmented words.
Our approach relies on unsupervised morpho-
logical segmentation. We do not in this paper con-
tribute to research in unsupervised morphological
segmentation; we only use it. The contribution
of this paper lies in proposing how to use the re-
sults of unsupervised morphological segmentation
in order to generate unseen words of the language.
We investigate several ways of doing so, and we
test them on seven low-resource languages. These
languages have very different morphological prop-
erties, and we show how our results differ depend-
ing on the morphological complexity of the lan-
guage. In our best result (on Assamese), we show
that our approach can predict 29% of the token-
based out-of-vocabulary with a small amount of
unlabeled training data.
The paper is structured as follows. We first dis-
cuss related work in Section 2. We then present
our method in Section 3, and present experimental
results in Section 4. We conclude with a discus-
sion of future work in Section 5.
2 Related Work
Approaches to Morphological Modeling
Computational morphology is a very active area
of research with a multitude of approaches that
vary in the degree of manual annotation needed,
and the amount of machine learning used. At one
extreme, we find systems that are painstakingly
and carefully designed by hand (Koskenniemi,
1983; Buckwalter, 2004; Habash and Rambow,
2006; D?etrez and Ranta, 2012). Next on the
continuum, we find work that focuses on defining
morphological models with limited lexica that
are then extended using raw text (Cl?ement et al,
2004; Forsberg et al, 2006). In the middle of
this continuum, we find efforts to learn complete
paradigms using fully supervised methods relying
on completely annotated data points with rich
morphological information (Durrett and DeNero,
2013; Eskander et al, 2013). Next, there is
work on minimally supervised methods that use
available resources such as dictionaries, bitexts,
and other additional morphological annotations
(Yarowsky and Wicentowski, 2000; Cucerzan and
Yarowsky, 2002; Neuvel and Fulop, 2002; Snyder
and Barzilay, 2008). At the other extreme, we
find unsupervised methods that learn morphology
models from unannotated data (Creutz and Lagus,
2007; Monson et al, 2008; Dreyer and Eisner,
2011; Sirts and Goldwater, 2013).
The work we present in this paper makes no
use of any morphological annotations whatsoever,
yet we are quite distinct from the approaches cited
above. We compare our work to two efforts specif-
ically. First, consider work in automatic mor-
phological segmentation learning from unanno-
tated data (Creutz and Lagus, 2007; Monson et
al., 2008). Unlike these approaches which provide
segmentations for training data and produce mod-
els that can be used to segment unseen words, our
approach can generate words that have not been
seen in the training data. The focus of efforts is
rather complementary: we actually use an off-the-
shelf unsupervised segmentation system (Creutz
and Lagus, 2007) as part of our approach. Second,
consider paradigm completion methods such as
the work of Dreyer and Eisner (2011). This effort
is closely related to our work although unlike it,
we make no assumptions about the data and do not
introduce any restrictions along the lines of deriva-
tion/inflectional morphology: Dreyer and Eisner
(2011) limited their work to verbal paradigms and
used annotated training data in addition to basic
assumptions about the problem such as the size
of the paradigms. In our approach, we have zero
annotated information and we do not distinguish
between inflectional and derivational morphology,
nor do we limit ourselves to a specific part-of-
speech (POS).
Vocabulary Expansion in HLT There have
been diverse approaches towards dealing with out-
of-vocabulary (OOV) words in ASR. In some
models, the approach is to expand the lexicon by
1350
adding new words or pronunciations. Ohtsuki et
al. (2005) propose a two-run model where in the
first run, the input speech is recognized by the
reference vocabulary and relevant words are ex-
tracted from the vocabulary database and added
thereafter to the reference vocabulary to build an
expanded lexicon. Word recognition is done in the
second run based on the lexicon. Lei et al (2009)
expanded the pronunciation lexicon via generat-
ing all possible pronunciations for a word be-
fore lattice generation and indexation. There are
also other methods for generating abbreviations in
voice search systems such as Yang et al (2012).
While all of these approaches involve lexicon ex-
pansion, they do not employ any morphological
information.
In the context of MT, several researchers have
addressed the problem of OOV words by relating
them to known in-vocabulary (INV) words. Yang
and Kirchhoff (2006) anticipated OOV words
that are potentially morphologically related using
phrase-based backoff models. Habash (2008) con-
sidered different techniques for vocabulary expan-
sion online. One of their techniques learned mod-
els of morphological mapping between morpho-
logically rich source words in Arabic that pro-
duce the same English translation. This was used
to relate an OOV word to a morphologically re-
lated INV word. Another technique expanded
the MT phrase tables with possible transliterations
and spelling alternatives.
3 Morphology-based Vocabulary
Expansion
3.1 Approach
Our approach to morphology-based vocabulary
expansion consists of three steps (Figure 1). We
start with a ?training? corpus of (unannotated)
words and generate a list of new (unseen) words
that expands the vocabulary of the training corpus.
1. Unsupervised Morphology Segmentation
The first step is to segment each word in the
training corpus into sequences of prefixes,
stem and suffixes, where the prefixes and suf-
fixes are optional.
3
2. FST-based Morphology Expansion We
then construct new word models using the
3
In this paper, we use an off-the-shelf system for this step
but plan to explore new methods in the future, such as joint
segmentation and expansion.
segmented stems and affixes. We explore two
different techniques for morphology-based
vocabulary expansion that we discuss below.
The output of these models is represented as
a weighted finite state machine (WFST).
3. Reranking Models Given that the size of the
expanded vocabulary can be quite large and
it may include a lot of over-generation, we
rerank the expanded set of words before tak-
ing the top n words to use in downstream
processes. We consider four reranking con-
ditions which we describe below.
Training Transcripts
Unsupervised
Morphology
Segmentation
Segmented Words
FST-based
Expansion Model
Expanded List
Reranking
Reranked Expansion
Figure 1: The flowchart of the lexicon expansion
system.
3.2 Morphology Expansion Techniques
As stated above, the input to the morphology ex-
pansion step is a list of words segmented into mor-
phemes: zero or more prefixes, one stem, and zero
or more suffixes. Figure 2a presents an example of
such input using English words (for clarity).
We use two different models of morphology ex-
pansion in this paper: Fixed Affix model and Bi-
gram Affix model.
3.2.1 Fixed Affix Expansion Model
In the Fixed Affix model, we construct a set of
fused prefixes from all the unique prefix sequences
in the training data; and we similarly construct a
1351
re+ pro+ duc +e
func +tion +al
re+ duc +e
re+ duc +tion +s
in
pro+ duct
concept +u +al + ly
(a) Training data with morpheme boundaries. Prefixes end with and suffixes start with ?+? signs.
30 1
repro
<epsilon>
re
pro
2
duc
func
in
concept
duct
e
tional
tions
utually
<epsilon>
(b) FST for the Fixed Affix expansion model
30 4re
<epsilon>
1pro
<epsilon>
2
duc
func
in
concept
duct
e
<epsilon>
5
tion
u
7
tion
6al
s
ly
<epsilon>
(c) FST for the Bigram Affix expansion model
Figure 2: Two models of word generation from morphologically annotated data. In our experiments, we
used weighted finite state machine. We use character-based WFST in the implementation to facilitate
analyzing inputs as well as word generation.
set of fused suffixes from all the unique suffix se-
quences in the training data. In other words, we
simply pick characters from beginning of the word
up to the first stem as the prefix and characters
from the first suffix to the end of the word as the
suffix. Everything in the middle is the stem. In
this model, each word has one single prefix and
one single suffix (each of which can be empty in-
dependently). The Fixed Affix model is simply
the concatenation of the disjunction of all prefixes
with the disjunction of all stems and the disjunc-
tion of all suffixes into one FST:
prefix? stem? suffix
The morpheme paths in the FST are weighted to
reflect their probability in the training corpus.
4
Figure 2b exemplifies a Fixed Affix model derived
from the example training data in Figure 2a.
4
We convert the probability into a cost by taking the neg-
ative of the log of the probability.
3.2.2 Bigram Affix Expansion Model
In the Bigram Affix model, we do the same for the
stem as in the Fixed Affix model, but for prefixes
and suffixes, we create a bigram language model
in the finite state machine. The advantage of this
technique is that unseen compound affixes can be
generated by our model. For example, the Fixed
Affix model in Figure 2b cannot generate the word
func+tion+al+ly since the suffix +tionally is not
seen in the training data. However, this word can
be generated in the Bigram Affix model as shown
in Figure 2c: there is a path passing 0? 4? 1?
2 ? 5 ? 6 ? 3 in the FST that can produce this
word. We expect this model to have better recall
for generating new words in the language because
of its affixation flexibility.
3.3 Reranking Techniques
The expanded models allow for a large number of
words to be generated. We limit the number of vo-
cabulary expansion using different thresholds af-
ter reranking or reweighing the WFSTs generated
1352
above. We consider four reranking conditions.
3.3.1 No Reranking (NRR)
The baseline reranking option is no reranking
(NRR). In this approach we use the weights in
the WFST, which are based on the independent
prefix/stem/suffix probabilities, to determine the
ranking of the expanded vocabulary.
3.3.2 Trigraph-based Reweighting (W?Tr)
We reweight the weights in the WFST model
(Fixed or Bigram) by composing it with a letter
trigraph language model (W?Tr). A letter tri-
graph LM is itself a WFST where each trigraph (a
sequence of three consequent letters) has an asso-
ciated weight equal to its negative log-likelihood
in the training data. This reweighting allows us
to model preferences of sequences of word letters
seen more in the training data. For example, in a
word like producttions, the trigraphs ctt and tti are
very rare and thus decrease its probability.
3.3.3 Trigraph-based Reranking (TRR)
When we compose our initial WFST with the tri-
graph FST, the probability of each generated word
from the new FST is equal to the product of the
probability of its morphemes and the probabilities
of each trigraph in that word. This basically makes
the model prefer shorter words and may degrade
the effect of morphology information. Instead of
reweighting the WFST, we get the n-best list of
generated words and rerank them using their tri-
graph probabilities. We will refer to this technique
as TRR.
3.3.4 Reranking Morpheme Boundaries
(BRR)
The last reranking technique reranks the n-best
generated word list with trigraphs that are incident
on the morpheme boundaries (in case of Bigram
Affix model, the last prefix and first suffix). The
intuition is that we already know that any mor-
pheme that is generated from the morphology FST
is already seen in the training data but the bound-
ary for different morphemes are not guaranteed to
be seen in the training data. For example, for the
word producttions, we only take into account the
trigraphs rod, odu, ctt and tti instead of all possible
trigraphs. We will refer to this technique as BRR.
4 Evaluation
4.1 Evaluation Data and Tools
Evaluation Data The IARPA Babel program is
a research program for developing rapid spoken
detection systems for under-resourced languages
(Harper, 2013). We use the IARPA Babel pro-
gram limited language pack data which consists
of 20 hours of telephone speech with transcrip-
tion. We use six languages which are known
to have rich morphology: Assamese (IARPA-
babel102b-v0.5a), Bengali (IARPA-babel103b-
v0.4b), Pashto (IARPA-babel104b-v0.4bY), Taga-
log (IARPA-babel106-v0.2g), Turkish (IARPA-
babel105b-v0.4) and Zulu (IARPA-babel206b-
v0.1e). Speech annotation such as silences and
hesitations are removed from transcription and all
words are turned into lower-case (for languages
using the Roman script ? Tagalog, Turkish and
Zulu). Moreover, in order to be able to perform a
manual error analysis, we include a language that
has rich morphology and of which the first author
is a native speaker: Persian. We sampled data from
the training and development set of the Persian de-
pendency treebank (Rasooli et al, 2013) to create
a comparable seventh dataset in Persian. Statis-
tics about the datasets are shown in Table 1. We
also conduct further experiments on just verbs and
nouns in the data set for Persian (Persian-N and
Persian V). As shown in Table 1, the training data
is very small and the OOV rate is high especially
in terms of types. For some languages that have
richer morphology such as Turkish and Zulu, the
OOV rate is much higher than other languages.
Word Generation Tools and Settings For un-
supervised learning of morphology, we use Mor-
fessor CAT-MAP (v. 0.9.2) which was shown to be
a very accurate morphological analyzer for mor-
phologically rich languages (Creutz and Lagus,
2007). In order to be able to analyze Unicode-
based data, we convert each character in our
dataset to some conventional ASCII character and
then train Morfessor on the mapped dataset; after
finishing the training, we map the data back to the
original character set. We use the default setting
in Morfessor for unsupervised learning.
For preparing the WFST, we use OpenFST (Ri-
ley et al, 2009). We get the top one million short-
est paths (i.e., least costly paths of words) and ap-
ply our reranking models on them. It is worth
pointing out that our WFSTs are character-based
1353
Language
Training Data Development Data
Type Token Type Token Type OOV% Token OOV%
Assamese 8694 73151 7253 66184 49.57 8.28
Bengali 9460 81476 7794 70633 50.65 8.47
Pashto 6968 115069 6135 108137 44.89 4.25
Persian 14047 71527 10479 42939 44.16 12.78
Tagalog 6213 69577 5480 64334 54.95 7.81
Turkish 11985 77128 9852 67042 56.84 12.34
Zulu 15868 65655 13756 57141 68.72 21.76
Persian-N 9204 31369 7502 18816 46.36 22.11
Persian-V 2653 11409 1332 7318 41.07 9.01
Table 1: Statistics of training and development data for morphology-based unsupervised word generation
experiments.
and thus we also have a morphological analyzer
that can give all possible segmentations for a given
word. By running the morphological analyzer on
the OOVs, we can have the potential upper bound
of OOV reduction by the system (labeled ??? in
Tables 2 and 3).
4.2 Lexicon Expansion Results
The results for lexicon expansion are shown in Ta-
ble 2 for types and Table 3 for tokens.
We use the trigraph WFST as our baseline
model. This model does not use any morphologi-
cal information. In this case, words are generated
according to the likelihood of their trigraphs, with-
out using any information from the morphologi-
cal segmentation. We call this model the trigraph
WFST (Tr. WFST). We consistently have better
numbers than this baseline in all of our models
except for Pashto when measured by tokens. ?
is the upper-bound OOV reduction for our expan-
sion model: for each word in the development set,
we ask if our model, without any vocabulary size
restriction at all, could generate it.
The best results (again, except for Pashto) are
achieved using one of the three reranking methods
(reranking by trigraph probabilities or morpheme
boundaries) as opposed to doing no reranking. To
our surprise, the Fixed Affix model does a slightly
better job in reducing out of vocabulary than the
Bigram Affix model. We can also see from the
results that reranking in general is very effective.
We also compare our models with the case that
there is much more training data and we do not do
vocabulary expansion at all. In Table 2 and Ta-
ble 3, ?FP? indicates the full language pack for
the Babel project data which is approximately six
to eight times larger than the limited pack training
data, and the full training data for Persian which
is approximately five times larger. We see that
the larger training data outperforms our methods
in all languages. However, from the results of?,
which is the upper-bound OOV reduction by our
expansion model, for some languages such as As-
samese, our numbers are close to the FP results
and for Zulu it is even better than FP.
We also study how OOV reduction is affected
by the size of the generated vocabulary. The
trends for different sizes of the lexicon expansion
by Fixed Affix model that is reranked by trigraph
probabilities is shown in Figure 3. As seen in the
results, for languages that have richer morphol-
ogy, it is harder to achieve results near to the up-
per bound. As an outlier, morphology does not
help for Pashto. One possible reason might be that
based on the results in Table 4, Morfessor does not
explore morphology in Pashto as well as other lan-
guages.
Morphological Complexity As for further anal-
ysis, we can study the correlation between mor-
phological complexity and hardness of reducing
OOVs. Much work has been done in linguis-
tics to classify languages (Sapir, 1921; Greenberg,
1960). The common wisdom is that languages
are not either agglutinative or fusional, but are
on a spectrum; however, no work to our knowl-
edge places all languages (or at least the ones we
worked on) on such a spectrum. We propose sev-
eral metrics. First, we can consider the number
of unique affixival morphemes in each language,
as determined by Morfessor. As shown in Table 4
(|pr| + |sf |), Zulu has the most morphemes and
Pashto the fewest. A second possible metric of the
1354
Language
Tr. Fixed Affix Model Bigram Affix Model FP
WFST NRR W?Tr TRR BRR ? NRR W?Tr TRR BRR ?
Assamese 15.94 24.03 28.46 28.15 27.15 48.07 23.50 28.15 27.84 26.59 51.02 50.96
Bengali 15.68 20.09 24.75 24.49 22.54 40.98 21.78 24.65 24.67 23.51 42.55 48.83
Pashto 18.70 19.03 19.28 19.24 18.63 25.13 19.43 18.81 18.92 18.77 25.24 64.96
Persian 12.83 18.95 18.39 19.30 19.99 50.11 18.58 18.09 18.65 18.84 53.13 58.45
Tagalog 11.39 14.61 16.51 16.21 16.81 35.64 14.45 16.01 15.81 16.74 38.72 53.64
Turkish 07.75 09.11 14.79 14.79 14.71 55.48 09.04 13.63 14.34 13.52 66.54 53.54
Zulu 07.63 11.87 12.96 13.87 13.68 66.73 12.04 12.35 13.69 13.75 82.38 35.62
Average 12.85 16.81 19.31 19.31 19.07 46.02 17.02 18.81 19.13 18.81 51.37 52.29
Persian-N 14.86 24.67 22.74 22.83 24.15 37.32 23.78 21.68 22.51 23.32 38.38 -
Persian-V 54.84 68.19 72.39 73.49 71.12 80.44 67.28 71.48 72.58 70.02 80.62 -
Table 2: Type-based expansion results for the 50k-best list for different models. Tr. WFST stands for
trigraph WFST, NRR for no reranking, W?Tr for trigraph reweighting, TRR for trigraph-based rereank-
ing, BRR for reranking morpheme boundary, and? for the upper bound of OOV reduction via lexicon
expansion if we produce all words. FP (full-pack data) shows the effect of using bigger data with the size
of about seven times larger than our data set, instead of using our unsupervised approach.
Language
Tr. Fixed Affix Model Bigram Affix Model FP
WFST NRR W?Tr TRR BRR ? NRR W?Tr TRR BRR ?
Assamese 18.07 25.70 29.43 29.12 28.13 47.88 25.34 29.06 28.82 27.64 50.31 58.03
Bengali 17.79 20.91 25.61 25.27 23.65 40.60 22.58 25.20 25.41 24.77 42.22 55.92
Pashto 21.27 19.40 19.94 19.92 18.59 25.45 19.68 19.40 19.29 18.72 25.58 71.46
Persian 14.78 20.77 20.32 21.30 22.03 51.00 20.63 19.72 20.61 20.95 54.01 63.10
Tagalog 12.88 14.55 16.88 16.36 16.60 33.95 14.37 16.12 16.12 16.38 37.07 61.53
Turkish 09.97 11.42 17.82 17.67 17.23 56.54 11.05 16.82 17.41 15.98 66.54 59.68
Zulu 08.85 13.70 14.72 15.62 15.67 68.07 13.70 14.07 15.47 15.60 87.90 41.27
Average 14.80 18.06 20.67 20.75 20.27 44.78 18.19 20.48 20.45 20.01 51.95 58.71
Persian-N 16.82 26.46 24.42 24.56 25.71 38.40 25.69 23.50 24.20 25.04 39.41 ?
Persian-V 60.09 71.47 75.57 76.48 73.60 82.55 70.56 74.81 75.72 72.53 82.70 ?
Table 3: Token-based expansion results for the 50k-best list for different models. Abbreviations are the
same as Table 2.
complexity of the morphology is by calculating
the average number of unique prefix-suffix pairs
in the training data after morpheme segmentation
which is shown as |If | in Table 4. Finally, a third
possible metric is the number of all possible words
that can be generated (|L|). These three metrics
correlate fairly well across the languages.
The metrics we propose also correlate with
commonly accepted classifications: e.g., Zulu and
Turkish (highly agglutinative) have higher scores
in terms of our |pr| + |sf |, |If | and |L| metrics in
Table 4 than other languages. The results from full
language packs in Table 3 also show that there is
a reverse interaction of morphological complexity
and the effect of blindly adding more data. Thus
for morphologically rich languages, adding more
data is less effective than for languages with poor
morphology.
The size of the languages (|L|) suggests that we
are suffering from vast overgeneration; we over-
generate because in our model any affix can at-
tach to any stem, which is not in general true.
Thus there is a lack of linguistic knowledge such
as paradigm information (Stump, 2001) for each
word category in our model. In other words, all
morphemes are treated the same in our model
which is not true in natural languages. One way
to tackle this problem is through an unsupervised
POS tagger. The challenge here is that fully unsu-
pervised POS taggers (without any tag dictionary)
are not very accurate (Christodoulopoulos et al,
2010). Another way is through using joint mor-
1355
Figure 3: Trends for token-based OOV reduction with different sizes for the Fixed Affix model with
trigraph reranking.
Language |pr| |stm| |sf| |L| |If|
Assamese 4 4791 564 10.8M 1.8
Bengali 3 6496 378 7.4M 1.5
Pashto 1 5395 271 1.5M 1.3
Persian 49 6998 538 184M 2.0
Tagalog 179 4259 299 228M 1.5
Turkish 45 5266 1801 427M 2.3
Zulu 2254 5680 427 5.5B 2.8
Persian-N 3 6121 268 4.9M 1.5
Persian-V 43 788 44 1.5M 3.4
Table 4: Information about the number of unique
morphemes in the Fixed Affix model for each
dataset including empty affixes. |L| shows the
upper bound of the number of possible unique
words that can be generated from the word gener-
ation model. |If | is the average number of unique
prefix-suffix pairs (including empty pairs) for each
stem.
phology and tagging models such as Frank et al
(2013).
Error Analysis on Turkish Unfortunately for
most languages we could not find an available
rule-based or supervised morphological analyzer
to verify the words generated by our model. The
only available tool for us is a Turkish finite-state
morphological analyzer (Oflazer, 1996) imple-
mented with the Xerox FST toolkit (Beesley and
Karttunen, 2003). As we can see in Table 5, the
system with the largest proportion of correct gen-
erated words reranks the expansion with trigraph
probabilities using a Fixed Affix model. Results
also show that we are overgenerating many non-
sense words that we ought to be pruning from our
results. Another observation is that the recognition
percentage of the morphological analyzer on INV
words is much higher than on OOVs, which shows
that OOVs in Turkish dataset are much harder to
analyze.
1356
Model Precision
Tr. WFST 17.19
Fixed Affix Model
NRR 13.36
W?Tr 25.66
TRR 26.30
BRR 25.14
Bigram Affix Model
NRR 12.94
W?Tr 24.21
TRR 25.39
BRR 23.45
Development
words 89.30
INVs 95.44
OOVs 84.64
Table 5: Results from running a hand-crafted
Turkish morphological analyzer (Oflazer, 1996)
on different expansions and on the development
set. Precision refers to the percentage of the words
are recognized by the analyzer. The results on de-
velopment are also separated into INV and OOV.
Error Analysis on Persian From the best 50k
word result for Persian (Fixed Affix Model:BRR),
we randomly picked 200 words and manually an-
alyzed them. 89 words are correct (45.5%) where
55.0% of these words are from noun affixation,
23.6% from verb clitics, 9.0% from verb inflec-
tions, 5.6% from incorrect affixations that acci-
dentally resulted in possible words, 4.5% from un-
inflected stems, and a few from adjective affixa-
tion. Among incorrectly generated words, 65.8%
are from combining a stem of one POS with af-
fixes from another POS (e.g., attaching a noun af-
fix to a verb stem), 14.4% from combining a stem
with affixes which are compatible with POS but
not allowed for that particular stem (e.g., there is
a noun suffix that can only attach to a subset of
noun stems), 9.0% are from wrong affixes pro-
duced by Morfessor and others are from incorrect
vowel harmony or double affixation.
In order to study the effect of vocabulary ex-
pansion more deeply, we trained a subset of all
nouns and verbs in the same dataset (also shown
in Table 1). Verbs in Persian have rich but more
or less regular morphology, while nouns, which
have many irregular cases, have rich morphol-
ogy but not as rich as verbs. The results in Ta-
ble 4 show that Morfessor captures these phenom-
ena. Furthermore, our results in Table 2 and Ta-
ble 3 show that our performance on OOV reduc-
tion with verbs is far superior to our performance
with nouns. We also randomly picked 200 words
from each of the experiments (noun and verbs)
to study the degree of correctness of generated
forms. For nouns, 94 words are correct and for
verbs only 71 words are correct. Most verb errors
are due to incorrect morpheme extraction by Mor-
fessor. In contrast, most noun errors result from
affixes that are only compatible with a subset of
all possible noun stems. This suggests that if we
conduct experiments using more accurate unsu-
pervised morphology and also have a more fine-
grained paradigm completion model, we might
improve our performance.
5 Conclusion and Future Work
We have presented an approach to generating new
words. This approach is useful for low-resource,
morphologically rich languages. It provides words
that can be used in HLT applications that require
target-language generation in this language, such
as ASR, OCR, and MT. An implementation of our
approach, named BabelGUM (Babel General Un-
supervised Morphology), will be publicly avail-
able. Please contact the authors for more infor-
mation.
In future work we will explore the possibil-
ity of jointly performing unsupervised morpho-
logical segmentation with clustering of words
into classes with similar morphological behavior.
These classes will extend POS classes. We will
tune the system for our purposes, namely OOV re-
duction.
Acknowledgements
We thank Anahita Bhiwandiwalla, Brian Kings-
bury, Lidia Mangu, Michael Picheny, Beno??t
Sagot, Murat Saraclar, and G?eraldine Walther for
helpful discussions. The project is supported by
the Intelligence Advanced Research Projects Ac-
tivity (IARPA) via Department of Defense U.S.
Army Research Laboratory (DoD/ARL) contract
number W911NF-12-C-0012. The U.S. Govern-
ment is authorized to reproduce and distribute
reprints for Governmental purposes notwithstand-
ing any copyright annotation thereon. Disclaimer:
The views and conclusions contained herein are
those of the authors and should not be interpreted
as necessarily representing the official policies
or endorsements, either expressed or implied, of
IARPA, DoD/ARL, or the U.S. Government.
1357
References
Michele Banko and Eric Brill. 2001. Scaling to very
very large corpora for natural language disambigua-
tion. In Proceedings of the 39th Annual Meeting
on Association for Computational Linguistics, ACL
?01, pages 26?33, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Kenneth R Beesley and Lauri Karttunen. 2003. Finite-
state morphology: Xerox tools and techniques.
CSLI, Stanford.
Tim Buckwalter. 2004. Buckwalter Arabic Morpho-
logical Analyzer Version 2.0. LDC catalog number
LDC2004L02, ISBN 1-58563-324-0.
Christos Christodoulopoulos, Sharon Goldwater, and
Mark Steedman. 2010. Two decades of unsu-
pervised pos induction: How far have we come?
In Proceedings of the 2010 Conference on Empiri-
cal Methods in Natural Language Processing, pages
575?584. Association for Computational Linguis-
tics.
Lionel Cl?ement, Beno??t Sagot, and Bernard Lang.
2004. Morphology based automatic acquisition of
large-coverage lexica. In Proceedings of the Fourth
International Conference on Language Resources
and Evaluation (LREC?04). European Language Re-
sources Association (ELRA).
Mathias Creutz and Krista Lagus. 2007. Unsuper-
vised models for morpheme segmentation and mor-
phology learning. ACM Transactions on Speech and
Language Processing (TSLP), 4(1):3.
Silviu Cucerzan and David Yarowsky. 2002. Boot-
strapping a multilingual part-of-speech tagger in one
person-day. In The 6th Conference on Natural Lan-
guage Learning (CoNLL-2002), pages 1?7.
Gr?egoire D?etrez and Aarne Ranta. 2012. Smart
paradigms and the predictability and complexity of
inflectional morphology. In Proceedings of the 13th
Conference of the European Chapter of the Associa-
tion for Computational Linguistics, pages 645?653.
Association for Computational Linguistics.
Markus Dreyer and Jason Eisner. 2011. Discover-
ing morphological paradigms from plain text using a
dirichlet process mixture model. In Proceedings of
the 2011 Conference on Empirical Methods in Nat-
ural Language Processing, pages 616?627. Associ-
ation for Computational Linguistics.
Greg Durrett and John DeNero. 2013. Supervised
learning of complete morphological paradigms. In
Proceedings of the 2013 Conference of the North
American Chapter of the Association for Compu-
tational Linguistics: Human Language Technolo-
gies, pages 1185?1195. Association for Computa-
tional Linguistics.
Ramy Eskander, Nizar Habash, and Owen Rambow.
2013. Automatic extraction of morphological lex-
icons from morphologically annotated corpora. In
Proceedings of the 2013 Conference on Empirical
Methods in Natural Language Processing, pages
1032?1043, Seattle, Washington, USA, October.
Association for Computational Linguistics.
Markus Forsberg, Harald Hammarstr?om, and Aarne
Ranta. 2006. Morphological lexicon extraction
from raw text data. Advances in Natural Language
Processing, pages 488?499.
Stella Frank, Frank Keller, and Sharon Goldwater.
2013. Exploring the utility of joint morphological
and syntactic learning from child-directed speech.
In Proceedings of the 2013 Conference on Empiri-
cal Methods in Natural Language Processing, pages
30?41. Association for Computational Linguistics.
Joseph H Greenberg. 1960. A quantitative approach to
the morphological typology of language. Interna-
tional journal of American linguistics, pages 178?
194.
Nizar Habash and Owen Rambow. 2006. MAGEAD:
A morphological analyzer and generator for the Ara-
bic dialects. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Compu-
tational Linguistics, pages 681?688, Sydney, Aus-
tralia.
Nizar Habash. 2008. Four techniques for online han-
dling of out-of-vocabulary words in Arabic-English
statistical machine translation. In Proceedings of the
46th Annual Meeting of the Association for Compu-
tational Linguistics on Human Language Technolo-
gies: Short Papers, pages 57?60. Association for
Computational Linguistics.
Mary Harper. 2013. The babel program and low
resource speech technology. In Automatic Speech
Recognition and Understanding Workshop (ASRU)
Invited talk.
Kimmo Koskenniemi. 1983. Two-Level Model for
Morphological Analysis. In Proceedings of the 8th
International Joint Conference on Artificial Intelli-
gence, pages 683?685.
Xin Lei, Wen Wang, and Andreas Stolcke. 2009.
Data-driven lexicon expansion for Mandarin broad-
cast news and conversation speech recognition. In
International conference on Acoustics, Speech and
Signal Processing (ICASSP), pages 4329?4332.
Christian Monson, Jaime Carbonell, Alon Lavie, and
Lori Levin. 2008. Paramor: Finding paradigms
across morphology. Advances in Multilingual and
Multimodal Information Retrieval, pages 900?907.
Sylvain Neuvel and Sean A Fulop. 2002. Unsuper-
vised learning of morphology without morphemes.
In Proceedings of the ACL-02 workshop on Morpho-
logical and phonological learning-Volume 6, pages
31?40. Association for Computational Linguistics.
1358
Kemal Oflazer. 1996. Error-tolerant finite-state recog-
nition with applications to morphological analysis
and spelling correction. Computational Linguistics,
22(1):73?89.
Katsutoshi Ohtsuki, Nobuaki Hiroshima, Masahiro
Oku, and Akihiro Imamura. 2005. Unsupervised
vocabulary expansion for automatic transcription of
broadcast news. In International conference on
Acoustics, Speech and Signal Processing (ICASSP),
pages 1021?1024.
Mohammad Sadegh Rasooli, Manouchehr Kouhestani,
and Amirsaeid Moloodi. 2013. Development of
a Persian syntactic dependency treebank. In Pro-
ceedings of the 2013 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
306?314. Association for Computational Linguis-
tics.
Michael Riley, Cyril Allauzen, and Martin Jansche.
2009. Openfst: An open-source, weighted finite-
state transducer library and its applications to speech
and language. In Human Language Technologies
Tutorials: The 2009 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 9?10.
Edward Sapir. 1921. Language: An introduction to
the study of speech. Harcourt, Brace and company
(New York).
Kairit Sirts and Sharon Goldwater. 2013. Minimally-
supervised morphological segmentation using adap-
tor grammars. Transactions for the ACL, 1:255?266.
Benjamin Snyder and Regina Barzilay. 2008. Un-
supervised multilingual learning for morphologi-
cal segmentation. In Proceedings of the 46th an-
nual meeting of the association for computational
linguistics: Human language Technologies (ACL-
HLT), pages 737?745. Association for Computa-
tional Linguistics.
Gregory T. Stump. 2001. A theory of paradigm struc-
ture. Cambridge.
Mei Yang and Katrin Kirchhoff. 2006. Phrase-based
backoff models for machine translation of highly in-
flected languages. In Proceedings of Conference of
the European Chapter of the Association for Com-
putational Linguistics (EACL), pages 41?48, Trento,
Italy.
Dong Yang, Yi-Cheng Pan, and Sadaoki Furui. 2012.
Vocabulary expansion through automatic abbrevia-
tion generation for Chinese voice search. Computer
Speech & Language, 26(5):321?335.
David Yarowsky and Richard Wicentowski. 2000.
Minimally supervised morphological analysis by
multimodal alignment. In Proceedings of the 38th
Annual Meeting on Association for Computational
Linguistics, pages 207?216.
1359
