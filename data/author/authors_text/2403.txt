Abstract
We introduce a method for using images for
word sense disambiguation, either alone, or in
conjunction with traditional text based
methods. The approach is based in recent work
on a method for predicting words for images
which can be learned from image datasets with
associated text. When word prediction is
constrained to a narrow set of choices such as
possible senses, it can be quite reliable, and we
use these predictions either by themselves or
to reinforce standard methods. We provide
preliminary results on a subset of the Corel
image database which has three to five
keywords per image. The subset was
automatically selected to have a greater
portion of keywords with sense ambiguity and
the word senses were hand labeled to provide
ground truth for testing. Results on this data
strongly suggest that images can help with
word sense disambiguation.
1 Introduction
In this paper we investigate using words and pictures to
disambiguate each other. Word sense disambiguation
has long been studied as an important problem in
natural language processing (Agirre and Rigau, 1995;
Gale et al, 1992; Manning and Sch?tze, 1999; Mihalcea
and Moldovan., 1998; Traupman and Wilensky, 2003;
Yarowsky, 1995). It is illustrated in Figure 1 with the
arguably overused ?bank? example. A priori, the word
?bank? has a number of meanings including financial
institution and a step or edge as in ?snow bank? or
?river bank?. Words which are spelt the same but have
different meanings are very common, and clearly can
confuse attempts to automatically deduce meaning from
language. Furthermore, they cannot simply be identified
as ambiguous and then ignored, as there are too many
such words and they do constrain the possible meanings
of a body of text.
Since the words are spelt the same, resolving what
they mean requires considering context. A purely
natural language based approach considers words near
the one in question. Thus in the bank example, words
like ?financial? or ?money? are strong hints that the
financial institution sense is meant. Interestingly,
despite much work, and a number of innovative ideas,
doing significantly better than choosing the most
common sense is difficult (Traupman and Wilensky,
2003).
In this work we present preliminary work on whether
an associated images can help in word sense
disambiguation. In the simplest application, text and
images might be analyzed in conjunction; for example,
a news photograph with a caption, or a larger document
with illustrations.
Word Sense Disambiguation with Pictures
Kobus Barnard, Matthew Johnson
Department of Computing Science,
721 Gould-Simpson, University of Arizona,
Tucson, Arizona, 85721-0077
{kobus, mjohnson}@cs.arizona.edu
David Forsyth
Computer Science Division,
University of California at Berkeley,
387 Soda Hall #1776
Berkeley California, 94720-1776
daf@cs.berkeley.edu
piggy bank coins currency
money
water grass trees banks
bank buildings trees city
bank machine money
currency bills
snow banks hills winter
Figure 1. Word sense ambiguity in the Corel dataset.
2 Predicting Words from Images
To integrate image information with text data we exploit
our previous work on linking images and words
(Barnard et al, 2001; Barnard et al, 2003; Barnard and
Forsyth, 2001; Duygulu et al, 2002). We have
developed a variety of methods which can be used to
predict words for image regions (region-labeling), and
entire images (auto-annotation). This is achieved in
practice by exploiting large image data sets with
associated text. Critically, we do not require that the text
be associated with the image regions, as such data is
rare. Region labeling is illustrated in Figure 2. It is
important to understand that we compute a posterior
over the complete vocabulary for each region (and/or
image), but for illustration we show the word for each
region which has maximal probability.
For the results reported in this paper we use a special
case of one of the models in (Barnard et al, 2003).
Specifically, we model the joint probability of words
and images regions as being generated by a collection of
nodes, each of which has a probability distribution over
both words and regions. The word probabilities are
provided by simple frequency tables, and the region
probability distribution are Gaussians over feature
vectors. We restrict the Gaussians to have diagonal
covariance (features are modeled as being independent).
Given an image region, its features imply a
probability of being generated from each node. These
probabilities are then used to weight the nodes for word
emission. Thus words are emitted conditioned on image
regions. In order to emit words for an entire image
(auto-annotation), as needed for our word sense
disambiguation method, we simply sum the
distributions for the N largest regions. Thus each region
is given equal weight, and the image words are forced to
be generated through region labeling.
To be consistent with the more general models
referenced above, we index the nodes by ?levels?, l.
Given a region (?blob?), b, and a word w, we have
? 
P(w | b) = P(w | l)P(b | l)P(l ) P(b)
l
? (1)
where P(l) is the level prior, P(w|l) is a frequency table,
and P(b|l) is a Gaussian over features. To estimate the
conditional density of words given blobs for the entire
image these probabilities are summed over the N largest
blobs. Parameters for the conditional probabilities
linking words and blobs are estimated from the word-
blob co-occurrence data using Expectation
Maximization (Dempster et al, 1977). For all
experiments reported in this paper we use 100 nodes.
3 Constrained Word Prediction
In most of our applications we have studied word
prediction from images in the case where prediction is
applied to a completely new image with no associated
words. However, if the new image has associated
words, our infrastructure for image/language
understanding can be exploited further. Here the main
task is not word prediction, but understanding the
relationship between the supplied words and the image
components, and hence the meaning of both. Since the
set of words of interest is known, the rest of the
vocabulary can be ignored in computation. Constraining
the vocabulary in this way makes a number of tasks
simpler. As much noise has been removed, the
relationship between the words and the image
components can be established more accurately. The
system is now chiefly determining the correspondence
relationships between known text and image regions.
One application is automatic labeling of images for
searching and browsing based on the semantics of the
image parts. As shown in Figure 3, the labeling
performance is much improved when we can constrain
the vocabulary to largely relevant words. The presence
of the words has removed ambiguity from the
interpretation of the image.
The second application is, of course, the
reverse?using the image to help reduce the ambiguity
of the words. We assume that the system has been
trained on a set of senses, S, for the vocabulary W. To
clarify the notation, we may have 
? 
bank ? W and
? 
bank _1,  bank _ 2 ? S
. Each element of S is the sense
of exactly one word in W. If we have posterior
probabilities over S based on the image, then for each
observed word, w, we can look at the corresponding
senses for w in S, and provide the sense which has the
highest posterior among the senses. More formally,
Figure 2. Illustration of labeling. Each region is
labeled with the maximally probable word, but a
probability distribution over all words is available for
each region.
(a)
(b)
Figure 3. Illustration of the observation that our ability to predict word-region correspondences increases
significantly when the words are constrained to a small set known to be relevant. We show two groups of
images which have a high probability of having a region associated with the word tiger, as computed by two
different processes. The region in the images with the highest posterior probability of ?tiger? is labeled as such.
In both cases the images shown were not used for training. In the top group (a) only image region features were
used to predict words. In the bottom group (b), words associated with the images were also available to the
program, and thus their the main task is to supply the correspondence between words and regions.
? 
P(s | w,I ) ? P(s | I)P(s | w)
(2)
where
? 
P(s | w) ?
1 if  s  is  a  sense  of  w
0  otherwise
? 
? 
? 
? 
? 
? 
(3)
4 Word Sense from Images and Text
To integrate the above with traditional word sense
disambiguation, we assume that we have a text only
method which provides a better value for
? 
P(s | w)
 in (2)
than the trivial one defined in (3). This strategy assumes
that the image provides information which is
independent of the text only method, allowing the
simplification from the factorization used in (2).
For this work, we do not compute a true probability
for 
? 
P(s | w)
, but rather a score which we use as a
surrogate. As it is the goal of this work to show that
improvement in word sense disambiguation is possible,
we use a simple word sense disambiguation method
based on the work in (Barnard et al, 2001) which itself
takes ideas from (Agirre and Rigau, 1995; Mihalcea and
Moldovan., 1998). Specifically we use the WordNet
semantic hierarchy (Miller et al, 1990) to define senses,
and give higher weight to senses more closely aligned
(lower in the tree) with the neighboring words. In the
experiments described below, the words are part of
keyword sets, and all keywords for an image are
considered neighbours.
For example, if we had the keywords [lion, pride,
rock] for a picture of a group of lions on a rock, the
word pride presents a problem to the disambiguator.  By
far, pride's most common meaning is that regarding it as
a deadly sin, however in this case we wish it to be
disambiguated as a group of lions. Even so, one can
look at the different WordNet sense hierarchies for pride
and find that one, namely:
pride
=> animal group
=> biological group
               => group, grouping
contains the words animal and biological, making it a
better fit for the hierarchy of lion.
With this structure in mind, our algorithm takes the
set of keywords and, for each keyword in the set,
performs a query such as the one shown above.  Then,
for each sense of the keyword, we perform the queries
for the other keywords, and for each of their senses we
examine the similarities between their hypernym trees.
We total up these similarities (shared nodes in the tree)
and for each sense of the keyword produce a subtotal for
that sense.  After we have performed this operation for
all senses we divide the subtotal by the complete total
for all senses to receive a score for that sense as the true
definition of the keyword.
5 Experiments
For our initial test, we studied word sense
disambiguation on the Corel image dataset which we
have used extensively for studying word prediction
from images. Each Corel image has 3-5 keywords
associated with it. Unfortunately, these keywords are
unusual in that they do not have much sense conflict
over the data set. Put differently, although a keyword
like ?head? has many senses, one sense predominates in
this data set.
To use the data despite this problem, we computed
all the senses from WordNet  (Miller et al, 1990) of all
the words for an initial set of 16,000 images, together
with the score for each sense using the method
described above. We then applied some heuristics to
create a subset of 1,800 images which had candidate
sense problems. Each word was then hand labeled with
the correct sense. The resulting dataset had only a
handful of words with ambiguous senses present in
sufficient quantity, but fortunately these were common
enough such that about 1/6 of the documents had true
word sense problems. The results reported below were
restricted to documents that had at least one word sense
ambiguity.
The data set prepared as above thus consists of a
vocabulary of word-sense combinations, together with a
human labeling of whether the sense was valid or not.
We restrict the vocabulary to word-sense pairs which
occur in at least 5 images. We represent the observed
senses for a word occurring in a document as a vector
over the senses for that word from the vocabulary. We
give all relevant senses of the word a score of one, and
incorrect senses a score of zero. We normalize this
vector so that its sum is one. Although potentially a
word could be ambiguous to a human examiner,
typically the word sense vector would simply contain a
single value of one, with the other values being zero.
We evaluate word-sense disambiguation strategies by
comparing the vector of observed word-senses
described above (the truth) to the vector containing
estimates of the relevance of each word-sense pair
corresponding to each occurring word. For example,
suppose an image has the word ?bank?, which maps to
bank_1 with hand labeling, and suppose that bank_1
and bank_3 are in the vocabulary, but no other senses of
bank. Then the vector
{?,bank_1(0.7),bank_3(0.3),?}
should be ranked better than
 {?,bank_1(0.3),bank_3(0.7),?}
when compared to the observed vector
{?,bank_1(1.0),bank_3(0.0),?} (hand-labeled)
To compare the vectors we simply normalize them and
take the dot-product.
For the experiments we divided the data into  training
data (75%), and test data (25%). We averaged results
for 10 separate runs using different samples for the test
and training sets. We restricted the computation of
results to those documents where there was clear sense
ambiguity. Because each such document typically had
only one sense problem amid 3 or 4 words without
sense problems, the baseline score using the measure
above is greater than 0.80 because any strategy will get
about 3 out or 4 correct for free. To clarify this further,
we include the results of randomly chosen among senses
when there is more than one available.
The results are shown in Table 1 strongly suggest
that images can help disambiguate senses. The na?ve
method of text based disambiguation is comparable to
chance, whereas adding image information substantially
increased the performance.
6 Conclusion
These preliminary studies strongly suggest that it is
worthwhile to explore combining image information
with more sophisticated text based words sense
disambiguation approaches.  However, while the
preliminary results are encouraging, it is critical that we
take the next step and apply the method to a data set
where there is more sense ambiguity. Possible
candidates which we are actively investigating include
the museum data used in (Barnard et al, 2001) and
news photos with captions available on the web.
In general we have found that it is fruitful to study
how image and text information can both compliment
each other and disambiguate one another. Different
representations of the same thing can help learn co-
constructed meaning. Properties which may be implicit
in one representation may be more explicit and thus
more amenable for automatic extraction in another.
Furthermore, relationships between the representations,
which can be learnt from large corpora, can be brought
to bear on the problem. In particular, in the case of
disambiguating words, we have shown that images can
provide a non-negligible amount of information which
can be exploited by more traditional approaches.
References
Agirre, E. and Rigau, G., 1995. A proposal for word
sense disambiguation using conceptual distance, 1st
International Conference on Recent Advances in
Natural Language Processing, Velingrad.
Barnard, K., Duygulu, P. and Forsyth, D., 2001.
Clustering Art, IEEE Conference on Computer
Vision and Pattern Recognition, Hawaii, pp. II:434-
441.
Barnard, K. et al, 2003. Matching Words and Pictures.
Journal of Machine Learning Research, 3: 1107-
1135.
Barnard, K. and Forsyth, D., 2001. Learning the
Semantics of Words and Pictures, International
Conference on Computer Vision, pp. II:408-415.
Dempster, A.P., Laird, N.M. and Rubin, D.B., 1977.
Maximum likelihood from incomplete data via the
EM algorithm. Journal of the Royal Statistical
Society. Series B (Methodological), 39(1): 1-38.
Duygulu, P., Barnard, K., de Freitas, J.F.G. and Forsyth,
D.A., 2002. Object recognition as machine
translation: Learning a lexicon for a fixed image
vocabulary, The Seventh European Conference on
Computer Vision, Copenhagen, Denmark, pp. IV:97-
112.
Gale, W., Church, K. and Yarowsky, D., 1992. One
Sense Per Discourse, DARPA Workshop on Speech
and Natural Language, New York, pp. 233-237.
Manning, C. and Sch?tze, H., 1999. Foundations of
Statistical Natural Language Processing. MIT Press.
Cambridge, MA.
Mihalcea, R. and Moldovan., D., 1998. Word sense
disambiguation based on semantic density,
COLING/ACL Workshop on Usage of WordNet in
Natural Language Processing Systems, Montreal.
Miller, G.A., Beckwith, R., Fellbaum, C., Gross, D. and
Miller, K.J., 1990. Introduction to WordNet: an on-
line lexical database. International Journal of
Lexicography, 3(4): 235 - 244.
Traupman, J. and Wilensky, R., 2003. Experiments in
Improving Unsupervised  Word Sense
Disambiguation. CSD-03-1227, Computer Science
Division, University of California Berkeley.
Yarowsky, D., 1995. Unsupervised Word Sense
Disambiguation Rivaling Supervised Methods, 33rd
Conference on Applied Natural Language
Processing. ACL, Cambridge.
Word sense disambiguation strategy Score
Na?ve text based method 0.858 (0.008)
Random sense choice 0.875 (0.012)
Image and text method 0.948 (0.015)
Table 1. Word sense disambiguation results on data
held out from training. The results are the average over
10 runs where a different 75% of the data was used for
training and the other 25% was held out for testing.
Results were computed only on images with at least
one ambiguous term. Because typically only one out of
four or five words was ambiguous, the baseline score is
quite high as reinforced by the random result. The
scoring is explained in ?5. Error estimates are in
parentheses.
Words and Pictures in the News
Jaety Edwards
UC Berkeley
jaety@cs.berkeley.edu
Ryan White
UC Berkeley
ryanw@cs.berkeley.edu
David Forsyth
UC Berkeley
daf@cs.berkeley.edu
Abstract
We discuss the properties of a collection of
news photos and captions, collected from the
Associated Press and Reuters. Captions have
a vocabulary dominated by proper names. We
have implemented various text clustering algo-
rithms to organize these items by topic, as well
as an iconic matcher that identifies articles that
share a picture. We have found that the spe-
cial structure of captions allows us to extract
some names of people actually portrayed in the
image quite reliably, using a simple syntactic
analysis. We have been able to build a directory
of face images of individuals from this collec-
tion.
1 Introduction
For the past year we have been building a collection of
captioned news photos and illustrated news articles. We
believe that, for many applications, words and pictures
together provide very rich information on document con-
tent. Photographs can link articles in ways that pure tex-
tual analysis may overlook or underestimate, and text
provides high level descriptions of image contents that
current vision techniques cannot obtain.
Our analysis of image captions has revealed various
journalistic conventions that we believe make this dataset
particularly appealing for a number of applications. Cap-
tions act as concise summaries of events, and we believe
we can use them to isolate the most pertinent words for
different topics. We have implemented various clustering
methods to explore this idea.
Captions are also tightly tied to the actual content of
the image. The difficulty here on the text side, is in identi-
fying those portions of a caption that refer to tangible ob-
jects, physically present in the image. On the image side,
we need to isolate objects of interest and solve the corre-
spondence problem between caption extracts and image
regions. We are exploring these issues in the context of
trying to build an automated celebrity directory and face
classifier.
2 The Collection
We have been collecting news photos with associated
captions since Feb. 2002. These photographs come from
Reuters and the AP, and we collect between 800 and 1500
per day. To date, we have collected more than 410,000
unique images. The vast majority of these photos are of
people, and the majority of that subset focus on one or
two individuals. Since November, we have also collected
more than 75,000 illustrated news articles from the BBC
and CNN. Both CNN and the BBC, along with the ma-
jority of all news outlets, make heavy use of news pho-
tographs from the AP and Reuters. In fact, the field is
dominated by just three agencies, Reuters, the Associated
Press and AFP.
Journalistic writing guidelines emphasize clarity. This
is certainly necessary in caption writing, where an author
has an average of 55 words to convey all salient details
about an image. A caption writer?s first responsibility is
to identify those portrayed. A caption will contain the
full names of those represented if they are known. Sec-
ond, the writer must describe the activity portrayed in the
photo. Finally, with whatever space is left, the author
may provide some broader context.
Our collection?s vocabulary reflects this ordering of
priorities. First, there is a heavy bias towards proper
names, and thus the number of unique terms in this set is
very large (See figure 1). The vocabulary of other word
classes, however, reflect the journalistic emphasis on sim-
plicity. People hug rather than embrace, talk rather than
converse.
Captions are also easy for humans to parse because
writers make use of stylized sentence structures. Just
Figure 1: Caption vocabularies exhibit statistical properties that distinguish them from other collections. A heavy
preponderance of proper names creates a very large vocabulary of capitalized words whose frequency distribution
has heavy tails. On the other hand, other word classes are somewhat restricted. In a six month period from July to
December, 2002, the corpus had 93,457 distinct terms. Of these, 60,182 were capitalized and 32,059 uncapitalized
(with 1216 numerical entries). About 1000 terms occur in both forms. Almost a third of all vocabulary terms occur
only once. On the left we plot term occurrence ordered by frequency for the 1000 most frequent capitalized (solid)
and uncapitalized ( dotted) terms. On the right we analyze the heavy tail, flipping the axes to show the number of
words that occur between one and twenty times. Here, capitalized (solid line) words outnumber uncapitalized (dotted
line) ones 2-1. We have used capitalization as a proxy for proper names. This obviously misrepresents initial words
of sentences, but these only represent on average 2 words out of every 50-60 word caption. The richness of our
vocabulary is thus largely due to proper names.
over 50% of the captions in our collection, for instance,
begin with a starting noun phrase that describes the cen-
tral figure of the photograph followed by a present tense
verb that describes the action they are performing (or if
in the passive voice, having performed upon them). Tex-
tual cues such as ?left?,?right? or a jersey number help to
clarify any potential correspondence problems between
multiple names in the caption and people in the image.
The news photos themselves, like their captions, are
also quite stylized with respect to choice of subject,
placement within the image, and photographic tech-
niques. A single individual will generally be centered in
the photo. Two people will be equally offset to the left
and right of center. A basketball player will most likely
have the ball in his hands. Each of these photographs will
employ a narrow depth of field to blur the background
and emphasize their subjects. Like the caption writer, the
photo journalist must convey a great deal of information
in a small amount of space. These conventions amount
to a shared language between photographer and reader
that places this single image in a far richer context. We
present example captions and their associated images in
figure 2.
The textual and photographic conventions we have il-
lustrated are simply trends we have noticed in our dataset,
ones that many individual captions and images in our col-
lection break. One of the benefits of scale, however, is
that we can throw away a great deal of data and still have
meaningful sets with which to work. These simpler sets
in turn may act as foundations from which to attack the
exceptions.
3 Linking Articles
We have linked and clustered articles in a variety of ways.
We have looked at how the specialized vocabulary and
statistics of our dataset affect the performance of two dif-
ferent textual clustering methods. We have also examined
how images might help to link together articles whose re-
lationships purely text based clustering overlooks or un-
dervalues. We have also used the clusterings built from
our captions to provide a topic structure for navigating
the news in general, and investigated various interfaces
to make this navigation more natural.
3.1 Iconic Matching
Sometimes the same photograph is used to illustrate dif-
ferent articles. This establishes links between articles that
might be difficult to discern solely from the text. To es-
tablish these links, we built an iconic matcher to find
copies of the same image in a database, even if that image
has been moderately cropped or given a border.
Images can change in many subtle ways between their
distribution and their actual publication. They may be
cropped, watermarked, and acquire compression arti-
facts. The news agency may overlay text or add borders.
An iconic matcher needs to be robust to these changes.
In addition, given a collection of this size, only minimal
image processing is practical.
The iconic matcher we have designed first resizes im-
ages to 128x128 pixels in order to accommodate different
aspect ratios. It then applies a Haar wavelet transforma-
tion separately to each color channel of the image. (Ja-
U.S. President George W. Bush (news - web sites) ponders a question at a news confer-
ence following his meeting with Czech President Vaclav Havel at Prague Castle, Wednes-
day, Nov. 20, 2002. Bush urged the NATO (news - web sites) allies to join a U.S.-led
?coalition of the willing? to ensure Iraq disarms. (AP Photo/CTK/ Michal Dolezal) Nov 20
5:52 AM
Sacramento Monarchs? Ruthie Bolton, center, grabs a loose ball from Charlotte Sting?s
Erin Buescher, bottom, as Monarchs? Andrea Nagy moves in during the first half Friday,
July 19, 2002, in Charlotte, N.C. (AP Photo/Rick Havner) Jul 19 8:24 PM
Figure 2: News captions follow many conventions. Individuals are identified with their full names whenever possible.
The first sentence describes the actors, and what they are doing. By looking for proper names immediately followed
by present tense verbs, we can reliably find names that reference people actually in the image. This is important, as
captions are also full of names (i.e. Vaclav Havel in caption 1) that do not actually appear. Often, the captions also
help place the person in the frame (i.e. ?center? in the second image).
cobs et al, 1995) The first coefficient in each channel,
then, represents the average image intensity; the second
is the deviation from the mean for the right and left side;
the third splits the left hand side similarly, and so on. Ro-
tating through the channels, we use the first 64 largest
coefficients as a signature for the image. We say a pair of
coefficients match if both are within a given threshold of
each other. Images match if their first three coefficients
(average color) all match, and if the number of additional
matching coefficients is above a certain count. Identical
images will of course receive a perfect score using this
measure.
To tune the parameters of the matcher, we found sam-
ple photos to which CNN or the BBC had added borders
and the corresponding borderless originals from AP and
Reuters. We then examined how these changes affected
the coefficients, and we set the matching thresholds to
respond positively even with these changes. This led to
fairly broad ranges, ?5 for the average color coefficients
and ?3 for the rest (out of 0-255 scale). However, we
also insisted that at least 42 coefficients actually match.
A visual scan of the sets returned by our iconic matcher
shows very few false positives. The matcher frequently
accommodates borders or croppings that change up to
10% of the pixels in the original image. Its response to
these changes, however, is somewhat unpredictable. Two
crops of similar appearance can have very different ef-
fects upon the Haar parameters if one pushes many fea-
tures into different quadrants of the image than the other.
As Table 1 indicates, a significant percentage (10%) of
those articles we have collected from the BBC and CNN
share an image with another article. This phenomenon
seems to stem from three main sources. Across collec-
tions, different authors may use the same image to illus-
trate similar stories. Authors may also use the same im-
Table 1: Iconic Matches
Iconic Matches Collection Stats
Collection Total Docs BBC CNN
BBC 16990 BBC 3793 223
CNN 8397 CNN 1233
A shared picture is a very good indication that two ar-
ticles are in some way topically related. Our iconic
matcher establishes those links. On the left, the figure
gives the total number of articles we collected in a three
month (Oct-Dec, 2002) period from the BBC and CNN
collection, and on the right, how often the same image
was used for multiple stories. We have split these iconic
matches out into inter-agency and intra-agency totals.
More than 10 % of the articles in our collection share
an image.
age over time to provide temporal continuity as the un-
derlying story changes. Finally, the same image may be
used to indicate a broad theme, while the articles them-
selves discuss quite different topics. A series of articles
on an oil spill of the coast of Spain, for example, moved
from simply reporting the incident, to investigating the
captain, to discussing the environmental impact, always
using an illustration of an oil drenched cormorant.
3.2 Text Clustering
Our second tool for grouping articles comes from auto-
mated clustering of the AP and Reuters caption texts. We
implemented two different clustering algorithms. Each
generates a probability distribution over K topics for each
document. Documents are then assigned to the maximum
likelihood cluster.
We fit two models to the data. The first was a simple
unigram mixture model. This model posits that caption
words are generated by sampling from a multinomial dis-
tribution over the corpus vocabulary V . Topics are sepa-
rate distributions over V . For a given number of topics,
we have |K| ? |V | parameters, where K is the set of top-
ics. The probability of a caption in this model is
?
k?K
(
p(k)
?
w?V
p(w|k)
)
(1)
We can fit this model using EM, with the assignments
of captions to topics as our hidden variables. In our im-
plementation, we initialize p(k) and p(w|k) with random
distributions.
Effectively, we treat each caption as an unordered set
of keywords. Although this is a simple language model,
we expected it to fit the captions fairly well. Given their
extremely short length, we believed captions to have a far
higher percentage of topically important (and thus dis-
criminative) words than one would find in a more generic
article collection. In longer documents researchers have
investigated modelling documents as mixtures of topics
(Blei et al, 2001), but we believed captions truly were
narrowly focused around a single topic.
Still, our original vocabulary contained over 90,000
terms. To fit this model, we trimmed the tail end of
the vocabulary. We applied two heuristics. removing all
words that happened less than 200 times. This seems a
drastic reduction, but as figure 1 illustrates, the tail of our
vocabulary is primarily proper names. Moreover, we col-
lect more than 1000 captions a day. A single word, espe-
cially a name, could therefore easily occur 200 times in
a single day. Since we are interested in topics of larger
temporal extent than a day, this reduction seems at least
somewhat justified. We also removed all words of three
characters or less. During fitting, we normalized the doc-
ument word count vectors to a constant length.
Unfortunately, the model was still overwhelmed by
common words, and the maximum likelihood configura-
tion invariably driven to make every topic equally likely
and every topic distribution almost exactly the same. We
therefore were forced to additionally remove very com-
mon words, namely the 2000 most frequent words from
the web at large.1 This heuristic almost certainly removes
some words strongly associated with specific topics (e.g.
?ball? with sports). Still, the remaining middle frequency
words do a good job of separating captions into quali-
tatively good topics, Our contention is that this middle
frequency is closely aligned with the true statistics of the
entire corpus.
Our second algorithm, which we call a ?two-level?
mixture model, attempts to deal with very common words
1Berkeley Digital Library ?Web Term Document Fre-
quency? http://elib.cs.berkeley.edu/docfreq/index.html
in a more principled way. The top level is a single multi-
nomial distribution ? shared by all captions. The second
level has K topic distributions, equivalent to the sim-
ple unigram model. For each caption word, we sam-
ple a Bernoulli random variable ? to decide whether to
draw from the top-level or second-level distribution. This
model shunts common shared words into the top-level
?junk? distribution, leaving the topic distributions to re-
flect truly distinctive words. The probability of a docu-
ment in this model is
?
k?K
(?
w?V
(?p(w|k)p(k) + (1? ?)p(w|?))
)
(2)
We used EM once again to estimate these parameters. Re-
gardless of starting position or |K|, ? consistently con-
verges to just over .5 (.53 with std .004).
3.3 Quantitative Analysis
We fit each of these models 10 different times each for
K = 10, 20, ...100 where K is the number of topics. To
compare the quality of fit between different values of K,
we held out 10% of the captions and compared the neg-
ative log likelihood of the held out data for each run. A
model which assigns the highest probability to the test set
will have the lowest log likelihood. In figure 3, we plot
the average negative log likelihood for each of these runs.
Across all K, EM converged in just a few iterations.
3.4 Evaluation and Interfaces
Quantitatively our models appear to fit well, but an anal-
ysis of their usefulness for interacting with these collec-
tions is more difficult. Our collection has no canonical
topical structure against which we can compare our re-
sults. However, we have built various tools to aid in the
qualitative evaluation of our results.
3.4.1 Temporal Structure
The clustering algorithms discussed in the previous
section take no account of the time at which an article or
caption was published. News topics, however, certainly
have temporal structure. Some, like baseball, happen dur-
ing a specific season. Others, like an election day, are
events that are heavily discussed for a brief period of time
and then fade. Our first interface illustrates these tempo-
ral relationships. We plot each topic over time. Each
timestep is approximately a day.2 Each element in the
plot of a cluster represents the percentage of captions in
that cluster collected during each time period. We have
constructed a web interface that lays out all K topics as
columns in a matrix, moving through time from top to
2A timestep actually represents 1000 chronologically con-
tiguous captions, but we collect an average of 1000 captions a
day.
Figure 3: We fit two clustering models. The first was a simple unigram mixture model with K topics. The second
?Two Level? model extended the first with a global distribution such that words could choose to come either from the
topic distribution associated with that document or from the global one. This figure plots the average and minimum
negative log likelihood attained for values of K = 10, 20, 30, ..., 100. The simple unigram model uses a vocabulary
filtered of common words, 2000 terms smaller than that used by the ?two level? model. This accounts for the higher
log likelihood numbers on the right. When fit with the full vocabulary, the simple unigram model invariably creates K
identical topics, losing all topic structure to the noise generated from common words.
bottom. One may click on any individual element to bring
up a list of captions specific to that period and cluster, as
well as the word distribution that characterizes this topic.
(Figure 4) The example in this figure utilizes the simple
unigram model. One striking aspect of this view is that
topics clearly do appear to have time signatures. Some
are periodic, others ramp up, others are extremely peaked.
We are contemplating methods to integrate these tempo-
ral features into our clustering methods.
3.4.2 2D Embedding
Our second interface attempts to lay out topics on the
plane in such a way that distances between them are se-
mantically meaningful. First, we use a symmetrized KL
divergence to define a distance metric between topic dis-
tribution pairs. We define a symmetric KL divergence
between two topics distributions ti and tj as
KLsymmetric = 12(KL(ti||tj) +KL(tj ||ti)) (3)
KL(ti||tj) =
?
w?V
((p(w|ti)log( p(w|ti)p(w|tj) )) (4)
where V is the corpus vocabulary.
We then use Principal Coordinate Analysis to project
into the plane in a way that in various senses ?best? pre-
serves distances. We finally calculate the likelihood for
all captions in a given cluster and illustrate the topic with
the image associated with the maximum likelihood cap-
tion.
In our interface, (Figure 5) one may click on any to-
ken to bring up a list of all images and articles associ-
ated with this topic. In this example, we have actually
used the topic structure generated with captions to orga-
nize the BBC and CNN combined dataset. The clusters in
this figure were built using the simple unigram clustering
model.
One aspect of Principal Coordinate Analysis that is un-
desirable in our case is that it tends to emphasize accu-
rately representing large distances at the expense of small
ones. In topic space, distances between topics only seem
to have semantic meaning up to some threshold. Beyond
this threshold they are simply unrelated. We are actively
working on implementing a modified version of Principal
Coordinate Analysis that will lend more weight to smaller
distances for this interface.
4 A Celebrity Face Detector
Our second area of investigation with this dataset is au-
tomated methods for establishing correspondences be-
tween textual words or phrases and image regions. To
this end, we are investigating the creation of an automated
celebrity classifier from the AP and Reuters photographs.
Brown et al (P. Brown and Mercer, 1993) have
effectively used co-occurrence statistics to build bilin-
gual translation models. Duygulu, Barnard and Forsyth
(Duygulu et al, 2002) have effectively used these ma-
chine translation algorithms to establish correspondences
between keywords and types of image regions in the
Corel image collection.
Unlike Corel, our photos are primarily of people, and
so it seemed natural to focus first on proper names as op-
posed to more general noun classes. Proper nouns are rel-
atively easy to extract from our captions. Caption writers
are quite consistent in how they record individual?s names
and titles. Simply selecting strings of capitalized words
with just a few heuristics to accommodate the beginnings
of sentences, middle initials, etc. performs well. Com-
mon names like the President?s will be written in multiple
Figure 4: In this figure we see a clustering of 50 topics laid out temporally. The captions are clustered without respect
to time using EM and the simple unigram language model. In this example we have used 50 clusters. Each column
represents a single topic, and each row is approximately a one day time slice starting July 19, 2002 at top and ending
on Dec 8, 2002 at the bottom. The brightness of the entry reflects the percentage of all captions in this topic that
occurred during this time slice. To illustrate, we have labeled certain portions of the figure with the realworld topics
or events with which certain topics/time periods seem most associated. Topics appear to have time signatures. Some,
like football or championship baseball, are periodic. Others, like election day, slowly build to a peak and then rapidly
fade. Other, unexpected events, such as the arrest of the D.C. area snipers and Moscow hostage situation ramp up
suddenly and then slowly fade over time. We are investigating adding this temporal information into our clustering
models.
ways, but even in these cases a single form is overwhelm-
ingly predominant. As for the images, face detectors are
a relatively mature piece of vision technology and we can
reliably extract a large set of face cutouts.
We could not directly apply the co-occurrence meth-
ods used in previous work. First, our captions are full of
proper nouns (institutions, locations, and other people)
that have no visual counterpart in the image. Duygulu
et al faced a similar problem for certain keywords in
the Corel dataset, but to a far smaller degree. They could
treat it as a noise problem. Here, it overwhelms the actual
signal.
The image side compounds our problem. Duygulu et
al. were able to cluster image regions into equivalence
classes based on a set of extracted features. We have no
similarity metric for faces. Typically, one induces a met-
ric by fitting a parametric model to the data and exploring
differences in parameter space. Parametric models per-
form poorly on faces, however, due to the variability of
facial expressions.
If one were to manually label our collection with
names and locations of the individuals portrayed in each
photo, we believe we could generate hundreds or thou-
sands of unique face images for many individuals. Given
this supervised dataset, it might be possible to generate
a non-parametric model of faces, fitting a set of models
for each expression. Manually annotating half a million
photographs is impractical. Instead we have leveraged
the special linguistic structure of our captions to create a
?photographic entity? detector. In other words, a proper
name finder optimized to return only those names that ac-
tually occur in the photograph.
4.1 Who Is In The Picture?
We are confronted with the problem of trying to identify
those proper names that actually identify people in the
photo. A small amount of linguistic analysis has been
extremely helpful. We tried various proper name detec-
tors, but it proved difficult to return only people, let alne
people who actually appear in the images. Once again,
however, we can exploit journalistic conventions. With
few exceptions, the first sentence of a caption describes
the activity and participants in the picture itself. In 51%
of our captions, the beginning of the first sentence follows
the pattern [Noun Phrase][Present Tense Action Verb],
or less commonly [Noun Phrase][Present Tense Passive
Figure 5: In this interface we have defined a two dimensional distance metric between topics and laid them out on the
plane, illustrated with representative photos from the collection. The right hand side illustrates a closeup of the upper
right corner of this space, an area dominated by sports related topics. In our interface one may click on any token
to bring up a list of additional images and articles associated with this topic. In this example, the topics have been
generated using captions from the AP and Reuters photos, but we are actually using this topic structure to navigate
articles from CNN and the BBC. We contend that clustering with these captions generates topic distributions that focus
on words highly relevant to the topic. Topics are defined as probability distributions across the corpus vocabulary.
Our 2D embedding is derived by calculating the symmetrized KL divergence between each pair of topics and using
principle coordinate analysis to project into two dimensions in a manner that ? best? preserves the distances in the
original high dimensional space. (Ripley, 1996)
Verb]. (see figure 2)
The detector we have built identifies potential proper
names as strings of two or more capitalized words fol-
lowed by a potential present tense verb. Words are clas-
sified as possible verbs by first applying a list of morpho-
logical rules to possible present tense singular forms, and
then comparing these to a database of known verbs. Both
the morphological rules and the list of verbs are from
WordNet. (Wor, 2003)
When there is more than one person in the photo, the
author often inserts a term directly following the proper
name to help disambiguate the correspondence. This will
either be a position such as left or right, or an identify-
ing characteristic. This second form is most frequently
used with sports figures and gives a jersey number. Our
proper name finder returns the name, the disambiguator
if it exists, and the verb.
Our classifier either accepts a caption and returns a pro-
posed name or rejects the caption. We tested it on the
same sample we used for clustering (146,870 captions).
The name finder accepts 47% of these captions. We man-
ually examined 400 rejected captions. Of these, 50%
were true misses, where the caption contained a name
that matched a face in the image. Another 35% were im-
ages of people but the caption either contained no proper
names or only proper names of people that did not appear
in the image. The final 15% were images that contained
no people.
We also examined 1000 accepted captions. In 85% of
this sample, the classifier accurately extracted the name
of at least one person in the image. Of these errors,
the vast majority still followed our pattern of [Noun
Phrase][Present Tense Verb], and the subject of the noun
phrase actually did appear in the picture. Our rules sim-
ply failed to accurately parse the noun phrase. Over half
of these mistakes, for instance, were due to the phrase
?[Proper Name] of the United States [Verb],? and our
classifier returned ?United States? instead of the cor-
rect individual. More robust proper name finders should
largely eliminate these sources of error. The more impor-
tant point is that captions are so carefully structured that
very simple rules accurately model most of the collec-
tion. We could conceivably even use our face classifier to
learn some of these structural rules. If we could use the
images to posit names, and even positions, we might be
able to use this as a handle for learning certain types of
caption structure. If a photo were to have two strong face
responses, for instance, we might learn to look for those
? left?,?right? indicators in the text. We would also like
to investigate the possible clustering of verbs from image
data.
4.2 Iterative Improvements
With aggressive pruning of questionable outputs from the
name and face finders, we believe we can generate an
effectively supervised dataset of faces for thousands of
Figure 6: Not every person named in a caption appears in a picture. However, quite simple syntactic analysis of
captions yields some names of persons whose faces are very likely to appear in the picture. By looking for items where
the picture has a single, large face detector response ? so there is only one face present ? and analysis of the caption
produces a single name, we produce a directory of news personalities that is quite accurate. The top row shows some
entries from this directory: there are relatively few instances of each face, because our tests are quite restrictive, but
the faces corresponding to each name are correct and are seen from a variety of angles, meaning we may be able
to build a non-parametric face model for some individuals by a completely automatic analysis. The next three rows
show some possible failure modes of our approach: First, our analysis of the caption could yield more than one name.
Second, there may be more than one large face in the image, with only the wrong face producing a face detector
response. Third, the syntax may occasionally follow a syntactic pattern our algorithm does not handle. We are able
to extract proper names from 68,496 of 146,870 captions, with an estimated 85% of these actually naming a person
in the image. Restricting ourselves solely to large face responses, we are able to produce a gazetteer of 452 distinct
names (621 images total), containing only 60 incorrectly filed images.
individuals, many of whom will have hundreds or even
thousands of distinct face images. From these we hope to
build a non- parametric model of faces. The next interest-
ing task will be to investigate whether we can then return
to the textual side, using our face models to learn more
about linguistic structures of the captions. Bootstrapping
by alternating between the two sides of a mixed dataset
seems a very powerful model.
5 Conclusion
News photo captions are an interesting dataset both for
their unique textual properties, and for the opportunities
they provide to exploit relationships between the text and
image contents. We have used these captions to illumi-
nate underlying topical structure in the collection. This
research indicates that captions act much like very short
summaries, emphasizing words that are strongly associ-
ated with underlying themes in the news. We are in-
vestigating how well this topical structure translates to
more general collections of news articles. We have also
shown that images can provide links between articles that
are missed by textual analysis alone. Separately, we are
investigating the possibility of building non-parametric
models of celebrity faces. This line of research indicates
that by combining a face detector with an analysis of the
linguistic conventions of the text, captions can be used as
an almost supervised dataset of people in the news.
References
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2001. Latent Dirichlet Allocation. In Neural Informa-
tion Processing Systems 14.
P. Duygulu, K. Barnard, J.F.G. De Freitas, and D.A.
Forsyth. 2002. Object recognition as machine transla-
tion: Learning a lexicon for a fixed image vocabulary.
Charles E. Jacobs, Adam Finkelstein, and David H.
Salesin. 1995. Fast multiresolution image query-
ing. Computer Graphics, 29(Annual Conference
Series):277?286.
Christopher D. Manning and Hinrich Schutze. 1999.
Foundations of Statistical Natural Language Process-
ing. MIT Press.
V.J. Della Pietra P. Brown, S.A. Della Pietra and R.L.
Mercer. 1993. The mathematics of statistical machine
translation: Parameter estimation. Computational Lin-
guistics, 32(2):263?311.
B. D. Ripley. 1996. Pattern Recognition and Neural Net-
works. Cambridge University Press, Cambridge.
2003. Wordnet, a lexical database for the English lan-
guage.
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 547?554,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Discriminating image senses by clustering with multimodal features
Nicolas Loeff
Dept. of Computer Science
University of Illinois, UC
loeff@uiuc.edu
Cecilia Ovesdotter Alm
Dept. of Linguistics
University of Illinois, UC
ebbaalm@uiuc.edu
David A. Forsyth
Dept. of Computer Science
University of Illinois, UC
daf@uiuc.edu
Abstract
We discuss Image Sense Discrimination
(ISD), and apply a method based on spec-
tral clustering, using multimodal features
from the image and text of the embedding
web page. We evaluate our method on a
new data set of annotated web images, re-
trieved with ambiguous query terms. Ex-
periments investigate different levels of
sense granularity, as well as the impact of
text and image features, and global versus
local text features.
1 Introduction and problem clarification
Semantics extends beyond words. We focus on im-
age sense discrimination (ISD)1 for web images
retrieved from ambiguous keywords, given a mul-
timodal feature set, including text from the doc-
ument which the image was embedded in. For
instance, a search for CRANE retrieves images of
crane machines, crane birds, associated other ma-
chinery or animals etc., people, as well as images
of irrelevant meanings. Current displays for im-
age queries (e.g. Google or Yahoo!) simply list
retrieved images in any order. An application is
a user display where images are presented in se-
mantically sensible clusters for improved image
browsing. Another usage of the presented model
is automatic creation of sense discriminated image
data sets, and determining available image senses
automatically.
ISD differs from word sense discrimination and
disambiguation (WSD) by increased complexity
in several respects. As an initial complication,
both word and iconographic sense distinctions
1Cf. (Schu?tze, 1998) for a definition of sense discrimina-
tion in NLP.
matter. Whereas a search term like CRANE can
refer to, e.g. a MACHINE or a BIRD; iconographic
distinctions could additionally include birds stand-
ing, vs. in a marsh land, or flying, i.e. sense-
distinctions encoded by further descriptive modi-
fication in text. Therefore, as the number of text
senses grow with corpus size, the iconographic
senses grow even faster, and enumerating icono-
graphic senses is extremely challenging; espe-
cially since dictionary senses do not capture icono-
graphic distinctions. Thus, we focus on image-
driven word senses for ISD, but we acknowledge
the importance of iconography for visual meaning.
Also, an image often depicts a related mean-
ing. E.g. a picture retrieved for SQUASH may
depict a squash bug (i.e. an insect on a leaf of
a squash plant) instead of a squash vegetable,
whereas this does not really apply in WSD, where
each instance concerns the ambiguous term itself.
Therefore, it makes sense to consider the divi-
sion between core sense, related sense, and un-
related sense in ISD, and, as an additional com-
plication, their boundaries are often blurred. Most
importantly, whereas the one-sense-per-discourse
assumption (Yarowsky, 1995) also applies to dis-
criminating images, there is no guarantee of
a local collocational or co-occurrence context
around the target image. Design or aesthetics may
instead determine image placement. Thus, con-
sidering local text around the image may not be as
helpful as local context is for standard WSD. In
fact, the query term may even not occur in the
text body. On the other hand, one can assume that
an image spotlights the web page topic and that it
highlights important document information. Also,
images mostly depict concrete senses. Lastly, ISD
from web data is complicated by web pages being
more domain-independent than news wire, the fa-
547
(a) squash flower (b) tennis? (c) hook (d) food (e) bow (f) speaker
Figure 1: Example RELATED images for (a) vegetable and (b) sports senses for SQUASH, and for (c-d) fish and (e-f) musical
instrument for BASS. Related senses are associated with the semantic field of a core sense, but the core sense is visually absent
or undeterminable.
Figure 2: Which fish or instruments are BASS? Image sense annotation is more vague and subjective than in text.
vored corpus for WSD. As noted by (Yanai and
Barnard, 2005), whereas current image retrieval
engines include many irrelevant images, a data set
of web images gives a more real-world point of
departure for image recognition.
Outline Section 2 discusses the corpus data and
image annotation. Section 3 presents the feature
set and the clustering model. Subsequently, sec-
tion 4 introduces the evaluation used, and dis-
cusses experimental work and results. In section
5, this work is positioned with respect to previous
work. We conclude with an outline of plans for
future work in section 6.
2 Data and annotation
Yahoo!?s image query API was used to obtain a
corpus of pairs of semantically ambiguous images,
in thumbnail and true size, and their correspond-
ing web sites for three ambiguous keywords in-
spired by (Yarowsky, 1995): BASS, CRANE, and
SQUASH. We apply query augmentation (cf. Ta-
ble 1), and exact duplicates were filtered out by
identical image URLs, but cases occurred where
both thumbnail and true-size image were included.
Also, some images shared the same webpage or
came from the same site. Generally, the lat-
ter gives important information about shared dis-
course topic, however the images do not necessar-
ily depict the same sense (e.g. a CRANE bird vs.
a meadow), and image features can separate them
into different clusters.
Annotation overview The images were anno-
tated with one of several labels by one of the au-
thors out of context (without considering the web
site and its text), after applying text-based filter-
ing (cf. section 3.1). For annotation purposes, im-
ages were numbered and displayed on a web page
in thumbnail size. In case the thumbnail was not
sufficient for disambiguation, the image linked at
its true size to the thumbnail was inspected.2 The
true-size view depended on the size of the orig-
inal picture and showed the image and its name.
However, the annotator tried to resist name influ-
ence, and make judgements based just on the im-
age. For each query, 2 to 4 core word senses (e.g.
squash vegetable and squash sport for SQUASH)
were distinguished from inspecting the data. How-
ever, because ?context? was restricted to the image
content, and there was no guarantee that the image
actually depicts the query term, additional anno-
tator senses were introduced. Thus, for most core
senses, a RELATED label was included, accounting
for meanings that seemed related to core meaning
but lacked a core sense object in the image. Some
examples for RELATED senses are in Fig. 1. In ad-
dition, for each query term, a PEOPLE label was
included because such images are common due to
the nature of how people take pictures (e.g. por-
traits of persons or group pictures of crowds, when
core or related senses did not apply), as was an
2We noticed a few cases where Yahoo! retrieved a thumb-
nail image different from the true size image.
548
Word (#Annot. images) QueryTerms Senses Coverage Examples of visual annotation cues
BASS
(2881)
5: bass, bass guitar,
bass instrument,
bass fishing, sea
bass
1. fish 35% any fish, people holding catch
2. musical instrument 28% any bass-looking instrument, playing
3. related: fish 10% fishing (gear, boats, farms), rel. food, rel. charts/maps
4. related: musical instrument 8% speakers, accessories, works, chords, rel. music
5. unrelated 12% miscellaneous (above senses not applicable)
6. people 7% faces, crowd (above senses not applicable)
CRANE
(2650)
5: crane,
construction cranes,
whooping crane,
sandhill crane,
origami cranes
1. machine 21% machine crane, incl. panoramas
2. bird 26% crane bird or chick
3. origami 4% origami bird
4. related: machine 11% other machinery, construction, motor, steering, seat
5. related: bird 11% egg, other birds, wildlife, insects, hunting, rel. maps/charts
6. related: origami 1% origami shapes (stars, pigs), paper folding
7. people 7% faces, crowd (above senses not applicable)
8. unrelated 18% miscellaneous (above senses not applicable)
9. karate 1% martial arts
SQUASH
(1948)
10: squash+: rules,
butternut, vegetable,
grow, game of,
spaghetti, winter,
types of, summer
1. vegetable 24% squash vegetable
2. sport 13% people playing, court, equipment
3. related:vegetable 31% agriculture, food, plant, flower, insect, vegetables
4. related:sport 6% other sports, sports complex
5. people 10% faces, crowd (above senses not applicable)
6. unrelated 16% miscellaneous (above senses not applicable)
Table 1: Web images for three ambiguous query terms were annotated manually out of context (without considering the
web page document). For each term, the number of annotated images, the query retrieval terms, the senses, their distribution,
and rough sample annotation guidelines are provided, with core senses marked in bold face. Because image retrieval engines
restrict hits to 1000 images, query expansion was conducted by adding narrowing query terms from askjeeves.com to
increase corpus size. We selected terms relevant to core senses, i.e. the main discrimination phenomenon.
UNRELATED label for irrelevant images which did
not fit other labels or were undeterminable.
For a human annotator, even when using more
natural word senses, assigning sense labels to im-
ages based on image alone is more challenging
and subjective than labeling word senses in tex-
tual context. First of all, the annotation is heav-
ily dependent on domain-knowledge and it is not
feasible for a layperson to recognize fine-grained
semantics. For example, it is straightforward for
the layperson to distinguish between a robin and a
crane, but determining whether a given fish should
have the common name bass applied to it, or
whether an instrument is indeed a bass instrument
or not, is extremely difficult (see Fig. 2; e.g. de-
ciding if a picture of a fish fillet is a picture of a
fish is tricky). Furthermore, most images display
objects only partially; for example just the neck
of a classical double bass instead of the whole in-
strument. In addition, scaling, proportions, and
components are key cues for object discrimina-
tion in real-life, e.g. for singling out an electric
bass from an electric guitar, but an image may
not provide these detail. Thus, senses are even
fuzzier for ISD than WSD labeling. Given that
laypeople are in the majority, it is fair to assume
their perspective and naiveness. This latter fact
also led to annotations? level of specificity differ-
ing according to search term. Annotation criteria
depended on the keyword term and its senses and
their coverage, as shown in Table 1. Neverthe-
less, several border-line cases for label assignment
occurred. Considering that the annotation task is
Keywordquery Filtering
Image feature 
extraction Text feature extraction
1. Compute pair-wise document affinities2. Compute eigenvalues3. Embed and cluster
Evaluation of purity
Figure 3: Overview of algorithm
quite subjective, this is to be expected. In fact,
one person?s labeling often appears as justifiable
as a contradicting label provided by another per-
son. We explore the vagueness and subjective na-
ture of image annotation further in a companion
paper (Alm, Loeff, Forsyth, 2006).
3 Model
Our goal is to provide a mapping between im-
ages and a set of iconographically coherent clus-
ters for a given query word, in an unsupervised
framework. Our approach involves extracting
and weighting unordered bags-of-words (BOWs;
henceforth) features from the webpage text, sim-
ple local and global features from the image, and
running spectral clustering on top. Fig. 3 shows an
overview of the implementation.
549
3.1 Feature extraction
Document and text filtering A pruning process
was used to filter out image-document pairs based
on e.g. language specification, exclusion of ?In-
dex of? pages, pages lacking an extractable target
image, or a cutoff threshold of number of tokens
in the body. For remaining documents, text was
preprocessed (e.g. lower-casing, removing punc-
tuation, tokens being very short, having numbers
or no vowels, etc.). We used a stop word list, but
avoided stemming to make the algorithm language
independent in other respects. When using image
features, grayscale images (no color histograms)
and images without salient regions (no keypoints
detected) were also removed.
Text features We used the following BOWs:
(a) tokens in the page body; (b) tokens in a ?10
window around the target image (if multiple, the
first was considered); (c) tokens in a ?10 window
around any instances of the query keyword (e.g.
squash); (d) tokens of the target image?s alt at-
tribute; (e) tokens of the title tag; (f) some meta
tokens.3 Tf-idf was applied to a weighted aver-
age of the BOWs. Webpage design is flexible, and
some inconsistencies and a certain degree of noise
remained in the text features.
Image features Given the large variability in
the retrieved image set for a given query, it is dif-
ficult to model images in an unsupervised fash-
ion. Simple features have been shown to provide
performance rivaling that of more elaborate mod-
els in object recognition (Csurka et al 2004) and
(Chapelle, Haffner, and Vapnik, 1999), and the
following image bags of features were considered:
Bags of keypoints: In order to obtain a compact
representation of the textures of an image, patches
are extracted automatically around interesting re-
gions or keypoints in each image. The keypoint
detection algorithm (Kadir and Brady, 2001) uses
a saliency measure based on entropy to select re-
gions. After extraction, keypoints were repre-
sented by a histogram of gradient magnitude of
the pixel values in the region (SIFT) (Lowe, 2004).
These descriptors were clustered using a Gaussian
Mixture with ? 300 components, and the result-
ing global patch codebook (i.e. histogram of code-
book entries) was used as lookup table to assign
each keypoint to a codebook entry.
3Adding to META content, keywords was an attribute, but
is irregular. Embedded BODY pairs are rare; thus not used.
Color histograms: Due to its similarity to
how humans perceive color, HSV (hue, saturation,
brightness) color space was used to bin pixel color
values for each image. Eight bins were used per
channel, obtaining an 83 dimensional vector.
3.2 Measuring similarity between images
For the BOWs text representation, we use the com-
mon measure of cosine similarity (cs) of two tf-
idf vectors (Jurafsky and Martin, 2000). The co-
sine similarity measure is also appropriate for key-
point representation as it is also an unordered bag.
There are several measures for histogram compar-
ison (i.e. L1, ?2). As in (Fowlkes et al 2004) we
use the ?2 distance measure between histograms
hi and hj .
?2i,j =
1
2
512?
k=1
(hi(k)? hj(k))2
hi(k) + hj(k)
(1)
3.3 Spectral Clustering
Spectral clustering is a powerful way to sepa-
rate non-convex groups of data. Spectral meth-
ods for clustering are a family of algorithms that
work by first constructing a pairwise-affinity ma-
trix from the data, computing an eigendecomposi-
tion of the data, embedding the data into this low-
dimensional manifold, and finally applying tradi-
tional clustering techniques (i.e. k-means) to it.
Consider a graph with a set of n vertices each
one representing an image document, and the
edges of the graph represent the pairwise affinities
between the vertices. Let W be an n?n symmet-
ric matrix of pairwise affinities. We define these
as the Gaussian-weighted distance
Wij = exp
(
??t(1? csti,j)? ?
k(1? cski,j)? ?
c?2i,j
)
,
(2)
where {?t, ?k, ?c} are scaling parameters for text,
keypoints, and color features.
It has been shown that the use of multiple eigen-
vectors of W is a valid space onto which the data
can be embedded (Ng, Jordan, Weiss, 2002). In
this space noise is reduced while the most signif-
icant affinities are preserved. After this, any tra-
ditional clustering algorithm can be applied in this
new space to get the final clusters. Note that this
is a nonlinear mapping of the original space. In
particular, we employ a variant of k-means, which
includes a selective step that is quasi-optimal in
a Vector Quantization sense (Ueda and Nakano,
1994). It has the added advantage of being more
550
robust to initialization than traditional k-means.
The algorithm follows,
1. For given documents, compute the affinity
matrix W as defined in equation 2.
2. Let D be a diagonal matrix whose (i, i)-th
element is the sum of W ?s i-th row, and de-
fine L = D?1/2WD?1/2.
3. Find the k largest eigenvectors V of L.
4. Define E as V , with normalized rows.
5. Perform clustering on the columns of E,
which represent the embedding of each im-
age into the new space, using a selective step
as in (Ueda and Nakano, 1994).
Why Spectral Clustering? Why apply a vari-
ant of k-means in the embedded space as opposed
to the original feature space? The k-means algo-
rithm cannot separate non-convex clusters. Fur-
thermore, it is unable to cope with noisy dimen-
sions (this is especially true in the case of the text
data) and highly non-ellipsoid clusters. (Ng, Jor-
dan, Weiss, 2002) stated that spectral clustering
outperforms k-means not only on these high di-
mensional problems, but also in low-dimensional,
multi-class data sets. Moreover, there are prob-
lems where Euclidean measures of distance re-
quired by k-means are not appropriate (for in-
stance histograms), or others where there is not
even a natural vector space representation. Also,
spectral clustering provides a simple way of com-
bining dissimilar vector spaces, like in this case
text, keypoint and color features.
4 Experiments and results
In the first set of experiments, we used all features
for clustering. We considered three levels of sense
granularity: (1) all senses (All), (2) merging re-
lated senses with their corresponding core sense
(Meta), (3) just the core senses (Core). For ex-
periments (1) and (2), we used 40 clusters and all
labeled images. For (3), we considered only im-
ages labeled with core senses, and thus reduced the
number of clusters to 20 for a more fair compari-
son. Results were evaluated according to global
cluster purity, cf. Equation 3.4
Global purity =
?
clusters
# of most common sense in cluster
total # images
(3)
4Purity did not include the small set of outlier images, de-
fined as images whose ratio of distances to the second closest
and closest clusters was below a threshold.
Word All senses Meta senses Core senses
BASS 6 senses 4 senses 2 senses
Median 0.60 0.73 0.94
Range 0.03 0.02 0.02
Baseline 0.35 0.45 0.55
CRANE 9 senses 6 senses 4 senses
Median 0.49 0.65 0.86
Range 0.05 0.07 0.07
Baseline 0.27 0.37 0.50
SQUASH 6 senses 4 senses 2 senses
Median 0.52 0.71 0.94
Range 0.03 0.04 0.03
Baseline 0.32 0.56 0.64
Table 2: Median and range of global clustering purity
for 5 runs with different initializations. For each keyword, the
table lists the number of senses, median, and range of global
cluster purity, followed by the baseline. All senses used the
full set of sense labels and 40 clusters. Meta senses merged
core senses with their respective related senses, considering
all images and using 40 clusters. Core senses were clustered
into 20 clusters, using only images labeled with core sense la-
bels. Purity was stable across runs, and peaked for Core. The
baseline reflected the frequency of the most common sense.
Word Img TxtWin BodyTxt Baseline
BASS
Median 0.71 0.83 0.93 0.55
Range 0.05 0.03 0.05
CRANE
Median 0.61 0.84 0.85 0.50
Range 0.07 0.04 0.05
SQUASH
Median 0.71 0.91 0.96 0.64
Range 0.05 0.04 0.03
Table 3: Global and local features? performance. Core
sense images were grouped into 20 clusters, on the basis of
individual feature types, and global cluster purity was mea-
sured. The table lists the median and range from 5 runs with
different initializations. Img included just image features;
TxtWin local tokens in a ?10 window around the target im-
age anchor; BodyTxt global tokens in the page BODY; and
Baseline uses the most common sense. Text performed bet-
ter than image features, and global text appeared better than
local. All features performed above the baseline.
Median and range results are reported for five
runs, given each condition, comparing against the
baseline (i.e. choosing the most common sense).
Table 2 shows that purity was surprisingly good,
stable across query terms, and that it was high-
est when only core sense data was considered. In
addition, purity tended to be slightly higher for
BASS, which may be related to the annotator being
less confident about its fine-grained sense distinc-
tions, and thus less strict for assigning core sense
labels for this query term.5 In addition, we looked
at the relative performance of individual global
and local features using 20 clusters and only core
5A slightly modified HTML extractor yielded similar re-
sults (?0-2% median, ?0-5% range cf. to Tables 2 - 4).
551
Figure 4: First 30 images from a CRANE BIRD cluster consisting of 81 images in the median run. Individual cluster purity
for all senses was 0.67, and for meta senses 0.83. Not all clusters were as pure as this one; global purity for all 40 cluster was
0.49. This cluster appeared to show some iconography; mostly standing cranes. Interestingly, another cluster contained several
images of flying cranes. Most weighted tokens: cranes whooping birds wildlife species. Table 1 has sense labels.
Figure 5: Global purity does not tell the whole story SQUASH VEGETABLE cluster of 22 images in the median run.
Individual cluster purity for all senses was 0.5, and for meta senses 1.0. Global purity for all 40 cluster was 0.52. This cluster
both shows visually coherent images, and a sensible meta semantic field. Most weighted tokens: chayote calabaza add bitter
cup. Presumably, some tokens reflect the vegetable?s use within the cooking domain.
sense data based on a particular feature. Table 3
shows that global text features were most infor-
mative (although not homogenously), but also that
each feature type performed better than the base-
line in isolation. This indicates that an optimal fea-
ture combination may improve over current per-
formance, using manually selected parameters. In
addition, purity is not the whole story. Figs. 4
and 5 show examples of two selected interesting
clusters obtained for CRANE and SQUASH, respec-
tively, using combined image and text features and
all individual senses.6 Inspection of image clus-
ters indicated that image features, both in isolation
and when used in combination, appeared to con-
6The UIUC-ISD data set and results are currently at
http://www.visionpc.cs.uiuc.edu/isd/.
tribute to more visually balanced clusters, espe-
cially in terms of colors and shading. This shows
that further exploring image features may be vi-
tal for attaining more subtle iconographic senses.
Moreover, as discussed in the introduction, images
are not necessarily anchored in the immediate text
which they refer to. This could explain why lo-
cal text features do not perform as well as global
ones. Lastly, in addition, Fig. 6 shows an example
of a partial cluster where the algorithm inferred a
specific related sense.
We also experimented with different number of
clusters for BASS. The results are in Table 4, lack-
ing a clear trend, with comparable variation to dif-
ferent initializations. This is surprising, since we
would expect purity to increase with number of
552
Figure 6: RELATED: SQUASH VEGETABLE cluster, consisting of 27 images. The algorithm discovered a specific SQUASH
BUG-PLANT sense, which appears iconographic. Individual cluster purity for all senses was 0.85, and individual meta purity:
1.0. Global purity for all 40 clusters: 0.52. Most weighted tokens: bugs bug beetle leaf-footed kentucky.
# Clusters 6 10 20 40 80
All
Median 0.61 0.55 0.58 0.60 0.61
Range 0.03 0.05 0.03 0.03 0.04
Meta
Median 0.75 0.70 0.70 0.73 0.72
Range 0.04 0.07 0.04 0.02 0.04
Table 4: Impact of cluster size? We ran BASS for different
number of clusters (5 runs each with distinct initializations),
and recorded median and range of global purity for all six
senses of the query term, and for the four meta senses, with-
out a clear trend.
clusters (Schu?tze, 1998), but may be due to the
spectral clustering. Inspection showed that 6 clus-
ters were dominated by core senses, whereas with
40 clusters a few were also dominated by RE-
LATED senses or PEOPLE. No cluster was domi-
nated by an UNRELATED label, which makes sense
since semantic linkage should be absent between
unrelated items.
5 Comparison to previous work
Space does not allow a complete review of the
WSD literature. (Yarowsky, 1995) demonstrated
that semi-supervised WSD could be successful.
(Schu?tze, 1998) and (Lin and Pantel, 2002a, b)
show that clustering methods are helpful in this
area.
While ISD has received less attention, image
categorization has been approached previously
by adding text features. For example, (Frankel,
Swain, and Athitsos, 1996)?s WebSeer system
attempted to mutually distinguish photos, hand-
drawn, and computer-drawn images, using a com-
bination of HTML markup, web page text, and im-
age information. (Yanai and Barnard, 2005) found
that adding text features could benefit identifying
relevant web images. Using text-annotated images
(i.e. images annotated with relevant keywords),
(Barnard and Forsyth, 2001) clustered them ex-
ploring a semantic hierarchy; similarly (Barnard,
Duygulu, and Forsyth, 2002) conducted art clus-
tering, and (Barnard and Johnson, 2005) used text-
annotated images to improve WSD. The latter pa-
per obtained best results when combining text and
image features, but contrary to our findings, im-
age features performed better in isolation than just
text. They did use a larger set of image features
and segmentation, however, we suspect that dif-
ferences can rather be attributed to corpus type. In
fact, (Yanai, Shirahatti, and Barnard, 2005) noted
that human evaluators rated images obtained via
a keyword retrieval method higher compared to
image-based retrieval methods, which they relate
to the importance of semantics for what humans
regard as matching, and because pictorial seman-
tics is hard to detect.
(Cai et al 2004) use similar methods to rank
visual search results. While their work does not
focus explicitly on sense and does not provide in-
depth discussion of visual sense phenomena, these
do appear in, for example, figs. 7 and 9 of their pa-
per. An interesting aspect of their work is the use
of page layout segmentation to associate text with
images in web documents. Unfortunately, the au-
553
thors only provide an illustrative query example,
and no numerical evaluation, making any com-
parison difficult. (Wang et al 2004) use similar
features with the goal to improve image retrieval
through similarity propagation, querying specific
web sites. (Fuji and Ishikawa, 2005) deal with
image ambiguity for establishing an online mul-
timedia encyclopedia, but their method does not
integrate image features, and appears to depend
on previous encyclopedic background knowledge,
limited to a domain set.
6 Conclusion
It is remarkable how high purity is, considering
that we are using relatively simple image and text
representation. In most corpora used to date for re-
search on illustrated text, word sense is an entirely
secondary phenomenon, whereas our data set was
collected as to emphasize possible ambiguities as-
sociated with word sense. Our results suggest that
a surprisingly degree of the meaning of an illus-
trated object is exposed on the surface.
This work is an initial attempt at addressing
the ISD problem. Future work will involve learn-
ing the algorithm?s parameters without supervi-
sion, and develop a semantically meaningful im-
age taxonomy. In particular, we intend to explore
the notion of iconographic senses; surprisingly
good results on image classification by (Chapelle,
Haffner, and Vapnik, 1999) using image features
suggest that iconography plays an important role
in the semantics of images. An important aspect
is to enhance our understanding of the interplay
between text and image features for this purpose.
Also, it remains an unsolved problem how to enu-
merate iconographic senses, and use them in man-
ual annotation and classification. Experimental
work with humans performing similar tasks may
provide increased insight into this issue, and can
also be used to validate clustering performance.
7 Acknowledgements
We are grateful to Roxana Girju and Richard
Sproat for helpful feedback, and to Alexander
Sorokin.
References
C. O. Alm, N. Loeff, and D. Forsyth. 2006. Challenges for
annotating images for sense disambiguation. ACL work-
shop on Frontiers in Linguistically Annotated Corpora.
K. Barnard and D. Forsyth. 2001. Learning the semantics of
words and pictures. ICCV, 408?415.
K. Barnard, P. Duygulu, and D. Forsyth. 2002. Modeling the
statistics of image features and associated text. SPIE.
K. Barnard and M. Johnson. 2005. Word sense disambigua-
tion with pictures. Artificial Intelligence, 167, 13?30.
D. Cai et al 2004. Hierarchical clustering of WWW image
search results using visual, textual and link information.
ACM Multimedia, 952-959.
O. Chapelle and P. Haffner and V. Vapnik. 1999. Support
vector machines for histogram-based image classification.
IEEE Neural Networks, 10(5), 1055?1064.
G. Csurka et al 2004. Visual categorization with bags
of keypoints. ECCV Int. Workshop on Stat. Learning in
Computer Vision.
C. Frankel, M. Swain, and V. Athitsos. 1996. WebSeer: an
image search engine for the World Wide Web. Univ. of
Chicago, Computer Science, Technical report #96-14.
C. Fowlkes, S. Belongie, F. Chung, and J. Malik. 2004.
Spectral grouping using the Nystro?m method. IEEE
PAMI, 26(2),214-225.
A. Fuji and T. Ishikawa. 2005. Toward the automatic com-
pilation of multimedia encyclopedias: associating images
with term descriptions on the web. IEEE WI, 536-542.
D. Jurafsky and J. Martin 2000. Speech and Language Pro-
cessing, Prentice Hall.
T. Kadir and M. Brady. 2001. Scale, saliency and image
description. Int. Journal of Computer Vision, 45 (2):83?
105.
D. Lin and P. Pantel. 2002a. Concept discovery from text.
COLING, 577?583.
D. Lowe. 2004. Distinctive image features from scale-
invariant keypoints. Int. Journal of Computer Vision,
60(2), 91?110.
A. Ng, M. Jordan, and Y. Weiss. 2002. On spectral cluster-
ing: analysis and an algorithm. NIPS 14.
P. Pantel and D. Lin. 2002b. Discovering word senses from
text. KDD, 613?619.
H. Schuetze. 1998. Automatic word sense discrimination.
Computational Linguistics, 24(1):97?123.
J. Shi and J. Malik. 2000. Normalized cuts and image seg-
mentation. IEEE PAMI, 22(8):888?905.
N. Ueda. and R. Nakano. 1994. A new competitive learn-
ing approach based on an equidistortion principle for
designing optimal vector quantizers. Neural Networks,
7(8):1211?1227.
X.-J. Wang et al 2004. Multi-model similarity propagation
and its application for image retrieval. MM,944?951.
K. Yanai and K. Barnard. 2005. Probabilistic web image
gathering. SIGMM, 57?64.
K. Yanai, N. V. Shirahatti, and K. Barnard. 2005. Evaluation
strategies for image understanding and retrieval. SIGMM,
217-226.
D. Yarowsky. 1995. Unsupervised word sense disambigua-
tion rivaling supervised methods. ACL, 189?196.
554
Proceedings of the Workshop on Frontiers in Linguistically Annotated Corpora 2006, pages 1?4,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Challenges for annotating images for sense disambiguation
Cecilia Ovesdotter Alm
Dept. of Linguistics
University of Illinois, UC
ebbaalm@uiuc.edu
Nicolas Loeff
Dept. of Computer Science
University of Illinois, UC
loeff@uiuc.edu
David A. Forsyth
Dept. of Computer Science
University of Illinois, UC
daf@uiuc.edu
Abstract
We describe an unusual data set of thou-
sands of annotated images with interest-
ing sense phenomena. Natural language
image sense annotation involves increased
semantic complexities compared to dis-
ambiguating word senses when annotating
text. These issues are discussed and illus-
trated, including the distinction between
word senses and iconographic senses.
1 Introduction
We describe a set of annotated images, each asso-
ciated with a sense of a small set of words. Build-
ing this data set exposes important sense phenom-
ena which not only involve natural language but
also vision. The context of our work is Image
Sense Discrimination (ISD), where the task is to
assign one of several senses to a web image re-
trieved by an ambiguous keyword. A compan-
ion paper introduces the task, presents an unsuper-
vised ISD model, drawing on web page text and
image features, and shows experimental results
(Loeff et al, 2006). The data was subject to single-
annotator labeling, with verification judgements
on a part of the data set as a step toward study-
ing agreement. Besides a test bed for ISD, the
data set may be applicable to e.g. multimodal word
sense disambiguation and cross-language image
retrieval. The issues discussed concern concepts,
and involve insights into semantics, perception,
and knowledge representation, while opening up a
bridge for interdisciplinary work involving vision
and NLP.
2 Related work
The complex relationship between annotations
and images has been explored by the library com-
munity, who study management practices for im-
age collections, and by the computer vision com-
munity, who would like to provide automated im-
age retrieval tools and possibly learn object recog-
nition methods.
Commercial picture collections are typically an-
notated by hand, e.g. (Enser, 1993; Armitage and
Enser, 1997; Enser, 2000). Subtle phenomena can
make this very difficult, and content vs. interpreta-
tion may differ; an image of the Eiffel tower could
be annotated with Paris or even love, e.g. (Ar-
mitage and Enser, 1997), and the resulting annota-
tions are hard to use, cf. (Markkula and Sormunen,
2000), or Enser?s result that a specialized indexing
language gives only a ?blunt pointer to regions of
the Hulton collections?, (Enser, 1993), p. 35.
Users of image collections have been well stud-
ied. Important points for our purposes are: Users
request images both by object kinds, and individ-
ual identities; users request images both by what
they depict and by what they are about; and that
text associated with images is extremely useful in
practice, newspaper archivists indexing largely on
captions (Markkula and Sormunen, 2000).
The computer vision community has stud-
ied methods to predict annotations from images,
e.g. (Barnard et al, 2003; Jeon et al, 2003; Blei
and Jordan, 2002). The annotations that are pre-
dicted most successfully tend to deal with ma-
terials whose identity can be determined without
shape analysis, like sky, sea and the like. More
complex annotations remain difficult. There is no
current theory of word sense in this context, be-
cause in most current collections, words appear in
the most common sense only. Sense is known to
be important, and image information can disam-
biguate word senses (Barnard and Johnson, 2005).
1
Word (#Annot. images) QueryTerms Senses Coverage Examples of visual annotation cues
BASS
(2881)
5: bass, bass guitar,
bass instrument,
bass fishing, sea
bass
1. fish 35% any fish, people holding catch
2. musical instrument 28% any bass-looking instrument, playing
3. related: fish 10% fishing (gear, boats, farms), rel. food, rel. charts/maps
4. related: musical instrument 8% speakers, accessories, works, chords, rel. music
5. unrelated 12% miscellaneous (above senses not applicable)
6. people 7% faces, crowds (above senses not applicable)
CRANE
(2650)
5: crane,
construction cranes,
whooping crane,
sandhill crane,
origami cranes
1. machine 21% machine crane, incl. panoramas
2. bird 26% crane bird or chick
3. origami 4% origami bird
4. related: machine 11% other machinery, construction, motor, steering, seat
5. related: bird 11% egg, other birds, wildlife, insects, hunting, rel. maps/charts
6. related: origami 1% origami shapes (stars, pigs), paper folding
7. people 7% faces, crowds (above senses not applicable)
8. unrelated 18% miscellaneous (above senses not applicable)
9. karate 1% martial arts
SQUASH
(1948)
10: squash+: rules,
butternut, vegetable,
grow, game of,
spaghetti, winter,
types of, summer
1. vegetable 24% squash vegetable
2. sport 13% people playing, court, equipment
3. related:vegetable 31% agriculture, food, plant, flower, insect, vegetables
4. related:sport 6% other sports, sports complex
5. people 10% faces, crowds (above senses not applicable)
6. unrelated 16% miscellaneous (above senses not applicable)
Table 1: Overview of annotated images for three ambiguous query terms, inspired by the WSD literature. For each term,
the number of annotated images, the expanded query retrieval terms (taken terms from askjeeves.com), the senses, their
distribution coverage, and rough sample annotation guidelines are provided, with core senses marked in bold.
(a) machine (b)
bird
(c) origami (d)
karate
(e) rel. to a (f) rel. to b (g)
rel. to c
(h)
people
(i) unrel.
Figure 1: CRANE images with clear senses: (a-d) core senses, (e-g) related senses, (h) people and (i) unrelated. Related
senses are associated with the semantic field of a core sense, but the core sense is visually absent or undeterminable.
3 Data set
The data set has images retrieved from a web
search engine. We deliberately focused on three
keywords, which cover a range of phenomena in
semantic ambiguity: BASS, CRANE, and SQUASH.
Table 1 gives an overview of the data set, anno-
tated by one author (CA).1 The webpage was not
considered to avoid bias, given the ISD task.
For each query, 2 to 4 core word senses were
distinguished from inspecting the data using com-
mon sense. We chose this approach rather than
ontology senses which tend to be incomplete or
too specific for our purposes. For example, the
origami sense of CRANE is not included in Word-
Net under CRANE, but for BASS three different
senses appear with fish. WordNet contains bird
as part of the description for the separate entry
origami, and some query expansion terms are hy-
ponyms which occur as separate WordNet entries
(e.g. bass guitar, sea bass, summer squash). Im-
ages may show multiple objects; a general strategy
preferred a core sense if it was included.
An additional complication is that given that the
images are retrieved by a search engine there is no
guarantee that they depict the query term, so ad-
ditional senses were introduced. Thus, for most
1We call the data set the UIUC-ISD data set. It is currently
at http://www.visionpc.cs.uiuc.edu/isd/.
core senses, a RELATED label was included for
meanings related to the semantic field of a core
sense. Also, a PEOPLE label was included since
such images may occur due to how people take
pictures (e.g. portraits of persons, group pictures,
or other representations of people outside core and
related senses). An UNRELATED label accounted
for images that did not fit other labels, or were ir-
relevant or undeterminable. In fact, distinguish-
ing between PEOPLE and UNRELATED was not al-
ways straightforward. Fig. 1 shows examples of
CRANE when sense assignment was quite straight-
forward. However, distinguishing image senses
was often not this clear. In fact, many border-line
cases occurred when one could argue for different
label assignments. Also, annotation cues are sub-
ject to interpretation, and disagreements between
judges are expected. They simply reflect that im-
age senses are located on a semantic continuum.
4 Why annotating image senses is hard
In general, annotating images involves special
challenges, such as what to annotate and how ex-
tensively. We assign an image one sense. Never-
theless, compared to disambiguating a word, sev-
eral issues are added for annotation. As noted
above, a core sense may not occur, and judge-
ments are characterized by increased subjectivity,
with semantics beyond prototypical and peripheral
2
(a) (b) (c) (d) (e) (f) (g) (h) (i) (j)
(k) (l) (m) (n) (o) (p) (q)
Figure 2: Annotating images is often challenging for different reasons. Are these images of CRANE birds? (a-c) depiction
(d-f) gradient change (g-h) partial display (i-j) domain knowledge (k) unusual appearance (l-n) distance (o-q) not animate.
exemplars. Also, the disambiguating context is
limited to image contents, rather than collocations
of an ambiguous token. Fig. 2 illustrates selected
challenging judgement calls for assigning or not
the bird sense of CRANE, as discussed below.
Depiction: Images may include man-made de-
pictions of an object in artistic depictions, and the
question is whether this counts as the object or
not, e.g. Fig. 2(a-c). Gradient changes: Recog-
nition is complicated by objects taking different
forms and shapes, cf. the insight by (Labov, 1973)
on gradual categories.2 For example, as seen in
Fig. 2(d-f), birds change with age; an egg may be
a bird, but a chick is, as is a fledgeling. Partial
display: Objects may be rendered in incomplete
condition. For example, Fig. 2(g-h) show merely
feathers or a bird neck. Domain knowledge: Peo-
ple may disagree due to differences in domain
knowledge, e.g. some non-experts may have a dif-
ficult time determining whether or not other sim-
ilar bird species can be distinguished from a bird
crane, cf. Fig. 2(i-j). This also affected annota-
tions? granularity depending on keyword, see Ta-
ble 1?s example cues. Unusual appearance: Ob-
jects may occur in less frequent visual appear-
ance, or lack distinguishing properties. For in-
stance, Fig. 2(k) illustrates how sunset background
masks birds? color information. Scale: The dis-
tance to objects may render them unclear and in-
fluence judgement accuracy, and people may dif-
fer in the degree of certainty required for assign-
ing a sense. For example, Fig. 2(l-n) show flying
or standing potential cranes at distance. Animate:
Fig. 2(o-q) raise the question whether dead, skele-
tal, or artificial objects are instantiations or not.
Other factors complicating the annotation task in-
clude image crowdedness disguising objects, cer-
tain entities having less salience, and lacking or
unclear reference to object proportions. Senses
2Function or properties may also influence (Labov, 1973).
may also be etymologically related or blend occa-
sionally, or be guided by cultural interpretations,
and so on.
Moreover, related senses are meant to capture
images associated with the semantic field of a core
sense. However, because the notion and borders of
a semantic field are non-specific, related senses
are tricky. Annotators may build associations
quite wildly, based on personal experience and
opinion, thus what is or is not a related sense may
very quickly get out of hand. For instance, a per-
son may by association reason that if bird cranes
occur frequently in fields, then an image of a field
alone should be marked as related. To avoid this,
guidelines attempted to restrict related senses, as
exemplified in Table 1, with some data-driven re-
visions during the annotation process. However,
guidelines are also based on judgement calls. Be-
sides, for abstract concepts like LOVE, differenti-
ating core versus related sense is not really valid.
Lastly, an additional complexity of image
senses is that in addition to traditional word
senses, images may also capture repeatedly oc-
curring iconographic patterns or senses. As illus-
trated in Fig. 3, the iconography of flying cranes
is quite different from that of standing cranes, as
regards motion, shape, identity, and color of figure
and ground, respectively. Mixed cases also occur,
e.g. when bird cranes are taking off or are about
to land in relation to flight. Iconographic senses
may compare to more complex linguistic struc-
tures than nominal categories, e.g. a modified NP
or clause, but are represented by image properties.
A policy for annotating iconographic senses is
still lacking. Image groups based on iconographic
senses seem to provide increased visual and se-
mantic harmony for the eye, but experiments are
needed to confirm how iconographic senses cor-
respond to humans? perception of semantic image
similarity, and at what level of semantic differen-
3
(a) (b) (c) (d) (e) (f) (g) (h)
Figure 3: Iconographic bird CRANE senses: (a-c) flying cranes, (d-f) standing cranes, and (g-h) mixed cases in-between.
(a) 5/2 (b) 1/4 (c) 4/1 (d) 4/1 (e) 4/8 (f) 8/2 (g) 8/1 (h) 6/8,5 (i) 4/1
Figure 4: Disagreement examples (sense numbers in Table 1): (a) crane or other bird? (b) toy crane or scales? (c) crane or
other steel structure/elevator? (d) crane or other machine? (e) company is related or not? (f) bird or abstract art? (g) crane in
background or not? (h) origami-related paper? (i) inside of crane? (and is inside sufficient to denote image as machine crane?)
tiation they become relevant for sense assessment.
Lastly, considering the challenges of image an-
notation, it is interesting to look at annotation dis-
agreements. Thus, another author (NL) inspected
CRANE annotations, and recorded disagreement
candidates, which amounted to 5%. Rejecting or
accepting a category label seems less hard than
independent annotation but still can give insights
into disagreement tendencies. Several disagree-
ments involved a core category vs. its related label
vs. unrelated, rather than two core senses. Also,
some disagreement candidates had tiny, fuzzy,
partial or peripheral potential sense objects, or
lacked distinguishing object features, so interpre-
tation became quite idiosyncratic. The disagree-
ment candidates were discussed together, result-
ing in 2% being true disagreements, 2% false dis-
agreements (resolved by consensus on CA?s la-
bels), and 1% annotation mistakes. Examples of
true disagreements are in Fig. 4. Often, both par-
ties could see each others? points, but opted for an-
other interpretation; this confirms that border lines
tend to merge, indicating that consistency is chal-
lenging and not always guaranteed. As the annota-
tion procedure advances, criteria may evolve and
modify the fuzzy sense boundaries.
5 Conclusion
This work draws attention to the need for consid-
ering natural language semantics in multi-modal
settings. Annotating image senses adds increased
complexity compared to word-sense annotation
in text due to factors such as image proper-
ties, subjective perception, and annotator domain-
knowledge. Moreover, the concept of related
senses as well as iconographic senses go beyond
and diversify the notion of word sense. In the fu-
ture, we would like to perform experimentation
with human subjects to explore both similarity
judgements for image pairs or groups, as well as
issues in interannotator agreement for image dis-
ambiguation, and, finally, to better understand the
role of iconography for semantic interpretation.
6 Acknowledgements
Thanks to anonymous reviewers, R. Girju and R.
Sproat for feedback. Any fallacies are our own.
References
L. H. Armitage and P. G. B. Enser. 1997. Analysis
of user need in image archives. J. of Inform. Sci.,
23(4):287?299.
K. Barnard and M. Johnson. 2005. Word sense disam-
biguation with pictures. Artif. Intel., 167:13?30.
K. Barnard, P. Duygulu, N. Freitas, D. Forsyth, D. Blei,
and M. I. Jordan. 2003. Matching words and pic-
tures. J. of Mach. Learn. Research, 3:1107?1135.
D. M. Blei and M. I. Jordan. 2002. Modeling anno-
tated data. Technical Report CSD-02-1202, Div. of
Computer Science, Univ. of California, Berkeley.
P. G. B. Enser. 1993. Query analysis in a visual infor-
mation retrieval context. J. of Doc. and Text Man-
agement, 1(1):25?52.
P. G. B. Enser. 2000. Visual image retrieval: seek-
ing the alliance of concept based and content based
paradigms. J. of Inform. Sci., 26(4):199?210.
J. Jeon, V. Lavrenko, and R. Manmatha. 2003. Auto-
matic image annotation and retrieval using crossme-
dia relevance models. In SIGIR, pages 119?126.
W. Labov. 1973. The boundaries of words and their
meanings. In C. J. Baily and R. Shuy, editors, New
ways of analyzing variation in English, pages 340?
373. Washington D.C: Georgetown Univ. Press.
N. Loeff, C. O. Alm, and D. A. Forsyth. 2006. Dis-
criminating image senses by clustering with multi-
modal features. In ACL (forthcoming).
M. Markkula and E. Sormunen. 2000. End-user
searching challenges indexing practices in the digital
newspaper photo archive. Inform. Retr., 1:259?285.
4
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, page 254, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
Keynote Address:
More Words and Bigger Pictures
David Forsyth
University of Illinois
Urbana-Champaign
daf@illinois.edu
Abstract
Object recognition is a little like translation: a pic-
ture (text in a source language) goes in, and a de-
scription (text in a target language) comes out. I
will use this analogy, which has proven fertile, to
describe recent progress in object recognition.
We have very good methods to spot some objects
in images, but extending these methods to produce
descriptions of images remains very difficult. The
description might come in the form of a set of words,
indicating objects, and boxes or regions spanned by
the object. This representation is difficult to work
with, because some objects seem to be much more
important than others, and because objects interact.
An alternative is a sentence or a paragraph describ-
ing the picture, and recent work indicates how one
might generate rich structures like this. Further-
more, recent work suggests that it is easier and more
effective to generate descriptions of images in terms
of chunks of meaning (?person on a horse?) rather
than just objects (?person?; ?horse?).
Finally, if the picture contains objects that are un-
familiar, then we need to generate useful descrip-
tions that will make it possible to interact with them,
even though we don?t know what they are.
About the Speaker
David Forsyth is currently a full professor at U. Illi-
nois at Urbana-Champaign, where he moved from
U.C Berkeley, where he was also full professor. He
has published over 130 papers on computer vision,
computer graphics and machine learning. He has
served as program chair and as general chair for var-
ious international conferences on computer vision.
He received an IEEE technical achievement award
for 2005 for his research and became an IEEE fellow
in 2009. His textbook, ?Computer Vision: A Mod-
ern Approach? (joint with J. Ponce and published by
Prentice Hall) is widely adopted as a course text. A
second edition appeared in 2011. He was named ed-
itor in chief of IEEE TPAMI for a term starting in
Jan 2013.
254
