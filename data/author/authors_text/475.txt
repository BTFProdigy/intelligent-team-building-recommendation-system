Proceedings of the BioNLP Workshop on Linking Natural Language Processing and Biology at HLT-NAACL 06, pages 116?117,
New York City, June 2006. c?2006 Association for Computational Linguistics
Refactoring Corpora
Helen L. Johnson
Center for Computational Pharmacology
U. of Colorado School of Medicine
helen.johnson@uchsc.edu
William A. Baumgartner, Jr.
Center for Computational Pharmacology
U. of Colorado School of Medicine
william.baumgartner@uchsc.edu
Martin Krallinger
Protein Design Group
Universidad Auto?noma de Madrid
martink@cnb.uam.es
K. Bretonnel Cohen
Center for Computational Pharmacology
U. of Colorado School of Medicine
kevin.cohen@gmail.com
Lawrence Hunter
Center for Computational Pharmacology
U. of Colorado School of Medicine
larry.hunter@uchsc.edu
Abstract
We describe a pilot project in semi-
automatically refactoring a biomedical
corpus. The total time expended was just
over three person-weeks, suggesting that
this is a cost-efficient process. The refac-
tored corpus is available for download at
http://bionlp.sourceforge.net.
1 Introduction
Cohen et al (2005) surveyed the usage rates of a
number of biomedical corpora, and found that most
biomedical corpora have not been used outside of
the lab that created them. Empirical data on corpus
design and usage suggests that one major factor af-
fecting usage is the format in which it is distributed.
These findings suggest that there would be a large
benefit to the community in refactoring these cor-
pora. Refactoring is defined in the software en-
gineering community as altering the internal struc-
ture of code without altering its external behav-
ior (Fowler et al, 1999). We suggest that in the con-
text of corpus linguistics, refactoring means chang-
ing the format of a corpus without altering its con-
tents, i.e. its annotations and the text that they de-
scribe. The significance of being able to refactor a
large number of corpora should be self-evident: a
likely increase in the use of the already extant pub-
licly available data for evaluating biomedical lan-
guage processing systems, without the attendant cost
of repeating their annotation.
We examined the question of whether corpus
refactoring is practical by attempting a proof-of-
concept application: modifying the format of the
Protein Design Group (PDG) corpus described in
Blaschke et al (1999) from its current idiosyncratic
format to a stand-off annotation format (WordF-
reak1) and a GPML-like (Kim et al, 2001) embed-
ded XML format.
2 Methods
The target WordFreak and XML-embedded formats
were chosen for two reasons. First, there is some
evidence suggesting that standoff annotation and
embedded XML are the two most highly preferred
corpus annotation formats, and second, these for-
mats are employed by the two largest extant curated
biomedical corpora, GENIA (Kim et al, 2001) and
BioIE (Kulick et al, 2004).
The PDG corpus we refactored was originally
constructed by automatically detecting protein-
protein interactions using the system described in
Blaschke et al (1999), and then manually review-
ing the output. We selected it for our pilot project
because it was the smallest publicly available cor-
pus of which we were aware. Each block of text has
a deprecated MEDLINE ID, a list of actions, a list of
proteins and a string of text in which the actions and
proteins are mentioned. The structure and contents
of the original corpus dictate the logical steps of the
refactoring process:
1. Determine the current PubMed identifier, given
the deprecated MEDLINE ID. Use the PubMed
identifier to retrieve the original abstract.
1http://venom.ldc.upenn.edu/
resources/info/wordfreak ann.html
116
2. Locate the original source sentence in the title
or abstract.
3. Locate the ?action? keywords and the entities
(i.e., proteins) in the text.
4. Produce output in the new formats.
Between each file creation step above, human cu-
rators verify the data. The creation and curation pro-
cess is structured this way so that from one step to
the next we are assured that all data is valid, thereby
giving the automation the best chance of performing
well on the subsequent step.
3 Results
The refactored PDG corpus is publicly available at
http://bionlp.sourceforge.net. Total time expended
to refactor the PDG corpus was 122 hours and 25
minutes, or approximately three person-weeks. Just
over 80% of the time was spent on the programming
portion. Much of that programming can be directly
applied to the next refactoring project. The remain-
ing 20% of the time was spent curating the program-
matic outputs.
Mapping IDs and obtaining the correct abstract
returned near-perfect results and required very little
curation. For the sentence extraction step, 33% of
the corpus blocks needed manual correction, which
required 4 hours of curation. (Here and below, ?cu-
ration? time includes both visual inspection of out-
puts, and correction of any errors detected.) The
source of error was largely due to the fact that the
sentence extractor returned the best sentence from
the abstract, but the original corpus text was some-
times more or less than one sentence.
For the protein and action mapping step, about
40% of the corpus segments required manual cor-
rection. In total, this required about 16 hours of cu-
ration time. Distinct sources of error included par-
tial entity extraction, incorrect entity extraction, and
incorrect entity annotation in the original corpus ma-
terial. Each of these types of errors were corrected.
4 Conclusion
The underlying motivation for this paper is the hy-
pothesis that corpus refactoring is practical, eco-
nomical, and useful. Erjavec (2003) converted the
GENIA corpus from its native format to a TEI P4
format. They noted that the translation process
brought to light some previously covert problems
with the GENIA format. Similarly, in the process of
the refactoring we discovered and repaired a number
of erroneous entity boundaries and spurious entities.
A number of enhancements to the corpus are now
possible that in its previous form would have been
difficult at best. These include but are not limited
to performing syntactic and semantic annotation and
adding negative examples, which would expand the
usefulness of the corpus. Using revisioning soft-
ware, the distribution of iterative feature additions
becomes simple.
We found that this corpus could be refactored with
about 3 person-weeks? worth of time. Users can take
advantage of the corrections that we made to the en-
tity component of the data to evaluate novel named
entity recognition techniques or information extrac-
tion approaches.
5 Acknowledgments
The authors thank the Protein Design Group at the Universidad
Auto?noma de Madrid for providing the original PDG protein-
protein interaction corpus, Christian Blaschke and Alfonso Va-
lencia for assistance and support, and Andrew Roberts for mod-
ifying his jTokeniser package for us.
References
Christian Blaschke, Miguel A. Andrade, and Christos Ouzou-
nis. 1999. Automatic extraction of biological information
from scientific text: Protein-protein interactions.
K. Bretonnel Cohen, Lynne Fox, Philip Ogren, and Lawrence
Hunter. 2005. Empirical data on corpus design and usage in
biomedical natural language processing. AMIA 2005 Sym-
posium Proceedings, pages 156?160.
Tomaz Erjavec, Yuka Tateisi, Jin-Dong Kim, and Tomoko Ohta.
2003. Encoding biomedical resources in TEI: the case of the
GENIA corpus.
Martin Fowler, Kent Beck, John Brant, William Opdyke, and
Don Roberts. 1999. Refactoring: improving the design of
existing code. Addison-Wesley.
Jin-Dong Kim, Tomoko Ohta, Yuka Tateisi, Hideki Mima, and
Jun?ichi Tsujii. 2001. Xml-based linguistic annotation of
corpus. In Proceedings of The First NLP and XML Work-
shop, pages 47?53.
S. Kulick, A. Bies, M. Liberman, M. Mandel, R. McDonald,
M. Palmer, A. Schein, and L. Ungar. 2004. Integrated anno-
tation for biomedical information extraction. Proceedings of
the HLT/NAACL.
117
Software Engineering, Testing, and Quality Assurance for Natural Language Processing, pages 23?30,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Software testing and the naturally occurring data
assumption in natural language processing?
K. Bretonnel Cohen William A. Baumgartner, Jr. Lawrence Hunter
Abstract
It is a widely accepted belief in natural lan-
guage processing research that naturally oc-
curring data is the best (and perhaps the only
appropriate) data for testing text mining sys-
tems. This paper compares code coverage us-
ing a suite of functional tests and using a large
corpus and finds that higher class, line, and
branch coverage is achieved with structured
tests than with even a very large corpus.
1 Introduction
In 2006, Geoffrey Chang was a star of the protein
crystallography world. That year, a crucial compo-
nent of his code was discovered to have a simple
error with large consequences for his research. The
nature of the bug was to change the signs (positive
versus negative) of two columns of the output. The
effect of this was to reverse the predicted ?handed-
ness? of the structure of the molecule?an impor-
tant feature in predicting its interactions with other
molecules. The protein for his work on which Chang
was best known is an important one in predicting
things like human response to anticancer drugs and
the likelihood of bacteria developing antibiotic re-
sistance, so his work was quite influential and heav-
ily cited. The consequences for Chang were the
withdrawal of 5 papers in some of the most presti-
gious journals in the world. The consequences for
the rest of the scientific community have not been
?K. Bretonnel Cohen is with The MITRE Corporation. All
three co-authors are at the Center for Computational Pharma-
cology in the University of Colorado School of Medicine.
quantified, but were substantial: prior to the retrac-
tions, publishing papers with results that did not
jibe with his model?s predictions was difficult, and
obtaining grants based on preliminary results that
seemed to contradict his published results was dif-
ficult as well. The Chang story (for a succinct dis-
cussion, see (Miller, 2006), and see (Chang et al,
2006) for the retractions) is an object illustration of
the truth of Rob Knight?s observation that ?For sci-
entific work, bugs don?t just mean unhappy users
who you?ll never actually meet: they mean retracted
publications and ended careers. It is critical that
your code be fully tested before you draw conclu-
sions from results it produces? (personal communi-
cation). Nonetheless, the subject of software testing
has been largely neglected in academic natural lan-
guage processing. This paper addresses one aspect
of software testing: the monitoring of testing efforts
via code coverage.
1.1 Code coverage
Code coverage is a numerical assessment of the
amount of code that is executed during the running
of a test suite (McConnell, 2004). Although it is
by no means a completely sufficient method for de-
termining the completeness of a testing effort, it is
nonetheless a helpful member of any suite of met-
rics for assessing testing effort completeness. Code
coverage is a metric in the range 0-1.0. A value of
0.86 indicates that 86% of the code was executed
while running a given test suite. 100% coverage is
difficult to achieve for any nontrivial application, but
in general, high degrees of ?uncovered? code should
lead one to suspect that there is a large amount of
23
code that might harbor undetected bugs simply due
to never having been executed. A variety of code
coverage metrics exist. Line coverage indicates the
proportion of lines of code that have been executed.
It is not the most revealing form of coverage assess-
ment (Kaner et al, 1999, p. 43), but is a basic part
of any coverage measurement assessment. Branch
coverage indicates the proportion of branches within
conditionals that have been traversed (Marick, 1997,
p. 145). For example, for the conditional if $a
&& $b, there are two possible branches?one is tra-
versed if the expression evaluates to true, and the
other if it evaluates to false. It is more informative
than line coverage. Logic coverage (also known as
multicondition coverage (Myers, 1979) and condi-
tion coverage (Kaner et al, 1999, p. 44) indicates the
proportion of sets of variable values that have been
tried?a superset of the possible branches traversed.
For example, for the conditional if $a || $b,
the possible cases (assuming no short-circuit logic)
are those of the standard (logical) truth table for that
conditional. These coverage types are progressively
more informative than line coverage. Other types of
coverage are less informative than line coverage. For
example, function coverage indicates the proportion
of functions that are called. There is no guarantee
that each line in a called function is executed, and all
the more so no guarantee that branch or logic cov-
erage is achieved within it, so this type of coverage
is weaker than line coverage. With the advent of
object-oriented programming, function coverage is
sometimes replaced by class coverage?a measure
of the number of classes that are covered.
We emphasize again that code coverage is not
a sufficient metric for evaluating testing complete-
ness in isolation?for example, it is by definition
unable to detect ?errors of omission,? or bugs that
consist of a failure to implement needed functional-
ity. Nonetheless, it remains a useful part of a larger
suite of metrics, and one study found that testing in
the absence of concurrent assessment of code cov-
erage typically results in only 50-60% of the code
being executed ((McConnell, 2004, p. 526), citing
Wiegers 2002).
We set out to question whether a dominant, if of-
ten unspoken, assumption of much work in contem-
porary NLP holds true: that feeding a program a
large corpus serves to exercise it adequately. We be-
gan with an information extraction application that
had been built for us by a series of contractors, with
the contractors receiving constant remote oversight
and guidance but without ongoing monitoring of the
actual code-writing. The application had benefitted
from no testing other than that done by the develop-
ers. We used a sort of ?translucent-box? or ?gray-
box? paradigm, meaning in this case that we treated
the program under test essentially as a black box
whose internals were inaccessible to us, but with the
exception that we inserted hooks to a coverage tool.
We then monitored three types of coverage?line
coverage, branch coverage, and class coverage?
under a variety of contrasting conditions:
? A set of developer-written functional tests ver-
sus a large corpus with a set of semantic rules
optimized for that corpus.
? Varying the size of the rule set.
? Varying the size of the corpus.
We then looked for coverage differences between
the various combinations of input data and rule sets.
In this case, the null hypothesis is that no differences
would be observed. In contrast with the null hypoth-
esis, the unspoken assumption in much NLP work
is that the null hypothesis does not hold, that the
primary determinant of coverage will be the size of
the corpus, and that the observed pattern will be that
coverage is higher with the large corpus than when
the input is not a large corpus.
2 Methods and materials
2.1 The application under test
The application under test was an information ex-
traction application known as OpenDMAP. It is de-
scribed in detail in (Hunter et al, 2008). It achieved
the highest performance on one measure of the
protein-protein interaction task in the BioCreative
II shared task (Krallinger et al, 2007). Its use in
that task is described specifically in (Baumgartner
Jr. et al, In press). It contains 7,024 lines of code
spread across three packages (see Table 1). One
major package deals with representing the seman-
tic grammar rules themselves, while the other deals
with applying the rules to and extracting data from
arbitrary textual input. (A minor package deals with
24
Component Lines of code
Parser 3,982
Rule-handling 2,311
Configuration 731
Total 7,024
Table 1: Distribution of lines of code in the application.
the configuration files and is mostly not discussed in
this paper.)
The rules and patterns that the system uses are
typical ?semantic grammar? rules in that they allow
the free mixing of literals and non-terminals, with
the non-terminals typically representing domain-
specific types such as ?protein interaction verb.?
Non-terminals are represented as classes. Those
classes are defined in a Prote?ge? ontology. Rules typ-
ically contain at least one element known as a slot.
Slot-fillers can be constrained by classes in the on-
tology. Input that matches a slot is extracted as one
of the participants in a relation. A limited regular
expression language can operate over classes, liter-
als, or slots. The following is a representative rule.
Square brackets indicate slots, curly braces indicate
a class, the question-mark is a regular expression op-
erator, and any other text is a literal.
{c-interact} := [interactor1]
{c-interact-verb} the?
[interactor2]
The input NEF binds PACS-2 (PMID 18296443)
would match that rule. The result would be the
recognition of a protein interaction event, with in-
teractor1 being NEF and interactor2 being PACS-2.
Not all rules utilize all possibilities of the rule lan-
guage, and we took this into account in one of our
experiments; we discuss the rules further later in the
paper in the context of that experiment.
2.2 Materials
In this work, we made use of the following sets of
materials.
? A large data set distributed as training data for
part of the BioCreative II shared task. It is de-
scribed in detail in (Krallinger et al, 2007).
Briefly, its domain is molecular biology, and
in particular protein-protein interactions?an
important topic of research in computational
Test type Number of tests
Basic 85
Pattern/rule 67
Patterns only 90
Slots 9
Slot nesting 7
Slot property 20
Total 278
Table 2: Distribution of functional tests.
bioscience, with significance to a wide range
of topics in biology, including understanding
the mechanisms of human diseases (Kann et
al., 2006). The corpus contained 3,947,200
words, making it almost an order of mag-
nitude larger than the most commonly used
biomedical corpus (GENIA, at about 433K
words). This data set is publicly available via
biocreative.sourceforge.net.
? In conjunction with that data set: a set of 98
rules written in a data-driven fashion by man-
ually examining the BioCreative II data de-
scribed just above. These rules were used in the
BioCreative II shared task, where they achieved
the highest score in one category. The set of
rules is available on our SourceForge site at
bionlp.sourceforge.net.
? A set of functional tests created by the primary
developer of the system. Table 2 describes the
breakdown of the functional tests across vari-
ous aspects of the design and functionality of
the application.
2.3 Assessing coverage
We used the open-source Cobertura tool
(Mark Doliner, personal communication;
cobertura.sourceforge.net) to mea-
sure coverage. Cobertura reports line coverage and
branch coverage on a per-package basis and, within
each package, on a per-class basis1.
The architecture of the application is such that
Cobertura?s per-package approach resulted in three
1Cobertura is Java-specific. PyDEV provides code coverage
analysis for Python, as does Coverage.py.
25
sets of coverage reports: for the configuration file
processing code, for the rule-processing code, and
for the parser code. We report results for the appli-
cation as a whole, for the parser code, and for the
rule-processing code. We did note differences in the
configuration code coverage for the various condi-
tions, but it does not change the overall conclusions
of the paper and is omitted from most of the discus-
sion due to considerations of space and of general
interest.
3 Results
We conducted three separate experiments.
3.1 The most basic experiment: test suite
versus corpus
In the most basic experiment, we contrasted
class, line, and branch coverage when running the
developer-constructed test suite and when running
the corpus and the corpus-based rules. Tables 3 and
4 show the resulting data. As the first two lines
of Table 3 show, for the entire application (parser,
rule-handling, and configuration), line coverage was
higher with the test suite?56% versus 41%?and
branch coverage was higher as well?41% versus
28% (see the first two lines of Table 3).
We give here a more detailed discussion of the re-
sults for the entire code base. (Detailed discussions
for the parser and rule packages, including granular
assessments of class coverage, follow.)
For the parser package:
? Class coverage was higher with the test suite
than with the corpus?88% (22/25) versus 80%
(20/25).
? For the entire parser package, line coverage
was higher with the test suite than with the
corpus?55% versus 41%.
? For the entire parser package, branch cover-
age was higher with the test suite than with the
corpus?57% versus 29%.
Table 4 gives class-level data for the two main
packages. For the parser package:
? Within the 25 individual classes of the parser
package, line coverage was equal or greater
with the test suite for 21/25 classes; it was not
just equal but greater for 14/25 classes.
? Within those 21 of the 25 individual classes
that had branching logic, branch coverage was
equal or greater with the test suite for 19/21
classes, and not just equal but greater for 18/21
classes.
For the rule-handling package:
? Class coverage was higher with the test suite
than with the corpus?100% (20/20) versus
90% (18/20).
? For the entire rules package, line coverage was
higher with the test suite than with the corpus?
63% versus 42%.
? For the entire rules package, branch coverage
was higher with the test suite than with the
corpus?71% versus 24%.
Table 4 gives the class-level data for the rules
package:
? Within the 20 individual classes of the rules
package, line coverage was equal or greater
with the test suite for 19/20 classes, and not just
equal but greater for 6/20 classes.
? Within those 11 of the 20 individual classes
that had branching logic, branch coverage was
equal or greater with the test suite for all
11/11 classes, and not just equal but greater for
(again) all 11/11 classes.
3.2 The second experiment: Varying the size of
the rule set
Pilot studies suggested (as later experiments veri-
fied) that the size of the input corpus had a negligible
effect on coverage. This suggested that it would be
worthwhile to assess the effect of the rule set on cov-
erage independently. We used simple ablation (dele-
tion of portions of the rule set) to vary the size of the
rule set.
We created two versions of the original rule set.
We focussed only on the non-lexical, relational pat-
tern rules, since they are completely dependent on
the lexical rules. Each version was about half the
26
Metric Functional tests Corpus, all rules nominal rules verbal rules
Overall line coverage 56% 41% 41% 41%
Overall branch coverage 41% 28% 28% 28%
Parser line coverage 55% 41% 41% 41%
Parser branch coverage 57% 29% 29% 29%
Rules line coverage 63% 42% 42% 42%
Rules branch coverage 71% 24% 24% 24%
Parser class coverage 88% (22/25) 80% (20/25)
Rules class coverage 100% (20/20) 90% (18/20)
Table 3: Application and package-level coverage statistics using the developer?s functional tests, the full corpus with
the full set of rules, and the full corpus with two reduced sets of rules. The highest value in a row is bolded. The final
three columns are intentionally identical (see explanation in text).
Package Line coverage >= Line coverage > Branch coverage >= Branch coverage >
Classes in parser package 21/25 14/25 19/21 18/21
Classes in rules package 19/20 6/20 11/11 11/11
Table 4: When individual classes were examined, both line and branch coverage were always higher with the functional
tests than with the corpus. This table shows the magnitude of the differences. >= indicates the number of classes that
had equal or greater coverage with the functional tests than with the corpus, and > indicates just the classes that had
greater coverage with the functional tests than with the corpus.
size of the original set. The first consisted of the
first half of the rule set, which happened to consist
primarily of verb-based patterns. The second con-
sisted of the second half of the rule set, which corre-
sponded roughly to the nominalization rules.
The last two columns of Table 3 show the
package-level results. Overall, on a per-package ba-
sis, there were no differences in line or branch cov-
erage when the data was run against the full rule set
or either half of the rule set. (The identity of the last
three columns is due to this lack of difference in re-
sults between the full rule set and the two reduced
rule sets.) On a per-class level, we did note minor
differences, but as Table 3 shows, they were within
rounding error on the package level.
3.3 The third experiment: Coverage closure
In the third experiment, we looked at how cover-
age varies as increasingly larger amounts of the cor-
pus are processed. This methodology is compara-
ble to examining the closure properties of a corpus
in a corpus linguistics study (see e.g. Chapter 6 of
(McEnery and Wilson, 2001)) (and as such may be
sensitive to the extent to which the contents of the
corpus do or do not fit the sublanguage model). We
counted cumulative line coverage as increasingly
large amounts of the corpus were processed, rang-
ing from 0 to 100% of its contents. The results for
line coverage are shown in Figure 1. (The results for
branch coverage are quite similar, and the graph is
not shown.) Line coverage for the entire application
is indicated by the thick solid line. Line coverage
for the parser package is indicated by the thin solid
line. Line coverage for the rules package is indi-
cated by the light gray solid line. The broken line
indicates the number of pattern matches?quantities
should be read off of the right y axis.
The figure shows quite graphically the lack of ef-
fect on coverage of increasing the size of the cor-
pus. For the entire application, the line coverage is
27% when an empty document has been read in, and
39% when a single sentence has been processed; it
increases by one to 40% when 51 sentences have
been processed, and has grown as high as it ever
will?41%?by the time 1,000 sentences have been
processed. Coverage at 191,478 sentences?that is,
3,947,200 words?is no higher than at 1,000 sen-
tences, and barely higher, percentagewise, than at a
single sentence.
An especially notable pattern is that the huge rise
27
Figure 1: Increase in percentage of line coverage as in-
creasing amounts of the corpus are processed. Left y axis
is the percent coverage. The x axis is the number of sen-
tences. Right y axis (scale 0-12,000) is the number of
rule matches. The heavy solid line is coverage for the en-
tire package, the thin solid line is coverage for the parser
package, the light gray line is coverage for the rules pack-
age, and the broken line is the number of pattern matches.
in the number of matches to the rules (graphed by
the broken line) between 5,000 sentences and 191K
sentences has absolutely no effect on code coverage.
4 Discussion
The null hypothesis?that a synthetic test suite
and a naturalistic corpus provide the same code
coverage?is not supported by the data shown here.
Furthermore, the widely, if implicitly, held assump-
tion that a corpus would provide the best testing data
can be rejected, as well. The results reported here
are consistent with the hypothesis that code cover-
age for this application is not affected by the size of
the corpus or by the size of the rule set, and that run-
ning it on a large corpus does not guarantee thorough
testing. Rather, coverage is optimized by traditional
software testing.
4.1 Related work
Although software testing is a first-class research
object in computer science, it has received little at-
tention in the natural language processing arena. A
notable exception to this comes from the grammar
engineering community. This has produced a body
of publications that includes Oepen?s work on test
suite design (Oepen et al, 1998), Volk?s work on test
suite encoding (Volk, 1998), Oepen et al?s work on
the Redwoods project (Oepen et al, 2002), Butt and
King?s discussion of the importance of testing (Butt
and King, 2003), Flickinger et al?s work on ?seman-
tics debugging? with Redwoods data (Flickinger et
al., 2005), and Bender et al?s recent work on test
suite generation (Bender et al, 2007). Outside of
the realm of grammar engineering, work on test-
ing for NLP is quite limited. (Cohen et al, 2004)
describes a methodology for generating test suites
for molecular biology named entity recognition sys-
tems, and (Johnson et al, 2007) describes the de-
velopment of a fault model for linguistically-based
ontology mapping, alignment, and linking systems.
However, when most researchers in the NLP com-
munity refer in print to ?testing,? they do not mean
it in the sense in which that term is used in soft-
ware engineering. Some projects have publicized as-
pects of their testing work, but have not published on
their approaches: the NLTK project posts module-
level line coverage statistics, having achieved me-
dian coverage of 55% on 116 Python modules2 and
38% coverage for the project as a whole; the MAL-
LET project indicates on its web site that it en-
courages the production of unit tests during devel-
opment, but unfortunately does not go into details
of their recommendations for unit-testing machine
learning code3.
4.2 Conclusions
We note a number of shortcomings of code cov-
erage. For example, poor coding conventions
can actually inflate your line coverage. Con-
sider a hypothetical application consisting only
of the following, written as a single line of code
with no line breaks: if (myVariable ==
1) doSomething elsif (myVariable
== 2) doSomethingElse elsif
(myVariable = 3) doYetAnotherThing
and a poor test suite consisting only of inputs that
will cause myVariable to ever have the value 1.
The test suite will achieve 100% line coverage for
2nltk.org/doc/guides/coverage
3mallet.cs.umass.edu/index.php/
Guidelines for writing unit tests
28
this application?and without even finding the error
that sets myVariable to 3 if it is not valued 1
or 2. If the code were written with reasonable line
breaks, code coverage would be only 20%. And,
as has been noted by others, code coverage can not
detect ?sins of omission??bugs that consist of the
failure to write needed code (e.g. for error-handling
or for input validation). We do not claim that code
coverage is wholly sufficient for evaluating a test
suite; nonetheless, it is one of a number of metrics
that are helpful in judging the adequacy of a testing
effort. Another very valuable one is the found/fixed
or open/closed graph (Black, 1999; Baumgartner Jr.
et al, 2007).
While remaining aware of the potential shortcom-
ings of code coverage, we also note that the data
reported here supports its utility. The developer-
written functional tests were produced without mon-
itoring code coverage; even though those tests rou-
tinely produced higher coverage than a large corpus
of naturalistic text, they achieved less than 60% cov-
erage overall, as predicted by Wiegers?s work cited
in the introduction. We now have the opportunity to
raise that coverage via structured testing performed
by someone other than the developer. In fact, our
first attempts to test the previously unexercised code
immediately uncovered two showstopper bugs; the
coverage analysis also led us to the discovery that
the application?s error-handling code was essentially
untested.
Although we have explored a number of dimen-
sions of the space of the coverage phenomenon, ad-
ditional work could be done. We used a relatively
naive approach to rule ablation in the second experi-
ment; a more sophisticated approach would be to ab-
late specific types of rules?for example, ones that
do or don?t contain slots, ones that do or don?t con-
tain regular expression operators, etc.?and monitor
the coverage changes. (We did run all three experi-
ments on a separate, smaller corpus as a pilot study;
we report the results for the BioCreative II data set
in this paper since that is the data for which the rules
were optimized. Results in the pilot study were en-
tirely comparable.)
In conclusion: natural language processing appli-
cations are particularly susceptible to emergent phe-
nomena, such as interactions between the contents
of a rule set and the contents of a corpus. These
are especially difficult to control when the evalua-
tion corpus is naturalistic and the rule set is data-
driven. Structured testing does not eliminate this
emergent nature of the problem space, but it does
allow for controlled evaluation of the performance
of your system. Corpora also are valuable evalua-
tion resources: the combination of a structured test
suite and a naturalistic corpus provides a powerful
set of tools for finding bugs in NLP applications.
Acknowledgments
The authors thank James Firby, who wrote the func-
tional tests, and Helen L. Johnson, who wrote the
rules that were used for the BioCreative data. Steve
Bethard and Aaron Cohen recommended Python
coverage tools. We also thank the three anonymous
reviewers.
References
William A. Baumgartner Jr., K. Bretonnel Cohen, Lynne
Fox, George K. Acquaah-Mensah, and Lawrence
Hunter. 2007. Manual curation is not sufficient
for annotation of genomic databases. Bioinformatics,
23:i41?i48.
William A. Baumgartner Jr., Zhiyong Lu, Helen L. John-
son, J. Gregory Caporaso, Jesse Paquette, Anna Linde-
mann, Elizabeth K. White, Olga Medvedeva, K. Bre-
tonnel Cohen, and Lawrence Hunter. In press. Con-
cept recognition for extracting protein interaction rela-
tions from biomedical text. Genome Biology.
Emily M. Bender, Laurie Poulson, Scott Drellishak, and
Chris Evans. 2007. Validation and regression test-
ing for a cross-linguistic grammar resource. In ACL
2007 Workshop on Deep Linguistic Processing, pages
136?143, Prague, Czech Republic, June. Association
for Computational Linguistics.
Rex Black. 1999. Managing the Testing Process.
Miriam Butt and Tracy Holloway King. 2003. Grammar
writing, testing and evaluation. In Ali Farghaly, editor,
A handbook for language engineers, pages 129?179.
CSLI.
Geoffrey Chang, Christopher R. Roth, Christopher L.
Reyes, Owen Pornillos, Yen-Ju Chen, and Andy P.
Chen. 2006. Letters: Retraction. Science, 314:1875.
K. Bretonnel Cohen, Lorraine Tanabe, Shuhei Kinoshita,
and Lawrence Hunter. 2004. A resource for construct-
ing customized test suites for molecular biology entity
identification systems. In HLT-NAACL 2004 Work-
shop: BioLINK 2004, Linking Biological Literature,
Ontologies and Databases, pages 1?8. Association for
Computational Linguistics.
29
Dan Flickinger, Alexander Koller, and Stefan Thater.
2005. A new well-formedness criterion for semantics
debugging. In Proceedings of the HPSG05 Confer-
ence.
Lawrence Hunter, Zhiyong Lu, James Firby, William
A. Baumgartner Jr., Helen L. Johnson, Philip V. Ogren,
and K. Bretonnel Cohen. 2008. OpenDMAP: An
open-source, ontology-driven concept analysis engine,
with applications to capturing knowledge regarding
protein transport, protein interactions and cell-specific
gene expression. BMC Bioinformatics, 9(78).
Helen L. Johnson, K. Bretonnel Cohen, and Lawrence
Hunter. 2007. A fault model for ontology mapping,
alignment, and linking systems. In Pacific Sympo-
sium on Biocomputing, pages 233?244. World Scien-
tific Publishing Company.
Cem Kaner, Hung Quoc Nguyen, and Jack Falk. 1999.
Testing computer software, 2nd edition. John Wiley
and Sons.
Maricel Kann, Yanay Ofran, Marco Punta, and Predrag
Radivojac. 2006. Protein interactions and disease. In
Pacific Symposium on Biocomputing, pages 351?353.
World Scientific Publishing Company.
Martin Krallinger, Florian Leitner, and Alfonso Valen-
cia. 2007. Assessment of the second BioCreative PPI
task: automatic extraction of protein-protein interac-
tions. In Proceedings of the Second BioCreative Chal-
lenge Evaluation Workshop.
Brian Marick. 1997. The craft of software testing:
subsystem testing including object-based and object-
oriented testing. Prentice Hall.
Steve McConnell. 2004. Code complete. Microsoft
Press, 2nd edition.
Tony McEnery and Andrew Wilson. 2001. Corpus Lin-
guistics. Edinburgh University Press, 2nd edition.
Greg Miller. 2006. A scientist?s nightmare: software
problem leads to five retractions. Science, 314:1856?
1857.
Glenford Myers. 1979. The art of software testing. John
Wiley and Sons.
S. Oepen, K. Netter, and J. Klein. 1998. TSNLP - test
suites for natural language processing. In John Ner-
bonne, editor, Linguistic Databases, chapter 2, pages
13?36. CSLI Publications.
Stephan Oepen, Kristina Toutanova, Stuart Shieber,
Christopher Manning, Dan Flickinger, and Thorsten
Brants. 2002. The LinGO Redwoods treebank: mo-
tivation and preliminary applications. In Proceedings
of the 19th international conference on computational
linguistics, volume 2.
Martin Volk. 1998. Markup of a test suite with SGML.
In John Nerbonne, editor, Linguistic databases, pages
59?76. CSLI Publications.
30
Proceedings of the Workshop on BioNLP: Shared Task, pages 50?58,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
High-precision biological event extraction with a concept recognizer
K. Bretonnel Cohen?, Karin Verspoor?, Helen L. Johnson, Chris Roeder,
Philip V. Ogren, William A. Baumgartner Jr., Elizabeth White, Hannah Tipney, and Lawrence Hunter
Center for Computational Pharmacology
University of Colorado Denver School of Medicine
PO Box 6511, MS 8303, Aurora, CO 80045 USA
kevin.cohen@gmail.com, karin.verspoor@ucdenver.edu, helen.linguist@gmail.com,
chris.roeder@ucdenver.edu, philip@ogren.info, william.baumgartner@ucdenver.edu,
elizabeth.white@colorado.edu, hannah.tipney@ucdenver.edu, larry.hunter@ucdenver.edu
Abstract
We approached the problems of event detec-
tion, argument identification, and negation and
speculation detection as one of concept recog-
nition and analysis. Our methodology in-
volved using the OpenDMAP semantic parser
with manually-written rules. We achieved
state-of-the-art precision for two of the three
tasks, scoring the highest of 24 teams at pre-
cision of 71.81 on Task 1 and the highest of 6
teams at precision of 70.97 on Task 2.
The OpenDMAP system and the rule set are
available at bionlp.sourceforge.net.
*These two authors contributed equally to the
paper.
1 Introduction
We approached the problem of biomedical event
recognition as one of concept recognition and anal-
ysis. Concept analysis is the process of taking a
textual input and building from it an abstract rep-
resentation of the concepts that are reflected in it.
Concept recognition can be equivalent to the named
entity recognition task when it is limited to locat-
ing mentions of particular semantic types in text, or
it can be more abstract when it is focused on recog-
nizing predicative relationships, e.g. events and their
participants.
2 BioNLP?09 Shared Task
Our system was entered into all three of the
BioNLP?09 (Kim et al, 2009) shared tasks:
? Event detection and characterization This
task requires recognition of 9 basic biological
events: gene expression, transcription, protein
catabolism, protein localization, binding, phos-
phorylation, regulation, positive regulation and
negative regulation. It requires identification
of the core THEME and/or CAUSE participants
in the event, i.e. the protein(s) being produced,
broken down, bound, regulated, etc.
? Event argument recognition This task builds
on the previous task, adding in additional argu-
ments of the events, such as the site (protein or
DNA region) of a binding event, or the location
of a protein in a localization event.
? Recognition of negations and speculations
This task requires identification of negations of
events (e.g. event X did not occur), and specu-
lation about events (e.g. We claim that event X
should occur).
3 Our approach
We used the OpenDMAP system developed at the
University of Colorado School of Medicine (Hunter
et al, 2008) for our submission to the BioNLP
?09 Shared Task on Event Extraction. OpenDMAP
is an ontology-driven, integrated concept analysis
system that supports information extraction from
text through the use of patterns represented in a
classic form of ?semantic grammar,? freely mixing
text literals, semantically typed basal syntactic con-
stituents, and semantically defined classes of enti-
ties. Our approach is to take advantage of the high
50
quality ontologies available in the biomedical do-
main to formally define entities, events, and con-
straints on slots within events and to develop pat-
terns for how concepts can be expressed in text that
take advantage of both semantic and linguistic char-
acteristics of the text. We manually built patterns for
each event type by examining the training data and
by using native speaker intuitions about likely ways
of expressing relationships, similar to the technique
described in (Cohen et al, 2004). The patterns char-
acterize the linguistic expression of that event and
identify the arguments (participants) of the events
according to (a) occurrence in a relevant linguistic
context and (b) satisfaction of appropriate semantic
constraints, as defined by our ontology. Our solution
results in very high precision information extraction,
although the current rule set has limited recall.
3.1 The reference ontology
The central organizing structure of an OpenDMAP
project is an ontology. We built the ontology
for this project by combining elements of several
community-consensus ontologies?the Gene Ontol-
ogy (GO), Cell Type Ontology (CTO), BRENDA
Tissue Ontology (BTO), Foundational Model of
Anatomy (FMA), Cell Cycle Ontology (CCO), and
Sequence Ontology (SO)?and a small number of
additional concepts to represent task-specific aspects
of the system, such as event trigger words. Combin-
ing the ontologies was done with the Prompt plug-in
for Prote?ge?.
The ontology included concepts representing each
event type. These were represented as frames, with
slots for the various things that needed to be re-
turned by the system?the trigger word and the var-
ious slot fillers. All slot fillers were constrained to
be concepts in some community-consensus ontol-
ogy. The core event arguments were constrained in
the ontology to be of type protein from the Sequence
Ontology (except in the case of regulation events,
where biological events themselves could satisfy the
THEME role), while the type of the other event argu-
ments varied. For instance, the ATLOC argument
of a gene expression event was constrained to be
one of tissue (from BTO), cell type (from CTO), or
cellular component (from GO-Cellular Component),
while the BINDING argument of a binding event was
constrained to be one of binding site, DNA, domain,
or chromosome (all from the SO and all tagged by
LingPipe). Table 1 lists the various types.
3.2 Named entity recognition
For proteins, we used the gold standard annota-
tions provided by the organizers. For other seman-
tic classes, we constructed a compound named en-
tity recognition system which consists of a LingPipe
GENIA tagging module (LingPipe, (Alias-i, 2008)),
and several dictionary look-up modules. The dictio-
nary lookup was done using a component from the
UIMA (IBM, 2009; Ferrucci and Lally, 2004) sand-
box called the ConceptMapper.
We loaded the ConceptMapper with dictionar-
ies derived from several ontologies, including the
Gene Ontology Cellular Component branch, Cell
Type Ontology, BRENDA Tissue Ontology, and
the Sequence Ontology. The dictionaries contained
the names and name variants for each concept in
each ontology, and matches in the input documents
were annotated with the relevant concept ID for the
match. The only modifications that we made to
these community-consensus ontologies were to re-
move the single concept cell from the Cell Type On-
tology and to add the synonym nuclear to the Gene
Ontology Cell Component concept nucleus.
The protein annotations were used to constrain the
text entities that could satisfy the THEME role in the
events of interest. The other named entities were
added for the identification of non-core event partic-
ipants for Task 2.
3.3 Pattern development strategies
3.3.1 Corpus analysis
Using a tool that we developed for visualizing the
training data (described below), a subset of the gold-
standard annotations were grouped by event type
and by trigger word type (nominalization, passive
verb, active verb, or multiword phrase). This orga-
nization helped to suggest the argument structures of
the event predicates and also highlighted the varia-
tion within argument structures. It also showed the
nature of more extensive intervening text that would
need to be handled for the patterns to achieve higher
recall.
Based on this corpus analysis, patterns were de-
veloped manually using an iterative process in which
individual patterns or groups of patterns were tested
51
Table 1: Semantic restrictions on Task 2 event arguments. CCO = Cell Cycle Ontology, FMA = Foundational Model
of Anatomy, other ontologies identified in the text.
Event Type Site AtLoc ToLoc
binding protein domain (SO),
binding site (SO), DNA
(SO), chromosome (SO)
gene expression gene (SO), biological
entity (CCO)
tissue (BTO), cell type
(CTO), cellular compo-
nent (GO)
localization cellular component
(GO)
cellular component
(GO)
phosphorylation amino acid (FMA),
polypeptide region (SO)
protein catabolism cellular component
(GO)
transcription gene (SO), biological
entity (CCO)
on the training data to determine their impact on per-
formance. Pattern writers started with the most fre-
quent trigger words and argument structures.
3.3.2 Trigger words
In the training data, we were provided annotations
of all relevant event types occurring in the training
documents. These annotations included a trigger
word specifying the specific word in the input text
which indicated the occurrence of each event. We
utilized the trigger words in the training set as an-
chors for our linguistic patterns. We built patterns
around the generic concept of, e.g. an expression
trigger word and then varied the actual strings that
were allowed to satisfy that concept. We then ran ex-
periments with our patterns and these varying sets of
trigger words for each event type, discarding those
that degraded system performance when evaluated
with respect to the gold standard annotations.
Most often a trigger word was removed from an
event type trigger list because it was also a trig-
ger word for another event type and therefore re-
duced performance by increasing the false positive
rate. For example, the trigger words ?level? and
?levels? appear in the training data trigger word lists
of gene expression, transcription, and all three regu-
lation event types.
The selection of trigger words was guided by a
frequency analysis of the trigger words provided in
the task training data. In a post-hoc analysis, we find
that a different proportion of the set of trigger words
was finally chosen for each different event type. Be-
tween 10-20% of the top frequency-ranked trigger
words were used for simple event types, with the
exception that phosphorylation trigger words were
chosen from the top 30%. For instance, for gene ex-
pression all of the top 15 most frequent trigger words
were used (corresponding to the top 16%). For com-
plex event types (the regulations) better performance
was achieved by limiting the list to between 5-10%
of the most frequent trigger words.
In addition, variants of frequent trigger words
were included. For instance, the nominalization ?ex-
pression? is the most frequent gene expression trig-
ger word and the verbal inflections ?expressed? and
?express? are also in the top 20%. The verbal inflec-
tion ?expresses? is ranked lower than the top 30%,
but was nonetheless included as a trigger word in the
gene expression patterns.
3.3.3 Patterns
As in our previous publications on OpenDMAP,
we refer to our semantic rules as patterns. For
this task, each pattern has at a minimum an event
argument THEME and an event-specific trigger
word. For example, {phosphorylation} :=
52
[phosphorylation nominalization][Theme],
where [phosphorylization nominalization]
represents a trigger word. Both elements are defined
semantically. Event THEMEs are constrained by
restrictions placed on them in the ontology, as
described above.
The methodology for creating complex event pat-
terns such as regulation was the same as for sim-
ple events, with the exception that the THEMEs
were defined in the ontology to also include bio-
logical processes. Iterative pattern writing and test-
ing was a little more arduous because these pat-
terns relied on the success of the simple event pat-
terns, and hence more in-depth analysis was re-
quired to perform performance-increasing pattern
adjustments. For further details on the pattern lan-
guage, the reader is referred to (Hunter et al, 2008).
3.3.4 Nominalizations
Nominalizations were very frequent in the train-
ing data; for seven out of nine event types, the most
common trigger word was a nominalization. In writ-
ing our grammars, we focused on these nominaliza-
tions. To write grammars for nominalizations, we
capitalized on some of the insights from (Cohen et
al., 2008). Non-ellided (or otherwise absent) argu-
ments of nominalizations can occur in three basic
positions:
? Within the noun phrase, after the nominaliza-
tion, typically in a prepositional phrase
? Within the noun phrase, immediately preceding
the nominalization
? External to the noun phrase
The first of these is the most straightforward to
handle in a rule-based approach. This is particu-
larly true in the case of a task definition like that
of BioNLP ?09, which focused on themes, since an
examination of the training data showed that when
themes were post-nominal in a prepositional phrase,
then that phrase was most commonly headed by of.
The second of these is somewhat more challeng-
ing. This is because both agents and themes can
occur immediately before the nominalization, e.g.
phenobarbital induction (induction by phenobarbi-
tal) and trkA expression (expression of trkA). To de-
cide how to handle pre-nominal arguments, we made
use of the data on semantic roles and syntactic posi-
tion found in (Cohen et al, 2008). That study found
that themes outnumbered agents in the prenominal
position by a ratio of 2.5 to 1. Based on this obser-
vation, we assigned pre-nominal arguments to the
theme role.
Noun-phrase-external arguments are the most
challenging, both for automatic processing and for
human interpreters; one of the major problems is
to differentiate between situations where they are
present but outside of the noun phrase, and situations
where they are entirely absent. Since the current im-
plementation of OpenDMAP does not have robust
access to syntactic structure, our only recourse for
handling these arguments was through wildcards,
and since they mostly decreased precision without a
corresponding increase in recall, we did not attempt
to capture them.
3.3.5 Negation and speculation
Corpus analysis of the training set revealed two
broad categories each for negation and speculation
modifications, all of which can be described in terms
of the scope of modification.
Negation
Broadly speaking, an event itself can be negated
or some aspect of an event can be negated. In other
words, the scope of a negation modification can be
over the existence of an event (first example below),
or over an argument of an existing event (second ex-
ample).
? This failure to degrade IkappaBalpha ...
(PMID 10087185)
? AP-1 but not NF-IL-6 DNA binding activity ...
(PMID 10233875)
Patterns were written to handle both types of
negation. The negation phrases ?but not? and ?but
neither? were appended to event patterns to catch
those events that were negated as a result of a
negated argument. For event negation, a more ex-
tensive list of trigger words was used that included
verbal phrases such as ?failure to? and ?absence of.?
The search for negated events was conducted in
two passes. Events for which negation cues fall out-
side the span of text that stretches from argument to
53
event trigger word were handled concurrently with
the search for events. A second search was con-
ducted on extracted events for negation cues that fell
within the argument to event trigger word span, such
as
. . . IL-2 does not induce I kappa B alpha degrada-
tion (PMID 10092783)
This second pass allowed us to capture one addi-
tional negation (6 rather than 5) on the test data.
Speculation
The two types of speculation in the training data
can be described by the distinction between ?de re?
and ?de dicto? assertions. The ?de dicto? assertions
of speculation in the training data are modifications
that call into question the degree of known truth of
an event, as in
. . . CTLA-4 ligation did not appear to affect the
CD28 - mediated stabilization (PMID 10029815)
The ?de re? speculation address the potential ex-
istence of an event rather that its degree of truth. In
these cases, the event is often being introduced in
text by a statement of intention to study the event, as
in
. . . we investigated CTCF expression
. . . [10037138]
To address these distinct types of speculation, two
sets of trigger words were developed. One set con-
sisted largely of verbs denoting research activities,
e.g. research, study, examine investigate, etc. The
other set consisted of verbs and adverbs that denote
uncertainty, and included trigger words such as sug-
gests, unknown, and seems.
3.4 Handling of coordination
Coordination was handled using the OpenNLP con-
stituent parser along with the UIMA wrappers that
they provide via their code repository. We chose
OpenNLP because it is easy to train a model, it in-
tegrates easily into a UIMA pipeline, and because
of competitive parsing results as reported by Buyko
(Buyko et al, 2006). The parser was trained using
500 abstracts from the beta version of the GENIA
treebank and 10 full-text articles from the CRAFT
corpus (Verspoor et al, In press). From the con-
stituent parse we extracted coordination structures
into a simplified data structure that captures each
conjunction along with its conjuncts. These were
provided to downstream components. The coordi-
nation component achieves an F-score of 74.6% at
the token level and an F-score of 57.5% at the con-
junct level when evaluated against GENIA. For both
measures the recall was higher than the precision by
4% and 8%, respectively.
We utilized the coordination analysis to identify
events in which the THEME argument was expressed
as a conjoined noun phrase. These were assumed to
have a distributed reading and were post-processed
to create an individual event involving each con-
junct, and further filtered to only include given (A1)
protein references. So, for instance, analysis of the
sentence in the example below should result in the
detection of three separate gene expression events,
involving the proteins HLA-DR, CD86, and CD40,
respectively.
NAC was shown to down-regulate the
production of cytokines by DC as well
as their surface expression of HLA-
DR, CD86 (B7-2), and CD40 molecules
. . . (PMID 10072497)
3.5 Software infrastructure
We took advantage of our existing infrastructure
based on UIMA (The Unstructured Information
Management Architecture) (IBM, 2009; Ferrucci
and Lally, 2004) to support text processing and data
analysis.
3.5.1 Development tools
We developed a visualization tool to enable the
linguistic pattern writers to better analyze the train-
ing data. This tool shows the source text one sen-
tence at a time with the annotated words highlighted.
A list following each sentence shows details of the
annotations.
3.6 Errors in the training data
In some cases, there were discrepancies between the
training data and the official problem definitions.
This was a source of problems in the pattern devel-
opment phase. For example, phosphorylation events
are defined in the task definition as having only a
THEME and a SITE. However, there were instances
in the training data that included both a THEME and
a CAUSE argument. When those events were identi-
fied by our system and the CAUSE was labelled, they
54
were rejected during a syntactic error check by the
test server.
4 Results
4.1 Official Results
We are listed as Team 13. Table 2 shows our re-
sults on the official metrics. Our precision was the
highest achieved by any group for Task 1 and Task
2, at 71.81 for Task 1 and 70.97 for task 2. Our re-
calls were much lower and adversely impacted our
F-measure; ranked by F-measure, we ranked 19th
out of 24 groups.
We noted that our results for the exact match met-
ric and for the approximate match metric were very
close, suggesting that our techniques for named en-
tity recognition and for recognizing trigger words
are doing a good job of capturing the appropriate
spans.
4.2 Other analysis: Bug fixes and coordination
handling
In addition to our official results, we also report in
Table 3 (see last page) the results of a run in which
we fixed a number of bugs. This represents our cur-
rent best estimate of our performance. The precision
drops from 71.81 for Task 1 to 67.19, and from 70.97
for Task 2 to 65.74, but these precisions are still
well above the second-highest precisions of 62.21
for Task 1 and 56.87 for Task 2. As the table shows,
we had corresponding small increases in our recall
to 17.38 and in our F-measure to 27.62 for Task 1,
and in our recall to 17.07 and F-measure to 27.10 for
Task 2.
We evaluated the effects of coordination handling
by doing separate runs with and without this ele-
ment of the processing pipeline. Compared to our
unofficial results, which had an overall F-measure
for Task 1 of 27.62 and for Task 2 of 27.10, a ver-
sion of the system without handling of coordination
had an overall F-measure for Task 1 of 24.72 and for
Task 2 of 24.21.
4.3 Error Analysis
4.3.1 False negatives
To better understand the causes of our low recall,
we performed a detailed error analysis of false neg-
atives using the devtest data. (Note that this section
includes a very small number of examples from the
devtest data.) We found five major causes of false
negatives:
? Intervening material between trigger words and
arguments
? Coordination that was not handled by our coor-
dination component
? Low coverage of trigger words
? Anaphora and coreference
? Appositive gene names and symbols
Intervening material For reasons that we detail
in the Discussion section, we avoided the use of
wildcards. This, and the lack of syntactic analy-
sis in the version of the system that we used (note
that syntactic analyses can be incorporated into an
OpenDMAP workflow), meant that if there was text
intervening between a trigger word and an argument,
e.g. in to efficiently [express] in developing thymo-
cytes a mutant form of the [NF-kappa B inhibitor]
(PMID 10092801), where the bracketed text is the
trigger word and the argument, our pattern would
not match.
Unhandled coordination Our coordination system
only handled coordinated protein names. Thus, in
cases where other important elements of the utter-
ance, such as the trigger word transcription in tran-
scription and subsequent synthesis and secretion
of galectin-3 (PMID 8623933) were in coordinated
structures, we missed the relevant event arguments.
Low coverage of trigger words As we discuss in
the Methods section, we did not attempt to cover
all trigger words, in part because some less-frequent
trigger words were involved in multiple event types,
in part because some of them were extremely low-
frequency and we did not want to overfit to the train-
ing data, and in part due to the time constraints of the
shared task.
Anaphora and coreference Recognition of some
events in the data would require the ability to do
anaphora and coreference resolution. For example,
in Although 2 early lytic transcripts, [BZLF1] and
[BHRF1], were also detected in 13 and 10 cases,
respectively, the lack of ZEBRA staining in any case
indicates that these lytic transcripts are most likely
55
Tasks 1 and 3 Task 2
Event class GS answer R P F R P F
Localization 174 (18) 18 (18) 10.34 100.00 18.75 9.77 94.44 17.71
Binding 347 (44) 110 (44) 12.68 40.00 19.26 12.32 39.09 18.74
Gene expression 722 (263) 306 (263) 36.43 85.95 51.17 36.43 85.95 51.17
Transcription 137 (18) 20 (18) 13.14 90.00 22.93 13.14 90.00 22.93
Protein catabolism 14 (4) 6 (4) 28.57 66.67 40.00 28.57 66.67 40.00
Phosphorylation 135 (30) 30 (30) 22.22 100.00 36.36 20.14 93.33 33.14
EVENT TOTAL 1529 (377) 490 (377) 24.66 76.94 37.35 24.30 76.12 36.84
Regulation 291 (9) 19 (9) 3.09 47.37 5.81 3.08 47.37 5.79
Positive regulation 983 (32) 65 (32) 3.26 49.23 6.11 3.24 49.23 6.08
Negative regulation 379 (10) 22 (10) 2.64 45.45 4.99 2.37 40.91 4.49
REGULATION TOTAL 1653 (51) 106 (51) 3.09 48.11 5.80 3.02 47.17 5.67
Negation 227 (4) 76 (4) 1.76 5.26 2.64
Speculation 208 (14) 105 (14) 6.73 13.33 8.95
MODIFICATION TOTAL 435 (18) 181 (18) 4.14 9.94 5.84
ALL TOTAL 3182 (428) 596 (428) 13.45 71.81 22.66 13.25 70.97 22.33
Table 2: Official scores for Tasks 1 and 2, and modification scores only for Task 3, from the approximate span
matching/approximate recursive matching table. GS = gold standard (true positives) (given for Tasks 1/3 only), answer
= all responses (true positives) (given for tasks 1/3 only), R = recall, P = precision, F = F-measure. All results are as
calculated by the official scoring application.
[expressed] by rare cells in the biopsies entering
lytic cycle (PMID 8903467), where the bracketed
text is the arguments and the trigger word, the syn-
tactic object of the verb is the anaphoric noun phrase
these lytic transcripts, so even with the addition of
a syntactic component to our system, we still would
not have recognized the appropriate arguments with-
out the ability to do anaphora resolution.
Appositives The annotation guidelines for proteins
apparently specified that when a gene name was
present in an appositive with its symbol, the symbol
was selected as the gold-standard argument. For this
reason, in examples like [expression] of Fas ligand
[FasL] (PMID 10092076), where the bracketed text
is the trigger word and the argument, the gene name
constituted intervening material from the perspec-
tive of our patterns, which therefore did not match.
We return to a discussion of recall and its implica-
tions for systems like ours in the Discussion section.
4.3.2 False positives
Although our overall rate of false positives was
low, we sampled 45 false positive events distributed
across the nine event types and reviewed them with
a biologist.
We noted two main causes of error. The most
common was that we misidentified a slot filler or
were missing a slot filler completely for an actual
event. The other main reason for false positives was
when we erroneously identified a (non)event. For
example, in coexpression of NF-kappa B/Rel and
Sp1 transcription factors (PMID 7479915), we mis-
takenly identified Sp1 transcription as an event.
5 Discussion
Our results demonstrate that it is possible to achieve
state-of-the art precision over a broad range of tasks
and event types using our approach of manually
constructed, ontologically typed rules?our preci-
sion of 71.81 on Task 1 was ten points higher than
the second-highest precision (62.21), and our preci-
sion of 70.97 on Task 2 was 14 points higher than
the second-highest precision (56.87). It remains the
case that our recall was low enough to drop our F-
measure considerably. Will it be the case that a sys-
tem like ours can scale to practical performance lev-
els nonetheless? Four factors suggest that it can.
The first is that there is considerable redundancy
in the data; although we have not quantified it for
this data set, we note that the same event is often
56
Tasks 1 and 3 Task 2
Event class GS answer R P F R P F
Localization 174 (33) 41 (33) 18.97 80.49 30.70 16.67 69.05 26.85
Binding 347 (62) 152 (62) 17.87 40.79 24.85 17.48 40.13 24.35
Gene expression 722 (290) 344 (290) 40.17 84.30 54.41 40.17 84.30 54.41
Transcription 137 (28) 31 (28) 20.44 90.32 33.33 20.44 90.32 33.33
Protein catabolism 14 (4) 6 (4) 28.57 66.67 40.00 28.57 66.67 40.00
Phosphorylation 135 (47) 48 (47) 34.81 97.92 51.37 32.37 84.91 46.88
EVENT TOTAL 1529 (464) 622 (464) 30.35 74.60 43.14 29.77 72.77 42.26
Regulation 291 (11) 31 (11) 3.78 35.48 6.83 3.77 35.48 6.81
Positive regulation 983 (60) 129 (60) 6.10 46.51 10.79 6.08 46.51 10.75
Negative regulation 379 (18) 41 (18) 4.75 43.90 8.57 4.49 41.46 8.10
REGULATION TOTAL 1653 (89) 201 (89) 5.38 44.28 9.60 5.31 43.78 9.47
Negation 227 (6) 129 (6) 2.64 4.65 3.37
Speculation 208 (25) 165 (25) 12.02 15.15 13.40
MODIFICATION TOTAL 435 (31) 294 (31) 7.13 10.54 8.50
ALL TOTAL 3182 (553) 823 (553) 17.38 67.19 27.62 17.07 65.74 27.10
Table 3: Updated results on test data for Tasks 1-3, with important bug fixes in the code base. See key above.
mentioned repeatedly, but for knowledge base build-
ing and other uses of the extracted information, it is
only strictly necessary to recognize an event once
(although multiple recognition of the same assertion
may increase our confidence in its correctness).
The second is that there is often redundancy
across the literature; the best-supported assertions
will be reported as initial findings and then repeated
as background information.
The third is that these recall results reflect an ap-
proach that made no use of syntactic analysis be-
yond handling coordination. There is often text
present in the input that cannot be disregarded with-
out either using wildcards, which generally de-
creased precision in our experiments and which
we generally eschewed, or making use of syntac-
tic information to isolate phrasal heads. Syntactic
analysis, particularly when combined with analysis
of predicate-argument structure, has recently been
shown to be an effective tool in biomedical infor-
mation extraction (Miyao et al, 2009). There is
broad need for this?for example, of the thirty lo-
calization events in the training data whose trigger
word was translocation, a full eighteen had inter-
vening textual material that made it impossible for
simple patterns like translocationof [Theme] or
[ToLoc]translocation to match.
Finally, our recall numbers reflect a very short de-
velopment cycle, with as few as four patterns writ-
ten for many event types. A less time-constrained
pattern-writing effort would almost certainly result
in increased recall.
Acknowledgments
We gratefully acknowledge Mike Bada?s help in
loading the Sequence Ontology into Prote?ge?.
This work was supported by NIH
grants R01LM009254, R01GM083649, and
R01LM008111 to Lawrence Hunter and
T15LM009451 to Philip Ogren.
References
Alias-i. 2008. LingPipe 3.1.2.
Ekaterina Buyko, Joachim Wermter, Michael Poprat, and
Udo Hahn. 2006. Automatically mapping an NLP
core engine to the biology domain. In Proceedings
of the ISMB 2006 joint BioLINK/Bio-Ontologies meet-
ing.
K. B. Cohen, L. Tanabe, S. Kinoshita, and L. Hunter.
2004. A resource for constructing customized test
suites for molecular biology entity identification sys-
tems. BioLINK 2004, pages 1?8.
K. Bretonnel Cohen, Martha Palmer, and Lawrence
Hunter. 2008. Nominalization and alternations in
biomedical language. PLoS ONE, 3(9).
57
D. Ferrucci and A. Lally. 2004. Building an example
application with the unstructured information manage-
ment architecture. IBM Systems Journal, 43(3):455?
475, July.
Lawrence Hunter, Zhiyong Lu, James Firby, William
A. Baumgartner Jr., Helen L. Johnson, Philip V. Ogren,
and K. Bretonnel Cohen. 2008. OpenDMAP: An
open-source, ontology-driven concept analysis engine,
with applications to capturing knowledge regarding
protein transport, protein interactions and cell-specific
gene expression. BMC Bioinformatics, 9(78).
IBM. 2009. UIMA Java framework. http://uima-
framework.sourceforge.net/.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview
of BioNLP?09 shared task on event extraction. In
Proceedings of Natural Language Processing in
Biomedicine (BioNLP) NAACL 2009 Workshop. To
appear.
Yusuke Miyao, Kenji Sagae, Rune Saetre, Takuya Mat-
suzaki, and Jun?ichi Tsujii. 2009. Evaluating contri-
butions of natural language parsers to protein-protein
interaction extraction. Bioinformatics, 25(3):394?400.
Karin Verspoor, K. Bretonnel Cohen, and Lawrence
Hunter. In press. The textual characteristics of tradi-
tional and Open Access scientific journals are similar.
BMC Bioinformatics.
58
Proceedings of the 2011 Workshop on Biomedical Natural Language Processing, ACL-HLT 2011, pages 38?45,
Portland, Oregon, USA, June 23-24, 2011. c?2011 Association for Computational Linguistics
Fast and simple semantic class assignment for biomedical text
K. Bretonnel Cohen
Computational Bioscience Program
U. Colorado School of Medicine
and
Department of Linguistics
U. of Colorado at Boulder
kevin.cohen@gmail.com
Tom Christiansen
Comput. Bioscience Prog.
U. Colorado Sch. of Medicine
tchrist@perl.com
William A. Baumgartner Jr.
Computational Bioscience Program
U. Colorado School of Medicine
william.baumgartner@ucdenver.edu
Karin Verspoor
Computational Bioscience Program
U. Colorado School of Medicine
karin.verspoor@ucdenver.edu
Lawrence E. Hunter
Computational Bioscience Program
U. Colorado School of Medicine
larry.hunter@ucdenver.edu
Abstract
A simple and accurate method for assigning
broad semantic classes to text strings is pre-
sented. The method is to map text strings
to terms in ontologies based on a pipeline of
exact matches, normalized strings, headword
matching, and stemming headwords. The
results of three experiments evaluating the
technique are given. Five semantic classes
are evaluated against the CRAFT corpus of
full-text journal articles. Twenty semantic
classes are evaluated against the correspond-
ing full ontologies, i.e. by reflexive match-
ing. One semantic class is evaluated against
a structured test suite. Precision, recall,
and F-measure on the corpus when evaluat-
ing against only the ontologies in the cor-
pus is micro-averaged 67.06/78.49/72.32 and
macro-averaged 69.84/83.12/75.31. Accuracy
on the corpus when evaluating against all
twenty semantic classes ranges from 77.12%
to 95.73%. Reflexive matching is generally
successful, but reveals a small number of er-
rors in the implementation. Evaluation with
the structured test suite reveals a number of
characteristics of the performance of the ap-
proach.
1 Introduction
Broad semantic class assignment is useful for a
number of language processing tasks, including
coreference resolution (Hobbs, 1978), document
classification (Caporaso et al, 2005), and informa-
tion extraction (Baumgartner Jr. et al, 2008). A
limited number of semantic classes have been stud-
ied extensively, such as assigning text strings to the
category gene or protein (Yeh et al, 2005;
Smith et al, 2008), or the PERSON, ORGANI-
ZATION, and LOCATION categories introduced in
the Message Understanding Conferences (Chinchor,
1998). A larger number of semantic classes have re-
ceived smaller amounts of attention, e.g. the classes
in the GENIA ontology (Kim et al, 2004), vari-
ous event types derived from the Gene Ontology
(Kim et al, 2009), and diseases (Leaman and Gon-
zalez, 2008). However, many semantic types have
not been studied at all. In addition, where ontolo-
gies are concerned, although there has been work
on finding mentions or evidence of specific terms in
text (Blaschke et al, 2005; Stoica and Hearst, 2006;
Davis et al, 2006; Shah et al, 2009), there has been
no work specifically addressing assigning multiple
very broad semantic classes with potential overlap.
In particular, this paper examines the problem of tak-
ing a set of ontologies and a text string (typically,
but not necessarily, a noun phrase) as input and de-
termining which ontology defines the semantic class
that that text string refers to. We make an equiva-
lence here between the notion of belonging to the
domain of an ontology and belonging to a specific
semantic class. For example, if a string in text refers
to something in the domain of the Gene Ontology,
we take it as belonging to a Gene Ontology seman-
tic class (using the name of the ontology only for
convenience); if a string in text refers to something
belonging to the domain of the Sequence Ontology,
we take it as belonging to a Sequence Ontology se-
mantic class. We focus especially on rapid, simple
methods for making such a determination.
The problem is most closely related to multi-class
38
classification, where in the case of this study we are
including an unusually large number of categories,
with possible overlap between them. A text string
might refer to something that legitimately belongs
to the domain of more than one ontology. For exam-
ple, it might belong to the semantic classes of both
the Gene Ontology and the Gene Regulation Ontol-
ogy; regulation is an important and frequent concept
in the Gene Ontology. This fact has consequences
for defining the notion of a false positive class as-
signment; we return to this issue in the Results sec-
tion.
2 Methods
2.1 Target semantic classes
The following ontologies were used to define se-
mantic classes:
? Gene Ontology
? Sequence Ontology
? Foundational Model of Anatomy
? NCBI Taxonomy
? Chemical Entities of Biological Interest
? Phenotypic Quality
? BRENDA Tissue/Enzyme Source
? Cell Type Ontology
? Gene Regulation Ontology
? Homology Ontology
? Human Disease Ontology
? Human Phenotype Ontology
? Mammalian Phenotype Ontology
? Molecule Role Ontology
? Mouse Adult Gross Anatomy Ontology
? Mouse Pathology Ontology
? Protein Modification Ontology
? Protein-Protein Interaction Ontology
? Sample Processing and Separation Techniques
Ontology
? Suggested Ontology for Pharmacogenomics
2.2 Methodology for assigning semantic class
We applied four simple techniques for attempting to
match a text string to an ontology. They are arranged
in order of decreasing stringency. That is, each sub-
sequent method has looser requirements for a match.
This both allows us to evaluate the contribution of
each component more easily and, at run time, allows
the user to set a stringency level, if the default is not
desired.
2.2.1 Exact match
The first and most stringent technique is exact
match. (This is essentially the only technique used
by the NCBO (National Center for Biomedical On-
tology) Annotator (Jonquet et al, 2009), although
it can also do substring matching.) We normalize
terms in the ontology and text strings in the input
for case and look for a match.
2.2.2 Stripping
All non-alphanumeric characters, including
whitespace, are deleted from the terms in the
ontology and from text strings in the input (e.g.
cadmium-binding and cadmium binding both
become cadmiumbinding) and look for a match.
2.2.3 Head nouns
This method involves a lightweight linguistic
analysis. We traversed each ontology and deter-
mined the head noun (see method below) of each
term and synonym in the ontology. We then pre-
pared a dictionary mapping from head nouns to lists
of ontologies in which those head nouns were found.
Head nouns were determined by two simple
heuristics (cf. (Collins, 1999)). For terms fitting the
pattern X of... (where of represents any preposi-
tion) the term X was taken as the head noun. For
all other terms, the rightmost word was taken as the
head noun. These two heuristics were applied in se-
quence when applicable, so that for example positive
regulation of growth (GO:0045927) becomes posi-
tive regulation by application of the first heuristic
and regulation by application of the second heuris-
tic. In the case of some ontologies, very limited pre-
39
processing was necessary?for example, it was nec-
essary to delete double quotes that appeared around
synonyms, and in some ontologies we had to delete
strings like [EXACT SYNONYM] from some terms
before extracting the head noun.
2.2.4 Stemming head nouns
In this technique, the headwords obtained by the
previous step were stemmed with the Porter stem-
mer.
2.3 Corpus and other materials
We made use of three sources in our evaluation.
One is the CRAFT (Colorado Richly Annotated Full
Text) corpus (Verspoor et al, 2009; Cohen et al,
2010a). This is a collection of 97 full-text journal
articles, comprising about 597,000 words, each of
which has been used as evidence for at least one an-
notation by the Mouse Genome Informatics group.
It has been annotated with a number of ontologies
and database identifiers, including:
? Gene Ontology
? Sequence Ontology
? Cell Type Ontology
? NCBI Taxonomy
? Chemical Entities of Biological Interest
(ChEBI)
In total, there are over 119,783 annotations. (For
the breakdown across semantic categories, see Ta-
ble 1.) All of these annotations were done by biolog-
ical scientists and have been double-annotated with
inter-annotator agreement in the nineties for most
categories.
The second source is the full sets of terms from
the twenty ontologies listed in the Introduction. All
of the twenty ontologies that we used were obtained
from the OBO portal. Version numbers are omitted
here due to space limitations, but are available from
the authors on request.
The third source is a structured test suite based on
the Gene Ontology (Cohen et al, 2010b). Structured
test suites are developed to test the performance
of a system on specific categories of input types.
This test set was especially designed to test diffi-
cult cases that do not correspond to exact matches
of Gene Ontology terms, as well as the full range of
types of terms. The test suite includes 300 concepts
from GO, as well as a number of transformations of
their terms, such as cells migrated derived from the
term cell migration and migration of cells derived
from cell migration, classified according to a num-
ber of linguistic attributes, such as length, whether
or not punctuation is included in the term, whether
or not it includes function (stop) words, etc. This
test suite determines at least one semantic category
that should be returned for each term. Unlike using
the entire ontologies, this evaluation method made
detailed error analysis possible. This test suite has
been used by other groups for broad characteriza-
tions of successes and failures of concept recogniz-
ers, and to tune the parameters of concept recogni-
tion systems.
2.4 Evaluation
We did three separate evaluations. In one, we com-
pared the output of our system against manually-
generated gold-standard annotations in the CRAFT
corpus (op. cit.). This was possible only for the on-
tologies that have been annotated in CRAFT, which
are listed above.
In the second evaluation, we used the entire on-
tologies themselves as inputs. In this method, all
responses should be the same?for example, every
term from the Gene Ontology should be classified
as belonging to the GO semantic class.
In the third, we utilized the structured test suite
described above.
2.4.1 Baselines
Two baselines are possible, but neither is optimal.
The first would be to use MetaMap (Aronson, 2001),
the industry standard for semantic category assign-
ment. (Note that MetaMap assigns specific cate-
gories, not broad ones.) However, MetaMap out-
puts only semantic classes that are elements of the
UMLS, which of the ontologies that we looked at,
includes only the Gene Ontology. The other is the
NCBO Annotator. The NCBO Annotator detects
only exact matches (or substring matches) to ontol-
ogy terms, so it is not clear that it is a strong enough
baseline to allow for a stringent analysis of our ap-
40
proach.
3 Results
We present our results in three sections:
? For the CRAFT corpus
? For the ontologies themselves
? For the Gene Ontology test suite
3.1 Corpus results
Table 1 (see next page) shows the results on the
CRAFT corpus if only the five ontologies that were
actually annotated in CRAFT are used as inputs.
The results are given for stemmed heads. Perfor-
mance on the four techniques that make up the ap-
proach is cumulative, and results for stemmed heads
reflects the application of all four techniques. In this
case, where we evaluate against the corpus, it is pos-
sible to determine false positives, so we can give
precision, recall, and F-measures for each semantic
class, as well as for the corpus as a whole. Micro-
averaged results were 67.06 precision, 78.49 recall,
and 72.32 F-measure. Macro-averaged results were
69.84 precision, 83.12 recall, and 75.31 F-measure.
Table 2 (see next page) shows the results for
the CRAFT corpus when all twenty ontologies are
matched against the corpus data, including the many
ontologies that are not annotated in the data. We
give results for just the five annotated ontologies
below. Rather than calculating precision, recall,
and F-measure, we calculate only accuracy. This
is because when classes other than the gold stan-
dard class is returned, we have no way of know-
ing if they are incorrect without manually examin-
ing them?that is, we have no way to identify false
positives. If the set of classes returned included the
gold standard class, a correct answer was counted. If
the classifier returned zero or more classes and none
of them was the gold standard, an incorrect answer
was counted. Results are given separately for each
of the four techniques. This allows us to evaluate
the contribution of each technique to the overall re-
sults; the value in each column is cumulative, so the
value for Stemmed head includes the contribution of
all four of the techniques that make up the general
approach. Accuracies of 77.12% to 95.73% were
achieved, depending on the ontology. We see that
the linguistic technique of locating the head noun
makes a contribution to all categories, but makes an
especially strong contribution to the Gene Ontology
and Cell Type Ontology classes. Stemming of head-
words is also effective for all five categories. We see
that exact match is effective only for those semantic
classes for which terminology is relatively fixed, i.e.
the NCBI taxonomy and chemical names. In some
of the others, matching natural language text is very
difficult by any technique. For example, of the 8,665
Sequence Ontology false negatives in the data re-
flected in the P/R/F values in Table 1, a full 2,050
are due to the single character +, which does not
appear in any of the twenty ontologies that we ex-
amined and that was marked by the annotators as a
Sequence Ontology term, wild type (SO:0000817).
3.2 Ontology results
As the second form of evaluation, we used the
terms from the ontologies themselves as the inputs
to which we attempted to assign a semantic class. In
this case, no annotation is required, and it is straight-
forwardly the case that each term in a given ontology
should be assigned the semantic class of that ontol-
ogy. We used only the head noun technique. We did
not use the exact match or stripping heuristics, since
they are guaranteed to return the correct answer, nor
did we use stemming. Thus, this section of the eval-
uation gives us a good indication of the performance
of the head noun approach.
As might be expected, almost all twenty on-
tologies returned results in the 97-100% correct
rate. However, we noted much lower performance
in two ontologies, the Sequence Ontology and the
Molecule Role Ontology. This lower performance
reflects a number of preprocessing errors or omis-
sions. The fact that we were able to detect these low-
performing ontologies indicates that our evaluation
technique in this experiment?trying to match terms
from an ontology against that ontology itself?is a
robust evaluation technique and should be used in
similar studies.
3.2.1 Structured test suite results
The third approach to evaluation involved use of
the structured test suite. The structured test suite re-
vealed a number of trends in the performance of the
system.
41
Ontology Annotations Precision Recall F-measure
Gene Ontology 39,626 66.31 73.06 69.52
Sequence Ontology 40,692 63.00 72.21 67.29
Cell Type Ontology 8,383 53.58 87.27 66.40
NCBI Taxonomy 11,775 96.24 92.51 94.34
ChEBI 19,307 70.07 90.53 79.00
Total (micro-averaged) 119,783 67.06 78.49 72.32
Total (macro-averaged) 69.84 83.12 75.31
Table 1: Results on the CRAFT corpus when only the CRAFT ontologies are used as input. Results are for stemmed
heads. Precision, recall, and F-measure are given for each semantic category in the corpus. Totals are micro-averaged
(over all tokens) and macro-averaged (over all categories), respectively. P/R/F are cumulative, so that the results for
stemmed heads reflect the application of all four techniques.
Ontology Exact Stripped Head noun Stemmed head
Gene Ontology 24.26 24.68 59.18 77.12
Sequence Ontology 44.28 47.63 56.63 73.33
Cell Type Ontology 25.26 25.80 70.09 88.38
NCBI Taxonomy 84.67 84.71 90.97 95.73
ChEBI 86.93 87.44 92.43 95.49
Table 2: Results on the CRAFT corpus when all twenty ontologies are used as input. Accuracy is given for each
technique. Accuracy is cumulative, so that accuracy in the final column reflects the application of all four techniques.
? The headword technique works very well for
recognizing syntactic variants. For example, if
the GO term induction of apoptosis is written
as apoptosis induction, the headword technique
allows it to be picked up.
? The headword technique works in situations
where text has been inserted into a term. For
example, if the GO term ensheathment of neu-
rons appears as ensheathment of some neu-
rons, the headword technique will allow it to be
picked up. If the GO term regulation of growth
shows up as regulation of vascular growth, the
headword technique will allow it to be picked
up.
? The headword stemming technique allows us to
pick up many verb phrases, which is important
for event detection and event coreference. For
example, if the GO term cell migration appears
in text as cells migrate, the technique will de-
tect it. The test suite also showed that failures
to recognize verb phrases still occur when the
morphological relationship between the nomi-
nal term and the verb are irregular, as for exam-
ple between the GO term growth and the verb
grows.
? The technique?s ability to handle coordination
is very dependent on the type of coordination.
For example, simple coordination (e.g. cell mi-
gration and proliferation) is handled well, but
complex coordination (e.g. cell migration, pro-
liferation and adhesion) is handled poorly.
? Stemming is necessary for recognition of plu-
rals, regardless of the length of the term in
words.
? The approach currently fails on irregular plu-
rals, due to failure of the Porter stemmer to han-
dle plurals like nuclei and nucleoli well.
? The approach handles classification of terms
that others have characterized as ?ungram-
matical,? such as transposition, DNA-mediated
(GO:0006313). This is important, because ex-
act matches will always fail on these terms.
42
4 Discussion
4.1 Related work
We are not aware of similar work that tries to assign
a large set of broad semantic categories to individ-
ual text strings. There is a body of work on selecting
a single ontology for a domain or text. (Mart??nez-
Romero et al, 2010) proposes a method for selecting
an ontology given a list of terms, all of which must
appear in the ontology. (Jonquet et al, 2009) de-
scribes an ontology recommender that first annotates
terms in a text with the Open Biomedical Annotator
service, then uses the sum of the scores of the indi-
vidual annotations to recommend a single ontology
for the domain as a whole.
4.2 Possible alternate approaches
Three possible alternative approaches exist, all of
which would have as their goal the returning of a sin-
gle best semantic class for every input. However, for
the use cases that we have identified?coreference
resolution, document classification, information ex-
traction, and curator assistance?we are more inter-
ested in wide coverage of a broad range of semantic
classes, so these approaches are not evaluated here.
However, we describe them for completeness and
for the use of researchers who might be interested
in pursuing single-class assignment.
4.2.1 Frequent words
One alternative approach would be to use simple
word frequencies. For example, for each ontology,
one could determine the N most frequent words, fil-
tering out stop words. At run time, check the words
in each noun phrase in the text against the lists of fre-
quent words. For every word from the text that ap-
peared in the list of frequent words from some ontol-
ogy, assign a score to each ontology in which it was
found, weighting it according to its position in the
list of frequent words. In theory, this could accom-
modate for the non-uniqueness of word-to-ontology
mappings, i.e. the fact that a single word might ap-
pear in the lists for multiple ontologies. However,
we found the technique to perform very poorly for
differentiating between ontologies and do not rec-
ommend it.
4.2.2 Measuring informativeness
If the system is desired to return only one sin-
gle semantic class per text string, then one approach
would be to determine the informativeness of each
word in each ontology. That is, we want to find the
maximal probability of an ontology given a word
from that ontology. This approach is very difficult
to normalize for the wide variability in size of the
many ontologies that we wanted to be able to deal
with.
4.2.3 Combining scores
Finally, one could conceivably combine scores for
matches obtained by the different strategies, weight-
ing them according to their stringency, i.e. exact
match receiving a higher weight than head noun
match, which in turn would receive a higher weight
than stemmed head noun match. This weighting
might also include informativeness, as described
above.
4.3 Why the linguistic method works
As pointed out above, the lightweight linguistic
method makes a large contribution to the perfor-
mance of the approach for some ontologies, partic-
ularly those for which the exact match and stripping
techniques do not perform well. It works for two
reasons, one related to the approach itself and one
related to the nature of the OBO ontologies. From
a methodological perspective, the approach is effec-
tive because headwords are a good reflection of the
semantic content of the noun phrase and they are
relatively easy to access via simple heuristics. Of
course simple heuristics will fail, as we can observe
most obviously in the cases where we failed to iden-
tify members of the ontologies in the second eval-
uation step. However, overall the approach works
well enough to constitute a viable tool for coref-
erence systems and other applications that benefit
from the ability to assign broad semantic classes to
text strings.
The approach is also able to succeed because of
the nature of the OBO ontologies. OBO ontologies
are meant to be orthogonal (Smith et al, 2007). A
distributional analysis of the distribution of terms
and words between the ontologies (data not shown
here, although some of it is discussed below), as well
as the false positives found in the corpus study, sug-
43
gests that orthogonality between the OBO ontolo-
gies is by no means complete. However, it holds
often enough for the headword method to be effec-
tive.
4.4 Additional error analysis
In the section on the results for the structured test
suite, we give a number of observations on contribu-
tions to errors, primarily related either to the char-
acteristics of individual words or to particular syn-
tactic instantiations of terms. Here, we discuss some
aspects of the distribution of lexical items and of the
corpus that contributed to errors.
? The ten most common headwords appear in
from 6-16 of the twenty ontologies. However,
they typically appear in one ontology at a fre-
quency many orders of magnitude greater than
their frequency in the other ontologies. Taking
this frequency data into account for just these
ten headwords would likely decrease false pos-
itives quite significantly.
? More than 50% of Gene Ontology terms share
one of only ten headwords. Many of our Gene
Ontology false negatives on the corpus are be-
cause the annotated text string does not contain
a word such as process or complex that is the
head word of the canonical term.
4.5 Future work
The heuristics that we implemented for extracting
headwords from OBO terms were very simple, in
keeping with our initial goal of developing an easy,
fast method for semantic class assignment. How-
ever, it is clear that we could achieve substantial per-
formance improvements from improving the heuris-
tics. We may pursue this track, if it becomes clear
that coreference performance would benefit from
this when we incorporate the semantic classification
approach into a coreference system.
On acceptance of the paper, we will make Perl and
Java versions of the semantic class assigner publicly
available on SourceForge.
4.6 Conclusion
The goal of this paper was to develop a simple ap-
proach to assigning text strings to an unprecedent-
edly large range of semantic classes, where mem-
bership in a semantic class is equated with belonging
to the semantic domain of a specific ontology. The
approach described in this paper is able to do that
at a micro-averaged F-measure of 72.32 and macro-
averaged F-measure of 75.31 as evaluated on a man-
ually annotated corpus where false positives can be
determined, and with an accuracy of 77.12-95.73%
when only true positives and false negatives can be
determined.
References
A. Aronson. 2001. Effective mapping of biomedical text
to the UMLS Metathesaurus: The MetaMap program.
In Proc AMIA 2001, pages 17?21.
William A. Baumgartner Jr., Zhiyong Lu, Helen L. John-
son, J. Gregory Caporaso, Jesse Paquette, Anna Linde-
mann, Elizabeth K. White, Olga Medvedeva, K. Bre-
tonnel Cohen, and Lawrence Hunter. 2008. Concept
recognition for extracting protein interaction relations
from biomedical text. Genome Biology, 9.
Christian Blaschke, Eduardo A. Leon, Martin Krallinger,
and Alfonso Valencia. 2005. Evaluation of BioCre-
ative assessment of task 2. BMC Bioinformatics, 6
Suppl 1.
J. Gregory Caporaso, William A. Baumgartner Jr..,
K. Bretonnel Cohen, Helen L. Johnson, Jesse Paque-
tte, and Lawrence Hunter. 2005. Concept recognition
and the TREC Genomics tasks. In The Fourteenth Text
REtrieval Conference (TREC 2005) Proceedings.
Nancy A. Chinchor. 1998. Overview of MUC-7/MET-2.
K. Bretonnel Cohen, Helen L. Johnson, Karin Verspoor,
Christophe Roeder, and Lawrence E. Hunter. 2010a.
The structural and content aspects of abstracts versus
bodies of full text journal articles are different. BMC
Bioinformatics, 11(492).
K. Bretonnel Cohen, Christophe Roeder, William
A. Baumgartner Jr., Lawrence Hunter, and Karin Ver-
spoor. 2010b. Test suite design for biomedical ontol-
ogy concept recognition systems. In Proceedings of
the Language Resources and Evaluation Conference.
Michael Collins. 1999. Head-driven statistical models
for natural language parsing. Ph.D. thesis, University
of Pennsylvania.
N. Davis, H. Harkema, R. Gaizauskas, Y. K. Guo,
M. Ghanem, T. Barnwell, Y. Guo, and J. Ratcliffe.
2006. Three approaches to GO-tagging biomedical
abstracts. In Proceedings of the Second International
Symposium on Semantic Mining in Biomedicine, pages
21?28, Jena, Germany.
Jerry R. Hobbs. 1978. Resolving pronoun references.
Lingua, 44:311?338.
44
C. Jonquet, N.H. Shah, and M.A. Musen. 2009. Pro-
totyping a biomedical ontology recommender ser-
vice. In Bio-Ontologies: Knowledge in Biology,
ISMB/ECCB SIG.
Jin-Dong Kim, Tomoko Ohta, Yoshimasa Tsuruoka,
Yuka Tateisi, and Nigel Collier. 2004. Introduction
to the bio-entity recognition task at JNLPBA. In Pro-
ceedings of the international joint workshop on natu-
ral language processing in biomedicine and its appli-
cations, pages 70?75.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview
of BioNLP?09 shared task on event extraction. In
BioNLP 2009 Companion Volume: Shared Task on En-
tity Extraction, pages 1?9.
Robert Leaman and Graciela Gonzalez. 2008. BAN-
NER: An executable survey of advances in biomedical
named entity recognition. In Pac Symp Biocomput.
Marcos Mart??nez-Romero, Jose? Va?zquez-Naya, Cris-
tian R. Munteanu, Javier Pereira, and Alejandro Pazos.
2010. An approach for the automatic recommendation
of ontologies using collaborative knowledge. In KES
2010, Part II, LNAI 6277, pages 74?81.
Nigam H. Shah, Nipun Bhatia, Clement Jonquet, Daniel
Rubin, Annie P. Chiang, and Mark A. Musen. 2009.
Comparison of concept recognizers for building the
Open Biomedical Annotator. BMC Bioinformatics,
10.
Barry Smith, Michael Ashburner, Cornelius Rosse,
Jonathan Bard, William Bug, Werner Ceusters,
Louis J. Goldberg, Karen Eilbeck, Amelia Ireland,
Christopher J. Mungall, The OBI Consortium, Neo-
cles Leontis, Philippe Rocca-Serra, Alan Ruttenberg,
Susanna-Assunta Sansone, Richard H. Scheuermann,
Nigam Shah, Patricia L. Whetzel, and Suzanna Lewis.
2007. The OBO Foundry: coordinated evolution of
ontologies to support biomedical data integration. Na-
ture Biotechnology, 25:1251?1255.
Larry Smith, Lorraine Tanabe, Rie Johnson nee Ando,
Cheng-Ju Kuo, I-Fang Chung, Chun-Nan Hsu, Yu-
Shi Lin, Roman Klinger, Christof Friedrich, Kuzman
Ganchev, Manabu Torii, Hongfang Liu, Barry Had-
dow, Craig Struble, Richard Povinelli, Andreas Vla-
chos, William Baumgartner, Jr., Lawrence Hunter,
Bob Carpenter, Richard Tzong-Han Tsai, Hong-
Jie Dai, Feng Liu, Yifei Chen, Chengjie Sun,
Sophia Katrenko, Pieter Adriaans, Christian Blaschke,
Rafael Torres Perez, Mariana Neves, Preslav Nakov,
Anna Divoli, Manuel Mana, Jacinto Mata-Vazquez,
and W. John Wilbur. 2008. Overview of BioCreative
II gene mention recognition. Genome Biology.
E. Stoica and M. Hearst. 2006. Predicting gene functions
from text using a cross-species approach. In Proceed-
ings of the 11th Pacific Symposium on Biocomputing.
Karin Verspoor, K. Bretonnel Cohen, and Lawrence
Hunter. 2009. The textual characteristics of traditional
and Open Access scientific journals are similar. BMC
Bioinformatics, 10.
A. Yeh, A. Morgan, M. Colosimo, and L. Hirschman.
2005. BioCreatve task 1A: gene mention finding eval-
uation. BMC Bioinformatics, 6(Suppl. 1).
45
