Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 244?253,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Incorporating Temporal and Semantic Information with Eye Gaze for
Automatic Word Acquisition in Multimodal Conversational Systems
Shaolin Qu Joyce Y. Chai
Department of Computer Science and Engineering
Michigan State University
East Lansing, MI 48824
{qushaoli,jchai}@cse.msu.edu
Abstract
One major bottleneck in conversational sys-
tems is their incapability in interpreting un-
expected user language inputs such as out-of-
vocabulary words. To overcome this problem,
conversational systems must be able to learn
new words automatically during human ma-
chine conversation. Motivated by psycholin-
guistic findings on eye gaze and human lan-
guage processing, we are developing tech-
niques to incorporate human eye gaze for au-
tomatic word acquisition in multimodal con-
versational systems. This paper investigates
the use of temporal alignment between speech
and eye gaze and the use of domain knowl-
edge in word acquisition. Our experiment re-
sults indicate that eye gaze provides a poten-
tial channel for automatically acquiring new
words. The use of extra temporal and domain
knowledge can significantly improve acquisi-
tion performance.
1 Introduction
Interpreting human language is a challenging prob-
lem in human machine conversational systems due
to the flexibility of human language behavior. When
the encountered vocabulary is outside of the sys-
tem?s knowledge, conversational systems tend to
fail. It is desirable that conversational systems can
learn new words automatically during human ma-
chine conversation. While automatic word acquisi-
tion in general is quite challenging, multimodal con-
versational systems offer an unique opportunity to
explore word acquisition. In a multimodal conversa-
tional system where users can talk and interact with
a graphical display, users? eye gaze, which occurs
naturally with speech production, provides a poten-
tial channel for the system to learn new words auto-
matically during human machine conversation.
Psycholinguistic studies have shown that eye gaze
is tightly linked to human language processing. Eye
gaze is one of the reliable indicators of what a per-
son is ?thinking about? (Henderson and Ferreira,
2004). The direction of eye gaze carries informa-
tion about the focus of the user?s attention (Just and
Carpenter, 1976). The perceived visual context in-
fluences spoken word recognition and mediates syn-
tactic processing of spoken sentences (Tanenhaus et
al., 1995). In addition, directly before speaking a
word, the eyes move to the mentioned object (Grif-
fin and Bock, 2000).
Motivated by these psycholinguistic findings, we
are investigating the use of eye gaze for automatic
word acquisition in multimodal conversation. Par-
ticulary, this paper investigates the use of tempo-
ral information about speech and eye gaze and do-
main semantic relatedness for automatic word ac-
quisition. The domain semantic and temporal in-
formation are incorporated in statistical translation
models for word acquisition. Our experiments show
that the use of domain semantic and temporal infor-
mation significantly improves word acquisition per-
formance.
In the following sections, we first describe the ba-
sic translation models for word acquisition. Then,
we describe the enhanced models that incorporate
temporal and semantic information about speech
and eye gaze for word acquisition. Finally, we
present the results of empirical evaluation.
244
(a) Raw gaze points (b) Processed gaze fixations
Figure 1: Domain scene with a user?s gaze fixations
2 Related Work
Word acquisition by grounding words to visual en-
tities has been studied in many language ground-
ing systems. For example, given speech paired with
video images of single objects, mutual information
between audio and visual signals was used to acquire
words by associating acoustic phone sequences with
the visual prototypes (e.g., color, size, shape) of ob-
jects (Roy and Pentland, 2002). Generative mod-
els were used to acquire words by associating words
with image regions given parallel data of pictures
and description text (Barnard et al, 2003). Differ-
ent from these works, in our work, the visual atten-
tion foci accompanying speech are indicated by eye
gaze. Eye gaze is an implicit and subconscious in-
put, which brings additional challenges in word ac-
quisition.
Eye gaze has been explored for word acquisition
in previous work. In (Yu and Ballard, 2004), given
speech paired with eye gaze information and video
images, a translation model was used to acquire
words by associating acoustic phone sequences with
visual representations of objects and actions. A re-
cent investigation on word acquisition from tran-
scribed speech and eye gaze in human machine con-
versation was reported in (Liu et al, 2007). In this
work, a translation model was developed to asso-
ciate words with visual objects on a graphical dis-
play. Different from these previous works, here
we investigate the incorporation of extra knowledge,
specifically speech-gaze temporal information and
domain knowledge, with eye gaze to facilitate word
acquisition.
3 Data Collection
We recruited users to interact with a simplified mul-
timodal conversational system to collect speech and
eye gaze data.
3.1 Domain
We are working on a 3D room decoration domain.
Figure 1 shows the 3D room scene that was shown
to the user in the experiments. There are 28 3D
objects (bed, chairs, paintings, lamp, etc.) in the
room scene. During the human machine conversa-
tion, the system verbally asked the user a question
(e.g., ?what do you dislike about the arrangement
of the room??) or issued a request (e.g., ?describe
the left wall?) about the room. The user provided
responses by speaking to the system.
During the experiments, users? speech was
recorded through an open microphone and users?
eye gaze was captured by an Eye Link II eye tracker.
Eye gaze data consists of the screen coordinates of
each gaze point that was captured by the eye tracker
at a sampling rate of 250hz.
3.2 Data Preprocessing
As for speech data, we collected 357 spoken utter-
ances from 7 users? experiments. The vocabulary
size is 480, among which 227 words are nouns and
adjectives. We manually transcribed the collected
speech.
As for gaze data, the first step is to identify gaze
fixation from raw gaze points. As shown in Fig-
ure 1(a), the collected raw gaze points are very noisy.
They can not be used directly for identifying gaze
fixated entities in the scene. We processed the raw
245
gaze data to eliminate invalid and saccadic gaze
points. Invalid gaze points occur when users look
off the screen. Saccadic gaze points occur during
ballistic eye movements between gaze fixations. Vi-
sion studies have shown that no visual processing
occurs in the human mind during saccades (i.e., sac-
cadic suppression) (Matin, 1974). Since eyes do not
stay still but rather make small, frequent jerky move-
ments, we average nearby gaze points to better iden-
tify gaze fixations. The processed eye gaze fixations
are shown in Figure 1(b).
1668 2096 32522692
[19] [22] [ ] [10]
[11]
[10]
[11]
[10]
[11]
This room has a chandelier
2572 2872 3170 3528 3736
speech stream
gaze stream
(ms)
(ms)
[fixated entity ID]
ts te
f: gaze fixation
( [19] ? bed_frame; [22] ? door; [10] ? bedroom; [11] ? chandelier )
Figure 2: Parallel speech and gaze streams
Figure 2 shows an excerpt of the collected speech
and gaze fixation in one experiment. In the speech
stream, each word starts at a particular timestamp.
In the gaze stream, each gaze fixation has a start-
ing timestamp ts and an ending timestamp te. Each
gaze fixation also has a list of fixated entities (3D ob-
jects). An entity e on the graphical display is fixated
by gaze fixation f if the area of e contains fixation
point of f .
Given the collected speech and gaze fixations, we
build parallel speech-gaze data set as follows. For
each spoken utterance and its accompanying gaze
fixations, we construct a pair of word sequence and
entity sequence (w, e). The word sequence w con-
sists of only nouns and adjectives in the utterance.
Each gaze fixation results in a fixated entity in the
entity sequence e. When multiple entities are fix-
ated by one gaze fixation due to the overlapping of
the entities, the forefront one is chosen. Also, we
merge the neighboring gaze fixations that contain
the same fixated entities. For the parallel speech and
gaze streams shown in Figure 2, the resulting word
sequence is w = [room chandelier] and the entity
sequence is e = [bed frame door chandelier].
4 Translation Models for Automatic Word
Acquisition
Since we are working on conversational systems
where users interact with a visual scene, we consider
the task of word acquisition as associating words
with visual entities in the domain. Given the par-
allel speech and gaze fixated entities {(w, e)}, we
formulate word acquisition as a translation problem
and use translation models to estimate word-entity
association probabilities p(w|e). The words with the
highest association probabilities are chosen as ac-
quired words for entity e.
4.1 Base Model I
Using the translation model I (Brown et al, 1993),
where each word is equally likely to be aligned with
each entity, we have
p(w|e) =
1
(l + 1)m
m?
j=1
l?
i=0
p(wj |ei) (1)
where l and m are the lengths of entity and word
sequences respectively. This is the model used in
(Liu et al, 2007) and (Yu and Ballard, 2004). We
refer to this model as Model-1 throughout the rest
of this paper.
4.2 Base Model II
Using the translation model II (Brown et al, 1993),
where alignments are dependent on word/entity po-
sitions and word/entity sequence lengths, we have
p(w|e) =
m?
j=1
l?
i=0
p(aj = i|j,m, l)p(wj |ei) (2)
where aj = i means that wj is aligned with ei.
When aj = 0, wj is not aligned with any entity (e0
represents a null entity). We refer to this model as
Model-2.
Compared to Model-1, Model-2 considers the or-
dering of words and entities in word acquisition.
EM algorithms are used to estimate the probabilities
p(w|e) in the translation models.
5 Using Speech-Gaze Temporal
Information for Word Acquisition
In Model-2, word-entity alignments are estimated
from co-occurring word and entity sequences in an
246
unsupervised way. The estimated alignments are de-
pendent on where the words/entities appear in the
word/entity sequences, not on when those words and
gaze fixated entities actually occur. Motivated by the
finding that users move their eyes to the mentioned
object directly before speaking a word (Griffin and
Bock, 2000), we make the word-entity alignments
dependent on their temporal relation in a new model
(referred as Model-2t):
p(w|e) =
m?
j=1
l?
i=0
pt(aj = i|j, e,w)p(wj |ei) (3)
where pt(aj = i|j, e,w) is the temporal alignment
probability computed based on the temporal dis-
tance between entity ei and word wj .
We define the temporal distance between ei and
wj as
d(ei, wj) =
?
??
??
0 ts(ei) ? ts(wj) ? te(ei)
te(ei) ? ts(wj) ts(wj) > te(ei)
ts(ei) ? ts(wj) ts(wj) < ts(ei)
(4)
where ts(wj) is the starting timestamp (ms) of word
wj , ts(ei) and te(ei) are the starting and ending
timestamps (ms) of gaze fixation on entity ei.
The alignment of word wj and entity ei is de-
cided by their temporal distance d(ei, wj). Based
on the psycholinguistic finding that eye gaze hap-
pens before a spoken word, wj is not allowed to
be aligned with ei when wj happens earlier than ei
(i.e., d(ei, wj) > 0). When wj happens no earlier
than ei (i.e., d(ei, wj) ? 0), the closer they are, the
more likely they are aligned. Specifically, the tem-
poral alignment probability of wj and ei in each co-
occurring instance (w, e) is computed as
pt(aj = i|j, e,w) =
{
0 d(ei, wj) > 0
exp[??d(ei,wj)]
?i exp[??d(ei,wj)]
d(ei, wj) ? 0
(5)
where ? is a constant for scaling d(ei, wj). In our
experiments, ? is set to 0.005.
An EM algorithm is used to estimate probabilities
p(w|e) in Model-2t.
?5000 ?4000 ?3000 ?2000 ?1000 0 10000
20
40
60
80
100
120
140
temporal distance of aligned word and entity (ms)
alig
nm
ent
 cou
nt
Figure 3: Histogram of truly aligned word and entity
pairs over temporal distance (bin width = 200ms)
For the purpose of evaluation, we manually anno-
tated the truly aligned word and entity pairs. Fig-
ure 3 shows the histogram of those truly aligned
word and entity pairs over the temporal distance of
aligned word and entity. We can observe in the fig-
ure that 1) almost no eye gaze happens after a spo-
ken word, and 2) the number of word-entity pairs
with closer temporal distance is generally larger than
the number of those with farther temporal distance.
This is consistent with our modeling of the tempo-
ral alignment probability of word and entity (Equa-
tion (5)).
6 Using Domain Semantic Relatedness for
Word Acquisition
Speech-gaze temporal alignment and occurrence
statistics sometimes are not sufficient to associate
words to an entity correctly. For example, suppose
a user says ?there is a lamp on the dresser? while
looking at a lamp object on a table object. Due
to their co-occurring with the lamp object, words
dresser and lamp are both likely to be associated
with the lamp object in the translation models. As
a result, word dresser is likely to be incorrectly ac-
quired for the lamp object. For the same reason, the
word lamp could be acquired incorrectly for the ta-
ble object. To solve this type of association prob-
lem, the semantic knowledge about the domain and
words can be helpful. For example, the knowledge
that the word lamp is more semantically related to
the object lamp can help the system avoid associat-
247
ing the word dresser to the lamp object. Therefore,
we are interested in investigating the use of semantic
knowledge in word acquisition.
On one hand, each conversational system has a
domain model, which is the knowledge representa-
tion about its domain such as the types of objects
and their properties and relations. On the other hand,
there are available resources about domain indepen-
dent lexical knowledge (e.g., WordNet (Fellbaum,
1998)). The question is whether we can utilize the
domain model and external lexical knowledge re-
source to improve word acquisition. To address this
question, we link the domain concepts in the domain
model with WordNet concepts, and define semantic
relatedness of word and entity to help the system ac-
quire domain semantically compatible words.
In the following sections, we first describe our
domain modeling, then define the semantic related-
ness of word and entity based on domain modeling
and WordNet semantic lexicon, and finally describe
different ways of using the semantic relatedness of
word and entity to help word acquisition.
6.1 Domain Modeling
We model the 3D room decoration domain as shown
in Figure 4. The domain model contains all do-
main related semantic concepts. These concepts are
linked to the WordNet concepts (i.e., synsets in the
format of ?word#part-of-speech#sense-id?). Each of
the entities in the domain has one or more properties
(e.g., semantic type, color, size) that are denoted by
domain concepts. For example, the entity dresser 1
has domain concepts SEM DRESSER and COLOR.
These domain concepts are linked to ?dresser#n#4?
and ?color#n#1? in WordNet.
Note that in the domain model, the domain con-
cepts are not specific to a certain entity, they are gen-
eral concepts for a certain type of entity. Multiple
entities of the same type have the same properties
and share the same set of domain concepts.
6.2 Semantic Relatedness of Word and Entity
We compute the semantic relatedness of a word w
and an entity e based on the semantic similarity be-
tween w and the properties of e. Specifically, se-
mantic relatedness SR(e, w) is defined as
SR(e, w) = max
i,j
sim(s(cie), sj(w)) (6)
?bed#n#1?
?picture#n#2? ?size#n#1?
?color#n#1?
?dresser#n#4?
COLOR
bed_framedresser_1
SIZESEM_DRESSER SEM_BED COLOR
Entities:
Domain 
concepts:
WordNet 
concepts:
Dom ain Model
Figure 4: Domain model with domain concepts linked to
WordNet synsets
where cie is the i-th property of entity e, s(c
i
e) is the
synset of property cie as designed in domain model,
sj(w) is the j-th synset of word w as defined in
WordNet, and sim(?, ?) is the similarity score of two
synsets.
We computed the similarity score of two synsets
based on the path length between them. The similar-
ity score is inversely proportional to the number of
nodes along the shortest path between the synsets as
defined in WordNet. When the two synsets are the
same, they have the maximal similarity score of 1.
The WordNet-Similarity tool (Pedersen et al, 2004)
was used for the synset similarity computation.
6.3 Word Acquisition with Word-Entity
Semantic Relatedness
We can use the semantic relatedness of word and
entity to help the system acquire semantically com-
patible words for each entity, and therefore improve
word acquisition performance. The semantic relat-
edness can be applied for word acquisition in two
ways: post process learned word-entity association
probabilities by rescoring them with semantic relat-
edness, or directly affect the learning of word-entity
associations by constraining the alignment of word
and entity in the translation models.
6.3.1 Rescoring with semantic relatedness
In the acquired word list for an entity ei, each
word wj has an association probability p(wj |ei) that
is learned from a translation model. We use the
248
semantic relatedness SR(ei, wj) to redistribute the
probability mass for each wj . The new association
probability is given by:
p?(wj |ei) =
p(wj |ei)SR(ei, wj)
?
j p(wj |ei)SR(ei, wj)
(7)
6.3.2 Semantic alignment constraint in
translation model
When used to constrain the word-entity alignment
in the translation model, semantic relatedness can be
used alone or used together with speech-gaze tempo-
ral information to decide the alignment probability
of word and entity.
? Using only semantic relatedness to constrain
word-entity alignments in Model-2s, we have
p(w|e) =
m?
j=1
l?
i=0
ps(aj = i|j, e,w)p(wj |ei)
(8)
where ps(aj = i|j, e,w) is the alignment prob-
ability based on semantic relatedness,
ps(aj = i|j, e,w) =
SR(ei, wj)
?
i SR(ei, wj)
(9)
? Using semantic relatedness and temporal infor-
mation to constrain word-entity alignments in
Model-2ts, we have
p(w|e) =
m?
j=1
l?
i=0
pts(aj = i|j, e,w)p(wj |ei)
(10)
where pts(aj = i|j, e,w) is the alignment
probability that is decided by both temporal re-
lation and semantic relatedness of ei and wj ,
pts(aj = i|j, e,w) =
ps(aj = i|j, e,w)pt(aj = i|j, e,w)
?
i ps(aj = i|j, e,w)pt(aj = i|j, e,w)
(11)
where ps(aj = i|j, e,w) is the semantic align-
ment probability in Equation (9), and pt(aj =
i|j, e,w) is the temporal alignment probability
given in Equation (5).
EM algorithms are used to estimate p(w|e) in
Model-2s and Model-2ts.
7 Grounding Words to Domain Concepts
As discussed above, based on translation models, we
can incorporate temporal and domain semantic in-
formation to obtain p(w|e). This probability only
provides a means to ground words to entities. In
conversational systems, the ultimate goal of word
acquisition is to make the system understand the se-
mantic meaning of new words. Word acquisition by
grounding words to objects is not always sufficient
for identifying their semantic meanings. Suppose
the word green is grounded to a green chair object,
so is the word chair. Although the system is aware
that green is some word describing the green chair,
it does not know that word green refers to the chair?s
color while the word chair refers to the chair?s se-
mantic type. Thus, after learning the word-entity as-
sociations p(w|e) by the translation models, we need
to further ground words to domain concepts of entity
properties.
We further apply WordNet to ground words to do-
main concepts. For each entity e, based on asso-
ciation probabilities p(w|e), we can choose the n-
best words as acquired words for e. Those n-best
words have the n highest association probabilities.
For each word w acquired for e, the grounded con-
cept c?e forw is chosen as the one that has the highest
semantic relatedness with w:
c?e = argmax
i
[
max
j
sim(s(cie), sj(w))
]
(12)
where sim(s(cie), sj(w)) is the semantic similarity
score defined in Equation (6).
8 Evaluation
We evaluate word acquisition performance of differ-
ent models on the data collected from our user stud-
ies (see Section 3).
8.1 Evaluation Metrics
The following metrics are used to evaluate the words
acquired for domain concepts (i.e., entity properties)
{cie}.
? Precision
?
e
?
i # words correctly acquired for c
i
e?
e
?
i # words acquired for c
i
e
249
? Recall
?
e
?
i # words correctly acquired for c
i
e
?
e
?
i # ground-truth
1 words of cie
? F-measure
2 ? precision ? recall
precision + recall
The metrics of precision, recall, and F-measure
are based on the n-best words acquired for the entity
properties. Therefore, we have different precision,
recall, and F-measure when n changes.
The metrics of precision, recall, and F-measure
only provide evaluation on the top n candidate
words. To measure the acquisition performance on
the entire ranked list of candidate words, we define
a new metric as follows:
? Mean Reciprocal Rank Rate (MRRR)
MRRR =
?
e
?Nei=1
1
index(wie)
?Nei=1
1
i
#e
where Ne is the number of all ground-truth
words {wie} for entity e, index(w
i
e) is the in-
dex of word wie in the ranked list of candidate
words for entity e.
Entities may have a different number of ground-
truth words. For each entity e, we calculate a Recip-
rocal Rank Rate (RRR), which measures how close
the ranks of the ground-truth words in the candidate
word list is to the best scenario where the top Ne
words are the ground-truth words for e. RRR is in
the range of (0, 1]. The higher the RRR, the better
is the word acquisition performance. The average of
RRRs across all entities gives the Mean Reciprocal
Rank Rate (MRRR).
Note that MRRR is directly based on the learned
word-entity associations p(w|e), it is in fact a mea-
sure of grounding words to entities.
1The ground-truth words were compiled and agreed upon by
two human judges.
8.2 Evaluation Results
To compare the effects of different speech-gaze
alignments on word acquisition, we evaluate the fol-
lowing models:
? Model-1 ? base model I without word-entity
alignment (Equation (1)).
? Model-2 ? base model II with positional align-
ment (Equation (2)).
? Model-2t ? enhanced model with temporal
alignment (Equation (3)).
? Model-2s ? enhanced model with semantic
alignment (Equation (8)).
? Model-2ts ? enhanced model with both tempo-
ral and semantic alignment (Equation (10)).
To compare the different ways of incorporating
semantic relatedness in word acquisition as dis-
cussed in Section 6.3.1, we also evaluate the follow-
ing models:
? Model-1-r ?Model-1 with semantic relatedness
rescoring of word-entity association.
? Model-2t-r ? Model-2t with semantic related-
ness rescoring of word-entity association.
Figure 5 shows the results of models with differ-
ent speech-gaze alignments. Figure 6 shows the re-
sults of models with semantic relatedness rescoring.
In Figure 5 & 6, n-best means the top n word candi-
dates are chosen as acquired words for each entity.
The Mean Reciprocal Rank Rates of all models are
compared in Figure 7.
8.2.1 Results of using different speech-gaze
alignments
As shown in Figure 5, Model-2 does not show a
consistent improvement compared to Model-1 when
a different number of n-best words are chosen as ac-
quired words. This result shows that it is not very
helpful to consider the index-based positional align-
ment of word and entity for word acquisition.
Figure 5 also shows that models considering
temporal or/and semantic information (Model-2t,
Model-2s, Model-2ts) consistently perform better
than the models considering neither temporal nor
250
1 2 3 4 5 6 7 8 9 100.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
n?best
prec
ision
Model?1Model?2Model?2tModel?2sModel?2ts
(a) precision
1 2 3 4 5 6 7 8 9 100.1
0.2
0.3
0.4
0.5
0.6
n?best
rec
all
Model?1Model?2Model?2tModel?2sModel?2ts
(b) recall
1 2 3 4 5 6 7 8 9 100.2
0.25
0.3
0.35
0.4
0.45
0.5
0.55
n?best
F?m
easu
re
Model?1Model?2Model?2tModel?2sModel?2ts
(c) F-measure
Figure 5: Performance of word acquisition when different types of speech-gaze alignment are applied
1 2 3 4 5 6 7 8 9 100.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
n?best
prec
ision
Model?1Model?2tModel?1?rModel?2t?r
(a) precision
1 2 3 4 5 6 7 8 9 100.1
0.2
0.3
0.4
0.5
0.6
n?best
rec
all
Model?1Model?2tModel?1?rModel?2t?r
(b) recall
1 2 3 4 5 6 7 8 9 100.2
0.25
0.3
0.35
0.4
0.45
0.5
0.55
n?best
F?m
easu
re
Model?1Model?2tModel?1?rModel?2t?r
(c) F-measure
Figure 6: Performance of word acquisition when semantic relatedness rescoring of word-entity association is applied
M?1 M?2 M?2t M?2s M?2ts M?1?r M?2t?r0.5
0.55
0.6
0.65
0.7
0.75
0.8
Models
Mea
n R
ecip
roca
l Ra
nk R
ate
Figure 7: MRRRs achieved by different models
semantic information (Model-1, Model-2). Among
Model-2t, Model-2s, and Model-2ts, it is found that
they do not make consistent differences.
As shown in Figure 7, the MRRRs of different
models are consistent with their performances on F-
measure. A t-test has shown that the difference be-
tween the MRRRs of Model-1 and Model-2 is not
statistically significant. Compared to Model-1, t-
tests have confirmed that MRRR is significantly im-
proved by Model-2t (t = 2.27, p < 0.02), Model-2s
(t = 3.40, p < 0.01), and Model-2ts(t = 2.60, p <
0.01). T-tests have shown no significant differences
among Model-2t, Model-2s, and Model-2ts.
8.2.2 Results of applying semantic relatedness
rescoring
Figure 6 shows that semantic relatedness rescor-
ing improves word acquisition. After semantic re-
latedness rescoring of the word-entity associations
learned by Model-1, Model-1-r improves the F-
measure consistently when a different number of
n-best words are chosen as acquired words. Com-
pared to Model-2t, Model-2t-r also improves the F-
measure consistently.
Comparing the two ways of using semantic relat-
edness for word acquisition, it is found that rescor-
ing word-entity association with semantic related-
ness works better. When semantic relatedness is
used together with temporal information to constrain
word-entity alignments in Model-2ts, word acqui-
251
Model Rank 1 Rank 2 Rank 3 Rank 4 Rank 5
M-1 table(0.173) dresser(0.067) area(0.058) picture(0.053) dressing(0.041)
M-2t table(0.146) dresser(0.125) dressing(0.061) vanity(0.051) fact(0.050)
M-2t-r table(0.312) dresser(0.241) vanity(0.149) desk(0.047) area(0.026)
Table 1: N-best candidate words acquired for the entity dresser 1 by different models
sition performance is not improved compared to
Model-2t. However, using semantic relatedness to
rescore word-entity association learned by Model-
2t, Model-2t-r further improves word acquisition.
As shown in Figure 7, the MRRRs of Model-
1-r and Model-2t-r are consistent with their per-
formances on F-measure. Compared to Model-2t,
Model-2t-r improves MRRR. A t-test has confirmed
that this is a significant improvement (t = 1.97, p <
0.03). Compared to Model-1, Model-1-r signifi-
cantly improves MRRR (t = 2.33, p < 0.02). There
is no significant difference between Model-1-r and
Model-2t/Model-2s/Model-2ts.
In Figures 5&6, we also notice that the recall
of the acquired words is still comparably low even
when 10 best word candidates are chosen for each
entity. This is mainly due to the scarcity of those
words that are not acquired in the data. Many of
the words that are not acquired appear less than 3
times in the data, which makes them unlikely to be
associated with any entity by the translation models.
When more data is available, we expect to see higher
recall.
8.3 An Example
Table 1 shows the 5-best words acquired by different
models for the entity dresser 1 in the 3d room scene
(see Figure 1). In the table, each word is followed by
its word-entity association probability p(w|e). The
correctly acquired words are shown in bold font.
As shown in the example, the baseline Model-1
learned 2 correct words in the 5-best list. Consid-
ering speech-gaze temporal information, Model-2t
learned one more correct word vanity in the 5-best
list. With semantic relatedness rescoring, Model-
2t-r further acquired word desk in the 5-best list
because of the high semantic relatedness of word
desk and the type of entity dresser 1. Although nei-
ther Model-1 nor Model-2t successfully acquired the
word desk in the 5-best list, the rank (=7) of the word
desk in Model-2t?s n-best list is much higher than the
rank (=21) in Model-1?s n-best list.
9 Conclusion
Motivated by the psycholinguistic findings, we in-
vestigate the use of eye gaze for automatic word ac-
quisition in multimodal conversational systems. Par-
ticularly, we investigate the use of speech-gaze tem-
poral information and word-entity semantic related-
ness to facilitate word acquisition. Our experiments
show that word acquisition is significantly improved
when temporal information is considered, which is
consistent with the previous psycholinguistic find-
ings about speech and eye gaze. Moreover, using
temporal information together with semantic relat-
edness rescoring further improves word acquisition.
Eye tracking systems are no longer bulky sys-
tems that prevent natural human machine commu-
nication. Display mounted gaze tracking systems
(e.g., Tobii) are completely non-intrusive, can toler-
ate head motion, and provide high tracking quality.
Integrating eye tracking with conversational inter-
faces is no longer beyond reach. Recent works have
shown that eye gaze can facilitate spoken language
processing in conversational systems (Qu and Chai,
2007; Prasov and Chai, 2008). Incorporating eye
gaze with automatic word acquisition provides an-
other potential approach to improve the robustness
of human machine conversation.
Acknowledgments
This work was supported by IIS-0347548 and IIS-
0535112 from the National Science Foundation.
The authors would like to thank Zahar Prasov for his
contribution on data collection. The authors would
also like to thank anonymous reviewers for their
valuable comments and suggestions.
References
Kobus Barnard, Pinar Duygulu, Nando de Freitas, David
Forsyth, David Blei, and Michael I. Jordan. 2003.
252
Matching words and pictures. Journal of Machine
Learning Research, 3:1107?1135.
Peter F. Brown, Stephen Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematic
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263?311.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press.
Zenzi M. Griffin and Kathryn Bock. 2000. What the eyes
say about speaking. Psychological Science, 11:274?
279.
JohnM. Henderson and Fernanda Ferreira, editors. 2004.
The interface of language, vision, and action: Eye
movements and the visual world. New York: Taylor
& Francis.
Marcel A. Just and Patricia A. Carpenter. 1976. Eye fix-
ations and cognitive processes. Cognitive Psychology,
8:441?480.
Yi Liu, Joyce Y. Chai, and Rong Jin. 2007. Au-
tomated vocabulary acquisition and interpretation in
multimodal conversational systems. In Proceedings of
the 45th Annual Meeting of the Association of Compu-
tational Linguistics (ACL).
E. Matin. 1974. Saccadic suppression: a review and an
analysis. Psychological Bulletin, 81:899?917.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. Wordnet::similarity - measuring the relat-
edness of concepts. In Proceedings of the Nineteenth
National Conference on Artificial Intelligence (AAAI-
04).
Zahar Prasov and Joyce Y. Chai. 2008. What?s in a
gaze? the role of eye-gaze in reference resolution in
multimodal conversational interfaces. In Proceedings
of ACM 12th International Conference on Intelligent
User interfaces (IUI).
Shaolin Qu and Joyce Y. Chai. 2007. An exploration
of eye gaze in spoken language processing for multi-
modal conversational interfaces. In Proceedings of the
Conference of the North America Chapter of the Asso-
ciation of Computational Linguistics (NAACL).
Deb K. Roy and Alex P. Pentland. 2002. Learning words
from sights and sounds, a computational model. Cog-
nitive Science, 26(1):113?146.
Michael K. Tanenhaus, Michael J. Spivey-Knowiton,
Kathleen M. Eberhard, and Julie C. Sedivy. 1995. In-
tegration of visual and linguistic information in spoken
language comprehension. Science, 268:1632?1634.
Chen Yu and Dana H. Ballard. 2004. A multimodal
learning interface for grounding spoken language in
sensory perceptions. ACM Transactions on Applied
Perceptions, 1(1):57?80.
253
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 217?224, Vancouver, October 2005. c?2005 Association for Computational Linguistics
A Salience Driven Approach to Robust Input Interpretation in 
Multimodal Conversational Systems  
 
Joyce Y. Chai                  Shaolin Qu              
Computer Science and Engineering 
Michigan State University  
East Lansing, MI 48824 
{jchai@cse.msu.edu,  qushaoli@cse.msu.edu} 
 
 
 
Abstract 
To improve the robustness in multimodal 
input interpretation, this paper presents a new 
salience driven approach. This approach is 
based on the observation that, during 
multimodal conversation, information from 
deictic gestures (e.g., point or circle) on a 
graphical display can signal a part of the 
physical world (i.e., representation of the 
domain and task) of the application which is 
salient during the communication.  This salient 
part of the physical world will prime what 
users tend to communicate in speech and in 
turn can be used to constrain hypotheses for 
spoken language understanding, thus 
improving overall input interpretation. Our 
experimental results have indicated the 
potential of this approach in reducing word 
error rate and improving concept identification 
in multimodal conversation.  
1 Introduction  
Multimodal conversational systems promote more 
natural and effective human machine communication 
by allowing users to interact with systems through 
multiple modalities such as speech and gesture 
(Cohen et al, 1996; Johnston et al, 2002; Pieraccini 
et al, 2004). Despite recent advances, interpreting 
what users communicate to the system is still a 
significant challenge due to insufficient recognition 
(e.g., speech recognition) and understanding (e.g., 
language understanding) performance. Significant 
improvement in the robustness of multimodal 
interpretation is crucial if multimodal systems are to 
be effective and practical for real world applications.  
Previous studies have shown that, in multimodal 
conversation, multiple modalities tend to complement 
each other (Cassell et al 1994). Fusing two or more 
modalities can be an effective means of reducing 
recognition uncertainties, for example, through 
mutual disambiguation (Oviatt 1999). For 
semantically-rich modalities such as speech and pen-
based gesture, mutual disambiguation usually 
happens at the fusion stage where partial semantic 
representations from individual modalities are 
disambiguated and combined into an overall 
interpretation (Johnston 1998, Chai et al, 2004a). 
One problem is that some critical but low probability 
information from individual modalities (e.g., 
recognized alternatives with low probabilities) may 
never reach the fusion stage. Therefore, this paper 
addresses how to use information from one modality 
(e.g., deictic gesture) to directly influence the 
semantic processing of another modality (e.g., spoken 
language understanding) even before the fusion stage.  
In particular we present a new salience driven 
approach that uses gesture to influence spoken 
language understanding. This approach is based on 
the observation that, during multimodal conversation, 
information from deictic gestures (e.g., point or 
circle) on a graphical interface can signal a part of the 
physical world (i.e., representation of the domain and 
task) of the application which is salient during the 
communication.  This salient part of the physical 
world will prime what users tend to communicate in 
speech and thus in turn can be used to constrain 
hypotheses for spoken language understanding. In 
particular, this approach incorporates a notion of 
salience from deictic gestures into language models 
for spoken language processing. Our experimental 
results indicate the potential of this approach in 
reducing word error rate and improving concept 
identification from spoken utterances. 
217
In the following sections, we first introduce the 
current architecture for multimodal interpretation. 
Then we describe our salience driven approach and 
present empirical results.  
2 
3 
Input Interpretation 
Input interpretation is the identification of semantic 
meanings in user inputs. In multimodal conversation, 
user inputs can come from multiple channels (e.g., 
speech and gesture). Thus, most work on input 
interpretation is based on semantic fusion that 
includes individual recognizers and a sequential 
integration processes as shown in Figure 1.  In this 
approach, a system first creates possible partial 
meaning representations from recognized hypotheses 
(e.g., N-best lists) independently of other modalities. 
For example, suppose a user says ?what is the price 
of this painting? and at the same time points to a 
position on the screen. The partial meaning 
representations from the speech input and the gesture 
input are shown in (a-b) in Figure 1. The system uses 
the partial meaning representations to disambiguate 
each other and combines compatible partial 
representations together into an overall semantic 
representation as in Figure1(c).  
In this architecture, the partial semantic 
representations from individual modalities are crucial 
for mutual disambiguation during multimodal fusion. 
The quality of partial semantic representations 
depends on how individual modalities are processed. 
For example, if the speech input is recognized as 
?what is the prize of this pant?, then the partial 
representation from the speech input will not be 
created in the first place. Without a candidate partial 
representation, it is not likely for multimodal fusion 
to reach an overall meaning of the input given this 
late fusion architecture. 
Thus, a problem with the semantics-based fusion 
approach is that information from multiple modalities 
is only used during the fusion stage to disambiguate 
or combine partial semantic representations. This late 
use of information from other sources in the 
pipelined process can cause the loss of some low 
probability information (e.g., recognized alternatives 
with low probabilities which did not make it to the N-
best list) which could be very crucial in terms of the 
overall interpretation.  It is desirable to use 
information from multiple sources at an earlier stage 
before partial representations are created from 
individual modalities. For example, in ((Bangalore 
and Johnston 2000), a finite-state approach was 
applied to tightly couple multimodal language 
processing (e.g., gesture and speech) and speech 
recognition to improve recognition hypotheses. To 
further address this issue, in this paper, we present a 
salience driven approach that particularly applies 
gesture information (e.g., pen-based deictic gestures) 
to robust spoken language understanding before 
multimodal fusion.  
Related Work on Salience Modeling 
We first give a brief overview on the notion of 
salience and how salience modeling is applied in 
earlier work on natural language and multimodal 
language processing.  
Linguistic salience describes the accessibility of 
entities in a speaker/hearer?s memory and its 
implication in language production and 
interpretation. Many theories on linguistic salience 
have been developed, including how the salience of 
entities affects the form of referring expressions as in 
the Givenness Hierarchy (Gundel et al, 1993) and 
the local coherence of discourse as in the Centering 
Theory (Grosz et al, 1995). Salience modeling is 
used for both language generation and language 
interpretation; the latter is more relevant to our work. 
Most salience-based interpretation has focused on 
reference resolution for both linguistic referring 
expressions (e.g., pronouns) (Lappin and Leass 1995) 
and multimodal expressions (Hul et al 1995; 
Eisenstein and Christoudias 2004).  
Speech Input Gesture Input
Speech 
Recognition
Language
Understanding
Gesture
Recognizer
Multimodal
Fusion
Semantic Representation
Gesture
Understanding
Semantic Representation Semantic Representation
What is the price of this painting Point to a position on the screen
Intent: Ask
Type: Painting
Aspect: Price
Type: Painting
Id: P23
Intent: Ask
Type: Painting
Aspect: Price
Id: P23
Type: Wall
Id: W1
(a) (b)
(c)
 
Figure 1: Semantics-based multimodal interpretation 
Visual salience considers an object salient when 
it attracts a user?s visual attention more than others. 
The cause of such attention depends on many factors 
including user intention, familiarity, and physical 
characteristics of objects. For example, an object may 
be salient when it has some properties the others do 
not have, such as it is the only one that is highlighted, 
or the only one of a certain size, category, or color 
218
(Landragin et al, 2001). Visual salience can also be 
useful in input interpretation, for example, for 
multimodal reference resolution (Kehler 2000) and 
cross-modal coreference interpretation (Byron et al, 
2005).  
We believe that salience modeling should go 
beyond reference resolution. Our view is that the 
salience not only affects the use of referring 
expressions (and thus is useful for interpreting 
referring expressions), but also influences the 
linguistic context of the referring expressions. The 
spoken utterances that contain these expressions tend 
to describe information relating to the salient objects 
(e.g., properties or actions). Therefore, our goal in 
this paper is to take salience modeling a step further 
from reference resolution, towards overall language 
understanding.  
4 
4.1 
A Salience Driven Approach 
The new salience driven approach is based on the 
cognitive theory of Conversation Implicature (Grice 
1975) and earlier empirical findings of user speech 
and gesture behavior in multimodal conversation 
(Oviatt 1999). The theory of Conversation 
Implicature (Grice 1975) states that speakers tend to 
make their contribution as informative as is required 
(for the current purpose of communication) and not 
make their contribution more informative than is 
required. In the context of multimodal conversation 
that involves speech and pen-based gesture, this 
theory indicates that users most likely will not make 
any unnecessary deictic gestures unless those 
gestures help in communicating users? intention. This 
is especially true since gestures usually take an extra 
effort from a user. When a pen-based gesture is 
intentionally delivered by a user, the information 
conveyed is often a crucial component in 
interpretation (Chai et al, 2005).  
Speech 
Recognition
Language
Understanding
Physical world representation
salient
e1 e2 e3 ???.
P(e)
d
isco
u
rse
Speech
Gesture
Gesture 
Recognition
Gesture
Understanding
Multimodal    Fusion
Semantic  Representation
Figure 2: The salience driven approach: the salience 
distribution calculated from gesture is used to tailor 
language models for spoken language understanding  
Speech and gesture also tend to complement each 
other. For example, when a speech utterance is 
accompanied by a deictic gesture (e.g., point or 
circle) on a graphical display, the speech input tends 
to issue commands or inquiries about properties of 
objects, and the deictic gestures tend to indicate the 
objects of interest. In addition, as shown in (Oviatt 
1999), the deictic gestures often occur before spoken 
utterances. Our previous work (Chai et al, 2004b) 
also showed that 85% of time gestures occurred 
before corresponding speech units. Therefore, 
gestures can be used as an earlier indicator to 
anticipate the content of communication in the 
subsequent spoken utterances.  
Overview 
The general idea of the salience based approach is 
shown in Figure 2. For each application domain, 
there is a physical world representation that captures 
domain knowledge (details are described later). A 
deictic gesture can activate several objects on the 
graphical display. This activation will signal a 
distribution of objects that are salient. The salient 
objects are mapped to the physical world 
representation to indicate a salient part of 
representation that includes relevant properties or 
tasks related to the salient objects. This salient part of 
the physical world is likely to be the potential content 
of the spoken communication, and thus can be used 
to tailor language models for spoken language 
understanding. This process is shown in the middle 
shaded box of Figure 2. It bridges gesture 
understanding and language understanding at a stage 
before multimodal fusion. Note that the use of 
gesture information can be applied at different stages: 
during speech recognition to generate hypotheses or 
post processing of recognized hypotheses before 
language understanding. In this paper, we focus on 
the latter.    
The physical world representation includes the 
following components:  
? Domain Model. This component captures the 
relevant knowledge about the domain including 
domain objects, properties of the objects, relations 
between objects, and task models related to objects. 
Previous studies have shown that domain knowledge 
219
can be used to improve spoken language 
understanding (Wai et al 2001).  Currently, we apply 
a frame-based representation where a frame 
represents an object (or a type of object) in the 
domain and frame elements represent attributes and 
tasks related to the objects. Each frame element is 
associated with a semantic tag which indicates the 
semantic content of that element. In the future, the 
domain model might also include knowledge about 
the interface, for example, visual properties and 
spatial relations between objects on the interface. 
w1 wn?? ??
Time
t2 t3 tn
)(eP
nt)|(
3tgeP
)( 3tnt?
)( 2tnt?
)( 1tnt?
wi wi+1
t1
)|(
2tgeP)|( 1tgeP
Figure 3: Salience modeling: the salience distribution 
at time tn is calculated by a joint effect of gestures 
that happen before tn.  ? Domain Grammar. This component specifies 
grammar and vocabularies used to process language 
inputs. There are two types of representation. The 
first type is a semantics-based context free grammar 
where each non-terminal symbol represents a 
semantic tag (indicating semantic information such as 
the semantic type of an object, etc). Each word (i.e., 
the terminal symbol) in the lexicon relates to one or 
more semantic tags. Some of these semantic tags are 
directly linked to the frame elements in the domain 
model since they represent certain properties or tasks. 
This grammar was manually developed.  
4.2 
The second type of representation is based on 
annotated user spoken utterances. The data are 
annotated in terms of relevant semantic information 
(i.e., using semantic tags) in the utterance and the 
intended objects of interest (which are directly linked 
to the domain model). Based on the annotated data, 
N-grams can be learned to represent the dependency 
of language in our domain.  
Based on the physical world representation, our 
approach supports the following operations:  
Salience modeling. This operation calculates a 
salience distribution of entities in the physical world. 
In our current investigation, we limit the scope of 
entities to a closed set of objects from our physical 
world representation since the system has knowledge 
about those objects. These entities could have 
different salience values depending on whether they 
are visible on the graphical display, gestured by a 
user, or mentioned in the prior conversation. In this 
paper, we focus on the salience modeling using 
gesture information only.  
Salience driven language understanding. This 
operation maps the salience distribution to the 
physical world representation and uses the salient 
world to influence spoken language understanding. 
Note that, in this paper, we are not concerned with 
acoustic models for speech recognition, but rather we 
are interested in the use of the salience distribution to 
prime language models and facilitate language 
understanding. 
Salience Modeling 
We use a vector er to represent entities in the physical 
world representation. For each entity e ek
r? , we use 
to represent its salience value at time tn.  For 
all the entities, we use P
)( kt eP n
)(e
nt
v  to represent a salience 
distribution at time tn. Figure 3 shows a sequence of 
words with corresponding gestures that occur at t1, t2, 
and t3. As shown in Figure 3, the salience distribution 
at any given time tn is influenced by a joint effect 
from this sequence of gestures that happen before tn 
etc. Depending on its time of occurrence, each 
gesture may have a different impact on the salience 
distribution at time tn. Note that although each 
gesture may have a short duration, here we only 
consider the beginning time of a gesture. Therefore, 
for an entity ek, its salience value at time tn is 
computed as follows: 
1
1
( ) ( | )
( )
( ) ( |
n i i
n
n i i
m
t t k t
i
t k m
t t t
e e i
)
g P e g
P e
g P e g
?
?
=
? =
=
?
??r
              (1) 
In Equation (1), m (m ? 1) is the number of 
gestures that have occurred before tn. The different 
impact of a gesture g  at time ti that contributes to 
the salience distribution at time tn is represented as 
the weight 
it
)(
in tt g? in Equation (1). Currently, we 
calculate the weight depending on the temporal 
distance as follows:  
)(]
2000
)(
exp[)( in
in
tt tt
tt
g
in
???=?             (2) 
Equation (2) indicates that at a given time tn 
(measured in milliseconds), the closer a gesture (at ti) 
is to the time tn, the higher impact this gesture has on 
the salience distribution (Chai et al, 2004b).  
It is worth mentioning that a deictic gesture on the 
graphic display (e.g., pointing and circling) could 
have ambiguous interpretation by itself. For example, 
220
given an interface, a point or a circle on the screen 
could result in selection of different entities with 
different probabilities. Therefore, in Equation (1), 
is the selection probability which indicates 
the likelihood of selecting an entity e given a gesture 
at time ti. This selection probability is calculated by a 
function of the distance between the location of the 
entity and the focus point of the recognized gesture 
on the display (Chai et al, 2004a). A normalization 
factor is incorporated to ensure that the summation of 
selection probabilities over all possible entities adds 
up to one.  
( | )
it
P e g
When no gesture is involved in a given input, the 
salience distribution at any given time is a uniform 
distribution. If one or more gestures are involved, 
then Equation (1) is used to calculate the salience 
distribution.  
4.3 
P W
Salience Driven Spoken Language 
Understanding 
The salience distribution of entities identified based 
on the gesture information (as described above) is 
used to constrain hypotheses for language 
understanding. More specifically, for each onset of a 
spoken word at time t (i.e., the beginning time stamp 
of a spoken word), the salience distribution at t can 
be calculated based on a sequence of gestures that 
happen before t by Equation (1). This salience 
distribution can then be used to prime language 
models for spoken language processing.   
Language Modeling 
We first give a brief background of language 
modeling. Given an observed speech utterance O, the 
goal of speech recognition is to find a sequence of 
words W* so that W P , 
where P(O|W) is the acoustic model and P(W) is the 
language model. In traditional speech recognition 
systems, the acoustic model provides the probability 
of observing the acoustic features given hypothesized 
word sequences and the language model provides the 
probability of a sequence of words. The language 
model is computed as follows: 
* arg max ( | ) ( )O W=
)|()...|()|()()( 112131211
?= nnn wwPwwwPwwPwPwP          
Using the Markov assumption, the language model 
can be approximated by a bigram model as in: 
?
=
?=
n
i
ii
n wwPwP
1
11 )|()(                                    
To improve the speech understanding results for 
spoken language interfaces, many systems have 
applied a loosely-integrated approach which 
decouples the language model from the acoustic 
model (Zue et al, 1991, Harper et al, 2000). This 
allows the development of powerful language models 
independent of the acoustic model, for example, 
utilizing topics of the utterances (Gildea and 
Hofmann 1999), syntactic or semantic labels 
(Heeman 1999), and linguistic structures (Chelba and 
Jelinek 2000, Wang and Harper 2002). Recently, we 
have seen work on language understanding based on 
environment (Schuler 2003) and language modeling 
using visual context (Roy and Mukherjee 2005). Our 
salience driven approach is inspired by this earlier 
work. Here, we do not address the acoustic model of 
speech recognition, but rather incorporate the 
salience distribution for language modeling. In 
particular, our focus is on investigating the effect of 
incorporating additional information from other 
modalities (e.g., gesture) with traditional language 
models.   
Primed Language Model 
The calculated salience distribution is used to prime 
the language model. More specifically, we use a 
class-based bigram model from (Brown et al 1992):  
)|()|()|( 11 ?? = iiiiii ccPcwPwwP                 (3) 
In Equation (3), ci is the class of the word wi, 
which could be a syntactic class or a semantic class. 
is the class transition probability, which 
reflects the grammatical formation of utterances. 
is the word class probability which 
measures the probability of seeing a word wi given a 
class ci. The class-based N-gram model can make 
better use of limited training data by clustering words 
into classes. A number of researchers have shown 
that the class-based N-gram model can successfully 
improve the performance of speech recognition 
(Jelinek 1990, Heeman 1999, Kneser and Ney 1993, 
Samuelsson and Reichl, 1999). 
)|( 1?ii ccP
)|( ii cwP
In our approach, the ?class? used in the class-
based bigram model comes from combined semantic 
and functional classes designed for our domain. For 
example, ?this? is tagged as Demonstrative, and 
?price? is tagged as AttrPrice. As shown in Equation 
(3), there are two types of parameter estimation. In 
terms of the class transition probability, as in earlier 
work, we directly use the annotated data. In terms of 
the word class distribution, we incorporate the notion 
of salience. We use the salience distribution to 
dynamically adjust the world class probability 
 as follows: )|( ii cwP
221
)(
)|(
)|,(
)|( kt
ee ki
kii
ii ePecP
ecwP
cwP
i
k
?
?
=
v
               (4) User  
index 
# of  
Inputs 
# inputs 
w/o gesture 
Baseline 
WER 
1 21 0 0.287 
2 31 0 0.335 
3 27 0 0.399 
4 10 0 0.680 
5 8 1 0.200 
6 36 0 0.387 
7 18 0 0.250 
8 25 1 0.278 
9 23 0 0.482 
10 11 0 0.117 
11 16 3 0.255 
Table 1: Related information about the evaluation 
data: user type, the number of turns per user, and the 
baseline word recognition rate.  
In Equation (4), P  is the salience value for an 
entity  at time ti (the onset of the spoken word wi), 
which can be calculated by Equation (1).  Equation 
(4) indicates that only information associated with the 
salient entities is used to estimate the word class 
distribution. In other words, the word class 
probability favors the salient physical world as 
indicated by the salience distribution
)( kt ei
ke
)(eP
it
v . More 
specifically, at time  ti, given a semantic class ci, the 
choice of word ?wi? is dependent on the salient 
physical world at the moment, which is represented 
as the salience distribution )(eP
it
v at time ti. For all wi, 
the summation of this word class probability is equal 
to one. Furthermore, given an entity ,  
and  are not dependent on time ti, but rather 
on the domain and the use of language expressions. 
Therefore they can be estimated based on the training 
data that are annotated in terms of semantic 
information and the intended objects of interest (as 
discussed in Section 4.1). Since the annotated data is 
very limited, the sparse data can become a problem 
for the maximum likelihood estimation. Therefore, a 
smoothing technique based on the Katz backoff 
model (Katz, 1987) is applied. For example, to 
calculate , if a word wi has one or more 
occurrences in the training data associated with the 
class ci and the entity , then its count is discounted 
by a fraction in the maximum likelihood estimation. 
If wi does not occur, then this approach backs off to 
the domain grammar and redistributes the remaining 
probability mass uniformly among words in the 
lexicon that are linked with class ci and entity e . 
ke )| ki ec
k
,( iwP
)| keP
,( iwP
( ic
)| ke
ke
ic
5 
                                                          
Evaluation 
We evaluated the salience model during post 
processing recognized hypotheses. Given possible 
hypotheses from a speech recognizer, we use the 
salience-based language model to identify the most 
likely sequence of words. The salience distribution 
based on gesture was used to favor words that are 
consistent with the attention indicated by gestures. 
The data collected from our previous user studies 
were used in our evaluation (Chai et al, 2004b). In 
these studies, users interacted with our multimodal 
interface using both speech and deictic gestures to 
find information about real estate properties. In 
particular, each user was asked to accomplish five 
tasks. Each of these tasks required the user to retrieve 
different types of information from our interface. For 
example, one task was to find the least expensive 
house in the most populated town. The data were 
recorded from eleven subjects including five non-
native speakers and six native speakers. Each user?s 
voice was individually trained before the study. Table 
1 shows the relevant information about the data such 
as the total number of inputs (or turns) from each 
subject, the number of speech alone inputs without 
any gesture, and the baseline recognition results 
without using salience-based post processing in terms 
of the word error rate (WER).  In total, we have 
collected 226 user inputs with an average of eight 
words per spoken utterance1. As shown in Table 1, 
the majority of inputs consisted of both speech and 
gesture. Since currently we only use gesture 
0.1
0.2
0.3
0.4
0.5
0.6
0.7
1 2 3 4 5 6 7 8 9 10 11
User index
W
o
rd
 E
rr
o
r 
R
a
te
Baseline Salience driven model
 
Figure 5: Comparison of the baseline and the result 
from post-processing in terms of WER  
1 The difference between the number of user inputs reported 
here and that in (Chai et al, 2004b) was caused by the situa-
tion where one intended user input (which was the unit for 
counting in our previous work) was split into a couple turns 
(which constitute the new counts here).  
222
information in salience modeling, our approach will 
not affect speech only inputs.  
To train the salience-based model, we applied the 
leave-one-out approach. The data from each user was 
held out as the testing data and the remaining users 
were used as the training data to acquire relevant 
probability estimations in Equation (3) and (4).  
Figure 5 shows the comparison results between 
the baseline and the salience-based model in terms of 
word error rate (WER). The word error rate as a 
result of salience-based post processing is 
significantly better than that from the baseline 
recognizer (t = 4.75, p < 0.001). The average WER 
reduction is about 12%.   
We further evaluated how the salience based 
model affects the final understanding results. This is 
because an improvement in WER may not directly 
lead to an improvement in understanding. We applied 
our semantic grammar on a sequence of words 
resulting from both the baseline and the salience-
based post-processing to identify key concepts. In 
total, there were 686 concepts from the transcribed 
speech utterances. Table 2 shows the evaluation 
results. Precision measures the percentage of correctly 
identified concepts out of the total number of 
concepts identified based on a sequence of words. 
Recall measures the percentage of correctly identified 
concepts out of the total number of intended concepts 
from user?s utterance. F-measurement combines 
precision and recall together as follows: 
1,
RecallPrecision
RecallPrecision)1(
2
2
=+
??+= ??
? whereF .  
Table 2 shows that, on average, the concept 
identification based on the word sequence resulting 
from the salience-based approach performs better 
than the baseline in terms of both precision and 
recall. Figure 6 provides two examples to show the 
difference between the baseline recognition and the 
salience-based post processing.   
The evaluation reported here is only an initial step 
based on a limited domain. The small scale in the 
number of objects and the vocabulary size can only 
demonstrate the potential of the salience-based 
approach to a limited degree.  To further understand 
the advantages and issues of this approach, we are 
currently working on a more complex domain with 
richer concepts and relations, as well as larger 
vocabularies.  
It is worth mentioning that the goal of this work is 
to explore whether salience modeling based on other 
modalities (e.g., gesture) can be used to prime 
traditional language models to facilitate spoken 
language processing. The salience driven approach 
based on additional modalities can be combined with 
more sophisticated language modeling (e.g., better 
parameter estimation) in the future.  
Example 1:
Transcription: What is the population of this town
Baseline recognition: What is the publisher of this time
Salience-based processing: what is the population of this town
Example 2:
Transcription: How much is this gray house
Baseline recognition: How much is this great house
Salience-based processing: How much is this gray house
Figure 6: Examples of utterances with baseline recogni-
tion and improved recognition from the salience-based 
processing.  
User # Baseline Salience-based 
Precision 80.3% 84.6% 
Recall 75.7% 83.8% 
F-measure 77.9% 84.2% 
Table2. Overall concept identification comparison 
between the baseline and the salience driven model. 
6 Conclusion and Future Work 
This paper presents a new salience driven approach 
to robust input interpretation in multimodal 
conversational systems. This approach takes 
advantage of rich information from multiple 
modalities. Information from deictic gestures is used 
to identify a part of the physical world that is salient 
at a given point of communication. This salient part 
of the physical world is then used to prime language 
models for spoken language understanding. Our 
experimental results have shown the potential of this 
approach in reducing word error rate and improving 
concept identification from spoken utterances in our 
application. Although currently we have only 
investigated the use of gesture information in salience 
modeling, the salience driven approach can be 
extended to include other modalities (e.g., eye gaze) 
and information (e.g., conversation context). Our 
future work will specifically investigate how to 
combine information from multiple sources in 
salience modeling and how to apply the salience 
models in different early stages of processing.  
 
 
223
Acknowledgement 
This work was supported by a CAREER grant IIS-0347548 
from the National Science Foundation. The authors would like 
to thank anonymous reviewers for their helpful comments and 
suggestions.  
References  
Bangalore, S. and Johnston, M. 2000. Integrating Multimodal 
Language Processing with Speech Recognition. In 
Proceedings of ICSLP.  
Brown, P., Della Pietra, V. J., deSouza, P. V., Lai, J. C, and 
Mercer, R. L. 1992. Class-based n-gram models of natural 
language. Computational Linguistics, 18(4):467-479.  
Byron, D., Mampilly, T., Sharma, V., and Xu, T. 2005. Utilizing 
Visual Attention for Cross-Modal Coreference Interpretation. 
Spring Lecture Notes in Computer Science: Proceedings of 
Context-05, page 83-96.  
Cassell, J., Stone, M., Douville, B., Prevost, S., Achorn, B., 
Steedman, M., Badler, N., and Pelachaud, C. 1994. Modeling 
the interaction between speech and gesture. Cognitive Science 
Society. 
Chai, J. Y., Prasov, Z., Blaim, J., and Jin, R. 2005.  Linguistic 
Theories in Efficient Multimodal Reference Resolution: an 
Empirical Investigation. The 10th International Conference on 
Intelligent User Interfaces (IUI-05), pp. 43-50, San Diego, 
CA.  
Chai, J. Y., Hong, P., Zhou, M. X, and Prasov, Z. 2004b. 
Optimization in Multimodal Interpretation. In Proceedings of 
ACL,  pp. 1-8, Barcelona, Spain. 
Chai, J. Y., Hong, P., and Zhou, M.  2004a. A Probabilistic 
Approach to Reference Resolution in Multimodal User 
Interfaces. Proceedings of 9th International Conference on 
Intelligent User Interfaces (IUI-04), pp. 70-77, Madeira, 
Portugal. 
Chelba, C. and Jelinek, F. 2000. Structured language modeling. 
Computer Speech and Language, 14(4):283?332. 
Cohen, P., Johnston, M., McGee, D., Oviatt, S., Pittman, J.; 
Smith, I., Chen, L., and Clow, J. 1996. Quickset: Multimodal 
Interaction for Distributed Applications. Proceedings of ACM 
Multimedia, 31? 40. 
Eisenstein J. and Christoudias. C. 2004. A salience-based 
approach to gesture-speech alignment. In Proceedings of 
HLT/NAACL?04.  
Gildea, D. and Hofmann, T. 1999. Topic-based language models 
using EM. In Proceedings of Eurospeech.  
Griffin, Z. M. 2001. Gaze durations during speech reflect word 
selection and phonological encoding. Cognition 82, B1-B14. 
Grosz, B. J., Joshi, A. K., and Weinstein, S. 1995. Centering: A 
framework for modeling the local coherence of discourse. 
Computational Linguistics, 21(2).  
Grice, H. P. Logic and Conversation. 1975. In Cole, P., and 
Morgan, J., eds. Speech Acts. New York, New York: 
Academic Press. 41-58. 
Gundel, J. K., Hedberg, N., and Zacharski, R. 1993. Cognitive 
Status and the Form of Referring Expressions in Discourse. 
Language 69(2):274-307.  
Harper, M.., White, C., Wang, W., Johnson, M., and Helzerman, 
R. 2000. The Effectiveness of Corpus-Induced Dependency 
Grammars for Post-processing Speech. Proceedings of the 
North American Association for Computational Linguistics, 
102-109. 
Heeman. P. 1999. POS tags and decision trees for language 
modeling. In Proceedings of the Conference on Empirical 
Methods in Natural Language Process (EMNLP). 
Huls, C., Bos, E., and Classen, W. 1995. Automatic Referent 
Resolution of Deictic and Anaphoric Expressions. 
Computational Linguistics, 21(1):59-79. 
Jelinek, F. 1990. Self-organized language modeling for speech 
recognition. In Waibel, A. and Lee, K. F. (Eds). Readings in 
Speech Recognition, pp. 450-506. 
Johnston, M. 1998. Unification-based Multimodal parsing, 
Proceedings of COLING-ACL.  
Johnston, M.,  Bangalore, S.,  Visireddy G., Stent, A., Ehlen, P., 
Walker, M., Whittaker, S., and Maloor, P. 2002. MATCH: An 
Architecture for Multimodal Dialog Systems, in Proceedings 
of the 40th ACL, Philadelphia, pp. 376-383.  
Katz, S. M. 1987. Estimation of probabilities from sparse data for 
the language model component of a speech recognizer. IEEE 
Transactions on Acoustics, Speech, and Signal Processing, 
35(3). 
Kehler, A. 2000. Cognitive Status and Form of Reference in 
Multimodal Human-Computer Interaction, Proceedings of 
AAAI?01. 
Kneser, R. and Ney, H. 1993. Improved clustering techniques for 
class-based statistical language modeling. In Eurospeech?93, 
pp. 973-976. 
Landragin, F., Bellalem, N., and Romary, L. 2001. Visual 
Salience and Perceptual Grouping in Multimodal Interactivity. 
In: First International Workshop on Information Presentation 
and Natural Multimodal Dialogue, Verona, Italy, pp. 151-155. 
Lappin, S., and Leass, H. 1994. An algorithm for pronominal 
anaphora resolution. Computational Linguistics, 20(4).  
Oviatt, S. 1999. Mutual Disambiguation of Recognition Errors in 
a Multimodal Architecture. In Proceedings of CHI. 
Pieraccini, R., Dayandhi, K., Bloom, J., Dahan, J.-G., Phillips, M., 
Goodman, B. R., Prasad, K. V., 2004. Multimodal 
Conversational Systems for Automobiles, Communications of 
the ACM, Vol. 47, No. 1, pp. 47-49 
Roy, D. and Mukherjee, N. 2005. Towards Situated Speech 
Understanding: Visual Context Priming of Language Models. 
Computer Speech and Language, 19(2): 227-248.  
Samuelsson, C. and Reichl, W. 1999. A class-based Language 
Model for Large Vocabulary Speech Recognition Extracted 
from Part-of-Speech Statistics. In IEEE ICASSP?99. 
Schuler, W. 2003. Using model-theoretic semantic interpretation 
to guide statistical parsing and word recognition in a spoken 
language interface. In  Proceedings of ACL, Sapporo, Japan. 
Wai, C., Pierraccinni, R., and Meng, H. 2001. A Dynamic 
Semantic Model for Rescoring Recognition Hypothesis. 
Proceedings of the ICASSP.  
Wang, W. and Harper. M. 2002. The superARV language model: 
In Investigating the effectiveness of tightly integrating 
multiple knowledge sources. In Proceedings  of EMNLP, 238?
247.  
Zue, V., Glass, J., Goodine, D., Leung, H., Phillips, M., Polifroni, 
J., and Seneff, S. 1991. Integration of Speech Recognition and 
Natural Language Processing in the MIT Voyager System. 
Proceedings of the ICASSP. 
224
Proceedings of NAACL HLT 2007, pages 284?291,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
An Exploration of Eye Gaze in Spoken Language Processing for Multimodal
Conversational Interfaces
Shaolin Qu Joyce Y. Chai
Department of Computer Science and Engineering
Michigan State University
East Lansing, MI 48824
{qushaoli,jchai}@cse.msu.edu
Abstract
Motivated by psycholinguistic findings,
we are currently investigating the role of
eye gaze in spoken language understand-
ing for multimodal conversational sys-
tems. Our assumption is that, during hu-
man machine conversation, a user?s eye
gaze on the graphical display indicates
salient entities on which the user?s atten-
tion is focused. The specific domain infor-
mation about the salient entities is likely
to be the content of communication and
therefore can be used to constrain speech
hypotheses and help language understand-
ing. Based on this assumption, this paper
describes an exploratory study that incor-
porates eye gaze in salience modeling for
spoken language processing. Our empiri-
cal results show that eye gaze has a poten-
tial in improving automated language pro-
cessing. Eye gaze is subconscious and in-
voluntary during human machine conver-
sation. Our work motivates more in-depth
investigation on eye gaze in attention pre-
diction and its implication in automated
language processing.
1 Introduction
Psycholinguistic experiments have shown that eye
gaze is tightly linked to human language process-
ing. Eye gaze is one of the reliable indicators of
what a person is ?thinking about? (Henderson and
Ferreira, 2004). The direction of gaze carries infor-
mation about the focus of the users attention (Just
and Carpenter, 1976). The perceived visual context
influences spoken word recognition and mediates
syntactic processing (Tanenhaus et al, 1995; Roy
and Mukherjee, 2005). In addition, directly before
speaking a word, the eyes move to the mentioned
object (Griffin and Bock, 2000).
Motivated by these psycholinguistic findings, we
are currently investigating the role of eye gaze in
spoken language understanding during human ma-
chine conversation. Through multimodal interfaces,
a user can look at a graphic display and converse
with the system at the same time. Our assumption
is that, during human machine conversation, a user?s
eye gaze on the graphical display can indicate salient
entities on which the user?s attention is focused. The
specific domain information about the salient enti-
ties is likely linked to the content of communication
and therefore can be used to constrain speech hy-
potheses and influence language understanding.
Based on this assumption, we carried out an ex-
ploration study where eye gaze information is in-
corporated in a salience model to tailor a language
model for spoken language processing. Our prelim-
inary results show that eye gaze can be useful in im-
proving spoken language processing and the effect
of eye gaze varies among different users. Because
eye gaze is subconscious and involuntary in human
machine conversation, our work also motivates sys-
tematic investigations on how eye gaze contributes
to attention prediction and its implications in auto-
mated language processing.
2 Related Work
Eye gaze has been mainly used in human machine
interaction as a pointing mechanism in direct manip-
ulation interfaces (Jacob, 1990; Jacob, 1995; Zhai
et al, 1999), as a facilitator in computer supported
human human communication (Velichkovsky, 1995;
Vertegaal, 1999); or as an additional modality dur-
ing speech or multimodal communication (Starker
and Bolt, 1990; Campana et al, 2001; Kaur et al,
284
2003; Qvarfordt and Zhai, 2005). This last area of
investigation is more related to our work.
In the context of speech and multimodal commu-
nication, studies have shown that speech and eye
gaze integration patterns can be modeled reliably for
users. For example, by studying patterns of eye gaze
and speech in the phrase ?move it there?, researchers
found that the gaze fixation closest to the intended
object begins, with high probability, before the be-
ginning of the word ?move? (Kaur et al, 2003). Re-
cent work has also shown that eye gaze has a poten-
tial to improve reference resolution in a spoken dia-
log system (Campana et al, 2001). Furthermore, eye
gaze also plays an important role in managing dia-
log in conversational systems (Qvarfordt and Zhai,
2005).
Salience modeling has been used in both natural
language and multimodal language processing. Lin-
guistic salience describes entities with their accessi-
bility in a hearer?s memory and their implications in
language production and interpretation. Linguistic
salience modeling has been used for language in-
terpretations such as reference resolution (Huls et
al., 1995; Eisenstein and Christoudias, 2004). Vi-
sual salience measures how much attention an en-
tity attracts from a user based on its visual proper-
ties. Visual salience can tailor users? referring ex-
pressions and thus can be used for multimodal refer-
ence resolution (Kehler, 2000). Our recent work has
also investigated salience modeling based on deic-
tic gestures to improve spoken language understand-
ing (Chai and Qu, 2005; Qu and Chai, 2006).
3 Data Collection
We conducted user studies to collect speech and eye
gaze data. In the experiments, a static 3D bedroom
scene was shown to the user. The system verbally
asked a user a list of questions one at a time about
the bedroom and the user answered the questions by
speaking to the system. Fig.1 shows the 14 questions
in the experiments. The user?s speech was recorded
through an open microphone and the user?s eye gaze
was captured by an Eye Link II eye tracker. From 7
users? experiments, we collected 554 utterances with
a vocabulary of 489 words. Each utterance was tran-
scribed and annotated with entities that were being
talked about in the utterance.
1 Describe this room.
2 What do you like/dislike about the arrangement?
3 Describe anything in the room that seems strange to
you.
4 Is there a bed in this room?
5 How big is the bed?
6 Describe the area around the bed.
7 Would you make any changes to the area around the
bed?
8 Describe the left wall.
9 How many paintings are there in this room?
10 Which is your favorite painting?
11 Which is your least favorite painting?
12 What is your favorite piece of furniture in the room?
13 What is your least favorite piece of furniture in the
room?
14 How would you change this piece of furniture to make
it better?
Figure 1: Questions for users in experiments
The collected raw gaze data consists of the screen
coordinates of each gaze point sampled at 4 ms.
As shown in Fig.2a, this raw data is not very use-
ful for identifying fixated entities. The raw gaze
data are processed to eliminate invalid and saccadic
gaze points, leaving only pertinent eye fixations.
Invalid gaze points occur when users look off the
screen. Saccadic gaze points occur during ballis-
tic eye movements between fixations. Vision stud-
ies have shown that no visual processing occurs dur-
ing saccades (i.e., saccadic suppression). It is well
known that eyes do not stay still, but rather make
small, frequent jerky movements. In order to best
determine fixation locations, nearby gaze points are
averaged together to identify fixations. The pro-
cessed eye gaze fixations can be seen in Fig.2b.
Fig.3 shows an excerpt of the collected speech
and gaze fixation with fixated entities. In the speech
stream, each word starts at a particular timestamp. In
the gaze stream, each gaze fixation f has a starting
timestamp tf and a duration Tf . Gaze fixations can
have different durations. An entity e on the graphi-
cal display is fixated by gaze fixation f if the area of
e contains the fixation point of f . One gaze fixation
can fall on multiple entities or no entity.
4 Salience Driven Language Modeling
Our goal is to use the domain specific information
about the salient entities on a graphical display, as
indicated by the user?s eye gaze, to help recognition
of the user?s utterances. In particular, we incorporate
this salient domain information in speech recogni-
tion via salience driven language modeling.
285
(a) Raw gaze points (b) Processed gaze fixations
Figure 2: Gaze fixations on a scene
8 596 968 1668 2096 32522692
tf Tf
[19] [ ] [17] [19] [22] [ ] [10]
[11]
[10]
[11]
[10]
[11]
This room has a chandelier
2572 2872 3170 3528 3736
( [19] ? bed_8; [17] ? lamp_2; [22] ? door_1; [10] ? bedroom; [11] ? chandelier_1 )
speech stream
gaze stream
(ms)
(ms)
[fixated entity]
f: gaze fixation
Figure 3: An excerpt of speech and gaze stream data
We first briefly introduce speech recognition. The
task of speech recognition is to, given an observed
spoken utterance O, find the word sequence W ?
such that W ? = argmax
W
p(O|W )p(W ), where
p(O|W ) is the acoustic model and p(W ) is the
language model. The acoustic model provides the
probability of observing the acoustic features given
hypothesized word sequences while the language
model provides the probability of a word sequence.
The language model is represented as:
p(W ) = p(wn1 ) =
n?
k=1
p(wk|w
k?1
1 ) (1)
Using first-order Markov assumption, the above lan-
guage model can be approximated by a bigram
model:
p(wn1 ) =
n?
k=1
p(wk|wk?1) (2)
In the following sections, we first introduce the
salience modeling based on eye gaze, then present
how the gaze-based salience models can be used to
tailor language models.
4.1 Gaze-based Salience Modeling
We first define a gaze fixation set F t0+Tt0 (e), which
contains all gaze fixations that fall on entity e within
a time window t0 ? (t0 + T ):
F t0+Tt0 (e) = {f |f falls on e within t0 ? (t0 + T )}
We model gaze-based salience in two ways.
4.1.1 Gaze Salience Model 1
Salience model 1 is based on the assumption that
when an entity has more gaze fixations on it than
other entities, this entity is more likely attended by
the user and thus has higher salience:
pt0,T (e) =
#elements in F t0+Tt0 (e)
?
e(#elements in F
t0+T
t0 (e))
(3)
Here, pt0,T (e) tells how likely it is that the user is
focusing on entity ewithin time period t0 ? (t0+T )
based on how many gaze fixations are on e among
all gaze fixations that fall on entities within t0 ?
(t0 + T ).
4.1.2 Gaze Salience Model 2
Salience model 2 is based on the assumption that
when an entity has longer gaze fixations on it than
other entities, this entity is more likely attended by
the user and thus has higher salience:
pt0,T (e) =
Dt0+Tt0 (e)
?
e D
t0+T
t0 (e)
(4)
where
Dt0+Tt0 (e) =
?
f?F
t0+T
t0
(e)
Tf (5)
Here, pt0,T (e) tells how likely it is that the user is
focusing on entity e within time period t0 ? (t0+ t)
286
based on how long e has been fixated by gaze fixa-
tions among the overall time length of all gaze fixa-
tions that fall on entities within t0 ? (t0 + T ).
4.2 Salience Driven N-gram Model
Salience models can be incorporated in different lan-
guage models, such as bigram models, class-based
bigram models, and probabilistic context free gram-
mar. Among these language models, the salience
driven bigram model based on deictic gesture has
been shown to achieve best performance on speech
recognition (Qu and Chai, 2006). In our initial in-
vestigation of gaze-based salience, we incorporate
the gaze-based salience in a bigram model.
The salience driven bigram probability is given
by:
ps(wi|wi?1) = (1 ? ?)p(wi|wi?1) +
?
?
e p(wi|wi?1, e)pt0,T (e) (6)
where pt0,T (e) is the salience distribution as mod-
eled in equations (3) and (4). In applying the
salience driven bigram model for speech recogni-
tion, we set t0 as the starting timestamp of the ut-
terance and T as the duration of the utterance. The
priming weight ? decides how much the original
bigram probability will be tailored by the salient
entities indicated by eye gaze. Currently, we set
? = 0.67 empirically. We also tried learning the
priming weight with an EM algorithm. However,
we found out that the learned priming weight per-
formed worse than the empirical one in our exper-
iments. This is probably due to insufficient devel-
opment data. Bigram probabilities p(wi|wi?1) were
estimated by the maximum likelihood estimation us-
ing Katz?s backoff method (Katz, 1987) with a fre-
quency cutoff of 1. The samemethod was used to es-
timate p(wi|wi?1, e) from the users? utterance tran-
scripts with entity annotation of e.
5 Application of Salience Driven LMs
The salience driven language models can be inte-
grated into speech processing in two stages: an early
stage before a word lattice (n-best list) is generated
(Fig.4a), or in a late stage where the word lattice
(n-best list) is post-processed (Fig.4b).
For the early stage integration, the gaze-based
salience driven language model is used together with
word lattice
(n-best list)speech
eye gaze
Speech Decoder
Language 
Model
Acoustic 
Model
(a) Early stage integration
word lattice
(n-best list) n-best list
eye gaze
Rescorer
speech
Speech Decoder
Language 
Model
Acoustic 
Model
Language 
Model
(b) Late stage integration
Figure 4: Integration of gaze-based salience driven
language model in speech processing
the acoustic model to generate the word lattice, typ-
ically by Viterbi search.
For the late stage integration, the gaze-based
salience driven language model is used to rescore the
word lattice generated by a speech recognizer with
a basic language model not involving salience mod-
eling. A* search can be applied to find the n-best
paths in the word lattice.
6 Evaluation
The evaluations were conducted on data collected
from user studies (Sec. 3). We evaluated the gaze-
based salience driven bigram models when applied
for speech recognition at early and late stages.
6.1 Evaluation Results
Users? speech was first segmented, then recognized
by the CMU Sphinx-4 speech recognizer using dif-
ferent language models. Evaluation was done by
a 14-fold cross validation. We compare the per-
formances of the early and late applications of two
gaze-based salience driven language models:
? S-Bigram1 ? salience driven language model
based on salience modeling 1 (Sec. 4.1.1)
? S-Bigram2 ? salience driven language model
based on salience modeling 2 (Sec. 4.1.2)
Table 1 and Table 2 show the results of early and
late application of the salience driven language mod-
els based on eye gaze. We can see that all word error
rates (WERs) are high. In the experiments, users
were instructed to only answer systems questions
one by one. There was no flow of a real conversa-
tion. In this setting, users were more free to express
287
themselves than in the situation where users believed
they were conversing with a machine. Thus, we ob-
serve much longer sentences that often contain dis-
fluencies. Here is one example:
System: ?How big is the bed??
User: ?I would to have to offer a guess that the bed,
if I look the chair that?s beside it [pause] in a rel-
ative angle to the bed, it?s probably six feet long,
possibly, or shorter, slightly shorter.?
The high WER was mainly caused by the com-
plexity and disfluencies of users? speech. Poor
speech recording quality is another reason for the
bad recognition performance. It was found that
the trigram model performed worse than the bigram
model in the experiment. This is probably due to the
sparseness of trigrams in the corpus. The amount of
data available is too small considering the vocabu-
lary size.
Language Model Lattice-WER WER
Bigram 0.613 0.707
Trigram 0.643 0.719
S-Bigram 1 0.605 0.690
S-Bigram 2 0.604 0.689
Table 1: WER of early application of LMs
Language Model Lattice-WER WER
S-Bigram 1 0.643 0.709
S-Bigram 2 0.643 0.710
Table 2: WER of late application of LMs
The S-Bigram1 and S-Bigram2 achieved similar
results in both early application (Table 1) and late
application (Table 2). In early application, the S-
Bigram1 model performed better than the trigram
model (t = 5.24, p < 0.001, one-tailed) and the
bigram model (t = 3.31, p < 0.001, one-tailed).
The S-Bigram2model also performed better than the
trigram model (t = 5.15, p < 0.001, one-tailed)
and the bigram model (t = 3.33, p < 0.001, one-
tailed) in early application. In late application, the
S-Bigram1 model performed better than the trigram
model (t = 2.11, p < 0.02, one-tailed), so did
the S-Bigram2 model (t = 1.99, p < 0.025, one-
tailed). However, compared to the bigram model,
the S-Bigram1 model did not change the recogni-
tion performance significantly (t = 0.38, N.S., two-
tailed) in late application, neither did the S-Bigram2
model (t = 0.50, N.S., two-tailed).
We also compare performances of the salience
driven language models for individual users. In early
application (Fig.5a), both the S-Bigram1 and the S-
Bigram2 model performed better than the baselines
of the bigram and trigrammodels for all users except
user 2 and user 7. T-tests have shown that these are
significant improvements. For user 2, the S-Bigram1
model achieved the sameWER as the bigrammodel.
For user 7, neither of the salience driven language
models improved recognition compared to the bi-
gram model. In late application (Fig.5b), only for
user 3 and user 4, both salience driven language
models performed better than the baselines of the bi-
gram and trigrammodels. These improvements have
also been confirmed by t-tests as significant.
1 2 3 4 5 6 70.4
0.5
0.6
0.7
0.8
0.9
1
User ID
WE
R
bigram trigram s?bigram1 s?bigram2
(a) WER of early application
1 2 3 4 5 6 70.4
0.5
0.6
0.7
0.8
0.9
1
User ID
WE
R
bigram trigram s?bigram1 s?bigram2
(b) WER of Late application
Figure 5: WERs of LMs for individual users
Comparing early and late application of the
salience driven language models, it is observed that
early application performed better than late applica-
tion for all users except user 3 and user 4. T-tests
have confirmed that these differences are significant.
288
It is interesting to see that the effect of gaze-based
salience modeling is different among users. For
two users (i.e., user 3 and user 4), the gaze-based
salience driven language models consistently out-
performed the bigram and trigram models in both
early application and late application. However, for
some other users (e.g., user 7), this is not the case. In
fact, the gaze-based salience driven language mod-
els performed worse than the bigram model. This
observation indicates that during language produc-
tion, a user?s eye gaze is voluntary and unconscious.
This is different from deictic gesture, which is more
intentionally delivered by a user. Therefore, incor-
porating this ?unconscious? mode of modality in
salience modeling requires more in-depth research
on the role of eye gaze in attention prediction during
multimodal human computer interaction.
6.2 Discussion
Gaze-based salience driven language models are
built on the assumption that when a user is fixat-
ing on an entity, the user is saying something re-
lated to the entity. With this assumption, gaze-based
salience driven language models have the potential
to improve speech recognition by biasing the speech
decoder to favor the words that are consistent with
the entity indicated by the user?s eye gaze, especially
when the user?s utterance contains words describing
unique characteristics of the entity. These particular
characteristics could be the entity?s name or physical
properties (e.g., color, material, size).
Utterance: ?a tree growing from the floor?
Gaze salience:
p(bedroom) = 0.2414 p(plant willow) = 0.2414
p(chair soft) = 0.2414 p(door 1) = 0.1378
p(bed 8) = 0.1378
Bigram n-best list:
sheet growing from a four
sheet growing from a for
sheet growing from a floor
. . .
S-Bigram2 n-best list:
a tree growing from the floor
a tree growing from the for
a tree growing from the floor a
. . .
Figure 6: N-best lists of utterance ?a tree growing
from the floor?
Fig.6 shows an example where the S-Bigram2
model in early application improved recognition of
the utterance ?a tree growing from the floor?. In
this example, the user?s gaze fixations accompany-
ing the utterance resulted in a list of candidate enti-
ties with fixating probabilities (cf. Eqn. (4)), among
which entities bedroom and plant willow were as-
signed higher probabilities. Two n-best lists, the Bi-
gram n-best list and the S-Bigram2 n-best list, were
generated by the speech recognizer when the bigram
model and the S-Bigram2 model were applied sep-
arately. The speech recognizer did not get the cor-
rect recognition when the bigram model was used,
but got the correct result when the S-Bigram2 model
was used.
Fig.7a and 7b show the word lattices of the ut-
terance generated by the speech recognizer using
the bigram model and the S-Bigram2 model respec-
tively. The n-best lists in Fig.6 were generated from
those word lattices. In the word lattices, each path
going from the start node<s> to the end node</s>
forms a recognition hypothesis. The bigram proba-
bilities along the edges are in the logarithm of base
10. In the bigram case, the path ?<s> a tree? has a
higher language score (summation of bigram prob-
abilities along the path) than ?<s> sheet?, and ?a
floor? has a higher language score than ?a full?.
However, these correct paths ?<s> a tree? and ?a
floor? (not exactly correct, but better than ?a full?)
do not appear in the best hypothesis in the result-
ing n-best list. This is because the system tries to
find an overall best hypothesis by considering both
language and acoustic score. Because of the noisy
speech, the incorrect hypotheses may happen to have
higher acoustic confidence than the correct ones. Af-
ter tailoring the bigram model with gaze salience,
the salient entity plant willow significantly increases
the probability of ?a tree? (from -1.3594 to -0.9913)
and ?tree growing? (from -3.1009 to -1.1887), while
it decreases the probability of ?sheet growing? (from
-3.0962 to -3.4534). This probability change is made
by the entity conditional probability p(wi|wi?1, e)
in tailoring of bigram by salience (cf. Eqn. (6)).
Probability p(wi|wi?1, e), trained from the anno-
tated utterances, reflects what words are more likely
to be spoken by a user while talking about an entity
e. The increased probabilities of ?a tree? and ?tree
growing? show that word ?tree? appears more likely
than ?sheet? when the user is talking about entity
289
</s>
i
-1.5043
forest
-0.8615
floor -0.3552
four
-0.5322
for
-0.9768
full
-1.9490
a
-3.1284
-2.8274
-3.0035
-2.9066
-3.0035of
-3.3940
-3.2691
-1.0280
from
-3.6386
-3.3376
-3.5137
-3.4168
-3.5137
-1.9339
kind
-0.2312
growing
-2.2662
-3.2942
going
-1.5911sheet
-2.4272
-3.0962
tree -3.1009
-3.5780
a
-1.3594
<s>
-3.9306
-3.0275
-1.5987
(a) Word lattice with bigram model
</s>
a
-1.2861
floor
-0.2570
-1.9165
forest
-0.7782
for
-1.2683
a
-2.4966
-2.9278
-3.6009
the
-1.2151
-3.2468
-3.7353
further
-3.6961
from
-3.9011
-4.2022
-3.3477
-1.9934
-0.1622
growing
-3.4626
-3.6233
tree
-1.1887
sheet
-3.4534
a
-0.9913
<s>
-2.3655
-3.8964
-1.5618
(b) Word lattice with S-Bigram 2
Figure 7: Word lattices of utterance ?a tree growing from the floor?
?plant willow. This is in accordance with our com-
mon sense. Likewise, the salient entity bedroom, of
which floor is a component, makes the probability of
the correct hypothesis ?the floor? much higher than
other hypotheses (?the for? and ?the forest?). These
enlarged language score differences make the cor-
rect hypotheses ?a tree? and ?the floor? win out in
the searching procedure despite the noisy speech.
Utterance: ?I like the picture with like a forest in it?
Gaze salience:
p(bedroom) = 0.5960 p(chandelier 1) = 0.4040
Bigram n-best list:
and i eight that picture rid like got five
and i eight that picture rid identifiable
and i eight that picture rid like got forest
. . .
S-Bigram2 n-best list:
and i that bedroom it like upside
and i that bedroom it like a five
and i that bedroom it like a forest
. . .
Figure 8: N-best lists of utterance ?I like the picture
with like a forest in it?
Unlike the active input mode of deictic gesture,
eye gaze is a passive input mode. The salience in-
formation indicated by eye gaze is not as reliable
as the one indicated by deictic gesture. When the
salient entities indicated by eye gaze are not the
true entities the user is referring to, the salience
driven language model can worsen speech recogni-
tion. Fig.8 shows an example where the S-Bigram2
model in early application worsened the recogni-
tion of a user?s utterance ?I like the picture with like
a forest in it? because of wrong salience informa-
tion. In this example, the user was talking about a
picture entity picture bamboo. However, this entity
was not salient, only entities bedroom and chande-
lier 1 were salient. As a result, the recognition with
the S-Bigram2 model becomes worse than the base-
line. The correct word ?picture? is missing and the
wrong word ?bedroom? appears in the result.
The failure to identify the actual referred entity
picture bamboo as salient in the above example can
also be caused by the visual properties of entities.
Smaller entities on the screen are harder to be fix-
290
ated by eye gaze than larger entities. To address this
issue, more reliable salience modeling that takes into
account the visual features is needed.
7 Conclusion
This paper presents an empirical exploration of in-
corporating eye gaze in spoken language processing
via salience driven language modeling. Our prelim-
inary results have shown the potential of eye gaze in
improving spoken language processing. Neverthe-
less, this exploratory study is only the first step in
our investigation. Many interesting research ques-
tions remain. During human machine conversation,
how is eye gaze aligned with speech production?
How reliable is eye gaze for attention prediction?
Are there any other factors such as interface design
and visual properties that will affect eye gaze behav-
ior and therefore attention prediction? The answers
to these questions will affect how eye gaze should be
appropriately modeled and used for language pro-
cessing.
Eye-tracking systems are no longer bulky, sta-
tionary systems that prevent natural human ma-
chine communication. Recently developed dis-
play mounted gaze-tracking systems (e.g., Tobii) are
completely non-intrusive, can tolerate head motion,
and provide high tracking quality. These features
have been demonstrated in several successful appli-
cations (Duchowski, 2002). Integrating eye tracking
with conversational interfaces is no longer beyond
reach. We believe it is time to conduct systematic
investigations and fully explore the additional chan-
nel provided by eye gaze in improving robustness of
human machine conversation.
8 Acknowledgments
This work was supported by a Career Award IIS-
0347548 and IIS-0535112 from the National Sci-
ence Foundation. The authors would like to thank
Zahar Prasov for his contribution on data collection
and thank anonymous reviewers for their valuable
comments and suggestions.
References
E. Campana, J. Baldridge, J. Dowding, B. Hockey, R. Reming-
ton, and L. Stone. 2001. Using eye movements to determine
referents in a spoken dialogue system. In Proceedings of the
Workshop on Perceptive User Interface.
J. Chai and S. Qu. 2005. A salience driven approach to ro-
bust input interpretation in multimodal conversational sys-
tems. In Proceedings of HLT/EMNLP?05.
A. T. Duchowski. 2002. A breath-first survey of eye tracking
applications. Behavior Research methods, Instruments, and
Computers, 33(4).
J. Eisenstein and C. M. Christoudias. 2004. A salience-based
approach to gesture-speech alignment. In Proceedings of
HLT/NAACL?04.
Z. M. Griffin and K. Bock. 2000. What the eyes say about
speaking. Psychological Science, 11:274?279.
J. M. Henderson and F. Ferreira. 2004. The interface of lan-
guage, vision, and action: Eye movements and the visual
world. New York: Taylor & Francis.
C. Huls, E. Bos, and W. Classen. 1995. Automatic referent res-
olution of deictic and anaphoric expressions. Computational
Linguistics, 21(1):59?79.
R. J. K. Jacob. 1990. What you look is what you get: Eye
movement-based interaction techniques. In Proceedings of
CHI?90.
R. J. K. Jacob. 1995. Eye tracking in advanced interface design.
In W. Barfield and T. Furness, editors, Advanced Interface
Design and Virtual Environments, pages 258?288. Oxford
University Press.
M. Just and P. Carpenter. 1976. Eye fixations and cognitive
processes. Cognitive Psychology, 8:441?480.
S. Katz. 1987. Estimation of probabilities from sparse data for
the language model component of a speech recogniser. IEEE
Trans. Acous., Speech and Sig. Processing, 35(3):400?401.
M. Kaur, M. Termaine, N. Huang, J. Wilder, Z. Gacovski,
F. Flippo, and C. S. Mantravadi. 2003. Where is ?it?? event
synchronization in gaze-speech input systems. In Proceed-
ings of ICMI?03.
A. Kehler. 2000. Cognitive status and form of reference in
multimodal human-computer interaction. In Proceedings of
AAAI?00.
S. Qu and J. Chai. 2006. Salience modeling based on non-
verbal modalities for spoken language understanding. In
Proceedings of ICMI?06.
P. Qvarfordt and S. Zhai. 2005. Conversing with the user based
on eye-gaze patterns. In Proceedings of CHI?05.
D. Roy and N. Mukherjee. 2005. Towards situated speech
understanding: Visual context priming of language models.
Computer Speech and Language, 19(2):227?248.
I. Starker and R. A. Bolt. 1990. A gaze-responsive self-
disclosing display. In Proceedings of CHI?90.
M. K. Tanenhaus, M. J. Spivey-Knowlton, K. M. Eberhard,
and J. E. Sedivy. 1995. Integration of visual and linguis-
tic information in spoken language comprehension. Science,
268:1632?1634.
B. M. Velichkovsky. 1995. Communicating attention-gaze po-
sition transfer in cooperative problem solving. Pragmatics
and Cognition, 3:99?224.
R. Vertegaal. 1999. The gaze groupware system: Mediating
joint attention in multiparty communication and collabora-
tion. In Proceedings of CHI?99.
S. Zhai, C. Morimoto, and S. Ihde. 1999. Manual and gaze
input cascaded (magic) pointing. In Proceedings of CHI?99.
291
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 188?195,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
The Role of Interactivity in Human-Machine Conversation for Automatic
Word Acquisition
Shaolin Qu Joyce Y. Chai
Department of Computer Science and Engineering
Michigan State University
East Lansing, MI 48824
{qushaoli,jchai}@cse.msu.edu
Abstract
Motivated by the psycholinguistic finding
that human eye gaze is tightly linked to
speech production, previous work has ap-
plied naturally occurring eye gaze for au-
tomatic vocabulary acquisition. However,
unlike in the typical settings for psycholin-
guistic studies, eye gaze can serve differ-
ent functions in human-machine conver-
sation. Some gaze streams do not link
to the content of the spoken utterances
and thus can be potentially detrimental to
word acquisition. To address this prob-
lem, this paper investigates the incorpo-
ration of interactivity in identifying the
close coupling of speech and gaze streams
for word acquisition. Our empirical re-
sults indicate that automatic identification
of closely coupled gaze-speech streams
leads to significantly better word acquisi-
tion performance.
1 Introduction
Spoken conversational interfaces have become in-
creasingly important in many applications such
as remote interaction with robots (Lemon et al,
2002), intelligent space station control (Aist et
al., 2003), and automated training and educa-
tion (Razzaq and Heffernan, 2004). As in any con-
versational system, one major bottleneck in con-
versational interfaces is robust language interpre-
tation. To address this problem, previous multi-
modal conversational systems have utilized pen-
based or deictic gestures (Bangalore and John-
ston, 2004; Qu and Chai, 2006) to improve in-
terpretation. Besides gestures, eye movements
that naturally occur during interaction provide an-
other important channel for language understand-
ing, for example, reference resolution (Byron et
al., 2005; Prasov and Chai, 2008). Recent work
has also shown that what users look at on the inter-
face (e.g., natural scenes or generated graphic dis-
plays) during speech production provides unique
opportunities for word acquisition, namely auto-
matically acquiring semantic meanings of spoken
words by grounding them to visual entities (Liu
et al, 2007) or domain concepts (Qu and Chai,
2008).
Psycholinguistic studies have shown that eye
gaze indicates a person?s attention (Just and Car-
penter, 1976), and eye movement can facilitate
spoken language comprehension (Tanenhaus et
al., 1995; Eberhard et al, 1995). It has been
found that users? eyes move to the mentioned ob-
ject directly before speaking a word (Meyer et
al., 1998; Rayner, 1998; Griffin and Bock, 2000).
This parallel behavior of eye gaze and speech pro-
duction motivates our previous work on word ac-
quisition (Liu et al, 2007; Qu and Chai, 2008).
However, in interactive conversation, human gaze
behavior is much more complex than in the typ-
ical controlled settings used in psycholinguistic
studies. There are different types of eye move-
ments (Kahneman, 1973). The naturally occur-
ring eye gaze during speech production may serve
different functions, for example, to engage in the
conversation or to manage turn taking (Nakano et
al., 2003). Furthermore, while interacting with a
graphic display, a user could be talking about ob-
jects that were previously seen on the display or
something completely unrelated to any object the
user is looking at. Therefore using every speech-
gaze pair for word acquisition can be detrimental.
The type of gaze that is mostly useful for word
acquisition is the kind that reflects the underlying
attention and tightly links to the content of the co-
occurring speech. Thus, one important question
is how to identify the closely coupled speech and
gaze streams to improve word acquisition.
To address this question, we develop an ap-
proach that incorporates interactivity (e.g., speech,
188
user activity, conversation context) with eye gaze
to identify closely coupled speech and gaze
streams. We further use the identified speech
and gaze streams to acquire words with a trans-
lation model. Our empirical evaluation demon-
strates that automatic identification of closely cou-
pled gaze-speech streams can lead to significantly
better word acquisition performance.
2 Related Work
Previous work has explored word acquisition by
grounding words to visual entities. In (Roy and
Pentland, 2002), given speech paired with video
images of objects, mutual information between
auditory and visual signals was used to acquire
words by associating acoustic phone sequences
with the visual prototypes (e.g., color, size, shape)
of objects. Given parallel pictures and descrip-
tion texts, generative models were used to acquire
words by associating words with image regions in
(Barnard et al, 2003). Different from this previous
work, in our work, the visual attention foci accom-
panying speech are indicated by eye gaze. As an
implicit and subconscious input, eye gaze brings
additional challenges in word acquisition.
Eye gaze has been explored for word acqui-
sition in previous work. In (Yu and Ballard,
2004), given speech paired with eye gaze and
video images, a translation model was used to
acquire words by associating acoustic phone se-
quences with visual representations of objects and
actions. Word acquisition from transcribed speech
and eye gaze during human-machine conversa-
tion has been investigated recently. In (Liu et
al., 2007), a translation model was developed to
associate words with visual objects on a graphi-
cal display. In our previous work (Qu and Chai,
2008), enhanced translation models incorporat-
ing speech-gaze temporal information and domain
knowledge were developed to improve word ac-
quisition. However, none of these previous works
has investigated the role of interactivity in word
acquisition, which is the focus of this paper.
3 Data Collection
We collected speech and eye gaze data through
user studies. This data set is different from the data
set used in our previous work (Qu and Chai, 2008).
The difference lies in two aspects: 1) the data for
this investigation was collected during mixed ini-
tiative human-machine conversation whereas the
data in (Qu and Chai, 2008) was based only on
question and answering; 2) user studies were con-
ducted in a more complex domain for this investi-
gation, which resulted in a richer data set that con-
tains a larger vocabulary.
3.1 Domain
Figure 1: Treasure hunting domain
Figure 1 shows the 3D treasure hunting domain
used in our work. In this application, the user
needs to consult with a remote ?expert? (i.e., an ar-
tificial system) to find hidden treasures in a castle
with 115 3D objects. The expert has some knowl-
edge about the treasures but can not see the cas-
tle. The user has to talk to the expert for advice
regarding finding the treasures. The application is
developed based on a game engine and provides an
immersive environment for the user to navigate in
the 3D space. During the experiment, each user?s
speech was recorded, and the user?s eye gaze was
captured by a Tobii eye tracker.
3.2 Data Preprocessing
From 20 users? experiments, we collected 3709 ut-
terances with accompanying gaze fixations. We
transcribed the collected speech. The vocabulary
size of the speech transcript is 1082, among which
227 are either nouns or adjectives. The user?s
speech was also automatically recognized online
by the Microsoft speech recognizer with a word
error rate (WER) of 48.1% for the 1-best recog-
nition. The vocabulary size of the 1-best speech
recognition is 3041, among which 1643 are either
nouns or adjectives.
The collected speech and gaze streams were au-
tomatically paired together by the system. Each
time the system detected a sentence boundary (in-
dicated by a long pause of 500 milliseconds) of the
user?s speech, it paired the recognized speech with
the gaze fixations that the system had been ac-
cumulating since the previously detected sentence
189
[table_vase]
speech stream
gaze stream
[fixated entity]
ts te
gaze fixation
[vase_purple] [vase_greek3][vase_greek3] [vase_greek3][vase_greek3]
There?s orangevase in anpurplea face
Figure 2: Accompanying gaze fixations and the 1-best recognition of a user?s utterance ?There?s a purple
vase and an orange vase.? (There are two incorrectly recognized words ?in? and ?face? in the 1-best
recognition)
boundary. Figure 2 shows a pair of user speech
and accompanying stream of gaze fixations. In
the speech stream, each spoken word was times-
tamped by the speech recognizer. In the gaze
stream, each gaze fixation has a starting timestamp
ts and an ending timestamp te provided by the eye
tracker. Each gaze fixation results in a fixated en-
tity (3D object). When multiple entities are fixated
by one gaze fixation due to the overlapping of en-
tities, the one in the forefront is chosen.
Given the paired speech and gaze streams, we
build a set of parallel word sequence and gaze fix-
ated entity sequence {(w, e)} for the task of word
acquisition. In section 6, we will evaluate word
acquisition in two settings: 1) word sequence w
contains all of the nouns/adjectives in the speech
transcript, and 2) w contains all of the recognized
nouns/adjectives in the 1-best speech recognition.
4 Word Acquisition With Eye Gaze
The task of word acquisition in our application is
to ground words to the visual entities. Specifi-
cally, given the parallel word and entity sequences
{(w, e)}, we want to find the best match between
the words and the entities. Following our previ-
ous work (Qu and Chai, 2008), we formulate word
acquisition as a translation problem and use trans-
lation models for word acquisition. For each en-
tity e, we first estimate the word-entity association
probability p(w|e) with a translation model, then
choose the words with the highest probabilities as
acquired words for e.
Inspired by the psycholinguistic findings that
users? eyes move to the mentioned object before
speaking a word (Meyer et al, 1998; Rayner,
1998; Griffin and Bock, 2000), in our previous
work (Qu and Chai, 2008), we have incorpo-
rated the gaze-speech temporal information in the
translation model as follows (referred as Model-2t
through the rest of this paper):
p(w|e) =
m?
j=1
l?
i=0
pt(aj = i|j, e,w)p(wj |ei)
where l and m are the lengths of entity and word
sequences respectively. In this equation, pt(aj =
i|j, e,w) is the temporal alignment probability
representing the probability thatwj is aligned with
ei, which is further defined by:
pt(aj = i|j, e,w) =
{
0 d(ei, wj) > 0
exp[??d(ei,wj)]?
i exp[??d(ei,wj)]
d(ei, wj) ? 0
where ? is a scaling factor, and d(ei, wj) is the
temporal distance between ei and wj . Based on
the psycholinguistic finding that eye gaze happens
before a spoken word, wj is not allowed to be
aligned with ei when wj happens earlier than ei
(i.e., d(ei, wj) > 0). When wj happens no earlier
than ei (i.e., d(ei, wj) ? 0), the closer they are, the
more likely they are aligned. An EM algorithm is
used to estimate p(w|e) and ? in the model.
Our evaluation in (Qu and Chai, 2008) has
shown that Model-2t that incorporates temporal
alignment between speech and eye gaze achieves
significantly better word acquisition performance
compared to the model where no temporal align-
ment is introduced. Therefore, this model is used
for the investigation in this paper.
5 Identification of Closely Coupled
Gaze-Speech Pairs
Successful word acquisition with the translation
models relies on the tight coupling between the
gaze fixations and the speech content. As men-
tioned earlier, not all gaze-speech pairs have this
tight coupling. In a gaze-speech pair, if the speech
190
does not have any word that relates to any of the
gaze fixated entities, this instance only adds noise
to word acquisition. Therefore, we should identify
the closely coupled gaze-speech pairs and only use
them for word acquisition.
In this section, we first describe the feature ex-
traction, then evaluate the application of a logis-
tic regression classifier to predict whether a gaze-
speech pair is a closely coupled gaze-speech in-
stance ? an instance where at least one noun or
adjective in the speech stream describes some en-
tity fixated by the gaze stream. For the training of
the classifier, we manually labeled each instance
as either a coupled instance or not based on the
speech transcript and the gaze fixations.
5.1 Feature Extraction
For a gaze-speech instance, the following sets of
features are automatically extracted.
5.1.1 Speech Features (S)
The following features are extracted from
speech:
? cw ? count of nouns and adjectives.
More nouns and adjectives are expected in
the user?s utterance describing entities.
? cw/ls ? normalized noun/adjective count.
The effect of speech length ls on cw is con-
sidered.
5.1.2 Gaze Features (G)
For each fixated entity ei, let lie be its temporal
fixation length. Note that several gaze fixations
may have the same fixated entity, lie is the total
length of all the gaze fixations that fixate on entity
ei. We extract the following features from gaze
stream:
? ce ? count of different gaze fixated entities.
Fewer fixated entities are expected when the
user is describing entities while looking at
them.
? ce/ls ? normalized entity count.
The effect of temporal spoken utterance
length ls on ce is considered.
? maxi(lie) ? maximal fixation length.
At least one fixated entity?s fixation is ex-
pected to be long enough when the user is
describing entities while looking at them.
? mean(lie) ? average fixation length.
The average gaze fixation length is expected
to be longer when the user is describing enti-
ties while looking at them.
? var(lie) ? variance of fixation lengths.
The variance of the fixation lengths is ex-
pected to be smaller when the user is describ-
ing entities while looking at them.
The number of gaze fixated entities is not only
determined by the user?s eye gaze, but also af-
fected by the visual scene. Let cse be the count
of all the entities that have been visible during the
time period concurrent with the gaze stream. We
also extract the following scene related feature:
? ce/cse ? scene-normalized fixated entity
count.
The effect of the visual scene on ce is consid-
ered.
5.1.3 User Activity Features (UA)
While interacting with the system, the user?s ac-
tivity can also be helpful in determining whether
the user?s eye gaze is tightly linked to the content
of the speech. The following features are extracted
from the user?s activities:
? maximal distance of the user?s movements ?
the maximal change of user position (3D co-
ordinates) during speech.
The user is expected to move within a smaller
range while looking at entities and describing
them.
? variance of the user?s positions
The user is expected to move less frequently
while looking at entities and describing them.
5.1.4 Conversation Context Features (CC)
While talking to the system (i.e., the ?expert?),
the user?s language and gaze behavior are influ-
enced by the state of the conversation. For each
gaze-speech instance, we use the previous sys-
tem response type as a nominal feature to predict
whether this is a closely coupled gaze-speech in-
stance.
In our treasure hunting domain, there are 8 types
of system responses in 2 categories:
System Initiative Responses:
? specific-see ? the system asks whether the
user sees a certain entity, e.g., ?Do you see
another couch??.
? nonspecific-see ? the system asks whether the
user sees anything, e.g., ?Do you see any-
thing else??, ?Tell me what you see?.
191
? previous-see ? the system asks whether the
user has previously seen something, e.g.,
?Have you previously seen a similar object??.
? describe ? the system asks the user to de-
scribe in detail what the user sees, e.g., ?De-
scribe it?, ?Tell me more about it?.
? compare ? the system asks the user to com-
pare what the user sees, e.g., ?Compare these
objects?.
? repair-request ? the system asks the user to
make clarification, e.g., ?I did not understand
that?, ?Please repeat that?.
? action-request ? the system asks the user to
take action, e.g., ?Go back?, ?Try moving it?.
User Initiative Responses:
? misc ? the system hands the initiative back
to the user without specifying further require-
ments, e.g., ?I don?t know?, ?Yes?.
5.2 Evaluation of Gaze-Speech Identification
Given the extracted features and the ?closely cou-
pled? label of each instance in the training set, we
train a logistic regression classifier (le Cessie and
van Houwelingen, 1992) to predict whether an in-
stance is a closely coupled gaze-speech instance.
Since the goal of identifying closely coupled
gaze-speech instances is to improve word acqui-
sition and we are only interested in acquiring
nouns and adjectives, only the instances with rec-
ognized nouns/adjectives are used for training the
logistic regression classifier. Among the 2969 in-
stances with recognized nouns/adjectives and gaze
fixations, 2002 (67.4%) instances are labeled as
?closely coupled?. The prediction is evaluated by
a 10-fold cross validation.
Feature sets Precision Recall
Null (baseline) 0.674 1
S 0.686 0.995
G 0.707 0.958
UA 0.704 0.942
CC 0.688 0.936
G + UA 0.719 0.948
G + UA + S 0.741 0.908
G + UA + CC 0.731 0.918
G + UA + CC + S 0.748 0.899
Table 1: Gaze-speech prediction performance for
the instances with 1-best speech recognition
Table 1 shows the prediction precision and re-
call when different sets of features are used. As
seen in the table, as more features are used, the
prediction precision goes up and the recall goes
down. It is important to note that prediction pre-
cision is more critical than recall for word acqui-
sition when sufficient amount data is available.
Noisy instances where the gaze is not coupled with
the speech content will only hurt word acquisi-
tion since they will guide the translation models
to ground words to the wrong entities. Although
higher recall can be helpful, its effect is expected
to be reduced when more data becomes available.
The results show that speech features (S) and
conversation context features (CC), when used
alone, do not improve prediction precision much
compared to the baseline of predicting all in-
stances as closely coupled (with a precision of
67.4%). When used alone, gaze features (G) and
user activity features (UA) are the two most use-
ful feature sets for increasing prediction precision.
When they are used together, the prediction pre-
cision is further increased. Adding either speech
features or conversation context features to gaze
and user activity features (G + UA + S/CC) further
increases the prediction precision. Using all fea-
tures (G + UA + CC + S) achieves the highest pre-
diction precision, which is significantly better than
the baseline: z = 5.93, p < 0.001. Therefore, we
choose to use all feature sets to identify the closely
coupled gaze-speech instances for word acquisi-
tion.
To compare the effects of the automatic gaze-
speech identification on word acquisition from
various speech input (1-best speech recognition,
speech transcript), we also use the logistic re-
gression classifier with all feature sets to iden-
tify the closely coupled gaze-speech instances for
the instances with speech transcript. For the in-
stances with speech transcript, there are 2948 in-
stances with nouns/adjectives and gaze fixations,
2128 (72.2%) of them being labeled as ?closely
coupled?. The prediction precision is 77.9% and
the recall is 93.8%. The prediction precision is
significantly better than the baseline of predicting
all instances as coupled: z = 4.92, p < 0.001.
6 Evaluation of Word Acquisition
Every conversational system has an initial vocabu-
lary where words are associated with domain con-
cepts of entities. In our evaluation, we assume that
192
the system?s vocabulary has one default word for
each entity that indicates the semantic type of the
entity. For example, the word ?barrel? is the de-
fault word for the entity barrel. For each entity,
we only evaluate those new words that are not in
the system?s vocabulary.
The acquired words are evaluated against the
?gold standard? words that were manually com-
piled for each entity and its properties based on
all users? speech transcripts. For the 115 entities
in our domain, each entity has 1 to 20 ?gold stan-
dard? words. The average number of ?gold stan-
dard? words for an entity is 6.7.
6.1 Evaluation Metrics
We evaluate the n-best acquired words (words
grounded to domain concepts of entities) using
precision, recall, and F-measure. When a differ-
ent n is chosen, we will have different precision,
recall, and F-measure.
We also evaluate the whole ranked candidate
word list on Mean Reciprocal Rank Rate (MRRR)
as in (Qu and Chai, 2008):
MRRR =
?
e
?Ne
i=1 1/index(w
i
e)
?Ne
i=1 1/i
#e
where Ne is the number of all ?gold standard?
words {wie} for entity e, index(w
i
e) is the index
of word wie in the ranked list of candidate words
for entity e.
MRRR measures how close the ranks of the
?gold standard? words in the candidate word lists
are to the best-case scenario where the top Ne
words are the ?gold standard? words for e. The
higher the MRRR, the better is the acquisition per-
formance.
6.2 Evaluation Results
We evaluate the effect of the closely coupled gaze-
speech instances on word acquisition from the 1-
best speech recognition and speech transcript. The
predicted closely coupled gaze-speech instances
are generated by a 10-fold cross validation with
the logistic regression classifier.
Figure 3 shows the precision, recall, and F-
measure of the n-best words acquired from 1-best
speech recognition by Model-2t using all instances
(all), predicted coupled instances (predicted), and
true (manually labeled) coupled instances (true).
As shown in the figure, using predicted coupled
instances achieves consistently better performance
1 2 3 4 5 6 7 8 9 10
0.2
0.25
0.3
0.35
0.4
0.45
n-best
Pre
cis
ion
all
predicted
true
(a) precision
1 2 3 4 5 6 7 8 9 100.05
0.1
0.15
0.2
0.25
0.3
0.35
n-best
Re
cal
l
all
predicted
true
(b) recall
1 2 3 4 5 6 7 8 9 100.1
0.15
0.2
0.25
0.3
n-best
F-m
eas
ure
all
predicted
true
(c) F-measure
Figure 3: Performance of word acquisition on 1-
best speech recognition
than using all instances. These results show that
the identification of coupled gaze-speech predic-
tion helps word acquisition. When the true cou-
pled instances are used, the performance is further
improved. This means that reliable identification
of coupled gaze-speech instances can lead to bet-
ter word acquisition.
Figure 4 shows the precision, recall, and F-
measure of the n-best words acquired from speech
transcript by Model-2t using all instances, pre-
dicted coupled instances, and true coupled in-
stances. Consistent with the performance based
on the 1-best speech recognition, we can observe
193
1 2 3 4 5 6 7 8 9 100.25
0.3
0.35
0.4
0.45
0.5
0.55
n-best
Pre
cis
ion
all
predicted
true
(a) precision
1 2 3 4 5 6 7 8 9 100.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
0.55
n-best
Re
cal
l
all
predicted
true
(b) recall
1 2 3 4 5 6 7 8 9 100.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
n-best
F-m
eas
ure
all
predicted
true
(c) F-measure
Figure 4: Performance of word acquisition on
speech transcript
that automatic identification of coupled instances
results in better word acquisition performance and
using the true coupled instances results in even
better performance.
Table 2 presents the MRRRs achieved by
Model-2t when words are acquired from differ-
ent speech input (speech transcript, 1-best recog-
nition) with different set of instances (all in-
stances, predicted coupled instances, true coupled
instances). These results also show the consis-
tent behavior. Using predicted coupled instances
achieves significantly better MRRR than using all
instances no matter the words are acquired from 1-
best speech recognition (t = 2.59, p < 0.006) or
speech transcript(t = 3.15, p < 0.002). When the
true coupled instances are used, the performances
are further improved for both 1-best recognition
(t = 2.29, p < 0.013) and speech transcript
(t = 5.21, p < 0.001) compared to using pre-
dicted coupled instances.
Instances All Predicted True
Transcript 0.462 0.480 0.526
1-best reco 0.343 0.369 0.390
Table 2: MRRRs based on different data set
The quality of speech recognition is critical to
word acquisition performance. Comparing word
acquisition based on speech transcript and 1-best
speech recognition, as expected, word acquisition
performance on speech transcript is much better
than on recognized speech. However, the acqui-
sition performance based on speech transcript is
still comparably low. For example, the recall of
acquired words is still below 55% even when the
10 best word candidates are acquired for each en-
tity. This is mainly due to the scarcity of words.
Many words appear less than three times in the
data, which makes them unlikely to be associated
with any entity by the translation model. When
more data is available, we expect to see better ac-
quisition performance.
Note that our current evaluation is based on a
two-stage approach, i.e., first identifying closely-
coupled streams based on supervised classifica-
tion and then automatically establishing mappings
between words and entities in an unsupervised
manner. There could be other approaches to ad-
dress the word acquisition problem (e.g., super-
vised learning to directly identify whether a word
is mapped to an object). Our two-stage approach
has the advantage of requiring minimum super-
vision since the models learned from the first
stage is application-independent and is potentially
portable to different domains.
7 Conclusions
Unlike in the typical settings for psycholinguistic
studies, human eye gaze can serve different func-
tions during human machine conversation. Some
gaze and speech streams may not be tightly cou-
pled and thus can be detrimental to word acqui-
sition. Therefore, this paper describes an ap-
proach that incorporates features from the interac-
194
tion context to identify closely coupled gaze and
speech streams. Our empirical results indicate
that the word acquisition based on these automati-
cally identified gaze-speech streams achieves sig-
nificantly better performance than the word acqui-
sition based on all gaze-speech streams. Our fu-
ture work will combine gaze-based word acquisi-
tion with multiple speech recognition hypotheses
(e.g., word lattices) to further improve word acqui-
sition and language interpretation performance.
Acknowledgments
This work was supported by grants IIS-0347548
and IIS-0535112 from the National Science Foun-
dation. We thank anonymous reviewers for their
valuable comments and suggestions.
References
G. Aist, J. Dowding, B. A. Hockey, M. Rayner,
J. Hieronymus, D. Bohus, B. Boven, N. Blaylock,
E. Campana, S. Early, G. Gorrell, and S. Phan.
2003. Talking through procedures: An intelligent
space station procedure assistant. In Proceedings of
the 10th Conference of the European Chapter of the
Association for Computational Linguistics (EACL).
S. Bangalore and M. Johnston. 2004. Robust multi-
modal understanding. In Proceedings of the Inter-
national Conference on Acoustics, Speech, and Sig-
nal Processing (ICASSP).
K. Barnard, P. Duygulu, N. de Freitas, D. Forsyth,
D. Blei, and M. Jordan. 2003. Matching words and
pictures. Journal of Machine Learning Research,
3:1107?1135.
D. Byron, T. Mampilly, V. Sharma, and T. Xu. 2005.
Utilizing visual attention for cross-modal corefer-
ence interpretation. In Proceedings of the Fifth
International and Interdisciplinary Conference on
Modeling and Using Context (CONTEXT-05), pages
83?96.
K. Eberhard, M. Spivey-Knowiton, J. Sedivy, and
M. Tanenhaus. 1995. Eye movements as a win-
dow into real-time spoken language comprehension
in natural contexts. Journal of Psycholinguistic Re-
search, 24:409?436.
Z. Griffin and K. Bock. 2000. What the eyes say about
speaking. Psychological Science, 11:274?279.
M. Just and P. Carpenter. 1976. Eye fixations and cog-
nitive processes. Cognitive Psychology, 8:441?480.
D. Kahneman. 1973. Attention and Effort. Prentice-
Hall, Inc., Englewood Cliffs.
S. le Cessie and J. van Houwelingen. 1992. Ridge
estimators in logistic regression. Applied Statistics,
41(1):191?201.
O. Lemon, A. Gruenstein, and S. Peters. 2002. Col-
laborative activities and multitasking in dialogue
systems. Traitement Automatique des Langues,
43(2):131?154.
Y. Liu, J. Chai, and R. Jin. 2007. Automated vocab-
ulary acquisition and interpretation in multimodal
conversational systems. In Proceedings of the 45th
Annual Meeting of the Association of Computational
Linguistics (ACL).
A. Meyer, A. Sleiderink, and W. Levelt. 1998. View-
ing and naming objects: eye movements during
noun phrase production. Cognition, 66(22):25?33.
Y. Nakano, G. Reinstein, T. Stocky, and J. Cassell.
2003. Towards a model of face-to-face grounding.
In Proceedings of the Annual Meeting of the Associ-
ation for Computational Linguistics (ACL).
Z. Prasov and J. Chai. 2008. What?s in a gaze? the role
of eye-gaze in reference resolution in multimodal
conversational interfaces. In Proceedings of ACM
12th International Conference on Intelligent User
interfaces (IUI).
S. Qu and J. Chai. 2006. Salience modeling based
on non-verbal modalities for spoken language un-
derstanding. In Proceedings of the International
Conference on Multimodal Interfaces (ICMI), pages
193?200.
S. Qu and J. Chai. 2008. Incorporating temporal and
semantic information with eye gaze for automatic
word acquisition in multimodal conversational sys-
tems. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 244?253.
K. Rayner. 1998. Eye movements in reading and in-
formation processing - 20 years of research. Psy-
chological Bulletin, 124(3):372?422.
L. Razzaq and N. Heffernan. 2004. Tutorial dialog in
an equation solving intelligent tutoring system. In
Proceedings of the Workshop on Dialog-based In-
telligent Tutoring Systems: State of the art and new
research directions.
D. Roy and A. Pentland. 2002. Learning words from
sights and sounds, a computational model. Cogni-
tive Science, 26(1):113?146.
M. Tanenhaus, M. Spivey-Knowiton, K. Eberhard, and
J. Sedivy. 1995. Integration of visual and linguis-
tic information in spoken language comprehension.
Science, 268:1632?1634.
C. Yu and D. Ballard. 2004. A multimodal learning
interface for grounding spoken language in sensory
perceptions. ACM Transactions on Applied Percep-
tions, 1(1):57?80.
195
