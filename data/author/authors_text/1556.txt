Proceedings of the Workshop on Language Technology for Cultural Heritage Data (LaTeCH 2007), pages 33?40,
Prague, 28 June 2007. c?2007 Association for Computational Linguistics
The Latin Dependency Treebank in a Cultural Heritage Digital Library
David Bamman
The Perseus Project
Tufts University
Medford, MA
david.bamman@tufts.edu
Gregory Crane
The Perseus Project
Tufts University
Medford, MA
gregory.crane@tufts.edu
Abstract
This paper describes the mutually benefi-
cial relationship between a cultural heritage
digital library and a historical treebank: an
established digital library can provide the
resources and structure necessary for effi-
ciently building a treebank, while a tree-
bank, as a language resource, is a valuable
tool for audiences traditionally served by
such libraries.
1 Introduction
The composition of historical treebanks is funda-
mentally different from that of modern ones. While
modern treebanks are generally comprised of news-
paper articles,1 historical treebanks are built from
texts that have been the focus of study for centuries,
if not millennia. The Penn-Helsinki Parsed Cor-
pus of Middle English (Kroch and Taylor, 2000),
for example, includes Chaucer?s 14th-century Par-
son?s Tale, while the York Poetry Corpus (Pintzuk
and Leendert, 2001) includes the entire text of Be-
owulf. The scholarship that has attended these texts
since their writing has produced a wealth of contex-
tual materials, including commentaries, translations,
and linguistic resources.
1To name just three, the Penn Treebank (Marcus et al, 1994)
is comprised of texts from the Wall Street Journal; the Ger-
man TIGER Treebank (Brants et al, 2002) is built from texts
taken from the Frankfurter Rundschau; and the Prague De-
pendency Treebank (Hajic?, 1998) includes articles from sev-
eral daily newspapers (Lidove? noviny and Mlada? fronta Dnes), a
business magazine (C?eskomoravsky? Profit) and a scientific jour-
nal (Vesm??r).
For the past twenty years, the Perseus digital li-
brary (Crane, 1987; Crane et al, 2001) has collected
materials of this sort to create an open reading envi-
ronment for the study of Classical texts. This envi-
ronment presents the Greek or Latin source text and
contextualizes it with secondary publications (e.g.,
translations, commentaries, references in dictionar-
ies), along with a morphological analysis of every
word in the text and variant manuscript readings as
well (when available).
We have recently begun work on syntactically an-
notating the texts in our collection to create a Latin
Dependency Treebank. In the course of developing
this treebank, the resources already invested in the
digital library have been crucial: the digital library
provides a modular structure on which to build addi-
tional services, contains a large corpus of Classical
source texts, and provides a wealth of contextual in-
formation for annotators who are non-native speak-
ers of the language.
In this the digital library has had a profound im-
pact on the creation of our treebank, but the influ-
ence goes both ways. The digital library is a heav-
ily trafficked website with a wide range of users, in-
cluding professional scholars, students and hobby-
ists. By incorporating the treebank as a language
resource into this digital library, we have the poten-
tial to introduce a fundamental NLP tool to an audi-
ence outside the traditional disciplines of computer
science or computational linguistics that would nor-
mally use it. Students of the language can profit
from the syntactic information encoded in a tree-
bank, while traditional scholars can benefit from the
textual searching it makes possible as well.
33
Figure 1: A screenshot of Vergil?s Aeneid from the Perseus digital library.
2 The Perseus Digital Library
Figure 1 shows a screenshot from our digital library.
In this view, the reader is looking at the first seven
lines of Vergil?s Aeneid. The source text is provided
in the middle, with contextualizing information fill-
ing the right column. This information includes:
? Translations. Here two English translations
are provided, one by the 17th-century English
poet John Dryden and a more modern one by
Theodore Williams.
? Commentaries. Two commentaries are also
provided, one in Latin by the Roman grammar-
ian Servius, and one in English by the 19th-
century scholar John Conington.
? Citations in reference works. Classical refer-
ence works such as grammars and lexica of-
ten cite particular passages in literary works as
examples of use. Here, all of the citations to
any word or phrase in these seven lines are pre-
sented at the right.
Additionally, every word in the source text is
linked to its morphological analysis, which lists
every lemma and morphological feature associated
with that particular word form. Here the reader has
clicked on arma in the source text. This tool reveals
that the word can be derived from two lemmas (the
verb armo and the noun arma), and gives a full mor-
phological analysis for each. A recommender sys-
tem automatically selects the most probable analysis
for a word given its surrounding context, and users
can also vote for the form they think is correct.2
3 Latin Dependency Treebank
Now in version 1.3, the Latin Dependency Treebank
is comprised of excerpts from four texts: Cicero?s
Oratio in Catilinam, Caesar?s Commentarii de Bello
Gallico, Vergil?s Aeneid and Jerome?s Vulgate.
Since Latin has a highly flexible word order, we
have based our annotation style on the dependency
grammar used by the Prague Dependency Tree-
bank (PDT) (Hajic?, 1998) for Czech (another non-
projective language) while tailoring it for Latin via
2These user contributions have the potential to significantly
improve the morphological tagging of these texts: any single
user vote assigns the correct morphological analysis to a word
89% of the time, while the recommender system does so with
an accuracy of 76% (Crane et al, 2006).
34
Date Author Words
63 BCE Cicero 1,189
51 BCE Caesar 1,486
19 BCE Vergil 2,647
405 CE Jerome 8,382
Total: 13,683
Table 1: Treebank composition by author.
the grammar of Pinkster (1990).3
In addition to the index of its syntactic head and
the type of relation to it, each word in the treebank
is also annotated with the lemma from which it is
inflected and its morphological code. We plan to re-
lease the treebank incrementally with each new ma-
jor textual addition (so that version 1.4, for instance,
will include the treebank of 1.3 plus Sallust?s Bellum
Catilinae, the text currently in production).
4 The Influence of a Digital Library
A cultural heritage digital library has provided a fer-
tile ground for our historical treebank in two funda-
mental ways: by providing a structure on which to
build new services and by providing reading support
to expedite the process of annotation.
4.1 Structure
By anchoring the treebank in a cultural heritage dig-
ital library, we are able to take advantage of a struc-
tured reading environment with canonical standards
for the presentation of text and a large body of dig-
itized resources, which include XML source texts,
morphological analyzers, machine-readable dictio-
naries, and an online user interface.
Texts. Our digital library contains 3.4 million
words of Latin source texts (along with 4.9 mil-
lion words of Greek). The texts are all public-
domain materials that have been scanned, OCR?d
and formatted into TEI-compliant XML. The value
of this prior labor is twofold: most immediately,
the existence of clean, digital editions of these
texts has saved us a considerable amount of time
and resources, as we would otherwise have to
3We are also collaborating with other Latin treebanks (no-
tably the Index Thomisticus on the works of Thomas Aquinas)
to create a common set of annotation guidelines to be used as a
standard for Latin of any period (Bamman et al, 2007).
create them before annotating them syntactically;
but their encoding as repurposeable XML docu-
ments in a larger library also allows us to refer
to them under standardized citations. The pas-
sage of Vergil displayed in Figure 1 is not simply
a string of unstructured text; it is a subdocument
(Book=1:card=1) that is itself part of a larger doc-
ument object (Perseus:text:1999.02.0055), with sis-
ters (Book=1:card=8) and children of its own (e.g.,
line=4). This XML structure allows us to situate any
given treebank sentence within its larger context.
Morphological Analysis. As a highly inflected
language, Latin has an intricate morphological sys-
tem, in which a full morphological analysis is the
product of nine features: part of speech, person,
number, tense, mood, voice, gender, case and de-
gree. Our digital library has included a morphologi-
cal analyzer from its beginning. This resource maps
an inflected form of a word (such as arma above)
to all of the possible analyses for all of the dictio-
nary entries associated with it. In addition to provid-
ing a common morphological standard, this mapping
greatly helps to constrain the problem of morpho-
logical tagging (selecting the correct form from all
possible forms), since a statistical tagger only needs
to consider the morphological analyses licensed by
the inflection rather than all possible combinations.
User interface. The user interface of our library
is designed to be modular, since different texts have
different contextual resources associated with them
(while some have translations, others may have
commentaries). This modularity allows us to easily
introduce new features, since the underlying archi-
tecture of the page doesn?t change ? a new feature
can simply be added.
Figure 2 presents a screenshot of the digital li-
brary with an annotation tool built into the inter-
face. In the widget on the right, the source text in
view (the first chunk of Tacitus? Annales) has been
automatically segmented into sentences; an annota-
tor can click on any sentence to assign it a syntac-
tic annotation. Here the user has clicked on the first
sentence (Vrbem Romam a principio reges habuere);
this action brings up an annotation screen in which
a partial automatic parse is provided, along with the
most likely morphological analysis for each word.
The annotator can then correct this automatic output
35
Figure 2: A screenshot of Tacitus? Annales from the Perseus digital library.
and move on to the next segmented sentence, with
all of the contextual resources still in view.
4.2 Reading support
Modern treebanks also differ from historical ones in
the fluency of their annotators. The efficient anno-
tation of historical languages is hindered by the fact
that no native speakers exist, and this is especially
true of Latin, a difficult language with a high de-
gree of non-projectivity. While the Penn Treebank
can report a productivity rate of between 750 and
1000 words per hour for their annotators after four
months of training (Taylor et al, 2003) and the Penn
Chinese treebank can report a rate of 240-480 words
per hour (Chiou et al, 2001), our annotation speeds
are significantly slower, ranging from 90 words per
hour to 281. Our best approach for Latin is to de-
velop strategies that can speed up the annotation pro-
cess, and here the resources found in a digital library
are crucial. There are three varieties of contextual
resources in our digital library that aid in the un-
derstanding of a text: translations, commentaries,
and dictionaries. These resources shed light on a
text, from the level of sentences to that of individual
words.
Translations. Translations provide reading sup-
port on a large scale: while loose translations may
not be able to inform readers about the meaning and
syntactic role of any single word, they do provide
a broad description of the action taking place, and
this can often help to establish the semantic struc-
ture of the sentence ? who did what to whom, and
how. In a language with a free word order (and with
poetry especially), this kind of high-level structure
can be important for establishing a quick initial un-
derstanding of the sentence before narrowing down
to individual syntactic roles.
Commentaries. Classical commentaries provide
information about the specific use of individ-
ual words, often noting morphological information
(such as case) for ambiguous words or giving ex-
planatory information for unusual structures. This
information often comes at crucial decision points
36
in the annotation process, and represents judgments
by authorities in the field with expertise in that par-
ticular text.
Figure 3: An excerpt from Conington?s commentary
on Vergil?s Aeneid (Conington, 1876), here referring
to Book 1, lines 4 and 5.
Machine-Readable Dictionaries. In addition to
providing lists of stems for morphological analyzers,
machine-readable dictionaries also provide valuable
reading support for the process of lemma selection.
Every available morphological analysis for a word is
paired with the word stem (a lemma) from which it is
derived, but analyses are often ambiguous between
different lemmas. The extremely common form est,
for example, is a third person singular present in-
dicative active verb, but can be inflected from two
different lemmas: the verb sum (to be) and the verb
edo (to eat). In this case, we can use the text already
tagged to suggest a more probable form (sum ap-
pears much more frequently and is therefore the like-
lier candidate), but in less dominant cases, we can
use the dictionary: since the word stems involved
in morphological analysis have been derived from
the dictionary lemmas, we can map each analysis
to a dictionary definition, so that, for instance, if an
annotator is unfamiliar with the distinction between
the lemmas occido1 (to strike down) and occido2 (to
fall), their respective definitions can clarify it.
Machine-readable dictionaries, however, are also
a valuable annotation resource in that they often pro-
vide exemplary syntactic information as part of their
definitions. Consider, for example, the following
line from Book 6, line 2 of Vergil?s Aeneid: et tan-
dem Euboicis Cumarum adlabitur oris (?and at last
it glides to the Euboean shores of Cumae?). The
noun oris (shores) here is technically ambiguous,
and can be derived from a single lemma (ora) as a
noun in either the dative or ablative case. The dic-
tionary definition of allabor (to glide), however, dis-
ambiguates this for us, since it notes that the verb is
often constructed with either the dative or the ac-
cusative case.
Figure 4: Definition of allabor (the dictionary entry
for adlabitur) from Lewis and Short (1879).
Every word in our digital library is linked to a list
of its possible morphological analyses, and each of
those analyses is linked to its respective dictionary
entry. The place of a treebank in a digital library
allows for this tight level of integration.
5 The Impact of a Historical Treebank
The traffic in our library currently exceeds 10 mil-
lion page views by 400,000 distinct users per month
(as approximated by unique IP addresses). These
users are not computational linguists or computer
scientists who would typically make use of a tree-
bank; they are a mix of Classical scholars, stu-
dents, and amateurs. These different audiences have
equally different uses for a large corpus of syntacti-
cally annotated sentences: for one group it can pro-
vide additional reading support, and for the other a
scholarly resource to be queried.
5.1 Treebank as Reading Support
Our digital library is predominantly a reading en-
vironment: source texts in Greek and Latin are
presented with attendant materials to help facilitate
their understanding. The broadest of these materials
are translations, which present sentence-level equiv-
alents of the original; commentaries provide a more
detailed analysis of individual words and phrases. A
37
treebank has the potential to be a valuable contex-
tual resource by providing syntactic information for
every word in a sentence, not simply those chosen
by a commentator for discussion.
5.2 Treebank as a Scholarly Resource
For Classical scholars, a treebank can also be used
as a scholarly resource. Not all Classicists are pro-
grammers, however, and many of those who would
like to use such a resource would profit little from
an XML source file. We have already released ver-
sion 1.3 of the Latin Dependency Treebank in its
XML source, but we also plan to incorporate it into
the digital library as an object to be queried. This
will yield a powerful range of search options, in-
cluding lemmatized and morpho-syntactic search-
ing, and will be especially valuable for research in-
volving lexicography and semantic classification.
Lemmatized searching. The ability to conduct a
lemma-based textual search has long been a desider-
atum in Classics,4 where any given Latin word form
has 3.1 possible analyses on average.5 Locating all
inflections of edo (to eat) in the texts of Caesar, for
example, would involve two things:
1. Searching for all possible inflections of the root
word. This amounts to 202 different word
forms attested in our texts (including com-
pounds with enclitics).
2. Eliminating all results that are homonyms de-
rived from a different lemma. Since several in-
flections of edo are homonyms with inflections
of the far more common sum (to be), many
of the found results will be false positives and
have to be discarded.
This is a laborious process and, as such, is rarely
undertaken by Classical scholars: the lack of such
a resource has constrained the set of questions we
4Both the Perseus Project and the Thesaurus Linguae Grae-
cae (http://www.tlg.uci.edu) allow users to search for all in-
flected forms of a lemma in their texts, but neither filters results
that are homonyms derived from different lemmas.
5Based on the average number of lemma + morphology
combinations for all unique word tokens in our 3.4 million word
corpus. The word form amor, for example, has 3 analyses: as
a first-person singular present indicative passive verb derived
from the lemma amo (to love) and as either a nominative or
vocative masculine singular noun derived from amor (love).
can ask about a text. Since a treebank encodes each
word?s lemma in addition to its morphological and
syntactic analysis, this information is now free for
the taking.
Morpho-syntactic searching. A treebank?s major
contribution to scholarship is that it encodes the
syntax of a sentence, along with a morphological
analysis of each word. These two together can be
combined into elaborate searches. Treebanks allow
scholars to find all instances of any particular con-
struction. For example:
? When the conjunction cum is the head of a sub-
ordinate clause whose verb is indicative, it is
often recognized as a temporal clause, qualify-
ing the time of the main clause?s action;
? When that verb is subjunctive, however, the
clause retains a different meaning, as either cir-
cumstantial, causal, or adversative.
These different clause types can be found by
querying the treebank: in the first case, by search-
ing for indicative verbs that syntactically depend on
cum; in the second, for subjunctive verbs that de-
pend on it. In version 1.3 of the Latin Dependency
Treebank, cum is the head of a subordinate clause
38 times: in 7 of these clauses an indicative verb de-
pends on it, while in 31 of them a subjunctive one
does. This type of searching allows us to gather sta-
tistical data while also locating all instances for fur-
ther qualitative analysis.6
Lexicography. Searching for a combination of
lemma and morpho-syntactic information can yield
powerful results, which we can illustrate with a
question from Latin lexicography: how does the
meaning of a word change across authors and over
time? If we take a single verb ? libero (to free, lib-
erate) ? we can chart its use in various authors by
asking a more specific question: what do different
Latin authors want to be liberated from? We can
imagine that an orator of the republic has little need
to speak of liberation from eternal death, while an
apostolic father is just as unlikely to speak of being
freed from another?s monetary debt.
6For the importance of a treebank in expediting morpho-
syntactic research in Latin rhetoric and historical linguistics, see
Bamman and Crane (2006).
38
We can answer this more general question by
transforming it into a syntactic one: what are the
most common complements of the lemma libero that
are expressed in oblique cases (e.g., ablative, geni-
tive, etc.) or as prepositional phrases? In a small test
of 100 instances of the lemma in Cicero and Jerome,
we find an interesting answer, presented in Table 2.
Cicero Jerome
periculo 14 manu 22
metu 8 morte 3
cura 6 ore 3
aere 3 latronibus 2
scelere 3 inimico 2
suspicione 3 bello 2
Table 2: Count of objects liberated from in Cicero
and Jerome that occur with frequency greater than 1
in a corpus of 100 sentences from each author con-
taining any inflected form of the verb libero.
The most common entities that Cicero speaks
of being liberated from clearly reflect the cares of
an orator of the republic: periculo (danger), metu
(fear), cura (care), and aere (debt). Jerome, how-
ever, uses libero to speak of liberation from a very
different set of things: his actors speak of deliver-
ance from manu (e.g., the hand of the Egyptians),
from ore (e.g., the mouth of the lion) and from
morte (death). A treebank encoded with lemma and
morpho-syntactic information lets us quantify these
typical arguments and thereby identify the use of the
word at any given time.
Named entity labeling. Our treebank?s place in
a digital library also means that complex searches
can draw on the resources that already lie therein.
Two of our major reference works include Smith?s
Dictionary of Greek and Roman Geography (1854),
which contains 11,564 place names, and Smith?s
Dictionary of Greek and Roman Biography and
Mythology (1873), which contains 20,336 personal
names. By mapping the lemmas in our treebank to
the entries in these dictionaries, we can determine
each lemma?s broad semantic class. After supple-
menting the Classical Dictionary with names from
the Vulgate, we find that the most common people
in the treebank are Iesus, Aeneas, Caesar, Catilina,
Satanas, Sibylla, Phoebus, Misenus and Iohannes;
the most common place names are Gallia, Babylon,
Troia, Hierusalem, Avernus and Sardis.
One use of such classification is to search for
verbs that are typically found with sentient agents.
We can find this by simply searching the treebank
for all active verbs with subjects known to be people
(i.e., subjects whose lemmas can be mapped to an
entry in Smith?s Dictionary). An excerpt of the list
that results is given in Table 3.
mitto to send
iubeo to order
duco to lead
impono to place
amo to love
incipio to begin
condo to hide
Table 3: Common verbs with people as subjects in
the Latin Dependency Treebank 1.3.
Aside from its intrinsic value of providing a cata-
logue of such verbs, a list like this is also useful for
classifying common nouns: if a verb is frequently
found with a person as its subject, all of its sub-
jects in general will likely be sentient as well. Table
4 presents a complete list of subjects of the active
voice of the verb mitto (to send) as attested in our
treebank.
angelus angel
Caesar Caesar
deus God
diabolus devil
Remi Gallic tribe
serpens serpent
ficus fig tree
Table 4: Subjects of active mitto in the Latin Depen-
dency Treebank 1.3.
Only two of these subjects are proper names (Cae-
sar and Remi) that can be found in Smith?s Dictio-
nary, but almost all of these nouns clearly belong
to the same semantic class ? angelus, deus, diabo-
lus and serpens (at least in this text) are entities with
cognition.
Inducing semantic relationships of this sort is the
typical domain of clustering techniques such as la-
39
tent semantic analysis (Deerwester et al, 1990), but
those methods generally work best on large corpora.
By embedding this syntactic resource in a digital li-
brary and linking it to external resources such as ref-
erence works, we can find similar semantic relation-
ships with a much smaller corpus.
6 Conclusion
Treebanks already fill a niche in the NLP community
by providing valuable datasets for automatic pro-
cesses such as parsing and grammar induction. Their
utility, however, does not end there. The linguis-
tic information that treebanks encode is of value to a
wide range of potential users, including professional
scholars, students and amateurs, and we must en-
courage the use of these resources by making them
available to such a diverse community. The digital
library described in this paper has proved to be cru-
cial for the development and deployment of our tree-
bank: since the natural intuitions of native speakers
are hard to come by for historical languagues, it is all
the more important to leverage the cultural heritage
resources we already have.
7 Acknowledgments
Grants from the Digital Library Initiative Phrase 2
(IIS-9817484) and the National Science Foundation
(BCS-0616521) provided support for this work.
References
David Bamman and Gregory Crane. 2006. The design
and use of a Latin dependency treebank. In Proceed-
ings of the Fifth Workshop on Treebanks and Linguistic
Theories (TLT2006), pages 67?78.
David Bamman, Marco Passarotti, Gregory Crane, and
Savina Raynaud. 2007. Guidelines for the syntactic
annotation of Latin treebanks, version 1.3. Technical
report, Tufts Digital Library, Medford.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang
Lezius, and George Smith. 2002. The TIGER tree-
bank. In Proceedings of the First Workshop on Tree-
banks and Linguistic Theories, pages 24?41, Sozopol.
Fu-Dong Chiou, David Chiang, and Martha Palmer.
2001. Facilitating treebank annotation using a statis-
tical parser. In Proceedings of the First International
Conference on Human Language Technology Research
HLT ?01, pages 1?4.
John Conington, editor. 1876. P. Vergili Maronis Opera.
The Works of Virgil, with Commentary. Whittaker and
Co, London.
Gregory Crane, Robert F. Chavez, Anne Mahoney,
Thomas L. Milbank, Jeffrey A. Rydberg-Cox,
David A. Smith, and Clifford E. Wulfman. 2001.
Drudgery and deep thought: Designing digital li-
braries for the humanities. Communications of the
ACM, 44(5):34?40.
Gregory Crane, David Bamman, Lisa Cerrato, Alison
Jones, David M. Mimno, Adrian Packel, David Scul-
ley, and Gabriel Weaver. 2006. Beyond digital in-
cunabula: Modeling the next generation of digital li-
braries. In ECDL 2006, pages 353?366.
Gregory Crane. 1987. From the old to the new: Integrat-
ing hypertext into traditional scholarship. In Hyper-
text ?87: Proceedings of the 1st ACM conference on
Hypertext, pages 51?56. ACM Press.
Scott C. Deerwester, Susan T. Dumais, Thomas K. Lan-
dauer, George W. Furnas, and Richard A. Harshman.
1990. Indexing by latent semantic analysis. Jour-
nal of the American Society of Information Science,
41(6):391?407.
Jan Hajic?. 1998. Building a syntactically annotated
corpus: The Prague Dependency Treebank. In Eva
Hajic?ova?, editor, Issues of Valency and Meaning.
Studies in Honor of Jarmila Panevova?, pages 12?19.
Prague Karolinum, Charles University Press.
A. Kroch and A. Taylor. 2000. Penn-Helsinki
Parsed Corpus of Middle English, second edi-
tion. http://www.ling.upenn.edu/hist-corpora/ppcme2-
release-2/.
Charles T. Lewis and Charles Short, editors. 1879. A
Latin Dictionary. Clarendon Press, Oxford.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313?330.
Harm Pinkster. 1990. Latin Syntax and Semantics. Rout-
ledge, London.
Susan Pintzuk and Plug Leendert. 2001. York-Helsinki
Parsed Corpus of Old English Poetry.
William Smith. 1854. A Dictionary of Greek and Roman
Geography. Walton and Maberly, London.
William Smith. 1873. A Dictionary of Greek and Roman
Biography and Mythology. Spottiswoode, London.
Ann Taylor, Mitchell Marcus, and Beatrice Santorini.
2003. The Penn Treebank: An overview. In Anne
Abeille?, editor, Treebanks: Building and Using Parsed
Corpora, pages 5?22. Kluwer Academic Publishers.
40
Proceedings of the 2009 Workshop on Text and Citation Analysis for Scholarly Digital Libraries, ACL-IJCNLP 2009, pages 80?87,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
Citations in the Digital Library of Classics: Extracting Canonical
References by Using Conditional Random Fields
Matteo Romanello, Federico Boschetti, Gregory Crane
The Perseus Project
Medford, MA, USA
matteo.romanello, federico.boschetti, gregory.crane{@tufts.edu}
Abstract
Scholars of Classics cite ancient texts by
using abridged citations called canonical
references. In the scholarly digital library,
canonical references create a complex tex-
tile of links between ancient and modern
sources reflecting the deep hypertextual
nature of texts in this field. This paper
aims to demonstrate the suitability of Con-
ditional Random Fields (CRF) for extract-
ing this particular kind of reference from
unstructured texts in order to enhance the
capabilities of navigating and aggregating
scholarly electronic resources. In partic-
ular, we developed a parser which recog-
nizes word level n-grams of a text as be-
ing canonical references by using a CRF
model trained with both positive and neg-
ative examples.
1 Introduction
In the field of Classics, canonical references are
the traditional way established by scholars to cite
primary sources within secondary sources. By
primary sources we mean essentially the ancient
texts that are the specific research object of Philol-
ogy, whereas by secondary sources we indicate all
the modern publications containing scholarly in-
terpretations about those ancient texts. This spe-
cific characteristic strongly differentiates canoni-
cal references from the typical references we usu-
ally find within research papers.
Canonical references are used to shortly refer to
the research object itself (in this case ancient texts)
rather than to the existing literature about a cer-
tain topic, as happens with references to other sec-
ondary sources. Given this distinction, canonical
references assume a role of primary importance as
the main entry point to the information contained
in scholarly digital libraries of Classics. To find a
parallel with other research fields, the role played
by those references is somewhat analogous with
that played by protein names in the medical liter-
ature or by notations of chemical compounds in
the field of Chemistry. As was recently shown by
Doms and Schroeder (2005) protein names can be
used to semantically index documents and thus to
enhance the information retrieval from a digital li-
brary of texts, provided that they are properly or-
ganized by using an ontology or a controlled vo-
cabulary. Moreover, by analyzing and indexing
such references as if they were backlinks (Lester,
2007) from a secondary to a primary source, it is
possible to provide quantitative data about the im-
pact of an ancient author for research in a particu-
lar disciplinary field, or in relation to a limited cor-
pus of texts (e.g., the papers published by schol-
arly journals in a given time interval).
In addition to serving as entry points to infor-
mation, canonical references can also be thought
of as a navigation apparatus that allows scholars
to browse seamlessly through ancient texts and
modern interpretations about them (Crane, 1987).
For every scholar working on the ancient histo-
riographer Herodotus, for instance, it would be
extremely useful to be able to easily access all
the secondary sources containing references to
Herodotus? works.
Therefore, the ability to automatically identify
canonical references within unstructured texts is a
first and necessary step to provide the users of dig-
ital libraries of Classics with a more sophisticated
way to access information and to navigate through
the texts that are already available to scholars of
other fields.
The volume of publicly available digitized
books constituting what has been called the Mil-
lion Book Library (Crane, 2006) has made it es-
sential to develop automatic and scalable tools
to automate the process of information extraction
from electronic resources. Furthermore, the obso-
80
lescence time for publications is far longer in Clas-
sics than in other disciplines, meaning that typi-
cally the value of a publication does not decrease
drastically after a certain time. As a result, schol-
ars in Classics may be the most potential benefi-
ciaries of the recent mass digitization initiatives,
since they have already started with many materi-
als out of copyright.
In this paper we describe how Conditional Ran-
dom Fields (Lafferty et al, 2001), the state of the
art model in automatic classification, can be suit-
ably applied to provide a scalable solution to this
problem.
2 Related work
Canonical references to primary sources can be
explored from at least three different angles: 1)
identification and extraction; 2) hypertextual nav-
igation; 3) semantics.
The identification and extraction of biblio-
graphic references from what we called secondary
sources (i.e. monographs, commentaries, journal
papers, etc.) is a well explored task for which ef-
fective tools already exist. Although the biggest
efforts in this direction have been made in the
scientific fields, those tools can also be suitably
adapted to the field of Classics, since they are es-
sentially based on machine learning techniques.
Several researchers recently focused on apply-
ing computational linguistics methods to automat-
ically extract information from both Classical texts
and modern texts about them, in order to support
the above described needs of scalability. Gerlach
and Crane (2008), and Kolak and Schilit (2008)
considered the identification of citations within
primary sources by analyzing the syntactic and
morphological features of texts, while (Smith and
Crane, 2001) dealt with the disambiguation of ge-
ographical names.
Looking at the problem of canonical references
from the user point of view, a digital library of
Classical texts such as the Perseus Digital Li-
brary1. already offers to the reader the ability to
navigate from secondary sources to the primary
sources they refer to, a process called reference
linking. The identification of references and the
attribution of semantics to them, however, was
done manually, and the navigation is limited to re-
sources contained in the same text collection. An
analogous reference linking system was proposed
1http://www.perseus.tufts.edu/hopper/
by Romanello (2008) as a value added service that
could be provided to readers of electronic journals
by leveraging semantic encoded canonical refer-
ences.
(Smith, 2009) provided an essential contribu-
tion to the research concerning the semantics of
canonical references. The Canonical Text Services
(CTS) protocol2 was developed by Smith for Har-
vard?s Center for Hellenic Studies; it is based on
URNs and is aimed at providing a machine action-
able equivalent to printed canonical references.
This protocol allows us to translate those refer-
ences into machine actionable URNs that can then
be resolved through resolution services against a
distributed digital library of texts. The innovative
aspect of the CTS protocol consists of a loose cou-
pling system by which the linking between pri-
mary and secondary sources can be realized. In-
stead of hard linking a canonical reference to just
one electronic edition of a primary source, by em-
bedding the CTS URNs inside (X)HTML pages, it
becomes possible to link it to an open ended num-
ber of resources as shown by (Romanello, 2007).
3 Canonical Text References
Canonical references present unique characteris-
tics when compared to bibliographic references to
modern publications. First of all, they do not re-
fer to physical facts of the referred work (such as
publication date or page number), but refer rather
to its logical and hierarchical structure. In addi-
tion, canonical references often provide additional
information needed by the reader to resolve the
reference. For example ?Archestr. fr. 30.1 Olson-
Sens? means line 1 of fragment 30 of the comic
poet Archestratus in the edition published by S. D.
Olson and A. Sens in 1994.
The specification of the edition according to
which a source is cited is an important piece of in-
formation to be considered. Indeed, since the aim
of Philology is to reconstruct for ancient works a
text that is as close as possible to the original one
(given that the original text may have been cor-
rupted over centuries of manuscript tradition), ed-
itors and scholars often disagree substantially as to
what readings and conjectures have to be included
in the established text.
Although some well established sets of abbre-
viations exist, scholars? practice of citing primary
2http://chs75.harvard.edu/projects/
diginc/techpub/cts
81
sources may noticeably differ according to style
preferences and the typographical needs of pub-
lishers, journals or research groups. Aeschylus?
name might appear in the abridged forms ?A.,
Aesch., Aeschyl.?, and similarly a collection of
fragments like Jacoby?s Die Fragmente der Gri-
eschischen Historiker may be abbreviated either
as FrGrHist or FGrHist.
Moreover, some highly specialized branches of
research exist within the field of Classics, such as
those dedicated to Epic poetry or Tragedy, or even
to a single author like Aeschylus or Homer. In
those specialized branches a common tendency to
use shorter references with a higher semantic den-
sity for the most cited authors can be observed.
For example, in publications containing thousands
of references to Homer?s Iliad and Odyssey, refer-
ences to these texts are often expressed with Greek
letters indicating the book number along with the
verse number (e.g., ?? 1? stands for the first verse
of the first book of Homer?s Odyssey). Lowercase
letters are used to refer to books of the Odyssey,
whereas uppercase letters refer to the books of the
Iliad, according to a practice developed in the IV
century B.C. by scholars of the library at Alexan-
dria.
In the actual practice of scholarly writing,
canonical references can appear with slightly dif-
ferent figures according to the needs of narrative.
Along with complete canonical references to a sin-
gle text passage, expressed as either a single value
or a range of values, other references can often be
found that are missing one or more components
that are normally present within canonical refer-
ences, such as an indication of the author name, of
the work title or of the editor name (e.g., ?Hom.
Od. 9.1, 9.2-3; Il 1.100?). This happens partic-
ularly in subsequent references to passages of the
same work.
Those differences that can be observed about
the appearance of canonical references require us
to apply different processing strategies to each
case. We focus on the task of automatically iden-
tifying complete references to primary sources.
Once those references have been identified in the
input document, we can find other anaphoric refer-
ences by applying some scope-based parsing. In-
deed, a canonical reference in the text constitutes
the reference scope for subsequent text passage in-
dications referring to the same work.
4 Methodology
Provided that scholars may use canonical refer-
ences with different abbreviation or citation styles,
it is nevertheless possible to identify within canon-
ical references common patterns in terms of token
features.
CRF is used to classify a token depending on
its features and is suitable to identify those feature
patterns (Culotta et al, 2006). During the training
phase, the CRF model learns what features make
it more likely for a token to belong to a given cat-
egory.
Our starting assumption is that it is possible
to determine if a sequence of tokens constitute a
canonical reference by evaluating (looking at) the
features of its tokens. Each token of a sequence is
assigned a category on the basis of a fixed num-
ber of features. Those token categories are in turn
used as features to classify the token sequence.
Starting from a dataset of canonical references
and applying the above described criteria to assign
features to the tokens, we obtain a training dataset
where each canonical reference is reduced to a to-
ken by removing whitespaces, and it is a assigned
as many as features as the category assigned to its
tokens.
Finally, in order to classify token sequences as
?references? or ?non-references? each canonical
reference is assigned a convenient label. The ob-
tained set of labelled references is used to train a
CRF model to identify canonical references within
unstructured texts.
4.1 Feature Extraction and Token
Categorization
For feature extraction phase, it was important to
identify both inclusive and exclusive token fea-
tures. Indeed, to extract canonical references with
a high level of precision, we need to identify not
only the characteristic features of tokens occurring
within actual references but also those characteris-
tic features for tokens occurring in sequences that
we want to be classified as non-references.
Even though the features are quite similar to
those used to identify modern bibliographic refer-
ences (Isaac Councill and Kan, 2008), they were
tuned to fit the specific needs of canonical refer-
ences to primary sources. We decided to record a
total of 9 features for each token, concerning the
following aspects:
82
1. Punctuation: information about the punctu-
ation concerning the presence of a final dot,
hyphen, quotation marks and brackets (either
single or paired), and marks used to divide
and structure sequences (i.e. comma, colon
and semicolon), which are particularly im-
portant for sequences of text passages.
2. Orthographic Case: the orthographic case
of a token is an essential piece of informa-
tion to be tracked. Author names when ab-
breviated still keep the initial as an upper-
case letter, whereas collections of texts (such
as collections of fragments) often present all
uppercase or mixed case letters (e.g., ?Tr-
GrFr?,?CGF?, ?FHG?, etc.).
3. Stopwords: given that the main language of
the input document is passed as a parame-
ter to the parser, we record in a separate fea-
ture information regarding whether a token is
a stopword in the input document language.
This feature is particularly important in deter-
mining more precisely the actual boundaries
of a canonical reference within the text.
4. Greek Words: since we deal with Unicode
UTF-8 text, we distinguish Greek letters and
words. This allows us to identify more pre-
cisely those references that contain Greek
text such as the above mentioned Homeric
references or references to the ancient lexica
(e.g., Harpocr., Lex. s.v. ??????????) since
they contain the lemma of the Greek word re-
ferred to, usually preceded by the abbrevia-
tion ?s.v.? (i.e. sub voce).
5. Number: Roman and Arabic numerals com-
bined in several figures are frequently used
to indicate the scope of a reference. Arabic
numerals that are used to represent modern
dates, however, are distinguished by using
a heuristic (for example, consider the prob-
lem of a footnote mark which gets appended
to a date). Nevertheless, sequences of both
numbers and punctuation marks are assigned
a specific value for this feature, since the
scope of a reference is commonly expressed
by dot and hyphen separated sequences such
as ?9.235-255?.
6. Dictionary matching: two features are as-
signed if a token matches a dictionary entry.
Three different dictionaries are used to ver-
ify if a token corresponds to a known canon-
ical abbreviation (e.g. ?Hom.? for Homer or
?Od.? for Odyssey) or to another kind of ab-
breviation, namely the abbreviations used by
philologists to shortly refer to pages, lines,
verses, etc. (?p?, ?pp.?, ?v.?, ?vv.?, ?cfr?, etc.)
or to abbreviations used for modern journals.
Abbreviations pertaining to the latter kind are
likely to introduce some noise during the n-
gram classification phase and thus are prop-
erly distinguished through a specific feature.
During preliminary analysis we particularly
observed that journal abbreviations were of-
ten confused with abbreviations for text col-
lections since - as we noted above - they share
the feature of having uppercase or mixed case
letters.
7. Fragment indication: canonical references to
fragments usually contain the indication ?fr.?
(and ?frr.? for more than one). Therefore
we expect tokens bearing this feature to occur
almost exclusively within references to frag-
mentary texts.
We extract from the training dataset those
unique patterns of these 9 token features that are
likely to be found within canonical references. In
order to ensure both the scalability and the ex-
tensibility of the suggested method to disciplinary
fields other than Classics, we did not assign an
identity feature to tokens or - in other words - the
actual string content is not considered as a token
feature. However, since this decision might de-
crease the overall precision of the system, we in-
troduced some features to record whether the to-
ken string occurs in one or more controlled dictio-
naries (e.g., list of widely adopted abbreviations).
An analogous consideration is valid also for
the dependency of the system from a specific lan-
guage. Even though the approach is substantially
language independent, the performances of our
system in terms of precision were improved by
using language specific lists of stopwords in or-
der to identify the actual boundaries of a canoni-
cal reference within the text. Currently we support
the most commonly used languages in the field of
Classics (English, French, German, Italian, Span-
ish).
Finally, it is worth noting that the use of italics is
a distinctive feature in particular for those tokens
83
that represent abbreviations of work titles. Since
we are dealing with plain text input documents,
however, and wish to keep the adopted approach
as generalizable as possible, this feature has not
been taken into account.
Token Features Cat.
F1 F2 F3 F4 F5 F6 F7 F8 F9
Od. ICP FDT NOD OTH OTH OTH CAB OTH OTH 1 c50
9.216-535. OTH FDT DSN OTH OTH OTH OTH OTH OTH 2 c6
Table 1: Categorization of tokens of the reference
?Od. 9.216-535? on the basis of their features.
Token Features Cat.
F1 F2
Od. 9.216-535 1 c50 2 c6 ref
Table 2: Categorization of the reference of Tab. 1
by using token categories as its features.
Feature Label
F1 Case
F2 Punctuation Mark
F3 Number
F4 Greek Sequence
F5 Stop Word
F6 Paired Brackets
F7 Contained in the 1st Dict.
F8 Contained in the 2nd Dict.
F9 Fragment Indication
Feature Value
CAB Canonical Abbreviation
DSN Dot Separated Number Plus Range
FDT Final Dot
ICP Initial Cap
NOD No Digit Sequence
OTH Other
Table 3: List of abbreviations used in Tab. 1, 2.
4.2 Positive and Negative Training
Since the main goal of our parser is to identify
canonical references by isolating them from the
surrounding context, both positive and negative
training examples are needed. Indeed, provided
two token sequences where the first contains just
a canonical reference (e.g., ?Od. 9.216-535?) and
the second additionally includes some tokens from
the context phrase (e.g.,?Od. 9.216-535, cfr. p.
29.?), without a negative training phrase both to-
ken sequences would have the same degree of sim-
ilarity. When weighted by the CRF model the
result would be that both sequences would share
the same number of features with one of the refer-
ences of the positive training. But since other se-
quences presenting features from both the positive
and negative training were included in the training,
and since such sequences were labelled as ?non-
references?, the end result is that a token sequence
with some tokens from a context phrase will be
less similar to a pure canonical reference.
The first step of the training phase is the ex-
traction of token features and the identification of
unique patterns of token features. At this stage
the processing units are the tokens of a reference.
Given a dataset of canonical references, each ref-
erence is firstly tokenized and each token is then
assigned 9 labels containing the values for the
above described features (see Section 4.1). Note
that in Tab. 1, 2 the labels and values of features
are indicated by the abbreviations given in Tab. 3.
The observed combinations of feature values
are then deduplicated and rearranged into unique
categories that are used to classify each token (see
Tab 1). These categories correspond to the uniques
combinations of features assigned to tokens of ref-
erences in the training dataset. Each category is
defined by a name such as ?c6? or ?c50?, where
?c? simply stands for ?category?? and ?6? or ?50?
are unique numeric identifiers. Besides, a numer-
ical prefix corresponding to the position of the to-
ken inside the canonical reference is then added to
the category name to form the identifier. Indeed,
the position of each token in the sequence is in
itself meaningful information, provided that indi-
cations of the reference scope (and other reference
components as well) tend to occur at the end of
the token sequence. What we obtain are category
identifiers such as ?1 c50? or ?2 c6?.
The second step is building the training dataset.
At this stage each canonical reference is reduced
to a single token which is assigned the label ?ref?
(i.e. reference) and which has as distinctive fea-
tures the category identifiers assigned to its tokens
(see Tab 2).
Finally, a such obtained dataset of labelled in-
stances is used to train our CRF model by us-
ing the Java CRF implementation provided by the
Mallet toolkit (McCallum, 2002).
84
4.3 Sequence Classification Process
The system we propose to identify canonical ref-
erences in unstructured texts is basically a binary
classifier. Indeed, it classifies as ?reference? or
?non-reference? a sequence of word level n-grams
depending on the features of its tokens. However,
in the training dataset the positive examples are
manually grouped by typology and different labels
(such as ?ref1?, ?ref2? etc.) are assigned to canon-
ical references pertaining to different types. This
is done in order to avoid associating too many fea-
tures to a single class and thus to maximize the
difference in terms of features between sequence
being references and non-references.
Since every token is assigned a certain number
of features and finally a category, the likelihood
for a token sequence to be a canonical reference
can be determined on the basis of its similarity, in
terms of token features, to the labelled references
of a training set.
Once the input document is tokenized into sin-
gle words, the n-grams are created by using a
window of variable dimensions ranging from the
minimum to the maximum length in terms of to-
kens that was observed for all the references in the
training dataset. For example, provided that the
shortest canonical reference in the training dataset
is 2 tokens long and the longest is 7 tokens long,
for each token are created 6 word level n-grams.
For the sake of performance, however, the num-
ber of n-grams to be created is determined for
each token at parsing time. First of all a threshold
value is passed to the parser as an option value.
The threshold is compared to the weight value as-
signed by the CRF model to the probability of a
token to be classified with a label, in our case
?ref? or ?noref?. For each token, if the first n-
gram is classified as not being a canonical refer-
ence the processing shifts to the next token, since
we observed that if the first n-gram is classified as
a non-reference the following n-grams of increas-
ing width never contain a reference. If the exam-
ined n-gram is classified as reference, another of
dimension n+1 is created: the parser passes on to
process the next token only if the current n-gram
is classified as a canonical reference with a like-
lihood value greater that that of the previous n-
gram.
5 Training and Evaluation Criteria
The system is based on both a positive and a neg-
ative training.
The dataset for the positive training is built by
labeling with the above explained criteria a start-
ing set of approximatively 50 canonical references
selected by an expert. The classifier trained with
those positive examples is then applied to a ran-
dom set of documents. Extracted candidate canon-
ical references are scored by the CRF model by as-
signing to each sequence of n-grams a value rep-
resenting the probability for the sequence to be a
canonical reference.
The first one hundred errors with the highest
score, due to the sharing of several features with
the actual canonical references, are marked as
non-references and added to the set of sequences
to use for the negative training. The negative
training is needed in order to precisely segment
a canonical reference and to correctly classify
those sequences that are most likely to be con-
fused with actual canonical references, such as se-
quences only partially containing a canonical ref-
erence or bibliographic references. In particular,
bibliographic references are misleading sequences
since they have several features in common with
canonical references, such as capitalized titles and
page numbers.
The overall performances of the system on
a random sample of 24 pages can be summa-
rized by: precision=81.01%, recall=94.11%, ac-
curacy=77.11%, F-score=0.8707. Analytical data
are provided in Tab. 4. Although the evaluation
was performed on pages drawn from a publica-
tion written in Italian, we expect to have analogous
performances on texts written in each of the cur-
rently supported languages (English, French, Ger-
man, Italian, Spanish) for the reasons described in
Section 4.1.
The results are encouraging, however, and some
further improvements could concern the recovery
of tokens wrongly included in or excluded from
the sequence identified by the parser.
6 Conclusion and Future Work
This paper has illustrated how the CRF model can
be suitably applied to the task of extracting canoni-
cal references from unstructured texts by correctly
classifying word level n-grams as references or
non-references.
85
Document # Precision Recall Accuracy F-Score
40 100.00% 100.00% 100.00% 1.0000
41 100.00% 100.00% 100.00% 1.0000
55 100.00% 100.00% 100.00% 1.0000
57 100.00% 100.00% 100.00% 1.0000
62 100.00% 100.00% 100.00% 1.0000
64 100.00% 100.00% 100.00% 1.0000
67 25.00% 25.00% 25.00% 0.2500
74 88.00% 87.50% 77.78% 0.8800
77 45.00% 90.00% 42.86% 0.6000
82 100.00% 100.00% 100.00% 1.0000
85 100.00% 90.00% 90.00% 0.9474
88 100.00% 100.00% 100.00% 1.0000
90 92.31% 92.31% 85.71% 0.4286
100 100.00% 100.00% 100.00% 1.0000
113 60.00% 100.00% 60.00% 0.7500
117 100.00% 100.00% 100.00% 1.0000
134 100.00% 75.00% 75.00% 0.8571
137 75.00% 100.00% 75.00% 0.8571
144 67.00% 100.00% 67.00% 0.8024
146 33.00% 100.00% 33.00% 0.4511
150 57.14% 100.00% 57.00% 0.7273
162 100.00% 100.00% 100.00% 1.0000
169 50.00% 75.00% 43.00% 0.6000
Overall 81.01% 94.11% 77.11% 0.8707
Table 4: Performance evaluation of the system.
Once automatically identified, canonical refer-
ences can have further semantic information added
to them. By combining and then applying tech-
niques of syntactic and semantic parsing to the
identified references, it is possible to extract infor-
mation such as the precise author name and work
title, the text passage referred to, and the reference
edition (either when implicitly assumed or explic-
itly declared).
The first important outcome of our work is that
such an automatic system allows us to elicit the
hidden tangle of references which links together
the primary and secondary sources of a digital li-
brary. Another important outcome is that unstruc-
tured texts could be analyzed on the basis of the
canonical references they contain, for example by
clustering techniques. Given a consistent corpus
of texts it would be possible to cluster it on the
basis of the distribution of canonical references
within documents in order to obtain a first topic
classification.
Among the benefits of the proposed approach
there is the possibility of applying it to texts per-
taining to specific branches of Classics, like Pa-
pyrology or Epigraphy. Indeed in those disci-
plines papyri and epigraphs are also often cited by
abridged references that are very similar in their
structure and features to the canonical text ref-
erences. In a similar way, a canonical reference
parser can be trained on a particular citation style
in order to tailor it to a consistent corpus of texts
with consequent improvements on the overall per-
formances.
Finally, since the task of automatic extraction
of canonical references has never been explored
before, we hope that in the future more resources
will be available for this task (such as training
datasets, golden standards, performance measure
to be compared, etc.), analogous to those already
existing for other more common tasks, like named
entity recognition or the extraction and labeling of
modern bibliographic references.
References
Gregory Crane. 1987. From the old to the new: in-
tergrating hypertext into traditional scholarship. In
Proceedings of the ACM conference on Hypertext,
pages 51?55, Chapel Hill, North Carolina, United
States. ACM.
Gregory Crane. 2006. What do you do with a million
books. D-Lib Magazine, 12(3).
Aron Culotta, Andrew Mccallum, and Jonathan Betz.
2006. Integrating probabilistic extraction models
and data mining to discover relations and patterns in
text. In Proceedings of the main conference on Hu-
man Language Technology Conference of the North
American Chapter of the Association of Computa-
tional Linguistics, pages 296?303, Morristown, NJ,
USA. Association for Computational Linguistics.
Andreas Doms and Michael Schroeder. 2005. GoP-
ubMed: exploring PubMed with the gene ontology.
Nucl. Acids Res., 33(suppl 2):783?786, July.
Andrea Ernst-Gerlach and Gregory Crane, 2008. Iden-
tifying Quotations in Reference Works and Primary
Materials, pages 78?87.
C. Lee Giles Isaac Councill and Min-Yen Kan. 2008.
Parscit: an open-source crf reference string pars-
ing package. In Bente Maegaard Joseph Mari-
ani Jan Odjik Stelios Piperidis Daniel Tapias Nico-
letta Calzolari (Conference Chair), Khalid Choukri,
editor, Proceedings of the Sixth International
Language Resources and Evaluation (LREC?08),
Marrakech, Morocco. European Language Re-
sources Association (ELRA). http://www.lrec-
conf.org/proceedings/lrec2008/.
86
Okan Kolak and Bill N. Schilit. 2008. Generating links
by mining quotations. In Proceedings of the nine-
teenth ACM conference on Hypertext and hyperme-
dia, pages 117?126, Pittsburgh, PA, USA. ACM.
John Lafferty, Andrew Mccallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proc. 18th International Conf. on
Machine Learning, pages 289, 282. Morgan Kauf-
mann, San Francisco, CA.
Frank Lester. 2007. Backlinks: Alternatives to the
citation index for determining impact. Journal of
Electronic Publishing, 10(2).
Andrew Kachites McCallum. 2002. MAL-
LET: a machine learning for language toolkit.
http://mallet.cs.umass.edu.
Matteo Romanello. 2007. A semantic linking sys-
tem for canonical references to electronic corpora.
Prague. to be next published in the proceedings of
the ECAL 2007 Electronic Corpora of Ancient Lan-
guages, held in Prague November 2007.
Matteo Romanello. 2008. A semantic linking frame-
work to provide critical value-added services for e-
journals on classics. In Susanna Mornati and Leslie
Chan, editors, ELPUB2008. Open Scholarship: Au-
thority, Community, and Sustainability in the Age of
Web 2.0 - Proceedings of the 12th International Con-
ference on Electronic Publishing held in Toronto,
Canada 25-27 June 2008 / Edited by: Leslie Chan
and Susanna Mornati.
David A. Smith and Gregory Crane. 2001. Disam-
biguating geographic names in a historical digital li-
brary. In ECDL ?01: Proceedings of the 5th Euro-
pean Conference on Research and Advanced Tech-
nology for Digital Libraries, pages 127?136, Lon-
don, UK. Springer-Verlag.
Neel Smith. 2009. Citation in classical studies. Digital
Humanities Quarterly, 3(1).
87
Proceedings of the 8th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities (LaTeCH) @ EACL 2014, pages 1?8,
Gothenburg, Sweden, April 26 2014. c?2014 Association for Computational Linguistics
A New Implementation for Canonical Text Services
Jochen Tiepmar
Christoph Teichmann
Gerhard Heyer
Computer Science Department
Leipzig University
billion-words@e-humanities.net
Monica Berti
Gregory Crane
Humboldt Chair of Digital Humanities
Leipzig University
monica.berti@uni-leipzig.de
crane@informatik.uni-leipzig.de
Abstract
This paper introduces a new implemen-
tation of the Canonical Text Services
(CTS) protocol intended to be capa-
ble of handling thousands of editions.
CTS was introduced for the Digital Hu-
manities and is based on a hierarchical
structuring of texts down to the level of
individual words mirroring traditional
practices of citing. The paper gives
an overview of CTS for those that are
unfamiliar and establishes its place in
the Digital Humanities research. Some
existing CTS implementations are dis-
cussed and it is explained why there
is a need for one that is able to scale
to much larger text collections. Eval-
uations are given that can be used to
illustrate the performance of the new
implementation.
1 Introduction
Canonical Text Services (CTS) (Smith, 2009)
1
is a standard that resulted from research in the
Digital Humanities community on citation in
a digital context. It consists of two parts: an
URN scheme that is used to express citations
and a protocol for the interaction of a client
and a server to identify text passages and re-
trieve them.
CTS is an attempt to formalize citation
practices which allow for a persistent identi-
fication of text passages and citations which
1
http://www.homermultitext.org/hmt-doc/
index.html
express an ontology of texts as well as links be-
tween texts (Smith and Blackwell, 2012). The
same citation scheme can be used across dif-
ferent versions of a text, even across language
borders.
All these properties make CTS attractive
as an approach to the presentation of large,
structured collections of texts. The frame-
work will have little impact however as long
as there is no implementation that can scale
to the amount of texts currently available for
Digital Humanities research and still perform
at a level that makes automatic processing of
texts attractive. Therefore the implementa-
tion of the scheme presented here allows for
large repositories without becoming infeasibly
slow.
2 Overview of Canonical Text
Services
For readers unfamiliar with Canonical Text
Services this section provides a short introduc-
tion to the CTS protocol and explains its role
in the wider context of the CITE architecture.
In order to make the explanations given in this
section a little more concrete they are followed
by example applications of CTS. Before we go
into the technical details of the CTS format,
a general review of the motivations and ap-
proaches behind CTS will be helpful.
CTS incorporates the idea that citations
provide an inherent ontology of text passages.
A citation of the first word of the first sen-
tence of section 1 in this paper, when made in
exactly that way, implies part-whole relation-
ships between the word and the sentence, the
sentence and the section and finally the section
and the whole article. Canonical Text Services
derive their name from the assumption that
each text which is included in a CTS reposi-
tory is associated with a canonical way of cit-
1
ing it which has been established by a com-
munity of researchers working with the text
or texts similar to it. Where no such schemes
exist they may be defined when a work first
enters a repository. These canonical citation
schemes are especially common in Classics re-
search from which much of the work on CTS
originated. Such schemes often abstract away
from any concrete manifestation of a text
2
in
favour of schemes that can be applied across
different incarnations. Returning to the ex-
ample task of citing portions of this article,
one could cite the same word by referencing
a specific line. The latter approach is prob-
lematic, since simply printing the article with
a different font size could completely change
the lines in which words appear. For this rea-
son canonical citations generally rely on logical
properties of the text.
Using logical properties of the text implies
that citations can be carried over from one
specific incarnation to another. It may even
be possible to apply the same citation scheme
to versions that are written in different lan-
guages. This means that different versions of
a text can form another element of a hier-
archy. Here the part-whole relations are re-
peated, with different versions of a text be-
longing to larger groups as explained in section
2.1. When such citations are coupled with a
service that resolves the citations and is capa-
ble of giving an inventory of all the citations
it can resolve, then this can be a powerful tool
in Digital Humanities research.
2.1 The CTS URN scheme
We give a short review of the structure of a
CTS URN used to identify a text passage
3
.
Any canonical reference must start with the
prefix:
urn:cts
which simply identifies the string as an URN
for a canonical citation. This is followed by
three parts that contain the main information
for every citation. The first of these parts iden-
tifies a name space in which the following el-
ements of the citation are meaningful. This
part allows for different authorities to define
2
For example a specific printing.
3
For a more extensive discussion see
http://www.homermultitext.org/hmt-docs/
specifications/ctsurn/
their own schemes for citing works. This sec-
tion is followed by an identifier of the work
that is cited. Finally the URN is completed
by a string identifying a text node or passage
in the work that is cited, which could corre-
spond to a specific word, a section or even the
complete work. To summarize the format of a
CTS URN is:
urn:cts:name_space:work_identifier:
passage_identifier
where the final part can be dropped in or-
der to identify the complete text of a work.
The ontology for the work level is given by
the URN scheme itself which requires that the
work identifier has the structure:
text_group.work.version.exemplar
here only the text group part is mandatory.
Every other section can be dropped if every
following section is also dropped. The text
group can be used for any collection of texts
that the manager of a CTS service would like
to group together such as all the works of an
author, all the works from a certain area or all
the works created at a certain time. The work
portion identifies a specific text within that
group. The version part refers to a specific
edition or translation of the work and finally
the exemplar identifier selects a specific exam-
ple of a version of a text. The latter three parts
of a work identifier correspond to levels of the
hierarchy posited by the Functional Require-
ments for Bibliographic Records (FRBR).
CTS URNs end with a passage identifier.
This identifier can further be divided into the
parts:
Citable_Node@Subsection
where the citable node must correspond to
some XML element within the text that is
cited. The hierarchy that is used in these
nodes is up to the person managing the cita-
tions. The hierarchy can be expressed by sep-
arating different levels with the delimiter ?.?
and every level can be omitted as long as all
following levels are also omitted. A subsection
can be used to select a smaller part of a citable
node by identifying words to select. There are
some additional options that can be used in a
CTS URN, among them the option to combine
passages into new subsections by using a range
operator, and the interested reader is encour-
aged to consult the official documentation for
2
the standard.
CTS URNs can be used to identify and con-
nect text passages. A natural task in connec-
tion with citations is the retrieval of collections
of citations and the text sections associated
with them. This task is addressed in the next
section.
2.2 The CTS Protocol
This section summarizes the CTS protocol for
the retrieval of text sections and citations
4
.
The first main request that the protocol de-
fines is:
GetPassage
which can be used to retrieve a passage of a
text associated to an URN in order to fulfil the
actual purpose of a citation. This request also
shows one of the main uses of the ontology that
is implied in the way works and passages are
cited. When a work identifier is ?incomplete?
then the service is allowed to pick an instance
of the work to deliver. When a passage identi-
fier is ?incomplete? then the passage delivered
includes all passages with identifiers that could
complete it.
The second main request is:
GetValidReff
which is used to obtain all the citations at
a certain level that the current repository can
support. Here it is possible to state how many
levels should be present in the answer.
The final request that will be discussed in
this section is:
GetCapabilities
which is used to obtain the different texts
known to a server and the way that they can be
cited i.e. the structure of their passage identi-
fiers.
With the given requests it is possible to fulfil
the main tasks of a citation software: find cita-
tions and/or resolve them. Other systems can
then build on top of these requests. One exam-
ple for an architecture that includes CTS ca-
pabilities in a wider framework is CITE which
is explained in the next section.
4
More information can be found at
http://www.homermultitext.org/hmt-docs/
specifications/cts/
2.3 CTS in the Context of the CITE
Architecture
The Collections, Indexes and Texts (CITE) ar-
chitecture is a large framework for reference to
the objects of study in Digital Humanities
5
.
The general design philosophy is to use URNs
as a modern way of encoding citations.
Besides providing a general framework for
referencing objects and texts, with the latter
task being implemented by CTS, CITE also
defines a standard for encoding relations be-
tween references. An example would be to link
a section of a text about geometry to a draw-
ing which it uses as an example. The CITE
architecture also includes protocols for resolv-
ing and obtaining the references that can be
defined within it. Since CTS takes care of ci-
tations concerning texts and the tasks associ-
ated with them, an implementation of the CTS
protocol is an important first step towards a
complete implementation of the CITE archi-
tecture.
2.4 Example Applications
In this section we review two example appli-
cations for the CTS/CITE infrastructure: the
generation of digital editions for the Classics
and creating editions of so called fragmentary
texts.
2.4.1 New Features of Digital Editions
Several features of a true digital edition have
already begun to emerge: they have been im-
plemented and they offer demonstrable ben-
efits that justify such added labour as they
demand. Each of the following features re-
quires the ability to identify precise words
and phrases in particular versions of a work.
The CTS/CITE architecture provides a mech-
anism to support core new functions within
the emerging form of born-digital editions:
1. Translators must work with the realiza-
tion that they are to be aligned to the
original and that they will, in fact, help
make the original source text itself in-
tellectually accessible to readers with no
knowledge of the source language. Ev-
ery reader should use the Greek and the
5
More information on CITE can be found
at http://www.homermultitext.org/hmt-doc/cite/
index.html
3
Latin. Ideally, translators should align
their own translations to the source text
and provide notes explaining where and
why the source text and translation can-
not be aligned.
2. We need multi-texts, i.e., editions that
can encapsulate the entire textual history
of a work so that readers can see not
only variants from the manuscript tra-
dition but also variations across editions
over time. No reader should ultimately
ever have to wonder how a new edition
varies from its predecessors. Encapsulat-
ing the full textual tradition of every work
will take a very long time but we can be-
gin by representing not only textual vari-
ants but also providing more than one dig-
itized edition. Again, scholars need the
functionality of the CTS/CITE architec-
ture to represent the relationships among
different versions of a work.
3. Editors of Greek and Latin texts must
encode, at the very least, their interpre-
tations of the morpho-syntactic functions
of every word in every text. This should,
in fact, impose little extra cost if editors
are agonizing, as they should, over every
word. Where the editor thinks that there
are multiple interpretations that should
be considered, then these should be pro-
vided along with an explanation of each.
The morpho-syntactic analyses are funda-
mental to modern linguistic analysis and
also provide a wholly new form of reading
support.
4. All proper names must be aligned to
authority lists such as the Pleiades
Gazetteer or the Perseus Smith Dictio-
nary of Greek and Roman Biography. We
also need conventions for encoding our
textual evidence for the relationship be-
tween different named entities (e.g., X is
the son of Y). Such annotations enable
new methods of analysing and visualis-
ing our sources with methods from ge-
ographic information systems and social
network analysis.
5. All instances of textual reuse need to be
annotated, including cases where we have
reason to believe particular words and
phrases are either quoted or paraphrased.
2.4.2 Fragmentary Texts
Among various example applications (Smith,
2009; Smith and Blackwell, 2012; Almas and
Beaulieu, 2013), the CTS/CITE Architecture
is being implemented by the Perseus Project
for representing fragmentary texts of Classical
lost authors. By fragmentary texts we mean
texts preserved only through quotations and
reuses by later authors, such as verbatim quo-
tations, paraphrases, allusions, translations,
and so on (Berti et al., 2009; Almas and Berti,
2013).
The first need for representing such texts
is to visualize them inside their embedding
context and this means to select the string of
words that belong to the portion of text which
is classifiable as reuse. The CTS/CITE Archi-
tecture provides us with a standard identifier
syntax for texts, passages, and related objects
and with APIs for services which can retrieve
objects identified via these protocols (Smith
and Blackwell, 2012).
For example, the following set of identifiers
might be used to represent a reuse of a lost text
of the Greek author Istros, which has been pre-
served by Athenaeus of Naucratis in the Deip-
nosophists, (Book 3, Chapter 6)
6
(Almas and
Berti, 2013):
urn:cts:greekLit:tlg0008.tlg001.
perseus-grc1:3.6@???????[1]-??????????[1]
is a CTS URN for a subset of passage 3.6 in
the perseus-grc1 edition of the work identified
by tlg001 in the group of texts associated with
Athenaeus, identified by tlg0008. The URN
further specifies a string of text in that pas-
sage starting at the first instance of the word
????????? and ending at the first instance of
the word ????????????.
urn:cite:perseus:lci.2
is a CITE URN identifier for the instance
of lost text being reused. This URN identifies
an object from the Perseus Collection of Lost
Content Items (lci) in which every item points
to a specific text reuse of a lost author as it is
represented in a modern edition.
6
For a prototype interface see http://perseids.
org/sites/berti_demo/ (source code at https://
github.com/PerseusDL/lci-demo)
4
These URNs represent distinct technology-
independent identifiers for the two cited ob-
jects, and by prefixing them with the http:
//data.perseus.org URI prefix (represent-
ing the web address at which they can be
resolved) we create stable URI identifiers for
them, making them compatible with linked
data best practices
7
:
http://data.perseus.org/citations/
urn:cts:greekLit:tlg0008.tlg001.
perseus-grc1:3.6@???????[1]-??????????[1]8
http://data.perseus.org/collections/
urn:cite:perseus:lci.2
The CITE Objects URNs may be organized
into various types of collections of data, such
as representations of text reuses in traditional
print editions, all text reuses attributed to a
specific author, all text reuses quoted by a
specific author, all text reuses dealing with a
specific topic, all text reuses attributed to a
specific time period, etc. CITE collections are
used to define and retrieve distinct digital rep-
resentations of discrete objects, including as-
sociated meta data about those objects. Ex-
ample CITE collections used to support the
encoding of text reuses for this project include
the abstract lost text entities themselves, digi-
tal images of manuscripts of the extant source
texts that quote those lost texts, commentaries
on instances of text reuse and linguistic anno-
tations of the quoted text (Almas et al., 2014).
3 Existing Implementations
There are two general purpose implementa-
tions of the CTS protocol that the authors of
this paper are aware of. The first is an imple-
mentation based on a XML database. This im-
plementation is part of the Alpheios project
9
.
Using a XML database seems natural consid-
ering the fact that the CTS architecture re-
quires data to take the form of XML files. It
would be interesting to compare the perfor-
mance of this implementation with that of the
one that will be presented here, but since the
Alpheios tool is not yet complete and has only
7
http://sites.tufts.edu/perseusupdates/
beta-features/perseus-stable-uris/
8
At the time of this writing, complete implemen-
tation of the CTS standard for resolution of passage
subreferences at the data.perseus.org address is still
pending.
9
http://alpheios.net/
been tested with a few hundred texts as in-
put
10
any comparison would seem unfair.
The second project to implement the CTS
protocol that we are aware of is based on a
SparQL endpoint
11
. Similar to the XML based
approach the use of SparQL for CTS is intu-
itive. The part-of relations that are implied
by the structure of URNs could easily be mod-
elled with triple expressions. The implementa-
tion has not yet been optimized to work with
large numbers of input texts and is therefore
not suited to a comparison with the tool pre-
sented in this paper. While the use of triples to
encode the logical relations seems natural, it
is necessary to reconstruct all relations already
implied by the structure of the URN Strings.
This means that there is a potential for opti-
mization that can be exploited by using the
structure of these strings in order to store all
information implicitly.
4 A New Implementation
So far this paper has argued that Canonical
Text Services can provide an important in-
frastructure for Digital Humanities research.
Recently it has also been highlighted (Crane
et al., 2012) that repositories of texts such as
the Internet Archive
12
have the potential to
allow Digital Humanities researchers to work
with text collections that encompass billions
or even trillions of words. CTS is one tool in
the attempt to handle this mass of data with-
out being overwhelmed by it. Since existing
implementations of the CTS protocol are not
yet able to scale to the data quantities that
the Digital Humanities community could pro-
vide, we found it necessary to create a new
implementation. In order to find out whether
our implementation can deal with such a large
number of texts, it will be necessary to give
an evaluation of performance. This section in-
troduces the main ideas concerning this new
implementation and shows that it is indeed ca-
pable of the required scaling.
10
Personal communication with Bridget Almas, the
main developer of the Alpheios CTS implementation.
11
The implementation can be found at https://
github.com/neelsmith/sparqlcts.
12
https://archive.org/index.php
5
4.1 Using the Tree Structure of the
Data
The main technical problem that needs to be
solved in order to generate an efficient imple-
mentation of the Canonical Text Services pro-
tocol is the efficient mapping of URNs to texts,
sections in these texts and the required meta
data. Both tasks require the fast mapping of
possible prefixes of valid identifiers. There are
two obvious solutions to this problem.
The first is the use of a prefix tree or trie
in order to be able to deal with underspecified
data. This would make it possible to read in
the portion of the URN that is specified and
then either have a copy of the text or text sec-
tion associated with this prefix associated with
the tree node or construct the necessary infor-
mation by visiting all daughter nodes. The
former choice would be more efficient in terms
of nodes visited, but the latter choice would
require less memory.
The second option is the use of
the lexicographic ordering of the
URNs. Consider the set of strings
S = {a.a.a, a.b.a, a.b.b, a.b.c, a.c.a, . . .}. If
all the strings are moved into a data structure
that respects the lexicographic ordering of the
strings, then all the strings matching a.b
?13
can be found by locating the position of the
largest string that is lexicographically smaller
than or equal to a.b
14
and then visiting all
following entries in the data structure until
one lexicographically equal to or greater than
a.c
15
is found. Since MySQL
16
already im-
plements the B-Tree (Bayer and McCreight,
1972) data structure to manage its table
indexes we chose this second approach for
our implementation. It is used for the work
identifiers to select a text that matches a
prefix. In the case of passage identifiers all
nodes that match a certain prefix are visited
and the required text is constructed. The first
approach of using prefix trees was also tested
but did not lead to a significant decrease in
the time or memory requirements since it was
not native to the database used.
13
Here
?
denotes an arbitrary sequence of charac-
ters.
14
In this case a.a.a.
15
In this case a.c.a.
16
See www.mysql.com.
4.2 Putting Everything into a
Relational Database
With the problem of handling the URNs
solved by tree structures, all that remains is to
manage the data that can be found by using
the URNs and keeping an index of the URNs.
Because the CTS standard requires that the
URNs of a work are ordered, this also means
that this ordering needs to be preserved. This
is achieved by simply keeping a column that
stores a numbering. It is ensured that this
numbering is sequential without gaps. This
means that it is possible to retrieve a certain
number of neighbours by simply increment-
ing and retrieving passages according to this
counter. As a result the efficient retrieval of
passages that span a range of URNs is possible
with only 3 requests, implemented by retriev-
ing the number of the first and last URNs in
the range and then merging all text chunks in
this range into one passage.
As mentioned earlier, the text of a retrieved
section is built up from smaller parts when
a node higher in the hierarchy is retrieved.
We thereby reduce the amount of memory re-
quired since only segments of the data need to
be stored. This is unlikely to be a bottleneck,
since we assume that the length of a text is
not a variable that can grow arbitrarily.
Meta data on the edition level is stored as
a simple data entry. For each individual URN
we store the language and type of its associ-
ated content.
4.3 Evaluation
Here we want to show that our implementa-
tion is able to scale to the large amounts of
data potentially available to Digital Humani-
ties researchers today and that it can handle
the large amounts of data potentially gener-
ated by cooperative editing. In order to do
this we designed tests that can be used to ac-
cess the performance of our Canonical Text
Services implementation. The following Tests
were used:
1. retrieve a list of all editions, then get all
the valid URNs and the full passage for
each edition
2. collect the first 1000 editions, then obtain
the first URN at the lowest level within
6
each edition and its second neighbour, re-
trieve the first full word for both
17
, finally
get the subsection between both words.
Test 1 measures the speed with which the
data store can be explored even with a large
number of editions and how quickly a passage
spanning the whole edition can be constructed.
It can be assumed that the time needed to exe-
cute will increase with the amount of editions
that are added to a repository and with the
length of the individual texts.
Test 2 checks how quickly the implementa-
tion can find subsections and is not expected
to take substantially longer for our implemen-
tation as the number of editions increases. It
is mainly intended to show that behaviour on
single texts is not impacted by the number of
editions managed and that the construction
of larger passages from elementary chunks is
handled efficiently.
Both tests were run by using a small seed
of data
18
that was copied repeatedly in order
to arrive at the number of necessary editions.
The data will be made available. Our imple-
mentation ran on a server with a 2.4 GHz CPU
and 1GB of RAM. The requests necessary for
our tests ran on a different machine in order
to factor in the problem of communication. In
future tests it would be possible to distribute
the requests between different clients to focus
more on this point.
Figure 1 contains the results for Test 1. The
amount of time taken is linear in the number
of editions since every new text was generated
once. While the construction of all the texts
took several hours for the larger collections,
the list of all editions was retrieved within a
second or less. There is a surprising spike that
could be due to factors external to our pro-
gram which could have a strong impact on
such comparatively short time measurements.
Figure 2 gives the results for Test 2. As ex-
pected the behaviour is not greatly impacted
by the number of editions in the collection.
The variation between the different numbers
of editions is within a second for the com-
plete task and the average time needed per
retrieval task varies by only ten milliseconds.
17
A word not containing special characters and
longer than 2 characters.
18
1000 editions.
T
i
m
e
 
i
n
 
M
i
l
l
i
s
e
c
o
n
d
s
0
5?10
6
10
7
1.5?10
7
2?10
7
2.5?10
7
Number Of Editions
0 2000 4000 6000 8000 10
4
T
i
m
e
 
i
n
 
M
i
l
l
i
s
e
c
o
n
d
s
0
200
400
600
800
1000
Number Of Editions
0 2000 4000 6000 8000 10
4
Figure 1: Evaluation results for test 1. The
upper graph shows the overall amount of time
taken to complete the test for different num-
bers of editions. The second graph shows the
time it took to just retrieve the list of all the
editions in the collection.
T
i
m
e
 
i
n
 
M
i
l
l
i
s
e
c
o
n
d
s
05?167
?
05. 167
?
052167
?
2167
?
25N167
?
ubmreO f E t di4ions
7 N777 ?777 . 777 2777 67
?
T
i
m
e
 
i
n
 
M
i
l
l
i
s
e
c
o
n
d
s
07
0N58
08
0058
27
2N58
ubmreO f E t di4ions
7 N777 ?777 . 777 2777 67
?
Figure 2: Evaluation results for test 2. The
upper graph gives the amount of overall time
elapsed in the retrieval of the subsections. The
lower graph gives the amount of time needed
on average per subsection retrieved. The av-
erage was rounded down.
7
Both measures show a slight increase as the
number of editions goes over 3000 but then
stabilise.
Overall the experiments show that handling
thousands of text is indeed feasible with our
implementation on a relatively modest server
even for the hardest possible task of recon-
structing all the texts in the collection from
their smallest parts. Subtasks that do not re-
quire retrieving all the texts show little impact
from increasing the number of editions.
5 Conclusion
This paper gave a short introduction into the
use of the Canonical Text Services Protocol
for Digital Humanities research. It also pre-
sented a new implementation of the CTS pro-
tocol that can handle large amounts of data.
The tools that we presented will be made avail-
able at:
http://ctstest.informatik.
uni-leipzig.de/
This address is also used to house the data
presented in the evaluation as well as some ad-
ditional statistics that were generated.
At the time of this writing a new version of
the CTS standard was close to completion. As
soon as it is published we plan to make our im-
plementation fully compliant. Currently there
are still some details in which our implemen-
tation diverges from this newest version of the
standard. Once this process is complete the
next step will be the creation of a perma-
nent CTS capable repository that will be inte-
grated with the CLARIN research infrastruc-
ture (Boehlke et al., 2013).
Acknowledgements
Parts of the work presented in this paper
are the result of the project ?Die Biblio-
thek der Milliarden Wo?rter?. This project is
funded by the European Social Fund. ?Die
Bibliothek der Milliarden Wo?rter? is a coop-
eration project between the Leipzig Univer-
sity Library, the Natural Language Processing
Group at the Institute of Computer Science at
Leipzig University, and the Image and Signal
Processing Group at the Institute of Computer
Science at Leipzig University.
References
Bridget Almas and Marie-Claire Beaulieu. 2013.
Developing a new integrated editing platform
for source documents in classics. Literary and
Linguistic Computing, 28(4):493?503.
Bridget Almas and Monica Berti. 2013. Perseids
collaborative platform for annotating text re-
uses of fragmentary authors. In DH-Case 2013.
Collaborative Annotations in Shared Environ-
ments: metadata, vocabularies and techniques in
the Digital Humanities.
Bridget Almas, Monica Berti, Dave Dubin, Greta
Franzini, and Simona Stoyanova. 2014. The
linked fragment: TEI and the encoding of text
reuses of lost authors. paper submitted to the
Journal of the Text Encoding Initiative - Issue 8
- Selected Papers from the 2013 TEI Conference.
Rudolf Bayer and Edward Meyers McCreight.
1972. Organization and maintenance of large or-
dered indexes. Acta Informatica, 1(3):173?189.
Monica Berti, Matteo Romanello, Alison Babeu,
and Gregory Crane. 2009. Collecting fragmen-
tary authors in a digital library. In Proceedings
of the 9th ACM/IEEE-CS joint conference on
Digital Libraries, pages 259?262.
Volker Boehlke, Gerhard Heyer, and Peter Witten-
burg. 2013. IT-based research infrastructures
for the humanities and social sciences ? devel-
opments, examples, standards, and technology.
it - Information Technology, 55(1):26?33.
Gregory Crane, Bridget Almas, Alison Babeu, Lisa
Cerrato, Matthew Harrington, David Bamman,
and Harry Diakoff. 2012. Student researchers,
citizen scholars and the trillion word library. In
Proceedings of the 12th ACM/IEEE-CS Joint
Conference on Digital Libraries, pages 213?222.
D. Neel Smith and Christopher W. Blackwell.
2012. Four URLs, limitless apps: Separation of
concerns in the Homer Multitext architecture.
In Donum natalicium digitaliter confectum Gre-
gorio Nagy septuagenario a discipulis collegis
familiaribus oblatum: A Virtual Birthday Gift
Presented to Gregory Nagy on Turning Seventy
by His Students, Colleagues, and Friends. The
Center of Hellenic Studies of Harvard Univer-
sity.
D. Neel Smith. 2009. Citation in classical studies.
Digital Humanities Quarterly, 3(1).
8
