Computation of modifier scope in NP by a language-neutral method 
 
Richard CAMPBELL 
Microsoft Research 
1 Microsoft Way 
Redmond, WA, USA 98052 
richcamp@microsoft.com 
 
Abstract  
The relative logical scope of multiple 
modifiers within NP is often semantically 
significant.  This paper proposes a structurally 
based method for computing the relative scope 
of such modifiers, based on their order, type, 
and syntactic complexity.  The algorithm is 
language-neutral, in that it works with 
minimal errors for a wide range of languages 
without language-specific stipulations. 
 
Introduction 
 
Noun phrases quite commonly have multiple 
modifiers, including quantifiers, attributive 
adjective phrases, relative clauses, possessors, 
appositives and the like.  As frequently noted in the 
literature (e.g., Shaw and Hatzivassiloglou, 1999), 
the linear order of modifiers can signify their 
logical scope (though other factors are involved, 
too), as in the English examples (1) and (2) 
(bracketing indicates logical scope): 
 
(1) my [favorite [new movie]] 
(2) my [new [favorite movie]] 
 
In (1) favorite modifies the phrase new movie; 
hence the NP refers to my favorite among the new 
movies (there may be an old movie I like better); in 
(2) new modifies favorite movie; hence the NP 
refers to my favorite movie, which has just become 
my favorite.   
 The computation of the scope of modifiers is 
of inherent linguistic interest:  it is necessary for 
determining the correct interpretation of NPs like 
(1) and (2).  It follows that it is potentially useful in 
any application that may depend on such an 
interpretation.  In addition, for multilingual 
applications such as transfer-based machine 
translation (MT) (as discussed for example by 
Richardson et al (2001)) modifier scope may itself 
be used as an abstract, language-neutral 
representation of their surface configuration, 
including linear order.  The generation component 
of the MT application could then make use of 
scope information, perhaps in addition to 
scope-independent ordering conventions (Malouf, 
2000), to generate the modifiers in the correct 
order. 
 The focus of the current paper is a method for 
computing the relative scope of modifiers based on 
structural information, which works independently 
of any particular language; that is, the same 
algorithm that computes the scope of the modifiers 
in the English NP (3)  also correctly computes 
modifier scope in (4), its French translation, even 
though the two examples do not have exactly 
parallel surface structures: 
 
(3) the [twenty-ninth [American state]] 
(4) le [vingt-neuvi?me [?tat am?ricain]] 
 the twenty-ninth      state American 
 
The proposed algorithm considers several 
structural factors in addition to linear order, 
including the type and internal structure of the 
modifiers themselves.  The algorithm described 
here is currently implemented in the NLPWin 
system at Microsoft Research (Heidorn, 2000). 
 The paper is organized as follows:  Section 1 
examines the various structural factors that 
determine modifier scope, and a preliminary 
algorithm for modifier scope assignment is 
proposed; in Section 2, we compare the predictions 
of the algorithm to a diverse set of examples from 
six languages, and propose a revised algorithm; 
Section 3 considers some related work; and 
Section 4 is a conclusion. 
 
1 Modifier scope 
 
English examples like (1) and (2) seem to show 
that linear order is a principal factor in determining 
the scope of modifiers.  Examination of a wider 
range of examples from a variety of languages 
makes it clear that matters are not that simple.  In 
this section, we explore some ways in which strict 
linear order is not sufficient to determine the scope 
of modifiers. 
 
1.1 Order of postnominal modifiers 
 
Strictly speaking, linear order plays only an 
indirect role in scope assignment, the relevant 
ordering factor being distance from the head noun:  
all else being equal, modifiers that are farther from 
the head noun in the surface structure have scope 
over modifiers that are closer to the head. 
 Linear order plays a different role in 
determining scope, depending on whether the 
modifiers precede or follow the head noun; this is 
illustrated by the Spanish examples (5) and (6): 
 
(5) una [[moneda americana] falsa] 
 a        coin      American   counterfeit 
 ?a counterfeit American coin? 
(6) una [[moneda falsa] americana]   
 ?an American counterfeit coin? 
 
The NPs in (5) and (6) differ only in modifier order 
and indeed this reflects their relative scope; but 
unlike the prenominal modifiers in (1) and (2), the 
postnominal modifiers in (5) and (6) have a scope 
order that is the reverse of their linear order:  (5) 
refers to an American coin that is counterfeit, e.g. a 
fake half-dollar, perhaps produced in Canada; (6) 
refers to a counterfeit coin from America, e.g. a 
fake Canadian dollar produced in the U.S. 
 The difference in the way that linear order of 
pre- and postnominal modifiers determines scope 
makes it clear that in a multilingual application 
such as MT, it is not sufficient merely to record the 
order of modifiers, since the order must be 
reversed going e.g. from Spanish to English.  A 
more straightforward way to record the 
information stored in the linear order of modifiers 
in the source language would be to record their 
relative distance from the head noun.  Given the 
frequent occurrence in many languages of NPs 
with both pre- and postnominal modifiers, as in (4), 
above, this can only be accomplished by recording 
the logical scope of the modifiers. 
 
1.2 Quantifier-like adjectives 
 
Relative distance from the head is only obvious in 
cases of multiple prenominal or multiple 
postnominal modifiers.  In many languages, 
however, it is relatively common for an NP to 
contain both pre- and postnominal modifiers, as in 
(4).  In such a case, the notion of relative distance 
from the head is unavailable as a guide to relative 
scope.  The following examples illustrate the 
problem: 
 
(7) the [heaviest [isotope found in nature]] 
(8) a [[new domain] devoted to insects and  
  worms] 
 
The prenominal adjective in (7) has wider scope 
than the postnominal participial phrase; thus the 
NP refers to the heaviest member of the set of 
isotopes of some element that are found in nature, 
and not e.g. to the heaviest isotope overall of that 
element.  In (8) on the other hand, the postnominal 
participial phrase has wider scope than the 
prenominal adjective:  the NP refers to a new 
domain or classification (invertebrates), which is 
devoted to insects and worms, and not, e.g., to a 
new member of the set of domains so devoted. 
 The relevant difference between these 
examples is the prenominal modifier:  superlatives, 
along with comparatives, ordinals, certain other 
adjectives that are quantifier- or determiner-like, 
such as another, certain, numerous, and only (all 
the preceding are henceforth referred to as ?q-like 
adjectives?), along with quantifiers take wider 
scope than most postnominal modifiers, while 
everyday modifiers like new generally take 
narrower scope when prenominal.  Assuming that 
the class of q-like adjectives can be identified, a 
scope assignment algorithm needs to take this into 
account as well. 
 
1.3 Nonrestrictive postnominal modifiers 
 
Nonrestrictive modifiers generally take very wide 
scope, typically having wider scope than any 
restrictive modifiers or quantifiers; compare (7), 
above, with (9): 
 
(9) the [[heaviest isotope], which is found in  
  nature] 
 
Unlike (7), this NP does refer to the heaviest 
isotope (of some element), and not just to the 
heaviest isotope found in nature, because the 
nonrestrictive relative clause has wider scope than 
the prenominal adjective. 
 In English and other languages, nonrestrictive 
postnominal relative clauses, adjective phrases, 
and participial clauses are easily identifiable by the 
preceding comma or other structural cues.  
Whether a modifier is restrictive or nonrestrictive 
is clearly relevant to the computation of its scope 
relative to other modifiers; however in some 
languages, notably German and Japanese, there are 
often no reliable structural cues to whether a 
relative clause is restrictive or not.  This part of the 
scope computation algorithm can therefore only be 
expected to work in languages where this 
information is available in the input. 
 
1.4 Competing principles of scope 
assignment 
 
In most cases, the various principles of scope 
assignment outlined in this section are not in 
conflict.  For instance, q-like adjectives, when 
prenominal, typically precede other prenominal 
adjectives.  In some cases this is not true, however, 
and in many such cases, relative distance from the 
head noun is a principle of last resort only.  A good 
illustration of this fact comes from Japanese, 
where all relative clauses are prenominal, and 
often precede other prenominal modifiers, such as 
adjective phrases and quantifiers; this is illustrated 
by the following examples: 
 
(10) 	
 
higai-ni          a-tta   shuyou-na    toshi 
damage-DAT encountered  major-ADN cities 
'major cities that were damaged? 
(11)   
 sakana-ga  tabe-ru    arayuru esa 
 fish-NOM eat-PRES all         bait 
 'all bait which fish eat' 
 
In (10) the relative clause has wider scope than the 
following adjective, but in (11) the quantifier 
arayuru ?all? has wider scope than the preceding 
relative clause.  The principle that such adjectives 
and quantifiers are assigned wider scope takes 
precedence over the principle that assigns scope on 
the basis of relative distance from the head noun. 
 
1.5 Algorithm for computing modifier 
scope ? First pass 
 
Based on these observations, a simple, 
language-neutral algorithm can be formulated to 
compute modifier scope based on structural factors.  
As a first step, we factor all modifiers into three 
categories:  nonrestrictive modifiers, quantifiers 
and q-like adjectives, and other modifiers.  For 
practical purposes, nonrestrictive modifiers are 
limited to postnominal relative clauses, adjective 
phrases, and participial clauses, that have some 
structural indication of their nonrestrictiveness, 
such as being preceded by a comma; in principle, 
however, any nonrestrictive modifier should fall 
into this category.  Q-like adjectives include 
comparatives, superlatives, ordinals, and modifiers 
(e.g. only) that are marked in the dictionary as 
being able to occur before a determiner.  Also, if a 
q-like adjective is prenominal, then any other 
adjective that precedes it is treated as if it were 
q-like; if the q-like adjective is postnominal, then 
any other adjective that follows it is treated as if 
q-like.  In this paper, PPs, possessors and 
appositive NPs are not treated.  The algorithm is 
described in (I): 
 
I. Computation of modifier scope 
 1. nonrestrictive modifiers have wider scope 
than all other groups; 
 2. quantifiers and q-like adjectives have 
wider scope than other modifiers not 
covered in (I.1); 
 3. within each group, assign wider scope to 
postnominal modifiers over prenominal 
modifiers; 
 4. among postnominal modifiers in the same 
group, or among prenominal modifiers in 
the same group, assign wider scope to 
modifiers farther from the head noun. 
 
Consider the NP in (12), which, though awkward, 
will serve as illustration of how (I) works: 
 
(12) [[two [other [[counterfeit [American coins]]  
  produced here]]], which I saved] 
 
The nonrestrictive (set off by commas) relative 
clause which I saved is assigned widest scope by 
(I.1); the quantifier two and the q-like adjective 
other have wider scope than the remaining 
modifiers (I.2), and two has wider scope than other 
(I.4); the participial clause produced here has 
wider scope than counterfeit and American (I.3); 
finally, counterfeit has wider scope than American 
(I.4). 
 For a more realistic illustration, consider again 
(3) and (4), repeated here: 
 
(3) the [twenty-ninth [American state]] 
(4) le [vingt-neuvi?me [?tat am?ricain]] 
 the twenty-ninth      state American 
 
In both (3) and (4), the ordinal is assigned wider 
scope than American/am?ricain, regardless of the 
latter?s position in the NP, by principle (I.2).   
Finally, consider again (5), repeated here: 
 
(5) una [[moneda americana] falsa] 
 a        coin       American   false 
 
By (I.4), falsa is assigned wider scope than 
americana.  
 
2 Examination of broader range of 
examples 
 
The algorithm in (I) is motivated by a small set of 
examples; a broader range of examples is needed 
to really determine whether (I), and especially its 
claim to language-neutrality, is viable.  The 
purpose is not to provide a statistical measure of 
the accuracy of the algorithm, but simply to 
provide a set of examples that is larger, more 
diverse and more realistic than could have been 
devised by introspection. 
 
2.1 Testing (I) 
 
Initial sets of examples from six languages, 
Chinese, English, French, German, Japanese and 
Spanish, were collected; the Chinese examples 
were taken from Du Zhe (?Reader?), for all other 
languages, the examples were taken from Encarta 
Encyclopedia.  To be selected, sentences had to 
contain an NP with exactly one prenominal and 
one postnominal modifier (thus (I.4) was not 
directly tested).  Several hundred such examples 
were then given to teams of two native speaker 
linguists per language to determine (a) whether it 
made any difference which modifier was assigned 
wider scope, and (b) if so, which one had wider 
scope; for an NP to make it into the final set of 
examples, both annotators had to agree on both (a) 
and (b).  Annotators aimed for approximately 100 
examples for which the answer to (a) was positive; 
in practice, between 50 and 100 examples for each 
language made it into the final set.   
 The annotated examples were then analyzed 
using the NLPWin system, which incorporates this 
algorithm, the results manually compared to the 
annotations, and differences (errors) examined.  In 
some cases, errors could be attributed to 
misanalysis or other deficiencies of the system, 
independently of the scope assignment algorithm; 
these might include syntactic errors (e.g. English 
court favorite analyzed with court as the head 
noun and favorite as a postnominal adjective), or 
lexical/morphological problems (e.g. Sp. 
numeroso ?numerous? not recognized as a 
quantifier, failure to treat Fr. Premier ministre as a 
single noun, etc.).  Overall, the results were quite 
good for English (3 errors in 97 examples, 
including system errors), German (6/81) and 
Chinese (2/66), mediocre for Spanish (13/60) and 
Japanese (15/59), and poor for French (46/78). 
 In addition to system errors, certain patterns 
emerged.  The most obvious one was that, in many 
languages, prenominal adjectives meaning ?same? 
or ?different?, which should take wider scope than 
a nonrestrictive postnominal modifier, were 
assigned narrower scope according to (I.3); a good 
example from Spanish is given below: 
 
(13) el [mismo [a?o que contrajo matrimonio con 
Carmen Polo]] 
?the same year that he contracted marriage 
with Carmen Polo? 
 
Adjectives meaning ?same?, ?different? and ?other? 
have much in common with comparatives; for 
example, English same, different, and other can 
take as- or than-complements.  Our system does 
not currently encode this similarity in any 
systematic way, but if it did so, examples like (13) 
would be handled correctly by (I.2).1 
 
2.2 Syntactic complexity 
 
An unexpected pattern among the errors, however, 
had to do with the internal structure of postnominal 
modifiers.  The French examples (14) and (15) 
have the same prenominal adjective: 
 
(14) de [nouvelles [valeurs culturelles]] 
       new         values  cultural 
 ?new cultural values? 
(15) un [[nouveau domaine] consacr? aux  
  insectes et aux vers] 
 ?a new domain devoted to insects and  
  worms? 
 
The relevant difference in this case is in the 
postnominal modifier:  in (14), the postnominal 
adjective is syntactically simple, containing no 
syntactic dependents of its own, while in (15) the 
postnominal participial clause is complex, 
containing a complement prepositional phrase.  
(I.3) incorrectly assigns wider scope to the 
postnominal modifier than to the prenominal 
modifier in (14); an unexpectedly large proportion 
of the French and Spanish errors from this example 
set were of this type. 
 This leads to a revision of the scope 
assignment algorithm that treats syntactically 
simple (unmodified) postnominal modifiers as a 
special case, getting assigned narrower scope than 
regular prenominal modifiers: 
 
II. Computation of modifier scope, revised 
 1. nonrestrictive modifiers have wider scope 
than all other groups; 
 2. quantifiers and q-like adjectives have 
wider scope than other modifiers not 
covered in (II.1);  
 3. syntactically complex postnominal 
modifiers that are not relative clauses have 
                                                     
1 The special status of such adjectives has been noted in 
other contexts:  For example, Hawkins (1978) groups 
same together with superlatives into a class of 
?unexplanatory? modifiers; Vieira and Poesio (2000), 
extending this class to include only and a few others, 
make use of them in identifying discourse-new definite 
descriptions. 
wider scope than other modifiers not 
covered by (II.1-2); 
 4.  prenominal modifiers not covered by 
(II.1-3) have wider scope than other 
modifiers not covered by (II.1-3); 
 5. otherwise, within each group, assign wider 
scope to postnominal modifiers over 
prenominal modifiers; 
 6. among postnominal modifiers in the same 
group, or among prenominal modifiers in 
the same group, assign wider scope to 
modifiers farther from the head noun. 
 
The difference between (I) and (II) is in (II.3) and 
(II.4), which ensure that syntactically complex 
postnominal modifiers have wider scope than 
non-quantificational prenominal ones, and that 
prenominal modifiers have wider scope than 
syntactically simple postnominal ones.  In this case, 
?syntactically complex? means (a) if not a 
coordinate structure, then there are non-head 
constituents; and (b) if a coordinate structure, then 
at least one of the conjuncts is syntactically 
complex.   
 Implementing the revised algorithm (II) into 
the system reduced the number of French and 
Spanish errors in the example set considerably:  
French went from 46 errors in 78 examples to 2; 
Spanish went from 13/60 to 5; all other languages 
remained essentially the same. 
 To ensure that the revision from (I) to (II) was 
not tailored specifically to the example set, a 
second set of randomly selected examples were 
annotated as before; the examples were then 
analyzed using the system incorporating (II), and 
the results compared to the annotation.  
 
2.3 Discussion  
 
The results bear out the essential correctness of (II), 
while at the same time highlighting areas in which 
further refinement is possible.  For each language 
there were just a handful of errors in this example 
set :  Chinese had 3 in 57 total examples, English 
10/98, French 4/55, German 4/45, Spanish 8/96 
and Japanese 8/55.  Since the test is not meant to be 
a statistical measure of accuracy, it is important to 
examine the errors.  Two English and two Spanish 
errors were system errors of the kind described in 
Section 2.1.  The problem of identifying 
nonrestrictive relative clauses arose as well, 
accounting for one English and one German error, 
and possibly all the Japanese errors.   
 Of the remainder, both the French and Spanish 
sets contained errors which suggest simple 
modifications to (II), e.g. where both the 
prenominal and postnominal modifiers were q-like, 
and for which (II.5) incorrectly assigns wider 
scope to the postnominal.  Another error that could 
be handled by a simple refinement of (II) is Sp. los 
grandes reba?os de ovejas no estabulados, ?the large 
unstabled herds of sheep?; in this case the 
postnominal adjective phrase no estabulados is 
treated as syntactically complex, and incorrectly 
assigned scope wider than grandes by (II.3); it seems 
simple enough to modify (II.3) to take account of 
negative morphemes and the like.   
 Another category of error that suggests a further 
refinement is suggested by the Spanish example (16) 
and by English (17): 
 
(16) una [[quinta cl?usula] que no tuvo efecto] 
 a        fifth    clause    that not took effect 
(17) [[longer poems] written from 1789 on]  
 
In both cases, (II.2) incorrectly assigns wider 
scope to the prenominal adjective than to the 
postnominal one.  Examples such as these suggest 
that q-like adjectives, or at least comparatives and 
ordinals, should be treated as in (II.2) (i.e., as 
taking wider scope than other modifiers) only in 
NPs that are definite.  One Spanish, one English, 
and two German examples of this kind occurred in 
this example set; since there are not many errors of 
this kind, it is not clear how much would be gained 
from this modification. 
 Aside from such systematic errors, the French 
set contained only one error that is irredeemable; 
i.e., which (II) could not handle even with perfect 
input, without being supplemented by lexically 
specific, and hence language-specific, rules; the 
Spanish set contained only three irredeemable 
errors; English had three, and German one. 
 While it is worth noting that (II) is not without 
counterexamples, it is significant that true 
counterexamples are evidently rare enough in 
actually occuring text, at least when compared to 
examples for which (II) predicts the correct scope 
assignment, that (II) appears to be very promising 
as a means for computing modifier scope in 
arbitrary languages. 
 
2.4 Why complexity? 
 
It is not clear exactly why the syntactic complexity 
of a postnominal modifier affects its scope relative 
to other modifiers in NP.  It may be significant that 
postnominal modifiers that are themselves 
modified tend to be participial or relative clauses, 
rather than adjective phrases.  Participial and 
relative clauses are always intersecting in their 
interpretation, meaning that the denotation of the 
noun + modifier construction is the intersection of 
the set denoted by the noun and the set denoted by 
the modifier;  adjectives, on the other hand, are 
often non-intersecting (Keenan and Faltz, 1985).  
It is possible then that a deeper principle underlies 
(II.3), namely that intersecting modifiers take 
wider scope than non-intersecting ones. 
 It remains to be explained why prenominal 
adjectives typically take wider scope than 
syntactically simple postnominal ones.  One 
possible explanation is that N + unmodified Adj 
combinations, such as Fr. valeurs culturelles in 
(14), are often analyzed by native speakers as a 
kind of compound; i.e., as though the Adj were 
incorporated into the N to form a complex word.   
 Typically the parts of a complex word cannot 
be individually syntactically modified.  It is to be 
expected, then, that a prenominal modifier such as 
nouvelles in (14) would be unable to modify 
valeurs by itself, but must modify the whole 
compound.  Moreover, it is to be expected that the 
adjective in the N + Adj compound could not have 
modifiers.  Consequently, a noun + [adjective + PP] 
construction, such as Fr. huiles originaires des 
r?gions m?diterran?ennes ?oils originating from 
mediterranean regions?, could not be analyzed as a 
compound; instead the postnominal adjective 
phrase would have to be a true phrasal modifier, 
taking wider scope than a prenominal modifier 
such as c?l?bres ?famous?, according to the general 
principles in (II). 
 This account is purely speculative, of course, 
and thus far untested.  Other kinds of explanation 
for (II.3) are possible as well, but limitations of 
space preclude substantial discussion of this issue. 
 
3 Related work 
 
Copestake et al (1995) briefly address the issue of 
adjective scope in an NP such as a fierce black cat.  
Since both adjectives are intersecting in this case, 
their relative scope is semantically irrelevant (a 
black fierce cat, though infelicitous, would mean 
the same thing).  Since a translation of this NP 
might have a different structure, e.g. Spanish gato 
feroz y negro lit. ?cat fierce and black?, Copestake 
et al argue that the logical form (LF) for such an 
NP ought to be flattened (i.e., no scope assigned to 
the adjectives), so as to ensure that the NP and its 
translation do not have syntactically different LFs, 
which is required for system-internal reasons. 
 If this conclusion is justified, it poses a 
problem for the approach taken in this paper, since 
(II) assigns wider scope to fierce.  Most 
dictionaries do not mark adjectives according to 
whether they are intersecting or not, nor do we 
know of any large corpora that are annotated in this 
way; therefore, it seems unlikely that (II) could be 
reliably turned off just in those cases where scope 
assignment is logically unnecessary. 
 Copestake et al?s conclusion is based on the 
system-internal assumption that transfer-based MT 
requires the LF of the input (e.g. gato feroz y negro) 
to have the same syntactic structure as the LF of 
the output (e.g. fierce black cat).  However, a 
transfer-based MT system in which transfer rules 
are learned from aligned corpora, such as 
described by Richardson et al (2001), does not 
have this requirement; hence the problem 
Copestake et al discuss does not arise. 
 
4 Conclusion 
 
The fact that (II) works so well across a variety of 
languages is of inherent linguistic interest, as it 
suggests that the cross-linguistic variation in word 
order within NP, while considerable, is 
nevertheless subject to universal principles that 
enable the relative scope of modifiers to be 
recovered.  These principles take account 
primarily of the type of modifier, but also of their 
placement relative to one another and, at least in 
the case of postnominal restrictive modifiers, their 
internal structure, assigning wider scope to 
modifiers that are themselves modified, perhaps 
because they are intersecting.  Application of the 
algorithm to more languages would of course be 
required to fully substantiate this claim. 
 The success of (II) is of substantial practical 
interest, as well, since it does not make 
unreasonable demands on the.  For example, 
although (II.3) may ultimately derive from a 
deeper principle that assigns wider scope to 
intersecting modifiers, it is not necessary to 
identify modifiers as (non-)intersecting for (II.3) to 
work, since syntactic complexity works well 
enough.  To work correctly, (II) requires 
quantifiers to be distinguished from adjectives, 
adjectives to be identified as superlative, 
comparative, ordinal or as able to occur before a 
determiner, and postnominal modifiers to be 
marked as nonrestrictive.  The first two 
requirements are reasonable things to expect of 
any parser; the third requirement is not easily met 
in all languages, but even in those languages where 
nonrestrictives are not easily identifiable, (II) 
works reasonable well. 
 
Acknowledgements 
 
I would like to thank my colleagues in the NLP 
group at MSR, especially Mike Carlson and 
Hisami Suzuki, for their help.   
 
References 
 
Copestake A., Flickinger D., Malouf R., Riehemann S. 
and Sag I. (1995) Translation using Minimal 
Recursion Semantics.  In ?Proceedings of TMI 6,? 
Leuven, Belgium. 
Hawkins J. (1978) Definiteness and Indefiniteness.  
Croom Helm, London, 316 p. 
Heidorn G.E. (2000) Intelligent Writing Assistance.  In 
?Handbook of Natural Language Processing?, R. 
Dale et al, ed., Marcel Dekker, New York, pp. 
181-207. 
Keenan E.L. and Faltz L.M. (1985) Boolean Semantics 
for Natural Language.  D. Reidel Publishing Co., 
Dordrecht, 387 p. 
Malouf R. (2000) The order of prenominal adjectives in 
natural language generation.  In ?Proceedings of the 
38th Annual Meeting of the ACL,? pp. 85-92. 
Richardson S., Dolan W., Menezes A. and Pinkham J. 
(2001) Achieving commercial-quality translation 
with example-based methods. In ?Proceedings of the 
VIIIth MT Summit?, Santiago de Compostela, Spain. 
293-298. 
Shaw J. and Hatzivassiloglou V. (1999) Ordering 
among premodifiers.  In ?Proceedings of the 37th 
Annual Meeting of the ACL,? pp. 135-143. 
Vieira R. and Poesio M. (2000) An Empirically Based 
System for Processing Definite Descriptions.  
Computational Linguistics 26/4, pp. 539-593. 
Using linguistic principles to recover empty categories 
Richard CAMPBELL 
Microsoft Research  
One Microsoft Way 
Redmond, WA 98052 
USA 
richcamp@microsoft.com 
 
Abstract 
This paper describes an algorithm for 
detecting empty nodes in the Penn Treebank 
(Marcus et al, 1993), finding their 
antecedents, and assigning them function tags, 
without access to lexical information such as 
valency.  Unlike previous approaches to this 
task, the current method is not corpus-based, 
but rather makes use of the principles of early 
Government-Binding theory (Chomsky, 
1981), the syntactic theory that underlies the 
annotation.  Using the evaluation metric 
proposed by Johnson (2002), this approach 
outperforms previously published approaches 
on both detection of empty categories and 
antecedent identification, given either 
annotated input stripped of empty categories 
or the output of a parser.  Some problems with 
this evaluation metric are noted and an 
alternative is proposed along with the results.  
The paper considers the reasons a principle-
based approach to this problem should 
outperform corpus-based approaches, and 
speculates on the possibility of a hybrid 
approach. 
1 Introduction 
Many recent approaches to parsing (e.g. Charniak, 
2000) have focused on labeled bracketing of the 
input string, ignoring aspects of structure that are 
not reflected in the string, such as phonetically null 
elements and long-distance dependencies, many of 
which provide important semantic information 
such as predicate-argument structure.  In the Penn 
Treebank (Marcus et al, 1993), null elements, or 
empty categories, are used to indicate non-local 
dependencies, discontinuous constituents, and 
certain missing elements.  Empty categories are 
coindexed with their antecedents in the same 
sentence.  In addition, if a node has a particular 
grammatical function (such as subject) or semantic 
role (such as location), it has a function tag 
indicating that role; empty categories may also 
have function tags.  Thus in the sentence below, 
who is coindexed with the empty category *T* in 
the embedded S; the function tag SBJ indicates that 
this empty category is the subject of that S: 
 
[WHNP-1 who] NP want [S [NP-SBJ-1*T*] to VP] 
 
Empty categories, with coindexation and function 
tags, allow a transparent reconstruction of 
predicate-argument structure not available from a 
simple bracketed string.   
In addition to bracketing the input string, a fully 
adequate syntactic analyzer should also locate 
empty categories in the parse tree, identify their 
antecedents, if any, and assign them appropriate 
function tags.  State-of-the-art statistical parsers 
(e.g. Charniak, 2000) typically provide a labeled 
bracketing only; i.e., a parse tree without empty 
categories.  This paper describes an algorithm for 
inserting empty categories in such impoverished 
trees, coindexing them with their antecedents, and 
assigning them function tags.  This is the first 
approach to include function tag assignment as part 
of the more general task of empty category 
recovery. 
Previous approaches to the problem (Collins, 
1997; Johnson, 2002; Dienes and Dubey, 2003a,b; 
Higgins, 2003) have all been learning-based; the 
primary difference between the present algorithm 
and earlier ones is that it is not learned, but 
explicitly incorporates principles of Government-
Binding theory (Chomsky, 1981), since that theory 
underlies the annotation.  The absence of rule-
based approaches up until now is not motivated by 
the failure of such approaches in this domain; on 
the contrary, no one seems to have tried a rule-
based approach to this problem.  Instead it appears 
that there is an understandable predisposition 
against rule-based approaches, given the fact that 
data-driven, especially machine-learning, 
approaches have worked so much better in many 
other domains.1 
Empty categories however seem different, in 
that, for the most part, their location and existence 
is determined, not by observable data, but by 
explicitly constructed linguistic principles, which 
                                                     
1Both Collins (1997: 19) and Higgins (2003: 100) are 
explicit about this predisposition. 
were consciously used in the annotation; i.e., 
unlike overt words and phrases, which correspond 
to actual strings in the data, empty categories are in 
the data only because linguists doing the 
annotation put them there.  This paper therefore 
explores a rule-based approach to empty category 
recovery, with two purposes in mind:  first, to 
explore the limits of such an approach; and second, 
to establish a more realistic baseline for future 
(possibly data-driven or hybrid) approaches. 
Although it does not seem likely that any 
application trying to glean semantic information 
from a parse tree would care about the exact string 
position of an empty category, the algorithm 
described here does try to insert empty categories 
in the correct position in the string.  The main 
reason for this is to facilitate comparison with 
previous approaches to the problem, which 
evaluate accuracy by including such information.  
In Section 5, however, a revised evaluation metric 
is proposed that does not depend on string position 
per se. 
Before proceeding, a note on terminology is in 
order.  I use the term detection (of empty 
categories) for the insertion of a labeled empty 
category into the tree (and/or string), and the term 
resolution for the coindexation of the empty 
category with its antecedent(s), if any.  The term 
recovery refers to the complete package:  
detection, resolution, and assignment of function 
tags to empty categories. 
2 Empty nodes in the Penn Treebank 
The major types of empty category in the Penn 
Treebank (PTB) are shown in Table 1, along with 
their distribution in section 24 of the Wall Street 
Journal portion of the PTB.   
 
Empty 
category type 
Count Description 
NP * 1044 NP trace or PRO 
NP *T* 265 Trace of WHNP 
*U* 227 Empty unit 
0 178 Empty complementizer 
ADVP *T* 97 Trace of WHADVP 
S *T* 76 Trace of topicalized 
quoted S 
WHNP 0 43 Null WHNP 
SBAR 41 Trace of topicalized 
non-quoted S 
WHADVP 0 25 Null WHADVP 
others 95  
Total: 2091  
Table 1:  Common empty categories and their 
distribution in section 24 of the PTB 
 
A detailed description of the categories and their 
uses in the treebank is provided in Chapter 4 of the 
annotation guidelines (Bies et al, 1995).  
Following Johnson (2002) and Dienes and Dubey 
(2003a), the compound empty SBAR consisting of 
an empty complementizer followed by *T* of 
category S is treated as a single item for purposes 
of evaluation.  This compound category is labeled 
SBAR in Table 1. 
The PTB annotation in general, but especially 
the annotation of empty categories, follows a 
modified version of Government-Binding (GB) 
theory (Chomsky, 1981).  In GB, the existence and 
location of empty categories is determined by the 
interaction of linguistic principles.  In addition, the 
type of a given empty category is determined by its 
syntactic context, with the result that the various 
types of empty category are in complementary 
distribution.  For example, the GB categories NP-
trace and PRO (which are conflated to a single 
category in the PTB) occur only in argument 
positions in which an overt NP could not occur, 
namely as the object of a passive verb or as the 
subject of certain kinds of infinitive. 
3 Previous work 
Previous approaches to this task have all been 
learning-based.  Collins? (1997) Model 3 integrates 
the detection and resolution of WH-traces in 
relative clauses into a lexicalized PCFG.  Collins? 
results are not directly comparable to the works 
cited below, since he does not provide a separate 
evaluation of the empty category detection and 
resolution task. 
Johnson (2002) proposes a pattern-matching 
algorithm, in which the minimal connected tree 
fragments containing an empty node and its 
antecedent(s) are extracted from the training 
corpus, and matched at runtime to an input tree.  
As in the present approach, Johnson inserts empty 
nodes as a post-process on an existing tree.  He 
proposes an evaluation metric (discussed further 
below), and presents results for both detection and 
detection plus resolution, given two different kinds 
of input:  perfect trees (with empty nodes removed) 
and parser output. 
Dienes and Dubey (2003a,b), on the other hand, 
integrate their empty node resolution algorithm 
into their own PCFG parser.  They first locate 
empty nodes in the string, taking a POS-tagged 
string as input, and outputting a POS-tagged string 
with labeled empty nodes inserted.  The PCFG 
parser is then trained, using the enhanced strings as 
input, without inserting any additional empty 
nodes.  Antecedent resolution is handled by a 
separate post-process.  Using Johnson?s (2002) 
evaluation metric, Dienes and Dubey present 
results on the detection task alone (i.e., inserting 
empty categories into the POS-tagged string), as 
well as on the combined detection and resolution 
tasks in combination with their parser.2 
Higgins (2003) considers only the detection and 
resolution of WH-traces, and only evaluates the 
results given perfect input.  Higgins? method, like 
Johnson?s (2002) and the present one, involves 
post-processing of trees.  Higgins? results are not 
directly comparable to the other works cited, since 
he assumes all WH-phrases as given, even those 
that are themselves empty. 
4 The recovery algorithm 
4.1 The algorithm 
The proposed algorithm for recovering empty 
categories is shown in Figure 1; the algorithm 
walks the tree from top to bottom, at each node X 
deterministically inserting an empty category of a 
given type (usually as a daughter of X) if the 
syntactic context for that type is met by X.  It 
makes four separate passes over the tree, on each 
pass applying a different set of rules. 
 
1   for each tree, iterate over nodes from top down 
2       for each node X 
3 try to insert NP* in X 
4 try to insert 0 in X 
5 try to insert WHNP 0  or WHADVP 0 in X 
6 try to insert *U* in X 
7 try to insert a VP ellipsis site in X 
8 try to insert S*T* or SBAR in X 
9 try to insert trace of topicalized XP in X 
10 try to insert trace of extraposition in X 
11   for each node X 
12 try to insert WH-trace in X 
13   for each node X 
14 try to insert NP-SBJ * in finite clause X 
15   for each node X 
16 if X = NP*, try to find antecedent for X 
Figure 1:  Empty category recovery algorithm 
 
The rules called by this algorithm that try to 
insert empty categories of a particular type specify 
the syntactic context in which that type of empty 
category can occur, and if the context exists, 
specify where to insert the empty category.  For 
example, the category NP*, which conflates the 
GB categories NP-trace and PRO, occurs typically3 
                                                     
2 It is unclear whether Dienes and Dubey?s evaluation 
of empty category detection is based on actual tags 
provided by the annotation (perfect input), or on the 
output of a POS-tagger. 
3 NP* is used in roles that go beyond the GB notions 
of NP-trace and PRO, including e.g. the subject of 
as the object of a passive verb or as the subject of 
an infinitive.  The rule which tries to insert this 
category and assign it a function tag is called in 
line 3 of Figure 1 and given in pseudo-code in 
Figure 2.  Some additional rules are given in the 
Appendix. 
 
if X is a passive VP & X has no complement S 
if there is a postmodifying dangling PP Y 
     then insert NP* before all postmodifiers of Y 
 else insert NP* before all postmodifiers of X 
else if X is a non-finite S and X has no subject 
 then insert NP-SBJ* after all premodifiers of X 
Figure 2:  Rule to insert NP* 
 
This rule, which accounts for about half the 
empty category tokens in the PTB, makes no use of 
lexical information such as valency of the verb, 
etc.  This is potentially a problem, since in GB the 
infinitives that can have NP-trace or PRO as 
subjects (raising and control infinitives) are 
distinguished from those that can have overt NPs 
or WH-trace as subjects (exceptional-Case-
marked, or ECM, infinitives), and the distinction 
relies on the class of the governing verb.   
Nevertheless, the rules that insert empty nodes 
do not have access to a lexicon, and very little 
lexical information is encoded in the rules:  
reference is made in the rules to individual 
function words such as complementizers, 
auxiliaries, and the infinitival marker to, but never 
to lexical properties of content words such as 
valency or the raising/ECM distinction.  In fact, the 
only reference to content words at all is in the rule 
which tries to insert null WH-phrases, called in 
line 5 of Figure 1:  when this rule has found a 
relative clause in which it needs to insert a null 
WH-phrase, it checks if the head of the NP the 
relative clause modifies is reason(s), way(s), 
time(s), day(s), or place(s); if it is, then it inserts 
WHADVP with the appropriate function tag, rather 
than WHNP. 
The rule shown in Figure 2 depends for its 
successful application on the system?s being able 
to identify passives, non-finite sentences, heads of 
phrases (to identify pre- and post-modifiers), and 
functional information such as subject; similar 
information is accessed by the other rules used in 
the algorithm.  Simple functions to identify 
passives, etc. are therefore called by the 
implemented versions of these rules.  Functional 
information, such as subject, can be gleaned from 
the function tags in the treebank annotation; the 
rules make frequent use of a variety of function 
tags as they occur on various nodes.  The output of 
                                                                                   
imperatives; see below. 
Charniak?s parser (Charniak, 2000), however, does 
not include function tags, so in order for the 
algorithm to work properly on parser output (see 
Section 5), additional functions were written to 
approximate the required tags.  Presumably, the 
accuracy of the algorithm on parser output would 
be enhanced by accurate prior assignment of the 
tags to all relevant nodes, as in Blaheta and 
Charniak (2000) (see also Section 5). 
Each empty category insertion rule, in addition 
to inserting an empty node in the tree, also may 
assign a function tag to the empty node.  This is 
illustrated in Figure 2, where the final line inserts 
NP* with the function tag SBJ in the case where it 
is the subject of an infinitive clause. 
The rule that inserts WH-trace (called in line 12 
in Figure 1) takes a WHXP needing a trace as 
input, and walks the tree until an appropriate 
insertion site is found (see Appendix for a fuller 
description).  Since this rule requires a WHXP as 
input, and that WHXP may itself be an empty 
category (inserted by an earlier rule), it is handled 
in a separate pass through the tree. 
A separate rule inserts NP* as the subject in 
sentences which have no overt subject, and which 
have not had a subject inserted by any of the other 
rules.  Most commonly, these are imperative 
sentences, but calling this rule in a separate pass 
through the tree, as in Figure 1, ensures that any 
subject position missed by the other rules is filled. 
Finally, a separate rule tries to find an 
antecedent for NP* under certain conditions.  The 
antecedent of NP* may be an empty node inserted 
by rules in any of the first three passes through the 
tree, even the subject of an imperative; therefore 
this rule is applied in a separate pass through the 
tree.  This rule is also fairly simple, assigning the 
local subject as antecedent for a non-subject NP*, 
while for an NP* in the subject position of a non-
finite S it searches up the tree, given certain 
locality conditions, for another NP subject. 
All the rules that insert empty categories are 
fairly simple, and derive straighforwardly from 
standard GB theory and from the annotation 
guidelines.  The most complex rule is the rule that 
inserts WH-trace when it finds a WHXP daughter 
of SBAR; most are about as simple as the rule 
shown in Figure 2, some more so.  Representative 
examples are given in the Appendix. 
4.2 Development method 
After implementing the algorithm, it was run over 
sections 1, 3, and 11 of the WSJ portion of the 
PTB, followed by manual inspection of the trees to 
perform error analysis, with revisions made as 
necessary to correct errors.  Initially sections 22 
and 24 were used for development testing.  
However, it was found that these two sections 
differ from each other substantially with respect to 
the annotation of antecedents of NP* (which is 
described somewhat vaguely in the annotation 
guidelines), so all of sections 2-21 were used as a 
development test corpus.  Section 23 was used 
only for the final evaluation, reported in Section 5 
below. 
5 Evaluation 
Following Johnson (2002), the system was 
evaluated on two different kinds of input:  first, on 
perfect input, i.e., PTB annotations stripped of all 
empty categories and information related to them; 
and second, on imperfect input, in this case the 
output of Charniak?s (2000) parser.  Each is 
discussed in turn below. 
5.1 Perfect input 
The system was run on PTB trees stripped of all 
empty categories.  To facilitate comparison to 
previous approaches, we used Johnson?s label and 
string position evaluation metric, according to 
which an empty node is identified by its label plus 
its string position, and evaluated the detection task 
alone.  We then evaluated detection and resolution 
combined, identifying each empty category as 
before, plus the label and string position of its 
antecedent, if any, again following Johnson?s 
work.   
The results are shown in Table 2.  Precision here 
and throughout is the percentage of empty nodes 
proposed by the system that are in the gold 
standard (section 23 of the PTB), recall is the 
percentage of empty nodes in the gold standard 
that are proposed by the system, and F1 is balanced 
f-measure; i.e., 2PR/(P+R). 
 
Task Prec. Rec. F1 
Detection only 94.9 91.1 93.0 
Detection + resolution 90.1 86.6 88.4 
Table 2:  Detection and resolution of empty cate-
gories given perfect input (label + string position 
method), expressed as percentage 
 
These results compare favorably to previously 
reported results, exceeding them mainly by 
achieving higher recall.  Johnson (2002) reports 
93% precision and 83% recall (F1 = 88%) for the 
detection task alone, and 80% precision and 70% 
recall (F1 = 75%) for detection plus resolution.  In 
contrast to Johnson (2002) and the present work, 
Dienes and Dubey (2003a) take a POS-tagged 
string, rather than a tree, as input; they report 
86.5% precision and 72.9% recall (F1 = 79.1%) on 
the detection task.  For Dienes and Dubey, the 
further task of finding antecedents for empty 
categories is integrated with their own PCFG 
parser, so they report no numbers directly relevant 
to the task of detection and resolution given perfect 
input. 
5.2 Parser output 
The system was also run using as input the output 
of Charniak?s parser (Charniak, 2000).  The 
results, again using the label and string position 
method, are given in Table 3. 
 
Task Prec. Rec. F1 
Detection only 85.2 81.7 83.4 
Detection + resolution 78.3 75.1 76.7 
Table 3:  Detection and resolution of empty 
categories on parser output (label + string position 
method), expressed as percentage 
 
Again the results exceed those previously reported.  
Johnson (2002) reports 85% precision and 74% 
recall (F1 = 79%) for detection and 73% precision 
and 63% recall (F1 = 68%) for detection plus 
resolution on the output of Charniak?s parser.  
Dienes and Dubey (2003b) integrate the results of 
their detection task into their own PCFG parser, 
and report 81.5% precision and 68.7% recall (F1 = 
74.6%) on the combined task of detection and 
resolution. 
5.3 Perfect input with no function tags 
The lower results on parser output obviously 
reflect errors introduced by the parser, but may 
also be due to the parser not outputting function 
tags on any nodes.  As mentioned in Section 4, it is 
believed that the results of the current method on 
parser output would improve if that output were 
reliably assigned function tags, perhaps along the 
lines of Blaheta and Charniak (2000).   
Testing this hypothesis directly is beyond the 
scope of the present work, but a simple experiment 
can give some idea of the extent to which the 
current algorithm relies on function tags in the 
input.  The system was run on PTB trees with all 
nodes stripped of function tags; the results are 
given in Table 4. 
 
Task Prec. Rec. F1 
Detection only 94.1 89.5 91.7 
Detection + resolution 89.5 85.2 87.3 
Table 4:  Detection and resolution of empty 
categories on PTB input without function tags 
(label + string position method), expressed as 
percentage 
 
While not as good as the results on perfect input 
with function tags, these results are much better 
than the results on parser output.  This suggests 
that function tag assignment should improve the 
results shown on parser output, but that the greater 
part of the difference between the results on perfect 
input and on parser output is due to errors 
introduced by the parser. 
5.4 Refining the evaluation 
The results reported in the previous subsections are 
quite good, and demonstrate that the current 
approach outperforms previously reported 
approaches on the detection and resolution of 
empty categories.  In this subsection some 
refinements to the evaluation method are 
considered. 
The label and string position method is useful if 
one sees the task as inserting empty nodes into a 
string, and thus is quite useful for evaluating 
systems that detect empty categories without parse 
trees, as in Dienes and Dubey (2003a).  However, 
if the task is to insert empty nodes into a tree, then 
the method leads both to false positives and to 
false negatives.  Suppose for example that the 
sentence When do you expect to finish? has the 
bracketing shown below, where ?1? and ?2? 
indicate two possible locations in the tree for the 
trace of the WHADVP: 
 
When do you [VP expect to [VP finish 1 ] 2 ] 
 
Suppose position 1 is correct; i.e. it represents the 
position of the trace in the gold standard.  Since 1 
and 2 correspond to the same string position, if a 
system inserts the trace in position 2, the string 
position evaluation method will count it as correct. 
This is a serious problem with the string-based 
method of evaluation, if one assumes, as seems 
reasonable, that the purpose of inserting empty 
categories into trees is to be able to recover 
semantic information such as predicate-argument 
structure and modification relations.  In the above 
example, it is clearly semantically relevant whether 
the system proposes that when modifies expect 
instead of finish. 
Conversely, suppose the sentence Who (besides 
me) cares? has the bracketing shown: 
 
Who [S 1 (besides me) 2 [VP cares]] 
 
Again suppose that position 1 represents the 
placement of the WHNP trace in the gold standard.  
If a system places the trace in position 2 instead, 
the string position method will count it as an error, 
since 1 and 2 have different string positions.  
However it is not at all clear what it means to say 
that one of those two positions is correct and the 
other not, since there is no semantic, grammatical, 
or textual indicator of its exact position.  If the task 
is to be able to recover semantic information using 
traces, then it does not matter in this case whether 
the system inserts the trace to the left or to the right 
of the parenthetical. 
Given that both false positives and false 
negatives are possible, I propose that future 
evaluations of this task should identify empty 
categories by their label and by their parent 
category, instead of, or perhaps in addition to, 
doing so by label and string position.  Since the 
parent of an empty node is always an overt node4, 
the parent could be identified by its label and string 
position (left and right edges).  Resolution is 
evaluated by a natural extension, by identifying the 
antecedent (which could itself be an empty 
category) according to its label and its parent?s 
label and string position.  This would serve to 
identify an empty category by its position in the 
tree, rather than in the string, and would avoid the 
false positives and false negatives described above. 
In addition to an evaluation based on tree 
position rather than string position, I propose to 
evaluate the entire recovery task, i.e., including 
function tag assignment, not just detection and 
resolution.  
The revised evaluation is still not perfect:  when 
inserting an NP* or NP*T* into a double-object 
construction, it clearly matters semantically 
whether it is the first or second object, though both 
positions have the same parent.5  Ideally, we would 
evaluate based on a richer set of grammatical 
relations than are annotated in the PTB, or perhaps 
based on thematic roles.  However, it is difficult to 
see how to accomplish this without additional 
annotation.  It is probable that constructions of this 
sort are relatively rare in the PTB in any case, so 
for now the proposed evaluation method, however 
imperfect, will suffice. 
The result of this revised evaluation, given 
perfect input, is presented in Table 5.  The first two 
rows are comparable to the string-based results in 
Table 2; the last row, showing the results of the 
full recovery task (i.e., including antecedents and 
function tags), is not much lower, suggesting that 
labeling empty categories with function tags does 
not pose any serious difficulties. 
 
                                                     
4  The only exception is the 0 complementizer and 
S*T* daughters of the SBAR category in Table 1; but 
since the entire SBAR is treated as a single empty node 
for evaluation purposes, this does not pose a problem. 
5 I am indebted to two ACL reviewers for calling this 
to my attention. 
Task Prec. Rec. F1 
Detection only 95.6 91.9 93.7 
Detection + resolution 90.8 87.3 89.0 
Recovery 
(det.+res.+func. tags) 
89.8 86.3 88.0 
Table 5:  Detection, resolution and recovery of 
empty categories given perfect input (label + 
parent method), expressed as percentage 
 
Three similar evaluations were also run, using 
parser output as input to the algorithm; the results 
are given in Table 6.   
 
Task Prec. Rec. F1 
Detection only 78.4 75.2 76.7 
Detection + resolution 72.3 69.3 70.8 
Recovery 
(det.+res.+func. tags) 
69.7 66.8 68.2 
Table 6:  Detection, resolution and recovery of 
empty categories on parser output (label + parent 
method), expressed as percentage 
 
The results here are less impressive, no doubt 
reflecting errors introduced by the parser in the 
labeling and bracketing of the parent category, a 
problem which does not affect a string-based 
evaluation.  However it does not seem reasonable 
to have an effective evaluation of empty node 
insertion in parser output that does not depend to 
some extent on the correctness of the parse.  The 
fact that our proposed evaluation metric depends 
more heavily on the accuracy of the input structure 
may be an unavoidable consequence of using a 
tree-based evaluation. 
6 Discussion 
The empty category recovery algorithm reported 
on here outperforms previously published 
approaches on the detection and resolution tasks; it 
also does well on the task of function tag 
assignment to empty categories, which has not 
been considered in other work.  As suggested in 
the introduction, the reason a rule-based approach 
works so well in this domain may be that empty 
categories are not naturally in the text, but are only 
inserted by the annotator, who is consciously 
following explicit linguistic principles, in this case, 
the principles of early GB theory. 
As a result, the recovery of empty categories is, 
for the most part, more amenable to a rule-based 
approach than to a learning approach.  It makes 
little sense to learn, for example, that NP* occurs 
as the object of a passive verb or as the subject of 
certain infinitives in the PTB, if that information is 
already explicit in the annotation guidelines. 
This is not to say that learning approaches have 
nothing to contribute to this task.  Information 
about individual lexical items, such as valency, the 
raising/ECM distinction, or subject vs. object 
control, which is presumably most robustly 
acquired from large amounts of data, would 
probably help in the task of detecting certain empty 
categories. 
Consider for example an input structure V [S to 
VP].  GB principles, which are enforced in the 
annotation guidelines, dictate that an empty 
category must be inserted as the subject of the 
infinitival S; but exactly which empty category, 
NP* or NP*T*, depends on properties of the 
governing verb, including whether it is a raising or 
control verb, such as seem or try, or an ECM verb, 
such as believe.  In the present algorithm, the rule 
that inserts NP* applies first, without access to 
lexical information of any kind, so NP* is inserted, 
instead of NP*T*, regardless of the value of V.  
This leads to some errors which might be corrected 
given learned lexical information.  Such errors are 
fewer than might have been expected, however:  
the present system achieved 97.7% precision and 
97.3% recall (F1 = 97.5%) on the isolated task of 
detecting NP*, even without lexical knowledge 
(see Table 7). 
A combined learning and rule-based algorithm 
might stand to make a bigger gain in the task of 
deciding whether NP* in subject position has an 
antecedent or not, and if it does, whether the 
antecedent is a subject or not.  The annotation 
guidelines and the theory that underlies it are less 
explicit on the principles underlying this task than 
they are on the other subtasks.  As a result, the 
accuracy of the current system drops considerably 
when this task is taken into account, from 97.5% 
F1 to 86.9% (see Table 7).  Dienes and Dubey 
(2003a), on the other hand, claim this as one of the 
strengths of their learning-based system. 
 
Empty 
category 
type 
Detection 
only (F1) 
Detection  
+ resolution (F1) 
NP* 97.5 86.9 
NP*T* 96.2 96.0 
*U* 98.6 - 
0 98.5 - 
ADVP*T* 79.9 79.9 
S*T* 92.7 92.7 
WHNP 0 92.4 - 
SBAR 84.4 84.4 
WHADVP 0 73.3 - 
Table 7:  F1 for detection and resolution of empty 
categories by type, using perfect input (label + 
parent method), expressed as percentage 
7 Conclusion 
In this paper I have presented an algorithm for the 
recovery of empty categories in PTB-style trees 
that otherwise lack them.  Unlike previous 
approaches, the current algorithm is rule-based 
rather than learning-based, which I have argued is 
appropriate for this task, given the highly 
theoretical nature of empty categories in the PTB.  
Moreover, the algorithm has no access to lexical 
information such as valency or verb class. 
Using the string-based evaluation metric 
proposed by Johnson (2002), the current system 
outperforms previously published algorithms on 
detection alone, as well as on detection combined 
with resolution, both on perfect input and in 
combination with parsing.  In addition, we have 
performed additional evaluation using a tree-based 
metric, and including an evaluation of function tag 
assignment as well. 
8 Acknowledgements 
I would like to thank Simon Corston-Oliver, Mark 
Johnson, and Hisami Suzuki for their helpful input. 
References  
Bies, A., M. Ferguson, K. Katz and R. MacIntyre.  
1995.  Bracketing Guidelines for Treebank II 
style Penn Treebank Project.  Linguistic Data 
Consortium. 
Blaheta, D. and E. Charniak.  2000.  Assigning 
Function Tags to Parsed Text.  In Proceedings of 
the North American Chapter of the Association 
for Computational Linguistics, pages 234-240. 
Charniak, E.  2000.  A maximum-entropy-inspired 
parser.  In In Proceedings of the North American 
Chapter of the Association for Computational 
Linguistics, pages 132-139. 
Chomsky, N.  1981.  Lectures on Government and 
Binding.  Foris Publications, Dordrecht. 
Collins, M.  1997.  Three Generative, Lexicalised 
Models for Statistical Parsing.  In Proceedings of 
the 35th Annual Meeting of the Association for 
Computational Linguistics, pages 16-23. 
Dienes, P. and A. Dubey.  2003a.  Deep Syntactic 
Processing by Combining Shallow Methods.  In 
Proceedings of the 41st Annual Meeting of the 
Association for Computational Linguistics, pages 
431-438. 
Dienes, P. and A. Dubey.  2003b.  Antecedent 
Recovery:  Experiments with a Trace Tagger.  In 
Proceedings of the Conference on Empirical 
Methods in Natural Language Processing, pages 
33-40. 
Higgins, D.  2003.  A machine-learning approach 
to the identification of WH gaps.  In Proceedings 
of the 10th Conference of the European Chapter 
of the Association for Computational Linguistics, 
pages 99-102. 
Johnson, M.  2002.  A simple pattern-matching 
algorithm for recovering empty nodes and their 
antecedents.  In Proceedings of the 40th Annual 
Meeting of the Association for Computational 
Linguistics, pages 136-143. 
Marcus, M., B. Santorini and M.A.Marcinkiewicz.  
1993.  Building a large annotated corpus of 
English:  The Penn Treebank.  Computational 
Linguistics, 19(2):313-330. 
 
Appendix:  Sample rules 
To insert 0 Comp: 
if X=SBAR & !Comp(X) & !WHXP daughter(X) 
& ? S daughter Y of X  
& !(parent(X)=NP & sister(X)=NP) 
 then insert 0 to left of Y 
 
To insert WHNP/WHADVP: 
if X=SBAR & parent(X)=NP  
& sister(X)=NP & !Comp(X)  
& !WHXP daughter(X) & ? S daughter Y of X 
 if head(parent(X)) in {reason(s) way(s)  
 time(s) day(s) place(s)} 
  then insert WHADVP to left of Y 
 else insert WHNP to left of Y 
 
To insert *U*: 
insert *U* / $ CD+ _ 
 
To insert WH-trace: 
if X=SBAR & ? S daughter Y of X  
& ? WHXP daughter W of X 
 then find trace(W) in Y 
 
To find trace(W) in X: 
 
insert trace: 
(for W = WHXP, insert XP*T*) 
if X has conjuncts 
 then find trace(W) in each conjunct of X 
else if X has a PP daughter Y with no object  
& W=WHNP 
 then insert *T* to right of P 
else if X=S and !subject(X) & W=WHNP 
 then insert *T* as last pre-mod of X 
else if X contains a VP Y 
 then find trace(W) in Y 
else if X contains ADJP or clausal complement Y 
& W=WHNP 
 then find trace(W) in Y 
else if W=WHNP  
& ? infinival rel. clause R, R=sister(W)  
& X=VP & X has an object NP  
& subject(R) is an empty node E 
 then insert *T* as last pre-mod of R 
 then delete E 
else if W=WHNP 
       then insert *T* as first post-mod of X 
else insert *T* as last post-mod of X 
 
assign function tag: 
if W = WHNP & *T* a pre-mod of S 
       then assign ?SBJ? to *T* 
if W = WHADVP & W is not empty  
       if W = ?why? 
 then assign ?PRP? to *T* 
       if W = ?when? 
 then assign ?TMP? to *T* 
       if W = ?where? 
 then assign ?LOC? to *T* 
       if W = ?how? 
 then assign ?MNR? to *T* 
else if W = WHADVP & parent(parent(W)) =NP  
       if head(sister(parent(W))) = ?reason(s)? 
 then assign ?PRP? to *T* 
       if head(sister(parent(W)))=?time(s)? or ?day(s)? 
 then assign ?TMP? to *T* 
       if head(sister(parent(W))) = ?place(s)? 
 then assign ?LOC? to *T* 
       if head(sister(parent(W))) = ?way(s)? 
 then assign ?MNR? to *T* 
 
Machine Translation as a testbed for multilingual analysis 
Richard Campbell, Carmen Lozano, Jessie Pinkham and Martine Smets* 
Microsoft Research 
One Microsoft Way 
Redmond, WA 98052  USA 
{richcamp, clozano, jessiep, martines}@microsoft.com 
*to whom all correspondence should be addressed 
 
 
Abstract 
We propose that machine translation (MT) is a 
useful application for evaluating and deriving 
the development of NL components, 
especially in a wide-coverage analysis system. 
Given the architecture of our MT system, 
which is a transfer system based on linguistic 
modules, correct analysis is expected to be a 
prerequisite for correct translation, suggesting 
a correlation between the two, given relatively 
mature transfer and generation components.  
We show through error analysis that there is 
indeed a strong correlation between the quality 
of the translated output and the subjectively 
determined goodness of the analysis.  We use 
this correlation as a guide for development of 
a coordinated parallel analysis effort in 7 
languages. 
1  Introduction 
The question of how to test natural language 
analysis systems has been central to all natural 
language work in the past two decades.  It is a 
difficult question, for which researchers have 
found only partial answers.  The most common 
answer is component testing, where the component 
is compared against a standard of goodness, 
usually the Penn Treebank for English (Marcus et 
al., 1993),  allowing a numerical score of precision 
and recall (e.g. Collins, 1997). 
 Such methods have limitations, however, and 
need to be supplemented by additional methods.  
One limitation is the availability of annotated 
corpora, which do not exist for all languages.  
Secondly, comparison to an annotated corpus can 
only measure how well a system produces the kind 
of analysis for which the corpus is annotated, e.g. 
labeled bracketing of surface syntax.  Evaluation 
of analysis of deeper, more semantically 
descriptive, levels requires additional annotated 
corpora, which may not exist.  A more 
fundamental limitation of such methods is that 
they measure the goodness of a grammar without 
taking into account what the grammar is good for.  
This limitation is overcome, we claim, only by 
measuring the goodness of a grammar by its 
success in real-world applications. 
 We propose that machine translation (MT) is a 
good application to evaluate and drive the 
development of analysis components when the 
transfer component is based on linguistic modules.  
Multi-lingual applications such as MT allow 
evaluation of system components that overcomes 
the limitations mentioned above, and therefore 
serves as a useful complement to other evaluation 
techniques.  Another significant advantage to 
using MT as a testbed for the analysis system is 
that it prioritizes analysis problems, highlighting 
those problems that have the greatest negative 
effect on translation output. 
 In this paper, we give an overview of 
NLPWin, a multi-application natural language 
analysis and generation system under development 
at Microsoft Research (Jensen et al, 1993; Gamon 
et al, 1997; Heidorn 2000), incorporating analysis 
systems for 7 languages (Chinese, English, French, 
German, Japanese, Korean and Spanish). Our 
discussion focuses on a description of the three 
components of the analysis system (called sketch, 
portrait and logical form) with a particular 
emphasis on the logical form derived as the end-
product, which serves as the medium for transfer 
in our MT system.  
 We also give an overview of the architecture 
of the MSR-MT system, and of the evaluation we 
use to measure correctness of the translations. We 
demonstrate the correlation between the scores 
assigned to translation outputs and the correctness 
of the analysis, using as illustration two language-
pairs at different stages of development:  Spanish-
English (SE) translation, as a testbed for the 
Spanish analysis system, and French-English (FE) 
translation, as a testbed for the French analysis 
system.   
2  Overview of the analysis component of 
NLPWin 
Analysis produces three representations for the 
input sentence: sketch, portrait and logical form1.  
Sketch is the initial tree representation for the 
sentence, along with its associated attribute-value 
structure. An example of sketch is given in Figure 
1, which shows the sketch tree for sentence (1). 
 
(1) 
Ce  format est pris   en charge par Windows 2000 
this format is   taken in charge by  Windows 2000 
?This format is supported by Windows 2000? 
 
 
Figure 1 :  Sketch analysis of (1) 
 
Attachment sites for post-modifiers are not 
determined in sketch.  In most cases, the 
information available as the syntactic tree is built 
is not sufficient to determine where e.g. 
prepositional phrases or relative clauses should be 
attached. Post-modifiers are thus systematically 
attached to the closest possible attachment site, 
and reattached, if necessary, by the reattachment 
module, a set of heuristic rules. 
 Reattachment rules apply to the sketch to 
produce the portrait; the portrait analysis of (1) is 
given in Figure 2, where the PP expressing the 
agent of the passive construction, originally 
attached to PP1 in sketch (see Figure 1) has been 
reattached at the sentence level. 
 
                                                     
1 The presentation of the analysis module is very 
simplified, but sufficient for our current discussion. 
More details can be found in the references.  
 
Figure 2: Portrait analysis of (1) 
 
 The portrait is the input to the computation of 
the logical form (LF), a labeled directed unordered 
graph representing the deep syntactic relations 
among the content words of the sentence (i.e., 
basic predicate-argument structure), along with 
some semantic information, such as functional 
relations expressed by certain prepositions.2 At 
this level, the difference between active and 
passive constructions is normalized; control 
relations and long-distance dependencies, such as 
subjects of infinitives, arguments associated with 
gaps, etc., are resolved.  The LF of (1) is shown in 
Figure 3.  Note that the surface subject of the 
passive is rendered as the Dobj (deep object) in 
LF, and the par-phrase as the Dsub (deep subject). 
 
 
Figure 3 :  LF analysis of (1) 
 
 Modifications to any of the analysis 
components are tested using monolingual 
regression files containing thousands of analyzed 
sentences; differences caused by the modification 
are examined manually by the linguist responsible 
for the change (Suzuki, 2002).  This process serves 
as an initial screening to ensure that modifications 
to the analysis have the desired effect. 
3 MSR-MT 
In this section we review the basics of the MSR-
MT translation system and its evaluation.  The 
reader is referred to Pinkham et al (2001) and 
Richardson et al (2001) for further details on the 
French and Spanish versions of the system. The 
overall architecture and basic component structure 
                                                     
2   LF as described here corresponds to the PAS 
representation of Campbell and Suzuki (2002). 
are the same for both the FE and SE versions of 
the system. 
3.1 Overview 
MSR-MT uses the broad coverage analysis system 
described in Section 2, a large multi-purpose 
source-language dictionary, a learned bilingual 
dictionary, an application independent target-
language generation component and a transfer 
component. 
 The transfer component consists of transfer 
patterns automatically acquired from sentence-
aligned bilingual corpora (described below) using 
an alignment algorithm described in detail in 
Menezes and Richardson (2001). Training takes 
place on aligned sentences which have been 
analyzed by the source- and target-language 
analysis systems to yield logical forms. The 
logical form structures, when aligned, allow the 
extraction of lexical and structural translation 
correspondences which are stored for use at 
runtime in the transfer database. See Figure 4 for 
an overview of the training process. 
 The transfer database is trained on 350,000 
pairs of aligned sentences from computer manuals 
for SE, and 500,000 pairs of aligned Canadian 
parliamentary data (the Hansard corpus) for FE.  
 
 
Figure 4:  MSR-MT training phase 
3.2   Evaluation of MSR-MT 
Seven evaluators are asked to evaluate the same 
set of sentences. For each sentence, raters are 
presented with a reference sentence, the original 
English sentence from which the human French 
and Spanish translations were derived, and MSR-
MT?s machine translation.3 In order to maintain 
                                                     
3 Microsoft manuals are written in English and 
translated by hand into other languages. We use these 
translations as input to our system, and translate them 
back into English. 
consistency among raters who may have different 
levels of fluency in the source language, raters are 
not shown the original French or Spanish sentence 
(for similar methodologies, see Ringger et al, 
2001; White et al, 1993).  
 All the raters enter scores reflecting the 
absolute quality of the translation as compared to 
the reference translation given. The overall score 
of a sentence is the average of the scores given by 
the seven raters. Scores range from 1 to 4, with 1 
meaning unacceptable (not comprehensible), 2 
meaning possibly acceptable (some information is 
transferred accurately), 3 meaning acceptable (not 
perfect, but accurate transfer of all important 
information, and 4 meaning ideal (grammatically 
correct and all the important information is 
transferred).  
4 Examples from FE and SE 
In this section we discuss specific examples to 
illustrate how results from MT evaluation help us 
to test and develop the analysis system. 
4.1  FE translation: the Hansard corpus 
The evaluation we are discussing in this section 
was performed in January 2002, at the beginning 
of our effort on the Hansard corpus. The 
evaluation was performed on a corpus of 250 
sentences, of which 55.6% (139 sentences) were 
assigned a score of 2 or lower, 30.4% (76 
sentences) were assigned a score greater than 2 but 
not greater than 3, and 14% (35 sentences) were 
assigned a score greater than 3. 
 Examination of French sentences receiving 
low-score translations led to the identification of 
some classes of analysis problems, such as the 
following: 
- mis-identification of vocatives 
- clefts not represented correctly 
- mis-analysis of ce qui / ce que free relatives 
- bad representation of complex inversion 
(pronoun-doubling of inverted subject) 
- no treatment of reflexives 
- fitted parses (i.e., not spanning the sentence) 
Most of the problematic structures are 
characteristic of spoken language as opposed to 
more formal, written styles (vocatives, clefts, 
direct questions), and had not been encountered in 
our previous work, which had involved mostly 
translation of technical manuals. Other problems 
(free relatives, reflexives) are analysis issues that 
we had not yet addressed. Fitted parses are parses 
that do not span the whole sentence, but are pieced 
together by the parser from partial parses; fitted 
parses usually result in poor translations. 
 Examples of translations together with their 
score are given in Table I. The source sentences 
are the French sentences, the reference sentence is 
the human translation to which the translation is 
compared by the evaluators, and the translation is 
the output of MSR-MT. Each of the three 
categories considered above is illustrated by an 
example. 
 Sentence (2) (with a score of 1.5) is a direct 
question with complex inversion and the doubled 
subject typical of that construction. In the LF for 
(2), les ministres des finances is analyzed as a 
modifier, because the verb r?unir already has a 
subject, the pronoun ils ?they?.  There are a couple 
of additional problems with this sentence: si is 
analyzed as the adverb meaning ?so? instead of as 
the conjunction meaning ?if?, and a direct question 
is analyzed as a complement clause; the sketch and 
LF analyses of this sentence are given in the 
Appendix..  The MSR-MT translation of this 
sentence has a very low score, reflecting the 
severity of the analysis problems. 
 The two other sentences, on the other hand, do 
not have analysis problems: the poor translation of 
(3) (score 2.16) is caused by bad alignment (droit 
translates as right instead of law), and the 
translation of (4) (score 3) is not completely fluent, 
but this is due to an English generation problem, 
rather than to a French analysis problem. This last 
sentence is the most correct with appropriate 
lexical items and has the highest score of the three. 
 Of the 139 sentences with score 2 or lower, 
73% were due to analysis problems, and 24% to 
alignment problems. Most of the rest had bugs 
related to the learned dictionary.  There were a few 
cases of very free translations, where the reference 
translation was very far from the French sentence, 
and our translation, based on the source sentence, 
was therefore penalized.  
 These figures show that, at this stage of 
development of our system, most of the problems 
in translation come from analysis. Translation can 
be improved by tackling analysis problems 
exhibited by the lowest scoring sentences, and, 
conversely, analysis issues can be discovered by 
looking at the sentences with the lowest translation 
score.  
 The next section gives examples of issues with 
the SE system, which is more mature than the FE 
system. 
 
4.2  SE translation: Technical manuals 
An evaluation of the Spanish-English MT system 
was also performed in January 2002, after work on 
the MT system had been progressing for 
approximately a year and a half.  The SE system 
was developed and tested using a corpus of 
sentences from Microsoft technical manuals.  A 
set of 600 unseen sentences was used for the 
evaluation.  
 Out of a total of 600 sentences, the number of 
sentences with a score from 3 to 4 was 251 (42%), 
the number of sentences with a score greater than 
2 but less than 3 was 186 (31%), and the 
remaining 163 sentences, (27%) had a score of 2 
or lower. Of these 163 sentences with the lowest 
scores, 50% (82 sentences) had analysis problems, 
and 17% of them (29 sentences) had fitted parses.  
A few of the fitted parses, 7 sentences out of 29, 
had faulty input, e.g. input that contained unusual 
characters or punctuation, typos, or sentence 
fragments.   
 Typical analysis problems that led to poor 
translation in the SE system include the following: 
- incorrect analysis of arguments in relative 
clauses,  especially those with a single 
argument (and a possible non-overt subject) 
- failure to identify the referent of clitic le (i.e. 
usted ?you?) in imperative sentences in LF 
- mis-analysis of Spanish reflexive or 
se constructions in LF 
- incorrect syntactic analysis of homographs 
- incorrect analysis of coordination  
- mis-identification of non-overt or controlled 
subjects  
- fitted parses  
 Table II contains sample sentences from the 
SE evaluation.  For each row, the second column 
displays the Spanish source sentence with the 
reference sentence in the next column, the 
translation produced by the MT system is in the 
fourth column, and the score for the translation 
assigned by the human evaluators in the last 
column.    
# Source  Reference Translation Score
(2) Si tel n'?tait pas le cas, pourquoi les 
ministres des Finances des provinces se 
seraient-ils r?unis hier pour essayer de 
s'entendre sur un programme commun ? 
soumettre au ministre des Finances? 
If that were not the case, 
why were the finance 
ministers of the provinces 
coalescing yesterday to try 
and come up with a joint 
program to bring to the 
finance minister?. 
Not was the case that they have 
the ministers met why 
yesterday Finances of the 
provinces trying to agree on a 
common program to bring 
Finances for the minister this so 
like? 
1.5 
(3) Nous ne pouvons pas appuyer cette 
motion apr?s que le Bloc qu?b?cois ait 
refus? de reconna?tre la primaut? du droit 
et de la d?mocratie pour  tous. 
 
We cannot support this 
motion after seeing the 
Bloc Quebecois refuse to 
recognize the rule of law 
and the principle of 
democracy for all. 
We cannot support this motion 
after the Bloc Quebecois has 
refused to recognize the rule of 
the right and democracy for all. 
2.16 
(4) En tant que membre de l'opposition 
officielle, je continuerai d'exercer des 
pressions sur le gouvernement pour qu'il 
tienne ses promesses ? cet ?gard. 
As a member of the official 
opposition I will continue 
to pressure the government 
to fulfil its promises in this 
regard. 
As member of the official 
opposition, I will continue to 
exercise pressures on the 
government for it to keep its 
promises in this regard. 
3 
Table I:  Examples of FE translation 
# Source Reference Translation Score
(5) Este procedimiento s?lo es aplicable si 
est? ejecutando una versi?n de idioma de 
Windows 2000 que no coincida con el 
idioma en el que desee escribir. 
This procedure applies only 
if you are running a 
language version of 
Windows 2000 that doesn't 
match the language you 
want to type 
This procedure only applies if 
you are running a Windows 
2000 language version that does 
not match the language that you 
want to type. 
3.8 
(6) Repita este proceso hasta que haya 
eliminado todos los componentes de red 
desde las propiedades de Red, haga clic 
en Aceptar y, a continuaci?n, haga clic 
en S? cuando se le pregunte si desea 
reiniciar el equipo. 
Repeat this process until 
you have deleted all of the 
network components from 
Network properties, click 
OK, and then click Yes 
when you are prompted to 
restart your computer. 
Repeat this process until you 
have deleted all of the network 
components from the Network 
properties, you click OK, and 
you click Yes then when asking 
that to restart the computer is 
wanted for him. 
2.0 
(7) En el siguiente ejemplo se muestra el 
nombre de la presentaci?n que se est? 
ejecutando en la ventana de presentaci?n 
con diapositivas uno. 
The following example 
displays the name of the 
presentation that's currently 
running in slide show 
window one. 
In the following example, the 
display name that is being run 
in the slide show window is 
displayed I join. 
1.4 
Table II:  Examples of SE translation 
 
 In the evaluation process, human evaluators 
compared the MT translation to the reference 
sentence, in the manner described in Section 4.1.   
 Example (5), with a score of 3.8, illustrates the 
fact that human evaluators considered the 
translation ?a Windows 2000 language version? to 
be a slightly worse translation than ?a language 
version of Windows 2000? for una version de 
idioma de Windows 2000; however the difference 
is so slight as to not be considered an analysis 
problem. 
 Example (6) illustrates the failure to identify 
usted ?you? (understood as the subject of the 
imperative) as the referent of the pronominal clitic 
le; as mentioned above, this is a common source of 
bad SE translations.  The last example (7) is a 
sentence with a fitted parse due to misanalysis of a 
word as its homograph :  uno is analyzed as the 
first person singular present form of the verb unir 
?join? instead of as the noun uno ?one?; the LF of 
this sentence is given in the Appendix. 
4.3 Discussion 
The examples discussed in this section are typical:  
The sentences for which MSR-MT produces better 
translations tend to be the ones with fewer analysis 
errors, while those which are misanalyzed tend to 
be mistranslated. 
 In this way, evaluation of MT output serves as 
one way to prioritize analysis problems; that is, to 
decide which among the many different analysis 
problems lead to the most serious problems.  For 
example, the poor quality of the translation of (2) 
highlights the need for an improved analysis of 
complex inversion in the French grammar, which 
will need to be incorporated into the sketch and/or 
LF components.  Similarly, the poor translation of 
(7) indicates the need to deal better with 
homographs in the Spanish morphological or 
sketch   component. 
 More generally, the analysis of FE and SE 
translation problems has led to the lists of analysis 
problems given in Sections 4.1 and 4.2, 
respectively.  Analysis problems identified in this 
way then become priorities for grammar/LF 
development. 
5 Conclusion 
We have outlined how the output of MT can be 
used as testbed for linguistic analysis in the source 
language, supplementing other methods.  The 
main advantage of this approach, in our view, is 
that it helps to prioritize analysis problems, 
highlighting those which have the most direct 
bearing on the application(s), the correct 
functioning of which is the main goal of the 
system. 
Acknowledgements 
This paper represents the work of many people in 
the NLP group at MSR; we acknowledge their 
contributions. 
References  
Campbell, R. and H. Suzuki.  2002.  Language-neutral 
representation of syntactic structure.  In R. Malaka, 
R. Porzel and M. Stube, eds., Proceedings of the First 
International Workshop on Scalable Natural 
Language Understanding. 
Collins, M. 1997. Three generative, lexicalised models 
for statistical parsing. Proceedings of the 35th Annual 
Meeting of the ACL, Madrid.  
Gamon, M., C. Lozano, J. Pinkham and T. Reutter. 
1997. Practical experience with grammar sharing in 
multilingual NLP. In Burstein J., Leacock C., eds, 
Proceedings of the Workshop on Making NLP Work, 
ACL Conference, Madrid. 
Heidorn, G.  2000.  Intelligent writing assistance.  In R. 
Dale, H. Moisl and H. Somers, eds., Handbook of 
Natural Language Processing. 
Jensen, K., G. Heidorn and S. Richardson, eds. 1993. 
Natural Language Processing: The PLNLP Approach, 
Boston, Kluwer. 
Marcus, M., B. Santorini, and M. Marcinkiewicz. 1993. 
Building a large annotated corpus of English: the 
Penn Treebank. In Proceedings of the 31st Annual 
Meeting of the ACL. 
Menezes, A. and S. Richardson. 2001. A Best-First 
Alignment Algorithm for Automatic Extraction of 
Transfer Mappings from Bilingual Corpora. In 
Proceedings of the Data-Driven MT workshop, ACL 
2001. 
Pinkham, J., M. Corston-Oliver, M. Smets and M. 
Pettenaro, 2001.  Rapid assembly of a large-scale 
French-English MT system. In Proceedings of the 
2001 MT Summit. 
Richardson, S., W.B. Dolan, A. Menezes and J. 
Pinkham. 2001. Achieving commercial-quality 
translation with example-based methods.  In 
Proceedings of the 2001 MT Summit. 
Ringger, E.K., M. Corston-Oliver, and R.C. Moore. 
2001. Using Word-Perplexity for Automatic 
Evaluation of Machine Translation. Unpublished ms. 
Suzuki, H.  2002.  A development environment for 
large-scale multi-lingual parsing systems.  Workshop 
on Grammar Engineering and Evaluation, COLING 
2002. 
White, J.S., T.A. O'Connell, and L.M. Carlson. 1993. 
Evaluation of machine translation. In Human 
Language Technology: Proceedings of a Workshop 
(ARPA). 206-210. 
Appendix 
 
Figure 5 :  Sketch analysis of (2) 
 
 
Figure 6 :  LF analysis of (2) 
 
 
Figure 7 :  LF analysis of (7)
 
        Task-focused Summarization of Email 
Simon Corston-Oliver, Eric Ringger, Michael Gamon and Richard Campbell 
Microsoft Research 
One Microsoft Way 
Redmond, WA 98052 USA 
{simonco, ringger, mgamon, richcamp}@microsoft.com 
 
 
 
 
Abstract 
 We describe SmartMail, a prototype system for 
automatically identifying action items (tasks) in 
email messages. SmartMail presents the user with 
a task-focused summary of a message. The 
summary consists of a list of action items extracted 
from the message. The user can add these action 
items to their ?to do? list. 
1 Introduction 
Email for many users has evolved from a mere 
communication system to a means of organizing 
workflow, storing information and tracking tasks 
(i.e. ?to do? items) (Bellotti et al, 2003; Cadiz et 
al., 2001). Tools available in email clients for 
managing this information are often cumbersome 
or even so difficult to discover that users are not 
aware that the functionality exists. For example, in 
one email client, Microsoft Outlook, a user must 
switch views and fill in a form in order to create a 
task corresponding to the current email message. 
By automatically identifying tasks that occur in the 
body of an email message, we hope to simplify the 
use of email as a tool for task creation and 
management. 
In this paper we describe SmartMail, a prototype 
system that automatically identifies tasks in email, 
reformulates them, and presents them to the user in 
a convenient interface to facilitate adding them to a 
?to do? list.  
SmartMail performs a superficial analysis of an 
email message to distinguish the header, message 
body (containing the new message content), and 
forwarded sections. 1  SmartMail breaks the 
                                                                 
1  This simple division into header, message body, and 
forwarded sections was sufficient for the corpus of email 
messages we considered. Messages containing original 
messages interleaved with new content were extremely 
message body into sentences, then determines 
the speech act of each sentence in the message 
body by consulting a machine-learned classifier. 
If the sentence is classified as a task, SmartMail 
performs additional linguistic processing to 
reformulate the sentence as a task description. 
This task description is then presented to the 
user. 
2 Data 
We collected a corpus of 15,741 email 
messages. The messages were divided into 
training, development test and blind test. The 
training set contained 106,700 sentences in 
message bodies from 14,535 messages. To 
avoid overtraining to individual writing styles, 
we limited the number of messages from a 
given sender to 50. To ensure that our 
evaluations are indicative of performance on 
messages from previously unencountered 
senders, we selected messages from 3,098 
senders, assigning all messages from a given 
sender to either the training or the test sets. 
Three human annotators labeled the message 
body sentences, selecting one tag from the 
following set: Salutation, Chit-chat (i.e., social 
discussion unrelated to the main purpose of the 
message), Task, Meeting (i.e., a proposal to 
meet), Promise, Farewell, various components 
of an email signature (Sig_Name, Sig_Title, 
Sig_Affiliation, Sig_Location, Sig_Phone, 
Sig_Email, Sig_URL, Sig_Other), and the 
default category ?None of the above?. The set of 
tags can be considered a set of application-
specific speech acts analogous to the rather 
particular tags used in the Verbmobil project, 
such as ?Suggest_exclude_date? and 
                                                                                                
uncommon in our corpus. Most senders were using 
Microsoft Outlook, which places the insertion point for 
new content at the top of the message. 
?Motivate_appointment? (Warnke et al, 1997; 
Mast et al, 1996) or the form-based tags of Stolcke 
et al (1998). 
All three annotators independently labeled 
sentences in a separate set of 146 messages not 
included in the training, development or blind test 
sets. We measured inter-annotator agreement for 
the assignment of tags to sentences in the message 
bodies using Cohen?s Kappa. Annotator 1 and 
annotator 2 measured 85.8%; annotator 1 and 
annotator 3 measured 82.6%; annotator 2 and 
annotator 3 measured 82.3%. We consider this 
level of inter-annotator agreement good for a novel 
set of application-specific tags. 
The development test and blind test sets of 
messages were tagged by all three annotators, and 
the majority tag for each sentence was taken. If any 
sentence did not have a majority tag, the entire 
message was discarded, leaving a total of 507 
messages in the development test set and 699 
messages in the blind test set. 
The set of tags was intended for a series of 
related experiments concerning linguistic 
processing of email. For example, greetings and 
chit-chat could be omitted from messages 
displayed on cell phones, or the components of an 
email signature could be extracted and stored in a 
contact database. In the current paper we focus 
exclusively on the identification of tasks. 
Annotators were instructed to mark a sentence 
as containing a task if it looked like an appropriate 
item to add to an on-going ?to do? list. By this 
criterion, simple factual questions would not 
usually be annotated as tasks; merely responding 
with an answer fulfills any obligation. Annotators 
were instructed to consider the context of an entire 
message when deciding whether formulaic endings 
to email such as Let me know if you have any 
questions were to be interpreted as mere social 
convention or as actual requests for review and 
comment. The following are examples of actual 
sentences annotated as tasks in our data: 
Since Max uses a pseudo-
random number generator, you 
could possibly generate the 
same sequence of numbers to 
select the same cases. 
 
Sorry, yes, you would have to 
retrain. 
 
An even fast [sic] thing 
would be to assign your own 
ID as a categorical feature. 
 
Michael, it?d be great if 
you could add some stuff re 
MSRDPS. 
 
Could you please remote 
desktop in and try running 
it on my machine. 
 
If CDDG has its own notion 
of what makes for good 
responses, then we should 
use that. 
 
3 Features 
Each sentence in the message body is described 
by a vector of approximately 53,000 features. 
The features are of three types: properties of the 
message (such as the number of addressees, the 
total size of the message, and the number of 
forwarded sections in the email thread), 
superficial features and linguistic features. 
The superficial features include word 
unigrams, bigrams and trigrams as well as 
counts of special punctuation symbols (e.g. @, 
/, #), whether the sentence contains words with 
so-called ?camel caps? (e.g., SmartMail), 
whether the sentence appears to contain the 
sender?s name or initials, and whether the 
sentence contains one of the addressees? names. 
The linguistic features were obtained by 
analyzing the given sentence using the NLPWin 
system (Heidorn 2000). The linguistic features 
include abstract lexical features, such as part-of-
speech bigrams and trigrams, and structural 
features that characterize the constituent 
structure in the form of context-free phrase 
structure rewrites (e.g., DECL:NP-VERB-NP; 
i.e., a declarative sentence consisting of a noun 
phrase followed by a verb and another noun 
phrase). Deeper linguistic analysis yielded 
features that describe part-of-speech 
information coupled with grammatical relations 
(e.g., Verb-Subject-Noun indicating a nominal 
subject of a verb) and features of the logical 
form analysis such as transitivity, tense and 
mood. 
 
4 Results 
We trained support vector machines (SVMs) 
(Vapnik, 1995) using an implementation of the 
sequential minimal optimization algorithm 
(Platt, 1999). We trained linear SVMs, which 
have proven effective in text categorization with 
large feature vectors (Joachims, 1998; Dumais et 
al., 1998).  
Figure 1 illustrates the precision-recall curve for 
the SVM classifier trained to distinguish tasks vs. 
non-tasks measured on the blind test set. 
We conducted feature ablation experiments on 
the development test set to assess the contribution 
of categories of features to overall classification 
performance. In particular we were interested in 
the role of linguistic analysis features compared to 
using only surface features. Within the linguistic 
features, we distinguished deep linguistic features 
(phrase structure features and semantic features) 
from POS n-gram features. We conducted 
experiments with three feature sets: 
1. all features (message level features + word 
unigram, bigram and trigram  
2. features + POS bigram and trigram 
features + linguistic analysis features) 
3. no deep linguistic features (no phrase 
structure or semantic features) 
4. no linguistic features at all (no deep 
linguistic features and no POS n-gram 
features) 
Based on these experiments on the development 
test set, we chose the feature set used for our run-
time applications.  
 
Figure 1 shows final results for these feature 
sets on the blind test set: for recall between 
approximately 0.2 and 0.4 and between 
approximately 0.5 and 0.6 the use of all features 
produces the best results. The distinction 
between the ?no linguistic features? and ?no 
deep linguistic features? scenarios is negligible; 
word n-grams appear to be highly predictive. 
Based on these results, we expect that for 
languages where we do not have an NLPWin 
parser, we can safely exclude the deeper 
linguistic features and still expect good 
classifier performance. 
 
 
Figure 2 illustrates the accuracy of 
distinguishing messages that contain tasks from 
those that do not, using all features. A message 
was marked as containing a task if it contained 
at least one sentence classified as a task. Since 
only one task has to be found in order for the 
entire message to be classified as containing a 
task, accuracy is substantially higher than on a 
per-sentence basis. In section 6, we discuss the 
scenarios motivating the distinction between 
sentence classification and message 
classification. 
 
 
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
Recall
Pr
ec
is
io
n
All features
No deep linguistic features
No linguistic features
 
 
Figure 1: Precision-Recall curves for ablation experiments 
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
Recall
P
re
ci
si
on
Per sentence
Per message
 
 
Figure 2: Precision-Recall curves comparing message classification and sentence classification 
 
 
5 Reformulation of Tasks 
SmartMail performs post-processing of sentences 
identified as containing a task to reformulate them 
as task-like imperatives. The process of 
reformulation involves four distinct knowledge-
engineered steps:  
1. Produce a logical form (LF) for the 
extracted sentence (Campbell and Suzuki, 
2001). The nodes of the LF correspond to 
syntactic constituents. Edges in the LF 
represent semantic and deep syntactic 
relations among nodes. Nodes bear 
semantic features such as tense, number 
and mood. 
2. Identify the clause in the logical form that 
contains the task; this may be the entire 
sentence or a subpart. We consider such 
linguistic properties as whether the clause 
is imperative, whether its subject is second 
person, and whether modality words such 
as please or a modal verb are used. All 
parts of the logical form not subsumed by 
the task clause are pruned. 
3. Transform the task portion of the LF to 
exclude extraneous words (e.g. please, 
must, could), extraneous subordinate 
clauses, adverbial modifiers, and vocative 
phrases. We replace certain deictic 
elements (i.e., words or phrases whose 
denotation varies according to the writer or 
the time and place of utterance) with non-
deictic expressions. For example, first 
person pronouns are replaced by either the 
name of the sender of the email or by a 
third person pronoun, if such a pronoun 
would unambiguously refer to the sender. 
Similarly, a temporal expression such as 
Thursday, which may refer to a different 
date depending on the week in which it is 
written, is replaced by an absolute date 
(e.g., 4/1/2004). 
4. Pass the transformed LF to a sentence 
realization module to yield a string 
(Aikawa et al, 2001). 
Below we illustrate the reformulation of tasks with 
some examples from our corpus. 
 
Example 1: 
On the H-1 visa issue, I am 
positive that you need to go 
to the Embassy in London to 
get your visa stamped into 
your passport. 
Reformulation: 
Go to the Embassy in London to 
get your visa stamped into 
your passport. 
 
In this example, the embedded sentential 
complement, that is, the part of the sentence 
following positive, is selected as the part of the 
sentence containing the task, because of the modal 
verb need and the second person subject; only that 
part of the sentence gets reformulated. The modal 
verb and the second person subject are deleted to 
form an imperative sentence. 
 
Example 2: 
Can you please send me the 
follow up information for the 
demo(s) listed in this Email 
ASAP. 
Reformulation: 
Send Kendall the follow up 
information for the demo 
listed in this Email ASAP. 
 
In this example, the whole sentence is selected 
as containing the task (modal verb, second person 
subject); modal elements including please are 
deleted along with the second person subject to 
form an imperative. In addition, the first person 
pronoun me is replaced by a reference to the 
sender, Kendall in this instance. 
 
Example 3: 
I've been Wednesday at the 
lecture on Amalgam you gave in 
the 113/1021 Room (which I 
really liked), and I've been 
wondering how feasible would 
it be to use Amalgam for 
learning requirements or code 
corpus structures and rules 
(and eventually rephrase them 
in some way). 
Reformulation: 
On June 5, 2002 Pablo wrote: 
?I've been Wednesday at the 
lecture on Amalgam you gave in 
the 113/1021 Room (which I 
really liked), and I've been 
wondering how feasible would 
it be to use Amalgam for 
learning requirements or code 
corpus structures and rules 
(and eventually rephrase them 
in some way).' 
 
This example illustrates what happens when 
NLPWin is unable to produce a spanning parse and 
hence a coherent LF; in this case NLPWin 
misanalyzed the clause following wondering as a 
main clause, instead of correctly analyzing it as a 
complement clause. SmartMail?s back-off strategy 
for non-spanning parses is to enclose the entire 
original sentence in quotes, prefixed with a matrix 
sentence indicating the date and the name of the 
sender. 
 
 
6 Task-Focused Summarization 
We have considered several scenarios for 
presenting the tasks that SmartMail identifies. 
Under the most radical scenario, SmartMail would 
automatically add extracted tasks to the user?s ?to 
do? list. This scenario has received a fairly 
negative reception when we have suggested it to 
potential users of a prototype. From an application 
perspective, this scenario is ?fail hard?; i.e., 
classification errors might result in garbage being 
added to the ?to do? list, with the result that the 
user would have to manually remove items. Since 
our goal is to reduce the workload on the user, this 
outcome would seem to violate the maxim ?First, 
do no harm?. 
 
Figure 3 and Figure 4 illustrate several ideas for 
presenting tasks to the user of Microsoft Outlook. 
Messages that contain tasks are flagged, using the 
existing flag icons in Outlook for proof of concept. 
Users can sort mail to see all messages containing 
tasks. This visualization amounts to summarizing 
the message down to one bit, i.e., +/- Task, and is 
conceptually equivalent to performing document 
classification. 
The right-hand pane in Figure 3 is magnified as 
Figure 4 and shows two more visualizations. At the 
top of the pane, the tasks that have been identified 
are presented in one place, with a check box beside 
them. Checking the box adds the task to the Tasks 
or ?to do? list, with a link back to the original 
message. This presentation is ?fail soft?: the user 
can ignore incorrectly classified tasks, or tasks that 
were correctly identified but which the user does 
not care to add to the ?to do? list. This list of tasks 
amounts to a task-focused summary of the 
document. This summary is intended to be read as 
a series of disconnected sentences, thus side-
stepping the issue of producing a coherent text 
from a series of extracted sentences. In the event 
that users prefer to view these extracted sentences 
as a coherent text, it may prove desirable to 
attempt to improve the textual cohesion by using 
anaphoric links, cue phrases and so on. 
Finally, Figure 3 also shows tasks highlighted in 
context in the message, allowing the user to skim 
the document and read the surrounding text. 
In the prototype we allow the user to vary the 
precision and recall of the classifier by adjusting a 
slider (not illustrated here) that sets the probability 
threshold on the probability of Task. 
 
Figure 3 and Figure 4 illustrate a convention that 
we observed in a handful of emails: proper names 
occur as section headings. These names have scope 
over the tasks enumerated beneath them, i.e. there 
is a list of tasks assigned to Matt, a list assigned to 
Eric or Mo, and a list assigned to Mo. SmartMail 
does not currently detect this explicit assignment 
of tasks to individuals. 
Important properties of tasks beyond the text of 
the message could also be automatically extracted. 
For example, the schema for tasks in Outlook 
includes a field that specifies the due date of the 
task. This field could be filled with date and time 
information extracted from the sentence containing 
the task. Similarly the content of the sentence 
containing the task or inferences about social 
relationships of the email interlocutors could be 
used to mark the priority of tasks as High, Low, or 
Normal in the existing schema. 
7 Conclusion 
In this paper we have presented aspects of 
SmartMail, which provides a task-oriented 
summary of email messages. This summary is 
produced by identifying the task-related sentences 
in the message and then reformulating each task-
related sentence as a brief (usually imperative) 
summation of the task. The set of tasks extracted 
and reformulated from a given email message is 
thus a task-focused summary of that message. 
We plan to conduct user studies by distributing 
the prototype as an Outlook add-in to volunteers 
who would use it to read and process their own 
mail over a period of several weeks. We intend to 
measure more than the precision and recall of our 
classifier by observing how many identified tasks 
users actually add to their ?to do? list and by 
administering qualitative surveys of user 
satisfaction. 
The ability to reformulate tasks is in principle 
separate from the identification of tasks. In our 
planned usability study we will distribute variants 
of the prototype to determine the effect of 
reformulation. Do users prefer to be presented with 
the extracted sentences with no additional 
processing, the tasks reformulated as described in 
Section 5, or an even more radical reformulation to 
a telegraphic form consisting of a verb plus object, 
such as Send information or Schedule subjects? 
 
 
 
 
 
 
 
Figure 3: Prototype system showing ways of visualizing tasks 
 
 
Figure 4: Magnified view of prototype system showing message with enumerated tasks 
 
 
 
8  Acknowledgements 
Many of the ideas presented here were formulated 
in discussion with Bob Atkinson, Dave Reed and 
Malcolm Pearson. Our thanks go to Jeff 
Stevenson, Margaret Salome and Kevin Gaughen 
for annotating the data. 
References 
Aikawa, Takako, Maite Melero, Lee Schwartz and 
Andi Wu. 2001. Multilingual natural language 
generation. EAMT. 
Bellotti, Victoria, Nicolas Ducheneaut, Mark 
Howard , Ian Smith. 2003. Taking email to 
task: the design and evaluation of a task 
management centered email tool. Proceedings 
of the conference on human factors in 
computing systems, pages 345-352. 
Cadiz, J. J., Dabbish, L., Gupta, A., & Venolia, G. 
D. 2001. Supporting email workflow. MSR-TR-
2001-88: Microsoft Research. 
Campbell, Richard and Hisami Suzuki. 2002. 
Language neutral representation of syntactic 
structure. Proceedings of SCANALU 2002. 
Dumais, Susan, John Platt, David Heckerman, 
Mehran Sahami 1998: Inductive learning 
algorithms and representations for text 
categorization. Proceedings of CIKM-98, pages 
148-155. 
Heidorn, George. 2000. Intelligent writing 
assistance. In R. Dale, H. Moisl and H. Somers, 
(eds.), Handbook of Natural Language 
Processing. Marcel Dekker. 
Joachims, Thorsten. 1998. Text categorization 
with support vector machines: Learning with 
many relevant features. Proceedings of ECML 
1998, pages 137-142. 
Mast, M., Kompe, R., Harbeck, S., Kiessling, A., 
Niemann, H., N?th, E., Schukat-Talamazzini, 
E. G. and Warnke., V. 1996. Dialog act 
classification with the help of prosody. ICSLP 
96. 
Platt, John. 1999. Fast training of SVMs using 
sequential minimal optimization. In B. 
Schoelkopf, C. Burges and A. Smola (eds.) 
Advances in Kernel Methods: Support Vector 
Learning, pages 185-208, MIT Press, 
Cambridge, MA.  
Stolcke, A., E. Shriberg, R. Bates, N. Coccaro, D. 
Jurafsky, R. Martin, M. Meteer, K. Ries, P. 
Taylor and C. Van Ess-Dykema. 1998. Dialog 
act modeling for conversational speech. 
Proceedings of the AAAI-98 Spring Symposium 
on Applying Machine Learning to Discourse 
Processing.  
Vapnik, V. 1995. The Nature of Statistical 
Learning Theory. Springer-Verlag, New York. 
Warnke, V., R. Kompe, H. Niemann and E. N?th. 
1997. Integrated dialog act segmentation and 
classification using prosodic features and 
language models. Proc. European Conf. on 
Speech Communication and Technology, vol 1, 
pages 207?210. 
 
