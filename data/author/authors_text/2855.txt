Accenting unknown words in a specialized language
Pierre Zweigenbaum and Natalia Grabar
DIAM ? STIM/DSI, Assistance Publique ? H?pitaux de Paris
& D?partement de Biomath?matiques, Universit? Paris 6
{ngr,pz}@biomath.jussieu.fr
Abstract
We propose two internal methods for ac-
centing unknown words, which both learn
on a reference set of accented words the
contexts of occurrence of the various ac-
cented forms of a given letter. One method
is adapted from POS tagging, the other is
based on finite state transducers.
We show experimental results for letter
e on the French version of the Medical
Subject Headings thesaurus. With the
best training set, the tagging method ob-
tains a precision-recall breakeven point
of 84.24.4% and the transducer method
83.84.5% (with a baseline at 64%) for
the unknown words that contain this let-
ter. A consensus combination of both in-
creases precision to 92.03.7% with a re-
call of 75%. We perform an error analysis
and discuss further steps that might help
improve over the current performance.
1 Introduction
The ISO-latin family, Unicode or the Universal
Character Set have been around for some time now.
They cater, among other things, for letters which can
bear different diacritic marks. For instance, French
uses four accented es (????) besides the unaccented
form e. Some of these accented forms correspond to
phonemic differences. The correct handling of such
accented letters, beyond US ASCII, has not been
immediate and general. Although suitable charac-
ter encodings are widely available and used, some
texts or terminologies are still, for historical rea-
sons, written with unaccented letters. For instance,
in the French version of the US National Library
of Medicine?s Medical Subject Headings thesaurus
(MeSH, (INS, 2000)), all the terms are written in
unaccented uppercase letters. This causes difficul-
ties when these terms are used in Natural Language
interfaces or for automatically indexing textual doc-
uments: a given unaccented word may match several
words, giving rise to spurious ambiguities such as,
e.g., marche matching both the unaccented marche
(walking) and the accented march? (market).
Removing all diacritics would simplify match-
ing, but would increase ambiguity, which is al-
ready pervasive enough in natural language pro-
cessing systems. Another of our aims, besides,
is to build language resources (lexicons, morpho-
logical knowledge bases, etc.) for the medi-
cal domain (Zweigenbaum, 2001) and to learn lin-
guistic knowledge from terminologies and cor-
pora (Grabar and Zweigenbaum, 2000), including
the MeSH. We would rather work, then, with lin-
guistically sound data in the first place.
We therefore endeavored to produce an accented
version of the French MeSH. This thesaurus in-
cludes 19,971 terms and 9,151 synonyms, with
21,475 different word forms. Human reaccentua-
tion of the full thesaurus is a time-consuming, error-
prone task. As in other instances of preparation of
linguistic resources, e.g., part-of-speech-tagged cor-
pora or treebanks, it is generally more efficient for a
human to correct a first annotation than to produce
it from scratch. This can also help obtain better con-
sistency over volumes of data. The issue is then to
find a method for (semi-)automatic accentuation.
The CISMeF team of the Rouen University Hos-
                                            Association for Computational Linguistics.
                            the Biomedical Domain, Philadelphia, July 2002, pp. 21-28.
                         Proceedings of the Workshop on Natural Language Processing in
pital already accented some 5,500 MeSH terms
that are used as index terms in the CISMeF online
catalog of French-language medical Internet sites
(Darmoni et al, 2000) (www.chu-rouen.fr/cismef).
This first means that less material has to be reac-
cented. Second, this accented portion of the MeSH
might be usable as training material for a learning
procedure.
However, the methods we found in the literature
do not address the case of ?unknown? words, i.e.,
words that are not found in the lexicon used by the
accenting system. Despite the recourse to both gen-
eral and specialized lexicons, a large number of the
MeSH words are in this case, for instance those in
table 1. One can argue indeed that the compila-
cryomicroscopie dactylolyse
decarboxylases decoquinate
denitrificans deoxyribonuclease
desmodonte desoxyadrenaline
dextranase dichlorobenzidine
dicrocoeliose diiodotyrosine
dimethylamino dimethylcysteine
dioctophymatoidea diosgenine
Table 1: Unaccented words not in lexicon.
tion of a larger lexicon should reduce the propor-
tion of unknown words. But these are for the most
part specialized, rare words, some of which we did
not find even in a large reference medical dictionary
(Garnier and Delamare, 1992). It is then reasonable
to try to accentuate automatically these unknown
words to help human domain experts perform faster
post-editing. Moreover, an automatic accentuation
method will be reusable for other unaccented textual
resources. For instance, the ADM (Medical Diagno-
sis Aid) knowledge base online at Rennes University
(Seka et al, 1997) is another large resource which is
still in unaccented uppercase format.
We first review existing methods (section 2). We
then present two trainable accenting methods (sec-
tion 3), one adapted from part-of-speech tagging, the
other based on finite-state transducers. We show ex-
perimental results for letter e on the French MeSH
(section 4) with both methods and their combina-
tion. We finally discuss these results (section 5) and
conclude on further research directions.
2 Background
Previous work has addressed text accentuation, with
an emphasis on the cases where all possible words
are assumed to be known (listed in a lexicon). The
issue in that case is to disambiguate unaccented
words when they match several possible accented
word forms in the lexicon ? the marche/march? ex-
amples in the introduction.
Yarowsky (1999) addresses accent restoration in
Spanish and in French, and notes that they can be
linked to part-of-speech ambiguities and to seman-
tic ambiguities which context can help to resolve.
He proposes three methods to handle these: N-gram
tagging, Bayesian classification and decision lists,
which obtain the best results. These methods rely
either on full words, on word suffixes or on parts-
of-speech. They are tested on ?the most problem-
atic cases of each ambiguity type?, extracted from
the Spanish AP Newswire. The agreement with hu-
man accented words reaches 78.4?98.4% depending
on ambiguity type.
Spriet and El-B?ze (1997) use an N-gram model
on parts-of-speech. They evaluate this method on a
19,000 word test corpus consisting of news articles
and obtain a 99.31% accuracy. In this corpus, only
2.6% of the words were unknown, among which
89.5% did not need accents. The resulting error rate
(0.3%) accounts for nearly one half of the total er-
ror rate, but is so small that it is not worth trying to
guess accentuation for unknown words.
The same kind of approach is used in project
R?ACC (Simard, 1998). Here again, unknown
words are left untouched, and account for one fourth
of the errors. We typed the words in table 1
through the demonstration interface of R?ACC on-
line at www-rali.iro.umontreal.ca/Reacc/: none of
these words was accented by the system (7 out of
16 do need accentuation).
When the unaccented words are in the lexicon,
the problem can also be addressed as a spelling cor-
rection task, using methods such as string edit dis-
tance (Levenshtein, 1966), possibly combined with
the previous approach (Ruch et al, 2001).
However, these methods have limited power when
a word is not in the lexicon. At best, they might say
something about accented letters in grammatical af-
fixes which mark contextual, syntactic constraints.
We found no specific reference about the accentua-
tion of such ?unknown? words: a method that, when
a word is not listed in the lexicon, proposes an ac-
cented version of that word. Indeed, in the above
works, the proportion of unknown words is too small
for specific steps to be taken to handle them. The sit-
uation is quite different in our case, where about one
fourth of the words are ?unknown?. Moreover, con-
textual clues are scarce in our short, often ungram-
matical terms.
We took obvious measures to reduce the number
of unknown words: we filtered out the words that
can be found in accented lexicons and corpora. But
this technique is limited by the size of the corpus that
would be necessary for such ?rare? words to occur,
and by the lack of availability of specialized French
lexicons for the medical domain.
We then designed two methods that can learn ac-
centing rules for the remaining unknown words: (i)
adapting a POS-tagging method (Brill, 1995) (sec-
tion 3.3); (ii) adapting a method designed for learn-
ing morphological rules (Theron and Cloete, 1997)
(section 3.4).
3 Accenting unknown words
3.1 Filtering out know words
The French MeSH was briefly presented in the in-
troduction; we work with the 2001 version. The part
which was accented and converted into mixed case
by the CISMeF team is that of November 2001. As
more resources are added to CISMeF on a regular
basis, a larger number of these accented terms must
now be available. The list of word forms that oc-
cur in these accented terms serves as our base lex-
icon (4861 word forms). We removed from this
list the ?words? that contain numbers, those that are
shorter than 3 characters (abbreviations), and con-
verted them in lower case. The resulting lexicon in-
cludes 4054 words (4047 once unaccented). This
lexicon deals with single words. It does not try to
register complex terms such as myocardial infarc-
tion, but instead breaks them into the two words my-
ocardial and infarction.
A word is considered unknown when it is not
listed in our lexicon. A first concern is to filter out
from subsequent processing words that can be found
in larger lexicons. The question is then to find suit-
able sources of additional words.
We used various specialized word lists found on
the Web (lexicon on cancer, general medical lex-
icon) and the ABU lexicon (abu.cnam.fr/DICO),
which contains some 300,000 entries for ?gen-
eral? French. Several corpora provided accented
sources for extending this lexicon with some med-
ical words (cardiology, haematology, intensive care,
drawn from the current state of the CLEF corpus
(Habert et al, 2001), and drug monographs). We
also used a word list extracted from the French ver-
sions of two other medical terminologies: the In-
ternational Classification of Diseases (ICD-10) and
the Microglossary for Pathology of the Systematized
Nomenclature of Medicine (SNOMED). This word
list contains 8874 different word forms. The total
number of word forms of the final word list was
276 445.
After application of this list to the MeSH, 7407
words were still not recognized. We converted these
words to lower case, removed those that did not in-
clude the letter e, were shorter than 3 letters (mainly
acronyms) or contained numbers. The remaining
5188 words, among which those listed in table 1,
were submitted to the following procedure.
3.2 Representing the context of a letter
The underlying hypotheses of this method are that
sufficiently regular rules determine, for most words,
which letters are accented, and that the context of
occurrence of a letter (its neighboring letters) is a
good basis for making accentuation decisions. We
attempted to compile these rules by observing the
occurrences of e???? in a reference list of words
(the training set, for instance, the part of the French
MeSH accented by the CISMeF team). In the fol-
lowing, we shall call pivot letter a letter that is part
of the confusion set e???? (set of letters to discrimi-
nate).
An issue is then to find a suitable description of
the context of a pivot letter in a word, for instance
the letter ? in excis?e. We explored and compared
two different representation schemes, which under-
lie two accentuation methods.
3.3 Accentuation as contextual tagging
This first method is based on the use of a part-of-
speech tagger: Brill?s (1995) tagger. We consider
each word as a ?string of letters?: each letter makes
one word, and the sequence of letters of a word
makes a sentence. The ?tag? of a letter is the ex-
pected accented form of this letter (or the same letter
if it is not accented). For instance, for the word en-
dometre (endometer), to be accented as endom?tre,
the ?tagged sentence? is e/e n/n d/d o/o m/m e/? t/t
r/r e/e (in the format of Brill?s tagger). The regular
procedure of the tagger then learns contextual accen-
tuation rules, the first of which are shown on table 2.
Brill Format Gloss
(1) e ? NEXT2TAG i e.i) e! ?
(2) e ? NEXT1OR2TAG o e.?o) e! ?
(3) e ? NEXT1OR2TAG a e.?a) e! ?
(4) e ? NEXT1OR2WD e e.?e) e! ?
(5) e ? NEXT2TAG h e.h) e! ?
(6) ? ? NEXTBIGRAM n e ?ne) ?! ?
(7) ? e NEXTBIGRAM m e ?me) ?! e
(8) e ? NEXTBIGRAM t r etr ) e! ?
(9) ? e NEXT1OR2OR3TAG x ?.?.?x) ?! e
(10) e ? NEXT1OR2TAG y e.?y) e! ?
(11) e ? NEXT2TAG u e.u) e! ?
(12) e ? SURROUNDTAG t i tei) e! ?
(13) ? ? NEXTBIGRAM s e ?se) ?! ?
Table 2: Accentuation correction rules, of the form
?change t
1
to t
2
if test true on x [y]?. NEXT2TAG =
second next tag, NEXT1OR2TAG = one of next 2 tags,
NEXTBIGRAM = next 2 words, NEXT1OR2OR3TAG = one
of next 3 tags, SURROUNDTAG = previous and next
tags,
Given a new ?sentence?, Brill?s tagger first assigns
each ?word? its mots frequent tag: this consists in
accenting no e. The contextual rules are then ap-
plied and successively correct the current accentu-
ation. For instance, when accenting the word flex-
ion, rule (1) first applies (if e with second next tag
= i, change to ?) and accentuates the e to yield fl?x-
ion (as in ...?mie). Rule (9) applies next (if ? with
one of next three tags = x, change to e) to correct
this accentuation before an x, which finally results
in flexion. These rules correspond to representations
of the contexts of occurrence of a letter. This rep-
resentation is mixed (left and right contexts can be
combined, e.g., in SURROUNDTAG, where both imme-
diate left and right tags are examined), and can ex-
tend to a distance of three letters left and right, but
in restricted combinations.
3.4 Mixed context representation
The ?mixed context? representation used by
Theron and Cloete (1997) folds the letters of a word
around a pivot letter: it enumerates alternately
the next letter on the right then on the left, until it
reaches the word boundaries, which are marked with
special symbols (here, ^ for start of word, and $ for
end of word). Theron & Cloete additionally repeat
an out-of-bounds symbol outside the word, whereas
we dispense with these marks. For instance, the
first e in excis?e (excised) is represented as the
mixed context in the right column of the first row of
table 3. The left column shows the order in which
the letters of the word are enumerated. The next two
rows explain the mixed context representations for
the two other es in the word. This representation
Word Mixed Context=Output
^ e x c i s ? e $
2 . 1 3 4 5 6 7 8 x
^ c i s e e $=e
^ e x c i s ? e $
8 7 6 5 4 2 . 1 3 e s $ i c x e
^
=?
^ e x c i s ? e $
8 7 6 5 4 3 2 . 1 $ e s i c x e
^
=e
Table 3: Mixed context representations.
caters for contexts of different sizes and facilitates
their comparison.
Each of these contexts is unaccented (it is meant
to be matched with representations of unaccented
words) and the original form of the pivot letter is
associated to the context as an output (we use the
symbol ?=? to mark this output). Each context is
thus converted into a transducer: the input tape is the
mixed context of a pivot letter, and the output tape is
the appropriate letter in the confusion set e????.
The next step is to determine minimal discrimi-
nating contexts (figure 1). To obtain them, we join
all these transducers (OR operator) by factoring their
common prefixes as a trie structure, i.e., a determin-
istic transducer that exactly represents the training
set. We then compute, for each state of this trans-
ducer and for each possible output (letter in the con-
fusion set) reachable from this state, the number of
paths starting from this state that lead to this output.
^allergie$, ^chirurgie$i
^r?fugi?$
^cytologie$
^?chographie$
^lipoatrophie$
r
h
u
o
e
?
e
e
$
g3 ?505 e,
65 e
6 e
1 ?
63 e
1 ?
86 e,
Figure 1: Trie of mixed contexts, each state showing
the frequency of each possible output.
We call a state unambiguous if all the paths from
this state lead to the same output. In that case, for
our needs, these paths may be replaced with a short-
cut to an exit to the common output (see figure 1).
This amounts to generalizing the set of contexts by
replacing them with a set of minimal discriminating
contexts.
Given a word that needs to be accented, the first
step consists in representing the context of each of
its pivot letters. For instance, the word biologie:
$igoloib^ . Each context is matched with the trans-
ducer in order to find the longest path from the start
state that corresponds to a prefix of the context string
(here, $igo). If this path leads to an output state, this
output provides the proposed accented form of the
pivot letter (here, e). If the match terminates earlier,
we have an ambiguity: several possible outputs can
be reached (e.g., h?morragie matches $ig).
We can take absolute frequencies into account to
obtain a measure of the support (confidence level)
for a given output O from the current state S: how
much evidence there is to support this decision. It
is computed as the number of contexts of the train-
ing set that go through S to an output state labelled
with O (see figure 1). The accenting procedure can
choose to make a decision only when the support
for that decision is above a given threshold. Table 4
Context Support Gloss Examples
$igo=e 65 ?ogie cytologie
$ih=e 63 ?hie lipoatrophie
$uqit=e 77 ?tique am?lanotique
u=e 247 -eu- activateur, calleux
x=e 68 -ex- excis?e
Table 4: Some minimal discriminating contexts.
shows some minimal discriminating contexts learnt
from the accented part of the French MeSH with a
high support threshold. However, in previous exper-
iments (Zweigenbaum and Grabar, 2002), we tested
a range of support thresholds and observed that the
gain in precision obtained by raising the support
threshold was minor, and counterbalanced by a large
loss in recall. We therefore do not use this device
here and accept any level of support.
Instead, we take into account the relative frequen-
cies of occurrence of the paths that lead to the dif-
ferent outputs, as marked in the trie. A probabilistic,
majority decision is made on that basis: if one of the
competing outputs has a relative frequency above a
given threshold, this output is chosen. In the present
experiments, we tested two thresholds: 0.9 (90% or
more of the examples must support this case; this
makes the correct decision for h?morragie) and 1
(only non-ambiguous states lead to a decision: no
decision for the first e in hemorragie, which we
leave unaccented).
Simpler context representations of the same fam-
ily can also be used. We examined right contexts
(a variable-length string of letters on the right of the
pivot letter) and left contexts (idem, on the left).
3.5 Evaluating the rules
We trained both methods, Brill and contexts (mixed,
left and right), on three training sets: the 4054 words
of the accented part of the MeSH, the 54,291 lem-
mas of the ABU lexicon and the 8874 words in the
ICD-SNOMED word list. To check the validity of
the rules, we applied them to the accented part of
the MeSH. The context method knows when it can
make a decision, so that we can separate the words
that are fully processed (f , all es have lead to deci-
sions) from those that are partially (p) processed or
not (n) processed at all. Let f
c
the number of correct
accentuations in f . If we decide to only propose an
accented form for the words that get fully accented,
we can compute recall R
f
and precision P
f
figures
as follows: R
f
=
f
c
f+p+n
and P
f
=
f
c
f
. Similar
measures can be computed for p and n, as well as
for the total set of words.
We then applied the accentuation rules to the 5188
accentable ?unknown? words of the MeSH. No gold
standard is available for these words: human vali-
dation was necessary. We drew from that set a ran-
dom sample containing 260 words (5% of the total)
which were reviewed by the CISMeF team. Because
of sampling, precision measures must include a con-
fidence interval.
We also tested whether the results of several meth-
ods can be combined to increase precision. We sim-
ply applied a consensus rule (intersection): a word
is accepted only if all the methods considered agree
on its accentuation.
The programs were developed in the Perl5 lan-
guage. They include a trie manipulation package
which we wrote by extending the Tree::Trie pack-
age, online on the Comprehensive Perl Archive Net-
work (www.cpan.org).
4 Results
The baseline of this task consists in accenting no e.
On the accented part of the MeSH, it obtains an ac-
curacy of 0.623, and on the test sample, 0.642. The
Brill tagger learns 80 contextual rules with MeSH
training (208 on ABU and 47 on CIM-SNOMED).
The context method learns 1,832 rules on the MeSH
training set (16,591 on ABU and 3,050 on CIM-
SNOMED).
Tables 5, 6 and 7 summarize the validation results
obtained on the accented part of the MeSH. Set de-
notes the subset of words as explained in section 3.5.
Cor. stands for the number of correctly accented
words.
Not surprizingly, the best global precision is ob-
tained with MeSH training (table 6). The mixed
context method obtains a perfect precision, whereas
Brill reaches 0.901 (table 5). ABU and CIM-
SNOMED training also obtain good results (table 7),
again better with the mixed context method (0.912?
0.931) than with Brill (0.871?0.895). We performed
the same tests with right and left contexts (table 6):
precision can be as good for fully processed words
(set f ) as that of mixed contexts, but recall is always
lower. The results of these two context variants are
therefore not kept in the following tables. Both pre-
cision and recall are generally slightly better with
the majority decision variant. If we concentrate on
the fully processed words (f ), precision is always
higher than the global result and than that of words
with no decision (n). The n class, whose words
are left unaccented, generally obtain a precision well
over the baseline. Partially processed words (p) are
always those with the worst precision.
training set cor. recall precisionci
MeSH 3646 0.899 0.9010.009
ABU 3524 0.869 0.8710.010
CIM-SNOMED 3621 0.893 0.8950.009
Table 5: Validation: Brill, 4054 words of accented
MeSH.
context set cor. recall precisionci
right n 1906 0.470 0.7470.017
p 943 0.233 0.8040.023
f 324 0.080 1.0000.000
tot 3173 0.783 0.7840.013
left n 743 0.183 0.6490.028
p 500 0.123 0.4280.028
f 1734 0.428 1.0000.000
tot 2977 0.734 0.7360.014
mixed n 7 0.002 1.0000.000
p 0 0.000 0.0000.000
f 4040 0.997 1.0000.000
tot 4047 0.998 1.0000.000
majority decision (0.9)
mixed n 2 0.000 1.0000.000
p 0 0.000 0.0000.000
f 4045 0.998 1.0000.000
tot 4047 0.998 1.0000.000
Table 6: Validation: different context methods,
MeSH training, 4054 words of accented MeSH.
Precision and recall for the unaccented part of
the MeSH are showed on tables 8 and 9. The
global results with the different training sets at
breakeven point, with their confidence intervals, are
not really distinguishable. They are clustered from
0.8190.047 to 0.8420.044, except the unambigu-
ous decision method trained on MeSH which stands
a bit lower at 0.8000.049 and the Brill tagger
trained on ABU (0.785). If we only consider fully
processed words, precision can reach 0.8840.043
(ICD-SNOMED training, majority decision), with a
recall of 0.731 (or 0.8760.043 / 0.758 with MeSH
training, majority decision).
Consensus combination of several methods (ta-
ble 8) does increase precision, at the expense of
recall. A precision/recall of 0.9200.037/0.750 is
ABU training (strict)
set cor. recall precisionci
n 368 0.091 0.8640.033
p 227 0.056 0.6680.050
f 3164 0.780 0.9640.006
tot 3759 0.927 0.9290.008
majority decision (0.9)
cor. recall precisionci
111 0.027 0.8600.060
77 0.019 0.5240.081
3585 0.884 0.9510.007
3773 0.931 0.9320.008
CIM-SNOMED training
n 176 0.043 0.7520.055
p 114 0.028 0.4250.059
f 3400 0.839 0.9590.007
tot 3690 0.910 0.9120.009
majority decision (0.9)
57 0.014 0.8030.093
51 0.013 0.3000.069
3607 0.890 0.9480.007
3715 0.916 0.9180.008
Table 7: Validation: mixed contexts, strict (thresh-
old = 1) and majority (threshold = 0.9) decisions,
4054 words of accented MeSH.
training set cor. recall precisionci
MeSH 219 0.842 0.8420.044
ABU 204 0.785 0.7850.050
CIM-SNOMED 218 0.838 0.8380.045
Combined methods
mesh/Brill + mesh/majority 195 0.750 0.9200.037
mesh/Brill + mesh/majority
f
185 0.712 0.9300.036
mesh+abu+cim-snomed/Brill 178 0.685 0.9270.037
+ mesh/majority
Table 8: Evaluation on the rest of the MeSH: Brill,
estimate on 5% sample (260 words).
obtained by combining Brill and the mixed context
method (majority decision), with MeSH training on
both sides. The same level of precision is obtained
with other combinations, but with lower recalls.
5 Discussion and Conclusion
We showed that a higher precision, which should
make human post-editing easier, can be obtained in
two ways. First, within the mixed context method,
three sets of words are separated: if only the ?fully
processed? words f are considered (table 9), preci-
sion/recall can reach 0.884/0.731 (CIM-SNOMED,
majority) or 0.876/0.758 (MeSH, majority). Second,
the results of several methods can be combined with
a consensus rule: a word is accepted only if all these
methods agree on its accentuation. The combination
of Brill mixed contexts (majority decision), for in-
stance with MeSH training on both sides, increases
precision to 0.9200.037 with a recall still at 0.750
(table 8).
The results obtained show that the methods pre-
sented here obtain not only good performance on
their training set, but also useful results on the tar-
MeSH training (strict)
set cor. recall precisionci
n 19 0.073 0.7310.170
p 15 0.058 0.4290.164
f 174 0.669 0.8740.046
tot 208 0.800 0.8000.049
majority decision
cor. recall precisionci
8 0.031 0.7270.263
11 0.042 0.4580.199
197 0.758 0.8760.043
216 0.831 0.8310.046
ABU training (strict)
n 30 0.115 0.8820.108
p 32 0.123 0.7110.132
f 153 0.588 0.8450.053
tot 215 0.827 0.8270.046
majority decision
13 0.050 0.9290.135
11 0.042 0.7860.215
194 0.746 0.8360.048
218 0.838 0.8380.045
CIM-SNOMED training
n 27 0.104 0.8180.132
p 19 0.073 0.4870.157
f 168 0.646 0.8940.044
tot 214 0.823 0.8230.046
majority decision
14 0.054 0.8240.181
9 0.035 0.3210.173
190 0.731 0.8840.043
213 0.819 0.8190.047
Table 9: Evaluation on the rest of the MeSH: mixed
contexts, estimate on same 5% sample.
get data. We believe these methods will allow us to
reduce dramatically the final human time needed to
accentuate useful resources such as the MeSH the-
saurus and ADM knowledge base.
It is interesting that a general-language lexicon
such as ABU can be a good training set for accent-
ing specialized-language unknown words, although
this is true with the mixed context method and the
reverse with the Brill tagger.
A study of the 44 errors made by the mixed con-
text method (table 9, MeSH training, majority deci-
sion: 216 correct out of 260) revealed the follow-
ing errors classes. MeSH terms contain some En-
glish words (academy, cleavage) and many Latin
words (arenaria, chrysantemi, denitrificans), some
of which built over proper names (edwardsiella).
These loan words should not bear accents; some of
their patterns are correctly processed by the meth-
ods presented here (i.e., unaccented eae$, ella$), but
others are not distinguishable from normal French
words and get erroneously accented (rena of are-
naria is erroneously processed as in r?nal; acad?my
as in acad?mie). A first-stage classifier might help
handle this issue by categorizing Latin (and English)
words and excluding them from processing. Our
first such experiments are not conclusive and add as
many errors as are removed.
Another class of errors are related with mor-
pheme boundaries: some accentuation rules which
depend on the start-of-word boundary would need
to apply to morpheme boundaries. For in-
stance, pilo/erection fails to receive the ? of r^e=?
(^?rection), apic/ectomie erroneously receives an ?
as in cc=? (c?cit?). An accurate morpheme seg-
menter would be needed to provide suitable input
to this process without again adding noise to it.
In some instances, no accentuation decision could
be made because no example had been learnt for a
specific context (e.g., accentuation of c?falo in ce-
faloglycine).
We also uncovered accentuation inconsistencies
in both the already accented MeSH words and the
validated sample (e.g., bacterium or bact?rium in
different compounds). Cross-checking on the Web
confirmed the variability in the accentuation of rare
words. This shows the difficulty to obtain consistent
human accentuation across large sets of complex
words. One potential development of the present au-
tomated accentuation methods could be to check the
consistency of word lists. In addition, we discovered
spelling errors in some MeSH terms (e.g., bethane-
chol instead of betanechol prevents the proper ac-
centuation of beta).
Finally, further testing is necessary to check the
relevance of these methods to other accented letters
in French and in other languages.
Acknowledgements
We wish to thank Magaly Douy?re, Beno?t Thirion
and St?fan Darmoni, of the CISMeF team, for pro-
viding us with accented MeSH terms and patiently
reviewing the automatically accented word samples.
References
[Brill1995] Eric Brill. 1995. Transformation-based error-
driven learning and natural language processing: A
case study in part-of-speech tagging. Computational
Linguistics, 21(4):543?565.
[Darmoni et al2000] St?fan J. Darmoni, J.-P. Leroy,
Beno?t Thirion, F. Baudic, Magali Douyere, and
J. Piot. 2000. CISMeF: a structured health resource
guide. Methods Inf Med, 39(1):30?35.
[Garnier and Delamare1992] M. Garnier and V. Dela-
mare. 1992. Dictionnaire des Termes de M?decine.
Maloine, Paris.
[Grabar and Zweigenbaum2000] Natalia Grabar and
Pierre Zweigenbaum. 2000. Automatic acquisition
of domain-specific morphological resources from the-
sauri. In Proceedings of RIAO 2000: Content-Based
Multimedia Information Access, pages 765?784,
Paris, France, April. C.I.D.
[Habert et al2001] Beno?t Habert, Natalia Grabar, Pierre
Jacquemart, and Pierre Zweigenbaum. 2001. Build-
ing a text corpus for representing the variety of medi-
cal language. In Corpus Linguistics 2001, Lancaster.
[INS2000] Institut National de la Sant? et de la Recherche
M?dicale, Paris, 2000. Th?saurus Biom?dical
Fran?ais/Anglais.
[Levenshtein1966] V. I. Levenshtein. 1966. Binary codes
capable of correcting deletions, insertions, and rever-
sals. Soviet Physics-Doklandy, pages 707?710.
[Ruch et al2001] Patrick Ruch, Robert H. Baud, Antoine
Geissbuhler, Christian Lovis, Anne-Marie Rassinoux,
and A. Rivi?re. 2001. Looking back or looking all
around: comparing two spell checking strategies for
documents edition in an electronic patient record. J
Am Med Inform Assoc, 8(suppl):568?572.
[Seka et al1997] LP Seka, C Courtin, and P Le Beux.
1997. ADM-INDEX: an automated system for index-
ing and retrieval of medical texts. In Stud Health Tech-
nol Inform, volume 43 Pt A, pages 406?410. Reidel.
[Simard1998] Michel Simard. 1998. Automatic inser-
tion of accents in French text. In Proceedings of the
Third Conference on Empirical Methods in Natural
Language Processing, Grenade.
[Spriet and El-B?ze1997] Thierry Spriet and Marc El-
B?ze. 1997. R?accentuation automatique de textes.
In FRACTAL 97, Besan?on.
[Theron and Cloete1997] Pieter Theron and Ian Cloete.
1997. Automatic acquisition of two-level morpholog-
ical rules. In Ralph Grishman, editor, Proceedings
of the Fifth Conference on Applied Natural Language
Processing, pages 103?110, Washington, DC, March-
April. ACL.
[Yarowsky1999] David Yarowsky. 1999. Corpus-based
techniques for restoring accents in Spanish and French
text. In Natural Language Processing Using Very
Large Corpora, pages 99?120. Kluwer Academic Pub-
lishers.
[Zweigenbaum and Grabar2002] Pierre Zweigenbaum
and Natalia Grabar. 2002. Accenting unknown words:
application to the French version of the MeSH. In
Workshop NLP in Biomedical Applications, pages
69?74, Cyprus, March. EFMI.
[Zweigenbaum2001] Pierre Zweigenbaum. 2001. Re-
sources for the medical domain: medical terminolo-
gies, lexicons and corpora. ELRA Newsletter, 6(4):8?
11.
Lexically-Based Terminology Structuring: Some Inherent Limits
Natalia Grabar and Pierre Zweigenbaum
STIM/DSI, Assistance Publique ? H?pitaux de Paris
& D?partement de Biomath?matiques, Universit? Paris 6
{ngr,pz}@biomath.jussieu.fr
http://www.biomath.jussieu.fr/?{ngr,pz}
Abstract
Terminology structuring has been the subject of
much work in the context of terms extracted from
corpora: given a set of terms, obtained from an ex-
isting resource or extracted from a corpus, identi-
fying hierarchical (or other types of) relations be-
tween these terms. The present paper focusses on
terminology structuring by lexical methods, which
match terms on the basis on their content words,
taking morphological variants into account. Exper-
iments are done on a ?flat? list of terms obtained
from an originally hierarchically-structured termi-
nology: the French version of the US National
Library of Medicine MeSH thesaurus. We com-
pare the lexically-induced relations with the original
MeSH relations: after a quantitative evaluation of
their congruence through recall and precision met-
rics, we perform a qualitative, human analysis of the
?new? relations not present in the MeSH. This anal-
ysis shows, on the one hand, the limits of the lex-
ical structuring method. On the other hand, it also
reveals some specific structuring choices and nam-
ing conventions made by the MeSH designers, and
emphasizes ontological commitments that cannot be
left to automatic structuring.
1 Background
Terminology structuring, i.e., organizing a set of
terms through semantic relations, is one of the dif-
ficult issues that have to be addressed when build-
ing terminological resources. These relations in-
clude subsumption or hyperonymy (the is-a re-
lation), meronymy (part-of and its variants), as
well as other, diverse relations, sometimes called
?transversal? (e.g., cause, or the general see also).
Various methods have been proposed
to discover relations between terms (see
Jacquemin and Bourigault (2002) for a review).
We divide them into internal and external meth-
ods, in the same way as McDonald (1993)
for proper names. Internal methods look
at the constituency of terms, and compare
terms based on the words they contain. Term
matching can rely directly on raw word forms
(Bodenreider et al, 2001), on morphological
variants (Jacquemin and Tzoukermann, 1999),
on syntactic structure (Bourigault, 1994;
Jacquemin and Tzoukermann, 1999) or on se-
mantic variants (synonyms, hyperonyms, etc.)
(Hamon et al, 1998). External methods take
advantage of the context in which terms occur:
they examine the behavior of terms in corpora.
Distributional methods group terms that occur
in similar contexts (Grefenstette, 1994). The
detection of appropriate syntactic patterns of
cooccurrence is another method to uncover re-
lations between terms in corpora (Hearst, 1992;
S?gu?la and Aussenac, 1999).
In previous work we applied lexical methods to
identify relations between terms on the basis on
their content words, taking morphological variants
into account. Our goal was then to assess the feasi-
bility of such structuring by studying it on an exist-
ing, hierarchically structured terminology. Ignoring
this existing structure and starting from the set of its
terms, we attempt to discover hierarchical term-to-
term links and compare them with the preexisting
relations.
Our goal in the present paper is to analyze ?new?
relations. ?New? means that these induced relations
are not present in the original hierarchical structure
of the MeSH thesaurus; they might nevertheless re-
flect useful links. Performing this analysis allows us
to propose a more precise evaluation of the methods
and their results and to point out some inherent lim-
its.
After the exposition of the data we used in our
experiments (section 2), we present methods (sec-
tion 3) for generating hierarchical links between
terms through the study of lexical inclusion and for
evaluating their quality with appropriate recall and
precision metrics. We then present the analysis of
some ?new? induced relations and attempt to pro-
pose a typology of term dependency in these rela-
tions (section 4). We finally discuss the limits of
lexical methods for the structuring task (section 5).
2 The MeSH biomedical thesaurus, and
associated morphological knowledge
We first present the existing hierarchically struc-
tured thesaurus, a ?stop word? list and morpholog-
ical knowledge involved in the present work.
2.1 The MeSH biomedical thesaurus
The Medical Subject Headings (MeSH,
NLM (2001a)) is one of the main international
medical terminologies (see, e.g., Cimino (1996)
for a presentation of medical terminologies). It is
a thesaurus specifically designed for information
retrieval in the biomedical domain. The MeSH is
used to index the international biomedical literature
in the Medline bibliographic database. The French
version of the MeSH (INSERM, 2000) contains
a translation of these terms (19,638 terms) plus
synonyms. It happens to be written in unaccented,
uppercase letters. Both the American and French
MeSH can be found in the UMLS Metathesaurus
(NLM, 2001b), which can be obtained through a
convention with the National Library of Medicine.
The concept names (main headings) which the
MeSH contains have been designed to reflect their
broad meanings and to facilitate their use by hu-
man indexers and librarians. In that, they follow a
tradition in information sciences, and are not nec-
essarily the expressions used in naturally occurring
biomedical documents. The MeSH can be consid-
ered as a fine-grained thesaurus: concepts are cho-
sen to insure a good coverage of the biomedical do-
main (Zweigenbaum, 1999).
As many other medical terminologies, the MeSH
has a hierarchical structure: ?narrower? concepts
(children) are related to ?broader? concepts (par-
ents). This both covers the usual is-a relation and
partitive relations (part-of, conceptual-part-of and
process-of ). The MeSH also includes see-also re-
lations, which we do not take into account in the
present experiments. This structure has also been
designed in the aim to be intellectually accessi-
ble to users: an indexer must be able to assign a
given concept to an article and a clinician must be
able to find a given concept in the tree hierarchy
(Nelson et al, 2001). To conclude, the MeSH team
aims to organize it in a clear and intuitive manner,
both for concept naming and concept placement.
The version of the French MeSH we used in these
experiments contains 19,638 terms, 26,094 direct
child-to-parent links and (under transitive closure)
95,815 direct or indirect child-to-ancestor links.
2.2 Stop word list
The aim of using a ?stop word? list is to remove from
term comparison very frequent words which are
considered not to be content-bearing, hence ?non-
significant? for terminology structuring. We used
in this experiment a short stop word list (15 word
forms). It contains the few frequent grammatical
words, such as articles and prepositions, that occur
in MeSH terms.
2.3 Morphological knowledge
The morphological knowledge involved consists of
lemma/derived-word or lemma/inflected form pairs
where the first is the ?normalized? form and the sec-
ond a ?variant? form.
Inflection produces the various forms of a given
word such as plural, feminine or the multiple forms
of a verb according to person, tense, etc.: inter-
vention ? interventions, acid ? acids. We per-
form the reverse process (lemmatization), reducing
an inflected form to its lemma (canonical form).
We worked with two alternate lexicons. The first
one is based on a general French lexicon (ABU,
abu.cnam.fr/DICO) which we have augmented with
pairs obtained from medical corpora processed
through a tagger/lemmatizer (in cardiology, hema-
tology, intensive care, and drug monographs): it to-
tals 219,759 pairs (where the inflected form is dif-
ferent from the lemma). The second lexicon, more
specialized and tuned to the vocabulary in medi-
cal terminologies, is the result of applying rules ac-
quired in previous work from two other medical ter-
minologies (ICD-10 and SNOMED) to the vocab-
ulary in the MeSH, ICD-10 and SNOMED (total:
2,889 pairs).
Derivation produces, e.g., the adjectival form of
a noun (noun aorta   adjective aortic), the nom-
inal form of a verb (verb intervene   noun inter-
vention), or the adverbial form of an adjective (ad-
jective human   adverb humanely). We perform
linguistically-motivated stemming to reduce a de-
rived word to its base word. For derivation, we also
used resources acquired in previous work which,
once combined with inflection pairs, results in 4,517
pairs.
Compounding, which combines several radicals,
often of Greek or Latin origin, to obtain complex
words (e.g., aorta + coronary yields aortocoro-
nary), has not been used because we do not have
a reliable procedure to segment a compound into its
component morphemes.
3 Acquiring links through lexical
inclusion of terms
The present work induces hierarchical relations be-
tween terms when the constituent words of one term
lexically include those of the second term (sec-
tion 3.1). When comparing these relations with
those that preexist in the MeSH, precision can reach
29.3% and recall 13.7% (section 3.2). We focus
here on the analysis of the relations that are not
found in the MeSH (section 3.3), which we develop
in the next section (section 4).
3.1 Lexical inclusion
The method we use here for inducing hierarchical
relations between terms is basically a test of lexical
inclusion: we check whether a term   (parent) is
?included? in another term  (child), i.e., whether
all words in   occur in  . We assume that this type
of inclusion is a clue of a hierarchical relation be-
tween terms, as in acides gras / acides gras indis-
pensables (fatty acids / fatty acids, essential).
To detect this type of relation, we test whether
all the content words of   occur in  . We do this
on segmented terms with a gradually increasing nor-
malization on word forms. Basic normalizations are
performed first: conversion to lower case, removal
of punctuation, of numbers and of ?stop words?.
Subsequent normalizations rely on morphological
ressources: lemmatization (with the two alternate
inflectional lexicons) and stemming with a deriva-
tional lexicon. Terms are indexed by their words to
speed up the computation of term inclusion over all
term pairs of the whole MeSH thesaurus.
3.2 Application to MeSH and quantification
This structuring method has been applied to the flat
list of 19,638 terms of the MeSH thesaurus. As ex-
pected, the number of links induced between terms
increases when applying inflectional normalization
and again with derivational normalization.
We evaluated the quality of the links obtained
with this approach by comparing them automati-
cally with the original structure of the MeSH and
computing recall and precision metrics. We sum-
marize here the main results; a detailed evaluation
can be found in (Grabar and Zweigenbaum, 2002).
Depending on the normalization, up to 29.3% of
the links found are correct (precision), and up to
13.7% of the direct MeSH links are found by lex-
ical inclusion (recall). We also examined whether
each term was correctly placed under one of its an-
cestors: this was true for up to 26% of the terms
(recall); and the placement advices were correct in
up to 58% of the cases (precision). The recall of
links increases when applying more complete mor-
phological knowledge (inflection then derivation).
The evolution of precision is opposite: injection of
more extensive morphological knowledge (deriva-
tion vs inflection) leads to taking more ?chances? for
generating links between terms: the precision with
no normalization (raw results) is 29.3% vs 22.5%
when using all normalizations (lem-stem-med). De-
pending on the type of normalization, the best pre-
cision obtained for links is 43%.
3.3 Human analysis of ?new? relations
The evaluations presented in the previous section
quantify the match between the induced relations
and existing MeSH relations. However, they give
no explanation for the fact that 70% of the induced
relations are not considered relevant by the MeSH.
This is what we study in the remainder of this paper:
why these terms are not hierarchically related in the
MeSH, and what kinds of relations exist between
them.
According to the position of the words of the
?parent? term in the ?child? term, we divide the
extra-MeSH relations into three sets:  the par-
ent concept is at the head position in the child con-
cept: absorption/absorption intestinale; 	 the par-
ent concept is at the tail (expansion) position in the
child concept: abdomen/tumeur abdomen; 
	 other
types of positions. Each set of relations is sam-
pled by randomly selecting a 20% subset, both with-
out normalization (raw) and with inflectional and
derivational normalizations (med-lem-stem). Ta-
ble 1 presents the number of analyzed relations (to-
tal = 194).
Normalizations Head Expan. Other
raw 22 31 14
lem-stem-med 37 57 33
Table 1: Relations to analyze: sample sizes.
4 An analysis of new, lexically-induced
relations
We first examine the issues encountered when try-
ing to identify the head of each term (section 4.1),
then review in turn each analyzed subset: head (sec-
tion 4.2), expansion (section 4.3) and other relations
(section 4.4).
4.1 Finding the head
In French, the semantic head of a noun phrase is
usually located at the beginning of this phrase (this
contrasts with English, where the semantic head is
generally at the end of NPs). Moreover, as is often
the case with terms, MeSH terms do not include de-
terminers, so that the semantic head is usually the
first word here. We therefore rely on a heuristic
for determining ?head? and ?expansion? subsets: the
head is the first word of the term, and the expansion
is the last word. This is correct most of the time, but
in some cases, the semantic head is positioned at the
end of the term, generally separated with a comma,
a tradition sometimes followed in thesauri:
filoviridae/filoviridae, infections,
leishmania/leishmania tropica, infection,
quinones/quinone reductases,
neurone/neurone moteur, maladie,
syndrome/bouche main pied, syndrome.
These cases must be hand-corrected and distributed
into the following classes.
We also encountered another kind of error, due to
overzealous derivational knowledge:
contracture/contraction musculaire,
biologie/testament biologique,
where contracture (a muscle disease) and con-
traction (normal muscle function) have both been
stemmed to the same base word; the expansion ad-
jective biologique is derived from the noun biologie,
but its sense is generally more specific than biolo-
gie.
4.2 ?Head? subset
Let us first discard a case where it seems that we
encountered a translation error. An examination of
the structure of the English MeSH and a search on
Web pages show that in the French MeSH, acide
linoleique alpha should read acide linolenique al-
pha, which is a kind of acide linolenique (and not a
kind of acide linoleique). The induced relation:
acide linoleique/acide linoleique alpha
is therefore incorrect; with the correct spelling, the
lexical inclusion:
acide linolenique/acide linolenique alpha
would reveal a correct hierarchical relation.
4.2.1 The head is not the ?genus? of the term
We encountered cases where the whole term did not
have an is-a relation with the head as defined above.
This happens in two types of situations.
The first situation is due to syntactic reasons. In
the following induced relation,
acides amines / acides amines, peptides et pro-
teines,
the larger term is an enumeration, with the sense
of a logical OR. It is therefore the genus term, of
which each of its components (e.g., acides amines)
is a sub-type.
The second situation is due to semantic reasons.
Lexical induction of hierarchical relations assumes
inheritance of the defining features of the genus
term (e.g., a fatty acid, essential is a kind of fatty
acid). However, it is well known that this is not al-
ways true: a plaster cat is not a cat (i.e., a mammal,
etc.). This is sometimes modeled as a type coercion
phenomenon. We found quite a few ?plaster cats? in
our terms:
personnalite/personnalite compulsive,
voix/voix oesophagienne.
For instance, personnalite here describes ?behavior-
response patterns that characterize the individual?,
whereas personnalite compulsive (compulsive per-
sonality disorder) describes a mental disorder. Dis-
orders (or diseases) are different objects than behav-
iors in the MeSH.
4.2.2 The head is ambiguous
This depends on the choice of term names in the ter-
minology (here, the MeSH). Terms like absorption,
investissement, etc., have specific senses that make
them polysemous. To determine a precise sense,
these terms have to be specialized by their contexts:
investissement/investissement (psychanalyse),
absorption/absorption cutanee,
goitre/goitre ovarien
Here, investissement alone (investment) has the fi-
nancial sense, whereas in investissement (psych-
analyse), it has its more generic sense. In a simi-
lar way, absorption has a specific meaning in chem-
istry, and goitre alone is a disorder of the thyroid
gland. These cases are often non-ambiguous in the
original English version of the same terms: for in-
stance, investissement (psychanalyse) (fr) is a trans-
lation of cathexis (en).
A related case occurs when the name of a parent
term is underspecified:
acides/acides pentanoiques,
acne/acne rosacee.
In these examples, acides means inorganic acids1
and acne means acne vulgaris, but the convention
adopted is to use these single words to name the cor-
responding concepts.
4.2.3 Ontological commitment
Finally, some induced links, although absent from
the MeSH, are potentially correct is-a links, but the
designers of the MeSH have made a different mod-
eling choice:
amyotrophies/amyotrophies spinales enfance,
hyperplasie/hyperplasie epitheliale focale,
centre public sante/centre public sante men-
tale,
rectocolite/rectocolite hemorragique,
penicillines/penicilline g.
A general representational choice in the MeSH,
as in some other medical terminologies (e.g.,
SNOMED), is to differentiate on the one hand signs
or symptoms and on the other hand diseases (a
more fully characterized pathological state). This
is the case for amyotrophies and hyperplasie (signs
or symptoms) vs amyotrophies spinales enfance and
hyperplasie epitheliale focale (disease of the ner-
vous system, of the mouth).
For some reason, a centre public sante mentale
(public mental health center) is considered not to
share all the attributes of a general centre public
sante (public health center), which prevents them
from being in a parent-child relationship: they are
only siblings in the MeSH thesaurus.
Penicillines, in the MeSH, have been chosen to
refer to a therapeutic class of drugs (under antibi-
otics, under chemical actions), whereas penicilline
g is considered as a chemical substance.
The structuring involved in these instances re-
flects the ontological commitments of the terminol-
1Note, though, that if inorganic acids was named this way,
it would be impossible to link it by lexical induction to other,
more specific types of inorganic acids.
ogy designers, and cannot be recovered by lexical
inclusion.2
4.3 ?Expansion? subset
When a ?parent? term is in ?expansion? position (end
position) in a ?child? term, we assume that the se-
mantic head of the child term is modified; the in-
duced relation is indeed expected not to be is-a.
Some of the main cases found are close to those for
the ?head? subset. Among others, we find again enu-
merations (see subsection 4.2.1):
immunodepresseurs / antineoplasiques et im-
munodepresseurs
and syntactic ambiguity (subsection 4.2.2):
oncogene/antigene viral oncogene,
where the word oncogene is a noun in the first term
and an adjective in the second one.
Many of the relations found in the ?expansion?
subset are partitive:
abdomen/muscle droit abdomen,
amerique centrale/indien amerique centrale,
argent/nitrate argent.
(human body parts, a continent and its peoples, and
chemical substances).
In some instances, a general type of link between
terms can be detected:
caused-by: myxome/virus myxome,
but in most other cases, we have what looks like a
specific thematic relation between a predicate and
its argument:
comportement alimentaire/troubles comporte-
ment alimentaire,
bovin/pneumonie interstitielle atypique bovin,
hopital/capacite lits hopital,
services sante/fermeture service sante,
macrophage/activation macrophage.
Note that some of these expansion relations involve
adjectival derivations of nouns:
cubitus/nerf cubital,
genes/epreuve complementation genetique.
2They might be amenable to distributional methods if their
contexts of occurrence are different enough.
4.4 ?Other? subset
In this last subset, the ?parent? term can be at any
position in the ?child? term other than head or ex-
pansion. It can also be non-contiguous, accepting
modifiers or some other intervening elements. All
these cases are actually similar to those of the ?ex-
pansion? subset except those of the form:
bacterie aerobie/bacterie gram-negatif aerobie
where bacterie remains the head of the term.
The following examples reproduce the general
cases of the ?expansion? subset with additional mod-
ifiers:
arteres/anevrysme artere iliaque,
hepatite b/virus hepatite b canard,
encephalite/virus encephalite equine ouest,
sommeil/troubles sommeil extrinseques,
irrigation/liquide irrigation endocanalaire,
maladie/assurance maladie personne agee.
In some of them, adjectival derivation is involved:
cellules/molecule-1 adhesion cellulaire vascu-
laire,
chimie/produits chimiques inorganiques,
dent/implantation dentaire sous-periostee.
Some relations are characteristic of the language
of chemical compounds:
cytochrome c/ubiquinol-cytochrome c reduc-
tase,
diphosphate/uridine diphosphate acide glu-
curonique,
lysine/histone-lysine n-methyltransferase.
The ?other? subset alo hosted the following mor-
phosyntactic ambiguity:
cilie/cellule ciliee externe
where the words cilie (noun, an invertebrate organ-
ism) and ciliee (inflected form of adjective cilie,
which characterizes a type of cell) are conflated by
lemmatization. This error is mainly due to the fact
that the MeSH is written with unaccented uppercase
letters: the adjective is actually spelled cili?, which
would be unambiguous here.
5 Synthesis
We presented in this paper a human analysis of auto-
matically, lexically-induced term relations that were
not found in the terminology from which the terms
were obtained (the MeSH thesaurus). This lexical
method considers that a term   is probably a par-
ent of a term  iff all the words of   occur in  .
This inclusion test is helped by morphological nor-
malization.
Morphological normalization was found to be
useful not only in identifying the already ex-
isting relations (section 3.2), but also for the
?new? relations. This confirms previous work by
Jacquemin and Tzoukermann (1999).
The occurrences of syntactic ambiguity
suggest that morphosyntactic tagging could
be useful. The methods specifically de-
signed for detection of syntactic and morpho-
syntactic term variants (Bourigault, 1994;
Jacquemin and Tzoukermann, 1999) might then be
more efficient and less error-prone. We must be
warned however that this may not be an easy task,
since most of the MeSH terms are not syntactically
well-formed (few determiners and prepositions,
inverted heads) and contain rare, technical words
that are likely to be absent from most electronic
lexicons.
Spurious relations may come from several
sources. A few cases are due to abusive morpho-
logical normalization; errors in term names (trans-
lation errors) were also uncovered. We made a dis-
tinction between ?head? and ?expansion? positions
of the ?parent? term in its ?child?. One would expect
that relations where the parent is in head position
would be correct; however, this is not always true.
The putative head of a term is sometimes not cor-
rectly identified because of specific thesaural con-
structs (the ?comma? form) and chemical constructs
(quinone reductases are a kind of reductases) which
display head inversion, and because of enumera-
tions. An additional situation is that of a term whose
actual syntactic head does not entertain an is-a re-
lation with it (the ?plaster cat?). Furthermore, the
head word may not have a stable meaning: it may
be syntactically ambiguous (cilie), polysemous (in-
vestissement) or underspecified (acne).
The remaining ?head? cases reveal specific mod-
eling options, or ?ontological commitments?, of the
terminology designers: the relations induced might
be considered semantically valid, but were dis-
carded in the MeSH because of overall structuring
choices. These choices cannot be predicted with the
lexical methods used here, and seem to be the most
resistant to attempts at automatic derivation. They
also show that what is correct is not necessarily use-
ful for a given terminology.
The ?expansion? cases may be useful to propose
other relations than is-a: we displayed partitive re-
lations, but left to further work a classification of the
remaining ones. The UMLS semantic network rela-
tions (NLM, 2001b) might be a relevant direction to
look into to represent such links.
References
Olivier Bodenreider, Anita Burgun, and Thomas C.
Rindflesch. 2001. Lexically-suggested hy-
ponymic relations among medical terms and their
representation in the UMLS. In URI INIST
CNRS, editor, TIA?2001 Terminologie et Intelli-
gence artificielle, pages 11?21, Nancy.
Didier Bourigault. 1994. Extraction et structura-
tion automatiques de terminologie pour l?aide ?
l?acquisition de connaissances ? partir de textes.
In Proceedings of the 9    Conference RFIA-
AFCET, pages 1123?1132, Paris, France, Jan-
uary. AFCET.
James J Cimino. 1996. Coding systems in health
care. In Jan H. van Bemmel and Alexa T. Mc-
Cray, editors, Yearbook of Medical Informat-
ics ?95 ? The Computer-based Patient Record,
pages 71?85. Schattauer, Stuttgart.
Natalia Grabar and Pierre Zweigenbaum. 2002.
Lexically-based terminology structuring: a fea-
sibility study. In LREC Workshop on Using Se-
mantics for Information Retrieval and Filtering,
pages 73?77, Las Palmas, Canaries, May. ELRA.
Gregory Grefenstette. 1994. Explorations in Auto-
matic Thesaurus Discovery. Natural Language
Processing and Machine Translation. Kluwer
Academic Publishers, London.
Thierry Hamon, Adeline Nazarenko, and C?cile
Gros. 1998. A step towards the detection of
semantic variants of terms in technical docu-
ments. In Christian Boitet, editor, Proceedings
of the 17    COLING, pages 498?504, Montr?al,
Canada, 10?14 August.
Marti A. Hearst. 1992. Automatic acquisition of
hyponyms from large text corpora. In Antonio
Zampolli, editor, Proceedings of the 14    COL-
ING, pages 539?545, Nantes, France, 23?28 July.
INSERM, 2000. Th?saurus Biom?dical
Fran?ais/Anglais. Institut National de la
Sant? et de la Recherche M?dicale, Paris.
Christian Jacquemin and Didier Bourigault. 2002.
Term extraction and automatic indexing. In Rus-
lan Mitkov, editor, Handbook of Computational
Linguistics. Oxford University Press, Oxford. To
appear.
Christian Jacquemin and ?velyne Tzoukermann.
1999. NLP for term variant extraction: A syn-
ergy of morphology, lexicon, and syntax. In
Tomek Strzalkowski, editor, Natural language
information retrieval, volume 7 of Text, speech
and language technology, chapter 2, pages 25?
74. Kluwer Academic Publishers, Dordrecht &
Boston.
David D. McDonald. 1993. Internal and external
evidence in the identification and semantic cate-
gorization of proper names. In Branimir Bogu-
raev and James Pustejovsky, editors, Corpus Pro-
cessing for Lexical Acquisition, pages 61?76.
MIT Press, Cambridge, MA.
Stuart J Nelson, Douglas Johnston, and Betsy L
Humphreys. 2001. Relationships in medical sub-
ject headings. In Carol A Bean and Rebecca
Green, editors, Relationships in the organization
of knowledge, New York. Kluwer Academic Pub-
lishers.
National Library of Medicine, Bethesda, Mary-
land, 2001a. Medical Subject Headings.
www.nlm.nih.gov/mesh/meshhome.html.
National Library of Medicine, Bethesda, Mary-
land, 2001b. UMLS Knowledge Sources Manual.
www.nlm.nih.gov/research/umls/.
Patrick S?gu?la and Nathalie Aussenac. 1999. Ex-
traction de relations s?mantiques entre termes
et enrichissement de mod?les du domaine. In
R?gine Teulier, editor, Actes de IC?99, June.
Pierre Zweigenbaum. 1999. Encoder l?information
m?dicale : des terminologies aux syst?mes de
repr?sentation des connaissances. Innovation
Strat?gique en Information de Sant?, (2?3):27?
47.
Proceedings of the Workshop on BioNLP, pages 89?96,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Exploring graph structure for detection of reliability zones within synonym
resources: Experiment with the Gene Ontology
Thierry Hamon
LIPN ? UMR 7030
Universite? Paris 13 ? CNRS
99 av. J-B Cle?ment
F-93430 Villetaneuse, France
thierry.hamon@lipn.univ-paris13.fr
Natalia Grabar
Centre de Recherche des Cordeliers
Universite? Paris Descartes, UMR S 872
INSERM, U872
HEGP AP-HP, 20 rue Leblanc
Paris, France
natalia.grabar@spim.jussieu.fr
Abstract
Computing the semantic similarity between
terms relies on existence and usage of seman-
tic resources. However, these resources, often
composed of equivalent units, or synonyms,
must be first analyzed and weighted in or-
der to define within them the reliability zones
where the semantic cohesiveness is stronger.
We propose an original method for acquisition
of elementary synonyms based on exploitation
of structured terminologies, analysis of syn-
tactic structure of complex (multi-unit) terms
and their compositionality. The acquired syn-
onyms are then profiled thanks to endogenous
lexical and linguistic indicators (other types
of relations, lexical inclusions, productivity),
which are automatically inferred within the
same terminologies. Additionally, synonymy
relations are observed within graph, and its
structure is analyzed. Particularly, we ex-
plore the usefulness of the graph theory no-
tions such as connected component, clique,
density, bridge, articulation vertex, and cen-
trality of vertices.
1 Introduction
In various tasks and applications of natural language
processing and of biomedical informatics (i.e., query
expansions, information retrieval, text mining, infor-
mation extraction or terminology matching), it is im-
portant to be able to decide whether two terms (i.e.,
acetone anabolism and acetone biosynthesis, repli-
cation of mitochondrial DNA and mtDNA replica-
tion) convey the same or different meaning. This is
particularly important for deciphering and comput-
ing semantic similarity between words and terms.
Lexicon of specific resources (synonym, morpho-
logical or orthographic variants) can be used for de-
tection of semantic similarity. However, depend-
ing on languages and domains, such resources are
not equally well described. Morphological descrip-
tion is the most complete for both general (Bur-
nage, 1990; Hathout et al, 2001) and biomedical
(NLM, 2007; Schulz et al, 1999; Zweigenbaum
et al, 2003) languages. But the situation is not as
successful at the semantic level: little synonym re-
sources can be found. If WordNet (Fellbaum, 1998)
proposes general language synonym relations for
English, the corresponding resources for other lan-
guages are not freely available. Moreover, the ini-
tiative for fitting WordNet to the biomedical area
(Smith and Fellbaum, 2004) seems to have been
abandoned, although there is a huge need for this
kind of resources.
In our previous work, we proposed to use the ex-
isting biomedical terminologies (i.e., Gene Ontology
(Gene Ontology Consortium, 2001), Snomed (Co?te?
et al, 1997), UMLS (NLM, 2007)), wich provide
complex terms, and to acquire from them lexical re-
sources of synonyms. Indeed, the use of complex
biomedical terms seems to be less suitable and gen-
eralizable as compared to lexical resources (Poprat
et al, 2008). Within the biological area, we pro-
posed to exploit the Gene Ontology (GO), and more
specifically to exploit compositional structure of its
terms (Hamon and Grabar, 2008). However, with
the acquisition of synonymy we faced two prob-
lems: (1) contextual character of these relations
(Cruse, 1986), i.e., two terms or words are con-
sidered as synonyms if they can occur within the
89
same context, which makes this relation more or
less broad depending on the usage; (2) ability of
automatic tools to detect and characterize these re-
lations, i.e., two terms or words taken out of their
context can convey different relations than the one
expected. Because we aim at acquiring synonymy
resources which could be used by various applica-
tions and on various corpora, we need to profile them
and possibly to detect the reliability zones. We pro-
posed to do this profiling through lexical and lin-
guistic indicators generated within the same termi-
nology (Grabar et al, 2008), such as productivity,
cooccurence with other types of relations (is-a,
part-of) and with lexical inclusion. These indi-
cators on reliability zones will be used for defining
the synonymity degree of terms and for preparing
the validation of the acquired synonym resources. In
the current work, we continue profiling the acquired
synonyms, but rely on the form of the graph built
from pairs of synonyms. We exploit for this some
notions of the graph theory (Diestel, 2005). In the
following of this paper, we first present our mate-
rial (sec. 2) and methods (sec. 3), we then present
and discuss results (sec. 4) and conclude with some
perspectives (sec. 5).
2 Material
We use the Gene Ontology (GO) as the original re-
source from which synonym lexicon (or elementary
synonym relations) are induced. The goal of the GO
is to produce a structured, common, controlled vo-
cabulary for describing the roles of genes and their
products in any organism. GO terms convey three
types of biological meanings: biological processes,
molecular functions and cellular components. Terms
are structured through four types of relationships:
subsumption is-a, meronymy part-of, syn-
onymy and regulates. The version, we used
in the current work, was downloaded in February
20081. It provides 26,057 concepts and their 79,994
terms. When we create pairs of terms, which we ex-
ploit with our methods, we obtain 260,399 is-a,
29,573 part-of and 459,834 synonymy relations.
There are very few regulates relations, therefore
we don?t exploit them in our work.
1Our previous work has been performed with an anterior
version of the GO.
3 Methods
GO terms present compositional structure, like
within the concept GO:0009073, where composi-
tionality can be observed through the substitution of
one of the components (underlined):
aromatic amino acid family biosynthesis
aromatic amino acid family anabolism
aromatic amino acid family formation
aromatic amino acid family synthesis
Compositionality of the GO terms has been ex-
ploited previously, for instance (Verspoor et al,
2003) propose to derive simple graphs from relations
between complex GO terms, (Mungall, 2004) ex-
ploits the compositionality as a mean for consistency
checking of the GO, (Ogren et al, 2005) use it for
enriching the GO with missing synonym terms. We
propose to exploit the compositionality for induction
of synonym lexical resources (i.e., biosynthesis, an-
abolism, formation, synthesis in the given example).
While the cited works are based on the string match-
ing within GO terms, our approach aims at exploit-
ing the syntactic analysis of terms, which makes it
independent from the graphical form of the analyzed
terms (like examples on fig. 1). Our method has sev-
eral steps: linguistic preprocessing of the GO terms
(sec. 3.1), induction of elementary semantic lexi-
con (sec. 3.2), and then the profiling the synonymy
lexicon through the lexical and linguistic indicators
(sec. 3.3), and through the analysis of connected
components built from the induced synonym pairs
(sec. 3.4). Steps 3.1 to 3.3 have been already de-
scribed in our previous work: we mention here the
main notions for the sake of clarity.
3.1 Preprocessing the GO terms: Ogmios NLP
platform
The aim of terminology preprocessing step is to
provide syntactic analysis of terms for computing
their syntactic dependency relations. We use the
Ogmios platform2 and perform: segmentation into
words and sentences; POS-tagging and lemmatiza-
tion (Schmid, 1994); and syntactic analysis3. Syn-
tactic dependencies between term components are
2http://search.cpan.org/?thhamon/Alvis-NLPPlatform/
3http://search.cpan.org/?thhamon/Lingua-YaTeA/
90
component
expansion headcomponent
replicationmtDNA
component
expansionhead
component
mitochondrial DNAreplication (of)
Figure 1: Parsing tree of the terms replication of mitochondrial DNA and mtDNA replication.
computed according to assigned POS tags and shal-
low parsing rules. Each term is considered as a syn-
tactic binary tree composed of two elements: head
component and expansion component. For instance,
replication is the head component of the two terms
analyzed on figure 1.
3.2 Acquiring the elementary semantic
relations
The notion of compositionality assumes that the
meaning of a complex expression is fully deter-
mined by its syntactic structure, the meaning of its
parts and the composition function (Partee, 1984).
On the basis of syntactically analysed terms, we ap-
ply a set of compositional rules: if the meaning M
of two complex terms A rel B and A? rel B, where
A is its head and B its expansion components, is
given as following:
M(A rel B) = f(M(A),M(B),M(rel))
M(A? rel B) = f(M(A?),M(B),M(rel))
for a given composition function f , if A rel B and
A? rel B are complex synonym terms and if B com-
ponents are identical (such as acetone within ace-
tone catabolism and acetone breakdown), then the
synonymy relation between components A and A?
{catabolism, breakdown} can be induced. The mod-
ification is also accepted on expansion component
B: from terms replication of mitochondrial DNA
and mtDNA replication (fig. 1), we can induce syn-
onymy between mitochondrial DNA and mtDNA.
Finally, the modification is also accepted for both
components A rel B and A? rel B?, such as in
nicotinamide adenine dinucleotide catabolism and
NAD breakdown, where one pair, i.e. {catabolism,
breakdown}, can be known from previously pro-
cessed synonyms and allow to induce the new pair
{nicotinamide adenine dinucleotide, NAD}. The
method is recursive and each induced elementary
synonym relation can then be propagated in order
to induce new elementary relations, which allows to
generate a more exhaustive lexicon of synonyms.
This method is not specific to the synonymy. As
it works at the syntactic level of terms, it there-
fore can be applied to other relationships: relation-
ship between elementary terms is inherited from
the relationship between complex terms. If we ex-
ploit complex terms related with part-of rela-
tions and if the compositionality rules can be ap-
plied, then we can induce elementary part-of re-
lations. For instance, complex terms cerebral cor-
tex development GO:0021987 and cerebral cortex
regionalization GO:0021796 have a part-of re-
lation between them, and we can induce the elemen-
tary part-of relation between their components
development and regionalization. Similarly, on the
basis of two GO terms that have is-a relation be-
tween them, cell activation GO:0001775 and astro-
cyte activation GO:0048143, we can induce the ele-
mentary is-a relation between cell and astrocyte.
3.3 Exploiting lexical and linguistic indicators
Several endogenously generated indicators are used
for profiling the induced lexicon of synonyms:
? Elementary is-a relations;
? Elementary part-of relations;
? Lexical inclusion: terms within each induced
synonymy pair are controlled for the lexical in-
clusion. If the test is positive, like in {DNA
binding, binding}, this would suggest that the
analyzed terms may convey a hierarchical rela-
tion: indeed, lexical subsumption marks often a
hierarchical subsumption (Kleiber and Tamba,
1990), which can be either is-a or part-of
relations;
? Productivity: number of originalGO pairs from
which this elementary relation is inferred. For
instance, synonymy relations {binding, DNA
91
(a) Connected component of synonyms (b) Clique of synonyms
Figure 2: Connected components formed with pairs of elementary synonym relations.
binding} and {cell, lymphocyte} are inferred
from only one original pair of GO synonyms,
while the pair {T-cell, T-lymphocyte} is sup-
ported by eight original GO synonym pairs.
Factors that would weaken synonymy relations and
make them less reliable are their co-occurrence with
lexical inclusions, is-a or part-of relations, and
their low productivity.
3.4 Exploiting the graph theory notions
Pairs of induced synonyms are observed through the
connected components they form: lexical entries are
nodes or vertices and relations between them are
edges or paths. For instance, connected component
2(a) contains four pairs of synonyms: {membrane
lumen, envelope lumen}, {membrane lumen, in-
termembrane space}, {envelope lumen, intermem-
brane space} and {intermembrane space, IMS}. On
each edge, we projected information associated with
the relation corresponding to this edge. For instance,
{membrane lumen, intermembrane space} relation
is labelled as synonymy SY N and shows 2 as pro-
ductivity value (it has been acquired from two origi-
nal pairs of synonyms within GO). If other relation-
ships (INCL, PAR, HIER) are associated to a
given synonymy relation, they are also indicated to-
gether with their productivity.
As a matter of fact, figure 2 presents two typical
examples of connected components we can obtain
(in these examples, both of them have four nodes):
? Connected component (fig. 2(a)) is a graph in
which any two vertices are connected to each
other by edges. Connected components have
not orphan vertices, which would remain not
connected to any other vertex.
? Clique, also called block (fig. 2(b)) is a par-
ticular case of connected components: clique
is a maximally connected component. In such
graphs, all the vertices are interconnected be-
tween them.
We propose to exploit four more notions of the graph
theory, which we assume can be useful for further
profiling of the acquired synonymy relations:
? Density of a connected component is the ra-
tio between the number of its edges and the
number of edges of the corresponding clique.
For instance, the connected component on fig-
ure 2(a) has 4 edges while the corresponding
clique would have 6 edges. In that respect,
this connected component has the dentisty of
0.67. Besides, the clique on figure 2(b) shows
the maximum density (i.e., 1). (For all the fig-
92
ures, we indicate their density, together with the
number of vertices and edges).
? Bridge is defined as an edge which re-
moval would increase the number of con-
nected components. For instance, within con-
nected component 2(a), removing the edge
{intermembrane space, IMS} would lead to the
creation of two new connected components:
(1) single-vertex component IMS, and (2) con-
nected component with three vertices inter-
membrane space, membrane lumen and enve-
lope lumen. Consequently articulation vertices
are defined as vertices which removal would in-
crease the number of connected components.
At figure 2(a), the articulation vertex is inter-
membrane space.
? The centrality of a vertex is defined as the num-
ber of shortest paths passing through it. For in-
stance, on figure 2(a), intermembrane space?s
centrality is 4, while the centrality of other ver-
tices is null.
4 Results and Discussion
4.1 Acquiring the elementary synonymy
relations and their lexical and linguistic
profiling
79 994 GO terms have been fully analyzed through
the Ogmios platform. Compositional rules (sec. 3.2)
have been applied and allowed to induce 9,085 se-
mantic relations among which: 3,019 synonyms,
3,243 is-a and 1,205 part-of. 876 lexical in-
clusions have discovered within all these elementary
pairs. 2,533 synonymy pairs are free of the lexical
profiling indicators. However, 486 synonymy rela-
tions (16%) cooccur with other relations, and the de-
tails of this cooccurrence is showed in table 1. We
can observe for instance that 142 synonym pairs are
also labelled as is-a relations, and 34 as part-of
relations. Productivity of the induced synonyms is
between 1 and 422 original complex GO terms.
Connected component on figure 3 illustrates
coocurrence of synonymy relations with other types
of relations: the pair {import, ion import} shows
synonym and inclusion relations; the pair {import,
uptake} shows synonym and hierarchical relations,
both acquired on seven original pairs of GO terms.
Figure 3: Connected component where synonymy rela-
tions cooccur with other relations.
Synonymy and other relations Number
syno ? is-a 142
syno ? par 34
syno ? incl 309
syno ? par ? is-a 14
syno ? incl ? is-a \ par 40
syno ? incl ? par \ is-a 2
syno ? incl ? is-a ? par 1
Table 1: Number of synonymy relations which cooccur
with other relations (is-a, part-of and lexical inclu-
sions incl).
4.2 Analysing the induced synonym pairs
through the graph theory
3,019 induced synonym pairs have been grouped
into 1,018 connected components. These compo-
nents contain 2 to 69 nodes, related among them
by 1 to 132 edges. Analyses of the connected
components have been performed with Perl pack-
age Graph and additionnal Perl scripts. Among
the studied connected components, we have 914
cliques composed of 2 (n=708), 3 (n=66), 4 (n=88),
5 (n=44) or 6 (n=8) nodes. The remaining 104
connected components are less dense with edges.
The density of the connected components is between
93
Figure 4: Connected component with three bridges: {ion homeostasis, homeostasis}, {homeostasis, regulation} and
{cell cycle control, regulation}.
0.0467 and 1 (in case of cliques). Among the 104
connected components, which are not cliques, we
detected 249 bridges: 0 to 35 depending on con-
nected components. In order to propose a general
approach exploiting graph theory notions for syn-
onym profiling we analyse the structure of three rep-
resentative connected components.
Density of the connected component 2(a) is 0.67.
It contains one bridge: {intermembrane space,
IMS}. This edge corresponds to the acronym and its
expanded form, which can cause its contextual char-
acter. Moreover, intermembrane space is the central
node of this connected component.
Connected component 3 (density=0.38) contains
two bridges {uptake, recycling} and {salvage, cy-
cling}, and three articulation vertices uptake, re-
cicling and salvage with the measures of central-
ity 16, 18 and 10 respectively. Indeed, the major-
ity of shortest paths pass by uptake and recicling
nodes. Otherwise, edges around the salvage ver-
tex are weakened because of the cooccurrence of
synonymy and hierarchical relations. As we have
already noticed, the edge {import, uptake} shows
the cooccurrence of synonymy and hierarchical re-
lations, but its productivity is rather high (seven for
each relation), which stregthens this edge.
Finally, connected component 4 (density=0.33)
contains three bridges {ion homeostasis, homeosta-
sis}, {homeostasis, regulation} and {cell cycle con-
trol, regulation} and three articulation vertices: reg-
ulation, cell cycle control and homeostasis with the
measures of centrality 52, 37 and 16 respectively.
The bridge {ion homeostasis, homeostasis} is weak-
ened by the cooccurrence of synonymy, hierarchi-
cal and lexical inclusion relations. Otherwise, other
edges seem to convey non ambiguous synonymy.
94
From the analyzed examples, we can see that the
graph theory may have several implications on pro-
filing of synonyms. However, these implications
must still be formalized and, possibly, expressed as
a single reliability indicator, alone or combined with
the lexical and linguistic clues.
First, within a connected component, with a given
number of nodes, higher the number of edges, higher
will be its density and closer it will be to a clique
(fig. 2(b)). Consequently, within a clique, the se-
mantic cohesion is more strong. Indeed, in these
cases, terms are far more strongly related between
them. But when the density value decreases the se-
mantic cohesiveness of connected components de-
creases as well. In other words, density is an indi-
cation on the semantic cohesiveness between terms
within connected components. As for bridges, we
assume that they indicate breaking points within
connected components, such as {cell cycle control,
regulation} within figure 4. The weak character
of these points can increased when the synonymy
relation co-occurs with other relationships (is-a,
part-of, lexical inclusion). Consequently, re-
moval of bridges can create connected components
with higher density and therefore with stronger syn-
onymy relations. Finally, the centrality of vertices
measure may be useful for identification of poly-
semic words or terms.
The connected components analysis can also in-
dicate the missing relations. For instance, if a con-
nected component, which is not a clique, has no
bridges but its density is not maximal, this would
indicate that it misses some correct synonymy rela-
tions which can be easily induced.
5 Conclusion and Perspectives
In this paper, we propose an original method for
inducing synonym lexicon from structured termi-
nologies. This method exploits the compositional-
ity principle and three rules based on syntactic de-
pendency analysis of terms. More specifically, we
explore various indicators for profiling the acquired
synonym relations, which is motivated by the fact
that synonymy is a contextual relation and its va-
lidity and universality is not guaranteed. We as-
sume the semantic cohesiveness of synonymy rela-
tions should be qualified and quantified. Thus, we
propose several indicators for profiling the inferred
synonymy relations and for detecting possible weak
and strong points. First, lexical and linguistic clues
are generated endogenously within the same termi-
nology: other types of elementary semantic relations
(is-a and part-of), lexical inclusions and pro-
ductivity of the acquired semantic relations. Then,
more specifically, this work is dedicated to explor-
ing of the usefulness of notions of the graph the-
ory. We propose to study the form and specificities
of connected components formed by synonymy re-
lations. We exploited the following notions from the
graph theory: distinction between connected com-
ponents and cliques, their density, bridges and artic-
ulation vertices within connected components, and
the centrality of their vertices. We observed that the
lexical indicators as well as connected components
characteristics are helpful for profiling the acquired
synonymy relations. These clues are intended to be
used for preparing the validation of this lexicon by
experts and also for its weighting in order to con-
trol and guarantee the specificity of lexicon during
its use by automatic tools.
Currently, we study separately the endogeneous
lexical indicators, and the characteristics of the con-
nected components. However, in the future, these
two types of clues should be combined. For this,
these indicators should be modelized in order to pro-
vide a weight of each edge. This weight can be
used for profiling of connected component through
the detection of strong and weak points. Notice
that the current version of the Graph package can-
not take into account this additional information on
edges and should be modified. Another perspective
is the better exploitation of the Gene Ontology and
taking into account the nature of synonymy relations
as they are labelled by thier creators: exact, broad,
narrow or related. Additionnally, for a more precise
profiling, the four relationships of GO (synonymy,
is-a, part-of and regulates) can be cross-
validated, while currently, we perform the validation
of synonymy relations through is-a and part-of
(and other indicators). We plan also to use the in-
duced relations and propagate them through corpora
and discover some of the missing synonyms (Hole
and Srinivasan, 2000). In this way, applying the
same compositionality principle, we can enrich and
extend the Gene Ontology: new synonyms of GO
95
terms and even other relations between GO terms
and terms from corpora can be detected. As noticed,
this method can be applied to other terminologies
and languages as far as structured terminological re-
sources and NLP tools exist. For instance, within
the context of search of clinical documents, we suc-
cessfully tested this method on the French part of the
UMLS (Grabar et al, 2009). From a more ontolog-
ical perspective, our method can be used for consis-
tency checking of a terminologies, like in (Mungall,
2004). Moreover, as this method performs syntactic
analysis of terms and their decomposition into se-
mantically independent components, it can be used
for the transformation of a pre-coordinated terminol-
ogy into a post-coordinated one.
References
G. Burnage. 1990. CELEX - A Guide for Users. Centre
for Lexical Information, University of Nijmegen.
Roger A. Co?te?, Louise Brochu, and Lyne Cabana, 1997.
SNOMED Internationale ? Re?pertoire d?anatomie
pathologique. Secre?tariat francophone international
de nomenclature me?dicale, Sherbrooke, Que?bec.
David A. Cruse. 1986. Lexical Semantics. Cambridge
University Press, Cambridge.
Reinhard Diestel. 2005. Graph Theory. Springer-Verlag
Heidelberg, New-York.
Christian Fellbaum. 1998. A semantic network of en-
glish: the mother of all WordNets. Computers and Hu-
manities. EuroWordNet: a multilingual database with
lexical semantic network, 32(2-3):209?220.
Gene Ontology Consortium. 2001. Creating the
Gene Ontology resource: design and implementation.
Genome Research, 11:1425?1433.
Natalia Grabar, Marie-Christine Jaulent, and Thierry Ha-
mon. 2008. Combination of endogenous clues for
profiling inferred semantic relations: experiments with
gene ontology. In JAMIA (AMIA 2008), pages 252?6,
Washington, USA.
Natalia Grabar, Paul-Christophe Varoutas, Philippe
Rizand, Alain Livartowski, and Thierry Hamon. 2009.
Automatic acquisition of synonym ressources and as-
sessment of their impact on the enhanced search in
ehrs. Methods of Information in Medicine, 48(2):149?
154. PMID 19283312.
Thierry Hamon and Natalia Grabar. 2008. Acquisition of
elementary synonym relations from biological struc-
tured terminology. In Computational Linguistics and
Intelligent Text Processing (5th International Confer-
ence on NLP, 2006), number 4919 in LNCS, pages 40?
51. Springer.
Nabil Hathout, Fiammetta Namer, and Georgette Dal.
2001. An experimental constructional database: the
MorTAL project. In P. Boucher, editor, Morphology
book. Cascadilla Press, Cambridge, MA.
WT Hole and S Srinivasan. 2000. Discovering missed
synonymy in a large concept-oriented metathesaurus.
In AMIA 2000, pages 354?8.
Georges Kleiber and Ire`ne Tamba. 1990. L?hyperonymie
revisite?e : inclusion et hie?rarchie. Langages, 98:7?
32, juin. L?hyponymie et l?hyperonymie (dir. Marie-
Franc?oise Mortureux).
CJ Mungall. 2004. Obol: integrating language and
meaning in bio-ontologies. Comparative and Func-
tional Genomics, 5(6-7):509?520.
NLM, 2007. UMLS Knowledge Sources Manual. Na-
tional Library of Medicine, Bethesda, Maryland.
www.nlm.nih.gov/research/umls/.
PV Ogren, KB Cohen, and L Hunter. 2005. Implica-
tions of compositionality in the Gene Ontology for its
curation and usage. In Pacific Symposium of Biocom-
puting, pages 174?185.
Barbara H Partee, 1984. Compositionality. F Landman
and F Veltman.
Michael Poprat, Elena Beisswanger, and Udo Hahn.
2008. Building a biowordnet using wordnet data struc-
tures and wordnet?s software infrastructure - a failure
story. In ACL 2008 workshop ?Software Engineering,
Testing, and Quality Assurance for Natural Language
Processing?, pages 31?9.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proceedings of the In-
ternational Conference on New Methods in Language
Processing, pages 44?49, Manchester, UK.
Stefan Schulz, Martin Romacker, Pius Franz, Albrecht
Zaiss, Rdiger Klar, and Udo Hahn. 1999. Towards a
multilingual morpheme thesaurus for medical free-text
retrieval. In Medical Informatics in Europe (MIE).
Barry Smith and Christian Fellbaum. 2004. Medical
wordnet: a new methodology for the construction and
validation of information. In Proc of 20th CoLing,
pages 371?382, Geneva, Switzerland.
Cornelia M Verspoor, Cliff Joslyn, and George J Papcun.
2003. The gene ontology as a source of lexical seman-
tic knowledge for a biological natural language pro-
cessing application. In SIGIR workshop on Text Anal-
ysis and Search for Bioinformatics, pages 51?56.
Pierre Zweigenbaum, Robert Baud, Anita Burgun, Fi-
ammetta Namer, E?ric Jarrousse, Natalia Grabar,
Patrick Ruch, Franck Le Duff, Benot Thirion, and
Ste?fan Darmoni. 2003. Towards a Unified Medical
Lexicon for French. In Medical Informatics in Europe
(MIE).
96
Proceedings of the 2012 Workshop on Biomedical Natural Language Processing (BioNLP 2012), pages 20?28,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
Semantic distance and terminology structuring methods for the
detection of semantically close terms
Marie Dupuch
CNRS UMR 8163 STL
Universite? Lille 1&3
59653 Villeneuve d?Ascq, France
dupuchm@hotmail.fr
Lae?titia Dupuch
Universite? Toulouse III Paul Sabatier
France
laetitia1dupuch@hotmail.com
Thierry Hamon
LIM&BIO (EA3969) UFR SMBH
Universite? Paris 13, France
thierry.hamon@univ-paris13.fr
Natalia Grabar
CNRS UMR 8163 STL
Universite? Lille 1&3
59653 Villeneuve d?Ascq, France
natalia.grabar@univ-lille3.fr
Abstract
The identification of semantically similar lin-
guistic expressions despite their formal differ-
ence is an important task within NLP appli-
cations (information retrieval and extraction,
terminology structuring...) We propose to de-
tect the semantic relatedness between biomed-
ical terms from the pharmacovigilance area.
Two approaches are exploited: semantic dis-
tance within structured resources and termi-
nology structuring methods applied to a raw
list of terms. We compare these methods and
study their complementarity. The results are
evaluated against the reference pharmacovigi-
lance data and manually by an expert.
1 Introduction
When an automatic system is able to identify that
different linguistic expressions convey the same or
similar meanings, this is a positive point for several
applications. For instance, when documents refer-
ring to muscle pain or cephalgia are searched, in-
formation retrieval system can also take advantage
of the synonyms, like muscle ache or headache, to
return more relevant documents and in this way to
increase the recall. This is also a great advantage
for systems designed for instance for text mining,
terminology structuring and alignment, or for more
specific tasks such as pharmacovigilance.
The pharmacovigilance area covers the identifi-
cation of adverse drug reactions (ADRs) in order
to improve the vigilance on the health products.
Pharmacovigilance reports are traditionally encoded
with normalised terms from the dedicated termi-
nologies, such as MedDRA (Medical Dictionary for
Drug Regulatory Activities) (Brown et al, 1999).
MedDRA is a relatively fine-grained terminology
with nearly 90,000 terms. This means that a given
pharmacovigilance report can be coded with dif-
ferent terms which have close meaning (Fescharek
et al, 2004), like muscle pain and muscle ache or
headache and cephalgia: although formally differ-
ent the terms from these pairs have the same mean-
ing. The difficulty is then to detect their semantic
closeness. Indeed, if this semantic information is
available, reports from the phramacovigilance data-
banks and mentionning similar adverse events can
be aggregated: the safety signal is intensified and
the safety regulation process is improved.
In order to aggregate the pharmacovigilance re-
ports, several types of semantic information from
MedDRA are used: (1) different hierarchical levels
of MedDRA between the five levels available; (2)
the SMQs (Standardized MedDRA Queries) which
group together terms associated to a given medical
condition such as Acute renal failure, Angioedema
or Embolic and thrombotic events; and (3) specific
resources (Bousquet et al, 2005; Iavindrasana et al,
2006; Alecu et al, 2008; Jaulent and Alecu, 2009).
The SMQs are defined by groups of experts through
a long and meticulous work consisting of the man-
ual study of the MedDRA structure and of the anal-
ysis of the scientific literature (CIOMS, 2004). 84
SMQs have been created so far. They become the
gold standard data of the pharmacovigilance area.
However, the SMQs currently suffer from the lack of
exhausitivity (Pearson et al, 2009): the set of SMQs
is not exhaustive because this is an ongoing work.
We assume that automatic approaches can be ex-
20
ploited to systematize and accelerate the process of
recruiting the semantically related MedDRA terms
and to build the SMQs. We propose to exploit two
approaches: methods dedicated to the terminology
structuring and semantic distance approaches. We
compare and combine the generated results. For the
evaluation, we compare the results with the existing
SMQs and also analyse them manually with an ex-
pert. Our work is different from previous work be-
cause we exploit the whole set of the available Med-
DRA terms, we apply several methods to cluster the
terms and we perform several types of evaluation.
2 Material
We exploit two kinds of material: material issued
from MedDRA and specific to the pharmacovigi-
lance area (sections 2.1 and 2.3), and linguistic re-
sources issued from general and biomedical lan-
guages (section 2.2). The MedDRA terms are struc-
tured into five hierarchical levels: SOC (System Or-
gan Class) terms belong to the first and the high-
est level, while LLT (Lowest Level Terms) terms be-
long to the fifth and the lowest level. Terms from
the fourth level PT (Preferred Terms) are usually ex-
ploited for the coding of the pharmacovigilance re-
ports. They are also used for the creation of SMQs.
A given PT term may belong to several SMQs.
2.1 Ontology ontoEIM
ontoEIM is an ontology of ADRs (Alecu et al,
2008) created through the projection of MedDRA
to SNOMED CT (Stearns et al, 2001). This projec-
tion is performed thanks to the UMLS (NLM, 2011),
where an important number of terminologies are al-
ready merged and aligned, among which MedDRA
and SNOMED CT. The current rate of alignment of
the PT MedDRA terms with SNOMED CT is weak
(version 2011): 51.3% (7,629 terms). Projection of
MedDRA to SNOMED CT allows to improve the
representation of the MedDRA terms:
? the structure of the MedDRA terms is parallel
to that of SNOMED CT, which makes it more
fine-grained (Alecu et al, 2008). The num-
ber of hierarchical levels within the ontoEIM
reaches 14, instead of five levels in MedDRA;
? the MedDRA terms receive formal defini-
tions: semantic primitives which decompose
the meaning. MedDRA terms can be described
along up to four axes from SNOMED CT, ex-
emplified here through the term Arsenical ker-
atosis: (1) Morphology (type of abnormal-
ity): Squamous cell neoplasm; (2) Topogra-
phy (anatomical localization): Skin structure;
(3) Causality (agent or cause of the abnormal-
ity): Arsenic AND OR arsenic compound; and
(4) Expression (manifestation of the abnormal-
ity): Abnormal keratinization. The formal def-
initions are not complete. For instance, only
12 terms receive formal definitions along these
four axes and 435 along three axes. This is due
to the incomplete alignment of the MedDRA
terms and to the fact these four elements are
not relevant for every term (their absence is not
always problematic).
2.2 Linguistic resources
Linguistic resources provide three kinds of pairs
of synonym words: (1) Medical synonyms ex-
tracted from the UMLS 2011AA (n=228,542) and
then cleaned up (n=73,093); (2) Medical syn-
onyms acquired from three biomedical terminolo-
gies thanks to the exploitation of their composition-
ality (Grabar and Hamon, 2010) (n=28,691); (3)
Synonyms from the general language provided by
WordNet (Fellbaum, 1998) (n=45,782). Among
the pairs of words recorded in these resources, we
can find {accord, concordance}, {aceperone, ac-
etabutone}, {adenazole, tocladesine}, {adrenaline,
epinephrine} or {bleeding, hemorrhage}. The last
two pairs are provided by medical and general re-
sources. However, the pair {accord, concordance}
is provided only by medical resources.
2.3 Standardized MedDRA Queries
We exploit 84 SMQs as reference data. Among these
SMQs, we distinguish 20 SMQs which are struc-
tured hierarchically. We also exploit 92 sub-SMQs,
which compose these 20 hierarchical SMQs.
3 Methods
Our method consists into four main steps (figure 1):
(1) computing of the semantic distance and similar-
ity between the MedDRA terms and their cluster-
ing (section 3.1), (2) the application of the termi-
nology structuring methods to acquire semantic re-
21
Abdominal abscess
Abdominal cavity
T
Abscess morphology
M
Pharyngal abscess
M
Neck structure
T
...
...
...
...
...
...
...
...
...
...
...
...
...
1o
o
o6
5
o 2
o 3
o
7
4o
POS?tagging
Syntactic analysis
Detection ofhierarchical relations
Detection of
synonymy relations
ontoEIM resource (Zhong et al 2002)(Leacock & Chodorow, 1998)
Computing of the semantic distance Clustering of MedDRA terms
and similarity
Term structuring
Lexical inclusion
Synoterm + resources
Genia taggerOgmios platformYaTeA
Pre?processing
Me
rgin
g of
 the
 clu
ster
s
Clustering within directed graphs
EvaluationRadiusHAC (with the R project)
SMQsHierarchical SMQs
sub?SMQs
Semantic distance and similarity approaches
Terminology structuring approach
(Rada et al 1989)
Strongly connected components
Faster
Faster
Raw list ofMedDRAterms
Figure 1: General schema of the experiment composed of four steps: (1) semantic distance approaches, (2) terminology
structuring approaches, (3) their combination and (4) their evaluation
lations between MedDRA terms and their cluster-
ing (section 3.2), (3) the merging of these two sets
of clusters (section 3.3), (4) the evaluation of the
merged clusters (section 3.4). We exploit Perl lan-
guage, R1 project and several NLP tools.
3.1 Semantic distance approach
The semantic distance and similarity approach is ap-
plied to the 7,629 PT MedDRA terms and their for-
mal definitions from ontoEIM. The two main steps
are: computing the distance or similarity (section
3.1.1) and clustering of terms (section 3.1.2).
3.1.1 Computing the semantic distance
Because we work with a tree-structured resource,
we exploit edge-based algorithms to compute the
distance or similarity between two terms t1 and t2:
two semantic distances (Rada (Rada et al, 1989)
and Zhong (Zhong et al, 2002)) and one seman-
tic similarity (Leacock and Chodorow, 1998). In
the following, we call them semantic distance al-
gorithms. For each algorithm, three paths may be
exploited: between the MedDRA terms but also be-
tween the elements of their formal definitions on
two axes (morphology M and topography T often
involved in diagnostics (Spackman and Campbell,
1http://www.r-project.org
1998)). For the illustration, let?s consider two Med-
DRA terms, Abdominal abscess and Pharyngeal ab-
scess defined as follows:
? Abdominal abscess: M = Abscess morphology,
T = Abdominal cavity structure
? Pharyngeal abscess: M = Abscess morphol-
ogy, T = Neck structure
The shortest paths sp are computed between these
two MedDRA terms and between their formal defi-
nitions, whose hierarchical structure is also inherited
from SNOMED CT. The weight of edges is set to 1
because all the relations are of the same kind (hier-
archical), and the value of each shortest path corre-
sponds to the sum of the weights of all its edges. The
semantic distance sd are then exploited to compute
the unique distance between the ADR terms from
MedDRA:
?
i?{ADR,M,T}
Wi ? sdi(t1, t2)
?
i?{ADR,M,T}
Wi
, where the
three axes {ADR,M, T} respectively correspond
to terms meaning the ADR, axis Morphology M
and axis Topography T ; t1 and t2 are two ADR
terms; Wi is the coefficient associated with each
of the three axes; and sdi is the semantic distance
computed on a given axis. We carry out several ex-
22
head
component componentexpansion headcomponent componentexpansion
pain muscle ache muscle
Figure 2: Syntactically analyzed terms (muscle pain and muscle ache) into their head and expansion components
periments. Semi-matrices 7629*7629 with semantic
distance between the terms are built.
3.1.2 Clustering of terms
An unsupervised creation of clusters is applied to
the semi-matrices. We exploit two approaches:
? R radius approach: every MedDRA term is
considered a possible center of a cluster and its
closest terms are clustered with it. The thresh-
olds tested correspond to the following inter-
vals: 2 and 3 for Rada, [0; 5.059] for LCH and
[0; 0.49] for Zhong. The intersection of these
clusters is not empty.
? HAC hierarchical ascendant classification is
performed through the R Project tools (hclust
function). Iteratively, this function chooses the
best centers for terms and builds the hierar-
chy of terms by progressively clustering those
which are closest to these centers. Then the
unique cluster with all the terms is split up.
Several splitting values between 100 and 7,000
are tested. These clusters are exclusive.
Clusters created with the radius approach are
merged in order to eliminate smaller clusters in-
cluded in bigger clusters and in order to aggregate
clusters which have an important intersection be-
tween them. For the intersection, we test several in-
tersection values within the interval [10; 90], which
means that two compared clusters may have between
10% and 90% of common terms.
3.2 Terminology structuring approach
The terminology structuring methods are applied to
a raw list of 18,209 MedDRA PTs. They allow
the detection of semantic relations between these
terms. The POS-tagging is done with Genia tag-
ger (Tsuruoka et al, 2005) and the syntactic analy-
sis with the YATEA parser (Aubin and Hamon, 2006).
Three kinds of methods are applied for the acquisi-
tion of synonymy and hierarchical relations: lexical
inclusions (section 3.2.1), morpho-syntactic variants
(section 3.2.2) and compositionality (section 3.2.3).
The terms are then clustered (section 3.2.4).
3.2.1 Lexical inclusion and hierarchy
The lexical inclusion hypothesis (Kleiber and
Tamba, 1990), which states that when a given term
is lexically included at the head syntactic position
in another term there is a semantic subsumption be-
tween them, allows to identify hierarchical relations
between terms. For instance, on figure 2, the short
term pain is the hierarchical parent and the long term
muscle pain is its hierarchical child because pain is
the syntactic head of muscle pain. The lexical inclu-
sions are computed on POS-tagged and syntactically
analyzed terms. We compute two kinds of lexical in-
clusions:
? syntactic dependencies on minimal syntactic
heads: the parent term corresponds to the short-
est lexical form of the syntactic head. For in-
stance, within the term kaolin cephalin clotting
time, the minimal head is time;
? syntactic dependencies on maximal syntactic
heads: the parent term is the most complete lex-
ical form of the syntactic head. Within the same
term kaolin cephalin clotting time, the maximal
head is cephalin clotting time.
Parent and child terms have to be MedDRA terms.
3.2.2 Morpho-syntactic variants
We exploit Faster (Jacquemin, 1996) for the in-
dentification of morpho-syntactic variants between
the PT terms. This tool applies several transforma-
tion rules, such as insertion (cardiac disease/cardiac
valve disease), morphological derivation (artery
restenosis/arterial restenosis) or permutation (aorta
coarctation/coarctation of the aorta). Each transfor-
mation rule is associated with hierarchical or syn-
onymy relations: the insertion introduces a hierar-
chical relation (cardiac valve disease is more spe-
cific than cardiac disease), while the permutation in-
troduces a synonymy relation. When several trans-
formations are involved, the detected relations may
23
be ambiguous: gland abscess and abscess of sali-
vary gland combines permutation (synonymy) and
insertion (hierarchy) rules. In such cases the hierar-
chical relation prevails.
3.2.3 Compositionality and synonymy
The synonymy relations are acquired in two ways.
First, the synonymy relation is established between
two simple MedDRA terms if this relation is pro-
vided by the linguitistic resources. Second, the
identification of synonym relations between com-
plex terms relies on the semantic compositionality
(Partee, 1984). Hence, two complex terms are con-
sidered synonyms if at least one of their compo-
nents at the same syntactic position (head or ex-
pansion) are synonyms. For instance, on figure 2,
given the synonymy relation between the two words
pain and ache, the terms muscle pain and muscle
ache are also identified as synonyms (Hamon and
Nazarenko, 2001). Three transformation rules are
applied: on the head component (figure 2), on the
expansion component and on both of them. We per-
form several experiments: each medical synonymy
resource is first used individually and then in com-
bination with WordNet.
3.2.4 Clustering of terms
The sets of terms related through the lexical in-
clusions are considered as directed graphs: the terms
are the nodes of the graph while the hierarchical re-
lations are the directed edges. We partition these di-
rected graphs and identify clusters of terms which
could correspond to or be part of the SMQs. Among
connected components and strongly connected com-
ponents, we choose to generate the strongly con-
nected components: they allow an intersection be-
tween clusters which means that a given term may
belong to several clusters (this is also the case with
the SMQs). Thus, within the directed graphs G we
have to identify the maximal sub-graphs H of G
where for each pair {x, y} of the nodes from H ,
there exists a directed edge from x to y (or from y to
x). To improve the coverage of the obtained clusters,
we also add the synonyms: if a term has a synonymy
relation with the term from a cluster then this term
is also included in this cluster. From a graph theory
point of view, the initial graph is augmented with
two edges going from and to the synonyms.
Methods and relationships #relations
Hierarchical relations
Maximal syntactic head 3,366
Minimal syntactic head 3,816
Morpho-syntactic variants 743
Medical synonyms
3 biomedical terminologies 1,879
UMLS/Filtered UMLS 190
Morpho-syntactic variants 100
Medical synonyms and WordNet
3 biomedical terminologies 1,939
UMLS/Filtered UMLS 227
Table 1: Hierarchical and synonymy relations generated
by terminology structuring methods
3.3 Merging of clusters from two approaches
We merge the clusters generated by the two ap-
proaches. The merging is performed on the inter-
section between the clusters. As previously, we test
intersection values within the interval [10; 90].
3.4 Evaluation
We give judgments on: (1) the correctness of the
generated relations, (2) their relevance according to
the reference data, (3) their relevance according to
the manual evaluation by an expert. The evaluation
is performed with three measures: precision P (per-
centage of the relevant terms clustered divided by
the total number of the clustered terms), recall R
(percentage of the relevant terms clustered divided
by the number of terms in the corresponding SMQ)
and F-measure F1. The association between the
SMQs and the clusters relies on the best F1.
4 Results
Semantic relations acquired with terminology struc-
turing are indicated in table 1. There is a small
difference between relations acquired through maxi-
mal and minimal syntactic heads, although the influ-
ence of medical resources for the acquisition of syn-
onymy varies according to the resources. WordNet
slightly increases the number of synonyms. Faster
generates a large set of hierarchical and synonymy
relations. MedDRA terms have also been processed
with semantic distance and clustered. The best
thresholds with the radius clustering are 2 for Rada,
24
Approach Hierarchical SMQs SMQs and sub-SMQs
#clusters interval mean #clusters interval mean
Semantic distance 2,667 [2; 1,206] 73 2,931 [2; 546] 17
Structuring (hierarchical) 690 [1; 134] 3.69 748 [1; 117] 3.43
Structuring (hierarchical+synonymy) 690 [1; 136] 4.11 748 [1; 119] 3.82
Merging (hierarchical) 2,732 [1; 1,220] 72.40 2,998 [1; 563] 24.44
Merging (hierarchical+synonymy) 2,732 [1; 1,269] 75.94 2,998 [1; 594] 26.03
Table 2: Number of clusters and their size (the interval and the mean number of terms per cluster) for individual
approaches and for their merging computed for hierarchical SMQs and also for SMQs and sub-SMQs
 0
 20
 40
 60
 80
 100
 0  10  20  30  40  50  60  70  80
precision
recallf?measure
(a) Semantic distance
 0
 20
 40
 60
 80
 100
 0  10  20  30  40  50  60  70  80
precision
recallf?measure
(b) Terminology structuring
Figure 3: Results (precision, recall and F-measure) for semantic distance and terminology structuring approaches
4.10 for LCH and 0 for Zhong. With the HAC, the
best results are obtained with 300 classes (number of
terms per class is within the interval [1; 98], mean
number of terms per class is 25.34). Our results
show that the best parameters for the semantic dis-
tance are the Rada distance, radius approach and no
formal definitions, while the best parameters for the
terminology structuring are maximal syntactic head
with hierarchical relations by Faster augmented with
synonyms. For the merging of the clusters we apply
50% intersection for hierarchical SMQs and 80% in-
tersection for SMQs and sub-SMQs. We exploit and
discuss these results. The percentage of the Med-
DRA terms involved by the terminology structur-
ing is the 32% with hierarchical relations, it reaches
40% when the synonymy is also considered. With
semantic distance, all the terms from ontoEIM (51%
of the MedDRA) are used.
Table 2 provides information on clusters: num-
ber of clusters, number of terms per cluster (their
interval and the mean number of terms per cluster).
In table 2, we first indicate the results for the indi-
vidual approaches, and then when the merging of
the approaches is performed. We observe that the
merging has a positive effect on the number and the
size of clusters: data generated by the individual ap-
proaches (and by synonymy) are complementary.
4.1 Correctness of the semantic relations
A manual analysis of the generated hierarchical re-
lations indicates that these relations are always cor-
rect: the constraint involved through the syntac-
tic analysis guarantees correct propositions. Nev-
ertheless, we observed a small number of syntac-
tic ambiguities. They appear within 144 pairs (5%)
with maximal syntactic heads and correspond to
pairs like: {anticonvulsant drug level, drug level},
{blood smear test, smear test}, {eye movement dis-
order, movement disorder}. Thus, within the first
25
 0
 20
 40
 60
 80
 100
 0  10  20  30  40  50  60  70  80
precision
recallf?measure
Figure 4: Results (precision, recall and F-measure) ob-
tained when the two approaches are merged
pair, there is an ambiguity on drug as two de-
pendencies seem possible: {anticonvulsant drug
level, drug level} as proposed by the system and
{anticonvulsant drug level, level}. But whatever the
syntactic analysis performed, the semantic relations
are correct.
4.2 Relevance of the generated clusters
Figures 3 and 4 provide quantitative evaluation of
the clusters: semantic distance (figure 3(a)), termi-
nology structuring (figure 3(b)), merging of these
two sets (figure 4). On figure 3, we can observe
that there is a great variability among the SMQs and
the two approaches. The positive result is that these
approaches are indeed complementary: their merg-
ing slightly increases performance. An analysis of
the clusters generated with terminology structuring
shows that: (1) hierarchical relations form the basis
of the clusters: they correspond to 96% of the in-
volved terms and show 69% precision. Only three
clusters do not contain hierarchical relations; (2)
Faster relations are involved in 50% of clusters and
show precision between 75 and 85%; (3) one third
of the clusters contains synonymy relations, which
precision varies between 55 and 69%; (4) relations
acquired with the UMLS resources are involved in
14% of clusters while their precision is only 38%.
We also performed a detailed qualitative analysis
of several SMQs and clusters with an expert. Table 3
presents the analysis for three SMQs: Angioedema,
Embolic and thrombotic events, arterial and Haemo-
dynamic oedema, effusions and fluid overload. It
indicates the number of terms in the SMQ and in
the corresponding clusters clu, as well as the num-
ber of common terms between them com and the
performance (precision P , recall R and F-measure
F ) when computed against the reference data Ref-
erence and also after the analysis performed by the
expert After expertise. The results obtained with
the two approaches are indicated: semantic dis-
tance sd and terminology structuring struc, as well
as their merging merg. In the colums Reference,
we can observe that the best F-measure values are
obtained with the terminology structuring method
for the SMQ Haemodynamic oedema, effusions and
fluid overload (F=45) and with the semantic distance
for the SMQ Embolic and thrombotic events, arte-
rial (F=32). The merging of the two methods sys-
tematically improves the results: in the given exam-
ples, for all three SMQs.
A detailed analysis of the generated noise indi-
cates that across the SMQs we have similar situa-
tions: we generate false positives (terms non rele-
vant for the medical conditions, such as Pulmonary
oedema, Gestational oedema, Spinal cord oedema
for the SMQ Angioedema), but also the SMQs may
contain non relevant terms or may miss relevant
terms (thus, Testicular oedema, Injection site ur-
ticaria, Bronchial eodema are missing in the SMQ
Angioedema). The expert evaluation (columns Af-
ter expertise in table 3) attempts to analyse also the
quality of the SMQs. The corrected performance
of the clusters is improved in several points, which
indicates that automatic approaches may provide a
useful basis for the creation of SMQs.
5 Discussion
Despite the incompleteness of the ontoEIM re-
source, the semantic distance approach is quite ef-
ficient and provides the core terms for the building
of the SMQs. Among the several algorithms tested,
the most simple algorithm (Rada et al, 1989), which
exploits the shortest path, leads to the best results,
while the additional information on the hierarchi-
cal depth exploited by other algorithms appears non
useful. The clustering method which allows the gen-
eration of non-disjoint clusters is the most efficient
as MedDRA terms may belong to several SMQs.
26
Number of terms Reference After expertise
SMQs SMQ clu com P R F P R F
Angioedemasd 52 32 13 40 25 30 43 26 33
Angioedemastruc 52 31 19 61 36 45 61 36 45
Angioedemamerg 52 33 21 63 42 50 71 48 57
Embolic and thrombotic events...sd 132 159 48 30 36 32 32 39 35.2
Embolic and thrombotic events...struc 132 13 12 92 9 16 92 9 16
Embolic and thrombotic events...merg 132 130 49 38 37 37.5 47 46 46.5
Haemodynamic oedema, effusions...sd 36 22 7 32 20 24 54 33 41
Haemodynamic oedema, effusions...struc 36 31 13 42 36 39 84 72 78
Haemodynamic oedema, effusions...merg 36 35 16 46 44 45 86 83 84.5
Table 3: Comparison between the two approaches (semantic distance sd and terminology structuring struc) and the
merging of the two approaches merg for three SMQs: Angioedema, Embolic and thrombotic events, arterial and
Haemodynamic oedema, effusions and fluid overload
Traditionnal classification methods, which produce
disjoint clusters, are less efficient for this task.
It has been surprising to observe that the contri-
bution of the generated hierarchical relations is so
important (table 1) and that these relations appear to
be so often correct for the creation of SMQs. In-
deed, because PT terms belong to the same hierar-
chical level of MedDRA, they should be hierarchi-
cally equivalent between them. In reality, within a
cluster, we can find several hierarchical levels of the
PT terms. This means that the hierarchical structure
of MedDRA could be more fine-grained and that in-
termediate hierarchical levels could be created. As
for the generated synonymy relations, their number
is low and they contribute in a lesser way to the
building of the clusters: this means that the PTs are
semantically differentiated between them.
Finally, the merging of these two approaches is
beneficial for the generation of clusters: the per-
formance is improved, although slightly. The two
approaches provide indeed complementary results.
The low recall and F-measure are due to the material
and methods exploited: ontoEIM contains only 51%
of the MedDRA terms to be processed while the ex-
ploited terminology structuring methods are not able
to detect more common features between the terms.
The difference between the results obtained
against the reference data and after the expert eval-
uation (table 3) show that the reference data are not
very precise. In previous work, it has already been
observed that some important PT terms can be miss-
ing in the SMQs (Pearson et al, 2009). With the
proposed automatic methods we could find some of
these terms. It has been also demonstrated that the
SMQs are over-inclusive (Mozzicato, 2007; Pear-
son et al, 2009). In the proposed analysis of the
SMQs, we have also found terms which have too
large meaning and which should not be included in
the SMQs.
6 Conclusion and Perspectives
We have applied two different approaches to the
clustering of pharmacovigilance terms with simi-
lar or close meaning. We performed a comparison
of the results obtained with these two approaches
and analysed their complementarity. Several experi-
ments have been carried out in order to test different
parameters which may influence the performance of
the methods. Although the automatic creation of the
SMQs is a difficult task, our results seem to indi-
cate that the automatic methods may be used as a
basis for the creation of new SMQs. The precision
of the clusters is often satisfactory, while their merg-
ing leads to the improvement of their completeness.
These approaches generate complementary data and
their combination provides more performant results.
Future studies will lead to the identification of
other parameters which influence the quality of clus-
ters and also other factors which may be exploited
for the merging of clusters. More robust distances
and clustering methods will also be used in future
work, as well as approaches for a better acquisi-
27
tion and evaluation of the hierarchical structure of
SMQs. We plan also to design corpora-based meth-
ods which may also to increase the recall of the re-
sults. We will perform an exhaustive analysis of the
nature of semantic relations which can be observed
within the SMQs and propose other methods to fur-
ther improve the coverage of the clusters. Different
filters will be tested to remove the true false posi-
tive relations between terms. The results will also
be evaluation by several experts, which will allow to
assess the inter-expert variation and its influence on
the results. Besides, the obtained clusters will also
be evaluated through their impact on the pharma-
covigilance tasks and through the exploring of the
pharmacovigilance databases.
References
I Alecu, C Bousquet, and MC Jaulent. 2008. A case
report: using snomed ct for grouping adverse drug re-
actions terms. BMC Med Inform Decis Mak, 8(1):4?4.
S Aubin and T Hamon. 2006. Improving term extrac-
tion with terminological resources. In FinTAL 2006,
number 4139 in LNAI, pages 380?387. Springer.
C Bousquet, C Henegar, A Lillo-Le Loue?t, P Degoulet,
and MC Jaulent. 2005. Implementation of auto-
mated signal generation in pharmacovigilance using a
knowledge-based approach. Int J Med Inform, 74(7-
8):563?71.
EG Brown, L Wood, and S Wood. 1999. The medical
dictionary for regulatory activities (MedDRA). Drug
Saf., 20(2):109?17.
CIOMS. 2004. Development and rational use of stan-
dardised MedDRA queries (SMQs): Retrieving ad-
verse drug reactions with MedDRA. Technical report,
CIOMS.
C Fellbaum. 1998. A semantic network of english: the
mother of all WordNets. Computers and Humanities.
EuroWordNet: a multilingual database with lexical se-
mantic network, 32(2-3):209?220.
R Fescharek, J Ku?bler, U Elsasser, M Frank, and
P Gu?thlein. 2004. Medical dictionary for regulatory
activities (MedDRA): Data retrieval and presentation.
Int J Pharm Med, 18(5):259?269.
N Grabar and T Hamon. 2010. Exploitation of linguis-
tic indicators for automatic weighting of synonyms
induced within three biomedical terminologies. In
MEDINFO 2010, pages 1015?9.
T Hamon and A Nazarenko. 2001. Detection of syn-
onymy links between terms: experiment and results.
In Recent Advances in Computational Terminology,
pages 185?208. John Benjamins.
J Iavindrasana, C Bousquet, P Degoulet, and MC Jaulent.
2006. Clustering WHO-ART terms using semantic
distance and machine algorithms. In AMIA Annu Symp
Proc, pages 369?73.
Christian Jacquemin. 1996. A symbolic and surgical ac-
quisition of terms through variation. In S. Wermter,
E. Riloff, and G. Scheler, editors, Connectionist, Sta-
tistical and Symbolic Approaches to Learning for Nat-
ural Language Processing, pages 425?438, Springer.
MC Jaulent and I Alecu. 2009. Evaluation of an ontolog-
ical resource for pharmacovigilance. In Stud Health
Technol Inform, pages 522?6.
G Kleiber and I Tamba. 1990. L?hyperonymie revisite?e :
inclusion et hie?rarchie. Langages, 98:7?32, juin.
C Leacock and M Chodorow, 1998. Combining local
context and WordNet similarity for word sense iden-
tification, chapter 4, pages 305?332.
P Mozzicato. 2007. Standardised MedDRA queries:
their role in signal detection. Drug Saf, 30(7):617?9.
NLM, 2011. UMLS Knowledge Sources Manual. Na-
tional Library of Medicine, Bethesda, Maryland.
www.nlm.nih.gov/research/umls/.
Barbara H. Partee. 1984. Compositionality. In F. Land-
man and F. Veltman, editors, Varieties of formal se-
mantics. Foris, Dordrecht.
RK Pearson, M Hauben, DI Goldsmith, AL Gould,
D Madigan, DJ O?Hara, SJ Reisinger, and
AM Hochberg. 2009. Influence of the Med-
DRA hierarchy on pharmacovigilance data mining
results. Int J Med Inform, 78(12):97?103.
R Rada, H Mili, E Bicknell, and M Blettner. 1989. De-
velopment and application of a metric on semantic
nets. IEEE Transactions on systems, man and cyber-
netics, 19(1):17?30.
K Spackman and K Campbell. 1998. Composi-
tional concept representation using SNOMED: To-
wards further convergence of clinical terminologies.
In Journal of American Medical Informatics Associ-
ation (JAMIA), pages 740?744.
MQ Stearns, C Price, KA Spackman, and AY Wang.
2001. SNOMED clinical terms: overview of the de-
velopment process and project status. In AMIA, pages
662?666.
Y Tsuruoka, Y Tateishi, JD Kim, T Ohta, J McNaught,
S Ananiadou, and J Tsujii. 2005. Developing a ro-
bust part-of-speech tagger for biomedical text. LNCS,
3746:382?392.
J Zhong, H Zhu, J Li, and Y Yu. 2002. Concep-
tual graph matching for semantic search. In 10th
International Conference on Conceptual Structures,
ICCS2002, LNCS 2393, Springer Verlag, pages 92?
106.
28
Proceedings of the 2012 Workshop on Biomedical Natural Language Processing (BioNLP 2012), pages 109?117,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
Combining Compositionality and Pagerank for the Identification of
Semantic Relations between Biomedical Words
Thierry Hamon
LIM&BIO UFR SMBH
Universite? Paris 13, France
thierry.hamon@univ-paris13.fr
Christopher Engstro?m
Division of Applied Mathematics
Ma?lardalen University
Va?stera?s, Sweden
Mounira Manser
LIM&BIO UFR SMBH
Universite? Paris 13, France
Zina Badji and Natalia Grabar
CNRS UMR 8163 STL
Universite? Lille 1&3
59653 Villeneuve d?Ascq, France
natalia.grabar@univ-lille3.fr
Sergei Silvestrov
Division of Applied Mathematics
Ma?lardalen University
Va?stera?s, Sweden
Abstract
The acquisition of semantic resources and re-
lations is an important task for several appli-
cations, such as query expansion, information
retrieval and extraction, machine translation.
However, their validity should also be com-
puted and indicated, especially for automatic
systems and applications. We exploit the com-
positionality based methods for the acquisi-
tion of synonymy relations and of indicators
of these synonyms. We then apply pager-
ank-derived algorithm to the obtained seman-
tic graph in order to filter out the acquired syn-
onyms. Evaluation performed with two inde-
pendent experts indicates that the quality of
synonyms is systematically improved by 10 to
15% after their filtering.
1 Introduction
Natural languages have extremely rich means to ex-
press or to hide semantic relations: these can be
more or less explicit. Nevertheless, the semantic
relations are important to various NLP tasks within
general or specialized languages (i.e., query expan-
sions, information retrieval and extraction, text min-
ing or machine translation) and their deciphering
must be tackled by automatic approaches. We fo-
cus in this work on synonymy relations. Thus, it
is important to be able to decide whether two terms
(i.e., anabolism and acetone anabolism, acetone an-
abolism and acetone biosynthesis, replication of mi-
tochondrial DNA and mtDNA replication) convey
the same, close or different meanings. According to
the ability of an automatic system to decipher such
relations, the answers of the system will be more or
less exhaustive. Several solutions may be exploited
when deciphering the synonymy relations:
1. Exploitation of the existing resources in which
the synonyms are already encoded. However,
in the biomedical domain, such resources are
not well described. If the morphological de-
scription is the most complete (NLM, 2007;
Schulz et al, 1999; Zweigenbaum et al, 2003),
little or no freely available synonym resources
can be found, while the existing terminologies
often lack the synonyms.
2. Exploitation and adaptation of the existing
methods (Grefenstette, 1994; Hamon et al,
1998; Jacquemin et al, 1997; Shimizu et al,
2008; Wang and Hirst, 2011).
3. Proposition of new methods specifically
adapted to the processed data.
Due to the lack of resources, we propose to ex-
ploit the solutions 2 and 3. In either of these situ-
ations, the question arises about the robustness and
the validity of the acquired relations. For instance,
(Hamon and Grabar, 2008) face two problems: (1)
contextual character of synonymy relations (Cruse,
1986), i.e., two words are considered as synonyms
if they can occur within the same context, which
makes this relation more or less broad depending on
the usage; (2) ability of automatic tools to detect and
characterize these relations, i.e., two words taken out
of their context can convey different relations than
the one expected. Our objective is to assess the relia-
bility of synonymy resources. We propose to weight
and to filter the synonym relations with the pager-
ank-derived algorithm (Brin and Page, 1998). When
109
head
componentcomponentexpansion
storagelipid
componentexpansionheadcomponent
retention lipids(of)
Figure 1: Parsing tree of the terms lipid storage and re-
tention of lipids
processing textual data, this algorithm has been pre-
viously applied in different contexts such as seman-
tic disambiguation (Mihalcea et al, 2004; Sinha and
Mihalcea, 2007; Agirre and Soroa, 2009), summa-
rization (Fernandez et al, 2009) and, more recently,
for the identification of synonyms (Sinha and Mi-
halcea, 2011). This last work takes into account the
usage of a given word in corpora and its known syn-
onyms from lexical resources. Other related works
propose also the exploitation of the random walk al-
gorithm for the detection of semantic relatedness of
words (Gaume, 2006; Hughes and Ramage, 2007)
and of documents (Hassan et al, 2007). Our work
is different from the previous work in several ways:
(1) the acquisition of synonymy is done on resources
provided by a specialized domain; (2) the pager-
ank algorithm is exploited for the filtering of seman-
tic relations generated with linguistically-based ap-
proaches; (3) the pagerank algorithm is adapted to
the small size of the processed data.
In the following of this paper, we present first the
material (section 2), then the method we propose
(section 3). We then describe the experiments per-
formed and the results (section 4), as well as their
evaluation and discussion (section 5). Finally, we
conclude and indicate some perspectives (section 6).
2 Material
We use the Gene Ontology (GO) as the original re-
source from which synonym lexicon (or elementary
synonym relations) are induced. The goal of the GO
is to produce a structured vocabulary for describing
the roles of genes and their products in any organ-
ism. GO terms are structured with four types of re-
lations: subsumption is-a, meronymy part-of,
synonymy and regulates. The version used in
the current work is issued from the UMLS 2011AA.
It provides 54,453 concepts and their 94,161 terms.
The generated pairs of terms have 119,430 is-a
and 101,254 synonymy relations.
3 Methods
Our method has several steps: preprocessing of GO
terms (section 3.1), induction of elementary syn-
onyms (section 3.2) and their characterization with
lexical and linguistic indicators (section 3.3), anal-
ysis of the synonymy graph, its weighting thanks to
the pagerank algorithm and its filtering (section 3.4).
We also perform an evaluation of the generated and
filtered synonymy relations (section 3.5).
In the following, we call original synonyms those
synonyms which are provided by GO, and we call
elementary synonyms those synonyms which are in-
duced by the compositionality based approach.
3.1 Preprocessing the GO terms: Ogmios NLP
platform
The aim of terminology preprocessing step is to
provide syntactic analysis of terms for computing
their syntactic dependency relations. We use the
Ogmios platform1 and perform: segmentation into
words and sentences; POS-tagging and lemmatiza-
tion (Tsuruoka et al, 2005); and syntactic analysis2.
Syntactic dependencies between term components
are computed according to assigned POS tags and
shallow parsing rules. Each term is considered as
a syntactic binary tree composed of two elements:
head component and expansion component. For in-
stance, lipid is the head component of the two terms
analyzed on figure 1.
3.2 Compositionality based induction of
synonyms
GO terms present compositional structure (Verspoor
et al, 2003; Mungall, 2004; Ogren et al, 2005). In
the example below (concept GO:0009073) the com-
positionality can be observed through the substitu-
tion of one of the components (underlined):
aromatic amino acid family biosynthesis
aromatic amino acid family anabolism
aromatic amino acid family formation
aromatic amino acid family synthesis
We propose to exploit the compositionality for in-
duction of synonym resources (i.e., biosynthesis, an-
abolism, formation, synthesis in the given example).
1http://search.cpan.org/?thhamon/Alvis-NLPPlatform/
2http://search.cpan.org/?thhamon/Lingua-YaTeA/
110
While the cited works are based on the string match-
ing, our approach exploits their syntactic analysis,
which makes it independent on their surface graphi-
cal form (like examples on figure 1).
Compositionality assumes that the meaning of a
complex expression is fully determined by its syn-
tactic structure, the meaning of its parts and the com-
position function (Partee, 1984). This assumption is
very often true in specialized langages, which are
known to be compositional. On the basis of syntac-
tically analysed terms, we apply a set of composi-
tional rules: if the meaningM of two complex terms
A rel B and A? rel B, where A is its head and B its
expansion components, is given as following:
M(A rel B) = f(M(A),M(B),M(rel))
M(A? rel B) = f(M(A?),M(B),M(rel))
for a given composition function f , if A rel B and
A? rel B are complex synonym terms and if B com-
ponents are identical (such as acetone within ace-
tone catabolism and acetone breakdown), then the
synonymy relation between components A and A?
{catabolism, breakdown} can be induced. The mod-
ification is also accepted on expansion component
B: from terms replication of mitochondrial DNA
and mtDNA replication (fig. 1), we can induce syn-
onymy between mitochondrial DNA and mtDNA.
Finally, the modification is also accepted for both
components A rel B and A? rel B?, such as in
nicotinamide adenine dinucleotide catabolism and
NAD breakdown, where one pair, i.e. {catabolism,
breakdown}, can be known from previously pro-
cessed synonyms and allow to induce the new
pair {nicotinamide adenine dinucleotide, NAD}. It
should noticed that rel depends on the original re-
lations: if the original terms are synonyms then the
elementary terms are also synonyms, if the original
terms are hierarchically related then the elementary
terms are also hierarchically related, etc.
3.3 Lexically-based profiling of the induced
elementary synonyms
In order to test and improve the quality of the in-
duced synonymy relations, we confront these syn-
onyms with approaches which allow to acquire the
hyperonymy relations. All these resources are endo-
geneously acquired from the same terminology GO:
? Each induced pair of synonyms is controlled
for the lexical inclusion (Kleiber and Tamba,
1990; Bodenreider et al, 2001). If the test is
positive, like in the pair {DNA binding, bind-
ing} this would suggest that this pair may con-
vey a hierarchical relation. Indeed, it has been
observed that lexical subsumption marks often
a hierarchical subsumption. Thus, in the pair
{DNA binding, binding}, binding is the hierar-
chical parent of DNA binding, while DNA bind-
ing has a more specific meaning than binding.
One can assume that the cooccurrence of syn-
onymy with the lexical subsumption makes the
synonymy less reliable;
? The same compositional method, as described
in the previous section, is applied to original
GO term pairs related through is-a relations.
In this way, we can also infer is-a elemen-
tary relations. Thus, if a pair of induced syn-
onyms is also induced through is-a relations,
i.e. {binding, DNA binding}, this also makes
the synonymy relations less reliable.
In summary, an induced synonymy relation is con-
sidered to be less reliable when it cooccurs with
a lexical inclusion or with is-a relation. For in-
stance, several edges from figure 2 present the cooc-
currence of synonymy relations with the is-a rela-
tions (such as, {holding, retention}, {retention, stor-
age} or {retention, sequestering}).
3.4 Pagerank-derived filtering of the induced
elementary synonyms
The induced semantic relations can be represented
as graphs where the nodes correspond to words and
the edges to one or more relations between given two
words. An example of what it can look like can be
seen on figure 2: the induced synonymy relations
may indeed cooccur with non-synonymy relations,
like the hierarchical relations is-a. We propose to
use a pagerank approach (Brin and Page, 1998) in
order to separate a given graph of synonym relations
into subsets (or groups) within which all the words
are considered as synonyms with each other but not
with any other word outside their subset. In order
not to influence the results by the varying size of
the graphs, we exploit a non-normalized version of
pagerank (Engstro?m, 2011). Thus, given the usual
111
storage sequestering
holding
retention
sequestration
syn(3), is?a(1)
syn(1), is?a(3)syn(1), is?a(2)
syn(2) syn(2)
syn(2)
Figure 2: An example of graph generated thanks to the
induced semantic relations: pairs related with synonymy
relations syn may also be related with non-synonymy
relations (like hierarchical relation is-a)
normalized version P (1)Si of pagerank:
Definition 1 P (1)S for system S is defined as the
eigenvector with eigenvalue one to the matrix
M = c(A+ gvT )T + (1? c)v1T
where g is a n ? 1 vector with zeros for nodes with
outgoing nodes and 1 for all dangling nodes, 0 <
c < 1, A is the linkmatrix with sum of every row
equal to one, v is a non-negative weightvector with
sum one.
As we mentioned, with the processed data we
have to use the non-normalized version of pagerank:
Definition 2 P (2)S for system S is defined as:
P (2)S =
P (1)S ||V ||1
d
,with d = 1?
?
cATP (1)S
where V is the part of a global weightvector corre-
sponding to the system S. We let V be the one vector
such that all words are weighted equally.
Looking at the example from figure 2, we start
from any node and then randomly either stop by a
probability c or choose (possibly weighted by edge-
weights) a new node by the probability 1 ? c from
any of those linked to the chosen node. The page-
rank of a node can then be seen as the sum of the
probabilities of all paths to the node in question
(starting in every node once including itself).
Usually A is a two-dimensional matrix in which
the sum of every row is equal to one and all non-
zero elements are equal between them. In order to
use different types of relations and different weights
on these relations we calculate cA. Given B, where
B contains the weights of different edges and their
type, we calculate A as:
Ai,j = (Bi,j,SY N/(Bi,j,OTHER + 1))/ni
where ni is the total number of edges connected to
node i. We treat all relations as symmetric relations
for the filtering algorithm when creating B. While
some relations aren?t symmetric it seems reasonable
to assume they affect the likelihood of synonyms in
both directions. We also do not distinguish non-
synonym relations among them. However, we try
a few variations on how to weight A such as assign-
ing different weights to synonym and non-synonym
relations or using a logarithmic scale to decrease the
effect of very different weights in B.
Further to the weighting, the rows of A do not
necessarily sum to one. We propose then not to
choose a specific value for c, but to threshold the
sum of every row in cA to 0.95. This means that for
most of the rows we set crow = 1/
?
Arow ? 0.95,
but for rows with a low sum we don?t increase the
strength of the links but rather keep them as they
are (crow = 1). Choosing the threshold can be
seen as choosing c in the ordinary pagerank formu-
lation. A low threshold means that only the immedi-
ate surrounding of a node may impact its pagerank,
while a high threshold means that distant nodes may
also have an impact. Higher threshold is also use-
ful to separate the pagerank of nodes and to make
slower the convergence when calculating the pager-
ank. When the sum of all rows is less than one and
all non-zero elements are positive we can guarantee
that the pagerank algorithm converges (Bryan and
Leise, 2006). We also use the Power Method modi-
fied for the non-normalized version of pagerank (En-
gstro?m, 2011). On the basis of these elements, we
apply the following algorithm for segmenting the
graph into groups of nodes:
1. Calculate weighted linkmatrix;
2. Calculate pagerank from uniform weightvector
vi;
112
3. Select the node with the highest pagerank;
4. Calculate pagerank from non-uniform
weightvector (zero vector with a single 1
for the selected node);
5. Nodes with P (2) > cutoff are selected as syn-
onyms with selected node and each other;
6. Remove the found synonym nodes from the
graph;
7. If the graph is non empty, restart from step 1;
8. Otherwise end: words belonging to the same
group are considered as synonyms.
We present the application of the algorithm on
the example from figure 2 using the cutoff =
1.5. We start by calculating the weights on the
links (weighted linkmatrix). For instance, given
the relation from storage to retention we have:
Ai,j = (Bi,j,SY N/(Bi,j,OTHER + 1))/ni =
(1/(2 + 1))/3 = 1/9. After computing the
weights for all the relations and thresholding the
sum of rows to 0.95, when the sum of weights
out of a node is larger than 0.95, we obtain fig-
ure 3. This gives the pagerank from uniform vec-
tor [4.8590, 7.7182, 16.4029, 16.1573, 15.4152], in
which we select the node storage with the highest
pagerank. Pagerank from non-uniform weightvec-
tor is then [0.5490, 1.0970, 4.7875, 4.0467, 3.9079],
in which we select the nodes with rank larger than
cutoff = 1.5 (storage, sequestration, sequestering)
as synonyms. After removing these nodes, we re-
calculate the weight matrix and repeate the algo-
rithm: the two remaining nodes are found to belong
to the same group. We then terminate the algorithm.
3.5 Evaluation protocol
The evaluation is performed against the manually
validated synonymy relations. This validation has
been done by two independent experts with the
background in biology. They were asked to vali-
date the induced synonyms acquired as the step 3.2
of the method. The inter-expert Cohen?s kappa is
0.75. On the basis of this evaluation, we compute
the precision: percentage of relations which allow to
correctly group terms within the connected compo-
nents and the groups. We compute two kinds of pre-
cision (Sebastiani, 2002): micro-precision which is
the classical conception of this measure obtained at
3: storage 4: sequestering
1: holding
2: retention
5: sequestration
0.060.07
0.5
0.44
0.480.48
0.45
0.11
0.45
0.08
0.95
0.44
Figure 3: Example from figure 2 with weighted links
the level of the relations, and macro-precision which
corresponds to the mean of the precisions obtained
at the level of connected components or groups. The
evaluation is done with the induced synonyms and
also after their filtering with the pagerank-derived
algorithm. This last evaluation leads to a better ob-
servation of the efficiency of the pagerank algorithm.
4 Experiments and Results
The GO terms have been fully processed with the
NLP tools (POS-tagging and syntactic analysis) in
order to prepare the next step, during which the ele-
mentary relations and the indicators are acquired.
4.1 Application of the lexical NLP methods
We applied the NLP method to the GO terms.
The application of the compositionality approach to
original synonymy and hierarchical relations gen-
erated 3,707 and 10,068 elementary relations, syn-
onymous and hierarchical respectivelly. Depend-
ing on the syntactic structure of the original terms,
the synonymy relations are induced between simple
or complex terms, but also between their abbrevi-
ated and full forms, between the morpho-syntactic
variants, etc. Very few of these synonyms exist
within GO or within the WordNet resource (Fell-
baum, 1998). We also detected 1,608 lexical in-
clusions. The lexical inclusions and the is-a re-
lations are preserved only if they cooccur with in-
113
duced synonymy relations. All these relations are
then grouped into connected components (figure 2):
the synonymy relations correspond to edges, term
components correspond to nodes, while the infor-
mation on is-a relations and on lexical inclusions
appears as reliability indicators of the synonymy
edges. A total of 2,017 connected components are
generated. The biggest connected component con-
tains 140 nodes and 183 edges. At this step, the con-
nected components are evaluated against the refer-
ence data: we compute the precision.
4.2 Filtering of the induced synonyms with the
pagerank-derived algorithm
We the apply the pagerank-derived algorithm to the
induced synonyms, but also to the combinations of
these synonyms with is-a relations and/or with
lexical inclusions. The objective is then to filter the
induced synonyms and to improve their reliability.
We perform seven experiments, in which the syn-
onymy and the indicators may receive the same im-
portance or may be weighted:
1. syn: only the elementary synonymy relations
are considered;
2. syn-isa: combination of synonymy and hierar-
chical is-a relations;
3. syn-incl: combination of synonymy relations
with lexical inclusions;
4. syn-isa-incl: combination of synonymy and hi-
erarchical relations with lexical inclusions;
5. syn-isa(535): combination of synonymy rela-
tions with lexical inclusions, using different
weights: (Ai,j = 5Bi,j,SY N/(3Bi,j,OTHER +
5))/ni;
6. syn-isa(353): combination of synonymy rela-
tions with lexical inclusions, using different
weights: (Ai,j = 3Bi,j,SY N/(5Bi,j,OTHER +
3))/ni.
7. syn-isa(log): combination of synonymy rela-
tions with lexical inclusions, using logarithmic
weights: (Ai,j = ((1/ln(2))ln(Bi,j,SY N +
1)/((1/ln(2))ln(Bi,j,OTHER + 2)))/ni.
According to the method described in section 3.4,
the connected components of the synonymy rela-
tions obtained in section 3.2 are segmented again
into one or more smaller and more homogeneous
groups. The number of groups varies between 745
and 1,798 across the experiments. Moreover, around
25% of the synonymy relations may be removed by
pagerank. These connected components and groups
can also be evaluated against the reference data and
we can compute the precision.
5 Evaluation and Discussion
The evaluation has been done by two indepen-
dent experts, with the Cohen?s kappa inter-expert
agreement 0.75. We exploit the reference data of
the two experts separately (we distinguish expert1
and expert2) and in common. We also distinguish
macro-precision and micro-precision. Finally, the
precision is first evaluated after the induction step
with the NLP methods, and then after the process-
ing of the acquired synonymy relations through the
pagerank-derived algorithm and their filtering.
For the weighting of the non-synonymy and syn-
onymy relations, we tested and applied several coef-
ficients: 5, 3 and 5 in experiment 5 (syn-isa535); 3,
5 and 3 in experiment 6 (syn-isa353), etc. Different
weights have been tested ranging from 1 to 7, as well
as the log variations. On the whole, these variations
have no significant impact on the results. But then, it
is very important to respect the dependence among
these coefficients and not to set them randomly.
The filtering of the synonymy relations has to con-
trol two factors: (1) the first is related to the fact
that the removed relations are to be true negatives
and that among them there should be no or a small
number of correct relations; while (2) the second is
related to the fact that the remaining relations are to
be true positives and that among them there should
be no or a small number of wrong relations.
Figure 4: Impact of the cutoff values on the filtering of
synonymy relations
114
miP_Exp1 maP_Exp1 miP_Exp2 maP_Exp2
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 0.9
 0.95
 1
 1  2  3  4  5  6  7
before Pagerank
maP_Exp1 and maP_Exp2before Pagerank
miP_Exp1
miP_Exp2 before Pagerank
(a) Connected components with terms
miP_Exp1 maP_Exp1 miP_Exp2 maP_Exp2
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 0.9
 0.95
 1
 1  2  3  4  5  6  7
before Pagerank
maP_Exp1 and maP_Exp2before Pagerank
miP_Exp1
miP_Exp2 before Pagerank
(b) Groups of terms
Figure 5: Evaluation of the results in terms of micro-precision miP and of macro-precision maP for connected
components and for groups of terms (performed according to the reference data provided by two experts)
On figure 4, we present the impact of the cut-
off values on the selection and filtering of the syn-
onyms. Like with other parameters, we have tested
several values between 0.5 and 4. This figure illus-
trates the distribution of the correctly removed rela-
tions. The cutoff values have an important impact
on the results: we can observe that the optimal cut-
off values are set between 1.5 and 2 because they
allow to remove the highest number of the wrong
relations. We have set the cutoff value to 1.5. The
choice of cutoff is an important factor for the defi-
nition of the amount of the links that are to be re-
moved: the higher the cutoff the higher the number
of clusters. On the data processed in this work, the
cutoff value has been defined experimentally thanks
to the observation of the processed data. For the gen-
eralization of this method to new unknown but sim-
ilar linguistic data (new terminology, new langage,
new domain...), the cutoff will be either set in order
to remove a certain predefined number of links or
will be defined from a typical sample of the data.
Contrary to the cutoff values, the choice of thresh-
old doesn?t greatly impact the results, although us-
ing a lower threshold makes it harder to choose a
good cutoff values since the ranking of different
nodes will be closer to each other.
As for the analysis of the precision and of the
relations which are correctly kept within the con-
nected components, let?s observe figure 5. On this
figure, we present the evaluation results performed
within the connected components with induced syn-
onyms (figure 5(a)) and within the groups of filtered
synonyms (figure 5(b)). On the y-axis we indicate
the precision values, and on the x-axis, we indicate
the different experiments performed as mentioned
above: 1 in which only synonyms are exploited, 2
in which synonyms are combined with hierarchical
is-a relations, 3 in which synonyms are combined
with lexical inclusions, etc. Horizontal lines corre-
spond to the precision obtained before the applica-
tion of the pagerank: they remain the same whatever
the experiment. These lines correspond to three ref-
erence data provided by the expert1, the expert2 and
by their common data. As for the points, they indi-
cate the precision obtained further to the pagerank:
it varies according to experiments and experts. On
the basis of figure 5, we can observe that:
? the difference between the expert evaluations is
very low (0.02);
? the pagerank allows to increase the precision
(between 0.10 and 0.15 for micro-precision,
while macro-precision varies by 0.05);
? the consideration of synonymy alone provides
performant results;
? the consideration of is-a relations improves
the results but lexical inclusions decrease them;
? the increased weight of some of the quality in-
dicators has no effect on the evaluation;
? macro-precision is superior to micro-precision
because our data contain mainly small groups,
115
while the few large connected components have
a very low precision;
? there is but a small difference between con-
nected components (figure 5(a)) and groups
(figure 5(b));
? the consideration of is-a relations and of lex-
ical inclusions provides the best precision but
the amount of the remaining synonyms is then
the lowest. As we explained, it is important
to keep the highest number of the correct re-
lations, although when a lot of relations is re-
moved, it is logical to obtain a higher precision.
This means that the combination of is-a re-
lations and of lexical inclusions is not suitable
because it removes too much of synonyms.
In relation with this last observation, is should be
noted that the balance between the removed and the
remaining relations is a subtle parameter.
The obtained results indicate that the pagerank is
indeed useful for the filtering of synonyms, although
the parameters exploited by this algorithm must be
defined accurately. Thus, it appears that synonymy
alone may be sufficient for this filtering. When the
quality indicators are considered, is-a relations are
suitable for this filtering because very often they pro-
pose true hierarchical relations. However, the lex-
ical inclusions have a negative effect of the filter-
ing. We assume this is due to the fact that the lexical
inclusions are ambiguous: they may convey hierar-
chical relations but also equivalence relations (Har-
alambous and Lavagnino, 2011). Indeed, contextu-
ally some terms may be shortened or may be subject
to an elision while their meaning is not impacted.
Currently, the pagerank is limited by the fact that
it is applied to a relatively small set of data while
it is designed to process very large data. Then, it
can be interesting to enrich the model and to be able
to take into account other quality indicators, such as
frequencies, productivity or other semantic relations
proposed within GO (part-of and regulates).
Moreover, we can also give a lesser weight to some
indicators (such as lexical inclusions) with penal-
ties and keep the strong weight for other indicators.
In the current model of the pagerank, we thresh-
old rows to < 0.95. However, we assume that the
algorithm may have problems with very large and
very connected graphs: the pagerank may spread
out in the graph too much and possibly allow the
first words with the highest pagerank to make groups
with only one word. This can be corrected if an addi-
tional calculation is added and when the group con-
tains only one word at step 5.
6 Conclusion and Perspectives
We propose an original approach for inducing syn-
onyms from terminologies and for their filtering.
The methods exploit the NLP methods, composi-
tionality principle and pagerank-derived algorithm.
This work is motivated by the fact that synonymy
is a contextual relation and its validity and univer-
sality are not guaranteed. We assume the seman-
tic cohesiveness of synonymy relations should be
qualified and quantified. The compositionality and
NLP methods allow to acquire endogeneously the
synonymy relations and the quality indicators, while
the pagerank-derived algorithm leads to the filtering
of the acquired synonyms. Its functionning is based
upon the synonymy relations and also upon the ac-
quired indicators (is-a relations and lexical inclu-
sions). It appears that the synonymy relations alone
provide good clues for their filtering. The is-a re-
lations are also fruitful, while the use of the lexical
inclusions appears not to be suitable.
In the future, we plan to add and test other indi-
cators. Other experiments will also be done with the
pagerank approach. For instance, it will be inter-
esting to propose a model which takes into account
that, within a cluster, words may be synonym with
some cluster words but not with all the words of the
cluster. This method can be adapted for the process-
ing of corpora and also applied to terms from other
terminologies. The acquired and filtered synonymy
relations will be exploited within the NLP applica-
tions in order to test the efficiency of these resources
and also the usefulness and efficiency of their filter-
ing. Moreover, the compositionality approach can
be adapted and exploited for the paraphrasing of the
biomedical terms and for the improvement of their
understanding by non expert people.
References
E Agirre and A Soroa. 2009. Personalizing PageRank
for word sense disambiguation. In EACL 2009, pages
33?41, Athens, Greece, March.
116
O Bodenreider, A Burgun, and TC Rindflesch. 2001.
Lexically-suggested hyponymic relations among med-
ical terms and their representation in the UMLS. In
URI INIST CNRS, editor, Terminologie et Intelligence
artificielle (TIA), pages 11?21, Nancy.
S Brin and L Page. 1998. The anatomy of a large-scale
hypertextual Web search engine. Computer Networks
and ISDN Systems, 30(1?7):107?117.
K Bryan and T Leise. 2006. The $25, 000, 000, 000
eigenvector: the linear algebra behind google. SIAM
Rev., 48(3):569?581.
David A. Cruse. 1986. Lexical Semantics. Cambridge
University Press, Cambridge.
C Engstro?m. 2011. Pagerank as a solution to a lin-
ear system, pagerank in changing systems and non-
normalized versions of pagerank. Master?s thesis,
Mathematics, Centre for Mathematical sciences, Lund
University. LUTFMA-3220-2011.
C Fellbaum. 1998. A semantic network of english: the
mother of all WordNets. Computers and Humanities.
EuroWordNet: a multilingual database with lexical se-
mantic network, 32(2-3):209?220.
S Fernandez, E SanJuan, and JM Torres-Moreno. 2009.
Re?sume?s de texte par extraction de phrases, algo-
rithmes de graphe et e?nergie textuelle. In Socie?te?
Francophone de Classification, pages 101?104.
B Gaume. 2006. Cartographier la forme du sens dans les
petits mondes lexicaux. In JADT.
G Grefenstette. 1994. Explorations in automatic the-
saurus discovery. Kluwer Academic Publishers.
T Hamon and N Grabar. 2008. Acquisition of elementary
synonym relations from biological structured termi-
nology. In Computational Linguistics and Intelligent
Text Processing (5th International Conference on NLP,
2006), number 4919 in LNCS, pages 40?51. Springer.
T Hamon, A Nazarenko, and C Gros. 1998. A step
towards the detection of semantic variants of terms
in technical documents. In COLING-ACL?98, pages
498?504.
Y Haralambous and E Lavagnino. 2011. La re?duction de
termes complexes dans les langues de spc?ialite?. TAL,
52(1):37?68.
S Hassan, R Mihalcea, and C Banea. 2007. Random-
walk term weighting for improved text classification.
In ICSC, pages 242?249.
T Hughes and D Ramage. 2007. Lexical semantic relat-
edness with random graph walks. In EMNLP-CoNLL,
pages 581?589. Association for Computational Lin-
guistics.
C Jacquemin, JL Klavans, and E Tzoukerman. 1997.
Expansion of multi-word terms for indexing and re-
trieval using morphology and syntax. In ACL/EACL
97), pages 24?31, Barcelona, Spain.
G Kleiber and I Tamba. 1990. L?hyperonymie revisite?e :
inclusion et hie?rarchie. Langages, 98:7?32, juin.
R Mihalcea, P Tarau, and E Figa. 2004. Pagerank on se-
mantic networks, with application to word sense dis-
ambiguation. In COLING, pages 1126?1132.
CJ Mungall. 2004. Obol: integrating language and
meaning in bio-ontologies. Comparative and Func-
tional Genomics, 5(6-7):509?520.
NLM, 2007. UMLS Knowledge Sources Manual. Na-
tional Library of Medicine, Bethesda, Maryland.
www.nlm.nih.gov/research/umls/.
PV Ogren, KB Cohen, and L Hunter. 2005. Implica-
tions of compositionality in the Gene Ontology for its
curation and usage. In Pacific Symposium of Biocom-
puting, pages 174?185.
BH Partee, 1984. Compositionality. F Landman and F
Veltman.
S Schulz, M Romacker, P Franz, A Zaiss, R Klar, and
U Hahn. 1999. Towards a multilingual morpheme
thesaurus for medical free-text retrieval. In Medical
Informatics in Europe (MIE), pages 891?4.
F Sebastiani. 2002. Machine learning in automated text
categorization. ACM Computing Surveys, 34(1):1?47.
N Shimizu, M Hagiwara, Y Ogawa, K Toyama, and
H Nakagawa. 2008. Metric learning for synonym ac-
quisition. In COLING, pages 793?800.
R Sinha and R Mihalcea. 2007. Unsupervised graph-
based word sense disambiguation using measures of
word semantic similarity. In IEEE International Con-
ference on Semantic Computing (ICSC 2007), pages
363?369.
RS Sinha and RF Mihalcea. 2011. Using centrality algo-
rithms on directed graphs for synonym expansion. In
FLAIRS.
Y Tsuruoka, Y Tateishi, JD Kim, T Ohta, J McNaught,
S Ananiadou, and J Tsujii. 2005. Developing a ro-
bust part-of-speech tagger for biomedical text. LNCS,
3746:382?392.
CM Verspoor, C Joslyn, and GJ Papcun. 2003. The Gene
Ontology as a source of lexical semantic knowledge
for a biological natural language processing applica-
tion. In SIGIR workshop on Text Analysis and Search
for Bioinformatics, pages 51?56.
T Wang and G Hirst. 2011. Exploring patterns in dictio-
nary definitions for synonym extraction. Natural Lan-
guage Engineering, 17.
P Zweigenbaum, R Baud, A Burgun, F Namer, E? Jar-
rousse, N Grabar, P Ruch, F Le Duff, B Thirion, and
S Darmoni. 2003. Towards a Unified Medical Lexicon
for French. In Medical Informatics in Europe (MIE),
pages 415?20.
117
Proceedings of the 5th International Workshop on Health Text Mining and Information Analysis (Louhi) @ EACL 2014, pages 101?105,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Tuning HeidelTime for identifying time expressions in clinical texts in
English and French
Thierry Hamon
LIMSI-CNRS, BP133, Orsay
Universit?e Paris 13
Sorbonne Paris Cit?e, France
hamon@limsi.fr
Natalia Grabar
CNRS UMR 8163 STL
Universit?e Lille 3
59653 Villeneuve d?Ascq, France
natalia.grabar@univ-lille3.fr
Abstract
We present work on tuning the Heideltime
system for identifying time expressions in
clinical texts in English and French lan-
guages. The main amount of the method
is related to the enrichment and adap-
tation of linguistic resources to identify
Timex3 clinical expressions and to nor-
malize them. The test of the adapted ver-
sions have been done on the i2b2/VA 2012
corpus for English and a collection of clin-
ical texts for French, which have been an-
notated for the purpose of this study. We
achieve a 0.8500 F-measure on the recog-
nition and normalization of temporal ex-
pressions in English, and up to 0.9431 in
French. Future work will allow to improve
and consolidate the results.
1 Introduction
Working with unstructured narrative texts is very
demanding on automatic methods to access, for-
malize and organize the information contained in
these documents. The first step is the indexing of
the documents in order to detect basic facts which
will allow more sophisticated treatments (e.g., in-
formation extraction, question/answering, visual-
ization, or textual entailment). We are mostly in-
terested in indexing of documents from the med-
ical field. We distinguish two kinds of indexing:
conceptual and contextual.
Conceptual indexing consists in finding out the
mentions of notions, terms or concepts contained
in documents. It is traditionally done thanks to
the exploitation of terminological resources, such
as MeSH (NLM, 2001), SNOMED International
(C?ot?e et al., 1993), SNOMED CT (Wang et al.,
2002), etc. The process is dedicated to the recog-
nition of these terms and of their variants in doc-
uments (Nadkarni et al., 2001; Mercer and Di
Marco, 2004; Bashyam and Taira, 2006; Schulz
and Hahn, 2000; Davis et al., 2006).
The purpose of contextual indexing is to go fur-
ther and to provide a more fine-grained annota-
tion of documents. For this, additional informa-
tion may be searched in documents, such as polar-
ity, certainty, aspect or temporality related to the
concepts. If conceptual indexing extracts and pro-
vides factual information, contextual indexing is
aimed to describe these facts with more details.
For instance, when processing clinical records, the
medical facts related to a given patient can be aug-
mented with the associated contextual informa-
tion, such as in these examples:
(1) Patient has the stomach aches.
(2) Patient denies the stomach aches.
(3) After taking this medication, patient
started to have the stomach aches.
(4) Two weeks ago, patient experienced the
stomach aches.
(5) In January 2014, patient experienced the
stomach aches.
In example (1), the information is purely fac-
tual, while it is negated in example (2). Example
(3) conveys also aspectual information (the med-
ical problem has started). In examples (4) and
(5), medical events are positioned in the time: rel-
ative (two weeks ago) and absolute (in January
2014). We can see that the medical history of pa-
tient can become more precise and detailed thanks
to such contextual information. In this way, fac-
tual information related to the stomach aches of
patient may receive these additional descriptions
which make each occurrence different and non-
redundant. Notice that the previous I2B2 contests
1
addressed the information extraction tasks related
to different kinds of contextual information.
1
https://www.i2b2.org/NLP
101
Temporality has become an important research
field in the NLP topics and several challenges ad-
dressed this taks: ACE (ACE challenge, 2004),
SemEval (Verhagen et al., 2007; Verhagen et al.,
2010; UzZaman et al., 2013), I2B2 2012 (Sun
et al., 2013). We propose to continue working
on the extraction of temporal information related
to medical events. This kind of study relies on
several important tasks when processing the nar-
rative documents : identification and normaliza-
tion of linguistic expressions that are indicative of
the temporality (Verhagen et al., 2007; Chang and
Manning, 2012; Str?otgen and Gertz, 2012; Kessler
et al., 2012), and their modelization and chain-
ing (Batal et al., 2009; Moskovitch and Shahar,
2009; Pustejovsky et al., 2010; Sun et al., 2013;
Grouin et al., 2013). The identification of tempo-
ral expressions provides basic knowledge for other
tasks processing the temporality information. The
existing available automatic systems such as Hei-
delTime (Str?otgen and Gertz, 2012) or SUTIME
(Chang and Manning, 2012) exploit rule-based
approaches, which makes them adaptable to new
data and areas. During a preliminary study, we
tested several such systems for identification of
temporal relations and found that HeidelTime has
the best combination of performance and adapt-
ability. We propose to exploit this automatic sys-
tems, to adapt and to test it on the medical clinical
documents in two languages (English and French).
In the following of this study, we introduce
the corpora (Section 2) and methods (Section 3).
We then describe and discuss the obtained results
(Section 4.2) and conclude (Section 5).
2 Material
Corpora composed of training and test sets are the
main material we work with. The corpora are in
two languages, English and French, and has com-
parable sizes. All the processed corpora are de-
identified. Corpora in English are built within the
I2B2 2012 challenge (Sun et al., 2013). The train-
ing corpus consists of 190 clinical records and the
test corpus of 120 records. The reference data con-
tain annotations of temporal expressions accord-
ing to the Timex3s guidelines: date, duration, fre-
quency and time (Pustejovsky et al., 2010). Cor-
pora in French are built on purpose of this study.
The clinical documents are issued from a French
hospital. The training corpus consists of 182 clin-
ical records and the test corpus of 120 records. 25
documents from the test set are annotated to pro-
vide the reference data for evaluation.
3 Method
HeidelTime is a cross-domain temporal tagger that
extracts temporal expressions from documents and
normalizes them according to the Timex3 anno-
tation standard, which is part of the markup lan-
guage TimeML (Pustejovsky et al., 2010). This
is a rule-based system. Because the source code
and the resources (patterns, normalization infor-
mation, and rules) are strictly separated, it is pos-
sible to develop and implement resources for ad-
ditional languages and areas using HeidelTime?s
rule syntax. HeidelTime is provided with modules
for processing documents in several languages,
e.g. French (Moriceau and Tannier, 2014). In En-
glish, several versions of the system exist, such as
general-language English and scientific English.
HeidelTime uses different normalization strate-
gies depending on the domain of the documents
that are to be processed: news, narratives (e.g.
Wikipedia articles), colloquial (e.g. SMS, tweets),
and scientific (e.g. biomedical studies). The news
strategy allows to fix the document creation date.
This date is important for computing and normal-
izing the relative dates, such as two weeks ago
or 5 days later, for which the reference point in
time is necessary: if the document creation date is
2012/03/24, two weeks ago becomes 2012/03/10.
Our method consists of three steps: tuning Hei-
delTime to clinical data in English and French
(Section 3.1), evaluation of the results (Section
3.2), and exploitation of the computed data for the
visualization of the medical events (Section 3.3).
3.1 Tuning HeidelTime
While HeidelTime proposes a good coverage of
the temporal expressions used in general language
documents, it needs to be adapted to specialized
areas. We propose to tune this tool to the medi-
cal domain documents. The tuning is done in two
languages (English and French). Tuning involves
three aspects:
1. The most important adaptation needed is re-
lated to the enrichment and encoding of lin-
guistic expressions specific to medical and
especially clinical temporal expressions, such
as post-operative day #, b.i.d. meaning twice
a day, day of life, etc.
102
2. The admission date is considered as the refer-
ence or starting point for computing relative
dates, such as 2 days later. For the identi-
fication of the admission date, specific pre-
processing step is applied in order to detect it
within the documents;
3. Additional normalizations of the temporal
expressions are done for normalizing the
durations in approximate numerical values
rather than in the undefined ?X?-value; and
for external computation for some durations
and frequencies due to limitations in Heidel-
Time?s internal arithmetic processor.
3.2 Evaluating the results
HeidelTime is tuned on the training set. It is evalu-
ated on the test set. The results generated are eval-
uated against the reference data with:
? precision P: percentage of the relevant tem-
poral expressions extracted divided by the to-
tal number of the temporal expressions ex-
tracted;
? recallR: percentage of the relevant temporal
expressions extracted divided by the number
of the expected temporal expressions;
? APR: the arithmetic average of the precision
and recall values
P+R
2
;
? F-measure F : the harmonic mean of the pre-
cision and recall values
P?R
P+R
.
3.3 Exploiting the results
In order to judge about the usefulness of the tem-
poral information extracted, we exploit it to build
the timeline. For this, the medical events are asso-
ciated with normalized and absolute temporal in-
formation. This temporal information is then used
to order and visualize the medical events.
4 Experiments and Results
4.1 Experiments
The experiments performed are the following.
Data in English and French are processed. Data in
two languages are processed by available versions
of HeidelTime: two existing versions (general lan-
guage and scientific language) and the medical
version created thanks to the work performed in
this study. Results obtained are evaluated against
the reference data.
4.2 Results
We added several new rules to HeidelTime (164
in English and 47 in French) to adapt the recog-
nition of temporal expressions in medical docu-
ments. Some cases are difficult to annotate. For
instance, it is complicated to decide whether some
expressions are concerned with dates or durations.
The utterance like 2 years ago (il y a 2 ans) is
considered to indicate the date. The utterance like
since 2010 (depuis 2010) is considered to indicate
the duration, although it can be remarked that the
beginning of the duration interval marks the begin-
ning of the process and its date. Another complex
situation appears with the relative dates:
? as already mentioned, date like 2 years ago
(il y a 2 ans) are to be normalized according
to the reference time point;
? a more complex situation appears with ex-
pressions like the day of the surgery (le jour
de l?op?eration) or at the end of the treatment
by antiobiotics (`a la fin de l?antibiothrapie),
for which it is necessary first to make the ref-
erence in time of the other medical event be-
fore being able to define the date in question.
In Table 1, we present the evaluation results for
English. On the training corpus, with the general
language version and the scientific version of Hei-
delTime, we obtain F-measure around 0.66: preci-
sion (0.77 to 0.79) is higher than recall (0.56). The
values of F-measure and APR are identical. The
version we adapted to the medical language pro-
vides better results for all the evaluation measures
used: F-measure becomes then 0.84, with preci-
sion up to 0.85 and recall 0.84. This is a good im-
provement of the automatic tool which indicates
that specialized areas, such as medical area, use
indeed specific lexicon and constructions. Inter-
estingly, on the test corpus, the results decrease
for the general language and scientific versions
of HeidelTime, but increase for the medical ver-
sion of HeidelTime, with F-measure 0.85. During
the I2B2 competition, the maximal F-measure ob-
tained was 0.91. With F-measure 0.84, our system
was ranked 10/14 on the English data. Currently,
we improve these previous results.
In Table 2, we present the results obtained on
the French test corpus (26 documents). Two ver-
sions of HeidelTime are applied: general lan-
guage, that is already available, and medical, that
has been developed in the presented work. We can
103
Versions of HeidelTime Training Test
P R APR F P R APR F
general language 0.7745 0.5676 0.6551 0.6551 0.8000 0.5473 0.6499 0.6499
scientific 0.7877 0.5676 0.6598 0.6598 0.8018 0.5445 0.6486 0.6486
medical 0.8478 0.8381 0.8429 0.8429 0.8533 0.8467 0.8500 0.8500
Table 1: Results obtained on training and test sets in English.
1990 1995 2000 2005 2010
n
e
g
a
t
i
v
e
m
a
m
m
o
g
r
a
m
b
i
l
a
t
e
r
a
l
b
r
e
a
s
t
m
a
s
s
e
s
l
e
f
t
s
i
m
p
l
e
m
a
s
t
e
c
t
o
m
y
c
h
e
s
t
w
a
l
l
n
o
d
u
l
e
s
d
e
c
r
e
a
s
e
i
n
t
h
e
p
r
e
t
r
a
c
h
e
a
l
n
o
d
e
s
t
h
i
o
t
e
p
a
,
V
e
l
b
a
n
,
M
e
t
h
o
t
r
e
x
a
t
e
c
o
m
p
l
a
i
n
t
s
o
f
f
e
v
e
r
Figure 1: Visualization of temporal data.
Versions of Test
HeidelTime P R F
general language 0.9030 0.9341 0.9183
medical 0.9504 0.9341 0.9422
Table 2: Results obtained on test set in French.
observe that the adapted version suits better the
content of clinical documents and improves the F-
measure values by 3 points, reaching up to 0.94.
The main limitation of the system is due to
the incomplete coverage of the linguistic expres-
sions (e.g. au cours de, mensuel (during, monthly)).
Among the current false positives, we can find ra-
tios (2/10 is considered as date, while it means lab
results), polysemous expressions (Juillet in rue du
14 Juillet (14 Juillet street)), and segmentation errors
(few days detected instead of the next few days).
These limitations will be fixed in the future work.
In Figure 1, we propose a visualization of the
temporal data, which makes use of the temporal
information extracted. In this way, the medical
events can be ordered thanks to their temporal an-
chors, which becomes a very useful information
presentation in clinical practice (Hsu et al., 2012).
The visualization of unspecified expressions (e.g.
later, sooner) is being studied. Although it seems
that such expressions often occur with more spe-
cific expressions (e.g. later that day).
5 Conclusion
HeidelTime, an existing tool for extracting
and normalizing temporal information, has been
adapted to the medical area documents in two
languages (English and French). It is evaluated
against the reference data, which indicates that
its tuning to medical documents is efficient: we
reach F-measure 0.85 in English and up to 0.94
in French. More complete data in French are be-
ing annotated, which will allow to perform a more
complete evaluation of the tuned version. We plan
to make the tuned version of HeidelTime freely
available. Automatically extracted temporal infor-
mation can be exploited for the visualization of the
clinical data related to patients. Besides, these data
can be combined with other kinds of contextual in-
formation (polarity, uncertainty) to provide a more
exhaustive picture of medical history of patients.
Acknowledgments
This work is partially performed under the grant
ANR/DGA Tecsan (ANR-11-TECS-012). The au-
thors are thankful to the CHU de Bordeaux for
making available the clinical documents.
104
References
ACE challenge. 2004. The ACE 2004 eval-
uation plan. evaluation of the recogni-
tion of ace entities, ace relations and ace
events. Technical report, ACE challenge.
http://www.itl.nist.gov/iad/mig/tests/ace/2004.
V Bashyam and Ricky K Taira. 2006. Indexing
anatomical phrases in neuro-radiology reports to the
UMLS 2005aa. In AMIA, pages 26?30.
Iyad Batal, Lucia Sacchi, Riccardo Bellazzi, and Milos
Hauskrecht. 2009. A temporal abstraction frame-
work for classifying clinical temporal data. In AMIA
Annu Symp Proc. 2009, pages 29?33.
Angel X. Chang and Christopher D. Manning. 2012.
SUTIME: A library for recognizing and normalizing
time expressions. In LREC, pages 3735?3740.
Roger A. C?ot?e, D. J. Rothwell, J. L. Palotay, R. S.
Beckett, and Louise Brochu. 1993. The Sys-
tematised Nomenclature of Human and Veterinary
Medicine: SNOMED International. College of
American Pathologists, Northfield.
Neil Davis, Henk Harlema, Rob Gaizauskas, Yikun
Guo, Moustafa Ghanem, Tom Barnwell, Yike Guo,
and Jon Ratcliffe. 2006. Three approaches to GO-
tagging biomedical abstracts. In Udo Hahn and
Michael Poprat, editors, SMBM, pages 21 ? 28, Jena,
Germany.
Cyril Grouin, Natalia Grabar, Thierry Hamon, Sophie
Rosset, Xavier Tannier, and Pierre Zweigenbaum.
2013. Hybrid approaches to represent the clini-
cal patient?s timeline. J Am Med Inform Assoc,
20(5):820?7.
William Hsu, Ricky K Taira, Suzie El-Saden,
Hooshang Kangarloo, and Alex AT Bui. 2012.
Context-based electronic health recond: toward pa-
tient specific healthcare. IEEE Transactions on
information technology in biomedicine, 16(2):228?
234.
Remy Kessler, Xavier Tannier, Caroline Hagge,
Vronique Moriceau, and Andr Bittar. 2012. Find-
ing salient dates for building thematic timelines. In
50th Annual Meeting of the Association for Compu-
tational Linguistics, pages 730?739.
Robert E Mercer and Chrysanne Di Marco. 2004. A
design methodology for a biomedical literature in-
dexing tool using the rhetoric of science. In HLT-
NAACL 2004, Workshop Biolink, pages 77?84.
Vronique Moriceau and Xavier Tannier. 2014. French
resources for extraction and normalization of tempo-
ral expressions with heideltime. In LREC.
Robert Moskovitch and Yuval Shahar. 2009. Medical
temporal-knowledge discovery via temporal abstrac-
tion. In AMIA Annu Symp Proc, pages 452?456.
P Nadkarni, R Chen, and C Brandt. 2001. Umls con-
cept indexing for production databases: a feasibility
study. J Am Med Inform Assoc, 8(1):80?91.
National Library of Medicine, Bethesda, Mary-
land, 2001. Medical Subject Headings.
www.nlm.nih.gov/mesh/meshhome.html.
James Pustejovsky, Kiyong Lee, Harry Bunt, and Lau-
rent Romary. 2010. ISO-TimeML: An interna-
tional standard for semantic annotation. In Nico-
letta Calzolari (Conference Chair), Khalid Choukri,
Bente Maegaard, Joseph Mariani, Jan Odijk, Ste-
lios Piperidis, Mike Rosner, and Daniel Tapias,
editors, Proceedings of the Seventh International
Conference on Language Resources and Evaluation
(LREC?10), Valletta, Malta, may. European Lan-
guage Resources Association (ELRA).
Stefan Schulz and Udo Hahn. 2000. Morpheme-
based, cross-lingual indexing for medical document
retrieval. Int J Med Inform, 58-59:87?99.
Jannik Str?otgen and Michael Gertz. 2012. Temporal
tagging on different domains: Challenges, strate-
gies, and gold standards. In Proceedings of the
Eigth International Conference on Language Re-
sources and Evaluation (LREC?12), pages 3746?
3753. ELRA.
Weiyi Sun, Anna Rumshisky, and
?
Ozlem Uzuner.
2013. Evaluating temporal relations in clinical text:
2012 i2b2 challenge. JAMIA, 20(5):806?813.
Naushad UzZaman, Hector Llorens, Leon Derczyn-
ski, James Allen, Marc Verhagen, and James Puste-
jovsky. 2013. Semeval-2013 task 1: Tempeval-3:
Evaluating time expressions, events, and temporal
relations. In Second Joint Conference on Lexical
and Computational Semantics (*SEM), Volume 2:
Proceedings of the Seventh International Workshop
on Semantic Evaluation (SemEval 2013), pages 1?9,
Atlanta, Georgia, USA, June. Association for Com-
putational Linguistics.
Marc Verhagen, Robert Gaizauskas, Frank Schilder,
Mark Hepple, Graham Katz, and James Pustejovsky.
2007. Semeval-2007 task 15: Tempeval tempo-
ral relation identification. In Proceedings of the
Fourth International Workshop on Semantic Evalua-
tions (SemEval-2007), pages 75?80, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
Marc Verhagen, Roser Sauri, Tommaso Caselli, and
James Pustejovsky. 2010. Semeval-2010 task 13:
Tempeval-2. In Proceedings of the 5th Interna-
tional Workshop on Semantic Evaluation, pages 57?
62, Uppsala, Sweden, July. Association for Compu-
tational Linguistics.
AY Wang, JH Sable, and KA Spackman. 2002. The
snomed clinical terms development process: refine-
ment and analysis of content. In AMIA, pages 845?
9.
105
Proceedings of the 3rd Workshop on Predicting and Improving Text Readability for Target Reader Populations (PITR) @ EACL 2014, pages 11?20,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Automatic diagnosis of understanding of medical words
Natalia Grabar
CNRS UMR 8163 STL
Universit?e Lille 3
59653 Villeneuve d?Ascq, France
natalia.grabar@univ-lille3.fr
Thierry Hamon
LIMSI-CNRS, BP133, Orsay
Universit?e Paris 13
Sorbonne Paris Cit?e, France
hamon@limsi.fr
Dany Amiot
CNRS UMR 8163 STL
Universit?e Lille 3
59653 Villeneuve d?Ascq, France
dany.amiot@univ-lille3.fr
Abstract
Within the medical field, very specialized
terms are commonly used, while their un-
derstanding by laymen is not always suc-
cessful. We propose to study the under-
standability of medical words by laymen.
Three annotators are involved in the cre-
ation of the reference data used for training
and testing. The features of the words may
be linguistic (i.e., number of characters,
syllables, number of morphological bases
and affixes) and extra-linguistic (i.e., their
presence in a reference lexicon, frequency
on a search engine). The automatic cate-
gorization results show between 0.806 and
0.947 F-measure values. It appears that
several features and their combinations are
relevant for the analysis of understandabil-
ity (i.e., syntactic categories, presence in
reference lexica, frequency on the general
search engine, final substring).
1 Introduction
The medical field has deeply penetrated our daily
life, which may be due to personal or family
health condition, watching TV and radio broad-
casts, reading novels and journals. Nevertheless,
the availability of this kind of information does not
guarantee its correct understanding, especially by
laymen, such as patients. The medical field has in-
deed a specific terminology (e.g., abdominoplasty,
hepatic, dermabrasion or hepatoduodenostomy)
commonly used by medical professionals. This
fact has been highlighted in several studies dedi-
cated for instance to the understanding of pharma-
ceutical labels (Patel et al., 2002), of information
provided by websites (Rudd et al., 1999; Berland
et al., 2001; McCray, 2005; Oregon Evidence-
based Practice Center, 2008), and more generally
the understanding between patients and medical
doctors (AMA, 1999; McCray, 2005; Jucks and
Bromme, 2007; Tran et al., 2009).
We propose to study the understanding of words
used in the medical field, which is the first step to-
wards the simplification of texts. Indeed, before
the simplification can be performed, it is neces-
sary to know which textual units may show under-
standing difficulty and should be simplified. We
work with data in French, such as provided by
an existing medical terminology. In the remain-
der, we present first some related work, especially
from specialized fields (section 2). We then intro-
duce the linguistic data (section 4) and methodol-
ogy (section 5) we propose to test. We present and
discuss the results (section 6), and conclude with
some directions for future work (section 7).
2 Studying the understanding of words
The understanding (of words) may be seen as a
scale going from I can understand to I cannot un-
derstand, and containing one or more intermediate
positions (i.e., I am not sure, I have seen it be-
fore but do not remember the meaning, I do not
know but can interpret). Notice that it is also re-
lated to the ability to provide correct explanation
and use of words. As we explain later, we con-
sider words out of context and use a three-position
scale. More generally, understanding is a complex
notion closely linked to several other notions stud-
ied in different research fields. For instance, lex-
ical complexity is studied in linguistics and gives
clues on lexical processes involved, that may im-
pact the word understanding (section 2.1). Work
in psycholinguistics is often oriented on study of
word opacity and the mental processes involved in
their understanding (Jarema et al., 1999; Libben et
al., 2003). Readability provides a set of methods
to compute and quantify the understandability of
words (section 2.3). The specificity of words to
specialized areas is another way to capture their
understandability (section 2.2). Finally, lexical
11
simplification aims at providing simpler words to
be used in a given context (section 2.3).
2.1 Linguistics
In linguistics, the question is closely related to lex-
ical complexity and compoundings. It has been
indeed observed that at least five factors, linguis-
tic and extra-linguistic, may be involved in the se-
mantic complexity of the compounds. One factor
is related to the knowledge of the components of
the complex words. Formal (how the words, such
as a?erenchyme, can be segmented) and seman-
tic (how the words can be understood and used)
points of view can be distinguished. A second
factor is that complexity is also due to the vari-
ety of morphological patterns and relations among
the components. For instance, ?erythrocyte (erythro-
cyte) and ovocyte (ovocyte) instantiate the [N1N2]
pattern in which N2 (cyte) can be seen as a con-
stant element (Booij, 2010), although the relations
between N1 and N2 are not of the same type in
these two compounds: in ?erythrocyte, N1 ?erythr(o)
denotes a property of N2 (color), while in ovo-
cyte, N1 ovo (egg) corresponds to a specific de-
velopment stage of female cells. Another factor
appears when some components are polysemous,
within a given field (i.e., medical field) or across
the fields. For instance, a?er(o) does not always
convey the same meaning: in a?eroc`ele, a?er- de-
notes ?air? (tumefaction (c`ele) formed by an air in-
filtration), but not in a?erasth?enie, which refers to
an asthenia (psychic disorder) observable among
jet pilots. Yet another factor may be due to the dif-
ference in the order of components: according to
whether the compounding is standard (in French,
the main semantic element is then on the left, such
as in pneu neige (snow tyre), which is fundamen-
tally a pneu (tyre)) or neoclassical (in French, the
main semantic element is then on the right, such as
?erythrocyte, which is a kind of cyte cell / corpuscle
with red color). It is indeed complicated for a user
without medical training to correctly interpret a
word that he does not know and for which he can-
not reuse the existing standard compounding pat-
terns. This difficulty is common to all Roman lan-
guages (Iacobini, 2003), but not to Germanic lan-
guages (L?udeling et al., 2002). Closely related is
the fact that with neoclassical compounds, a given
component may change its place according to the
global semantics of the compounds, such as path-
in pathology, polyneuropathe, cardiopathy. Fi-
nally, the formal similarity between some deriva-
tion processes (such as the derivation in -oide, like
in lipoid) and neoclassical compounding (such as
-ase in lipase), which apply completely different
interpretation patterns (Iacobini, 1997; Amiot and
Dal, 2005), can also make the understanding more
difficult.
2.2 Terminology
In the terminology field, the automatic identifica-
tion of difficulty of terms and words remains im-
plicit, while this notion is fundamental in termi-
nology (W?uster, 1981; Cabr?e and Estop`a, 2002;
Cabr?e, 2000). The specificity of terms to a given
field is usually studied. The notion of understand-
ability can be derived from it. Such studies can
be used for filtering the terms extracted from spe-
cialized corpora (Korkontzelos et al., 2008). The
features exploited include for instance the pres-
ence and the specificity of pivot words (Drouin
and Langlais, 2006), the neighborhood of the term
in corpus or the diversity of its components com-
puted with statistical measures such as C-Value or
PageRank (Daille, 1995; Frantzi et al., 1997; May-
nard and Ananiadou, 2000). Another possibility is
to check whether lexical units occur within refer-
ence terminologies and, if they do, they are con-
sidered to convey specialized meaning (Elhadad
and Sutaria, 2007).
2.3 NLP studies
The application of the readability measures is an-
other way to evaluate the complexity of words and
terms. Among these measures, it is possible to dis-
tinguish classical readability measures and com-
putational readability measures (Franc?ois, 2011).
Classical measures usually rely on number of let-
ters and/or of syllables a word contains and on
linear regression models (Flesch, 1948; Gunning,
1973), while computational readability measures
may involve vector models and a great variabil-
ity of features, among which the following have
been used to process the biomedical documents
and words: combination of classical readability
formulas with medical terminologies (Kokkinakis
and Toporowska Gronostaj, 2006); n-grams of
characters (Poprat et al., 2006), manually (Zheng
et al., 2002) or automatically (Borst et al., 2008)
defined weight of terms, stylistic (Grabar et al.,
2007) or discursive (Goeuriot et al., 2007) fea-
tures, lexicon (Miller et al., 2007), morphologi-
cal features (Chmielik and Grabar, 2011), combi-
12
Categories A1 (%) A2 (%) A3 (%) Unanimity (%) Majority (%)
1. I can understand 8,099 (28) 8,625 (29) 7,529 (25) 5,960 (26) 7,655 (27)
2. I am not sure 1,895 (6) 1,062 (4) 1,431 (5) 61 (0.3) 597 (2)
3. I cannot understand 19,647 (66) 19,954 (67) 20,681 (70) 16,904 (73.7) 20,511 (71)
Total annotations 29,641 29,641 29,641 22,925 28,763
Table 1: Number (and percentage) of words assigned to reference categories by three annotators (A1, A2
and A3), and in the derived datasets unanimity and majority.
nations of different features (Wang, 2006; Zeng-
Treiler et al., 2007; Leroy et al., 2008).
Specific task has been dedicated to the lexi-
cal simplification within the SemEval challenge in
2012
1
. Given a short input text and a target word
in English, and given several English substitutes
for the target word that fit the context, the goal
was to rank these substitutes according to how
?simple? they are (Specia et al., 2012). The par-
ticipants applied rule-based and/or machine learn-
ing systems. Combinations of various features
have been used: lexicon from spoken corpus
and Wikipedia, Google n-grams, WordNet (Sinha,
2012); word length, number of syllables, latent se-
mantic analysis, mutual information and word fre-
quency (Jauhar and Specia, 2012); Wikipedia fre-
quency, word length, n-grams of characters and of
words, random indexing and syntactic complexity
of documents (Johannsen et al., 2012); n-grams
and frequency from Wikipedia, Google n-grams
(Ligozat et al., 2012); WordNet and word fre-
quency (Amoia and Romanelli, 2012).
3 Aims of the present study
We propose to investigate how the understandabil-
ity of French medical words can be diagnosed with
NLP methods. We rely on the reference annota-
tions performed by French speakers without medi-
cal training, which we associate with patients. The
experiments performed rely on machine learning
algorithms and a set of 24 features. The medical
words studied are provided by an existing medical
terminology.
4 Linguistic data and their preparation
The linguistic data are obtained from the medical
terminology Snomed International (C?ot?e, 1996).
This terminology?s aim is to describe the whole
medical field. It contains 151,104 medical terms
structured into eleven semantic axes such as dis-
1
http://www.cs.york.ac.uk/semeval-2012/
orders and abnormalities, procedures, chemical
products, living organisms, anatomy, social sta-
tus, etc. We keep here five axes related to the
main medical notions (disorders, abnormalities,
procedures, functions, anatomy). The objective
is not to consider axes such as chemical products
(trisulfure d?hydrog`ene (hydrogen sulfide)) and living
organisms (Sapromyces, Acholeplasma laidlawii)
that group very specific terms hardly known by
laymen. The 104,649 selected terms are tokenized
and segmented into words (or tokens) to ob-
tain 29,641 unique words: trisulfure d?hydrog`ene
gives three words (trisulfure, de, hydrog`ene).
This dataset contains compounds (abdominoplas-
tie (abdominoplasty), dermabrasion (dermabrasion)),
constructed (cardiaque (cardiac), acineux (acinic),
lipo??de (lipoid)) and simple (acn?e (acne), fragment
(fragment)) words. These data are annotated by
three speakers 25-40 year-old, without medical
training, but with linguistic background. We ex-
pect the annotators to represent the average knowl-
edge of medical words amongst the population as
a whole. The annotators are presented with a list of
terms and asked to assign each word to one of the
three categories: (1) I can understand the word;
(2) I am not sure about the meaning of the word;
(3) I cannot understand the word. The assumption
is that the words, which are not understandable by
the annotators, are also difficult to understand by
patients. These manual annotations correspond to
the reference data (Table 1).
5 Methodology
The proposed method has two aspects: gener-
ation of the features associated to the analyzed
words and a machine learning system. The main
research question is whether the NLP methods
can distinguish between understandable and non-
understandable medical words and whether they
can diagnose these two categories.
13
5.1 Generation of the features
We exploit 24 linguistic and extra-linguistic fea-
tures related to general and specialized languages.
The features are computed automatically, and can
be grouped into ten classes:
Syntactic categories. Syntactic categories and
lemmas are computed by TreeTagger (Schmid,
1994) and then checked by Flemm (Namer, 2000).
The syntactic categories are assigned to words
within the context of their terms. If a given word
receives more than one category, the most fre-
quent one is kept as feature. Among the main
categories we find for instance nouns, adjectives,
proper names, verbs and abbreviations.
Presence of words in reference lexica. We ex-
ploit two reference lexica of the French language:
TLFi
2
and lexique.org
3
. TLFi is a dictionary of the
French language covering XIX and XX centuries.
It contains almost 100,000 entries. lexique.org is a
lexicon created for psycholinguistic experiments.
It contains over 135,000 entries, among which in-
flectional forms of verbs, adjectives and nouns. It
contains almost 35,000 lemmas.
Frequency of words through a non specialized
search engine. For each word, we query the
Google search engine in order to know its fre-
quency attested on the web.
Frequency of words in the medical terminology.
We also compute the frequency of words in the
medical terminology Snomed International.
Number and types of semantic categories asso-
ciated to words. We exploit the information on the
semantic categories of Snomed International.
Length of words in number of their characters
and syllables. For each word, we compute the
number of its characters and syllables.
Number of bases and affixes. Each lemma
is analyzed by the morphological analyzer D?erif
(Namer and Zweigenbaum, 2004), adapted to the
treatment of medical words. It performs the de-
composition of lemmas into bases and affixes
known in its database and it provides also seman-
tic explanation of the analyzed lexemes. We ex-
ploit the morphological decomposition informa-
tion (number of affixes and bases).
Initial and final substrings of the words. We
compute the initial and final substrings of differ-
ent length, from three to five characters.
2
http://www.atilf.fr/
3
http://www.lexique.org/
Number and percentage of consonants, vowels
and other characters. We compute the number and
the percentage of consonants, vowels and other
characters (i.e., hyphen, apostrophe, comas).
Classical readability scores. We apply two clas-
sical readability measures: Flesch (Flesch, 1948)
and its variant Flesch-Kincaid (Kincaid et al.,
1975). Such measures are typically used for eval-
uating the difficulty level of a text. They exploit
surface characteristics of words (number of char-
acters and/or syllables) and normalize these values
with specifically designed coefficients.
5.2 Machine learning system
The machine learning algorithms are used to study
whether they can distinguish between words un-
derstandable and non-understandable by laymen
and to study the importance of various features for
the task. The functioning of machine learning al-
gorithms is based on a set of positive and nega-
tive examples of the data to be processed, which
have to be described with suitable features such
as those presented above. The algorithms can then
detect the regularities within the training dataset to
generate a model, and apply the generated model
to process new unseen data. We apply various al-
gorithms available within the WEKA (Witten and
Frank, 2005) platform.
The annotations provided by the three annota-
tors constitute our reference data. We use on the
whole five reference datasets (Table 1): 3 sets of
separate annotations provided by the three anno-
tators (29,641 words each); 1 unanimity set, on
which all the annotators agree (n=22,925); 1 ma-
jority set, for which we can compute the major-
ity agreement (n=28,763). By definition, the two
last datasets should present a better coherence and
less annotation ambiguity because some ambigui-
ties have been resolved by unanimity or by major-
ity vote.
5.3 Evaluation
The inter-annotator agreement is computed with
the Cohen?s Kappa (Cohen, 1960), applied to pairs
of annotators, which values are then leveraged to
obtain the unique average value; and Fleiss? Kappa
(Fleiss and Cohen, 1973), suitable for processing
data provided by more than two annotators. The
interpretation of the scores are for instance (Landis
and Koch, 1977): substantial agreement between
0.61 and 0.80, almost perfect agreement between
0.81 and 1.00.
14
With machine learning, we perform a ten-fold
cross-validation, which means that the evaluation
test is performed ten times on different randomly
generated test sets (1/10 of the whole dataset),
while the remaining 9/10 of the whole dataset is
used for training the algorithm and creating the
model. In this way, each word is used during the
test step. The success of the applied algorithms is
evaluated with three classical measures: R recall,
P precision and F F-measure. In the perspective
of our work, these measures allow evaluating the
suitability of the methodology to the distinction
between understandable and non-understandable
words and the relevance of the chosen features.
The baseline corresponds to the assignment of
words to the biggest category, e.g., I cannot under-
stand, which represents 66 to 74%, according to
datasets. We can also compute the gain, which is
the effective improvement of performance P given
the baseline BL (Rittman, 2008):
P?BL
1?BL
.
6 Automatic analysis of
understandability of medical words:
Results and Discussion
We address the following aspects: annotations
(inter-annotator agreement, assignment of words
to three categories), quantitative results provided
by the machine learning algorithms, impact of the
individual features on the distinction between cat-
egories, and usefulness of the method.
6.1 Annotations and inter-annotator
agreement
The time needed for performing the manual ref-
erence annotations depends on annotators and
ranges from 3 to 6 weeks. The annotation results
presented in Table 1 indicate that the annotators
1 and 2 often provide similar results on their un-
derstanding of the medical words, while for the
third annotator the task appears to be more difficult
as he indicates globally a higher number of non-
understandable words. The non-understandable
words are the most frequent for all annotators and
cover 66 to 70% of the whole dataset. The inter-
annotator agreement shows substantial agreement:
Fleiss? Kappa 0.735 and Cohen?s Kappa 0.736.
This is a very good result, especially when work-
ing with linguistic data for which the agreement is
usually difficult to obtain.
The evolution of annotations per category (Fig-
ure 1), such as provided by the annotators, can dis-
 0
 5000
 10000
 15000
 20000
 0  5000  10000  15000  20000  25000
Nu
mb
er i
n e
ach
 ca
teg
ory
Words
I cannot understand
I can understand
I am not sure
A1A2A3
Figure 1: Evolution of the annotations within the
reference data.
tinguish easily between the three categories: (1)
the most frequently chosen category is I cannot
understand and it grows rapidly with new words;
(2) the next most frequently chosen category is I
can understand, although it grows more slowly;
(3) the third category, which gathers the words on
which the annotators show some hesitation, is very
small. Given the proximity between the lines in
each category, we can conclude that the annota-
tors have similar difficulties in understanding the
words from the dataset.
6.2 Quantitative results obtained with
machine learning
P R F
J48 0.876 0.889 0.881
RandomForest 0.880 0.892 0.884
REPTree 0.874 0.890 0.879
DecisionTable 0.872 0.891 0.880
LMT 0.876 0.895 0.884
SMO 0.858 0.876 0.867
Table 2: Performance obtained on the majority
dataset with various algorithms.
We tested several machine learning algorithms
to discover which of them are the most suitable
to the task at hand. In Table 2, with results com-
puted on the majority dataset, we can observe that
the algorithms provide with similar performance
(between 0.85 and 0.90 P and R). In the remain-
ing of the paper, we present results obtained with
J48 (Quinlan, 1993). Table 3 shows P , R and
F values for the five datasets: three annotators,
majority and unanimity datasets. We can observe
15
that, among the three annotators, it is easier to
reproduce the annotations of the third annotator:
we gain then 0.040 with F comparing to the two
other annotators. The results become even better
with the majority dataset (F=0.881), and reach F
up to 0.947 on the unanimity dataset. As we ex-
pected, these two last datasets present less annota-
tion ambiguity. The best categorization results are
observed with I can understand and I cannot un-
derstand categories, while the I am not sure cate-
gory is poorly managed by machine learning algo-
rithms. Because this category is very small, the av-
erage performance obtained on all three categories
remains high.
A1 A2 A3 Una. Maj.
P 0.794 0.809 0.834 0.946 0.876
R 0.825 0.826 0.862 0.949 0.889
F 0.806 0.814 0.845 0.947 0.881
Table 3: J48 performance obtained on five datasets
(A1, A2, A3, unanimity and majority).
In Table 4, we indicate the gain obtained by J48
compared to baseline: it ranges from 0.13 to 0.20,
which is a good improvement, despite the cate-
gory I am not sure that is difficult to discriminate.
We also indicate the accuracy obtained on these
datasets.
A1 A2 A3 Una. Maj.
BL 0.66 0.67 0.70 0.74 0.71
F 0.806 0.814 0.845 0.947 0.881
gain 0.14 0.13 0.14 0.20 0.16
Acc. 0.825 0.826 0.862 0.948 0.889
Table 4: Gain obtained for F by J48 on five
datasets (A1, A2, A3, unanimity and majority).
6.3 Impact of individual features on
understandability of medical words
To observe the impact of individual features, we
did several iterations of experiments during which
we incrementally increased the set of features: we
started with one feature and then, at each iteration,
we added one new feature, up to the 24 features
available. We tried several random orders. The
test presented here is done again on the majority
dataset. Figures 2 present the results obtained in
terms of P , R and F . Globally, we can observe
that some features show positive impact while oth-
ers show negative or null impact:
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 5  10  15  20
Per
for
ma
nce
Feature subsets
POS?tag
initial substrings
final substrings
word length reference lexica
web frequency
PrecisionRecallF?measure
Figure 2: Impact of individual features.
? with the syntactic categories (POS-tags)
alone we obtain P and R between 0.65 and
0.7. The performance is then close to the
baseline performance. Often, proper names
and abbreviations are associated with the
non-understandable words. There is no dif-
ference between TreeTagger alone and the
combination of TreeTagger with Flemm;
? the initial and final substrings have positive
impact. Among the final substrings, those
with three and four characters (ie, -omie of
-tomie (meaning cut), -phie of -rraphie (mean-
ing stitch), -?emie (meaning blood)) show posi-
tive impact, but substrings with five charac-
ters have negative impact and the previously
gained improvement is lost. We may con-
clude that the five-character long final sub-
strings may be too specific;
? the length of words in characters have neg-
ative impact on the categorization results.
There seems to be no strong link between this
feature and the understanding of words: short
and long words may be experienced as both
understandable or not by annotators;
? the presence of words in the reference lexica
(TLFI and lexique.org) is beneficial to both
precision and recall. We assume these lexica
may represent common lexical competence
of French speakers. For this reason, words
that are present in these lexica, are also easier
to understand;
? the frequencies of words computed through
a general search engine are beneficial.
16
Words with higher frequencies are often as-
sociated with a better understanding, al-
though the frequency range depends on the
words. For instance, coccyx (coccyx) or drain
(drain) show high frequencies (1,800,000 and
175,000,000, respectively) and they belong
indeed to the I can understand category.
Words like colique (diarrhea) or clitoridien
(clitoral) show lower frequencies (807,000 and
9,821, respectively), although they belong to
the same category. On contrary, other words
with quite high frequencies, like coagulase
(coagulase), clivage (cleavage) or douve (fluke)
(655,000, 1,350,000 and 1,030,000, respec-
tively) are not understood by the annotators.
According to these experiments, our results point
out that, among the most efficient features, we can
find syntactic categories, presence of words in the
reference lexica, frequencies of words on Google
and three- and four-character end substring. In
comparison to the existing studies, such as those
presented during the SemEval challenge (Specia
et al., 2012), we propose to exploit a more com-
plete set of features, several of which rely on the
NLP methods (e.g., syntactic tagging, morpholog-
ical analysis). Especially the syntactic tagging ap-
pears to be salient for the task. In comparison to
work done on general language data (Gala et al.,
2013), our experiment shows better results (be-
tween 0.825 and 0.948 accuracy against 0.62 ac-
curacy in the cited work), which indicates that spe-
cialized domains have indeed very specific words.
Additional tests should be performed to obtain a
more detailed impact of the features.
6.4 Usefulness of the method
We applied the proposed method to words from
discharge summaries. The documents are pre-
processed according to the same protocol and the
words are assigned the same features as previ-
ously (section 5). The model learned on the una-
nimity set is applied. The results are shown in
Figure 3. Among the words categorized as non-
understandable (in red and underlined), we find:
? abbreviations (NIHSS, OAP, NaCl, VNI);
? technical medical terms (hypoesth?esie
(hypoesthesia), par?esie (paresia), throm-
bolyse (thrombolysis), iatrog`ene (iatrogenic),
oxyg?enoth?erapie (oxygen therapy), d?esaturation
(desaturation));
Figure 3: Detection of non-understandable words
within discharge summaries.
? medication names (CALCIPARINE);
In the example from Figure 3, three types of errors
can be distinguished when common words are cat-
egorized as non-understandable:
? inflected forms of words (suites (conse-
quences), cardiologiques (cardiological));
? constructed forms of words (thrombolys?e
(with thrombolysis));
? hyphenated words (post-r?eanimation (post
emergency medical service)).
Notice that in other processed documents, other
errors occur. For instance, misspelled words and
words that miss accented characters (probleme
instead of probl`eme (problem), realise instead of
r?ealis?e (done), particularite instead particularit?e
(particularity)) are problematic. Another type of er-
rors may occur when technical words (e.g. pro-
lapsus (prolapsus), paroxysme (paroxysm), tricuspide
(tricuspid)) are considered as understandable.
Besides, only isolated words are currently pro-
cessed, which is the limitation of the current
method. Still, consideration of complex medi-
cal terms, that convey more complex medical no-
tions, should also be done. Such terms may indeed
change the understanding of words, as in these ex-
amples: AVC isch?emique (ischemic CVA (cerebrovas-
cular accident)), embolie pulmonaire basale droite
(right basal pulmonary embolism), d?esaturation `a 83 %
17
(desaturation at 83%), anticoagulation curative (cu-
rative anticoagulation). In the same way, numerical
values may also arise misunderstanding of medi-
cal information. Processing of these additional as-
pects (inflected and constructed forms of words,
hyphenated or misspelled words, complex terms
composed with several words and numerical val-
ues) is part of the future work.
6.5 Limitations of the current study
We proposed several experiments for analyzing
the understandability of medical words. We tried
to analyze these data from different points of view
to get a more complete picture. Still, there are
some limitations. These are mainly related to the
linguistic data and to their preparation.
The whole set of the analyzed words is large:
almost 30,000 entries. We assume it is possi-
ble that annotations provided may show some
intra-annotator inconsistencies due for instance to
the tiredness and instability of the annotators (for
instance, when a given unknown morphological
components is seen again and again, the meaning
of this component may be deduced by the anno-
tator). Nevertheless, in our daily life, we are also
confronted to the medical language (our personal
health or health of family or friend, TV and ra-
dio broadcast, various readings of newspapers and
novels) and then, it is possible that the new med-
ical notions may be learned during the annotation
period of the words, which lasted up to four weeks.
Nevertheless, the advantage of the data we have
built is that the whole set is completely annotated
by each annotator.
When computing the features of the words, we
have favored those, which are computed at the
word level. In the future work, it may be interest-
ing to take into account features computed at the
level of morphological components or of complex
terms. The main question will be to decide how
such features can be combined all together.
The annotators involved in the study have a
training in linguistics, although their relation with
the medical field is poor: they have no specific
health problems and no expertise in medical ter-
minology. We expect they may represent the av-
erage level of patients with moderate health lit-
eracy. Nevertheless, the observed results may re-
main specific to the category of young people with
linguistic training. Additional experiments are re-
quired to study this aspect better.
7 Conclusion and Future research
We proposed a study of words from the medi-
cal field, which are manually annotated as under-
standable, non-understandable and possibly un-
derstandable to laymen. The proposed approach
is based on machine learning and a set with 24
features. Among the features, which appear to be
salient for the diagnosis of understandable words,
we find for instance the presence of words in the
reference lexica, their syntactic categories, their fi-
nal substring, and their frequencies on the web.
Several features and their combinations can be dis-
tinguished, which shows that the understandability
of words is a complex notion, which involves sev-
eral linguistic and extra-linguistic criteria.
The avenue for future research includes for in-
stance the exploitation of corpora, while currently
we use features computed out of context. We
assume indeed that corpora may provide addi-
tional relevant information (semantic or statistical)
for the task aimed in this study. Additional as-
pects related to the processing of documents (in-
flected and constructed forms of words, hyphen-
ated or misspelled words, complex terms com-
posed with several words and numerical values) is
another perspective. Besides, the classical read-
ability measures exploited have been developed
for the processing of English language. Working
with French-language data, we should use mea-
sures, which are adapted to this language (Kandel
and Moles, 1958; Henry, 1975). In addition, we
can also explore various perspectives, which ap-
pear from the current limitations, such as comput-
ing and using features computed at different levels
(morphological components, words and complex
terms), applying other classical readability mea-
sures adapted to the French language, and adding
new reference annotations provided by laymen
from other social-professional categories.
Acknowledgments
This work is performed under the grant
ANR/DGA Tecsan (ANR-11-TECS-012) and
the support of MESHS (COMETE project). The
authors are thankful to the CHU de Bordeaux for
making available the clinical documents.
References
AMA. 1999. Health literacy: report of the council
on scientific affairs. Ad hoc committee on health lit-
18
eracy for the council on scientific affairs, American
Medical Association. JAMA, 281(6):552?7.
D Amiot and G Dal. 2005. Integrating combining
forms into a lexeme-based morphology. In Mediter-
ranean Morphology Meeting (MMM5), pages 323?
336.
M Amoia and M Romanelli. 2012. Sb: mmsystem -
using decompositional semantics for lexical simpli-
fication. In *SEM 2012, pages 482?486, Montr?eal,
Canada, 7-8 June. Association for Computational
Linguistics.
GK Berland, MN Elliott, LS Morales, JI Algazy,
RL Kravitz, MS Broder, DE Kanouse, JA Munoz,
JA Puyol, M Lara, KE Watkins, H Yang, and
EA McGlynn. 2001. Health information on the in-
ternet. accessibility, quality, and readability in en-
glish ans spanish. JAMA, 285(20):2612?2621.
Geert Booij. 2010. Construction Morphology. Oxford
University Press, Oxford.
A Borst, A Gaudinat, C Boyer, and N Grabar. 2008.
Lexically based distinction of readability levels of
health documents. In MIE 2008. Poster.
MT Cabr?e and R Estop`a. 2002. On the units of spe-
cialised meaning uses in professional com- muni-
cation. In International Network for Terminology,
pages 217?237.
TM Cabr?e. 2000. Terminologie et linguistique: la
thorie des portes. Terminologies nouvelles, 21:10?
15.
J Chmielik and N Grabar. 2011. D?etection de la
sp?ecialisation scientifique et technique des docu-
ments biom?edicaux gr?ace aux informations mor-
phologiques. TAL, 51(2):151?179.
Jacob Cohen. 1960. A coefficient of agreement
for nominal scales. Educational and Psychological
Measurement, 20(1):37?46.
RA C?ot?e, 1996. R?epertoire d?anatomopathologie de la
SNOMED internationale, v3.4. Universit?e de Sher-
brooke, Sherbrooke, Qu?ebec.
B Daille. 1995. Rep?erage et extraction de terminologie
par une approche mixte statistique et linguistique.
Traitement Automatique des Langues (T.A.L.), 36(1-
2):101?118.
P Drouin and P Langlais. 2006. valuation du potentiel
terminologique de candidats termes. In JADT, pages
379?388.
N Elhadad and K Sutaria. 2007. Mining a lexicon
of technical terms and lay equivalents. In BioNLP,
pages 49?56.
JL Fleiss and J Cohen. 1973. The equivalence of
weighted kappa and the intraclass correlation coef-
ficient as measures of reliability. Educational and
Psychological Measurement, 33:613?619.
R Flesch. 1948. A new readability yardstick. Journal
of Applied Psychology, 23:221?233.
T Franc?ois. 2011. Les apports du traitements automa-
tique du langage la lisibilit du franais langue tran-
gre. Phd thesis, Universit Catholique de Louvain,
Louvain.
KT Frantzi, S Ananiadou, and J Tsujii. 1997. Auto-
matic term recognition using contextual clues. In
MULSAIC IJCAI, pages 73?79.
N Gala, T Franc?ois, and C Fairon. 2013. Towards a
french lexicon with difficulty measures: NLP help-
ing to bridge the gap between traditional dictionaries
and specialized lexicons. In eLEX-2013.
L Goeuriot, N Grabar, and B Daille. 2007. Car-
act?erisation des discours scientifique et vulgaris?e en
franc?ais, japonais et russe. In TALN, pages 93?102.
N Grabar, S Krivine, and MC Jaulent. 2007. Classifi-
cation of health webpages as expert and non expert
with a reduced set of cross-language features. In
AMIA, pages 284?288.
R Gunning. 1973. The art of clear writing. McGraw
Hill, New York, NY.
G Henry. 1975. Comment mesurer la lisibilit. Labor,
Bruxelles.
C Iacobini. 1997. Distinguishing derivational pre-
fixes from initial combining forms. In First mediter-
ranean conference of morphology, Mytilene, Island
of Lesbos, Greece, septembre.
C Iacobini, 2003. Composizione con elementi neoclas-
sici, pages 69?96.
Gonia Jarema, Cline Busson, Rossitza Nikolova,
Kyrana Tsapkini, and Gary Libben. 1999. Process-
ing compounds: A cross-linguistic study. Brain and
Language, 68(1-2):362?369.
SK Jauhar and L Specia. 2012. Uow-shef: Sim-
plex ? lexical simplicity ranking based on contextual
and psycholinguistic features. In *SEM 2012, pages
477?481, Montr?eal, Canada, 7-8 June. Association
for Computational Linguistics.
A Johannsen, H Mart??nez, S Klerke, and A S?gaard.
2012. Emnlp@cph: Is frequency all there is to sim-
plicity? In *SEM 2012, pages 408?412, Montr?eal,
Canada, 7-8 June. Association for Computational
Linguistics.
R Jucks and R Bromme. 2007. Choice of words
in doctor-patient communication: an analysis of
health-related internet sites. Health Commun,
21(3):267?77.
L Kandel and A Moles. 1958. Application de lindice
de flesch la langue franaise. Cahiers tudes de
Radio-Tlvision, 19:253?274.
19
JP Kincaid, RP Jr Fishburne, RL Rogers, and
BS Chissom. 1975. Derivation of new readabil-
ity formulas (automated readability index, fog count
and flesch reading ease formula) for navy enlisted
personnel. Technical report, Naval Technical Train-
ing, U. S. Naval Air Station, Memphis, TN.
D Kokkinakis and M Toporowska Gronostaj. 2006.
Comparing lay and professional language in cardio-
vascular disorders corpora. In Australia Pham T.,
James Cook University, editor, WSEAS Transactions
on BIOLOGY and BIOMEDICINE, pages 429?437.
I Korkontzelos, IP Klapaftis, and S Manandhar. 2008.
Reviewing and evaluating automatic term recogni-
tion techniques. In GoTAL, pages 248?259.
JR Landis and GG Koch. 1977. The measurement of
observer agreement for categorical data. Biometrics,
33:159?174.
G Leroy, S Helmreich, J Cowie, T Miller, and
W Zheng. 2008. Evaluating online health informa-
tion: Beyond readability formulas. In AMIA 2008,
pages 394?8.
Gary Libben, Martha Gibson, Yeo Bom Yoon, and Do-
miniek Sandra. 2003. Compound fracture: The role
of semantic transparency and morphological head-
edness. Brain and Language, 84(1):50?64.
AL Ligozat, C Grouin, A Garcia-Fernandez, and
D Bernhard. 2012. Annlor: A na??ve notation-
system for lexical outputs ranking. In *SEM 2012,
pages 487?492.
A L?udeling, T Schmidt, and S Kiokpasoglou. 2002.
Neoclassical word formation in german. Yearbook
of Morphology, pages 253?283.
D Maynard and S Ananiadou. 2000. Identifying terms
by their family and friends. In Proceedings of COL-
ING 2000, pages 530?536, Saarbrucken, Germany.
A McCray. 2005. Promoting health literacy. J of Am
Med Infor Ass, 12:152?163.
T Miller, G Leroy, S Chatterjee, J Fan, and B Thoms.
2007. A classifier to evaluate language specificity of
medical documents. In HICSS, pages 134?140.
Fiammetta Namer and Pierre Zweigenbaum. 2004.
Acquiring meaning for French medical terminology:
contribution of morphosemantics. In Annual Sym-
posium of the American Medical Informatics Asso-
ciation (AMIA), San-Francisco.
F Namer. 2000. FLEMM : un analyseur flexionnel du
franc?ais `a base de r`egles. Traitement automatique
des langues (TAL), 41(2):523?547.
Oregon Evidence-based Practice Center. 2008. Bar-
riers and drivers of health information technology
use for the elderly, chronically ill, and underserved.
Technical report, Agency for healthcare research and
quality.
V Patel, T Branch, and J Arocha. 2002. Errors in inter-
preting quantities as procedures : The case of phar-
maceutical labels. International journal of medical
informatics, 65(3):193?211.
M Poprat, K Mark?o, and U Hahn. 2006. A lan-
guage classifier that automatically divides medical
documents for experts and health care consumers.
In MIE 2006 - Proceedings of the XX International
Congress of the European Federation for Medical
Informatics, pages 503?508, Maastricht.
JR Quinlan. 1993. C4.5 Programs for Machine Learn-
ing. Morgan Kaufmann, San Mateo, CA.
R Rittman. 2008. Automatic discrimination of genres.
VDM, Saarbrucken, Germany.
R Rudd, B Moeykens, and T Colton, 1999. Annual
Review of Adult Learning and Literacy, page ch 5.
H Schmid. 1994. Probabilistic part-of-speech tagging
using decision trees. In Proceedings of the Inter-
national Conference on New Methods in Language
Processing, pages 44?49, Manchester, UK.
R Sinha. 2012. Unt-simprank: Systems for lexical
simplification ranking. In *SEM 2012, pages 493?
496, Montr?eal, Canada, 7-8 June. Association for
Computational Linguistics.
L Specia, SK Jauhar, and R Mihalcea. 2012. Semeval-
2012 task 1: English lexical simplification. In *SEM
2012, pages 347?355.
TM Tran, H Chekroud, P Thiery, and A Julienne. 2009.
Internet et soins : un tiers invisible dans la relation
m?edecine/patient ? Ethica Clinica, 53:34?43.
Y Wang. 2006. Automatic recognition of text diffi-
culty from consumers health information. In IEEE,
editor, Computer-Based Medical Systems, pages
131?136.
I.H. Witten and E. Frank. 2005. Data mining: Practi-
cal machine learning tools and techniques. Morgan
Kaufmann, San Francisco.
Eugen W?uster. 1981. L?tude scientifique gnrale de la
terminologie, zone frontalire entre la linguistique, la
logique, l?ontologie, l?informatique et les sciences
des choses. In G. Rondeau et H. Felber, editor,
Textes choisis de terminologie, volume I. Fonde-
ments thoriques de la terminologie, pages 55?114.
GISTERM, Universit de Laval, Qubec. sous la di-
rection de V.I. Siforov.
Q Zeng-Treiler, H Kim, S Goryachev, A Keselman,
L Slaugther, and CA Smith. 2007. Text charac-
teristics of clinical reports and their implications for
the readability of personal health records. In MED-
INFO, pages 1117?1121, Brisbane, Australia.
W Zheng, E Milios, and C Watters. 2002. Filtering
for medical news items using a machine learning ap-
proach. In AMIA, pages 949?53.
20
Proceedings of the 4th International Workshop on Computational Terminology, pages 94?103,
Dublin, Ireland, August 23 2014.
Unsupervised method for the acquisition of general language paraphrases
for medical compounds
Natalia Grabar
CNRS UMR 8163 STL
Universit?e Lille 3
59653 Villeneuve d?Ascq, France
natalia.grabar@univ-lille3.fr
Thierry Hamon
LIMSI-CNRS, BP133, Orsay
Universit?e Paris 13
Sorbonne Paris Cit?e, France
hamon@limsi.fr
Abstract
Medical information is widespread in modern society (e.g. scientific research, medical blogs,
clinical documents, TV and radio broadcast, novels). Moreover, everybody?s life may be con-
cerned with medical problems. However, the medical field conveys very specific and often
opaque notions (e.g., myocardial infarction, cholecystectomy, abdominal strangulated hernia,
galactose urine), that are difficult to understand by lay people. We propose an automatic method
based on the morphological analysis of terms and on text mining for finding the paraphrases of
technical terms. Analysis of the results and their evaluation indicate that we can find correct
paraphrases for 343 terms. Depending on the semantics of the terms, error rate of the extractions
ranges between 0 and 59%. This kind of resources is useful for several Natural Language Pro-
cessing applications (i.e., information extraction, text simplification, question and answering).
1 Background
Medical and health information is widespread in the modern society in light of pressing health concerns
and of maintaining of healthy lifestyles. Besides, it is also available through modern media: scientific
research, articles, medical blogs and fora, clinical documents, TV and radio broadcast, novels, discussion
fora, epidemiological alerts, etc. Still, availability of medical and health information does not guaran-
tee its easy and correct understanding by lay people. The medical field conveys indeed very technical
notions, such as in example (1).
(1) myocardial infarction, cholecystectomy, erythredema polyneuropathy, acromegaly, galactosemia
Although technical, these notions are nevertheless important for patients (AMA, 1999; McCray, 2005;
Eysenbach, 2007; Oregon Evidence-based Practice Center, 2008). It has been shown that in several
situations such notions cannot be correctly understood by patients: the steps needed for the medication
preparing and use (Patel et al., 2002); the instructions on drugs from patient package inserts, and the
information delivered in informed consensus and health brochures: it appears that among the 2,600
patients recruited in two hospitals, 26% to 60% cannot manage information available in these sources
(Williams et al., 1995); health information in different languages (English, Spanish, French) provided in
websites created for patients require high reading level (Berland et al., 2001; Hargrave et al., 2003; Kusec,
2004) and remains difficult to manage by patients, which can be negative for the communication between
patients and medical professionals, and the healthcare process (Tran et al., 2009). This situation sets the
context of our work. Our objective is to propose method for the automatic acquisition of paraphrases for
technical medical notions. More particularly, we propose to concentrate on terms and their words that
show neoclassical compounding word formation (Booij, 2010; Iacobini, 1997; Amiot and Dal, 2005),
such as in the example (1). Such words often involve Latin and Greek roots or bases, which makes them
more difficult to understand, as such words must be decomposed first (see examples (2) and (3)). To our
knowledge, this kind of approach has not been applied for the acquisition of laymen paraphrases.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
94
(2) myocardial is formed with Latin myo (muscle) and Greek cardia (heart)
(3) cholecystectomy is formed with Greek chole (bile), Latin cystis (bladder), and Greek ectomy (surgical
removal)
Our work is related to the following research topics:
? Readability. The readability studies the ease in which text can be understood. Two kinds of readabil-
ity measures are distinguished: classical and computational (Franc?ois, 2011). Classical measures
are usually based on number of characters and/or syllables in words, sentences or documents and on
linear regression models (Flesch, 1948; Gunning, 1973; Dubay, 2004). Computational measures,
that are more recent, can involve vectorial models and a great variety of descriptors. These de-
scriptors, usually specific to the texts processed, are for instance: combination of classical measures
with medical terminologies (Kokkinakis and Toporowska Gronostaj, 2006); n-grams of characters
(Poprat et al., 2006); discursive descriptors (Goeuriot et al., 2007); lexicon (Miller et al., 2007);
morphological descriptors (Chmielik and Grabar, 2011); combination of various descriptors (Wang,
2006; Zeng-Treiler et al., 2007; Leroy et al., 2008; Franc?ois and Fairon, 2013).
? Lexical simplification. The lexical simplification helps to make text easier to understand. Lexical
simplification of texts in English has been addressed during the SemEval 2012 challenge
a
. Given
a short input text and a target word in English, and given several English substitutes for the tar-
get word that fit the context, the goal was to rank these substitutes according to how simple they
are (Specia et al., 2012). Several clues have been applied: lexicon extracted from oral corpus and
Wikipedia, Google n-grams, WordNet (Sinha, 2012); word length, number of syllables, mutual in-
formation and frequency of words (Jauhar and Specia, 2012); frequency in Wikipedia, word length,
n-grams of characters and of words, syntactic complexity of documents (Johannsen et al., 2012);
n-grams, frequency in Wikipedia, n-Google grams (Ligozat et al., 2012); WordNet and word fre-
quency (Amoia and Romanelli, 2012).
? Dedicated resources. The building of resources suitable for performing the simplification is an-
other related research topics. Such resources are mainly two-fold lexica in which specialized and
non-specialized vocabularies are aligned (in examples (4) to (6), the technical terms are followed
by their non-technical equivalents). The first initiative of the kind appeared with the collaborative
effort Consumer Health Vocabulary (Zeng and Tse, 2006) (examples in (4)). One of the meth-
ods was applied to the most frequently occurring medical queries aligned to the UMLS (Unified
Medical Language System) concepts (Lindberg et al., 1993). Another work exploited a small cor-
pus and several statistical association measures for building aligned lexicon with technical terms
from the UMLS and their lay equivalents (Elhadad and Sutaria, 2007). Similar work in other lan-
guages followed. In French, researchers proposed methods for the acquisition of syntactic variation
(Del?eger and Zweigenbaum, 2008; Cartoni and Del?eger, 2011) from comparable specialized and
non-specialized corpora, that led to the detection of verb/noun variations (examples in (5)) and a
larger set of syntactic variations (examples in (6)). Besides, research on the acquisition of termi-
nological variation (Hahn et al., 2001), synonymy (Fern?andez-Silva et al., 2011) and paraphrasing
(Max et al., 2012) is also relevant to outline the topics.
(4) {myocardial infarction, heart attack}, {abortion, termination of pregnancy}, {acrodynia, pink
disease}
(5) {consommation r?eguli`ere, consommer de fac?on r?eguli`ere} (regular use), {g?ene `a la lecture,
emp?eche de lire} (reading difficulty), {?evolution de l?affection, la maladie ?evolue} (evolution of the
condition)
(6) {retard de cicatrisation, retarder la cicatrisation} (delay the healing), {apports caloriques, apport
en calories} (calorie supply), {calculer les doses, doses sont calcul?ees} (calculate the dose), {efficacit?e
est renforc?ee, renforcer son efficacit?e} (improve the efficiency)
a
http://www.cs.york.ac.uk/semeval-2012/
95
Our work is closely related to the building of resources dedicated to the lexical simplification. Our
objective is to propose method for paraphrasing the technical medical terms (i.e. medical compounds) in
expressions that are easier to understand by lay people. This aspect is seldom addressed: we can observe
that only some examples in (4) are concerned with the paraphrasing of technical and compound terms
(myocardial infarction, acrodynia). We work with the French data. Contrary to previous work, we do
not use comparable corpora with technical and non-technical texts. Instead, we exploit terms from an
existing medical terminology and corpora built from social media sources. We assume that this kind
of corpora may provide lay people equivalents for technical terms. We also rely on the morphological
analysis of technical terms. The expected result is to obtain pairs like {myocardial, heart muscle} or
{cholecystectomy, removal of gall bladder}. In the following, we start with the presentation of the
resources used (section 2), we present then the steps of the methodology (section 3). We describe and
discuss the obtained results (section 4) and conclude with some directions for future work (section 5).
2 Resources
2.1 Medical terms
The material processed is issued from the French part of the UMLS. It provides syntactically simple terms
that contain one word only (acrodynia), and syntactically complex terms that contain more than one word
(myocardial infarction). Syntactically complex terms are segmented in words. Each term is associated
to semantic types. When a given word receives more than one semantic type, a manual post-processing
allows to disambiguate it: each word is assigned to one semantic type only. Among the semantic types
available, we consider the three most common in the medical practice to which the lay people are the
most exposed: Anatomy (616 words): describe human body anatomy (e.g. abdominopelvic); Disorders
(2,283 words): describe medical problems and their signs (e.g. infarction, diabetes); Procedures (1,271
words): describe procedures which may be performed by medical staff to detect or cure disorders (e.g.
cholecystectomy). In what follows, word and term can be exchangeable and mean either the graphical
unit provided by the segmentation, or the medical notion.
2.2 Corpora
Wiki LesDiab DiabDoct HT Dos
Number of pages/threads 17,525 6,939 387,435 67,652 8,319
Number of articles/messages 17,525 1,438 22,431 12,588 1,124
Number of words 4,326,880 624,571 35,059,868 6,788,361 836,520
Table 1: Size of the corpora exploited.
We use several corpora collected from the social media sources (their sizes are indicated in Table 1):
1. Wiki contains French Wikipedia articles downloaded in February 2014, of which we keep those that
are categorized under the medical category Portail de la m?edecine;
2. LesDiab is collected from the discussion forum Les diab?etiques
b
posted between June and July
2013. It is dedicated to diabetes;
3. DiabDoct is collected in June 2011 from the discussion forum Diab`ete of Doctissimo
c
4. HT is collected in May 2013 from the discussion forum Hypertension of Doctissimo
d
5. Dos is collected in May 2013 from the discussion forum Douleurs de dos (backache) of Doctissimo
e
b
http://www.lesdiabetiques.com/modules.php?name=Forums
c
http://forum.doctissimo.fr/sante/diabete/liste sujet-1.htm
d
http://forum.doctissimo.fr/sante/hypertension-problemes-cardiaques/liste sujet-1.htm
e
http://forum.doctissimo.fr/sante/douleur-dos/liste sujet-1.htm
96
The Wiki corpus contains encyclopaedic information on several medical notions from Wikipedia. Thanks
to the collaborative writing of the articles, these contain mostly correct information about the topics
concerned. Other corpora are collected from the dedicated fora (e.g. diabetes or backache). We assume
that people involved in these discussions may show low, middle or high degree of knowledge about the
disorders and related notions. We expect that all our corpora are written in a simple style and that they
contain paraphrases of technical terms. From Table 1, we can observe that the corpora vary in size.
3 Methodology for the automatic acquisition of paraphrases for medical compounds
The methodology is designed for analyzing the neoclassical medical compounds and for searching their
non-technical paraphrases in corpora. In our approach, the paraphrases may occur alone, such as heart
muscle, without being accompanied by their technical compounds (myocarde). In this case, we need first
to acquire the knowledge needed for their automatic detection. We propose to rely on the morphological
analysis of terms. The method is composed of four main steps: the processing of terms, the processing of
corpora, the extraction of layman paraphrases for technical terms, and the evaluation of the extractions.
3.1 The processing of medical terms
To reach the morphological information on terms we apply three specific processing:
1. Morpho-syntactic tagging and lemmatization of terms. The terms are morpho-syntactically tagged
and lemmatized with TreeTagger for French (Schmid, 1994). The morpho-syntactic tagging is
done in context of the terms. If a given word receives more than one tag, the most frequent one is
kept. At this step, we obtain term lemmas with their part-of-speech tags, such as in example (7).
(7) myocardique/A (myocardial/A), chol?ecystectomie/N (cholecystectomy/N), polyneuropathie/N
(polyneuropathy/N), acrom?egalie/N (acromegaly/N), galactos?emie/N (galactosemia/N)
2. Morphological analysis. The lemmas are then morphologically analyzed with D?eriF (Namer,
2009). This tool performs the analysis of lemmas in order to detect their morphological structure,
to decompose them into their components (bases and affixes), and to semantically analyze their
structure. We give some examples of the morphological analysis in (8).
(8) myocardique/A: [[[myo N*] [carde N*] NOM] ique ADJ]
chol?ecystectomie/N: [[chol?ecysto N*] [ectomie N*] NOM]
polyneuropathie/N: [poly [[neur N*] [pathie N*] NOM] NOM]
acrom?egalie/N: [[acr N*] [m?egal N*] ie NOM]
galactos?emie/N: [[galactose NOM] [?em N*] ie NOM]
The computed bases and affixes are associated with syntactic categories (NOM, ADJ, V). When a
given base is suppletive (does not exist in modern French but was borrowed from Latin or Greek
languages), D?eriF assigns the most probable category (e.g. N* for nouns, A* for adjectives). For
instance, the analysis of myocardique/A indicates that this word contains the suppletive noun bases
myo N* (muscle) and carde N* (heart), and the affix -ique/ADJ. We can observe that some bases can
be decomposed further (e.g. galactose in galact (milk) and ose (sugars), cholecystectomy in chole (bile)
and cystis (bladder)). The words that contain more than one base are considered to be compounds
and are processed in the further steps of the method.
3. Association of morphological components with French words. The bases are ?translated? with words
from modern French. We use for this resource built in previous work (Zweigenbaum and Grabar,
2003; Namer, 2003) (see some examples in (9)).
(9) myocardique/A: myo=muscle (muscle), carde=coeur (heart)
chol?ecystectomie/N: chol?ecysto=v?esicule biliaire (gall bladder), ectomie=ablation (removal)
97
polyneuropathie/N: poly=nombreux (several), neuro=nerf (nerve), pathie=maladie (disorder)
acrom?egalie/N: acr=extr?emit?e (extremity), m?egal=grandeur (size)
galactos?emie/N: galactose=galactose (galactose), ?em=sang (blood)
Some words can remain technical (e.g., galactose, v?esicule biliaire), while other components totally
lose their technical meaning (e.g. m?egal=grandeur (size), poly=nombreux (several)).
3.2 The processing of corpora
The corpora are first segmented in words and sentences. Then, we also perform morpho-syntactic tagging
and lemmatization with TreeTagger for French.
3.3 The extraction of layman paraphrases corresponding to technical terms
French words corresponding to the morphological decomposition of terms (examples in (9)) are projected
on corpora in order to extract sentences and their segments which can provide the layman paraphrases for
the corresponding technical terms. Sentences that contain the translated French words are extracted as
candidates for proposing the paraphrases. Additionally, the segments delimited by these words are also
extracted. We consider the co-occurrence of the words issued from the morphological decomposition in
a sliding graphical window of n words. In the experiments presented, the window size n is fixed to 10
words. Smaller or larger windows show less performance.
(10) Les causes de tachycardie ventriculaire sont superposables `a celles des extrasystoles ventric-
ulaires: infarctus du myocarde, insuffisance cardiaque, hypertrophie du muscle du coeur et
prolapsus de la valve mitrale.
The sentence in (10) contains words muscle and coeur, underlined in the example, that correspond to
the morphological components of myocardique (see examples in (9)). For this reason, this sentence is
extracted, as well as the segment delimited by these two words muscle du coeur (heart muscle).
3.4 The evaluation
The objective of the evaluation is to assess whether the proposed method is valid for the acquisition
of paraphrases for technical medical terms. The obtained results are evaluated manually by a com-
puter scientist with no training in biomedicine, but with background in computational linguistics and
morphology. We analyze the candidates for paraphrases from several points of view: Are the French
words corresponding to the components extracted correctly? Do these French words provide valid can-
didates for paraphrases? How easy are these paraphrases to be understood by laymen or by non-experts
in medicine? During the evaluation related to the second point (Do these French words provide valid
candidates for paraphrases?), we distinguish four situations:
1. the extraction is correct: e.g. myocardique paraphrased in muscle du coeur (heart muscle);
2. the extraction suffers from the incorrect morphological decomposition or from the wrong ?trans-
lation? in French: e.g. p?erianal is ?translated? in autour (around) and an (meaning year as it is). The
?translation? of this last word an is not correct and should be anus (anus) instead. Because of the
wrong ?translation?, we collect a lot of incorrect segments like autour de 30 ans (around 30 years);
3. the extraction should be post-processed but contains the correct paraphrase: e.g. spondylarthrose,
?translated? in vert`ebre (vertebra) and arthrose (arthrosis), is paraphrased in arthrose que l?on ne voyait
pas sur la vert`ebre (arthrosis that was not seen on the vertebra), while the correct paraphrase from this
segment should be arthrose sur la vert`ebre (arthrosis on the vertebra);
4. the extraction is wrong and can provide no useful information.
This evaluation allows to estimate precision of the results in three versions: strong precision P
strong
(only
the correct extractions are considered (extractions from 1)); weak precision P
weak
(correct extractions
and extractions that need post-processing are considered (extractions from 1 and 3)); rate of incorrect
extractions %
incorrect
(the percentage of the incorrect extractions is computed (extractions from 4)).
98
4 Results and Discussion
4.1 The morphological analysis of terms
We generate the morphological analysis for 218 single words from the anatomy semantic type, 1,789 dis-
order words and 1,023 procedure words: over 70% of words are morphologically analyzed. Among these
words, we observe compounds (myocardique) and words formed with affixes (e.g. r?eadaptation derived
from adaptation, derived in its turn from adapter). The remaining words may be simple (e.g. abc`es
(abscess), l`epre (leprosy), cicatrice (scar)) or contain bases and affixes that are not managed by D
?erif (e.g.
pneumostrongylose (pneumostrongylosis), lagophtalmie (lagophthalmos), n?ecatorose (necatorosis)). Among the
generated decompositions by D?erif, we can find some cases with ambiguous decomposition that occur
when medical terms can be decomposed in several possible ways, among which only one is semantically
correct. For instance, posturographie (posturography) is decomposed into: [post [[uro N*] [graphie N*]
NOM] NOM], which may be glossed as control during the period which follows the therapy done on
the urinary system. From the formal point of view, such decomposition is very possible, although it is
weak semantically. For the term posturographie, the right decomposition is: [[posturo N*] [graphie
N*] NOM], which is related to the definition of the optimal body position when walking or sitting. As
indicated above, some terms (e.g. p?erianal) can be incorrectly ?translated? in French.
4.2 The preprocessing of corpora
Our main difficulty at this step is related to the processing of forum messages and to their segmentation
into sentences. In addition to possible and frequent spelling and grammatical errors, forum messages
have also a very specific punctuation, which may be missing or convey personal feelings and emotions.
This seriously impedes the possibility to provide the correct segmentation in sentences, and means that,
because of the missing punctuation, the mapping of decomposed terms with corpora may be done with
bigger text segments in which the semantic relations between the mapped components may be weak or
non-existent, and provide incorrect extractions. We plan to combine the current method with the syntactic
analysis in order to ensure that stronger syntactic and semantic relations exist between the components.
4.3 The extraction of paraphrases and their evaluation
We present the results on extraction of sentences and paraphrases from the corpora processed. In Table
2, for the three semantic types of terms (anatomy ana., disorders dis., and procedures pro.) from each
corpus, we indicate the following information: the number of different sentences extracted (sentences),
yje number of different terms (uniq. terms), the number of correct paraphrases (correct), the number
of paraphrases that are possibly correct (pos. correct), the number of paraphrases which morphological
analysis and ?translation? should be improved (morph. ana.), and the number of incorrect paraphrases
(incorrect). The last three lines indicate the precision values: strong precision (P
strong
), weak precision
(P
weak
) and incorrect extractions (%
incorrect
).
Number of Wiki LesDiab DiabDoct HT Dos
ana dis pro ana dis pro ana dis pro ana dis pro ana dis pro
sentences 1238 4003 999 15 71 10 721 2901 564 246 1233 678 42 708 30
uniq. terms 93 382 154 7 30 5 35 204 48 29 133 42 13 44 13
correct 469 1571 364 3 32 4 227 1189 67 114 637 38 12 466 13
pos. correct 270 868 93 3 7 - 40 332 5 10 85 9 3 98 2
morph. ana. 41 155 323 1 2 6 100 3 394 22 - 591 2 1 12
incorrect 462 1424 220 8 30 - 354 1 98 100 511 40 25 135 3
P
strong
38 39 36 20 45 40 32 40 12 46 52 6 29 66 43
P
weak
60 61 46 40 55 40 37 52 13 50 59 7 36 80 50
%
incorrect
40 39 54 53 42 0 49 47 17 41 41 41 59 20 10
Table 2: Results on the paraphrases extracted and evaluated.
99
From the data presented in Table 2, we can propose several observations: (1) the Wiki corpus, that is
not the largest in our dataset, provides the largest number of extractions (sentences and unique terms);
(2) among the three semantic types (anatomy, disorders and procedures), the number of paraphrases ex-
tracted for disorders is the largest in all corpora; (3) the largest set of paraphrases, that suffer from the
incorrect morphological decomposition or ?translation?, is obtained for the procedure terms. According
to these observations, P
strong
ranges between 20 to 46% for anatomy, 39 and 66% for disorders, and 6 to
43 for procedures. The P
weak
values, that takes into account the paraphrases that need post-processing,
show the increase by 0 to 28% by comparison with the P
strong
values. The %
incorrect
values indicate
that anatomy terms show the largest rate (40 to 59%) of incorrect paraphrases: it is possible that the
anatomy terms present the lowest rate of compositionality. The incorrect paraphrases are between 20
and 47 among the disorder terms, and between 0 to 54 among the procedure terms. The syntactic anal-
ysis may help to improve the current results. On the whole, the proposed method allows to extract the
paraphrases for 722 different terms from the corpora processed. Within the evaluated set of extractions,
these paraphrases are correct for 273 terms; while 343 terms are provided with correct paraphrases and
paraphrases that need to be post-processed. Most of the extracted paraphrases are noun phrases, and, at
a lesser extent, verb phrases. We present some examples of the correct paraphrases extracted:
- dorsalgie (dorsalgia): douleur dans le dos (pain in the back)
- my?elocyte (myelocyte): cellules dans la moelle osseuse (cells of the bone marrow)
- lombalgie (lombalgia): douleurs dans les reins (pain in kidney)
- gastralgie (gastralgia): douleurs `a l?estomac (stomach pain)
- desmorrhexie (desmorrhexia): rupture des ligaments (ligamentous rupture)
- h?epatite (hepatitis): inflammation du foie (liver inflammation)
We can find several types of paraphrases that suffer from incorrect decomposition or ?translation?:
? syringomy?elie (syringomyelia) is currently ?translated? in moelle (marrow or spinal cord) and canal (canal).
This term means a disorder in which a cyst or cavity forms within the spinal cord. We assume that
a more correct ?translation? of this term should be: moelle (marrow or spinal cord) and cavit?e (cavity);
? sous-dural is ?translated? in sous (sub) and dur (hard). The term is related to specific space in brain
that can be opened by the separation of the arachnoid mater from the dura mater. Concerning its
?translation?, we assume that dure-m`ere (dura mater) should be used instead of dur (hard). Besides,
the names of anatomical locations often remains difficult to understand. We assume that even when
terms are decomposed and ?translated? correctly, the paraphrases for such terms may be not suitable
for laymen: other types of explanations (e.g. schemes or pictures) should be used instead;
? hyper?emie (hyperaemia) is ?translated? in hyper and sang (blood). The term means the increase of
blood flow to different tissues in the body. This term is not fully compositional because the notion
of tissues is absent, while necessary for its understanding. The proposed extractions for this term
mainly come from corpora related to diabetes, in which hyper and hypo are often used in relation
with the hyperglycemia or hypoglycemia. This means that hyper should be ?translated? with other
words, such as increase or elevated;
? h?et?erotopie is translated in autre (another) and endroit (place). The term means the displacement of an
organ from its normal position and that [an organ] is found in another place than the one expected.
This term brings no correct candidates for paraphrases because: it is not fully compositional and its
?translation? provides very common words widely used in the corpora.
Among the incorrect extractions we can find: (1) more terms with non-compositional semantics (such
as ost?eodermie (osteoderm), causalgie (causalgia), ad?eno??de (adenoid), or xanthochromie (xanthochromia)) for
which the extracted paraphrases capture only part of the meaning; and (2) extractions that must be con-
troled by the syntactic analysis (e.g. petite boule de peau qui a sortie entre l?ongle et... (small skinball
that appeared between the nail and...) for micronychie (micronychia)) to make them more grammatical. Para-
phrases extracted from the Wiki corpus cover larger range of medical terms, while those extracted from
100
fora dedicated to a given medical topics are redundant. On the whole, we can consider that the currently
proposed method allows extracting interesting candidates as the paraphrases of technical terms, that are
indeed much easier to understand than the technical terms by themselves.
If we compare the obtained results with those presented in previous work, we can observe that:
? we extract paraphrases for larger number of terms: 343 terms with correct and possibly correct
paraphrases (722 terms with paraphrases in total) in our work against a total of 65 and 82 in (Del?eger
and Zweigenbaum, 2008), 109 in (Cartoni and Del?eger, 2011), and 152 in (Elhadad and Sutaria,
2007). In our work, the terms may receive more than one paraphrase;
? the precision values we obtain are comparable with those indicated in previous work: 67% and 60%
in (Del?eger and Zweigenbaum, 2008), 66% in (Cartoni and Del?eger, 2011), and 58% in (Elhadad
and Sutaria, 2007);
? in the cited work, the content of the corpora is explored but no reference is done to the set of terms
expected to be found. Because we work with a termset, we can compute the recall. If we consider the
terms that can be analyzed morphologically (3,030 terms), and for which we can find the paraphrases
with the proposed method, the recall value is close to 10% with the correct paraphrases (299 terms),
and to 24% with all the paraphrases extracted (722 terms). Yet, it is not sure that all of the terms, that
have been analyzed morphologically, can be provided with paraphrases in the corpora processed.
Besides, we should not forget that the nature of compounds and the decomposition of terms into com-
ponents also mean that specific semantic relations exist between these components (Namer and Zweigen-
baum, 2004; Booij, 2010). These are inherent to the syntactic constructions extracted. The characteristics
of these relations will be described and modeled in future work.
5 Conclusions and Future work
We propose to exploit social media texts in order to detect paraphrases for technical medical terms,
concentrating particularly on neoclassical compounds (e.g., myocardial, cholecystectomy, galactose,
acromegaly). The work is done in French. The method relies on the morphological analysis of terms,
on the ?translation? of the components of terms in modern French words (e.g. {card, heart}), and on
the projection of these words on corpora. The method allows extracting correct and possibly correct
paraphrases for up to 343 technical terms. For covering larger set of terms, additional corpora must be
treated. The extracted paraphrases are easier to understand than the original technical terms. Moreover,
the semantic relations among the components, although non explicated, are conveyed by the paraphrases.
We can consider that the method proves to be efficient and promising for the creation of lexicon suit-
able for the simplification of medical texts. Besides, the purpose of the method is to cover neoclassical
compound terms that are usually non treated with automatic approaches, as they do not present clear
formal similarity with their paraphrases. One of the difficulties we have currently is related to the lack of
constrains on the extracted segments. In future work, we plan to apply the syntactic analysis for parsing
the extracted sentences. Another possibility is to compute the probability for a given paraphrase to be
correct, which can rely for instance on frequency of the extracted paraphrases, on their syntactic struc-
ture, etc. In order to make the extraction of paraphrases more exhaustive, we will apply the method to
other corpora and we will use additional resources (synonyms, associative resources) for performing the
approximate mapping of paraphrases. In future work, we will take into account syntactically complex
terms and not only simple words. The very objective of our work is to exploit and test the resource
created for the simplification of medical texts.
Acknowledgments
The authors acknowledge the support of the Universit?e Paris 13 (project BQR Bonus Quality Research,
2011), the support of the MESHS Lille projet
?
Emergent CoMeTe, and the support of the French Agence
Nationale de la Recherche (ANR) and the DGA, under the Tecsan grant ANR-11-TECS-012.
101
References
AMA. 1999. Health literacy: report of the council on scientific affairs. Ad hoc committee on health literacy for
the council on scientific affairs, American Medical Association. JAMA, 281(6):552?7.
D Amiot and G Dal. 2005. Integrating combining forms into a lexeme-based morphology. In Mediterranean
Morphology Meeting (MMM5), pages 323?336.
M Amoia and M Romanelli. 2012. Sb: mmsystem - using decompositional semantics for lexical simplification.
In *SEM 2012, pages 482?486, Montr?eal, Canada, 7-8 June. Association for Computational Linguistics.
GK Berland, MN Elliott, LS Morales, JI Algazy, RL Kravitz, MS Broder, DE Kanouse, JA Munoz, JA Puyol,
M Lara, KE Watkins, H Yang, and EA McGlynn. 2001. Health information on the internet. accessibility,
quality, and readability in english ans spanish. JAMA, 285(20):2612?2621.
Geert Booij. 2010. Construction Morphology. Oxford University Press, Oxford.
B Cartoni and L Del?eger. 2011. Dcouverte de patrons paraphrastiques en corpus comparable: une approche base
sur les n-grammes. In TALN.
J Chmielik and N Grabar. 2011. D?etection de la sp?ecialisation scientifique et technique des documents
biom?edicaux gr?ace aux informations morphologiques. TAL, 51(2):151?179.
L Del?eger and P Zweigenbaum. 2008. Paraphrase acquisition from comparable medical corpora of specialized
and lay texts. In AMIA 2008, pages 146?50.
William H. Dubay. 2004. The principles of readability. Impact Information. Available at
http://almacenplantillasweb.es/wp-content/uploads/2009/11/The-Principles-of-Readability.pdf.
N Elhadad and K Sutaria. 2007. Mining a lexicon of technical terms and lay equivalents. In BioNLP, pages 49?56.
Gunther Eysenbach. 2007. Poverty, human development, and the role of ehealth. J Med Internet Res, 9(4):e34.
S Fern?andez-Silva, J Freixa, and MT Cabr?e. 2011. A proposed method for analysing the dynamics of cognition
through term variation. Terminology, 17(1):49?73.
R Flesch. 1948. A new readability yardstick. Journal of Applied Psychology, 23:221?233.
T Franc?ois and C Fairon. 2013. Les apports du TAL `a la lisibilit?e du franc?ais langue ?etrang`ere. TAL, 54(1):171?
202.
T Franc?ois. 2011. Les apports du traitements automatique du langage la lisibilit du franais langue trangre. Phd
thesis, Universit Catholique de Louvain, Louvain.
L Goeuriot, N Grabar, and B Daille. 2007. Caract?erisation des discours scientifique et vulgaris?e en franc?ais,
japonais et russe. In TALN, pages 93?102.
R Gunning. 1973. The art of clear writing. McGraw Hill, New York, NY.
Udo Hahn, Martin Honeck, Michael Piotrowsky, and Stefan Schulz. 2001. Subword segmentation - leveling out
morphological variations for medical document retrieval. In AMIA, 229-33.
Darren Hargrave, Ute Bartels, Loretta Lau, Carlos Esquembre, and
?
Eric Bouffet. 2003. ?evaluation de la qualit?e
de l?information m?edicale francophone accessible au public sur internet : application aux tumeurs c?er?ebrales de
l?enfant. Bulletin du Cancer, 90(7):650?5.
C Iacobini. 1997. Distinguishing derivational prefixes from initial combining forms. In First mediterranean
conference of morphology, Mytilene, Island of Lesbos, Greece, septembre.
SK Jauhar and L Specia. 2012. Uow-shef: Simplex ? lexical simplicity ranking based on contextual and psycholin-
guistic features. In *SEM 2012, pages 477?481, Montr?eal, Canada, 7-8 June. Association for Computational
Linguistics.
A Johannsen, H Mart??nez, S Klerke, and A S?gaard. 2012. Emnlp@cph: Is frequency all there is to simplicity?
In *SEM 2012, pages 408?412, Montr?eal, Canada, 7-8 June. Association for Computational Linguistics.
D Kokkinakis and M Toporowska Gronostaj. 2006. Comparing lay and professional language in cardiovascular
disorders corpora. In Australia Pham T., James Cook University, editor, WSEAS Transactions on BIOLOGY
and BIOMEDICINE, pages 429?437.
102
Sanja Kusec. 2004. Les sites web relatifs au diab`ete, sont-ils lisibles ? Dib`ete et soci?et?e, 49(3):46?48.
G Leroy, S Helmreich, J Cowie, T Miller, and W Zheng. 2008. Evaluating online health information: Beyond
readability formulas. In AMIA 2008, pages 394?8.
AL Ligozat, C Grouin, A Garcia-Fernandez, and D Bernhard. 2012. Annlor: A na??ve notation-system for lexical
outputs ranking. In *SEM 2012, pages 487?492.
DA Lindberg, BL Humphreys, and AT McCray. 1993. The unified medical language system. Methods Inf Med,
32(4):281?291.
Aur?elien Max, Houda Bouamor, and Anne Vilnat. 2012. Generalizing sub-sentential paraphrase acquisition across
original signal type of text pairs. In EMNLP, pages 721?31.
A McCray. 2005. Promoting health literacy. J of Am Med Infor Ass, 12:152?163.
T Miller, G Leroy, S Chatterjee, J Fan, and B Thoms. 2007. A classifier to evaluate language specificity of medical
documents. In HICSS, pages 134?140.
Fiammetta Namer and Pierre Zweigenbaum. 2004. Acquiring meaning for French medical terminology: contri-
bution of morphosemantics. In Annual Symposium of the American Medical Informatics Association (AMIA),
San-Francisco.
F Namer. 2003. Automatiser l?analyse morpho-s?emantique non affixale: le syst`eme D?eriF. Cahiers de Gram-
maire, 28:31?48.
F Namer. 2009. Morphologie, Lexique et TAL : l?analyseur D?eriF. TIC et Sciences cognitives. Hermes Sciences
Publishing, London.
Oregon Evidence-based Practice Center. 2008. Barriers and drivers of health information technology use for the
elderly, chronically ill, and underserved. Technical report, Agency for healthcare research and quality.
V Patel, T Branch, and J Arocha. 2002. Errors in interpreting quantities as procedures : The case of pharmaceutical
labels. International journal of medical informatics, 65(3):193?211.
M Poprat, K Mark?o, and U Hahn. 2006. A language classifier that automatically divides medical documents
for experts and health care consumers. In MIE 2006 - Proceedings of the XX International Congress of the
European Federation for Medical Informatics, pages 503?508, Maastricht.
H Schmid. 1994. Probabilistic part-of-speech tagging using decision trees. In ICNMLP, pages 44?49, Manchester,
UK.
R Sinha. 2012. Unt-simprank: Systems for lexical simplification ranking. In *SEM 2012, pages 493?496,
Montr?eal, Canada, 7-8 June. Association for Computational Linguistics.
L Specia, SK Jauhar, and R Mihalcea. 2012. Semeval-2012 task 1: English lexical simplification. In *SEM 2012,
pages 347?355.
TM Tran, H Chekroud, P Thiery, and A Julienne. 2009. Internet et soins : un tiers invisible dans la relation
m?edecine/patient ? Ethica Clinica, 53:34?43.
Y Wang. 2006. Automatic recognition of text difficulty from consumers health information. In IEEE, editor,
Computer-Based Medical Systems, pages 131?136.
MV Williams, RM Parker, DW Baker, NS Parikh, K Pitkin, WC Coates, and JR Nurss. 1995. Inadequate func-
tional health literacy among patients at two public hospitals. JAMA, 274(21):1677?82.
QT Zeng and T Tse. 2006. Exploring and developing consumer health vocabularies. JAMIA, 13:24?29.
Q Zeng-Treiler, H Kim, S Goryachev, A Keselman, L Slaugther, and CA Smith. 2007. Text characteristics of
clinical reports and their implications for the readability of personal health records. In MEDINFO, pages 1117?
1121, Brisbane, Australia.
Pierre Zweigenbaum and Natalia Grabar. 2003. Corpus-based associations provide additional morphological
variants to medical terminologies. In AMIA.
103
Proceedings of the 4th International Workshop on Computational Terminology, pages 114?124,
Dublin, Ireland, August 23 2014.
Towards Automatic Distinction between Specialized and Non-Specialized
Occurrences of Verbs in Medical Corpora
Ornella Wandji Tchami, Natalia Grabar
CNRS UMR 8163 STL
Universit?e Lille 3
59653 Villeneuve d?Ascq, France
ornwandji@yahoo.fr, natalia.grabar@univ-lille3.fr
Abstract
The medical field gathers people of different social statuses, such as students, pharmacists, man-
agers, biologists, nurses and mainly medical doctors and patients, who represent the main actors.
Despite their different levels of expertise, these actors need to interact and understand each other
but the communication is not always easy and effective. This paper describes a method for a con-
trastive automatic analysis of verbs in medical corpora, based on the semantic annotation of the
verbs nominal co-occurents. The corpora used are specialized in cardiology and distinguished
according to their levels of expertise (high and low). The semantic annotation of these corpora is
performed by using an existing medical terminology. The results indicate that the same verbs oc-
curring in the two corpora show different specialization levels, which are indicated by the words
(nouns and adjectives derived from medical terms) they occur with.
1 Introduction
The medical field gathers people of different social statuses, such as medical doctors, students, pharma-
cists, managers, biologists, nurses, imaging experts and of course patients. These actors have different
levels of expertise ranging from low (typically, the patients) up to high (e.g., medical doctors, pharma-
cists, medical students). Despite their different levels of expertise, these actors need to interact. But their
mutual understanding might not always be completely successful. This situation specifically applies to
patients and medical doctors who are the two main actors within the medical field (McCray, 2005; Zeng-
Treiler et al., 2007). Beyond the medical field, this situation can also apply to other domains (e.g., law,
economics, biology). The research question is closely linked to the readability studies (Dubay, 2004),
whose purpose is to address the ease with which a document can be read and understood by people, and
also the ease with which the corresponding information can be exploited by the people later. As noticed,
one source of difficulty may be due to the specific and specialized notions that are used : for instance,
abdominoplasty, hymenorrhaphy, escharotomy in medical documents, affidavit, allegation, adjudication
in legal documents, etc. This difficulty occurs at the lexical and conceptual level. Another difficulty
may come from complex syntactic structures (e.g., coordinated or subordinated phrases) that can occur
in such documents. Hence, this difficulty is of syntactic nature. With very simple features, reduced to the
length of words and sentences, the classical readability scores address these two aspects (Flesch, 1948;
Dale and Chall, 1948; Bormuth, 1966; Kincaid et al., 1975). Typically, such scores do not account for the
semantics of the documents. In recent readability approaches, the semantics is being taken into account
through several features, such as: medical terminologies (Kokkinakis and Toporowska Gronostaj, 2006);
stylistics of documents (Grabar et al., 2007; Goeuriot et al., 2007); lexicon used (Miller et al., 2007);
morphological information (Chmielik and Grabar, 2011); and combination of various features (Wang,
2006; Zeng-Treiler et al., 2007; Leroy et al., 2008; Franc?ois and Fairon, 2013).
We propose to continue studying the readability level of specialized documents through the semantic
features. More precisely, we propose to perform a comparative analysis of verbs observed in medical
corpora written in French. These corpora are differentiated according to their levels of expertise and
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
114
thereby they represent the patients and the medical doctors? languages. Our study focuses on verbs and
their co-occurents (nouns and adjectives deriving from medical terms), and aims to investigate on the
verb semantics, according to the types of constructions and to the words with which the verb occurs in
the corpora. In order to achieve this, we pay a particular attention to the syntactic and semantic features
of the verbs? co-occurents in the studied texts.
Our method is based on the hypothesis according to which the meaning of a verb can be influenced
or determined by its context of appearance (L?Homme, 2012) and by its arguments. Indeed, various
studies on specialized languages have shown that the verb is not specialized by itself (L?Homme, 1998;
Lerat, 2002). Rather, being a predicative unit that involves participants called arguments, the verb can be
specialized or not, depending on its argumental structure and the nature of these arguments.
In our study, the description of verbs is similar to the one performed in Frame Semantics (FS) (Fill-
more, 1982), since we provide semantic information about the verbs co-occurents. The Frame Semantics
framework is increasingly used for the description of lexical units in different languages (Atkins et al.,
2003; Pad?o and Pitel, 2007; Burchardt et al., 2009; Borin et al., 2010; Koeva, 2010) and specialized fields
(Dolbey et al., 2006; Schmidt, 2009; Pimentel, 2011). Among other things, Frame Semantics provides
for a full description of the semantic and syntactic properties of lexical units. FS puts forward the notion
of ?frames?, which are defined as conceptual scenarios that underlie lexical realizations in language. A
frame comprises a frame evoking lexical units (ULs) and the Frame Elements (FEs), which represent the
participants to the verbal process. For instance, in FrameNet (Ruppenhofer et al., 2006), the frame CURE
is described as a situation that involves some specific Frame Elements, (such as HEALER, AFFLICTION,
PATIENT, TREATMENT), and includes a lexical unit such as cure, alleviate, heal, incurable, treat.
1
In our
approach, an FS-like modeling should allow us to describe the semantic properties of verbs. Using this
framework, we will be able to highlight the differences between the studied verbs usages through their
various frames and, by doing so, uncover the linguistic differences observed in corpora of different levels
of expertise. However, the FS framework will be adapted in order to fit our own objectives. Indeed, the
automatic annotation of the verbs co-occurents into frames will rely on the use of a terminology (C?ot?e,
1996) which provides a semantic category for each recorded term. These categories (e.g., anatomy, dis-
orders, procedures, chemical products) typically apply to the verb co-occurents and should be evocative
of the semantics of these co-occurents and the semantic properties of verbs: we consider that the se-
mantic categories represent the frame elements which are lexically realized by the terms, while the verbs
represent the frame evoking lexical units.
In a previous study, we have looked at the behavior of four verbs (observer (observe), d?etecter (detect),
d?evelopper (develop), and activer (activate)) in medical corpora written by medical doctors by contrast to
texts written by patients (Wandji Tchami et al., 2013). The results showed that in the corpus written by
doctors some verbs tend to have specific meanings, according to the type of arguments that surround
them. In the current work, we try to go further by enhancing our method (improved semantic annotation,
automated analysis of verbs) and by distinguishing specialized and non-specialized occurrences of verbs.
In the next sections, we present the material used (section 2), the method designed (section 3). We
then introduce the results and discuss them (section 4), and conclude with future work (section 5).
2 Material
We use several kinds of material: the corpora to be processed (section 2.1), the semantic resources
(section 2.2), a resource with verbal forms and lemmas (section 2.3) and a list of stopwords (section 2.4).
2.1 Corpora
We study two medical corpora dealing with the specific field of cardiology (heart disorders and treat-
ments). These corpora are distinguished according to their levels of expertise and their discursive speci-
ficities (Pearson, 1998): Expert corpus contains expert documents written by medical experts for medical
experts. This corpus typically contains scientific publications, and show a high level of expertise. The
1
https://framenet.icsi.berkeley.edu/fndrupal
115
corpus is collected through the CISMeF portal
2
, which indexes French language medical documents and
assigns them categories according to the topic they deal with (e.g., cardiology, intensive care) and to their
levels of expertise (i.e., for medical experts, medical students or patients). Forum corpus contains non-
expert documents written by patients for patients. This corpus contains messages from the Doctissimo
forum Hypertension Problemes Cardiaques
3
. It shows low level of expertise, although technical terms
may also be used. The size of corpora in terms of occurrences of words is indicated in Table 1. We can
see that, in number of occurrences, these two corpora are comparable as for their sizes.
Corpus Size (occ of words)
Expert 1,285,665
Forum 1,588,697
Table 1: Size of the two corpora studied.
2.2 Semantic resources
The semantic annotation of corpora is performed using the Snomed International terminology (C?ot?e,
1996). This resource provides terms which use is suitable for the NLP processing of documents, as these
are expressions close to those used in real documents. It is structured into several semantic axes:
T : TOPOGRAPHY or ANATOMICAL LOCATIONS (e.g., coeur (heart), cardiaque (cardiac), digestif (diges-
tive), vaisseau (vessel));
S: SOCIAL STATUS (e.g., mari (husband), soeur (sister), m`ere (mother), ancien fumeur (former smoker),
donneur (donnor));
P: PROCEDURES (e.g., c?esarienne (caesarean), transducteur `a ultrasons (ultrasound transducer), t?el?e-
expertise (tele-expertise));
L: LIVING ORGANISMS, such as bacteries and viruses (e.g., Bacillus, Enterobacter, Klebsiella,
Salmonella), but also human subjects (e.g., patients (patients), traumatis?es (wounded), tu (you));
J : PROFESSIONAL OCCUPATIONS (e.g., ?equipe de SAMU (ambulance team), anesth?esiste (anesthesiologist),
assureur (insurer), magasinier (storekeeper));
F : FUNCTIONS of the organism (e.g., pression art?erielle (arterial pressure), m?etabolique (metabolic),
prot?einurie (proteinuria), d?etresse (distress), insuffisance (deficiency));
D: DISORDERS and pathologies (e.g., ob?esit?e (obesity), hypertension art?erielle (arterial hypertension), can-
cer (cancer), maladie (disease));
C: CHEMICAL PRODUCTS (e.g., m?edicament (medication), sodium, h?eparine (heparin), bleu de m?ethyl`ene
(methylene blue));
A: PHYSICAL AGENTS (e.g., proth`eses (prosthesis), tube (tube), accident (accident), cath?eter (catheter)).
Further to our previous work (Wandji Tchami et al., 2013), we have added another semantic axis E STUD-
IES, that groups terms related to the scientific work and experiments (e.g., m?ethode (method), hypoth`ese
(hypothesis)...). Such notions are quite frequent in the corpora, while they are missing in the terminology
used. The only semantic category of Snomed that we ignore in this analysis contains modifiers (e.g.,
aigu (acute), droit (right), ant?erieur (anterior)), which are meaningful only in combination with other terms.
Besides, such descriptors can occur within medical and non-medical contexts.
As stated above, we expect these semantic categories to be indicative of frame elements (FEs), while
the individual terms should correspond to lexical realizations of those FEs, as in Framenet. For instance,
2
http://www.cismef.org/
3
http://forum.doctissimo.fr/sante/hypertension-problemes-cardiaques/liste sujet-1.htm
116
the Snomed category DISORDERS should allow us to discover and group under a single label terms that
denote the same notion (e.g., hypertension (hypertension), ob?esit?e (obesity)) related to the FE DISORDER.
The existing terminologies may not provide the entire coverage of the domain notions (Chute et al.,
1996; Humphreys et al., 1997; Hole and Srinivasan, 2000; Penz et al., 2004). For this reason, we
attempted to complete the coverage of the Snomed International terminology in relation with the corpora
used. We addressed this question in two ways:
? We computed the plural forms for simple terms that contain one word only. The motivation for this
processing is that the terminologies often record terms in singular forms, while the documents may
contain singular and plural forms of these terms.
? We tried to detect the misspellings of the terms using the string edit distance (Levenshtein, 1966).
This measure considers three operations: deletion, addition and substitution of characters. Each
operations cost is set to 1. For instance, the Levenshtein distance between ambolie and embolie is
1, that corresponds to the substitution of a by e. The minimal length of the processed words should
not be lesser than six characters, because with shorter words the propositions contain too much of
errors. The motivation for this kind of processing is that it is possible and frequent to find misspelled
words in real documents, especially in the forum discussions (Balahur, 2013).
In both cases, the computed forms inherit the semantic type of the terms from the terminology. For
instance, ambolie inherits the D DISORDER semantic type of embolie. Besides, we also added the
medication names from the Th?eriaque resource
4
. These are assigned to the C CHEMICAL PRODUCTS
semantic type. The whole resource contains 158,298 entries.
2.3 Resource with verbal forms
We have built a resource with inflected forms of verbs: 177,468 forms for 1,964 verbs. The resource
is built from the information available online
5
. The resource contains simple (consulte, consultes, con-
sultons (consult)) and complex (ai consult?e, avons consult?e (have consulted)) verbal forms. This resource is
required for the lemmatization of verbs (section 3.3).
2.4 List of stopwords
The list of stopwords contains grammatical units, such as prepositions, determinants, pronouns and con-
junctions. It provides 263 entries.
3 Method
We first perform the description of verbs in a way similar to FS and then compare the observations
made in the two corpora processed. The proposed method comprises three steps: corpora pre-processing
(section 3.1), semantic annotation (section 3.2), and contrastive analysis of verbs (section 3.3). The
method relies on some existing tools and on specifically designed Perl scripts.
3.1 Corpora pre-processing
The corpora are collected online from the websites indicated above and properly formatted. The corpora
are then analyzed syntactically using the Bonsai parser (Candito et al., 2010). Its output contains sen-
tences segmented into syntactic chunks (e.g., NP, PP, VP) in which words are assigned parts of speech,
as shown in the example that follows:
Le traitement repose sur les d?eriv?es thiazidiques, plus accessibles, disponibles sous forme de
m?edicaments g?en?eriques.
(The treatment is based on thiazidic derivates, more easily accessible, and available as generic drugs.)
((SENT (NP (DET Le) (NC traitement)) (VN (V repose)) (PP (P sur) (NP (DET les) (NC
4
http://www.theriaque.org/
5
http://leconjugueur.lefigaro.fr/frlistedeverbe.php
117
d?eriv?es) (AP (ADJ thiazidiques) (COORD (PONCT ,) (NP (DET les) (ADV plus) (ADJ acces-
sibles)) (PONCT ,) (AP (ADJ disponibles)))) (PP (P sous forme de) (NP (NC m?edicaments)
(AP (ADJ g?en?eriques)))))))))
The syntactic parsing was performed in order to identify the syntactic chunks, nominal and verbal, to
prepare the recognition and annotation of the terms they contain and to better the recognition of verbs.
The Bonsai parser was chosen: it is adapted for french texts and it provides several hierarchical syntactic
levels within the sentences and phrases. For instance, the phrase m?edicaments g?en?eriques (generic drugs)
is syntactically analyzed as NP: (NP (NC m?edicaments) (AP (ADJ g?en?eriques)))) that contains one NP
m?edicaments and two APs g?en?eriques and the final dot. The VP of the sentence contains the verb
repose (is based). As we can observe, the output of the Bonsai parser neither provides the lemmas of the
forms nor the syntactic dependencies between the constituents. So our study concentrates on the verbs
co-occurences with nouns, noun phrases and some relationnal adjectives. The further analysis of the
corpora is based on this output.
3.2 Semantic annotation
The Bonsai format is first converted into the XML format: we work on the XML-tree structure. The
semantic annotation of the corpora is done automatically. For this task, the Snomed International termi-
nology was chosen because it is suitable for french and it offers a better outreach of the french medical
language. We perform the projection of terms from the terminology on the syntactically parsed texts :
? All the chunks (NPs, PPs, APs and VPs) are processed from the largest to the smallest chunks,
within which we try to recognize the terminology entries which co-occur with the verbs in the
corpora. Indeed, at this stage, since our chunker does not provide dependency relations, we can
only work on nouns and noun phrases that co-occur with the verbs. For instance, the largest chunk
(NP (NC m?edicaments) (AP (ADJ g?en?eriques)))) gives m?edicaments g?en?eriques, (generic drugs) that is
not known in the terminology. We then test m?edicaments (drugs) and g?en?eriques (generic), of which
m?edicaments (drugs) is found in the terminology and tagged with the C CHEMICAL PRODUCTS
semantic type.
? Those VPs in which no terms have been identified are considered to be verbal forms or verbs.
Examples of corpora enriched with the semantic information are shown in Figures 1 (expert corpus)
and 2 (forum corpus). In these Figures, verbs are in bold characters, semantic labels for the verbs co-
occurents are represented by different colors: DISORDERS in red, FUNCTIONS in purple, ANATOMY in
clear blue. These semantic categories, provided by the terminological resource, label the words that are
likely to correspond to FEs.
Figure 1: Examples of annotations in expert corpus
We can see that in the two corpora, there are both short and long sentences. Besides, the terms
recognized are often atomic. For instance, we do not recognize complex terms embolie pulmonaire and
thrombose du tronc, but their simple atomic components embolie, pulmonaire, thrombose and tronc.
Also, some terms match none of the terminology?s entries because they are part of VPs, such as cath?eter
in Figure 1.
118
Figure 2: Examples of annotations in forum corpus.
3.3 Automatic analysis of verbs
For the analysis of the verbs, we extract information related to verbs and to the words with which they
occur. Currently, only sentences with one VP are processed 8 842 sentences for the expert corpus and
10 563 for the forum corpus.
? Lemmatization of verbs. As we noticed, the syntactic parser?s output does not provide the lemmas.
For the lemmatization of the verbs, we use the verbal resource described in section 2.3. Hence, the
content of the verbal chunk is analyzed:
? it may contain a simple or complex verbal form that exists in the resource, in which case we
record the corresponding lemma;
? if the whole chunk doesnot appear in the resource, we check out its atomic components: if all
or some of these components are known, we record the corresponding lemmas. This case may
apply to passive structures (a ?et?e conseill?e (has been advised)), insertions (est souvent conseill?e is
often advised) or negations (n?est pas conseill?e (is not advised)): in these cases, the lemmas are
avoir ?etre conseiller, ?etre conseiller and ?etre conseiller. These lemmas will be normalized in
the further step: the head verb will be chosen automatically and considered as the main lemma
within the verbal phrase;
? finally, the VPs may consist of words that are not known in the verb resource. These may be
morphologically contructed verbs (r?e?evaluer (reevaluate)) or, words from other parts of speech,
errouneously considered as verbs (e.g., t?el?ed?eclaration, art?erielle, stroke). This is unfortu-
nately a very frequent case.
? Extraction of information related to the verb co-occurents. For the extraction of these information,
we consider all the verbs appearing in sentences with one VP. For each verb, we distinguish between:
? semantically annotated co-occurents, that are considered to be specialized;
? and the remaining content of the sentence (except the words that are part of the stoplist), more
precisely noun phrases, is considered to contain non specialized co-occurents.
In both cases, for each verb, we compute the number and the percentage of words in each of the
above mentionned categories of co-occurents.
Finally, we provide a general analysis of the corpora. For each verb, we compute: the number of occur-
rences in each corpus, the total, minimal, maximal and average numbers of co-occurents, both specialized
and non-specialized. On the basis of this information, we analyse the differences and similarities which
may exist between the use of verbs in the two corpora studied. The purpose is to provide information
about the specialized and non-specialized occurrences of verbs.
4 Results and Discussion
4.1 Corpora pre-processing
The parsing, done with the Bonsai parser, provided the syntactic annotation of corpora into syntactic
constituents. We have noticed some limitations:
? The Bonsai parser does not perform the lemmatization of lexical units whereas we needed to extract
the verbs lemmas. The use of external resources made it possible to overcome this limitation;
119
? The verbal chunks do not always contain verbal constituents, but can contain other parts of speech
(e.g., t?el?ed?eclaration, art?erielle, stroke) and even punctuation. This is an important limitation for
our work, mainly because we focus on verbs. Therefore, if we cannot extract the verbs properly,
this can obviously have a negative impact on the final results. These limitations, resulting from the
Bonsai parser, highlight some of the issues that characterize the state of arts as far as the syntatic
analysis for French is concerned. For the future work, we are planning to try other syntactic parsers
for French.
4.2 Semantic annotation
Concerning the semantic annotation we have made several observations:
? Some annotations are missing, such as site d?insertion (insertion site) that can be labeled as TOPOG-
RAPHY or risque (risk) as FUNCTION. This limitation is also related to the annotation of the forum
corpus, that often contains misspellings or non-specialized equivalents of the terms. This limitation
must be addressed in future work in order to detect new terms or the variations of the existing terms
to make the annotation more exhaustive;
? Other annotations are erroneous, such as or (ou) in French annotated as CHEMICALS (gold)) in
English-language sentences. In future, the sentences in English will be forehand filtered out at the
processing stage;
? The terminological variation and the syntactic parsing provided by Bonsai make the recognition
of several complex terms difficult. As we noticed previously, we mainly recognize simple atomic
terms. For the current purpose, this is not a real limitation: the main objective is to detect the spe-
cialized and non-specialized words that co-occur with the verbs. Still, the number and semantic
types of these words co-occuring with verbs can become biased. For instance, instead of one DIS-
ORDER term embolie pulmonaire (air embolism), we obtain one DISORDER term embolie (embolism)
and one ANATOMY term pulmonaire (air).
4.3 Automatic analysis of verbs
The contrastive analysis of the words, co-occuring with verbs, provides the main results of the proposed
study.
Corpus Total
V
Total
coocc
Total
sp?coocc
Total
?sp?coocc
A
sp?coocc
/V A
?sp?coocc
/V
Expert Ex 545 17632 8354 9272 15 17
Forum Fo 592 10852 5545 5307 9 8
Table 2: General information related to the verbs and their co-occurent words: total and average numbers
of co-occurents
In Table 2, we compute the total number of verbs (Total
V
), the total number of words co-occuring with
verbs per corpus (Total
coocc
), the total number of non specialized co-occurents per corpus (N
sp?coocc
),
the average number of specialized co-occurents per verb (A
sp?coocc
/V ), the average number of non
specialized per verb (A
?sp?coocc
/V ). We can notice that the forum corpus provides slightly more verbs
than the expert corpus. This observation might be considered to be obvious, since the forum corpus is a
bit larger than the expert corpus. But if we combine this with the fact that the numbers and average
numbers of co-occurents (specialized and non-specialized) are higher in the expert corpus, then the
observation start making sense, since these results can be related to the confirmation by (Condamines
and Bourigault, 1999) of the fact that nominal forms tend to be more frequent in specialized texts,
whereas verbal forms tend to be more frequent in non-specialized texts. However, it is important to notice
that some candidates in the list of non-specialized co-occurents have to be filtered out, such as adverbs
(conform?ement, r?eguli`erement, pr?ecoc?ement, partiellement) and non relationnal adjectives (variables,
inconscients, diff?erents). The abundance of adverbs in the expert corpus (Table 4) by contrast to the forum
120
corpus, where their presence seems to be less important, is consistent with the previous work, which show
that non-specialized documents tend to have simpler syntactic and semantic structures (Wandji Tchami
et al., 2013) and less adverbs (Brouwers et al., 2012).
Verbs N
occ
N
coocc
N
sp?coocc
%
sp?coocc
N
?sp?coocc
%
?sp?coocc
A
sp?coocc
A
?sp?coocc
Ex Fo Ex Fo Ex Fo Ex Fo Ex Fo Ex Fo Ex Fo Ex Fo
augmenter 21 14 122 52 62 26 51.5 56.2 60 26 48.4 43.7 2.9 1.8 2.8 1.8
causer 5 7 26 27 17 19 72 68.2 9 8 28 31.72 3.4 2.7 1.8 1.1
favoriser 10 6 56 22 38 17 70.5 77.3 18 5 29.4 22.6 3.8 2.8 1.8 0.8
prescrire 6 29 30 108 16 71 58.9 69.7 14 37 41 30.2 2.6 2.4 2.3 1.2
provoquer 7 15 60 64 32 37 57 70.2 28 27 42.9 29.7 4.5 2.4 4 1.8
risquer 7 7 18 13 12 11 1.7 1.5 6 2 0.8 0.2 78.5 90 21.42 10
signaler 12 4 73 14 32 7 46.9 48.3 41 7 53 51.6 2.6 1.7 3.4 1.7
subir 4 24 20 98 15 54 76.1 63 5 44 23.8 36.9 3.7 2.5 1.2 1.8
traiter 24 17 107 67 66 34 65 60.2 41 33 34.9 39.7 2.7 2 1.7 1.9
Table 3: Information on some verbs that occur in Expert Ex and Forum Fo corpora
In Table 3, we give similar information but for with individual verbs. For each verb, in every corpus,
we compute the number of occurence (N
occ
), the number of words (N
coocc
) occuring with the verb, the
number of specialized co-occurents (N
sp?coocc
), the percentage of specialized co-occurents (%
sp?coocc
),
the number of non specialized co-occurents (N
?sp?coocc
), the percentage of non specialized co-occurents
(%
?sp?coocc
), the average number of specialized co-occurents (A
sp?coocc
) and the average number of non
specialized co-occurents (A
?sp?coocc
). These verbs are chosen because they occur in the two corpora
studied and because they are sufficiently frequent as compared to others. In our opinion, these verbs may
receive specialized and non-specialized meanings according to their usage. Indeed, Table 3 shows that
these verbs behave differently according to the corpus. On the one hand, there are verbs (e.g., augmenter,
favoriser, signaler, traiter, risquer) that occur with an important number of specialized co-occurents in the
Experts Ex corpus while they have lower numbers of specialized co-occurents in the Forum Fo corpus.
On the other hand, there are verbs (e.g., causer, subir, prescrire) that have more specialized co-occurents
in the Forum corpus than in the Expert corpus. If we consider the number of occurrences of these verbs,
we can definitely notice that some of them (e.g. causer and subir) regularly occur with more specialized
co-occurents in the Expert corpus (although with lower number of specialized co-occurents) than in the
Forum corpus. This means that their frames involve different numbers of specialized co-occurents, that
are higher in the Expert corpus.
In table 4, we show the frequent co-occurents for five verbs. We can propose two main observations:
? Some verbs involve an important number of specialized co-occurents, that have different semantic
types in the Expert and Forum corpora. For instance, the verb augmenter provides a total of 88
specialized co-occurents that belong to nine semantic types (D, P , S , J , C, F , T , L and A). The
most frequent among them are F (27), D (18), T (15), and P (9), and occur mostly in the Expert
corpus. These might be more general verbs, with weaker specific selectional restrictions.
? Other verbs frequently occur with specialized terms that belong to a specific semantic type. This
most frequent label can be specific to one corpus only or simultaneously to the two. For instance,
for the verb prescrire, the most frequent labels are the same in the two corpora: C, J , P and T
terms. Traiter frequently occurs, in the two corpora, with C and D terms.
The general observation is that, for a given verb, the Expert corpus shows more sophisticated syntactic
structures with higher number of specialized co-occurents. Besides, some verbs may show similar or
different behavior in the two corpora studied. According to the objectives of the proposed work, we con-
sider that an important presence of specialized terms in a sentence or corpus indicates a very specialized
use and meaning of the verbs. Quantitative and qualitative analysis of the data support this first study
and results.
121
sp? coocc ?sp? coocc
verbs Expert Forum Expert Forum
augmenter thrombolyses/F ,
gliomes/D, O2/C,
r?etinopathie/D,
Glasgow/P ,
myocardique/T
BNP/P , infarctus/D,
lasilix/C,
mouvements/A,
tabac/L
inf?erieur, ?egal,
score, groupe,
inconscients,
pr?ecoc?ement
heures, l?eg`erement
prescrire protocole/P ,
anticoagulant/C,
BNP/P
comprim?e/C,
diur?etique/C,
m?edecin/J
ministre, publica-
tion, r?eguli`erement
jour, matin, vari-
ables
produire pression/F ,
contraction/D
spasmes/F ,
coronnaires/T ,
stenosees/D
gauche, grande,
onde, ant?erograde,
diff?erents
g?en?eral, d?eja
traiter hypoglyc?emies/D,
pr?evention/P
insuffisance/F ,
cardiaque/T ,
an?evrismes/D
r?eccurentes, cas,
partiellement
succ`es, pr`es de, suite
provoquer fibrose/D, tissus/A,
nerveux/T ,
Vibrio/L,
vomissements/F
extrasystoles/T,
AVC/D, p`ere/S,
malaise/F ,
mouvement/A
secondaires, volon-
tairement, in-
satisfaisantes,
relativement, peu,
alimentaire, stri?es
diff?erent, beaucoup,
g?enant, angoissant,
mini, gros, longue,
petite, soir?ee
subir patient/J ,
arthroplastie/P
pose/P ,
fibrillation/F ,
AVC/D
raison,fixateur,
externe
fuite, grade
Table 4: Description of the verbs co-occurents
5 Conclusion
We have proposed an automatic method to distinguish between specialized and non-specialized occur-
rences of verbs in medical corpora. This work is intended to enhance the previous study (Wandji-2013).
Indeed, the method used has changed from semi-automatic to completely automatic; and a new task is
performed in order to enhance the annotation process : the syntactic parsing of the corpora. Also, some
new materials are used namely the Bonsai parser, the resource of verbal forms, the stoplist. There is an
increase in the quantity of data analyzed; all the verbs of the various corpora were considered in this
study. The annotation is based on an approach similar to Frame Semantics, considering the fact that
semantic information related to the verbs co-occurents are provided through the use of a medical termi-
nology. Though our method is still under development, it has helped to notice that some verbs regularly
co-occur with specialized terms in a given context or corpus while in another, the same verbs mostly
occurs with general language words. This observation takes us back to the issue of text readability, de-
scribed in the introduction. Indeed, the verbs whose occurences are characterized by the predominance
of specialized terms, can be considered as sources of reading difficulties for non experts in medecine.
6 Future work
We plan to extend this study in different ways. The recognition of the verb neighbors must be improved
with the main objective to make the annotations more exhaustive. In this study, we have portrayed the
verbs behaviors and their relations with the words with which they occur in the corpora. However, our
aim is to automatically identify the verbs arguments, among his co-occurents. We also plan to peform
an automatic distinction between : the syntactic functions (subject, object, etc.) of the verbs arguments
and the core and non-core elements. We also plan to compute the dependency relations within sentences,
122
either by using another chunker or by integrating to our treatment chain a tool that can perform this task.
In addition, we will concentrate on the description of semantic frames of the medical verbs and on the
identification of other eventual reading difficulties that might be related to the verbs usages in the corpora.
As indicated above, we processed sentences that have only one verbal phrase (8 842 for the Forum
corpus and 10 563 for the Expert corpus). In the future, we will process other sentences, coordinated
or subordinated, which will be segmented into simple propositions before the processing. Another point
is related to the exploitation of these findings for the simplification of medical documents at two levels:
syntactic and lexical. Finally, working at a fine-grained verbal semantics, we can distinguish the uses of
verbs according to whether their semantics and frames remain close or indicate different meanings.
Acknowledgements
The authors acknowledge the support of the French Agence Nationale de la Recherche (ANR) and the
DGA, under the Tecsan grant ANR-11-TECS-012.
References
S Atkins, M Rundell, and H Sato. 2003. The contribution of framenet to practical lexicography. International
Journal of Lexicography, 16(3):333?357.
A Balahur. 2013. Sentiment analysis in social media texts. In Computational Approaches to Subjectivity, Senti-
ment and Social Media Analysis, pages 120?128.
L Borin, D Dann?ells, M Forsberg, M Toporowska Gronostaj, and D Kokkinakis. 2010. The past meets the present
in the swedish framenet++. In 14th EURALEX International Congress, pages 269?281.
J Bormuth. 1966. Readability: A new approach. Reading research quarterly, 1(3):79?132.
Laetitia Brouwers, Delphine Bernhard, Anne-Laure Ligozat, and Thomas Franc?ois. 2012. Simplification syntax-
ique de phrases pour le franc?ais. In TALN, pages 211?224.
A Burchardt, K Erk, A Frank, A Kowalski, S Pad?o, and M Pinkal, 2009. Using FrameNet for the semantic analysis
of German: Annotation, representation, and automation, pages 209?244.
M Candito, J Nivre, P Denis, and E Anguiano. 2010. Benchmarking of statistical dependency parsers for french.
In International Conference on Computational Linguistics, pages 108?116.
J Chmielik and N Grabar. 2011. D?etection de la sp?ecialisation scientifique et technique des documents
biom?edicaux gr?ace aux informations morphologiques. TAL, 51(2):151?179.
CG Chute, SP Cohn, KE Campbell, DE Oliver, and JR Campbell. 1996. The content coverage of clinical classifi-
cations. for the computer-based patient record institute?s work group on codes & structures. J Am Med Inform
Assoc, 3(3):224?33.
Anne Condamines and Didier Bourigault. 1999. Alternance nom/verbe : explorations en corpus sp?ecialis?es. In
Cahiers de l?Elsap, pages 41?48, Caen, France.
RA C?ot?e, 1996. R?epertoire d?anatomopathologie de la SNOMED internationale, v3.4. Universit
?e de Sherbrooke,
Sherbrooke, Qu?ebec.
E Dale and JS Chall. 1948. A formula for predicting readability. Educational research bulletin, 27:11?20.
AM Dolbey, M Ellsworth, and J Scheffczyk. 2006. BioFrameNet: A domain-specific FrameNet extension with
links to biomedical ontologies. In KR-MED. 87-94.
William H. Dubay. 2004. The principles of readability. Impact Information. Available at
http://almacenplantillasweb.es/wp-content/uploads/2009/11/The-Principles-of-Readability.pdf.
C Fillmore, 1982. Frame Semantics, pages 111?137.
R Flesch. 1948. A new readability yardstick. Journal of Applied Psychology, 23:221?233.
T Franc?ois and C Fairon. 2013. Les apports du TAL `a la lisibilit?e du franc?ais langue ?etrang`ere. TAL, 54(1):171?
202.
123
L Goeuriot, N Grabar, and B Daille. 2007. Caract?erisation des discours scientifique et vulgaris?e en franc?ais,
japonais et russe. In TALN, pages 93?102.
N Grabar, S Krivine, and MC Jaulent. 2007. Classification of health webpages as expert and non expert with a
reduced set of cross-language features. In AMIA, pages 284?288.
WT Hole and S Srinivasan. 2000. Discovering missed synonymy in a large concept-oriented metathesaurus. In
AMIA 2000, pages 354?8.
BL Humphreys, AT McCray, and ML Cheh. 1997. Evaluating the coverage of controlled health data termi-
nologies: report on the results of the NLM/AHCPR large scale vocabulary test. J Am Med Inform Assoc,
4(6):484?500.
JP Kincaid, RP Jr Fishburne, RL Rogers, and BS Chissom. 1975. Derivation of new readability formulas (au-
tomated readability index, fog count and flesch reading ease formula) for navy enlisted personnel. Technical
report, Naval Technical Training, U. S. Naval Air Station, Memphis, TN.
S Koeva. 2010. Lexicon and grammar in bulgarian framenet. In LREC?10.
D Kokkinakis and M Toporowska Gronostaj. 2006. Comparing lay and professional language in cardiovascular
disorders corpora. In Australia Pham T., James Cook University, editor, WSEAS Transactions on BIOLOGY
and BIOMEDICINE, pages 429?437.
P Lerat. 2002. Qu?est-ce que le verbe sp?ecialis?e? le cas du droit. Cahiers de Lexicologie, 80:201?211.
G Leroy, S Helmreich, J Cowie, T Miller, and W Zheng. 2008. Evaluating online health information: Beyond
readability formulas. In AMIA 2008, pages 394?8.
V. I. Levenshtein. 1966. Binary codes capable of correcting deletions, insertions and reversals. Soviet physics.
Doklady, 707(10).
MC L?Homme. 1998. Le statut du verbe en langue de sp?ecialit?e et sa description lexicographique. Cahiers de
lexicologie, 73(2):61?84.
Marie-Claude L?Homme. 2012. Le verbe terminologique: un portrait des travaux r?ecents. In CMLF 2012, pages
93?107.
A McCray. 2005. Promoting health literacy. J of Am Med Infor Ass, 12:152?163.
T Miller, G Leroy, S Chatterjee, J Fan, and B Thoms. 2007. A classifier to evaluate language specificity of medical
documents. In HICSS, pages 134?140.
S Pad?o and G Pitel. 2007. Annotation pr?ecise du francais en s?emantique de r?oles par projection cross-linguistique.
In TALN 2007.
J Pearson. 1998. Terms in Context, volume 1 of Studies in Corpus Linguistics. John Benjamins, Amster-
dam/Philadelphia.
JF Penz, SH Brown, JS Carter, PL Elkin, VN Nguyen, SA Sims, and MJ Lincoln. 2004. Evaluation of snomed
coverage of veterans health administration terms. In Medinfo, pages 540?4.
J Pimentel. 2011. Description de verbes juridiques au moyen de la s?emantique des cadres. In TOTH.
J Ruppenhofer, M Ellsworth, MRL Petruck, C R. Johnson, and J Scheffczyk. 2006. Framenet ii: Extended theory
and practice. Technical report, FrameNet. Available online http://framenet.icsi.berkeley.edu.
T Schmidt, 2009. The Kicktionary ? A Multilingual Lexical Resource of Football Language, pages 101?134.
O Wandji Tchami, MC L?Homme, and N Grabar. 2013. Discovering semantic frames for a contrastive study of
verbs in medical corpora. In Terminologie et intelligence artificielle (TIA), Villetaneuse.
Y Wang. 2006. Automatic recognition of text difficulty from consumers health information. In IEEE, editor,
Computer-Based Medical Systems, pages 131?136.
Q Zeng-Treiler, H Kim, S Goryachev, A Keselman, L Slaugther, and CA Smith. 2007. Text characteristics of
clinical reports and their implications for the readability of personal health records. In MEDINFO, pages 1117?
1121, Brisbane, Australia.
124
