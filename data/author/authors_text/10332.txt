Mining Chinese-English Parallel Corpora from the Web 
Bo Li 
School of Computer Science 
Wuhan University 
Wuhan, 430072, China 
whulibo@gmail.com 
Juan Liu 
School of Computer Science 
Wuhan University 
Wuhan, 430072, China 
liujuan@whu.edu.cn 
 
 
Abstract 
Parallel corpora are a crucial resource in 
research fields such as cross-lingual infor-
mation retrieval and statistical machine 
translation, but only a few parallel corpora 
with high quality are publicly available 
nowadays. In this paper, we try to solve the 
problem by developing a system that can 
automatically mine high quality parallel 
corpora from the World Wide Web. The 
system contains a three-step process. The 
system uses a web spider to crawl certain 
hosts at first. Then candidate parallel web 
page pairs are prepared from the 
downloaded page set. At last, each candi-
date pair is examined based on multiple 
standards. We develop novel strategies for 
the implementation of the system, which 
are then proved to be rather effective by the 
experiments towards a multilingual website. 
1 Introduction 
Parallel corpora consisting of text in parallel trans-
lation plays an important role in data-driven natu-
ral language processing technologies such as statis-
tical machine translation (Brown et al, 1990) and 
cross-lingual information retrieval (Landauer and 
Littman, 1990; Oard, 1997). But the fact is that 
only a few parallel corpora with high quality are 
publicly available such as the United Nations pro-
ceedings and the Canadian Parliament proceedings 
(LDC, 1999). These corpora are usually small in 
size, specializing in narrow areas, usually with fees 
and licensing restrictions, or sometimes out-of-date. 
For language pairs such as Chinese and English, 
the lack of parallel corpora is more severe. The 
lack of such kind of resource has been an obstacle 
in the development of the data-driven natural lan-
guage processing technologies. But the intense 
human labor involved in the development of paral-
lel corpora will still make it very hard to change 
the current situation by hand. 
The number of websites containing web pages in 
parallel translation increases considerably these 
years, which gives hope that we can construct par-
allel corpora with high quality in a big scale more 
easily. In this paper, we present a system named 
Parallel Corpus Mining System (PCMS) which can 
automatically collect Chinese-English parallel web 
corpora from the Web. Similar with previous work, 
PCMS uses a three-step process. First, the web 
spider WebZip1 is used to crawl the hosts specified 
by users. In the second step, candidate parallel web 
page pairs are prepared from the raw web page set 
fetched based on some outer features of the web 
pages. A novel strategy is designed to utilize all 
these features to construct high quality candidate 
parallel page pairs, which can raise the perform-
ance and reduce the time complexity of the system. 
In the third step, candidate page pairs are evaluated 
based on multiple standards in which page struc-
ture and content are both considered. The actually 
parallel page pairs are saved. 
The content-based strategy in the PCMS system 
is implemented mainly based on the vector space 
model (VSM). We design a novel implementation 
of VSM to bilingual text, which is called bilingual 
vector space model (BVSM). In previous content-
based work, they usually use coarse criterions to 
measure the similarity of bilingual text. For exam-
                                                 
1 http://www.spidersoft.com/webzip/default.asp 
847
ple, Ma and Liberman (1999) measured the content 
similarity by the count of parallel token pairs in the 
text which are weak at representing the actual con-
tent of the text. VSM was considered for evaluat-
ing the similarity of bilingual text in (Chen et al, 
2004), but unfortunately the particular description 
of the implementation which was a bit complex 
was not mentioned in their work, and the time 
complexity of their system was rather high. Be-
sides, there are also some other types of methods 
for mining parallel corpora from the web such as 
the work in (Resnik, 1998), (Resnik and Smith, 
2003) and (Zhang et al, 2006). Most of these 
methods are unbalanced between precision and 
recall or computationally too complex. We detail 
the implementation of BVSM in the PCMS system 
in this paper. The experiments conducted to a spe-
cific website show that PCMS can achieve a better 
overall result than relative work reported. 
The structure of the paper is as follows. The sys-
tem architecture of PCMS is introduced in Section 
2. We introduce the details of the step for prepar-
ing candidate web page pairs in Section 3. The 
next step, candidate page pair evaluation, is de-
scribed in Section 4. We discuss the results of the 
experiments and conclude the paper in the last two 
sections. 
2 The PCMS System 
The PCMS system is designed to mine parallel 
corpora automatically from the web. As has been 
clarified above, the system employs a three-step 
process. The first is a web page fetching step. 
There are some tools to do the job and the PCMS 
system uses WebZip to fetch all the web pages 
from specific hosts. We usually choose some sites 
which probably contain high quality parallel web 
pages such as the site of the ministry of foreign 
affairs of China. After the web pages are obtained 
from the servers, the web pages which are too 
small, for example smaller than 5k bytes, are ex-
cluded from the page set. Then for each page in the 
page set, the HTML source of the web page is 
parsed and the noise such as the advertisement is 
excluded from the raw web page. The second is the 
candidate parallel page pair preparation step. The 
web pages are paired according to the URL simi-
larity and some other features of the web pages. 
The third is the candidate parallel page pair evalua-
tion step which is the key section of the PCMS 
system. Both web page structure and content are 
considered in this step. The candidate parallel page 
pairs prepared by the second step are first filtered 
by the structure-based criterion and then evaluated 
by the content-based criterion. We develop novel 
strategies for the third step and describe it in detail 
in the following sections.  
3 Candidate Parallel Pair Preparation 
The web spider can fetch a great many web pages 
in different languages from certain hosts. Usually 
the language of a web page can be identified by 
some feature strings of the URL. For example, the 
URLs of many English web pages contain strings 
such as e, en, eng and english which are called 
language identification strings. The language iden-
tification strings are usually attached to the other 
part of the URL with symbols such as ???, ?/? and 
?_?. The number of web pages downloaded by the 
web spider is very large, so the pairs produced will 
be a huge amount if we treat each web page in lan-
guage A and each in language B as a candidate pair, 
which will then make the third step of the system 
computationally infeasible. Parallel web pages 
usually have similar URLs. For example, the web 
page P1 in Chinese and P2 in English are parallel: 
Web page P1  URL2:  
www.fmprc.gov.cn/chn/wjdt/wshd/t358904.htm 
Web page P2  URL:  
www.fmprc.gov.cn/eng/wjdt/wshd/t358905.htm 
We can see that the URL of page P1 and the URL 
of page P2 share most of the strings such as 
www.fmprc.gov.cn, wjdt, and wshd. In some other 
cases, the similarity between the URLs of parallel 
web pages may be not that direct but should still be 
obvious.  
In PCMS, a novel strategy is designed to meas-
ure the URL similarity of the candidate web page 
pair. Before the URL similarity evaluation process, 
the language identification strings of the URLs 
should be substituted by a uniform string which 
seldom occurs in normal URLs. For example, the 
language identification strings such as en, eng, cn 
and chn are substituted by the string *** which 
seldom occurs in normal URLs. For example, the 
above page P1 after the URL substitution process 
is www.fmprc.gov.cn/***/wjdt/wshd/t358904.htm. 
After the substitution process, the similarity of the 
                                                 
2 The protocol string HTTP is omitted here. 
848
new URLs is evaluated. For evaluating the URL 
similarity of web page PA in language A and web 
page PB in language B, the following criterions are 
considered. 
Criterion 1: URL length difference. 
It can be found that the length of the URLs of 
parallel web pages is usually similar. The length of 
the URL here refers to the number of directories in 
the URL string. For example, the URL of the 
above web page P1 contains the directories ***3, 
wjdt and wshd, and then the URL length of P1 is 3. 
If two web pages PA and PB are parallel, the URL 
length of PA and PB should be similar. The URL 
length difference criterion is define as 
( ) ( )
 ( , )=
( ) ( )
len PA len PB
URL diff PA PB
len PA len PB
?
+        (1) 
where URL diff(PA,PB) is the URL length differ-
ence between PA and PB, len(PA) is the URL 
length of page PA and len(PB) is the URL length 
of PB. The value of URL length difference is be-
tween 0 and 1, and the more similar two URLs are, 
the smaller the value is. If the URL lengths of PA 
and PB are the same, the URL length difference 
between PA and PB should be 0. 
Criterion 2: URL directory similarity. 
Besides URL length, URL directory information 
is also considered in the candidate page pair prepa-
ration step. It can be observed that the URLs of 
parallel web pages usually share similar directory 
structure which can be represented by the common 
directories in the URLs. For example, the above 
web page P1 and web page P2 share the directories 
***, wjdt and wshd. To measure the URL directory 
similarity of the web page PA and the web page PB, 
a criterion is defined as  
2* ( , )
 ( , )
( ) ( )
comdir PA PB
URL dirsim PA PB
len PA len PB
= +   (2) 
where URL dirsim(PA,PB) is the URL directory 
similarity of page PA and page PB, comdir(PA,PB) 
is the number of common directories PA and PB 
share, len(PA) and len(PB) are the same as above. 
The value of URL directory similarity is between 0 
and 1. The bigger the value is, the more similar the 
two pages are. When two web pages have the same 
URLs, the URL directory similarity should be 1. 
                                                 
3 The language identification strings of the URL have been 
substituted by the uniform string ***. 
Criterion 3: Similarity of some other features. 
Some other features such as the file size of the 
web page and the time the page created can help to 
filter the nonparallel web page pairs with low cost. 
Based on the combination of the above criteri-
ons, the web page pairs of which the similarity ex-
cesses certain threshold are treated as the candidate 
parallel pairs, which are then to be processed by 
the following evaluation step. 
4 Candidate Parallel Pair  Evaluation 
It is the key section of the system to evaluate the 
candidate parallel web page pairs. Though content-
based methods are what the candidate parallel page 
pair evaluation step mainly relies on, the structure 
of the web pages is also considered in the evalua-
tion step of the PCMS system for it can help to 
filter out some page pairs that are obviously non-
parallel at low cost. The candidate parallel page 
pair set is first filtered by the structure-based strat-
egy which is similar with the one in (Resnik, 1998), 
and we consider some more structure relative fea-
tures such as color and font. A loose constrain is 
set on the structure similarity criterion, because it 
is merely a preliminary filter step to reduce the 
scale of the problem. 
After the structure-based filter stage, the page 
pairs left are then to be evaluated by the content-
based stage which is the key of the candidate paral-
lel page pair evaluation step. The performance of 
the PCMS system relies mainly on this module. In 
the content-based stage, the candidate page pairs 
are first filtered based on some content related fea-
tures and then the page pairs left are evaluated by 
the BVSM model. 
4.1 The Content Related Feature-based Filter 
In the first part of the content-based strategy, some 
content related features such as time stamp and 
navigation text are combined to construct a pre-
liminary step to filter the candidate page pair set 
and reduce the number of pairs to be processed by 
BVSM. Many web pages contain time stamps 
which identify the time when the web pages were 
constructed. If two pages are parallel, the time 
when they are constructed should be similar. Navi-
gation text usually demonstrates the type informa-
tion of the content of the web page. For example, a 
web page with anchor text Home-News-China is 
probable about the news which happened in China. 
849
So if two web pages are parallel, their navigation 
text if there is any should be similar. To evaluate 
the similarity of two pieces of navigation text in 
two languages, we need a bilingual navigation text 
wordlist. For each layer, for example news, in one 
navigation text, if its translation ?? xin-wen ap-
pears in the other navigation text, the similarity 
count will be added by 1. The similarity between 
two pieces of navigation text is defined as 
         
2 *
NC NE
count
similarity
layer layer
= +                  (3) 
where layerNC demonstrates the layer count of the 
navigation text of the Chinese web page and 
layerNE is that of the English web page. For exam-
ple, the layerNE of the navigation text Home-News-
China is 3. If the similarity gotten from formula (3) 
is below certain threshold, the corresponding web 
page pair will not be considered as parallel. 
4.2 The BVSM Model 
In the second part of the content-based strategy, 
BVSM is implemented to evaluate the similarity of 
candidate parallel page pairs. VSM is an important 
technology for representing text and has been ap-
plied to some other research areas. But this model 
is usually applicable to monolingual text process-
ing problem. For bilingual text processing, we 
should design a new strategy to use VSM for the 
new problem. A bilingual dictionary is a must for 
importing VSM to bilingual problem. We give a 
brief introduction to the bilingual dictionary we 
use first. Each entry line of the dictionary consists 
of three parts. The first part is the English word, 
the middle is a list separator and the last is the cor-
responding Chinese word. A sample of the diction-
ary can be found in Appendix A. For each English 
word, there may be some Chinese words serving as 
its translations. The same conclusion can be gotten 
for each Chinese word. 
Based on the bilingual dictionary, we can repre-
sent the Chinese and English web pages as vectors 
respectively. First, we give every English word in 
the bilingual dictionary a unique ID according to 
its position in the dictionary beginning from 1. For 
example, the ID of the English word in the first 
row is 1, and the ID of the next new English word 
in the dictionary is 2 and so forth. For convenience, 
we denote the Chinese web page as C and the Eng-
lish web page as E in each web page pair. We then 
can represent each web page as follows. 
For E, we extract all the words from the web 
page and stem them first. The length of the vector 
of E equals the length of the bilingual dictionary 
which is the number of the different English words 
in the dictionary. For each dimension of the vector, 
for example k, we assign the number of the words 
with ID k occurring in all the words extracted to it. 
If certain words in the bilingual dictionary never 
occur in E, we assign the value 0 to the corre-
sponding dimensions which are identified by the 
IDs of those words. If some words in E haven?t 
occurred in the dictionary, we just ignore them. 
For C, the procedure to construct a vector is 
more complex. In the PCMS system, the proce-
dures of word segmentation and POS for Chinese 
are finished in a single run. The length of the vec-
tor of C equals to that of the vector of E. As has 
been pointed out, one Chinese word may corre-
spond to more than one English word in the bilin-
gual dictionary. For example in Appendix A, the 
Chinese word ?? fang-qi corresponds to aban-
don, depart and leave. In the vector of E, each di-
mension strands for the count of a single English 
word with a unique ID occurring in the English 
text. In order to construct a vector for C which is 
comparable to the vector of E, a single Chinese 
word in C should contribute to more than one di-
mension of the vector of C. In order to distribute 
the count/weight of each Chinese word to the cor-
responding dimensions of the vector of C, we first 
count the number of each entry which is a Chinese 
word with a specific POS, for example (?? , 
Verb), in C. Then for each entry, we distribute its 
count to all the dimensions identified by the IDs of 
the English words which the Chinese word in the 
entry corresponds to. The count distribution proc-
ess is detailed below. 
If the Chinese word in the entry Cent is a con-
tent word which we call here to mean that it carries 
the main content of a language including noun, 
verb and adjective, we will divide the correspond-
ing English words in the bilingual dictionary into 
four separate classes: the words that haven?t ap-
peared in the English text (C4), the words that have 
the same POS with the entry (C1), the words that 
have similar POS with the entry (C2) and the other 
words (C3). For convenience, the count of the entry 
Cent in C is denoted as N1234. If the capacity of C4 
is 0 which means there are no words belonging to 
the class C4, then N1234 is all devoted to the words 
850
in C1, C2 and C3, else a certain proportion, for ex-
ample 10%, of N1234 is assigned to all the words in 
C4 averagely and the left of N1234 is assigned to the 
words in C1, C2 and C3. Similarly, we denote the 
count left to words in C1, C2 and C3 as N123, and 
then if the capacity of C3 is 0, N123 is all denoted to 
the words in C1 and C2, else a certain proportion of 
N123 is denoted to all the words in C3 averagely and 
the left of N123 is devoted to the words in C1 and C2. 
For words in C1 and C2, the count distribution 
strategy is similar. 
If the Chinese word in the entry Cent is not a 
content word, we classify the corresponding Eng-
lish words into two classes: the words that haven?t 
appeared in the English text (C2) and the other 
words (C1). The same method as above is used to 
distribute the count. 
4.3 Similarity Evaluation Criterions 
Based on the above strategies, the two web pages 
can be represented by their vectors respectively. 
Then the next step is to calculate the similarity of 
the two vectors, which is also the similarity of the 
two web pages. Some comments were given on 
different similarity measures such as Euclidence 
distance, Inner product, Cosine coefficient, Dice 
coefficient and Jaccard coefficient in (Chen et al, 
2004). It was suggested that for a pair of docu-
ments to be considered parallel, we could expect 
that these two documents contained the two corre-
sponding sets of translated terms and each corre-
sponding term was carrying an identical contextual 
significance in each of the document respectively. 
For that, the Jaccard coefficient is more appropri-
ate for the calculation of the similarity score. 
While in our experiments, we find that Cosine co-
efficient is more suitable. Because the size of the 
bilingual dictionary is small and we exclude all the 
words which are not in the dictionary from the text 
of the web pages when we construct the vectors, it 
is possible that the counterparts of some words in 
one web page can not be found in its correspond-
ing web page. Though we have done some smooth 
work in the BVSM model, there is still a gap be-
tween the assumptions by Chen et al (2004) and 
the situation of our problem. The second reason we 
think is that the translation process by human is 
almost sentence to sentence, but not word to word. 
As a result, it is normal that there are no words in 
one language serving as the translation for certain 
words in the other language. Based on the Cosine 
coefficient criterion, the similarity between two 
vectors which are represented by (x1, x2, x3, ?, xp) 
and (y1, y2, y3, ?, yp) respectively is 
1
2 2
1 1
*
cos
p
i i
i
p p
i i
i i
x y
inecoefficient
x y
=
= =
=
?
? ?
       (4) 
The similarity measure is between 0 and 1, and 
the bigger the value is, the more similar the two 
vectors are. We set a certain threshold for the simi-
larity measure based on our experience in PCMS. 
5 Experiments and Discussion 
In this section, we practice the experiments de-
signed to evaluate the performance of the PCMS 
system and compare it with similar work earlier. 
5.1 Evaluation Standards 
Precision and recall are two widely used evalua-
tion standards in the area of natural language proc-
essing. In our experiments, we define precision as 
the proportion of page pairs in parallel translation 
to the total page pairs produced by the PCMS sys-
tem. Recall is defined as the proportion of page 
pairs in parallel translation produced by the PCMS 
system to the total parallel page pairs in the whole 
web page set. 
The number of pairs in parallel translation 
should be calculated from the human annotated 
page pairs. We ask a native Chinese speaker who 
has a fluent English tongue to annotate these page 
pairs. To calculate the recall, we need to know the 
number of parallel pairs in the web page set. It is 
hard to count out the actual number of the parallel 
pairs in the page set because the web page set is 
really too big. We build a relatively smaller test set 
to test the recall of the PCMS system. 
5.2 Parallel Corpus Construction 
In order to construct a high quality parallel corpus 
in the experiments, the website of the ministry of 
foreign affairs of China (http://www.fmprc.gov.cn) 
is chosen to be crawled. After the rough observa-
tion, it is found that a huge number of web pages 
fetched are in parallel translation. We get a web 
page set consisting of 40262 Chinese web pages 
and 17324 English web pages by the tool WebZip. 
After the preprocess step, the web pages left are to 
851
be examined by the core modules of PCMS. It 
takes nearly 3 hours to finish the task on a PC with 
a P4 2.0G CPU and 512MB RAM, which is faster 
than the early systems. To evaluate the precision of 
the system, we randomly choose a subset of the 
web page pairs which PCMS gives as output, and 
get a web page set of 500 web page pairs. We 
manually annotate it and find that there are 479 
truly parallel page pairs among them. Then the 
precision is about 96%. We analysis the 21 non-
parallel pairs the PCMS system gives and find that 
most of these web pages are short web pages con-
taining limited text. To obtain the recall of the 
PCMS system, we construct a test page set consist-
ing of 350 parallel page pairs and 150 nonparallel 
page pairs. The ratio 350/150 is decided based on 
rough estimation of the whole page set. The PCMS 
system is examined on the test set, which produces 
337 page pairs which are truly parallel, thus a re-
call of 96%. We analysis the 13 parallel pages 
which are recognized as nonparallel by the PCMS 
system and find that most of them are short web 
pages. We then come to the conclusion that the 
drawback that BVSM is weak at representing short 
text leads to the system?s failure to identify the 
parallel web page pairs. Though the model has 
some drawbacks, the overall result consisting of 
performance and time complexity is much better 
than the former similar work. 
6 Conclusion 
The paper presents a web-based parallel corpus 
construction system PCMS. The system first 
fetches all the web pages from specific hosts, and 
then prepares candidate parallel web page pairs 
based on features such as URL and web page file 
size. At last the candidate pairs are examined by a 
two-stage similarity evaluation process in which 
the structure and content of the web pages are both 
considered. To enhance the performance of the 
PCMS system, we design some novel strategies for 
the implementation of these steps. The results of 
the experiments show the high performance and 
low time complexity of the PCMS system. All in 
all, the PCMS system is a reliable and effective 
tool for mining parallel corpora from the web. 
References 
Brown, P. F., Cocke, J., Pietra, S. D., Pietra, V. J. D., 
Jelinek, F., Lafferty, J. D., et al (1990). A statistical 
approach to machine translation. Computational Lin-
guistics, 16(2), 79-85. 
Chen, J., Chau, R., and Yeh, C. H. (2004). Discovering 
parallel text from the World Wide Web. In Proc. of 
DMWI-04, Dunedin, New Zealand. 
Landauer, T. K. and Littman, M. L. (1990). Fully auto-
matic cross-language document retrieval using latent 
semantic indexing. In Proc. of the 6th Annual Con-
ference of the UW Centre for the New Oxford English 
Dictionary and Text Research, Waterloo, Ontario. 
LDC. (1999). Linguistic Data Consortium (LDC) home 
page. http://www.ldc.upenn.edu 
Ma, X. and Liberman, M. Y. (1999). BITS: A method 
for bilingual text search over the web. In Proc. of the 
Machine Translation Summit VII. 
Oard, D. W. (1997). Cross-language text retrieval re-
search in the USA. In Proc. of the 3rd ERCIM 
DELOS Workshop, Zurich, Switzerland. 
Resnik, P. (1998). Parallel strands: A preliminary inves-
tigation into mining the web for bilingual text. In 
Proc. of AMTA-98, Langhorne, PA. 
Resnik, P. and Smith, N. A. (2003). The web as a paral-
lel corpus. Computational Linguistics, 29(3), 349-
380. 
Zhang, Y., Wu, K., Gao, J., and Vines, P. (2006). 
Automatic acquisition of Chinese-English parallel 
corpus from the web. In Proceedings of ECIR-06, 
London. 
Appendix A: A Sample Bilingual Dictionary 
abandon --- ?? 
abandon --- ?? 
abandon --- ?? 
abandon --- ?? 
abc --- ?? 
abc --- ?? 
abc --- ?? 
abc --- ?? 
?? 
depart --- ?? 
depart --- ?? 
depart --- ?? 
depart --- ?? 
?? 
leave --- ?? 
leave --- ?? 
leave --- ?? 
leave --- ?? 
?? 
852
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 644?652,
Beijing, August 2010
Improving Corpus Comparability for Bilingual Lexicon Extraction from
Comparable Corpora
Bo Li, Eric Gaussier
Laboratoire d?Informatique de Grenoble (LIG)
Universite? de Grenoble
firstname.lastname@imag.fr
Abstract
Previous work on bilingual lexicon extrac-
tion from comparable corpora aimed at
finding a good representation for the usage
patterns of source and target words and at
comparing these patterns efficiently. In
this paper, we try to work it out in an-
other way: improving the quality of the
comparable corpus from which the bilin-
gual lexicon has to be extracted. To do
so, we propose a measure of comparabil-
ity and a strategy to improve the qual-
ity of a given corpus through an iterative
construction process. Our approach, be-
ing general, can be used with any existing
bilingual lexicon extraction method. We
show here that it leads to a significant im-
provement over standard bilingual lexicon
extraction methods.
1 Introduction
Bilingual dictionaries are an essential resource
in many multilingual natural language process-
ing (NLP) tasks such as machine translation (Och
and Ney, 2003) and cross-language information
retrieval (CLIR) (Ballesteros and Croft, 1997).
Hand-coded dictionaries are of high quality, but
expensive to build and researchers have tried,
since the end of the 1980s, to automatically
extract bilingual lexicons from parallel corpora
(see (Chen, 1993; Kay and Ro?scheisen, 1993;
Melamed, 1997a; Melamed, 1997b) for early
work). Parallel corpora are however difficult to
get at in several domains, and the majority of
bilingual collections are comparable and not par-
allel. Due to their low cost of acquisition, sev-
eral researchers have tried to exploit such com-
parable corpora for bilingual lexicon extraction
(Fung and McKeown, 1997; Fung and Yee, 1998;
Rapp, 1999; De?jean et al, 2002; Gaussier et al,
2004; Robitaille et al, 2006; Morin et al, 2007;
Yu and Tsujii, 2009). The notion of comparability
is however a loose one, and comparable corpora
range from lowly comparable ones to highly com-
parable ones and parallel ones. For data-driven
NLP techniques, using better corpora often leads
to better results, a fact which should be true for
the task of bilingual lexicon extraction. This point
has largely been ignored in previous work on the
subject. In this paper, we develop a well-founded
strategy to improve the quality of a comparable
corpus, so as to improve in turn the quality of the
bilingual lexicon extracted. To do so, we first pro-
pose a measure of comparability which we then
use in a method to improve the quality of the ex-
isting corpus.
The remainder of the paper is organized as fol-
lows: Section 2 introduces the experimental mate-
rials used for the different evaluations; compara-
bility measures are then presented and evaluated
in Section 3; in Section 4, we detail and evaluate
a strategy to improve the quality of a given corpus
while preserving its vocabulary; the method used
for bilingual lexicon extraction is then described
and evaluated in Section 5. Section 6 is then de-
voted to a discussion, prior to the conclusion given
in Section 7.
2 Experimental Materials
For the experiments reported here, several cor-
pora were used: the parallel English-French
Europarl corpus (Koehn, 2005), the TREC
644
(http://trec.nist.gov/) Associated Press corpus
(AP, English) and the corpora used in the
multilingual track of CLEF (http://www.clef-
campaign.org) which includes the Los Angeles
Times (LAT94, English), Glasgow Herald (GH95,
English), Le Monde (MON94, French), SDA
French 94 (SDA94, French) and SDA French 95
(SDA95, French). In addition to these exist-
ing corpora, two monolingual corpora from the
Wikipedia dump1 were built. For English, all
the articles below the root category Society with
a depth less than 42 were retained. For French,
all the articles with a depth less than 7 below the
category Socie?te? are extracted. As a result, the
English corpus Wiki-En consists of 367,918 doc-
uments and the French one Wiki-Fr consists of
378,297 documents.
The bilingual dictionary used in our experi-
ments is constructed from an online dictionary.
It consists of 33,372 distinct English words and
27,733 distinct French words, which constitutes
75,845 translation pairs. Standard preprocessing
steps: tokenization, POS-tagging and lemmatiza-
tion are performed on all the linguistic resources.
We will directly work on lemmatized forms of
content words (nouns, verbs, adjectives, adverbs).
3 Measuring Comparability
As far as we can tell, there are no practical mea-
sures with which we can judge the degree of com-
parability of a bilingual corpus. In this paper, we
propose a comparability measure based on the ex-
pectation of finding the translation for each word
in the corpus. The measure is light-weighted and
does not depend on complex resources like the
machine translation system. For convenience, the
following discussions will be made in the context
of the English-French comparable corpus.
3.1 The Comparability Measure
For the comparable corpus C, if we consider the
translation process from the English part Ce to the
1The Wikipedia dump files can be downloaded at
http://download.wikimedia.org. In this paper, we use the En-
glish dump file on July 13, 2009 and the French dump file on
July 7, 2009.
2There are several cycles in the category tree of
Wikipedia. It is thus necessary to define a threshold on the
depth to make the iterative process feasible.
French part Cf , a comparability measure Mef can
be defined on the basis of the expectation of find-
ing, for each English word we in the vocabulary
Cve of Ce, its translation in the vocabulary Cvf of Cf .
Let ? be a function indicating whether a transla-
tion from the translation set Tw of w is found in
the vocabulary Cv of a corpus C, i.e.:
?(w, Cv) =
{
1 iff Tw ? Cv 6= ?
0 else
Mef is then defined as:
Mef (Ce, Cf ) = E(?(w, Cvf )|w ? Cve )
=
?
w?Cve
?(w, Cvf ) ? Pr(w ? Cve )? ?? ?
Aw
= |C
v
e |
|Cve ? Dve |
?
w?Cve?Dve
Aw
where Dve is the English part of a given, inde-
pendent bilingual dictionaryD, and where the last
equality is based on the fact that, the compara-
ble corpus and the bilingual dictionary being in-
dependent of one another, the probability of find-
ing the translation in Cvf of a word w is the same
for w is in Cve ? Dve and in Cve\Dve 3. Furthermore,
the presence of common words suggests that one
should rely on a presence/absence criterion rather
than on the number of occurrences to avoid a bias
towards common words. Given the natural lan-
guage text, our evaluation will show that the sim-
ple presence/absence criterion can perform very
well. This leads to Pr(w ? Cve ) = 1/|Cve |, and
finally to:
Mef (Ce, Cf ) =
1
|Cve ? Dve |
?
w?Cve?Dve
?(w, Cvf )
This formula shows that Mef is actually the pro-
portion of English words translated in the French
part of the comparable corpus. Similarly, the
counterpart of Mef , Mfe, is defined as:
Mfe(Ce, Cf ) =
1
|Cvf ? Dvf |
?
w?Cvf?Dvf
?(w, Cve )
3The fact can be reliable only when a substantial part of
the corpus vocabulary is covered by the dictionary. Fortu-
nately, the constraint is satisfied in most applications where
the common but not the specialized corpora like the medical
corpora are involved.
645
and measures the proportion of French words in
Cvf translated in the English part of the compara-
ble corpus. A symmetric version of these mea-
sures is obtained by considering the proportion of
the words (both English and French) for which a
translation can be found in the corpus:
M(Ce, Cf )
=
?
w?Cve?Dve ?(w, C
v
f ) +
?
w?Cvf?Dvf ?(w, C
v
e )
|Cve ? Dve |+ |Cvf ? Dvf |
We now present an evaluation of these measures
on artificial test corpora.
3.2 Validation
In order to test the comparability measures, we de-
veloped gold-standard comparability scores from
the Europarl and AP corpora. We start from the
parallel corpus, Europarl, of which we degrade
the comparability by gradually importing some
documents from either Europarl or AP. Three
groups (Ga, Gb, Gc) of comparable corpora are
built in this fashion. Each group consists of test
corpora with a gold-standard comparability rang-
ing, arbitrarily, from 0 to 1 and corresponding to
the proportion of documents in ?parallel? transla-
tion. The first group Ga is built from Europarl
only. First, the Europarl corpus is split into 10
equal parts, leading to 10 parallel corpora (P1, P2,
. . . , P10) with a gold-standard comparability arbi-
trarily set to 1. Then for each parallel corpus, e.g.
Pi, we replace a certain proportion p of the En-
glish part with documents of the same size from
another parallel corpus Pj(j 6= i), producing the
new corpus P ?i with less comparability which is
the gold-standard comparability 1 ? p. For each
Pi, as p increases, we obtain several comparable
corpora with a decreasing gold-standard compara-
bility score. All the Pi and their descendant cor-
pora constitute the group Ga. The only difference
betweenGb andGa is that, inGb, the replacement
in Pi is done with documents from the AP cor-
pus and not from Europarl. In Gc, we start with
10 final, comparable corpora P ?i from Ga. These
corpora have a gold-standard comparability of 0
in Ga, and of 1 in Gc. Then each P ?i is further
degraded by replacing certain portions with docu-
ments from the AP corpus.
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
0.83
0.84
0.85
0.86
0.87
0.88
M
Gold-standard comparability scores
Scores from the comparability
 metric
Figure 1: Evolution of M wrt gold-standard
on the corpus group Gc (x-axis: gold-standard
scores, y-axis: M scores)
We then computed, for each comparable cor-
pus in each group, its comparability according
to one of the comparability measures. Figure 1
plots the measure M for ten comparable corpora
and their descendants from Gc, according to their
gold-standard comparability scores. As one can
note, the measure M is able to capture almost all
the differences in comparability and is strongly
correlated with the gold-standard. The correla-
tion between the different measures and the gold-
standard is finally computed with Pearson corre-
lation coefficient. The results obtained are listed
in Table 1. As one can note, Mfe performs worst
among the three measures, the reason being that
the process to construct Gb and Gc yields unbal-
anced bilingual corpora, the English section being
larger than the French one. Translations of French
words are still likely to be found in the English
corpus, even though the corpora are not compara-
ble. On all the 3 groups,M performs best and cor-
relates very well with the gold standard, meaning
that M was able to capture all the differences in
comparability artificially introduced in the degra-
dation process we have considered. This is the
measure we will retain in the following parts.
Mef Mfe M
Ga 0.897 0.770 0.936
Gb 0.955 0.190 0.979
Gc 0.940 -0.595 0.960
Table 1: Correlation scores for the different com-
parability measures on the 3 groups of corpora
646
Having established a measure for the degree of
comparability of bilingual corpora, we now turn
to the problem of improving the quality of com-
parable corpora.
4 Improving Corpus Quality
We here try to improve the quality of a given cor-
pus C, which we will refer to as the base corpus,
by extracting the highly comparable subpart CH
which is above a certain degree of comparability
? (Step 1), and by enriching the lowly comparable
part CL with texts from other sources (Step 2). As
we are interested in extracting information related
to the vocabulary of the base corpus, we want the
newly built corpus to contain a substantial part of
the base corpus. This can be achieved by preserv-
ing in Step 1 as many documents from the base
corpus as possible (e.g. by considering low values
of ?), and by using in step 2 sources close to the
base corpus.
4.1 Step 1: Extracting CH
The strategy consisting of building all the possible
sub-corpora of a given size from a given compa-
rable corpora is not realistic as soon as the num-
ber of documents making up the corpora is larger
than a few thousands. In such cases, better ways
for extracting subparts have to be designed. The
strategy we have adopted here aims at efficiently
extracting a subpart of C above a certain degree of
comparability and is based on the following prop-
erty.
Property 1. Let d1e and d2e (resp. d1f and d2f )
be two English (resp. French) documents from a
bilingual corpus C. We consider, as before, that
the bilingual dictionary D is independent from C.
Let (d1e ?, d1f ?) be such that: d1e ? ? d1e, d1f ? ? d1f ,
which means d1e ? is a subpart of d1e and d1f ? is a
subpart of d1f .
We assume:
(i) |d1e?d2e||d2e| =
|d1f?d2f |
|d2f |
(ii) Mef (d1e ?, d1f ) ?Mef (d2e, d2f )
Mfe(d1e, d1f
?) ?Mfe(d2e, d2f )
Then:
M(d2e, d2f ) ?M(d1e ? d2e, d1f ? d2f )
Proof [sketch]: Let B = (d1e ? d2e) ? Dve )\(d2e ?
Dve ). One can show, by exploiting condition (ii),
that:
?
w?B
?(w, d1f ? d2f ) ? |B|Mef (d2e, d2f )
and similarly that:
?
w?d2e?Dve
?(w, d1f ? d2f ) ? |d2e ? Dve |Mef (d2e, d2f )
Then exploiting condition (i), and the indepen-
dence between the corpus and the dictionary, one
arrives at:
?
w?(d1e?d2e)?Dve ?(w, d
1
f ? d2f )
|(d1e ? d2e) ? Dve |+ |(d1f ? d2f ) ? Dvf |
?
|d2e ? Dve |Mef (d2e, d2f )
|d2e ? Dve |+ |d2f ? Dvf |
The same development on Mfe completes the
proof. 2
Property 1 shows that one can incrementally ex-
tract from a bilingual corpus a subpart with a guar-
anteed minimum degree of comparability ? by it-
eratively adding new elements, provided (a) that
the new elements have a degree of comparability
of at least ? and (b) that they are less comparable
than the currently extracted subpart (conditions
(ii)). This strategy is described in Algorithm 1.
Since the degree of comparability is always above
a certain threshold and since the new documents
selected (d2e, d2f ) are the most comparable among
the remaining documents, condition (i) is likely
to be satisfied, as this condition states that the in-
crease in the vocabulary from the second docu-
ments to the union of the two is the same in both
languages. Similarly, considering new elements
by decreasing comparability scores is a necessary
step for the satisfaction of condition (ii), which
states that the current subpart should be uniformly
more comparable than the element to be added.
Hence, the conditions for property 1 to hold are
met in Algorithm 1, which finally yields a corpus
with a degree of comparability of at least ?.
4.2 Step 2: Enriching CL
This step tries to absorb knowledge from other
resources, which will be called external corpus,
647
Algorithm 1
Input:
English document set Cde of C
French document set Cdf of C
Threshold ?
Output:
CH , consisting of the English document set Se
and the French document set Sf
1: Initialize Se = ?,Sf = ?, temp = 0;
2: repeat
3: (de, df ) = argmax
de?Cde ,df?Cdf
M(de, df );
4: temp = max
de?Cde ,df?Cdf
M(de, df );
5: if temp ? ? then
6: Add de into Se and add df into Sf ;
7: Cde = Cde\de, Cdf = Cdf\df ;
8: end if
9: until Cde = ? or Cdf = ? or temp < ?
10: return CH ;
to enrich the lowly comparable part CL which is
the left part in C during the creation of CH . One
choice for obtaining the external corpus CT is to
fetch documents which are likely to be compara-
ble from the Internet. In this case, we first ex-
tract representative words for each document in
CL, translate them using the bilingual dictionary
and retrieve associated documents via a search en-
gine. An alternative approach is of course to use
existing bilingual corpora. Once CT has been con-
structed, the lowly comparable part CL can be en-
riched in exactly the same way as in section 4.1:
First, Algorithm 1 is used on the English part of
CL and the French part of CT to get the high-
quality document pairs. Then the French part of
CL is enriched with the English part of CT by the
same algorithm. All the high-quality document
pairs are then added to CH to constitute the final
result.
4.3 Validation
We use here GH95 and SDA95 as the base cor-
pus C0. In order to illustrate that the efficiency
of the proposed algorithm is not confined to a
specific external resource, we consider two ex-
ternal resources: (a) C1T made of LAT94, MON94
and SDA94, and (b) C2T consisting of Wiki-En and
Wiki-Fr. The number of documents in all the cor-
pora after elimination of short documents (< 30
words) is listed in Table 2.
C0 C1T C2T
English 55,989 109,476 367,918
French 42,463 87,086 378,297
Table 2: The size of the corpora in the experiments
For the extraction of the highly comparable part
CH from the base corpus C0, we set ? to 0.3
so as to extract a substantial subpart of C0. Af-
ter this step, corresponding to Algorithm 1, we
have 20,124 English-French document pairs in
CH . The second step is to enrich the lowly compa-
rable part CL of the base corpus documents from
the external resources C1T and C2T . The final cor-
pora we obtain consist of 46,996 document pairs
for C1 (with C1T ) and of 54,402 document pairs for
C2 (with C2T ), size similar to the one of C0. The
proportion of documents (columns ?D-e? and ?D-
f?), sentences (columns ?S-e? and ?S-f?) and vo-
cabulary (columns ?V-e? and ?V-f?) of C0 found
in C1 and C2 is given in Table 3. As one can note,
the final corpora obtained through the method pre-
sented above preserve most of the information
from the base corpus. Especially for the vocab-
ulary, the final corpora cover nearly all the vocab-
ulary of the base corpus. Considering the compa-
rability scores, the comparability of C1 is 0.912
and the one of C2 is 0.916. Both of them are
more comparable than the base corpus of which
the comparability is 0.882.
From these results of the intrinsic evaluation,
one can conclude that the strategy developed to
improve the corpus quality while preserving most
of its information is efficient: The corpora ob-
tained here, C1 and C2, are more comparable than
the base corpus C0 and preserve most of its infor-
mation. We now turn to the problem of extracting
bilingual lexicons from these corpora.
5 Bilingual Lexicon Extraction
Following standard practice in bilingual lexicon
extraction from comparable corpora, we rely on
the approach proposed by Fung and Yee (1998).
In this approach, each word w is represented as a
648
D-e D-f S-e S-f V-e V-f
C1 0.669 0.698 0.821 0.805 0.937 0.981
C2 0.785 0.719 0.893 0.807 0.968 0.987
Table 3: Proportion of documents, sentences and
vocabulary of C0 covered by the result corpora
context vector consisting of the weight a(wc) of
each context word wc, the context being extracted
from a window running through the corpus. Once
context vectors for English and French words have
been constructed, a general bilingual dictionaryD
can be used to bridge them by accumulating the
contributions from words that are translation of
each other. Standard similarity measures, as the
cosine or the Jaccard coefficient, can then be ap-
plied to compute the similarity between vectors.
For example, the cosine leads to:
sc(we, wf ) =
?
(wce,wcf )?D a(w
c
e)a(wcf )
???we? ? ???wf?
(1)
5.1 Using Algorithm 1 pseudo-Alignments
The process we have defined in the previous sec-
tion to improve the quality of a given corpus while
preserving its vocabulary makes use of highly
comparable document pairs, and thus provides
some loose alignments between the two corpora.
One can thus try to leverage the above approach
to bilingual lexicon extraction by re-weighting
sc(we, wf ) by a quantity which is large if we and
wf appear in many document pairs with a high
comparability score, and small otherwise. In this
section, we can not use the alignments in algo-
rithm 1 directly because the alignments in the
comparable corpus should not be 1 to 1 and we
did not try to find the precise 1 to 1 alignments in
algorithm 1.
Let ? be the threshold used in algorithm 1 to
construct the improved corpus and let ?(de, df )
be defined as:
?(de, df ) =
{
1 iff M(de, df ) ? ?
0 else
Let He (resp. Hf ) be the set of documents con-
taining word we (resp. wf ). We define the joint
probability of we and wf as being proportional
to the number of comparable document pairs they
belong to, where two documents are comparable
if their comparability score is above ?, that is:
p(we, wf ) ?
?
de?He,df?Hf
?(de, df )
The marginal probability p(we) can then be writ-
ten as:
p(we)?
?
wf?Cvf
p(we, wf )
?
?
de?He
?
df?Cdf
|df | ? ?(de, df )
Assuming that all df in Cdf have roughly the
same vocabulary size and all de have the same
number of comparable counterparts in Cdf , then
the marginal probability can be simplified as:
p(we) ? |He|. By resorting to the exponential
of the point-wise mutual information, one finally
obtains the following weight:
pi(we, wf ) =
p(we, wf )
p(we) ? p(wf )
? 1|He| ? |Hf |
?
de?He,df?Hf
?(de, df )
which has the desired property: It is large if the
two words appear in comparable document pairs
more often than chance would predict, and small
otherwise. We thus obtain the revised similarity
score for we and wf :
scr(we, wf ) = sc(we, wf ) ? pi(we, wf ) (2)
5.2 Validation
In order to measure the performance of the bilin-
gual lexicon extraction method presented above,
we divided the original dictionary into 2 parts:
10% of the English words (3,338 words) together
with their translations are randomly chosen and
used as the evaluation set, the remaining words
(30,034 words) being used to compute context
vectors and similarity between them. In this
study, the weight a(wc) used in the context vec-
tors (see above) are taken to be the tf-idf score
of wc: a(wc) = tf-idf(wc). English words not
649
present in Cve or with no translation in Cvf are ex-
cluded from the evaluation set. For each English
word in the evaluation set, all the French words
in Cvf are then ranked according to their similar-
ity with the English word (using either equation 1
or 2). To evaluate the quality of the lexicons ex-
tracted, we first retain for each English word its
N first translations, and then measure the preci-
sion of the lists obtained, which amounts in this
case to the proportion of lists containing the cor-
rect translation (in case of multiple translations, a
list is deemed to contain the correct translation as
soon as one of the possible translations is present).
This evaluation procedure has been used in pre-
vious work (e.g. (Gaussier et al, 2004)) and is
now standard for the evaluation of lexicons ex-
tracted from comparable corpora. In this study,
N is set to 20. Furthermore, several studies have
shown that it is easier to find the correct transla-
tions for frequent words than for infrequent ones
(Pekar et al, 2006). To take this fact into account,
we distinguished different frequency ranges to as-
sess the validity of our approach for all frequency
ranges. Words with frequency less than 100 are
defined as low-frequency words (WL), whereas
words with frequency larger than 400 are high-
frequency words (WH ), and words with frequency
in between are medium-frequency words (WM ).
We then tested the standard method based on
the cosine similarity (equation 1) on the corpora
C0, CH , C?H , C1 and C2. The results obtained are
displayed in Table 4, and correspond to columns
2-6. They show that the standard approach per-
forms significantly better on the improved corpora
C1/C2 than on the base corpus C0. The overall pre-
cision is increased by 5.3% on C1 (corresponding
to a relative increase of 26%) and 9.5% on C2 (cor-
responding to a relative increase of 51%), even
though the low-frequency words, which dominate
the overall precision, account for a higher pro-
portion in C1 (61.3%) and C2 (61.3%) than in
C0 (56.2%). For the medium and high frequency
words, the precision is increased by over 11% on
C1 and 16% on C2. As pointed out in other stud-
ies, the performance for the low-frequency words
is usually bad due to the lack of context informa-
tion. This explains the relatively small improve-
ment obtained here (only 2.2% on C1 and 6.7%
on C2). It should also be noticed that the perfor-
mance of the standard approach is better on C2
than on C1, which may be due to the fact that C2
is slightly larger than C1 and thus provides more
information or to the actual content of these cor-
pora. Lastly, if we consider the results on the cor-
pus CH which is produced by only choosing the
highly comparable part from C0, the overall preci-
sion is increased by only 1.9%, which might come
from the fact that the size of CH is less than half
the size of C0. We also notice the better results on
CH than on C?H of the same size which consists of
randomly choosing documents from C0.
The results obtained with the refined approach
making use of the comparable document pairs
found in the improved corpus (equation 2) are
also displayed in Table 4 (columns ?C1 new? and
?C2 new?). From these results, one can see that
the overall precision is further improved by 2.0%
on C1 and 2.3% on C2, compared with the stan-
dard approach. For all the low, medium and
high-frequency words, the precision has been im-
proved, which demonstrates that the information
obtained through the corpus enrichment process
contributes to improve the quality of the extracted
bilingual lexicons. Compared with the original
base corpus C0, the overall improvement of the
precision on both C1 and C2 with the refined ap-
proach is significant and important (respectively
corresponding to a relative improvement of 35%
and 62%), which also demonstrates that the effi-
ciency of the refined approach is not confined to a
specific external corpus.
6 Discussion
It is in a way useless to deploy bilingual lexicon
extraction techniques if translation equivalents are
not present in the corpus. This simple fact is at the
basis of our approach which consists in construct-
ing comparable corpora close to the original cor-
pus and which are more likely to contain transla-
tion equivalents as they have a guaranteed degree
of comparability. The pseudo-alignments identi-
fied in the construction process are then used to
leverage state-of-the-art bilingual lexicon extrac-
tion methods. This approach to bilingual lexicon
extraction from comparable corpora radically dif-
fers, to our knowledge, from previous approaches
650
C0 CH C?H C1 C2 C1 new > C1, > C0 C2 new > C2, > C0
WL 0.114 0.144 0.125 0.136 0.181 0.156 2.0%, 4.2% 0.205 2.4%, 9.1%
WM 0.233 0.313 0.270 0.345 0.401 0.369 2.4%, 3.6% 0.433 3.2%, 20.0%
WH 0.417 0.456 0.377 0.568 0.633 0.581 1.3%, 16.4% 0.643 1.0%, 22.6%
All 0.205 0.224 0.189 0.258 0.310 0.278 2.0%, 7.3% 0.333 2.3%, 12.8%
Table 4: Precision of the different approaches on different corpora
which are mainly variants of the standard method
proposed in (Fung and Yee, 1998) and (Rapp,
1999). For example, the method developed in
(De?jean et al, 2002) and (Chiao and Zweigen-
baum, 2002) involves a representation of dictio-
nary entries with context vectors onto which new
words are mapped. Pekar et al (2006) smooth
the context vectors used in the standard approach
in order to better deal with low frequency words.
A nice geometric interpretation of these processes
is proposed in (Gaussier et al, 2004), which fur-
thermore introduces variants based on Fisher ker-
nels, Canonical Correlation Analysis and a com-
bination of them, leading to an improvement of
the F1-score of 2% (from 0.14 to 0.16) when con-
sidering the top 20 candidates. In contrast, the ap-
proach we have developed yields an improvement
of 7% (from 0.13 to 0.20) of the F-1 score on C2,
again considering the top 20 candidates. More im-
portant, however, is the fact that the approach we
have developed can be used in conjunction with
any existing bilingual extraction method, as the
strategies for improving the corpus quality and the
re-weighting formula (equation 2) are general. We
will assess in the future whether substantial gains
are also attained with other methods.
Some studies have tried to extract subparts of
comparable corpora to complement existing par-
allel corpora. Munteanu (2004) thus developed a
maximum entropy classifier aiming at extracting
those sentence pairs which can be deemed paral-
lel. The step for choosing similar document pairs
in this work resembles some of our steps. How-
ever their work focuses on high quality and spe-
cific documents pairs, as opposed to the entire cor-
pus of guaranteed quality we want to build. In
this latter case, the cross-interaction between doc-
uments impacts the overall comparability score,
and new methods, as the one we have introduced,
need to be proposed. Similarly, Munteanu and
Marcu (2006) propose a method to extract sub-
sentential fragments from non-parallel corpora.
Again, the targeted elements are very specific
(parallel sentences or sub-sentences) and limited,
and the focus is put on a few sentences which can
be considered parallel. As already mentioned, we
rather focus here on building a new corpus which
preserves most of the information in the original
corpus. The construction process we have pre-
sented is theoretically justified and allows one to
preserve ca. 95% of the original vocabulary.
7 Conclusion
We have first introduced in this paper a compara-
bility measure based on the expectation of find-
ing translation word pairs in the corpus. We have
then designed a strategy to construct an improved
comparable corpus by (a) extracting a subpart of
the original corpus with a guaranteed compara-
bility level, and (b) by completing the remaining
subpart with external resources, in our case other
existing bilingual corpora. We have then shown
how the information obtained during the construc-
tion process could be used to improve state-of-
the-art bilingual lexicon extraction methods. We
have furthermore assessed the various steps of
our approach and shown: (a) that the compara-
bility measure we introduced captures variations
in the degree of comparability between corpora,
(b) that the construction process we introduced
leads to an improved corpus preserving most of
the original vocabulary, and (c) that the use of
pseudo-alignments through simple re-weighting
yields bilingual lexicons of higher quality.
Acknowledgements
This work was supported by the French National
Research Agency grant ANR-08-CORD-009.
651
References
Ballesteros, Lisa and W. Bruce Croft. 1997. Phrasal
translation and query expansion techniques for
cross-language information retrieval. In Proceed-
ings of the 20th ACM SIGIR, pages 84?91, Philadel-
phia, Pennsylvania, USA.
Chen, Stanley F. 1993. Aligning sentences in bilingual
corpora using lexical information. In Proceedings
of the 31st Annual Conference of the Association for
Computational Linguistics, pages 9?16, Columbus,
Ohio, USA.
Chiao, Yun-Chuang and Pierre Zweigenbaum. 2002.
Looking for candidate translational equivalents in
specialized, comparable corpora. In Proceedings
of the 19th International Conference on Computa-
tional Linguistics, pages 1?7, Taipei, Taiwan.
De?jean, Herve?, Eric Gaussier, and Fatia Sadat. 2002.
An approach based on multilingual thesauri and
model combination for bilingual lexicon extraction.
In Proceedings of the 19th International Conference
on Computational Linguistics, pages 1?7, Taipei,
Taiwan.
Fung, Pascale and Kathleen McKeown. 1997. Finding
terminology translations from non-parallel corpora.
In Proceedings of the 5th Annual Workshop on Very
Large Corpora, pages 192?202, Hong Kong.
Fung, Pascale and Lo Yuen Yee. 1998. An IR ap-
proach for translating new words from nonparallel,
comparable texts. In Proceedings of the 17th inter-
national conference on Computational linguistics,
pages 414?420, Montreal, Quebec, Canada.
Gaussier, E., J.-M. Renders, I. Matveeva, C. Goutte,
and H. De?jean. 2004. A geometric view on bilin-
gual lexicon extraction from comparable corpora.
In Proceedings of the 42nd Annual Meeting of the
Association for Computational Linguistics, pages
526?533, Barcelona, Spain.
Kay, Martin and Martin Ro?scheisen. 1993. Text-
translation alignment. Computational Linguistics,
19(1):121?142.
Koehn, Philipp. 2005. Europarl: A parallel corpus
for statistical machine translation. In Proceedings
of MT Summit 2005.
Melamed, I. Dan. 1997a. A portable algorithm
for mapping bitext correspondence. In Proceed-
ings of the 35th Annual Meeting of the Association
for Computational Linguistics and the 8th Confer-
ence of the European Chapter of the Association for
Computational Linguistics, pages 305?312, Madrid,
Spain.
Melamed, I. Dan. 1997b. A word-to-word model
of translational equivalence. In Proceedings of the
35th Annual Meeting of the Association for Compu-
tational Linguistics and the 8th Conference of the
European Chapter of the Association for Computa-
tional Linguistics, pages 490?497, Madrid, Spain.
Morin, Emmanuel, Be?atrice Daille, Koichi Takeuchi,
and Kyo Kageura. 2007. Bilingual terminology
mining - using brain, not brawn comparable cor-
pora. In Proceedings of the 45th Annual Meeting
of the Association for Computational Linguistics,
pages 664?671, Prague, Czech Republic.
Munteanu, Dragos Stefan and Daniel Marcu. 2006.
Extracting parallel sub-sentential fragments from
non-parallel corpora. In Proceedings of the 21st In-
ternational Conference on Computational Linguis-
tics and the 44th annual meeting of the Association
for Computational Linguistics, pages 81?88, Syd-
ney, Australia.
Munteanu, Dragos Stefan, Alexander Fraser, and
Daniel Marcu. 2004. Improved machine translation
performance via parallel sentence extraction from
comparable corpora. In Proceedings of the HLT-
NAACL 2004, pages 265?272, Boston, MA., USA.
Och, Franz Josef and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Pekar, Viktor, Ruslan Mitkov, Dimitar Blagoev, and
Andrea Mulloni. 2006. Finding translations for
low-frequency words in comparable corpora. Ma-
chine Translation, 20(4):247?266.
Rapp, Reinhard. 1999. Automatic identification of
word translations from unrelated English and Ger-
man corpora. In Proceedings of the 37th Annual
Meeting of the Association for Computational Lin-
guistics, pages 519?526, College Park, Maryland,
USA.
Robitaille, Xavier, Yasuhiro Sasaki, Masatsugu
Tonoike, Satoshi Sato, and Takehito Utsuro. 2006.
Compiling French-Japanese terminologies from the
web. In Proceedings of the 11st Conference of the
European Chapter of the Association for Computa-
tional Linguistics, pages 225?232, Trento, Italy.
Yu, Kun and Junichi Tsujii. 2009. Extracting bilin-
gual dictionary from comparable corpora with de-
pendency heterogeneity. In Proceedings of HLT-
NAACL 2009, pages 121?124, Boulder, Colorado,
USA.
652
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 473?478,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Clustering Comparable Corpora For Bilingual Lexicon Extraction
Bo Li, Eric Gaussier
UJF-Grenoble 1 / CNRS, France
LIG UMR 5217
firstname.lastname@imag.fr
Akiko Aizawa
National Institute of Informatics
Tokyo, Japan
aizawa@nii.ac.jp
Abstract
We study in this paper the problem of enhanc-
ing the comparability of bilingual corpora in
order to improve the quality of bilingual lexi-
cons extracted from comparable corpora. We
introduce a clustering-based approach for en-
hancing corpus comparability which exploits
the homogeneity feature of the corpus, and
finally preserves most of the vocabulary of
the original corpus. Our experiments illus-
trate the well-foundedness of this method and
show that the bilingual lexicons obtained from
the homogeneous corpus are of better quality
than the lexicons obtained with previous ap-
proaches.
1 Introduction
Bilingual lexicons are an important resource in mul-
tilingual natural language processing tasks such as
statistical machine translation (Och and Ney, 2003)
and cross-language information retrieval (Balles-
teros and Croft, 1997). Because it is expensive to
manually build bilingual lexicons adapted to dif-
ferent domains, researchers have tried to automat-
ically extract bilingual lexicons from various cor-
pora. Compared with parallel corpora, it is much
easier to build high-volume comparable corpora, i.e.
corpora consisting of documents in different lan-
guages covering overlapping information. Several
studies have focused on the extraction of bilingual
lexicons from comparable corpora (Fung and McK-
eown, 1997; Fung and Yee, 1998; Rapp, 1999;
De?jean et al, 2002; Gaussier et al, 2004; Robitaille
et al, 2006; Morin et al, 2007; Garera et al, 2009;
Yu and Tsujii, 2009; Shezaf and Rappoport, 2010).
The basic assumption behind most studies on lex-
icon extraction from comparable corpora is a dis-
tributional hypothesis, stating that words which are
translation of each other are likely to appear in simi-
lar context across languages. On top of this hypoth-
esis, researchers have investigated the use of better
representations for word contexts, as well as the use
of different methods for matching words across lan-
guages. These approaches seem to have reached a
plateau in terms of performance. More recently, and
departing from such traditional approaches, we have
proposed in (Li and Gaussier, 2010) an approach
based on improving the comparability of the cor-
pus under consideration, prior to extracting bilingual
lexicons. This approach is interesting since there is
no point in trying to extract lexicons from a corpus
with a low degree of comparability, as the probabil-
ity of finding translations of any given word is low
in such cases. We follow here the same general idea
and aim, in a first step, at improving the compara-
bility of a given corpus while preserving most of
its vocabulary. However, unlike the previous work,
we show here that it is possible to guarantee a cer-
tain degree of homogeneity for the improved corpus,
and that this homogeneity translates into a signifi-
cant improvement of both the quality of the resulting
corpora and the bilingual lexicons extracted.
2 Enhancing Comparable Corpora: A
Clustering Approach
We first introduce in this section the comparability
measure proposed in former work, prior to describ-
ing the clustering-based algorithm to improve the
473
quality of a given comparable corpus. For conve-
nience, the following discussion will be made in the
context of the English-French comparable corpus.
2.1 The Comparability Measure
In order to measure the degree of comparability of
bilingual corpora, we make use of the measure M
developed in (Li and Gaussier, 2010): Given a com-
parable corpus P consisting of an English part Pe
and a French part Pf , the degree of comparability of
P is defined as the expectation of finding the trans-
lation of any given source/target word in the tar-
get/source corpus vocabulary. Let ? be a function
indicating whether a translation from the translation
set Tw of the word w is found in the vocabulary Pv
of a corpus P , i.e.:
?(w,P) =
{
1 iff Tw ? Pv 6= ?
0 else
and letD be a bilingual dictionary withDve denoting
its English vocabulary andDvf its French vocabulary.
The comparability measure M can be written as:
M(Pe,Pf ) (1)
=
?
w?Pe?Dve
?(w,Pf ) +
?
w?Pf?Dvf
?(w,Pe)
#w(Pe ? Dve ) + #w(Pf ? D
v
f )
where #w(P) denotes the number of different
words present in P . One can find from equa-
tion 1 that M directly measures the proportion of
source/target words translated in the target/source
vocabulary of P .
2.2 Clustering Documents for High Quality
Comparable Corpora
If a corpus covers a limited set of topics, it is more
likely to contain consistent information on the words
used (Morin et al, 2007), leading to improved bilin-
gual lexicons extracted with existing algorithms re-
lying on the distributional hypothesis. The term ho-
mogeneity directly refers to this fact, and we will say,
in an informal manner, that a corpus is homogeneous
if it covers a limited set of topics. The rationale for
the algorithm we introduce here to enhance corpus
comparability is precisely based on the concept of
homogeneity. In order to find document sets which
are similar with each other (i.e. homogeneous), it
is natural to resort to clustering techniques. Further-
more, since we need homogeneous corpora for bilin-
gual lexicon extraction, it will be convenient to rely
on techniques which allows one to easily prune less
relevant clusters. To perform all this, we use in this
work a standard hierarchical agglomerative cluster-
ing method.
2.2.1 Bilingual Clustering Algorithm
The overall process retained to build high quality,
homogeneous comparable corpora relies on the fol-
lowing steps:
1. Using the bilingual similarity measure defined
in Section 2.2.2, cluster English and French
documents so as to get bilingual dendrograms
from the original corpus P by grouping docu-
ments with related content;
2. Pick high quality sub-clusters by threshold-
ing the obtained dendrograms according to the
node depth, which retains nodes far from the
roots of the clustering trees;
3. Combine all these sub-clusters to form a new
comparable corpus PH , which thus contains
homogeneous, high-quality subparts;
4. Use again steps (1), (2) and (3) to enrich the
remaining subpart of P (denoted as PL, PL =
P \ PH ) with external resources.
The first three steps aim at extracting the most com-
parable and homogeneous subpart of P . Once this
has been done, one needs to resort to new corpora
if one wants to build an homogeneous corpus with
a high degree of comparability from PL. To do so,
we simply perform, in step (4), the clustering and
thresholding process defined in (1), (2) and (3) on
two comparable corpora: The first one consists of
the English part of PL and the French part of an ex-
ternal corpus PT ; The second one consists of the
French part of PL and the English part of PT . The
two high quality subparts obtained from these two
new comparable corpora in step (4) are then com-
bined with PH to constitute the final comparable
corpus of higher quality.
474
2.2.2 Similarity Measure
Let us assume that we have two document sets (i.e.
clusters) C1 and C2. In the task of bilingual lexi-
con extraction, two document sets are similar to each
other and should be clustered if the combination of
the two can complement the content of each single
set, which relates to the notion of homogeneity. In
other words, both the English part Ce1 of C1 and the
French part Cf1 of C1 should be comparable to their
counterparts (respectively the same for the French
part Cf2 of C2 and the English part C
e
2 of C2). This
leads to the following similarity measure for C1 and
C2:
sim(C1, C2) = ? ?M(Ce1, C
f
2 )+ (1??) ?M(C
e
2, C
f
1 )
where ? (0 ? ? ? 1) is a weight controlling the
importance of the two subparts (Ce1 , C
f
2 ) and (C
e
2 ,
Cf1 ). Intuitively, the larger one, containing more in-
formation, of the two comparable corpora (Ce1 , C
f
2 )
and (Ce2 , C
f
1 ) should dominate the overall similar-
ity sim(C1, C2). Since the content relatedness in the
comparable corpus is basically reflected by the re-
lations between all the possible bilingual document
pairs, we use here the number of document pairs to
represent the scale of the comparable corpus. The
weight ? can thus be defined as the proportion of
possible document pairs in the current comparable
corpus (Ce1 , C
f
2 ) to all the possible document pairs,
which is:
? =
#d(Ce1) ?#d(C
f
2 )
#d(Ce1) ?#d(C
f
2 ) + #d(C
e
2) ?#d(C
f
1 )
where #d(C) stands for the number of documents in
C. However, this measure does not integrate the rel-
ative length of the French and English parts, which
actually impacts the performance of bilingual lexi-
con extraction. If a 1-to-1 constraint is too strong
(i.e. assuming that all clusters should contain the
same number of English and French documents),
having completely unbalanced corpora is also not
desirable. We thus introduce a penalty function ?
aiming at penalizing unbalanced corpora:
?(C) =
1
(1 + log(1 + |#d(C
e)?#d(Cf )|
min(#d(Ce)),#d(Cf ))
)
(2)
The above penalty function leads us to a new simi-
larity measure siml which is the one finally used in
the above algorithm:
siml(C1, C2) = sim(C1, C2) ? ?(C1 ? C2) (3)
3 Experiments and Results
The experiments we have designed in this paper aim
at assessing (a) whether the clustering-based algo-
rithm we have introduced yields corpora of higher
quality in terms of comparability scores, and (b)
whether the bilingual lexicons extracted from such
corpora are of higher quality. Several corpora were
used in our experiments: the TREC1 Associated
Press corpus (AP, English) and the corpora used
in the CLEF2 campaign including the Los Ange-
les Times (LAT94, English), the Glasgow Herald
(GH95, English), Le Monde (MON94, French), SDA
French 94 (SDA94, French) and SDA French 95
(SDA95, French). In addition, two monolingual cor-
pora Wiki-En and Wiki-Fr were built by respectively
retrieving all the articles below the category Society
and Socie?te? from the Wikipedia dump files3. The
bilingual dictionary used in the experiments is con-
structed from an online dictionary. It consists of
33k distinct English words and 28k distinct French
words, constituting 76k translation pairs. In our ex-
periments, we use the method described in this pa-
per, as well as the one in (Li and Gaussier, 2010)
which is the only alternative method to enhance cor-
pus comparability.
3.1 Improving Corpus Quality
In this subsection, the clustering algorithm described
in Section 2.2.1 is employed to improve the quality
of the comparable corpus. The corpora GH95 and
SDA95 are used as the original corpus P0 (56k En-
glish documents and 42k French documents). We
consider two external corpora: P1T (109k English
documents and 87k French documents) consisting of
the corpora LAT94, MON94 and SDA94; P2T (368k
English documents and 378k French documents)
consisting of Wiki-En and Wiki-Fr.
1http://trec.nist.gov
2http://www.clef-campaign.org
3The Wikipedia dump files can be downloaded at
http://download.wikimedia.org. In this paper, we use the En-
glish dump file on July 13, 2009 and the French dump file on
July 7, 2009.
475
P0 P1? P2? P1 P2 P1 > P0 P2 > P0
Precision 0.226 0.277 0.325 0.295 0.461 0.069, 30.5% 0.235, 104.0%
Recall 0.103 0.122 0.145 0.133 0.212 0.030, 29.1% 0.109, 105.8%
Table 1: Performance of the bilingual lexicon extraction from different corpora (best results in bold)
After the clustering process, we obtain the result-
ing corpora P1 (with the external corpus P1T ) and
P2 (with P2T ). As mentioned before, we also used
the method described in (Li and Gaussier, 2010)
on the same data, producing resulting corpora P1?
(with P1T ) and P
2? (with P2T ) from P
0. In terms
of lexical coverage, P1 (resp. P2) covers 97.9%
(resp. 99.0%) of the vocabulary of P0. Hence, most
of the vocabulary of the original corpus has been
preserved. The comparability score of P1 reaches
0.924 and that of P2 is 0.939. Both corpora are
more comparable than P0 of which the comparabil-
ity is 0.881. Furthermore, both P1 and P2 are more
comparable than P1? (comparability 0.912) and P2?
(comparability 0.915), which shows homogeneity is
crucial for comparability. The intrinsic evaluation
shows the efficiency of our approach which can im-
prove the quality of the given corpus while preserv-
ing most of its vocabulary.
3.2 Bilingual Lexicon Extraction Experiments
To extract bilingual lexicons from comparable cor-
pora, we directly use here the method proposed by
Fung and Yee (1998) which has been referred to
as the standard approach in more recent studies
(De?jean et al, 2002; Gaussier et al, 2004; Yu and
Tsujii, 2009). In this approach, each word w is rep-
resented as a context vector consisting of the words
co-occurring with w in a certain window in the cor-
pus. The context vectors in different languages are
then bridged with an existing bilingual dictionary.
Finally, a similarity score is given to any word pair
based on the cosine of their respective context vec-
tors.
3.2.1 Experiment Settings
In order to measure the performance of the lexi-
cons extracted, we follow the common practice by
dividing the bilingual dictionary into 2 parts: 10%
of the English words (3,338 words) together with
their translations are randomly chosen and used as
the evaluation set, the remaining words being used
to compute the similarity of context vectors. En-
glish words not present in Pe or with no translation
in Pf are excluded from the evaluation set. For each
English word in the evaluation set, all the French
words in Pf are then ranked according to their sim-
ilarity with the English word. Precision and recall
are then computed on the first N translation candi-
date lists. The precision amounts in this case to the
proportion of lists containing the correct translation
(in case of multiple translations, a list is deemed to
contain the correct translation as soon as one of the
possible translations is present). The recall is the
proportion of correct translations found in the lists
to all the translations in the corpus. This evaluation
procedure has been used in previous studies and is
now standard.
3.2.2 Results and Analysis
In a first series of experiments, bilingual lexicons
were extracted from the corpora obtained by our ap-
proach (P1 and P2), the corpora obtained by the
approach described in (Li and Gaussier, 2010) (P1?
and P2?) and the original corpus P0, with the fixed
N value set to 20. Table 1 displays the results ob-
tained. Each of the last two columns ?P1 > P0?
and ?P2 > P0? contains the absolute and the rel-
ative difference (in %) w.r.t. P0. As one can note,
the best results (in bold) are obtained from the cor-
pora P2 built with the method we have described in
this paper. The lexicons extracted from the enhanced
corpora are of much higher quality than the ones ob-
tained from the original corpus . For instance, the
increase of the precision is 6.9% (30.5% relatively)
in P1 and 23.5% (104.0% relatively) in P2, com-
pared with P0. The difference is more remarkable
withP2, which is obtained from a large external cor-
pus P2T . Intuitively, one can expect to find, in larger
corpora, more documents related to a given corpus,
an intuition which seems to be confirmed by our re-
sults. One can also notice, by comparing P2 and
P2? as well as P1 and P1?, a remarkable improve-
ment when considering our approach and the early
476
methodology.
Intuitively, the value N plays an important role
in the above experiments. In a second series of ex-
periments, we let N vary from 1 to 300 and plot the
results obtained with different evaluation measure in
Figure 1. In Figure 1(a) (resp. Figure 1(b)), the x-
axis corresponds to the values taken by N, and the y-
axis to the precision (resp. recall) scores for the lexi-
cons extracted on each of the 5 corporaP0,P1?,P2?,
P1 and P2. A clear fact from the figure is that both
the precision and the recall scores increase accord-
ing to the increase of the N values, which coincides
with our intuition. As one can note, our method con-
sistently outperforms the previous work and also the
original corpus on all the values considered for N .
0 100 200 300
0.0
0.2
0.4
0.6
0.8
N
Precision
P
2
P
2'
P
1
P
1'
P
0
(a) Precision
0 100 200 300
0.0
0.1
0.2
0.3
0.4
N
Recall
P
2
P
2'
P
1
P
1'
P
0
(b) Recall
Figure 1: Performance of bilingual lexicon extraction
from different corpora with varied N values from 1 to
300. The five lines from the top down in each subfigure
are corresponding to the results for P2, P2?, P1, P1? and
P0 respectively.
4 Discussion
As previous studies on bilingual lexicon extrac-
tion from comparable corpora radically differ on
resources used and technical choices, it is very
difficult to compare them in a unified framework
(Laroche and Langlais, 2010). We compare in this
section our method with some ones in the same vein
(i.e. enhancing bilingual corpora prior to extract-
ing bilingual lexicons from them). Some works like
(Munteanu et al, 2004) and (Munteanu and Marcu,
2006) propose methods to extract parallel fragments
from comparable corpora. However, their approach
only focuses on a very small part of the original cor-
pus, whereas our work aims at preserving most of
the vocabulary of the original corpus.
We have followed here the general approach in
(Li and Gaussier, 2010) which consists in enhancing
the quality of a comparable corpus prior to extract-
ing information from it. However, despite this latter
work, we have shown here a method which ensures
homogeneity of the obtained corpus, and which fi-
nally leads to comparable corpora of higher quality.
In turn such corpora yield better bilingual lexicons
extracted.
Acknowledgements
This work was supported by the French National Re-
search Agency grant ANR-08-CORD-009.
References
Lisa Ballesteros and W. Bruce Croft. 1997. Phrasal
translation and query expansion techniques for cross-
language information retrieval. In Proceedings of the
20th ACM SIGIR, pages 84?91, Philadelphia, Pennsyl-
vania, USA.
Herve? De?jean, Eric Gaussier, and Fatia Sadat. 2002.
An approach based on multilingual thesauri and model
combination for bilingual lexicon extraction. In Pro-
ceedings of the 19th International Conference on
Computational Linguistics, pages 1?7, Taipei, Taiwan.
Pascale Fung and Kathleen McKeown. 1997. Find-
ing terminology translations from non-parallel cor-
pora. In Proceedings of the 5th Annual Workshop on
Very Large Corpora, pages 192?202, Hong Kong.
Pascale Fung and Lo Yuen Yee. 1998. An IR approach
for translating new words from nonparallel, compara-
ble texts. In Proceedings of the 17th international con-
477
ference on Computational linguistics, pages 414?420,
Montreal, Quebec, Canada.
Nikesh Garera, Chris Callison-Burch, and David
Yarowsky. 2009. Improving translation lexicon induc-
tion from monolingual corpora via dependency con-
texts and part-of-speech equivalences. In CoNLL 09:
Proceedings of the Thirteenth Conference on Compu-
tational Natural Language Learning, pages 129?137,
Boulder, Colorado.
E. Gaussier, J.-M. Renders, I. Matveeva, C. Goutte, and
H. De?jean. 2004. A geometric view on bilingual
lexicon extraction from comparable corpora. In Pro-
ceedings of the 42nd Annual Meeting of the Associ-
ation for Computational Linguistics, pages 526?533,
Barcelona, Spain.
Audrey Laroche and Philippe Langlais. 2010. Revisiting
context-based projection methods for term-translation
spotting in comparable corpora. In Proceedings of
the 23rd International Conference on Computational
Linguistics (Coling 2010), pages 617?625, Beijing,
China, August.
Bo Li and Eric Gaussier. 2010. Improving corpus
comparability for bilingual lexicon extraction from
comparable corpora. In Proceedings of the 23rd In-
ternational Conference on Computational Linguistics,
pages 644?652, Beijing, China.
Emmanuel Morin, Be?atrice Daille, Koichi Takeuchi, and
Kyo Kageura. 2007. Bilingual terminology mining -
using brain, not brawn comparable corpora. In Pro-
ceedings of the 45th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 664?671,
Prague, Czech Republic.
Dragos Stefan Munteanu and Daniel Marcu. 2006. Ex-
tracting parallel sub-sentential fragments from non-
parallel corpora. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
the 44th annual meeting of the Association for Compu-
tational Linguistics, pages 81?88, Sydney, Australia.
Dragos Stefan Munteanu, Alexander Fraser, and Daniel
Marcu. 2004. Improved machine translation perfor-
mance via parallel sentence extraction from compara-
ble corpora. In Proceedings of the HLT-NAACL 2004,
pages 265?272, Boston, MA., USA.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Reinhard Rapp. 1999. Automatic identification of word
translations from unrelated English and German cor-
pora. In Proceedings of the 37th Annual Meeting of
the Association for Computational Linguistics, pages
519?526, College Park, Maryland, USA.
Xavier Robitaille, Yasuhiro Sasaki, Masatsugu Tonoike,
Satoshi Sato, and Takehito Utsuro. 2006. Compil-
ing French-Japanese terminologies from the web. In
Proceedings of the 11st Conference of the European
Chapter of the Association for Computational Linguis-
tics, pages 225?232, Trento, Italy.
Daphna Shezaf and Ari Rappoport. 2010. Bilingual lex-
icon generation using non-aligned signatures. In Pro-
ceedings of the 48th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 98?107, Up-
psala, Sweden.
Kun Yu and Junichi Tsujii. 2009. Extracting bilingual
dictionary from comparable corpora with dependency
heterogeneity. In Proceedings of HLT-NAACL 2009,
pages 121?124, Boulder, Colorado, USA.
478
