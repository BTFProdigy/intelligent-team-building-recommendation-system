Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1803?1812, Dublin, Ireland, August 23-29 2014.
Situated Incremental Natural Language Understanding using a
Multimodal, Linguistically-driven Update Model
Casey Kennington
CITEC, Bielefeld University
ckennington
1
Spyros Kousidis
Bielefeld University
spyros.kousidis
2
1
@cit-ec.uni-bielefeld.de
2
@uni-bielefeld.de
David Schlangen
Bielefeld University
david.schlangen
2
Abstract
A common site of language use is interactive dialogue between two people situated together in
shared time and space. In this paper, we present a statistical model for understanding natural
human language that works incrementally (i.e., does not wait until the end of an utterance to
begin processing), and is grounded by linking semantic entities with objects in a shared space.
We describe our model, show how a semantic meaning representation is grounded with properties
of real-world objects, and further show that it can ground with embodied, interactive cues such
as pointing gestures or eye gaze.
1 Introduction
Dialogue between co-located participants is possibly the most common form of language use (Clark,
1996). It is highly interactive (time is shared between two participants), interlocutors can refer to ob-
jects in their visual field (space is also shared), and visual cues such as gaze or pointing gestures often
play a role (shared time and space). Most computational dialogue research focuses only one of these
constraints.
In this paper, we present a model that processes incrementally (i.e., can potentially work interactively),
can make use of the visual world by symbolically representing objects in a scene, and incorporate gaze
and gestures. The model can learn from conversational data and can potentially be used in an application
for a situated dialogue system, such as an autonomous robot.
In the following section we will provide background and present related work. That will be followed
by a description of the task and the model. In Section 4 we will show how our model performs in two
experiments, the first uses speech and a visual scene, the second incorporates visual cues.
2 Background and Related Work
2.1 Background: Incremental Dialogue Processing
Dialogue systems that process incrementally produce behavior that is perceived by human users to be
more natural than systems that use a turn-based approach (Aist et al., 2006; Skantze and Schlangen, 2009;
Skantze and Hjalmarsson, 2010). Incremental dialogue has seen improvements in speech recognition
(Baumann et al., 2009), speech synthesis (Buschmeier et al., 2012), and dialogue management (Bu? et
al., 2010; Selfridge et al., 2012). Futhermore, architectures for incremental dialogue systems have been
proposed (Schlangen and Skantze, 2009; Schlangen and Skantze, 2011) and incremental toolkits are also
available (Baumann and Schlangen, 2012).
In this paper, we approach natural language understanding (NLU), which aims to map an utterance to
an intention, as a component in the incremental model of dialogue processing as described in (Schlangen
and Skantze, 2011; Schlangen and Skantze, 2009), where incremental systems consist of a network of
processing modules. Each module has a left buffer and a right buffer, where a typical module takes input
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1803
Figure 1: Example of an IU network composed of words, parts of speech (POS), a semtic representation
(Robust Minimal Recursion Semantics; RMRS), and NLU modules. Solid arrows represent GRIN links
and the dotted lines represent SLLs. The utterance take the red cross is represented as word IUs, which
are GRIN by the part of speech tags, phrase-structure parse, semantic representation, and the intention.
Note that red and cross are GRIN by the same syntactic IU, which in turn is GRIN by two semantic IUs.
Succeeding levels of IUs are shifted slightly to the right, representing a processing delay. The X14 slot
in the bolded NLU frame refers to the cross-shaped object in the game board on the right.
from its left buffer, performs some kind of processing on that data, and places the processed result onto
its right buffer. The data are packaged as the payload of incremental units (IU) which are passed between
modules. The IUs themselves are also interconnected via so-called same level links (SLL) and grounded-
in links (GRIN), the former allowing the linking of IUs as a growing sequence, the latter allowing that
sequence to convey what IUs directly affect them. See Figure 1 for an example; each layer represents a
module in the IU-module network and each node is an IU in the IU network. The focus of this paper is
the top layer (module), but how it is produced depends on the layers below it.
2.2 Related Work
The work presented in this paper connects and extends recent work in grounded semantics (Roy, 2005;
Hsiao et al., 2008; Liu et al., 2012; Chai et al., 2014), which aims to connect language with the world,
but typically does not work incrementally; semantic parsing / statistical natural language understanding
via logical forms (Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009), dependency-based
compositional semantics (Liang et al., 2011), neural networks (Huang and Er, 2010), Markov Logic
Networks (Meurs et al., 2008; Meza-Ruiz et al., 2008), and Dynamic Bayesian Networks (Meurs et al.,
2009); see also overviews of NLU in (De Mori et al., 2008; Tur and De Mori, 2011), but typically neither
provide situated interpretations nor incremental specifications of the representations; incremental NLU
(DeVault et al., 2009; DeVault et al., 2011; Aist et al., 2007; Schlangen and Skantze, 2009), which
focuses on incrementality, but not on situational grounding; as well as integration of gaze into language
understanding (Prasov and Chai, 2010).
We move beyond this work in that we present a model that is incremental, uses a form of grounded se-
mantics, can easily incorporate multi-modal information sources, and which inference can be performed
quickly, satisfying the demands of real-time dialogue.
3 Task and Model
3.1 Task
The task for our model is as follows: to compute at any moment a distribution over possible intentions
which the speaker wanted to convey in the utterance, expressed as semantic frames, given the unfolding
utterance and information about the state of the world in which the utterance is happening. The slots of
these frames are to be filled with semantic constants, that is, they are uniquely resolved, if appropriate,
to objects in the shared environment. This is illustrated in Figure 1 where the words of the utterance give
1804
rise to the part-of-speech tags, the incrementally growing syntax, semantic representation, and, finally,
the intention. Note how x14 in the bolded NLU frame resolves to an object identifier for a real object in
the shared scene (red cross in the bottom-left of the game board shown on the right in the figure).
3.2 Model
Kennington et al., (2013) presented a simple, incremental model of NLU, which is an update model
(i.e., increments build on previous ones) and which can potentially work in real time and in situated
environments. The goal of the model is to recover I , the intention of the speaker behind the utterance,
word by word. We observe U , the current word (or in this paper, a semantic meaning representation,
see below) and an unobserved mediating variable R which represents visual or abstract properties of the
object of the intention. Formally, we are interested in P (I|U), the probability of a certain intention I
underlying utterance U . We assume a latent variable R (pRoperties of entities in the world), and build
a generative model (that is, model the joint P (I,R, U)). Going from P (I,R|U) and making certain
independence assumptions, we arrive at
P (I|U) =
P (I)
P (U)
?
r?R
P (U |R = r)P (R = r|I) (1)
That is, we assume that R is only conditional on I , and U is only conditional on R, and we can move
P (I) and P (U) out of the summation, as they do not depend on R. This is an update model in the usual
sense that the posterior (P (I|U)) at one step becomes the prior (P (I)) at the next. P (R|I) provides the
link between the intentions and the properties.
Another variant of the model which we will use in this paper is as follows: we rewrite P (U |R) using
Bayes? rule, which cancels P (U) and introduces P (R) into the summation, but P (R) can be dropped
since (in this work) it can be approximated with a uniform distribution, yielding:
P (I|U) = P (I)
?
r?R
P (R = r|U)P (R = r|I) (2)
There are, however, three important differences between the realisation of our model and the one
presented in Kennington et al., (2013), all of which are a direct result of replacing, as we do here, the n-
gram model represented by P (U |R) with output from a parser that produces a Robust Minimal Recursion
Semantics (RMRS) semantic representation (Copestake, 2007). Such a representation provides our model
with a structured way to abstract over the surface forms. We will first give a brief explanation of the
RMRS framework, then describe each of the three differences between our model and that of Kennington
et al., (2013), namely (1) how the language grounds with the world, (2) how the frame is built, and (3)
when to consider evidence for the slots in the frame.
RMRS RMRS is a framework for representing semantics that factors a logical form into elementary
predicates (EP). For example in Table 1, the first row represents the first word of an utterance, take, and
the corresponding RMRS representation; the EPs take and addressee are produced. The EPs in this exam-
ple have anchor variables and in most cases, an EP has an argument entity. Relations between EPs can be
expressed via argument relations, e.g., for take in the table, there is an ARG1 relation, denoting addressee
as the first argument of the predicate take. Other relations include ARG2 and BV (relating determiners to
the words they modify). A full example of an utterance and corresponding RMRS representation can be
found in Table 1, where each row in the word column makes up the words of the example utterance.
In this paper we are interested in processing utterances incrementally. As argued in Peldzsus et al.,
(2012), RMRS is amenable to incremental processing by allowing for underspecification in how relations
are represented (RMRS can also underspecify scope, but we don?t consider that here). Table 1 has an
example of an underspecified relation: when the second word the is uttered, the RMRS segment predicts
that the entity represented by x14 will be the ARG2 relation of the EP for take, but the actual word that
1805
word RMRS segment
take a7 : addressee(x8), a1 : take(e2), ARG1(a1, x8)
the a13 : def(), ARG2(a1, x14), BV (a13, x14)
red a33 : red(e34), ARG1(a33, x14)
cross a19 : cross(x14)
next to a49 : next(e50), ARG1(a49, x14), ARG2(a49, x53)
the a52 : def(), BV (a52, x53)
blue a72 : blue(e73), ARG1(a72, x53)
piece a58 : piece(x53)
Table 1: Example RMRS representation for the utterance take the red cross next to the blue piece. Each
row represents an increment of the utterance.
produces the EP that has x14 as an argument has not yet been uttered. Each row in the table represents
what we would want an RMRS parser to produce for our model at each word increment.
A more detailed explanation of RMRS can be found in Copestake (2007). We will now discuss the
three key differences of our model with that of previous work.
(1) Grounding Semantics with the Visual World In Kennington et al., (2013), the utterance was
represented via n-grams, which was used to ground with the world. Here, we ground RMRS structures
with the world. For example, Figure 1 shows which words produced which RMRS increments; our model
learns the co-occurances between those increments and properties of objects (real properties such as
colors, shapes, and spatial placements, or abstract properties; e.g., take is a property of the action take).
(2) Building the Frame In this paper, intentions are represented as frames. However, unlike Kenning-
ton et al., (2013), we don?t assume beforehand that we know the slots of the frame. To determine the
slots, we turn again to RMRS and build a slot for each entity that is produced (more on this below). This
kind of frame, coupled with the RMRS representation, shows not just a meaning representation, but also
interpretation of the representation in the current model (the real situation / visual domain of discourse),
outputted incrementally making our model fully incremental in the sense of Heintze et al., (2010). The
final, bolded NLU frame in Figure 1 shows the addressee (in this case, the dialogue system) as the recip-
ient of the request, the request itself is a take request, where the object to be taken is obj5, as indexed
by the real world, and that object happens to be red (i.e., e12 represents the notion of redness).
(3) Driven by Sematics Another important difference is when to consider the semantic evidence and
when to ignore it, in terms of when to apply the model for interpretation of the slots. In Kennington et
al., (2013), each slot in the frame was processed at each increment in the entire utterance, regardless of
whether n-grams in that segment contributed to the interpretation of that slot. In our approach, again,
we turn to RMRS. At each word increment, RMRS produces a corresponding, underspecified semantic
meaning represenation which is added to at the next increment. Our model takes the new information
and only attempts to process the interpretation for those ?active? entities. For example, by the time red is
uttered in Figure 1, the processing for entities x8, e2, and e12 is complete, but the processing for x14
is under way, and active as long as x14 is referenced as an entity in the RMRS increment.
With these important extensions, our model of NLU is highly driven by the semantic meaning repre-
sentation that is being built incrementally for the utterance. We will now show through two experiments
how our approach improves upon previous work.
4 Experiments
Similar to Kennington et al., (2013), we use the model represented formally in Equation 2, where
P (R|U) is realised using a maximum entropy classifier (ME) that predicts properties from RMRS evi-
dence.
1
We use the German RMRS parser described in Peldszus et al (2012), Peldszus and Schlangen
(2012) which is a top-down PCFG parser that builds RMRS structure incrementally with the parse.
We train an individual model for each RMRS entity type (e.g., e and x), where the features are the
entity type, relations, and predicates of an RMRS increment and the class label are the visual properties.
1
http://opennlp.apache.org/
1806
The RMRS representations are not checked for accuracy (i.e., they do not represent ground truth); we use
the top-predicted output of the RMRS parser explained in Peldszus et al (2012).
4.1 Pento Puzzle with Speech
Figure 2: Example Pen-
tomino Board
?
?
?
ACTION rotate
OBJECT obj4
RESULT clockwise
?
?
?
Figure 3: Pento gold frame ex-
ample
?
?
?
?
X8 addr
E2 rotate
X14 obj4
E21 clockwise
?
?
?
?
Figure 4: Pento frame example
from our model
Data and Task The Pentomino domain (Fern?andez et al., 2007) contains task-oriented conversational
data which has been used in several situated dialogue studies (Heintze et al., 2010; Peldszus et al., 2012;
Kennington and Schlangen, 2012; Kennington et al., 2013). This corpus was collected in a Wizard-of-Oz
study, where the user goal was to instruct the computer to pick up, delete, rotate or mirror puzzle tiles on
a rectangular board (as in Figure 2), and place them onto another board. For each utterance, the corpus
records the state of the game board before the utterance, the immediately preceding system action, and
the intended interpretation of the utterance (as understood by the Wizard) in the form of a semantic frame
specifying action-type and arguments, where those arguments are objects occurring in the description of
the state of the board. The language of the corpus is German. See Figure 2 for a sample source board,
and Figure 3 for an annotated frame.
The task that we want our model to perform is as follows: given information about the state of the
world (i.e., game board), previous system action, and the ongoing utterance, incrementally build the
frame by providing the interpretation of each RMRS entity, represented as a distribution over all possible
interpretations for that entity (i.e., domain of discourse).
Procedure To make our work comparable to previous work, results were obtained by averaging the
results of a 10-fold validation on 1489 Pento boards (i.e., utterances+context, as in (Kennington and
Schlangen, 2012)). We used a separate set of 168 boards for small-scale, held-out experiments. For
incremental processing, we used INPROTK.
2
We calculate accuracies by comparing against a gold frame,
with assumptions. We check to see if the slot values (3 slots in total) exist in the frame our model
produces. If a gold slot value exists in any slot produced by our model, it is counted as correct (it is
difficult to tell which slot from our model?s frame maps to which slot in the gold frame, we leave this for
future work). A fully correct frame would contain all three values. For example, each of the values for the
gold slots in Figure 3 exist in the example frame our model would produce in Figure 4, marking each gold
slot as correct, and the entire frame as correct since all three were correct together. To directly compare
with previous work, we will use the gold slot names action, object, and result in the Results
section. We perform training and evaluation on hand-transcribed data and on automatically transcribed
data, using the incremental speech recogniser (Sphinx4) in InproTK. We report results on sentence-level
and incremental evaluations.
On the incremental level, we followed previously used metrics for evaluation:
first correct: how deep into the utterance do we make the first correct guess?
first final: how deep into the utterance do we make the correct guess, without subsequent changes?
edit overhead: what is the ratio of unnecessary edits / sentence length, where the only necessary edit is
the first prediction for an entity?
Results Figure 5 shows the results of our evaluation in graph and table form. As expected, our model
dramatically improved the result value, which generally is verbally represented towards the end of
2
https://bitbucket.org/inpro/inprotk
1807
ME+RMRS ME+NGRAMS MLN P
frame 78.75 74.08 74.76
(63.0) (67.2) (61.2)
action 92.11 93.62 92.62
object 90.44 90.79 84.71 64.3
result 94.0 82.34 86.65
Figure 5: Comparison of accuracies in Pento using the model presented here ME+RMRS, (Kennington
et al., 2013) ME+NGRAMS, (Kennington and Schlangen, 2012) MLN, (Peldszus et al., 2012) P; paren-
theses denote results from automatically transcribed speech. Bolded values represent the highest values
for that row. Note that the column chart begins at 60%. The chart and table show the same information.
an utterance. This resulted in a dramatic increase in frame accuracy (a somewhat strict metric). Our
model fares better than previous work using speech (in parentheses in the figure), but is outperformed by
the n-gram approach. These results are encouraging, however we leave improvements on automatically
transcribed speech to future work.
Incremental Table 2 shows the incremental results of Kennington et al.,(2013), and Table 3 shows
our results. Utterances are binned into short, normal, and long utterance lengths (1-6, 7-8, 9-17 words,
respectively; 7-8 word utterances were the most represented). Previous work processed all three slots
throughout the ongoing utterance, whereas the model presented here only processed entities (that could
give rise to these slots) as dictated by the RMRS. This causes a later overall first correct, but an overall
earlier first final, with a much narrower window between them. This represents an ideal system that waits
for processing a slot until it needs to, but comes to a final decision quickly, without changing its mind
later. This is further evidenced by the edit overhead which is lower here than previous work. This has
implications in real-time systems that need to define operating points; i.e., a dialogue system would need
to wait for specific information before making a decision.
action 1-6 7-8 9-14
first correct (% into utt.) 5.78 2.56 3.64
first final (% into utt.) 38.26 36.10 30.84
edit overhead 2.37
object 1-6 7-8 9-14
first correct (% into utt.) 7.39 7.5 10.11
first final (% into utt.) 44.7 44.18 35.55
edit overhead 4.6
result 1-6 7-8 9-14
first correct (% into utt.) 15.16 23.23 20.88
first final (% into utt.) 42.55 40.57 35.21
edit overhead 10.19
Table 2: Incremental Results for Pento slots with
varying sentence lengths, Kennington et al.,(2013),
Edit overhead represents all lengths of utterances.
action 1-6 7-8 9-14
first correct (% into utt.) 12.03 7.8 12.59
first final (% into utt.) 37.84 26.02 24.11
edit overhead 1.57
object 1-6 7-8 9-14
first correct (% into utt.) 30.64 17.66 14.46
first final (% into utt.) 32.27 19.20 15.79
edit overhead 3.1
result 1-6 7-8 9-14
first correct (% into utt.) 59.72 54.50 48.94
first final (% into utt.) 62.80 64.13 60.72
edit overhead 7.71
Table 3: Incremental Results for Pento slots with
varying sentence lengths, current work. Edit over-
head represents all lengths of utterances.
4.2 Pento Puzzle with Speech, Gaze, and Deixis
Data and Task The second experiment uses data also from the Pentomino domain, as described in
(Kousidis et al., 2013; Kennington et al., 2013), also a Wizard-of-Oz study consisting of 7 participants,
example in Figure 1. The user was to select a puzzle tile (out of a possible 15) on a game board shown
on a large monitor, and then describe this piece to the ?system? (wizard). Speech, eye gaze (tracked by
Seeingmachines FaceLab) and pointing gestures (tracked by Microsoft Kinect) were recorded. After the
participant uttered a confirmation, the wizard began a new episode, generating a new random board and
1808
the process repeated.
The task for the NLU in this experiment was reference resolution. The information available to our
model for these data included the utterance (hand-transcribed) the visual context (game board), gaze
information, and deixis (pointing) information, where a rule-based classifier predicted from the motion
capture data the quadrant of the screen at which the participant was pointing. These data were very noisy
(and hence, realistic) despite the constrained conditions of the task; the participants were not required to
say things a certain way (as long as it was understood by the wizard), their hand movements potentially
covered their faces which interfered with the eye tracker, and each participant had a different way of
pointing (e.g., different gesture space, handedness, distance of hand from body when pointing, alignment
of hand with face, etc.).
Procedure Removing the utterances which were flagged by the wizard (i.e., when the wizard mis-
understood the participant) and the utterances of one of the participants (who had misunderstood the
task) left a total of 1051 utterances. We used 951 for development and training the model, and 100 for
evaluation. We give results as resolution accuracy. All models were trained on hand-transcribed data,
but two evaluations were performed: one with hand-transcribed data, and one with speech automatically
transcribed by the Google Web Speech API.
3
Gaze and deixis are incorporated by incrementally com-
puting properties to be provided to our NLU model; i.e., a tile has a property in R of being gazed at
if it is gazed at for some interval of time, or tiles in a quadrant of the screen have the property of being
pointed at. Figure 6 shows an example utterance, gaze, and gesture activity over time and how they
are reflected in the model. Our baseline model is the NLU without using gaze or deixis information;
random accuracy is 7%. We will compare our model with that of an NGRAM (up to trigram) model in the
evaluations, for each of the conditions (baseline, deixis, gaze, deixis and gaze).
We also include the percentage of the time the gold tile is in the top 2 and top 4 rankings (out of 15);
situations in which a dialogue system could at least provide alternatives in a clarification request (if it
could detect that it should have low confidence in the best prediction; which we didn?t investigate here).
For gaze, we also make the naive assumption that over the utterance the participant (who in this case is
the speaker) will gaze at his chosen intended tile most of the time.
Figure 6: Human activity (top) aligned with how modalities are reflected in the model for Gaze and Point
(bottom) over time for example utterance: take the yellow t from this group here. The intervals of the
properties are denoted by square brackets.
Results Table 4 shows the results of our evaluation. Overall, the model that uses RMRS outperforms
the model that uses NGRAMS under all conditions using hand-transcribed data. The results for speech tell
a different story; speech with NGRAMS is generally better ? an effect of the model here relying on parser
output. Overall, both model types increase performance when using hand-transcribed or automatically-
transcribed speech when incorporating other modalities, particularly pointing. Furthermore, the Top 2
and Top 4 columns show that this model has an overall good distribution, especially in the case of RMRS
and pointing, where the target object is in the top four ranks 90% of the time. This would allow a real-
time system to ask a specific clarification request to the human, with a high confidence that the object is
among the top four ranking objects.
Incremental For further incremental results, Figure 7 shows the rank of each object on an example
board using our baseline model for the utterance nimm das rote untere kreuz (take the red below cross /
3
The Web Speech API Specificiation: https://dvcs.w3.org/hg/speech-api/raw-file/tip/
speechapi.html
1809
NLU Acc Top 2 Top 4
NGRAMS 68% 83% 87%
(speech) NGRAMS 44% 57% 69%
RMRS 73% 82% 88%
(speech) RMRS 36% 54% 66%
NLU + Pointing Acc Top 2 Top 4
NGRAMS 70% 83% 88%
(speech) NGRAMS 46% 60% 72%
RMRS 78% 85% 90%
(speech) RMRS 40% 56% 73%
NLU + Gaze Acc Top 2 Top 4
NGRAMS 68% 84% 88%
(speech) NGRAMS 43% 59% 71%
RMRS 74% 81% 88%
(speech) RMRS 39% 54% 67%
NLU + Gaze + Point Acc Top Top
NGRAMS 70% 84% 87%
(speech) NGRAMS 45% 61% 65%
RMRS 77% 85% 89%
(speech) RMRS 41% 56% 74%
Table 4: Results for Experiment 2. The highest scores for each column are in bold. Four evaluations are
compared under four different settings; Acc denotes accuracy (referent in top position), Top 2 and Top
4 respectively show the percentage of time the referent was between those ranks and the top.
take the red cross below). Once das (the) is uttered, RMRS makes an X entity and the model begins to
interpret. The initial distribution appears to be quite random as das does not have high co-occurence with
any particular object property. Once rote (red) is uttered, all non-red objects fall to the lowest ranks in
the distribution. Once untere (under / below) is uttered, all of the red pieces in the bottom two quadrants
increase overall in rank. Finally, as kreuz (cross) is uttered, the two crosses receive the highest ranks,
the bottom one being the highest rank and intended object. Note the rank of the cross in the top left
quadrant over time; it began with a fairly high rank, which moved lower once untere was uttered, then
moved into second rank once kreuz was uttered. As the utterance progresses the rank of the intended
object decreases, showing that our model predicted the correct piece at the appropriate word.
... das rote untere kreuz
Figure 7: Example of reference resolution for the utterance: nimm das rote untere kreuz / take the red
below cross; objects are annotated with their rank in the distribution as outputed by the NLU model at
each increment. The board size has been adjusted for formatting purposes.
5 Discussion and Conclusions
We have presented a model of NLU that uses a semantic representation to recover the intention of a
speaker utterance. Our model is general in that it doesn?t fit a template or ontology like other NLU ap-
proaches (though we would need to determine how a dialogue manager would make use of such a frame),
and grounds the semantic representation with a symbolic representation of the visual world. It works in-
crementally and can incorporate other modalities incrementally. It improves overall upon previous work
that used a similar model, but relied on n-grams. Our model implicitely handles complex utterances that
use spatial language. However, we leave important aspects, such as negation in an utterance, to future
work (they were not very common in our data).
The experiments in this paper were done off-line, but we have a real-time system currently working.
Our model incorporates in real-time the gesture and gaze information as it is picked up by the sensors,
as well as the speech of the user. We leave a full evaluation using this interactive setup with human
participants for future work.
Acknowledgements Thanks to the anonymous reviewers for their useful comments.
1810
References
Gregory Aist, James Allen, Ellen Campana, Lucian Galescu, Carlos A Gomez Gallo, Scott Stoness, Mary Swift,
and Michael Tanenhaus. 2006. Software architectures for incremental understanding of human speech. In
Proceedings of InterspeechICSLP.
Gregory Aist, James Allen, Ellen Campana, Carlos Gomez Gallo, Scott Stoness, Mary Swift, and Michael K
Tanenhaus. 2007. Incremental understanding in human-computer dialogue and experimental evidence for
advantages over nonincremental methods. In Proceedings of Decalog (Semdial 2007), Trento, Italy.
Timo Baumann and David Schlangen. 2012. The InproTK 2012 Release. In NAACL.
Timo Baumann, Michaela Atterer, and David Schlangen. 2009. Assessing and Improving the Performance of
Speech Recognition for Incremental Systems. In Proceedings of NAACL-HLT 2009, Boulder, USA, June.
Hendrik Buschmeier, Timo Baumann, Benjamin Dosch, Stefan Kopp, and David Schlangen. 2012. Combining
Incremental Language Generation and Incremental Speech Synthesis for Adaptive Information Presentation.
In Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages
295?303, Seoul, South Korea, July. Association for Computational Linguistics.
Okko Bu? Timo Baumann, and David Schlangen. 2010. Collaborating on Utterances with a Spoken Dialogue
System Using an ISU-based Approach to Incremental Dialogue Management. In Proceedings of the SIGdial
2010 Conference, pages 233?236, Tokyo, Japan, September.
Joyce Y Chai, Lanbo She, Rui Fang, Spencer Ottarson, Cody Littley, Changsong Liu, and Kenneth Hanson. 2014.
Collaborative Effort towards Common Ground in Situated Human-Robot Dialogue. In HRI?14, pages 33?40,
Bielefeld, Germany.
Herbert H Clark. 1996. Using Language. Cambridge University Press.
Ann Copestake. 2007. Semantic composition with (robust) minimal recursion semantics. In Proceedings of
the Workshop on Deep Linguistic Processing - DeepLP ?07, page 73, Morristown, NJ, USA. Association for
Computational Linguistics.
Renato De Mori, Frederic B?echet, Dilek Hakkani-t?ur, Michael Mctear, Giuseppe Riccardi, and Gokhan Tur. 2008.
Spoken Language Understanding. IEEE Signal Processing Magazine, (May):50?58, May.
David DeVault, Kenji Sagae, and David Traum. 2009. Can I finish?: learning when to respond to incremental
interpretation results in interactive dialogue. In Proceedings of the 10th SIGdial, number September, pages
11?20. Association for Computational Linguistics.
David DeVault, Kenji Sagae, and David Traum. 2011. Incremental Interpretation and Prediction of Utterance
Meaning for Interactive Dialogue. Dialogue & Discourse, 2(1):143?170.
Raquel Fern?andez, Tatjana Lucht, and David Schlangen. 2007. Referring under restricted interactivity conditions.
In Proceedings of the 8th SIGdial Workshop on Discourse and Dialogue, pages 136?139.
Silvan Heintze, Timo Baumann, and David Schlangen. 2010. Comparing local and sequential models for statistical
incremental natural language understanding. In Proceedings of the 11th Annual Meeting of the Special Interest
Group on Discourse and Dialogue, pages 9?16. Association for Computational Linguistics.
Kai-yuh Hsiao, Soroush Vosoughi, Stefanie Tellex, Rony Kubat, and Deb Roy. 2008. Object schemas for ground-
ing language in a responsive robot. Connection Science2, 20(4):253?276.
Guangpu Huang and Meng Joo Er. 2010. A Hybrid Computational Model for Spoken Language Understanding.
In 11th International Conference on Control, Automation, Robotics, and Vision, number December, pages 7?10,
Singapore. IEEE.
Casey Kennington and David Schlangen. 2012. Markov Logic Networks for Situated Incremental Natural Lan-
guage Understanding. In Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse
and Dialogue, pages 314?322, Seoul, South Korea. Association for Computational Linguistics.
Casey Kennington, Spyros Kousidis, and David Schlangen. 2013. Interpreting Situated Dialogue Utterances: an
Update Model that Uses Speech, Gaze, and Gesture Information. In SIGdial 2013.
Spyros Kousidis, Casey Kennington, and David Schlangen. 2013. Investigating speaker gaze and pointing be-
haviour in human-computer interaction with the mint.tools collection. In SIGdial 2013.
1811
Percy Liang, Michael Jordan, and Dan Klein. 2011. Learning Dependency-Based Compositional Semantics. In
Proceedings of the 49th ACLHLT, pages 590?599, Portland, Oregon. Association for Computational Linguistics.
Changsong Liu, Rui Fang, and Joyce Chai. 2012. Towards Mediating Shared Perceptual Basis in Situated Dia-
logue. In Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue,
pages 140?149, Seoul, South Korea, July. Association for Computational Linguistics.
Marie-Jean Meurs, Frederic Duvert, Fabrice Lefevre, and Renato De Mori. 2008. Markov Logic Networks for
Spoken Language Interpretation. Information Systems Journal, (1978):535?544.
Marie-Jean Meurs, Fabrice Lef`evre, and Renato De Mori. 2009. Spoken Language Interpretation: On the Use
of Dynamic Bayesian Networks for Semantic Composition. In IEEE International Conference on Acoustics,
Speech, and Signal Processing, pages 4773?4776.
Ivan Meza-Ruiz, Sebastian Riedel, and Oliver Lemon. 2008. Accurate Statistical Spoken Language Understanding
from Limited Development Resources. In IEEE International Conference on Acoustics, Speech, and Signal
Processing, pages 5021?5024. IEEE.
Andreas Peldszus and David Schlangen. 2012. Incremental Construction of Robust but Deep Semantic Represen-
tations for Use in Responsive Dialogue Systems. In Proceedings of the Workshop on Advances in Discourse
Analysis and its Computational Aspects, pages 59?76, Mumbai, India, December. The COLING 2012 Organiz-
ing Committee.
Andreas Peldszus, Okko Bu?, Timo Baumann, and David Schlangen. 2012. Joint Satisfaction of Syntactic and
Pragmatic Constraints Improves Incremental Spoken Language Understanding. In Proceedings of the 13th
EACL, pages 514?523, Avignon, France, April. Association for Computational Linguistics.
Zahar Prasov and Joyce Y Chai. 2010. Fusing Eye Gaze with Speech Recognition Hypotheses to Resolve Ex-
ophoric References in Situated Dialogue. In EMNLP 2010, number October, pages 471?481.
Deb Roy. 2005. Grounding words in perception and action: computational insights. Trends in Cognitive Sciences,
9(8):389?396, August.
David Schlangen and Gabriel Skantze. 2009. A General, Abstract Model of Incremental Dialogue Processing. In
Proceedings of the 10th EACL, number April, pages 710?718, Athens, Greece. Association for Computational
Linguistics.
David Schlangen and Gabriel Skantze. 2011. A General, Abstract Model of Incremental Dialogue Processing.
Dialoge & Discourse, 2(1):83?111.
Ethan O Selfridge, Iker Arizmendi, Peter A Heeman, and Jason D Williams. 2012. Integrating Incremental
Speech Recognition and POMDP-Based Dialogue Systems. In Proceedings of the 13th Annual Meeting of the
Special Interest Group on Discourse and Dialogue, pages 275?279, Seoul, South Korea, July. Association for
Computational Linguistics.
Gabriel Skantze and Anna Hjalmarsson. 2010. Towards Incremental Speech Generation in Dialogue Systems. In
Proceedings of SigDial 2010, pages 1?8, Tokyo, Japan, September.
Gabriel Skantze and David Schlangen. 2009. Incremental dialogue processing in a micro-domain. Proceedings
of the 12th Conference of the European Chapter of the Association for Computational Linguistics on EACL 09,
(April):745?753.
Gokhan Tur and Renato De Mori. 2011. Spoken Language Understanding: Systems for Extracting Semantic
Information from Speech. Wiley.
Luke S Zettlemoyer and Michael Collins. 2007. Online Learning of Relaxed CCG Grammars for Parsing to
Logical Form. Computational Linguistics, (June):678?687.
Luke S Zettlemoyer and Michael Collins. 2009. Learning context-dependent mappings from sentences to logical
form. Proceedings of the Joint Conference of the 47th ACL and the 4th AFNLP: Volume 2 - ACL-IJCNLP ?09,
2:976.
1812
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 485?489,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
DFKI Hybrid Machine Translation System for WMT 2011
- On the Integration of SMT and RBMT
Jia Xu and Hans Uszkoreit and Casey Kennington and David Vilar and Xiaojun Zhang
DFKI GmbH, Language Technology Lab
Stuhlsatzenhausweg 3
D-66123 Saarbru?cken Germany
{Jia.Xu,uszkoreit,David.Vilar}@dfki.de, {bakuzen,xiaojun.zhang.iiken}@gmail.com
Abstract
We present the DFKI hybrid translation sys-
tem at the WMT workshop 2011. Three SMT
and two RBMT systems are combined at the
level of the final translation output. The trans-
lation results show that our hybrid system sig-
nificantly outperformed individual systems by
exploring strengths of both rule-based and sta-
tistical translations.
1 Introduction
Machine translation (MT), in particular the statisti-
cal approach to it, has undergone incremental im-
provements in recent years. While rule-based ma-
chine translation (RBMT) maintains competitive-
ness in human evaluations. Combining the advan-
tages of both approaches have been investigated by
many researchers such as (Eisele et al, 2008).
Nonetheless, significant improvements over statis-
tical approaches still remain to be shown. In this
paper, we present the DFKI hybrid system in the
WMT workshop 2011. Our system is different from
the system of the last year (Federmann et al, 2010),
which is based on the shallow phrase substitution.
In this work, two rule-based translation systems are
applied. In addition, three statistical machine trans-
lation systems are built, including a phrase-based,
a hierarchical phrase-based and a syntax-based sys-
tem. Instead of combining with rules or post-editing,
we perform system combination on the final transla-
tion hypotheses. We applied the CMU open toolkit
(Heafield and Lavie, 2010) among numerous com-
bination methods such as (Matusov, 2009), (Sim et
al., 2007) and (He et al, 2008). The final transla-
tion output outperforms each individual output sig-
nificantly.
2 Individual translation systems
2.1 Phrase-based system
We use the IBM model 1 and 4 (Brown et al, 1993)
and Hidden-Markov model (HMM) (Vogel et al,
1996) to train the word alignment using the mgiza
toolkit1. We applied the EMS in Moses (Koehn et
al., 2007) to build up the phrase-based translation
system. Features in the log-linear model include
translation models in two directions, a language
model, a distortion model and a sentence length
penalty. A dynamic programming beam search al-
gorithm is used to generate the translation hypoth-
esis with maximum probability. We applied a 5-
gram mixture language model with each sub-model
trained on one fifth of the monolingual corpus with
Kneser-Ney smoothing using SRILM toolkit (Stol-
cke, 2002). We did not perform any tuning, because
it hurts the evaluation performance in our experi-
ments.
2.2 Syntax-based system
To capture the syntactic structure, we also built a
tree-based system using the same configuration of
EMS in Moses (Koehn et al, 2007). Tree-based
models operate on so-called grammar rules, which
include variables in the mapping rules. To increase
the diversity of models in combination, the lan-
guage model in each individual translation system
is trained differently. For the tree-based system,
we applied a 4-gram language model with Kneser-
Ney smoothing using SRILM toolkit (Stolcke, 2002)
trained on the whole monolingual corpus. The
test2007 news part is applied to tune the feature
weights using mert, because the tuning on test2007
1http://geek.kyloo.net/software/doku.php/mgiza:overview
485
improves the translation performance more than the
tuning on test2008 in a small-scale experiment for
the tree-based system.
2.3 Hierarchical phrase-based system
For the hierarchical system, we used the open source
hierarchical phrased-based system Jane, developed
at RWTH and free for non-commercial use (Vi-
lar et al, 2010). This approach is an extension
of the phrase-based approach, where the phrases
are allowed to have gaps (Chiang, 2007). In this
way long-range dependencies and reorderings can
be modeled in a consistent statistical framework.
The system uses a fairly standard setup, trained
using the bilingual data provided by the organizers,
word aligned using the mgiza. Two 5-gram language
models were used during decoding: one trained on
the monolingual part of the bilingual training data,
and a larger one trained on the additional news data.
Decoding was carried out using the cube pruning al-
gorithm. The tuning is performed on test2008 with-
out further experiments.
2.4 Rule-based systems
We applied two rule-based translation systems, the
Lucy system (Lucy, 2011) and the Linguatec sys-
tem (Aleksic? and Thurmair, 2011). The Lucy sys-
tem is a recent offspring of METAL. The Linguatec
system is a modular system consisting of grammar,
lexicon and morphological analyzers based on logic
programming using slot grammar.
3 Hybrid translation
A hybrid approach combining rule-based and sta-
tistical machine translation is usually investigated
with an in-box integration, such as multi-way trans-
lation (Eisele et al, 2008), post-editing (Ueffing et
al., 2008) or noun phrase substitution (Federmann
et al, 2010). However, significant improvements
over state-of-the-art statistical machine translation
are still expected. In the meanwhile system combi-
nation methods for instance described in (Matusov,
2009), (Sim et al, 2007) and (He et al, 2008) are
mostly evaluated to combine statistical translation
systems, rule-based systems are not considered. In
this work, we integrate the rule-based and statistical
machine translation system on the level of the final
PBT Syntax
PBT-2010 18.32
Max80words 20.65 21.10
Max100words 20.78
+Compound 21.52 22.13
+Newparallel 21.77
Table 1: Translation performance BLEU[%] on
phrase/syntax-based system using various settings eval-
uated on test10.
translation hypothesis and treat the rule-based sys-
tem anonymously as an individual system. In this
way an black-box integration is allowed using the
current system combination techniques.
We applied the CMU open toolkit (Heafield
and Lavie, 2010) MEMT, a package by Kenneth
Heafield to combine the translation hypotheses. The
language model is trained on the target side of the
parallel training corpus using SRILM (Stolcke,
2002). We used only the Europarl part to train lan-
guage models for tuning and all target side of paral-
lel data to train language models for decoding. The
beam size is set to 80, and 300 nbest is considered.
4 Translation experiments
4.1 MT Setup
The parallel training corpus consists of 1.8
million German-English parallel sentences from
Europarl-v6 (Koehn, MT Summit 2005) and news-
commentary with 48 million tokenized German
words and 54 million tokenized English words re-
spectively. The monolingual training corpus con-
tains the target side of the parallel training cor-
pus and the additional monolingual language model
training data downloaded from (SMT, 2011). We
did not apply the large-scale Gigaword corpus, be-
cause it does not significantly reduce the perplexity
of our language model but raises the computational
requirement heavily.
4.2 Single systems
For each individual translation system, different
configurations are experimented to achieve a higher
translation quality. We take phrase- and syntax-
based translation system as examples. Table 1
presents official submission result on DE-EN by
486
PBT+Syntax 20.37
PBT+Syntax+HPBT 20.78
PBT+HPBT+Linguatec+Lucy 20.27
PBT+Syntax+HPBT+Linguatec+Lucy 20.81
Table 2: Translation performance BLEU[%] on test2011
using hybrid system tuned on test10 with various settings
(DE-EN).
DFKI in 2010. In 2010?s translation system only
Europarl parallel corpus was applied, and the trans-
lation output was evaluated as 18.32% in the BLEU
score. In 2011, we added the News Commentary
parallel corpus and trained the language model on all
monolingual data provided by (SMT, 2011) except
for Gigaword. As shown in Table 1, if we increase
the maximum sentence length of the training cor-
pus from 80 to 100, the BLEU score increases from
20.65% to 20.78%. In the error analysis, we found
that many OOVs come from the compound words
in German. Therefore, we applied the compound
splitting for both German and English by activating
the corrensponding settings in the EMS in Moses.
This leads to a further improvement of nearly 1%
in the BLEU score. As we add the new parallel
corpus provided on the homepage of SMT work-
shop in 2011 (SMT, 2011) to the corpus in 2010,
a slight improvement can be achieved. Within one
year, the score for the DFKI PBT system DE-EN has
improved by nearly 3.5% absolute and 20% relative
BLEU score points, as shown in Table 1.
In the phrase-based translation, the tuning was not
applied, because it improves the results on the held-
out data but hurts the results on the evaluation set.
In our observation, the decrease is in the range of
0.01% to 1% in the BLEU score. However tun-
ing does help for the Tree-based system. Therefore
we applied the test2007 to optimize the parameters,
which enhanced the BLEU score from 17.52% to
21.10%. The compound splitting also improves the
syntax system, with about 1% in the BLEU score.
We did not add the new parallel corpus into the train-
ing for syntax system due to its larger computational
requirement than that of the phrase-based system.
Test10 Test08 Test11
Hybrid-2010 17.43
PBT 21.77 20.70 20.40
Syntax 22.13 20.50 20.49
HPBT 19.21 18.26 17.06
Linguatec 16.59 16.07 15.97
Lucy 16.57 16.66 16.68
Hybrid-2011 23.88 21.13 21.25
Table 3: Translation performance BLEU[%] on three test
sets using different translation systems in 2011 submis-
sion (DE-EN).
Test10 Test11
Hybrid-2010 14.42
PBT 15.46 14.05
Linguatec 14.92 12.92
Lucy 13.77 13.0
Hybrid-2011 15.55 15.83
Table 4: Translation performance BLEU[%] on two test
sets using different translation systems in 2011 submis-
sion (EN-DE).
4.3 Hybrid system
We applied test10 as the held-out data to tune
the German-English and English-German transla-
tion systems. For experiments, we applied a small-
scaled 4-gram language model trained only on the
target side of the Europarl parallel training data. As
shown in Table 2, different combinations are per-
formed on the hypotheses generated from single sys-
tems. We first combined the PBT with syntax sys-
tem, then together with the HPBT system. The
translation result in the BLEU score performs best
when we combine all three statistical machine trans-
lation systems and two rule-based systems together.
4.4 Evaluation results
For the decoding during the WMT evaluation, we
applied a larger 4-gram language model trained on
the target side of all parallel training corpus. As
shown in Table 3, in last year?s evaluation the DFKI
hybrid translation result was evaluated as 17.34% in
the BLEU score. In 2011, among all the transla-
tion systems, the syntax system performs the best
on test10 and test11, while the PBT performs the
487
SRC Diese Verordnung wurde vom Gesundheitsministerium in diesem Jahr einigermassen gemildert - die Ku?hlschrankpflicht fiel weg.
REF It was mitigated by the Ministry of Health this year - the obligation to have a refrigerator has been removed.
PBT This regulation by the Ministry of Health in this year - somewhat mitigated the fridge duty fell away.
Syntax This regulation was somewhat mitigated by the Ministry of Health this year - the refrigerator duty fell away.
HPBT This regulation was by the Ministry of Health in reasonably Dokvadze this year - the Ku?hlschrankpflicht fell away.
Linguatec This ordinance was soothed to some extent by the brazilian ministry of health this year, the refrigerator duty was discontinued.
Lucy This regulation was quite moderated by the Department of Health, Education and Welfare this year - the refrigerator duty was omitted.
Hybrid This regulation was somewhat mitigated by the Ministry of Health this year - the fridge duty fell away.
SRC Die Deregulierung und Bakalas ehemalige Bergarbeiterwohnungen sind ein brisantes Thema.
REF Deregulation and Bakala ?s former mining flats are local hot topic.
PBT The deregulation and Bakalas former miners? homes are a sensitive issue.
Syntax The deregulation and Bakalas former miners? homes are a sensitive issue.
HPBT The deregulation and Bakalas former Bergarbeiterwohnungen are a hot topic.
Linguatec Former miner flats are an explosive topic the deregulation and Bakalas.
HPBT The deregulation and Bakalas former miner apartments are an explosive topic.
Hybrid The deregulation and Bakalas former miners? apartments are a sensitive issue.
Table 5: Examples of translation output by the different systems.
best on test08. The rule-based sytems, Linguatec
and Lucy are expected to have a higher score in the
human evaluation than in the automatic evaluation.
Furthermore, as we can see from Table 3, there is
still room to improve the Jane system, with better
modeling, configurations or even higher-order lan-
guage model. Using the hybrid system we success-
fully improved the translation result to 23.88% on
test10. The hybrid system outperforms the best sin-
gle system by 0.43% and 0.76% in the BLEU score
on the test08 and test11, respectively.
For the translation from English to German, the
translation result of last year?s submission was eval-
uated as 14.42% in the BLEU score, as shown in Ta-
ble 4. In this year, the phrase-based translation result
is 15.46% in the BLEU score. We only set up one
statistical translation system due to time limitation.
With the respect of the BLEU score, phrase-based
translation outperforms rule-based translations. Be-
tween rule-based translation systems, Linguatec per-
forms better on the test10 (14.92%) and Lucy per-
forms better on the test11 (13.0%). Combining three
translation hypotheses leads to a smaller improve-
ment (from 15.46% to 15.55%) on the test10 and a
greater improvement (from 14.05% to 15.83%) on
the test11 in the BLEU score over the single best
translation system. Comparing to last year?s trans-
lation output, the improvement is over one percent
absolutely (from 14.42% to 15.55%) in the BLEU
score on the test10.
4.5 Output examples
Table 5 shows two translation examples from the
MT output of the test2011. We list the source sen-
tence in German and its reference translation as
well as the translation results generated by different
translation systems. As can be seen from Table 5,
the translation quality of source sentences is greatly
improved using the hybrid system over the single in-
dividual systems. Translations of words and word
orderings are more appropriate by the hybrid sys-
tem.
5 Conclusion and future work
We presented the DFKI hybrid translation system
submitted in the WMT workshop 2011. The hy-
brid translation is performed on the final translation
output by individual systems, including a phrase-
based system, a syntax-based system, a hierarchical
phrase-based system and two rule-based systems.
Combining the results from statistical and rule-
based systems significantly improved the translation
performance over the single-best system, which is
shown by the automatic evaluation scores and the
output examples. Despite of the encouraging results,
there is still room to improve our system, such as the
tuning in the phrase-based translation and a better
language model in the combination.
488
References
Vera Aleksic? and Gregor Thurmair. 2011. Personal
translator at wmt2011 - a rule-based mt system with
hybrid components. In Proceedings of WMT work-
shop.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and R. L. Mercer. 1993. The mathematics of
statistical machine translation: Parameter estimation.
Computational Linguistics, 19(2):263?311, June.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228,
June.
Andreas Eisele, Christian Federmann, Hans Uszkoreit,
Herve? Saint-Amand, Martin Kay, Michael Jellinghaus,
Sabine Hunsicker, Teresa Herrmann, and Yu Chen.
2008. Hybrid architectures for multi-engine machine
translation. In Proceedings of Translating and the
Computer 30, pages ASLIB, ASLIB/IMI, London,
United Kingdom, November.
Christian Federmann, Andreas Eisele, Hans Uszkoreit,
Yu Chen, Sabine Hunsicker, and Jia Xu. 2010. Fur-
ther experiments with shallow hybrid mt systems. In
Proceedings of the Joint Fifth Workshop on Statistical
Machine Translation and MetricsMATR, pages 237?
248, Uppsala, Sweden. John Benjamins.
Xiaodong He, Mei Yang, Jianfeng Gao, Patrick Nguyen,
and Robert Moore. 2008. Indirect-hmm-based hy-
pothesis alignment for combining outputs from ma-
chine translation systems. In Proceedings of EMNLP,
October.
Kenneth Heafield and Alon Lavie. 2010. Voting on n-
grams for machine translation system combination. In
Proc. Ninth Conference of the Association for Machine
Translation in the Americas, Denver, Colorado, Octo-
ber.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of ACL.
Philipp Koehn. MT Summit 2005. Europarl: A parallel
corpus for statistical machine translation.
Lucy. 2011. Home page of software lucy and services.
http://www.lucysoftware.com.
Evgeny Matusov. 2009. Combining Natural Language
Processing Systems to Improve Machine Translation
of Speech. Ph.D. thesis, Department of Electrical
and Computer Engineering, Johns Hopkins University,
Baltimore, MD.
K. C. Sim, W. J. Byrne, M. J. F. Gales, H. Sahbi, and
P. C. Woodland. 2007. Consensus network decoding
for statistical machine translation system combination.
In IN IEEE INT. CONF. ON ACOUSTICS, SPEECH,
AND SIGNAL PROCESSING.
SMT. 2011. Sixth workshop on statistical machine trans-
lation home page. http://www.statmt.org/wmt11/.
Andreas Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In Proceedings of the International
Conference On Spoken Language Processing, pages
901?904, Denver, Colorado, September.
Nicola Ueffing, Jens Stephan, Evgeny Matusov, Lo ic
Dugast, George F. Foster, Roland Kuhn, Jean Senel-
lart, and Jin Yang. 2008. Tighter integration of rule-
based and statistical mt in serial system combination.
In Proceedings of COLING 2008, pages 913?920.
David Vilar, Daniel Stein, Matthias Huck, and Hermann
Ney. 2010. Jane: Open Source Hierarchical Trans-
lation, Extended with Reordering and Lexicon Mod-
els. In Proceedings of the Joint Fifth Workshop on Sta-
tistical Machine Translation and MetricsMATR, pages
262?270, Uppsala, Sweden, July.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In COLING ?96: The 16th Int. Conf. on Com-
putational Linguistics, pages 836?841, Copenhagen,
Denmark, August.
489
Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 314?323,
Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational Linguistics
Markov Logic Networks for
Situated Incremental Natural Language Understanding
Casey Kennington David Schlangen
CITEC Dialogue Systems Group and Faculty of Linguistics and Literary Studies
Universita?t Bielefeld, Bielefeld, Germany
ckennington@cit-ec.uni-bielefeld.de
david.schlangen@uni-bielefeld.de
Abstract
We present work on understanding natural lan-
guage in a situated domain, that is, language
that possibly refers to visually present enti-
ties, in an incremental, word-by-word fashion.
Such type of understanding is required in con-
versational systems that need to act immedi-
ately on language input, such as multi-modal
systems or dialogue systems for robots. We
explore a set of models specified as Markov
Logic Networks, and show that a model that
has access to information about the visual con-
text of an utterance, its discourse context, as
well as the linguistic structure of the utter-
ance performs best. We explore its incremen-
tal properties, and also its use in a joint pars-
ing and understanding module. We conclude
that MLNs offer a promising framework for
specifying such models in a general, possibly
domain-independent way.
1 Introduction
We speak situated in time and space. Speech by ne-
cessity unfolds sequentially in time; and in a conver-
sation, all speech but that of the opening utterance is
preceded by other speech belonging to the same con-
versation. In many, if not most, conversational situa-
tions speaker and addressee are co-located in space,
and their speech may refer to their shared situation.
Most current spoken dialogue systems attempt to
abstract from this fact, however. They work in do-
mains where physical co-location is not necessary,
such as information look-up, and they quantize time
into discrete turn units by endpointing utterances
(see discussion in (Aist et al, 2007; Schlangen and
Skantze, 2009)).
In this paper we present our current work on over-
coming these abstractions for the task of natural lan-
guage understanding (NLU). We have created a sta-
tistical model that can be trained on conversational
data and which can be used as an NLU module for
an incremental, situated dialogue system (such as
that described in (Bu? et al, 2010)). We show that
this model beats baseline approaches by a wide mar-
gin, and that making available the full set of infor-
mation comprising visual context, discourse context,
and linguistic structure gives significantly better re-
sults than any subset of these information sources on
their own.
The paper is structured as follows: we first dis-
cuss related work and introduce some background,
and then describe in detail our set of experiments,
and present and analyse our results. We close with a
general discussion of this work and possible future
extensions.
2 Related Work and Background
The work in this paper builds on, connects and ex-
tends several strands of research: grounded seman-
tics (Roy, 2005), which worries about the connec-
tion between language and the situation in which
it is used, but often does not go beyond the word
level to include linguistic structure information and
does not work incrementally;1 statistical NLU (see
e.g. (Zettlemoyer and Collins, 2009; Liang et al,
1But see (Spranger et al, 2010); for recent attempts that par-
tially overcome these limitations.
314
2011)), which tries to infer linguistic structures au-
tomatically, but normally stops at generating, not in-
terpreting semantic representations, and works with
(the text of) full utterances and not incrementally on
speech data; and incremental NLU, which is a less
intensely studied field, but where previous contri-
butions (such as (DeVault et al, 2009; Devault et
al., 2011; Aist et al, 2007; Schlangen and Skantze,
2009)) have not dealt with learned grounded seman-
tics.
We go beyond this earlier work in that we study
a model that is incremental, can use linguistic struc-
ture, and learns from conversational data a semantics
that connects the utterance to its visual and discourse
context. We have looked at individual components
of this before (grounded semantics in (Siebert and
Schlangen, 2008); incremental reference resolution
in (Schlangen et al, 2009); incremental general NLU
in (Heintze et al, 2010); interaction between incre-
mental parsing and reference resolution in (Peldszus
et al, 2012)), but use a more sophisticated model in
this work and show that tackling these tasks jointly
improves performance.
MLNSystem
Context/World
Language/RMRS
Context/Discourse
Prediction:actionobjectresult
Figure 1: NLU Data Flow
We apply Markov Logic Networks (MLNs,
(Richardson and Domingos, 2006)) as the machine
learning technique in our experiments. MLNs have
recently received attention in language processing
fields like co-reference resolution (Chen, 2009), se-
mantic role labeling (Meza-Ruiz and Riedel, 2009),
spoken (albeit neither situational nor incremental)
NLU (Meurs et al, 2008), and web information ex-
traction (Satpal et al, 2011). The framework of-
fers a convenient way of specifying factor functions
on sets of random variables for undirected graphical
models (Markov Random Fields, see (Kindermann
and Snell, 1980)), in such a way that the factors
correspond to weighted first order formulae and the
joint distribution of random variables corresponds to
probabilities of groundings of formulae. In this way,
MLNs offer a helpful bridge between symbolic rep-
resentation and stochastic inference. Weights of for-
mulae can be specified by hand or learned from data;
we used the latter capability.
Figure 1 shows data flow in our task. We use com-
binations of situated context, previous context, and
linguistic information as evidence to an MLN, and
infer what action is to be taken, what object is to be
acted upon, and specifications of the manner of exe-
cution.
3 Experiments
We will now describe our experiments with using
Markov Logic Networks for situated incremental
natural language understanding.
3.1 Data and Task
For our experiments, we used task-oriented con-
versational data from the Pentomino domain
(Ferna?ndez et al, 2007); more specifically, we
worked with the corpus also used recently in
(Heintze et al, 2010) and (Peldszus et al, 2012).
This corpus was collected in a Wizard-of-Oz study,
where the user goal was to instruct the computer to
pick up, delete, rotate or mirror puzzle tiles on a
rectangular board (as in Figure 2), and place them
onto another one. For each utterance, the corpus
records the state of the game board before the utter-
ance, the immediately preceding system action, and
the intended interpretation of the utterance (as un-
derstood by the Wizard) in the form of a semantic
frame specifying action-type and arguments, where
those arguments are objects occurring in the descrip-
tion of the state of the board. The language of the
corpus is German.
Figure 2: Example Pentomino Board
For this study, we were interested in the potential
contribution of linguistic structure to the NLU task.
315
To this end, we produced for each utterance an in-
cremental sequence of parses and corresponding se-
mantic representations (as RMRS structures (Copes-
take, 2007), i.e. underspecified semantic representa-
tions), using the parser described in (Peldszus et al,
2012). These representations were not further man-
ually checked for appropriateness, and hence do not
necessarily represent ground truth.
As in (Peldszus et al, 2012), we discarded ut-
terances without clear semantic alignments. One
major difference from them is that we do include
the 661 utterances that used pronouns to refer to
pieces, leaving us with 1687 utterances, 5.43 words
per utterance (sd 2.36), with a vocabulary of 237 dis-
tinct words. These were transcribed utterances and
not automatic speech recognition output, so our re-
sults represent an upper-bound on real world perfor-
mance.
The task that we wanted our model to tackle can
then be stated as follows: given information about
the current state of the world (i.e., the game board),
the previous system action, and about the (possibly
still not-yet completed) utterance, predict an inter-
pretation for the utterance, in the form of such a
frame. The elements of the frame may be speci-
fied separately; as argued in (Heintze et al, 2010),
this is the most appropriate format for incremental
processing since it provides a rough alignment be-
tween parts of the utterance and parts of its inter-
pretation. Figure 3 illustrates such a desired output
from the model. In more general terms, what we
want our model to learn then is how, in a given dis-
course context, language connects to the world. To
explore what information contributes to this, we will
systematically vary in our experiments what is avail-
able to the learner.
3.2 Representation
As mentioned above, Markov Logic allows the spec-
ification of knowledge bases through first order for-
mulae. A straightforward representation of the game
board would simply assert salient properties of the
individual objects such as their colour, shape, po-
sition, etc.; for the topmost object in Figure 2 this
could be colour(yellow) ? shape(g) ? pos(2, 1).
However, in pre-experiments on held-out data, we
found that a more parsimonious representation ac-
tually worked better, in which there is only one
n word interpretation
1 rotate action:rotate
2 the ...
3 yellow argument:yellow objects
4 piece argument:yellow pieces
5 next ...
6 to ...
7 the ...
8 yellow argument:yellow pieces
by yellow objects
9 plus argument:yellow piece
next to unique yellow plus
10 clockwise option:clockwise
Figure 3: Incremental interpretation of a 10-word utter-
ance. Only changes to the frame are shown, e.g. when
predictions about different frame elements are made. For
illustration, sets of objects are represented by descrip-
tions; in the system, these would be sets of object identi-
fiers.
abstract property that only implicitly does a typ-
ing into different features of the objects; again, for
the topmost piece from the figure this would be
piece(p)? property(p, yellow)? property(p, g)?
property(p, row0)?property(p, col1). This repre-
sentation follows a Davidsonian form of represent-
ing the relations between predicates.
The properties of the objects that we represented
in this way were colour, shape, its row and column,
horizontal percentage from the center and vertical
percentage from the center.
The utterance itself forms another source of in-
formation about the situation. In the simplest form,
it could be represented just through assertions of
the words which are part of it, e.g. word(rotate) ?
word(the) ? word(yellow) ? . . . . As mentioned
above, we were interested in whether a more de-
tailed linguistic analysis could provide more useful
information to a model of situated semantics; we
represented this information by extracting some of
the relations of the RMRS representation for each ut-
terance (-prefix) and converting them to a slightly
simpler form. Figure 4 shows the RMRS representa-
tion of an example utterance and the corresponding
simplified representation that we derive from it (la-
bels as defined by RMRS and quotes required by and
the MLN are removed for simplicity). We represent
words as RMRS EPs (elementary predicates); i.e., by
316
their lemma and with additional identifiers as argu-
ments, which can be used to relate the EP to other
RMRS structure. In the variants of the model that
only look at words, the other arguments can sim-
ply be ignored in the MLN template. The final ar-
gument for EP is the board identifier, which remains
unchanged during an utterance.
RMRS
a33:yellow(e34)
a19:NN(x14)
ARG1(a49,x14)
ARG2(a49,x53)
a49:nextto(e50)
BV(a52,x53)
RSTR(a52,h60)
BODY(a52,h61)
a52:def()
ARG1(a72,x53)
a72:yellow(e73)
a58:plus(x53)
MLN
EP(a33,yellow,e34,1)
EP(a19,NN,x14,1)
RMRS(ARG1,a49,x14,1)
RMRS(ARG2,a49,x53,1)
EP(a49,nextto,e50,1)
RMRS(BV,a52,x53,1)
EP(a52,def,,1)
RMRS(ARG1,a72,x53,1)
EP(a72,yellow,e73,1)
EP(a58,plus,x53,1)
Figure 4: RMRS and MLN for yellow piece next to the
yellow plus
Finally, the previous system action and, during
learning but not testing, the interpretation that is
to be predicted needs to be represented. This is
done through predicates action(), argument() and
option() for the interpretation of the current utter-
ances and corresponding predicates for that of the
previous one.
To summarise, each problem instance is hence
represented as a conjunction of predicates encoding
a) the (world) situational context (the state of the
game board), b) the discourse context (in the form
of the previous action), and c) the (possibly as-yet
partial) utterance, linguistically analysed.
3.3 Model and Decision Rule
The actual model is now formed by the MLN tem-
plates that specify the relations between the predi-
cates; in particular those between those representing
the available information (evidence) and the predi-
cates that represent the information that is to be pre-
dicted (or, in MLN terminology, whose most likely
values are to be inferred). Figure 5 illustrates graph-
ically how our model makes these connections, sep-
arately for each frame element that is to be predicted.
These graphs show that for action and
option, we assume an influence both of the words
Action
Argument
Option
PrevAction PrevOptionEP
RMRS
Property
EP
EP
Property
EP
Figure 5: MLN relations between predicates
present in the utterance (denoted by EP; see above)
and of the previous value of these slots on the cur-
rent one. The previous context that is used for train-
ing and evaluation is taken from the corpus anno-
tation files. The structure for argument is some-
what more complicated; this is where the linguis-
tic information coming from the RMRSs comes into
play, and also where the connection between lan-
guage and properties of the visual scene is made.
The actual template that defines our MLN is shown
in Figure 6.
1 EP (a1, a2,+w, a3, b) ? Action(+a, b)
2 PrevAction(+a, b) ? Action(+a, b)
3 EP (a1, a2,+w, a3, b)) ? Option(+o, b)
4 PrevOption(+o, b) ? Option(+o, b)
5 EP (a1, a2,+w, a3, b)) ? Property(p,+pr, b)
? Argument(p, b)
6 EP (a1, a2, w1, a3, b) ?RMRS(+t, a4, a3, b)
?RMRS(+t, a4, a5, b) ? EP (a5, a6, w2, a5, )
?Property(p,+pr, b) ? Argument(p, b)
Figure 6: The MLN template specifying our model
Our MLN system gives us probability distribu-
tions over all possible groundings of the frame pred-
icates, but as we are interested in single best candi-
dates (or the special value unknown, if no guess
can be made yet), we applied an additional deci-
sion rule to the output of the MLN component. If
the probability of the highest candidate is below a
threshold, unknown is returned, otherwise that can-
didate is returned. Ties are broken by random se-
lection. The thresholds for each frame element /
predicate were determined empirically on held-out
data so that a satisfactory trade-off between letting
through wrong predictions and changing correct re-
317
Type Class Acc.
Action majority put 33.55
Argument majority tile-3 20.98
Option majority na 27.08
Frame majority take, tile-3, na 3.67
Action Contextual 42.24
Table 1: Majority class and Action contextual baselines
sults to unknown was achieved.
3.4 Parameter Training Procedure, Baselines,
Metrics
All results reported below were obtained by aver-
aging results of a 10-fold validation on 1489 Pento
boards (i.e., utterances + context). We used a sep-
arate set of 168 boards for small-scale, held-out
experiments. For learning and inference we used
the Alchemy system (Domingos et al, 2006), us-
ing the discriminative training option (Singla and
Domingos, 2005).2 Inference was performed on the
Action, Argument, and Option predicates; a sin-
gle answer was derived from the distributions deliv-
ered by alchemy in the way described in the previous
section.
To be able to assess our results, we devised two
kinds of baselines for the full utterance. The sim-
plest is just the majority class. Table 1 shows ac-
curacy when choosing the majority class, both for
the frame elements individually (where this baseline
is quite high) and for the most frequent full frame
(which, unsurprisingly, only reaches a very low ac-
curacy). Action can be predicted with somewhat
more accuracy if not the overall most frequent value
is chosen but that given the previous action (i.e.,
when Action is conditioned on PreviousAction).
The accuracy for this method, where the conditional
distribution was determined on the 1489 boards and
tested on the remaining 168 boards, is shown in the
Table under ?action contextual?.
We give our results below as f-score, slot accuracy
and frame accuracy based on comparison to a gold
representation. To compute the f-score, we count a
prediction of unknown as a false negative (since for
our test utterance a value should always have been
predicted) and a wrong prediction as a false posi-
2http://alchemy.cs.washington.edu/
tive; i.e., a frame with one correct slot and the rest as
unknown has perfect precision, but only 1/3 recall.
Slot accuracy counts the number of slots that are
correct, and frame accuracy only counts fully cor-
rect frames. Hence, these metrics are successively
more strict. Which one most accurately predicts per-
formance of the model in the context of a dialogue
system depends on properties of the further compo-
nents: if they can act on partial frames, then an f-
score that start highs and continually improves as the
utterance goes on is desired; if not, then what?s rel-
evant is when in the utterance high frame accuracy
can be reached.
Using the best model variant, we further com-
pare two parsing/NLU feedback strategies, where the
feedback is to provide aid to the syntactic/RMRS
parser as to which parses to prune (as in (Peldszus
et al, 2012)). If a candidate parse does not resolve
to anything, then the parse score is degraded. (Peld-
szus et al, 2012) use a rule-based reference resolu-
tion component to provide this feedback signal. We
explore what the effects are of exchanging this for
a learned feedback strategy using our MLN model.
This model, however, does not provide discrete ref-
erent sets, but instead gives a probability distribution
over all possible pieces. We therefore simply mul-
tiplied each parse by the probability of the highest
probable piece, so that low probabilities effectively
result in pruning a parse.
On the incremental level, we followed Schlangen
et al (2009) by using a subset of their incremental
metrics, with a modification on the edit overhead:
first correct: how deep into the utterance do we
make the first correct guess?
first final: how deep into the utterance do we make
the correct guess, and don?t subsequently change our
minds?
edit overhead: ratio of unnecessary edits / sentence
length, where the only necessary edit is that going
from unknown to the final, correct result anywhere
in the sentence)
We also follow their assumption that as the sen-
tence progresses incrementally, the earlier the frame
prediction can be made, the better. This is an impor-
tant part of our threshold decision rule, because we
also assume that no decision is better than a bad de-
cision. A comparison between first correct and first
final would reveal how well this assumption is real-
318
W E R P FScore Slot Frame
5 5 5 5 92.18 88.88 74.76 1
{86.76} {81.61} {61.21}
5 5 5 81.06 72.59 34.36
{68.20} {58.61} {19.19}
5 5 5 91.63 88.03 72.68 2
{86.47} {80.69} {58.18}
5 5 75.44 65.72 22.55
5 5 5 72.29 61.61 24.56
5 5 18.15 12.10 0.0
5 5 72.34 61.67 24.63
5 18.32 12.21 0.0
5 5 5 90.68 85.68 63.75 4
5 5 68.94 56.26 0.0
5 5 90.67 85.68 63.89 3
5 69.10 56.39 0.0
5 5 72.29 61.61 24.56
5 18.15 12.10 0.0
5 72.30 61.63 24.69
18.15 12.10 0.0
Table 2: Comparison of combinations using World, EPs
(words), RMRS and Previous context. Number in brack-
ets are for tests on automatically transcribed speech.
ized. A good model would have the two numbers
fairly close together, and the prediction would be
best if both were lower, meaning good predictions
earlier in the sentence. The edit overhead further
sheds light on this distinction by showing what per-
centage of the time edits were made unnecessarily
throughout a sentence.
The procedure on the incremental level is simi-
lar to the full utterance procedure, except that for
incremental evaluation the f-score, slot accuracy,
and frame accuracies were calculated word for word
against the final gold representation.
3.5 Results
Since we were interested in the relative contributions
of our different kinds of information sources (visual
context, discourse context, words, linguistic struc-
ture), we trained and tested variant of the model de-
scribed above that had access to only parts of the full
information (by removing the appropriate predicates
from the MLN template). We report results in Table 2
for these different variants; here just as results after
the final word of the utterance, i.e., we?re not yet
Feedback Predictor FScore Slot Frame
HC HC 38.2
HC Full 92.26 88.94 74.69
none Full 92.18 88.88 74.76
Full Full 92.29 89.01 74.96
Table 3: Feedback strategies comparison for hard-coded
(HC), automatic (MLN) and no feedback (none)
looking at the incremental performance. For easier
reference, some lines are indexed with their rank ac-
cording to frame accuracy. The tope three lines also
contain a bracketed entry which represents automat-
ically transcribed utterances (also trained on manu-
ally transcribed data as in (Peldszus et al, 2012)).
First, it should be pointed out that the full model
(which has access to all information types) performs
rather well, giving a fully correct interpretation for
74% of all frames. As the somewhat higher f-score
indicates, some of the loss of frame accuracy is not
due to wrong predictions but rather to staying unde-
cided (choosing unknown)?a behaviour that could
be advantageous in some applications.
The next line shows that much of the informa-
tion required to reach this accuracy comes not from
the visual context or an analysis of the language but
from the discourse context; without access to it, ac-
curacy drops to 22%. However, the advantage of
having access to discourse context only really comes
out when access to the utterance is given as well
(rows indexed with 3 and 4, and 1 and 2). The model
that just goes by previous context can only achieve
an accuracy of 24%
Connecting discourse context to language alone
only brings accuracy to around 65% (rows 3 and 4);
only when the visual context is provided as well can
the best accuracy be reached. This is a pleasing re-
sult, as it shows that the model is indeed capable
of making the desired connection between language
and world; as none of it was not explicitly given,
which words and linguistic structure linked to which
properties was completely learned by the discrimi-
native training.
For the automatically transcribed results, all ver-
sions take a hit especially with regards to frame ac-
curacy. These also show that previous context and
linguistic structure contribute to increased perfor-
mance.
319
action 1-6 7-8 9-14
first correct (% into utt.) 4.43 9.17 6.80
first final (% into utt.) 29.47 31.57 28.47
edit overhead 4.28
argument 1-6 7-8 9-14
first correct (% into utt.) 12.12 11.14 8.08
first final (% into utt.) 38.26 36.10 30.84
edit overhead 5.72
option 1-6 7-8 9-14
first correct (% into utt.) 7.62 27.75 26.73
first final (% into utt.) 45.13 56.68 59.36
edit overhead 13.96
Table 4: Incremental Results for Action, Argument, and
Option with varying sentence lengths
3.5.1 Feedback Results
Table 3 shows the various feedback strategies. HC
refers to the hard-coded version of feedback as in
(Peldszus et al, 2012). None means no feedback
was used, which is the setting of the parser as it was
used for the RMRS structures used in Table 2. MLN
refers using our learned model to provide feedback.
The column ?Predictor? shows what model was used
to make the final prediction at the end of the utter-
ance. Overall, MLN performed much better on pre-
dicting the frame than the HC system (first row vs the
other rows); but one should keep in mind that much
of that improvement is presumably due to it having
access to discourse context.
The last three lines show that, as (Peldszus et
al., 2012) observed, providing feedback during pars-
ing does offer benefits; both HC-MLN and MLN-
MLN significantly improve over NONE-MLN (for f-
score: one-sided t(1489) = -3.313, p-value < 0.001,
and t(1489) = -3.67, p-value < 0.001, respectively;
significance-level Bonferroni corrected for multiple
comparisons; similar numbers for other metrics).
There was no significance when comparing HC with
MLN. This is an interesting result, indicating that
even though our model performs better at accurately
picking out referents, it provides a less useful feed-
back signal. This may be due to the way we com-
pute this signal; we leave further exploration to fu-
ture work.
3.5.2 Incremental Results
Table 4 shows the incremental results. Rows in-
volving first correct and first final represent aver-
age percentage into the utterance, where the utter-
ances were binned for lengths 1-6, 7-8, and 10-17
(?short?, ?normal?, ?long? utterances, respectively).
The boundaries of the bins were determined by look-
ing at the distribution of utterance lengths, which
looked like a normal distribution with 7 and 8-word
utterances having the highest representation. Our
model makes very early predictions (low first cor-
rect), but those predictions don?t always remain sta-
ble, and there is an edit overhead which leads to a
final correct decision only later in the sentence (first
final). For action and argument, the final deci-
sion is typically made within the first third of the ut-
terance. For option, it comes between the first and
second third of the sentence; this reflects typical ut-
terance structure, where the words that describe the
option (?spiegle es horizontal?; mirror it horizon-
tally) usually come later in the sentence.
A final way to show incremental progress is in
Figures 7 and 8 for sentences of ?normal? length
(7-8 words). These show how accurate the pre-
diction was for each incremental step into the sen-
tence, both for the model with and that without ac-
cess to discourse context. Where first correct and
first final help identify specific points in the process-
ing of an utterance, for this graph each incremental
step is compared with the gold result. Figure 8, for
the model variant without access to discourse con-
text, shows that there is little impact on prediction
of action or option, but a significant and con-
stant impact on the quality of predicting argument
(i.e., of doing reference resolution); this is due to
some extent to the presence of anaphoric references
which simply cannot be resolved without access to
context.
Taken together, the incremental statistics help de-
termine an ?operating point? for later modules that
consume NLU output. Under the assumption that the
ongoing utterance will be one of normal length (this
of course cannot be known in advance), the strength
with which a decision of the predictor can be be-
lieved at the current point into the utterance can be
read off the graphs.
Some discussion on speed efficiency: Using
320
lll
l l l l l l l
l l ll
0.0 0.2 0.4 0.6 0.8
0.0
0.2
0.4
0.6
0.8
1.0
% into sentence
acc
ura
cy
l action
argument
optionfscore
Figure 7: incremental accuracies
l
l
l l
l l l l l l
l l ll
0.0 0.2 0.4 0.6 0.8
0.0
0.2
0.4
0.6
0.8
1.0
% into sentence
acc
ura
cy
l action
argument
optionfscore
Figure 8: incremental accuracies, no discourse context
MLNs did not introduce any noticeable speed effi-
ciency reduction in non-feedback models. In feed-
back models which used Auto, many more calls
to MLN were used, which greatly slowed down the
model.
3.6 Model Analysis
Examining the utterances that were not correctly in-
terpreted, we found that words dealing with the ar-
gument occured most frequently, specifically words
involving spatial language where the argument was
described in relation to another piece. This is some-
what disappointing, as we were hoping that RMRS
structure might help learn such constructions.
However, basic spatial expressions were learned
successfully, as can be illustrated by Figure 9. It
shows shows the probability distributions for the ut-
terances left and bottom right, on a 5x5 board we
generated for analysis, where each field was filled
with the same kind of piece of the same colour
(thus making these properties non-distinguishing).
The darker the gradient in the Figure the higher the
probability. The Figure shows that model success-
fully marks the fields closer to the left (or bottom-
right, respectively) as having higher probability. In-
terestingly, ?left? seems to have some confusability
with ?right? for the model, indicating perhaps that
it picked up on the general type of description (?far
side?). Further investigation of model properties is
left to future work, however.
left bottom right
Figure 9: probability gradient for left and bottom right
4 Conclusions
Markov logic networks are effective in expressing
models for situated incremental natural language un-
derstanding in a domain like Pentomino. We have
shown that various aspects of situated language use,
like previous context and the current state of the
world, all play a role in NLU. We have also shown
that semantic representations like RMRS can im-
prove performance, and we further verified that in-
cremental feedback between parser and NLU can im-
prove performance (Peldszus et al, 2012). MLNs
also provide an easy-to-read trained model which
can be easily analyzed. However, there is a trade-off
in that MLNs take some time to design, which still is
an intellectual task. Furthermore, inference in MLNs
is still not as efficient as other methods, which can
cause a slowdown in applications where very many
inference steps are required, such as the feedback
model.
In future work, we will further explore how to best
integrate linguistic information from the RMRSs,
specifically in spatial language; as well as look into
improvements in speed performance. Future work
will focus on interaction with live ASR. We will also
investigate using this setup for automatically trained
natural language generation.
321
Acknowledgements: Thanks to Andreas Peld-
szus for help with data and to the reviewers.
References
Gregory Aist, James Allen, Ellen Campana, Car-
los Gomez Gallo, Scott Stoness, Mary Swift, and
Michael K. Tanenhaus. 2007. Incremental under-
standing in human-computer dialogue and experimen-
tal evidence for advantages over nonincremental meth-
ods. In Proceedings of Decalog 2007, the 11th Inter-
national Workshop on the Semantics and Pragmatics
of Dialogue, Trento, Italy.
Okko Bu?, Timo Baumann, and David Schlangen. 2010.
Collaborating on utterances with a spoken dialogue
system using an isu-based approach to incremental
dialogue management. In Proceedings of the SIG-
dial 2010 Conference, pages 233?236, Tokyo, Japan,
September.
Fei Chen. 2009. Coreference Resolution with Markov
Logic. Association for the Advancement of Artificial
Intelligence.
Ann Copestake. 2007. Semantic composition with (ro-
bust) minimal recursion semantics. In Proceedings of
the Workshop on Deep Linguistic Processing - DeepLP
?07, page 73, Morristown, NJ, USA. Association for
Computational Linguistics.
D. DeVault, Kenji Sagae, and David Traum. 2009. Can I
finish?: learning when to respond to incremental inter-
pretation results in interactive dialogue. In Proceed-
ings of the SIGDIAL 2009 Conference: The 10th An-
nual Meeting of the Special Interest Group on Dis-
course and Dialogue, number September, pages 11?
20. Association for Computational Linguistics.
David Devault, Kenji Sagae, and David Traum. 2011.
Incremental Interpretation and Prediction of Utterance
Meaning for Interactive Dialogue. Dialoge & Dis-
course, 2(1):143?170.
Pedro Domingos, Stanley Kok, Hoifung Poon, and
Matthew Richardson. 2006. Unifying logical and sta-
tistical AI. American Association of Artificial Intelli-
gence.
Raquel Ferna?ndez, Tatjana Lucht, and David Schlangen.
2007. Referring under restricted interactivity condi-
tions. In Proceedings of the 8th SIGdial Workshop on
Discourse and Dialogue, pages 136?139.
Silvan Heintze, Timo Baumann, and David Schlangen.
2010. Comparing local and sequential models for sta-
tistical incremental natural language understanding. In
Proceedings of the 11th Annual Meeting of the Special
Interest Group on Discourse and Dialogue, pages 9?
16. Association for Computational Linguistics.
Ross Kindermann and J. Laurie Snell. 1980. Markov
random fields and their applications. In In Practice,
volume 1 of Contemporary Mathematics, page 142.
American Mathematical Society.
Percy Liang, Michael Jordan, and Dan Klein. 2011.
Learning Dependency-Based Compositional Seman-
tics. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 590?599, Port-
land, Oregon. Association for Computational Linguis-
tics.
Marie-jean Meurs, Frederic Duvert, Fabrice Lefevre, and
Renato De Mori. 2008. Markov Logic Networks for
Spoken Language Interpretation. Information Systems
Journal, (1978):535?544.
Ivan Meza-Ruiz and Sebastian Riedel. 2009. Jointly
identifying predicates, arguments and senses using
Markov logic. In Proceedings of Human Language
Technologies: The 2009 Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics on - NAACL ?09, number June,
page 155, Morristown, NJ, USA. Association for
Computational Linguistics.
Andreas Peldszus, Okko Bu?, Timo Baumann, and David
Schlangen. 2012. Joint Satisfaction of Syntactic
and Pragmatic Constraints Improves Incremental Spo-
ken Language Understanding. In Proceedings of the
13th Conference of the European Chapter of the As-
sociation for Computational Linguistics, pages 514?
523, Avignon, France, April. Association for Compu-
tational Linguistics.
Matthew Richardson and Pedro Domingos. 2006.
Markov logic networks. Machine Learning, 62(1-
2):107?136.
Deb Roy. 2005. Grounding words in perception and ac-
tion: computational insights. Trends in Cognitive Sci-
ences, 9(8):389?396, August.
Sandeepkumar Satpal, Sahely Bhadra, S Sundararajan
Rajeev, and Rastogi Prithviraj. 2011. Web Infor-
mation Extraction Using Markov Logic Networks.
Learning, pages 1406?1414.
David Schlangen and Gabriel Skantze. 2009. A General,
Abstract Model of Incremental Dialogue Processing.
In Proceedings of the 12th Conference of the Euro-
pean Chapter of the ACL (EACL 2009), number April,
pages 710?718, Athens, Greece. Association for Com-
putational Linguistics.
David Schlangen, Timo Baumann, and Michaela At-
terer. 2009. Incremental Reference Resolution: The
Task, Metrics for Evaluation, and a {B}ayesian Filter-
ing Model that is Sensitive to Disfluencies. In Pro-
ceedings of the SIGDIAL 2009 Conference, number
September, pages 30?37, London, UK. Association for
Computational Linguistics.
322
Alexander Siebert and David Schlangen. 2008. A Sim-
ple Method for Resolution of Definite Reference in a
Shared Visual Context. In Proceedings of the 9th SIG-
dial Workshop on Discourse and Dialogue, number
June, pages 84?87, Columbus, Ohio. Association for
Computational Linguistics.
Parag Singla and Pedro Domingos. 2005. Discrimina-
tive Training of Markov Logic Networks. Computing,
20(2):868?873.
Michael Spranger, Martin Loetzsch, and Simon Pauw.
2010. Open-ended Grounded Semantics. In Euro-
pean Conference on Artificial Intelligence 2010, Lis-
bon, Portugal. Volume 215 Frontiers in Artificial In-
telligence and Applications.
Luke S. Zettlemoyer and Michael Collins. 2009. Learn-
ing context-dependent mappings from sentences to
logical form. Proceedings of the Joint Conference of
the 47th Annual Meeting of the ACL and the 4th Inter-
national Joint Conference on Natural Language Pro-
cessing of the AFNLP: Volume 2 - ACL-IJCNLP ?09,
2:976.
323
Proceedings of the SIGDIAL 2013 Conference, pages 173?182,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Interpreting Situated Dialogue Utterances:
an Update Model that Uses Speech, Gaze, and Gesture Information
Casey Kennington
CITEC, Bielefeld University
ckennington1
Spyros Kousidis
Bielefeld University
spyros.kousidis2
1@cit-ec.uni-bielefeld.de
2@uni-bielefeld.de
David Schlangen
Bielefeld University
david.schlangen2
Abstract
In situated dialogue, speakers share time
and space. We present a statistical model
for understanding natural language that
works incrementally (i.e., in real, shared
time) and is grounded (i.e., links to en-
tities in the shared space). We describe
our model with an example, then estab-
lish that our model works well on non-
situated, telephony application-type utter-
ances, show that it is effective in ground-
ing language in a situated environment,
and further show that it can make good use
of embodied cues such as gaze and point-
ing in a fully multi-modal setting.
1 Introduction
Speech by necessity unfolds over time, and in spo-
ken conversation, this time is shared between the
participants. Speakers are also by necessity lo-
cated, and in face-to-face conversation, they share
their (wider) location (that is, they are co-located).
The constraints that arise from this set of facts are
often ignored in computational research on spoken
dialogue, and where they are addressed, typically
only one of the two is addressed.
Here, we present a model that computes in an
incremental fashion an intention representation for
dialogue acts that may comprise both spoken lan-
guage and embodied cues such as gestures and
gaze, where these representations are grounded in
representations of the shared visual context. The
model is trained on conversational data and can be
used as an understanding module in an incremen-
tal, situated dialogue system.
Our paper begins with related work and back-
ground and then specifies in an abstract way the
task of the model. We describe our model formally
in Section 4, followed by three experiments with
the model, the first establishing it with a traditional
spoken language understanding (SLU) setting, the
second to show that our model works well under
situated conditions, and the third shows that our
model can make use of embodied cues. We fin-
ish the paper with a general discussion and future
work.
2 Related Work and Background
The work presented in this paper connects and ex-
tends several areas of research: grounded seman-
tics (Roy, 2005; Hsiao et al, 2008; Liu et al,
2012), which aims to connect language with the
world, but typically does not work incrementally;
semantic parsing / statistical natural language un-
derstanding (NLU), which aims to map an utter-
ance to its meaning representation (using vari-
ous routes and approaches, such as logical forms
(Zettlemoyer and Collins, 2007; Zettlemoyer and
Collins, 2009), dependency-based compositional
semantics (Liang et al, 2011), neural networks
(Huang and Er, 2010), Markov Logic Networks
(MLN) (Meurs et al, 2008; Meza-Ruiz et al,
2008), and dynamic Bayesian networks (Meurs
et al, 2009); see also overviews in (De Mori et
al., 2008; Wang et al, 2011)), but typically nei-
ther provides situated interpretations nor incre-
mental specifications of the representations; incre-
mental NLU (DeVault et al, 2009; DeVault et al,
2011; Aist et al, 2007; Schlangen and Skantze,
2009), which focuses on incrementality, but not
on situational grounding; integration of gaze into
language understanding (Prasov and Chai, 2010),
which was not incremental.
We move beyond this work in that we present a
model that is incremental, uses a form of grounded
semantics, can easily incorporate multi-modal in-
formation sources, and finally on which inference
can be performed quickly, satisfying the demands
of real-time dialogue. The model brings together
aspects we?ve previously looked into separately:
grounded semantics in (Siebert and Schlangen,
173
2008); incremental interpretation (reference res-
olution) in (Schlangen et al, 2009); incremental
general NLU in (Heintze et al, 2010); and a more
sophisticated approach that handled all of these us-
ing markov logic networks, but did not work in
real-time or with multi-modal input (Kennington
and Schlangen, 2012).
3 The Task
The task for our model is as follows: to compute at
any moment a distribution over possible intentions
(expressed as semantic frames), given the unfold-
ing utterance and possibly information about the
state of the world in which the utterance is hap-
pening. The slots of these frames are to be filled
with semantic constants, that is, they are uniquely
resolved; if appropriate, to objects in the shared
environment.
This is illustrated in Figure 1, where for
three successive incremental units (Schlangen and
Skantze, 2009) (that is, successively available bits
of information pertaining to the same act, such as
words of an utterance, or information about speech
accompanying gesture) three distributions over in-
tentions are shown.1
[   ]
fe: a
[   ]
fe: b
[   ]
fe: a
IU
1
IU
2
IU
3
Donnerstag, 2. Mai 2013
Figure 1: Schematic Illustration of Task
4 Our Model
More formally, the goal of the model is to recover
I , the intention of the speaker behind her utter-
ance, in an incremental fashion, that is, word by
word. We make the assumption that the set of
possible intentions is finite, and that they consist
of (combinations of) entities (where however even
actions like taking are considered ?entities?; more
on this below). We observe U , the current word
that the speaker uttered as part of their utterance
(and features derived from that). We also assume
that there is an unobserved mediating variable R,
1Here, no links between these intention representations
are shown. The model we present in the next section is
an update model, that is, it builds the representation at step
tn based on that at tn?1; other possibilities are explored in
(Heintze et al, 2010) and (Kennington and Schlangen, 2012).
which represents the (visual or abstract) proper-
ties of the (visually present, or abstract) object
of the intention. So, what we need to calculate
is P (I|U,R), even though ultimately we?re inter-
ested only in P (I|U). By definition of conditional
probability, P (I|U,R) = P (I, U,R)?P (U,R)?1.
We factorise P (I, U,R) as indicated in the follow-
ing:
P (I|R,U) = P (R|I)P (I)P (U |R)P (U,R) (1)
That is, we make the assumption that R is con-
ditional only on I , and U is conditional only on
R. Marginalizing over R gets us the model we?re
interested in (and it amounts to a not uncommon
tagging model with a hidden layer):
P (I|U) = P (I)
?
r?R
P (U |R = r)P (R = r|I)
P (U,R = r)
(2)
Where we can move P (I) out of the summation,
as it is not dependent on R. Hence, we need three
models, P (I), P (U |R) and P (R|I), to compute
P (I|U). Figure 2 shows how these three models
interact over time.
It?2
Rt?2
Ut?2
It?1
Rt?1
Ut?1
It
Rt
Ut
Figure 2: Our model represented as an unrolled
DBN over three words.
Each sub-model will now be explained.
P(I) At the beginning of the computation for an
incoming sentence, we set the prior P (I) to a uni-
form distribution (or, if there is reason to do so, a
different distribution to encode initial expectations
about intentions; i.e., prior gaze information). For
later words, it is set to the posteriori of the pre-
vious step, and so this constitutes a Bayesian up-
dating of belief (with a trivial, constant transition
model that equates P (It?1) and P (It)).2
2In that sense, our incremental understanding could be
called ?intra-sentential belief tracking,? in analogy to the cur-
rent effort to track system belief about user intentions across
turns (Ma et al, 2012; Williams, 2010).
174
The other models represent knowledge about
links between intentions and object properties,
P (R|I), and knowledge about language use,
P (U |R). We now explain how this knowledge is
acquired.
P(R|I) The model P (R|I) provides the link be-
tween objects (as occurring in the intentions) and
their properties. Here we follow, to our knowl-
edge, a novel approach, by deriving this distribu-
tion directly from the scene representation. This
is best explained by looking at the overall model
in a generative way. First, the intention is gener-
ated, P (I), then based on that a property, P (R|I).
We assume that with equal probability one of the
properties that the intended object actually has is
picked to be verbalised, leaving zero probability
for the ones that it does not have. This in a way is
a rationality assumption: a rational speaker will, if
at all, mention properties that are realised and not
others (at least in non-negative contexts).
P(U|R), learned directly The other model,
P (U |R), can be learned directly from data by
(smoothed) Maximum Likelihood estimation. For
training, we assume that the property R that is
picked out for verbalisation is actually observable.
In our data, we know which properties the refer-
ent actually has, and so we can simply count how
often a word (and its derived features) co-ocurred
with a given property, out of all cases where that
property was present.
P(U|R), via P(R|U) Instead of directly learn-
ing a model of the data, we can learn a discrimina-
tive model that connects words and properties.
In Equation 2, we can rewrite P (U |R) using
Bayes? Rule:
P (I|U) = P (I)
?
r?R
P (U)P (R = r|U)P (R = r|I)
P (R = r)P (U,R = r) (3)
P (U) is a constant when computing P (I|U) for
all possible values of I whose actual value does
not change the rank of each intention, and so can
be dropped. P (R) can be approximated with a
uniform distribution, and can also be dropped,
yielding:
P (I|U) = P (I)
?
r?R
P (R = r|U)P (R = r|I)
P (U,R = r)
(4)
Other models could also be learned here; we chose
a discriminative model to show that our model
works under varied circumstances.
word red round square green
the 0.03 0.02 0.02 0.02
red 0.82 0.009 0.09 0.01
ball 0.02 0.9 0.02 0.07
Table 1: P (U |R) for our toy domain for some
values of U and R; we assume that this model is
learned from data (columns are excerpted from a
distribution over a larger vocabulary).
int. red round square green
obj1 0.5 0.5 0 0
obj2 0.5 0 0.5 0
Table 2: P (R|I), for our example domain.
Properties An important part of our model is
the set of properties. Properties can be visual
properties such as color or shape or spatial prop-
erties (left-of, below, etc.). Though not the fo-
cus of this paper, they could also be concep-
tual properties (the verb run can have the proper-
ties of movement, use of legs, and quick).
Another example, New York has the property of
being New York. (That is generally sufficient
enough to denote New York, but note that descrip-
tive properties (e.g., ?location of the Empire State
Building?) could be used as well.) The purpose
of the properties is to ground intentions with lan-
guage in a more fine-grained way than the words
alone.
We will now give an example of the generative
approach as in Equation 2 (it is straight-forward to
do the same for the discriminative model).
4.1 Example
The task is reference resolution in a shared visual
context: there is an intention to refer to a visible
object. For this example, there are two objects
obj1 and obj2, and four properties to describe
those objects, red, round, square and green.
The utterance for which we want to track a dis-
tribution over possible referents, going word-by-
word, is the red ball. obj1 happens to be a red
ball, with properties red and round; obj2 is a
red cube, with the properties red and square.
We now need the models P (U |R) and P (R|I).
We assume the former is learned from data, and
for the four properties and three words gives us re-
sults as shown in Table 1 (that is, P (U = the|R =
red) = 0.03). The model P (R|I) can be read off
the representation of the scene: if you intend to
175
refer to object obj1 (I = obj1), you can either
pick the property red or the property round, so
both get a probability of 0.5 and all others 0; sim-
ilar for obj2 and red and square (Table 2).
Table 3 now shows an application of the full
model to our example utterance. The cells
in the columns labeled with properties show
P (U |R)P (R|I) for the appropriate properties and
intentions (objects), the column ? shows results
after marginalizing over R. The final column then
factors in P (I) with a uniform prior for the first
word, and the respectively previous distribution
for all others, and normalises.
I U red rnd. sq. ? P (I|U)
obj1 the .015 .01 0 .025 .5
obj2 .015 0 .01 .025 .5
obj1 red .41 .0045 0 .41 .47
obj2 .41 0 .045 .46 .53
obj1 ball .01 .45 0 .46 .96
obj2 .01 0 .01 .02 .04
Table 3: Application of utterance the red ball,
where obj1 is the referred object
As these numbers show, the model behaves as
expected: up until ball, the utterance does not
give enough information to decide for either ob-
ject probabilities are roughly equal, once ball is
uttered obj1 is the clear winner.
This illustrated how the model works in princi-
ple and showed that it yields the expected results
in a simple toy domain. In the next section we will
show that this works in more realistic domains.
5 Experiments
Our model?s task is to predict a semantic frame,
where the required slots of the frame are known
beforehand and each slot value is predicted us-
ing a separate model P (I|U). We realise P (U |R)
as a Naive Bayes classifier (NB) which counts co-
occurrences of utterance features (words, bigrams,
trigrams; so U is actually a tuple, not a single vari-
able) and properties (but naively treats features as
independent), and which is smoothed using add-
one smoothing. As explained earlier, P (I) repre-
sents a uniform distribution at the beginning of an
utterance, and the posteriori of the previous step,
for later words. We also train a discriminative
model, P (R|U), using a maximum entropy classi-
fier (ME) using the same features as NB to classify
properties.3
3http://opennlp.apache.org/
5.1 A Non-Situated Baseline using ATIS
We performed an initial test of our model using
a corpus in traditional NLU: the air travel infor-
mation system (ATIS) corpus (Dahl et al, 1994)
using the pre-processed corpus as in (Meza-Ruiz
et al, 2008). In ATIS, the main task is to predict
the slot attributes (the values were simply words
from the utterance); however, the GOAL slot (rep-
resenting the overall utterance intent) was was al-
ways present, the value of which required a predic-
tion. We tested our model?s ability to predict the
GOAL slot (using very simple properties; the prop-
erty of a GOAL intention is itself, i.e., the property
of flight is flight) and found encouraging re-
sults (the GOAL slot baseline is 71.6%, see (Tur et
al., 2010); our NB and ME models obtained scores
of 77% and 77.9% slot value prediction accura-
cies, respectively). How our model works under
more complicated settings will now be explained.
5.2 Puzzle Domain: Speech-Only
Figure 3: Example
Pentomino Board
?
??
ACTION rotate
OBJECT object-4
RESULT clockwise
?
??
Figure 4: Pento
frame example
Data and Task The Pentomino domain
(Ferna?ndez et al, 2007) contains task-oriented
conversational data; more specifically, we worked
with the corpus also used recently in (Heintze et
al., 2010; Peldszus et al, 2012; Kennington and
Schlangen, 2012). This corpus was collected in
a Wizard-of-Oz study, where the user goal was
to instruct the computer to pick up, delete, rotate
or mirror puzzle tiles on a rectangular board (as
in Figure 3), and place them onto another one.
For each utterance, the corpus records the state of
the game board before the utterance, the immedi-
ately preceding system action, and the intended
interpretation of the utterance (as understood
by the Wizard) in the form of a semantic frame
specifying action-type and arguments, where
those arguments are objects occurring in the
description of the state of the board. The language
of the corpus is German. An example frame is
given in Figure 4.
176
The task that we want our model to perform is
as follows: given information about the state of
the world (i.e., game board), previous system ac-
tion, and the ongoing utterance, predict the values
of the frame. To this end, three slot values need
to be predicted, one of which links to the visual
scene. Each slot value will be predicted by an in-
dividual instantiation of our model (i.e., each has
a different I to predict). Generally, we want our
model to learn how language connects to the world
(given discourse context, visual context, domain
context, etc.). We used a combination of visual
properties (color, shape, and board position), and
simple properties to ground the utterance with I .
Our model gives probability distributions over
all possible slot values, but as we are interested
in single best candidates (or the special value
unknown if no guess can be made yet), we ap-
plied an additional decision rule to the output of
our model. If the probability of the highest candi-
date is below a threshold, unknown is returned,
otherwise that candidate is returned. Ties are bro-
ken by random selection. The thresholds for each
slot value were determined empirically on held-
out data so that a satisfactory trade-off between
letting through wrong predictions and changing
correct results to unknown was achieved.
Procedure All results were obtained by aver-
aging the results of a 10-fold validation on 1489
Pento boards (i.e., utterances+context, as in (Ken-
nington and Schlangen, 2012)). We used a sep-
arate set of 168 boards for small-scale, held-out
experiments. As this data set has been used
in previous work, we use previous results as
baselines/comparisons. For incremental process-
ing, we used InproTK (Baumann and Schlangen,
2012).4
On the incremental level, we followed
(Schlangen et al, 2009) and (Kennington and
Schlangen, 2012) for evaluation, but use a subset
of their incremental metrics, with a modification
on the edit overhead:
first correct: how deep into the utterance do we
make the first correct guess?
first final: how deep into the utterance do we
make the correct guess, and don?t subsequently
change our minds?
edit overhead: what is the ratio of unnecessary
edits / sentence length, where the only necessary
edit is that going from unknown to the final,
4http://sourceforge.net/projects/inprotk/
correct result anywhere in the sentence)?
Results The results for full utterances are given
in Table 4. Both of our model types work better
than (Heintze et al, 2010) which used support vec-
tor machines and conditional random fields, and
(Peldszus et al, 2012) which was rule-based (but
did not include utterances with pronouns like we
do here). The NB version did not work well in
comparison to (Kennington and Schlangen, 2012)
which used MLN, but the ME version did in most
metrics. Overall these are nice results as they
are achieved using a more straightforward model
with rather simple features (with room for exten-
sions). Another welcome result is performance
from noisy data (trained and evaluated on automat-
ically transcribed speech; ASR); the ME version of
our model is robust and performs well in compar-
ison to previous work.
NB ME K H P
fscore 81.16 92.26 92.18 76.9
(74.5) (89.4) (86.8)
slot 73.62 88.91 88.88
(66.4) (85.1) (81.6)
frame 42.57 74.08 74.76
(34.2) (67.2) (61.2)
action 80.05 93.62 92.62
object 76.27 90.79 84.71 64.3
result 64.4 82.34 86.65
Table 4: Comparison of results from Pento: Naive
Bayes NB, Maximum Entropy ME, (Kennington
and Schlangen, 2012) K, (Heintze et al, 2010)
H, (Peldszus et al, 2012) P; values in parenthe-
ses denote results from automatically transcribed
speech.
A big difference between our current model
and MLN is the way incrementality is realised:
MLN was restart incremental in that at each incre-
ment, features from the full utterance prefix were
used, not just the latest word; the present model is
fully incremental in that a prior belief is updated
based only on the new information. This, how-
ever, seems to lead our model to perform with less
accuracy for the result slot, which usually oc-
curs at the end of the sentence.
Incremental Table 5 shows the incremental
results in the same way as (Kennington and
Schlangen, 2012). Utterances are binned into
short, normal, and long utterance lengths (1-6,
7-8, 9-17 words, respectively) as determined by
looking at the distribution of utterance lengths,
which appeared as a normal distribution with 7 and
177
das graue Teil in der ersten Reihe nehmen
Figure 5: Example of reference resolution for the utterance: das graue Teil in der ersten Reihe nehmen /
the gray piece in the first row take; lighter cell background means higher probability assigned to piece.
8-word utterances having highest representation.
In comparison with (Kennington and Schlangen,
2012), our model generally takes longer to come
to a first correct for action, but is earlier for the
other two slots. For first final, our model always
takes longer, albeit with lower edit overhead. This
tells us that our model is more careful than the
MLN one; it waits longer before making a final de-
cision and it doesn?t change its mind as much in
the process, which arguably is desired behaviour
for incremental systems.
action 1-6 7-8 9-14
first correct (% into utt.) 5.78 2.56 3.64
first final (% into utt.) 38.26 36.10 30.84
edit overhead 2.37
object 1-6 7-8 9-14
first correct (% into utt.) 7.39 7.5 10.11
first final (% into utt.) 44.7 44.18 35.55
edit overhead 4.6
result 1-6 7-8 9-14
first correct (% into utt.) 15.16 23.23 20.88
first final (% into utt.) 42.55 40.57 35.21
edit overhead 10.19
Table 5: Incremental Results for Pento slots with
varying sentence lengths.
Figure 5 illustrates incremental performance by
showing the distribution over the pieces (using the
ME model; lighter means higher probability) for
the utterance das graue Teil in der ersten Reihe
nehmen (the gray piece in the first row take / take
the gray piece in the first row) for each word in
the utterance. When the first word, das is uttered,
it already assigns probabilities to the pieces with
some degree of confidence (note that in German,
das (the) denotes the neuter gender, and the piece
on the right with the lowest probability is often re-
ferred to by a noun (Treppe) other than neuter).
Once graue (gray) is uttered, the distribution is
now more even upon the three gray pieces, which
remains largely the same when Teil (piece) is ut-
tered. The next two words, in der (in the) give
more probability to the left gray piece, but once er-
sten Reihe (first row) is uttered, the most probable
piece becomes the correct one, the second piece
from the left on the top.
5.3 Puzzle Domain: Speech, Gaze and Deixis
Data and Task Our final experiment uses newly
collected data (Kousidis et al, 2013), again from
the Pentomino domain. In this Wizard-of-Oz
study, the participant was confronted with a Pento
game board containing 15 pieces in random col-
ors, shapes, and positions, where the pieces were
grouped in the four corners of the screen (exam-
ple in Figure 6). The users were seated at a table
in front of the screen. Their gaze was then cali-
brated with an eye tracker (Seeingmachines Face-
Lab) placed above the screen and their arm move-
ments (captured by a Microsoft Kinect, also above
the screen) were calibrated by pointing to each
corner of the screen, then the middle of the screen.
They were then given task instructions: (silently)
choose a Pento tile on the screen and then instruct
the computer game system to select this piece by
describing and pointing to it. When a piece was se-
lected (by the wizard), the participant had to utter
a confirmation (or give negative feedback) and a
new board was generated and the process repeated
(each instance is denoted as an episode). The ut-
terances, board states, arm movements, and gaze
information were recorded, as in (Kousidis et al,
2012). The wizard was instructed to elicit point-
ing gestures by waiting to select the participant-
referred piece by several seconds, unless a point-
ing action by the participant had already occurred.
When the wizard misunderstood, or a technical
problem arose, the wizard had an option to flag
the episode. In total, 1214 episodes were recorded
from 8 participants (all university students). All
but one were native speakers; the non-native spoke
proficient German (see Appendix for a set of ran-
dom example utterances).
The task in this experiment was reference res-
olution (i.e., filling a single-slot frame). The in-
formation available to our model for these data
include the utterance (ASR-transcribed and repre-
sented as words, bigrams, and trigrams), the vi-
178
Figure 6: Example Pento board for gaze and deixis
experiment; yellow piece in the top-right quadrant
has been ?selected? by the wizard after the partic-
ipant utterance.
sual context (game board), gaze information, and
deixis (pointing) information, where a rule-based
classifier predicted from the motion capture data
the quadrant of the screen at which the participant
was pointing. These data were very noisy (and
hence, realistic) despite the constrained conditions
of the task: the participants were not required to
say things a certain way (as long as it was under-
stood by the wizard); their hand movements poten-
tially covered their faces which interfered with the
eye tracker; each participant had a different way of
pointing (each had their own gesture space, hand-
edness, distance of hand from body when point-
ing, alignment of hand with face, etc.). Also, the
episodes were not split into individual utterances,
but rather interpreted as one; this indicates that the
model can deal with belief tracking over whole in-
teractions (here, if the wizard did not respond, the
participant had to clarify her intent in some way,
producing a new utterance).
Procedure Removing the flagged utterances and
the utterances of one of the participants (who had
misunderstood the task) left us with a total of 1051
utterances. We used 951 for development (fine-
tuning of parameters, see below), and 100 for eval-
uation. Evaluation was leave-one-out (i.e., 100
fold cross validation) where the training data were
all other 1050 utterances. For this experiment, we
only used the ME model as it performed much bet-
ter in the previous experiment. We give results
as resolution accuracy. We incorporate gaze and
deixis information in two ways: (1) We computed
the distribution over tiles gazed at, and quadrant
of the screen pointed at during the interval before
and during an utterance. The distributions were
then combined at the end of the utterance with the
NLU distribution (denoted as Gaze and Point); that
is, Gaze and Point had their own P (I) which were
evenly interpolated with the INLU P (I|U), and (2)
we incrementally computed properties to be pro-
vided to our INLU model; i.e., a tile has a prop-
erty in R of being looked at if it is gazed at for
some interval of time, or tiles in a quadrant of the
screen have the property of being pointed at.
These models are denoted as Gaze-F and Point-F.
As an example, Figure 7 shows an example utter-
ance, gaze, and gesture activity over time and how
they are reflected in the model (the utterance is the
observed U , where the gaze and gesture become
properties in R for the tiles that they affect). Our
baseline model is the NLU without using gaze or
deixis information; random accuracy is 7%.
We also include the percentage of the time
the gold tile is in the top 2 and top 4 rankings
(out of 15); situations in which a dialogue sys-
tem could at least provide alternatives in a clar-
ification request (if it could detect that it should
have low confidence in the best prediction; which
we didn?t investigate here). Importantly, these re-
sults are achieved with automatically transcribed
utterances; hand transcriptions do not yet exist for
these data. For gaze, we also make the naive as-
sumption that over the utterance the participant
(who in this case is the speaker) will gaze at his
chosen intended tile most of the time.
Figure 7: Human activity (top) aligned with how
modalities are reflected in the model for Gaze-F
and Point-F (bottom) over time for example utter-
ance: take the yellow tile.
Results See Table 6 for results. The models that
have access to gaze and pointing gestures can re-
solve better than those that do not. Our findings
are consistent in that referential success with gaze
alone approaches 20% (a rate found by (Pfeiffer,
2010) in a different setting). Another interest-
ing result is that the Gaze-F and Point-F variants,
that continuously integrate multi-modal informa-
tion, perform the same as or better than their non-
incremental counterparts (where the distributions
are weighted once at the end of the utterance).
179
Version Acc Top 2 Top 4
Gaze 18%
(baseline) NLU 50% 59% 77%
NLU + Gaze 53% 62% 80%
NLU + Point 52% 65% 90%
NLU + Gaze + Point 53% 70% 91%
NLU + Gaze-F 53% 65% 78%
NLU + Point-F 57% 68% 88%
NLU+Gaze-F+Point-F 56% 69% 85%
Table 6: Accuracies for reference resolution task
when considering NLU, gaze and pointing infor-
mation before and during the utterance (Gaze and
Point), and gaze and pointing information when
considered as properties to the NLU model (Gaze-
F and Point-F).
Incremental We also include incremental re-
sults when using gaze and deixis. We binned the
sentences in the same way as in the previous ex-
periment (the distribution of sentence lengths was
similar). Figure 8 shows how the NLU model base-
line, the (NLU+) Gaze-F, Point-F, and Gaze-F +
Point-F models perform incrementally for utter-
ances of lengths 7-8. All models increase mono-
tonically, except for Point-F at one point in the ut-
terance and Gaze-F at the end. It would appear that
the gaze as an information source is a good early
indicator of speaker intent, but should be trusted
less as the utterance progresses. Deixis is more
trustworthy overall, and the two taken together of-
fer a more stable model. Table 7 shows the re-
sults using the previously explained incremental
metrics. All models have little edit overhead, but
don?t make the correct final decision until well into
the utterances. This was expected due to the noisy
data. A consumer of the output of these models
would need to wait longer to trust the results given
by the models (though the number of words of the
utterance can never be known beforehand).
6 Discussion and Conclusions
We presented a model for the interpretation of
utterances in situated dialogue that a) works in-
crementally and b) can ground meanings in the
shared context. Taken together, the three experi-
ments we?ve reported give good evidence that our
model has the potential to be used as a success-
ful NLU component of an interactive dialogue sys-
tem. Our model can process at a speed which is
faster than the ongoing utterance, which will al-
low it to be useful in real-time, interactive exper-
iments. And, crucially, our model is able to inte-
Figure 8: Incremental process for referential accu-
racy; comparing NLU, Gaze-F, Point-F, and Gaze-
F + Point-F for utterances of length 7-8.
NLU 1-6 7-8 9-14
first correct (% into utt.) 22.2 37.2 30
first final (% into utt.) 82.4 82.4 74.8
edit overhead 2.95
Gaze-F 1-6 7-8 9-14
first correct (% into utt.) 23 32 31.1
first final (% into utt.) 84.1 81.5 75.4
edit overhead 2.89
Point-F 1-6 7-8 9-14
first correct (% into utt.) 21.4 30 23.3
first final (% into utt.) 83.5 80 72.3
edit overhead 2.59
Gaze-F + Point-F 1-6 7-8 9-14
first correct (% into utt.) 16.7 31 28
first final (% into utt.) 81.5 81 73.9
edit overhead 2.67
Table 7: Incremental results for Pento slots with
varying sentence lengths.
grate information from various sources, including
gaze and deixis. We expect the model to scale to
larger domains; the number of computations that
are required grows with |I| ? |R|.
Our model makes use of properties which are
used to connect an utterance to an intention.
Knowing which properties to use requires empir-
ical testing to determine which ones are useful.
We are working on developing principled meth-
ods for selecting such properties and their con-
tribution (i.e., properties should not be uniform).
Future work also includes better use of linguistics
(instead of just n-grams), building a more sophis-
ticated DBN model that has fewer independence
assumptions, e.g. tracking properties as well by
making Rt depended on Rt?1. We are also in
the process of using the model interactively; as a
proof-of-concept, we were trivially able to plug it
into an existing dialogue manager for Pento do-
mains (see (Bu? et al, 2010)).
180
Acknowledgements: Thanks to the anony-
mous reviewers for their useful comments and
feedback. This work was partially funded through
a DFG Emmy Noether grant.
Appendix A: Example Utterances (Pento
Speech)
1. nimm die Bru?cke in der oberen Reihe
2. nimm das Teil in der mittleren Reihe das zweite
Teil in der mittleren Reihe
3. und setz ihn in die Mitte links
4. dreh das nach links
5. a?hm und setz ihn oben links in die Ecke
6. nimm bitte den gelben Winkel oben
7. bewege das Ka?stchen die Treppe unten links
8. lo?sche das Teil in der Mitte
9. nimm die gelbe Kru?cke aus der zweiten Reihe
oben
10. und verschiebe es in die erste Zeile dritte
Spalte
Appendix B: Example Utterances (Speech,
Gaze and Deixis)
(as recognised by the ASR)
1. dieses teil genau st es oben links t
2. das t mit vier rechts oben ist d es direkt hier
rechts
3. gru?ne von rechts uh fla?che
4. das obere gru?ne za?hl hm so es obersten hohles
e rechts oben ecke
5. a?hm das hintere kreuz unten links rechts rechts
6. a?h das einzige blaue symbol oben rechts
7. das einzige gru?n okay oben rechts
8. hm innerhalb diesem blauen striche vorne hm
so genau in die genau rechts
9. und das sind dann nehmen diese fu?nf zeichen
oben na?mlich genau das in der mitte so
10. oben links is die untere
References
Gregory Aist, James Allen, Ellen Campana, Car-
los Gomez Gallo, Scott Stoness, Mary Swift, and
Michael K Tanenhaus. 2007. Incremental under-
standing in human-computer dialogue and experi-
mental evidence for advantages over nonincremen-
tal methods. In Proceedings of Decalog (Semdial
2007), Trento, Italy.
Timo Baumann and David Schlangen. 2012. The In-
proTK 2012 Release. In NAACL.
Okko Bu? Timo Baumann, and David Schlangen.
2010. Collaborating on Utterances with a Spoken
Dialogue System Using an ISU-based Approach to
Incremental Dialogue Management. In Proceedings
of SIGdial, pages 233?236.
Deborah A Dahl, Madeleine Bates, Michael Brown,
William Fisher, Kate Hunicke-Smith, David Pallett,
Christine Pao, Alexander Rudnicky, and Elizabeth
Shriberg. 1994. Expanding the scope of the ATIS
task: the ATIS-3 corpus. In Proceedings of the
workshop on Human Language Technology, HLT
?94, pages 43?48, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Renato De Mori, Frederic Be?chet, Dilek Hakkani-tu?r,
Michael Mctear, Giuseppe Riccardi, and Gokhan
Tur. 2008. Spoken Language Understanding. IEEE
Signal Processing Magazine, pages 50?58, May.
David DeVault, Kenji Sagae, and David Traum. 2009.
Can I finish?: learning when to respond to incremen-
tal interpretation results in interactive dialogue. In
Proceedings of the 10th SIGdial, pages 11?20. As-
sociation for Computational Linguistics.
David DeVault, Kenji Sagae, and David Traum. 2011.
Incremental Interpretation and Prediction of Utter-
ance Meaning for Interactive Dialogue. Dialogue &
Discourse, 2(1):143?170.
Raquel Ferna?ndez, Tatjana Lucht, and David
Schlangen. 2007. Referring under restricted
interactivity conditions. In Proceedings of the 8th
SIGdial, pages 136?139.
Silvan Heintze, Timo Baumann, and David Schlangen.
2010. Comparing local and sequential models for
statistical incremental natural language understand-
ing. In Proceedings of the 11th SIGdial, pages 9?16.
Association for Computational Linguistics.
Kai-yuh Hsiao, Soroush Vosoughi, Stefanie Tellex,
Rony Kubat, and Deb Roy. 2008. Object schemas
for grounding language in a responsive robot. Con-
nection Science2, 20(4):253?276.
Guangpu Huang and Meng Joo Er. 2010. A Hybrid
Computational Model for Spoken Language Under-
standing. In 11th International Conference on Con-
trol, Automation, Robotics, and Vision, pages 7?10,
Singapore. IEEE.
Casey Kennington and David Schlangen. 2012.
Markov Logic Networks for Situated Incremental
Natural Language Understanding. In Proceedings
of the 13th SIGdial, pages 314?323, Seoul, South
Korea, July. Association for Computational Linguis-
tics.
Spyros Kousidis, Thies Pfeiffer, Zofia Malisz, Petra
Wagner, and David Schlangen. 2012. Evaluat-
ing a minimally invasive laboratory architecture for
recording multimodal conversational data. In Proc.
of the Interdisciplinary Workshop on Feedback Be-
haviours in Dialogue.
181
Spyros Kousidis, Casey Kennington, and David
Schlangen. 2013. Investigating speaker gaze and
pointing behaviour in human-computer interaction
with the mint.tools collection. In Proceedings of the
14th SIGdial.
Percy Liang, Michael Jordan, and Dan Klein. 2011.
Learning Dependency-Based Compositional Se-
mantics. In Proceedings of the 49th ACLHLT, pages
590?599, Portland, Oregon. Association for Compu-
tational Linguistics.
Changsong Liu, Rui Fang, and Joyce Chai. 2012. To-
wards Mediating Shared Perceptual Basis in Situ-
ated Dialogue. In Proceedings of the 13th SIGdial,
pages 140?149, Seoul, South Korea, July. Associa-
tion for Computational Linguistics.
Yi Ma, Antoine Raux, Deepak Ramachandran, and
Rakesh Gupta. 2012. Landmark-Based Location
Belief Tracking in a Spoken Dialog System. In Pro-
ceedings of the 13th SIGdial, pages 169?178, Seoul,
South Korea, July. Association for Computational
Linguistics.
Marie-Jean Meurs, Frederic Duvert, Fabrice Lefevre,
and Renato De Mori. 2008. Markov Logic Net-
works for Spoken Language Interpretation. Infor-
mation Systems Journal, pages 535?544.
Marie-Jean Meurs, Fabrice Lefe`vre, and Renato De
Mori. 2009. Spoken Language Interpretation: On
the Use of Dynamic Bayesian Networks for Seman-
tic Composition. In IEEE International Conference
on Acoustics, Speech, and Signal Processing, pages
4773?4776.
Ivan Meza-Ruiz, Sebastian Riedel, and Oliver Lemon.
2008. Accurate Statistical Spoken Language Un-
derstanding from Limited Development Resources.
In IEEE International Conference on Acoustics,
Speech, and Signal Processing, pages 5021?5024.
IEEE.
Andreas Peldszus, Okko Bu?, Timo Baumann, and
David Schlangen. 2012. Joint Satisfaction of Syn-
tactic and Pragmatic Constraints Improves Incre-
mental Spoken Language Understanding. In Pro-
ceedings of the 13th EACL, pages 514?523, Avi-
gnon, France, April. Association for Computational
Linguistics.
Thies Pfeiffer. 2010. Understanding multimodal deixis
with gaze and gesture in conversational interfaces.
Ph.D. thesis, Bielefeld University.
Zahar Prasov and Joyce Y Chai. 2010. Fusing Eye
Gaze with Speech Recognition Hypotheses to Re-
solve Exophoric References in Situated Dialogue.
In EMNLP 2010, number October, pages 471?481.
Deb Roy. 2005. Grounding words in perception and
action: computational insights. Trends in Cognitive
Sciences, 9(8):389?396, August.
David Schlangen and Gabriel Skantze. 2009. A Gen-
eral, Abstract Model of Incremental Dialogue Pro-
cessing. In Proceedings of the 10th EACL, pages
710?718, Athens, Greece. Association for Compu-
tational Linguistics.
David Schlangen, Timo Baumann, and Michaela At-
terer. 2009. Incremental Reference Resolution: The
Task, Metrics for Evaluation, and a Bayesian Filter-
ing Model that is Sensitive to Disfluencies. In Pro-
ceedings of the 10th SIGdial, pages 30?37, London,
UK. Association for Computational Linguistics.
Alexander Siebert and David Schlangen. 2008. A Sim-
ple Method for Resolution of Definite Reference in
a Shared Visual Context. In Proceedings of the 9th
SIGdial, pages 84?87, Columbus, Ohio. Association
for Computational Linguistics.
Gokhan Tur, Dilek Hakkani-tu?r, and Larry Heck. 2010.
What Is Left to Be Understood by ATIS? In IEEE
Workshop on Spoken Language Technologies, pages
19?24, Berkeley, California. IEEE.
Ye-Yi Wang, Li Deng, and Alex Acero. 2011. Seman-
tic Frame-based Spoken Language Understanding.
Wiley.
Jason D Williams. 2010. Incremental partition re-
combination for efficient tracking of multiple dia-
log states. Acoustics Speech and Signal Processing
ICASSP 2010, pages 5382?5385.
Luke S Zettlemoyer and Michael Collins. 2007. On-
line Learning of Relaxed CCG Grammars for Pars-
ing to Logical Form. Computational Linguistics,
pages 678?687.
Luke S Zettlemoyer and Michael Collins. 2009.
Learning context-dependent mappings from sen-
tences to logical form. Proceedings of the Joint
Conference of the 47th ACL and the 4th AFNLP:
Volume 2 - ACL-IJCNLP ?09, 2:976.
182
Proceedings of the SIGDIAL 2013 Conference, pages 319?323,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Investigating speaker gaze and pointing behaviour
in human-computer interaction with the mint.tools collection
Spyros Kousidis Casey Kennington David Schlangen
Dialogue Systems Group / CITEC / SFB 673
Bielefeld University
spyros.kousidis@uni-bielefeld.de
Abstract
Can speaker gaze and speaker arm move-
ments be used as a practical informa-
tion source for naturalistic conversational
human?computer interfaces? To investi-
gate this question, we recorded (with eye
tracking and motion capture) a corpus of
interactions with a (wizarded) system. In
this paper, we describe the recording, anal-
ysis infrastructure that we built for such
studies, and analysis we performed on
these data. We find that with some initial
calibration, a ?minimally invasive?, sta-
tionary camera-based setting provides data
of sufficient quality to support interaction.
1 Introduction
The availability of sensors such as Microsoft
Kinect and (almost) affordable eye trackers bring
new methods of naturalistic human-computer in-
teraction within reach. Studying the possibilities
of such methods requires building infrastructure
for recording and analysing such data (Kousidis et
al., 2012a). We present such an infrastructure?
the mint.tools collection (see also (Kousidis et
al., 2012b))1?and present results of a study we
performed on whether speaker gaze and speaker
arm movements can be turned into an information
source for an interactive system.
2 The mint.tools Collection
The mint.tools collection comprises tools (and
adaptations to existing tools) for recording and
analysis of multimodal data. The recording archi-
tecture (Figure 1) is highly modular: each infor-
mation source (sensor) runs on its own dedicated
workstation and transmits its data via the local area
network. In the setup described in this paper, we
1Available at http://dsg-bielefeld.de/
mint/.
kinect.srv
faceLab.srv
MINT.tools
f
o
r
 
a
c
q
u
i
s
i
t
i
o
n
f
o
r
 
a
n
a
l
y
s
i
s
instantIO
instant
player
kinect.srv
faceLab.srv
.xio
mumodo.py /
IPython
ELAN.mod
MPI, ELAN
annotation
tool
fame.rc
fame.rp
Fraunhofer instant reality
Figure 1: Overview of components of mint.tools;
our contributions denoted by italics font. Top mid-
dle shows example lab setup; middle right shows
corresponding VR scene, visualising motion cap-
ture and tracking of head posture, eye and gaze
perform motion capture via Microsoft Kinect and
head, eye and gaze tracking via Seeingmachines
Facelab 5.2 We have developed specialised plug-
ins that connect these sensors to the central com-
ponent in our architecture, Instantreality.3 This
is a VR environment we use for monitoring the
recording process by visualising a reconstructed
3D scene in real-time. A logging component si-
multaneously streams the timestamped and inte-
grated sensor data to disk, ensuring that all data are
synchronised. The data format is a shallow XML
representation of timed, typed events.
The tracking equipment used in this setting is
camera-based, providing for a minimally invasive
setting, as subjects are not required to wear any
equipment or tracking markers. In addition to the
tracking sensors, video and audio are recorded us-
2http://www.microsoft.com/en-us/
kinectforwindows/, http://www.
seeingmachines.com/product/facelab/, re-
spectively
3Built by IGD Fraunhofer, http://www.
instantreality.org
319
ing one HD camera. The AV channel is synchro-
nised with the stream data from the sensors by
means of a timecode in view of the camera.
Representative of the high modularity and flexi-
bility of the mint.tools architecture is the ease with
which components can be added. For the setting
described here, a GUI was created which connects
to the VR environment as an additional sensor,
transmitting all of its state updates, which then
are synchronously logged together with all other
stream data from the trackers. This allows us to
recreate the full scene (subject behaviour and the
stimuli they received) in the virtual reality envi-
ronment, for later inspection (see below Figure 6).
The analysis part of the mint.tools collection
comprises a package for the Python programming
language (described below) and a version of the
ELAN annotation tool (Lausberg and Sloetjes,
2009), which we modified to control the replay of
the virtual reality scene; this makes it possible to
view video, annotations and the 3D reconstruction
at the same time and in synchronisation.
Sensors are represented as nodes in a node-tree
within the 3D environment. The values of data
fields in these nodes are continuously updated as
new data is received from the network. Using
more than one sensor of the same type means sim-
ply another instantiation of that node type within
the tree. In this way, our architecture facilitates
tracking many people or complex setups where
many sensors are required to cover an area.
3 Procedure / The TAKE Corpus
Our experiment is a Wizard-of-Oz scenario in
which subjects (7 in total) were situated in front of
a 40? screen displaying random Pentomino boards
(Ferna?ndez et al, 2007). Each board configura-
tion had exactly 15 Pentomino pieces of various
colours and shapes, divided in four grids located
near the four corners of the screen (see Figure 3
below). At the beginning of the session, a head and
gaze model were created for the subject within the
FaceLab software. Next, the subjects were asked
to point (with their arm stretched) at the four cor-
ners and the center of the screen (with each hand),
to calibrate to their pointing characteristics.
In the main task, subjects were asked to
(silently) choose a piece and instruct the ?system?
to select it, using speech and/or pointing gestures.
A wizard then selected the indicated piece, caus-
ing it to be highlighted. Upon approval by the
subject, the wizard registered the result and a new
board was created. We denote the time-span from
the creation of a board to the acknowledgement
by the subject that the correct piece was selected
an episode. The wizard had the option to not im-
mediately highlight the indicated piece, in order
to elicit a more detailed description of the piece
or a pointing gesture. What we were interested
in learning from these data was whether speaker
gaze and arm movements could be turned into sig-
nals that can support a model of situated language
understanding. We focus here on the signal pro-
cessing and analysis that was required; the model
is described in (Kennington et al, 2013).
4 Analysis and Results
We perform the analyses described in this sec-
tion using the analysis tools in the mint.tools col-
lection, mumodo.py. This is a python package
we have developed that interfaces our recorded
stream data with powerful, freely available, sci-
entific computing tools written in the Python pro-
gramming language.4 mumodo.py facilitates im-
porting streamed data into user-friendly, easily
manageable structures such as dataframes (tables
with extended database functionality), or compati-
ble formats such as Praat TextGrids (Boersma and
Weenink, 2013) and ELAN tiers. In addition, mu-
modo.py can remote-control playback in ELAN
and Instant Reality for the purpose of data view-
ing and annotation.
4.1 Gaze
Our post-processing and analysis of the gaze data
focuses primarily on the detection of eye fixations
in order to determine the pentomino pieces that the
subjects look at while speaking. This knowledge
is interesting from a reference resolution point of
view. Although Koller et al(2012) explored lis-
tener gaze in that context, it is known that gaze pat-
terns differ in interactions, depending on whether
one speaks or listens (Jokinen et al, 2009).
Facelab provides a mapping between a person?s
gaze vector and the screen, which yields an in-
tersection point in pixel coordinates. However,
due to limitations to the accuracy of the calibra-
tion procedure and noise in the data, it is pos-
4Especially IPython and Pandas, as collected for exam-
ple in https://www.enthought.com/products/
epd/. Example of finished analyses using this package
can be found at http://dsg-bielefeld.de/mint/
mintgaze.html
320
sible that the gaze vector does not intersect the
model of the screen when the subject is looking at
pieces near screen corners. For this reason, we first
perform offline linear interpolation, artificially ex-
tending the screen by 200 pixels in each direction,
by means of linear regression of the x, y compo-
nents of the gaze vector with the x, y pixel coordi-
nates, respectively (R2 > 0.95 in all cases). Fig-
ure 2 shows the probability density function of in-
tersection points before (left) and after this process
(right), for one of the subjects. We see on the right
plot that many intersection points fall outside the
viewable screen area, denoted by the shaded rect-
angle.
Figure 2: Probability density function of gaze in-
tersections on screen before (left) and after inter-
polating for points 200 pixels around screen edges
(right). Shaded rectangle shows screen size
In order to detect the eye fixations, we use two
common algorithms, namely the I-DT and ve-
locity algorithms, as described in (Nystro?m and
Holmqvist, 2010). The I-DT algorithm requires
the points to lie within a pre-defined ?dispersion?
area (see Figure 3), while the velocity algorithm
requires the velocity to remain below a thresh-
old. In both algorithms, a minimum fixation time
threshold is also used, while a fixation centroid is
calculated as the midpoint of all points in a fixa-
tion. Increasing the minimum fixation time thresh-
old and decreasing the dispersion area or velocity
(depending on the algorithm) results in fewer fix-
ations being detected.
Figure 3: Fixation detection using the I-DT algo-
rithm, circles show the dispersion radius threshold
Gaze fixations can be combined with informa-
tion on the pentomino board in order to determine
which piece is being looked at. To do this, we cal-
culate the euclidean distance between each piece
and the fixation centroid, and assign the piece a
probability of being gazed at, which is inversely
proportional to its distance from the centroid.
Figure 4 illustrates the gazing behaviour of the
subjects during 1051 episodes: After an initial
rapid scan of the whole screen (typically before
they start speaking), subjects fixate on the piece
they are going to describe (the ?gold piece?). This
is denoted by the rising number of fixations on the
gold piece between seconds 5?10. At the same
time, the average rank of the gold piece is higher
(i.e. closer to 1, hence lower in the plot). Subse-
quently, the average rank drops as subjects tend to
casually look around the screen for possible dis-
tractors (i.e. pieces that are identical or similar to
the gold piece).
We conclude from this analysis that, especially
around the onset of the utterance, gaze can provide
a useful signal about intended referents.
Figure 4: Average Rank and Counts over time (all
episodes)
4.2 Pointing Gestures
We detect pointing gestures during which the arm
is stretched from Kinect data (3D coordinates of
20 body joints) using two different methods. The
first is based on the distance of the hand joint from
the body (Sumi et al, 2010). We define the body
as a plane, using the coordinates of the two shoul-
ders, shoulder-center and head joints, and use a
threshold beyond which a movement is considered
a possible pointing gesture.
The second detection method uses the idea that,
while the arm is stretched, the vectors defined by
the hand and elbow, and hand and shoulder joints,
respectively, should be parallel, i.e. have a dot
product close to 1 (vectors are first normalised).
321
Figure 5: detection of pointing thresholds by dis-
tance of left(blue) or right(green) hand from body
In reality, the arm is never strictly a straight line,
hence a threshold (0.95-0.98) is set, depending on
the subject. The result of this process is an an-
notation tier of pointing gestures (for each hand),
similar to the one shown in Figure 5. To make
pointing gesture detection more robust, we only
consider gestures identified by both methods, i.e.
the intersection of the two annotation tiers.
Further, we want to map the pointing gestures to
locations on the screen. Following a methodology
similar to Pfeiffer (2010), we define two methods
of determing pointing direction: (a) the extension
of the arm, i.e. the shoulder-hand vector, and (b)
the hand-head vector, which represents the subjec-
tive point-of-view (looking through the tip of one?s
finger). Figure 6 shows both vectors: depending
on the subject and the target point, we have found
that both of these vectors perform equally well, by
considering the gaze intersection point (green dot
on screen) and assuming that subjects are looking
where they are pointing.
Figure 6: Hand-to-head and hand-to-shoulder
pointing vectors
In order to map the pointing gestures to ac-
tual locations on the screen, we use the calibra-
tion points acquired at the beginning of the ses-
sion, and plot their intersections to the screen
plane, which we compute analytically, as we al-
ready have a spatial model of both the vector in
question (Kinect data) and the screen location (In-
stantreality model).
Based on the pointing gestures we have de-
tected, we look at the pointing behaviour of par-
ticipants as a function of the presence of distrac-
tors. This knowledge can be used in designing
system responses in a multimodal interactive en-
viroment or in training models to expect pointing
gestures depending on the state of the scene. Fig-
ure 7 shows the result from 868 episodes (a subset
that satisfies minor technical constraints). Overall,
the subjects pointed in 60% of all episodes. Pieces
on the board may share any of three properties:
shape, colour, and location (being in the same cor-
ner on the screen). The left plot shows that sub-
jects do not point more than normal when only
one property is shared, regardless of how many
such distractors are present, while they point in-
creasingly more when pieces that share two or all
three properties exist. The plot on the right shows
that subjects point more when the number of same
colour pieces increases (regardless of position and
shape) and even more when identical pieces occur
anywhere on the board. Interestingly, shape by it-
self does not appear to be considered a distractor
by the subjects.
Figure 7: Frequency of pointing gestures as a
function of the presence of distractors. Dot size
denotes the confidence of each point, based on
sample size
5 Conclusions
We have presented a detailed account of analysis
procedures on multimodal data acquired from ex-
periments in situated human-computer interaction.
These analyses have been facilitated by mint.tools,
our collection of software components for mul-
timodal data acquisition, annotation and analysis
and put to use in (Kennington et al, 2013). We
will continue to further improve our approach for
manageable and easily reproducible analysis.
322
References
Paul Boersma and David Weenink. 2013. Praat: do-
ing phonetics by computer (version 5.3.48)[com-
puter program]. retrieved may 1, 2013.
Raquel Ferna?ndez, Andrea Corradini, David
Schlangen, and Manfred Stede. 2007. To-
wards Reducing and Managing Uncertainty in
Spoken Dialogue Systems. In Proceedings of the
7th International Workshop on Computational
Semantics (IWCS?07), pages 1?3.
Kristiina Jokinen, Masafumi Nishida, and Seiichi Ya-
mamoto. 2009. Eye-gaze experiments for conversa-
tion monitoring. In Proceedings of the 3rd Interna-
tional Universal Communication Symposium, pages
303?308. ACM.
Casey Kennington, Spyros Kousidis, and David
Schlangen. 2013. Interpreting situated dialogue ut-
terances: an update model that uses speech, gaze,
and gesture information. In Proceedings of SIGdial
2013.
Alexander Koller, Maria Staudte, Konstantina Garoufi,
and Matthew Crocker. 2012. Enhancing referen-
tial success by tracking hearer gaze. In Proceed-
ings of the 13th Annual Meeting of the Special Inter-
est Group on Discourse and Dialogue, pages 30?39.
Association for Computational Linguistics.
Spyros Kousidis, Thies Pfeiffer, Zofia Malisz, Petra
Wagner, and David Schlangen. 2012a. Evaluat-
ing a minimally invasive laboratory architecture for
recording multimodal conversational data. In Proc.
of the Interdisciplinary Workshop on Feedback Be-
haviours in Dialogue.
Spyros Kousidis, Thies Pfeiffer, and David Schlangen.
2012b. Mint.tools: Tools and adaptors supporting
acquisition, annotation and analysis of multimodal
corpora. In to appear in Proc. of Interspeech 2013.
Hedda Lausberg and Han Sloetjes. 2009. Coding ges-
tural behavior with the neuroges-elan system. Be-
havior research methods, 41(3):841?849.
Marcus Nystro?m and Kenneth Holmqvist. 2010. An
adaptive algorithm for fixation, saccade, and glis-
sade detection in eyetracking data. Behavior re-
search methods, 42(1):188?204.
Thies Pfeiffer. 2010. Understanding multimodal deixis
with gaze and gesture in conversational interfaces.
Ph.D. thesis, Bielefeld University, Technical Fac-
ulty.
Yasuyuki Sumi, Masaharu Yano, and Toyoaki Nishida.
2010. Analysis environment of conversational struc-
ture with nonverbal multimodal data. In Interna-
tional Conference on Multimodal Interfaces and the
Workshop on Machine Learning for Multimodal In-
teraction, page 44. ACM.
323
Proceedings of the of the EACL 2014 Workshop on Dialogue in Motion (DM), pages 68?72,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Situationally Aware In-Car Information Presentation
Using Incremental Speech Generation: Safer, and More Effective
Spyros Kousidis
1
, Casey Kennington
1,2
, Timo Baumann
4
, Hendrik Buschmeier
2,3
,
Stefan Kopp
2,3
, and David Schlangen
1
1
Dialogue Systems Group,
2
CITEC,
3
Sociable Agents Group ? Bielefeld University
4
Department of Informatics, Natural Language Systems Division ? University of Hamburg
spyros.kousidis@uni-bielefeld.de
Abstract
Holding non-co-located conversations
while driving is dangerous (Horrey and
Wickens, 2006; Strayer et al., 2006),
much more so than conversations with
physically present, ?situated? interlocutors
(Drews et al., 2004). In-car dialogue
systems typically resemble non-co-located
conversations more, and share their
negative impact (Strayer et al., 2013). We
implemented and tested a simple strategy
for making in-car dialogue systems aware
of the driving situation, by giving them
the capability to interrupt themselves
when a dangerous situation is detected,
and resume when over. We show that this
improves both driving performance and
recall of system-presented information,
compared to a non-adaptive strategy.
1 Introduction
Imagine you are driving on a relatively free high-
way at a constant speed and you are talking with the
person next to you. Suddenly, you need to overtake
another car. This requires more attention from you;
you check the mirrors before you change lanes, and
again before you change back. Plausibly, an attent-
ive passenger would have noticed your attention
being focused more on the driving, and reacted to
this by interrupting their conversational contribu-
tion, resuming when back on the original lane.
Using a driving simulation setup, we implemen-
ted a dialogue system that realises this strategy. By
employing incremental output generation, the sys-
tem can interrupt and flexibly resume its output.
We tested the system using a variation of a stand-
ard driving task, and found that it improved both
driving performance and recall, as compared to a
non-adaptive baseline system.
Figure 1: Overview of our system setup: human
controls actions of a virtual car; events are sent to
DM, which controls the speech output.
2 The Setup
2.1 The Situated In-Car System
Figure 1 shows an overview of our system setup,
with its main components: a) the driving simulator
that presents via computer graphics the driving task
to the user; b) the dialogue system, that presents,
via voice output, information to the user (here, cal-
endar entries).
Driving Simulation For the driving simulator,
we used the OpenDS Toolkit,
1
connected to a steer-
ing wheel and a board with an acceleration and
brake pedal, using standard video game hardware.
We developed our own simple driving scenarios
(derived from the ?ReactionTest? task, which is dis-
tributed together with OpenDS) that specified the
driving task and timing of the concurrent speech,
as described below. We modified OpenDS to pass
real-time data (e.g. car position/velocity/events in
the simulation, such as a gate becoming visible
or a lane change) using the mint.tools architec-
ture (Kousidis et al., 2013). In addition, we have
bridged INPROTK (Baumann and Schlangen, 2012)
with mint.tools via the Robotics Service Bus (RSB,
Wienke and Wrede (2011)) framework.
1http://www.opends.eu/
68
Figure 2: Driver?s view during experiment. The
green signal on the signal-bridge indicates the tar-
get lane.
Dialogue System Using INPROTK, we imple-
mented a simple dialogue system. The notion of
?dialogue? is used with some liberty here: the user
did not interact directly with the system but rather
indirectly (and non-intentionally) via driving ac-
tions. Nevertheless, we used the same modularisa-
tion as in more typical dialogue systems by using a
dialoge management (DM) component that controls
the system actions based on the user actions. We
integrated OpenDial (Lison, 2012) as the DM into
INPROTK,
2
though we only used it to make simple,
deterministic decisions (there was no learned dia-
logue policy) based on the state of the simulator
(see below). We used the incremental output gen-
eration capabilities of INPROTK, as described in
(Buschmeier et al., 2012).
3 Experiment
We evaluated the adaptation strategy in a driving
simulation setup, where subjects performed a 30
minute, simulated drive along a straight, five-lane
road, during which they were occasionally faced
with two types of additional tasks: a lane-change
task and a memory task, which aim to measure the
driving performance and the driver?s ability to pay
attention to speech while driving, respectively. The
two tasks occured in isolation or simultaneoulsy.
The Lane-Change Task The driving task we
used is a variant of the well-known lane-change
task (LCT), which is standardised in (ISO, 2010):
It requires the driver to react to a green light posi-
tioned on a signal gate above the road (see Figure 2).
The driver (otherwise instructed to remain in the
middle lane) must move to the lane indicated by
2
OpenDial can be found at http://opendial.
googlecode.com/.
Table 1: Experiment conditions.
Lane Change Presentation mode Abbreviation
Yes CONTROL CONTROL_LANE
Yes ADAPTIVE ADAPTIVE_LANE
Yes NO_TALK NO_TALK_LANE
No CONTROL CONTROL_EMPTY
the green light, remain there until a tone is sounded,
and then return again to the middle lane. OpenDS
gives a success or fail result to this task depending
on whether the target lane was reached within 10
seconds (if at all) and the car was in the middle lane
when the signal became visible. We also added a
speed constraint: the car maintained 40 km/h when
the pedal was not pressed, with a top speed of 70
km/h when fully pressed. During a Lane-change,
the driver was to maintain a speed of 60 km/h, thus
adding to the cognitive load.
The Memory Task We tested the attention of
the drivers to the generated speech using a simple
true-false memory task. The DM generated utter-
ances such as ?am Samstag den siebzehnten Mai
12 Uhr 15 bis 14 Uhr 15 hast du ?gemeinsam Essen
im Westend mit Martin? ? (on Saturday the 17th
of May from 12:15?14:15 you are meeting Mar-
tin for Lunch). Each utterance had 5 information
tokens: day, time, activity, location and partner,
spoken by a female voice. After utterance comple-
tion, and while no driving distraction occurred, a
confirmation question was asked by a male voice,
e.g. ?Richtig oder Falsch? ? Freitag? (Right or
wrong? ? Friday). The subject was then required
to answer true or false by pressing one of two re-
spective buttons on the steering wheel. The token
of the confirmation question was chosen randomly,
although tokens near the beginning of the utterance
(day and time) were given a higher probability of
occurrence. The starting time of the utterance re-
lative to the gate was varied randomly between 3
and 6 seconds before visibility. Figure 3 gives a
schematic overview of the task and describes the
strategy we implemented for interrupting and re-
suming speech, triggered by the driving situation.
3.1 Conditions
Table 1 shows the 4 experiment conditions, de-
noting if a lane change was signalled, and what
presentation strategy was used. Each condition ap-
peared exactly 11 times in the scenario, for a total
of 44 episodes. The order of episodes was randomly
69
t1
t
2
suc
gate
lane t
3
0
1
2
3
4
am Samstag den siebzehn- den siebzehnten Mai ?
am Samstag den siebzehnten Mai um 12 Uhr hast du ?Besprechung mit Peter?
ADAPTIVE
CONTROL
Figure 3: Top view of driving task: as the car moves to the right over time, speech begins at t
1
, the gate with
the lane-change indicator becomes visible at t
2
, where in the adaptive version speech pauses. Successful
lane change is detected at suc; successful change back to the middle lane is detected at lane, and resumes.
(If no change back is detected, the interruption times out at t
3
). All red-dotted lines denote events sent
from OpenDS to the Dialogue Manager.
generated for each subject. With this design, sub-
jects perceive conditions to be entirely random.
3.2 Dependent Variables
The dependent variables for the Memory task
are (a) whether the subject?s answer was correct
(true/false), and (b) the response delay, which is
the time from the end of the clarification ques-
tion to the time the true or false button was
pressed. For the driving task, the dependent vari-
ables are the OpenDS performance measurements
success/failure (as defined above) and reaction time
(time to reach the target lane).
3.3 Procedure
After signing a consent form, subjects were led into
the experiment room, where seat position and audio
level were adjusted, and were given written instruc-
tions. Next, the OpenDS scenario was initiated. The
scenario started with 10 successive lane-change sig-
nal gates without speech, for driving training. An
experimenter provided feedback during training
while the subjects familiarized themselves with the
driving task. Following the training gates came a
clearly-marked ?START? gate, signifying the be-
ginning of the experiment to the subjects (at this
point, the experimenter left). There was a ?FINISH?
gate at the end of the scenario. The whole stretch of
road was 23 km and took approximately 30 minutes
to complete. After the driving task, the subjects
were given a questionnaire, which asked them to
identify the information presentation strategies and
assign a preference.
Table 2: Subjects?
judgement of task
difficulty.
Diff. Freq.
4 (easy) 8
3 7
2 1
1 (hard) 1
Table 3: Subjects? system
preference.
Preference Freq.
ADAPTIVE 3
CONTROL 9
Neither 5
4 Results
In total, 17 subjects (8 male, 9 female, aged 19-
36) participated in the study. All of the subjects
were native German speakers affiliated with AN-
ONYMIZED University. As reported in the post-test
questionnaire, all held a driving license, two had
previous experience with driving simulators and
only one had previous experience with spoken dia-
logue systems. Table 2 shows the subjects? assess-
ment of difficulty, while Table 3 shows their prefer-
ence between the different strategies. Most subjects
found the task relatively easy and either prefer the
speech not to adapt or have no preference.
Memory task The overall percentages of correct
answers to the system?s recall questions (across all
subjects) are shown in Table 4. We see that the sub-
jects? performance in this task is considerably bet-
ter when the system adapts to the driving situation
(ADAPTIVE_LANE condition) rather than speaking
through the lane change (CONTROL_LANE con-
dition). In fact, the performance in the ADAPT-
IVE_LANE condition is closer to the control upper
70
Table 4: Performance in memory task per condi-
tion.
Condition Percentage
CONTROL_EMPTY 169/180 (93.9%)
ADAPTIVE_LANE 156/172 (90.7%)
CONTROL_LANE 150/178 (84.3%)
Table 5: Success in driving task per condition (as
reported by OpenDS).
Condition Success
NOTALK_LANE 175/185 (94.6%)
ADAPTIVE_LANE 165/174 (94.8%)
CONTROL_LANE 165/180 (91.7%)
bound (CONTROL_EMPTY condition). We tested
significance of the results using a generalized lin-
ear mixed model with CONDITION and SUBJECT
as factors, which yields a p-value of 0.027 when
compared against a null model in which only SUB-
JECT is a factor. No significant effects of between-
subjects factors gender, difficulty or preference
were found. In addition, the within-subject variable
time did not have any significant effect (subjects do
not improve in the memory task with time).
The average response delay (from the end of
the recall question to the button press) per condi-
tion across all subjects is shown in Figure 4. Sub-
jects reply slower to the recall questions in the
CONTROL_LANE condition, while their perform-
ance in the ADAPTIVE_LANE condition is indis-
tinguishable from the CONTROL_EMPTY condi-
tion (in which there is no distraction). Addition-
ally, there is a general decreasing trend of response
delay with time, which means that users get ac-
quainted with the task (type of information, format
of question) over time. Both factors (condition
and time) are significant (repeated measures AN-
OVA, 2x2 factorial design, F
condition
= 3.858, p =
0.0359,F
time
= 4.672, p= 0.00662). No significant
effects were found for any of the between-subject
factors (gender, difficulty, preference).
Driving task The success rate in the lane-change
task per condition is shown in Table 5. Here too
we find that the performance is lower in the CON-
TROL_LANE condition, while ADAPTIVE_LANE
does not seem to affect driving performance, when
compared to the NOTALK_LANE condition. The
effect is significant (p = 0.01231) using the same
GLMM approach and factors as above.
ADAPTIVE_LANE CONTROL_EMPTY CONTROL_LANECondition0
500
1000
1500
2000
2500
3000
3500
4000
User
 Res
pons
e De
lay (
ms)
Figure 4: User answer response delay under three
conditions.
5 Discussion, Conclusions, Future Work
We have developed and tested a driving simula-
tion scenario where information is presented by a
spoken dialogue system. Our system has the unique
ability (compared to today?s commercial systems)
to adapt its speech to the driving situation: it in-
terrupts itself when a dangerous situation occurs
and later resumes with an appropriate continuation.
Using this strategy, information presentation had
no impact on driving, and dangerous situations no
impact on information recall. In contrast, a system
that blindly spoke while the driver was distracted
by the lane-change task resulted in worse perform-
ance in both tasks: subjects made more errors in
the memory task and also failed more of the lane-
change tasks, which could prove dangerous in a
real situation.
Interestingly, very few of the subjects preferred
the adaptive version of the system in the post-task
questionnaire. Among the reasons that they gave
for this was their inability to control the interrup-
tions/resumptions of the system. We plan to ad-
dress the issue of control by allowing future ver-
sions of our system to accept user signals, such as
speech or head gestures; it will be interesting to see
whether this will impact driving performance or not.
Further, more sophisticated presentation strategies
(e.g., controlling the complexity of the generated
language in accordance to the driving situation) can
be tested in this framework.
Acknowledgments This research was partly sup-
ported by the Deutsche Forschungsgemeinschaft
(DFG) in the CRC 673 ?Alignment in Communic-
71
ation? and the Center of Excellence in ?Cognit-
ive Interaction Technology? (CITEC). The authors
would like to thank Oliver Eckmeier and Michael
Bartholdt for helping implement the system setup,
as well as Gerdis Anderson and Fabian Wohlge-
muth for assisting as experimenters.
References
Timo Baumann and David Schlangen. 2012. The In-
proTK 2012 release. In NAACL-HLT Workshop on
Future directions and needs in the Spoken Dialog
Community: Tools and Data (SDCTD 2012), pages
29?32, Montr?al, Canada.
Hendrik Buschmeier, Timo Baumann, Benjamin
Dosch, Stefan Kopp, and David Schlangen. 2012.
Combining incremental language generation and in-
cremental speech synthesis for adaptive information
presentation. In Proceedings of the 13th Annual
Meeting of the Special Interest Group on Discourse
and Dialogue, pages 295?303, Seoul, South Korea.
Frank A. Drews, Monisha Pasupathi, and David L.
Strayer. 2004. Passenger and cell-phone conver-
sations in simulated driving. In Proceedings of the
48th Annual Meeting of the Human Factors and Er-
gonomics Society, pages 2210?2212, New Orleans,
USA.
William J. Horrey and Christopher D. Wickens. 2006.
Examining the impact of cell phone conversations
on driving using meta-analytic techniques. Human
Factors, 48:196?205.
ISO. 2010. Road vehicles ? Ergonomic aspects of
transport information and control systems ? Simu-
lated lane change test to assess in-vehicle second-
ary task demand. ISO 26022:2010, Geneva, Switzer-
land.
Spyros Kousidis, Thies Pfeiffer, and David Schlangen.
2013. MINT.tools: Tools and adaptors supporting
acquisition, annotation and analysis of multimodal
corpora. In Interspeech 2013, Lyon, France. ISCA.
Pierre Lison. 2012. Probabilistic dialogue models with
prior domain knowledge. In Proceedings of the 13th
Annual Meeting of the Special Interest Group on Dis-
course and Dialogue, pages 179?188, Seoul, South
Korea.
David L Strayer, Frank A Drews, and Dennis J Crouch.
2006. A comparison of the cell phone driver and the
drunk driver. Human Factors, 48:381?91.
David L Strayer, Joel M Cooper, Jonna Turrill, James
Coleman, and Nate Medeiros. 2013. Measuring
cognitive distraction in the automobile. Technical
report, AAA Foundation for Traffice Safety.
J Wienke and S Wrede. 2011. A middleware for col-
laborative research in experimental robotics. In Sys-
tem Integration (SII), 2011 IEEE/SICE International
Symposium on, pages 1183?1190.
72
Proceedings of the SIGDIAL 2014 Conference, pages 84?88,
Philadelphia, U.S.A., 18-20 June 2014.
c?2014 Association for Computational Linguistics
InproTK
S
: A Toolkit for Incremental Situated Processing
Casey Kennington
CITEC, Dialogue Systems
Group, Bielefeld University
ckennington
1
Spyros Kousidis
Dialogue Systems Group
Bielefeld University
spyros.kousidis
2
1
@cit-ec.uni-bielefeld.de
2
@uni-bielefeld.de
David Schlangen
Dialogue Systems Group
Bielefeld University
david.schlangen
2
Abstract
In order to process incremental situated
dialogue, it is necessary to accept infor-
mation from various sensors, each track-
ing, in real-time, different aspects of the
physical situation. We present extensions
of the incremental processing toolkit IN-
PROTK which make it possible to plug in
such multimodal sensors and to achieve
situated, real-time dialogue. We also de-
scribe a new module which enables the use
in INPROTK of the Google Web Speech
API, which offers speech recognition with
a very large vocabulary and a wide choice
of languages. We illustrate the use of these
extensions with a description of two sys-
tems handling different situated settings.
1 Introduction
Realising incremental processing of speech in-
and output ? a prerequisite to interpretation and
possibly production of speech concurrently with
the other dialogue participant ? requires some fun-
damental changes in the way that components
of dialogue systems operate and communicate
with each other (Schlangen and Skantze, 2011;
Schlangen and Skantze, 2009). Processing situ-
ated communication, that is, communication that
requires reference to the physical setting in which
it occurs, makes it necessary to accept (and fuse)
information from various different sensors, each
tracking different aspects of the physical situation,
making the system multimodal (Atrey et al., 2010;
Dumas et al., 2009; Waibel et al., 1996).
Incremental situated processing brings together
these requirements. In this paper, we present a col-
lection of extensions to the incremental process-
ing toolkit INPROTK (Baumann and Schlangen,
2012) that make it capable of processing situ-
ated communication in an incremental fashion:
we have developed a general architecture for
plugging in multimodal sensors whith we denote
INPROTK
S
, which includes instantiations for mo-
tion capture (via e.g. via Microsoft Kinect and
Leap Motion) and eye tracking (Seeingmachines
FaceLAB). We also describe a new module we
built that makes it possible to perform (large vo-
cabulary, open domain) speech recognition via the
Google Web Speech API. We describe these com-
ponents individually and give as use-cases in a
driving simulation setup, as well as real-time gaze
and gesture recognition.
In the next section, we will give some back-
ground on incremental processing, then describe
the new methods of plugging in multimodal sen-
sors, specifically using XML-RPC, the Robotics
Service Bus, and the InstantReality framework.
We then explain how we incorporated the Google
Web Speech API into InproTK, offer some use
cases for these new modules, and conclude.
2 Background: The IU model, INPROTK
As described in (Baumann and Schlangen, 2012),
INPROTK realizes the IU-model of incremen-
tal processing (Schlangen and Skantze, 2011;
Schlangen and Skantze, 2009), where incremental
systems consist of a network of processing mod-
ules. A typical module takes input from its left
buffer, performs some kind of processing on that
data, and places the processed result onto its right
buffer. The data are packaged as the payload of
incremental units (IUs) which are passed between
modules.
The IUs themselves are also interconnected via
so-called same level links (SLL) and grounded-in
links (GRIN), the former allowing the linking of
IUs as a growing sequence, the latter allowing that
sequence to convey what IUs directly affect it (see
Figure 1 for an example). A complication partic-
ular to incremental processing is that modules can
?change their mind? about what the best hypothe-
84
Figure 1: Example of IU network; part-of-speech
tags are grounded into words, tags and words have
same level links with left IU; four is revoked and
replaced with forty.
sis is, in light of later information, thus IUs can be
added, revoked, or committed to a network of IUs.
INPROTK determines how a module network is
?connected? via an XML-formatted configuration
file, which states module instantiations, includ-
ing the connections between left buffers and right
buffers of the various modules. Also part of the
toolkit is a selection of ?incremental processing-
ready? modules, and so makes it possible to realise
responsive speech-based systems.
3 InproTK and new I/O: InproTK
S
The new additions introduced here are realised as
INPROTK
S
modules. The new modules that input
information to an INPROTK
S
module network are
called listeners in that they ?listen? to their respec-
tive message passing systems, and modules that
output information from the network are called
informers. Listeners are specific to their method
of receiving information, explained in each sec-
tion below. Data received from listeners are pack-
aged into an IU and put onto the module?s right
buffer. Listener module left buffers are not used
in the standard way; left buffers receive data from
their respective message passing protocols. An in-
former takes all IUs from its left buffer, and sends
their payload via that module?s specific output
method, serving as a kind of right buffer. Figure
2 gives an example of how such listeners and in-
formers can be used. At the moment, only strings
can be read by listeners and sent by informers; fu-
ture extensions could allow for more complicated
data types.
Listener modules add new IUs to the network;
correspondingly, further modules have to be de-
signed in instatiated systems then can make use
of these information types. These IUs created by
the listeners are linked to each other via SLLs.
As with audio inputs in previous version of IN-
PROTK, these IUs are considered basedata and not
explictly linked via GRINs in the sensor data. The
modules defined so far also simply add IUs and do
not revoke.
We will now explain the three new methods of
getting data into and out of INPROTK
S
.
3.1 XML-RPC
XML-RPC is a remote procedure call protocol
which uses XML to encode its calls, and HTTP as a
transport mechanism. This requires a server/client
relationship where the listener is implemented as
the server on a specified port.
1
Remote sensors
(e.g., an eye tracker) are realised as clients and can
send data (encoded as a string) to the server using
a specific procedural call. The informer is also re-
alised as an XML-RPC client, which sends data to a
defined server. XML-RPC was introduced in 1998
and is widely implemented in many programming
languages.
Mic
Motion !Sensor
ASR
Listener
NLU
Speaker DMNLG
Informer
InproTKs
Logger
Gesture Classifier
Figure 2: Example architecture using new mod-
ules: motion is captured and processed externally
and class labels are sent to a listener, which adds
them to the IU network. Arrows denote connec-
tions from right buffers to left buffers. Information
from the DM is sent via an Informer to an external
logger. External gray modules denote input, white
modules denote output.
3.2 Robotics Service Bus
The Robotics Service Bus (RSB) is a middleware
environment originally designed for message-
passing in robotics systems (Wienke and Wrede,
2011).
2
As opposed to XML-RPC which requires
1
The specification can be found at http://xmlrpc.
scripting.com/spec.html
2
https://code.cor-lab.de/projects/rsb
85
point-to-point connections, RSB serves as a bus
across specified transport mechanisms. Simply,
a network of communication nodes can either in-
form by sending events (with a payload), or lis-
ten, i.e., receive events. Informers can send in-
formation on a specific scope which establishes
a visibility for listeners (e.g., a listener that re-
ceives events on scope /one/ will receive all events
that fall under the /one/ scope, whereas a listener
with added constants on the scope, e.g., /one/two/
will not receive events from different added con-
stants /one/three/, but the scope /one/ can listen
on all three of these scopes). A listener mod-
ule is realised in INPROTK
S
by setting the de-
sired scope in the configuration file, allowing IN-
PROTK
S
seamless interconnectivity with commu-
nication on RSB.
There is no theoretical limit to the number of in-
formers or listeners; events from a single informer
can be received by multiple listeners. Events are
typed and any new types can be added to the avail-
able set. RSB is under active development and is
becoming more widely used. Java, Python, and
C++ programming languages are currently sup-
ported. In our experience, RSB makes it particu-
larly convenient for setting distributed sensor pro-
cessing networks.
3.3 InstantReality
In (Kousidis et al., 2013), the InstantReality
framework, a virtual reality environment, was
used for monitoring and recording data in a real-
time multimodal interaction.
3
Each information
source (sensor) runs on its own dedicated work-
station and transmits the sensor data across a net-
work using the InstantIO interface. The data can
be received by different components such as In-
stantPlayer (3D visualization engine; invaluable
for monitoring of data integrity when recording
experimental sessions) or a logger that saves all
data to disk. Network communication is achieved
via multicast, which makes it possible to have any
number of listeners for a server and vice-versa.
The InstantIO API is currently available in C++
and Java. It comes with a non-extensible set of
types (primitives, 2D and 3D vectors, rotations,
images, sounds) which is however adequate for
most tracking applications. InstantIO listeners and
informers are easily configured in INPROTK
S
con-
figuration file.
3
http://www.instantreality.org/
3.4 Venice: Bridging the Interfaces
To make these different components/interfaces
compatible with each other, we have developed a
collection of bridging tools named Venice. Venice
serves two distinct functions. First, Venice.HUB,
which pushes data to/from any of the following
interfaces: disk (logger/replayer), InstantIO, and
RSB. This allows seamless setup of networks for
logging, playback, real-time processing (or com-
binations; e.g, for simulations), minimizing the
need for adaptations to handle different situations.
Second, Venice.IPC allows interprocess communi-
cation and mainly serves as a quick and efficient
way to create network components for new types
of sensors, regardless of the platform or language.
Venice.IPC acts as a server to which TCP clients
(a common interface for sensors) can connect. It
is highly configurable, readily accepting various
sensor data outputs, and sends data in real-time to
the InstantIO network.
Both Venice components operate on all three
major platforms (Linux, Windows, Mac OS X),
allowing great flexibility in software and sensors
that can be plugged in the architecture, regardless
of the vendor?s native API programming language
or supported platform. We discuss some use cases
in section 5.
4 Google Web Speech
One barrier to dialogue system development is
handling ASR. Open source toolkits are available,
each supporting a handful of languages, with each
language having a varying vocabulary size. A step
in overcoming this barrier is ?outsourcing? the
problem by making use of the Google Web Speech
API.
4
This interface supports many languages, in
most cases with a large, open domain of vocabu-
lary. We have been able to access the API directly
using INPROTK
S
, similar to (Henderson, 2014).
5
INPROTK
S
already supports an incremental vari-
ant of Sphinx4; a system designer can now choose
from these two alternatives.
At the moment, only the Google Chrome
browser implements the Web Speech API. When
the INPROTK
S
Web Speech module is invoked,
it creates a service which can be reached from
4
The Web Speech API Specificiation: https:
//dvcs.w3.org/hg/speech-api/raw-file/
tip/speechapi.html
5
Indeed, we used Matthew Henderson?s webdial project
as a basis: https://bitbucket.org/matthen/
webdialog
86
the Chrome browser via an URL (and hence, mi-
crophone client, dialogue processor and speech
recogniser can run on different machines). Navi-
gating to that URL shows a simple web page where
one can control the microphone. Figure 3 shows
how the components fit together.
While this setup improves recognition as com-
pared to the Sphinx4-based recognition previously
only available in INPROTK, there are some ar-
eas of concern. First, there is a delay caused by
the remote processing (on Google?s servers), re-
quiring alignment with data from other sensors.
Second, the returned transcription results are only
?semi-incremental?; sometimes chunks of words
are treated as single increments. Third, n-best lists
can only be obtained when the API detects the end
of the utterance (incrementally, only the top hy-
pothesis is returned). Fourth, the results have a
crude timestamp which signifies the end of the au-
dio segment. We use this timestamp in our con-
struction of word IUs, which in informal tests have
been found to be acceptable for our needs; we de-
fer more systematic testing to future work.
Figure 3: Data flow of Google Web Speech API:
Chrome browser controls the microphone, sends
audio to API and receives incremental hypotheses,
which are directly sent to InproTK
S
.
5 INPROTK
S
in Use
We exemplify the utility of INPROTK
S
in two ex-
periments recently performed in our lab.
In-car situated communication We have tested
a ?pause and resume? strategy for adaptive in-
formation presentation in a driving simulation
scenario (see Figure 4), using INPROTK
S
and
OpenDS (Math et al., 2013). Our dialogue man-
ager ? implemented using OpenDial (Lison, 2012)
? receives trigger events from OpenDS in order to
update its state, while it verbalises calendar events
and presents them via speech. This is achieved
by means of InstantIO servers we integrated into
OpenDS and respective listeners in INPROTK
S
. In
turn, InstantIO informers send data that is logged
Figure 4: Participant performing driving test while
listening to iNLG speech delivered by InProTK
S
.
by Venice.HUB. The results of this study are pub-
lished in (Kousidis et al., 2014). Having available
the modules described here made it surprisingly
straightforward to implement the interaction with
the driving simulator (treated as a kind of sensor).
Real-time gaze fixation and pointing gesture
detection Using the tools described here, we
have recently tested a real-time situated commu-
nication environment that uses speech, gaze, and
gesture simultaneously. Data from a Microsoft
Kinect and a Seeingmachines Facelab eye tracker
are logged in realtime to the InstantIO network.
A Venice.HUB component receives this data and
sends it over RSB to external components that
perform detection of gaze fixation and pointing
gestures, as described in (Kousidis et al., 2013).
These class labels are sent in turn over RSB to
INPROTK
S
listeners, aggregating these modalities
with the ASR in a language understanding module.
Again, this was only enabled by the framework de-
scribed here.
6 Conclusion
We have developed methods of providing mul-
timodal information to the incremental dialogue
middleware INPROTK. We have tested these
methods in real-time interaction and have found
them to work well, simplifying the process of
connecting external sensors necessary for multi-
modal, situated dialogue. We have further ex-
tended its options for ASR, connecting the Google
Web Speech API. We have also discussed Venice,
a tool for bridging RSB and InstantIO interfaces,
which can log real-time data in a time-aligned
manner, and replay that data. We also offered
some use-cases for our extensions.
INPROTK
S
is freely available and accessible.
6
6
https://bitbucket.org/inpro/inprotk
87
Acknowledgements Thank you to the anony-
mous reviewers for their useful comments and to
Oliver Eickmeyer for helping with InstantReality.
References
Pradeep K. Atrey, M. Anwar Hossain, Abdulmotaleb
El Saddik, and Mohan S. Kankanhalli. 2010. Multi-
modal fusion for multimedia analysis: a survey, vol-
ume 16. April.
Timo Baumann and David Schlangen. 2012. The In-
proTK 2012 Release. In NAACL.
Bruno Dumas, Denis Lalanne, and Sharon Oviatt.
2009. Multimodal Interfaces : A Survey of Princi-
ples , Models and Frameworks. In Human Machine
Interaction, pages 1?25.
Matthew Henderson. 2014. The webdialog Frame-
work for Spoken Dialog in the Browser. Technical
report, Cambridge Engineering Department.
Spyros Kousidis, Casey Kennington, and David
Schlangen. 2013. Investigating speaker gaze and
pointing behaviour in human-computer interaction
with the mint.tools collection. In SIGdial 2013.
Spyros Kousidis, Casey Kennington, Timo Baumann,
Hendrik Buschmeier, Stefan Kopp, and David
Schlangen. 2014. Situationally Aware In-Car Infor-
mation Presentation Using Incremental Speech Gen-
eration: Safer, and More Effective. In Workshop on
Dialog in Motion, EACL 2014.
Pierre Lison. 2012. Probabilistic Dialogue Mod-
els with Prior Domain Knowledge. In Proceedings
of the 13th Annual Meeting of the Special Interest
Group on Discourse and Dialogue, pages 179?188,
Seoul, South Korea, July. Association for Computa-
tional Linguistics.
Rafael Math, Angela Mahr, Mohammad M Moniri,
and Christian M?uller. 2013. OpenDS: A new
open-source driving simulator for research. GMM-
Fachbericht-AmE 2013.
David Schlangen and Gabriel Skantze. 2009. A Gen-
eral, Abstract Model of Incremental Dialogue Pro-
cessing. In Proceedings of the 10th EACL, number
April, pages 710?718, Athens, Greece. Association
for Computational Linguistics.
David Schlangen and Gabriel Skantze. 2011. A Gen-
eral, Abstract Model of Incremental Dialogue Pro-
cessing. Dialoge & Discourse, 2(1):83?111.
Alex Waibel, Minh Tue Vo, Paul Duchnowski, and Ste-
fan Manke. 1996. Multimodal interfaces. Artificial
Intelligence Review, 10(3-4):299?319.
Johannes Wienke and Sebastian Wrede. 2011. A
middleware for collaborative research in experimen-
tal robotics. In System Integration (SII), 2011
IEEE/SICE International Symposium on, pages
1183?1190.
88
