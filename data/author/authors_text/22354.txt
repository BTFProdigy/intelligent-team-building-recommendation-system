Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 787?798,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
ReferItGame: Referring to Objects in Photographs of Natural Scenes
Sahar Kazemzadeh
1?
Vicente Ordonez
1?
Mark Matten
2
Tamara L. Berg
1
1
University of North Carolina at Chapel Hill, Chapel Hill, NC 27599, USA
2
The Bishop?s School, San Diego, CA 92037, USA
vicente@cs.unc.edu,tlberg@cs.unc.edu
Abstract
In this paper we introduce a new game
to crowd-source natural language referring
expressions. By designing a two player
game, we can both collect and verify refer-
ring expressions directly within the game.
To date, the game has produced a dataset
containing 130,525 expressions, referring
to 96,654 distinct objects, in 19,894 pho-
tographs of natural scenes. This dataset is
larger and more varied than previous REG
datasets and allows us to study referring
expressions in real-world scenes. We pro-
vide an in depth analysis of the resulting
dataset. Based on our findings, we design
a new optimization based model for gen-
erating referring expressions and perform
experimental evaluations on 3 test sets.
1 Introduction
Much of everyday language and discourse con-
cerns the visual world around us, making under-
standing the relationship between objects in the
physical world and language describing those ob-
jects an important challenge problem for AI. From
robotics, to image search, to situated language
learning, and natural language grounding, there
are a number of research areas that would bene-
fit from a better understanding of how people refer
to physical entities in the world.
Recent advances in automatic computer vision
methods have started to make technologies for rec-
ognizing thousands of object categories a near re-
ality (Perronnin et al., 2012; Deng et al., 2012;
Deng et al., 2010; Krizhevsky et al., 2012). As a
result, there has been a spurt of recent work trying
to estimate higher level semantics, including ex-
citing efforts to automatically produce natural lan-
guage descriptions of images and video (Farhadi et
?
Indicates equal author contribution.
al., 2010; Kulkarni et al., 2011; Yang et al., 2011;
Ordonez et al., 2011; Kuznetsova et al., 2012;
Feng and Lapata, 2013). Common challenges en-
countered in these pursuits include the fact that
descriptions can be highly task dependent, open-
ended, and difficult to evaluate automatically.
Therefore, we look at the related, but more fo-
cused problem of referring expression generation
(REG). Previous work on REG has made signif-
icant progress toward understanding how people
generate expressions to refer to objects (a recent
survey of techniques is provided in Krahmer and
van Deemter (2012)). In this paper, we study the
relatively unexplored setting of how people refer
to objects in complex photographs of real-world
cluttered scenes. One initial stumbling block to
examining this scenario is lack of existing rele-
vant datasets, as previous collections for studying
REG have used relatively focused domains such
as graphics generated objects (van Deemter et al.,
2006; Viethen and Dale, 2008), crafts (Mitchell et
al., 2010), or small everyday (home and office) ob-
jects arrayed on a simple background (Mitchell et
al., 2013a; FitzGerald et al., 2013).
In this paper, we collect a new large-scale cor-
pus, currently containing 130,525 expressions, re-
ferring to 96,654 distinct objects, in 19,894 pho-
tographs of real world scenes. Some examples
from our dataset are shown in Figure 5. To con-
struct this corpus efficiently, we design a new two
player referring expression game (ReferItGame)
to crowd-source the data collection. Popular-
ized by efforts like the ESP game (von Ahn and
Dabbish, 2004) and Peekaboom (von Ahn et al.,
2006b), Human Computation based games can be
an effective way to engage users and collect large
amounts of data inexpensively. Two player games
can also automate verification of human provided
annotations.
Our resulting corpus is both more real-world
and much bigger than previous datasets, allowing
787
us to examine referring expression generation in
a new setting at large scale. To understand and
quantify this new dataset, we perform an exten-
sive set of analyses. One significant difference
from previous work is that we study how refer-
ring expressions vary for different categories. We
find that an object?s category greatly influences the
types of attributes used in their referring expres-
sion (e.g. people use color words to describe cars
more often than mountains). Additionally, we find
that references to an object are sometimes made
with respect to other nearby objects, e.g. ?the ball
to left of the man?. Interestingly, the types of ref-
erence objects (i.e. ?the man?) used in referring
expressions is also biased toward some categories.
Finally, we find that the word used to refer to the
object category itself displays consistencies across
people. This notion is related to ideas of entry-
level categories from Psychology (Rosch, 1978).
Given these findings, we propose an optimiza-
tion model for generating referring expressions
that jointly selects which attributes to include in
the expression, and what attribute values to gener-
ate. This model incorporates both visual models
for selecting attribute-values and object category
specific priors. Experimental evaluations indicate
that our proposed model produces reasonable re-
sults for REG.
In summary, contributions of our paper include:
? A two player online game to collect and ver-
ify natural language referring expressions.
? A new large-scale dataset containing natural
language expressions referring to objects in
photographs of real world scenes.
? Analyses of the collected dataset, including
studying category-specific variations in refer-
ring expressions.
? An optimization based model to generate
referring expressions for objects in real-
world scenes with experimental evaluations
on three labeled test sets.
The rest of the paper is organized as follows.
First we outline related work from the vision and
language communities (?2). Then we describe our
online game for collecting referring expressions
(?3) and provide an analysis of our new Refer-
ItGame Dataset (?4). Finally, we present and eval-
uate our model for generating referring expres-
sions (?5) and discuss conclusions and future work
(?6).
2 Related Work
Referring Expression Generation: There has
been a long history of research on understanding
how people generate referring expressions, dating
back to the 1970s (Winograd, 1972). One com-
mon approach is the Incremental Algorithm (Dale
and Reiter, 1995; Dale and Reiter, 2000) which
uses logical expressions for generation. Much
work in REG follows the Gricean maxims (Grice,
1975) which provide principles for how people
will behave in conversation.
Recently, there has been progress examining
other aspects of the referring expression prob-
lem such as understanding what types of attributes
are used (Mitchell et al., 2013a), modeling varia-
tions between speakers (Viethen and Dale, 2010;
Viethen et al., 2013; Van Deemter et al., 2012;
Mitchell et al., 2013b), incorporating visual classi-
fiers (Mitchell et al., 2011), producing algorithms
to refer to object sets (Ren et al., 2010; FitzGerald
et al., 2013), or examining impoverished percep-
tion REG (Fang et al., 2013). A good survey of
work in this area is provided in Krahmer and van
Deemter (2012). We build on past work, extending
models to generate attributes jointly in a category
specific framework.
Referring Expression Datasets: Some initial
datasets in REG used graphics engines to pro-
duce images of objects (van Deemter et al., 2006;
Viethen and Dale, 2008). Recently more realis-
tic datasets have been introduced, consisting of
craft objects like pipecleaners, ribbons, and feath-
ers (Mitchell et al., 2010), or everyday home
and office objects such as staplers, combs, or
rulers (Mitchell et al., 2013a), arrayed on a sim-
ple background. These datasets helped moved re-
ferring expression generation research into the do-
main of real world objects. We seek to further
these pursuits by constructing a dataset of natural
objects in photographs of the real world.
Image & Video Description Generation: Re-
cent research on automatic image description has
followed two main directions. Retrieval based
methods (Aker and Gaizauskas, 2010; Farhadi et
al., 2010; Ordonez et al., 2011; Feng and Lap-
ata, 2010; Feng and Lapata, 2013) retrieve exist-
ing captions or phrases to describe a query image.
Bottom up methods (Kulkarni et al., 2011; Yang
et al., 2011; Yao et al., 2010) rely on visual classi-
fiers to first recognize image content and then con-
struct captions from scratch, perhaps with some
788
Figure 1: An example game. Player 1 (left) sees an image with an object outlined in red (the man)
and provides a referring expression for the object (?man in red shirt on horse?). Player 2 (right) sees
the image and the expression from Player 1 and must localize the correct object by clicking on it (click
indicated by the red square). Elapsed time and current scores are also provided.
input from natural language statistics. Very re-
cently, these ideas have been extended to produce
descriptions for videos (Guadarrama et al., 2013;
Barbu et al., 2012). Like these methods, we gen-
erate descriptions for natural scenes, but focus on
referring to particular objects rather than provid-
ing an overall description of an image or video.
Human Computation Games: Games can be
a useful tool for collecting large amounts of la-
beled data quickly. Human Computation Games
were first introduced by Luis von Ahn in the ESP
game (von Ahn and Dabbish, 2004) for image la-
beling, and later extended to segment objects (von
Ahn et al., 2006b), collect common-sense knowl-
edge (von Ahn et al., 2006a), or disambiguate
words (Seemakurty et al., 2010). Recently, crowd
games have also been introduced into the com-
puter vision community for tasks like fine grained
category recognition (Deng et al., 2013). These
games can be released publicly on the web or
used on Mechanical Turk to enhance and encour-
age turker participation (Deng et al., 2013). In-
spired by the success of previous games, we cre-
ate a game to collect and verify natural language
expressions referring to objects in natural scenes.
3 Referring Expression Game
(ReferItGame)
In this section we describe our referring expres-
sion game (ReferItGame
?
), a simple two player
game where players alternate between generating
expressions referring to objects in images of nat-
ural scenes, and clicking on the locations of de-
scribed objects. An example game is shown in
Figure 1.
?
Available online at http://referitgame.com
3.1 Game Play
Player 1: is shown an image with an object out-
lined in red and provided with a text box in which
to write a referring expression. Player 2: is shown
the same image and the referring expression writ-
ten by Player 1 and must click on the location of
the described object (note, Player 2 does not see
the object segmentation). If Player 2 clicks on
the correct object, then both players receive game
points and the Player 1 and Player 2 roles swap for
the next image. If Player 2 does not click on the
correct object then no points are received and the
players remain in their current roles.
This provides us with referring expressions for
our dataset and verification that the expressions
are valid since they led to correct object localiza-
tions. Expressions written for games where the
object was not correctly localized are kept and re-
leased with the dataset for future study, but are not
included in our final dataset analyses or statistics.
A game timer encourages players to write expres-
sions quickly, resulting in more natural expres-
sions. Also, IP addresses are filtered to prevent
people from simultaneously playing both roles.
3.2 Playing Against the Computer
To promote engagement, we implement a single
player version of the game. When a player con-
nects, if there is another player online then the two
people are paired. If there are currently no other
available players, then the person plays a ?canned?
game against the computer. If at any point another
person connects, the canned game ends and the
player is paired with the new person.
To implement canned games we seed the
game with 5000 pre-recorded referring expression
games (5 referring expressions and resulting clicks
789
for each of 1000 objects) collected using Ama-
zon?s Mechanical Turk service. Implementing an
automated version of Player 1 is simple; we just
show the person one of the pre-collected referring
expressions and they click as usual.
Automating the role of Player 2 is a bit more
complicated. In this case, we compare the per-
son?s written expression against the pre-recorded
expressions for the same object. For this compar-
ison we use a parser to lemmatize the words in an
expression and then compute cosine similarity be-
tween expressions with a bag of words representa-
tion. Based on this measure the closest matching
expression is determined. If there is no similarity
between the newly generated expression and the
canned expressions, the expression is deemed in-
correct and a random click location (outside of the
object) is generated. If there is a successful match
with a previously generated expression, then the
canned click from the most similar pre-recorded
game is used. More complex similarities could be
used, but since we require real-time performance
in our game setting we use this simple implemen-
tation which works well for our expressions.
4 ReferItGame Dataset
In this section we describe the ReferItGame
dataset
?
, including images and labels, processing
the dataset, and analysis of the collection.
4.1 Images and Labels
We build our dataset of referring expressions
on top of the ImageCLEF IAPR image retrieval
dataset (Grubinger et al., 2006). This dataset is
a collection of 20,000 images available free of
charge without copyright restrictions, depicting a
variety of aspects of everyday life, from sports,
to animals, to cities, and landscapes. Crucial for
our purposes, the SAIAPR TC-12 expansion (Es-
calante et al., 2010) includes segmentations of
each image into regions indicating the locations of
constituent objects. 238 different object categories
are labeled, including animals, people, buildings,
objects, and background elements like grass or
sky. This provides us with information regarding
object category, object location, and object size, as
well as the location and categories of other objects
present in the same image.
?
Available at http://tamaraberg.com/referitgame
4.2 Collecting the Dataset
From the ImageCLEF dataset, we created a total
of over 100k distinct games (one per object labeled
in the dataset). For the games we imposed an or-
dering to allow for collecting the most interesting
expressions first. Initially we prioritized games
for objects in images with multiple objects of the
same category. Once these games were completed,
we prioritized ordering based on object category to
include a comprehensive range of objects. Finally,
after successfully collecting referring expressions
from the prioritized games, we posted games for
the remaining objects. In order to evaluate consis-
tency of expression generation across people, we
also include a probability of repeating previously
played games during collection.
To date, we have collected 130,525 successfully
completed games. This includes 10,431 canned
games (a person playing against the computer, not
including the initial seed set) and 120,094 real
games (two people playing). 96,654 distinct ob-
jects from 19,984 photographs are represented in
the dataset. This covers almost all of the objects
present in the IAPR corpus. The remaining ob-
jects from the collection were either too small or
too ambiguous to result in successful games.
For data collection, we posted the game online
for anyone on the web to play and encouraged par-
ticipation through social media and the survey sec-
tion of reddit. In this manner we collected over
4 thousand referring expressions over a period of
3 weeks. To speed up data collection, we also
posted the game on Mechanical Turk. Turkers
were paid upon completion of 10 correct games
(games where Player 2 clicks on the correct object
of interest). Turkers were pre-screened to have ap-
proval ratings above 80% and to be located in the
US for language consistency.
4.3 Processing the Dataset
Because of the size of the dataset, hand annotation
of all referring expressions is prohibitive. There-
fore, similar to past work (FitzGerald et al., 2013),
we design an automatic method to pre-process the
expressions and extract object and attribute men-
tions. These automatically processed expressions
are used only for analysis and model training. We
also fully hand label portions of the dataset for
evaluation (?5.2).
By examining the expressions in the collected
dataset, we define a set of attributes with broad
790
S ::= subject word
color word
?
::= rel(S, color word)
color word
?
=color word
|
prep in(S, color word)
color word
?
=color word
size word
?
::= rel(S, size word)
size word
?
=size word
abs loc word
?
::= rel(S, abs loc word)
abs loc word
?
=abs loc word
|
prep on(S, orientation word) ? ?prep of(S, )
abs loc word
?
=on+orientation word
rel loc word
?
::= RL
RL ::= prep rel loc word(S, object word)
RL=rel loc word
|
prep on(S, orientation word) ? prep of(S, object word)
RL=on orientation word
|
prep to(S, orientation word) ? prep of(S, object word)
RL=to orientation word
|
prep at(S, orientation word) ? prep of(S, object word)
RL=at orientation word
generic word
?
::= amod(S, generic word)
Figure 2: Templates for parsing attributes from referring expressions (?4.3).
coverage of the attribute types used in the re-
ferring expressions. We define the set of at-
tributes for a referring expression as a 7-tupleR =
{r
1
, r
2
, r
3
, r
4
, r
5
, r
6
, r
7
}:
? r
1
is an entry-level category attribute,
? r
2
is a color attribute,
? r
3
is a size attribute,
? r
4
is an absolute location attribute,
? r
5
is a relative location relation attribute,
? r
6
is a relative location object attribute,
? r
7
is a generic attribute,
Color and size attributes refer to the object color
(e.g. ?blue?) and object size (e.g. ?tiny?) respec-
tively. Absolute location refers to the location of
the object in the image (e.g. ?top of the image?).
Relative location relation and relative location ob-
ject attributes allow for referring expressions that
localize the object with respect to another object
in the picture (e.g. ?the car to the left of the tree?).
Generic attributes cover all less frequently ob-
served attribute types (e.g. ?wooden? or ?round?).
The entry-level category attribute is related to
the concept of entry-level categories first proposed
by Psychologists in the 1970s (Rosch, 1978) and
recently explored in visual recognition (Ordonez
et al., 2013). The idea of entry-level categories is
that an object can belong to many different cate-
gories; an indigo bunting is an oscine, a bird, a
vertebrate, a chordate, and so on. But, a person
looking at a picture of one would probably call it
a bird (unless they are very familiar with ornithol-
ogy). Therefore, we include this attribute to cap-
ture how people name object categories in refer-
ring expressions.
Parsing the referring expressions: We parse
the expressions using the most recent version
of the StanfordCoreNLP parser (Socher et al.,
2013). We begin by traversing the parse tree in a
breadth-first manner and selecting the head noun
of the sentence to determine the object of the
referring expression, denoted as subject word.
We pre-define a dictionary of attribute-values
(color word, size word, abs location word,
rel location word) for each of the attributes
based on the observed data using a combination
of POS-tagging and manual labeling.
We then apply a template-based approach on the
collapsed dependency relations to recover the set
of attributes (the main template rules are shown
in Figure 2). The relationship rel indicates any
linguistic binary relationship between the subject
word S and another word, including the amod re-
lationship. Orientation word captures the words
like left, right, top and bottom. For generic word
we consider any modifier words other than those
captured by our other attributes (color, size, loca-
tion).
Using this template-based parser we can
for instance parse the following expression:
?Red flower on top of pedestal?. The first
rule would match the prep(S, color word)
relation, effectively recovering the attribute
color word
?
as ?red?. The second rule would
match the prep on(S, orientation word) ?
prep of(S, object word) relations, recovering
rel loc word
?
as ?on top of ? and object word
as ?pedestal?.
The accuracy of our parser based processing is
91%. This was evaluated on 4,500 expressions
791
00.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 0.2 0.4 0.6 0.8 1
Plot D: Object Locations for Abs-Loc 
Attributes 
Right Left Bottom Middle Top Front
0 2000 4000 6000
sky-blue
man
group-of-persons
ground
rock
cloud
grass
woman
trees
mountain
vegetation
wall
sky
window
building
ocean
sky-light
tree
car
person
house
floor
couple-of-persons
face-of-person
plant
hat
street
hill
fabric
bed
bottle
lamp
sand-beach
chair
door
child-boy
painting
palm
river
bicycle
Plot A: Attribute Breakdown by Category 
None Color Size Absolute Location Relative Location  Other
0 
50% 
1 
41% 
2 
9% 
Plot C: Number of Attributes Per 
Expression 
0 0.1 0.2 0.3 0.4 0.5 0.6
N
o
n
e
Si
ze
R
el
at
iv
e
Lo
ca
ti
o
n
Plot F: Attribute Use Frequencies 
Overall Single Multiple
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
None Small Big Tall Little Tiny Large Short Huge Long
Plot E: Object Area vs Indicated Size 
0 200 400 600
people
shirt
side
guy
head
tree
man
corner
background
building
wall
woman
table
bed
water
hat
girl
mountain
person
sign
car
pic
lady
window
sky
foot
boat
foreground
jacket
ground
hand
bike
group
cloud
horse
grass
rock
house
Plot B: Most Frequently Used Relative 
Objects 
Figure 3: Analyses of the ReferItGame Dataset. Plot A shows frequency and attribute occurrence for
common object categories. Plot B shows objects frequently used as reference points, ie ?to the left of the
man?. Plot C shows frequencies of using 0, 1 or 2 attributes within the same expression. Plot D shows
object locations vs location words used. Plot E shows normalized object size vs size words used (bars
show 1
st
through 3
rd
quartiles). Plot F shows the frequency of usage of each attribute type for images
containing either a single instance of the object category or multiple instances of the category.
792
People Man Woman 
Car Bottle Street 
Color Objects 
Beige 
Red 
Yellow 
Blue 
Black 
Figure 4: Left: Tag clouds showing entry-Level category words used in referring expressions to name
various object categories, with word size indicating frequency. For example, this indicates that ?streets?
are often called ?road?, sometimes ?ground?, sometimes ?roadway?, etc. Right: example objects pre-
dicted to portray some of our color attribute values. Note sometimes our color predictor is quite accurate,
and sometimes it makes mistakes (see the man in a red shirt predicted as ?yellow?).
that were manually parsed by a human annotator.
4.4 Dataset Analysis
In the resulting dataset, we have a range of cov-
erage over objects. For 10,304 of the objects we
have 2 or more referring expressions while for the
rest of the objects we have collected only one ex-
pression. This creates a dataset that emphasizes
breadth while also containing enough data to study
speaker variation.
Multiple attribute analyses are provided in Fig-
ure 3. We find that most expressions use 0, 1, or
2 attributes (in addition to the entry-level attribute
object word), with very few expressions contain-
ing more than 2 attributes (frequencies are shown
in Fig 3c). We also examine what types of at-
tributes are used most frequently, according to ob-
ject category in Fig 3a, and when associated with
single or multiple occurrences of the same object
category in an image in Fig 3f. The frequency
of attribute usage in images containing multiple
objects of the same type increases for all types,
compared to single object occurrences. Perhaps
more interestingly, the use of different attributes is
highly category dependent. People use more at-
tribute words overall to describe some categories,
like ?man?, ?woman?, or ?plant?, and the distribu-
tion of attribute types also varies by category. For
example, color attributes are used more frequently
for categories like ?car? or ?woman? than for cat-
egories like ?sky? or ?rock?.
We also examine which objects are most fre-
quently used as points of reference, e.g.,?the chair
next to the man? in Fig 3b. We observe that peo-
ple and some background categories like ?tree? or
?wall? are often used to help localize objects in
referring expressions. Additionally, we provide
plots showing the relationship between object lo-
cation in the image and use of absolute location
words, Fig 3d, as well as size words vs object area,
Fig 3e.
Finally, we study entry-level category attribute-
values to understand how people name objects in
referring expressions. Tag clouds indicating the
frequencies of words used to name various ob-
ject categories are provided in Fig 4 (left). Ob-
jects like ?street? are usually referred to as ?road?,
but sometimes they are called ?ground?, ?road-
way?, etc. ?Bottles? are usually called ?bottle?,
but sometimes referred to as ?coke? or ?beer?. In-
terestingly, ?man? is usually called ?man? while
?woman? is most often called ?person? in the re-
ferring expressions.
5 Generating Referring Expressions
In this section we describe our proposed genera-
tion model and provide experimental evaluations
on three test sets.
5.1 Generation Model
Given an input tuple I = {P, S}, where P is a
target object and S is a scene (image containing
multiple objects), our goal is to generate an output
referring expression, R. For instance, the repre-
sentation R for the referring expression: The big
old white cabin beside the tree would be R =
{cabin, white, big,?, beside, tree, old}.
To generate referring expressions we construct
vocabularies V
r
i
with candidate values for each at-
tribute r
i
? R, where attribute vocabulary V
r
i
con-
tains the set of words observed in our parsed refer-
ring expressions for attribute r
i
plus an additional
793
Image Human Expressions Generated Expressions 
picture on the wall picture picture 
Baseline:[picture, white, , right, , , ]  Full: [picture, , , , prep_on, wall, ]   
Door white door middle white door 
Baseline:[door, white, , right, , , ]  Full:[door, white, , right, , , ] 
big gated window on right of white section black big window right brown railings on right 
Baseline:[window, white, , right, , , ]  Full:[window, brown, , right, , , ]   
white shirt man white shirt on right man on right 
Baseline:[man, white, , right, , , ]  Full:[man, white, , right, , , ]   
building on right behind guys blue right building building on right 
Baseline:[building, white, , right, , , ]  Full:[building, white, , right, , , ]  
Image Human Expressions Generated Expressions 
picture santa the santa picture 
Baseline:[picture, white, , right, , , ]  Full:[picture, , , , prep_on, plant, ]   
right doorway right brown door right door 
Baseline:[door, , , right, prep_on, person, ]   Full:[door, , , right, prep_above, person, ]  
with flag window top 2nd left 2nd window top left 
Baseline:[window, , , right, prep_on, person, ]  Full:[window, , , left, prep_above, door, ]   
red guy left sitting left bottom guy red shirt lef 
Baseline:[man, , , right, prep_on, wall, ]  Full:[man, , , left, prep_in, woman, ]   
buildings buildings buildings 
Baseline:[building, white, , right, , , ]   Full:[building, brown, , middle, , ,   
Figure 5: Example results, including human generated expressions, baseline and full model generated
expressions. For some images the model does well at mimicking human expressions (left). For others it
does not generate the correct attributes (right).
? value indicating that the attribute should be om-
mited from the referring expression entirely.
In this way, our framework can jointly deter-
mine which attributes to include in the expression
(e.g.,?size? and ?color?) and what attribute values
to generate (e.g.,?small? and ?blue?) from the list
of all possible values. We enforce a constraint to
always include an ?entry-level category? attribute
(e.g. ?boy?) so that we always generate a word
referring to the object.
We pose our problem as an optimization where
we map a tuple {P, S} to a referring expression
R
?
as:
R
?
= argmax
R
E(R,P, S)
s. t. f
i
(R) ? b
i
(1)
Where the objective function E is decomposed as:
E(R,P, S) = ?
6
?
i=2
?
i
(r
i
, P, S)
+ ?
7
?
i=1
?
i
(r
i
, type(P ))
+
?
i>j
?
i,j
(r
i
, r
j
)
(2)
Where ?
i
is the compatibility function between an
attribute-value for r
i
and the properties of the ob-
served scene S and object P (described in ?5.1.1).
The terms ?
i
and ?
i,j
are unary and pairwise pri-
ors computed based on observed co-occurrence
statistics of attribute-values for r
i
with categories
(where type(P ) denotes the type or category of an
object) and between pairs of attribute-values (de-
scribed in ?5.1.2). Attributes r
1
and r
7
are mod-
eled only in the priors since we do not have visual
models for these attributes.
The constraints f
i
(R) ? b
i
are restricted to be
linear constraints and are used to impose hard con-
straints on the solution. The first such constraint is
used to control the verbosity (length) of the gener-
ated referring expression using a constraint func-
tion that imposes a minimum attribute length re-
quirement by restricting the number of entries r
i
that can take value ? in the solution.
?
i
1[r
i
= ?] ? 7? ?(P, S) (3)
Where 1[.] is the indicator function and ?(P, S) is
a term that allows us to change the length require-
ment based on the object and scene (so that images
with a larger number of objects of the same type
have a larger length requirement).
Finally we add hard constraints such that r
5
= ?
?? r
6
= ?, so that relative location and relative
object attributes are produced together.
5.1.1 Content-based potentials
Potentials ?
i
are defined for attributes r
2
to r
6
.
Attribute r
7
represents a variety of different at-
tributes, e.g. material or shape attributes, but
we lack sufficient data to train visual models for
these infrequent attribute terms. Therefore, we
model these attributes using only prior statistics-
based potentials (?5.1.2). Visual recognition mod-
els for recognizing entry-level object categories
794
could also be incorporated for modeling r
1
, but we
leave this as future work.
Color attribute:
?
2
(r
2
= c
k
, P, S) = sim(hist
c
k
, hist(P ))
Where hist(P ) is the HSV color histogram of the
object P . We compute similarity sim using cosine
similarity, and hist
c
k
is the mean histogram of all
objects in our training data that were referred to
with color attribute-value c
k
? V
r
2
.
Size attribute:
?
3
(r
3
= s
k
, P, S) =
1
?
s
k
?
2pi
e
?
(
size(P )??
s
k
)
2
/
2?
2
s
k
Where size(P ) is the size of object P normalized
by image size. We model the probabilities of each
size word s
k
? V
r
3
as a Gaussian learned on our
training set.
Absolute-location attribute:
?
4
(r
4
= a
k
, P, S) =
1
?
(2pi)
n
|?
a
k
|
e
?
1
2
(loc(P )??
a
k
)
T
?
a
k
?1
(loc(P )??
a
k
)
Where loc(P ) are the 2-dimensional coordi-
nates of the object P normalized to be ? [0 ? 1].
Parameters ?
a
k
and ?
a
k
are estimated from
training data for each absolute location word
a
k
? V
r
4
.
Relative-location and Relative object:
?
5
(r
5
= l
k
, P, S) =
1[l
k
= ?] ? g(count(type(P ), S))
If there are a larger number of objects of the same
type in the image we find that the probability of us-
ing a relative-location-object increases (e.g., ?the
car to the right of the man?). For images where P
was the only object of that category type, the prob-
ability of using a relative-location-object is 0.12.
This increases to 0.22 when there were two ob-
jects of the same type and further increases to 0.26
for additional objects of the same type. There-
fore, we model the probability of selecting rela-
tive location value l
k
? V
r
5
as a function g, where
count(type(P ), S) counts the number of objects
in the scene S of the same category type as the
object P .
?
6
(r
6
= o
k
, P, S) =
1[o
k
? objectsnear(location(P ), S)]
The above expression filters out potential relative
objects o
k
? V
r
6
that are not located in sufficient
proximity to object P or are not present in the im-
age at all.
5.1.2 Prior statistics-based potentials
Prior statistics-based potentials are modeled for all
of the attributes r
1
- r
7
. Note that these potentials
do not depend on specific attribute-values but only
on the given object category type(P ).
Unary prior potentials ?
i
are defined as:
?
i
(r
i
, type(P )) =
|D|
?
j=1
1[(r
(j)
i
6= ) ? (type(P
(j)
) = type(P ))]
|D|
?
j=1
1[type(P
(j)
) = type(P )]
+
|D|
?
j=1
1[r
(j)
i
6= ]
|D|
+ ?
Where D = {P
(j)
, S
(j)
, R
(j)
} is our training
dataset and ? is a small additive smoothing term.
The two terms in the above expression represent
category-specific counts and global counts of the
number of times a given attribute r
i
was output in
a referring expression in training data. Pairwise
prior potentials ?
i,j
are defined as:
?
i<j
?
i,j
(r
i
, r
j
) =
?
i<j
?
(1)
i,j
(r
i
, r
j
) + ?
(2)
5,6
(r
5
, r
6
)
?
(1)
i,j
(r
i
, r
j
) =
{
1 if r
i
= r
j
= ?
C + ? o.w.
?
(2)
5,6
(r
5
= a, r
6
= b) =
|D|
?
t=1
1[(r
(t)
5
= a) ? (r
(t)
6
= b)]
|D|
where C =
|D|?
t=1
1[(r
(t)
i
6=) ? (r
(t)
j
6=)]
|D|
. The pairwise
potential ?
(1)
i,j
captures the pairwise statistics of
how frequently people use pairs of attribute types.
795
SOURCE PREC(%) RECALL(%)
Baseline - A 27.92 43.27
Full Model - A 36.28 53.44
Baseline - B 29.87 50.57
Full Model - B 36.68 59.80
Baseline - C 28.85 37.41
Full Model - C 37.73 48.54
Table 1: Baseline Model & Full Model perfor-
mance on the three test sets (A,B,C).
For instance how frequently people use both color
and size attributes to refer to an object. The pair-
wise potential ?
(2)
i,j
produces a cohesion score be-
tween relative-location words and relative-object
words based on global dataset statistics.
5.2 Experiments
We implement the proposed model using commer-
cial binary integer linear programming software
(IBM ILOG CPLEX). This requires introducing
a set of indicator variables for each of our multi-
valued attributes and another set of indicator vari-
ables to model pairwise interactions between our
variables, as well as incorporating additional con-
sistency constraints between variables. Model pa-
rameters (? and ?) are tuned on data randomly
sampled from the training set.
Test Sets: We evaluate our model on three test
sets, each containing 500 objects. For each ob-
ject in the test sets we collect 3 referring expres-
sions using the ReferItGame and manually label
the attributes mentioned in each expression. We
find human agreement to be 72.31% on our dataset
(where we measure agreement as mean match-
ing accuracy of attribute values for pairs of users
across images in our test sets). The three test
sets are created to evaluate different aspects of our
data.
Test Set A contains objects sampled randomly
from the entire dataset. This test set is meant to
closely resemble the full dataset distribution. The
goal of the other two test sets is to sample expres-
sions for ?interesting? objects. We first identify
categories that are mainly related to background
content elements, e.g. ?sky, ground, floor, sand,
sidewalk, etc?. We consider these categories to
be potentially less interesting for study than cat-
egories like people, animals, cars, etc. Test Set B
contains objects sampled from the most frequently
occurring object categories in the dataset, selected
to contain a balanced number of objects from each
category, excluding the less interesting categories.
Test Set C contains objects sampled from images
that contain at least 2 objects of the same category,
excluding the less interesting categories.
Results: Qualitative examples are shown in Fig 5
comparing our results to the human produced ex-
pressions. For some images (left) we do quite well
at predicting the correct attributes and values. For
others we do less well (right). We also show exam-
ple objects predicted for some color words in Fig 4
(right). We see that our model can fail in several
ways, such as generating the wrong attribute-value
due to inaccurate predictions by visual models or
selecting incorrect attributes to include in the gen-
erated expression.
Quantitative results: precision and recall mea-
sures for the 3 test sets are reported in Table 1,
including evaluation of a baseline version of our
model which incorporates only the prior potentials
(?5.1.2) without any content based estimates. We
see that our model performs reasonably on both
measures, and outperforms the baseline by a large
margin on all test sets, with highest performance
on the broadly sampled interesting category test
set. Note that our problem is somewhat differ-
ent than traditional REG where the input is often
attribute-value pairs and the task is to select which
pairs to include in the expression. Our goal is to
jointly select which attributes to include and what
values to predict from a list of all possible values
for the attribute.
6 Conclusions & Future Work
In this paper we have introduced a new game to
crowd-source referring expressions for objects in
natural scenes. We have used this game to pro-
duce a new large-scale dataset with analysis. We
have also proposed an optimization based model
for REG and performed experimental evaluations.
Future work includes developing fully automatic
visual recognition methods for REG in real world
scenes, and incorporating linguistically inspired
models for entry-level category prediction.
Acknowledgments
This work was funded by NSF Awards #1417991
and #1444234. M.M. was supported by the Stony
Brook Simons Summer Research Program for
High School students. We also thank Alex Berg
for many helpful discussions.
796
References
Ahmet Aker and Robert Gaizauskas. 2010. Generating
image descriptions using dependency relational pat-
terns. In Association for Computational Linguistics
(ACL).
Andrei Barbu, Alexander Bridge, Zachary Burchill,
Dan Coroian, Sven J. Dickinson, Sanja Fi-
dler, Aaron Michaux, Sam Mussman, Siddharth
Narayanaswamy, Dhaval Salvi, Lara Schmidt,
Jiangnan Shangguan, Jeffrey Mark Siskind, Jar-
rell W. Waggoner, Song Wang, Jinlian Wei, Yifan
Yin, and Zhiqi Zhang. 2012. Video in sentences
out. In Uncertainty in Artificial Intelligence (UAI).
Robert Dale and Ehud Reiter. 1995. Computational
interpretations of the gricean maxims in the gener-
ation of referring expressions. Cognitive Science
(CogSci), 19:233264.
Robert Dale and Ehud Reiter. 2000. Building natural
language generation systems. In Cambridge Univer-
sity Press.
Jia Deng, Alexander C. Berg, Kai Li, and Fei-Fei Li.
2010. What does classifying more than 10,000 im-
age categories tell us? In European Conference on
Computer Vision (ECCV).
Jia Deng, Alex Berg, Sanjeev Satheesh, Hao Su, Aditya
Khosla, and Fei-Fei Li. 2012. Large scale vi-
sual recognition challenge. In http://www.image-
net.org/challenges/LSVRC/2012/index.
Jia Deng, Jonathan Krause, and Li Fei-Fei. 2013. Fine-
grained crowdsourcing for fine-grained recognition.
In IEEE Conference on Computer Vision and Pattern
Recognition (CVPR).
Hugo Jair Escalante, Carlos A. Hernandez, Jesus A.
Gonzalez, A. Lopez-Lopez, Manuel Montes, Ed-
uardo F. Morales, L. Enrique Sucar, Luis Villasenor,
and Michael Grubinger. 2010. The segmented and
annotated iapr tc-12 benchmark. Computer Vision
and Image Understanding (CVIU).
Rui Fang, Changsong Liu, Lanbo She, and Joyce Chai.
2013. Towards situated dialogue: Revisiting refer-
ring expression generation. In Empirical Methods
on Natural Language Processing (EMNLP).
Ali Farhadi, Mohsen Hejrati, Mohammad Amin
Sadeghi, Peter Young, Cyrus Rashtchian, Julia
Hockenmaier, and David Forsyth. 2010. Every
picture tells a story: generating sentences for im-
ages. In European Conference on Computer Vision
(ECCV).
Yansong Feng and Mirella Lapata. 2010. How many
words is a picture worth? automatic caption genera-
tion for news images. In Association for Computa-
tional Linguistics (ACL).
Yansong Feng and Mirella Lapata. 2013. Automatic
caption generation for news images. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence,
35(4):797?812.
Nicholas FitzGerald, Yoav Artzi, and Luke Zettle-
moyer. 2013. Learning distributions over logical
forms for referring expression generation. In Em-
pirical Methods on Natural Language Processing
(EMNLP).
H. Paul Grice. 1975. Logic and conversation. page
4158.
Michael Grubinger, Paul D. Clough, Henning Muller,
and Thomas Deselaers. 2006. The iapr benchmark:
A new evaluation resource for visual information
systems. In Proceedings of the International Work-
shop OntoImage (LREC).
Sergio Guadarrama, Niveda Krishnamoorthy, Girish
Malkarnenkar, Subhashini Venugopalan, Raymond
Mooney, Trevor Darrell, and Kate Saenko. 2013.
Youtube2text: Recognizing and describing arbitrary
activities using semantic hierarchies and zero-shot
recognition. In International Conference on Com-
puter Vision (ICCV).
Emiel Krahmer and Kees van Deemter. 2012. Compu-
tational generation of referring expressions: A sur-
vey. In Computational Linguistics, volume 38, page
173218.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton.
2012. Imagenet classification with deep convolu-
tional neural networks. In Neural Information Pro-
cessing Systems (NIPS).
Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Sim-
ing Li, Yejin Choi, Alexander C Berg, and Tamara L
Berg. 2011. Babytalk: Understanding and generat-
ing simple image descriptions. In IEEE Computer
Vision and Pattern Recognition (CVPR).
Polina Kuznetsova, Vicente Ordonez, Alex Berg,
Tamara L Berg, and Yejin Choi. 2012. Collective
generation of natural image descriptions. In Associ-
ation for Computational Linguistics (ACL).
Margaret Mitchell, Kees van Deemter, and Ehud Re-
iter. 2010. Natural reference to objects in a visual
domain. In International Natural Language Gener-
ation Conference (INLG).
Margaret Mitchell, Kees van Deemter, and Ehud Reiter.
2011. Two approaches for generating size modifiers.
In European Workshop on Natural Language Gener-
ation.
Margaret Mitchell, Ehud Reiter, and Kees van Deemter.
2013a. Typicality and object reference. In Cognitive
Science (CogSci).
Margaret Mitchell, Kees van Deemter, and Ehud Reiter.
2013b. Generating expressions that refer to visible
objects. In North American Chapter of the Associa-
tion for Computational Linguistics (NAACL).
797
Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg.
2011. Im2text: Describing images using 1 million
captioned photographs. In Neural Information Pro-
cessing Systems (NIPS).
Vicente Ordonez, Jia Deng, Yejin Choi, Alexander C.
Berg, and Tamara L. Berg. 2013. From large scale
image categorization to entry-level categories. In In-
ternational Conference on Computer Vision (ICCV).
Florent Perronnin, Zeynep Akata, Zaid Harchaoui, and
Cordelia Schmid. 2012. Towards good practice
in large-scale learning for image classification. In
Computer Vision and Pattern Recognition (CVPR).
Yuan Ren, Kees Van Deemter, and Jeff Z Pan. 2010.
Charting the potential of description logic for the
generation of referring expressions. In International
Natural Language Generation Conference (INLG).
Eleanor Rosch. 1978. Principles of categorization.
Cognition and Categorization, page 2748.
Nitin Seemakurty, Jonathan Chu, Luis von Ahn, and
Anthony Tomasic. 2010. Word sense disambigua-
tion via human computation. In Human Computa-
tion Workshop.
Richard Socher, John Bauer, Christopher D. Manning,
and Andrew Y. Ng. 2013. Parsing With Composi-
tional Vector Grammars. In Association for Compu-
tational Linguistics (ACL).
Kees van Deemter, Ielka van der Sluis, and Albert Gatt.
2006. Building a semantically transparent corpus
for the generation of referring expressions. In In-
ternational Conference on Natural Language Gen-
eration (INLG).
Kees Van Deemter, Albert Gatt, Roger PG van Gompel,
and Emiel Krahmer. 2012. Toward a computational
psycholinguistics of reference production. In Topics
in Cognitive Science, volume 4(2), page 166183.
Jette Viethen and Robert Dale. 2008. The use of spa-
tial relations in referring expression generation. In
International Natural Language Generation Confer-
ence (INLG).
Jette Viethen and Robert Dale. 2010. Speaker-
dependent variation in content selection for referring
expression generation. In Australasian Language
Technology Workshop.
Jette Viethen, Margaret Mitchell, and Emiel Krahmer.
2013. Graphs and spatial relations in the generation
of referring expressions. In European Workshop on
Natural Language Generation.
Luis von Ahn and Laura Dabbish. 2004. Labeling im-
ages with a computer game. In ACM Conf. on Hu-
man Factors in Computing Systems (CHI).
Luis von Ahn, Mihir Kedia, and Manuel Blum. 2006a.
Verbosity: A game for collecting common-sense
knowledge. In ACM Conference on Human Factors
in Computing Systems (CHI).
Luis von Ahn, Ruoran Liu, and Manuel Blum. 2006b.
Peekaboom: A game for locating objects in images.
In ACM Conference on Human Factors in Comput-
ing Systems (CHI).
Terry Winograd. 1972. Understanding natural lan-
guage. Cognitive Psychology, 3(1):1191.
Yezhou Yang, Ching Lik Teo, Hal Daume III, and Yian-
nis Aloimonos. 2011. Corpus-guided sentence gen-
eration of natural images. In Empirical Methods on
Natural Language Processing (EMNLP).
Benjamin Z. Yao, Xiong Yang, Liang Lin, Mun Wai
Lee, and Song-Chun Zhu. 2010. I2t: Image parsing
to text description. Proc. IEEE, 98(8).
798
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 359?368,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Collective Generation of Natural Image Descriptions
Polina Kuznetsova, Vicente Ordonez, Alexander C. Berg,
Tamara L. Berg and Yejin Choi
Department of Computer Science
Stony Brook University
Stony Brook, NY 11794-4400
{pkuznetsova,vordonezroma,aberg,tlberg,ychoi}@cs.stonybrook.edu
Abstract
We present a holistic data-driven approach
to image description generation, exploit-
ing the vast amount of (noisy) parallel im-
age data and associated natural language
descriptions available on the web. More
specifically, given a query image, we re-
trieve existing human-composed phrases
used to describe visually similar images,
then selectively combine those phrases
to generate a novel description for the
query image. We cast the generation pro-
cess as constraint optimization problems,
collectively incorporating multiple inter-
connected aspects of language composition
for content planning, surface realization
and discourse structure. Evaluation by hu-
man annotators indicates that our final
system generates more semantically cor-
rect and linguistically appealing descrip-
tions than two nontrivial baselines.
1 Introduction
Automatically describing images in natural lan-
guage is an intriguing, but complex AI task, re-
quiring accurate computational visual recogni-
tion, comprehensive world knowledge, and natu-
ral language generation. Some past research has
simplified the general image description goal by
assuming that relevant text for an image is pro-
vided (e.g., Aker and Gaizauskas (2010), Feng
and Lapata (2010)). This allows descriptions to
be generated using effective summarization tech-
niques with relatively surface level image under-
standing. However, such text (e.g., news articles
or encyclopedic text) is often only loosely related
to an image?s specific content and many natu-
ral images do not come with associated text for
summarization.
In contrast, other recent work has focused
more on the visual recognition aspect by de-
tecting content elements (e.g., scenes, objects,
attributes, actions, etc) and then composing de-
scriptions from scratch (e.g., Yao et al (2010),
Kulkarni et al (2011), Yang et al (2011), Li
et al (2011)), or by retrieving existing whole
descriptions from visually similar images (e.g.,
Farhadi et al (2010), Ordonez et al (2011)). For
the latter approaches, it is unrealistic to expect
that there will always exist a single complete de-
scription for retrieval that is pertinent to a given
query image. For the former approaches, visual
recognition first generates an intermediate rep-
resentation of image content using a set of En-
glish words, then language generation constructs
a full description by adding function words and
optionally applying simple re-ordering. Because
the generation process sticks relatively closely
to the recognized content, the resulting descrip-
tions often lack the kind of coverage, creativ-
ity, and complexity typically found in human-
written text.
In this paper, we propose a holistic data-
driven approach that combines and extends the
best aspects of these previous approaches ? a)
using visual recognition to directly predict indi-
vidual image content elements, and b) using re-
trieval from existing human-composed descrip-
tions to generate natural, creative, and inter-
359
esting captions. We also lift the restriction of
retrieving existing whole descriptions by gather-
ing visually relevant phrases which we combine
to produce novel and query-image specific de-
scriptions. By judiciously exploiting the corre-
spondence between image content elements and
phrases, it is possible to generate natural lan-
guage descriptions that are substantially richer
in content and more linguistically interesting
than previous work.
At a high level, our approach can be moti-
vated by linguistic theories about the connection
between reading activities and writing skills,
i.e., substantial reading enriches writing skills,
(e.g., Hafiz and Tudor (1989), Tsang (1996)).
Analogously, our generation algorithm attains a
higher level of linguistic sophistication by read-
ing large amounts of descriptive text available
online. Our approach is also motivated by lan-
guage grounding by visual worlds (e.g., Roy
(2002), Dindo and Zambuto (2010), Monner and
Reggia (2011)), as in our approach the mean-
ing of a phrase in a description is implicitly
grounded by the relevant content of the image.
Another important thrust of this work is col-
lective image-level content-planning, integrating
saliency, content relations, and discourse struc-
ture based on statistics drawn from a large
image-text parallel corpus. This contrasts with
previous approaches that generate multiple sen-
tences without considering discourse flow or re-
dundancy (e.g., Li et al (2011)). For example,
for an image showing a flock of birds, generating
a large number of sentences stating the relative
position of each bird is probably not useful.
Content planning and phrase synthesis can
be naturally viewed as constraint optimization
problems. We employ Integer Linear Program-
ming (ILP) as an optimization framework that
has been used successfully in other generation
tasks (e.g., Clarke and Lapata (2006), Mar-
tins and Smith (2009), Woodsend and Lapata
(2010)). Our ILP formulation encodes a rich
set of linguistically motivated constraints and
weights that incorporate multiple aspects of the
generation process. Empirical results demon-
strate that our final system generates linguisti-
cally more appealing and semantically more cor-
rect descriptions than two nontrivial baselines.
1.1 System Overview
Our system consists of two parts. For a query
image, we first retrieve candidate descriptive
phrases from a large image-caption database us-
ing measures of visual similarity (?2). We then
generate a coherent description from these can-
didates using ILP formulations for content plan-
ning (?4) and surface realization (?5).
2 Vision & Phrase Retrieval
For a query image, we retrieve relevant candi-
date natural language phrases by visually com-
paring the query image to database images from
the SBU Captioned Photo Collection (Ordonez
et al, 2011) (1 million photographs with asso-
ciated human-composed descriptions). Visual
similarity for several kinds of image content are
used to compare the query image to images from
the database, including: 1) object detections for
89 common object categories (Felzenszwalb et
al., 2010), 2) scene classifications for 26 com-
mon scene categories (Xiao et al, 2010), and
3) region based detections for stuff categories
(e.g. grass, road, sky) (Ordonez et al, 2011).
All content types are pre-computed on the mil-
lion database photos, and caption parsing is per-
formed using the Berkeley PCFG parser (Petrov
et al, 2006; Petrov and Klein, 2007).
Given a query image, we identify content el-
ements present using the above classifiers and
detectors and then retrieve phrases referring to
those content elements from the database. For
example, if we detect a horse in a query im-
age, then we retrieve phrases referring to vi-
sually similar horses in the database by com-
paring the color, texture (Leung and Malik,
1999), or shape (Dalal and Triggs, 2005; Lowe,
2004) of the detected horse to detected horses
in the database images. We collect four types of
phrases for each query image as follows:
[1] NPs We retrieve noun phrases for each
query object detection (e.g., ?the brown cow?)
from database captions using visual similar-
ity between object detections computed as an
equally weighted linear combination of L2 dis-
360
tances on histograms of color, texton (Leung and
Malik, 1999), HoG (Dalal and Triggs, 2005) and
SIFT (Lowe, 2004) features.
[2] VPs We retrieve verb phrases for each
query object detection (e.g. ?boy running?)
from database captions using the same mea-
sure of visual similarity as for NPs, but restrict-
ing the search to only those database instances
whose captions contain a verb phrase referring
to the object category.
[3] Region/Stuff PPs We collect preposi-
tional phrases for each query stuff detection (e.g.
?in the sky?, ?on the road?) by measuring visual
similarity of appearance (color, texton, HoG)
and geometric configuration (object-stuff rela-
tive location and distance) between query and
database detections.
[4] Scene PPs We also collect prepositonal
phrases referring to general image scene context
(e.g. ?at the market?, ?on hot summer days?,
?in Sweden?) based on global scene similarity
computed using L2 distance between scene clas-
sification score vectors (Xiao et al, 2010) com-
puted on the query and database images.
3 Overview of ILP Formulation
For each image, we aim to generate multiple
sentences, each sentence corresponding to a sin-
gle distinct object detected in the given image.
Each sentence comprises of the NP for the main
object, and a subset of the corresponding VP,
region/stuff PP, and scene PP retrieved in ?2.
We consider four different types of operations
to generate the final description for each image:
T1. Selecting the set of objects to describe (one
object per sentence).
T2. Re-ordering sentences (i.e., re-ordering ob-
jects).
T3. Selecting the set of phrases for each sen-
tence.
T4. Re-ordering phrases within each sentence.
The ILP formulation of ?4 addresses T1 & T2,
i.e., content-planning, and the ILP of ?5 ad-
dresses T3 & T4, i.e., surface realization.1
1It is possible to create one conjoined ILP formulation
to address all four operations T1?T4 at once. For com-
4 Image-level Content Planning
First we describe image-level content planning,
i.e., abstract generation. The goals are to (1) se-
lect a subset of the objects based on saliency and
semantically compatibility, and (2) order the se-
lected objects based on their content relations.
4.1 Variables and Objective Function
The following set of indicator variables encodes
the selection of objects and ordering:
ysk =
?
?
?
1, if object s is selected
for position k
0, otherwise
(1)
where k = 1, ..., S encodes the position (order)
of the selected objects, and s indexes one of the
objects. In addition, we define a set of variables
indicating specific pairs of adjacent objects:
yskt(k+1) =
{
1, if ysk = yt(k+1) = 1
0, otherwise
(2)
The objective function, F , that we will maxi-
mize is a weighted linear combination of these
indicator variables and can be optimized using
integer linear programming:
F =
?
s
Fs ?
S?
k=1
ysk ?
?
st
Fst ?
S?1?
k=1
yskt(k+1) (3)
where Fs quantifies the salience/confidence of
the object s, and Fst quantifies the seman-
tic compatibility between the objects s and t.
These coefficients (weights) will be described in
?4.3 and ?4.4. We use IBM CPLEX to optimize
this objective function subject to the constraints
introduced next in ?4.2.
4.2 Constraints
Consistency Constraints: We enforce consis-
tency between indicator variables for indivisual
objects (Eq. 1) and consecutive objects (Eq. 2)
so that yskt(k+1) = 1 iff ysk = 1 and yt(k+1) = 1:
?stk, yskt(k+1) ? ysk (4)
yskt(k+1) ? yt(k+1) (5)
yskt(k+1) + (1? ysk) + (1? yt(k+1)) ? 1 (6)
putational and implementation efficiency however, we opt
for the two-step approach.
361
To avoid empty descriptions, we enforce that the
result includes at least one object:
?
s
ys1 = 1 (7)
To enforce contiguous positions be selected:
?k = 2, ..., S ? 1,
?
s
ys(k+1) ?
?
s
ysk (8)
Discourse constraints: To avoid spurious de-
scriptions, we allow at most two objects of the
same type, where cs is the type of object s:
?c ? objTypes,
?
{s: cs=c}
S?
k=1
ysk ? 2 (9)
4.3 Weight Fs: Object Detection
Confidence
In order to quantify the confidence of the object
detector for the object s, we define 0 ? Fs ? 1
as the mean of the detector scores for that object
type in the image.
4.4 Weight Fst: Ordering and
Compatibility
The weight 0 ? Fst ? 1 quantifies the compat-
ibility of the object pairing (s, t). Note that in
the objective function, we subtract this quan-
tity from the function to be maximized. This
way, we create a competing tension between the
single object selection scores and the pairwise
compatibility scores, so that variable number of
objects can be selected.
Object Ordering Statistics: People have bi-
ases on the order of topic or content flow. We
measure these biases by collecting statistics on
ordering of object names from the 1 million im-
age descriptions in the SBU Captioned Dataset
(Ordonez et al, 2011). Let ford(w1, w2) be
the number of times w1 appeared before w2.
For instance, ford(window, house) = 2895 and
ford(house, window) = 1250, suggesting that
people are more likely to mention a window be-
fore mentioning a house/building2. We use these
ordering statistics to enhance content flow. We
define score for the order of objects using Z-score
for normalization as follows:
F?st =
ford(cs, ct)?mean(ford)
std dev(ford)
(10)
2We take into account synonyms.
We then transform F?st so that F?st ? [0,1], and
then set Fst = 1 ? F?st so that smaller values
correspond to better choices.
5 Surface Realization
Recall that for each image, the computer vi-
sion system identifies phrases from descriptions
of images that are similar in a variety of aspects.
The result is a set of phrases representing four
different types of information (?2). From this
assortment of phrases, we aim to select a subset
and glue them together to compose a complete
sentence that is linguistically plausible and se-
mantically truthful to the content of the image.
5.1 Variables and Objective Function
The following set of variables encodes the selec-
tion of phrases and their ordering in construct-
ing S? sentences.
xsijk =
?
?????
?????
1, if phrase i of type j
is selected
for position k
in sentence s
0, otherwise
(11)
where k = 1, ..., N encodes the ordering of the
selected phrases, and j indexes one of the four
phrases types (object-NPs, action-VPs, region-
PPs, scene-PPs), i = 1, ...,M indexes one of
the M candidate phrases of each phrase type,
and s = 1, ..., S? encodes the sentence (object).
In addition, we define indicator variables for
adjacent pairs of phrases: xsijkpq(k+1) = 1 if
xsijk = xspq(k+1) = 1 and 0 otherwise. Finally,
we define the objective function F as:
F =
?
sij
Fsij ?
N?
k=1
xsijk
?
?
sijpq
Fsijpq ?
N?1?
k=1
xsijkpq(k+1) (12)
where Fsij weights individual phrase goodness
and Fsijpq adjacent phrase goodness. All coeffi-
cients (weights) will be described in Section 5.3
and 5.4.
We optionally prepend the first sentence in a
generated description with a cognitive phrase.3
3We collect most frequent 200 phrases of length 1-
7 that start a caption from the SBU Captioned Photo
Collection.
362
ILP: I think this is a boy?s bike 
lied in saltwater for quite a 
while. 
 
HMM: I liked the way bicycles 
leaning against a wall in 
Copenhagen Denmark in a 
windy sky in a Singapore 
bathroom. Boy?s bike lied in 
saltwater for quite a while in a 
windy sky in a Singapore 
bathroom. Fruit rubbing his 
face in the encrusted snow in a 
windy sky in a Singapore 
bathroom. 
 
Human: You re nobody in 
Oxford, unless you have a old 
bike with a basket 
 
ILP:  
This is a photo of this little flower sprouted up in defiance against grass.  
Bright yellow flowers growing in a rock garden at Volcan Mombacho. 
 
HMM: These was taken on the flowers growing in a rock garden in the field in two sorts. This 
little flower sprouted up in defiance in the field in two sorts. A full open flower sprouted up in 
defiance in the field in gardens. Bright yellow flowers growing in a rock garden in the field. 
 
Human: Yellow flower in my field 
 
ILP: Found trucks parked on first avenue in the east village. 
 
HMM: This is the first cellar door left back bedroom in center 
and clothes dryer to the right to the building in the house. 
This HUGE screen hanging on the wall outside a burned down 
building in the house. My truck parked on first avenue in the 
east village by the glass buildings in the house. 
 
Human: Flat bed Chisholms truck on display at the vintage 
vehicle rall y at Astley Green Colliery near Leigh Lancs 
 
Figure 1: ILP & HMM generated captions. In HMM generated captions, underlined phrases show redundancy
across different objects (due to lack of discourse constraints), and phrases in boldface show awkward topic
flow (due to lack of content planning). Note that in the bicycle image, the visual recognizer detected two
separate bicycles and some fruits, as can be seen in the HMM result. Via collective image-level content
planning (see ?4), some of these erroneous detection can be corrected, as shown in the ILP result. Spurious
and redundant phrases can be suppressed via discourse constraints (see ?5).
These are generic constructs that are often used
to start a description about an image, for in-
stance, ?This is an image of...?. We treat these
phrases as an additional type, but omit corre-
sponding variables and constraints for brevity.
5.2 Constraints
Consistency Constraints: First we enforce
consistency between the unary variables (Eq.
11) and the pairwise variables so that xsijkpqm =
1 iff xsijk = 1 and xspqm = 1:
?ijkpqm, xsijkpqm ? xsijk (13)
xsijkpqm ? xspqm (14)
xsijkpqm + (1? xsijk) + (1? xspqm) ? 1 (15)
Next we include constraints similar to Eq. 8
(contiguous slots are filled), but omit them for
brevity. Finally, we add constraints to ensure at
least two phrases are selected for each sentence,
to promote informative descriptions.
Linguistic constraints: We include linguisti-
cally motivated constraints to generate syntacti-
cally and semantically plausible sentences. First
we enforce a noun-phrase to be selected to en-
sure semantic relevance to the image:
?s,
?
ik
xsiNPk = 1 (16)
Also, to avoid content redundancy, we allow at
most one phrase of each type:
?sj,
?
i
N?
k=1
xsijk ? 1 (17)
Discourse constraints: We allow at most
one prepositional scene phrase for the whole de-
scription to avoid redundancy:
For j = PPscene,
?
sik
xsijk ? 1 (18)
We add constraints that prevent the inclusion of
more than one phrase with identical head words:
?s, ij, pq with the same heads,
N?
k=1
xsijk +
N?
k=1
xspqk ? 1 (19)
5.3 Unary Phrase Selection
Let Msij be the confidence score for phrase
xsij given by the image?phrase matching al-
gorithm (?2). To make the scores across dif-
ferent phrase types comparable, we normalize
them using Z-score: Fsij = norm?(Msij) =
(Msij ? meanj)/devj , and then transform the
values into the range of [0,1].
5.4 Pairwise Phrase Cohesion
In this section, we describe the pairwise phrase
cohesion score Fsijpq defined for each xsijpq in
363
ILP: I like the way the clouds hanging down by 
the ground in Dupnitsa of Avikwalal. 
 
Human: Car was raised on the wall over a bridge 
facing traffic..paramedics were attending the 
driver on the ground 
ILP: This is a photo of this bird hopping 
around eating things off of the ground by 
river. 
Human: IMG_6892 Lookn up in the sky its a 
bird its a plane its ah..... you 
ILP: This is a sporty little red convertible made for 
a great day in Key West FL. This car was in the 4th 
parade of the apartment buildings. 
 
Human: Hard rock casino exotic car show in June 
ILP: Taken in front of my cat sitting in a shoe 
box. Cat likes hanging around in my recliner. 
 
Human: H happily rests his armpit on a 
warm Gatorade bottle of water (a small 
bottle wrapped in a rag) 
Figure 2: In some cases (16%), ILP generated captions were preferred over human written ones!
the objective function (Eq. 12). Via Fsijpq,
we aim to quantify the degree of syntactic and
semantic cohesion across two phrases xsij and
xspq. Note that we subtract this cohesion score
from the objective function. This trick helps the
ILP solver to generate sentences with varying
number of phrases, rather than always selecting
the maximum number of phrases allowed.
N-gram Cohesion Score: We use n-gram
statistics from the Google Web 1-T dataset
(Brants and Franz., 2006) Let Lsijpq be the set
of all n-grams (2 ? n ? 5) across xsij and xspq.
Then the n-gram cohesion score is computed as:
FNGRAMsijpq = 1?
?
l?Lsijpq
NPMI(l)
size(Lsijpq)
(20)
NPMI(ngr) =
PMI(ngr)? PMImin
PMImax ? PMImin
(21)
Where NPMI is the normalized point-wise mu-
tual information.4
Co-occurrence Cohesion Score: To cap-
ture long-distance cohesion, we introduce a co-
occurrence-based score, which measures order-
preserved co-occurrence statistics between the
head words hsij and hspq 5. Let f?(hsij , hspq)
be the sum frequency of all n-grams that start
with hsij , end with hspq and contain a prepo-
sition prep(spq) of the phrase spq. Then the
4We include the n-gram cohesion for the sentence
boundaries as well, by approximating statistics for sen-
tence boundaries with punctuation marks in the Google
Web 1-T data.
5For simplicity, we use the last word of a phrase as
the head word, except VPs where we take the main verb.
co-occurrence cohesion is computed as:
FCOsijpq =
max(f?)? f?(hsij , hspq)
max(f?)?min(f?)
(22)
Final Cohesion Score: Finally, the pairwise
phrase cohesion score Fijpq is a weighted sum of
n-gram and co-occurrence cohesion scores:
Fsijpq =
? ? FNGRAMsijpq + ? ? F
CO
sijpq
?+ ?
(23)
where ? and ? can be tuned via grid search,
and FNGRAMijpq and F
CO
ijpq are normalized ? [0, 1]
for comparability. Notice that Fsijpq is in the
range [0,1] as well.
6 Evaluation
TestSet: Because computer vision is a challeng-
ing and unsolved problem, we restrict our query
set to images where we have high confidence that
visual recognition algorithms perform well. We
collect 1000 test images by running a large num-
ber (89) of object detectors on 20,000 images
and selecting images that receive confident ob-
ject detection scores, with some preference for
images with multiple object detections to obtain
good examples for testing discourse constraints.
Baselines: We compare our ILP approaches
with two nontrivial baselines: the first is an
HMM approach (comparable to Yang et al
(2011)), which takes as input the same set of
candidate phrases described in ?2, but for de-
coding, we fix the ordering of phrases as [ NP
? VP ? Region PP ? Scene PP] and find the
best combination of phrases using the Viterbi
algorithm. We use the same rich set of pairwise
364
Hmm Hmm Ilp Ilp
cognitive phrases: with w/o with w/o
0.111 0.114 0.114 0.116
Table 1: Automatic Evaluation
ILP selection rate
ILP V.S. HMM (w/o cogn) 67.2%
ILP V.S. HMM (with cogn) 66.3%
Table 2: Human Evaluation (without images)
ILP selection rate
ILP V.S. HMM (w/o cogn) 53.17%
ILP V.S. HMM (with cogn) 54.5%
ILP V.S. Retrieval 71.8%
ILP V.S. Human 16%
Table 3: Human Evaluation (with images)
phrase cohesion scores (?5.4) used for the ILP
formulation, producing a strong baseline6.
The second baseline is a recent Retrieval
based description method (Ordonez et al, 2011),
that searches the large parallel corpus of im-
ages and captions, and transfers a caption from
a visually similar database image to the query.
This again is a very strong baseline, as it ex-
ploits the vast amount of image-caption data,
and produces a description high in linguistic
quality (since the captions were written by hu-
man annotators).
Automatic Evaluation: Automatically quan-
tifying the quality of machine generated sen-
tences is known to be difficult. BLEU score
(Papineni et al, 2002), despite its simplicity
and limitations, has been one of the common
choices for automatic evaluation of image de-
scriptions (Farhadi et al, 2010; Kulkarni et al,
2011; Li et al, 2011; Ordonez et al, 2011), as
it correlates reasonably well with human evalu-
ation (Belz and Reiter, 2006).
Table 1 shows the the BLEU @1 against the
original caption of 1000 images. We see that the
ILP improves the score over HMM consistently,
with or without the use of cognitive phrases.
6Including other long-distance scores in HMM decod-
ing would make the problem NP-hard and require more
sophisticated decoding, e.g. ILP.
Grammar Cognitive Relevance
HMM 3.40(?=.82) 3.40(?=.88) 2.25(?=1.37)
ILP 3.56(?=.90) 3.60(?=.98) 2.37(?=1.49)
Hum. 4.36(?=.79) 4.77(?=.66) 3.86(?=1.60)
Table 4: Human Evaluation: Multi-Aspect Rating
(? is a standard deviation)
Human Evaluation I ? Ranking: We com-
plement the automatic evaluation with Mechan-
ical Turk evaluation. In ranking evaluation, we
ask raters to choose a better caption between
two choices7. We do this rating with and with-
out showing the images, as summarized in Ta-
ble 2 & 3. When images are shown, raters evalu-
ate content relevance as well as linguistic quality
of the captions. Without images, raters evaluate
only linguistic quality.
We found that raters generally prefer ILP gen-
erated captions over HMM generated ones, twice
as much (67.2% ILP V.S. 32.8% HMM), if im-
ages are not presented. However the difference is
less pronounced when images are shown. There
could be two possible reasons. The first is that
when images are shown, the Turkers do not try
as hard to tell apart the subtle difference be-
tween the two imperfect captions. The second
is that the relative content relevance of ILP gen-
erated captions is negating the superiority in lin-
guistic quality. We explore this question using
multi-aspect rating, described below.
Note that ILP generated captions are exceed-
ingly (71.8 %) preferred over the Retrieval
baseline (Ordonez et al, 2011), despite the gen-
erated captions tendency to be more prone to
grammatical and cognitive errors than retrieved
ones. This indicates that the generated captions
must have substantially better content relevance
to the query image, supporting the direction of
this research. Finally, notice that as much as
16% of the time, ILP generated captions are pre-
ferred over the original human generated ones
(examples in Figure 2).
Human Evaluation II ? Multi-Aspect Rat-
ing: Table 4 presents rating in the 1?5 scale (5:
perfect, 4: almost perfect, 3: 70?80% good, 2:
7We present two captions in a randomized order.
365
Found MIT boy 
gave me this 
quizical expression. 
One of the most shirt 
in the wall of the 
house. 
Grammar Problems 
Here you can see a 
bright red flower taken 
near our apartment in 
Torremolinos the Costa 
Del Sol. 
Content Irrelevance 
This is a shoulder bag with 
a blended rainbow effect. 
Cognitive Absurdity 
Here you can see a cross 
by the frog in the sky. 
Figure 3: Examples with different aspects of prob-
lems in the ILP generated captions.
50?70% good, 1: totally bad) in three different
aspects: grammar, cognitive correctness,8 and
relevance. We find that ILP improves over HMM
in all aspects, however, the relevance score is no-
ticeably worse than scores of two other criteria.
It turns out human raters are generally more
critical against the relevance aspect, as can be
seen in the ratings given to the original human
generated captions.
Discussion with Examples: Figure 1 shows
contrastive examples of HMM vs ILP gener-
ated captions. Notice that HMM captions
look robotic, containing spurious and redundant
phrases due to lack of discourse constraints, and
often discussing an awkward set of objects due
to lack of image-level content planning. Also
notice how image-level content planning under-
pinned by language statistics helps correct some
of the erroneous vision detections. Figure 3
shows some example mistakes in the ILP gen-
erated captions.
7 Related Work & Discussion
Although not directly focused on image descrip-
tion generation, some previous work in the realm
of summarization shares the similar problem of
content planning and surface realization. There
8E.g., ?A desk on top of a cat? is grammatically cor-
rect, but cognitively absurd.
are subtle, but important differences however.
First, sentence compression is hardly the goal
of image description generation, as human writ-
ten descriptions are not necessarily succinct.9
Second, unlike summarization, we are not given
with a set of coherent text snippet to begin with,
and the level of noise coming from the visual
recognition errors is much higher than that of
starting with clean text. As a result, choosing
an additional phrase in the image description is
much riskier than it is in summarization.
Some recent research proposed very elegant
approaches to summarization using ILP for col-
lective content planning and/or surface realiza-
tion (e.g., Martins and Smith (2009), Woodsend
and Lapata (2010), Woodsend et al (2010)).
Perhaps the most important difference in our
approach is the use of negative weights in the
objective function to create the necessary ten-
sion between selection (salience) and compatibil-
ity, which makes it possible for ILP to generate
variable length descriptions, effectively correct-
ing some of the erroneous vision detections. In
contrast, all previous work operates with a pre-
defined upper limit in length, hence the ILP was
formulated to include as many textual units as
possible modulo constraints.
To conclude, we have presented a collective
approach to generating natural image descrip-
tions. Our approach is the first to systematically
incorporate state of the art computer vision
to retrieve visually relevant candidate phrases,
then produce images descriptions that are sub-
stantially more complex and human-like than
previous attempts.
Acknowledgments T. L. Berg is supported
in part by NSF CAREER award #1054133; A.
C. Berg and Y. Choi are partially supported by
the Stony Brook University Office of the Vice
President for Research. We thank K. Yam-
aguchi, X. Han, M. Mitchell, H. Daume III, A.
Goyal, K. Stratos, A. Mensch, J. Dodge for data
pre-processing and useful initial discussions.
9On a related note, the notion of saliency also differs
in that human written captions often digress on details
that might be tangential to the visible content of the
image. E.g., ?This is a dress my mom made.?, where the
picture does not show a woman making the dress.
366
References
Ahmet Aker and Robert Gaizauskas. 2010. Gen-
erating image descriptions using dependency rela-
tional patterns. In ACL.
Anja Belz and Ehud Reiter. 2006. Comparing au-
tomatic and human evaluation of nlg systems.
In EACL 2006, 11st Conference of the European
Chapter of the Association for Computational Lin-
guistics, Proceedings of the Conference, April 3-7,
2006, Trento, Italy. The Association for Computer
Linguistics.
Thorsten Brants and Alex Franz. 2006. Web 1t 5-
gram version 1. In Linguistic Data Consortium.
James Clarke and Mirella Lapata. 2006. Constraint-
based sentence compression: An integer program-
ming approach. In Proceedings of the COL-
ING/ACL 2006 Main Conference Poster Sessions,
pages 144?151, Sydney, Australia, July. Associa-
tion for Computational Linguistics.
Navneet Dalal and Bill Triggs. 2005. Histograms of
oriented gradients for human detection. In Pro-
ceedings of the 2005 IEEE Computer Society Con-
ference on Computer Vision and Pattern Recogni-
tion (CVPR?05) - Volume 1 - Volume 01, CVPR
?05, pages 886?893, Washington, DC, USA. IEEE
Computer Society.
Haris Dindo and Daniele Zambuto. 2010. A prob-
abilistic approach to learning a visually grounded
language model through human-robot interaction.
In IROS, pages 790?796. IEEE.
Ali Farhadi, Mohsen Hejrati, Mohammad Amin
Sadeghi, Peter Young, Cyrus Rashtchian, Julia
Hockenmaier, and David Forsyth. 2010. Every
picture tells a story: generating sentences for im-
ages. In ECCV.
Pedro F. Felzenszwalb, Ross B. Girshick, David
McAllester, and Deva Ramanan. 2010. Object
detection with discriminatively trained part based
models. tPAMI, Sept.
Yansong Feng and Mirella Lapata. 2010. How many
words is a picture worth? automatic caption gen-
eration for news images. In ACL.
Fateh Muhammad Hafiz and Ian Tudor. 1989. Ex-
tensive reading and the development of language
skills. ELT Journal, 43(1):4?13.
Girish Kulkarni, Visruth Premraj, Sagnik Dhar,
Siming Li, Yejin Choi, Alexander C Berg, and
Tamara L Berg. 2011. Babytalk: Understand-
ing and generating simple image descriptions. In
CVPR.
Thomas K. Leung and Jitendra Malik. 1999. Rec-
ognizing surfaces using three-dimensional textons.
In ICCV.
Siming Li, Girish Kulkarni, Tamara L. Berg, Alexan-
der C. Berg, and Yejin Choi. 2011. Compos-
ing simple image descriptions using web-scale n-
grams. In Proceedings of the Fifteenth Confer-
ence on Computational Natural Language Learn-
ing, pages 220?228, Portland, Oregon, USA, June.
Association for Computational Linguistics.
David G. Lowe. 2004. Distinctive image features
from scale-invariant keypoints. Int. J. Comput.
Vision, 60:91?110, November.
Andre Martins and Noah A. Smith. 2009. Summa-
rization with a joint model for sentence extraction
and compression. In Proceedings of the Workshop
on Integer Linear Programming for Natural Lan-
guage Processing, pages 1?9, Boulder, Colorado,
June. Association for Computational Linguistics.
Derek D. Monner and James A. Reggia. 2011. Sys-
tematically grounding language through vision in
a deep, recurrent neural network. In Proceed-
ings of the 4th international conference on Arti-
ficial general intelligence, AGI?11, pages 112?121,
Berlin, Heidelberg. Springer-Verlag.
Vicente Ordonez, Girish Kulkarni, and Tamara L.
Berg. 2011. Im2text: Describing images using 1
million captioned photographs. In Neural Infor-
mation Processing Systems (NIPS).
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In ACL.
Slav Petrov and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In HLT-NAACL.
Slav Petrov, Leon Barrett, Romain Thibaux, and
Dan Klein. 2006. Learning accurate, com-
pact, and interpretable tree annotation. In COL-
ING/ACL.
Deb K. Roy. 2002. Learning visually-grounded
words and syntax for a scene description task.
Computer Speech and Language, In review.
Wai-King Tsang. 1996. Comparing the effects of
reading and writing on writing performance. Ap-
plied Linguistics, 17(2):210?233.
Kristian Woodsend and Mirella Lapata. 2010. Au-
tomatic generation of story highlights. In Pro-
ceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 565?
574, Uppsala, Sweden, July. Association for Com-
putational Linguistics.
Kristian Woodsend, Yansong Feng, and Mirella
Lapata. 2010. Title generation with quasi-
synchronous grammar. In Proceedings of the 2010
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP ?10, pages 513?523,
Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
367
Jianxiong Xiao, James Hays, Krista A. Ehinger,
Aude Oliva, and Antonio Torralba. 2010. Sun
database: Large-scale scene recognition from
abbey to zoo. In CVPR.
Yezhou Yang, Ching Teo, Hal Daume III, and Yian-
nis Aloimonos. 2011. Corpus-guided sentence gen-
eration of natural images. In Proceedings of the
2011 Conference on Empirical Methods in Nat-
ural Language Processing, pages 444?454, Edin-
burgh, Scotland, UK., July. Association for Com-
putational Linguistics.
Benjamin Z. Yao, Xiong Yang, Liang Lin, Mun Wai
Lee, and Song-Chun Zhu. 2010. I2t: Image pars-
ing to text description. Proc. IEEE, 98(8).
368
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 790?796,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Generalizing Image Captions for Image-Text Parallel Corpus
Polina Kuznetsova, Vicente Ordonez, Alexander Berg,
Tamara Berg and Yejin Choi
Department of Computer Science
Stony Brook University
Stony Brook, NY 11794-4400
{pkuznetsova,vordonezroma,aberg,tlberg,ychoi}@cs.stonybrook.edu
Abstract
The ever growing amount of web images
and their associated texts offers new op-
portunities for integrative models bridging
natural language processing and computer
vision. However, the potential benefits of
such data are yet to be fully realized due
to the complexity and noise in the align-
ment between image content and text. We
address this challenge with contributions
in two folds: first, we introduce the new
task of image caption generalization, for-
mulated as visually-guided sentence com-
pression, and present an efficient algo-
rithm based on dynamic beam search with
dependency-based constraints. Second,
we release a new large-scale corpus with
1 million image-caption pairs achieving
tighter content alignment between images
and text. Evaluation results show the in-
trinsic quality of the generalized captions
and the extrinsic utility of the new image-
text parallel corpus with respect to a con-
crete application of image caption transfer.
1 Introduction
The vast number of online images with accom-
panying text raises hope for drawing synergistic
connections between human language technolo-
gies and computer vision. However, subtleties and
complexity in the relationship between image con-
tent and text make exploiting paired visual-textual
data an open and interesting problem.
Some recent work has approached the prob-
lem of composing natural language descriptions
for images by using computer vision to retrieve
images with similar content and then transferring
?A house being 
pulled by a boat.? 
?I saw her in the light 
of her reading lamp 
and sneaked back to 
her door with the 
camera.? 
?Sections of the 
bridge sitting in the 
Dyer Construction 
yard south of 
Cabelas Driver.? 
Circumstantial 
information that is not 
visually present 
Visually relevant, 
but with overly 
extraneous details 
Visually truthful, 
but for an uncommon 
situation 
Figure 1: Examples of captions that are not readily
applicable to other visually similar images.
text from the retrieved samples to the query im-
age (e.g. Farhadi et al (2010), Ordonez et al
(2011), Kuznetsova et al (2012)). Other work
(e.g. Feng and Lapata (2010a), Feng and Lapata
(2010b)) uses computer vision to bias summariza-
tion of text associated with images to produce de-
scriptions. All of these approaches rely on ex-
isting text that describes visual content, but many
times existing image descriptions contain signifi-
cant amounts of extraneous, non-visual, or other-
wise non-desirable content. The goal of this paper
is to develop techniques to automatically clean up
visually descriptive text to make it more directly
usable for applications exploiting the connection
between images and language.
As a concrete example, consider the first image
in Figure 1. This caption was written by the photo
owner and therefore contains information related
to the context of when and where the photo was
taken. Objects such as ?lamp?, ?door?, ?camera?
are not visually present in the photo. The second
image shows a similar but somewhat different is-
sue. Its caption describes visible objects such as
?bridge? and ?yard?, but ?Cabelas Driver? are
overly specific and not visually detectable. The
790
Dependency Constraints with Examples Additional Dependency ConstraintsConstraints Sentence Dependency
advcl*(?) Taken when it was running... taken?running acomp*(?), advmod(?), agent*(?), attr(?)
amod(?) A wooden chair in the living room chair? wooden auxpass(?), cc*(?),complm(?), cop*(?)
aux(?) This crazy dog was jumping... jumping?was csubj*/csubjpass*(?),expl(?), mark*(?)
ccomp*(?) I believe a bear was in the box... believe?was infmod*(?), mwe(?), nsubj*/nsubjpass*(?)
prep(?) A view from the balcony view?from npadvmod(?), nn(?), conj*(?), num*(?)
det(?) A cozy street cafe... cafe?A number(?), parataxis(?),?
dobj*(?) A curious cow surveys the road... surveys?road partmod*(?), pcomp*(?), purpcl*(?)
iobj*(?) ...rock gives the water the color gives?water possessive(?), preconj*(?), predet*(?)
neg(?) Not a cloud in the sky... cloud?Not prt(?), quantmod(?), rcmod(?), ref(?)
pobj*(?) This branch was on the ground... on?ground rel*(?), tmod*(?), xcomp*(?), xsubj(?)
Table 1: Dependency-based Constraints
text of the third image, ?A house being pulled by a
boat?, pertains directly to the visual content of the
image, but is unlikely to be useful for tasks such as
caption transfer because the depiction is unusual.1
This phenomenon of information gap between the
visual content of the images and their correspond-
ing narratives has been studied closely by Dodge
et al (2012).
The content misalignment between images and
text limits the extent to which visual detectors
can learn meaningful mappings between images
and text. To tackle this challenge, we introduce
the new task of image caption generalization that
rewrites captions to be more visually relevant and
more readily applicable to other visually similar
images. Our end goal is to convert noisy image-
text pairs in the wild (Ordonez et al, 2011) into
pairs with tighter content alignment, resulting in
new simplified captions over 1 million images.
Evaluation results show both the intrinsic quality
of the generalized captions and the extrinsic util-
ity of the new image-text parallel corpus. The new
parallel corpus will be made publicly available.2
2 Sentence Generalization as Constraint
Optimization
Casting the generalization task as visually-guided
sentence compression with lightweight revisions,
we formulate a constraint optimization problem
that aims to maximize content selection and lo-
cal linguistic fluency while satisfying constraints
driven from dependency parse trees. Dependency-
based constraints guide the generalized caption
1Open domain computer vision remains to be an open
problem, and it would be difficult to reliably distinguish pic-
tures of subtle visual differences, e.g., pictures of ?a water
front house with a docked boat? from those of ?a floating
house pulled by a boat?.
2Available at http://www.cs.stonybrook.edu/
?ychoi/imgcaption/
to be grammatically valid (e.g., keeping articles
in place, preventing dangling modifiers) while re-
maining semantically compatible with respect to a
given image-text pair (e.g., preserving predicate-
argument relations). More formally, we maximize
the following objective function:
F (y;x) = ?(y;x, v) + ?(y;x)
subject to C(y;x, v)
where x = {xi} is the input caption (a sentence),
v is the accompanying image, y = {yi} is the
output sentence, ?(y;x, v) is the content selection
score, ?(y;x) is the linguistic fluency score, and
C(y;x, v) is the set of hard constraints. Let l(yi)
be the index of the word in x that is selected as the
i?th word in the output y so that xl(yi) = yi. Then,
we factorize ?(?) and ?(?) as:
?(y;x, v) =
?
i
?(yi, x, v) =
?
i
?(xl(yi), v)
?(y;x) =
?
i
?(yi, ..., yi?K)
=
?
i
?(xl(yi), ..., xl(yi?K))
where K is the size of local context.
Content Selection ? Visual Estimates:
The computer vision system used consists of 7404
visual classifiers for recognizing leaf level Word-
Net synsets (Fellbaum, 1998). Each classifier is
trained using labeled images from the ImageNet
dataset (Deng et al, 2009) ? an image database
of over 14 million hand labeled images orga-
nized according to the WordNet hierarchy. Image
similarity is represented using a Spatial Pyramid
Match Kernel (SPM) (Lazebnik et al, 2006) with
Locality-constrained Linear Coding (Wang et al,
2010) on shape based SIFT features (Lowe, 2004).
791
  (a) (b)
0 1 2 3 4 5 6 7 80
200400
600800
# of s
enten
ces (
in tho
usan
ds)
0 1 2 3 40
400
800
1200
# of s
enten
ces (
in tho
usan
ds)
Figure 2: Number of sentences (y-axis) for each
average (x-axis in (a)) and maximum (x-axis in
(b)) number of words with future dependencies
Models are linear SVMs followed by a sigmoid to
produce probability for each node.3
Content Selection ? Salient Topics:
We consider Tf.Idf driven scores to favor salient
topics, as those are more likely to generalize
across many different images. Additionally, we
assign a very low content selection score (??) for
proper nouns and numbers and a very high score
(larger then maximum idf or visual score) for the
2k most frequent words in our corpus.
Local Linguistic Fluency:
We model linguistic fluency with 3-gram condi-
tional probabilities:
?(xl(yi), xl(yi?1), xl(yi?2)) (1)
= p(xl(yi)|xl(yi?2), xl(yi?1))
We experiment with two different ngram statis-
tics, one extracted from the Google Web 1T cor-
pus (Brants and Franz., 2006), and the other com-
puted from the 1M image-caption corpus (Or-
donez et al, 2011).
Dependency-driven Constraints:
Table 1 defines the list of dependencies used
as constraints driven from the typed dependen-
cies (de Marneffe and Manning, 2009; de Marn-
effe et al, 2006). The direction of arrows indi-
cate the direction of inclusion requirements. For
example, dep(X ?? Y ), denotes that ?X? must
be included whenever ?Y ? is included. Similarly,
dep(X ?? Y ) denotes that ?X? and ?Y ? must
either be included together or eliminated together.
We determine the uni- or bi-directionality of these
constraints by manually examining a few example
sentences corresponding to each of these typed de-
pendencies. Note that some dependencies such as
det(??) would hold regardless of the particular
3Code was provided by Deng et al (2012).
Method-1 (M1) v.s. Method-2 (M2) M1 winsover M2
SALIENCY ORIG 76.34%
VISUAL ORIG 81.75%
VISUAL SALIENCY 72.48%
VISUAL VISUAL W/O CONSTR 83.76%
VISUAL NGRAM-ONLY 90.20%
VISUAL HUMAN 19.00%
Table 2: Forced Choice Evaluation (LM Corpus =
Google)
lexical items, while others, e.g., dobj(??) may
or may not be necessary depending on the context.
Those dependencies that we determine as largely
context dependent are marked with * in Table 1.
One could consider enforcing all dependency
constraints in Table 1 as hard constraints so that
the compressed sentence must not violate any of
those directed dependency constraints. Doing so
would lead to overly conservative compression
with least compression ratio however. Therefore,
we relax those that are largely context dependent
as soft constraints (marked in Table 1 with *) by
introducing a constant penalty term in the objec-
tive function. Alternatively, the dependency based
constraints can be learned statistically from the
training corpus of paired original and compressed
sentences. Since we do not have such in-domain
training data at this time, we leave this exploration
as future research.
Dynamic Programming with Dynamic Beam:
The constraint optimization we formulated corre-
sponds to an NP-hard problem. In our work, hard
constraints are based only on typed dependencies,
and we find that long range dependencies occur in-
frequently in actual image descriptions, as plotted
in Figure 2. With this insight, we opt for decoding
based on dynamic programming with dynamically
adjusted beam.4 Alternatively, one can find an ap-
proximate solution using Integer Linear Program-
ming (e.g., Clarke and Lapata (2006), Clarke and
Lapata (2007), Martins and Smith (2009)).
3 Evaluation
Since there is no existing benchmark data for im-
age caption generalization, we crowdsource evalu-
ation using Amazon Mechanical Turk (AMT). We
empirically compare the following options:
4The required beam size at each step depends on how
many words have dependency constraints involving any word
following the current one ? beam size is at most 2p, where p
is the max number of words dependent on any future words.
792
Big elm tree over 
the house is no 
their anymore. 
? Tree over the house. 
Abandonned 
houses in the 
forest. 
? Houses in the 
     forest. 
A woman paints a tree in 
bloom near the duck pond 
in the Boston Public 
Garden, April 15, 2006. 
? A tree in bloom . 
Pillbox in field 
behind a pub 
car park. 
? Pub car. 
Flowering tree in 
mixed forest at 
Wakehurst. 
? Flowering tree  
    in forest. 
The insulbrick matches 
the yard. This is outside 
of medina ohio near the 
tonka truck house. 
? The yard. This is 
     outside the house. 
Query Image Retrieved Images 
Figure 3: Example Image Caption Transfer
Method LM strict matching semantic matchingCorpus BLEU P R F BLEU P R F
ORIG N/A 0.063 0.064 0.139 0.080 0.215 0.220 0.508 0.276
SALIENCY Image Corpus 0.060 0.074 0.077 0.068 0.302 0.411 0.399 0.356
VISUAL Image Corpus 0.060 0.075 0.075 0.068 0.305 0.422 0.397 0.360
SALIENCY Google Corpus 0.064 0.070 0.101 0.074 0.286 0.337 0.459 0.340
VISUAL Google Corpus 0.065 0.071 0.098 0.075 0.296 0.354 0.457 0.350
Table 3: Image Description Transfer: performance in BLEU and F1 with strict & semantic matching.
? ORIG: original uncompressed captions
? HUMAN: compressed by humans (See ? 3.2)
? SALIENCY: linguistic fluency + saliency-based
content selection + dependency constraints
? VISUAL: linguistic fluency + visually-guided
content selection + dependency constraints
? x W/O CONSTR: method xwithout dependency
constraints
? NGRAM-ONLY: linguistic fluency only
3.1 Intrinsic Evaluation: Forced Choice
Turkers are provided with an image and two cap-
tions (produced by different methods) and are
asked to select a better one, i.e., the most relevant
and plausible caption that contains the least extra-
neous information. Results are shown in Table 2.
We observe that VISUAL (full model with visually
guided content selection) performs the best, being
selected over SALIENCY (content-selection with-
out visual information) in 72.48% cases, and even
over the original image caption in 81.75% cases.
This forced-selection experiment between VI-
SUAL and ORIG demonstrates the degree of noise
prevalent in the image captions in the wild. Of
course, if compared against human-compressed
captions, the automatic captions are preferred
much less frequently ? in 19% of the cases. In
those 19% cases when automatic captions are pre-
ferred over human-compressed ones, it is some-
times that humans did not fully remove informa-
tion that is not visually present or verifiable, and
other times humans overly compressed. To ver-
ify the utility of dependency-based constraints,
we also compare two variations of VISUAL, with
and without dependency-based constraints. As ex-
pected, the algorithm with constraints is preferred
in the majority of cases.
3.2 Extrinsic Evaluation: Image-based
Caption Retrieval
We evaluate the usefulness of our new image-text
parallel corpus for automatic generation of image
descriptions. Here the task is to produce, for a
query image, a relevant description, i.e., a visu-
ally descriptive caption. Following Ordonez et al
(2011), we produce a caption for a query image
by finding top k most similar images within the
1M image-text corpus (Ordonez et al, 2011) and
then transferring their captions to the query im-
age. To compute evaluation measures, we take the
average scores of BLEU(1) and F-score (unigram-
based with respect to content-words) over k = 5
candidate captions.
Image similarity is computed using two global
(whole) image descriptors. The first is the GIST
feature (Oliva and Torralba, 2001), an image de-
scriptor related to perceptual characteristics of
scenes ? naturalness, roughness, openness, etc.
The second descriptor is also a global image de-
scriptor, computed by resizing the image into a
?tiny image? (Torralba et al, 2008), which is ef-
fective in matching the structure and overall color
of images. To find visually relevant images, we
compute the similarity of the query image to im-
793
Huge wall of glass 
at the Conference 
Centre in 
Yohohama  Japan.  
? Wall of glass  
My footprint in a 
sand box 
? A sand box  
James the cat is 
dreaming of running 
in a wide green 
valley 
? Running in 
a valley (not 
relevant) 
This little boy was so 
cute. He was flying his 
spiderman kite all by 
himself on top of Max 
Patch  
? This little boy was so 
cute. He was flying 
(semantically odd) 
A view of the post office 
building in Manila from 
the other side of the 
Pasig River  
? A view of the post 
office building from 
the side  
Cell phone shot of 
a hat stall in the 
Northeast Market, 
Baltimore, MD.  
? Cell phone shot.  
(visually not 
verifiable) 
Figure 4: Good (left three, in blue) and bad examples (right three, in red) of generalized captions
ages in the whole dataset using an unweighted sum
of gist similarity and tiny image similarity.
Gold standard (human compressed) captions are
obtained using AMT for 1K images. The results
are shown in Table 3. Strict matching gives credit
only to identical words between the gold-standard
caption and the automatically produced caption.
However, words in the original caption of the
query image (and its compressed caption) do not
overlap exactly with words in the retrieved cap-
tions, even when they are semantically very close,
which makes it hard to see improvements even
when the captions of the new corpus are more gen-
eral and transferable over other images. Therefore,
we also report scores based on semantic matching,
which gives partial credits to word pairs based on
their lexical similarity.5 The best performing ap-
proach with semantic matching is VISUAL (with
LM = Image corpus), improving BLEU, Precision,
F-score substantially over those of ORIG, demon-
strating the extrinsic utility of our newly gener-
ated image-text parallel corpus in comparison to
the original database. Figure 3 shows an example
of caption transfer.
4 Related Work
Several recent studies presented approaches to
automatic caption generation for images (e.g.,
Farhadi et al (2010), Feng and Lapata (2010a),
Feng and Lapata (2010b), Yang et al (2011),
Kulkarni et al (2011), Li et al (2011), Kuznetsova
et al (2012)). The end goal of our work differs in
that we aim to revise original image captions into
5We take Wu-Palmer Similarity as similarity mea-
sure (Wu and Palmer, 1994). When computing BLEU with
semantic matching, we look for the match with the highest
similarity score among words that have not been matched be-
fore. Any word matched once (even with a partial credit) will
be removed from consideration when matching next words.
descriptions that are more general and align more
closely to the visual image content.
In comparison to prior work on sentence com-
pression, our approach falls somewhere between
unsupervised to distant-supervised approach (e.g.,
Turner and Charniak (2005), Filippova and Strube
(2008)) in that there is not an in-domain train-
ing corpus to learn generalization patterns directly.
Future work includes exploring more direct su-
pervision from human edited sample generaliza-
tion (e.g., Knight and Marcu (2000), McDonald
(2006)) Galley and McKeown (2007), Zhu et al
(2010)), and the inclusion of edits beyond dele-
tion, e.g., substitutions, as has been explored by
e.g., Cohn and Lapata (2008), Cordeiro et al
(2009), Napoles et al (2011).
5 Conclusion
We have introduced the task of image caption gen-
eralization as a means to reduce noise in the paral-
lel corpus of images and text. Intrinsic and extrin-
sic evaluations confirm that the captions in the re-
sulting corpus align better with the image contents
(are often preferred over the original captions by
people), and can be practically more useful with
respect to a concrete application.
Acknowledgments
This research was supported in part by the Stony
Brook University Office of the Vice President for
Research. Additionally, Tamara Berg is supported
by NSF #1054133 and NSF #1161876. We thank
reviewers for many insightful comments and sug-
gestions.
794
References
Thorsten Brants and Alex Franz. 2006. Web 1t 5-gram
version 1. In Linguistic Data Consortium.
James Clarke and Mirella Lapata. 2006. Constraint-
based sentence compression: An integer program-
ming approach. In Proceedings of the COL-
ING/ACL 2006 Main Conference Poster Sessions,
pages 144?151, Sydney, Australia, July. Association
for Computational Linguistics.
James Clarke and Mirella Lapata. 2007. Modelling
compression with discourse constraints. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 1?11, Prague, Czech Republic, June.
Association for Computational Linguistics.
Trevor Cohn and Mirella Lapata. 2008. Sentence
compression beyond word deletion. In Proceedings
of the 22nd International Conference on Compu-
tational Linguistics (Coling 2008), pages 137?144,
Manchester, UK, August. Coling 2008 Organizing
Committee.
Joao Cordeiro, Gael Dias, and Pavel Brazdil. 2009.
Unsupervised induction of sentence compression
rules. In Proceedings of the 2009 Workshop
on Language Generation and Summarisation (UC-
NLG+Sum 2009), pages 15?22, Suntec, Singapore,
August. Association for Computational Linguistics.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2009. Stanford typed dependencies manual.
Marie-Catherine de Marneffe, Bill Maccartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses.
In Language Resources and Evaluation Conference
2006.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. 2009. ImageNet: A Large-Scale Hi-
erarchical Image Database. In Conference on Com-
puter Vision and Pattern Recognition.
Jia Deng, Jonathan Krause, Alexander C. Berg, and
L. Fei-Fei. 2012. Hedging your bets: Optimizing
accuracy-specificity trade-offs in large scale visual
recognition. In Conference on Computer Vision and
Pattern Recognition.
Jesse Dodge, Amit Goyal, Xufeng Han, Alyssa Men-
sch, Margaret Mitchell, Karl Stratos, Kota Yam-
aguchi, Yejin Choi, Hal Daume III, Alex Berg, and
Tamara Berg. 2012. Detecting visual text. In Pro-
ceedings of the 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
762?772, Montre?al, Canada, June. Association for
Computational Linguistics.
Ali Farhadi, Mohsen Hejrati, Mohammad Amin
Sadeghi, Peter Young1, Cyrus Rashtchian, Julia
Hockenmaier, and David Forsyth. 2010. Every pic-
ture tells a story: generating sentences for images.
In European Conference on Computer Vision.
Christiane D. Fellbaum, editor. 1998. WordNet: an
electronic lexical database. MIT Press.
Yansong Feng and Mirella Lapata. 2010a. How many
words is a picture worth? automatic caption genera-
tion for news images. In Association for Computa-
tional Linguistics.
Yansong Feng and Mirella Lapata. 2010b. Topic mod-
els for image annotation and text illustration. In Hu-
man Language Technologies.
Katja Filippova and Michael Strube. 2008. Depen-
dency tree based sentence compression. In Proceed-
ings of the Fifth International Natural Language
Generation Conference, INLG ?08, pages 25?32,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Michel Galley and Kathleen McKeown. 2007. Lex-
icalized Markov grammars for sentence compres-
sion. In Human Language Technologies 2007:
The Conference of the North American Chapter of
the Association for Computational Linguistics; Pro-
ceedings of the Main Conference, pages 180?187,
Rochester, New York, April. Association for Com-
putational Linguistics.
Kevin Knight and Daniel Marcu. 2000. Statistics-
based summarization - step one: Sentence compres-
sion. In AAAI/IAAI, pages 703?710.
Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Sim-
ing Li, Yejin Choi, Alexander C Berg, and Tamara L
Berg. 2011. Babytalk: Understanding and gener-
ating simple image descriptions. In Conference on
Computer Vision and Pattern Recognition.
Polina Kuznetsova, Vicente Ordonez, Alexander Berg,
Tamara Berg, and Yejin Choi. 2012. Collective gen-
eration of natural image descriptions. In Proceed-
ings of the 50th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 359?368, Jeju Island, Korea, July. As-
sociation for Computational Linguistics.
Svetlana Lazebnik, Cordelia Schmid, and Jean Ponce.
2006. Beyond bags of features: Spatial pyramid
matching. In Conference on Computer Vision and
Pattern Recognition, June.
Siming Li, Girish Kulkarni, Tamara L. Berg, Alexan-
der C. Berg, and Yejin Choi. 2011. Composing
simple image descriptions using web-scale n-grams.
In Proceedings of the Fifteenth Conference on Com-
putational Natural Language Learning, pages 220?
228, Portland, Oregon, USA, June. Association for
Computational Linguistics.
David G. Lowe. 2004. Distinctive image features from
scale-invariant keypoints. Int. J. Comput. Vision,
60:91?110, November.
795
Andre Martins and Noah A. Smith. 2009. Summariza-
tion with a joint model for sentence extraction and
compression. In Proceedings of the Workshop on
Integer Linear Programming for Natural Language
Processing, pages 1?9, Boulder, Colorado, June. As-
sociation for Computational Linguistics.
Ryan T. McDonald. 2006. Discriminative sentence
compression with soft syntactic evidence. In EACL
2006, 11st Conference of the European Chapter of
the Association for Computational Linguistics, Pro-
ceedings of the Conference, April 3-7, 2006, Trento,
Italy. The Association for Computer Linguistics.
Courtney Napoles, Chris Callison-Burch, Juri Ganitke-
vitch, and Benjamin Van Durme. 2011. Paraphras-
tic sentence compression with a character-based
metric: Tightening without deletion. In Proceed-
ings of the Workshop on Monolingual Text-To-Text
Generation, pages 84?90, Portland, Oregon, June.
Association for Computational Linguistics.
Aude Oliva and Antonio Torralba. 2001. Modeling the
shape of the scene: a holistic representation of the
spatial envelope. International Journal of Computer
Vision.
Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg.
2011. Im2text: Describing images using 1 million
captioned photographs. In Neural Information Pro-
cessing Systems (NIPS).
Antonio Torralba, Rob Fergus, and William T. Free-
man. 2008. 80 million tiny images: a large dataset
for non-parametric object and scene recognition.
Pattern Analysis and Machine Intelligence, 30.
Jenine Turner and Eugene Charniak. 2005. Super-
vised and unsupervised learning for sentence com-
pression. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguis-
tics (ACL?05), pages 290?297, Ann Arbor, Michi-
gan, June. Association for Computational Linguis-
tics.
Jinjun Wang, Jianchao Yang, Kai Yu, Fengjun Lv,
T. Huang, and Yihong Gong. 2010. Locality-
constrained linear coding for image classification.
In Conference on Computer Vision and Pattern
Recognition (CVPR).
Zhibiao Wu and Martha Palmer. 1994. Verbs seman-
tics and lexical selection. In Proceedings of the 32nd
annual meeting on Association for Computational
Linguistics, ACL ?94, pages 133?138, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Yezhou Yang, Ching Teo, Hal Daume III, and Yiannis
Aloimonos. 2011. Corpus-guided sentence genera-
tion of natural images. In Proceedings of the 2011
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 444?454, Edinburgh, Scot-
land, UK., July. Association for Computational Lin-
guistics.
Zhemin Zhu, Delphine Bernhard, and Iryna Gurevych.
2010. A monolingual tree-based translation model
for sentence simplification. In Proceedings of the
23rd International Conference on Computational
Linguistics (Coling 2010), pages 1353?1361, Bei-
jing, China, August. Coling 2010 Organizing Com-
mittee.
796
