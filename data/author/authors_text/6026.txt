Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 947?954,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Automatic Set Expansion for List Question Answering
Richard C. Wang, Nico Schlaefer, William W. Cohen, and Eric Nyberg
Language Technologies Institute
Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh PA 15213
{rcwang,nico,wcohen,ehn}@cs.cmu.edu
Abstract
This paper explores the use of set expan-
sion (SE) to improve question answering (QA)
when the expected answer is a list of entities
belonging to a certain class. Given a small
set of seeds, SE algorithms mine textual re-
sources to produce an extended list including
additional members of the class represented
by the seeds. We explore the hypothesis that
a noise-resistant SE algorithm can be used to
extend candidate answers produced by a QA
system and generate a new list of answers that
is better than the original list produced by the
QA system. We further introduce a hybrid ap-
proach which combines the original answers
from the QA system with the output from the
SE algorithm. Experimental results for several
state-of-the-art QA systems show that the hy-
brid system performs better than the QA sys-
tems alone when tested on list question data
from past TREC evaluations.
1 Introduction
Question answering (QA) systems are designed to
retrieve precise answers to questions posed in nat-
ural language. A list question expects a list as its
answer, e.g. Name the coffee-producing countries in
South America. The ability to answer list questions
has been tested as part of the yearly TREC QA eval-
uation (Dang et al, 2006; Dang et al, 2007). This
paper focuses on the use of set expansion to improve
list question answering. A set expansion (SE) algo-
rithm receives as input a few members of a class or
set, and mines various textual resources (e.g. web
pages) to produce an extended list including addi-
tional members of the class or set that are not in the
input. A well-known online SE system is Google
Sets1. This system is publicly accessible, but since it
is a proprietary system that might be changed at any
time, its results cannot be replicated reliably. We ex-
plore the hypothesis that a SE algorithm, when care-
fully designed to handle noisy inputs, can be applied
to the output from a QA system to produce an overall
list of answers for a given question that is better than
the answers produced by the QA system itself. We
propose to exploit large, redundant sources of struc-
tured and/or semi-structured data and use linguistic
analysis to seed a shallow analysis of these sources.
This is a hard problem since the linguistic evidence
used as seeds is noisy. More precisely, we combine
the QA system Ephyra (Schlaefer et al, 2007) with
the SE system SEAL (Wang and Cohen, 2007) to
create a hybrid approach that performs better than
either system by itself when tested on data from the
TREC 13-15 evaluations. In addition, we apply our
SE algorithm to answers generated by the five QA
systems that performed the best on the list questions
in the TREC 15 evaluation and report improvements
in F1 scores for four of these systems.
Section 2 of the paper gives an overview of the
QA and SE systems used for our experiments. Sec-
tion 3 describes how the SE system was adapted to
deal with noisy seeds produced by QA systems, and
Section 4 presents the details of the experimental de-
sign. Experimental results are discussed in Section
5, and the paper concludes in Section 6 with a dis-
cussion of planned future work.
1http://labs.google.com/sets
947
2 System Overview
2.1 Ephyra Question Answering System
Ephyra (Schlaefer et al, 2006; Schlaefer et al,
2007) is a QA system that has been evaluated in
the TREC QA track (Dang et al, 2006; Dang et al,
2007). The system combines three answer extrac-
tion techniques for factoid and list questions: (1) an
answer type classification approach; (2) a syntactic
pattern learning and matching approach; and (3) a
semantic extractor that uses a semantic role label-
ing system. The answer type based extractor clas-
sifies questions by their answer types and extracts
candidates of the expected types. The Ephyra pat-
tern matching approach learns textual patterns that
relate question key terms to possible answers and
applies these patterns to candidate sentences to ex-
tract factoid answers. The semantic approach gener-
ates a semantic representation of the question that is
based on predicate-argument structures and extracts
answer candidates from similar structures in the cor-
pus. The source code of the answer extractors is in-
cluded in OpenEphyra, an open source release of the
system.2
The answer candidates from these extractors are
combined and ranked by a statistical answer selec-
tion framework (Ko et al, 2007), which estimates
the probability of an answer based on a number of
answer validation and similarity features. Valida-
tion features use resources such as gazetteers and
Wikipedia to verify an answer, whereas similarity
features measure the syntactic and semantic simi-
larity to other candidates, e.g. using string distance
measures and WordNet relations.
2.2 Set Expander for Any Language (SEAL)
Set expansion (SE) refers to expanding a given par-
tial set of objects into a more complete set. SEAL3
(Wang and Cohen, 2007) is a SE system which ac-
cepts input elements (seeds) of some target set St
and automatically finds other probable elements of
St in semi-structured documents such as web pages.
SEAL also works on unstructured text, but its ex-
traction mechanism benefits from structuring ele-
ments such as HTML tags. The algorithm is in-
dependent of the human language from which the
2http://www.ephyra.info/
3http://rcwang.com/seal
Figure 1: Examples of SEAL?s input and output. English
entities are reality TV shows, Chinese entities are popular
Taiwanese food, and Japanese entities are famous cartoon
characters.
Figure 2: An example graph constructed by SEAL. Every
edge from node x to y actually has an inverse relation
edge from node y to x that is not shown here (e.g. m1 is
extracted by w1).
seeds are taken, and also independent of the markup
language used to annotate the documents. Examples
of SEAL?s input and output are shown in Figure 1.
In more detail, SEAL comprises three major com-
ponents: the Fetcher, the Extractor, and the Ranker.
The Fetcher focuses on retrieving web pages. The
URLs of the web pages come from top results re-
trieved from Google and Yahoo! using the concate-
nation of all seeds as the query. The Extractor au-
tomatically constructs page-specific extraction rules,
or wrappers, for each page that contains the seeds.
Every wrapper is defined by two character strings,
which specify the left-context and right-context nec-
essary for an entity to be extracted from a page.
These strings are chosen to be maximally-long con-
texts that bracket at least one occurrence of every
seed string on a page. Most of the wrappers con-
948
tain HTML tags, which illustrates the importance
of structuring information in the source documents.
All entity mentions bracketed by these contextual
strings derived from a particular page are extracted
from the same page. Finally, the Ranker builds a
graph, and then ranks the extracted mentions glob-
ally based on the weights computed by performing a
random graph walk.
An example graph is shown in Figure 2, where
each node di represents a document, wi a wrapper,
and mi an extracted entity mention. The graph mod-
els the relationship between documents, wrappers,
and mentions. In order to measure the relative im-
portance of each node within the graph, the Ranker
performs a graph walk until all node weights con-
verge. The idea is that nodes are weighted higher
if they are connected to many other highly weighted
nodes.
We apply this SE algorithm to answer candidates
for list questions generated by Ephyra and other
TREC QA systems to find additional instances of
correct answers that were not in the original candi-
date set.
3 Proposed Approach
SEAL was originally designed to handle only rele-
vant input seeds. When provided with a mixture of
relevant and irrelevant answers from a QA system,
the performance would suffer. In this section, we
propose three modifications to SEAL to improve its
ability to handle noisy input seeds.
3.1 Aggressive Fetcher
For each expansion, SEAL?s fetcher concatenates all
seeds and sends them as one query to the search
engines. However, when the seeds are noisy, the
documents fetched are constrained by the irrele-
vant seeds, which decreases the chance of finding
good documents. To overcome this problem, we
designed an aggressive fetcher (AF) that increases
the chance of composing queries containing only
relevant seeds. It sends a two-seed query for ev-
ery possible pair of seeds to the search engines. If
there are n input seeds, then the total number of
queries sent would be (n2
). For example, suppose
SEAL is given a set of noisy seeds: Boston, Seattle
and Carnegie-Mellon (assuming Carnegie-Mellon is
irrelevant), then by using AF, one query will contain
only relevant seeds (as shown in Table 1). The docu-
ments are then collected and sent to SEAL?s extrac-
tor for learning wrappers.
Queries Quality
-AF #1: Boston Seattle Carnegie-Mellon Low
+AF
#1: Boston Seattle High
#2: Boston Carnegie-Mellon Low
#3: Seattle Carnegie-Mellon Low
Table 1: Example queries and their quality given
the seeds Boston, Seattle and Carnegie-Mellon, where
Carnegie-Mellon is assumed to be irrelevant.
3.2 Lenient Extractor
SEAL?s extractor requires the longest common
contexts to bracket at least one instance of every
seed per web page. However, when seeds are noisy,
such common contexts usually do not exist or
are too short to be useful. To solve this problem,
we propose a lenient extractor (LE) which only
requires the contexts to bracket at least one in-
stance of a minimum of two seeds, instead of every
seed. This increases the chance of finding longest
common contexts that bracket only relevant seeds.
For instance, suppose SEAL is given the seeds
from the previous example (Boston, Seattle and
Carnegie-Mellon) and the passage below. Then the
extractor would learn the wrappers shown in Table 2.
?While attending a hearing in Boston City
Hall, Alan, a professor at Boston University,
met Tina, his former student at Seattle Univer-
sity, who is studying at Carnegie-Mellon University
Art School and will be working in Seattle City Hall.?
Learned Wrappers
-LE #1: at [...] University
+LE #1: at [...] University#2: in [...] City Hall
Table 2: Wrappers learned by SEAL?s extractor when
given the passage in Section 3.2 and the seeds Boston,
Seattle and Carnegie-Mellon.
949
As illustrated, with lenient extraction, SEAL is
now able to learn the second wrapper because it
brackets one instance of at least two seeds (Boston
and Seattle). This can be very helpful if the list
question is asking for city names rather than univer-
sity names. The extractor then uses these wrappers
to extract additional answer candidates, by search-
ing for other strings that fit into the placeholders of
the wrappers. Note that the example was simplified
for ease of presentation. The wrappers are actually
character-based (as opposed to word-based) and are
likely to contain HTML tags when generated from
real web pages.
3.3 Hinted Expander
Most QA systems use keywords from the question to
guide the retrieval of relevant documents and the ex-
traction of answer candidates. We believe these key-
words are also important for SEAL to identify ad-
ditional instances of correct answers. For example,
if the seeds are George Washington, John Adams,
and Thomas Jefferson, then without using any con-
text from the question, SEAL would output a mix-
ture of founding fathers and presidents of the U.S.A.
To solve this problem, we devised a hinted expan-
sion (HE) technique that utilizes the context given
in the question to constrain SEAL?s search space on
the Web. This is achieved by appending keywords
from the question to every query that is sent to the
search engines. The rationale is that the retrieved
documents will also match the keywords, which may
increase the chance of finding those documents that
contain our desired set of answers.
4 Experimental Design
We conducted experiments in two phases. In the
first phase, we evaluated the SE approach by apply-
ing SEAL to answers generated by Ephyra. In the
second phase, we evaluated the approach by apply-
ing SEAL to the output from QA systems that per-
formed the best on the list questions in the TREC 15
evaluation. In both phases, the answers found by
SEAL were retrieved from the Web instead of the
AQUAINT newswire corpus used in the TREC eval-
uations. However, we rejected answers if they could
only be found in the Web and not in the AQUAINT
corpus to avoid an unfair advantage over the QA
systems: TREC participants were allowed to extract
candidates from the Web (or any other source), but
they had to identify a supporting document in the
AQUAINT corpus for each answer and thus could
not return answers that were not covered by the cor-
pus.
Preliminary experiments showed that we can ob-
tain a good balance between the amount and quality
of the documents fetched by using only rare ques-
tion terms as hint words. In particular, we select the
three question words that occur least frequently in a
sample of the AQUAINT corpus as hints. The can-
didate answers were evaluated by using the answer
keys, composed of regular expression patterns, ob-
tained from the TREC website. We did not extend
the patterns with additional correct answers found in
our experiments. These answer keys were not offi-
cially used in the TREC evaluation; thus the baseline
scores we computed for Ephyra and other QA sys-
tems in our experiments are slightly different from
those officially reported.
4.1 Ephyra
We evaluated our SE approach on Ephyra using the
list questions from TREC 13, 14, and 15 (55, 93, and
89 questions, respectively). For each question, the
top four answer candidates from Ephyra were given
as input seeds to SEAL. Initial experiments showed
that by adding additional seeds, the effectiveness of
our approach can be improved at the expense of a
longer runtime.
We report both mean average precision (MAP)
and F1 scores. For the F1 scores, we drop answer
candidates with low confidence scores by applying
a relative cut-off threshold: an answer candidate is
dropped if the ratio of its confidence score and the
score of the top answer is below a threshold. An
optimal threshold for a question is a threshold that
maximizes the F1 score for that particular question.
For each TREC dataset, we conducted three ex-
periments: (1) evaluation of answer candidates us-
ing MAP; (2) evaluation using average F1 with an
optimal threshold for each question; and (3) eval-
uation using average F1 with thresholds trained by
5-fold cross validation. For each of those 5-fold val-
idations, only one threshold was determined for all
questions in the training folds.
950
Ephyra Ephyra?s SEAL SEAL+LE SEAL+LE SEAL+LE
Top 4 Ans. +AF +AF+HE
TREC 13 25.95% 21.39% 23.76% 31.43% 34.22% 35.26%
TREC 14 14.45% 8.71% 14.47% 17.04% 16.58% 18.82%
TREC 15 13.42% 9.02% 13.17% 16.87% 17.12% 18.95%
Table 3: Mean average precision of Ephyra, its top four answers, and various SEAL configurations, where LE is
Lenient Extractor, AF is Aggressive Fetcher, and HE is Hinted Expander.
Ephyra Ephyra?s SEAL SEAL+LE SEAL+LE SEAL+LE
Top 4 Ans. +AF +AF+HE
TREC 13 35.74% 26.29% 30.53% 36.47% 40.08% 40.80%
TREC 14 22.83% 14.05% 20.62% 22.81% 22.66% 24.88%
TREC 15 22.42% 14.57% 19.88% 23.30% 24.04% 25.65%
Table 4: Average F1 of Ephyra, its top four answers, and various SEAL configurations when using an optimal threshold
for each question.
4.2 Top QA Systems
We evaluated two SE approaches, SEAL and Google
Sets, on the five QA systems that performed the best
on the list questions in TREC 15. For each question,
the top four answer candidates4 from those systems
were given as input seeds to SEAL and Google Sets.
Unlike the candidates found by Ephyra, these can-
didates were provided without confidence scores;
hence, we assumed they all have a score of 1.0. In
our experiments with SEAL, we first determined a
single threshold that optimizes the average of the F1
scores of the top five systems in both TREC 13 and
14. We then obtained evaluation results for the top
systems in TREC 15 by using this trained threshold.
When performing hinted expansion, the keywords
(or hint words) for each question were extracted by
Ephyra?s question analysis component. In our exper-
iments with Google Sets, we requested Small Sets of
items and again measured the performance in terms
of F1 scores. We also tried requesting Large Sets but
the results were worse.
5 Results and Discussion
In Tables 3 and 4, we present evaluation results for
all answers from Ephyra, only the top four answers,
and various configurations of SEAL using the top
four answers as seeds. Table 3 shows the MAP for
4Obtained from http://trec.nist.gov/results
each dataset (TREC 13, 14, and 15), and Table 4
shows for each dataset the average F1 score when
using optimal per-question thresholds. The results
indicate that SEAL achieves the best performance
when configured with all three proposed extensions.
In terms of MAP, the best-configured SEAL im-
proves the quality of the input answers (relatively)
by 65%, 116%, 110% for each dataset respectively,
and improves Ephyra?s overall performance by 36%,
30%, 41%. In terms of optimal F1, SEAL improves
the quality of the input answers by 55%, 77%, 76%
and Ephyra?s overall performance by 14%, 9%, 14%
respectively. These results illustrate that a SE sys-
tem is capable of improving a QA system?s perfor-
mance on list questions, if we know how to select
good thresholds.
In practice, the thresholds are unknown and must
be estimated from a training set. Table 5 shows eval-
uation results using 5-fold cross validation for each
dataset (TREC 13, 14, and 15) independently, and
the combination of all three datasets (All). For each
validation, we determine the threshold that maxi-
mizes the F1 score on the training folds, and we
also determine the F1 score on the test fold by ap-
plying the trained threshold. We repeat this valida-
tion for each of the five test folds and present the av-
erage threshold and F1 score for each configuration
and dataset. The F1 scores give an estimate of the
performance on unseen data and allow a fair com-
951
Ephyra SEAL+LE+AF+HE Hybrid
Avg. F1 Avg. Threshold Avg. F1 Avg. Threshold Avg. F1 Avg. Threshold
TREC 13 25.55% 0.3808 30.71% 0.3257 29.04% 0.0796
TREC 14 15.78% 0.2636 15.60% 0.1889 17.13% 0.0108
TREC 15 15.19% 0.1192 15.64% 0.2581 16.47% 0.0123
All 18.03% 0.2883 19.15% 0.2606 19.59% 0.0164
Table 5: Average F1 of Ephyra, the best-configured SEAL, and the hybrid system, along with thresholds trained by
5-fold cross validation.
TREC 15 Baseline Top 4 Ans. Google Sets SEAL+LE+AF+HE Hybrid
QA Systems Avg. F1 Avg. F1 Avg. F1 ?F1 Avg. F1 ?F1 Avg. F1 ?F1
lccPA06 44.96% 32.67% 37.89% -15.72% 40.00% -11.04% 45.30% 0.76%
cuhkqaepisto 18.27% 17.02% 15.96% -12.68% 19.75% 8.08% 19.13% 4.70%
NUSCHUAQA1 18.40% 14.99% 16.70% -9.21% 18.74% 1.86% 18.06% -1.81%
FDUQAT15A 19.71% 14.32% 18.79% -4.63% 19.78% 0.38% 20.61% 4.57%
QACTIS06C 17.52% 15.22% 17.05% -2.72% 18.45% 5.26% 18.38% 4.85%
Average 23.77% 18.84% 21.28% -10.49% 23.34% -1.81% 24.30% 2.20%
Table 6: Average F1 of the QA systems, their top four answers, Google Sets, the best-configured SEAL, the hybrid
system, and their relative improvements over the QA systems.
parison across systems. Here, we also introduce a
hybrid system (Hybrid) that intersects the answers
found by both systems by multiplying their proba-
bilistic scores.
Tables 3, 4, and 5 show that the effectiveness of
the SE approach depends on the quality of the initial
answer candidates. The improvements are most ap-
parent for the TREC 13 dataset, where Ephyra has
a much higher performance compared to TREC 14
and 15. However, the best-configured SEAL did not
improve the F1 score on TREC 14, as reported in
Table 5. We suspect that this is due to the compar-
atively low quality of Ephyra?s top four answers for
this dataset. The experiments also illustrate that by
intersecting the answer candidates found by Ephyra
and SEAL, we can eliminate poor answer candi-
dates and partially compensate for the low preci-
sion of Ephyra on the harder TREC datasets. How-
ever, this comes at the expense of a lower recall,
which slightly hurts the performance on the compar-
atively easier TREC 13 questions. We also evaluated
Google Sets on top four answers from Ephyra for
TREC 13-15 and obtained F1 scores of 12%, 11%,
and 9% respectively (compared to 29%, 17%, and
16% for our hybrid approach with trained thresh-
olds).
Table 6 shows F1 scores for the SE approach
applied to the output from the five QA systems
with the highest performance on the list questions
in TREC 15. Again, Hybrid intersects the answers
found by the QA system and SEAL by multiplying
their confidence scores. Two thresholds were trained
separately on the top five systems in both TREC 13
and 14; one for SEAL (0.2376) and another for Hy-
brid (0.2463). As shown, the performance of Google
Sets is worse than SEAL and Hybrid, but better than
the top four answers on average. We believe our SE
system outperforms Google Sets because we have
methods to handle noisy inputs (i.e. AF, LE) and a
method for guiding the SE algorithm to search in the
right space on the Web (i.e. HE).
The results show that both SEAL and Hybrid are
capable of improving four out of the five systems.
We observed that one reason why SEAL did not im-
prove ?lccPA06? was the incompleteness of the an-
swer keys. Table 7 shows one of many examples
where SEAL was penalized for finding additional
correct answers. As illustrated, Hybrid improved
all systems except ?NUSCHUAQA1?. The reason
is that even though SEAL improved the baseline,
their overlapping answer set is too small; thus hurt-
ing the recall of Hybrid substantially. Unfortunately,
952
Question 154.6: Name titles of movies, other than ?Superman? movies, that
Christopher Reeve acted in.
lccPA06 (F1: 75%) SEAL+LE+AF+HE (F1: 40%)
+Rear Window +Rear Window
+The Remains of the Day +The Remains of the Day
+Snakes and Ladders -The Bostonians
-Superman -Somewhere in Time
-Village of the Damned
-In the Gloaming
Table 7: Example of SEAL being penalized for finding correct answers (all are correct except the last one). Answers
found in the answer keys are marked with ?+?. All four answers from ?lccPA06? were used as seeds.
Question 170.6: What are the titles of songs written by John Prine?
NUSCHUAQA1 (F1: 25%) SEAL+LE+AF+HE (F1: 44%)
+I Just Want to Dance With You +I Just Want to Dance With You
-Titled In Spite of Ourselves +Christmas in Prison
+Christmas in Prison +Sam Stone
-Grammy - Winning -Grandpa was a Carpenter
-Sabu Visits the Twin Cities Alone
+Angel from Montgomery
Table 8: Example demonstrating SEAL?s ability to handle noisy input seeds. All four answers from ?NUSCHUAQA1?
were used as seeds. Again, SEAL is penalized for finding correct answers (all answers are correct).
for the top TREC 15 systems we only had access to
the answers that were actually submitted by the par-
ticipants, whereas for Ephyra we could utilize the
entire list of generated answer candidates, includ-
ing those that fell below the cutoff threshold for list
questions. Nevertheless, the hybrid approach could
improve the baseline by more than 2% on average
in terms of F1 score. Table 8 shows that the best-
configured SEAL is capable of expanding only the
relevant seeds even when given a set of noisy seeds.
Neither Google Sets nor the original SE algorithm
without the proposed extensions could expand these
seeds with additional candidates.
On average, SEAL required about 5 seconds for
querying the search engines, 10 seconds for crawl-
ing the Web, 20 seconds for extracting answer can-
didates from the web pages, and 5 seconds for rank-
ing the candidates. Note that the SE system has not
been optimized extensively. The runtime of the web
page retrieval step and much of the search is due to
network latency and can be reduced if the search is
performed locally.
6 Conclusion and Future Work
We have shown that our SE approach is capable of
improving the performance of QA systems on list
questions by utilizing only their top four answer can-
didates as seeds. We have also illustrated a feasible
and effective method for integrating a SE approach
into any QA system. We would like to emphasize
that for each of the experiments we conducted, all
that the SE system received as input were the top
four noisy answers from a QA system and three key-
words from the TREC questions. We have shown
that higher quality candidates support more effec-
tive set expansion. In the future, we will investigate
how to utilize more answer candidates from the QA
system and determine the minimal quality of those
candidates required for SE approach to make an im-
provement.
We have also shown that, in terms of F1 scores
with trained thresholds, the hybrid method improves
the Ephyra QA system on all datasets and also im-
proves four out of the five systems that performed
953
the best on the list questions in TREC 15. How-
ever, the final list of answers only comprises candi-
dates found by both the QA system and the SE al-
gorithm. In future experiments, we will investigate
other methods of merging answer candidates, such
as taking the union of answers from both systems.
We expect further improvements from adding can-
didates that are found only by the QA system, but
it is unclear how the confidence measures from the
two systems can be combined effectively.
We would also like to emphasize that the SE ap-
proach is entirely language independent, and thus
can be readily applied to answer candidates in other
languages. In future experiments, we will investi-
gate its performance on question answering tasks in
languages such as Chinese and Japanese.
As pointed out previously, the performance of the
SE approach highly depends on the accuracy of the
seeds. However, QA systems are usually not op-
timized to provide few high-precision results, but
treat precision and recall as equally important. This
leaves room for further improvements, e.g. by ap-
plying stricter answer validation techniques to the
seeds used for SE.
We also plan to analyze the effectiveness of our
approach across different question types and evalu-
ate it on more complex questions such as the rigid
list questions in the new TAC QA evaluation, which
ask for opinion holders and subjects.
Acknowledgements
This work was supported in part by the Google Re-
search Awards program, IBM Open Collaboration
Agreement #W0652159, and the Defense Advanced
Research Projects Agency (DARPA) under Contract
No. NBCHD030010.
References
H.T. Dang, J. Lin, and D. Kelly. 2006. Overview of the
TREC 2006 question answering track. Proceedings of
the Fifteenth Text REtrieval Conference.
H.T. Dang, D. Kelly, and J. Lin. 2007. Overview of the
TREC 2007 question answering track. Proceedings of
the Sixteenth Text REtrieval Conference.
J. Ko, L. Si, and E. Nyberg. 2007. A probabilistic frame-
work for answer selection in question answering. Pro-
ceedings of NAACL-HLT.
N. Schlaefer, P. Gieselmann, and G. Sautter. 2006. The
Ephyra QA system at TREC 2006. Proceedings of the
Fifteenth Text REtrieval Conference.
N. Schlaefer, G. Sautter, J. Ko, J. Betteridge, M. Pathak,
and E. Nyberg. 2007. Semantic extensions of the
Ephyra QA system in TREC 2007. To appear in: Pro-
ceedings of the Sixteenth Text REtrieval Conference.
R.C. Wang and W.W. Cohen. 2007. Language-
independent set expansion of named entities using the
web. Proceedings of IEEE International Conference
on Data Mining.
954
Integrated Information Management: An Interactive,
Extensible Architecture for Information Retrieval
Eric Nyberg
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213
ehn@cs.cmu.edu
Hal Daume
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213
hcd@cs.cmu.edu
1. INTRODUCTION
Most current IR research is focused on specific technologies,
such as filtering, classification, entity extraction, question answer-
ing, etc. There is relatively little research on merging multiple tech-
nologies into sophisticated applications, due in part to the high cost
of integrating independently-developed text processing modules.
In this paper, we present the Integrated Information Management
(IIM) architecture for component-based development of IR appli-
cations1. The IIM architecture is general enough to model different
types of IR tasks, beyond indexing and retrieval. Rather than pro-
viding a single framework or toolkit, our goal is to create a higher-
level framework which is used to build a variety of different class
libraries or toolkits for different problems. Another goal is to pro-
mote the educational use of IR software, from an ?exploratory pro-
gramming? perspective. For this reason, it is also important to pro-
vide a graphical interface for effective task visualization and real-
time control.
Prior architecture-related work has focused on toolkits or class
libraries for specific types of IR or NLP problems. Examples in-
clude the SMART system for indexing and retrieval [17], the FIRE
[18] and InfoGrid [15] class models for information retrieval ap-
plications, and the ATTICS [11] system for text categorization and
machine learning. Some prior work has also focused on the user
interface, notably FireWorks [9] and SketchTrieve [9]2. Other sys-
tems such as GATE [4] and Corelli [20] have centered on specific
approaches to NLP applications.
The Tipster II architecture working group summarized the re-
quirements for an ideal IR architecture [6], which include:
  Standardization. Specify a standard set of functions and in-
terfaces for information services.
  Rapid Deployment. Speed up the initial development of new
applications.

This work is supported by National Science Foundation (KDI)
grant number 9873009.

For further discussion on how these systems compare with the
present work, see Section 7.
.
  Maintainability. Use standardized modules to support plug-
and-play updates.
  Flexibility. Enhance performance by allowing novel combi-
nations of existing components.
  Evaluation. Isolate and test specific modules side-by-side in
the same application.
One of the visions of the Tipster II team was a ?marketplace of
modules?, supporting mix-and-match of components developed at
different locations. The goals of rapid deployment and flexibility
require an excellent user interface, with support for drag-and-drop
task modeling, real-time task visualization and control, and uni-
form component instrumentation for cross-evaluation. The mod-
ules themselves should be small, downloadable files which run on
a variety of hardware and software platforms. This vision is in
fact a specialized form of component-based software engineering
(CBSE) [14], where the re-use environment includes libraries of
reusable IR components, and the integration process includes real-
time configuration, control, and tuning.
Section 2 summarizes the architectural design of IIM. Section
3 provides more detail regarding the system?s current implementa-
tion in Java. In Section 5 we describe three different task libraries
that have been constructed using IIM?s generic modules. Current
instrumentation, measurement, and results are presented in Section
6. We conclude in Section 7 with some relevant comparisons of
IIM to related prior work.
2. ARCHITECTURAL DESIGN
IIM uses a flow-based (pipe and filter [16]) processing model.
Information processing steps are represented as nodes in a graph.
Each edge in the graph represents a flow connection between a par-
ent node and a child node; the documents produced by the parent
node are passed to each child node. In IIM, the flow graph is re-
ferred to as a node chain. A sample node chain is shown in Figure
1. The IIM class model includes six basic node types, which can
be used to model a variety of IR problems:
1. Source. Generates a document stream (from a static collec-
tion, web search, etc.) and passes documents one at a time to
its child node(s).
2. Filter. Passes only documents which match the filter to its
child node(s).
3. Annotator. Adds additional information to the document re-
garding a particular region in the document body.
Figure 1: IIM User Interface
4. Sink. Creates and passes either a single document or a col-
lection to its child node(s), after pooling the input documents
it receives.
5. Transformer. Creates and passes on a single new document,
presumably the result of processing its input document.
6. Renderer. Produces output for documents received (to disk,
to screen, etc.).
The IIM class model is embedded in a Model-View-Controller
architecture [5], which allows the system to be run with or with-
out the graphical interface. Pre-stored node chains can be executed
directly from the shell, or as a background process, completely by-
passing all user interaction when optimal performance is required.
The Controller subsystem and interface event dispatching subsys-
tem must run as separate threads to support dynamic update of pa-
rameters in a running system. The View (user interface) should
support: a) plug-and-play creation of new node chains; b) support
for saving, loading and importing new node chains; c) dynamic vi-
sualization of a task?s status; and d) direct manipulation of a node?s
parameters at any time.
In addition to the nodes themselves, IIM supports two other im-
portant abstractions for IR task flows:
  Macro Nodes. Certain sequences of nodes are useful in more
than one application, so it is convenient to store them to-
gether as a single reusable unit, or macro node. IIM allows
the user to export a portion of a node chain as a macro node
to be loaded into the Node Library and inserted into a new
chain as a single node. The user may specify which of the
properties of the original nodes are visible in the exported
macro node (see Figure 3).
  Controllers. Some IR tasks require iteration through multiple
runs; the system?s behavior on each successive trial is mod-
ified based on feedback from a previous run. For example,
a system might wish to ask for more documents or perform
query expansion if the original query returns an insufficient
number of relevant documents. IIM includes a Controller in-
terface, which specifies methods for sending feedback from
Figure 2: Node Interface and Subtypes.
one node to another. The user can implement a variety of
controllers, depending on the needs of the particular applica-
tion.
3. JAVA IMPLEMENTATION
In the IIM Java implementation, nodes are specified by the ab-
stract interface Node and its six abstract subinterfaces: Source, Fil-
ter, Annotator, Transformer, Sink and Renderer (see Figure 2). Any
user-defined Java class which implements one of the Node subin-
terfaces can be loaded into IIM and used in a node chain. The
visualization of a node is represented by a separate Java class, Box,
which handles all of the details related to drawing the node and
various visual cues in the node chain display.
The graphical user interface (Figure 1) is implemented as a set
of Java Swing components:
  Node Chain Display. The canvas to the right displays the cur-
rent node chain, as described in the previous section. While
Figure 3: Exporting A Macro Node.
the node chain is running, IIM provides two types of visual
feedback regarding task progress. To indicate the percentage
of overall run-time that the node is active, the border color
of each node varies from bright green (low) to bright red
(high). To indicate the amount of output per node per unit
of time spent (throughput), the system indicates bytes per
second as a text label under each node. A rectangular me-
ter at the right of each node provides a graphic visualization
of relative throughput; the node with the highest throughput
will have a solid red meter, while other nodes will have a
meter level which shows their throughput as a percentage of
maximum throughput.
  Node Library. The tree view to the upper left displays the
library of nodes currently available on the user?s machine for
building and extending node chains. New nodes or node di-
rectories can be downloaded from the web and added while
the system is running. The component loader examines each
loaded class using Java?s reflection capabilities, and places it
in the appropriate place(s) in the component tree according
to which of the Node subinterfaces it implements.
  Node Property Editor. The Property Editor (table view) to
the lower left in Figure 1 displays the properties of a selected
node, which the user can update by clicking on it and enter-
ing a new value.
  Node Chain Editor. IIM supports dynamic, interactive ma-
nipulation of node chains. The left side of the toolbar at the
top of the IIM Window contains a set of chain editing but-
tons. These allow the user to create, modify and tune new
node chains built from pre-existing components.
  Transport Bar. IIM uses a tape transport metaphor to model
the operation of the node chain on a given data source. The
?Play?, ?Pause? and ?Rewind? buttons in the toolbar (right
side) allow the user to pause the system in mid-task to adjust
component parameters, or to start a task over after the node
chain has been modified.
The run-time Controller subsystem is implemented as a Java
class called ChainRunner, which can be invoked with or without
a graphical interface component. ChainRunner is implemented as
a Thread object separate from the Java Swing event dispatching
thread, so that user actions can be processed concurrently with the
ongoing operation of a node chain on a particular task.
4. IIM COMPONENTS
The current IIM system includes a variety of nodes which im-
plement the different IIM component interfaces. These nodes are
described in this section.
4.1 Source Nodes
  EditableSource. Prompts the user to interactively enter sam-
ple documents (used primarily for testing, or entering queries).
  WebSource. Generic support for access to web search en-
gines (e.g., Google). Includes multithreading support for si-
multaneous retrieval of multiple result documents.
  NativeBATSource. Generic support for access to document
collections stored on local disk. Implemented in C, with a
Java wrapper that utilized the Java Native Interface (JNI).
4.2 Filter Nodes
  SizeFilter. Only passes documents which are above a user-
defined size threshold.
  RegexpFilter. Only passes documents which match a user-
defined regular expression; incorporates the GNU regexp pack-
age.
4.3 Annotator Nodes
  NameAnnotator. Locates named entities (currently, person
names) in the body of the document, and adds appropriate
annotations to the document.
  IVEAnnotator. For each named entity (person) annotation,
checks a networked database for supplemental information
about that individual. An interface to a database of informa-
tion about individuals, publications, and organizations, cre-
ated as part of the Information Validation and Evaluation
project at CMU [12]. Implemented using Java Database Con-
nectivity (JDBC).
  BrillAnnotator. Accepts a user-defined annotation (e.g., PAS-
SAGE) and adds a new annotation created by calling the Brill
Tagger [1] on the associated text. Implemented via a TCP/IP
socket protocol which accesses a remote instance of the tag-
ger running as a network service.
  ChartAnnotator. Accepts a user-defined annotation, and adds
new annotations based on the results of bottom-up chart pars-
ing with a user-defined grammar. The user can select which
linguistic categories (e.g., NP VP, etc.) are to be annotated.
  RegexpAnnotator. Annotates passages which match a user-
defined regular expression.
4.4 Transformer Nodes
  BrillTransformer. Similar to the BrillAnnotator (see above),
but operates directly on the document body (does not create
separate annotations).
  Inquery. Accepts a query (represented as an input document)
and retrieves a set of documents from the Inquery search en-
gine [2]. Accesses an Inquery server running as a networked
service, using TCP/IP sockets.
  WordNet. Accepts a document, and annotates each word with
a hypernym retrieved from WordNet [19]. Accesses a Word-
Net server running as a networked service, using TCP/IP
sockets.
4.5 Sink Nodes
  Ranker. Collects documents and sorts them according to a
user-defined comparator. The current implementation sup-
ports sorting by document size or by annotation count.
  CooccuranceSink. Builds a matrix of named entity associ-
ations within a given text window; uses NAME annotations
created by the NameAnnotator (see above). The output of
this node is a special subclass of Document, called Matrix-
Document, which stores the association matrix created from
the document collection.
  QAnswer. Collects a variety of annotations from documents
relevant to a particular query (e.g., ?What is Jupiter??), and
uses them to synthesize an answer.
4.6 Renderer Nodes
  StreamRenderer. Outputs any documents it receives to a
user-specified file stream (or to standard output, by default).
  DocumentViewer. Pops up a document display window, which
allows the user to browse documents as they are accepted by
this node.
  MatrixRenderer. A two-dimensional visualization of the as-
sociation matrix created by the CoocurrenceSink (see above).
Accepts instances of MatrixDocument.
5. IIM APPLICATIONS
The initial set of component nodes has been used as the basis for
three experimental applications:
  Filtering and Annotation. An interactive node chain that al-
lows the user to annotate and collect documents matching
any regular expression; the resulting collection can then be
viewed interactively (with highlighted annotations) in a pop-
up viewer window.
  Named Entity Association. A node chain which performs
named-entity annotation using a phi-square measure[3], pro-
ducin a MatrixDocument object (a user-defined Document
subclass, which represents the association matrix). Note that
the addition of a specialized Document subclass does not re-
quire recompilation of IIM (although the user must take care
that specialized document objects are properly handled by
user-defined nodes).
  Question Answering. A node chain which answers ?What
is? questions by querying the web for relevant documents,
finding relevant passages [8, 10], and synthesizing answers
from the results of various regular expression matches3.
6. PERFORMANCE
In order to support accurate side-by-side evaluation of different
modules, IIM implements two kinds of instrumentation for run-
time performance data:
  Per-Node Run Time. The ChainRunner and Box classes au-
tomatically maintain run-time statistics for every node in a
chain (including user-defined nodes). These statistics are
printed at the end of every run.
  Node-Specific Statistics. For user-defined nodes, it may be
useful to report task-specific statistics (e.g., for an Annota-
tor, the total number of annotations, the average annotation
size, etc.). IIM provides a class called Options, which con-
tains a set of optional interfaces that can be implemented to
customize a node?s behavior. Any node that wishes to report
task-specific statistical data can implement the ReportsStatis-
tics interface, which is called by the ChainRunner when the
chain finishes.
An example of the statistical data produced by the system is
shown in Figure 4. The system is careful to keep track of time
spent ?inside? the nodes, as well as the overall clock time taken for
the task. This allows the user to determine how much overhead is
added by the IIM system itself.
The throughput speed of the prototype system is acceptably fast,
averaging better than 50M of text per minute on a sample filter-
ing task (530M of web documents), running on a typical Pentium
III PC with 128M RAM. IIM requires about 10M of memory (in-
cluding the Java run-time environment) for the core system and
user interface, with additional memory requirements depending on
the size of the document stream and the sophistication of the node
chain4. Although the core system is implemented in Java, we have
also implemented nodes in C++, using appropriate wrapper classes
and the Java Native Interface (JNI). This technique allows us to im-
plement critical, resource-intensive nodes using native code, with-
out sacrificing the benefits of the Java-based core system.
7. DISCUSSION
The preliminary results of the IIM prototype are promising. IIM?s
drag-and-drop component library makes it possible to build and
tune a new application in a matter of minutes, greatly reducing the
amount of effort required to integrate and reuse existing modules.

We are currently expanding this application to include part of
speech tagging and syntactic parsing, both of which are straight-
forwardly modeled as examples of the Annotator interface.

Node chains which create a high volume of annotations per doc-
ument use more memory, as do node chains which create new col-
lections, transform documents, etc.
Figure 4: Statistics for a Node Chain.
In the future, we hope this high degree of flexibility will encourage
greater experimentation and the creation of new aggregate systems
from novel combinations of components, leading to a true ?market-
place of modules?.
Building extensible architectures as ?class library plus applica-
tion framework? is not a new idea, and has been discussed before
with respect to information retrieval systems [7, 18, 9]. One might
claim that any new IR architecture should adopt a similar design
pattern, given the proven benefits of separating the modules from
the application framework (flexibility, extensibility, high degree of
reuse, easy integration, etc.). To some extent, IIM consolidates,
refines and/or reimplements ideas previously published in the liter-
ature. Specifically, the following characteristics of the IIM archi-
tecture can be directly compared with prior work:
  The IIM classes Renderer, Document, MultiDocument, and
annotations on Document can be considered alternative im-
plementations of the InfoGrid classes Visualizer, Document,
DocumentSet and DocumentPart [15]. However, in IIM an-
notations are ?lightweight?, meaning that they do not require
the instantiation of a separate user object, but can be mod-
eled as simple String instances in Java when a high degree of
annotation requires optimal space efficiency.
  The use of color to indicate status of a node is also used in
the SketchTrieve system [18].
  IIM?s visualization of the document flow as a ?node chain?
can be compared to the ?wire and dock? approach used in
other IR interfaces [9, 4, 13].
  The use of a Property Editor to customize component behav-
ior is an alternative approach to the IrDialogs provided by
the FireWorks toolkit [9] for display and update of a compo-
nent?s state.
Nevertheless, IIM is at once simpler and more general than sys-
tems such as InfoGrid [15] and FIRE [18]. One could claim that
IIM supports a higher degree of informality [9] than FIRE, since it
enforces no type-checking on node connectivity. Since all tasks are
modeled abstractly as document flows, nodes need only implement
one of the Node sub-interfaces, and each node chain must begin
with a Source. Another point of comparison is the task-specific
detail present in the FIRE class hierarchy. In IIM, task-specific ob-
jects are left up to the developer (for example, representing particu-
lars of access control on information sources, or details of indexing
and retrieval, such as Index, Query, etc.).
Hendry and Harper [9] have used the degree of user control as
a dimension of comparison for IR architectures. At one extreme
are systems which allow dynamic view and access to the run-time
state of components, while at the other lie systems which hide im-
plementation detail and perform some functions automatically, for
improved performance. In their comparison of SketchTrieve and
InfoGrid, Hendry and Harper note that ?a software architecture
should provide abstractions for implementing both these?. In IIM,
the use of macro nodes can hide component details from the end
user, especially when the component?s parameter values have been
tuned in advance for optimal performance.
8. ONGOING RESEARCH
While the initial results reported here show promise, we are still
evaluating the usability of IIM in terms of trainability (how fast
does a novice learn the system), reusability (how easily a novice
can build new applications from existing node libraries) and ease
of integration (effort required to integrate external components and
systems). The current version of IIM lacks the explicit document
management component found in systems like GATE [4] and Corelli
[20]; we are in the process of adding this functionality for the offi-
cial release of IIM.
The IIM system (source code, class documentation, and node
libraries) will be made available via the web as one of our final
project milestones later in 2001. Anyone interested in using the
system or participating in ongoing research and development is in-
vited to visit the IIM web site and join the IIM mailing list:

	  ffJAVELIN: A Flexible, Planner-Based Architecture for Question Answering
Eric Nyberg
Language Technologies Institute
Carnegie Mellon University
ehn@cs.cmu.edu
Robert Frederking
Language Technologies Institute
Carnegie Mellon University
ref@cs.cmu.edu
Abstract
The JAVELIN system integrates a flexible,
planning-based architecture with a variety of
language processing modules to provide an
open-domain question answering capability on
free text. The demonstration will focus on how
JAVELIN processes questions and retrieves the
most likely answer candidates from the given
text corpus. The operation of the system will be
explained in depth through browsing the repos-
itory of data objects created by the system dur-
ing each question answering session.
1 Introduction
Simple factoid questions can now be answered reason-
ably well using pattern matching. Some systems (Soub-
botin and Soubbotin, 2002) use surface patterns enhanced
with semantic categories and question types in order to
model the likelihood of answers given the question. Fur-
thermore, Hovy et al (Hovy et al, 2002) have obtained
good results using only surface patterns pre-extracted
from the web. However, pattern-based approaches don?t
represent the meaning of the patterns they use, and it is
not clear whether they can be generalized for more diffi-
cult, non-factoid questions.
Open domain question answering is a complex, multi-
faceted task, where question type, information availabil-
ity, user needs, and a combination of text processing tech-
niques (statistical, NLP, etc.) must be combined dynami-
cally to determine the optimal answer. For more complex
questions, a more flexible and powerful control mech-
anism is required. For example, LCC (D. Moldovan
and Surdeanu, 2002) has implemented feedback loops
which ensure that processing constraints are met by re-
trieving more documents or expanding question terms.
The LCC system includes a passage retrieval loop, a
lexico-semantic loop and a logic proving loop. The
IBM PIQUANT system (Carroll et al, 2002) combines
knowledge-based agents using predictive annotation with
a statistical approach based on a maximum entropy model
(Ittycheriah et al, 2001).
exe
Domain
Model
Planner 
Data
Repository
JAVELIN 
GUI
Execution
Manager
process history
and data
JAVELIN operator
(action) models
question
answer
ack
.
.
.
dialog
response
exe
results
exe
results
results
Question
Analyzer
Information
Extractor
Answer
Generator
Retrieval
Strategist
Answer
Justification
Web
Browser
Figure 1: The JAVELIN architecture. The Planner con-
trols execution of the individual components via the Ex-
ecution Manager.
Both the LCC and IBM systems represent a depar-
ture from the standard pipelined approach to QA archi-
tecture, and both work well for straightforward factoid
questions. Nevertheless, both approaches incorporate a
pre-determined set of processing steps or strategies, and
have limited ability to reason about new types of ques-
tions not previously encountered. Practically useful ques-
tion answering in non-factoid domains (e.g., intelligence
analysis) requires more sophisticated question decom-
position, reasoning, and answer synthesis. For these
hard questions, QA architectures must define relation-
ships among entities, gather information from multiple
sources, and reason over the data to produce an effec-
tive answer. As QA functionality becomes more sophis-
ticated, the set of decisions made by a system will not
be captured by pipelined architectures or multi-pass con-
straint relaxation, but must be modeled as a step-by-step
decision flow, where the set of processing steps is deter-
mined at run time for each question.
This demonstration illustrates the JAVELIN QA archi-
tecture (Nyberg et al, 2002), which includes a general,
modular infrastructure controlled by a step-by-step plan-
ning component. JAVELIN combines analysis modules,
information sources, user discourse and answer synthe-
sis as required for each question-answering interaction.
JAVELIN also incorporates a global memory, or repos-
                                                               Edmonton, May-June 2003
                                                            Demonstrations , pp. 19-20
                                                         Proceedings of HLT-NAACL 2003
itory, which maintains a linked set of object dependen-
cies for each question answering session. The repository
can be used to provide a processing summary or answer
justification for the user. The repository also provides a
straightforward way to compare the results of different
versions of individual processing modules running on the
same question. The modularity and flexibility of the ar-
chitecture provide a good platform for component-based
(glass box) evaluation (Nyberg and Mitamura, 2002).
2 Demonstration Outline
The demonstration will be conducted on a laptop con-
nected to the Internet. The demonstration will feature
the JAVELIN graphical user interface (a Java application
running on the laptop) and the JAVELIN Repository (the
central database of JAVELIN result objects, accessed via
a web browser). A variety of questions will be asked of
the system, and the audience will be able to view the sys-
tem?s answers along with a detailed trace of the steps that
were taken to retrieve the answers.
Figure 2: An Answer Justification.
Figure 2 shows the top-level result returned by
JAVELIN. The preliminary answer justification includes
the selected answer along with a variety of hyperlinks
that can be clicked to provide additional detail regarding
the system?s analysis of the question, the documents re-
trieved, the passages extracted, and the full set of answer
candidates. The justification also provides drill-down ac-
cess to the steps taken by the Planner module in reason-
ing about how to best answer the given question. Figure 3
shows additional detail that is exposed when the ?Docu-
ments Returned? and ?Request Fills? links are activated.
Acknowledgements
The research described in this paper was supported in part
by a grant from ARDA under the AQUAINT Program
Phase I. The current version of the JAVELIN system was
conceived, designed and constructed with past and cur-
rent members of the JAVELIN team at CMU, including:
Figure 3: Partial Answer Detail.
Jamie Callan, Jaime Carbonell, Teruko Mitamura, Kevyn
Collins-Thompson, Krzysztof Czuba, Michael Duggan,
Laurie Hiyakumoto, Ning Hu, Yifen Huang, Curtis Hut-
tenhower, Scott Judy, Jeongwoo Ko, Anna Kups?c?, Lucian
Lita, Stephen Murtagh, Vasco Pedro, David Svoboda, and
Benjamin Van Durme.
References
J. Carroll, J. Prager, C. Welty, K. Czuba, and D. Ferrucci.
2002. A multi-strategy and multi-source approach to
question answering.
S. Harabagiu D. Moldovan, M. Pasca and M. Surdeanu.
2002.
E. Hovy, U. Hermjakob, and D. Ravichandran. 2002. A
question/answer typology with surface text patterns.
A. Ittycheriah, M. Franz, W. Zhu, and A. Ratnaparkhi.
2001. Question answering using maximum-entropy
components.
E. Nyberg and T. Mitamura. 2002. Evaluating qa sys-
tems on multiple dimensions.
E. Nyberg, T. Mitamura, J. Carbonell, J. Callan,
K. Collins-Thompson, K. Czuba, M. Duggan,
L. Hiyakumoto, N. Hu, Y. Huang, J. Ko, L. Lita,
S. Murtagh, V. Pedro, and D. Svoboda. 2002. The
javelin question-answering system at trec 2002.
M. Soubbotin and S. Soubbotin. 2002. Use of patterns
for detection of likely answer strings: A systematic ap-
proach.
Correction Grammars for Error Handling in a Speech Dialog System 
 
Hirohiko Sagawa Teruko Mitamura Eric Nyberg 
Language Technologies Institute, Carnegie Mellon University 
Pittsburgh, PA 15213, U.S.A. 
{hsagawa, teruko, ehn}@cs.cmu.edu 
 
 
 
Abstract 
Speech recognition errors are inevitable in a 
speech dialog system. This paper presents an 
error handling method based on correction 
grammars which recognize the correction 
utterances which follow a recognition error. 
Correction grammars are dynamically created 
from existing grammars and a set of 
correction templates. We also describe a 
prototype dialog system which incorporates 
this error handling method, and provide 
empirical evidence that this method can 
improve dialog success rate and reduce the 
number of dialog turns required for error 
recovery. 
1 Introduction 
In a dialog system, speech recognition errors are 
inevitable and often make smooth communication 
between a user and a system difficult. Figure 1 shows an 
example of a dialog between a user and a system which 
illustrates a system error. The system misrecognized 
?Tokyo? in the user?s utterance (U1) as ?Kyoto? (S3). If 
the system correctly recognized the user?s utterance, the 
user could answer ?yes? at U3 and the weather is 
reported (S6). However, in this case, the user must 
correct the error at U3 and the turns from S4 to U5 are 
required to recover from the error. The dialog system 
must recognize the user?s response to the system error 
(correction utterance). Otherwise, more turns (or a 
complete restart) will be required to correct the error. 
Therefore, an error handling method which corrects a 
system error and returns to the normal dialog flow 
smoothly is an important requirement for practical 
dialog systems.  
Recent work related to error handling in speech 
dialog systems has mainly focused on error detection. 
Walker et al (2000), Bosch et al (2001) and 
Kazemzadeh et al (2003) extracted several parameters 
(e.g., acoustic, lexical and semantic) from a dialog 
corpus, and analyzed the differences between correction 
utterances and the other utterances in a dialog. They 
also tried to detect system errors by using these 
parameters as input to machine learning methods. 
However, the issue of error recovery is not addressed. 
Danieli (1996) and LuperFoy & Duff (1996) 
proposed error detection methods based on plan 
matching. An error is detected when the intention or the 
parameter expressed in the user?s utterance is not 
consistent with the system?s assumptions and/or 
limitations. In these studies, the correction utterances 
are assumed to be recognized correctly. 
Kitaoka et al (2003) proposed a method to detect 
system errors based on the similarity of speech patterns 
and hypotheses overlapping in the recognition result. 
They also proposed a method to improve the recognition 
accuracy for correction utterances by selecting a speech 
recognition grammar according to the results of the 
error detection. 
The previous studies assumed that the rules for 
speech recognition or natural language processing of 
correction utterances were prepared in advance (Danieli , 
1996; LuperFoy & Duff, 1996). These rules are 
indispensable because the correction utterance often 
includes the information required to correct the error. 
The correction utterance depends on the dialog context, 
especially on the user?s utterances prior to the system 
error. Therefore it is difficult for the system designer to 
prepare these rules in advance when the dialog flow 
becomes complex. To solve this problem, a method that 
can automatically create the rules to interpret correction 
utterances is desirable. 
In this paper, we will propose a method to 
dynamically create the rules to recognize correction 
utterances and repair recognition errors based on the 
dialog context. A prototype dialog system which 
incorporates the proposed method has been developed, 
S1:  Please tell me the area. 
U1: Tokyo. 
S2: Please tell me the date. 
U2: Tomorrow. 
S3: Would you like to know the weather for Kyoto 
tomorrow? 
U3: No, Tokyo. 
S4: Did you say Tokyo? 
U4: Yes. 
S5:  Would you like to know the weather for Tokyo 
tomorrow? 
U5: Yes. 
S6: The weather for Tokyo tomorrow is fine. 
Correction utterance 
System error 
 
Figure 1. Example of a dialog with a system error 
and we present experimental results which show the 
effectiveness of the approach. 
2 CAMMIA Dialog System 
Our current approach focuses on dialog systems which 
incorporate speech recognition modules utilizing regular 
grammars. The CAMMIA system is an example of such 
a dialog system (Nyberg et al, 2002). 
The CAMMIA system is a client-server dialog 
management system based on VoiceXML. Each dialog 
scenario in this system is described in the format of 
DialogXML. The system has the initiative in the dialog, 
and dialogs are oriented around slot-filling for particular 
queries or requests. The server sends a VoiceXML data 
file to the client VoiceXML interpreter for a particular 
dialog turn, compiled from the DialogXML scenario 
according to the current dialog context. The VoiceXML 
data includes system prompts, names of grammar files 
and valid transitions to subsequent dialog states. The 
client interacts with the user according to the 
VoiceXML data.  
Figure 2 shows an example of a grammar rule used 
in the CAMMIA system. The regular grammar rule can 
be represented as a transition network. The following 
sentences are recognized by the rule in Figure 2: 
? I would like to know the weather for Tokyo. 
? I would like to know the weather for Tokyo tomorrow. 
3 Error Handling Based on Correction 
Grammars 
To recognize the user?s utterances in a dialog system, a 
grammar for potential user utterances must be prepared 
in advance for each dialog context. For error handling, it 
is also necessary to anticipate correction utterances and 
prepare a correction grammar. We propose a method to 
automatically create the correction grammar based on 
the current dialog context; error detection and repair is 
implemented using the correction grammar. 
To create the correction grammar, the system must 
know the user?s utterances prior to the error, because 
correction utterances typically depend on them. If the 
user?s utterances are consistent with what the system is 
expecting, the correction grammar can be generated 
based on the grammar previously in use by the speech 
recognizer. Therefore, the sequence of grammars used 
in the dialog so far is stored in the grammar history as 
the dialog context, and the correction grammar is 
created using the grammars in this history. 
Most of the forms of correction utterances can be 
expected in advance because correction utterances 
include many repetitions of words or phrases from 
previous turns (Kazemzadeh et al, 2003). We assume 
that the rules to generate the correction grammar can be 
prepared as templates; the correction grammar is created 
by inserting information extracted from the grammar 
history into a template. 
Figure 3 shows an example of a process flow in a 
dialog system which performs error handling based on a 
correction grammar. The ?system prompt n? is the 
process to output the n-th prompt to the user. The 
correction grammar is created based on the grammar 
used in the ?user response n-1?, which is the process to 
recognize the (n-1)-th user utterance, and it is used in 
the ?user response n? together with the ?grammar n? 
which is used to recognize the n-th normal user?s 
utterance. The system detects the error when the user?s 
utterance is recognized using the correction grammar, 
and then transits into the ?correction of errors? to 
modify the error. The grammar history in Figure 3 
stores only the grammar used in the last recognition 
process. The number of grammars stored in the history 
can be changed depending on the dialog management 
strategy and error handling requirements. 
4 Generation of Correction Grammar 
The correction grammar is created as follows. 
(1) Copying the grammar rules in the history 
The user often repeats the same utterance when the 
system misunderstood what s/he spoke. To detect 
when the user repeats exactly the same utterance, the 
grammar rules in the grammar history are copied into 
the correction grammar. 
(2) Inserting the rules in the history into the template 
When the user tries to correct the system error, some 
 
1 
2 
4 
?I would like to know 
the weather for? 
?Tokyo? 
?tomorrow? 
3 
?Tokyo? 
 
Figure 2. Example of the grammar rule used in the 
CAMMIA system 
 
System prompt n-1
Grammar n-1
Generation of 
correction grammar 
Correction 
grammar n-1 
Grammar n 
. 
. 
. 
. 
. 
. 
Template 
Recognized by 
correction grammar ? 
Yes 
No 
User response n-1
System prompt n 
User response n 
Correction of errors 
System prompt n+1 
 
Figure 3. Process flow: Error handling based on a 
correction grammar 
phrases are often added to the original utterance 
(Kitaoka, 2003). The template mentioned above is 
used to support this type of correction utterance. An 
example of the correction grammar rule generated by 
this method is shown in Figure 4. The ?null? in Figure 
4 implies a transition with no condition, and the ?X? 
shows where the original rule is embedded. In this 
example, the created grammar rule in Figure 4(c) 
corresponds to the following sentences: 
? No, I?d like to know the weather for Tokyo. 
? I said I?d like to know the weather for Tokyo. 
(3) Inserting slot-values into the template 
The user often repeats only words or phrases which 
the system is focusing on (Kazemzadeh et al, 2003). 
In a slot-filling dialog, these correspond to the slot 
values. Therefore, correction grammar rules are also 
created by extracting the slot values from the grammar 
in the history and inserting them into the template. If 
there are several slot values that can be corrected at 
the same time, all of their possible combinations and 
permutations are also generated. An example is shown 
in Figure 5. In Figure 5(b), the slot-values are 
?Tokyo? and ?tomorrow?. The grammar rule in Figure 
5(c) includes each slot value plus their combination(s), 
and represents the following sentences: 
? I said Tokyo. 
? I said tomorrow. 
? I said Tokyo tomorrow. 
5 Prototype System with Error Handling 
We have implemented the proposed error handling 
method for a set of Japanese dialog scenarios in the 
CAMMIA system. We added to this system: a) a 
process to create a correction grammar file when the 
system sends a grammar file to the client, b) a process to 
repair errors based on the recognition result, and c) 
transitions to the repair action when the user?s utterance 
is recognized by the correction grammar. 
There are two types of errors: task transition errors 
and slot value errors. If the error is a task transition error, 
the modification process cancels the current task and 
transits to the new task as specified by the correction 
utterance. When the error is a slot value error, the slot 
value is replaced by the value given in the correction 
utterance. However, if the new value is identical to the 
old one, we assume a recognition error and the second 
candidate in the recognition result is used. This 
technique requires a speech recognizer that can output 
N-best results; we used Julius for SAPI (Kyoto Univ., 
2002) for this experiment. 
6 Experiments 
We carried out an experiment to verify whether the 
proposed method works properly in a dialog system. In 
this experiment, dialog systems with and without the 
error handling method were compared. In this 
experiment, a weather information dialog was selected 
as the task for the subjects and about 1200 dialog 
instances were analyzed (both with and without error 
handling). The dialog flow was the same as shown in 
Figure 1. The grammar included 500 words for place 
names, and 69 words for the date. The subjects were 
instructed in advance on the utterance patterns allowed 
by the system, and used only those patterns during the 
experiment. A summary of the collected data is shown 
in Table 1. When error handling is disabled, the system 
returns to the place name query when the user denies the 
system?s confirmation, e.g. it returns from U3 to S1 in 
Figure 1. A comparison of the number of turns in these 
two systems is shown in Table 2. ?1 error? in Table 2 
means that the dialog included one error and ?2 errors? 
means that the same error was repeated. 
The success rate for the task and the average number 
of turns in the dialog (including errors) are tabulated.  
Dialogs including more than 3 errors were regarded as 
incomplete tasks in the calculation of the success rate. 
The results are shown in Table 3. 
 
1 
X 2 
?no? 
null 
?I said? 
 
(a) Template 
 
1 
3 
?I would like
 
to know 
the weather for? 
2 
?Tokyo? 
 
(b) Grammar rule in the history 
 
3 4 
?I would like
 
to 
know the  
weather for? 
?Tokyo? 
1 
2 5 
?no? 
null 
?I said? 
 
(c) Created correction grammar rule 
Figure 4. Correction grammar created by inserting 
the original rule into a template 
  
1 
X 2 
null 
?I said? 
 
(a) Template 
 
1 
3 
?I would like to know 
the weather for? 
2 
?Tokyo? 
?tomorrow? 
 
(b) Grammar rule in the history 
 
3 
4 
?Tokyo? 
1 
2 5 
null 
?I said? 
?tomorrow? 
?Tokyo? 
?tomorrow? 
 
(c) Created correction grammar rule 
Figure 5. Correction grammar rules created by 
inserting slot values into a template 
7 Discussion 
The task completion rate was improved from 86.4% to 
93.4% when the proposed error handling method was 
used. The average number of turns was reduced by 3 
turns as shown in Table 3. This result shows that the 
proposed error handling method was working properly 
and effectively. 
One reason that the success rate was improved is 
that the proposed method prevents the repetition of 
errors. When the error handling method is not 
implemented, errors can be easily repeated. The error 
handling method can avoid repeated errors by selecting 
the second candidate in the recognition result even when 
the correction utterance is also misrecognized. There 
were 7 dialogs in which the correction utterance was 
correctly recognized by selecting the second candidate. 
However, there were 13 dialogs in which the error 
was not repaired by one correction utterance. There are 
two explanations. One is that there are insertion errors 
in speech recognition which causes words not spoken to 
appear in the recognition result. For example, the 
system prompt S4 for U3 in Figure 1 becomes as 
follows: 
S4: Did you say Tokyo yesterday? 
In this case, the user has to speak more correction 
utterances. The second explanation is that the 
recognition result did not always include the correct 
result within the first two candidates. It is not clear that 
extending the repair mechanism to always consider 
additional recognition candidates (3rd, 4th, etc.) is a 
viable technique, given the drop off in recognition 
accuracy; more study is required. 
8 Conclusions 
In this paper, we proposed an error handling method 
based on dynamic generation of correction grammars to 
recognize the user corrections which follow system 
errors. The correction grammars detect system errors 
and also repair the dialog flow, improving task 
completion rates and reducing the average number of 
dialog turns. We developed a prototype dialog system 
using the proposed method, and demonstrated 
empirically that the success rate improved by 7.0%, and 
the number of turns was reduced by 3. 
The creation of rules for correction utterances based 
on the dialog history could be applicable to dialog 
systems which use speech recognition or natural 
language processing and other kinds of rules beyond 
regular grammars; we plan to study this in future work. 
We are also planning to develop an algorithm to 
improve the precision of corrections that are based on 
the set of recognition candidates for the correction 
utterance and an error recovery strategy. We also plan to 
apply the proposed method to other types of dialogs, 
such as user-initiative dialogs and mixed-initiative 
dialogs. 
References 
Abe Kazemzadeh, Sungbok Lee and Shrikanth 
Narayanan. 2003. Acoustic Correlates of User 
Response to Error in Human-Computer Dialogues, 
Proceedings of ASRU 2003: 215-220. 
Antal van den Bosch, Emiel Krahmer and Marc 
Swerts. 2001. Detecting problematic turns in 
human-machine interactions: Rule-Induction 
Versus Memory-Based Learning Approaches, 
Proceedings of ACL 2001: 499-507. 
Eric Nyberg, Teruko Mitamura, Paul Placeway, 
Michael Duggan, Nobuo Hataoka. 2002. Dynamic 
Dialog Management with VoiceXML, Proceedings 
of HLT-2002. 
Kyoto University, 2002, Julius Open-Source Real-
Time Large Vocabulary Speech Recognition 
Engine, http://julius.sourceforge.jp. 
Marilyn Walker, Jerry Wright and Irene Langkilde. 
2000. Using Natural Language Processing and 
Discourse Features to Identify Understanding 
Errors in a Spoken Dialogue System, Proceedings 
of ICML-2000: 1111-1118. 
Morena Danieli. 1996. On the Use of Expectations for 
Detecting and Repairing Human-Machine 
Miscommunication, Proceedings of AAAI 
Workshop on Detection, Repair and Prevention of 
Human-Machine Miscommunication: 87-93. 
Norihide Kitaoka, Kaoko Kakutani and Seiichi 
Nakagawa. 2003. Detection and Recognition of 
Correction Utterance in Spontaneously Spoken 
Dialog, Proceedings of EUROSPEECH 2003: 625-
628. 
Susann LuperFoy and David Duff. 1996. Disco: A 
Four-Step Dialog Recovery Program, The 
Proceedings of the AAAI Workshop on Detection, 
Repair and Prevention of Human-Machine 
Miscommunication: 73-76. 
Table 1. Summary of the collected data 
 w/o error handling w/ error handling 
# of users 2 male, 1 female 2 male, 1 female 
# of dialog 603 596 
# of error dialog 66 61 
 
Table 2. Number of turns in the dialog 
 No error 1 error 2 errors 
w/o error handling 13 19 
w/ error handling 7 11 13 
 
Table 3. Success rate and average number of turns 
 Success rate Ave. # turns 
w/o error handling 86.4% 14.6 
w/ error handling 93.4% 11.6 
 
Proceedings of NAACL HLT 2007, pages 524?531,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
A Probabilistic Framework for Answer Selection in Question Answering
Jeongwoo Ko1, Luo Si2, Eric Nyberg1
1Language Technologies Institute, Carnegie Mellon, Pittsburgh, PA 15213
2Department of Computer Science, Purdue University, West Lafayette, IN 47907
jko@cs.cmu.edu, lsi@cs.purdue.edu, ehn@cs.cmu.edu
Abstract
This paper describes a probabilistic an-
swer selection framework for question an-
swering. In contrast with previous work
using individual resources such as ontolo-
gies and the Web to validate answer can-
didates, our work focuses on developing
a unified framework that not only uses
multiple resources for validating answer
candidates, but also considers evidence of
similarity among answer candidates in or-
der to boost the ranking of the correct an-
swer. This framework has been used to se-
lect answers from candidates generated by
four different answer extraction methods.
An extensive set of empirical results based
on TREC factoid questions demonstrates
the effectiveness of the unified framework.
1 Introduction
Question answering aims at finding exact answers
to a user?s natural language question from a large
collection of documents. Most QA systems com-
bine information retrieval with extraction techniques
to identify a set of likely candidates and then uti-
lize some selection strategy to generate the final
answers (Prager et al, 2000; Clarke et al, 2001;
Harabagiu et al, 2001). Since answer extractors
may be based on imprecise empirical methods, the
selection process can be very challenging, as it often
entails identifying correct answer(s) amongst many
incorrect ones.
Questio
n Ques
tion Analys
isQu
ery D
ocumen
t
Retriev
al Corpus
Docs
Answe
r
Extract
ionAn
swer candida
tes An
swer Selecti
on Answe
r
Shang
hai
FT94
2-20
16
0.5
Taiw
an
FBIS
3-4532
0
0.4
Shang
hai
FBIS
3-58
0.64
Shang
hai
WSJ9
2011
0-00
13
0.65
Hong
 Kon
g
AP88
0603
-026
8
0.7
Beijin
g
Docu
ment
 
extra
cted
Score
Answ
er 
cand
idate
s
Whic
h city
 in C
hina 
has t
he 
large
st nu
mber
 of fo
reign
 
finan
cial c
omp
anies
?
Figure 1: A traditional QA pipeline architecture
Figure 1 shows a traditional QA architecture with
an example question. Given the question ?Which
city in China has the largest number of foreign fi-
nancial companies??, the answer extraction com-
ponent produces a ranked list of five answer can-
didates. Due to imprecision in answer extraction,
an incorrect answer (?Beijing?) was ranked at the
top position. The correct answer (?Shanghai?) was
extracted from two documents with different confi-
dence scores and ranked at the third and the fifth po-
sitions. In order to select ?Shanghai? as the final
answer, we need to address two issues:
? Answer Validation. How do we identify correct
answer(s) amongst incorrect ones? Validating
an answer may involve searching for facts in
a knowledge base, e.g. IS-A(Shanghai,
city), IS-IN(Shanghai, China).
? Answer Similarity. How do we exploit evi-
dence of similarity among answer candidates?
524
For example, when there are redundant an-
swers (?Shanghai?, as above) or several an-
swers which represent a single instance (e.g.
?Clinton, Bill? and ?William Jefferson Clin-
ton?) in the candidate list, how much should we
boost the answer candidate scores?
To address the first issue, several answer selec-
tion approaches have used semantic resources. One
of the most common approaches relies on Word-
Net, CYC and gazetteers for answer validation or
answer reranking; answer candidates are pruned
or discounted if they are not found within a re-
source?s hierarchy corresponding to the expected an-
swer type (Xu et al, 2003; Moldovan et al, 2003;
Prager et al, 2004). In addition, the Web has been
used for answer reranking by exploiting search en-
gine results produced by queries containing the an-
swer candidate and question keywords (Magnini et
al., 2002), and Wikipedia?s structured information
has been used for answer type checking (Buscaldi
and Rosso, 2006).
To use more than one resource for answer
type checking of location questions, Schlobach
et al (2004) combined WordNet with geographi-
cal databases. However, in their experiments the
combination actually hurt performance because of
the increased semantic ambiguity that accompanies
broader coverage of location names. This demon-
strates that the method used to combine potential
answers may matter as much as the choice of re-
sources.
To address the second issue we must determine
how to detect and exploit answer similarity. As an-
swer candidates are extracted from different docu-
ments, they may contain identical, similar or com-
plementary text snippets. For example, the United
States may be represented by the strings ?U.S.?,
?United States? or ?USA? in different documents. It
is important to detect this type of similarity and ex-
ploit it to boost answer confidence, especially for list
questions that require a set of unique answers. One
approach is to incorporate answer clustering (Kwok
et al, 2001; Nyberg et al, 2003; Jijkoun et al,
2006). For example, we might merge ?April 1912?
and ?14 Apr 1912? into a cluster and then choose
one answer as the cluster head. However, clustering
raises new issues: how to choose the cluster head
and how to calculate the scores of the clustered an-
swers.
Although many QA systems individually address
these issues in answer selection, there has been lit-
tle research on generating a generalized probabilistic
framework that allows any validation and similarity
features to be easily incorporated.
In this paper we describe a probabilistic answer
selection framework to address the two issues. The
framework uses logistic regression to estimate the
probability that an answer candidate is correct given
multiple answer validation features and answer sim-
ilarity features. Experimental results on TREC
factoid questions (Voorhees, 2004) show that our
framework significantly improved answer selection
performance for four different extraction techniques,
when compared to default selection using the indi-
vidual candidate scores produced by each extractor.
This paper is organized as follows: Section 2 de-
scribes our answer selection framework and Section
3 lists the features that generate similarity and va-
lidity scores for factoid questions. In Section 4, we
describe the experimental methodology and the re-
sults. Section 5 describes how we intend to extend
our framework to handle complex questions. Finally
Section 6 concludes with suggestions for future re-
search.
2 Method
Answer validation is based on an estimate of the
probability P (correct(Ai)|Ai, Q), where Q is a
question and Ai is an answer candidate to the ques-
tion. Answer similarity is is based on an estimate
of the probability P (correct(Ai)|Ai, Aj), where Aj
is similar to Ai. Since both probabilities influ-
ence answer selection performance, it is important
to combine them in a unified framework and es-
timate the probability of an answer candidate as:
P (correct(Ai)|Q,A1, ..., An).
In this paper, we propose a proba-
bilistic framework that directly estimates
P (correct(Ai)|Q,A1, ..., An) using multiple
answer validation features and answer similarity
features. The framework was implemented with
logistic regression, which is a statistical machine
learning technique used to predict the probability
of a binary variable from input variables. Logistic
525
P (correct(Ai)|Q,A1, ..., An) (1)
? P (correct(Ai)|val1(Ai), ..., valK1(Ai), sim1(Ai), ..., simK2(Ai))
=
exp(?0 +
K1?
k=1
?kvalk(Ai) +
K2?
k=1
?ksimk(Ai))
1 + exp(?0 +
K1?
k=1
?kvalk(Ai) +
K2?
k=1
?ksimk(Ai))
where, simk(Ai) =
N?
j=1(j 6=i)
sim?k(Ai, Aj).
~?, ~?,~? = argmax
~?,~?,~?
R?
j=1
Nj?
i=1
logP (correct(Ai)|val1(Ai), ..., valK1(Ai), sim1(Ai), ..., simK2(Ai)) (2)
regression has been successfully employed in many
applications including multilingual document merg-
ing (Si and Callan, 2005). In our previous work (Ko
et al, 2006), we showed that logistic regression
performed well in merging three resources to vali-
date answers to location and proper name questions.
We extended this approach to combine multiple
similarity features with multiple answer validation
features. The extended framework estimates the
probability that an answer candidate is correct given
the degree of answer correctness and the amount
of supporting evidence provided in a set of answer
candidates (Equation 1).
In Equation 1, each valk(Ai) is a feature function
used to produce an answer validity score for an an-
swer candidate Ai. Each sim?k(Ai, Aj) is a similar-
ity function used to calculate an answer similarity
between Ai and Aj . K1 and K2 are the number of
answer validation and answer similarity features, re-
spectively. N is the number of answer candidates.
To incorporate multiple similarity features, each
simk(Ai) is obtained from an individual similarity
metric. For example, if Levenshtein distance is used
as one similarity metric, simk(Ai) is calculated by
summing N-1 Levenshtein distances between one
answer candidate and all other candidates. As some
string similarity metrics (e.g. Levenshtein distance)
produce a number between 0 and 1 (where 1 means
two strings are identical and 0 means they are differ-
ent), similarity scores less than some threshold value
are ignored.
The parameters ?, ?, ? were estimated from train-
ing data by maximizing the log likelihood as shown
in Equation 2, where R is the number of training
questions and Nj is the number of answer candidates
for each question Qj . For parameter estimation, we
used the Quasi-Newton algorithm (Minka, 2003).
To select correct answers, the initial answer candi-
date set is reranked according to the estimated prob-
ability of each candidate. For factoid questions, the
top answer is selected as the final answer to the ques-
tion. As logistic regression can be used for binary
classification with a default threshold of 0.5, we can
also use the framework to classify incorrect answers:
if the probability of an answer candidate is lower
than 0.5, it is considered to be a wrong answer and
is filtered out of the answer list. This is useful in
deciding whether or not a valid answer exists in the
corpus, an important aspect of the TREC QA evalu-
ation (Voorhees, 2004).
3 Feature Representation
This section details the features used to generate an-
swer validity scores and answer similarity scores for
our answer selection framework.
526
3.1 Answer Validation Features
Each answer validation feature produces a validity
score which predicts whether or not an answer can-
didate is a correct answer for the question. This task
can be done by exploiting external QA resources
such as the Web, databases, and ontologies. For fac-
toid questions, we used gazetteers and WordNet in a
knowledge-based approach; we also used Wikipedia
and Google in a data-driven approach.
3.1.1 Knowledge-based Features
In order to generate answer validity scores using
gazetteers and WordNet, we reused the algorithms
described in our previous work (Ko et al, 2006).
Gazetteers: Gazetteers provide geographic
information, which allows us to identify
strings as instances of countries, their cities,
continents, capitals, etc. For answer selec-
tion, we used three gazetteer resources: the
Tipster Gazetteer, the CIA World Factbook
(https://www.cia.gov/cia/publications/factbook/inde
x.html) and information about the US states pro-
vided by 50states.com (http://www.50states.com).
These resources were used to assign an answer
validity score between -1 and 1 to each candidate
(Figure 2). A score of 0 means the gazetteers did
not contribute to the answer selection process for
that candidate. For some numeric questions, range
checking was added to validate numeric questions
similarly to Prager et al (2004). For example, given
the question ?How many people live in Chile??,
if an answer candidate is within ? 10% of the
population stated in the CIA World Factbook, it
receives a score of 1.0. If it is in the range of 20%,
its score is 0.5. If it significantly differs by more
than 20%, it receives a score of -1.0. The threshold
may vary based on when the document was written
and when the census was taken1.
WordNet: The WordNet lexical database includes
English words organized in synonym sets, called
synsets (Fellbaum, 1998). We used WordNet in or-
der to produce an answer validity score between -1
and 1, following the algorithm in Figure 3. A score
1The ranges used here were found to work effectively, but
were not explicitly validated or tuned.
 
 
  
1)  If the answer candidate directly matches the gazetteer 
answer for the question, its gazetteer score is 1.0. (e.g. 
Given the question ?What continent is Togo on??, the 
candidate ?Africa? receives a score of 1.0.) 
2)  If the answer candidate occurs in the gazetteer within 
the subcategory of the expected answer type, its score 
is 0.5. (e.g., Given the question ?Which city in China 
has the largest number of foreign financial 
companies??, the candidates ?Shanghai? and ?Boston? 
receive a score of 0.5 because they are both cities.) 
3)  If the answer candidate is not the correct semantic 
type, its score is -1. (e.g., Given the question ?Which 
city in China has the largest number of foreign 
financial companies??, the candidate ?Taiwan? 
receives a score of -1 because it is not a city.) 
4) Otherwise, the score is 0.0. 
Figure 2: Validity scoring with gazetteers.
 
 
 
 
 
 
 
 
 
 
 
1)  If the answer candidate directly matches WordNet, its 
WordNet score is 1.0. (e.g. Given the question ?What is 
the capital of Uruguay??, the candidate ?Montevideo? 
receives a score of 1.0.) 
2)  If the answer candidate?s hypernyms include a 
subcategory of the expected answer type, its score is 
0.5. (e.g., Given the question ?Who wrote the book 
?Song of Solomon??", the candidate ?Mark Twain? 
receives a score of 0.5 because its hypernyms include 
?writer?.) 
3)  If the answer candidate is not the correct semantic 
type, this candidate receives a score of -1. (e.g., Given 
the question ?What state is Niagara Falls located in??, 
the candidate ?Toronto? gets a score of -1 because it is 
not a state.) 
4) Otherwise, the score is 0.0. 
Figure 3: Validity scoring with WordNet.
of 0 means that WordNet does not contribute to the
answer selection process for a candidate.
3.1.2 Data-driven Features
Wikipedia and Google were used in a data-driven
approach to generate answer validity scores.
Wikipedia: Wikipedia (http://www.wikipedia.org)
is a multilingual free on-line encyclopedia. Fig-
ure 4 shows the algorithm used to generate an
answer validity score from Wikipedia. If there
is a Wikipedia document whose title matches an
answer candidate, the document is analyzed to
obtain the term frequency (tf) and the inverse term
527
For
 eac
h an
swe
r ca
ndi
date
 A i,
1. I
niti
aliz
e th
e W
ikip
edia
 sco
re: 
ws(
A i) 
= 0
2. S
earc
h fo
r a 
Wik
iped
ia d
ocu
men
t wh
ose
 titl
e is
 A i
3. I
f a 
doc
um
ent 
is fo
und
, ca
lcul
ate 
tf.id
fsc
ore
 of 
A ii
n th
e 
retr
ieve
d W
ikip
edia
 doc
um
ent
ws(
A i) 
= (1
+lo
g(tf
)) ?
 (1+
log
(idf
))
4. I
f no
t, fo
r ea
ch q
ues
tion
 key
wor
d K
j,
4.1.
 Sea
rch
 for
 a W
ikip
edia
 doc
um
ent 
that
 inc
lud
es K
j
4.2.
 If a
 doc
um
ent 
is fo
und
, ca
lcul
ate 
tf.id
fsc
ore
 of 
A i 
ws(
A i) 
+= 
(1+
log
(tf)
) ? 
(1+
log
(idf
))
Figure 4: Validity scoring with Wikipedia
1 )
1( 2)(
)(
?
+
?
=
d
scs
scs
For
 eac
h an
swe
r ca
ndi
date
 A i,
1. I
niti
aliz
e th
e G
oog
le s
cor
e: g
s(A
i) =
 0
2. F
or e
ach
 sni
ppe
t s:
2.1.
 Ini
tial
ize 
the 
snip
pet 
co-
occ
urre
nce
 sco
re: 
cs(s
) = 
1
2.2.
 Fo
r ea
ch q
ues
tion
 key
wor
d k
in s
: 
2.2.
1 C
om
put
e di
stan
ce d
, th
e m
inim
um
 num
ber
 of 
wor
ds b
etw
een
 ka
nd t
he a
nsw
er c
and
idat
e 
2.2.
2 U
pda
te th
e sn
ipp
et c
o-o
ccu
rren
ce s
cor
e:
2.3.
 gs(
A i) 
= g
s(A
i) +
 cs(
s)
3. N
orm
aliz
e th
e G
oog
le s
cor
e (d
ivid
ing
 it b
y a 
con
stan
t C
)
Figure 5: Validity scoring with Google
frequency (idf) of the candidate, from which a
tf.idf score is calculated. When there is no matched
document, each question keyword is also processed
as a back-off strategy, and the answer validity score
is calculated by summing the tf.idf scores. To
calculate word frequency, the TREC Web Corpus
(http://ir.dcs.gla.ac.uk/test collections/wt10g.html)
was used as a large background corpus.
Google: Following Magnini et al (2002), we used
Google to generate a numeric score. A query con-
sisting of an answer candidate and question key-
words was sent to the Google search engine. To
calculate a score, the top 10 text snippets returned
by Google were then analyzed using the algorithm
in Figure 5.
3.2 Answer Similarity Features
We calculate the similarity between two answer can-
didates using multiple string distance metrics and a
list of synonyms.
3.2.1 String Distance Metrics
There are several different string distance metrics
to calculate the similarity of short strings. We used
five popular string distance metrics: Levenshtein,
Jaccard, Jaro, Jaro-Winkler, and Cosine similarity.
3.2.2 Synonyms
Synonyms can be used as another metric to calcu-
late answer similarity. We defined a binary similar-
ity score for synonyms.
sim(Ai, Aj) =
{
1, if Ai is a synonym of Aj
0, otherwise
To get a list of synonyms, we used three knowl-
edge bases: WordNet, Wikipedia and the CIA World
Factbook. WordNet includes synonyms for English
words. Wikipedia redirection is used to obtain an-
other set of synonyms. For example, ?Calif.? is redi-
rected to ?California? in Wikipedia, and ?William
Jefferson Clinton? is redirected to ?Bill Clinton?.
The CIA World Factbook includes five different
names for a country: conventional long form, con-
ventional short form, local long form, local short
form and former name. For example, the conven-
tional long form of Egypt is ?Arab Republic of
Egypt?, the conventional short form is ?Egypt?, the
local short form is ?Misr?, the local long form is
?Jumhuriyat Misr al-Arabiyah? and the former name
is ?United Arab Republic (with Syria)?. All are con-
sidered to be synonyms of ?Egypt?.
In addition, manually generated rules are used to
obtain synonyms for different types of answer can-
didates (Nyberg et al, 2003):
? Dates are converted into the ISO 8601 date for-
mat (YYYY-MM-DD) (e.g., ?April 12 1914?
and ?12th Apr. 1914? are converted into ?1914-
04-12? and considered as synonyms).
? Temporal expressions are converted into the
HH:MM:SS format (e.g., ?six thirty five p.m.?
and ?6:35 pm? are converted into ?18:35:xx?
and considered as synonyms).
? Numeric expression are converted into sci-
entific notation (e.g, ?one million? and
?1,000,000? are converted into ?1e+06? and
considered as synonyms).
528
? Representative entities are converted into the
represented entity when the expected answer
type is COUNTRY (e.g., ?the Egyptian govern-
ment? is changed to ?Egypt? and ?Clinton ad-
ministration? is changed to ?U.S.?).
4 Experiment
This section describes the experiments we used
to evaluate our answer selection framework. The
JAVELIN QA system (Nyberg et al, 2006) was used
as a testbed for the evaluation.
4.1 Experimental Setup
A total of 1760 factoid questions from the TREC8-
12 QA evaluations served as a dataset, with 5-fold
cross validation.
To better understand how the performance of our
framework varies for different extraction techniques,
we tested it with four JAVELIN answer extraction
modules: FST, LIGHTv1, LIGHTv2 and SVM (Ny-
berg et al, 2006). FST is an answer extractor based
on finite state transducers that incorporate a set of
extraction patterns (both manually-created and gen-
eralized patterns). LIGHTv1 is an extractor that se-
lects answer candidates using a non-linear distance
heuristic between the keywords and an answer can-
didate. LIGHTv2 is another extractor based on a
different distance heuristic, originally developed as
part of a multilingual QA system. SVM is an extrac-
tor that uses Support Vector Machines to discrimi-
nate between correct and incorrect answers.
Answer selection performance was measured by
average accuracy: the number of correct top answers
divided by the number of questions where at least
one correct answer exists in the candidate list pro-
vided by an extractor. The baseline was calculated
with the answer candidate scores provided by each
individual extractor; the answer with the best extrac-
tor score was chosen, and no validation or similarity
processing was performed. For Wikipedia, we used
a version downloaded in Nov. 2005, which con-
tained 1,811,554 articles.
4.2 Results and Analysis
We first analyzed the average accuracy when us-
ing individual validation features. Figure 6 shows
the effect of the individual answer validation fea-
tures on different extraction outputs. The combina-
0.00.10.20.30.40.50.60.70.80.91.0
AL
L
GL
W
IKI
W
N
GZ
Ba
se
lin
e
Average Accuracy
 FS
T
 Li
gh
tV1
 Li
gh
tV2
 SV
M
Figure 6: Average accuracy of individual answer
validation features (GZ: gazetteers, WN: WordNet,
WIKI: Wikipedia, GL: Google, ALL: combination
of all features).
tion of all features significantly improved the per-
formance when compared to answer selection using
a single feature. Comparing the data-driven features
with the knowledge-based features, the data-driven
features (such as Wikipedia and Google) increased
performance more than the knowledge-based fea-
tures (such as gazetteers and WordNet); our intuition
is that the knowledge-based features covered fewer
questions. The biggest improvement was found with
candidates produced by the SVM extractor: a 242%
improvement over the baseline. It was mostly be-
cause SVM tended to produce several answer can-
didates with the same or very similar confidence
scores, but our framework could select the correct
answer among many incorrect ones by exploiting
answer validation features.
Table 1 shows the effect of individual similarity
features on different extractors when using 0.3 and
0.5 as a similarity threshold, respectively. When
comparing five different string similarity features
(Levenshtein, Jaro, Jaro-Winkler, Jaccard and Co-
sine similarity), Levenshtein and Jaccard tended to
perform better than the others. When comparing
synonym features with string similarity features,
synonyms performed slightly better.
We also analyzed answer selection performance
when combining all six similarity features (?All? in
Table 1). Combining all similarity features did not
improve the performance except for the FST extrac-
tor, because including five string similarity features
529
Similarity FST LIGHTv1 LIGHTv2 SVM
feature 0.3 0.5 0.3 0.5 0.3 0.5 0.3 0.5
Levenshtein 0.728 0.728 0.471 0.455 0.399 0.400 0.381 0.383
Jaro 0.708 0.705 0.422 0.440 0.373 0.378 0.274 0.282
Jaro-Winkler 0.701 0.705 0.426 0.442 0.374 0.379 0.277 0.275
Jaccard 0.738 0.738 0.438 0.448 0.452 0.448 0.382 0.390
Cosine 0.738 0.738 0.436 0.435 0.418 0.422 0.380 0.378
Synonyms 0.745 0.745 0.458 0.458 0.442 0.442 0.412 0.412
Lev+Syn 0.748 0.751 0.460 0.466 0.445 0.448 0.420 0.412
Jac+Syn 0.742 0.742 0.456 0.465 0.440 0.445 0.396 0.396
All 0.755 0.755 0.405 0.425 0.435 0.431 0.303 0.302
Table 1: Average accuracy using individual similarity features under different thresholds: 0.3 and 0.5
(?Lev+Syn?: the combination of Levenshtein with synonyms, ?Jac+Syn?: the combination of Jaccard and
synonyms, ?All?: the combination of all similarity metrics)
Baseline Sim Val All
FST 0.658 0.751 0.855 0.877
LIGHTv1 0.394 0.466 0.612 0.628
LIGHTv2 0.343 0.448 0.578 0.582
SVM 0.169 0.420 0.578 0.586
Table 2: Average accuracy of individual features
(Sim: merging similarity features, Val: merging val-
idation features, ALL: combination of all features).
provided too much redundancy to the logistic regres-
sion. We also compared the combination of Leven-
shtein with synonyms and the combination of Jac-
card with synonyms, and then chose Levenshtein
and synonyms as the two best similarity features in
our framework.
We also analyzed the degree to which the average
accuracy was affected by answer similarity and val-
idation features. Table 2 compares the average ac-
curacy using the baseline, the answer similarity fea-
tures, the answer validation features and all feature
combinations. As can be seen, the similarity fea-
tures significantly improved performance, so we can
conclude that exploiting answer similarity improves
answer selection performance. The validation fea-
tures also significantly improved the performance.
When combining both sets of features together,
the answer selection performance increased for all
four extractors: an average of 102% over the base-
line, 30% over the similarity features and 1.82%
over the validation features. Adding the similarity
features to the validation features generated small
but consistent improvement in all configurations.
We expect more performance gain from similar-
ity features when merging similar answers returned
from all four extractors.
5 Extensions for Complex Questions
Although we conducted our experiments on fac-
toid questions, our framework can be easily ex-
tended to handle complex questions, which require
longer answers representing facts or relations (e.g.,
?What is the relationship between Alan Greenspan
and Robert Rubin??). As answer candidates are
long text snippets, different features should be used
for answer selection. Possible validation features
include question keyword inclusion and predicate
structure match (Nyberg et al, 2005). For exam-
ple, given the question ?Did Egypt sell Scud mis-
siles to Syria??, the key predicate from the ques-
tion is Sell(Egypt, Syria, Scud missile). If there is
a sentence which contains the predicate structure
Buy(Syria, Scud missile, Egypt), we can calculate
the predicate structure distance and use it as a val-
idation feature. For answer similarity, we intend to
explore novelty detection approaches evaluated in
Allan et al (2003).
6 Conclusion
In this paper, we described our answer selection
framework for estimating the probability that an an-
swer candidate is correct given multiple answer vali-
530
dation and similarity features. We conducted a series
of experiments to evaluate the performance of the
framework and analyzed the effect of individual val-
idation and similarity features. Empirical results on
TREC questions show that our framework improved
answer selection performance in the JAVELIN QA
system by an average of 102% over the baseline,
30% over the similarity features alone and 1.82%
over the validation features alone.
We plan to improve our framework by adding reg-
ularization and selecting the final answers among
candidates returned from all extractors. As our
current framework is based on the assumption that
each answer is independent, we are building another
probabilistic framework which does not require any
independence assumption, and uses an undirected
graphical model to estimate the joint probability of
all answer candidates.
7 Acknowledgments
This work was supported in part by ARDA/DTO
Advanced Question Answering for Intelli-
gence (AQUAINT) program award number
NBCHC040164.
References
J. Allan, C. Wade, and A. Bolivar. 2003. Retrieval and
novelty detection at the sentence level. In Proceedings
of SIGIR.
D. Buscaldi and P. Rosso. 2006. Mining Knowledge
from Wikipedia for the Question Answering task. In
Proceedings of the International Conference on Lan-
guage Resources and Evaluation.
C. Clarke, G. Cormack, and T. Lynam. 2001. Exploiting
redundancy in question answering. In Proceedings of
SIGIR.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. MIT Press.
S. Harabagiu, D. Moldovan, M. Pasca, R. Mihalcea,
M. Surdeanu, R. Bunescu, R. Grju, V. Rus, and
P. Morarescu. 2001. FALCON: Boosting knowledge
for answer engines. In Proceedings of TREC.
V. Jijkoun, J. van Rantwijk, D. Ahn, E. Tjong Kim Sang,
and M. de Rijke. 2006. The University of Amsterdam
at CLEF@QA 2006. In Working Notes CLEF.
J. Ko, L. Hiyakumoto, and E. Nyberg. 2006. Exploit-
ing semantic resources for answer selection. In Pro-
ceedings of the International Conference on Language
Resources and Evaluation.
C. Kwok, O. Etzioni, and D. S. Weld. 2001. Scal-
ing question answering to the web. In Proceedings of
WWW10 Conference.
B. Magnini, M. Negri, R. Pervete, and H. Tanev. 2002.
Comparing statistical and content-based techniques for
answer validation on the web. In Proceedings of the
VIII Convegno AI*IA.
T. Minka. 2003. A Comparison of Numerical Optimizers
for Logistic Regression. Unpublished draft.
D. Moldovan, D. Clark, S. Harabagiu, and S. Maiorano.
2003. Cogex: A logic prover for question answering.
In Proceedings of HLT-NAACL.
E. Nyberg, T. Mitamura, J. Carbonell, J. Callan,
K. Collins-Thompson, K. Czuba, M. Duggan,
L. Hiyakumoto, N. Hu, Y. Huang, J. Ko, L. Lita,
S. Murtagh, V. Pedro, and D. Svoboda. 2003. The
JAVELIN Question-Answering System at TREC 2002.
In Proceedings of the Text REtrieval Conference.
E. Nyberg, T. Mitamura, R. Frederking, M. Bilotti,
K. Hannan, L. Hiyakumoto, J. Ko, F. Lin, V. Pedro,
and A. Schlaikjer. 2006. JAVELIN I and II Systems at
TREC 2005. In Proceedings of TREC.
E. Nyberg, T. Mitamura, R. Frederking, V. Pedro,
M. Bilotti, A. Schlaikjer, and K. Hannan. 2005. Ex-
tending the javelin qa system with domain semantics.
In Proceedings of AAAI-05 Workshop on Question An-
swering in Restricted Domains.
J. Prager, E. Brown, A. Coden, and D. Radev. 2000.
Question answering by predictive annotation. In Pro-
ceedings of SIGIR.
J. Prager, J. Chu-Carroll, K. Czuba, C. Welty, A. Itty-
cheriah, and R. Mahindru. 2004 IBM?s Piquant in
Trec2003. In Proceedings of TREC.
S. Schlobach, M. Olsthoorn, and M. de Rijke. 2004.
Type checking in open-domain question answering. In
Proceedings of European Conference on Artificial In-
telligence.
L. Si and J. Callan. 2005 CLEF2005: Multilingual
retrieval by combining multiple multilingual ranked
lists. In Proceedings of Cross-Language Evaluation
Forum.
E. Voorhees. 2004. Overview of the TREC 2003 ques-
tion answering track. In Proceedings of TREC.
J. Xu, A. Licuanan, J. May, S. Miller, and R. Weischedel.
2003. TREC 2002 QA at BBN: Answer Selection and
Confidence Estimation. In Proceedings of TREC.
531
Proceedings of the NAACL HLT Student Research Workshop and Doctoral Consortium, pages 55?60,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Interactive Annotation Learning with Indirect Feature Voting
Shilpa Arora and Eric Nyberg
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{shilpaa,ehn}@cs.cmu.edu
Abstract
We demonstrate that a supervised annotation
learning approach using structured features
derived from tokens and prior annotations per-
forms better than a bag of words approach.
We present a general graph representation for
automatically deriving these features from la-
beled data. Automatic feature selection based
on class association scores requires a large
amount of labeled data and direct voting can
be difficult and error-prone for structured fea-
tures, even for language specialists. We show
that highlighted rationales from the user can
be used for indirect feature voting and same
performance can be achieved with less labeled
data.We present our results on two annotation
learning tasks for opinion mining from prod-
uct and movie reviews.
1 Introduction
Interactive Annotation Learning is a supervised ap-
proach to learning annotations with the goal of min-
imizing the total annotation cost. In this work, we
demonstrate that with additional supervision per ex-
ample, such as distinguishing discriminant features,
same performance can be achieved with less anno-
tated data. Supervision for simple features has been
explored in the literature (Raghavan et al, 2006;
Druck et al, 2008; Haghighi and Klein, 2006). In
this work, we propose an approach that seeks super-
vision from the user on structured features.
Features that capture the linguistic structure in
text such as n-grams and syntactic patterns, referred
to as structured features in this work, have been
found to be useful for supervised learning of annota-
tions. For example, Pradhan et al (2004) show that
using features like syntactic path from constituent
to predicate improves performance of a semantic
parser. However, often such features are ?hand-
crafted? by domain experts and do not generalize to
other tasks and domains. In this work, we propose
a general graph representation for automatically ex-
tracting structured features from tokens and prior an-
notations such as part of speech, dependency triples,
etc. Gamon (2004) shows that an approach using
a large set of structured features and a feature selec-
tion procedure performs better than an approach that
uses a few ?handcrafted? features. Our hypothesis
is that structured features are important for super-
vised annotation learning and can be automatically
derived from tokens and prior annotations. We test
our hypothesis and present our results for opinion
mining from product reviews.
Deriving features from the annotation graph gives
us a large number of very sparse features. Fea-
ture selection based on class association scores such
as mutual information and chi-square have often
been used to identify the most discriminant features
(Manning et al, 2008). However, these scores are
calculated from labeled data and they are not very
meaningful when the dataset is small. Supervised
feature selection, i.e. asking the user to vote for the
most discriminant features, has been used as an al-
ternative when the training dataset is small. Ragha-
van et al (2006) and Druck et al (2008) seek feed-
back on unigram features from the user for docu-
ment classification tasks. Haghighi and Klein (2006)
ask the user to suggest a few prototypes (examples)
for each class and use those as features. These ap-
proaches ask the annotators to identify globally rel-
55
evant features, but certain features are difficult to
vote on without the context and may take on very
different meanings in different contexts. Also, all
these approaches have been demonstrated for uni-
gram features and it is not clear how they can be
extended straightforwardly to structured features.
We propose an indirect approach to interactive
feature selection that makes use of highlighted ra-
tionales from the user. A rationale (Zaidan et al,
2007) is the span of text a user highlights in support
of his/her annotation. Rationales also allow us to
seek feedback on features in context. Our hypothe-
sis is that with rationales, we can achieve same per-
formance with lower annotation cost and we demon-
strate this for opinion mining from movie reviews.
In Section 2, we describe the annotation graph
representation and motivate the use of structured
features with results on learning opinions from prod-
uct reviews. In Section 3, we show how rationales
can be used for identifying the most discriminant
features for opinion classification with less training
data. We then list the conclusions we can draw from
this work, followed by suggestions for future work.
2 Learning with Structured Features
In this section, we demonstrate that structured fea-
tures help in improving performance and propose a
formal graph representation for deriving these fea-
tures automatically.
2.1 Opinions and Structured Features
Unigram features such as tokens are not sufficient
for recognizing all kinds of opinions. For example,
a unigram feature good may seem useful for identi-
fying opinions, however, consider the following two
comments in a review: 1) This camera has good fea-
tures and 2) I did a good month?s worth of research
before buying this camera. In the first example,
the unigram good is a useful feature. However, in
the second example, good is not complementing the
camera and hence will mislead the classifier. Struc-
tured features such as part-of-speech, dependency
relations etc. are needed to capture the language
structure that unigram features fail to capture.
2.2 Annotation Graph and Features
We define the annotation graph as a quadruple: G =
(N,E,?, ?), where N is the set of nodes, E is the
set of edges E ? N ? N , ? = ?N ? ?E is a
set of labels for nodes and edges. ? is the label-
ing function ? : N ? E ? ?, that assigns labels to
nodes and edges. In this work, we define the set of
labels for nodes, ?N as tokens, part of speech and
dependency annotations and set of labels for edges,
?E as relations, ?E = {leftOf, parentOf, restricts}.
The leftOf relation is defined between two adjacent
nodes. The parentOf relation is defined between the
dependency type and its attributes. For example, for
the dependency triple ?nsubj perfect camera?, there
is a parentOf relation between the dependency type
?nsubj? and tokens ?perfect? and ?camera?. The re-
stricts relation exists between two nodes a and b if
their textual spans overlap completely and a restricts
how b is interpreted. For a word with multiple senses
the restricts relation between the word and its part of
speech, restricts the way the word is interpreted, by
capturing the sense of the word in the given context.
The Stanford POS tagger (Toutanova and Manning,
2000) and the Stanford parser (Klein and Manning,
2003) were used to produce the part of speech and
dependency annotations.
Features are defined as subgraphs, G? =
(N ?, E?,??, ??) in the annotation graph G, such that
N ? ? N ,E? ? N ??N ? andE? ? E, ?? = ??N???E
where ??N ? ?N and ??E ? ?E and ?? : N ??E? ???. For a bag of words approach that only uses to-
kens as features, ??N = T , where T is the token
vocabulary and E = ? and ?E = ? (where ? is the
null set). We define the degree of a feature subgraph
as the number of edges it contains. For example, the
unigram features are the feature subgraphs with no
edges i.e. degree = 0. Degree? 1 features are the
feature subgraphs with two nodes and an edge. In
this paper, we present results for feature subgraphs
with degree = 0 and degree = 1.
Figure 1 shows the partial annotation graph for
two comments discussed above. The feature sub-
graph that captures the opinion expressed in 1(a),
can be described in simple words as ?camera has
features that are good?. This kind of subject-object
relationship with the same verb, between the ?cam-
era? and what?s being modified by ?good?, is not
present in the second example (1(b)). A slight modi-
fication of 1(b), I did a month?s worth of research be-
fore buying this good camera does express an opin-
ion about the camera. A bag of words approach that
uses only unigram features will not be able to differ-
56
entiate between these two examples; structured fea-
tures like dependency relation subgraphs can capture
this linguistic distinction between the two examples.
P24:amod
[16,29]
P23:JJ
[16,20]
P22:dobj
[12,29]
P21:nsubj
[5,15]
restricts
parentOf
parentOf
parentOf
(a)
(b)
Figure 1: The figure shows partial annotation graphs for two examples.
Only some of the nodes and edges are shown for clarity. Spans of nodes
in brackets are the character spans.
2.3 Experiments and Results
The dataset we used is a collection of 244 Amazon?s
customer reviews (2962 comments) for five products
(Hu and Liu, 2004). A review comment is annotated
as an opinion if it expresses an opinion about an as-
pect of the product and the aspect is explicitly men-
tioned in the sentence. We performed 10-fold cross
validation (CV) using the Support Vector Machine
(SVM) classifier in MinorThird (Cohen, 2004) with
the default linear kernel and chi-square feature se-
lection to select the top 5000 features. As can be
seen in Table 1, an approach using degree ? 0 fea-
tures, i.e. unigrams, part of speech and dependency
triples together, outperforms using any of those fea-
tures alone and this difference is significant. Us-
ing degree ? 1 features with two nodes and an
edge improves performance further. However, using
degree?0 features in addition to degree?1 features
does not improve performance. This suggests that
when using higher degree features, we may leave out
the features with lower degree that they subsume.
Features Avg F1 Outperforms
unigram [uni] 65.74 pos,dep
pos-unigram [pos] 64 dep
dependency [dep] 63.18 -
degree-0 [deg-0] 67.77 uni,pos,dep
degree-1 [deg-1] 70.56 uni,pos,dep,deg-0, deg-*
(deg-0 + deg-1) [deg-*] 70.12 uni,pos,dep,deg-0
Table 1: The table reports the F-measure scores averaged over ten cross
validation folds. The value in bold in the Avg F1 column is the best
performing feature combination. For each feature combination in the
row, outperforms column lists the feature combinations it outperforms,
with significant differences highlighted in bold (paired t-test with p <
0.05 considered significant).
3 Rationales & Indirect Feature voting
We propose an indirect feature voting approach that
uses user-highlighted rationales to identify the most
discriminant features. We present our results on
Movie Review data annotated with rationales.
3.1 Data and Experimental Setup
The data set by Pang and Lee (2004) consists of
2000 movie reviews (1000-pos, 1000-neg) from the
IMDb review archive. Zaidan et al (2007) provide
rationales for 1800 reviews (900-pos, 900-neg). The
annotation guidelines for marking rationales are de-
scribed in (Zaidan et al, 2007). An example of a
rationale is: ?the movie is so badly put together
that even the most casual viewer may notice the mis-
erable pacing and stray plot threads?. For a test
dataset of 200 reviews, randomly selected from 1800
reviews, we varied the training data size from 50 to
500 reviews, adding 50 reviews at a time. Training
examples were randomly selected from the remain-
ing 1600 reviews. During testing, information about
rationales is not used.
We used tokens1, part of speech and dependency
triples as features. We used the KStem stemmer
(Krovetz, 1993) to stem the token features. In or-
der to compare the approaches at their best perform-
ing feature configuration, we varied the total num-
ber of features used, choosing from the set: {1000,
2000, 5000, 10000, 50000}. We used chi-square
feature selection (Manning et al, 2008) and the
SVM learner with default settings from the Minor-
third package (Cohen, 2004) for these experiments.
We compare the following approaches:
Base Training Dataset (BTD): We train a model
from the labeled data with no feature voting.
1filtering the stop words using the stop word list: http:
//www.cs.cmu.edu/?shilpaa/stop-words-ial-movie.
txt
57
Rationale annotated Training Dataset (RTD):
We experimented with two different settings for in-
direct feature voting: 1) only using features that
overlap with rationales (RTD(1, 0)); 2) features
from rationales weighted twice as much as features
from other parts of the text (RTD(2, 1)). In general,
R(i, j) describes an experimental condition where
features from rationales are weighted i times and
other features are weighted j times. In Minorthird,
weighing a feature two times more than other fea-
tures is equivalent to that feature occurring twice as
much.
Oracle voted Training Data (OTD): In order to
compare indirect feature voting to direct voting on
features, we simulate the user?s vote on the features
with class association scores from a large dataset
(all 1600 documents used for selecting training doc-
uments). This is based on the assumption that the
class association scores, such as chi-square, from a
large dataset can be used as a reliable discriminator
of the most relevant features. This approach of sim-
ulating the oracle with large amount of labeled data
has been used previously in feature voting (Ragha-
van et al, 2006).
3.2 Results and Discussion
In Table 2, we present the accuracy results for the
four approaches described in the previous section.
We compare the best performing feature configura-
tions for three approaches - BTD, RTD(1, 0) and
RTD (2,0). As can be seen, RTD(1, 0) always per-
forms better than BTD. As expected, improvement
with rationales is greater and it is significant when
the training dataset is small. The performance of
all approaches converge as the training data size in-
creases and hence we only present results up to train-
ing dataset size of 500 examples in this paper.
Since our goal is to evaluate the use of rationales
independently of how many features the model uses,
we also compared the four approaches in terms of
the accuracy averaged over five feature configura-
tions. Due to space constraints, we do not include
the table of results. On average RTD(1, 0) signif-
icantly outperforms BTD when the total training
dataset is less than 350 examples. When the train-
ing data has fewer than 400 examples, RTD(1, 0)
also significantly outperforms RTD(2, 1).
OTD with simulated user is an approximate up-
#Ex Approach Number of Features1000 2000 5000 10000 50000
50
OTD 67.63 66.30 62.90 52.17 55.03
BTD 58.10 57.47 52.67 51.80 55.03
RTD(1,0)* 55.43 55.93 61.63 61.63 61.63
RTD(2,1) 57.77 57.53 52.73 52.30 56.33
100
OTD 71.97 71.07 70.27 69.37 64.33
BTD 64.17 64.43 62.70 56.63 64.37
RTD(1,0)* 65.43 63.27 65.13 67.23 67.23
RTD(2,1) 64.27 63.93 62.47 56.10 63.77
150
OTD 73.83 74.83 74.20 74.00 63.83
BTD 66.17 67.77 68.60 64.33 60.47
RTD(1,0)* 69.30 68.30 67.27 71.30 71.87
RTD(2,1) 68.00 67.07 68.43 63.57 58.90
200
OTD 74.83 75.87 75.70 75.10 56.97
BTD 71.63 71.37 72.57 71.53 58.90
RTD(1,0) 72.23 72.63 71.63 73.80 73.93
RTD(2,1) 71.20 71.10 73.03 70.77 57.87
250
OTD 75.63 76.90 77.70 77.67 62.20
BTD 72.60 73.57 74.73 75.20 58.93
RTD(1,0) 73.00 73.57 73.57 74.70 76.70
RTD(2,1) 72.87 73.90 74.63 75.40 57.43
300
OTD 76.57 77.67 78.93 78.43 68.17
BTD 72.97 74.13 74.93 76.57 63.83
RTD(1,0) 74.43 74.83 74.67 74.73 77.67
RTD(2,1) 72.67 74.53 74.37 76.53 61.30
350
OTD 76.47 78.20 80.20 79.80 71.73
BTD 74.43 74.30 74.73 77.27 66.80
RTD(1,0) 75.07 76.20 75.80 75.20 78.53
RTD(2,1) 74.63 75.70 74.80 78.23 64.93
400
OTD 77.97 78.93 80.53 80.60 75.27
BTD 75.83 76.77 76.47 78.93 70.63
RTD(1,0) 75.17 76.40 75.83 76.00 79.23
RTD(2,1) 75.73 76.07 76.80 78.50 68.20
450
OTD 77.67 79.20 80.57 80.73 77.13
BTD 75.73 76.80 77.80 78.80 74.37
RTD(1,0)* 74.83 76.50 76.23 76.47 80.40
RTD(2,1) 75.87 76.87 77.87 78.87 71.80
500
OTD 78.03 80.10 81.27 81.67 79.87
BTD 75.27 77.33 79.37 80.30 75.73
RTD(1,0) 75.77 77.63 77.47 77.27 81.10
RTD(2,1) 75.83 77.47 79.50 79.70 74.50
Table 2: Accuracy performance for four approaches, five feature con-
figurations and increasing training dataset size. Accuracy reported is
averaged over five random selection of training documents for three ran-
domly selected test datasets. The numbers in bold in a row represents
the best performing feature configuration for a given approach and train-
ing dataset size. The approach in bold represents the best performing
approach among BTD, RTD(1, 0) and RTD(2, 1) for a given train-
ing dataset size. ?*? indicates significant improvement in performance
over BTD (paired t-test with p < 0.05 considered significant).
per bound for rationale based approaches. It tells
us how far we are from direct supervision on struc-
tured features. On average, OTD significantly out-
performed RTD(1, 0) for training data size of 100,
150, 400, 450 and 500 examples but not always.
As can be seen from Table 2, difference between
OTD and RTD(1, 0) reduces with more training
data, since with more data and hence more rationales
we get better feature coverage.
Results presented here show that for a given train-
ing dataset, we can boost the performance by ask-
58
ing the user to label rationales. However, there is
an additional cost associated with the rationales. It
is important to evaluate how much total annotation
cost rationales can save us while achieving the de-
sired performance. In Figure 2, we compare the
number of training examples an approach needs to
achieve a given level of performance. As can be
seen, RTD(1, 0) needs fewer training examples to
achieve the same performance as BTD. The differ-
ence is large initially when the total number of train-
ing examples is small (50 forRTD(1, 0) and 150 for
BTD to achieve a performance between 66? 67).
Figure 2: The Figure shows the number of examples needed by the
two approaches, RTD(1, 0) and BTD, to achieve an accuracy in the
given range.
Comparison with Zaidan et al (2007): Zaidan
et al (2007) conclude that using only features from
rationales performs worse than both: 1) using all the
features in the documents, and 2) using features that
do not overlap with the rationales. The results pre-
sented in this paper seem to contradict their results.
However, they only experimented with unigram fea-
tures and only one approach to using features from
rationales, RTD(1, 0) and not RTD(2, 1). In order
to compare our work directly with theirs, we exper-
imented with an equivalent set of unigram features.
In Table 3, we present the results using same num-
ber of total features (17744) as Zaidan et al (2007).
As can be seen from the table, when only unigram
features are used,RTD(2, 1) outperformsBTD but
RTD(1, 0) performs worse than BTD. Thus, our
results are consistent with (Zaidan et al, 2007) i.e.
using unigram features only from the rationales does
not boost performance.
From Table 3, we also analyze the improvement
in performance when part of speech and depen-
dency features are used in addition to the unigram
features i.e. using all degree ? 0 subgraph fea-
#Ex Approach uni uni-pos uni-pos-dep
100
OTD 68.6 68.8 61.6
BTD 68.6 68.8 52.2
RTD(1,0) 68.2 68.1 69.0*
RTD(2,0) 70.0 67.0 51.7
200
OTD 73.6 73.8 75.3
BTD 73.6 73.8 67.1
RTD(1,0) 73.9 73.2 73.9*
RTD(2,0) 75.3* 70.3 65.2
300
OTD 76.2 76.1 79.1
BTD 76.2 76.1 73.7
RTD(1,0) 75.0 74.9 77.1*
RTD(2,0) 77.5* 73.3 74.8
400
OTD 77.4 76.8 79.9
BTD 77.4 76.8 76.2
RTD(1,0) 75.9 75.9 77.0
RTD(2,0) 78.0 74.7 77.7*
500
OTD 78.1 78.1 80.0
BTD 78.1 78.1 78.4
RTD(1,0) 76.3 76.2 77.6
RTD(2,0) 78.2 75.4 79.0
Table 3: The Table reports accuracy for four approaches in a setting
similar to (Zaidan et al, 2007). Accuracy reported is averaged over ten
random selection of training documents for two randomly selected test
datasets.The numbers in bold are the best among BTD, RTD(1, 0),
RTD(2, 1) for a given feature combination. ?*? highlights the signif-
icant improvement in performance over BTD (using paired t-test, with
p < 0.05 considered significant).
tures. For RTD(1, 0), adding these features im-
proves performance for all data sizes with signifi-
cant improvement for dataset size of 300 and 500 ex-
amples. RTD(1, 0) also significantly outperforms
BTD when all three features are used. For direct
voting on features (OTD), a significant improve-
ment with these structured features is seen when the
training dataset size is greater than 200 examples.
For BTD and RTD(2, 1) approaches, there is no
significant improvement with these additional fea-
tures. In the future, we plan to investigate further
the benefit of using higher degree subgraph features
for opinion mining from the movie review data.
Comparing ranking of features:We also com-
pared the features that the rationales capture to what
the oracle will vote for as the most relevant features.
Features are ranked based on chi-square scores used
in feature selection. We compare the ranked list of
features from RTD(1, 0), BTD and OTD and use
a weighted F-measure score for evaluating the top
100 ranked features by each approach. This measure
is inspired by the Pyramid measure used in Summa-
rization (Nenkova and Passonneau, 2004). Instead
of using counts in calculating F-measure, we used
the chi-square score assigned to the features by the
oracle dataset, in order to give more weight to the
more discriminant features. As can be seen from
59
Table 4, RTD(1, 0) outperforms BTD in captur-
ing the important features when the datasize set is
small (< 300) and this difference is significant. Be-
yond 300 examples, as the data size increases,BTD
outperforms RTD(1, 0). This implies that the ra-
tionales alone are able to capture the most relevant
features when the dataset is small.
100 200 300 400 500 600 700
RO 47.70 53.80 57.68 59.54 62.13 60.86 61.56
TO 31.22 44.43 52.98 60.57 64.61 67.10 70.39
Table 4: Weighted F-measure performance comparison of ranked list
of features from RTD(1, 0) & OTD(RO) and BTD & OTD(TO).
Results are averaged over ten random selections of the training data for
a randomly selected test dataset. Significant differences are highlighted
in bold (paired t-test with p < 0.05 considered significant).
4 Conclusion and Future Work
In this work, we demonstrated that using structured
features boosts performance of supervised annota-
tion learning. We proposed a formal annotation
graph representation that can be used to derive these
features automatically. However, the space of pos-
sible feature subgraphs can grow very large with
more prior annotations. Standard feature selection
techniques based on class association scores are less
effective when the dataset is small. Feature voting
from the user for identifying the relevant features
is limited to simple features. Supplementary input
from the user in terms of highlighted rationales can
be used instead to prune the feature space. The pro-
posed approach is general and can be applied to a
variety of problems and features.
In this work, we presented our results with
degree ? 0 and degree ? 1 feature subgraphs.
We will extend our algorithm to automatically ex-
tract higher degree features from the annotation
graph. For the rationale annotated training data
(RTD(i, j)), we experimented with two possible
values for i and j. We aim to learn these weights
empirically using a held out dataset. Rationales are
associated with an additional cost per example and
hence two approaches, with and without the ratio-
nales, are not directly comparable in terms of the
number of examples. In the future, we will conduct
an annotation experiment with real users to evaluate
the usefulness of rationales in terms of clock time.
Acknowledgments
We would like to thank Dr. Carolyn P. Rose for
her help with statistical analysis of the results. We
would also like to thank all the anonymous review-
ers for their helpful comments.
References
Cohen W. Minorthird: Methods for Identifying Names
and Ontological Relations in Text using Heuristics for
Inducing Regularities from Data. 2004. (http://
minorthird.sourceforge.net/).
Druck G., Mann G. and McCallum A. Learning from la-
beled features using generalized expectation criteria.
In Proceedings of the ACM SIGIR, 2008.
Michael Gamon. 2004. Sentiment classification on cus-
tomer feedback data: noisy data, large feature vectors,
and the role of linguistic analysis. In Proceedings of
COLING, 2005.
Haghighi A. and Klein D. Prototype-driven learning for
sequence models. In Proceedings of the NAACL HLT
2006.
Minqing Hu and Bing Liu. 2004. Mining and Summariz-
ing Customer Reviews. In Proc. of the ACM SIGKDD
International Conference on Knowledge Discovery &
Data Mining.
Klein D. and Manning C. Accurate Unlexicalized Pars-
ing. In Proceedings of ACL 2003.
Krovetz R. Viewing Morphology as an Infer-
ence Process. http://ciir.cs.umass.edu/
pubfiles/ir-35.pdf
Manning C., Raghavan P. and Schu?tze H. Introduction to
Information Retrieval. Cambridge University Press.
2008.
Nenkova A. and Passonneau R. Evaluating Content Se-
lection In Summarization: The Pyramid Method. In
Proceedings of HLT-NAACL 2004.
Pang B. and Lee L. ?A Sentimental Education: Sen-
timent Analysis Using Subjectivity Summarization
Based on Minimum Cuts? In Proceedings of the ACL,
2004.
Sameer S. Pradhan, Wayne Ward, Kadri Hacioglu, James
H. Martin, Daniel Jurafsky. 2004. Shallow Seman-
tic Parsing using Support Vector Machines. In Pro-
ceedings of HLT/NAACL-2004,Boston, MA, May 2-
7, 2004
Raghavan H., Madani O. and Jones R. Active Learning
with Feedback on Both Features and Instances. Jour-
nal of Machine Learning Research, 2006.
Toutanova K. and Manning C. Enriching the Knowledge
Sources Used in a Maximum Entropy Part-of-Speech
Tagger. In Proceedings of EMNLP/VLC-2000.
Zaidan O., Eisner J. and Piatko C. Using ?annotator ra-
tionales? to improve machine learning for text catego-
rization. In Proceedings of NAACL-HLT 2007.
Zaidan O. and Eisner J. Modeling Annotators: A Genera-
tive Approach to Learning from Annotator Rationales.
In Proceedings of EMNLP 2008.
60
Resource Analysis for Question Answering
Lucian Vlad Lita
Carnegie Mellon University
llita@cs.cmu.edu
Warren A. Hunt
Carnegie Mellon University
whunt@andrew.cmu.edu
Eric Nyberg
Carnegie Mellon University
ehn@cs.cmu.edu
Abstract
This paper attempts to analyze and bound the utility
of various structured and unstructured resources in
Question Answering, independent of a specific sys-
tem or component. We quantify the degree to which
gazetteers, web resources, encyclopedia, web doc-
uments and web-based query expansion can help
Question Answering in general and specific ques-
tion types in particular. Depending on which re-
sources are used, the QA task may shift from com-
plex answer-finding mechanisms to simpler data ex-
traction methods followed by answer re-mapping in
local documents.
1 Introduction
During recent years the Question Answering (QA)
field has undergone considerable changes: question
types have diversified, question complexity has in-
creased, and evaluations have become more stan-
dardized - as reflected by the TREC QA track
(Voorhees, 2003). Some recent approaches have
tapped into external data sources such as the Web,
encyclopedias, databases in order to find answer
candidates, which may then be located in the spe-
cific corpus being searched (Dumais et al, 2002; Xu
et al, 2003). As systems improve, the availability
of rich resources will be increasingly critical to QA
performance. While on-line resources such as the
Web, WordNet, gazetteers, and encyclopedias are
becoming more prevalent, no system-independent
study has quantified their impact on the QA task.
This paper focuses on several resources and their
inherent potential to provide answers, without con-
centrating on a particular QA system or component.
The goal is to quantify and bound the potential im-
pact of these resources on the QA process.
2 Related Work
More and more QA systems are using the Web as
a resource. Since the Web is orders of magni-
tude larger than local corpora, redundancy of an-
swers and supporting passages allows systems to
produce more correct, confident answers (Clarke et
al., 2001; Dumais et al, 2002). (Lin, 2002) presents
two different approaches to using the Web: access-
ing the structured data and mining the unstructured
data. Due to their complementary nature of these
approaches, hybrid systems are likely to perform
better (Lin and Katz, 2003).
Definitional questions (?What is X??, ?Who
is X??) are especially compatible with structured
resources such as gazetteers and encyclopedias.
The top performing definitional systems (Xu et
al., 2003) at TREC extract kernel facts similar to
a question profile built using structured and semi-
structured resources: WordNet (Miller et al, 1990),
Merriam-Webster dictionary www.m-w.com), the
Columbia Encyclopedia (www.bartleby.com),
Wikipedia (www.wikipedia.com), a biog-
raphy dictionary (www.s9.com) and Google
(www.google.com).
3 Approach
For the purpose of this paper, resources consist of
structured and semi-structured knowledge, such as
the Web, web search engines, gazetteers, and ency-
clopedias. Although many QA systems incorporate
or access such resources, few systems quantify in-
dividual resource impact on their performance and
little work has been done to estimate bounds on re-
source impact to Question Answering. Independent
of a specific QA system, we quantify the degree to
which these resources are able to directly provide
answers to questions.
Experiments are performed on the 2,393 ques-
tions and the corresponding answer keys provided
through NIST (Voorhees, 2003) as part of the TREC
8 through TREC 12 evaluations.
4 Gazetteers
Although the Web consists of mostly unstructured
and loosely structured information, the available
structured data is a valuable resource for question
answering. Gazetteers in particular cover several
frequently-asked factoid question types, such as
?What is the population of X?? or ?What is the cap-
ital of Y??. The CIA World Factbook is a database
containing geographical, political, and economi-
cal profiles of all the countries in the world. We
also analyzed two additional data sources contain-
ing astronomy information (www.astronomy.com)
and detailed information about the fifty US states
(www.50states.com).
Since gazetteers provide up-to-date information,
some answers will differ from answers in local
corpora or the Web. Moreover, questions requir-
ing interval-type answers (e.g. ?How close is the
sun??) may not match answers from different
sources which are also correct. Gazetteers offer
high precision answers, but have limited recall since
they only cover a limited number of questions (See
Table 1).
CIA All
Q-Set #qtions R P R P
TREC8 200 4 100% 6 100%
TREC9 693 8 100% 22 79%
TREC10 500 14 100% 23 96%
TREC11 500 8 100% 20 100%
TREC12 500 2 100% 11 92%
Overall 2393 36 100% 82 91%
Table 1: Recall (R): TREC questions can be directly
answered directly by gazetteers - shown are results
for CIA Factbook and All gazetteers combined. Our
extractor precision is Precision (P).
5 WordNet
Wordnets and ontologies are very common re-
sources and are employed in a wide variety of di-
rect and indirect QA tasks, such as reasoning based
on axioms extracted from WordNet (Moldovan et
al., 2003), probabilistic inference using lexical rela-
tions for passage scoring (Paranjpe et al, 2003), and
answer filtering via WordNet constraints (Leidner et
al., 2003).
Q-Set #qtions All Gloss Syns Hyper
TREC 8 200 32 22 7 13
TREC 9 693 197 140 73 75
TREC 10 500 206 148 82 88
TREC 11 500 112 80 29 46
TREC 12 500 93 56 10 52
Overall 2393 641 446 201 268
Table 2: Number of questions answerable using
WordNet glosses (Gloss), synonyms (Syns), hyper-
nyms and hyponyms (Hyper), and all of them com-
bined All.
Table 2 shows an upper bound on how many
TREC questions could be answered directly using
WordNet as an answer source. Question terms and
phrases were extracted and looked up in WordNet
glosses, synonyms, hypernyms, and hyponyms. If
the answer key matched the relevant WordNet data,
then an answer was considered to be found. Since
some answers might occur coincidentally, we these
results to represent upper bounds on possible utility.
6 Structured Data Sources
Encyclopedias, dictionaries, and other web
databases are structured data sources that are often
employed in answering definitional questions (e.g.,
?What is X??, ?Who is X??). The top-performing
definitional systems at TREC (Xu et al, 2003)
extract kernel facts similar question profiles built
using structured and semi-structured resources:
WordNet (Miller et al, 1990), the Merriam-
Webster dictionary www.m-w.com), the Columbia
Encyclopedia (www.bartleby.com), Wikipedia
(www.wikipedia.com), a biography dictionary
(www.s9.com) and Google (www.google.com).
Table 3 shows a number of data sources and
their impact on answering TREC questions. N-
grams were extracted from each question and run
through Wikipedia and Google?s define operator
(which searches specialized dictionaries, definition
lists, glossaries, abbreviation lists etc). Table 3
show that TREC 10 and 11 questions benefit the
most from the use of an encyclopedia, since they
include many definitional questions. On the other
hand, since TREC 12 has fewer definitional ques-
tions and more procedural questions, it does not
benefit as much from Wikipedia or Google?s define
operator.
Q-Set #qtions WikiAll Wiki1st DefOp
TREC 8 200 56 5 30
TREC 9 693 297 49 71
TREC 10 500 225 45 34
TREC 11 500 155 19 23
TREC 12 500 124 12 27
Overall 2393 857 130 185
Table 3: The answer is found in a definition ex-
tracted from Wikipedia WikiAll, in the first defi-
nition extracted from Wikipedia Wiki1st, through
Google?s define operator DefOp.
7 Answer Type Coverage
To test coverage of different answer types, we em-
ployed the top level of the answer type hierarchy
used by the JAVELIN system (Nyberg et al, 2003).
The most frequent types are: definition (e.g. ?What
is viscosity??), person-bio (e.g. ?Who was La-
can??), object(e.g. ?Name the highest mountain.?),
process (e.g. ?How did Cleopatra die??), lexicon
(?What does CBS stand for??)temporal(e.g. ?When
is the first day of summer??), numeric (e.g. ?How
tall is Mount Everest??), location (e.g. ?Where is
Tokyo??), and proper-name (e.g. ?Who owns the
Raiders??).
AType #qtions WikiAll DefOp Gaz WN
object 1003 426 92 58 309
lexicon 50 25 3 0 26
defn 178 105 9 11 112
pers-bio 39 15 11 0 17
process 138 23 6 9 16
temporal 194 63 14 0 50
numeric 121 27 13 10 18
location 151 69 21 2 47
proper 231 76 10 0 32
Table 4: Coverage of TREC questions divided by
most common answer types.
Table 4 shows TREC question coverage broken
down by answer type. Due to temporal consistency,
numeric questions are not covered very well. Al-
though the process and object types are broad an-
swer types, the coverage is still reasonably good.
As expected, the definition and person-bio answer
types are covered well by these resources.
8 The Web as a Resource
An increasing number of QA systems are using the
web as a resource. Since the Web is orders of mag-
nitude larger than local corpora, answers occur fre-
quently in simple contexts, which is more conducive
to retrieval and extraction of correct, confident an-
swers (Clarke et al, 2001; Dumais et al, 2002;
Lin and Katz, 2003). The web has been employed
for pattern acquisition (Ravichandran et al, 2003),
document retrieval (Dumais et al, 2002), query ex-
pansion (Yang et al, 2003), structured information
extraction, and answer validation (Magnini et al,
2002) . Some of these approaches enhance exist-
ing QA systems, while others simplify the question
answering task, allowing a less complex approach
to find correct answers.
8.1 Web Documents
Instead of searching a local corpus, some QA sys-
tems retrieve relevant documents from the web (Xu
et al, 2003). Since the density of relevant web doc-
uments can be higher than the density of relevant
local documents, answer extraction may be more
successful from the web. For a TREC evaluation,
answers found on the web must also be mapped to
relevant documents in the local corpus.
0 10 20 30 40 50 60 70 80 90 100
0
100
200
300
400
500
600
700
800
900
1000
Web Retrieval Performance For QA
document rank
# 
qu
es
tio
ns
Correct Doc Density
First Correct Doc
Figure 1: Web retrieval: relevant document density
and rank of first relevant document.
In order to evaluate the impact of web docu-
ments on TREC questions, we performed an ex-
periment where simple queries were submitted to
a web search engine. The questions were to-
kenized and filtered using a standard stop word
list. The resulting keyword queries were used to
retrieve 100 documents through the Google API
(www.google.com/api). Documents containing the
full question, question number, references to TREC,
NIST, AQUAINT, Question Answering and similar
content were filtered out.
Figure 1 shows the density of documents contain-
ing a correct answer, as well as the rank of the first
document containing a correct answer. The sim-
ple word query retrieves a relevant document for
almost half of the questions. Note that for most
systems, the retrieval performance should be supe-
rior since queries are usually more refined and addi-
tional query expansion is performed. However, this
experiment provides an intuition and a very good
lower bound on the precision and density of current
web documents for the TREC QA task.
8.2 Web-Based Query Expansion
Several QA systems participating at TREC have
used search engines for query expansion (Yang et
al., 2003). The basic query expansion method
utilizes pseudo-relevance feedback (PRF) (Xu and
Croft, 1996). Content words are selected from ques-
tions and submitted as queries to a search engine.
The top n retrieved documents are selected, and k
terms or phrases are extracted according to an op-
timization criterion (e.g. term frequency, n-gram
frequency, average mutual information using cor-
pus statistics, etc). These k items are used in the
expanded query.
We experimented by using the top 5, 10, 15, 20,
0 5 10 15 20 25 30 35 40 45 50
100
200
300
400
500
600
700
800
900
1000
1100
Answer frequency using PRF
# PRF terms
# 
qu
es
tio
ns
Top 5 documents
Top 10 documents
Top 15 documents
Top 20 documents
Top 50 documents
Top 100 documents
Figure 2: Finding a correct answer in PRF expan-
sion terms - applied to 2183 questions for witch an-
swer keys exist.
50, and 100 documents retrieved via the Google API
for each question, and extracted the most frequent
fifty n-grams (up to trigrams). The goal was to de-
termine the quality of query expansion as measured
by the density of correct answers already present
in the expansion terms. Even without filtering n-
grams matching the expected answer type, simple
PRF produces the correct answer in the top n-grams
for more than half the questions. The best correct
answer density is achieved using PRF with only 20
web documents.
8.3 Conclusions
This paper quantifies the utility of well-known and
widely-used resources such as WordNet, encyclope-
dias, gazetteers and the Web on question answering.
The experiments presented in this paper represent
loose bounds on the direct use of these resources in
answering TREC questions. We reported the perfor-
mance of these resources on different TREC collec-
tions and on different question types. We also quan-
tified web retrieval performance, and confirmed that
the web contains a consistently high density of rel-
evant documents containing correct answers even
when simple queries are used. The paper also
shows that pseudo-relevance feedback alone using
web documents for query expansions can produce
a correct answer for fifty percent of the questions
examined.
9 Acknowledgements
This work was supported in part by the Advanced
Research and Development Activity (ARDA)?s
Advanced Question Answering for Intelligence
(AQUAINT) Program.
References
C.L.A. Clarke, G.V. Cormack, and T.R. Lynam.
2001. Exploiting redundancy in question answer-
ing. SIGIR.
S. Dumais, M. Banko, E. Brill, J. Lin, and A. Ng.
2002. Web question answering: Is more always
better? SIGIR.
J. Leidner, J. Bos, T. Dalmas, J. Curran, S. Clark,
C. Bannard, B. Webber, and M. Steedman. 2003.
Qed: The edinburgh trec-2003 question answer-
ing system. TREC.
J. Lin and B. Katz. 2003. Question answering from
the web using knowledge annotation and knowl-
edge mining techniques. CIKM.
J. Lin. 2002. The web as a resource for question
answering: Perspectives and challenges. LREC.
B. Magnini, M. Negri, R. Pervete, and H. Tanev.
2002. Is it the right answer? exploiting web re-
dundancy for answer validation. ACL.
G.A. Miller, R. Beckwith, C. Fellbaum, D. Gross,
and K. Miller. 1990. Five papers on wordnet. In-
ternational Journal of Lexicography.
D. Moldovan, D. Clark, S. Harabagiu, and S. Maio-
rano. 2003. Cogex: A logic prover for question
answering. ACL.
E. Nyberg, T. Mitamura, J. Callan, J. Carbonell,
R. Frederking, K. Collins-Thompson, L. Hiyaku-
moto, Y. Huang, C. Huttenhower, S. Judy, J. Ko,
A. Kupsc, L.V. Lita, V. Pedro, D. Svoboda, and
B. Vand Durme. 2003. A multi strategy approach
with dynamic planning. TREC.
D. Paranjpe, G. Ramakrishnan, and S. Srinivasan.
2003. Passage scoring for question answering via
bayesian inference on lexical relations. TREC.
D. Ravichandran, A. Ittycheriah, and S. Roukos.
2003. Automatic derivation of surface text pat-
terns for a maximum entropy based question an-
swering system. HLT-NAACL.
E.M. Voorhees. 2003. Overview of the trec 2003
question answering track. TREC.
J. Xu and W.B. Croft. 1996. Query expansion using
local and global analysis. SIGIR.
J. Xu, A. Licuanan, and R. Weischedel. 2003. Trec
2003 qa at bbn: Answering definitional ques-
tions. TREC.
H. Yang, T.S. Chua, S. Wang, and C.K. Koh. 2003.
Structured use of external knowledge for event-
based open domain question answering. SIGIR.
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 784?791,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Language-independent Probabilistic Answer Ranking
for Question Answering
Jeongwoo Ko, Teruko Mitamura, Eric Nyberg
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
{jko, teruko, ehn}@cs.cmu.edu
Abstract
This paper presents a language-independent
probabilistic answer ranking framework for
question answering. The framework esti-
mates the probability of an individual an-
swer candidate given the degree of answer
relevance and the amount of supporting evi-
dence provided in the set of answer candi-
dates for the question. Our approach was
evaluated by comparing the candidate an-
swer sets generated by Chinese and Japanese
answer extractors with the re-ranked answer
sets produced by the answer ranking frame-
work. Empirical results from testing on NT-
CIR factoid questions show a 40% perfor-
mance improvement in Chinese answer se-
lection and a 45% improvement in Japanese
answer selection.
1 Introduction
Question answering (QA) systems aim at find-
ing precise answers to natural language questions
from large document collections. Typical QA sys-
tems (Prager et al, 2000; Clarke et al, 2001;
Harabagiu et al, 2000) adopt a pipeline architec-
ture that incorporates four major steps: (1) question
analysis, (2) document retrieval, (3) answer extrac-
tion and (4) answer selection. Question analysis is
a process which analyzes a question and produces a
list of keywords. Document retrieval is a step that
searches for relevant documents or passages. An-
swer extraction extracts a list of answer candidates
from the retrieved documents. Answer selection is a
process which pinpoints correct answer(s) from the
extracted candidate answers.
Since the first three steps in the QA pipeline may
produce erroneous outputs, the final answer selec-
tion step often entails identifying correct answer(s)
amongst many incorrect ones. For example, given
the question ?Which Chinese city has the largest
number of foreign financial companies??, the an-
swer extraction component produces a ranked list of
five answer candidates: Beijing (AP880603-0268)1,
Hong Kong (WSJ920110-0013), Shanghai (FBIS3-
58), Taiwan (FT942-2016) and Shanghai (FBIS3-
45320). Due to imprecision in answer extraction,
an incorrect answer (?Beijing?) can be ranked in
the first position, and the correct answer (?Shang-
hai?) was extracted from two different documents
and ranked in the third and the fifth positions. In or-
der to rank ?Shanghai? in the top position, we have
to address two interesting challenges:
? Answer Similarity. How do we exploit simi-
larity among answer candidates? For example,
when the candidates list contains redundant an-
swers (e.g., ?Shanghai? as above) or several an-
swers which represent a single instance (e.g.
?U.S.A.? and ?the United States?), how much
should we boost the rank of the redundant an-
swers?
? Answer Relevance. How do we identify
relevant answer(s) amongst irrelevant ones?
This task may involve searching for evi-
dence of a relationship between the answer
1Answer candidates are shown with the identifier of the
TREC document where they were found.
784
and the answer type or a question key-
word. For example, we might wish to query
a knowledge base to determine if ?Shang-
hai? is a city (IS-A(Shanghai, city)),
or to determine if Shanghai is in China
(IS-IN(Shanghai, China)).
The first challenge is to exploit redundancy in the
set of answer candidates. As answer candidates are
extracted from different documents, they may con-
tain identical, similar or complementary text snip-
pets. For example, ?U.S.? can appear as ?United
States? or ?USA? in different documents. It is im-
portant to detect redundant information and boost
answer confidence, especially for list questions that
require a set of unique answers. One approach is
to perform answer clustering (Nyberg et al, 2002;
Jijkoun et al, 2006). However, the use of cluster-
ing raises additional questions: how to calculate the
score of the clustered answers, and how to select the
cluster label.
To address the second question, several answer
selection approaches have used external knowledge
resources such as WordNet, CYC and gazetteers for
answer validation or answer reranking. Answer can-
didates are either removed or discounted if they are
not of the expected answer type (Xu et al, 2002;
Moldovan et al, 2003; Chu-Carroll et al, 2003;
Echihabi et al, 2004). The Web also has been used
for answer reranking by exploiting search engine re-
sults produced by queries containing the answer can-
didate and question keywords (Magnini et al, 2002).
This approach has been used in various languages
for answer validation. Wikipedia?s structured in-
formation was used for Spanish answer type check-
ing (Buscaldi and Rosso, 2006).
Although many QA systems have incorporated in-
dividual features and/or resources for answer selec-
tion in a single language, there has been little re-
search on a generalized probabilistic framework that
supports answer ranking in multiple languages using
any answer relevance and answer similarity features
that are appropriate for the language in question.
In this paper, we describe a probabilistic answer
ranking framework for multiple languages. The
framework uses logistic regression to estimate the
probability that an answer candidate is correct given
multiple answer relevance features and answer sim-
ilarity features. An existing framework which was
originally developed for English (Ko et al, 2007)
was extended for Chinese and Japanese answer
ranking by incorporating language-specific features.
Empirical results on NTCIR Chinese and Japanese
factoid questions show that the framework signifi-
cantly improved answer selection performance; Chi-
nese performance improved by 40% over the base-
line, and Japanese performance improved by 45%
over the baseline.
The remainder of this paper is organized as fol-
lows: Section 2 contains an overview of the answer
ranking task. Section 3 summarizes the answer rank-
ing framework. In Section 4, we explain how we
extended the framework by incorporating language-
specific features. Section 5 describes the experimen-
tal methodology and results. Finally, Section 6 con-
cludes with suggestions for future research.
2 Answer Ranking Task
The relevance of an answer to a question can be es-
timated by the probability P(correct(Ai) |Ai, Q),
where Q is a question and Ai is an answer can-
didate. To exploit answer similarity, we estimate
the probability P (correct(Ai) |Ai, Aj), where Aj
is similar to Ai. Since both probabilities influence
overall answer ranking performance, it is important
to combine them in a unified framework and es-
timate the probability of an answer candidate as:
P (correct(Ai)|Q,A1, ..., An).
The estimated probability is used to rank answer
candidates and select final answers from the list. For
factoid questions, the top answer is selected as a fi-
nal answer to the question. In addition, we can use
the estimated probability to classify incorrect an-
swers: if the probability of an answer candidate is
lower than 0.5, it is considered to be a wrong answer
and is filtered out of the answer list. This is useful
in deciding whether or not a valid answer to a ques-
tion exists in a given corpus (Voorhees, 2002). The
estimated probability can also be used in conjunc-
tion with a cutoff threshold when selecting multiple
answers to list questions.
3 Answer Ranking Framework
This section summarizes our answer ranking frame-
work, originally developed for English answers (Ko
785
P (correct(Ai)|Q,A1, ..., An)
? P (correct(Ai)|rel1(Ai), ..., relK1(Ai), sim1(Ai), ..., simK2(Ai))
=
exp(?0 +
K1?
k=1
?krelk(Ai) +
K2?
k=1
?ksimk(Ai))
1 + exp(?0 +
K1?
k=1
?krelk(Ai) +
K2?
k=1
?ksimk(Ai))
where, simk(Ai) =
N?
j=1(j 6=i)
sim?k(Ai, Aj).
Figure 1: Estimating correctness of an answer candidate given a question and a set of answer candidates
et al, 2007). The model uses logistic regression
to estimate the probability of an answer candidate
(Figure 1). Each relk(Ai) is a feature function used
to produce an answer relevance score for an an-
swer candidate Ai. Each sim?k(Ai, Aj) is a similar-
ity function used to calculate an answer similarity
between Ai and Aj . K1 and K2 are the number of
answer relevance and answer similarity features, re-
spectively. N is the number of answer candidates.
To incorporate multiple similarity features, each
simk(Ai) is obtained from an individual similarity
metric, sim?k(Ai, Aj). For example, if Levenshtein
distance is used as one similarity metric, simk(Ai)
is calculated by summing N-1 Levenshtein distances
between one answer candidate and all other candi-
dates.
The parameters ?, ?, ? were estimated from train-
ing data by maximizing the log likelihood. We used
the Quasi-Newton algorithm (Minka, 2003) for pa-
rameter estimation.
Multiple features were used to generate answer
relevance scores and answer similarity scores; these
are discussed below.
3.1 Answer Relevance Features
Answer relevance features can be classified into
knowledge-based features or data-driven features.
1) Knowledge-based features
Gazetteers: Gazetteers provide geographic infor-
mation, which allows us to identify strings as in-
stances of countries, their cities, continents, capitals,
etc. For answer ranking, three gazetteer resources
were used: the Tipster Gazetteer, the CIA World
Factbook and information about the US states pro-
vided by 50states.com. These resources were used
to assign an answer relevance score between -1 and
1 to each candidate. For example, given the question
?Which city in China has the largest number of for-
eign financial companies??, the candidate ?Shang-
hai? receives a score of 0.5 because it is a city in the
gazetteers. But ?Taiwan? receives a score of -1.0 be-
cause it is not a city in the gazetteers. A score of 0
means the gazetteers did not contribute to the answer
selection process for that candidate.
Ontology: Ontologies such as WordNet contain
information about relationships between words and
general meaning types (synsets, semantic categories,
etc.). WordNet was used to identify answer rele-
vance in a manner analogous to the use of gazetteers.
For example, given the question ?Who wrote the
book ?Song of Solomon???, the candidate ?Mark
Twain? receives a score of 0.5 because its hyper-
nyms include ?writer?.
2) Data-driven features
Wikipedia: Wikipedia was used to generate an an-
swer relevance score. If there is a Wikipedia docu-
ment whose title matches an answer candidate, the
document is analyzed to obtain the term frequency
(tf) and the inverse term frequency (idf) of the can-
didate, from which a tf.idf score is calculated. When
there is no matched document, each question key-
word is also processed as a back-off strategy, and the
answer relevance score is calculated by summing the
tf.idf scores obtained from individual keywords.
Google: Following Magnini et al (2002), a query
consisting of an answer candidate and question key-
786
words was sent to the Google search engine. Then
the top 10 text snippets returned by Google were
analyzed to generate an answer relevance score by
computing the minimum number of words between
a keyword and the answer candidate.
3.2 Answer Similarity Features
Answer similarity is calculated using multiple string
distance metrics and a list of synonyms.
String Distance Metrics: String distance metrics
such as Levenshtein, Jaro-Winkler, and Cosine sim-
ilarity were used to calculate the similarity between
two English answer candidates.
Synonyms: Synonyms can be used as another
metric to calculate answer similarity. If one answer
is synonym of another answer, the score is 1. Other-
wise the score is 0. To get a list of synonyms, three
knowledge bases were used: WordNet, Wikipedia
and the CIA World Factbook. In addition, manually
generated rules were used to obtain synonyms for
different types of answer candidates. For example,
?April 12 1914? and ?12th Apr. 1914? are converted
into ?1914-04-12? and treated as synonyms.
4 Extensions for Multiple Languages
We extended the framework for Chinese and
Japanese QA. This section details how we incor-
porated language-specific resources into the frame-
work. As logistic regression is based on a proba-
bilistic framework, the model does not need to be
changed to support other languages. We only re-
trained the model for individual languages. To sup-
port Chinese and Japanese QA, we incorporated new
features for individual languages.
4.1 Answer Relevance Features
We replaced the English gazetteers and WordNet
with language-specific resources for Japanese and
Chinese. As Wikipedia and the Web support mul-
tiple languages, the same algorithm was used in
searching language-specific corpora for the two lan-
guages.
1) Knowledge-based features
The knowledge-based features involve searching for
facts in a knowledge base such as gazetteers and
WordNet. We utilized comparable resources for
Chinese and Japanese. Using language-specific re-
#Articles
Language Nov. 2005 Aug. 2006
English 1,811,554 3,583,699
Japanese 201,703 446,122
Chinese 69,936 197,447
Table 1: Articles in Wikipedia for different lan-
guages
sources, the same algorithms were applied to gener-
ate an answer relevance score between -1 and 1.
Gazetteers: There are few available gazetteers
for Chinese and Japanese. Therefore, we extracted
location data from language-specific resources. For
Japanese, we extracted Japanese location informa-
tion from Yahoo2, which contains many location
names in Japan and the relationships among them.
For Chinese, we extracted location names from the
Web. In addition, we translated country names pro-
vided by the CIA World Factbook and the Tipster
gazetteers into Chinese and Japanese names. As
there is more than one translation, top 3 translations
were used.
Ontology: For Chinese, we used HowNet (Dong,
2000) which is a Chinese version of WordNet.
It contains 65,000 Chinese concepts and 75,000
corresponding English equivalents. For Japanese,
we used semantic classes provided by Gengo
GoiTaikei3. Gengo GoiTaikei is a Japanese lexicon
containing 300,000 Japanese words with their asso-
ciated 3,000 semantic classes. The semantic infor-
mation provided by HowNet and Gengo GoiTaikei
was used to assign an answer relevance score be-
tween -1 and 1.
2) Data-driven features
Wikipedia: As Wikipedia supports more than 200
language editions, the approach used in English can
be used for different languages without any modifi-
cation. Table 1 shows the number of text articles in
three different languages. Wikipedia?s current cov-
erage in Japanese and Chinese does not match its
coverage in English, but coverage in these languages
continues to improve.
To supplement the small corpus of Chi-
nese documents available, we used Baidu
2http://map.yahoo.co.jp/
3http://www.kecl.ntt.co.jp/mtg/resources/GoiTaikei
787
(http://baike.baidu.com), which is similar to
Wikipedia but contains more articles written in
Chinese. We first search for Chinese Wikipedia.
When there is no matching document in Wikipedia,
each answer candidate is sent to Baidu and the
retrieved document is analyzed in the same way to
analyze Wikipedia documents.
The idf score was calculated using word statis-
tics from Japanese Yomiuri newspaper corpus and
the NTCIR Chinese corpus.
Google: The same algorithm was applied to ana-
lyze Japanese and Chinese snippets returned from
Google. But we restricted the language to Chi-
nese or Japanese so that Google returned only Chi-
nese or Japanese documents. To calculate the dis-
tance between an answer candidate and question
keywords, segmentation was done with linguistic
tools. For Japanese, Chasen4 was used. For Chinese
segmentation, a maximum-entropy based parser was
used (Wang et al, 2006).
3) Manual Filtering
Other than the features mentioned above, we man-
ually created many rules for numeric and temporal
questions to filter out invalid answers. For example,
when the question is looking for a year as an answer,
an answer candidate which contains only the month
receives a score of -1. Otherwise, the score is 0.
4.2 Answer Similarity Features
The same features used for English were applied
to calculate the similarity of Chinese/Japanese an-
swer candidates. To identify synonyms, Wikipedia
were used for both Chinese and Japanese. EIJIRO
dictionary was used to obtain Japanese synonyms.
EIJIRO is a English-Japanese dictionary contain-
ing 1,576,138 words and provides synonyms for
Japanese words.
As there are several different ways to represent
temporal and numeric expressions (Nyberg et al,
2002; Greenwood, 2006), language-specific conver-
sion rules were applied to convert them into a canon-
ical format; for example, a rule to convert Japanese
Kanji characters to Arabic numbers is shown in Fig-
ure 2.
4http://chasen.aist-nara.ac.jp/hiki/ChaSen
0.25
??
??
1993-
07-04
1993 
? 7 ?
4 ?
50 %
??
1993-
07-04
??
??
?
??
??
3E+11
 ?
3,000
??
3E+11
 ?
??
 ? ?
Norm
alized
 answ
er stri
ng
Origin
al ans
wer st
ring
Figure 2: Example of normalized answer strings
5 Experiments
This section describes the experiments to evaluate
the extended answer ranking framework for Chinese
and Japanese QA.
5.1 Experimental Setup
We used Chinese and Japanese questions provided
by the NTCIR (NII Test Collection for IR Sys-
tems), which focuses on evaluating cross-lingual
and monolingual QA tasks for Chinese, Japanese
and English. For Chinese, a total of 550 fac-
toid questions from the NTCIR5-6 QA evaluations
served as the dataset. Among them, 200 questions
were used to train the Chinese answer extractor and
350 questions were used to evaluate our answer
ranking framework. For Japanese, 700 questions
from the NTCIR5-6 QA evaluations served as the
dataset. Among them, 300 questions were used to
train the Japanese answer extractor and 400 ques-
tions were used to evaluate our framework.
Both the Chinese and Japanese answer extractors
use maximum-entropy to extract answer candidates
based on multiple features such as named entity, de-
pendency structures and some language-dependent
features.
Performance of the answer ranking framework
was measured by average answer accuracy: the
number of correct top answers divided by the num-
ber of questions where at least one correct answer
exists in the candidate list provided by an extrac-
tor. Mean Reciprocal Rank (MRR5) was also used
to calculate the average reciprocal rank of the first
correct answer in the top 5 answers.
The baseline for average answer accuracy was
calculated using the answer candidate likelihood
scores provided by each individual extractor; the
788
TO
P1
TO
P3
MR
R5
0.00.10.20.30.40.50.60.70.80.91.0
Ja
pa
ne
se
 A
ns
we
r S
ele
cti
on
 
 B
as
eli
ne
 Fr
am
ew
ork
TO
P1
TO
P3
MR
R5
0.00.10.20.30.40.50.60.70.80.91.0
Ch
ine
se
 A
ns
we
r S
ele
cti
on
 
Avgerage Accuracy
 B
as
eli
ne
 Fr
am
ew
ork
Figure 3: Performance of the answer ranking framework for Chinese and Japanese answer selection (TOP1:
average accuracy of top answer, TOP3: average accuracy of top 3 answers, MRR5: average of mean recip-
rocal rank of top 5 answers)
answer with the best extractor score was chosen,
and no validation or similarity processing was per-
formed.
3-fold cross-validation was performed, and we
used a version of Wikipedia downloaded in Aug
2006.
5.2 Results and Analysis
We first analyzed the average accuracy of top 1, top3
and top 5 answers. Figure 3 compares the average
accuracy using the baseline and the answer selec-
tion framework. As can be seen, the answer rank-
ing framework significantly improved performance
on both Chinese and Japanese answer selection. As
for the average top answer accuracy, there were 40%
improvement over the baseline (Chinese) and 45%
improvement over the baseline (Japanese).
We also analyzed the degree to which the average
accuracy was affected by answer similarity and rel-
evance features. Table 2 compares the average top
answer accuracy using the baseline, the answer rel-
evance features, the answer similarity features and
all feature combinations. Both the similarity and the
relevance features significantly improved answer se-
lection performance compared to the baseline, and
combining both sets of features together produced
the best performance.
We further analyzed the utility of individual rele-
vance features (Figure 4). For both languages, filter-
ing was useful in ruling out wrong answers. The im-
Baseline Rel Sim All
Chinese 0.442 0.482 0.597 0.619
Japanese 0.367 0.463 0.502 0.532
Table 2: Average top answer accuracy of individ-
ual features (Rel: merging relevance features, Sim:
merging similarity features, ALL: merging all fea-
tures).
pact of the ontology was more positive for Japanese;
we assume that this is because the Chinese ontol-
ogy (HowNet) contains much less information over-
all than the Japanese ontology (Gengo GoiTaikei).
The comparative impact of Wikipedia was similar.
For Chinese, there were many fewer Wikipedia doc-
uments available. Even though we used Baidu as a
supplemental resource for Chinese, this did not im-
prove answer selection performance. On the other
hand, the use of Wikipedia was very helpful for
Japanese, improving performance by 26% over the
baseline. This shows that the quality of answer
relevance estimation is significantly affected by re-
source coverage.
When comparing the data-driven features with the
knowledge-based features, the data-driven features
(such as Wikipedia and Google) tended to increase
performance more than the knowledge-based fea-
tures (such as gazetteers and WordNet).
Table 3 shows the effect of individual similar-
ity features on Chinese and Japanese answer selec-
789
Ba
se
lin
eF
IL
ON
T
GA
Z
GL
W
IK
I
All
0.3
0
0.3
5
0.4
0
0.4
5
0.5
0
0.5
5
Avg. Top Answer Accuracy
 C
hin
es
e
 Ja
pa
ne
se
Figure 4: Average top answer accuracy of individ-
ual answer relevance features.(FIL: filtering, ONT,
ontology, GAZ: gazetteers, GL: Google, WIKI:
Wikipedia, ALL: combination of all relevance fea-
tures)
Chinese Japanese
0.3 0.5 0.3 0.5
Cosine 0.597 0.597 0.488 0.488
Jaro-Winkler 0.544 0.518 0.410 0.415
Levenshtein 0.558 0.544 0.434 0.449
Synonyms 0.527 0.527 0.493 0.493
All 0.588 0.580 0.502 0.488
Table 3: Average accuracy using individual similar-
ity features under different thresholds: 0.3 and 0.5
(?All?: combination of all similarity metrics)
tion. As some string similarity features (e.g., Lev-
enshtein distance) produce a number between 0 and
1 (where 1 means two strings are identical and 0
means they are different), similarity scores less than
a threshold can be ignored. We used two thresh-
olds: 0.3 and 0.5. In our experiments, using 0.3
as a threshold produced better results in Chinese.
In Japanese, 0,5 was a better threshold for individ-
ual features. Among three different string similar-
ity features (Levenshtein, Jaro-Winkler and Cosine
similarity), cosine similarity tended to perform bet-
ter than the others.
When comparing synonym features with string
similarity features, synonyms performed better than
string similarity in Japanese, but not in Chinese. We
had many more synonyms available for Japanese
Data-driven features All features
Chinese 0.606 0.619
Japanese 0.517 0.532
Table 4: Average top answer accuracy when using
data-driven features v.s. when using all features.
and they helped the system to better exploit answer
redundancy.
We also analyzed answer selection performance
when combining all four similarity features (?All?
in Table 3). Combining all similarity features im-
proved the performance in Japanese, but hurt the
performance in Chinese, because adding a small set
of synonyms to the string metrics worsened the per-
formance of logistic regression.
5.3 Utility of data-driven features
In our experiments we used data-driven fea-
tures as well as knowledge-based features. As
knowledge-based features need manual effort to ac-
cess language-specific resources for individual lan-
guages, we conducted an additional experiment only
with data-driven features in order to see how much
performance gain is available without the manual
work. As Google, Wikipedia and string similarity
metrics can be used without any additional manual
effort when extended to other languages, we used
these three features and compared the performance.
Table 4 shows the performance when using data-
driven features v.s. all features. It can be seen that
data-driven features alone achieved significant im-
provement over the baseline. This indicates that the
framework can easily be extended to any language
where appropriate data resources are available, even
if knowledge-based features and resources for the
language are still under development.
6 Conclusion
In this paper, we presented a generalized answer se-
lection framework which was applied to Chinese and
Japanese question answering. An empirical evalu-
ation using NTCIR test questions showed that the
framework significantly improves baseline answer
selection performance. For Chinese, the perfor-
mance improved by 40% over the baseline. For
Japanese, the performance improved by 45% over
790
the baseline. This shows that our probabilistic
framework can be easily extended for multiple lan-
guages by reusing data-driven features (with new
corpora) and adding language-specific resources
(ontologies, gazetteers) for knowledge-based fea-
tures.
In our previous work, we evaluated the perfor-
mance of the framework for English QA using ques-
tions from past TREC evaluations (Ko et al, 2007).
The experimental results showed that the combina-
tion of all answer ranking features improved per-
formance by an average of 102% over the baseline.
The relevance features improved performance by an
average of 99% over the baseline, and the similar-
ity features improved performance by an average of
46% over the baseline. Our hypothesis is that answer
relevance features had a greater impact for English
QA because the quality and coverage of the data re-
sources available for English answer validation is
much higher than the quality and coverage of ex-
isting resources for Japanese and Chinese. In future
work, we will continue to evaluate the robustness of
the framework. It is also clear from our comparison
with English QA that more work can and should be
done in acquiring data resources for answer valida-
tion in Chinese and Japanese.
Acknowledgments
We would like to thank Hideki Shima, Mengqiu
Wang, Frank Lin, Justin Betteridge, Matthew
Bilotti, Andrew Schlaikjer and Luo Si for their valu-
able support. This work was supported in part
by ARDA/DTO AQUAINT program award number
NBCHC040164.
References
D. Buscaldi and P. Rosso. 2006. Mining Knowledge
from Wikipedia for the Question Answering task. In
Proceedings of the International Conference on Lan-
guage Resources and Evaluation.
J. Chu-Carroll, J. Prager, C. Welty, K. Czuba, and D. Fer-
rucci. 2003. A Multi-Strategy and Multi-Source Ap-
proach to Question Answering. In Proceedings of Text
REtrieval Conference.
C. Clarke, G. Cormack, and T. Lynam. 2001. Exploiting
redundancy in question answering. In Proceedings of
SIGIR.
Zhendong Dong. 2000. Hownet:
http://www.keenage.com.
A. Echihabi, U. Hermjakob, E. Hovy, D. Marcu, E. Melz,
and D. Ravichandran. 2004. How to select an answer
string? In T. Strzalkowski and S. Harabagiu, editors,
Advances in Textual Question Answering. Kluwer.
Mark A. Greenwood. 2006. Open-Domain Question An-
swering. Thesis.
S. Harabagiu, D. Moldovan, M. Pasca, R. Mihalcea,
M. Surdeanu, R. Bunsecu, R. Girju, V. Rus, and
P. Morarescu. 2000. Falcon: Boosting knowledge for
answer engines. In Proceedings of TREC.
V. Jijkoun, J. van Rantwijk, D. Ahn, E. Tjong Kim Sang,
and M. de Rijke. 2006. The University of Amsterdam
at CLEF@QA 2006. In Working Notes CLEF.
J. Ko, L. Si, and E. Nyberg. 2007. A Probabilistic Frame-
work for Answer Selection in Question Answering. In
Proceedings of NAACL/HLT.
B. Magnini, M. Negri, R. Pervete, and H. Tanev. 2002.
Comparing statistical and content-based techniques for
answer validation on the web. In Proceedings of the
VIII Convegno AI*IA.
T. Minka. 2003. A Comparison of Numerical Optimizers
for Logistic Regression. Unpublished draft.
D. Moldovan, D. Clark, S. Harabagiu, and S. Maiorano.
2003. Cogex: A logic prover for question answering.
In Proceedings of HLT-NAACL.
E. Nyberg, T. Mitamura, J. Carbonell, J. Callan,
K. Collins-Thompson, K. Czuba, M. Duggan,
L. Hiyakumoto, N. Hu, Y. Huang, J. Ko, L. Lita,
S. Murtagh, V. Pedro, and D. Svoboda. 2002. The
JAVELIN Question-Answering System at TREC 2002.
In Proceedings of Text REtrieval Conference.
J. Prager, E. Brown, A. Coden, and D. Radev. 2000.
Question answering by predictive annotation. In Pro-
ceedings of SIGIR.
E. Voorhees. 2002. Overview of the TREC 2002 ques-
tion answering track. In Proceedings of Text REtrieval
Conference.
M. Wang, K. Sagae, and T. Mitamura. 2006. A Fast, Ac-
curate Deterministic Parser for Chinese. In Proceed-
ings of COLING/ACL.
J. Xu, A. Licuanan, J. May, S. Miller, and R. Weischedel.
2002. TREC 2002 QA at BBN: Answer Selection and
Confidence Estimation. In Proceedings of Text RE-
trieval Conference.
791
 
		
Towards Light Semantic Processing for Question Answering
Benjamin Van Durme?, Yifen Huang?, Anna Kups?c??+, Eric Nyberg?
?Language Technologies Institute, Carnegie Mellon University
+Polish Academy of Sciences
{vandurme,hyifen,aniak,ehn}@cs.cmu.edu
Abstract
The paper1 presents a lightweight knowledge-
based reasoning framework for the JAVELIN
open-domain Question Answering (QA) sys-
tem. We propose a constrained representation
of text meaning, along with a flexible unifica-
tion strategy that matches questions with re-
trieved passages based on semantic similarities
and weighted relations between words.
1 Introduction
Modern Question Answering (QA) systems aim at pro-
viding answers to natural language questions in an open-
domain context. This task is usually achieved by com-
bining information retrieval (IR) with information extrac-
tion (IE) techniques, modified to be applicable to unre-
stricted texts. Although semantics-poor techniques, such
as surface pattern matching (Soubbotin, 2002; Ravichan-
dran and Hovy, 2002) or statistical methods (Ittycheriah
et al, 2002), have been successful in answering fac-
toid questions, more complex tasks require a consider-
ation of text meaning. This requirement has motivated
work on QA systems to incorporate knowledge process-
ing components such as semantic representation, ontolo-
gies, reasoning and inference engines, e.g., (Moldovan et
al., 2003), (Hovy et al, 2002), (Chu-Carroll et al, 2003).
Since world knowledge databases for open-domain tasks
are unavailable, alternative approaches for meaning rep-
resentation must be adopted. In this paper, we present
our preliminary approach to semantics-based answer de-
tection in the JAVELIN QA system (Nyberg et al, 2003).
In contrast to other QA systems, we are trying to realize
a formal model for a lightweight semantics-based open-
domain question answering. We propose a constrained
semantic representation as well as an explicit unification
1The authors appear in alphabetical order.
framework based on semantic similarities and weighted
relations between words. We obtain a lightweight roboust
mechanism to match questions with answer candidates.
The organization of the paper is as follows: Section 2
briefly presents system components; Section 3 discusses
syntactic processing strategies; Sections 4 and 5 describe
our preliminary semantic representation and the unifica-
tion framework which assigns confidence values to an-
swer candidates. The final section contains a summary
and future plans.
2 System Components
The JAVELIN system consists of four basic components:
a question analysis module, a retrieval engine, a passage
analysis module (supporting both statistical and NLP
techniques), and an answer selection module. JAVELIN
also includes a planner module, which supports feedback
loops and finer control over specific components (Nyberg
et al, 2003). In this paper we are concerned with the two
components which support linguistic analysis: the ques-
tion analysis and passage understanding modules (Ques-
tion Analyzer and Information Extractor, respectively).
The relevant aspects of syntactic processing in both mod-
ules are presented in Section 3, whereas the semantic rep-
resentation is introduced in Section 4.
3 Parsing
The system employs two different parsing techniques:
a chart parser with hand-written grammars for ques-
tion analysis, and a lexicalized, broad coverage skipping
parser for passage analysis. For question analysis, pars-
ing serves two goals: to identify the finest answer focus
(Moldovan et al, 2000; Hermjakob, 2001), and to pro-
duce a grammatical analysis (f-structure) for questions.
Due to the lack of publicly available parsers which have
suitable coverage of question forms, we have manually
developed a set of grammars to achieve these goals. On
the other hand, the limited coverage and ambiguity in
these grammars made adopting the same approach for
passage analysis inefficient. In effect, we use two dis-
tinct parsers which provide two syntactic representations,
including grammatical functions. These syntactic struc-
tures are then transformed into a common semantic rep-
resentation discussed in Section 4.
( (Brill-pos VBN)
(adjunct (
(object (
(Brill-pos WRB)
(atype temporal)
(cat n)
(ortho When)
(q-focus +)
(q-token +)
(root when)
(tokens 1)))
(time +)))
(cat v)
(finite +)
(form finite)
(modified +)
(ortho founded)
(passive +)
(punctuation (
(Brill-pos ".")
(cat punct)
(ortho ?)
(root ?)
(tokens 6)))
(qa (
(gap (
(atype temporal)
(path (*MULT* adjunct
object))))
(qtype entity)))
(root found)
(subject (
(BBN-name person)
(Brill-pos NNP)
(cat n)
(definite +)
(gen-pn +)
(human +)
(number sg)
(ortho "Wendy?s")
(person third)
(proper-noun +)
(root wendy)
(tokens 3)))
(tense past)
(tokens 5))
Figure 1: When was Wendy?s founded: KANTOO f-
structure
3.1 Questions
The question analysis consists of two steps: lexical pro-
cessing and syntactic parsing. For the lexical process-
ing step, we have integrated several external resources:
the Brill part-of-speech tagger (Brill, 1995), BBN Identi-
Finder (BBN, 2000) (to tag named entities such as proper
names, time expressions, numbers, etc.), WordNet (Fell-
baum, 1998) (for semantic categorization), and the KAN-
TOO Lexifier (Nyberg and Mitamura, 2000) (to access a
syntactic lexicon for verb valence information).
The hand-written grammars employed in the project
are based on the Lexical Functional Grammar (LFG) for-
malism (Bresnan, 1982), and are used with the KANTOO
parser (Nyberg and Mitamura, 2000). The parser out-
puts a functional structure (f-structure) which specifies
the grammatical functions of question components, e.g.,
subject, object, adjunct, etc. As illustrated in Fig. 1, the
resulting f-structure provides a deep, detailed syntactic
analysis of the question.
3.2 Passages
Passages selected by the retrieval engine are processed
by the Link Grammar parser (Grinberg et al, 1995). The
parser uses a lexicalized grammar which specifies links,
i.e., grammatical functions, and provides a constituent
structure as output. The parser covers a wide range of
syntactic constructions and is robust: it can skip over un-
recognized fragments of text, and is able to handle un-
known words.
An example of the passage analysis produced by the
Link Parser is presented in Fig. 2. Links are treated as
predicates which relate various arguments. For exam-
ple, O in Fig. 2 indicates that Wendy?s is an object of the
verb founded. In parallel to the Link parser, passages are
tagged with the BBN IdentiFinder (BBN, 2000), in or-
der to group together multi-word proper names such as
R. David Thomas.
4 Semantic Representation
At the core of our linguistic analysis is the semantic rep-
resentation, which bridges the distinct representations of
the functional structure obtained for questions and pas-
sages. Although our semantic representation is quite sim-
ple, it aims at providing the means of understanding and
processing broad-coverage linguistic data. The represen-
tation uses the following main constructs:2
? formula is a conjunction of literals and represents
the meaning of the entire sentence (or question);
? literal is a predicate relation over two terms; in par-
ticular, we distinguish two types of literals: extrin-
sic literal, a literal which relates a label to a label,
and intrinsic literal, a literal which relates a label to
a word;
2The use of terminology common in the field of formal logic
is aimed at providing an intuitive understanding to the reader,
but is not meant to give the impression that our work is built on
a firm logic-theoretic framework.
+------------------------Xp------------------------+
| +-------MVp-------+ |
+--------Wd-------+ +------O------+ | |
| +-G-+---G--+---S---+ +--YS-+ +-IN+ |
| | | | | | | | | |
LEFT-WALL R. David Thomas founded.v Wendy ?s.p in 1969 .
Constituent tree:
(S (NP R. David Thomas)
(VP founded
(NP (NP Wendy ?s))
(PP in
(NP 1969)))
.)
Figure 2: R. David Thomas founded Wendy?s in 1969.: Link Grammar parser output
? predicate is used to capture relations between
terms;
? term is either a label, a variable which refers to a
specific entity or an event, or a word, which is either
a single word (e.g., John) or a sequence of words
separated by whitespace (e.g., for proper names such
as John Smith).
The BNF syntax corresponding to this representation
is given in (1).
(1) <formula> := <literal>+
<literal> := <pred>(<term>,<term>)
<term> := <label>|<word>
<word> := |[a-nA-Z0-9\s]+|
<label> := [a-z]+[0-9]+
<pred> := [A-Z_-]+
With the exception of the unary ANS predicate which
indicates the sought answer, all predicates are binary re-
lations (see examples in Fig. 3). Currently, most pred-
icate names are based on grammatical functions (e.g.,
SUBJECT, OBJECT, DET) which link events and entities
with their arguments. Unlike in (Moldovan et al, 2003),
names of predicates belong to a fixed vocabulary, which
provides a more sound basis for a formal interpretation.
Names of labels and terms are restricted only by the syn-
tax in (1). Examples of semantic representations for the
question When was Wendy?s founded? and the passage R.
David Thomas founded Wendy?s in 1969. are shown in
Fig. 4.
Note that our semantic representation reflects the
?canonical? structure of an active sentence. This design
decision was made in order to eliminate structural differ-
ences between semantically equivalent structures. Hence,
at the semantic level, all passive sentences correspond to
their equivalents in the active form. Semantic representa-
tion of questions is not always derived directly from the
f-structure. For some types of questions, e.g., definition
When was Wendy?s R. David Thomas founded
founded? Wendy?s in 1969.
ROOT(x6,|Wendy?s|) ROOT(x6,|Wendy?s|)
ROOT(x2,|found|) ROOT(x2,|ound|)
ADJUNCT(x2,x1) ADJUNCT(x2,x1)
OBJECT(x2,x6) OBJECT(x2,x6)
SUBJECT(x2,x7) SUBJECT(x2,x7)
ROOT(x7,|R. David Thomas|)
TYPE(x2,|event|) TYPE(x2,|event|)
TENSE(x2,|past|)
ROOT(x1,x9) ROOT(x1,|1969|)
TYPE(x1,|time|) TYPE(x1,|time|)
ANS(x9)
Figure 4: An example of question and passage semantic
representation
questions such as What is the definition of hazmat?, spe-
cialized (dedicated) grammars are used, which allows us
to more easily arrive at an appropriate representation of
meaning. Also, in the preliminary implementation of the
unification algorithm (see Section 5), we have adopted
some simplifying assumptions, and we do not incorpo-
rate sets in the current representation.
The present formalism can quite successfully handle
questions (or sentences) which refer to specific events or
relations. However, it is more difficult to represent ques-
tions like What is the relationship between Jesse Ventura
and Target Stores?, which seek a relation between enti-
ties or a common event they participated in. In the next
section, we discuss the unification scheme which allows
us to select answer candidates based on the proposed rep-
resentation.
5 Fuzzy Unification
A unification algorithm is required to match question rep-
resentations with the representations of extracted pas-
sages which might contain answers. Using a precursor
predicate example comments
ROOT ROOT(x13,|John|) the root form of entity/event x13
OBJECT OBJECT(x2,x3) x3 is the object of verb
or preposition x2
SUBJECT SUBJECT(x2,x3) x3 is the subject of verb x2
DET DET(x2,x1) x1 is a determiner/quantifier of x2
TYPE TYPE(x3,|event|) x3 is of the type event
TENSE TENSE(x1,|present|) x1 is a verb in present tense
EQUIV EQUIV(x1,x3) semantic equivalence:
apposition: ?John, a student of CMU?
equality operator in copular sentences:
?John is a student of CMU?
ATTRIBUTE ATTRIBUTE(x1,x3) x3 is an adjective modifier of x1:
adjective-noun: ?stupid John?
copular constructions: ?John is stupid?
PREDICATE PREDICATE(x2,x3) copular constructions: ?Y is x3?
ROOT(x2,|be|) SUBJECT(x2,Y)
PREDICATE(x2,x3)
POSSESSOR POSSESSOR(x2,x4) x4 is the possessor of x2
?x4?s x2? or ?x2 of x4?
AND AND(x3,x1) ?John and Mary laughed.?
AND(x3,x2) ROOT(x1,|John|) ROOT(x2,|Mary|)
ROOT(x4,|laugh|) TYPE(x4,|event|)
AND(x3,x1)
AND(x3,x2)
SUBJECT(x4,x3)
ANS ANS(x1) only for questions: x1 indicates the answer
Figure 3: Examples of predicates
to the representation presented above, we constructed
an initial prototype using a traditional theorem prover
(Kalman, 2001). Answer extraction was performed by at-
tempting a unification between logical forms of the ques-
tion and retrieved passages. Early tests showed that a uni-
fication strategy based on a strict boolean logic was not
as flexible as we desired, given the lack of traditional do-
main constraints that one normally possesses when con-
sidering this type of approach. Unless a retrieved pas-
sage exactly matched the question, as in Fig. 4, the sys-
tem would fail due to lack of information. For instance,
knowing that Benjamin killed Jefferson. would not an-
swer the question Who murdered Jefferson?, using a strict
unification strategy.
This has led to more recent experimentation with prob-
abilistic models that perform what we informally refer to
as fuzzy unification.3 The basic idea of our unification
strategy is to treat relationships between question terms
as a set of weighted constraints. The confidence score
assigned to each extracted answer candidate is related to
the number of constraints the retrieved passage satisfies,
along with a measure of similarity between the relevant
terms.
5.1 Definitions
In this section, we present definitions which are necessary
for discussion of the similarity measure employed by our
fuzzy unification framework.
Given a user query Q, where Q is a formula, we re-
trieve a set of passages P. Our task to is find the best
passage Pbest ? P from which an answer candidate can
be extracted. An answer candidate exists within a pas-
sage P if the result of a fuzzy unification between Q and
P results in the single term of ANS(x0) being ground in a
term from P .
(2) Pbest = argmaxP? Psim(Q,P )
The restriction that an answer candidate must be found
within a passage P must be made explicit, as our no-
tion of fuzzy unification is such that a passage can unify
against a query with a non-zero level of confidence even
if one or more constraints from the query are left unsat-
isfied. Since the final goal is to find and return the best
possible answer, we are not concerned with those pas-
sages which seem highly related yet do not offer answer
candidates.
In Section 4, we introduced extrinsic literals where
predicates serve as relations over two labels. Extrinsic lit-
erals can be thought of as relations defined over distinct
3Fuzzy unification in a formal setting generally refers to a
unification framework that is employed in the realm of fuzzy
logics. Our current representation is of an ad-hoc nature, but
our usage of this term does foreshadow future progression to-
wards a representation scheme dependent on such a formal,
non-boolean model.
entities in our formula. For example, SUBJECT(x1, x2)
is an extrinsic literal, while ROOT(x1, |Benjamin|) is not.
The latter has been defined as an intrinsic literal in Sec-
tion 4 and it relates a label and a word.
This terminology is motivated by the intuitive distinc-
tion between intrinsic and extrinsic properties of an entity
in the world. We use this distinction as a simplifying as-
sumption in our measurements of similarity, which we
will now explain in more detail.
5.2 Similarity Measure
Given a set of extrinsic literals PE and QE from a pas-
sage and the question, respectively, we measure the sim-
ilarity between QE and a given ordering of PE as the
geometric mean of the similarity between each pair of
extrinsic literals from the sets QE and PE .
Let O be the set of all possible orderings of PE , O
an element of O, QEj literal j of QE , and Oj literal j of
ordering O. Then:
(3) sim(Q,P )= sim(QE , PE)
= maxO? O(
?n
j=0 sim(QEj , Oj))
1
n
The similarity of two extrinsic literals, lE and lE? , is
computed by the square root of the similarity scores of
each pair of labels, multiplied by the weight of the given
literal, dependent on the equivilance of the predicates
p, p? of the respective literals lE , lE? . If the predicates are
not equivilant, we rely on the engineers tactic of assign-
ing an epsilon value of similarity, where  is lower than
any possible similarity score4. Note that the similarity
score is still dependent on the weight of the literal, mean-
ing that failing to satisfy a heavier constraint imposes a
greater penalty than if we fail to satisfy a constraint of
lesser importance.
Let tj and t?j be the respective j-th term of lE , lE
?
.
Then:
(4) sim(lE , lE?) = weight(lE)?
{(sim(t0,t?0)?sim(t1,t?1))
1
2 ,p=p?
,otherwise
The weight of a literal is meant to capture the relative
importance of a particular constraint in a query. In stan-
dard boolean unification the importance of a literal is uni-
form, as any local failure dooms the entire attempt.5 In a
non-boolean framework the importance of one literal vs.
another becomes an issue. As an example, given a ques-
tion concerning a murder we might be more interested in
the suspect?s name than in the fact that he was tall. This
4The use of a constant value of  is ad hoc, and we are in-
vestigating more principled methods for assigning this penalty.
5That is to say, classic unification is usually an all or nothing
affair.
idea is similar to that commonly seen in information re-
trieval systems which place higher relative importance on
terms in a query that are judged a priori to posses higher
information value. While our prototype currently sets all
literals with a weight of 1.0, we are investigating methods
to train these weights to be specific to question type.
Per our definition, all terms within an extrinsic literal
will be labels. Thus, in equation (10), t0 is a label, as is
t1, and so on. Given a pair of labels, b and b?, we let I, I ?
be the respective sets of intrinsic literals from the formula
containing b, b? such that for all intrinsic literals lI ? I ,
the first term of lI is b, and likewise for b?, I ?.
Much like similarity between two formulae, the sim-
ilarity between two labels relies on finding the maximal
score over all possible orderings of a set of literals.
Now let O be the set of all possible orderings of I ?, O
an element of O, Ij the j-th literal of I , and Oj the j-th
literal of O. Then:
(5) sim(b, b?) = maxO? O(
?n
j=0 sim(Ij , Oj))
1
n
We measure the similarity between a pair of intrinsic
literals as the similarity between the two words multi-
plied by the weight of the first literal, dependent on the
predicates p, p? of the respective literals being equivilant.
(6)
sim(lI , lI
?
) = weight(lI) ?
{sim(t1,t?1),p=p?
,otherwise
The similarity between two words is currently measured
using a WordNet distance metric, applying weights intro-
duced in (Moldovan et al, 2003). We will soon be inte-
grating metrics which rely on other dimensions of simi-
larity.
5.3 Example
We now walk through a simple example in order to
present the current framework used to measure the level
of constraint satisfaction (confidence score) achieved by
a given passage. While a complete traversal of even a
small passage would exceed the space available here, we
will present a single instance of each type of usage of the
sim() function.
If we limit our focus to only a few key relationships,
we get the following analysis of a given question and pas-
sage.
(7) Who killed Jefferson?
ANS(x0), ROOT(x1,x0), ROOT(x2,|kill|),
ROOT(x3,|Jefferson|), TYPE(x2,|event|),
TYPE(x1,|person|), TYPE(x3,|person|), SUB-
JECT(x2,x1), OBJECT(x2,x3)
(8) Benjamin murdered Jefferson.
ROOT(y1,|Benjamin|), ROOT(y2,|murder|),
ROOT(y3,|Jefferson|), TYPE(y2,|event|),
TYPE(y1,|person|), TYPE(y3,|person|), SUB-
JECT(y2,y1), OBJECT(y2,y3)
Computing the similarity between two formulae,
(loosely referred to here by their original text), gives the
following:
(9) sim[|Who killed Jefferson?|,
|Benjamin murdered Jefferson.|] =
(sim[ SUBJECT(x2,x1), SUBJECT(y2,y1)]?
sim[ OBJECT(x2,x3), OBJECT(y2,y3)]) 12
The similarity between the given extrinsic literals shar-
ing the predicate SUBJECT:
(10) sim[SUBJECT(x2,x1), SUBJECT(y2,y1)] =
(sim[x2, y2] ? sim[x1, y1]) 12 ?
weight[SUBJECT(x2,x1)]
In order to find the result of this extrinsic similarity
evaluation, we need to determine the similarity between
the paired terms, (x1,y1) and (x2,y2). The similarity be-
tween x1 and y1 is measured as:
(11) sim[x2, y2] =
(sim[ROOT(x2,|kill|), ROOT(y2,|murder|)]?
sim[TYPE(x2,|event|), TYPE(y2,|event|)]) 12
The result of this function depends on the combined
similarity of the intrinsic literals that relate the given
terms to values. The similarity between one of these in-
trinsic literal pairs is measured by:
(12) sim[ROOT(x2,|kill|), ROOT(y2,|murder|)] =
sim[|kill|, |murder|]?weight[ROOT(x2,|kill|)]
Finally, the similarity between a pair of words is com-
puted as:
(13) sim[|kill|, |murder|] = 0.8
As stated earlier, our similarity metrics at the word
level are currently based on recent work on WordNet dis-
tance functions. We are actively developing methods to
complement this approach.
6 Summary and Future Work
The paper presents a lightweight semantic processing
technique for open-domain question answering. We pro-
pose a uniform semantic representation for questions and
passages, derived from their functional structure. We also
describe the unification framework which allows for flex-
ible matching of query terms with retrieved passages.
One characteristics of the current representation is that
it is built from grammatical functions and does not uti-
lize a canonical set of semantic roles and concepts. Our
overall approach in JAVELIN was to start with the sim-
plest form of meaning-based matching that could extend
simple keyword-based approaches. Since it was possi-
ble to extract grammatical functions from unrestricted
text fairly quickly (using KANTOO for questions and the
Link Grammar parser for answer passages), this frame-
work provides a reasonable first step. We intend to extend
our representation and unification algorithm by incorpo-
rating the Lexical Conceptual Structure Database (Dorr,
2001), which will allow us to use semantic roles instead
of grammatical relations as predicates in the represen-
tation. We also plan to enrich the representation with
temporal expressions, incorporating the ideas presented
in (Han, 2003).
Another limitation of the current implementation is
the limited scope of the similarity function. At present,
the similarity function is based on relationships found
in WordNet, and only relates words which belong to the
same syntactic category. We plan to extend our similar-
ity measure by using name lists, gazetteers and statistical
cooccurrence in text corpora. A complete approach to
word similarity will also require a suitable algorithm for
reference resolution. Unrestricted text makes heavy use
of various forms of co-reference, such as anaphora, def-
inite description, etc. We intend to adapt the anaphora
resolution algorithms used in KANTOO for this purpose,
but a general solution to resolving definite reference (e.g.,
the use of ?the organization? to refer to ?Microsoft?) is a
topic for ongoing research.
Acknowledgments
Research presented in this paper has been supported in
part by an ARDA grant under Phase I of the AQUAINT
program. The authors wish to thank all members of the
JAVELIN project for their support in preparing the work
presented in this paper. We are also grateful to two anony-
mous reviewers, Laurie Hiyakumoto and Kathryn Baker
for their comments and suggestions for improving this
paper. Needless to say, all remaining errors and omis-
sions are entirely our responsibility.
References
BBN Technologies, 2000. IdentiFinder User Manual.
Joan Bresnan, editor. 1982. The Mental Representation
of Grammatical Relations. MIT Press Series on Cog-
nitive Theory and Mental Representation. The MIT
Press, Cambridge, MA.
Eric Brill. 1995. Transformation-based error-driven
learning and natural language processing: A case study
in part-of-speech tagging. Computational Linguistics,
21:543?565.
Jenifer Chu-Carroll, John Prager, Christopher Welty,
Krzysztof Czuba, and David Ferrucci. 2003. A multi-
strategy and multi-source approach to question an-
swering. In TREC 2002 Proceedings.
Bonnie J. Dorr. 2001. LCS Database Docu-
mentation. HTML Manual. available from
http://www.umiacs.umd.edu/?bonnie/LCS Data-
base Documentation.html.
Christiane Fellbaum. 1998. WordNet: An Electronic Lex-
ical Database. MIT Press.
Dennis Grinberg, John Lafferty, and Daniel Sleator.
1995. A robust parsing algorithm for link grammars.
In Proceedings of the Fourth International Workshop
on Parsing Technologies, Prague, September.
Benjamin Han. 2003. Text temporal analysis. Research
status summary. Draft of January 2003.
Ulf Hermjakob. 2001. Parsing and question classifica-
tion for question answering. In Proceedings of the
Workshop on Open-Domain Question Answering at
ACL-2001.
Eduard Hovy, Ulf Hermjakob, and Chin-Yew Lin. 2002.
The use of external knowledge in factoid qa. In Pro-
ceedings of the TREC-10 Conference.
Abraham Ittycheriah and Salim Roukos. 2003. IBM?s
statistical question answering system ? TREC-11. In
TREC 2002 Proceedings.
Abraham Ittycheriah, Martin Franz, and Salim Roukos.
2002. IBM?s statistical question answering system ?
TREC-10. In TREC 2001 Proceedings.
John A. Kalman. 2001. Automated Reasoning with OT-
TER. Rinton Press.
Dan Moldovan, Sanda Harabagiu, Marius Pasca, Rada
Mihalcea, Roxana Girju, Richard Goodrum, and Vasile
Rus. 2000. The structure and performance of an open-
domain question answering system. In Proceedings of
the Conference of the Association for Computational
Linguistics (ACL-2000).
Dan Moldovan, Sanda Harabagiu, Roxana Girju, Paul
Morarescu, Finley Lacatusu, Adrian Novischi, Adriana
Badulescu, and Orest Bolohan. 2003. LCC tools for
question answering. In TREC 2002 Proceedings.
Eric Nyberg and Teruko Mitamura. 2000. The KAN-
TOO machine translation environment. In Proceed-
ings of AMTA 2000.
Eric Nyberg, Teruko Mitamura, Jaime Carbonell, Jaime
Callan, Kevyn Collins-Thompson, Krzysztof Czuba,
Michael Duggan, Laurie Hiyakumoto, Ng Hu, Yifen
Huang, Jeongwoo Ko, Lucian V. Lita, Stephen
Murtagh, Vasco Pedro, and David Svoboda. 2003. The
JAVELIN question answering system at TREC 2002.
In TREC 2002 Proceedings.
Deepak Ravichandran and Eduard Hovy. 2002. Learning
surface text patterns for a Question Answering system.
In Proceedings of the ACL Conference.
Martin M. Soubbotin. 2002. Patterns of potential answer
expressions as clues to the right answer. In TREC 2001
Proceedings.
Proceedings of the NAACL HLT Workshop on Active Learning for Natural Language Processing, pages 18?26,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Estimating Annotation Cost for Active Learning
in a Multi-Annotator Environment
Shilpa Arora, Eric Nyberg and Carolyn P. Rose?
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{shilpaa,ehn,cprose}@cs.cmu.edu
Abstract
We present an empirical investigation of the
annotation cost estimation task for active
learning in a multi-annotator environment. We
present our analysis from two perspectives:
selecting examples to be presented to the user
for annotation; and evaluating selective sam-
pling strategies when actual annotation cost
is not available. We present our results on a
movie review classification task with rationale
annotations. We demonstrate that a combina-
tion of instance, annotator and annotation task
characteristics are important for developing an
accurate estimator, and argue that both corre-
lation coefficient and root mean square error
should be used for evaluating annotation cost
estimators.
1 Introduction
Active Learning is the process of selectively query-
ing the user to annotate examples with the goal
of minimizing the total annotation cost. Annota-
tion cost has been traditionally measured in terms
of the number of examples annotated, but it has
been widely acknowledged that different examples
may require different annotation effort (Settles et al,
2008; Ringger et al, 2008).
Ideally, we would use actual human annotation
cost for evaluating selective sampling strategies, but
this will require conducting several user studies, one
per strategy on the same dataset. Alternatively, we
may be able to simulate the real user by an annota-
tion cost estimator that can then be used to evaluate
several selective sampling strategies without having
to run a new user study each time. An annotation
cost estimator models the characteristics that can
differentiate the examples in terms of their annota-
tion time. The characteristics that strongly correlate
with the annotation time can be used as a criterion
in selective sampling strategies to minimize the total
annotation cost.
In some domains, the annotation cost of an ex-
ample is known or can be calculated exactly before
querying the user. For example, in biological ex-
periments it might be calculable from the cost of
the equipment and the material used (King et al,
2004). In NLP, sometimes a simplifying assumption
is made that the annotation cost for an example can
be measured in terms of its length (e.g. seconds of
voicemail annotated (Kapoor et al, 2007); number
of tokens annotated (Tomanek et al, 2007)). An-
other assumption is that the number of user anno-
tation actions can be used as a proxy for annota-
tion cost of an example (e.g. number of brackets
added for parsing a sentence (Hwa, 2000); number
of clicks for correcting named entities (Kristjannson
et al, 2004)). While these are important factors in
determining the annotation cost, none of them alone
can fully substitute for the actual annotation cost.
For example, a short sentence with a lot of embed-
ded clauses may be more costly to annotate than a
longer sentence with simpler grammatical structure.
Similarly, a short sentence with multiple verbs and
discontinuous arguments may take more time to an-
notate with semantic roles than a longer sentence
with a single verb and simple subject-verb-object
structure (Carreras and Ma?rquez, 2004).
What further complicates the estimation of anno-
tation cost is that even for the same example, anno-
tation cost may vary across annotators (Settles et al,
2008). For example, non-native speakers of English
were found to take longer time to annotate part of
18
speech tags (Ringger et al, 2008). Often multiple
annotators are used for creating an annotated cor-
pus to avoid annotator bias, and we may not know
all our annotators beforehand. Annotation cost also
depends on the user interface used for annotation
(Gweon et al, 2005), and the user interface may
change during an annotation task. Thus, we need
a general annotation cost estimator that can predict
annotation cost for a given annotator and user inter-
face. A general estimator can be built by using an-
notator and user interface characteristics in addition
to the instance characteristics for learning an anno-
tation cost model, and training on data from mul-
tiple annotators and multiple user interfaces. Such
a general estimator is important for active learning
research where the goal is to compare selective sam-
pling strategies independent of the annotator and the
user interface.
In this work, we investigate the annotation cost es-
timation problem for a movie review classification
task in a multi-annotator environment with a fixed
user interface. We demonstrate that a combination
of instance, annotation task and annotator charac-
teristics is important for accurately estimating the
annotation cost. In the remainder of the paper, we
first present a survey of related work and an analysis
of the data collected. We then describe the features
used for our supervised learning approach to anno-
tation cost estimation, followed by the experimental
setup and results. Finally, we conclude with some
future directions we would like to explore.
2 Related work
There has been some recent research effort in using
supervised learning for estimating annotation cost.
The most closely related work is that by Settles et al
(2008) and Ringger et al (2008). Settles et al (2008)
present a detailed analysis of annotation cost for four
NLP applications: named entity recognition, image
retrieval, speculative vs. definite distinction, and in-
formation extraction. They study the effect of do-
main, annotator, jitter, order of examples, etc., on
the annotation cost.
Results from Settles et al (2008) are promising
but leave much room for improvement. They used
only instance level features such as number of en-
tities, length, number of characters, percentage of
non-alpha numeric characters, etc. for annotation
cost estimation. For three of their tasks, the corre-
lation between the estimated and actual annotation
times was in the range (R = 0.587 to 0.852). Note
that the percentage of variance accounted for by a
model is obtained by squaring the R value from the
correlation coefficient. Thus, an R value of 0.587
indicates that only about 34% (R2) of the variance
is accounted for, so the model will make incorrect
predictions about ranking in the majority of cases.
Nevertheless, we acknowledge that our results are
not substantially better, although we argue that this
work contributes to the pool of knowledge that will
hopefully lead to better performance in the future.
Settles et al (2008) train and test their estimator
on data from the same annotator. Thus, in order
to use their model for a new annotator, we would
need to first collect data for that annotator and train
a model. In our work, a group of annotators anno-
tate the same text, and we train and test on different
annotators. We also show that using characteristics
of the annotators and annotation task in addition to
the instance characteristics improves performance.
Ringger et al (2008) use linear regression for an-
notation cost estimation for Part-Of-Speech (POS)
tagging. About 30 annotators annotated 36 different
instances each. The authors present about 13 de-
scriptive statistics of the data, annotator and annota-
tion task, but in their model they only used number
of tokens in the sentence and the number of correc-
tions needed as features. They report that the other
variables didn?t have a significant effect when eval-
uated using a Bayesian Information Criterion (from
the R package).
Ringger et al (2008) noticed that nativeness of
the annotator did have an effect on the annotation
time, but they chose not to include that feature
in their model as they expected to have a similar
mix of skills and background in their target anno-
tators. However, if annotation times differ substan-
tially across annotators, then not accounting for this
difference will reduce the performance of the model.
Also, the low adjusted correlation value for their
model (R = 0.181) indicates that there is only a
weak correlation between the annotation time and a
linear combination of the length of the example and
the number of corrections.
19
3 Analysis and Experiments
In this section, we present our annotation methodol-
ogy and analysis of the data we collected, followed
by a description of the features we used.We then
present our experimental setup followed by a dis-
cussion of our results.
3.1 Annotation Methodology and Data
Analysis
In this work, we estimate the annotation cost for a
movie review classification task. The data we used
were collected as part of a graduate course. Twenty
annotators (students and instructors) were grouped
into five groups of four each. The groups were cre-
ated such that each group had similar variance in
annotator characteristics such as department, educa-
tional experience, programming experience, etc. We
used the first 200 movie reviews from the dataset
provided by Zaidan et al (2007), with an equal dis-
tribution of positive and negative examples. Each
group annotated 25 movie reviews randomly se-
lected from the 200 reviews and all annotators in
each group annotated all 25 reviews. In addition
to voting positive or negative for a review, annota-
tors also annotated rationales (Zaidan et al, 2007),
spans of text in the review that support their vote.
Rationales can be used to guide the model by identi-
fying the most discriminant features. In related work
(Arora and Nyberg, 2009), we ascertain that with ra-
tionales the same performance can be achieved with
less annotated data. The annotation task with ra-
tionales involved a variety of user actions: voting
positive or negative, highlighting spans of text and
adding rationale annotations. We used the same an-
notation guidelines as Zaidan et al (2007). The data
has been made available for research purposes1. Fig-
ure 1 shows a screenshot of the GUI used. We per-
formed an analysis of our data similar to that con-
ducted by Settles et al (2008). We address the fol-
lowing main questions.
Are the annotation times variable enough? If all
examples take a similar time to annotate, then the
number of examples can be used as an approxima-
tion for the annotation cost. Figure 2 shows the his-
togram of averaged annotation times (averaged over
1www.cs.cmu.edu/?shilpaa/datasets/ial/
ial-uee-mr-v0.1.zip
Figure 1: The GUI used for the annotation task. The user
selects the review (segment) to annotate from the list in the
right panel. The review text is displayed in the left panel. The
user votes positive or negative using the radio buttons. Ratio-
nales are added by selecting a span of text and right clicking
to select the rationale tag. The start/stop button can be used to
pause the current task.
Figure 2: Distribution of averaged annotation times
4 annotators in a group). As can be seen from the
mean (? = 165 sec.) and the standard deviation
(? = 68.85), there is a meaningful variance in the
annotation times.
How do the annotation times vary across annota-
tors? A strong correlation between annotation times
from different annotators on a set of instances sug-
gests that there are certain characteristics of these in-
stances, independent of the annotator characteristics,
that can determine their ranking based on the time it
takes to annotate them. We evaluated the pairwise
correlation for all pairs of annotators in each group
(Table 1). As can be seen, there is significant pair-
wise correlation in more than half of the pairs of an-
notators that differ in nativeness (10/16). However,
not all such pairs of annotators are associated with
significant correlation. This suggests that it is im-
portant to consider both instance and annotator char-
acteristics for estimating annotation time.
20
group Avg-Na(Std) Avg-CR(Std) #sign-pairs
0 2.25(0.96) 0.54(0.27) 4/6 (4/5)
1 1.75(0.5) 0.45(0.08) 5/6 (2/3)
2 1(0) 0.13(0.17) 0/6 (0/0)
3 1.75(0.96) 0.36(0.12) 2/6 (1/5)
4 2.75(0.5) 0.47(0.04) 6/6 (3/3)
Avg. 1.9(0.58) 0.39(0.21) 17/30 (10/16)
Table 1: The Table shows the average nativeness and average
pairwise correlation between annotation times for the mem-
bers of each group (and their standard deviation). #sign-pairs
shows the fraction of pairwise correlations within the groups
that were significant (p < 0.05). In brackets, is the fraction
of correlations between annotators with different nativeness
within the groups that were significant.
The box plot in Figure 3 shows the distribution
of annotation times across annotators. As can be
seen, some annotators take in general much longer
than others, and the distribution of times is very dif-
ferent across annotators. For some, the annotation
times vary a lot, but not so much for others. This
suggests that using annotator characteristics as fea-
tures in addition to the instance characteristics may
be important for learning a better estimator.
Figure 3: Box plot shows the annotation time (in sec) dis-
tribution (y-axis) for an annotator (x-axis) for a set of 25 doc-
uments. g0-a1 represents annotator 1 of group 0 and g0-avg
represents the average annotation time. A box represents the
middle 50% of annotation times, with the line representing the
median. Whiskers on either side span the 1st and 4th quartiles
and asterisks indicate the outliers.
3.2 Feature Design
We group the features in the following three cat-
egories: Instance, Annotation Task and Annotator
characteristics.
3.2.1 Instance characteristics
Instance characteristics capture the properties of
the example the user annotates. Table 2 describes
the instance based features we used and the intu-
ition supporting their use for annotation cost esti-
mation. Table 3 shows the mean and standard de-
viation of each of these characteristics, and as can
be seen, these characteristics do vary across exam-
ples and hence these features can be beneficial for
distinguishing examples.
3.2.2 Annotation Task characteristics
Annotation task characteristics are those that can
be captured only during or after the annotation task.
We used the number of rationales as a feature from
this category. In addition to voting for movie re-
views as positive or negative, the user also adds ra-
tionales that support their vote. More rationales im-
ply more work since the user must look for the rele-
vant span of text and perform the physical action of
selecting the span and adding an annotation for each
rationale. Table 3 shows the distribution of the aver-
age Number of Rationales (NR) per example (aver-
aged over the four annotators for a given set).
3.2.3 Annotator characteristics
The annotation cost of an example may vary
across annotators. As reported in Table 1, the aver-
age correlation for annotators on the same document
is low (R = 0.39) with 17 out of 30 pairwise correla-
tions being significant. Thus, it is important to con-
sider annotator characteristics, such as whether the
annotator is a native speaker of English, their educa-
tion level, reading ability, etc. In this work, we only
use nativeness of the annotator as a feature and plan
to explore other characteristics in the future. We as-
signed each annotator a nativeness value. A value
of 3 was given to an annotator whose first language
is English. A value of 2 was given to an annotator
who has a different first language but has either been
educated in English or has been in the United States
for a long time. A value of 1 was assigned to the re-
maining annotators. Among the 20 annotators in the
study, there were 8 annotators with nativeness value
of 1, and 6 each for nativeness values of 2 and 3.
Table 1 shows the average and standard deviation of
the nativeness score in each group.
21
Feature Definition Intuition
Character
Length (CL)
Length of review in
terms of number of
characters
Longer documents
take longer to anno-
tate
Polar word
Count (PC)
Number of words
that are polar (strong
subjective words
from the lexicon
(Wilson et al, 2005))
More subjectivity
implies user would
need more time to
judge positive vs.
negative
Stop word
Percent (SC)
Percentage of words
that are stop words
A high percentage
of stop words im-
plies that the text is
not very complex and
hence easier to read.
Avg. Sen-
tence Length
(SL)
Average of the char-
acter length of sen-
tences in the review
Long sentences in a
review may make it
harder to read.
Table 2: Instance characteristics
Feature Mean Standard Deviation
CL 2.25 0.92
PC 41.50 20.39
SP 0.45 0.03
SL 121.90 28.72
NR 4.80 2.30
Table 3: Mean and the standard deviation for the feature oc-
currences in the data.
3.3 Evaluation Metric
We use both Root Mean Square (RMS) error and
Correlation Coefficient (CRCoef) to evaluate our
model, since the two metrics evaluate different as-
pects of an estimate. RMS is a way to quantify the
amount by which an estimator differs from the true
value of the quantity being estimated. It tells us how
?off? our estimate is from the truth. CRCoef on the
other hand measures the strength and direction of a
linear relationship between two random variables. It
tells us how well correlated our estimate is with the
actual annotation time. Thus, for evaluating how ac-
curate our model is in predicting annotation times,
RMS is a more appropriate metric. For evaluating
the utility of the estimated annotation cost as a cri-
terion for ranking and selecting examples for user?s
annotation, CRCoef is a better metric.
3.4 Experiments & Results
We learn an annotation cost estimator using the Lin-
ear Regression and SMO Regression (Smola and
Scholkopf, 1998) learners from the Weka machine
learning toolkit (Witten and Frank, 2005). As men-
tioned earlier, we have 5 sets of 25 documents each,
and each set was annotated by four annotators. The
results reported are averaged over five folds, where
each set is one fold, and two algorithms (Linear Re-
gression and SMO Regression). Varying the algo-
rithm helps us find the most predictive feature com-
binations across different algorithms. Since each set
was annotated by different annotators, we never train
and test on the data from same annotators. We used
the JMP2 and Minitab3 statistical tools for our analy-
sis. We used an ANOVA model with Standard Least
Squares fitting to compare the different experimen-
tal conditions. We make all comparisons in terms
of both the CRCoef and the RMS metrics. For sig-
nificance results reported, we used 2-tailed paired
T-test, considering (p < 0.05) as significant.
We present our results and analysis in three parts.
We first compare the four instance characteristics,
annotator and annotation task characteristics; and
their combination. We then present an analysis
of the interaction between features and annotation
time. Finally, we compare the ranking of features
based on the two evaluation metrics we used.
3.4.1 Comparing characteristics for annotation
cost estimation
Instance Characteristics: We compare the four
instance characteristics described in Section 3.2.1
and select the most predictive characteristic for fur-
ther analysis with annotator and annotation task
characteristics. As can be seen in Table 4, character
length performs the best, and it is significantly better
than stop word percent and average sentence length.
Character length also outperforms polar word count,
but this difference is not significant. Because of the
large significant difference between the performance
of stop word percent and average sentence length,
compared to character length, we do not consider
them for further analysis.
Feature Combinations: In Table 5, we compare
the feature combinations of instance, annotator and
annotation task characteristics. The table also shows
the weights for the features used and the constant for
the linear regression model trained on all the data. A
missing weight for a feature indicates that it wasn?t
used in that feature combination.
2http://www.jmp.com/software/
3http://www.minitab.com/
22
Feature CR-Coef RMS
CL 0.358 104.51
PC 0.337 105.92
SP -0.041* 114.34*
SL 0.042* 114.50*
Table 4: CR-Coef and RMS results for Character Length
(CL), Polar word Count (PC), Stop word Percent (SP) and av-
erage Sentence Length (SL). Best performance is highlighted
in bold. ? marks the results significantly worse than the best.
We use only the best performing instance charac-
teristic, the character length. The length of an ex-
ample has often been substituted for the annotation
cost (Kapoor et al, 2007; Tomanek et al, 2007).
We show in Table 5 that certain feature combina-
tions significantly outperform character length. The
combination of all three features (last row) performs
the best for both CRCoef and RMS, and this result
is significantly better than the character length (third
row). The combination of number of rationales and
nativeness (fourth row) also outperforms character
length significantly in CRCoef. This suggests that
the number of rationales we expect or require in a re-
view and the annotator characteristics are important
factors for annotation cost estimation and should be
considered in addition to the character length.
CL NR AN Const. CR-Coef RMS
-29.33 220.77 0.135? 123.93?
17.59 82.81 0.486 95.29
0.027 61.53 0.357? 104.51?
19.11 -40.78 153.21 0.55+ 96.04
0.028 32.79 120.18 0.397? 109.85?
0.02 15.15 17.57 0.553+ 90.27+
0.021 16.64 -41.84 88.09 0.626+ 88.44+
Table 5: CR-Coef and RMS results for seven feature com-
binations of Character Length (CL), Number of Rationales
(NR) and Annotator Nativeness (AN). The values in feature
and ?Const.? columns are weights and constant for the linear
regression model trained on all the data. The numbers in bold
are the results for the best feature combination. ? marks the
results significantly worse than the best. + marks the results
significantly better than CL.
The impact of the nativeness feature is somewhat
mixed. Adding the nativeness feature always im-
proves the correlation and for RMS, it helps when
added to the combined feature (CL+NR) but not oth-
erwise. Although this improvement with addition
of the nativeness feature is not significant, it does
suggest that annotator characteristics might be im-
portant to consider. To investigate this further, we
evaluated our assumption that native speakers take
less time to annotate. For each set, we compared the
average annotation times (averaged over examples)
against the nativeness values. For all sets, annotators
with nativeness value of 3 always took less time on
average than those with nativeness value of 2 or 1.
Between 2 and 1, there were no reliable differences.
Sometimes annotators with value of 1 took less time
than annotators with value of 2. Also, for group 2
which had all annotators with nativeness value of 1,
we observed a poor correlation between annotators
(Table 1). This suggest two things: 1) our assign-
ment of nativeness value may not be accurate and
we need other ways of quantifying nativeness, 2)
there are other annotator characteristics we should
take into consideration.
PC CL NR AN Const. CR RMS
0.027 61.53 0.358ab 104.5x
2.2 74.20 0.337a 105.9x
0.7 0.019 60.89 0.355b 104.9x
0.028 -32.8 120.2 0.397ab 109.8x
2.3 -35.5 135.1 0.382a 111.1x
1.1 0.016 -34.3 121.8 0.395b 109.9x
0.02 15.1 17.57 0.553a 90.27x
1.5 15.1 32.02 0.542a 91.65x
0.0 0.02 15.1 17.57 0.554a 90.40x
0.021 16.6 -41.8 88.09 0.626a 88.44x
1.6 16.5 -43.5 102.8 0.614a 90.42y
0.0 0.021 16.6 -41.8 88.09 0.626a 88.78x
Table 6: Each block of 3 rows in this table compares the
performance of Character Length (CL) and Polar word Count
(PC) in combination with Number of Rationales (NR) and An-
notator Nativeness (AN) features. The values in feature and
?Const.? columns are weights and constant for the linear re-
gression model trained on all the data. Best performance is
highlighted in bold. Results in a block not connected by same
letter are significantly different.
Polar word Count and Character Length: As we
saw in Table 4, the difference between character
length and polar word count is not significant. We
further compare these two instance characteristics
in the presence of the annotator and annotation task
characteristics. Our goal is to ascertain whether
character length performs better than polar word
count, or vice versa, and whether this difference is
significant. We also evaluate whether using both
performs better than using any one of them alone.
The results presented in Table 6 help us answer these
questions. For all feature combinations character
length, with and without polar word count, performs
23
better than polar word count, but this difference is
not significant except in three cases. These results
suggests that polar word count can be used as an al-
ternative to character length in annotation cost esti-
mation.
3.4.2 Interaction between Features and
Annotation Time
As a post-experiment analysis, we studied the
interaction between the features we used and an-
notation time, and the interaction among features
themselves. Table 7 reports the pairwise correlation
(Pearson, 1895) for these variables, calculated over
all 125 reviews. As can be seen, all features have
significant correlation with annotation time except
stop words percentage and average sentence length.
Note that number of rationales has higher correla-
tion with annotation time (R = 0.529) than charac-
ter length (R = 0.417). This suggests that number
of rationales may have more influence than charac-
ter length on annotation time, and a low correlation
between number of rationales and character length
(R = 0.238) indicates that it might not be the case
that longer documents necessarily contain more ra-
tionales. Annotating rationales requires cognitive
effort of identifying the right span and manual ef-
fort to highlight and add an annotation, and hence
more rationales implies more annotation time. We
also found some examples in our data where docu-
ments with substantially different lengths but same
number of rationales took a similar time to anno-
tate. One possible explanation for this observation is
user?s annotation strategy. If the annotator chooses
to skim through the remaining text when enough ra-
tionales are found, two examples with same number
of rationales but different lengths might take similar
time. We plan to investigate the effect of annotator?s
strategy on annotation time in the future.
A negative correlation of nativeness with annota-
tion time (R = ?0.219) is expected, since native
speakers (AN = 3) are expected to take less anno-
tation time than non-native speakers (AN = {2, 1}),
although this correlation is low. A low correla-
tion between number of rationales and nativeness
(R = 0.149) suggests that number of rationales
a user adds may not be influenced much by their
nativeness value. A not significant low correlation
(R = ?0.06) between character length and native-
AT CL NR AN PC SP SL
AT 1
CL 0.42 1
NR 0.53 0.24 1
AN -0.22 0.06 0.15 1
PC 0.4 0.89 0.28 0.11 1
SP 0.03 0.06 0.14 0.03 0.04 1
SL 0.08 0.15 0.01 -0.01 0.14 -0.13 1
Table 7: Correlation between Character Length (CL), Num-
ber of Rationales (NR), Annotator Nativeness (AN), Polar
word Count (PC), Stop word Percent (SP), average Sentence
Length (SL) and Annotation Time (AT), calculated over all
documents (125) and all annotators (20). Significant corre-
lations are highlighted in bold.
ness provides no evidence that reviews with different
lengths were distributed non-uniformly across anno-
tators with different nativeness.
The number of polar words in a document has a
similar correlation with annotation time as character
length (R = 0.4). There is also a strong correla-
tion between character length and polar word count
(R = 0.89). Since reviews are essentially people?s
opinions, we can expect longer documents to have
more polar words. This also explains why there is no
significant difference in performance for polar word
count and character length (Table 4). A more useful
feature may be the information about the number of
positive and negative polar words in a review, since a
review with both positive and negative opinions can
be difficult to classify as positive or negative. We
plan to explore these variations of the polar word
feature in the future. We also plan to investigate how
we can exploit this dependence between characteris-
tics for annotation cost estimation.
3.4.3 CRCoef Vs. RMS
We presented our results using correlation coef-
ficient and root mean squared error metrics. Ta-
ble 8 shows the ranking of the feature combinations
from better to worse for both these metrics and as
we can see, there is a difference in the order of fea-
ture combinations for the two metrics. Also, signif-
icance results differ in some cases for the two met-
rics. These differences suggest that features which
correlate well with the annotation times (higher CR-
Coef rank) can give an accurate ranking of examples
based on their annotation cost, but they may not be
as accurate in their absolute estimate for simulating
annotators and thus might have a lower RMS rank.
Thus, it is important to evaluate the user effort esti-
24
mator in terms of both these metrics so that the right
estimator can be chosen for a given objective.
Rank CR-Coef RMS
1 (CL+NR+AN) (CL+NR+AN)
2 (CL+NR) (CL+NR)
3 (NR+AN) (NR)
4 (NR) (NR+AN)
5 (CL+AN) (CL)
6 (CL) (CL+AN)
7 (AN) (AN)
Table 8: Ranking of feature combinations.
4 Towards a General Annotation Cost
Estimator
Our multi-annotator environment allows us to train
and test on data from different annotators by using
annotator characteristics as features in the annota-
tion cost estimation. A model trained on data from a
variety of annotators can be used for recommend-
ing examples to annotators not represented in our
training data but with similar characteristics. This
is important since we may not always know all our
annotators before building the model, and training
an estimator for each new annotator is costly. Also,
in active learning research, the goal is to evaluate
selective sampling approaches independently of the
annotator. Choosing annotators for supervised an-
notation cost estimation such that the within group
variance in annotator characteristics is high will give
us a more generic estimator and a stricter evaluation
criterion. Thus, we have a framework that has the
potential to be used to build a user-independent an-
notation cost estimator for a given task.
However, this framework is specific to the User
Interface (UI) used. A change in the user interface
might require recollecting the data from all the an-
notators and training a model on the new data. For
example, if annotating rationales was made signif-
icantly faster in a new UI design, it would have
a major impact on annotation cost. An alternative
would be to incorporate UI features in our model and
train it on several different UIs or modifications of
the same UI, which will allow us to use our trained
model with a new user interface or modifications of
the existing UIs, without having to recollect the data
and retrain the model. A few UI features that can be
used in our context are: adding a rationale annota-
tion, voting positive or negative, etc. The units for
expressing these features will be the low-level user
interface actions such as number of clicks, mouse
drags, etc. For example, in our task, adding a ra-
tionale annotation requires one mouse drag and two
clicks, and adding a vote requires one click. In a dif-
ferent user interface, adding a rationale annotation
might require just one mouse drag.
Using UI features raises a question of whether
they can replace the annotation task features; e.g.,
whether the UI feature for adding rationale anno-
tation can replace the number of rationales feature.
Our hypothesis is that number of rationales has more
influence on annotation time than just the manual ef-
fort of annotating them. It also requires the cognitive
effort of finding the rationale, deciding its span, etc.
We aim to explore incorporating UI features in our
annotation cost estimation model in the future.
5 Conclusion and Future Work
In this work we presented a detailed investigation of
annotation cost estimation for active learning with
multiple annotators. We motivated the task from two
perspectives: selecting examples to minimize anno-
tation cost and simulating annotators for evaluating
active learning approaches. We defined three cate-
gories of features based on instance, annotation task
and annotator characteristics. Our results show that
using a combination of features from all three cate-
gories performs better than any one of them alone.
Our analysis was limited to a small dataset. In the
future, we plan to collect a larger dataset for this task
and explore more features from each feature group.
With the multi-annotator annotation cost estima-
tor proposed, we also motivated the need for a gen-
eral estimator that can be used with new annotators
or user interfaces without having to retrain. We aim
to explore this direction in the future by extending
our model to incorporate user interface features. We
also plan to use the annotation cost model we devel-
oped in an active learning experiment.
Acknowledgments
We would like to thank Hideki Shima for his help
with the task setup and Jing Yang for helpful discus-
sions. We would also like to thank all the anony-
mous reviewers for their helpful comments.
25
References
Shilpa Arora and Eric Nyberg. 2009. Interactive An-
notation Learning with Indirect Feature Voting. In
Proceedings of NAACL-HLT 2009 (Student Research
Workshop).
Xavier Carreras and Llu?`s Ma?rquez. 2004. Intro-
duction to the CoNLL-2004 Shared Task: Seman-
tic Role Labeling. http://www.lsi.upc.edu/
?srlconll/st04/st04.html.
Gahgene Gweon, Carolyn Penstein Ros?e, Joerg Wittwer
and Matthias Nueckles. 2005. Supporting Efficient
and Reliable Content Analysis Using Automatic Text
Processing Technology. In proceedings of INTER-
ACT 2005: 1112-1115.
Robbie A. Haertel, Kevin D. Seppi, Eric K. Ringger and
Janes L. Cattoll. 2008. Return on Investment for Ac-
tive Learning. In proceedings of NIPS Workshop on
Cost Sensitive Learning.
Rebecca Hwa. 2000. Sample Selection for Statistical
Grammar Induction. In proceedings of joint SIGDAT
conference on Empirical Methods in NLP and Very
Large Corpora.
Ashish Kapoor, Eric Horvitz and Sumit Basu. 2007. Se-
lective supervision:Guiding supervised learning with
decision-theoretic active learning. In proceedings of
IJCAI, pages 877-882.
Ross D. King, Kenneth E. Whelan, Ffion M. Jones, Philip
G. K. Reiser, Christopher H. Bryant, Stephen H. Mug-
gleton, Douglas B. Kell and Stephen G. Oliver. 2004.
Functional Genomics hypothesis generation and ex-
perimentation by a robot scientist. In proceedings of
Nature, 427(6971):247-52.
Trausti Kristjansson, Aron Culotta, Paul Viola and An-
drew Mccallum. 2004. Interactive Information Ex-
traction with Constrained Conditional Random Fields.
In proceedings of AAAI.
Karl Pearson. 1895. Correlation Coefficient. Royal So-
ciety Proceedings, 58, 214.
Eric Ringger, Marc Carmen, Robbie Haertel, Kevin
Seppi, Deryle Lonsdale, Peter McClanahan, Janes L.
Cattoll and Noel Ellison. 2008. Assessing the Costs of
Machine-Assisted Corpus Annotation through a User
Study. In proceedings of LREC.
Burr Settles, Mark Craven and Lewis Friedland. 2008.
Active Learning with Real Annotation Costs. In pro-
ceedings of NIPS Workshop on Cost Sensitive Learn-
ing.
Alex J. Smola and Bernhard Scholkopf 1998. A Tutorial
on Support Vector Regression. NeuroCOLT2 Techni-
cal Report Series - NC2-TR-1998-030.
Katrin Tomanek, Joachim Wermter and Udo Hahn. 2007.
An approach to text corpus construction which cuts an-
notation costs and maintains reusability of annotated
data. In proceedings of EMNLP-CoNLL, pp. 486-
495.
Theresa Wilson, Janyce Wiebe and Paul Hoffmann.
2005. Recognizing Contextual Polarity in Phrase-
Level Sentiment Analysis. In proceedings of
HLT/EMNLP, Vancouver, Canada.
Ian H. Witten and Eibe Frank. 2005. Data Mining: Prac-
tical machine learning tools and techniques. 2nd Edi-
tion, Morgan Kaufmann, San Francisco.
Omar Zaidan, Jason Eisner and Christine Piatko. 2007.
Using ?annotator rationales? to improve machine
learning for text categorization. In Proceedings of
NAACL-HLT, pp. 260-267, Rochester, NY.
26
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 248?258,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Metaphor Detection with Cross-Lingual Model Transfer
Yulia Tsvetkov Leonid Boytsov Anatole Gershman Eric Nyberg Chris Dyer
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213 USA
{ytsvetko, srchvrs, anatoleg, ehn, cdyer}@cs.cmu.edu
Abstract
We show that it is possible to reliably dis-
criminate whether a syntactic construction
is meant literally or metaphorically using
lexical semantic features of the words that
participate in the construction. Our model
is constructed using English resources,
and we obtain state-of-the-art performance
relative to previous work in this language.
Using a model transfer approach by piv-
oting through a bilingual dictionary, we
show our model can identify metaphoric
expressions in other languages. We pro-
vide results on three new test sets in Span-
ish, Farsi, and Russian. The results sup-
port the hypothesis that metaphors are
conceptual, rather than lexical, in nature.
1 Introduction
Lakoff and Johnson (1980) characterize metaphor
as reasoning about one thing in terms of another,
i.e., a metaphor is a type of conceptual mapping,
where words or phrases are applied to objects and
actions in ways that do not permit a literal inter-
pretation. They argue that metaphors play a fun-
damental communicative role in verbal and writ-
ten interactions, claiming that much of our every-
day language is delivered in metaphorical terms.
There is empirical evidence supporting the claim:
recent corpus studies have estimated that the pro-
portion of words used metaphorically ranges from
5% to 20% (Steen et al, 2010), and Thibodeau and
Boroditsky (2011) provide evidence that a choice
of metaphors affects decision making.
Given the prevalence and importance of
metaphoric language, effective automatic detec-
tion of metaphors would have a number of ben-
efits, both practical and scientific. Language pro-
cessing applications that need to understand lan-
guage or preserve meaning (information extrac-
tion, machine translation, dialog systems, senti-
ment analysis, and text analytics, etc.) would have
access to a potentially useful high-level bit of in-
formation about whether something is to be under-
stood literally or not. Second, scientific hypothe-
ses about metaphoric language could be tested
more easily at a larger scale with automation.
However, metaphor detection is a hard problem.
On one hand, there is a subjective component: hu-
mans may disagree whether a particular expres-
sion is used metaphorically or not, as there is no
clear-cut semantic distinction between figurative
and metaphorical language (Shutova, 2010). On
the other, metaphors can be domain- and context-
dependent.
1
Previous work has focused on metaphor identi-
fication in English, using both extensive manually-
created linguistic resources (Mason, 2004; Gedi-
gian et al, 2006; Krishnakumaran and Zhu, 2007;
Turney et al, 2011; Broadwell et al, 2013) and
corpus-based approaches (Birke and Sarkar, 2007;
Shutova et al, 2013; Neuman et al, 2013; Shutova
and Sun, 2013; Hovy et al, 2013). We build on
this foundation and also extend metaphor detec-
tion into other languages in which few resources
may exist. Our work makes the following con-
tributions: (1) we develop a new state-of-the-art
English metaphor detection system that uses con-
ceptual semantic features, such as a degree of ab-
stractness and semantic supersenses;
2
(2) we cre-
ate new metaphor-annotated corpora for Russian
and English;
3
(3) using a paradigm of model trans-
fer (McDonald et al, 2011; T?ackstr?om et al, 2013;
Kozhenikov and Titov, 2013), we provide sup-
port for the hypothesis that metaphors are concep-
1
For example, drowning students could be used metaphor-
ically to describe the situation where students are over-
whelmed with work, but in the sentence a lifeguard saved
drowning students, this phrase is used literally.
2
https://github.com/ytsvetko/metaphor
3
http://www.cs.cmu.edu/
?
ytsvetko/
metaphor/datasets.zip
248
tual (rather than lexical) in nature by showing that
our English-trained model can detect metaphors in
Spanish, Farsi, and Russian.
2 Methodology
Our task in this work is to define features that dis-
tinguish between metaphoric and literal uses of
two syntactic constructions: subject-verb-object
(SVO) and adjective-noun (AN) tuples.
4
We give
examples of a prototypical metaphoric usage of
each type:
? SVO metaphors. A sentence containing a
metaphoric SVO relation is my car drinks
gasoline. According to Wilks (1978), this
metaphor represents a violation of selectional
preferences for the verb drink, which is nor-
mally associated with animate subjects (the
car is inanimate and, hence, cannot drink in
the literal sense of the verb).
? AN metaphors. The phrase broken promise
is an AN metaphor, where attributes from
a concrete domain (associated with the con-
crete word broken) are transferred to a more
abstract domain, which is represented by the
relatively abstract word promise. That is, we
map an abstract concept promise to a concrete
domain of physical things, where things can
be literally broken to pieces.
Motivated by Lakoff?s (1980) argument that
metaphors are systematic conceptual mappings,
we will use coarse-grained conceptual, rather than
fine-grained lexical features, in our classifier. Con-
ceptual features pertain to concepts and ideas as
opposed to individual words or phrases expressed
in a particular language. In this sense, as long as
two words in two different languages refer to the
same concepts, their conceptual features should
be the same. Furthermore, we hypothesize that
our coarse semantic features give us a language-
invariant representation suitable for metaphor de-
tection. To test this hypothesis, we use a cross-
lingual model transfer approach: we use bilingual
dictionaries to project words from other syntactic
constructions found in other languages into En-
glish and then apply the English model on the de-
rived conceptual representations.
4
Our decision to focus on SVO and AN metaphors is jus-
tified by corpus studies that estimate that verb- and adjective-
based metaphors account for a substantial proportion of all
metaphoric expressions, approximately 60% and 24%, re-
spectively (Shutova and Teufel, 2010; Gandy et al, 2013).
Each SVO (or AN) instance will be represented
by a triple (duple) from which a feature vector
will be extracted.
5
The vector will consist of the
concatenation of the conceptual features (which
we discuss below) for all participating words, and
conjunction features for word pairs.
6
For example,
to generate the feature vector for the SVO triple
(car, drink, gasoline), we compute all the features
for the individual words car, drink, gasoline and
combine them with the conjunction features for
the pairs car drink and drink gasoline.
We define three main feature categories (1) ab-
stractness and imageability, (2) supersenses, (3)
unsupervised vector-space word representations;
each category corresponds to a group of features
with a common theme and representation.
? Abstractness and imageability. Abstract-
ness and imageability were shown to be use-
ful in detection of metaphors (it is easier to
invoke mental pictures of concrete and im-
ageable words) (Turney et al, 2011; Broad-
well et al, 2013). We expect that abstract-
ness, used in conjunction features (e.g., a
feature denoting that the subject is abstract
and the verb is concrete), is especially use-
ful: semantically, an abstract agent perform-
ing a concrete action is a strong signal of
metaphorical usage.
Although often correlated with abstractness,
imageability is not a redundant property.
While most abstract things are hard to visu-
alize, some call up images, e.g., vengeance
calls up an emotional image, torture calls up
emotions and even visual images. There are
concrete things that are hard to visualize too,
for example, abbey is harder to visualize than
banana (B. MacWhinney, personal commu-
nication).
? Supersenses. Supersenses
7
are coarse se-
mantic categories originating in WordNet.
For nouns and verbs there are 45 classes:
26 for nouns and 15 for verbs, for example,
5
Looking at components of the syntactic constructions in-
dependent of their context has its limitations, as discussed
above with the drowning students example; however, it sim-
plifies the representation challenges considerably.
6
If word one is represented by features u ? R
n
and word
two by features v ? R
m
then the conjunction feature vector
is the vectorization of the outer product uv
>
.
7
Supersenses are called ?lexicographer classes? in Word-
Net documentation (Fellbaum, 1998), http://wordnet.
princeton.edu/man/lexnames.5WN.html
249
noun.body, noun.animal, verb.consumption,
or verb.motion (Ciaramita and Altun, 2006).
English adjectives do not, as yet, have a sim-
ilar high-level semantic partitioning in Word-
Net, thus we use a 13-class taxonomy of ad-
jective supersenses constructed by Tsvetkov
et al (2014) (discussed in ?3.2).
Supersenses are particularly attractive fea-
tures for metaphor detection: coarse sense
taxonomies can be viewed as semantic con-
cepts, and since concept mapping is a pro-
cess in which metaphors are born, we
expect different supersense co-occurrences
in metaphoric and literal combinations.
In ?drinks gasoline?, for example, map-
ping to supersenses would yield a pair
<verb.consumption, noun.substance>, con-
trasted with <verb.consumption, noun.food>
for ?drinks juice?. In addition, this coarse
semantic categorization is preserved in trans-
lation (Schneider et al, 2013), which makes
supersense features suitable for cross-lingual
approaches such as ours.
? Vector space word representations. Vec-
tor space word representations learned us-
ing unsupervised algorithms are often effec-
tive features in supervised learning methods
(Turian et al, 2010). In particular, many such
representations are designed to capture lex-
ical semantic properties and are quite effec-
tive features in semantic processing, includ-
ing named entity recognition (Turian et al,
2009), word sense disambiguation (Huang et
al., 2012), and lexical entailment (Baroni et
al., 2012). In a recent study, Mikolov et
al. (2013) reveal an interesting cross-lingual
property of distributed word representations:
there is a strong similarity between the vec-
tor spaces across languages that can be eas-
ily captured by linear mapping. Thus, vector
space models can also be seen as vectors of
(latent) semantic concepts, that preserve their
?meaning? across languages.
3 Model and Feature Extraction
In this section we describe a classification model,
and provide details on mono- and cross-lingual
implementation of features.
3.1 Classification using Random Forests
To make classification decisions, we use a random
forest classifier (Breiman, 2001), an ensemble of
decision tree classifiers learned from many inde-
pendent subsamples of the training data. Given
an input, each tree classifier assigns a probabil-
ity to each label; those probabilities are averaged
to compute the probability distribution across the
ensemble. Random forest ensembles are partic-
ularly suitable for our resource-scarce scenario:
rather than overfitting, they produce a limiting
value of the generalization error as the number
of trees increases,
8
and no hyperparameter tuning
is required. In addition, decision-tree classifiers
learn non-linear responses to inputs and often out-
perform logistic regression (Perlich et al, 2003).
9
Our random forest classifier models the probabil-
ity that the input syntactic relation is metaphorical.
If this probability is above a threshold, the relation
is classified as metaphoric, otherwise it is literal.
We used the scikit-learn toolkit to train our
classifiers (Pedregosa et al, 2011).
3.2 Feature extraction
Abstractness and imageability. The MRC psy-
cholinguistic database is a large dictionary listing
linguistic and psycholinguistic attributes obtained
experimentally (Wilson, 1988).
10
It includes,
among other data, 4,295 words rated by the de-
grees of abstractness and 1,156 words rated by the
imageability. Similarly to Tsvetkov et al (2013),
we use a logistic regression classifier to propagate
abstractness and imageability scores from MRC
ratings to all words for which we have vector space
representations. More specifically, we calculate
the degree of abstractness and imageability of all
English items that have a vector space representa-
tion, using vector elements as features. We train
two separate classifiers for abstractness and im-
ageability on a seed set of words from the MRC
database. Degrees of abstractness and imageabil-
ity are posterior probabilities of classifier predic-
tions. We binarize these posteriors into abstract-
concrete (or imageable-unimageable) boolean in-
dicators using pre-defined thresholds.
11
Perfor-
8
See Theorem 1.2 in (Breiman, 2001) for details.
9
In our experiments, random forests model slightly out-
performed logistic regression and SVM classifiers.
10
http://ota.oucs.ox.ac.uk/headers/
1054.xml
11
Thresholds are equal to 0.8 for abstractness and to 0.9
for imageability. They were chosen empirically based on ac-
250
mance of these classifiers, tested on a sampled
held-out data, is 0.94 and 0.85 for the abstractness
and imageability classifiers, respectively.
Supersenses. In the case of SVO relations, we
incorporate supersense features for nouns and
verbs; noun and adjective supersenses are used in
the case of AN relations.
Supersenses of nouns and verbs. A lexical item
can belong to several synsets, which are associ-
ated with different supersenses. Degrees of mem-
bership in different supersenses are represented
by feature vectors, where each element corre-
sponds to one supersense. For example, the word
head (when used as a noun) participates in 33
synsets, three of which are related to the super-
sense noun.body. The value of the feature corre-
sponding to this supersense is 3/33 ? 0.09.
Supersenses of adjectives. WordNet lacks
coarse-grained semantic categories for adjectives.
To divide adjectives into groups, Tsvetkov et al
(2014) use 13 top-level classes from the adapted
taxonomy of Hundsnurscher and Splett (1982),
which is incorporated in GermaNet (Hamp and
Feldweg, 1997). For example, the top-level
classes in GermaNet include: adj.feeling (e.g.,
willing, pleasant, cheerful); adj.substance (e.g.,
dry, ripe, creamy); adj.spatial (e.g., adjacent, gi-
gantic).
12
For each adjective type in WordNet,
they produce a vector with a classifier posterior
probabilities corresponding to degrees of mem-
bership of this word in one of the 13 semantic
classes,
13
similar to the feature vectors we build
for nouns and verbs. For example, for a word
calm the top-2 categories (with the first and second
highest degrees of membership) are adj.behavior
and adj.feeling.
Vector space word representations. We em-
ploy 64-dimensional vector-space word represen-
tations constructed by Faruqui and Dyer (2014).
14
Vector construction algorithm is a variation on
traditional latent semantic analysis (Deerwester
et al, 1990) that uses multilingual information
to produce representations in which synonymous
words have similar vectors. The vectors were
curacy during cross-validation.
12
For the full taxonomy see http://www.sfs.
uni-tuebingen.de/lsd/adjectives.shtml
13
http://www.cs.cmu.edu/
?
ytsvetko/
adj-supersenses.tar.gz
14
http://www.cs.cmu.edu/
?
mfaruqui/soft.
html
trained on the news commentary corpus released
by WMT-2011,
15
comprising 180,834 types.
3.3 Cross-lingual feature projection
For languages other than English, feature vectors
are projected to English features using translation
dictionaries. We used the Babylon dictionary,
16
which is a proprietary resource, but any bilingual
dictionary can in principle be used. For a non-
English word in a source language, we first ob-
tain all translations into English. Then, we av-
erage all feature vectors related to these transla-
tions. Consider an example related to projection
of WordNet supersenses. A Russian word ??????
is translated as head and brain. Hence, we select
all the synsets of the nouns head and brain. There
are 38 such synsets (33 for head and 5 for brain).
Four of these synsets are associated with the su-
persense noun.body. Therefore, the value of the
feature noun.body is 4/38 ? 0.11.
4 Datasets
In this section we describe a training and testing
dataset as well a data collection procedure.
4.1 English training sets
To train an SVO metaphor classifier, we employ
the TroFi (Trope Finder) dataset.
17
TroFi includes
3,737 manually annotated English sentences from
the Wall Street Journal (Birke and Sarkar, 2007).
Each sentence contains either literal or metaphori-
cal use for one of 50 English verbs. First, we use a
dependency parser (Martins et al, 2010) to extract
subject-verb-object (SVO) relations. Then, we fil-
ter extracted relations to eliminate parsing-related
errors, and relations with verbs which are not in
the TroFi verb list. After filtering, there are 953
metaphorical and 656 literal SVO relations which
we use as a training set.
In the case of AN relations, we construct and
make publicly available a training set contain-
ing 884 metaphorical AN pairs and 884 pairs
with literal meaning. It was collected by two
annotators using public resources (collections of
metaphors on the web). At least one additional
person carefully examined and culled the col-
lected metaphors, by removing duplicates, weak
metaphors, and metaphorical phrases (such as
15
http://www.statmt.org/wmt11/
16
http://www.babylon.com
17
http://www.cs.sfu.ca/
?
anoop/students/
jbirke/
251
drowning students) whose interpretation depends
on the context.
4.2 Multilingual test sets
We collect and annotate metaphoric and literal test
sentences in four languages. Thus, we compile
eight test datasets, four for SVO relations, and
four for AN relations. Each dataset has an equal
number of metaphors and non-metaphors, i.e., the
datasets are balanced. English (EN) and Russian
(RU) datasets have been compiled by our team
and are publicly available. Spanish (ES) and Farsi
(FA) datasets are published elsewhere (Levin et al,
2014). Table 1 lists test set sizes.
SVO AN
EN 222 200
RU 240 200
ES 220 120
FA 44 320
Table 1: Sizes of the eight test sets. Each dataset is
balanced, i.e., it has an equal number of metaphors
and non-metaphors. For example, English SVO
dataset has 222 relations: 111 metaphoric and 111
literal.
We used the following procedure to compile the
EN and RU test sets. A moderator started with seed
lists of 1000 most common verbs and adjectives.
18
Then she used the SketchEngine, which pro-
vides searching capability for the TenTen Web cor-
pus,
19
to extract sentences with words that fre-
quently co-occurred with words from the seed
lists. From these sentences, she removed sen-
tences that contained more than one metaphor, and
sentences with non-SVO and non-AN metaphors.
Remaining sentences were annotated by several
native speakers (five for English and six for Rus-
sian), who judged AN and SVO phrases in con-
text. The annotation instructions were general:
?Please, mark in bold all words that, in your opin-
ion, are used non-literally in the following sen-
tences. In many sentences, all the words may be
used literally.? The Fleiss? Kappas for 5 English
and 6 Russian annotators are: EN-AN = .76, RU-
18
Selection of 1000 most common verbs and adjectives
achieves much broader lexical and domain coverage than
what can be realistically obtained from continuous text. Our
test sentence domains are, therefore, diverse: economic, po-
litical, sports, etc.
19
http://trac.sketchengine.co.uk/wiki/
Corpora/enTenTen
AN = .85, EN-SVO = .75, RU-SVO = .78. For the fi-
nal selection, we filtered out low-agreement (<.8)
sentences.
The test candidate sentences were selected by
a person who did not participate in the selection
of the training samples. No English annotators of
the test set, and only one Russian annotator out
of 6 participated in the selection of the training
samples. Thus, we trust that annotator judgments
were not biased towards the cases that the system
is trained to process.
5 Experiments
5.1 English experiments
Our task, as defined in Section 2, is to classify
SVO and AN relations as either metaphoric or lit-
eral. We first conduct a 10-fold cross-validation
experiment on the training set defined in Section
4.1. We represent each candidate relation using
the features described in Section 3.2, and evalu-
ate performance of the three feature categories and
their combinations. This is done by computing an
accuracy in the 10-fold cross validation. Experi-
mental results are given in Table 2, where we also
provide the number of features in each feature set.
SVO AN
# FEAT ACC # FEAT ACC
AbsImg 20 0.73
?
16 0.76
?
Supersense 67 0.77
?
116 0.79
?
AbsImg+Sup. 87 0.78
?
132 0.80
?
VSM 192 0.81 228 0.84
?
All 279 0.82 360 0.86
Table 2: 10-fold cross validation results for three
feature categories and their combination, for clas-
sifiers trained on English SVO and AN training
sets. # FEAT column shows a number of features.
ACC column reports an accuracy score in the 10-
fold cross validation. Statistically significant dif-
ferences (p < 0.01) from the all-feature combina-
tion are marked with a star.
These results show superior performance over
previous state-of-the-art results, confirming our
hypothesis that conceptual features are effective
in metaphor classification. For the SVO task, the
cross-validation accuracy is about 10% better than
that of Tsvetkov et al (2013). For the AN task,
the cross validation accuracy is better by 8% than
the result of Turney et al (2011) (two baseline
252
methods are described in Section 5.2). We can
see that all types of features have good perfor-
mance on their own (VSM is the strongest feature
type). Noun supersense features alone allows us to
achieve an accuracy of 75%, i.e., adjective super-
sense features contribute 4% to adjective-noun su-
persense feature combination. Experiments with
the pairs of features yield better results than in-
dividual features, implying that the feature cate-
gories are not redundant. Yet, combining all fea-
tures leads to even higher accuracy during cross-
validation. In the case of the AN task, a difference
between the All feature combination and any other
combination of features listed in Table 2 is statis-
tically significant (p < 0.01 for both the sign and
the permutation test).
Although the first experiment shows very high
scores, the 10-fold cross-validation cannot fully
reflect the generality of the model, because all
folds are parts of the same corpus. They are col-
lected by the same human judges and belong to the
same domain. Therefore, experiments on out-of-
domain data are crucial. We carry out such exper-
iments using held-out SVO and AN EN test sets,
described in Section 4.2 and Table 1. In this ex-
periment, we measure the f -score. We classify
SVO and AN relations using a classifier trained on
the All feature combination and balanced thresh-
olds. The values of the f -score are 0.76, both for
SVO and AN tasks. This out-of-domain experi-
ment suggests that our classifier is portable across
domains and genres.
However, (1) different application may have
different requirements for recall/precision, and (2)
classification results may be skewed towards hav-
ing high precision and low recall (or vice versa). It
is possible to trade precision for recall by choos-
ing a different threshold. Thus, in addition to
giving a single f -score value for balanced thresh-
olds, we present a Receiver Operator Characteris-
tic (ROC) curve, where we plot a fraction of true
positives against the fraction of false positives for
100 threshold values in the range from zero to one.
The area under the ROC curve (AUC) can be in-
terpreted as the probability that a classifier will as-
sign a higher score to a randomly chosen positive
example than to a randomly chosen negative ex-
ample.
20
For a randomly guessing classifier, the
ROC curve is a dashed diagonal line. A bad classi-
20
Assuming that positive examples are labeled by ones,
and negative examples are labeled by zeros.
fier has an ROC curve that goes close to the dashed
diagonal or even below it.
0.0 0.2 0.4 0.6 0.8 1.0
False Positive Rate
0.0
0.2
0.4
0.6
0.8
1.0
T
r
u
e
 
P
o
s
i
t
i
v
e
 
R
a
t
e
Supersenses (area = 0.77)
AbsImg (area = 0.73)
VSM (area = 0.8)
All (area = 0.79)
(a) SVO
0.0 0.2 0.4 0.6 0.8 1.0
False Positive Rate
0.0
0.2
0.4
0.6
0.8
1.0
T
r
u
e
 
P
o
s
i
t
i
v
e
 
R
a
t
e
AbsImg (area = 0.9)
Supersenses (area = 0.86)
VSM (area = 0.89)
All (area = 0.92)
(b) AN
Figure 1: ROC curves for classifiers trained using
different feature sets (English SVO and AN test
sets).
According to ROC plots in Figure 1, all three
feature sets are effective, both for SVO and for
AN tasks. Abstractness and Imageability features
work better for adjectives and nouns, which is in
line with previous findings (Turney et al, 2011;
Broadwell et al, 2013). It can be also seen that
VSM features are very effective. This is in line
with results of Hovy et al (2013), who found that
it is hard to improve over the classifier that uses
only VSM features.
5.2 Comparison to baselines
In this section, we compare our method to state-of-
the-art methods of Tsvetkov et al (2013) and of
Turney et al (2011), who focused on classifying
SVO and AN relations, respectively.
In the case of SVO relations, we use software
253
and datasets from Tsvetkov et al (2013). These
datasets, denoted as an SVO-baseline, consist of
98 English and 149 Russian sentences. We train
SVO metaphor detection tools on SVO relations
extracted from TroFi sentences and evaluate them
on the SVO-baseline dataset. We also use the same
thresholds for classifier posterior probabilities as
Tsvetkov et al (2013). Our approach is different
from that of Tsvetkov et al (2013) in that it uses
additional features (vector space word representa-
tions) and a different classification method (we use
random forests while Tsvetkov et al (2013) use
logistic regression). According to Table 3, we ob-
tain higher performance scores for both Russian
and English.
EN RU
SVO-baseline 0.78 0.76
This work 0.86 0.85
Table 3: Comparing f -scores of our SVO
metaphor detection method to the baselines.
In the case of AN relations, we use the dataset
(denoted as an AN-baseline) created by Turney
et al (2011) (see Section 4.1 in the referred pa-
per for details). Turney et al (2011) manu-
ally annotated 100 pairs where an adjective was
one of the following: dark, deep, hard, sweet,
and worm. The pairs were presented to five
human judges who rated each pair on a scale
from 1 (very literal/denotative) to 4 (very non-
literal/connotative). Turney et al (2011) train
logistic-regression employing only abstractness
ratings as features. Performance of the method
was evaluated using the 10-fold cross-validation
separately for each judge.
We replicate the above described evaluation
procedure of Turney et al (2011) using their
model and features. In our classifier, we use the
All feature combination and the balanced thresh-
old as described in Section 5.1.
According to results in Table 4, almost all of the
judge-specific f -scores are slightly higher for our
system, as well as the overall average f -score.
In both baseline comparisons, we obtain perfor-
mance at least as good as in previously published
studies.
5.3 Cross-lingual experiments
In the next experiment we corroborate the main
hypothesis of this paper: a model trained on En-
AN-baseline This work
Judge 1 0.73 0.75
Judge 2 0.81 0.84
Judge 3 0.84 0.88
Judge 4 0.79 0.81
Judge 5 0.78 0.77
average 0.79 0.81
Table 4: Comparing AN metaphor detection
method to the baselines: accuracy of the 10-
fold cross validation on annotations of five human
judges.
glish data can be successfully applied to other
languages. Namely, we use a trained English
model discussed in Section 5.1 to classify literal
and metaphoric SVO and AN relations in English,
Spanish, Farsi and Russian test sets, listed in Sec-
tion 4.2. This time we used all available features.
Experimental results for all four languages, are
given in Figure 2. The ROC curves for SVO and
AN tasks are plotted in Figure 2a and Figure 2b,
respectively. Each curve corresponds to a test set
described in Table 1. In addition, we perform an
oracle experiment, to obtain actual f -score values
for best thresholds. Detailed results are shown in
Table 5.
Consistent results with high f -scores are ob-
tained across all four languages. Note that higher
scores are obtained for the Russian test set. We hy-
pothesize that this happens due to a higher-quality
translation dictionary (which allows a more accu-
rate model transfer). Relatively lower (yet rea-
sonable) results for Farsi can be explained by a
smaller size of the bilingual dictionary (thus, fewer
feature projections can be obtained). Also note
that, in our experience, most of Farsi metaphors
are adjective-noun constructions. This is why the
AN FA dataset in Table 1 is significantly larger
than SVO FA. In that, for the AN Farsi task we
observe high performance scores.
Figure 2 and Table 5 confirm, that we ob-
tain similar, robust results on four very differ-
ent languages, using the same English classi-
fiers. We view this result as a strong evidence of
language-independent nature of our metaphor de-
tection method. In particular, this shows that pro-
posed conceptual features can be used to detect se-
lectional preferences violation across languages.
To summarize the experimental section, our
metaphor detection approach obtains state-of-the-
254
0.0 0.2 0.4 0.6 0.8 1.0
False Positive Rate
0.0
0.2
0.4
0.6
0.8
1.0
T
r
u
e
 
P
o
s
i
t
i
v
e
 
R
a
t
e
EN (area = 0.79)
ES (area = 0.71)
FA (area = 0.69)
RU (area = 0.89)
(a) SVO
0.0 0.2 0.4 0.6 0.8 1.0
False Positive Rate
0.0
0.2
0.4
0.6
0.8
1.0
T
r
u
e
 
P
o
s
i
t
i
v
e
 
R
a
t
e
EN (area = 0.92)
ES (area = 0.73)
FA (area = 0.83)
RU (area = 0.8)
(b) AN
Figure 2: Cross-lingual experiment: ROC curves
for classifiers trained on the English data using a
combination of all features, and applied to SVO
and AN metaphoric and literal relations in four test
languages: English, Russian, Spanish, and Farsi.
art performance in English, is effective when ap-
plied to out-of-domain English data, and works
cross-lingually.
5.4 Examples
Manual data analysis on adjective-noun pairs sup-
ports an abstractness-concreteness hypothesis for-
mulated by several independent research studies.
For example, in English we classify as metaphoric
dirty word and cloudy future. Word pairs dirty
diaper and cloudy weather have same adjectives.
Yet they are classified as literal. Indeed, diaper
is a more concrete term than word and weather
is more concrete than future. Same pattern is ob-
served in non-English datasets. In Russian, ????-
??? ???????? ?sick society? and ?????? ????
?empty sound? are classified as metaphoric, while
SVO AN
EN 0.79 0.85
RU 0.84 0.77
ES 0.76 0.72
FA 0.75 0.74
Table 5: Cross-lingual experiment: f -scores for
classifiers trained on the English data using a com-
bination of all features, and applied, with optimal
thresholds, to SVO and AN metaphoric and literal
relations in four test languages: English, Russian,
Spanish, and Farsi.
??????? ??????? ?sick grandmother? and ??-
???? ????? ?empty cup? are classified as literal.
Spanish example of an adjective-noun metaphor
is a well-known m?usculo econ?omico ?economic
muscle?. We also observe that non-metaphoric ad-
jective noun pairs tend to have more imageable ad-
jectives, such as literal derecho humano ?human
right?. In Spanish, human is more imageable than
economic.
Verb-based examples that are correctly clas-
sified by our model are: blunder escaped no-
tice (metaphoric) and prisoner escaped jail (lit-
eral). We hypothesize that supersense features are
instrumental in the correct classification of these
examples: <noun.person,verb.motion> is usually
used literally, while <noun.act,verb.motion> is
used metaphorically.
6 Related Work
For a historic overview and a survey of
common approaches to metaphor detection,
we refer the reader to recent reviews by
Shutova et al (Shutova, 2010; Shutova et al,
2013). Here we focus only on recent approaches.
Shutova et al (2010) proposed a bottom-up
method: one starts from a set of seed metaphors
and seeks phrases where verbs and/or nouns be-
long to the same cluster as verbs or nouns in seed
examples.
Turney et al (2011) show how abstractness
scores could be used to detect metaphorical AN
phrases. Neuman et al (2013) describe a Concrete
Category Overlap algorithm, where co-occurrence
statistics and Turney?s abstractness scores are used
to determine WordNet supersenses that corre-
spond to literal usage of a given adjective or verb.
For example, given an adjective, we can learn that
it modifies concrete nouns that usually have the
255
supersense noun.body. If this adjective modifies
a noun with the supersense noun.feeling, we con-
clude that a metaphor is found.
Broadwell et al (2013) argue that metaphors
are highly imageable words that do not belong
to a discussion topic. To implement this idea,
they extend MRC imageability scores to all dic-
tionary words using links among WordNet super-
senses (mostly hypernym and hyponym relations).
Strzalkowski et al (2013) carry out experiments
in a specific (government-related) domain for four
languages: English, Spanish, Farsi, and Russian.
Strzalkowski et al (2013) explain the algorithm
only for English and say that is the same for Span-
ish, Farsi, and Russian. Because they heavily
rely on WordNet and availability of imageability
scores, their approach may not be applicable to
low-resource languages.
Hovy et al (2013) applied tree kernels to
metaphor detection. Their method also employs
WordNet supersenses, but it is not clear from the
description whether WordNet is essential or can
be replaced with some other lexical resource. We
cannot compare directly our model with this work
because our classifier is restricted to detection of
only SVO and AN metaphors.
Tsvetkov et al (2013) propose a cross-lingual
detection method that uses only English lexical re-
sources and a dependency parser. Their study fo-
cuses only on the verb-based metaphors. Tsvetkov
et al (2013) employ only English and Russian
data. Current work builds on this study, and incor-
porates new syntactic relations as metaphor candi-
dates, adds several new feature sets and different,
more reliable datasets for evaluating results. We
demonstrate results on two new languages, Span-
ish and Farsi, to emphasize the generality of the
method.
A words sense disambiguation (WSD) is a re-
lated problem, where one identifies meanings of
polysemous words. The difference is that in the
WSD task, we need to select an already existing
sense, while for the metaphor detection, the goal
is to identify cases of sense borrowing. Studies
showed that cross-lingual evidence allows one to
achieve a state-of-the-art performance in the WSD
task, yet, most cross-lingual WSD methods em-
ploy parallel corpora (Navigli, 2009).
7 Conclusion
The key contribution of our work is that we show
how to identify metaphors across languages by
building a model in English and applying it?
without adaptation?to other languages: Spanish,
Farsi, and Russian. This model uses language-
independent (rather than lexical or language spe-
cific) conceptual features. Not only do we estab-
lish benchmarks for Spanish, Farsi, and Russian,
but we also achieve state-of-the-art performance
in English. In addition, we present a comparison
of relative contributions of several types of fea-
tures. We concentrate on metaphors in the con-
text of two kinds of syntactic relations: subject-
verb-object (SVO) relations and adjective-noun
(AN) relations, which account for a majority of all
metaphorical phrases.
Future work will expand the scope of metaphor
identification by including nominal metaphoric re-
lations as well as explore techniques for incor-
porating contextual features, which can play a
key role in identifying certain kinds of metaphors.
Second, cross-lingual model transfer can be im-
proved with more careful cross-lingual feature
projection.
Acknowledgments
We are extremely grateful to Shuly Wintner for a
thorough review that helped us improve this draft;
we also thank people who helped in creating the
datasets and/or provided valuable feedback on this
work: Ed Hovy, Vlad Niculae, Davida Fromm,
Brian MacWhinney, Carlos Ram??rez, and other
members of the CMU METAL team. This work
was supported by the U.S. Army Research Labo-
ratory and the U.S. Army Research Office under
contract/grant number W911NF-10-1-0533.
References
Marco Baroni, Raffaella Bernardi, Ngoc-Quynh Do,
and Chung-chieh Shan. 2012. Entailment above the
word level in distributional semantics. In Proc. of
EACL, pages 23?32.
Julia Birke and Anoop Sarkar. 2007. Active learning
for the identification of nonliteral language. In Proc.
of the Workshop on Computational Approaches to
Figurative Language, FigLanguages ?07, pages 21?
28.
Leo Breiman. 2001. Random forests. Machine Learn-
ing, 45(1):5?32.
256
George Aaron Broadwell, Umit Boz, Ignacio Cases,
Tomek Strzalkowski, Laurie Feldman, Sarah Taylor,
Samira Shaikh, Ting Liu, Kit Cho, and Nick Webb.
2013. Using imageability and topic chaining to lo-
cate metaphors in linguistic corpora. In Social Com-
puting, Behavioral-Cultural Modeling and Predic-
tion, pages 102?110. Springer.
Massimiliano Ciaramita and Yasemin Altun. 2006.
Broad-coverage sense disambiguation and informa-
tion extraction with a supersense sequence tagger. In
Proc. of EMNLP, pages 594?602.
Scott C. Deerwester, Susan T Dumais, Thomas K. Lan-
dauer, George W. Furnas, and Richard A. Harshman.
1990. Indexing by latent semantic analysis. JASIS,
41(6):391?407.
Manaal Faruqui and Chris Dyer. 2014. Improving
vector space word representations using multilingual
correlation. In Proc. of EACL. Association for Com-
putational Linguistics.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. Language, Speech and
Communication. MIT Press.
Lisa Gandy, Nadji Allan, Mark Atallah, Ophir Frieder,
Newton Howard, Sergey Kanareykin, Moshe Kop-
pel, Mark Last, Yair Neuman, and Shlomo Arga-
mon. 2013. Automatic identification of conceptual
metaphors with limited knowledge. In Proc. of the
Twenty-Seventh AAAI Conference on Artificial Intel-
ligence, pages 328?334.
Matt Gedigian, John Bryant, Srini Narayanan, and Bra-
nimir Ciric. 2006. Catching metaphors. In Pro-
ceedings of the 3rd Workshop on Scalable Natural
Language Understanding, pages 41?48.
Birgit Hamp and Helmut Feldweg. 1997. Germanet-
a lexical-semantic net for German. In Proc. of
ACL workshop Automatic Information Extraction
and Building of Lexical Semantic Resources for NLP
Applications, pages 9?15.
Dirk Hovy, Shashank Srivastava, Sujay Kumar Jauhar,
Mrinmaya Sachan, Kartik Goyal, Huiying Li, Whit-
ney Sanders, and Eduard Hovy. 2013. Identifying
metaphorical word use with tree kernels. In Proc. of
the First Workshop on Metaphor in NLP, page 52.
Eric H. Huang, Richard Socher, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Improving word
representations via global context and multiple word
prototypes. In Proc. of ACL, pages 873?882.
Franz Hundsnurscher and Jochen Splett. 1982. Se-
mantik der Adjektive des Deutschen. Number 3137.
Westdeutscher Verlag.
Mikhail Kozhenikov and Ivan Titov. 2013. Cross-
lingual transfer of semantic role labeling models. In
Proc. of ACL, pages 1190?1200.
Saisuresh Krishnakumaran and Xiaojin Zhu. 2007.
Hunting elusive metaphors using lexical resources.
In Proc. of the Workshop on Computational ap-
proaches to Figurative Language, pages 13?20.
George Lakoff and Mark Johnson. 1980. Conceptual
metaphor in everyday language. The Journal of Phi-
losophy, pages 453?486.
Lori Levin, Teruko Mitamura, Davida Fromm, Brian
MacWhinney, Jaime Carbonell, Weston Feely,
Robert Frederking, Anatole Gershman, and Carlos
Ramirez. 2014. Resources for the detection of con-
ventionalized metaphors in four languages. In Proc.
of LREC.
Andr?e F. T. Martins, Noah A. Smith, Eric P. Xing, Pe-
dro M. Q. Aguiar, and M?ario A. T. Figueiredo. 2010.
Turbo parsers: dependency parsing by approximate
variational inference. In Proc. of ENMLP, pages 34?
44.
Zachary J Mason. 2004. CorMet: a computational,
corpus-based conventional metaphor extraction sys-
tem. Computational Linguistics, 30(1):23?44.
Ryan McDonald, Slav Petrov, and Keith Hall. 2011.
Multi-source transfer of delexicalized dependency
parsers. In Proc. of EMNLP.
Tomas Mikolov, Quoc V. Le, and Ilya Sutskever. 2013.
Exploiting similarities among languages for Ma-
chine Translation. CoRR, abs/1309.4168.
Roberto Navigli. 2009. Word sense disambiguation:
A survey. ACM Comput. Surv., 41(2):10:1?10:69,
February.
Yair Neuman, Dan Assaf, Yohai Cohen, Mark Last,
Shlomo Argamon, Newton Howard, and Ophir
Frieder. 2013. Metaphor identification in large texts
corpora. PloS one, 8(4):e62343.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine learn-
ing in Python. Journal of Machine Learning Re-
search, 12:2825?2830.
Claudia Perlich, Foster Provost, and Jeffrey S. Si-
monoff. 2003. Tree induction vs. logistic regres-
sion: a learning-curve analysis. Journal of Machine
Learning Research, 4:211?255.
Nathan Schneider, Behrang Mohit, Chris Dyer, Kemal
Oflazer, and Noah A Smith. 2013. Supersense tag-
ging for Arabic: the MT-in-the-middle attack. In
Proc. of NAACL-HLT, pages 661?667.
Ekaterina Shutova and Lin Sun. 2013. Unsupervised
metaphor identification using hierarchical graph fac-
torization clustering. In Proc. of NAACL-HLT,
pages 978?988.
257
Ekaterina Shutova and Simone Teufel. 2010.
Metaphor corpus annotated for source-target domain
mappings. In Proc. of LREC, pages 3255?3261.
Ekaterina Shutova, Lin Sun, and Anna Korhonen.
2010. Metaphor identification using verb and noun
clustering. In Proc. of COLING, pages 1002?1010.
Ekaterina Shutova, Simone Teufel, and Anna Korho-
nen. 2013. Statistical metaphor processing. Com-
putational Linguistics, 39(2):301?353.
Ekaterina Shutova. 2010. Models of metaphor in NLP.
In Proc. of ACL, pages 688?697.
Gerard J Steen, Aletta G Dorst, J Berenike Her-
rmann, Anna A Kaal, and Tina Krennmayr.
2010. Metaphor in usage. Cognitive Linguistics,
21(4):765?796.
Tomek Strzalkowski, George Aaron Broadwell, Sarah
Taylor, Laurie Feldman, Boris Yamrom, Samira
Shaikh, Ting Liu, Kit Cho, Umit Boz, Ignacio Cases,
et al 2013. Robust extraction of metaphors from
novel data. In Proc. of the First Workshop on
Metaphor in NLP, page 67.
Oscar T?ackstr?om, Dipanjan Das, Slav Petrov, Ryan
McDonald, and Joakim Nivre. 2013. Token and
type constraints for cross-lingual part-of-speech tag-
ging. TACL, 1:1?12.
Paul H Thibodeau and Lera Boroditsky. 2011.
Metaphors we think with: The role of metaphor in
reasoning. PLoS One, 6(2):e16782.
Yulia Tsvetkov, Elena Mukomel, and Anatole Gersh-
man. 2013. Cross-lingual metaphor detection using
common semantic features. In The 1st Workshop on
Metaphor in NLP 2013, page 45.
Yulia Tsvetkov, Nathan Schneider, Dirk Hovy, Archna
Bhatia, Manaal Faruqui, and Chris Dyer. 2014.
Augmenting English adjective senses with super-
senses. In Proc. of LREC.
Joseph Turian, Lev Ratinov, Yoshua Bengio, and Dan
Roth. 2009. A preliminary evaluation of word rep-
resentations for named-entity recognition. In NIPS
Workshop on Grammar Induction, Representation of
Language and Language Learning, pages 1?8.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proc. of ACL, pages
384?394.
Peter D. Turney, Yair Neuman, Dan Assaf, and Yohai
Cohen. 2011. Literal and metaphorical sense iden-
tification through concrete and abstract context. In
Proc. of EMNL, pages 680?690.
Yorick Wilks. 1978. Making preferences more active.
Artificial Intelligence, 11(3):197?223.
Michael Wilson. 1988. MRC Psycholinguistic
Database: Machine-usable dictionary, version 2.00.
Behavior Research Methods, Instruments, & Com-
puters, 20(1):6?10.
258
Coling 2008: Proceedings of the 2nd workshop on Information Retrieval for Question Answering (IR4QA), pages 1?8
Manchester, UK. August 2008
Improving Text Retrieval Precision and
Answer Accuracy in Question Answering Systems
Matthew W. Bilotti and Eric Nyberg
Language Technologies Institute
Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh, PA 15213 USA
{ mbilotti, ehn }@cs.cmu.edu
Abstract
Question Answering (QA) systems are of-
ten built modularly, with a text retrieval
component feeding forward into an answer
extraction component. Conventional wis-
dom suggests that, the higher the quality of
the retrieval results used as input to the an-
swer extraction module, the better the ex-
tracted answers, and hence system accu-
racy, will be. This turns out to be a poor
assumption, because text retrieval and an-
swer extraction are tightly coupled. Im-
provements in retrieval quality can be lost
at the answer extraction module, which can
not necessarily recognize the additional
answer candidates provided by improved
retrieval. Going forward, to improve ac-
curacy on the QA task, systems will need
greater coordination between text retrieval
and answer extraction modules.
1 Introduction
The task of Question Answering (QA) involves
taking a question phrased in natural human lan-
guage and locating specific answers to that ques-
tion expressed within a text collection. Regard-
less of system architecture, or whether the sys-
tem is operating over a closed text collection or
the web, most QA systems use text retrieval as a
first step to narrow the search space for the an-
swer to the question to a subset of the text col-
lection (Hirschman and Gaizauskas, 2001). The
remainder of the QA process amounts to a gradual
narrowing of the search space, using successively
more finely-grained filters to extract, validate and
present one or more answers to the question.
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
Perhaps the most popular system architecture in
the QA research community is the modular archi-
tecture, in most variations of which, text retrieval
is represented as a separate component, isolated
by a software abstraction from question analysis
and answer extraction mechanisms. The widely-
accepted pipelined modular architecture imposes a
strict linear ordering on the system?s control flow,
with the analysis of the input question used as in-
put to the text retrieval module, and the retrieved
results feeding into the downstream answer extrac-
tion components.
Proponents of the modular architecture naturally
view the QA task as decomposable, and to a cer-
tain extent, it is. The modules, however, can never
be fully decoupled, because question analysis and
answer extraction components, at least, depend on
a common representation for answers and perhaps
also a common set of text processing tools. This
dependency is necessary to enable the answer ex-
traction mechanism to determine whether answers
exist in retrieved text, by analyzing it and compar-
ing it against the question analysis module?s an-
swer specification. In practice, the text retrieval
component does not use the common representa-
tion for scoring text; either the question analysis
module or an explicit query formulation compo-
nent maps it into a representation queryable by the
text retrieval component.
The pipelined modular QA system architecture
also carries with it an assumption about the com-
positionality of the components. It is easy to ob-
serve that errors cascade as the QA process moves
through downstream modules, and this leads to the
intuition that maximizing performance of individ-
ual modules minimizes the error at each stage of
the pipeline, which, in turn, should maximize over-
all end-to-end system accuracy.
It is a good idea to pause to question what this
intuition is telling us. Is end-to-end QA system
performance really a linear function of individual
1
[ARG0 [PERSON John]] [TARGET loves] [ARG1 [PERSON Mary]]
Figure 1: Example OpenEphyra semantic representation for the sentence, John loves Mary. Note that
John is identified as the ARG0, the agent, or doer, of the love action. Mary is identified as the ARG1, the
patient, or to whom the love action is being done. Both John and Mary are also identified as PERSON
named entity types.
components? Is component performance really ad-
ditive? This paper argues that the answer is no,
not in general, and offers the counterexample of a
high-precision text retrieval system that can check
constraints against the common representation at
retrieval time, which is integrated into a publicly-
available pipelined modular QA system that is oth-
erwise unchanged.
Ignoring the dependency between the answer
extraction mechanism and the text retrieval com-
ponent creates a problem. The answer extraction
module is not able to handle the more sophisti-
cated types of matches provided by the improved
text retrieval module, and so it ignores them, leav-
ing end-to-end system performance largely un-
changed. The lesson learned is that a module im-
proved in isolation does not necessarily provide an
improvement in end-to-end system accuracy, and
the paper concludes with recommendations for fur-
ther research in bringing text retrieval and answer
extraction closer together.
2 Improving Text Retrieval in Isolation
This section documents an attempt to improve the
performance of a QA system by substituting its
existing text retrieval component with for high-
precision retrieval system capable of checking lin-
guistic and semantic constraints at retrieval time.
2.1 The OpenEphyra QA System
OpenEphyra is the freely-available, open-source
version of the Ephyra1 QA system (Schlaefer et
al., 2006; Schlaefer et al, 2007). OpenEphyra is a
pipelined modular QA system having four stages:
question analysis, query generation, search and an-
swer extraction and selection. OpenEphyra also
includes support for answer projection, or the use
of the web to find answers to the question, which
are then used to find supporting text in the cor-
pus. Answer projection support was disabled for
the purposes of this paper.
1See: http://www.ephyra.info
The common representation in OpenEphyra is
a verb predicate-argument structure, augmented
with named entity types, in which verb arguments
are labeled with semantic roles in the style of Prop-
Bank (Kingsbury et al, 2002). This feature re-
quires the separate download2 of a semantic parser
called ASSERT (Pradhan et al, 2004), which was
trained on PropBank. See Figure 1 for an example
representation for the sentence, John loves Mary.
OpenEphyra comes packaged with standard
baseline methods for answer extraction and se-
lection. For example, it extracts answers from
retrieved text based on named entity instances
matching the expected answer type as determined
by the question analysis module. It can also look
for predicate-argument structures that match the
question structure, and can extract the argument
corresponding to the argument in the question rep-
resenting the interrogative phrase. OpenEphyra?s
default answer selection algorithm filters out an-
swers containing question keyterms, merges sub-
sets, and combines scores of duplicate answers.
2.2 Test Collection
The corpus used in this experiment is the
AQUAINT corpus (Graff, 2002), the standard
corpus for the TREC3 QA evaluations held in
2002 through 2005. The corpus was prepared
using MXTerminator (Reynar and Ratnaparkhi,
1997) for sentence segmentation, BBN Identi-
finder (Bikel et al, 1999) for named entity recog-
nition, as well as the aforementioned ASSERT
for identification of verb predicate-argument struc-
tures and PropBank-style semantic role labeling of
the arguments.
The test collection consists of 109 questions
from the QA track at TREC 2002 with extensive
document-level relevance judgments (Bilotti et al,
2004; Lin and Katz, 2006) over the AQUAINT
corpus. A set of sentence-level judgments was pre-
2See: http://www.cemantix.org
3Text REtrieval Conferences organized by the U.S. Na-
tional Institute of Standards and Technology
2
Existing query #combine[sentence]( #any:person first person reach
south pole )
Top-ranked result Dufek became the first person to land an airplane at the South Pole.
Second-ranked result He reached the North Pole in 1991.
High-precision query #combine[sentence]( #max( #combine[target]( scored
#max( #combine[./arg1]( #any:person ))
#max( #combine[./arg2](
#max( #combine[target]( reach
#max( #combine[./arg1]( south pole )))))))))
Top-ranked result [ARG1 Norwegian explorer [PERSON Roald Admundsen]] [TARGET becomes]
(relevant) [ARG2 [ARG0 first man] to [TARGET reach] [ARG1 [LOCATION South Pole]]]
Figure 2: Retrieval comparison between OpenEphrya?s existing text retrieval component, and the high-
precision version it was a replaced with, for question 1475, Who was the first person to reach the South
Pole? Note that the top two results retrieved by the existing text retrieval component are not relevant,
and the top result from the high-precision component is relevant. The existing component does retrieve
this answer-bearing sentence, but ranks it third.
pared by manually determining whether each sen-
tence matching the TREC-provided answer pattern
for a given question was answer-bearing according
to the definition that an answer-bearing sentence
completely contains and supports the answer to the
question, without requiring inference or aggrega-
tion outside of that sentence. Questions without
any answer-bearing sentences were removed from
the test collection, leaving 91 questions.
Questions were manually reformulated so that
they contain predicates. For example, question
1432, Where is Devil?s Tower? was changed to
Where is Devil?s Tower located?, because AS-
SERT does not cover verbs, including be and have,
that do not occur in its training data. Hand-
corrected ASSERT parses for each question were
were cached in the question analysis module. Re-
formulated questions are used as input to both the
existing and high-precision text retrieval modules,
to avoid advantaging one system over the other.
2.3 High-Precision Text Retrieval
OpenEphyra?s existing text retrieval module was
replaced with a high-precision text retrieval sys-
tem based on a locally-modified version of the In-
dri (Strohman et al, 2005) search engine, a part of
the open-source Lemur toolkit4. While the existing
version of the text retrieval component supports
querying on keyterms, phrases and placeholders
4See: http://www.lemurproject.org
for named entity types, the high-precision version
also supports retrieval-time constraint-checking
against the semantic representation based on verb
predicate-argument structures, PropBank-style se-
mantic role labels, and named entity recognition.
To make use of this expanded text retrieval ca-
pability, OpenEphyra?s query formulation module
was changed to source pre-prepared Indri queries
that encode using structured query operators the
predicate-argument and named entity constraints
that match the answer-bearing sentences for each
question. If questions have multiple queries asso-
ciated with them, each query is evaluated individu-
ally, with the resulting ranked lists fused by Round
Robin (Voorhees et al, 1994). Round Robin,
which merges ranked lists by taking the top-ranked
element from each list in order followed by lower-
ranking elements, was chosen because Indri, the
underlying retrieval engine, gives different queries
scores that are not comparable in general, making
it difficult to choose a fusion method that uses re-
trieval engine score as a feature.
Figure 2 shows a comparison of querying and
retrieval behavior between OpenEphyra?s existing
text retrieval module and the high-precision ver-
sion with which it is being replaced for question
1475, Who was the first person to reach the South
Pole? The bottom of the figure shows an answer-
bearing sentence with the correct answer, Roald
Admundsen. The predicate-argument structure, se-
3
mantic role labels and named entities are shown.
The high-precision text retrieval module sup-
ports storing of extents representing sentences, tar-
get verbs and arguments and named entity types
as fields in the index. At query time, con-
straints on these fields can be checked using struc-
tured query operators. The queries in Figure 2
are shown in Indri syntax. Both queries begin
with #combine[sentence], which instructs
Indri to score and rank sentence extents, rather
than entire documents. The query for the ex-
isting text retrieval component contains keyterms
as well an #any:type operator that matches in-
stances of the expected answer type, which in this
case is person. The high-precision query encodes
a verb predicate-argument structure. The nested
#combine[target] operator scores a sentence
by the predicate-argument structures it contains.
The #combine[./role] operators are used to in-
dicate constraints on specific argument roles. The
dot-slash syntax tells Indri that the argument ex-
tents are related to but not enclosed by the target
extent. Throughout, the #max operator is used to
select the best matching extent in the event that
more than one satisfy the constraints.
Figure 3 compares average precision at the top
twenty ranks over the entire question set between
OpenEphyra?s existing text retrieval module and
the high-precision text retrieval module, showing
that the latter performs better.
2.4 Results
To determine what effect improving text retrieval
quality has on the end-to-end QA system, it suf-
fices to run the system on the entire test collection,
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
1 6 11 16
High-precision
Existing
Rank
Av
er
ag
e 
Pr
ec
isi
on
 a
t R
an
k
Figure 3: Comparison of average precision at top
twenty ranks between OpenEphyra?s existing text
retrieval module, and the high-precision version
that took its place.
replace the text retrieval component with the high-
precision version while holding the other modules
constant, and repeat the test run. Table 1 summa-
rizes the MAP, average end-to-end system accu-
racy (whether the top-ranked returned answer is
correct), and the mean reciprocal rank (MRR) of
the correct answer (one over the rank at which the
correct answer is returned). If the correct answer
to a question is returned beyond rank twenty, the
reciprocal rank for that question is considered to
be zero.
Table 1: Summary of end-to-end QA system ac-
curacy and MRR when the existing text retrieval
module is replaced with a high-precision version
Retrieval MAP Accuracy MRR
Existing 0.3234 0.1099 0.2080
High-precision 0.5487 0.1319 0.2020
Table 1 shows that, despite the improvement in
average precision, the end-to-end system did not
realize a significant improvement in accuracy or
MRR. Viewed in the aggregate, the results are dis-
couraging, because it seems that the performance
gains realized after the text retrieval stage of the
pipeline are lost in downstream answer extraction
components.
Figure 4 compares OpenEphyra both before and
after the integration of the high-precision text re-
trieval component on the basis of average precision
and answer MRR. The horizontal axis plots the dif-
ference in average precision; a value of positive
one indicates that the high-precision version of the
module was perfect, ranking all answer-bearing
sentences at the top of the ranked list, and that the
existing version retrieved no relevant text at all.
Negative one indicates the reverse. The vertical
axis plots the difference in answer MRR. As be-
fore, positive one indicates that the high-precision
component led the system to rank the correct an-
swer first, and the existing component did not, and
negative one indicates the reverse. The zero point
on each axis is where the high-precision and ex-
isting text retrieval components performed equally
well.
The expectation is that there will be a posi-
tive correlation between average precision and an-
swer MRR; when the retrieval component provides
higher quality results, the job of the answer extrac-
tion module should be easier. This is illustrated
in the bottom portion of Figure 4, which was cre-
4
-1
-0.5
0
0.5
1
-1 -0.5 0 0.5 1
Difference in Average Precision
Ideal Answer Extraction
OpenEphyra
Di
ffe
re
nc
e 
in
 A
ns
we
r M
RR
-1
-0.5
0
0.5
1
-1 -0.5 0 0.5 1
Figure 4: Scatter plot comparing the difference in
average precision between the high-precision re-
trieval component and the existing retrieval com-
ponent on the horizontal axis, to the difference in
answer MRR on the vertical axis. Ideally, there
would be a high correlation between the two; as av-
erage precision improves, so should answer MRR.
ated by assuming that the answer extraction mod-
ule could successfully extract answers without er-
ror from all answer-bearing sentences returned by
the text retrieval component.
Interestingly, actual extraction performance,
shown in the top portion of Figure 4, bears lit-
tle resemblance to the ideal. Note the large con-
centration of data points along the line represent-
ing zero difference in answer MRR. This indicates
that, regardless of improvement in average pre-
cision of the results coming out of the retrieval
module, the downstream answer extraction perfor-
mance remains the same as it was when the ex-
isting text retrieval component was in use. This
occurs because the answer extraction module does
not know how to extract answers from some of the
types of answer-bearing sentences retrieved by the
high-precision version of the retrieval module and
not by the existing version.
There are several data points in the top right-
hand quadrant of the top half of Figure 4, indicat-
ing that for some questions, answer extraction was
able to improve as average precision improved.
This is likely due to better rankings for types of
answer-bearing sentences that answer extraction
already knows how to handle. Data points occur-
ring in the lower right-hand portion of the graph in-
dicate depressed answer extraction performance as
average precision is increasing. This phenomenon
can be explained by the higher-precision text re-
trieval module ranking answer-bearing sentences
that answer extraction can not handle ahead of
those that it can handle.
3 Failure Analysis
The results presented in the previous section con-
firm that an improvement made to the text retrieval
component, in isolation, without a corresponding
improvement to the downstream answer extraction
modules, can fail to translate into a corresponding
improvement in end-to-end QA system accuracy.
The increased average precision in the retrieved re-
sults is coming in the form of answer-bearing sen-
tences of types that the answer extraction machin-
ery does not know how to handle. To address this
gap in answer extraction coverage, it is first nec-
essary to examine examples of the types of errors
made by the OpenEphyra answer extraction mod-
ule, summarized in Table 2.
Question 1497, What was the original name be-
fore ?The Star Spangled Banner?? is an exam-
ple of a question for which OpenEphyra?s answer
extraction machinery failed outright. An answer-
bearing sentence was retrieved, however, contain-
ing the answer inside a quoted phrase: His poem
was titled ?Defense of Fort M?Henry? and by
November 1814 had been published as ?The Star-
Spangled Banner?. The expected answer type of
this question does not match a commonly-used
named entity type, so OpenEphrya?s named entity-
based answer extractor found no candidates in this
sentence. Predicate-argument structure-based an-
swer extraction fails as well because the old and
new names do not appear within the same struc-
ture. Because OpenEphyra does not include sup-
port for positing quoted phrases as answer candi-
dates, no answer to this question can be found de-
spite the fact that an answer-bearing sentence was
retrieved.
Question 1417, Who was the first person to run
the mile in less than four minutes? is an exam-
ple of a question for which average precision im-
proved greatly, by 0.7208, but for which extraction
quality remained the same. The existing text re-
trieval module ranks 14 sentences ahead of the first
answer-bearing sentence, but only one contains a
named entity of type person, so despite the im-
provement in retrieval quality, the correct answer
5
Table 2: Summary of end-to-end QA system results on the question set
Result Type Count
Extraction failure 42
Retrieval better, extraction same 20
Retrieval better, extraction worse 13
Retrieval better, extraction better 10
Retrieval worse, extraction better 3
Retrieval worse, extraction worse 3
Total 91
moves up only one rank in the system output.
For ten questions, extraction performance does
improve as average precision improves. Ques-
tion 1409, Which vintage rock and roll singer was
known as ?The Killer?? For each of these ques-
tions, OpenEphyra?s existing text retrieval module
could not rank an answer-bearing sentence highly
or retrieve one at all. Adding the high-precision
version of the text retrieval component solved this
problem. In each case, named entity-based an-
swer extraction was able extract the correct an-
swer. These eleven questions range over a variety
of answer types, and have little in common except
for the fact that there are relatively few answer-
bearing sentences in the corpus, and large numbers
of documents matched by a bag-of-words query
formulated using the keyterms from the question.
There are three questions for which extraction
performance degrades as retrieval performance de-
grades. Question 1463, What is the North Korean
national anthem? is an example. In this case,
there is only one relevant sentence, and, owing
to an annotation error, it has a predicate-argument
structure that is very generic, having North Korea
as the only argument: Some of the North Korean
coaches broke into tears as the North?s anthem,
the Patriotic Song, played. The high-precision re-
trieval component retrieved a large number of sen-
tences matching the that predicate-argument struc-
ture, but ranked the one answer-bearing sentence
very low.
Some questions actually worsened in terms of
the reciprocal rank of the correct answer when av-
erage precision improved. An example is question
1504, Where is the Salton Sea? The high-precision
text retrieval module ranked answer-bearing sen-
tences such as The combination could go a long
way to removing much of the pesticides, fertilizers,
raw sewage carried by the river into the Salton
Sea, the largest lake in California, but a failure
of the named entity recognition tool did not iden-
tify California as an instance of the expected an-
swer type, and therefore it was ignored. Sen-
tences describing other seas near other locations
provided answers such as Central Asia, Russia,
Turkey and Ukraine that were ranked ahead of Cal-
ifornia, which was eventually extracted from an-
other answer-bearing sentence.
And finally, for some questions, high-precision
retrieval was more of a hindrance than a help,
retrieving more noise than answer-bearing sen-
tences. A question for which this is true is ques-
tion 1470, When did president Herbert Hoover
die? The high-precision text retrieval module uses
a predicate-argument structure to match the target
verb die, theme Hoover and a date instance oc-
curring in a temporal adjunct. Interestingly, the
text collection contains a great deal of die struc-
tures that match partially, including those referring
to deaths of presidents of other nations, and those
referring to the death of J. Edgar Hoover, who was
not a U.S. president but the first director of the U.S.
Federal Bureau of Investigation (FBI). False posi-
tives such as these serve to push the true answer
down on the ranked list of answers coming out of
the QA system.
4 Improving Answer Extraction
The answer extraction and selection algorithms
packaged with OpenEphyra are widely-accepted
baselines, but are not sophisticated enough to
extract answer candidates from the additional
answer-bearing text retrieved by the high-precision
text retrieval module, which can check linguistic
and semantic constraints at query time.
The named-entity answer extraction method se-
lects any candidate answer that is an instance of
the expected answer type, so long as it co-occur
with query terms. Consider question 1467, What
6
year did South Dakota become a state? Given
that the corpus consists of newswire text report-
ing on current events, years that are contempo-
rary to the corpus often co-occur with the ques-
tion focus, as in the following sentence, Monaghan
also seized about $87,000 from a Santee account
in South Dakota in 1997. Of the top twenty an-
swers returned for this question, all but four are
contemporary to the corpus or in the future. Min-
imal sanity-checking on candidate answers could
save the system the embarrassment of returning a
date in the future as the answer. Going one step
further would involve using external sources to de-
termine that 1997 is too recent to be the year a state
was admitted to the union.
OpenEphyra?s predicate-argument structure-
based answer extraction algorithm can avoid
some of these noisy answers by comparing some
constraints from the question against the retrieved
text and only extracting answers if the constraints
are satisfied. Consider question 1493, When was
Davy Crockett born? One relevant sentence says
Crockett was born Aug. 17, 1786, in what is now
eastern Tennessee, and moved to Lawrenceburg
in 1817. The SRL answer extraction algorithm
extracts Aug. 17, 1786 because it is located in an
argument labeled argm-tmp with respect to the
verb, and ignores the other date in the sentence,
1817. The named entity-based answer extraction
approach proposes both dates as answer candi-
dates, but the redundancy-based answer selection
prefers 1786.
The predicate-argument structure-based answer
extraction algorithm is limited because it only ex-
tracts arguments from text that shares the structure
as the question. The high-precision text retrieval
approach is actually able to retrieve additional
answer-bearing sentences with different predicate-
argument structures from the question, but answer
extraction is not able to make use of it. Consider
the sentence, At the time of his 100 point game with
the Philadelphia Warriors in 1962, Chamberlain
was renting an apartment in New York. Though
this sentence answers the question What year did
Wilt Chamberlain score 100 points?, its predicate-
argument structure is different from that of the
question, and predicate-argument structure-based
answer extraction will ignore this result because it
does not contain a score verb.
In addition to answer extraction, end-to-end per-
formance could be improved by focusing on an-
swer selection. OpenEphyra does not include sup-
port for sanity-checking the answers it returns,
and its default answer selection mechanism is
redundancy-based. As a result, nonsensical an-
swers are occasionally retrieved, such as moon
for question 1474, What is the lowest point on
Earth? Sophisticated approaches, however, do ex-
ist for answer validation and justification, includ-
ing use of resources such as gazetteers and ontolo-
gies (Buscaldi and Rosso, 2006), Wikipedia (Xu
et al, 2002), the Web (Magnini et al, 2002), and
combinations of the above (Ko et al, 2007).
5 Conclusions
This paper set out to challenge the assumption of
compositionality in pipelined modular QA systems
that suggests that an improvement in an individual
module should lead to an improvement in the over-
all end-to-end system performance. An attempt
was made to validate the assumption by showing
an improvement in the end-to-end system accu-
racy of an off-the-shelf QA system by substitut-
ing its existing text retrieval component for a high-
precision retrieval component capable of checking
linguistic and semantic constraints at query time.
End-to-end system accuracy remained roughly un-
changed because the downstream answer extrac-
tion components were not able to extract answers
from the types of the answer-bearing sentences re-
turned by the improved retrieval module.
The reality of QA systems is that there is a
high level of coupling between the different system
components. Ideally, text retrieval should have an
understanding of the kinds of results that answer
extraction is able to utilize to extract answers, and
should not offer text beyond the capabilities of the
downstream modules. Similarly, question analy-
sis and answer extraction should be agreeing on
a common representation for what constitutes an
answer to the question so that answer extraction
can use that information to locate answers in re-
trieved text. When a retrieval module is available
that is capable of making use of the semantic rep-
resentation of the answer, it should do so, but an-
swer extraction needs to know what it can assume
about incoming results so that it does not have to
re-check constraints already guaranteed to hold.
The coupling between text retrieval and answer
extraction is important for a QA system to per-
form well. Improving the quality of text retrieval
is essential because once the likely location of
7
the answer is narrowed down to a subset of the
text collection, anything not retrieved text can not
be searched for answers in downstream modules.
Equally important is the role of answer extraction.
Even the most relevant retrieved text is useless to
a QA system unless answers can be extracted from
it. End-to-end QA system performance can not
be improved by improving text retrieval quality
in isolation. Improvements in answer extraction
must keep pace with progress on text retrieval tech-
niques to reduce errors resulting from a mismatch
in capabilities. Going forward, research on the lin-
guistic and semantic constraint-checking capabili-
ties of text retrieval systems to support the QA task
can drive research in answer extraction techniques,
and in QA systems in general.
References
Bikel, D., R. Schwartz, and R. Weischedel. 1999. An
algorithm that learns what?s in a name. Machine
Learning, 34(1?3):211?231.
Bilotti, M., B. Katz, and J. Lin. 2004. What works bet-
ter for question answering: Stemming or morpholog-
ical query expansion? In Proceedings of the Infor-
mation Retrieval for Question Answering (IR4QA)
Workshop at SIGIR 2004.
Bilotti, M., P. Ogilvie, J. Callan, and E. Nyberg. 2007.
Structured retrieval for question answering. In Pro-
ceedings of the 30th Annual International ACM SI-
GIR Conference on Research and Development in In-
formation Retrieval.
Buscaldi, D. and P. Rosso. 2006. Mining knowledge
from wikipedia for the question answering task. In
Proceedings of the International Conference on Lan-
guage Resources and Evaluation.
Cui, H., R. Sun, K. Li, M. Kan, and T. Chua. 2005.
Question answering passage retrieval using depen-
dency relations. In Proceedings of the 28th Annual
International ACM SIGIR Conference on Research
and Development in Information Retrieval.
Graff, D. 2002. The AQUAINT Corpus of English
News Text. Linguistic Data Consortium (LDC). Cat.
No. LDC2002T31.
Hirschman, L. and R. Gaizauskas. 2001. Natural
language question answering: The view from here.
Journal of Natural Language Engineering, Special
Issue on Question Answering, Fall?Winter.
Kingsbury, P., M. Palmer, and M. Marcus. 2002.
Adding semantic annotation to the penn treebank. In
Proceedings of the 2nd International Conference on
Human Language Technology Research (HLT 2002).
Ko, J., L. Si, and E. Nyberg. 2007. A probabilistic
graphical model for joint answer ranking in question
answering. In Proceedings of the 30th Annual Inter-
national ACM SIGIR Conference on Research and
Development in Information Retrieval.
Lin, J. and B. Katz. 2006. Building a reusable test col-
lection for question answering. Journal of the Amer-
ican Society for Information Science and Technol-
ogy, 57(7):851?861.
Magnini, B., M. Negri, R. Pervete, and H. Tanev. 2002.
Comparing statistical and content-based techniques
for answer validation on the web. In Proceedings of
the VIIIo Convegno AI*IA.
Narayanan, S. and S. Harabagiu. 2004. Question an-
swering based on semantic structures. In Proceed-
ings of the 20th international conference on Compu-
tational Linguistics.
Pradhan, S., W. Ward, K. Hacioglu, J. Martin, and
D. Jurafsky. 2004. Shallow semantic parsing using
support vector machines. In Proceedings of the Hu-
man Language Technology Conference of the North
American Chapter of the Association for Computa-
tional Linguistics (HLT-NAACL 2004).
Reynar, J. and A. Ratnaparkhi. 1997. A maximum en-
tropy approach to identifying sentence boundaries.
In Proceedings of the Fifth Conference on Applied
Natural Language Processing.
Schlaefer, N., P. Gieselmann, and G. Sautter. 2006.
The ephyra qa system at trec 2006. In Proceedings
of the Fifteenth Text REtrieval Conference (TREC).
Schlaefer, N., J. Ko, J. Betteridge, G. Sautter,
M. Pathak, and E. Nyberg. 2007. Semantic exten-
sions of the ephyra qa system for trec 2007. In Pro-
ceedings of the Sixteenth Text REtrieval Conference
(TREC).
Strohman, T., D. Metzler, H. Turtle, and W. B. Croft.
2005. Indri: A language model-based search engine
for complex queries. In Proceedings of the Interna-
tional Conference on Intelligence Analysis.
Sun, R., J. Jiang, Y. Tan, H. Cui, T. Chua, and M. Kan.
2005. Using syntactic and semantic relation analysis
in question answering. In Proceedings of the Four-
teenth Text REtrieval Conference (TREC-14).
Voorhees, E., N. Gupta, and B. Johnson-Laird. 1994.
The collection fusion problem. In Proc. of TREC-3.
Xu, J., A. Licuanan, J. May, S. Miller, and
R. Weischedel. 2002. Trec 2002 qa at bbn: Answer
selection and confidence estimation. In Proceedings
of the Text REtrieval Conference.
8
Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, pages 131?139,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Sentiment Classification using Automatically Extracted Subgraph Features
Shilpa Arora, Elijah Mayfield, Carolyn Penstein-Rose? and Eric Nyberg
Language Technologies Institute
Carnegie Mellon University
5000 Forbes Avenue, Pittsburgh, PA 15213
{shilpaa, emayfiel, cprose, ehn}@cs.cmu.edu
Abstract
In this work, we propose a novel representa-
tion of text based on patterns derived from lin-
guistic annotation graphs. We use a subgraph
mining algorithm to automatically derive fea-
tures as frequent subgraphs from the annota-
tion graph. This process generates a very large
number of features, many of which are highly
correlated. We propose a genetic program-
ming based approach to feature construction
which creates a fixed number of strong classi-
fication predictors from these subgraphs. We
evaluate the benefit gained from evolved struc-
tured features, when used in addition to the
bag-of-words features, for a sentiment classi-
fication task.
1 Introduction
In recent years, the topic of sentiment analysis has
been one of the more popular directions in the field
of language technologies. Recent work in super-
vised sentiment analysis has focused on innovative
approaches to feature creation, with the greatest im-
provements in performance with features that in-
sightfully capture the essence of the linguistic con-
structions used to express sentiment, e.g. (Wilson et
al., 2004), (Joshi and Rose?, 2009)
In this spirit, we present a novel approach that
leverages subgraphs automatically extracted from
linguistic annotation graphs using efficient subgraph
mining algorithms (Yan and Han, 2002). The diffi-
culty with automatically deriving complex features
comes with the increased feature space size. Many
of these features are highly correlated and do not
provide any new information to the model. For ex-
ample, a feature of type unigram POS (e.g. ?cam-
era NN?) doesn?t provide any additional informa-
tion beyond the unigram feature (e.g. ?camera?),
for words that are often used with the same part of
speech. However, alongside several redundant fea-
tures, there are also features that provide new infor-
mation. It is these features that we aim to capture.
In this work, we propose an evolutionary ap-
proach that constructs complex features from sub-
graphs extracted from an annotation graph. A con-
stant number of these features are added to the un-
igram feature space, adding much of the represen-
tational benefits without the computational cost of a
drastic increase in feature space size.
In the remainder of the paper, we review prior
work on features commonly used for sentiment anal-
ysis. We then describe the annotation graph rep-
resentation proposed by Arora and Nyberg (2009).
Following this, we describe the frequent subgraph
mining algorithm proposed in Yan and Han (2002),
and used in this work to extract frequent subgraphs
from the annotation graphs. We then introduce our
novel feature evolution approach, and discuss our
experimental setup and results. Subgraph features
combined with the feature evolution approach gives
promising results, with an improvement in perfor-
mance over the baseline.
2 Related Work
Some of the recent work in sentiment analysis has
shown that structured features (features that capture
syntactic patterns in text), such as n-grams, depen-
dency relations, etc., improve performance beyond
131
the bag of words approach. Arora et al (2009) show
that deep syntactic scope features constructed from
transitive closure of dependency relations give sig-
nificant improvement for identifying types of claims
in product reviews. Gamon (2004) found that using
deep linguistic features derived from phrase struc-
ture trees and part of speech annotations yields sig-
nificant improvements on the task of predicting sat-
isfaction ratings in customer feedback data. Wilson
et al (2004) use syntactic clues derived from depen-
dency parse tree as features for predicting the inten-
sity of opinion phrases1.
Structured features that capture linguistic patterns
are often hand crafted by domain experts (Wilson
et al, 2005) after careful examination of the data.
Thus, they do not always generalize well across
datasets and domains. This also requires a signif-
icant amount of time and resources. By automati-
cally deriving structured features, we might be able
to learn new annotations faster.
Matsumoto et al (2005) propose an approach that
uses frequent sub-sequence and sub-tree mining ap-
proaches (Asai et al, 2002; Pei et al, 2004) to derive
structured features such as word sub-sequences and
dependency sub-trees. They show that these features
outperform bag-of-words features for a sentiment
classification task and achieve the best performance
to date on a commonly-used movie review dataset.
Their approach presents an automatic procedure for
deriving features that capture long distance depen-
dencies without much expert intervention.
However, their approach is limited to sequences
or tree annotations. Often, features that combine
several annotations capture interesting characteris-
tics of text. For example, Wilson et al (2004), Ga-
mon (2004) and Joshi and Rose? (2009) show that
a combination of dependency relations and part of
speech annotations boosts performance. The anno-
tation graph representation proposed by Arora and
Nyberg (2009) is a formalism for representing sev-
eral linguistic annotations together on text. With an
annotation graph representation, instances are rep-
resented as graphs from which frequent subgraph
patterns may be extracted and used as features for
learning new annotations.
1Although, in this work we are classifying sentences and not
phrases, similar clues may be used for sentiment classification
in sentences as well
In this work, we use an efficient frequent sub-
graph mining algorithm (gSpan) (Yan and Han,
2002) to extract frequent subgraphs from a linguis-
tic annotation graph (Arora and Nyberg, 2009). An
annotation graph is a general representation for ar-
bitrary linguistic annotations. The annotation graph
and subgraph mining algorithm provide us a quick
way to test several alternative linguistic representa-
tions of text. In the next section, we present a formal
definition of the annotation graph and a motivating
example for subgraph features.
3 Annotation Graph Representation and
Feature Subgraphs
Arora and Nyberg (2009) define the annotation
graph as a quadruple: G = (N,E,?, ?), where
N is the set of nodes, E is the set of edges, s.t.
E ? N ? N , and ? = ?N ? ?E is the set of la-
bels for nodes and edges. ? : N ? E ? ? is the
labeling function for nodes and edges. Examples of
node labels (?N ) are tokens (unigrams) and annota-
tions such as part of speech, polarity etc. Examples
of edge labels (?E) are leftOf, dependency type etc.
The leftOf relation is defined between two adjacent
nodes. The dependency type relation is defined be-
tween a head word and its modifier.
Annotations may be represented in an annotation
graph in several ways. For example, a dependency
triple annotation ?good amod movie?, may be repre-
sented as a d amod relation between the head word
?movie? and its modifier ?good?, or as a node d amod
with edges ParentOfGov and ParentOfDep to the
head and the modifier words. An example of an an-
notation graph is shown in Figure 1.
The instance in Figure 1 describes a movie review
comment, ?interesting, but not compelling.?. The
words ?interesting? and ?compelling? both have pos-
itive prior polarity, however, the phrase expresses
negative sentiment towards the movie. Heuristics for
special handling of negation have been proposed in
the literature. For example, Pang et al (2002) ap-
pend every word following a negation, until a punc-
tuation, with a ?NOT? . Applying a similar technique
to our example gives us two sentiment bearing fea-
tures, one positive (?interesting?) and one negative
(?NOT-compelling?), and the model may not be as
sure about the predicted label, since there is both
132
positive and negative sentiment present.
In Figure 2, we show three discriminating sub-
graph features derived from the annotation graph in
Figure 1. These subgraph features capture the nega-
tive sentiment in our example phrase. The first fea-
ture in 2(a) captures the pattern using dependency
relations between words. A different review com-
ment may use the same linguistic construction but
with a different pair of words, for example ?a pretty
good, but not excellent story.? This is the same lin-
guistic pattern but with different words the model
may not have seen before, and hence may not clas-
sify this instance correctly. This suggests that the
feature in 2(a) may be too specific.
In order to mine general features that capture the
rhetorical structure of language, we may add prior
polarity annotations to the annotation graph, us-
ing a lexicon such as Wilson et al (2005). Fig-
ure 2(b) shows the subgraph in 2(a) with polar-
ity annotations. If we want to generalize the pat-
tern in 2(a) to any positive words, we may use the
feature subgraph in Figure 2(c) with X wild cards
on words that are polar or negating. This feature
subgraph captures the negative sentiment in both
phrases ?interesting, but not compelling.? and ?a
pretty good, but not excellent story.?. Similar gener-
alization using wild cards on words may be applied
with other annotations such as part of speech anno-
tations as well. By choosing where to put the wild
card, we can get features similar to, but more pow-
erful than, the dependency back-off features in Joshi
and Rose? (2009).
 
U_interesting U_, U_but U_not U_compelling U_. 
D_conj-but 
D_neg 
L_POSITIVE L_POSITIVE 
polQ polQ 
posQ 
P_VBN 
posQ 
P_, 
posQ 
P_CC 
posQ 
P_RB 
posQ 
P_JJ 
posQ 
P_. 
Figure 1: Annotation graph for sentence ?interesting, but not
compelling.? . Prefixes: ?U? for unigrams (tokens), ?L? for po-
larity, ?D? for dependency relation and ?P? for part of speech.
Edges with no label encode the ?leftOf? relation between words.
4 Subgraph Mining Algorithms
In the previous section, we demonstrated that sub-
graphs from an annotation graph can be used to iden-
 
U_interesting U_not U_compelling 
D_conj-but 
D_neg 
(a)
 
U_interesting U_not U_compelling 
D_conj-but 
D_neg 
L_POSITIVE L_POSITIVE 
polQ polQ 
(b)
 
X X X 
D_conj-but 
D_neg 
L_POSITIVE L_POSITIVE 
polQ polQ 
(c)
Figure 2: Subgraph features from the annotation graph in
Figure 1
tify the rhetorical structure used to express senti-
ment. The subgraph patterns that represent general
linguistic structure will be more frequent than sur-
face level patterns. Hence, we use a frequent sub-
graph mining algorithm to find frequent subgraph
patterns, from which we construct features to use in
the supervised learning algorithm.
The goal in frequent subgraph mining is to find
frequent subgraphs in a collection of graphs. A
graph G? is a subgraph of another graph G if there
exists a subgraph isomorphism2 from G? to G, de-
noted by G? ? G.
Earlier approaches in frequent subgraph mining
(Inokuchi et al, 2000; Kuramochi and Karypis,
2001) used a two-step approach of first generating
the candidate subgraphs and then testing their fre-
quency in the graph database. The second step in-
volves a subgraph isomorphism test, which is NP-
complete. Although efficient isomorphism testing
algorithms have been developed making it practical
to use, with lots of candidate subgraphs to test, it can
2http://en.wikipedia.org/wiki/Subgraph_
isomorphism_problem
133
still be very expensive for real applications.
gSpan (Yan and Han, 2002) uses an alternative
pattern growth based approach to frequent subgraph
mining, which extends graphs from a single sub-
graph directly, without candidate generation. For
each discovered subgraph G, new edges are added
recursively until all frequent supergraphs of G have
been discovered. gSpan uses a depth first search tree
(DFS) and restricts edge extension to only vertices
on the rightmost path. However, there can be multi-
ple DFS trees for a graph. gSpan introduces a set of
rules to select one of them as representative. Each
graph is represented by its unique canonical DFS
code, and the codes for two graphs are equivalent if
the graphs are isomorphic. This reduces the compu-
tational cost of the subgraph mining algorithm sub-
stantially, making gSpan orders of magnitude faster
than other subgraph mining algorithms. With sev-
eral implementations available 3, gSpan has been
commonly used for mining frequent subgraph pat-
terns (Kudo et al, 2004; Deshpande et al, 2005). In
this work, we use gSpan to mine frequent subgraphs
from the annotation graph.
5 Feature Construction using Genetic
Programming
A challenge to overcome when adding expressive-
ness to the feature space for any text classification
problem is the rapid increase in the feature space
size. Among this large set of new features, most
are not predictive or are very weak predictors, and
only a few carry novel information that improves
classification performance. Because of this, adding
more complex features often gives no improvement
or even worsens performance as the feature space?s
signal is drowned out by noise.
Riloff et al (2006) propose a feature subsump-
tion approach to address this issue. They define a
hierarchy for features based on the information they
represent. A complex feature is only added if its
discriminative power is a delta above the discrimi-
native power of all its simpler forms. In this work,
we use a Genetic Programming (Koza, 1992) based
approach which evaluates interactions between fea-
3http://www.cs.ucsb.edu/?xyan/software/
gSpan.htm, http://www.kyb.mpg.de/bs/people/
nowozin/gboost/
tures and evolves complex features from them. The
advantage of the genetic programing based approach
over feature subsumption is that it allows us to eval-
uate a feature using multiple criteria. We show that
this approach performs better than feature subsump-
tion.
A lot of work has considered this genetic pro-
gramming problem (Smith and Bull, 2005). The
most similar approaches to ours are taken by Kraw-
iec (2002) and Otero et al (2002), both of which use
genetic programming to build tree feature represen-
tations. None of this work was applied to a language
processing task, though there has been some sim-
ilar work to ours in that community, most notably
(Hirsch et al, 2007), which built search queries for
topic classification of documents. Our prior work
(Mayfield and Rose?, 2010) introduced a new feature
construction method and was effective when using
unigram features; here we extend our approach to
feature spaces which are even larger and thus more
problematic.
The Genetic Programming (GP) paradigm is most
advantageous when applied to problems where there
is not a correct answer to a problem, but instead
there is a gradient of partial solutions which incre-
mentally improve in quality. Potential solutions are
represented as trees consisting of functions (non-leaf
nodes in the tree, which perform an action given
their child nodes as input) and terminals (leaf nodes
in the tree, often variables or constants in an equa-
tion). The tree (an individual) can then be inter-
preted as a program to be executed, and the output
of that program can be measured for fitness (a mea-
surement of the program?s quality). High-fitness in-
dividuals are selected for reproduction into a new
generation of candidate individuals through a breed-
ing process, where parts of each parent are combined
to form a new individual.
We apply this design to a language processing
task at the stage of feature construction - given many
weakly predictive features, we would like to com-
bine them in a way which produces a better feature.
For our functions we use boolean statements AND
and XOR, while our terminals are selected randomly
from the set of all unigrams and our new, extracted
subgraph features. Each leaf?s value, when applied
to a single sentence, is equal to 1 if that subgraph is
present in the sentence, and 0 if the subgraph is not
134
present.
The tree in Figure 3 is a simplified example of our
evolved features. It combines three features, a uni-
gram feature ?too? (centre node) and two subgraph
features: 1) the subgraph in the leftmost node oc-
curs in collocations containing ?more than? (e.g.,
?nothing more than? or ?little more than?), 2) the
subgraph in the rightmost node occurs in negative
phrases such as ?opportunism at its most glaring?
(JJS is a superlative adjective and PRP$ is a pos-
sessive pronoun). A single feature combining these
weak indicators can be more predictive than any part
alone.
!"#$
!"#$
%&'(($
%&)(*+$
,&-*+-&'./0$
%&1'2$
3"4&3#35$
3"4&664$
,&-(22$
Figure 3: A tree constructed using subgraph features and GP
(Simplified for illustrative purposes)
In the rest of this section, we first describe the
feature construction process using genetic program-
ming. We then discuss how fitness of an individual
is measured for our classification task.
5.1 Feature Construction Process
We divide our data into two sets, training and test.
We again divide our training data in half, and train
our GP features on only one half of this data4 This is
to avoid overfitting the final SVM model to the GP
features. In a single GP run, we produce one feature
to match each class value. For a sentiment classifica-
tion task, a feature is evolved to be predictive of the
positive instances, and another feature is evolved to
be predictive of the negative documents. We repeat
this procedure a total of 15 times (using different
seeds for random selection of features), producing
a total of 30 new features to be added to the feature
space.
4For genetic programming we used the ECJ toolkit
(http://cs.gmu.edu/?eclab/projects/ecj/).
5.2 Defining Fitness
Our definition of fitness is based on the concepts
of precision and recall, borrowed from informa-
tion retrieval. We define our set of documents
as being comprised of a set of positive documents
P0, P1, P2, ...Pu and a set of negative documents
N0, N1, N2, ...Nv. For a given individual I and doc-
ument D, we define hit(I,D) to equal 1 if the state-
ment I is true of that document and 0 otherwise. Pre-
cision and recall of an individual feature for predict-
ing positive documents5 is then defined as follows:
Prec(I) =
u
?
i=0
hit(I, Pi)
u
?
i=0
hit(I, Pi) +
v
?
i=0
hit(I,Ni)
(1)
Rec(I) =
u
?
i=0
hit(I, Pi)
u
(2)
We then weight these values to give significantly
more importance to precision, using the F? measure,
which gives the harmonic mean between precision
and recall:
F?(I) =
(1 + ?2)? (Prec(I)?Rec(I))
(?2 ? Prec(I)) +Rec(I) (3)
In addition to this fitness function, we add two
penalties to the equation. The first penalty applies to
prevent trees from becoming overly complex. One
option to ensure that features remain moderately
simple is to simply have a maximum depth beyond
which trees cannot grow. Following the work of
Otero et al (2002), we penalize trees based on the
number of nodes they contain. This discourages
bloat, i.e. sections of trees which do not contribute to
overall accuracy. This penalty, known as parsimony
pressure, is labeled PP in our fitness function.
The second penalty is based on the correlation be-
tween the feature being constructed, and the sub-
graphs and unigrams which appear as nodes within
that individual. Without this penalty, a feature may
5Negative precision and recall are defined identically, with
obvious adjustments to test for negative documents instead of
positive.
135
often be redundant, taking much more complexity
to represent the same information that is captured
with a simple unigram. We measure correlation us-
ing Pearson?s product moment, defined for two vec-
tors X , Y as:
?x,y =
E[(X ? ?X)(Y ? ?Y )]
?X?Y
(4)
This results in a value from 1 (for perfect align-
ment) to -1 (for inverse alignment). We assign a
penalty for any correlation past a cutoff. This func-
tion is labeled CC (correlation constraint) in our fit-
ness function.
Our fitness function therefore is:
Fitness = F 1
8
+ PP + CC (5)
6 Experiments and Results
We evaluate our approach on a sentiment classifi-
cation task, where the goal is to classify a movie
review sentence as expressing positive or negative
sentiment towards the movie.
6.1 Data and Experimental Setup
Data: The dataset consists of snippets from Rot-
ten Tomatoes (Pang and Lee, 2005) 6. It consists
of 10662 snippets/sentences total with equal num-
ber positive and negative sentences (5331 each).
This dataset was created and used by Pang and Lee
(2005) to train a classifier for identifying positive
sentences in a full length review. We use the first
8000 (4000 positive, 4000 negative) sentences as
training data and evaluate on remaining 2662 (1331
positive, 1331 negative) sentences. We added part
of speech and dependency triple annotations to this
data using the Stanford parser (Klein and Manning,
2003).
Annotation Graph: For the annotation graph rep-
resentation, we used Unigrams (U), Part of Speech
(P) and Dependency Relation Type (D) as labels for
the nodes, and ParentOfGov and ParentOfDep as la-
bels for the edges. For a dependency triple such as
?amod good movie?, five nodes are added to the an-
notation graph as shown in Figure 4(a). ParentOf-
Gov and ParentOfDep edges are added from the
6http://www.cs.cornell.edu/people/pabo/
movie-review-data/rt-polaritydata.tar.gz
D_amod
U_good
P_JJ
P_NN
U_movie
ParentofGov
ParentofGovParentofDep
ParentofDep
(a)
D_amod
U_good
P_NN
ParentofGov
ParentofDep
(b)
D_amod
X
P_JJ
P_NN
X
posQ
ParentofGov
ParentofDep
posQ
(c)
Figure 4: Annotation graph and a feature subgraph for
dependency triple annotation ?amod good camera?. (c)
shows an alternative representation with wild cards
dependency relation node D amod to the unigram
nodes U good and U movie. These edges are also
added for the part of speech nodes that correspond
to the two unigrams in the dependency relation, as
shown in Figure 4(a). This allows the algorithm to
find general patterns, based on a dependency rela-
tion between two part of speech nodes, two unigram
nodes or a combination of the two. For example,
a subgraph in Figure 4(b) captures a general pat-
tern where good modifies a noun. This feature ex-
ists in ?amod good movie?, ?amod good camera?
and other similar dependency triples. This feature is
similar to the the dependency back-off features pro-
posed in Joshi and Rose? (2009).
The extra edges are an alternative to putting wild
cards on words, as proposed in section 3. On the
other hand, putting a wild card on every word in
the annotation graph for our example (Figure 4(c)),
will only give features based on dependency rela-
tions between part of speech annotations. Thus, the
wild card based approach is more restrictive than
136
adding more edges. However, with lots of edges, the
complexity of the subgraph mining algorithm and
the number of subgraph features increases tremen-
dously.
Classifier: For our experiments we use Support
Vector Machines (SVM) with a linear kernel. We
use the SVM-light7 implementation of SVM with
default settings.
Parameters: The gSpan algorithm requires setting
the minimum support threshold (minsup) for the
subgraph patterns to extract. Support for a subgraph
is the number of graphs in the dataset that contain
the subgraph. We experimented with several values
for minimum support and minsup = 2 gave us the
best performance.
For Genetic Programming, we used the same pa-
rameter settings as described in Mayfield and Rose?
(2010), which were tuned on a different dataset8
than one used in this work, but it is from the same
movie review domain. We also consider one alter-
ation to these settings. As we are introducing many
new and highly correlated features to our feature
space through subgraphs, we believe that a stricter
constraint must be placed on correlation between
features. To accomplish this, we can set our correla-
tion penalty cutoff to 0.3, lower than the 0.5 cutoff
used in prior work. Results for both settings are re-
ported.
Baselines: To the best of our knowledge, there is
no supervised machine learning result published on
this dataset. We compare our results with the fol-
lowing baselines:
? Unigram-only Baseline: In sentiment analysis,
unigram-only features have been a strong base-
line (Pang et al, 2002; Pang and Lee, 2004).
We only use unigrams that occur in at least
two sentences of the training data same as Mat-
sumoto et al (2005). We also filter out stop
words using a small stop word list9.
? ?2 Baseline: For our training data, after filter-
ing infrequent unigrams and stop words, we get
7http://svmlight.joachims.org/
8Full movie review data by Pang et al (2002)
9http://nlp.stanford.edu/
IR-book/html/htmledition/
dropping-common-terms-stop-words-1.html
(with one modification: removed ?will?, added ?this?)
8424 features. Adding subgraph features in-
creases the total number of features to 44, 161,
a factor of 5 increase in size. Feature selec-
tion can be used to reduce this size by select-
ing the most discriminative features. ?2 feature
selection (Manning et al, 2008) is commonly
used in the literature. We compare two methods
of feature selection with ?2, one which rejects
features if their ?2 score is not significant at the
0.05 level, and one that reduces the number of
features to match the size of our feature space
with GP.
? Feature Subsumption (FS): Following the idea
in Riloff et al (2006), a complex feature
C is discarded if IG(S) ? IG(C) ? ?,
where IG is Information Gain and S is
a simple feature that representationally sub-
sumes C, i.e. the text spans that match S
are a superset of the text spans that match
C. In our work, complex features are sub-
graph features and simple features are uni-
gram features contained in them. For example,
(D amod) Edge ParentOfDep (U bad) is
a complex feature for which U bad is a sim-
ple feature. We tried same values for ? ?
{0.002, 0.001, 0.0005}, as suggested in Riloff
et al (2006). Since all values gave us same
number of features, we only report a single re-
sult for feature subsumption.
? Correlation (Corr): As mentioned earlier,
some of the subgraph features are highly corre-
lated with unigram features and do not provide
new knowledge. A correlation based filter for
subgraph features can be used to discard a com-
plex feature C if its absolute correlation with its
simpler feature (unigram feature) is more than
a certain threshold. We use the same threshold
as used in the GP criterion, but as a hard filter
instead of a penalty.
6.2 Results and Discussion
In Table 1, we present our results. As can be
seen, subgraph features when added to the unigrams,
without any feature selection, decrease the perfor-
mance. ?2 feature selection with fixed feature space
size provides a very small gain over unigrams. All
other feature selection approaches perform worse
137
Settings #Features Acc. ?
Uni 8424 75.66 -
Uni + Sub 44161 75.28 -0.38
Uni + Sub, ?2 sig. 3407 74.68 -0.98
Uni + Sub, ?2 size 8454 75.77 +0.11
Uni + Sub, (FS) 18234 75.47 -0.19
Uni + Sub, (Corr) 18980 75.24 -0.42
Uni + GP (U) ? 8454 76.18 +0.52
Uni + GP (U+S) ? 8454 76.48 +0.82
Uni + GP (U+S) ? 8454 76.93 +1.27
Table 1: Experimental results for feature spaces with un-
igrams, with and without subgraph features. Feature se-
lection with 1) fixed significance level (?2 sig.), 2) fixed
feature space size (?2 size), 3) Feature Subsumption (FS)
and 4) Correlation based feature filtering (Corr)). GP fea-
tures for unigrams only {GP(U)}, or both unigrams and
subgraph features {GP(U+S)}. Both the settings from
Mayfield and Rose? (2010) (?) and more stringent correla-
tion constraint (?) are reported. #Features is the num-
ber of features in the training data. Acc is the accuracy
and ? is the difference from unigram only baseline. Best
performing feature configuration is highlighted in bold.
than the unigram-only approach. With GP, we ob-
serve a marginally significant gain (p < 0.1) in per-
formance over unigrams, calculated using one-way
ANOVA. Benefit from GP is more when subgraph
features are used in addition to the unigram features,
for constructing more complex pattern features. Ad-
ditionally, our performance is improved when we
constrain the correlation more severely than in previ-
ously published research, supporting our hypothesis
that this is a helpful way to respond to the problem
of redundancy in subgraph features.
A problem that we see with ?2 feature selection is
that several top ranked features may be highly cor-
related. For example, the top 5 features based on ?2
score are shown in Table 2; it is immediately obvi-
ous that the features are highly redundant.
With GP based feature construction, we can con-
sider this relationship between features, and con-
struct new features as a combination of selected un-
igram and subgraph features. With the correlation
criterion in the evolution process, we are able to
build combined features that provide new informa-
tion compared to unigrams.
The results we present are for the best perform-
(D advmod) Edge ParentOfDep (U too)
U too
U bad
U movie
(D amod) Edge ParentOfDep (U bad)
Table 2: Top features based on ?2 score
ing parameter configuration that we tested, after a
series of experiments. We realize that this places us
in danger of overfitting to the particulars of this data
set, however, the data set is large enough to partially
mitigate this concern.
7 Conclusion and Future Work
We have shown that there is additional information
to be gained from text beyond words, and demon-
strated two methods for increasing this information -
a subgraph mining approach that finds common syn-
tactic patterns that capture sentiment-bearing rhetor-
ical structure in text, and a feature construction
technique that uses genetic programming to com-
bine these more complex features without the redun-
dancy, increasing the size of the feature space only
by a fixed amount. The increase in performance that
we see is small but consistent.
In the future, we would like to extend this work to
other datasets and other problems within the field of
sentiment analysis. With the availability of several
off-the-shelf linguistic annotators, we may add more
linguistic annotations to the annotation graph and
richer subgraph features may be discovered. There
is also additional refinement that can be performed
on our genetic programming fitness function, which
is expected to improve the quality of our features.
Acknowledgments
This work was funded in part by the DARPA Ma-
chine Reading program under contract FA8750-09-
C-0172, and in part by NSF grant DRL-0835426.
We would like to thank Dr. Xifeng Yan and Marisa
Thoma for the gSpan code.
References
Shilpa Arora, Mahesh Joshi and Carolyn P. Rose?. 2009.
Identifying Types of Claims in Online Customer Re-
138
views. Proceedings of the HLT/NAACL.
Shilpa Arora and Eric Nyberg. 2009. Interactive Anno-
tation Learning with Indirect Feature Voting. Proceed-
ings of the HLT/NAACL (Student Research Work-
shop).
Tatsuya Asai, Kenji Abe, Shinji Kawasoe, Hiroshi
Sakamoto and Setsuo Arikawa. 2002. Efficient sub-
structure discovery from large semi-structured data.
Proceedings of SIAM Int. Conf. on Data Mining
(SDM).
Mukund Deshpande , Michihiro Kuramochi , Nikil Wale
and George Karypis. 2005. Frequent Substructure-
Based Approaches for Classifying Chemical Com-
pounds. IEEE Transactions on Knowledge and Data
Engineering.
Michael Gamon. 2004. Sentiment classification on cus-
tomer feedback data: noisy data, large feature vec-
tors, and the role of linguistic analysis, Proceedings
of COLING.
Laurence Hirsch, Robin Hirsch and Masoud Saeedi.
2007. Evolving Lucene Search Queries for Text Clas-
sification. Proceedings of the Genetic and Evolution-
ary Computation Conference.
Mahesh Joshi and Carolyn P. Rose?. 2009. Generalizing
Dependency Features for Opinion Mining. Proceed-
ings of the ACL-IJCNLP Conference (Short Papers).
Akihiro Inokuchi, Takashi Washio and Hiroshi Motoda.
2000. An Apriori-based Algorithm for Mining Fre-
quent Substructures from Graph Data. Proceedings
of PKDD.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. Proceedings of the main con-
ference of the ACL.
John Koza. 1992. Genetic Programming: On the Pro-
gramming of Computers by Means of Natural Selec-
tion. MIT Press.
Krzysztof Krawiec. 2002. Genetic programming-based
construction of features for machine learning and
knowledge discovery tasks. Genetic Programming and
Evolvable Machines.
Taku Kudo, Eisaku Maeda and Yuji Matsumoto. 2004.
An Application of Boosting to Graph Classification.
Proceedings of NIPS.
Michihiro Kuramochi and George Karypis. 2002. Fre-
quent Subgraph Discovery. Proceedings of ICDM.
Christopher D. Manning, Prabhakar Raghavan and Hin-
rich Schtze. 2008. Introduction to Information Re-
trieval. Proceedings of PAKDD.
Shotaro Matsumoto, Hiroya Takamura and Manabu Oku-
mura. 2005. Sentiment Classification Using Word
Sub-sequences and Dependency Sub-trees. Proceed-
ings of PAKDD.
Elijah Mayfield and Carolyn Penstein-Rose?. 2010.
Using Feature Construction to Avoid Large Feature
Spaces in Text Classification. Proceedings of the Ge-
netic and Evolutionary Computation Conference.
Fernando Otero, Monique Silva, Alex Freitas and Julio
Nievola. 2002. Genetic Programming for Attribute
Construction in Data Mining. Proceedings of the Ge-
netic and Evolutionary Computation Conference.
Bo Pang, Lillian Lee and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment Classication using Ma-
chine Learning Techniques. Proceedings of EMNLP.
Bo Pang and Lillian Lee. 2004. A Sentimental Educa-
tion: Sentiment Analysis Using Subjectivity Summa-
rization Based on Minimum Cuts. Proceedings of the
main conference of ACL.
Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting
class relationships for sentiment categorization with
respect to rating scales. Proceedings of the main con-
ference of ACL.
Jian Pei, Jiawei Han, Behzad Mortazavi-asl, Jianyong
Wang, Helen Pinto, Qiming Chen, Umeshwar Dayal
and Mei-chun Hsu. 2004. Mining Sequential Pat-
terns by Pattern-Growth: The PrefixSpan Approach.
Proceedings of IEEE Transactions on Knowledge and
Data Engineering.
Ellen Riloff, Siddharth Patwardhan and Janyce Wiebe.
2006. Feature Subsumption for Opinion Analysis.
Proceedings of the EMNLP.
Matthew Smith and Larry Bull. 2005. Genetic Program-
ming with a Genetic Algorithm for Feature Construc-
tion and Selection. Genetic Programming and Evolv-
able Machines.
Theresa Wilson, Janyce Wiebe and Rebecca Hwa. 2004.
Just How Mad Are You? Finding Strong and Weak
Opinion Clauses. Proceedings of AAAI.
Theresa Wilson, Janyce Wiebe and Paul Hoff-
mann. 2005. Recognizing Contextual Polarity
in Phrase-Level Sentiment Analysis. Proceedings of
HLT/EMNLP.
Xifeng Yan and Jiawei Han. 2002. gSpan: Graph-
Based Substructure Pattern Mining. UIUC Techni-
cal Report, UIUCDCS-R-2002-2296 (shorter version
in ICDM?02).
139
Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 106?114,
Portland, Oregon, USA, 23?24 June 2011. c?2011 Association for Computational Linguistics
Assessing Benefit from Feature Feedback in Active Learning
for Text Classification
Shilpa Arora
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
shilpaa@cs.cmu.edu
Eric Nyberg
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
ehn@cs.cmu.edu
Abstract
Feature feedback is an alternative to instance
labeling when seeking supervision from hu-
man experts. Combination of instance and
feature feedback has been shown to reduce the
total annotation cost for supervised learning.
However, learning problems may not benefit
equally from feature feedback. It is well un-
derstood that the benefit from feature feed-
back reduces as the amount of training data
increases. We show that other characteristics
such as domain, instance granularity, feature
space, instance selection strategy and propor-
tion of relevant text, have a significant effect
on benefit from feature feedback. We estimate
the maximum benefit feature feedback may
provide; our estimate does not depend on how
the feedback is solicited and incorporated into
the model. We extend the complexity mea-
sures proposed in the literature and propose
some new ones to categorize learning prob-
lems, and find that they are strong indicators
of the benefit from feature feedback.
1 Introduction
Linear classifiers model the response as a weighted
linear combination of the features in input instances.
A supervised approach to learning a linear classifier
involves learning the weights for the features from
labeled data. A large number of labeled instances
may be needed to determine the class association of
the features and learn accurate weights for them. Al-
ternatively, the user may directly label the features.
For example, for a sentiment classification task, the
user may label features, such as words or phrases,
as expressing positive or negative sentiment. Prior
work (Raghavan et al, 2006; Zaidan et al, 2007)
has demonstrated that users are able to reliably pro-
vide useful feedback on features.
Direct feedback on a list of features (Raghavan et
al., 2006; Druck et al, 2008) is limited to simple fea-
tures like unigrams. However, unigrams are limited
in the linguistic phenomena they can capture. Struc-
tured features such as dependency relations, paths in
syntactic parse trees, etc., are often needed for learn-
ing the target concept (Pradhan et al, 2004; Joshi
and Rose?, 2009). It is not clear how direct feature
feedback can be extended straightforwardly to struc-
tured features, as they are difficult to present visu-
ally for feedback and may require special expertise
to comprehend. An alternative approach is to seek
indirect feedback on structured features (Arora and
Nyberg, 2009) by asking the user to highlight spans
of text, called rationales, that support the instance
label (Zaidan et al, 2007). For example, when clas-
sifying the sentiment of a movie review, rationales
are spans of text in the review that support the senti-
ment label for the review.
Assuming a fixed cost per unit of work, it might
be cheaper to ask the user to label a few features, i.e.
identify relevant features and their class association,
than to label several instances. Prior work (Ragha-
van et al, 2006; Druck et al, 2008; Druck et al,
2009; Zaidan et al, 2007) has shown that a combi-
nation of instance and feature labeling can be used
to reduce the total annotation cost required to learn
the target concept. However, the benefit from feature
feedback may vary across learning problems. If we
can estimate the benefit from feature feedback for a
106
given problem, we can minimize the total annotation
cost for achieving the desired performance by select-
ing the optimal annotation strategy (feature feedback
or not) at every stage in learning. In this paper, we
present the ground work for this research problem by
analyzing how benefit from feature feedback varies
across different learning problems and what charac-
teristics of a learning problem have a significant ef-
fect on benefit from feature feedback.
We define a learning problem (P = {D, G, F , L,
I , S}) as a tuple of the domain (D), instance gran-
ularity (G), feature representation (F ), labeled data
units (L), amount of irrelevant text (I) and instance
selection strategy (S).
With enough labeled data, we may not benefit
from feature feedback. Benefit from feature feed-
back also depends on the features used to represent
the instances. If the feature space is large, we may
need several labeled instances to identify the rele-
vant features, while relatively fewer labeled features
may help us quickly find these relevant features.
Apart from the feature space size, it also matters
what types of features are used. When hand crafted
features from a domain expert are used (Pradhan et
al., 2004) we expect to gain less from feature feed-
back as most of the features will be relevant. On
the other hand, when features are extracted automat-
ically as patterns in annotation graphs (Arora et al,
2010) feature feedback can help to identify relevant
features from the large feature space.
In active learning, instances to be labeled are se-
lectively sampled in each iteration. Benefit from fea-
ture feedback will depend on the instances that were
used to train the model in each iteration. In the case
of indirect feature feedback through rationales or di-
rect feature feedback in context, instances selected
will also determine what features receive feedback.
Hence, instance selection strategy should affect the
benefit from feature feedback.
In text classification, an instance may contain a
large amount of text, and even a simple unigram
representation will generate a lot of features. Often
only a part of the text is relevant for the classifica-
tion task. For example, in movie reviews, often the
reviewers talk about the plot and characters in addi-
tion to providing their opinion about the movie. Of-
ten this extra information is not relevant to the clas-
sification task and bloats the feature space without
adding many useful features. With feature feedback,
we hope to filter out some of this noise and improve
the model. Thus, the amount of irrelevant informa-
tion in the instance should play an important role in
determining the benefit from feature feedback. We
expect to see less of such noise when the text in-
stance is more concise. For example, a movie review
snippet (about a sentence length) tends to have less
irrelevant text than a full movie review (several sen-
tences). In addition to analyzing document instances
with varying amount of noise, we also compare the
benefit from feature feedback for problems with dif-
ferent granularity. Granularity for a learning prob-
lem is defined based on the average amount of text
in its instances.
Benefit from feature feedback will also depend on
how feedback is solicited from the user and how it
is incorporated back into the model. Independently
from these factors, we estimate the maximum pos-
sible benefit and analyze how it varies across prob-
lems. Next we describe measures proposed in the
literature and propose some new ones for categoriz-
ing learning problems. We then discuss our experi-
mental setup and analysis.
2 Related Work
There has been little work on categorizing learn-
ing problems and how benefit from feature feedback
varies with them. To the best of our knowledge
there is only one work in this area by Raghavan et
al. (2007). They categorize problems in terms of
their feature complexity. Feature complexity is de-
fined in terms of the minimum number of features
required to learn a good classifier (close to maxi-
mum performance). If the concept can be described
by a weighted combination of a few well-selected
features, it is considered to be of low complexity.
In this estimate of complexity, an assumption is
made that the best performance is achieved when
the learner has access to all available features and
not for any subset of the features. This is a reason-
able assumption for text classification problems with
robust learners like SVMs together with appropriate
regularization and sufficient training data.
Instead of evaluating all possible combinations of
features to determine the minimum number of fea-
tures required to achieve close to the best perfor-
107
mance, feature complexity is estimated using an in-
telligent ranking of the features. This ranking is
based on their discriminative ability determined us-
ing a large amount of labeled data (referred to as
oracle) and a feature selection criterion such as In-
formation Gain (Rijsbergen, 1979). It is intuitive
that the rate of learning, i.e., the rate at which per-
formance improves as we add more features to the
model, is also associated with problem complexity.
Raghavan et al (2007) define the feature learning
convergence profile (pfl) as the area under the fea-
ture learning curve (performance vs. number of fea-
tures used in training), given by:
pfl =
?log2N
t=1 F1(M, 2
t)
log2N ? F1(M,N)
(1)
where F1(M, 2t) is the F1 score on the test data
when using all M instances for training with top
ranked 2t features. The features are added at an ex-
ponentially increasing interval to emphasize the rel-
ative increase in feature space size. The three feature
complexity measures proposed by Raghavan et al
(2007) are the following: 1) Feature size complex-
ity (Nf ): Logarithm (base 2) of the number of fea-
tures needed to achieve 95% of the best performance
(when all instances are available), 2) Feature profile
complexity (Fpc), given by Fpc = 1 ? pfl, and 3)
Combined feature complexity (Cf ) , Cf = Fpc ? nf ,
incorporates both the learning profile and the num-
ber of features required.
In order to evaluate the benefit from feature feed-
back, Raghavan et al (2007) use their tandem learn-
ing approach of interleaving instance and feature
feedback (Raghavan et al, 2006), referred to as
interactive feature selection (ifs). The features
are labeled as ?relevant? (feature discriminates well
among the classes), or ?non-relevant/don?t know?.
The labeled features are incorporated into learning
by scaling the value of the relevant features by a con-
stant factor in all instances.
Raghavan et al (2007) measure the benefit from
feature feedback as the gain in the learning speed
with feature feedback. The learning speed measures
the rate of performance improvement with increas-
ing amount of supervision. It is defined in terms of
the convergence profile similar to feature learning
convergence profile in Equation 1, except in terms
of the number of labeled units instead of the num-
ber of features. A labeled unit is either a labeled
instance or an equivalent set of labeled features with
the same annotation time. The benefit from feature
feedback is then measured as the difference in the
convergence profile with interactive feature selec-
tion (pifs) and with labeled instances only (pal).
Raghavan et al (2007) analysed 9 corpora and
358 binary classification tasks. Most of these cor-
pora, such as Reuters (Lewis, 1995), 20-newsgroup
(Lang, 1995), etc., have topic-based category la-
bels. For all classification tasks, they used simple
and fixed feature space containing only unigram fea-
tures (n-gram features were added where it seemed
to improve performance). They observed a negative
correlation (r = ?0.65) between the benefit from
feature feedback and combined feature complexity
(Cf ), i.e., feature feedback accelerates active learn-
ing by an amount that is inversely proportional to
the feature complexity of the problem. If a concept
can be expressed using a few well-selected features
from a large feature space, we stand to benefit from
feature feedback as few labeled features can provide
this information. On the other hand, if learning a
concept requires all or most of the features in the
feature space, there is little knowledge that feature
feedback can provide.
3 Estimating Maximum Benefit &
Additional Measures
In this section, we highlight some limitations of the
prior work that we address in this work.
Raghavan et al (2007) only varied the domain
among different problems they analyzed, i.e, only
the variable D in our problem definition (P =
{D,G,F, L, I, S}). However, as motivated in the
introduction, other characteristics are also important
when categorizing learning problems and it is not
clear if we will observe similar results on problems
that differ in these additional characteristics. In this
work, we apply their measures to problems that dif-
fer in these characteristics in addition to the domain.
Analysis in Raghavan et al (2007) is specific to
their approach for incorporating feature feedback
into the model, which may not work well for all do-
mains and datasets as also mentioned in their work
(Section 6.1). It is not clear how their results can be
108
extended to alternate approaches for seeking and in-
corporating feature feedback. Thus, in this work we
analyze the maximum benefit a given problem can
get from feature feedback independent of the feed-
back solicitation and incorporation approach.
Raghavan et al (2007) analyze benefit from fea-
ture feedback at a fixed training data size of 42 la-
beled units. However, the difference between learn-
ing problems may vary with the amount of labeled
data. Some problems may benefit significantly from
feature feedback even at relatively larger amount of
labeled data. On the other hand, with very large
training set, the benefit from feature feedback can
be expected to be small and not significant for all
problems and all problems will look similar. Thus,
we evaluate the benefit from feature feedback at dif-
ferent amount of labeled data.
Raghavan et al (2007) evaluate benefit from fea-
ture feedback in terms of the gain in learning speed.
However, the learning rate does not tell us how much
improvement we get in performance at a given stage
in learning. In fact, even if at every point in the
learning curve performance with feature feedback
was lower than performance without feature feed-
back, the rate of convergence to the corresponding
maximum performance may still be higher when us-
ing feature feedback. Thus, in this work, in addi-
tion to evaluating the improvement in the learning
speed, we also evaluate the improvement in the ab-
solute performance at a given stage in learning.
3.1 Determining the Maximum Benefit
Annotating instances with or without feature feed-
back may require different annotation time. It is
only fair to compare different annotation strategies
at same annotation cost. Raghavan et al (2006)
found that on average labeling an instance takes the
same amount of time as direct feedback on 5 fea-
tures. Zaidan et al (2007) found that on average
it takes twice as much time to annotate an instance
with rationales than to annotate one without ratio-
nales. In our analysis, we focus on feedback on fea-
tures in context of the instance they occur in, i.e., in-
direct feature feedback through rationales or direct
feedback on features that occur in the instance be-
ing labeled. Thus, based on the findings in Zaidan et
al. (2007), we assume that on average annotating an
instance with feature feedback takes twice as much
time as annotating an instance without feature feed-
back. We define a currency for annotation cost as
Annotation cost Units (AUs). For an annotation bud-
get of a AUs, we compare two annotation strategies
of annotating a instances without feature feedback
or a2 instances with feature feedback.
In this work, we only focus on using feature feed-
back as an alternative to labeled data, i.e., to pro-
vide evidence about features in terms of their rele-
vance and class association. Thus, the best feature
feedback can do is provide as much evidence about
features as evidence from a large amount of labeled
data (oracle). Let F1(k,Nm) be the F1 score of a
model trained with features that occur in m train-
ing instances (Nm) and evidence for these features
from k instances (k ? m). For an annotation budget
of a AUs, we define the maximum improvement in
performance with feature feedback (IPa) as the dif-
ference in performance with feature feedback from
oracle on a2 training instances and performance with
a training instances without feature feedback.
IPa = F1(o,Na
2
)? F1(a,Na) (2)
where o is the number of instances in the oracle
dataset (o >> a). We also compare annotation
strategies in terms of the learning rate similar to
Raghavan et al (2007), except that we estimate and
compare the maximum improvement in the learning
rate. For an annotation budget of a AUs, we define
the maximum improvement in learning rate from 0
to a AUs (ILR0?a) as follows.
ILR0?a = pcp
wFF ? pcp
woFF (3)
where pcpwFF and pcpwoFF are the convergence
profiles with and without feature feedback at same
annotation cost, calculated as follows.
pcp
wFF =
?log2 a2
t=1 F1(o,N2t)
log2 a2 ? F1(o,Na2 )
(4)
pcp
woFF =
?log2a
t=2 F1(2
t, N2t)
(log2a? 1)? F1(a,Na)
(5)
where 2t denotes the training data size in iteration
t. Like Raghavan et al (2007), we use exponen-
tially increasing intervals to emphasize the relative
increase in the training data size, since adding a few
109
labeled instances earlier in learning will give us sig-
nificantly more improvement in performance than
adding the same number of instances later on.
3.2 Additional Metrics
The feature complexity measures require an ?ora-
cle?, simulated using a large amount of labeled data,
which is often not available. Thus, we need mea-
sures that do not require an oracle.
Benefit from feature feedback will depend on the
uncertainty of the model on its predictions, since it
suggests uncertainty on the features and hence scope
for benefit from feature feedback. We use the proba-
bility of the predicted label from the model as an es-
timate of the model?s uncertainty. We evaluate how
benefit from feature feedback varies with summary
statistics such as mean, median and maximum prob-
ability from the model on labels for instances in a
held out dataset.
4 Experiments, Results and Observations
In this section, we describe the details of our exper-
imental setup followed by the results.
4.1 Data
We analyzed three datasets: 1) Movie reviews
with rationale annotations by Zaidan et al (2007),
where the task is to classify the sentiment (posi-
tive/negative) of a review, 2) Movie review snippets
from Rotten Tomatoes (Pang and Lee., 2005), and 3)
WebKB dataset with the task of classifying whether
or not a webpage is a faculty member?s homepage.
Raghavan et al (2007) found that the webpage clas-
sification task has low feature complexity and ben-
efited the most from feature feedback. We compare
our results on this task and the sentiment classifica-
tion task on the movie review datasets.
4.2 Experimental Setup
Table 1 describes the different variables and their
possible values in our experiments. We make a log-
ical distinction for granularity based on whether an
instance in the problem is a document (several sen-
tences) or a sentence. Labeled data is composed of
instances and their class labels with or without fea-
ture feedback. As discussed in Section 3.1, instances
with feature feedback take on average twice as much
time to annotate as instances without feature feed-
back. Thus, we measure the labeled data in terms of
the number of annotation cost units which may mean
different number of labeled instances based on the
annotation strategy. We used two feature configura-
tions of ?unigram only? and ?unigram+dependency
triples?. The unigram and dependency annotations
are derived from the Stanford Dependency Parser
(Klein and Manning, 2003).
Rationales by definition are spans of text in a re-
view that convey the sentiment of the reviewer and
hence are the part of the document most relevant for
the classification task. In order to vary the amount
of irrelevant text, we vary the amount of text (mea-
sured in terms of the number of characters) around
the rationales that is included in the instance repre-
sentation. We call this the slack around rationales.
When using the rationales with or without the slack,
only features that overlap with the rationales (and
the slack, if used) are used to represent the instance.
Since we only have rationales for the movie review
documents, we only studied the effect of varying the
amount of irrelevant text on this dataset.
Variable Possible Values
Domain (D) {Movie Review classifica-
tion (MR), Webpage classi-
fication (WebKB)}
Instance Granu-
larity (G)
{document (doc), sentence
(sent)}
Feature Space (F ) {unigram only (u), uni-
gram+dependency (u+d)}
Labeled Data
(#AUs) (L)
{64, 128, 256, 512, 1024}
Irrelevant Text (I) {0, 200, 400, 600,? }
Instance Selection
Strategy (S))
{deterministic (deter), un-
certainty (uncert)}
Table 1: Experiment space for analysis of learning prob-
lems (P = {D,G,F, L, I, S})
For all our experiments, we used Support Vec-
tor Machines (SVMs) with linear kernel for learn-
ing (libSVM (Chang and Lin, 2001) in Minorthird
(Cohen, 2004)). For identifying the discrimina-
tive features we used the information gain score.
For all datasets we used 1800 total examples with
equal number of positive and negative examples. We
110
held out 10% of the data for estimating model?s un-
certainty as explained in Section 3.2. The results
we present are averaged over 10 cross validation
folds on the remaining 90% of the data (1620 in-
stances). In a cross validation fold, 10% data is used
for testing (162 instances) and all of the remaining
1458 instances are used as the ?oracle? for calculat-
ing the feature complexity measures and estimating
the maximum benefit from feature feedback as dis-
cussed in Sections 2 and 3.1 respectively. The train-
ing data size is varied from 64 to 1024 instances
(from the total of 1458 instances for training in a
fold), based on the annotation cost budget. Instances
with their label are added to the training set either in
the original order they existed in the dataset, i.e. no
selective sampling (deterministic), or in the decreas-
ing order of current model?s uncertainty on them.
Uncertainty sampling in SVMs (Tong and Koller,
2000) selects the instances closest to the decision
boundary since the model is expected to be most un-
certain about these instances. In each slice of the
data, we ensured that there is equal distribution of
the positive and negative class. SVMs do not yield
probabilistic output but a decision boundary, a com-
mon practice is to fit the decision values from SVMs
to a sigmoid curve to estimate the probability of the
predicted class (Platt, 1999).
4.3 Results and Analysis
To determine the effect of various factors on benefit
from feature feedback, we did an ANOVA analysis
with Generalized Linear Model using a 95% confi-
dence interval. The top part of Table 2 shows the
average F1 score for the two annotation strategies
at same annotation cost. As can be seen, with fea-
ture feedback, we get a significant improvement in
performance.
Next we analyze the significance of the effect of
various problem characteristics discussed above on
benefit from feature feedback in terms of improve-
ment in performance (IP ) at given annotation cost
and improvement in learning rate (ILR). Improve-
ment in learning rate is calculated by comparing
the learning profile for the two annotation strategies
with increasing amount of labeled data, up to the
maximum annotation cost of 1024 AUs.
As can be seen from the second part of Table 2,
most of the factors have a significant effect on bene-
fit from feature feedback. The benefit is significantly
higher for the webpage classification task than the
sentiment classification task in the movie review do-
main. We found that average feature complexity for
the webpage classification task (Nf = 3.07) to be
lower than average feature complexity for the senti-
ment classification task (Nf = 5.18) for 1024 train-
ing examples. Lower feature complexity suggests
that the webpage classification concept can be ex-
pressed with few keywords such as professor, fac-
ulty, etc., and with feature feedback we can quickly
identify these features. Sentiment on the other hand
can be expressed in a variety of ways which explains
the high feature complexity.
The benefit is more for document granularity than
sentence granularity, which is intuitive as feature
space is substantially larger for documents and we
expect to gain more from the user?s feedback on
which features are important. This difference is sig-
nificant for improvement in the learning rate and
marginally significant for improvement in perfor-
mance. Note that here we are comparing docu-
ments (with or without rationale slack) and sen-
tences. However, documents with low rationale
slack should have similar amount of noise as a sen-
tence. Also, a significant difference between do-
mains suggests that documents in WebKB domain
might be quite different from those in Movie Review
domain. This may explain the marginal significant
difference between benefit for documents and sen-
tences. To understand the effect of granularity alone,
we compared the benefit from feature feedback for
documents (without removing any noise) and sen-
tences in movie review domain only and we found
that this difference in also not significant. Thus, con-
trary to our intuition, sentences and documents seem
to benefit equally from feature feedback.
The benefit is more when the feature space is
larger and more diverse, i.e., when dependency fea-
tures are used in addition to unigram features. We
found that on average adding dependency features
to unigram features increases the feature space by
a factor of 10. With larger feature space, feature
feedback can help to identify a few relevant features.
As can also be seen, feature feedback is more help-
ful when there is more irrelevant text, i.e., there is
noise that feature feedback can help to filter out.
Unlike improvement in performance, the improve-
111
ment in learning rate does not decrease monoton-
ically as the amount of rationale slack decreases.
This supports our belief that improvement in perfor-
mance does not necessarily imply improvement in
the learning rate. We saw similar result when com-
paring benefit from feature feedback at different in-
stance granularity. Improvement in learning rate for
problems with different granularity was statistically
significant but improvement in performance was not
significant. Thus, both metrics should be used when
evaluating the benefit from feature feedback.
We also observe that when training examples are
selectively sampled as the most uncertain instances,
we gain more from feature feedback than without
selective sampling. This is intuitive as instances
the model is uncertain about are likely to contain
features it is uncertain about and hence the model
should benefit from feedback on features in these in-
stances. Next we evaluate how well the complexity
measures proposed in Raghavan et al (2007) corre-
late with improvement in performance and improve-
ment in learning rate.
V ar. V alues AvgF1 Group
Strat.
wFF 78.2 A
woFF 68.2 B
V ar. V alues AvgIP GrpIP AvgILR GrpILR
D
WebKB 11.9 A 0.32 A
MR 8.0 B 0.20 B
G
Doc 10.9 A 0.30 A
Sent 9.0 A 0.22 B
F
u+d 12.1 A 0.30 A
u 7.8 B 0.22 B
I
? 12.8 A 0.34 A
600 11.2 A B 0.23 B
400 11.1 A B 0.26 A B
200 9.8 B 0.26 A B
0 4.8 C 0.21 B
S
Uncer. 12.7 A 0.32 A
Deter. 7.1 B 0.20 B
Table 2: Effect of variables defined in Table 1 on benefit
from feature feedback. AvgIP is the average increase in
performance (F1) and AvgILR is the average increase in
the learning rate. Different letters in GrpIP and GrpILR
indicate significantly different results.
For a given problem with an annotation cost bud-
get of a AUs, we calculate the benefit from feature
feedback by comparing the performance with fea-
ture feedback on a2 instances and the performance
without feature feedback on a instances as described
in Section 3.1. The feature complexity measures are
calculated using a2 instances, since it should be the
characteristics of these a2 training instances that de-
termine whether we would benefit from feature feed-
back on these a2 instances or from labeling new
a
2
instances. As can be seen from Table 3, the correla-
tion of feature complexity measures with both mea-
sures of benefit from feature feedback is strong, neg-
ative and significant. This suggests that problems
with low feature complexity, i.e. concepts that can
be expressed with few well-selected features, benefit
more from feature feedback.
It is intuitive that the benefit from feature feed-
back decreases as amount of labeled data increases.
We found a significant negative correlation (?0.574)
between annotation budget (number of AUs) and
improvement in performance with feature feedback.
However, note that this correlation is not very
strong, which supports our belief that factors other
than the amount of labeled data affect benefit from
feature feedback.
Measure R(IP ) R(ILR)
Nf -0.625 -0.615
Fpc -0.575 -0.735
Cf -0.603 -0.629
Table 3: Correlation coefficient (R) for feature size com-
plexity (Nf ), feature profile complexity (Fpc) and com-
bined feature complexity (Cf ) with improvement in per-
formance (IP ) and improvement in learning rate (ILR).
All results are statistically significant (p < 0.05)
Feature complexity measures require an ?oracle?
simulated using a large amount of labeled data
which is not available for real annotation tasks.
In Section 3.2, we proposed measures based on
model?s uncertainty that do not require an oracle.
We calculate the mean, maximum and median of
the probability scores from the learned model on in-
stances in the held out dateset. We found a signifi-
cant but low negative correlation of these measures
with improvement in performance with feature feed-
back (maxProb = ?0.384, meanProb = ?0.256,
medianProb = ?0.242). This may seem counter-
intuitive. However, note that when the training data
is very small, the model might be quite certain about
112
its prediction even when it is wrong and feature feed-
back may help by correcting the model?s beliefs. We
observed that these probability measures have only
medium and significant positive correlation (around
0.5) with training datasize. Also, the held out dataset
we used may not be representative of the whole set
and using a larger dataset may give us more accurate
estimate of the model?s uncertainty. There are also
other ways to measure the model?s uncertainty, for
example, in SVMs the distance of an instance from
the decision boundary gives us an estimate of the
model?s uncertainty about that instance. We plan to
explore additional measures for model?s uncertainty
in the future.
5 Conclusion and Future Work
In this work, we analyze how the benefit from fea-
ture feedback varies with different problem charac-
teristics and how measures for categorizing learning
problems correlate with benefit from feature feed-
back. We define a problem instance as a tuple of
domain, instance granularity, feature representation,
labeled data, amount of irrelevant text and selective
sampling strategy.
We compare the two annotation strategies, with
and without feature feedback, in terms of both im-
provement in performance at a given stage in learn-
ing and improvement in learning rate. Instead of
evaluating the benefit from feature feedback us-
ing a specific feedback incorporation approach, we
estimate and compare how the maximum benefit
from feature feedback varies across different learn-
ing problems. This tells us what is the best feature
feedback can do for a given learning problem.
We find a strong and significant correlation be-
tween feature complexity measures and the two
measures of maximum benefit from feature feed-
back. However, these measures require an ?ora-
cle?, simulated using a large amount of labeled data
which is not available in real world annotation tasks.
We present measures based on the uncertainty of the
model on its prediction that do not require an oracle.
The proposed measures have a low but significant
correlation with benefit from feature feedback. In
our current work, we are exploring other measures
of uncertainty of the model. It is intuitive that a met-
ric that measures the uncertainty of the model on
parameter estimates should correlate strongly with
benefit from feature feedback. Variance in param-
eter estimates is one measure of uncertainty. The
Bootstrap or Jacknife method (Efron and Tibshirani,
1994) of resampling from the training data is one
way of estimating variance in parameter estimates
that we are exploring.
So far only a linear relationship of various mea-
sures with benefit from feature feedback has been
considered. However, some of these relationships
may not be linear or a combination of several mea-
sures together may be stronger indicators of the ben-
efit from feature feedback. We plan to do further
analysis in this direction in the future.
We only considered one selective sampling strat-
egy based on model?s uncertainty which we found
to provide more benefit from feature feedback. In
the future, we plan to explore other selective sam-
pling strategies. For example, density-based sam-
pling (Donmez and Carbonell, 2008) selects the in-
stances that are representative of clusters of simi-
lar instances, and may facilitate more effective feed-
back on a diverse set of features.
In this work, feature feedback was simulated us-
ing an oracle. Feedback from the users, however,
might be less accurate. Our next step will be to ana-
lyze how the benefit from feature feedback varies as
the quality of feature feedback varies.
Our eventual goal is to estimate the benefit from
feature feedback for a given problem so that the right
annotation strategy can be selected for a given learn-
ing problem at a given stage in learning and the total
annotation cost for learning the target concept can
be minimized. Note that in addition to the charac-
teristics of the labeled data analyzed so far, expected
benefit from feature feedback will also depend on
the properties of the data to be labeled next for the
two annotation strategies - with or without feature
feedback.
Acknowledgments
We thank Carolyn P. Rose?, Omid Madani, Hema
Raghavan, Jaime Carbonell, Pinar Donmez and
Chih-Jen Lin for helpful discussions, and the re-
viewers for their feedback. This work is supported
by DARPA?s Machine Reading program under con-
tract FA8750-09-C-0172.
113
References
Shilpa Arora and Eric Nyberg. 2009. Interactive annota-
tion learning with indirect feature voting. In Proceed-
ings of NAACL-HLT 2009 (Student Research Work-
shop).
Shilpa Arora, Elijah Mayfield, Carolyn Penstein Rose?,
and Eric Nyberg. 2010. Sentiment classification
using automatically extracted subgraph features. In
Proceedings of the Workshop on Emotion in Text at
NAACL.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIB-
SVM: a library for support vector machines. Soft-
ware available at http://www.csie.ntu.edu.
tw/?cjlin/libsvm.
William W. Cohen. 2004. Minorthird: Methods for iden-
tifying names and ontological relations in text using
heuristics for inducing regularities from data.
Pinar Donmez and Jaime G. Carbonell. 2008. Paired
Sampling in Density-Sensitive Active Learning. In
Proceedings of the International Symposium on Arti-
ficial Intelligence and Mathematics.
Gregory Druck, Gideon Mann, and Andrew McCallum.
2008. Learning from labeled features using general-
ized expectation criteria. In SIGIR ?08: Proceedings
of the 31st annual international ACM SIGIR confer-
ence on Research and development in information re-
trieval, pages 595?602, New York, NY, USA. ACM.
Gregory Druck, Burr Settles, and Andrew McCallum.
2009. Active learning by labeling features. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing (EMNLP). Association
for Computational Linguistics.
B. Efron and R.J. Tibshirani. 1994. An introduction to
the bootstrap. Monographs on Statistics and Applied
Probability. Chapman and Hall/CRC, New York.
Mahesh Joshi and Carolyn Penstein Rose?. 2009. Gen-
eralizing dependency features for opinion mining. In
ACL-IJCNLP ?09: Proceedings of the ACL-IJCNLP
2009 Conference Short Papers, pages 313?316, Mor-
ristown, NJ, USA. Association for Computational Lin-
guistics.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In ACL ?03: Proceedings
of the 41st Annual Meeting on Association for Compu-
tational Linguistics, pages 423?430, Morristown, NJ,
USA. Association for Computational Linguistics.
K. Lang. 1995. NewsWeeder: Learning to filter net-
news. In 12th International Conference on Machine
Learning (ICML95), pages 331?339.
D. Lewis. 1995. The reuters-21578 text categorization
test collection.
Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting
class relationships for sentiment categorization with
respect to rating scales. In Proceedings of ACL.
John C. Platt. 1999. Probabilistic outputs for support
vector machines and comparisons to regularized like-
lihood methods. In ADVANCES IN LARGE MARGIN
CLASSIFIERS, pages 61?74. MIT Press.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James H.
Martin, and Dan Jurafsky. 2004. Shallow seman-
tic parsing using support vector machines. In Pro-
ceedings of the Human Language Technology Con-
ference/North American chapter of the Association of
Computational Linguistics (HLT/NAACL).
Hema Raghavan, Omid Madani, and Rosie Jones. 2006.
Active learning with feedback on features and in-
stances. Journal of Machine Learning Research,
7:1655?1686.
Hema Raghavan, Omid Madani, and Rosie Jones. 2007.
When will feature feedback help? quantifying the
complexity of classification problems. In IJCAI Work-
shop on Human in the Loop Computing.
C. J. Van Rijsbergen. 1979. Information Retrieval. But-
terworths, London, 2 edition.
Simon Tong and Daphne Koller. 2000. Support vector
machine active learning with applications to text clas-
sification. In JOURNAL OF MACHINE LEARNING
RESEARCH, pages 999?1006.
Omar Zaidan, Jason Eisner, and Christine Piatko. 2007.
Using ?annotator rationales? to improve machine
learning for text categorization. In Human Language
Technologies: Proceedings of the Annual Conference
of the North American Chapter of the Association for
Computational Linguistics (NAACL-HLT), pages 260?
267, Rochester, NY, April.
114
