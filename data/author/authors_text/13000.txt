Tagging with Hidden Markov Models Using Ambiguous Tags
Alexis Nasr
LaTTice - Universite? Paris 7
anasr@linguist.jussieu.fr
Fre?de?ric Be?chet
Laboratoire d?Informatique
d?Avignon
frederic.bechet@lia.univ-avignon.fr
Alexandra Volanschi
LaTTice - Universite? Paris 7
avolansk@linguist.jussieu.fr
Abstract
Part of speech taggers based on Hidden
Markov Models rely on a series of hypothe-
ses which make certain errors inevitable.
The idea developed in this paper consists
in allowing a limited, controlled ambiguity
in the output of the tagger in order to avoid
a number of errors. The ambiguity takes
the form of ambiguous tags which denote
subsets of the tagset. These tags are used
when the tagger hesitates between the dif-
ferent components of the ambiguous tags.
They are introduced in an existing lexicon
and 3-gram database. Their lexical and
syntactic counts are computed on the basis
of the lexical and syntactic counts of their
constituents, using impurity functions. The
tagging process itself, based on the Viterbi
algorithm, is unchanged. Experiments con-
ducted on the Brown corpus show a recall of
0.982, for an ambiguity rate of 1.233 which
is to be compared with a baseline recall of
0.978 for an ambiguity rate of 1.414 using
the same ambiguous tags and with a recall
of 0.955 corresponding to the one best solu-
tion of standard tagging (without ambigu-
ous tags).
1 Introduction
Taggers are commonly used as pre-processors
for more sophisticated treatments like full syn-
tactic parsing or chunking. Although taggers
achieve high accuracy, they still make some
mistakes that quite often impede the following
stages. There are at least two solutions to this
problem. The first consists in devising more so-
phisticated taggers either by providing the tag-
ger with more linguistic knowldge or by refining
the tagging process, through better probability
estimation, for example. The second strategy
consists in allowing some ambiguity in the out-
put of the tagger. It is the second solution that
was chosen in this paper. We believe that this
is an instance of a more general problem in se-
quential natural language processing chains, in
which a module takes as input the output of
the preceding module. Since we cannot, in most
cases, expect a module to produce only correct
solutions, modules should be able to deal with
ambiguous input and ambiguous output. In our
case, the input is non ambiguous while the out-
put is ambiguous. From this perspective, the
quality of the tagger is evaluated by the trade-
off it achieves between accuracy and ambiguity.
The introduction of ambiguous tags in the
tagger output raises the question of the process-
ing of these ambiguous tags in the post-tagging
stages of the application. Leaving some ambigu-
ity in the output of the tagger only makes sense
if these other processes can handle it. In the
case of a chunker, ambiguous tags can be taken
into account through the use of weighted finite
state machines, as proposed in (Nasr and Volan-
schi, 2004). In the case of a syntactic parser,
such a device can usually deal with some ambi-
guity and discard the incorrect elements of an
ambiguous tag when they do not lead to a com-
plete analysis of the sentence. The parser itself
acts, in a sense, as a tagger since, while pars-
ing the sentence, it chooses the right tag among
a set of possible tags for each word. The rea-
son why we still need a tagger and don?t let the
parser do the job is time and space complexity.
Parsers are usually more time and space con-
suming than taggers and highly ambiguous tags
assignments can lead to prohibitive processing
time and memory requirements.
The tagger described in this paper is based
on the standard Hidden Markov Model archi-
tecture (Charniak et al, 1993; Brants, 2000).
Such taggers assign to a sequence of words
W = w1 . . . wn , the part of speech tag sequence
T? = t?1 . . . t?n which maximizes the joint prob-
ability P (T,W ) where T ranges over all possi-
ble tag sequences of length n. The probability
P (T,W ) is itself decomposed into a product of
2n probabilities, n lexical probabilities P (wi|ti)
(emission probabilities of the HMM) and n syn-
tactic probabilites (transition probabilities of the
HMM). Syntactic probabilities model the prob-
ability of the occurrence of tag ti given a history
which is the knowledge of the h preceding tags
(ti?1 . . . ti?h). Increasing the length of the his-
tory increases the predictive power of the tag-
ger but also the number of parameters to esti-
mate and therefore the amount of training data
needed. Histories of length 2 constitute a com-
mon trade-off for part of speech tagging.
We define an ambiguous tag as a tag that de-
notes a subset of the original tagset. In the re-
mainder of the paper, tags will be represented
as subscripted capitals T : T1, T2 . . .. Ambigu-
ous tags will be noted with multiple subscripts.
T1,3,5 for example, denotes the set {T1, T3, T5}.
We define the ambiguity of an ambiguous tag as
the cardinality of the set it denotes. This notion
is extended to non ambiguous tags, which can
be seen as singletons, their ambiguity is there-
fore equal to 1.
Ambiguous tags are actually new tags whose
lexical and syntactic probability distributions
are computed on the basis of lexical and syn-
tactic distributions of their constituents. The
lexical and syntactic probability distributions of
Ti1,...,in should be computed in such a way that,
when a word in certain context can be tagged
as Ti1 , . . . , Tin with probabilities that are close
enough, the tagger should choose the ambiguous
tag Ti1,...,in .
The idea of changing the tagset in order to im-
prove tagging accuracy has already been tested
by several researchers. (Tufis? et al, 2000) re-
ports experiments of POS tagging of Hungarian
with a large tagset (about one thousand differ-
ent tags). In order to reduce data sparseness
problems, they devise a reduced tagset which is
used for tagging. The same kind of idea is de-
veloped in (Brants, 1995). The major difference
between these approaches and ours, is that they
devise the reduced tagset in such a way that, af-
ter tagging, a unique tag of the extended tagset
can be recovered for each word. Our perspective
is significantly different since we allow unrecov-
erable ambiguity in the output of the tagger and
leave to the other processing stages the task of
reducing it. In the HMM based taggers frame-
work, our work bears a certain resemblance with
(Brants, 2000) who distinguishes between reli-
able and unreliable tag assignments using prob-
abilities computed by the tagger. Unreliable
tag assignments are those for which the prob-
ability is below a given threshold. He shows
that taking into account only reliable assign-
ments can significantly improve the accuracy,
from 96.6% to 99.4%. In the latter case, only
64.5% of the words are reliably tagged. For the
remaining 35.5%, the accuracy is 91.6%. These
figures show that taking into account probabil-
ities computed by the tagger discriminates well
these two situations. The main difference be-
tween his work and ours is that he does not
propose a way to deal with unreliable assign-
ments, which we treat using ambiguous tags.
The paper is structured as follows: section 2
describes how the probability distributions of
the ambiguous tags are estimated. Section 3
presents an iterative method to automatically
discover good ambiguous tags as well as an ex-
periment on the Brown corpus. Section 4 con-
cludes the paper.
2 Computing probability
distributions for ambiguous tags
Probabilistic models for part of speech taggers
are built in two stages. In a first stage, counts
are collected from a tagged training corpus
while in the second, probabilities are computed
on the basis of these counts. Two type of counts
are collected: lexical counts, noted Cl(w, T )
indicating how many times word w has been
tagged T in the training corpus and syntactic
counts Cs(T1, T2, T3) indicating how many
times the tag sequence T1, T2, T3 occurred in
the training corpus. Lexical counts are stored
in a lexicon and syntactic counts in a 3-gram
database.
These real counts will be used to compute
fictitious counts for ambiguous tags on the basis
of which probability distributions will be esti-
mated. The rationale behind the computation
of the counts (lexical as well as syntactic) of an
ambiguous tag T1...j is that they must reflect
the homogeneity of the counts of {T1 . . . Tj}. If
they are all equal, the count of T1...j should be
maximal.
Impurity functions (Breiman et al, 1984) per-
fectly model this behavior1: an impurity func-
tion ? is a function defined on the set of all N-
tuples of numbers (p1, . . . , pN ) satisfying ?j ?
[1, . . . , N ], pj ? 0 and
?N
j=1 pj = 1 with the fol-
lowing properties:
1Entropy would be another candidate for such compu-
tation. The same experiments have also been conducted
using entropy and lead to almost the same results.
? ? reaches its maximum at the point
( 1N , . . . , 1N )
? ? achieves its minimum at the points
(1, 0, . . . , 0), (0, 1, . . . , 0), . . . (0, 0, . . . , 1)
Given an impurity function ?, we define the
impurity measure of a N-tuple of counts C =
(c1, . . . , cN ) as follows :
I(c1, . . . , cN ) = ?(f1, . . . , fN ) (1)
where fi is the relative frequency of ci in C:
fi =
ci
?N
k=1 ck
The impurity function we have used is the
Gini impurity criteria:
?(f1, . . . , fN ) =
?
i6=j
fifj
whose maximal value is equal to N?1N .
The impurity measure will be used to com-
pute both lexical and syntactic fictitious counts
as described in the two following sections.
2.1 Lexical counts
Lexical counts for an ambiguous tag T1,...,n are
computed using lexical impurity Il(w, T1,...,n)
which measures the impurity of the n-tuple
(Cl(w, T1), . . . , Cl(w, Tn)):
Il(w, T1,...,n) = I(Cl(w, T1), . . . , Cl(w, Tn))
A high lexical impurity Il(w, T1,...,n) means
that w is ambiguous with respect to the differ-
ent classes T1, . . . , Tn. It reaches its maximum
when w has the same probability to belong to
any of them. The lexical count Cl(w, T1,...,n) is
computed using the following formula:
Cl(w, T1,...,n) = Il(w, T1,...,n)
n
?
i=1
Cl(w, Ti)
This formula is used to update a lexicon, for
each lexical entry, the counts of the ambiguous
tags are computed and added to the entry. The
two entries daily and deals whose original counts
are represented below2:
daily RB 32 JJ 41
deals NNS 1 VBZ 13
2RB, JJ, NNS and VBZ stand respectively for adverb,
adjective, plural noun and verb (3rd person singular,
present).
are updated to3:
daily RB 32 JJ 41 JJ_RB 36
deals NNS 1 VBZ 13 NNS_VBZ 2
2.2 Syntactic counts
Syntactic counts of the form Cs(X,Y, T1,...,n)
are computed using syntactic impurity
Is(X,Y, T1,...,n) which measures the impurity of
the n-tuple I(Cs(X,Y, T1), . . . , Cs(X,Y, Tn)) :
Is(X, Y, T1,...,n) = I(Cs(X, Y, T1), . . . , Cs(X, Y, Tn))
A maximum syntactic impurity means that
all the tags T1, . . . , Tn have the same probabil-
ity of occurrence after the tag sequence X Y .
If any of them has a probability of occurrence
equal to zero after such a tag sequence, the im-
purity is also equal to zero. The syntactic count
Cs(X,Y, T1,...,n) is computed using the following
formula:
Cs(X, Y, T1,...,n) = Is(X, Y, T1,...,n)
n
?
i=1
Cs(X, Y, Ti)
Such a formula is used to update the 3-
gram database in three steps. First, syntac-
tic counts of the form Cs(X,Y, T1,...,n) (with
X and Y unambiguous) are computed, then
syntactic counts of the form Cs(X,T1,...,n, Y )
(with X unambiguous and Y possibly ambigu-
ous) and eventually, syntactic counts of the form
Cs(T1,...,n, X, Y ) (for X and Y possibly ambigu-
ous). The following four real 3-grams:
A A A 100 A A B 100
A B A 10 A B B 1000
will give rise to following five fictitious ones:
A A A_B 100 A A_B A 18
A A_B A_B 31 A A_B B 181
A B A_B 19
which will be added to the 3-gram database.
Note that the real 3-grams are not modified dur-
ing this operation.
Once the lexicon and the 3-gram database
have been updated, both real and fictitious
counts are used to estimate lexical and syntactic
probability distribution. These probability dis-
tributions constitute the model. The tagging
process itself, based on the Viterbi search algo-
rithm, is unchanged.
3The fictitious counts were rounded to the nearest
integer.
2.3 Data sparseness
The introduction of new tags in the tagset in-
creases the number of states in the HMM and
therefore the number of parameters to be esti-
mated. It is important to notice that even if the
number of parameters increases, the model does
not become more sensitive to data sparseness
problems than the original model was. The rea-
son is that fictitious counts are computed based
on actual counts. The occurrence, in the train-
ing corpus, of an event (as the occurrence of
a sequence of tags or the occurrence of a word
with a given tag) is used for estimating both the
probability of the event associated to the sim-
ple tag and the probabilities of the events asso-
ciated with the ambiguous tags which contain
the simple tag. For example, the occurrence of
the word w with tag T , in the training corpus,
will be used to estimate the lexical probabil-
ity P (w|T ) as well as the lexical probabilities
P (w|T ?) for every ambiguous tag T ? of which T
may be a component.
3 Learning ambiguous tags from
errors
Since ambiguous tags are not given a priori,
candidates can be selected based on the errors
made by the tagger. The idea developed in this
section consists in learning iteratively ambigu-
ous tags on the basis of the errors made by a
tagger. When a word w tagged T1 in a refer-
ence corpus has been wrongly tagged T2 by the
tagger, that means that T1 and T2 are lexically
and syntactically ambiguous, with respect to w
and a given context. Consequently, T1,2 is a po-
tential candidate for an ambiguous tag.
The process of discovering ambiguous tags
starts with a tagged training corpus whose
tagset is called T0. A standard tagger, M0,
is trained on this corpus. M0 is used to tag
the training corpus. A confusion matrix is then
computed and the most frequent error is se-
lected to form an ambiguous tag which is added
to T0 to constitute T1. M0 is then updated
with the new ambiguous tag to constitue M1,
as described in section 2. The process is iter-
ated : the training corpus is tagged with Mi,
the most frequent error is used to constitue Ti+1
and a new tagger Mi+1 is built, based on Mi.
The process continues until the result of the tag-
ging on the development corpus converges or the
number of iterations has reached a given thresh-
old.
3.1 Experiments
The model described in section 2 has been
tested on the Brown corpus (Francis and
Kuc?era, 1982), tagged with the 45 tags of the
Penn treebank tagset (Marcus et al, 1993),
which constitute the initial tagset T0. The cor-
pus has been divided in a training corpus of
961, 3 K words, a development corpus of 118, 6
K words and a test corpus of 115, 6 K words.
The development corpus was used to detect the
convergence and the final model was evaluated
on the test corpus. The iterative tag learning
algorithm converged after 50 iterations.
A standard trigram model (without ambigu-
ous tags) M0 was trained on the training cor-
pus using the CMU-Cambridge statistical lan-
guage modeling toolkit (Clarkson and Rosen-
feld, 1997). Smoothing was done through back-
off on bigrams and unigrams using linear dis-
counting (Ney et al, 1994).
The lexical probabilities were estimated on
the training corpus. Unknown words (words of
the development and test corpus not present in
the lexicon) were taken into account by a sim-
ple technique: the words of the development
corpus not present in the training corpus were
used to estimate the lexical counts of unknown
words Cl(UNK, t). During tagging, if a word is
unknown, the probability distribution of word
UNK is used. The development corpus contains
4097 unknown words (3.4% of the corpus) and
the test corpus 3991 (3.3%).
3.1.1 Evaluation measures
The result of the tagging process consists in a
sequence of ambiguous and non ambiguous tags.
This result can no longer be evaluated using ac-
curacy alone (or word error rate), as it is usu-
ally the case in part of speech tagging, since the
introduction of ambiguous tags allows the tag-
ger to assign multiple tags to a word. This is
why two measures have been used to evaluate
the output of the tagger with respect to a gold
standard: the recall and the ambiguity rate.
Given an output of the tagger T = t1 . . . tn,
where ti is the tag associated to word i by the
tagger, and a gold reference R = r1 . . . rn where
r1 is the correct tag for word wi, the recall of T
is computed as follows :
REC(T ) =
?n
i=1 ?(ri ? ti)
n
where ?(p) equals to 1 if predicate p is true
and 0 otherwise. A recall of 1 means that for
every word occurrence, the correct tag is an el-
ement of the tag given by the tagger.
The ambiguity rate of T is computed as fol-
lows :
AMB(T ) =
?n
i=1 AMB(ti)
n
where AMB(ti) is the ambiguity of tag ti. An
ambiguity rate of 1 means that no ambiguous
tag has been introduced. The maximum ambi-
guity rate for the development corpus (when all
the possible tags of a word are kept) is equal to
2.4.
3.1.2 Baseline models
The successive modelsMi are based on the dif-
ferent tagsets Ti. Their output is evaluated with
the two measures described above. But these
figures by themselves are difficult to interpret if
we cannot compare them with the output of an-
other tagging process based on the same tagset.
The only point of comparision at hand is model
M0 but it is based on tagset T0, which does not
contain ambiguous tags. In order to create such
a point of comparison, a baseline model Bi is
built at every iteration. The general idea is to
replace in the training corpus, all occurrences of
tags that appear as an element of an ambigu-
ous tag of Ti by the ambiguous tag itself. After
the replacement stage, a model Bi is computed
and used to tag the development corpus. The
output of the tagging is evaluated using recall
and ambiguity rate and can be compared to the
output of model Mi.
The replacement stage described above is ac-
tually too simplistic and gives rise to very poor
baseline models. There are two problems with
this approach. The first is that a tag Ti can ap-
pear as a member of several ambiguous tags and
we must therefore decide which one to choose.
The second, is that a word tagged Ti in the ref-
erence corpus might be unambiguous, it would
therefore be ?unfair? to associate to it an am-
biguous tag. This is the reason why the replace-
ment step is more elaborate. At iteration i, for
each couple (wj , Tj) of the training corpus, a
lookup is done in the lexicon, which gives access
to all the possible non ambiguous tags word wj
can have. If there is an ambiguous tag T in
Ti such that all its elements are possible tags of
wj then, couple (wj , Tj) is replaced with (wj , T )
in the corpus. If several ambiguous tags fulfill
this condition, the ambiguous tag which has the
highest lexical count for wj is chosen.
Another simple way to build a baseline would
be to produce the n best solutions of the tag-
ger, then take for each word of the input the
tags associated to it in the different solutions
and make an ambiguous tag out of these tags.
This solution was not adopted for two reasons.
The first is that this method mixes tags from
different solutions of the tagger and can lead
to completely incoherent tags sequences. It is
difficult to measure the influence of this inco-
herence on the post-tagging stages of the ap-
plication and we didn?t try to measure it em-
pirically. But the idea of potentially producing
solutions which are given very poor probabili-
ties by the model is unappealing. The second
reason is that we cannot control anymore which
ambiguous tags will be created (although this
feature might be desirable in some cases). It
will be therefore difficult to compare the result
with our models (the tagsets will be different).4
3.1.3 Results
The results of the successive models have been
plotted in figure 1 and summarized in table 1,
which also shows the results on the test corpus.
For each iteration i, recall and ambiguity rates
of modelsMi and Bi on the development corpus
were computed. The results show, as expected,
that recall and ambiguity rate increase with the
increase of the number of ambiguous tags added
to the tagset. This is true for both models Mi
and Bi. The figure also shows that recall of Bi,
for a given i, is generally a bit lower than Mi
while its ambiguity is higher. Figure 2 shows
that for the same recall Bi introduces more am-
biguous tags than Mi.
The list of the 20 first ambiguous tags created
during the process is represented below :
1 IN_RB 11 IN_WDT_WP
2 DT_IN_WDT_WP 12 VBD_VBN
3 JJ_VBN 13 JJ_NN_NNP_NNS_RB_VBG
4 NN_VB 14 JJ_NN_NNP
5 JJ_NN 15 JJ_NN_NNP_NNS_RB
6 IN_RB_RP 16 JJR_RBR
4As a point of comparison we will nevertheless give a
few figures here. For low values of n, the n best solutions
have better recall for a given value of the ambiguity rate.
For instance, the 4 best tagger output yields a recall of
0.9767 for an ambiguity rate of 1.12, while, for the same
ambiguity rate, the iterative method obtains a 0.9604 re-
call. However, the 0.982 recall value which we attained
at the end of the iterative ambiguous tag learning pro-
cedure, corresponding to an ambiguity rate of 1.23, was
also reached by keeping the 7 best solutions of the tag-
ger, with an ambiguity rate of 1.20 (only slightly better
than ours).
 0.95
 0.96
 0.97
 0.98
 0.99
 1
 0  5  10  15  20  25  30  35  40  45  50
 1
 1.05
 1.1
 1.15
 1.2
 1.25
 1.3
 1.35
 1.4
 1.45
 1.5
re
ca
ll
am
bi
gu
ity
iterations
recall
ambiguity
recall (baseline)
ambiguity (baseline)
Figure 1: Recall and ambiguity rate of the suc-
cessive models on development corpus
 1
 1.05
 1.1
 1.15
 1.2
 1.25
 1.3
 1.35
 0.955  0.96  0.965  0.97  0.975
am
bi
gu
ity
recall
Model Mi
Baseline
Figure 2: Comparing ambiguity rates for a fixed
value of recall
7 NNPS_NNS 17 NN_VBG
8 VB_VBP 18 CD_NN
9 JJ_RB 19 WDT_WP
10 DT_RB 20 JJ_NN_NNP_NNS
Model DEV TEST
REC AMB REC AMB
M0 = B0 0.955 1 0.955 1
B40 0.978 1.414 0.979 1.418
M40 0.980 1.232 0.982 1.232
Table 1: Results on development and test cor-
pus
3.1.4 Model efficiency
The original idea of our method consists in cor-
recting errors that were made by M0, through
the introduction of ambiguous tags. Ideally, we
would like models Mi with i > 0 to introduce
an ambiguous tag only where M0 made a mis-
take. Unfortunately, it is not always the case.
We have classified the use of ambiguous tags
into four situations function of their influence
on both recall and ambiguity rate as indicated
in table 2, where G stands for the gold standard.
In situations 1 and 2 model M0 made a mis-
take. In situation 1, the mistake was corrected
by the introduction of the ambiguous tag while
in situation 2 it was not. In situations 3 and 4,
modelM0 did not make a mistake. In situation
3 the introduction of the ambiguous tag did not
create a mistake while it did in situation 4.
Situation G M0 Mi REC AMB
1 T1 T2 T1,2 + +
2 T3 T4 T1,2 0 +
3 T1 T1 T1,2 0 +
4 T3 T3 T1,2 ? +
Table 2: Influence of the introduction of an am-
biguous tag on recall and ambiguity rates
The frequency of each situation for some of
the 20 first ambiguous tags has been reported
in table 3. The last column of the table indicates
the frequency of the ambiguous tag (number of
occurrences of this tag divided by the sum of
occurrences of all ambiguous tags). The figures
show that ambiguous tags are not very efficient:
only a moderate proportion of their occurrences
(24% on average) actually corrected an error.
While we are very rarely confronted with sit-
uation 4 which decreases recall and increases
ambiguity (0.5% on average), in the vast ma-
jority of cases ambiguous tags simply increase
the ambiguity without correcting any mistakes.
Ambiguous tags behave quite differently with
respect to the four situations described above.
In the best cases (tag 6), 46% of the occurrences
corrected an error, and the tag is used one out of
ten times the tagger selects an ambiguous tag,
as opposed to tag 19 , which corrected errors in
48% of the cases but is not frequently used. The
worst configuration is tag 9, which, although
not chosen very often, corrects an error in 13%
of the occurrences and increases the ambiguity
in 85% of its occurrences.
A more detailed evaluation of the basic tag-
ging mistakes has suggested a better adapted
and more subtle method of using the ambiguous
tags which may at the same time constitute a di-
rection for future work. While the vast majority
of mistakes are due to mixing up word classes,
such as the -ing forms used as adjectives, as
nouns or as verbs, about one third of the mis-
takes concern only 25 common words such as
that, out, there, on, off, etc. Using the ambigu-
Tag 1 2 3 4 freq
1 0.220 0.026 0.746 0.006 0.126
5 0.129 0.014 0.852 0.002 0.165
6 0.461 0.000 0.538 0.000 0.107
9 0.133 0.012 0.850 0.003 0.082
19 0.483 0.064 0.419 0.032 0.012
AVG 0.241 0.029 0.722 0.005
Table 3: Error analysis of some ambiguous tags
ous tags for these words alone has yielded a re-
call of 0.965 on the test corpus (25% errors less
than model M0) while keeping the ambiguity
rate very low (1.04). With this procedure, 35%
of the ambiguous tags occurrences corrected an
error made byM0 and 59% increased the ambi-
guity. The result can be improved by designing
two sets of ambiguous tags: one to be used for
this set of words, and one for the word-classes
most often mistaken.
4 Conclusions and Future Work
We have presented a method for computing the
probability distributions associated to ambigu-
ous tags, denoting subsets of the tagset, in an
HMM based part of speech tagger. An iterative
method for discovering ambiguous tags, based
on the mistakes made by the tagger allowed to
reach a recall of 0.982 for an ambiguity rate of
1.232. These figures can be compared to the
baseline model which achieves a recall of 0.979
and an ambiguity rate of 1.418 using the same
ambiguous tags. An analysis of ambiguous tags
showed that they do not always behave in the
way expected; some of them introduce a lot of
ambiguity without correcting many mistakes.
This work will be developed in two directions.
The first one concerns the study of the differ-
ent behaviour of ambiguous tags which could
be influenced by computing differently the ficti-
tious counts of each ambiguous tag, based on its
behaviour on a development corpus in order to
force or prevent its introduction during tagging.
The second direction concerns experiments on
supertagging (Bangalore and Joshi, 1999) fol-
lowed by a parsing stage the tagging stage asso-
ciates to each word a supertag. The supertags
are then combined by the parser to yield a parse
of the sentence. Errors of the supertagger (al-
most one out of 5 words is attributed the wrong
supertag) often impede the parsing stage. The
idea is therefore to allow some ambiguity during
the supertagging stage, leaving to the parser the
task of selecting the right supertag using syntac-
tic constraints that are not available to the tag-
ger. Such experiments will constitute one way
of testing the viability of our approach.
References
Srinivas Bangalore and Aravind K. Joshi. 1999.
Supertagging: An approach to almost pars-
ing. Computational Linguistics, 25(2):237?
265.
Thorsten Brants. 1995. Tagset reduction with-
out information loss. In ACL?95, Cambridge,
USA.
Thorsten Brants. 2000. Tnt - a statistical
part-of-speech tagger. In Sixth Applied Natu-
ral Language Processing Conference, Seattle,
USA.
L. Breiman, J. H. Friedman, R. A. Olshen, and
C. J. Stone. 1984. Classification and Re-
gression Trees. Wadsworth & Brooks, Pacific
Grove, California.
Eugene Charniak, Curtis Hendrickson, Neil Ja-
cobson, and Mike Perkowitz. 1993. Equa-
tions for part-of-speech tagging. In 11th Na-
tional Conference on Artificial Intelligence,
pages 784?789.
Philip Clarkson and Ronald Rosenfeld. 1997.
Statistical language modeling using the cmu-
cambridge toolkit. In Eurospeech.
Nelson Francis and Henry Kuc?era. 1982. Fre-
quency Analysis of English Usage: Lexicon
and Grammar. Houghton Mifflin, Boston.
Mitchell Marcus, Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building
a large annotated corpus of english: The
penn treebank. Computational Linguistics,
9(2):313?330, june. Special Issue on Using
Large Corpora.
Alexis Nasr and Alexandra Volanschi. 2004.
Couplage d?un e?tiqueteur morpho-syntaxique
et d?un analyseur partiel repre?sente?s sous
la forme d?automates finis ponde?re?s. In
TALN?2004, pages 329?338, Fez, Morocco.
H. Ney, U. Essen, and R. Kneser. 1994.
On structuring probabilistic dependencies in
stochastic language modelling. Computer
Speech and Language, 8:1?38.
Dan Tufis?, Pe?ter Dienes, Csaba Oravecz, and
Tama?s Va?radi. 2000. Principled hidden
tagset design for tiered tagging of hungarian.
In LREC, Athens, Greece.
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 491?498, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Robust Named Entity extraction from large spoken archives
Beno??t Favre
Thales, MMP Laboratory
Colombes, France
benoit.favre@
fr.thalesgroup.com
Fre?de?ric Be?chet
LIA, University of Avignon
Avignon, France
frederic.bechet@
univ-avignon.fr
Pascal Noce?ra
LIA, University of Avignon
Avignon, France
pascal.nocera@
univ-avignon.fr
Abstract
Traditional approaches to Information Ex-
traction (IE) from speech input simply
consist in applying text based methods to
the output of an Automatic Speech Recog-
nition (ASR) system. If it gives satis-
faction with low Word Error Rate (WER)
transcripts, we believe that a tighter inte-
gration of the IE and ASR modules can
increase the IE performance in more dif-
ficult conditions. More specifically this
paper focuses on the robust extraction of
Named Entities from speech input where
a temporal mismatch between training and
test corpora occurs. We describe a Named
Entity Recognition (NER) system, de-
veloped within the French Rich Broad-
cast News Transcription program ESTER,
which is specifically optimized to pro-
cess ASR transcripts and can be integrated
into the search process of the ASR mod-
ules. Finally we show how some meta-
data information can be collected in or-
der to adapt NER and ASR models to new
conditions and how they can be used in a
task of Named Entity indexation of spoken
archives.
1 Introduction
Named Entity Recognition (NER) is a crucial step
in many Information Extraction (IE) tasks. It
has been a specific task in several evaluation pro-
grams such as the Message Understanding Confer-
ences (MUC), the Conferences on Natural Language
Learning (CoNLL), the DARPA HUB-5 program or
more recently the French ESTER Rich Transcription
program on Broadcast News data. Most of these
conferences have studied the impact of using tran-
scripts generated by an Automatic Speech Recogni-
tion (ASR) system rather than written texts. It ap-
pears from these studies that unlike other IE tasks,
NER performance is greatly affected by the Word
Error Rate (WER) of the transcripts processed. To
tackle this problem, different ideas have been pro-
posed: modeling explicitly the ASR errors (Palmer
and Ostendorf, 2001) or using the ASR system
alternate hypotheses found in word lattices (Sar-
aclar and Sproat, 2004). However performance in
NER decreases dramatically when processing high
WER transcripts like the ones that are obtained
with unmatched conditions between the ASR train-
ing model and the data to process. This paper in-
vestigates this phenomenon in the framework of the
NER task of the French Rich Transcription program
of Broadcast News ESTER (Gravier et al, 2004).
Several issues are addressed:
? how to jointly optimize the ASR and the NER
models ?
? what is the impact in term of ASR and NER
performance of a temporal mismatch between
the corpora used to train and test the models
and how can it be recovered by means of meta-
data information ?
? Can metadata information be used for indexing
large spoken archives ?
491
After a quick overview of related works in IE
from speech input, we present the ESTER evaluation
program ; then we introduce a NER system tightly
integrated to the ASR process and show how it
can successfully index high WER spoken databases
thanks to metadata.
2 Information extraction from large
spoken archives
The NIST Topic Detection and Tracking (Fiscus and
Doddington, 2002) and TREC document retrieval
evaluation programs has studied the impact of recog-
nition errors in the overall performance of Informa-
tion Extraction systems for tasks like story segmen-
tation or topic detection and retrieval. This impact
has been shown to be very limited compared to clean
text corpora. The main explanation for this phe-
nomenon is the redundancy effect: themes, topics
are very likely to be represented in texts by many
occurrences of salient words characterizing them.
Therefore, even if some of these words are missing,
numerical Information Extraction methods can use
the remaining salient words and discard the noise
generated by ASR errors.
However, this phenomenon is not true for tasks
related to the extraction of fine grained entities, like
Named Entities. Indeed, several studies have shown
that F-measure and WER are strongly correlated :
0.7 points of F-measure lost for each additional 1%
of WER according to (Miller et al, 2000) on the ex-
periments of 1998 NIST Hub-4 evaluations (Przy-
bocki et al, 1999).
Despite the continuous improvement of ASR
techniques, high WER transcriptions are inevitable
in difficult conditions like those found in large spo-
ken archives like in the MALACH project (Ramab-
hadran et al, 2003). Moreover, Named Entities ex-
traction performance is greatly affected by a mis-
match between training and testing data. This is
due mainly because proper names, which represent
most of the Named Entity items, are a very dynamic
category of words, strongly related to the period of
time representing the documents to process. There-
fore this mismatch is inevitable when dealing with
archives spreading over a long period of time and
containing multiple domain information.
One way of tackling this problem is to gather
metadata information related to the documents to
process. This information can be newspaper corpora
related to the same period of time, abstract describ-
ing the document content, or simply lists of terms or
entities likely to occur. Although such collected data
can be used to update the ASR and NER models,
the potential gain is rather small unless the metadata
corpus gathered fits perfectly the document to pro-
cess and is of a reasonable size. But another way
of exploiting this metadata information is to use it
as set of index terms that are going to be explicitly
looked for in the processed documents. We present
in section 7 an implementation of this idea that uses
word lattices as input.
3 The ESTER Named Entity evaluation
program
This work has been done within the framework of
the French Rich Transcription program of Broadcast
News ESTER. ESTER is organized by l?Association
Francophone de la Communication Parle?e (AFCP),
la De?le?gation Ge?ne?rale pour l?Armement (DGA)
and the Evaluation language Resources Distribution
Agency (ELDA). The ESTER corpus is made of 100
hours of Broadcast News data (from 6 French speak-
ing radio channels), manually transcribed, and la-
beled with a tagset of about 30 Named Entity cate-
gories folded in 8 main types:
? persons (pers): human beings, fiction charac-
ters, animals;
? locations (loc): geographical, traffic lines, elec-
tronic and real addresses, dial numbers;
? organizations (org): political, business, non
profit;
? geo-socio-political groups (gsp): clans, fami-
lies, nations, administrative regions;
? amounts (amount): durations, money, lengths,
temperature, age, weight and speed;
? time (time): relative and absolute time expres-
sions, hours;
? products (prod): art, printings, awards and ve-
hicles;
492
? facilities (fac): buildings, monuments.
This data is divided in 3 sets: a training set (84%),
a development set(8%) and a test set (8%). There
is a 6 month gap difference between the training
corpus and the test corpus while the development
corpus matches the training data from a temporal
point of view: the training corpus contains Broad-
cast News spreading from 2002 to December 2003;
the development corpus contains news from 2003;
the test corpus has been recorded in October 2004.
There are also 2 new radio channels in the test cor-
pus which were not in the training data.
For these reasons the development data is called
the matched corpus as the recording conditions
match those of the training corpus and the test data
is called the unmatched corpus. As a consequence,
we can study the effect of unmatched conditions on
ASR as well as IE performance and propose solu-
tions for dealing with this problem.
One of the main characteristics of the ESTER cor-
pus is the size of the NE tagset and the high ambi-
guity rate among the NE categories (eg. administra-
tive regions and geographical locations): 83% of the
matched corpus entities occur in the training corpus
and 40% of them are ambiguous whereas only 61%
of the unmatched corpus entities occur in the train-
ing corpus and 32% of them are ambiguous.
The most commonly used measures for evaluat-
ing NE extraction performance are Slot Error Rate
(SER) and F-measure. SER is very similar to WER
because it takes into account fine grained errors like
insertions, deletions and substitutions (entity type
and extent). The scoring process is based on the
same alignment between reference and hypothesis
data than the one obtained for measuring WER and
SER is known for being more accurate and penal-
izing than F-measure. Both measures weights can
be adjusted to favor recall or precision and therefore
adapted to a specific task.
SER = 100 ?
?
e?E ?e|e|
|Ref slots| F? =
(1+?2)RP
R+?2P
R = |Correct slots||Ref slots| P =
|Correct slots|
|Hyp slots|
with e ? E being an error type (insertion, dele-
tion, type, extent, type+extent, multiple) and ?e its
weight (resp. 1, 1, .5, .5, .8, 1.5) ; P is the precision
and R the recall; F1 is used in this paper.
4 Extracting NE from written text vs. ASR
output
As previously mentioned in section 2, WER and
SER performance are strongly correlated. Besides
the intrinsic difficulties of ASR (robustness to noise,
speaker variation, lack of coverage of the Language
Models used, . . . ), there is a source of errors which
is particularly important in IE from speech input:
the Out-Of-Vocabulary (OOV) word phenomenon.
Indeed, ASR models are built on huge textual cor-
pus and only contain the most frequent words to
limit computation and memory usage. If this is the
right approach to WER reduction, it is certainly not
valuable to information extraction where unlikely
events are considered as important. For instance,
many document retrieval models use inverse docu-
ment frequency (rareness) as a word weighting pa-
rameter. So, unlikely proper names are not in reach
of the ASR transcription system and hence cannot
be spotted by a Named Entity extraction module.
In addition to Out-of-Vocabulary words, two other
phenomenons have also a strong impact on NER
performance: the insertion of erroneous proper
names that automatically trigger the insertion of an
entity and spontaneous speech phenomenons. These
speech dysfluencies (hesitations, filled pauses, false
starts...) reduce the quality of the transcript because
they are usually not covered by language models
(built from textual data) or artificially introduced.
One should remove these from the transcript to im-
prove the quality of the labeling.
In order to deal with ASR errors two approaches
have been proposed:
? modeling explicitly the ASR errors, thanks to
a development corpus and a set of confidence
measures, in order to detect the possible er-
rors of the 1-best word string hypothesis (with
the type of errors) before extracting the NEs
(Palmer and Ostendorf, 2001);
? exploiting a search space bigger than the 1-best
hypothesis alone, either by taking into account
an n-best list (Zhai et al, 2004) or the whole
word lattice (Saraclar and Sproat, 2004).
493
The method proposed in this paper is close to this
second approach where the whole word lattice out-
put by the ASR system is used in order to increase
NER performance from noisy input.
We will present also in the next section a new
strategy for adapting NER models to ASR tran-
scripts, based on one of the main characteristics of
such transcripts: a closed vocabulary is used by
the ASR system. To our knowledge this has never
been fully exploited by NER systems. Indeed while
the key point of NER systems on written text is
their generalization capabilities when processing un-
known words, this feature is not relevant for ASR
transcripts as the system cannot generate words out
of the lexicon (there are no unknown words). There-
fore we propose here to fully exploit this constraint
(close vocabulary): since the OOV words cannot ap-
pear in the ASR transcripts, the NER models can
by over-trained on the words belonging to the ASR
lexicon. This is going to be developed in the next
section.
5 Robust Named Entity extraction
We have developed in this study two NER systems:
one is based on the freely available NLP tool Ling-
pipe1, adapted and trained on the French ESTER
corpus and dedicated to process text input. This sys-
tem is going to be called NERtext in the experiment
section. The second NER system has been devel-
oped for this study and is specifically built for being
tightly integrated with the ASR processes. The two
main features of this system, called NERasr in the
following, are its ability to process word lattices and
the fact that the NER models are trained for a spe-
cific ASR lexicon. These two systems are going to
be presented in the next sections.
5.1 Text-based NER system: NERtext
Among all the different methods that have been pro-
posed for NER, one can find rule based models
(Cunningham et al, 2002), Maximum Entropy mod-
els (Brothwick et al, 1998), Conditionnal Random
Fields or probabilistic HMM-based models (Bikel et
al., 1999).
Lingpipe implements an HMM-based model. It
maximizes the probability of a tag sequence Ti over
1Lingpipe: http://alias-i.com/lingpipe/
a word sequence Wi. A context of two preceding
words and one preceding tag is used to approximate
this probability. Generalization is done through a
simple process: words occurring with low frequency
are replaced by feature based categories (capitalized,
contains digits, . . . ). In this approach, there must be
one tag per word. Words starting and ending entities
are labeled with special tags. Because some features
are lacking in ASR transcripts (e.g. capitalization,
digits, sentence boundaries, . . . ) some word lists for
each kind of features are added as presented in (Ap-
pelt and Martin, 1999).
5.2 ASR-based NER system: NERasr
Errors occurring in ASR output lead NER systems
to overgenerate NE detections. This is due to both
erroneous words insertions in the ASR transcripts
as well as some abusive generalization made by the
NER systems. If these generalization capabilities
are very important for processing unknown words
in written texts, they can be an handicap in a closed-
vocabulary situation like the one observed when pro-
cessing ASR output. In order to reduce and con-
trol the insertion rate of our NER system, we im-
plemented a two level approach: the first level is
made of NE grammars coded as Finite State Ma-
chine (FSM) transducers and the second level is a
statistical HMM-based tagger.
5.2.1 NE transducers
To each NE category is attached a set of regular
grammars, extracted from the ESTER training cor-
pus and generalized thanks to the annotation guide-
lines and web-gathered word lists. Theses grammars
are represented by Finite State Machines (FSMs)
(thanks to the AT&T GRM/FSM toolkit (Allauzen et
al., 2003)). These FSMs are transducers that accept
word sequences on the input symbols and output NE
labels on the output symbols. They are all grouped
together in a single transducer, called Tgram, with
a filler model that accepts any string of words. Be-
cause these FSMs are lexicalized with the words of
the ASR lexicon, one can control the generalization
capabilities of the grammars thanks to the occur-
rence contexts of these words in the training corpus.
During the NER process, the first step is to compose
the FSM representing the NE transducer and the out-
put of the ASR module (either a 1-best word string
494
or a word lattice, both encoded as an FSM called G).
5.2.2 NE tagger
The result of the composition of the NE trans-
ducer with the ASR output is an FSM (G ? Tgram)
containing all the possible parsing made by the NE
grammars. In order to find the best analysis a sta-
tistical model is used to decide between entity types
and entities boundaries. This model is a 2nd order
n-gram model (trigram) represented by a weighted
FSM (called Ttagger) with the same framework as
the grammars. The most likely NE label sequence
is obtained by finding the best path in the FSM:
G?Tgram ?Ttagger. This corresponds to maximize
the following probability:
PW =
n
?
i=1
P (Wi, Ti|Wi?1, Ti?1,Wi?2, Ti?2)
This model is similar to the one implemented in
Lingpipe but it uses different smoothing methods.
Similarly, first and last words of entities are repre-
sented by special tags (this helps getting more accu-
rate boundaries) and low frequency words (appear-
ing less than a fixed number of times in the training
corpus) are generalized using their Part-Of-Speech
tags. The key points of this approach are that it has a
better control of the generalization capabilities than
a feature based NER system, thanks to the NE gram-
mars; it integrates the closed vocabulary constraint
of the ASR systems; and it is not limited to the 1-
best word hypothesis but can use the full ASR search
space (through word lattices) in order to detect en-
tities. Processing word lattices allows us to output,
at the end of the extraction process, an n-best list of
NE hypotheses. To each hypothesis are attached two
scores:
? the likelihood score given by the ASR model to
the best word string supporting this NE hypoth-
esis in the word lattice;
? the probability P (Wn, Tn, . . . ,W0, T0) given
by the NE tagger to the sequence of NE la-
bels T0, . . . , Tn and the sequence of words
W0, . . . ,Wn.
From this n-best list we can estimate the Oracle
performance of the NER system. This measure is
the recall measure upper bound than can be obtained
by extracting all the possible entities from a word
lattice, thanks to the NE transducers, and simulating
a perfect strategy that always take the right decision
in choosing among all the possible entities.
Decision strategies on such an n-best of NE hy-
pothesis can also involve other levels of informa-
tion on the document to process like the date or
the theme, for example. In the evaluation presented
in the next section we compare this Oracle perfor-
mance measure to the results of the simplest deci-
sion strategy which consists in choosing the NE hy-
pothesis with the highest likelihood.
5.3 Evaluation
The evaluation presented in Tables 1 and 2 is per-
formed using the Slot Error Rate and the F-measure
on the matched and unmatched corpora presented in
section 3.
corpus matched unmatched
tagger SER F-m SER F-m
NERtext 21 84 27 79
NERasr 23 84 37 74
WER 0 0
Figure 1: F-measure and Slot Error Rate measures
on the ESTER reference corpora (matched and un-
matched) for both NER systems
corpus matched unmatched
tagger SER F-m SER F-m Oracle
NERtext 42 72 55 63 61.9
NERasr 41 73 54 63 76.9
WER 21.2 26.4
Figure 2: F-measure, Slot Error Rate and Oracle re-
call measures on the ASR output of the matched and
unmatched corpora for both NER systems
Figure 1 presents SER and F-measure on the two
test sets (matched and unmatched) for the text ori-
ented (NERtext) and the speech oriented (NERasr)
NER systems, on clean text (manual transcripts).
Figure 2 shows the results obtained on the ASR tran-
scripts.
As expected on manually transcribed data,
NERtext obtains better results than NERasr (which
495
has poorer generalization capabilities). On the ASR
outputs the results obtained by both systems are
comparable however NERasr has the advantage of
processing word lattices, leading to an interesting
Oracle performance. We are studying now more
elaborate decision strategies in order to take fully
advantage of this feature.
The decrease in F-measure observed between the
reference and the ASR transcripts is similar to the
one obtained in other studies (Miller et al, 2000).
One observation that can be made on these results is
the impact of the time mismatch between the train-
ing and the test corpora. A 6 month difference in the
unmatched corpus leads to a very big drop in both
SER and F-measure. This can be explained by the
fact that NEs are very time-dependent. We are going
now to present some methods developed to tackle
this problem.
6 Updating Language and NE models with
metadata information
The only mismatch between the training and the un-
matched corpus of our experiments is a 6 months
temporal mismatch, therefore we collected a cor-
pus of newsletters made on a daily basis by the
French newspaper Le Monde corresponding to these
6 months. These newsletters contain an abstract of
the news of each day. We make the following two
hypotheses:
? firstly these newsletters are related to the same
time period as the unmatched corpus, there-
fore integrating them into the ASR models (lex-
icon+Language Model) should help reducing
the OOV word effect;
? secondly because they represent an abstract of
the news of each day, the Named Entities oc-
curring in a particular newsletter should contain
all the major events of the corresponding day
and therefore constitutes a useful list of terms
that can be used for indexing a Broadcast News
document related to the same period of time.
6.1 NE distribution analysis
This newsletter corpus contains 1M words and after
being tagged by the NERtext system, 140k entities
were extracted. To check the relevance of this cor-
pus for adapting the models to the unmatched test
corpus, we studied the distribution of the words and
the entities for each day, from January to December
2004. The unmatched test corpus is made of Broad-
cast News ranging from October 10th to October
16th 2004. The following observations were made:
72% of the NEs and 60% of the words contained in
them occur only one day in this corpus; the inter-
section of the NEs occurring in both the newsletter
of a particular day and the entities belonging to the
unmatched test corpus shows a peak, illustrated by
figure 3, for the days of the test corpus; at this peak,
25% of the NEs are used the same day in the two
corpora.
 0
 5
 10
 15
 20
 25
-20 -15 -10 -5  0  5  10  15  20
%
 e
nt
itie
s 
m
at
ch
in
g
 time window in days
Figure 3: Percentage of entities of the unmatched
corpus occuring at least n days earlier or later in the
newsletter corpus (at a window of 0 days, entities
appear on the same day in both corpus).
The first observation matches those presented in
(Whittaker, 2001) and validates our approach which
consists in carefully adapting the ASR and NER
models with data corresponding to the exact period
of time as the one of the documents to process: by
taking into account a larger period of time for the
adaptation corpus, the necessity of restraining the
models to the most frequent entities would lead to
discard low frequency terms that can be crucial for
characterizing the news of a given day.
If the second observation clearly highlights the
correlation between the NE distribution in both cor-
pora, it also points out that only 25% of the enti-
ties of the unmatched corpus occur in the newslet-
ters corresponding to the same days. Therefore the
potential improvement in the overall NER perfor-
mance is clearly limited. This will be confirmed in
the next section, however one can think that if these
496
entities are shared, for a given day, by both corpora,
it is because they represent the key topics of this day
and therefore they can be considered as very rele-
vant indexing terms for applications like document
retrieval. This last point is developed in section 7.
6.2 Model adaptation
Several studies (Whittaker, 2001; Federico and
Bertoldi, 2001; Chen et al, 2004) propose adap-
tation methods of a general language model to the
possibly small corpora extracted from these kinds of
metadata information (an overview of these meth-
ods can be found in (Bellegarda, 2004)). Depending
on the adaptation method and the kind of metadata
information used, some gains in performance have
been reported. But it appears that the choice of the
metadata and the size of the adaptation corpus col-
lected are critical in this adaptation process: if the
adaptation corpus is not exactly related to the topics
of the document to process, no real gains are ob-
tained (e.g. (Chen et al, 2004) reports that the best
gains were obtained with a story-based adaptation
method).
From all these previous works, our system imple-
ments the following adaptation process:
? the text corpus corresponding to the newsletters
is added to the ASR language model by means
of a linear interpolation;
? proper names occuring twice or more in the
newsletter corpus are added to the ASR lexi-
con;
? for the same days as those of the unmatched
corpus, this cutoff is suppressed and all the
proper names are added;
? the Named Entity wordlists and grammars are
also enriched with these proper names and en-
tities extracted from the collected corpus.
1K new proper names were added to the 65K
word ASR lexicon. The general OOV reduction ob-
tained was 0.14% leading to an absolute WER re-
duction of 0.3%. Similarly the SER decreased of
about 0.3% thanks to this adaptation and the Ora-
cle recall measure in the word lattices was improved
by an absolute 3%. These improvements are not
significant enough to justify the use of this kind of
metadata information for improving the general per-
formance of both ASR and NER processes. How-
ever, if we focus now on the entities occurring in the
newsletters corresponding to the exact days of the
unmatched corpus, the improvement is much more
significant, as presented in the next section.
7 Named Entity Indexation
As previously mentioned, 25% of the unmatched
corpus entities occur in the newsletters correspond-
ing to the same day as those of the unmatched test.
In order to measure the improvement obtained with
our adaptation technique on these particular entities,
we did the following experiment:
? a set of 352 entities was selected from the
newsletters related to same period of time as the
test, these entities represent the indexing terms
that are going to be looked for in the word lat-
tices of the unmatched corpus;
? the NERasr system was then applied to these
word lattices with two conditions: the word
lattices and the NER models before adaptation
and those obtained after adaptation with the
newsletter corpus;
? precision, recall, F-measure and Oracle error
rate were estimated for both conditions.
Condition Prec. Recall F-m Oracle
no adaptation 87.0 75.7 80.9 83.6
with adaptation 87.5 83.9 85.7 92
Figure 4: Extraction results on the selected NEs on
the unmatched corpus with and without adaptation
of the ASR and NER models on the newsletter cor-
pus
As we can see in table 4, the adaptation process
increases very significantly the recall measure of the
NE extraction. This is particularly relevant in some
IE tasks like the document retrieval task.
8 Conclusion
We have presented in this paper a robust Named En-
tity Recognition system dedicated to process ASR
transcripts. The FSM-based approach allows us to
497
control the generalization capabilities of the system
while the statistical tagger provides good labeling
decisions. The main feature of this system is its
ability to extract n-best lists of NE hypothesis from
word lattices leaving the decision strategy choosing
to either emphasize the recall or the precision of the
extraction, according to the task targeted. A compar-
ison between this approach and a standard approach
based on the NLP tools Lingpipe validates our hy-
potheses. This integration of the ASR and the NER
processes is particularly important in difficult con-
ditions like those that can be found in large spoken
archives where the training corpus does not match
all the documents to process. A study of the use of
metadata information in order to adapt the ASR and
NER models to a specific situation showed that if the
overall improvement is small, some salient informa-
tion related to the metadata added can be better ex-
tracted by means of this adaptation.
References
Cyril Allauzen, Mehryar Mohri, and Brian Roark. 2003.
Generalized algorithms for constructing statistical lan-
guage models. In ACL?03, Sapporo, Japan.
D. Appelt and D. Martin. 1999. Named entity extraction
from speech: Approach and results using the TextPro
system. In Proceedings Darpa Broadcast News Work-
shop.
Jerome R. Bellegarda. 2004. Statistical language model
adaptation: review and perspectives. Speech Commu-
nication, 42 Issue 1:93?108.
Daniel M. Bikel, Richard L. Schwartz, and Ralph M.
Weischedel. 1999. An algorithm that learns what?s
in a name. volume 24, pages 211?231.
Andrew Brothwick, John Sterling, Eugene Agichtein,
and Ralph Grishman. 1998. Exploiting diverse knowl-
edge sources via maximum entropy in named entity
recognition.
Langzhou Chen, Jean-Luc Gauvain, Lori Lamel, and
Gilles Adda. 2004. Dynamic language modeling for
broadcast news. In In International Conference on
Speech and Language Processing, pages 1281?1284.
H. Cunningham, D. Maynard, K. Bontcheva, and
V. Tablan. 2002. GATE: A framework and graphi-
cal development environment for robust NLP tools and
applications. In Proceedings of the 40th Anniversary
Meeting of the Association for Computational Linguis-
tics.
M. Federico and N. Bertoldi. 2001. Broadcast news LM
adaptation using contemporary texts. In Proceedings
of European Conference on Speech Communication
and Technology (Eurospeech), pages 239?242, Aal-
borg, Denmark.
Jonathan G. Fiscus and George R. Doddington. 2002.
Topic detection and tracking evaluation overview.
Topic detection and tracking: event-based information
organization, pages 17?31.
G. Gravier, J.F. Bonastre, E. Geoffrois, S. Galliano,
K. McTait, and K. Choukri. 2004. ESTER, une cam-
pagne d?e?valuation des syste`mes d?indexation automa-
tique d?e?missions radiophoniques en franc?ais. In Proc.
Journe?es d?Etude sur la Parole (JEP).
David Miller, Sean Boisen, Richard Schwartz, Rebecca
Stone, and Ralph Weischedel. 2000. Named entity
extraction from noisy input: Speech and OCR. In Pro-
ceedings of ANLP-NAACL 2000, pages 316?324.
D. D. Palmer and M. Ostendorf. 2001. Improving in-
formation extraction by modeling errors in speech rec-
ognizer output. In Proceedings of the First Interna-
tional Conference on Human Language Technology
Research.
M. A. Przybocki, J. G. Fiscus, J. S. Garofolo, and D. S.
Pallett. 1999. 1998 Hub-4 Information Extraction
Evaluation. In Proceedings Of The DARPA Broad-
cast News Workshop, pages 13?18. Morgan Kaufmann
Publishers.
Bhuvana Ramabhadran, Jing Huang, and Michael
Picheny. 2003. Towards automatic transcription of
large spoken archives - english ASR for the MALACH
project. In Proc. IEEE International Conference on
Acoustics, Speech and Signal Processing, ICASSP,
pages 216?219.
Murat Saraclar and Richard Sproat. 2004. Lattice-based
search for spoken utterance retrieval. In HLT-NAACL
2004: Main Proceedings, pages 129?136, Boston,
Massachusetts, USA. Association for Computational
Linguistics.
E. W. D. Whittaker. 2001. Temporal adaptation of lan-
guage models. In Adaptation Methods for Speech
Recognition, ISCA Tutorial and Research Workshop
(ITRW), August. LM Adaptation for information re-
trieval of spoken news/radio programs (i.e. Speech-
Bot).
Lufeng Zhai, Pascale Fung, Richard Schwartz, Marine
Carpuat, and Dekai Wu. 2004. Using n-best lists
for named entity recognition from chinese speech.
In HLT-NAACL 2004: Short Papers, pages 37?40,
Boston, Massachusetts, USA. Association for Compu-
tational Linguistics.
498
 	


Mining Spoken Dialogue Corpora for System Evaluation and
Modeling
Frederic Bechet
LIA-CNRS
University of Avignon, France
frederic.bechet@
lia.univ-avignon.fr
Giuseppe Riccardi
AT&T Labs
Florham Park, NJ, USA
dsp3@
research.att.com
Dilek Hakkani-Tur
AT&T Labs
Florham Park, NJ, USA
dtur@
research.att.com
Abstract
We are interested in the problem of modeling
and evaluating spoken language systems in the
context of human-machine dialogs. Spoken di-
alog corpora allow for a multidimensional anal-
ysis of speech recognition and language under-
standing models of dialog systems. Therefore
language models can be directly trained based
either on the dialog history or its equivalence
class (or cluster). In this paper we propose an
algorithm to mine dialog traces which exhibit
similar patterns and are identified by the same
class. For this purpose we apply data clustering
methods to large human-machine spoken dia-
logue corpora. The resulting clusters can be
used for system evaluation and language mod-
eling. By clustering dialog traces we expect to
learn about the behavior of the system with re-
gards to not only the automation rate but the
nature of the interaction (e.g. easy vs difficult
dialogs). The equivalence classes can also be
used in order to automatically adapt the lan-
guage model, the understanding module and the
dialogue strategy to better fit the kind of in-
teraction detected. This paper investigates dif-
ferent ways for encoding dialogues into multi-
dimensional structures and different clustering
methods. Preliminary results are given for clus-
ter interpretation and dynamic model adapta-
tion using the clusters obtained.
1 Introduction
The deployment of large scale automatic spoken
dialog systems, like How May I Help You?SM
(HMIHY) (Gorin et al, 1997), makes avail-
able large corpora of real human-machine di-
alog interactions. Traditionally, this data is
used for supervised system evaluation. For in-
stance, in (Kamm et al, 1999) they propose a
static analysis aimed at measuring the perfor-
mance of a dialog system, especially in an at-
tempt to automatically estimate user satisfac-
tion. In (Hastie et al, 2002), a dynamic strat-
egy in the error handling process is proposed. In
all these studies, supervised learning techniques
are used in order to classify dialogs to predict
user satisfaction or dialog failures.
A novel approach to the exploitation of dialog
corpora is for speech recognition and language
understanding modeling. In fact, such corpora
allow for a multidimensional analysis of speech
and language models of dialog systems. Our
work differs from previous studies in the algo-
rithmic approach and learning scenario.
First we use unsupervised speech mining
techniques. We apply data clustering methods
to large spoken dialog corpora. Two kinds of
clustering methods are used: a hierarchical one
based on decision trees and the optimization of
a statistical criterion; a partitional one based
on a k-means algorithm applied to vectors rep-
resenting the dialogs. We interpret the clusters
obtained and define a label for each of them.
Second we perform our analyses on large cor-
pora of real dialogs collected from deployed sys-
tems. These log files contain a trace of the inter-
action between the users and a particular sys-
tem at a certain point in time. Our goal is to
highlight the structures behind these traces.
Lastly, we investigate several ways of encod-
ing a dialog into a multidimensional structure in
order to apply data clustering methods. Infor-
mation about the system channel and the user
channel are discussed and two ways of encoding
are proposed, one for hierarchical clustering and
the other for partitional clustering.
The clusters obtained can be used to learn
about the behavior of the system with regards
to the automation rate and the type of inter-
action (e.g. easy vs difficult dialog). More-
over, the clusters can be used on-the-fly to auto-
matically adapt the language model, the under-
standing module and even the dialog strategy
to better fit the kind of interaction detected.
In this study, we present two levels of clus-
tering: clustering at the utterance level and the
? System: How may I help you?
? User: Hello?
? Call-type: Hello
? System: Hello, how may I help you?
? User: I have a question.
? Call-type: Ask(Info)
? System: OK, What is your question?
? User: I would like to know my account
balance.
? Call-type: Request(Account Balance)
? System: I can help you with that. What
is your account number?
? User: ...
Figure 1: An example dialog from our corpus.
dialog level. Clustering at the utterance is for
modeling the language people use in a specific
dialog context; clustering at the dialog level al-
lows us to characterize the whole interaction be-
tween the users and a system. In the next sec-
tion we describe the corpora data structure. In
section 3 we describe the clustering algorithms.
In sections 4 and 5 we report on experiments
and results for utterance-based and dialog clus-
tering, respectively.
2 Dialog corpora
The corpora is collected from an automatic call
routing system where the aim is to transfer the
user to the right route in a large call center.
An example dialog from a customer care ap-
plication is given in Figure 1. After the greet-
ing prompt, the speaker?s response is recognized
using an automatic speech recognizer (ASR).
Then, the intent of the speaker is identified from
the recognized sequence, using a spoken lan-
guage understanding (SLU) component. This
step can be framed as a classification problem,
where the aim is to classify the intent of the user
into one of the predefined call-types (Gorin et
al., 1997). Then, the user would be engaged in a
dialog via clarification or confirmation prompts
until a final route is determined.
3 Hierarchical and Partitional
clustering
The goal of clustering is to reduce the amount
of data by categorizing or grouping similar data
items together. Clustering methods can be di-
vided into two basic types: hierarchical and par-
titional clustering. A lot of different algorithms
have been proposed for both types of clustering
methods in order to split a data set into clusters.
Hierarchical clustering proceeds successively
by either merging smaller clusters into larger
ones, or by splitting larger clusters. The result
of the algorithm is a tree of clusters. By cutting
the tree at a desired level a clustering of the
data items into disjoint groups is obtained. We
use in this study a decision-tree based clustering
method.
Partitional clustering, on the other hand, at-
tempts to directly decompose the data set into
a set of disjoint clusters. A criterion function is
used in order to estimate the distance between
the samples of the different clusters. By mini-
mizing this function between the samples of the
same clusters and maximizing it among the dif-
ferent clusters, the clustering algorithm itera-
tively finds the best cluster distribution accord-
ing to the criteria used. We use in this study a
k-means algorithm applied to vectors encoding
the dialogs. The number of clusters is fixed in
advance.
4 Clustering at the utterance level
Performing clustering at the utterance level in a
dialog corpus aims to capture different kinds of
language that people would use in a specific di-
alog context. This is a way of grouping together
turns of dialogs belonging to completely differ-
ent requests but sharing some common proper-
ties according to their dialog contexts.
The immediate application of such a study
can be the training of specialized Language
Models (LMs) that can be used in replacement
of a generic one once a specific situation is de-
tected.
4.1 Decision-tree based clustering
In order to obtain utterance clusters from which
we can build LMs we use a decision tree method
based on an optimization criterion that has a
direct influence on the recognition process: the
perplexity measure of the Language Model on
the manually transcribed training corpus. We
decide to use this criterion because even if there
is no evidence that a gain in perplexity results
in a Word Error Rate (WER) reduction, these
two quantities are generally related.
The clustering algorithm chosen is based on a
decision-tree approach inspired by the Semantic
Classification Tree method proposed in (Kuhn
and Mori, 1995) and already used for corpus
clustering in (Esteve et al, 2001) and (Bechet
et al, 2003). One originality of this kind of deci-
sion tree is the dynamic generation of questions
during the growing process of a tree.
4.2 Decision tree features
Each turn of the spoken dialog corpus used for
the clustering process is represented by a mul-
tidimensional structure. Two kinds of channel
can be considered in order to define the features:
the system channel (which contains all the infor-
mation managed by the system like the prompts
or the states of the dialog) and the user chan-
nel (which contain the utterances uttered by the
user with all their characteristics: length, vo-
cabulary, perplexity, semantic calltypes, etc.).
Because the clusters obtained are going to be
used dynamically by training specific LMs on
each of them, we used mostly system channel
features in these experiments. On the HMIHY
corpus we used the following features:
? prompt text: this is the word string ut-
tered by the system before each user?s ut-
terance;
? prompt category: prompt category ac-
cording to the kind of information re-
quested (conf if the prompt asks for a con-
firmation, numerical if the information re-
quested is a numerical value like a phone
number, other in all the other cases);
? dialog state: a label given by the Dialog
Manager characterizing the current state of
the dialog;
? dialog history: the string of dialog state
labels given by the Dialog Manager during
the previous turns of the same dialog
? utterance lengths: the utterance lengths
are estimated on the transcriptions (man-
ual or automatic) and represented by a set
of discrete symbols l0 for less than 5 words,
l1 between 5 and 10 words, l2 between 10
and 15 and l3 for more than 15 words);
The only feature that does not belong to the
system channel is the utterance lengths. This
feature is part of the user channel but it can
be estimated rather accurately from the word
graph produced during the speech recognition
process.
4.3 Results on the hierarchical
clustering
This experiment was made on the HMIHY cor-
pus. The training corpus used to grow the
clustering-tree comprises about 102k utterances
from live customer traffic. The test corpus was
made of 7k utterances.
After the clustering process we obtained the
6 clusters represented in table 1.
The size of each cluster is calculated accord-
ing to the number of words of all the utterances
belonging to it. This number is expressed as
a percentage of the total number of words of
the training corpus (column % words of table
1). One can notice that the cluster sizes are
not homogeneous. Indeed more than 70% of
the words of the training corpus are in the same
cluster. This result is not surprising: indeed the
open ended prompts like How may I help you ?
represent a very large chunk of all the possible
prompts and moreover most of the answers to
these prompts are quite long with more than 15
words. It is therefore very difficult to split a
chunk where all the utterances share the same
characteristics.
It is interesting to see the features considered
relevant in the tree splitting of the training cor-
pus. These 6 clusters contain the following type
of utterances:
? C1: answers to a prompt asking for a
phone number and containing between 10
and 15 words;
? C2: answers to the confirmation prompt
Are you phoning from your home phone ?
containing between 5 and 10 words;
? C3: answers to the same confirmation
prompt containing less than 5 words;
? C4: answers to other prompts and contain-
ing between 5 and 10 words;
? C5: answers to other prompts and contain-
ing between 10 and 15 words;
? C6: answers to other prompts and contain-
ing more than 15 words;
As we can see, 3 kinds of interaction are dis-
tinguished: request for a phone number, re-
quest for confirmation and other. These interac-
tions correspond to the different types of system
prompts defined in section 4.2.
It is interesting to notice that first it is al-
ways a specific prompt and not the prompt cat-
egory numeric, conf or other which is chosen by
the tree, and second that an utterance length is
systematically attached to each prompt. This
means that these prompts (which are very fre-
quent) have their own behaviors independently
from the other prompts part of the same prompt
category.
Utterance lengths are very strong and reli-
able indicators for characterizing an answer to a
given prompt. Cluster 1 contains mostly phone
numbers, cluster 2 contains confirmation an-
swers with explanation (mostly negative), and
cluster 3 contains confirmation answers without
explanation (mostly positive). We observe that
no dialog state label or dialog history label was
chosen as feature by the tree in the clustering
process. One possible explanation is the lim-
ited length of the dialogs in the HMIHY corpus
(4 turns on average). Therefore the dialog con-
text and history are negligible compared to the
system prompt alone.
Perplexity WER %
C % words 1-pass 2-pass 1-pass 2-pass
1 1.8 18.6 13.9 11.3 11.1
2 1.3 5.0 3.2 14.5 12.5
3 1.2 3.2 1.5 4.4 2.5
4 4.7 11 7.4 19.2 18
5 13.8 11.3 9.5 19.7 18.8
6 73.9 38.4 27.4 30.8 29.8
Table 1: Results for each cluster obtained with
the hierarchical clustering method, at the utter-
ance level, on the HMIHY corpus
By using these clusters for training specific
LMs and by dynamically choosing a specific
LM according to the dialog context for perform-
ing LM rescoring, we obtain the perplexity and
Word-Error-Rate (WER) results of table 1 on
the HMIHY test corpus. Significant perplex-
ity improvement can be seen for all the clus-
ters between the first and the second pass. On
the whole test corpus, the perplexity drops from
25.3 to 18.5, so a relative improvement of 26.8%.
On the speech recognition side, even if the de-
crease in WER is not as significant, we obtain
a gain for all of them.
4.4 K-means clustering
In order to split the utterances into clusters
more equally, we decided to use another cluster-
ing method based on a partitional k-means al-
gorithm. In these experiments we do not try to
explicitly optimize a specific criterion, like the
perplexity, but we just want to group together
utterances sharing common properties and put
utterances that are very dissimilar into differ-
ent clusters. Perplexity measures obtained with
LMs trained on such clusters will tell us if this
method splits the utterances according to the
language used. The first step in this process is
to encode the dialogs into vectors, one vector
for each dialog.
4.5 Representing utterances as feature
vectors
The decision-tree method used symbolic fea-
tures in order to generate the questions that
split the corpus. The k-means clustering
method is purely numerical therefore we need
here to encode dialog turns into numerical fea-
ture vectors. According to what we learned
from the previous experiments we decide to put
in the vectors only the semantic calltype labels.
Indeed as 70% of the utterances share the same
prompt and the same utterance length category,
it did not seem relevant to us to put these fea-
tures in the vectors as we are looking for clusters
that are quite balanced in size. The calltype la-
bels are relevant as we are interested here in
clusters that can be semantically different.
Therefore, a feature vector representing an
utterance is a first order statistic on the call-
types. A component value is the number of oc-
currences of the corresponding calltype within
the utterance. We kept the 34 most frequent
calltype labels in the training corpus in order
to build the vectors. They cover over 96% of
the calltype occurrences on the HMIHY train-
ing corpus.
4.6 Results on the partitional
clustering
This experiment is made on the same applica-
tion than the last one (HMIHY) but on another
data set. The training corpus contains 10k di-
alogs and 35.5k utterances and the test corpus
contains 1.4k dialogs and 5k utterances. The
number of clusters is set to 5. This value has
the advantages of both providing a good con-
vergence to the k-means process and splitting
rather equally the training corpus.
Table 2 illustrates the partitional clustering
of utterances on this corpus. As we can see,
the distribution of the total amount of words
of the corpus among the clusters is much more
even than the one obtained with the hierarchi-
cal clustering. The largest cluster contains only
38.5% of the words compared to 73.9% previ-
ously. To check if these clusters can be useful
for training specific LMs, we first split the test
corpus according to the clustering model esti-
mated on the training data. We use here, to
build the vectors encoding the test corpus, au-
tomatic calltypes calculated by the Spoken Lan-
guage Understanding Module. Then we com-
pare the perplexity measures obtained with a
general LM trained on the whole training cor-
pus and the one obtained with a specific LM
adapted on the corresponding cluster. Table 2
shows these results: the gain in perplexity ob-
tained is smaller than the one obtained with the
other method. Indeed the total perplexity on
the test corpus is 21.3 and the one obtained with
the specific LMs is 17.8, so a gain of 16% com-
pared to the 26.8% obtained previously. How-
ever this result is not surprising as the previous
method was designed for specifically decreasing
the perplexity measure.
Perplexity
C % words utt. length 1-pass 2-pass
1 8.6 5.5 16.9 12.6
2 20.3 17.8 25.5 21.0
3 14.5 11.6 21.6 18.2
4 38.5 7.5 19.6 17.2
5 18.1 8.6 22.8 18.6
Table 2: Results for each cluster obtained with
the partitional clustering method, at the utter-
ance level, on the HMIHY corpus
5 Clustering at the dialog level
As we have seen in the previous section, specific
dialog situations (like those obtained with the
hierarchical clustering) proved to be more effi-
cient than the semantic channel (represented by
the calltype labels) for clustering utterances in
relation with the language used, at least from
the perplexity point of view.
However, for clustering dialogs rather than
utterances, the semantic channel is the main
channel that we are going to use because in this
case we want to characterize the whole interac-
tion between a system and a user rather than
just the language used.
For example, an utterance like I want to pay
my bill can be used in very different dialog con-
texts: in a standard interaction if this request
is expressed just after the opening prompt or at
a different stage of the dialog. To capture this
kind of phenomena, we have to cluster at the
dialog level rather than the utterance level.
After describing how we encode dialogs into
feature vectors in the next section, we present
in section 5.2 some preliminary work on the in-
terpretation of the clusters obtained.
5.1 Representing dialogs as feature
vectors
We use here the partitional clustering method.
Each dialog is represented by a vector contain-
ing three kinds of information representing the
interaction:
1. the number of turns in the dialog: associ-
ated to other features this parameter can
be a strong indicator that the dialog is go-
ing fine (associated with a lot of confirma-
tions or item values) or that the interaction
is difficult (lot of repetitions for example).
2. first order statistics data on the calltype
labels: these are the 34 calltypes pre-
sented in section 2 and representing both
application-specific requests (Pay Bill) and
dialog-based concepts like Yes, No, I want
to talk to somebody, Help, etc. . . .
3. second order statistics data on the calltype
labels: we chose the bigrams of the previ-
ous calltypes that had the highest weighted
Mutual Information and we store in the
vectors their frequencies. These features al-
low us to observe certain patterns like rep-
etition of a request, request followed by a
confirmation, people asking twice for a rep-
resentative, etc. . . .
5.2 Analyzing the clusters obtained
The experiment reported here is made on the
HMIHY corpus. The vectors used contain 55
components: 1 for the number of turns, 34 for
the calltypes and 20 for the bigrams on the call-
types. The number of clusters to be found by
the k-means clustering algorithm was set to 5.
Firstly because as we want to give an interpreta-
tion to each cluster we need to keep a relatively
small number of them. Secondly because this
number leads to a fast convergence of the k-
mean clustering process. The clustering model
is obtained on the training corpus and applied
to the test corpus. Table 3 shows the distribu-
tion of the dialogs among the clusters, on the
training corpus, ranging from 35.4% for C1 to
7% for C2.
There are two ways of analyzing the clusters
obtained: from the language point of view and
from the semantic point of view. Table 3 illus-
trates the language channel features with the
average amount of turns (#turn), the average
utterance lengths (utt. length) and the per-
plexity measure (pplex). This perplexity mea-
sure is obtained with a 3-gram LM trained on
the whole training corpus and applied to the
manual transcriptions of each cluster, of the test
and the training corpus.
The differences in utterance lengths are not
as big as those observed in section 4.6. This is
an indicator that, unlike the clustering at the
utterance level, the clusters obtained represent
similar dialog situation. It is the way the dia-
log progresses, the dialog pattern, rather than
the theme of the dialog which distinguishes the
clusters. The lengths of the dialogs and the per-
plexity measures are indicators of these different
dialog patterns.
The results on the test corpus presented in
Table 3 are obtained with automatic calltypes,
completely unsupervised. The F-measure score
on the detection of these calltypes on the test
corpus is 75%. As one can see, having errors in
the calltypes detected does not affect too much
the characteristics of the clusters obtained.
Training corpus
C % dialogs #turn utt. length pplex
1 35.4% 3.9 7.3 7.32
2 7% 3.2 11.7 9.9
3 22.3% 3.5 9 7.3
4 10.4% 3.9 12.4 9.5
5 25% 2.9 9.3 7.9
Test corpus
C % dialogs #turn utt. length pplex
1 32.4% 4.0 8.5 17.2
2 7.4% 3.3 11 30.2
3 20.8% 3.5 7.5 13.2
4 12.0% 3.8 11.6 28.2
5 27.5% 2.8 8.3 17
Table 3: Language features attached to the
dialog clusters obtained with the partitional
method, at the dialog level, on the HMIHY cor-
pus
In order to analyze the clusters on the seman-
tic channel we plot the weighed Mutual Infor-
mation, wMI(ci; tj), between each cluster, ci,
and each vector component, tj . This measure is
estimated in the following way:
wMI(ci; tj) = P (ci; tj)log
P (ci; tj)
P (ci)P (tj)
This plot is shown on Figure 2 for the HMIHY
training corpus. By grouping together com-
ponents corresponding to a phenomenon we
want to observe we are able to characterize
more closely each cluster. In the color-coded
graph, x-axis corresponds to call-type unigrams
or selected call-type pairs, y-axis corresponds
to each cluster. The color of each rectangle
shows the degree of correlation, determined by
the weighted mutual information between call-
type and the cluster. As also can be seen from
the color spectrum on the right hand side, dark
red means high correlation (top), and dark blue
means reverse correlation (bottom).
?0.01
?0.008
?0.006
?0.004
?0.002
0
0.002
0.004
0.006
0.008
0.01
Call?types
Cl
us
te
rs
Weighted MI(Call?type;Cluster)
w
M
I(C
all
?ty
pe
;C
lus
ter
)
5 10 15 20 25 30 35 40 45 50
1
2
3
4
5
Figure 2: Weighted Mutual Information mea-
sures between and clusters on the HMIHY train-
ing corpus. The color of each rectangle shows
the degree of correlation, determined by the
weighted mutual information between the vec-
tor components and the cluster. As can be seen
from the color spectrum on the right, dark red
means high correlation, and dark blue means
reverse correlation.
We chose to analyze the clusters according to
3 dimensions:
1. Request = the kind of request expressed
by the user. We split all the request call-
types into two categories: the easy requests
that correspond to the calltypes well de-
tected by the SLU module and the difficult
requests that contain all the calltypes that
are often badly recognized.
2. Understanding = to try to character-
ize the understanding of a user by the
system we use two features: the bigrams
request + yes (conf) that can be indica-
tors that the request is understood because
the following concept expressed by the user
is yes; the bigrams request + request
(repeat) which indicate that the same re-
quest is repeated twice in a row, which can
indicate that the system misunderstood it.
3. Problems = we grouped in this category
the features that can be related to some
problems the user have during the interac-
tion. These features are: request for help
(help), two requests in a row for a rep-
resentative (rep) and a calltype indicat-
ing that no meaningful information is ex-
tracted from the user?s utterance (null).
Request Underst. Problems
C easy dif conf repeat help rep null
1 + ? + = ? ? ?
2 ? + ? + + ? +
3 + ? = = = + =
4 ? + + + + + +
5 ? + + + = = ?
Table 4: Interaction features attached to the
dialog clusters obtained with the partitional
method, at the dialog level, on the HMIHY cor-
pus
Table 4 represents the 5 clusters from figure 2,
ternary encoded according to these semantic di-
mensions. A ?+? means a higher value in the
wMI scale, ??? means a lower value in wMI scale
and ? =? stands for 0 in the wMI scale. Accord-
ing to the results presented in Tables 3 and 4,
the following description can be given to the 5
clusters:
? Cluster C1 : easy requests, expressed clearly
(low perplexity, short utterances), under-
stood correctly (conf=?+?), and with no
evidence of a problem.
? Cluster C2 : difficult requests, with
confusing formulation (high perplexity,
long utterances), often misunderstood
(repeat=?+?), leading users to have prob-
lems (request for help, no info).
? Cluster C3 : rather similar to C1 except
that people ask more often for a representa-
tive, maybe because the requests are not al-
ways recognized immediately (conf=?=?).
? Cluster C4 : difficult requests, with confus-
ing formulation (high perplexity, long ut-
terances), leading users to have problems
(request for help, no info), but unlike C2
eventually leading to a success (high num-
ber of turns and conf=?+?).
? Cluster C5 : difficult requests, expressed
clearly (low perplexity, rather short ut-
terances), understood correctly (conf=?+?)
even after some repetition because these
are difficult requests, and with no evidence
of a problem.
Figure 3 gives portions of two example dialogs
from clusters C1 and C2. As can be seen, in the
Example dialog from C1:
? System: How may I help you?
? User: I want to pay a bill.
? System Call-type: Make(Payment)
? System: Do you want to arrange a bill
payment?
? User: Yes.
? System Call-type: Yes
Example dialog from C2:
? System: How may I help you?
? User: Yes somebody just called me from
I don?t know it?s collect or something like
that and it had to do with A T and T and
when...
? System Call-type: Explain(Bill)
? System: Do you have a question about a
specific charge on your bill?
? User: No
? System Call-type: No
? System: I?m sorry . How may I help you?
? User: It just said call you guys for help
about trying to get through to a long dis-
tant number and it?s not getting through
when I called the long distant ...
? System Call-type: Other
Figure 3: Two example dialogs from our corpus.
first dialog, the user?s responses are very clear,
whereas in the second one they are very vague
and complicated.
6 Using clusters for Speech Mining
In the previous section we have interpreted the
clusters obtained with the respech to the dialog
contexts. Each cluster is identified by various
features computed, characterizing the language
and the interaction. These features can be seen
as a set of tools allowing the designer of a Spo-
ken Dialog System to monitor the behavior of
the system. Indeed, once a clustering model
has been build on a manually labeled training
corpus, this model can be applied in a fully un-
supervised way to non transcribed data and all
the features presented in Figure 2 and Tables 3
and 4 can be extracted. Even if mistakes in
the calltype detection occur, the general struc-
ture of the clusters is stable as shown on the
HMIHY test corpus in Table 3 and in Figure 4
that plot the same parameters for the test cor-
pus with automatic calltypes that Figure 2 has
for the training corpus with manual labels.
?0.01
?0.008
?0.006
?0.004
?0.002
0
0.002
0.004
0.006
0.008
0.01
Call?types
Cl
us
te
rs
Weighted MI(Call?type;Cluster)
w
M
I(C
all
?ty
pe
;C
lus
ter
)
5 10 15 20 25 30 35 40 45 50
1
2
3
4
5
Figure 4: Weighted Mutual Information mea-
sures between calltype labels and clusters on the
HMIHY test corpus with automatic calltype la-
bels
7 Conclusions
We presented in this paper an application
of data clustering methods to large Human-
Computer spoken dialog corpora. Different
ways for encoding dialogs into multidimensional
structures (symbolic and numerical) and dif-
ferent clustering methods have been proposed.
Preliminary results are given for cluster inter-
pretation and dynamic model adaptation using
the clusters obtained.
Acknowledgements
The authors would like to thank Driss Matrouf
for providing the k-means clustering tools used
in this study.
References
Frederic Bechet, Giuseppe Riccardi, and Dilek
Hakkani-Tur. 2003. Multi-channel sentence
classification for spoken dialogue language
modeling. In Proceedings of Eurospeech?03,
Geneve, Switzerland.
Yannick Esteve, Frederic Bechet, Alexis Nasr,
and Renato De Mori. 2001. Stochastic fi-
nite state automata language model triggered
by dialogue states. In Proceedings of Eu-
rospeech?01, volume 1, pages 725?728, Aal-
borg, Danemark.
A.L. Gorin, G. Riccardi, and J.H. Wright. 1997.
How May I Help You? Speech Communica-
tion, 23:113?127.
Helen Wright Hastie, Rashmi Prasad, and Mar-
ilyn A. Walker. 2002. What?s the trou-
ble: Automatically identifying problematic
dialogs in darpa communicator dialog sys-
tems. In Proceedings of the Association of
Computational Linguistics Meeting, Philadel-
phia, USA.
Candace Kamm, Marilyn A. Walker, and Diane
Litman. 1999. Evaluating spoken language
systems. In Proceedings of American Voice
Input/Output Society AVIOS.
R. Kuhn and R. De Mori. 1995. The applica-
tion of semantic classification trees to natu-
ral language understanding. IEEE Trans. on
Pattern Analysis and Machine Intelligence,
17(449-460).
Bridging the Gap: Academic and Industrial Research in Dialog Technologies Workshop Proceedings, pages 48?55,
NAACL-HLT, Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Experiments on the France Telecom 3000 Voice Agency corpus: academic
research on an industrial spoken dialog system?
Ge?raldine Damnati
France Te?le?com R&D
TECH/SSTP/RVA
2 av. Pierre Marzin
22307 Lannion Cedex 07, France
geraldine.damnati@orange-ftgroup.com
Fre?de?ric Be?chet Renato De Mori
LIA
University of Avignon
AGROPARC, 339 ch. des Meinajaries
84911 Avignon Cedex 09, France
frederic.bechet,renato.demori
@univ-avignon.fr
Abstract
The recent advances in speech recognition
technologies, and the experience acquired
in the development of WEB or Interac-
tive Voice Response interfaces, have facil-
itated the integration of speech modules
in robust Spoken Dialog Systems (SDS),
leading to the deployment on a large scale
of speech-enabled services. With these
services it is possible to obtain very large
corpora of human-machine interactions by
collecting system logs. This new kinds of
systems and dialogue corpora offer new
opportunities for academic research while
raising two issues: How can academic re-
search take profit of the system logs of
deployed SDS in order to build the next
generation of SDS, although the dialogues
collected have a dialogue flow constrained
by the previous SDS generation? On the
other side, what immediate benefits can
academic research offer for the improve-
ment of deployed system? This paper ad-
dresses these aspects in the framework of
the deployed France Telecom 3000 Voice
Agency service.
?This work is supported by the 6th Framework Research
Programme of the European Union (EU), Project LUNA,
IST contract no 33549. The authors would like to thank
the EU for the financial support. For more information
about the LUNA project, please visit the project home-page,
www.ist-luna.eu .
1 Introduction
Since the deployment on a very large scale of the
AT&T How May I Help You? (HMIHY) (Gorin et
al., 1997) service in 2000, Spoken Dialogue Sys-
tems (SDS) handling a very large number of calls are
now developed from an industrial point of view. Al-
though a lot of the remaining problems (robustness,
coverage, etc.) are still spoken language process-
ing research problems, the conception and the de-
ployment of such state-of-the-art systems mainly re-
quires knowledge in user interfaces.
The recent advances in speech recognition tech-
nologies, and the experience acquired in the devel-
opment of WEB or Interactive Voice Response inter-
faces have facilitated the integration of speech mod-
ules in robust SDS.
These new SDS can be deployed on a very large
scale, like the France Telecom 3000 Voice Agency
service considered in this study. With these services
it is possible to obtain very large corpora of human-
machine interactions by collecting system logs. The
main differences between these corpora and those
collected in the framework of evaluation programs
like the DARPA ATIS (Hemphill et al, 1990) or the
French Technolangue MEDIA (Bonneau-Maynard
et al, 2005) programs can be expressed through the
following dimensions:
? Size. There are virtually no limits in the
amount of speakers available or the time
needed for collecting the dialogues as thou-
sands of dialogues are automatically processed
every day and the system logs are stored.
Therefore Dialog processing becomes similar
48
to Broadcast News processing: the limit is not
in the amount of data available, but rather in the
amount of data that can be manually annotated.
? Speakers. Data are from real users. The speak-
ers are not professional ones or have no reward
for calling the system. Therefore their behav-
iors are not biased by the acquisition protocols.
Spontaneous speech and speech affects can be
observed.
? Complexity. The complexity of the services
widely deployed is necessarily limited in order
to guarantee robustness with a high automation
rate. Therefore the dialogues collected are of-
ten short dialogues.
? Semantic model. The semantic model of such
deployed system is task-oriented. The inter-
pretation of an utterance mostly consists in the
detection of application-specific entities. In an
application like the France Telecom 3000 Voice
Agency service this detection is performed by
hand-crafted specific knowledge.
The AT&T HMIHY corpus was the first large dia-
logue corpus, obtained from a deployed system, that
has the above mentioned characteristics. A service
like the France Telecom 3000 Voice Agency service
has been developed by a user interface development
lab. This new kind of systems and dialogue corpora
offer new opportunities for academic research that
can be summarized as follows:
? How can academic research take profit of the
system logs of deployed SDS in order to build
the next generation of SDS, although the di-
alogues collected have a dialogue flow con-
strained by the previous SDS generation?
? On the other side, what immediate benefits can
academic research offer for the improvement
of deployed system, while waiting for the next
SDS generation?
This paper addresses these aspects in the frame-
work of the deployed FT 3000 Voice Agency ser-
vice. Section 3 presents how the ASR process can
be modified in order to detect and reject Out-Of-
Domain utterances, leading to an improvement in
the understanding performance without modifying
the system. Section 4 shows how the FT 3000 cor-
pus can be used in order to build stochastic models
that are the basis of a new Spoken Language Un-
derstanding strategy, even if the current SLU system
used in the FT 3000 service is not stochastic. Sec-
tion 5 presents experimental results obtained on this
corpus justifying the need of a tighter integration be-
tween the ASR and the SLU models.
2 Description of the France Telecom 3000
Voice Agency corpus
The France Telecom 3000 (FT3000) Voice Agency
service, the first deployed vocal service at France
Telecom exploiting natural language technologies,
has been made available to the general public in Oc-
tober 2005. FT3000 service enables customers to
obtain information and purchase almost 30 differ-
ent services and access the management of their ser-
vices. The continuous speech recognition system re-
lies on a bigram language model. The interpretation
is achieved through the Verbateam two-steps seman-
tic analyzer. Verbateam includes a set of rules to
convert the sequence of words hypothesized by the
speech recognition engine into a sequence of con-
cepts and an inference process that outputs an inter-
pretation label from a sequence of concepts.
2.1 Specificities of interactions
Given the main functionalities of the application,
two types of dialogues can be distinguished. Some
users call FT 3000 to activate some services they
have already purchased. For such demands, users
are rerouted toward specific vocal services that are
dedicated to those particular tasks. In that case, the
FT3000 service can be seen as a unique automatic
frontal desk that efficiently redirects users. For such
dialogues the collected corpora only contain the in-
teraction prior to rerouting. It can be observed in that
case that users are rather familiar to the system and
are most of the time regular users. Hence, they are
more likely to use short utterances, sometimes just
keywords and the interaction is fast (between one or
two dialogue turns in order to be redirected to the
demanded specific service).
Such dialogues will be referred as transit dia-
logues and represent 80% of the calls to the FT3000
49
service. As for the 20% other dialogues, referred to
as other, the whole interaction is proceeded within
the FT3000 application. They concern users that are
more generally asking for information about a given
service or users that are willing to purchase a new
service. For these dialogues, the average utterance
length is higher, as well as the average number of
dialogue turns.
other transit
# dialogues 350 467
# utterances 1288 717
# words 4141 1454
av. dialogue length 3.7 1.5
av. utterance length 3.2 2.0
OOV rate (%) 3.6 1.9
disfluency rate (%) 2.8 2.1
Table 1: Statistics on the transit and other dialogues
As can be observed in table 1 the fact that users
are less familiar with the application in the other dia-
logues implies higher OOV rate and disfluency rate1.
An important issue when designing ASR and SLU
models for such applications that are dedicated to
the general public is to be able to handle both naive
users and familiar users. Models have to be robust
enough for new users to accept the service and in
the meantime they have to be efficient enough for
familiar users to keep on using it. This is the reason
why experimental results will be detailed on the two
corpora described in this section.
2.2 User behavior and OOD utterances
When dealing with real users corpora, one has to
take into account the occurrence of Out-Of-Domain
(OOD) utterances. Users that are familiar with a ser-
vice are likely to be efficient and to strictly answer
the system?s prompts. New users can have more di-
verse reactions and typically make more comments
about the system. By comments we refer to such
cases when a user can either be surprised what am
I supposed to say now?, irritated I?ve already said
that or even insulting the system. A critical aspect
for other dialogues is the higher rate of comments
uttered by users. For the transit dialogues this phe-
nomenon is much less frequent because users are fa-
1by disfluency we consider here false starts and filled pauses
miliar to the system and they know how to be effi-
cient and how to reach their goal. As shown in ta-
ble 2, 14.3% of the other dialogues contain at least
one OOD comment, representing an overall 10.6%
of utterances in these dialogues.
other transit
# dialogues 350 467
# utterances 1288 717
# OOD comments 137 24
OOD rate (%) 10.6 3.3
dialogues with OOD (%) 14.3 3.6
Table 2: Occurrence of Out-Of-Domain comments
on the transit and other dialogues
Some utterances are just comments and some con-
tain both useful information and comments. In the
next section, we propose to detect these OOD se-
quences and to take this phenomenon into account
in the global SLU strategy.
3 Handling Out-Of-Domain utterances
The general purpose of the proposed strategy is to
detect OOD utterances in a first step, before entering
the Spoken Language Understanding (SLU) mod-
ule. Indeed standard Language Models (LMs) ap-
plied to OOD utterances are likely to generate erro-
neous speech recognition outputs and more gener-
ally highly noisy word lattices from which it might
not be relevant and probably harmful to apply SLU
modules.
Furthermore, when designing a general interac-
tion model which aims at predicting dialogue states
as proposed in this paper, OOD utterances are as
harmful for state prediction as can be an out-of-
vocabulary word for the prediction of the next word
with an n-gram LM.
This is why we propose a new composite LM that
integrates two sub-LMs: one LM for transcribing in-
domain phrases, and one LM for detecting and delet-
ing OOD phrases. Finally the different SLU strate-
gies proposed in this paper are applied only to the
portions of signal labeled as in-domain utterances.
50
3.1 Composite Language Model for decoding
spontaneous speech
As a starting point, the comments have been manu-
ally annotated in the training data in order to easily
separate OOD comment segments from in-domain
ones. A specific bigram language model is trained
for these comment segments. The comment LM was
designed from a 765 words lexicon and trained on
1712 comment sequences.
This comment LM, called LMOOD has been in-
tegrated in the general bigram LMG. Comment
sequences have been parsed in the training corpus
and replaced by a OOD tag. This tag is added to
the general LM vocabulary and bigram probabilities
P ( OOD |w) and P (w| OOD ) are trained along
with other bigram probabilities (following the prin-
ciple of a priori word classes). During the decoding
process, the general bigram LM probabilities and the
LMOOD bigram probabilities are combined.
3.2 Decision strategy
Given this composite LM, a decision strategy is ap-
plied to select those utterances for which the word
lattice will be processed by the SLU component.
This decision is made upon the one-best speech
recognition hypotheses and can be described as fol-
lows:
1. If the one-best ASR output is a single OOD
tag, the utterance is simply rejected.
2. Else, if the one-best ASR output contains an
OOD tag along with other words, those words
are processed directly by the SLU component,
following the argument that the word lattice for
this utterance is likely to contain noisy infor-
mation.
3. Else (i.e. no OOD tag in the one-best ASR
output), the word-lattice is transmitted to fur-
ther SLU components.
It will be shown in the experimental section that
this pre-filtering step, in order to decide whether a
word lattice is worth being processed by the higher-
level SLU components, is an efficient way of pre-
venting concepts and interpretation hypothesis to be
decoded from an uninformative utterance.
3.3 Experimental setup and evaluation
The models presented are trained on a corpus col-
lected thanks to the FT3000 service. It contains real
dialogues from the deployed service. The results
presented are obtained on the test corpus described
in section 2.
The results were evaluated according to 3 crite-
ria: the Word Error Rate (WER), the Concept Error
Rate (CER) and the Interpretation Error Rate (IER).
The CER is related to the correct translation of an
utterance into a string of basic concepts. The IER is
related to the global interpretation of an utterance
in the context of the dialogue service considered.
Therefore this last measure is the most significant
one as it is directly linked to the performance of the
dialogue system.
IER all other transit
size 2005 717 1288
LMG 16.5 22.3 13.0
LMG + OOD 15.0 18.6 12.8
Table 3: Interpretation error rate according to the
Language Model
Table 3 presents the IER results obtained with the
strategy strat1 with 2 different LMs for obtaining
W? : LMG which is the general word bigram model;
and LMG + OOD which is the LM with the OOD com-
ment model. As one can see, a very significant im-
provement, 3.7% absolute, is achieved on the other
dialogues, which are the ones containing most of
the comments. For the transit dialogues a small im-
provement (0.2%) is also obtained.
4 Building stochastic SLU strategies
4.1 The FT3000 SLU module
The SLU component of the FT3000 service consid-
ered in this study contains two stages:
1. the first one translates a string of words W =
w1, . . . , wn into a string of elementary con-
cepts C = c1, . . . , cl by means of hand-written
regular grammars;
2. the second stage is made of a set of about 1600
inference rules that take as input a string of con-
cepts C and output a global interpretation ? of
51
a message. These rules are ordered and the
first match obtained by processing the concept
string is kept as the output interpretation.
These message interpretations are expressed by an
attribute/value pair representing a function in the vo-
cal service.
The models used in these two stages are manually
defined by the service designers and are not stochas-
tic. We are going now to present how we can use a
corpus obtained with such models in order to define
an SLU strategy based on stochastic processes.
4.2 Semantic knowledge representation
The actual FT3000 system includes semantic knowl-
edge represented by hand-written rules. These rules
can also be expressed in a logic form. For this rea-
son, some basic concepts are now described with the
purpose of showing how logic knowledge has been
integrated in a first probabilistic model and how it
can be used in a future version in which optimal poli-
cies can be applied.
The semantic knowledge of an application is a
knowledge base (KB) containing a set of logic for-
mulas. Formulas return truth and are constructed
using constants which represent objects and may be
typed, variables, functions which are mappings from
tuples of objects to objects and predicates which
represent relations among objects. An interpretation
specifies which objects, functions and relations in
the domain are represented by which symbol. Basic
inference problem is to determine whether KB |= F
which means that KB entails a formula F .
In SLU, interpretations are carried on by binding
variables and instantiating objects based on ASR re-
sults and inferences performed in the KB. Hypothe-
ses about functions and instantiated objects are writ-
ten into a Short Term Memory (STM).
A user goal is represented by a conjunction of
predicates. As dialogue progresses, some predi-
cates are grounded by the detection of predicate tags,
property tags and values. Such a detection is made
by the interpretation component. Other predicates
are grounded as a result of inference. A user goal G
is asserted when all the atoms of its conjunction are
grounded and asserted true.
Grouping the predicates whose conjunction is the
premise for asserting a goal Gi is a process that goes
through a sequence of states: S1(Gi), S2(Gi), . . .
Let ?ik be the content of the STM used for as-
serting the predicates grounded at the k-th turn of a
dialogue. These predicates are part of the premise
for asserting the i-th goal.
Let Gi be an instance of the i-th goal asserted after
grounding all the predicates in the premise.
?ik can be represented by a composition from a
partial hypothesis ?ik? 1 available at turn k ? 1, the
machine action ak?1 performed at turn k ? 1 and
the semantic interpretation ?ik i.e.:
?ik = ?
(
?ik, ak?1,?ik?1
)
Sk(Gi) is an information state that can lead to a
user?s goal Gi and ?ik is part of the premise for as-
serting Gi at turn k.
State probability can be written as follows:
P (Sk(Gi)|Yk) = P
(
Gi|?ik
)
P
(
?ik|Yk
) (1)
where P
(
Gi|?ik
)
is the probability that Gi is the
type of goal that corresponds to the user interac-
tion given the grounding predicates in ?ik. Yk is the
acoustic features of the user?s utterance at turn k.
Probabilities of states can be used to define a be-
lief of the dialogue system.
A first model allowing multiple dialog state se-
quence hypothesis is proposed in (Damnati et al,
2007). In this model each dialog state correspond
to a system state in the dialog automaton. In order
to deal with flexible dialog strategies and following
previous work (Williams and Young, 2007), a new
model based on a Partially Observable Markov De-
cision Process (POMDP) is currently studied.
If no dialog history is taken into account,
P
(
?ik|Y
)
comes down to P
(
?ik|Y
)
, ?ik being a
semantic attribute/value pair produced by the Ver-
bateam interpretation rules.
The integration of this semantic decoding process
in the ASR process is presented in the next section.
5 Optimizing the ASR and SLU processes
With the stochastic models proposed in section 4,
different strategies can be built and optimized. We
are interested here in the integration of the ASR and
SLU processes. As already shown by previous stud-
ies (Wang et al, 2005), the traditional sequential ap-
proach that first looks for the best sequence of words
52
W? before looking for the best interpretation ?? of an
utterance is sub-optimal. Performing SLU on a word
lattice output by the ASR module is an efficient way
of integrating the search for the best sequence of
words and the best interpretation. However there are
real-time issues in processing word lattices in SDS,
and therefore they are mainly used in research sys-
tems rather than deployed systems.
In section 3 a strategy is proposed for selecting
the utterances for which a word lattice is going to be
produced. We are going now to evaluate the gain in
performance that can be obtained thanks to an inte-
grated approach on these selected utterances.
5.1 Sequential vs. integrated strategies
Two strategies are going to be evaluated. The first
one (strat1) is fully sequential: the best sequence of
word W? is first obtained with
W? = argmax
W
P (W |Y )
Then the best sequence of concepts C? is obtained
with
C? = argmax
C
P (C|W? )
Finally the interpretation rules are applied to C? in
order to obtain the best interpretation ??.
The second strategy (strat2) is fully integrated: ??
is obtained by searching at the same time for W? and
C? and ??. In this case we have:
?? = argmax
W,C,?
P (?|C)P (C|W )P (W |Y )
The stochastic models proposed are implemented
with a Finite State Machine (FSM) paradigm thanks
to the AT&T FSM toolkit (Mohri et al, 2002).
Following the approach described in (Raymond
et al, 2006), the SLU first stage is implemented by
means of a word-to-concept transducer that trans-
lates a word lattice into a concept lattice. This con-
cept lattice is rescored with a Language Model on
the concepts (also encoded as FSMs with the AT&T
GRM toolkit (Allauzen et al, 2003)).
The rule database of the SLU second stage is en-
coded as a transducer that takes as input concepts
and output semantic interpretations ?. By applying
this transducer to an FSM representing a concept lat-
tice, we directly obtain a lattice of interpretations.
The SLU process is therefore made of the com-
position of the ASR word lattice, two transducers
(word-to-concepts and concept-to-interpretations)
and an FSM representing a Language Model on the
concepts. The concept LM is trained on the FT3000
corpus.
This strategy push forward the approach devel-
opped at AT&T in the How May I Help You? (Gorin
et al, 1997) project by using richer semantic mod-
els than call-types and named-entities models. More
precisely, the 1600 Verbateam interpretation rules
used in this study constitute a rich knowledge base.
By integrating them into the search, thanks to the
FSM paradigm, we can jointly optimize the search
for the best sequence of words, basic concepts, and
full semantic interpretations.
For the strategy strat1 only the best path is kept in
the FSM corresponding to the word lattice, simulat-
ing a sequential approach. For strat2 the best inter-
pretation ?? is obtained on the whole concept lattice.
error WER CER IER
strat1 40.1 24.4 15.0
strat2 38.2 22.5 14.5
Table 4: Word Error Rate (WER), Concept Error
Rate (CER) and Interpretation Error Rate (IER) ac-
cording to the SLU strategy
The comparison among the two strategies is given
in table 4. As we can see a small improvement is ob-
tained for the interpretation error rate (IER) with the
integrated strategy (strat2). This gain is small; how-
ever it is interesting to look at the Oracle IER that
can be obtained on an n-best list of interpretations
produced by each strategy (the Oracle IER being the
lowest IER that can be obtained on an n-best list of
hypotheses with a perfect Oracle decision process).
This comparison is given in Figure 1. As one can
see a much lower Oracle IER can be achieved with
strat2. For example, with an n-best list of 5 interpre-
tations, the lowest IER is 7.4 for strat1 and only 4.8
for strat2. This is very interesting for dialogue sys-
tems as the Dialog Manager can use dialogue con-
text information in order to filter such n-best lists.
53
 4
 5
 6
 7
 8
 9
 10
 1  2  3  4  5  6  7  8  9  10
O
ra
cl
e 
IE
R
size of the n-best list of interpretations
sequential search (strat1)
integrated search (strat2)
Figure 1: Oracle IER according to an n-best list of interpretations for strategies strat1 and strat2
5.2 Optimizing WER, CER and IER
Table 4 also indicates that the improvements ob-
tained on the WER and CER dimensions don?t al-
ways lead to similar improvements in IER. This is
due to the fact that the improvements in WER and
CER are mostly due to a significant reduction in the
insertion rates of words and concepts. Because the
same weight is usually given to all kinds of errors
(insertions, substitutions and deletions), a decrease
in the overall error rate can be misleading as inter-
pretation strategies can deal more easily with inser-
tions than deletions or substitutions. Therefore the
reduction of the overall WER and CER measures is
not a reliable indicator of an increase of performance
of the whole SLU module.
level 1-best Oracle hyp.
WER 33.7 20.0
CER 21.2 9.7
IER 13.0 4.4
Table 5: Error rates on words, concepts and interpre-
tations for the 1-best hypothesis and for the Oracle
hypothesis of each level
These results have already been shown for WER
by previous studies like (Riccardi and Gorin, 1998)
IER
from word Oracle 9.8
from concept Oracle 7.5
interpretation Oracle 4.4
Table 6: IER obtained on Oracle hypotheses com-
puted at different levels.
or more recently (Wang et al, 2003). They are il-
lustrated by Table 5 and Table 6. The figures shown
in these tables were computed on the subset of utter-
ances that were passed to the SLU component. Ut-
terances for which an OOD has been detected are
discarded. In Table 5 are displayed the error rates
obtained on words, concepts and interpretations both
on the 1-best hypothesis and on the Oracle hypothe-
sis (the one with the lowest error rate in the lattice).
These Oracle error rates were obtained by looking
for the best hypothesis in the lattice obtained at the
corresponding level (e.g. looking for the best se-
quence of concepts in the concept lattice). As for Ta-
ble 6, the mentioned IER are the one obtained when
applying SLU to the Oracles hypotheses computed
for each level. As one can see the lowest IER (4.4)
is not obtained on the hypotheses with the lowest
WER (9.8) or CER (7.5).
54
6 Conclusion
This paper presents a study on the FT3000 corpus
collected from real users on a deployed general pub-
lic application. Two problematics are addressed:
How can such a corpus be helpful to carry on re-
search on advanced SLU methods eventhough it has
been collected from a more simple rule-based dia-
logue system? How can academic research trans-
late into short-term improvements for deployed ser-
vices? This paper proposes a strategy for integrating
advanced SLU components in deployed services.
This strategy consists in selecting the utterances for
which the advanced SLU components are going to
be applied. Section 3 presents such a strategy that
consists in filtering Out-Of-Domain utterances dur-
ing the ASR first pass, leading to significant im-
provement in the understanding performance.
For the SLU process applied to in-domain utter-
ances, an integrated approach is proposed that looks
simultaneously for the best sequence of words, con-
cepts and interpretations from the ASR word lat-
tices. Experiments presented in section 5 on real
data show the advantage of the integrated approach
towards the sequential approach. Finally, section 4
proposes a unified framework that enables to define
a dialogue state prediction model that can be applied
and trained on a corpus collected through an already
deployed service.
References
Cyril Allauzen, Mehryar Mohri, and Brian Roark. 2003.
Generalized algorithms for constructing statistical lan-
guage models. In 41st Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL?03), Sap-
poro, Japan.
Helene Bonneau-Maynard, Sophie Rosset, Christelle Ay-
ache, Anne Kuhn, and Djamel Mostefa. 2005. Se-
mantic annotation of the french media dialog corpus.
In Proceedings of the European Conference on Speech
Communication and Technology (Eurospeech), Lis-
boa, Portugal.
Geraldine Damnati, Frederic Bechet, and Renato
De Mori. 2007. Spoken Language Understanding
strategies on the France Telecom 3000 voice agency
corpus. In Proceedings of the International Con-
ference on Acoustics, Speech and Signal Processing
(ICASSP), Honolulu, USA.
A. L. Gorin, G. Riccardi, and J.H. Wright. 1997. How
May I Help You ? In Speech Communication, vol-
ume 23, pages 113?127.
Charles T. Hemphill, John J. Godfrey, and George R.
Doddington. 1990. The ATIS spoken language sys-
tems pilot corpus. In Proceedings of the workshop on
Speech and Natural Language, pages 96?101, Hidden
Valley, Pennsylvania.
Mehryar Mohri, Fernando Pereira, and Michael Ri-
ley. 2002. Weighted finite-state transducers in
speech recognition. Computer, Speech and Language,
16(1):69?88.
Christian Raymond, Frederic Bechet, Renato De Mori,
and Geraldine Damnati. 2006. On the use of finite
state transducers for semantic interpretation. Speech
Communication, 48,3-4:288?304.
Giuseppe Riccardi and Allen L. Gorin. 1998. Language
models for speech recognition and understanding. In
Proceedings of the International Conference on Spo-
ken Langage Processing (ICSLP), Sidney, Australia.
Ye-Yi Wang, A. Acero, and C. Chelba. 2003. Is word
error rate a good indicator for spoken language under-
standing accuracy? In Automatic Speech Recognition
and Understanding workshop - ASRU?03, St. Thomas,
US-Virgin Islands.
Ye-Yi Wang, Li Deng, and Alex Acero. 2005. Spoken
language understanding. In Signal Processing Maga-
zine, IEEE, volume 22, pages 16?31.
Jason D. Williams and Steve Young. 2007. Partially ob-
servable markov decision processes for spoken dialog
systems. Computer, Speech and Language, 21:393?
422.
55
On the use of confidence for statistical decision in dialogue strategies
Christian Raymond1 Fre?de?ric Be?chet1 Renato de Mori1 Ge?raldine Damnati2
1 LIA/CNRS, University of Avignon France 2 France Telecom R&D, Lannion, France
christian.raymond,frederic.bechet,renato.demori@lia.univ-avignon.fr
geraldine.damnati@rd.francetelecom.com
Abstract
This paper describes an interpretation and deci-
sion strategy that minimizes interpretation er-
rors and perform dialogue actions which may
not depend on the hypothesized concepts only,
but also on confidence of what has been rec-
ognized. The concepts introduced here are ap-
plied in a system which integrates language
and interpretation models into Stochastic Finite
State Transducers (SFST). Furthermore, acous-
tic, linguistic and semantic confidence mea-
sures on the hypothesized word sequences are
made available to the dialogue strategy. By
evaluating predicates related to these confi-
dence measures, a decision tree automatically
learn a decision strategy for rescoring a n-best
list of candidates representing a user?s utter-
ance. The different actions that can be then per-
formed are chosen according to the confidence
scores given by the tree.
1 Introduction
There is a wide consensus in the scientific community
that human-computer dialogue systems based on spoken
natural language make mistakes because the Automatic
Speech Recognition (ASR) component may not hypothe-
size some of the pronounced words and the various levels
of knowledge used for recognizing and reasoning about
conceptual entities are imprecise and incomplete. In spite
of these problems, it is possible to make useful applica-
tions with dialogue systems using spoken input if suit-
able interpretation and decision strategies are conceived
that minimize interpretation errors and perform dialogue
actions which may not depend on the hypothesized con-
cepts only, but also on confidence of what has been rec-
ognized.
This paper introduces some concepts developed for
telephone applications in the framework of stochastic
models for interpretation and dialogue strategies, a good
overview of which can be found in (Young, 2002).
The concepts introduced here are applied in a system
which integrates language and interpretation models into
Stochastic Finite State Transducers (SFST). Furthermore,
acoustic, linguistic and semantic confidence measures on
the hypothesized word sequences are made available to
the dialogue strategy. A new way of using them in the
dialogue decision process is proposed in this paper.
Most of the Spoken language Understanding Systems
(SLU) use semantic grammars with semantic tags as non-
terminals (He and Young, 2003) with rules for rewriting
them into strings of words.
The SFSTs of the system used for the experiments de-
scribed here, represent knowledge for the basic building
blocks of a frame-based semantic grammar. Each block
represents a property/value relation. Different SFSTs
may share words in the same sentence. Property/value
hypotheses are generated with an approach described in
(Raymond et al, 2003) and are combined into a sentence
interpretation hypothesis in which the same word may
contribute to more than one property/value pair. The di-
alogue strategy has to evaluate the probability that each
component of each pair has been correctly hypothesized
in order to decide to perform an action that minimizes the
risk of user dissatisfaction.
2 Overview of the decoding process
The starting point for decoding is a lattice of word
hypotheses generated with an n-gram language model
(LM). Decoding is a search process which detects com-
binations of specialized SFSTs and the n-gram LM. The
output of the decoding process consists of a n-best list
of conceptual interpretations ?. An interpretation ? is
a set of property/value pairs sj = (cj, vj) called con-
cepts. cj is the concept tag and vj is the concept value
of sj . Each concept tag cj is represented by a SFST and
can be related either to the dialogue application (phone
number, date, location expression, etc.) or to the dia-
logue management (confirmation, contestation, etc.). To
each string of words recognized by a given SFST cj is
associated a value vj representing a normalized value
for the concept detected. For example, to the word
phrase: on July the fourteenth, detected by a
SFST dedicated to process dates, is associated the value:
????/07/14.
The n-best list of interpretations output by the decod-
ing process is structured according to the different con-
cept tag strings that can be found in the word lattice. To
each concept tag string is attached another n-best list on
the concept values. This whole n-best is called a struc-
tured n-best. After presenting the statistical model used
in this study, we will describe the implementation of this
decoding process.
3 Statistical model
The contribution of a sequence of words W to a concep-
tual structure ? is evaluated by the posterior probability
P (? | Y ), where Y is the description of acoustic features.
Such a probability is computed as follows:
P (? | Y ) =
?
W?SW P (Y | W )P (? | W )
?P (W )?
?
W?SW P (Y | W )P (W )?
(1)
where P (Y | W ) is provided by the acoustic models,
P (W ) is computed with the LM. Exponents ? and ? are
respectively a semantic and a syntactic fudge factor. SW
corresponds to the set of word strings that can be found in
the word lattice. P (? | W ) is computed by considering
that thus:
P (? | W ) = P (s1 | W ).
J
?
j=2
P (sj | sj?11 W ) (2)
P (sj | sj?11 W ) ? P (sj | W )
If the conceptual component sj is hypothesized with
a sentence pattern pij(W ) recognized in W and pik(W )
triggers a pair sk and there is a training set with which
the probabilities P (pik(W ) | sk) ?k, can be estimated,
then the posterior probability can be obtained as follows:
P (sj | W ) =
P (pij(W ) | sj)P (sj)
?K
k=1 P (pik(W ) | sk)P (sk)
(3)
where P (sk) is a unigram probability of conceptual
components.
4 Structured N-best list
N-best lists are generally produced by simply enumerat-
ing the n best paths in the word graphs produced by Au-
tomatic Speech Recognition (ASR) engines. The scores
used in such graphs are usually only a combination of
acoustic and language model scores, and no other linguis-
tic levels are involved. When an n-best word hypothesis
list is generated, the differences between the hypothesis
i and the hypothesis i+1 are often very small, made of
only one or a few words. This phenomenon is aggravated
when the ASR word graph contains a low confidence
area, due for example to an Out-Of-Vocabulary word, to
a noisy input or to a speech disfluency.
This is the main weakness of this approach in a Spoken
Dialogue context: not all words are important to the Di-
alogue Manager, and all the n-best word hypotheses that
differ only between each other because of some speech
disfluency effects can be considered as equals. That?s
why it is important to generate not only a n-best list of
word hypotheses but rather a n-best list of interpretations,
each of them corresponding to a different meaning from
the Dialogue Manager point of view.
We propose here a method for directly extracting such
a structured n-best from a word lattice output by an ASR
engine. This method relies on operations between Finite
State Machines and is implemented thanks to the AT&T
FSM toolkit (see (Mohri et al, 2002) for more details).
4.1 Word-to-Concept transducer
Each concept ck of the dialogue application is associated
with an FSM. These FMSs are called acceptors (Ak for
the concept ck). In order to process strings of words that
don?t belong to any concept, a filler model, called AF
is used. Because the same string of words can?t belong
to both a concept model and the background text, all the
paths contained in the acceptors Ak are removed from the
filler model AF in the following way:
AF = ? ? ?
m
?
k=1
Ak
where ? is the word lexicon of the application and m
is the number of concepts used.
All these acceptors are now turned into transducers
that take words as input symbols and start or end con-
cept tags as output symbols. Indeed, all acceptors Ak be-
come transducers Tk where the first transition emits the
symbol <Ck> and the last transition the symbol </Ck>.
Similarly the filler model becomes the transducer Tbk
which emits the symbols <BCK> and </BCK>. Except
these start and end tags, no other symbols are emitted: all
words in the concept or background transducers emit an
empty symbol.
Finally all these transducers are linked together in a
single model called Tconcept as presented in figure 1.
FILLER
out=<BCK>
in=<start>
out=<>
in=<> in=<>
out=</BCK>
in=<end>
out=<>
out=<>
in=<start>
out=<>
in=w1
in=w2
out=<>
in=<end>
out=<>
in=<>
out=</C1>out=<C1>
in=<>
out=<C2>
in=<>
out=</C2>
in=<>
in=<>
out=<Cn>
in=<>
out=</Cn>
in=<>
out=<>
in=<>
out=<>in=<>
out=<>
SFST C1
SFST Cn
SFST C2
Figure 1: Word-to-Concept Transducer
4.2 Processing the ASR word lattice
The ASR word lattice is coded by an FSM: an acceptor L
where each transition emits a word. The cost function for
a transition corresponds to the acoustic score of the word
emitted.
The first step in the word lattice processing consists
of rescoring each transition of L by means of a 3-gram
Language Model (LM) in order to obtain the probabili-
ties P (W ) of equation 1. This is done by composing the
word lattice with a 3-gram LM also coded as an FSM (see
(Allauzen et al, 2003) for more details about statistical
LMs and FSMs).
The resulting FSM is then composed with the trans-
ducer TConcept in order to obtain the word-to-concept
transducer L?. A path in L? corresponds to a word string
if only the input symbols of the transducer are considered
and its score is the one expressed by equation 1; simi-
larly by considering only the output symbols, a path in L?
corresponds to a concept tag string.
The structured n-best list is directly obtained from L?:
by extracting the n-best concept tag strings (output label
paths) we obtain an n-best list on the conceptual interpre-
tations. The score of each conceptual interpretation is the
sum of all the word strings (input label paths) in the word
lattice producing the same interpretation.
Finally, for every conceptual interpretations C kept at
the previous step, a local n-best list on the word strings is
calculated by selecting in L? the best paths outputting the
string C .
The resulting structured n-best is illustrated by the fol-
lowing example. If we keep the 2 best conceptual in-
terpretations C1, C2 of a transducer L? and, for each of
these, the 2 best word strings, we obtain:
1 : C1 = <c1_1,c1_2,..,c1_x>
1.1 : W1.1 = <v1.1_1,v1.1_2,..,v1.1_x>
1.2 : W1.2 = <v1.2_1,v1.2_2,..,v1.2_x>
2 : C2 = <c2_1,c2_2,..,c2_y>
2.1 : W2.1 = <v2.1_1,v2.1_2,..,v2.1_y>
2.2 : W2.2 = <v2.2_1,v2.2_2,..,v2.2_y>
where <ci_1,ci_2,..,ci_y> is the conceptual
interpretation at the rank i in the n-best list; Wi.j is the
word string ranked j of interpretation i; and vi.j_k
is the concept value of the kth concept ci_k of the jth
word string of interpretation i.
5 Use of correctness probabilities
In order to select a particular interpretation ? (concep-
tual interpretation + concept values) from the structured
n-best list, we are now interested in computing the proba-
bility that ? is correct, given a set of confidence measures
M : P (? | M ). The choice of the confidence measures
determines the quality of the decision strategy. Those
used in this study are briefly presented in the next sec-
tions.
5.1 Confidence measures
5.1.1 Acoustic confidence measure (AC)
This confidence measure relies on the comparison of
the acoustic likelihood provided by the speech recogni-
tion model for a given hypothesis to the one that would
be provided by a totally unconstrained phoneme loop
model. In order to be consistent with the general model,
the acoustic units are kept identical and the loop is over
context dependent phonemes. This confidence measure
is used at the utterance level and at the concept level (see
(Raymond et al, 2003) for more details).
5.1.2 Linguistic confidence measure (LC)
In order to assess the impact of the absence of ob-
served trigrams as a potential cause of recognition errors,
a Language Model consistency measure is introduced.
This measure is simply, for a given word string candi-
date, the ratio between the number of trigrams observed
in the training corpus of the Language Model vs. the total
number of trigrams in the same word string. Its computa-
tion is very fast and the confidence scores obtained from
it give interesting results as presented in (Este`ve et al,
2003).
5.1.3 Semantic confidence measure (SC)
Several studies have shown that text classification tools
(like Support Vector Machines or Boosting algorithms)
can be an efficient way of labeling an utterance transcrip-
tion with a semantic label such as a call-type (Haffner et
al., 2003) in a Spoken Dialogue context. In our case, the
semantic labels attached to an utterance are the different
concepts handled by the Dialogue Manager. One classi-
fier is trained for each concept tag in the following way:
Each utterance of a training corpus is labeled with a
tag, manually checked, indicating if a given concept oc-
curs or not in the utterance. In order to let the classi-
fier model the context of occurrence of a concept rather
than its value we removed most of the concept headwords
from the list of criterion used by the classifier.
During the decision process, if the interpretation eval-
uated contains 2 concepts c1 and c2, then the classifiers
corresponding to c1 and c2 are used to give to the utter-
ance a confidence score of containing these two concepts.
The text classifier used in the experimental section
is a decision-tree classifier based on the Semantic-
Classification-Trees introduced for the ATIS task
by (Kuhn and Mori, 1995) and used for semantic disam-
biguation in (Be?chet et al, 2000).
5.1.4 Rank confidence measure (R)
To the previous confidence measures we added the
rank of each candidate in its n-best. This rank contains
two numbers: the rank of the interpretation of the utter-
ance and the rank of the utterance among those having
the same interpretation.
5.2 Decision Tree based strategy
As the dependencies of these measures are difficult to es-
tablish, their values are transformed into symbols by vec-
tor quantization (VQ) and conjunctions of these symbols
expressing relevant statistical dependencies are obtained
by a decision tree which is trained with a development
set of examples. At the leaves probabilities P (M |?) are
obtained when ? represents any correct hypothesis, the
case in which only the properties have been correctly rec-
ognized or both properties and values have errors. With
these probabilities we are now able to estimate P (? | M )
in the following way:
P (? | M) = 1
1 + P (M |??)P (??)P (M |?)P (?)
(4)
where ?? indicates that the interpretation in question is
incorrect and P (M |??) = 1 ? P (M |?).
6 From hypotheses to actions
Once concepts have been hypothesized, a dialog system
has to decide what action to perform. Let A = aj be
the set of actions a system can perform. Some of them
can be requests for clarification or repetition. In partic-
ular, the system may request the repetition of the entire
utterance. Performing an action has a certain risk and the
decision about the action to perform has to be the one that
minimizes the risk of user dissatisfaction.
It is thus possible that some or all the hypothesized
components of a conceptual structure ? do not corre-
spond to the user intention because the word sequence
W based on which the conceptual hypothesis has been
generated contains some errors. In particular, there are
requests for clarification or repetition which should be
performed right after the interpretation of an utterance in
order to reduce the stress of the user. It is important to
notice that actions consisting in requests for clarification
or repetition mostly depend on the probability that the in-
terpretation of an utterance is correct, rather than on the
utterance interpretation.
The decoding process described in section 2 provides
a number of hypotheses containing a variable number of
pairs sj = (cj, vj) based on the score expressed by equa-
tion 1.
P (? | M ) is then computed for these hypotheses. The
results can be used to decide to accept an interpretation
or to formulate a clarification question which may imply
more hypotheses.
For simplification purpose, we are going to consider
here only two actions: accepting the hypothesis with the
higher P (? | M ) or rejecting it. The risk associated to the
acceptation decision is called ?fa and corresponds to the
cost of a false acceptation of an incorrect interpretation.
Similarly the risk associated to the rejection decision is
called ?fr and corresponds to the cost of a false rejection
of a correct interpretation. In a spoken dialogue context,
?fa is supposed to be higher than ?fr .
The choice of the action to perform is determined by
a threshold ? on P (? | M ). This threshold is tuned on
a development corpus by minimizing the total risk R ex-
pressed as follows:
R = ?fa ?
Nfa
Ntotal
+ ?fr ?
Nfr
Ntotal
(5)
Nfa and Nfr are the numbers of false acceptation and
false rejection decisions on the development corpus for a
given value of ?. Ntotal is the total number of examples
available for tuning the strategy.
The final goal of the strategy is to make negligible Nfa
and the best set of confidence measures is the one that
minimizes Nfr . In fact, the cost of these cases is lower
because the corresponding action has to be a request for
repetition.
Instead of simply discarding an utterance if P (? | M )
is below ?, another strategy we are investigating consists
of estimating the probability that the conceptual interpre-
tation alone (without the concept values) is correct. This
probability can be estimated the same way as P (? | M )
and can be used to choose a third kind of actions: accept-
ing the conceptual meaning of an utterance but asking for
clarifications about the values of the concepts.
A final decision about the strategy to be adopted should
be based on statistics on system performance to be col-
lected and updated after deploying the system on the tele-
phone network.
7 Experiments
7.1 Application domain
The application domain considered in this study is a
restaurant booking application developed at France Tele-
com R&D. At the moment, we only consider in our strat-
egy the most frequent concepts related to the application
domain: PLACE, PRICE and FOOD TYPE. They can be
described as follows:
? PLACE: an expression related to a restaurant loca-
tion (eg. a restaurant near Bastille);
? PRICE: the price range of a restaurant (eg. less than
a hundred euros);
? FOOD TYPE: the kind of food requested by the
caller (eg. an Indian restaurant).
These entities are expressed in the training corpus by
short sequences of words containing three kinds of to-
ken: head-words like Bastille, concept related words like
restaurant and modifier tokens like near.
A single value is associated to each concept entity
simply be adding together the head-words and some
modifier tokens. For example, the values associated to
the three contexts presented above are: Bastille ,
less+hundred+euros and indian.
In the results section a concept detected is considered a
success only if the tag exists in the reference corpus and if
both values are identical. It?s a binary decision process:
a concept can be considered as a false detection even if
the concept tag is correct and if the value is partially cor-
rect. The measure on the errors (insertion, substitution,
deletion) of these concept/value tokens is called in this
paper the Understanding Error Rate, by opposition to the
standard Word Error Rate measure where all words are
considered equals.
7.2 Experimental setup
Experiments were carried out on a dialogue corpus pro-
vided by France Telecom R&D. The task has a vocabu-
lary of 2200 words. The language model used is made
of 44K words. For this study we selected utterances cor-
responding to answers to a prompt asking for the kind
of restaurant the users were looking for. This corpus has
been cut in two: a development corpus containing 511
utterances and a test corpus containing 419 utterances.
This development corpus has been used to train the deci-
sion tree presented in section 5.2. The Word Error Rate
on the test corpus is 22.7%.
7.3 Evaluation of the rescoring strategy
Table 1 shows the results obtained with a rescoring strat-
egy that selects, from the structured n-best list, the hy-
pothesis with the highest P (? | M ). The baseline re-
sults are obtained with a standard maximum-likelihood
approach choosing the hypothesis maximizing the proba-
bility P (? | Y ) of equation 1. No rejection is performed
in this experiment.
The size of the n-best lists was set to 12 items: the first
4 candidates of the first 3 interpretations in the structured
n-best list. The gain obtained after rescoring is very sig-
nificant and justify our 2-step approach that first extract
an n-best list of interpretations thanks to P (? | Y ) and
then choose the one with the highest confidence accord-
ing to a large set of confidence measures M . This gain
can be compared to the one obtained on the Word Error
Rate measure: the WER drops from 21.6% to 20.7% af-
ter rescoring on the development corpus and from 22.7%
to 22.5% on the test corpus. It is clear here that the
WER measure is not an adequate measure in a Spoken
Dialogue context as a big reduction in the Understanding
Error Rate might have very little effect on the Word Error
Rate.
Corpus baseline rescoring UER reduction %
Devt. 15.0 12.4 17.3%
Test 17.7 14.5 18%
Table 1: Understanding Error Rate results with and with-
out rescoring on structured n-best lists (n=12) (no rejec-
tion)
7.4 Evaluation of the decision strategy
In this experiment we evaluate the decision strategy con-
sisting of accepting or rejecting an hypothesis ? thanks to
a threshold on the probability P (? | M ). Figure 2 shows
the curve UER vs. utterance rejection on the development
and test corpora. As we can see very significant improve-
ments can be achieved with very little utterance rejection.
For example, at a 5% utterance rejection operating point,
the UER on the development corpus drops from 15.0% to
8.6% (42.6% relative improvement) and from 17.7% to
11.4% (35.6% relative improvement).
By using equation 5 for finding the operating point
minimizing the risk fonction (with a cost ?fa = 1.5 ?
?fr) on the development corpus we obtain:
? on the development corpus: UER=6.5 utterance re-
jection=13.1
? on the test corpus: UER=9.6 utterance rejec-
tion=15.9
46
8
10
12
14
16
18
0 5 10 15 20
un
de
rst
an
din
g e
rro
r r
ate
utterance rejection (%)
devt
test
Figure 2: Understanding Error Rate vs. utterance rejec-
tion on the development and test corpora
8 Conclusion
This paper describes an interpretation and decision strat-
egy that minimizes interpretation errors and perform dia-
logue actions which may not depend on the hypothesized
concepts only, but also on confidence of what has been
recognized. The first step in the process consists of gen-
erating a structured n-best list of conceptual interpreta-
tions of an utterance. A set of confidence measures is
then used in order to rescore the n-best list thanks to a de-
cision tree approach. Significant gains in Understanding
Error Rate are achieved with this rescoring method (18%
relative improvement). The confidence score given by the
tree can also be used in a decision strategy about the ac-
tion to perform. By using this score, significant improve-
ments in UER can be achieved with very little utterance
rejection. For example, at a 5% utterance rejection op-
erating point, the UER on the development corpus drops
from 15.0% to 8.6% (42.6% relative improvement) and
from 17.7% to 11.4% (35.6% relative improvement). Fi-
nally the operating point for a deployed dialogue system
can be chosen by explicitly minimizing a risk function on
a development corpus.
References
Cyril Allauzen, Mehryar Mohri, and Brian Roark. 2003.
Generalized algorithms for constructing statistical lan-
guage models. In 41st Annual Meeting of the Associa-
tion for Computational Linguistics (ACL?03), Sapporo,
Japan.
Fre?de?ric Be?chet, Alexis Nasr, and Franck Genet. 2000.
Tagging unknown proper names using decision trees.
In 38th Annual Meeting of the Association for Compu-
tational Linguistics, Hong-Kong, China, pages 77?84.
Yannick Este`ve, Christian Raymond, Renato De Mori,
and David Janiszek. 2003. On the use of linguistic
consistency in systems for human-computer dialogs.
IEEE Transactions on Speech and Audio Processing,
(Accepted for publication, in press).
Patrick Haffner, Gokhan Tur, and Jerry Wright. 2003.
Optimizing SVMs for complex call classification. In
IEEE International Conference on Acoustics, Speech
and Signal Processing, ICASSP?03, Hong-Kong.
Y. He and S. Young. 2003. A data-driven spoken lan-
guage understanding system. In Automatic Speech
Recognition and Understanding workshop - ASRU?03,
St. Thomas, US-Virgin Islands.
R. Kuhn and R. De Mori. 1995. The application of se-
mantic classification trees to natural language under-
standing. IEEE Trans. on Pattern Analysis and Ma-
chine Intelligence, 17(449-460).
Mehryar Mohri, Fernando Pereira, and Michael Ri-
ley. 2002. Weighted finite-state transducers in
speech recognition. Computer, Speech and Language,
16(1):69?88.
Christian Raymond, Yannick Este`ve, Fre?de?ric Be?chet,
Renato De Mori, and Ge?raldine Damnati. 2003. Belief
confirmation in spoken dialogue systems using confi-
dence measures. In Automatic Speech Recognition and
Understanding workshop - ASRU?03, St. Thomas, US-
Virgin Islands.
Steve Young. 2002. Talking to machines (statisti-
cally speaking). In International Conference on Spo-
ken Language Processing, ICSLP?02, pages 113?120,
Denver, CO.
Conceptual Language Models for Dialog systems 
Renato De Mori 
LIA CNRS  BP 1228  
84911 Avignon Cedex 9 - France 
renato.demori, @lia.univ-avignon.fr 
Frederic B?chet 
LIA CNRS  BP 1228  
84911 Avignon Cedex 9 - France  
frederic.bechet, @lia.univ-avignon.fr 
 
1 
2 
Introduction 
The purpose of computer speech understanding is to 
find conceptual representations from signs coded into 
the speech signal. 
 
Contrary to speech interpretation by humans in which 
the same discourse may be interpreted differently by 
different subjects, for practical applications of computer 
understanding the result of interpretation should be 
unique for a given signal. Usually it is represented by an 
object which is an instance of class corresponding to a 
semantic structure which can be fairly complex even if 
it is built with instances of conceptual constituents be-
longing to a small set of major ontological categories.  
 
The mapping process that leads to a semantic interpreta-
tion can be derived manually because human interpreta-
tion of sentences can be completely explained with a 
logical formalism or it can be inferred by machine 
learning algorithms in order to ensure a large coverage 
of possible sentence patterns. Theories and practical 
implementations of these approaches are proposed in 
[1],[2]. 
 
 Limitations of coverage in the manual approach and in 
precision of machine learning can be reduced by making 
manually a detailed analysis of a limited number of ex-
amples and generalizing each analysis with automatic 
methods. In particular, a well structured lexicon can be 
very useful, in which the meaning of words is repre-
sented together with suggestions of possible syntactic 
and conceptual structures.  
 
Word associations found with networks of word rela-
tions [3] can also be useful for suggesting compositions 
of semantic constituents into conceptual structures. 
Thus, given an observed example, other examples can 
be manually derived and generalized automatically. 
 
Computer understanding of a spoken sentences is prob-
lem solving activity whose central engine is a search 
process involving various types of models.  
 
Searching for concepts can be combined with searching 
for words. This suggests that statistical language models 
(LMs) could be adapted based on expectations of con-
cepts predicted by a system belief. With this perspec-
tive, it is important to notice that, while the observation 
of only certain words may be sufficient for hypothesiz-
ing a conceptual structure, complete details of word 
phrases expressing a conceptual structure have to be 
known in order to adapt a generic LM to the expectation 
of such a structure.  
 
This paper introduces a search method and a learning 
paradigm based on the just introduced considerations.  
 
The search engine built with this method finds the best 
common path between the system knowledge repre-
sented by the composition of Stochastic Finite State 
Transducers (SFST) and a Stochastic Finite State 
Automaton (SFSA) representing the lattice of word hy-
potheses generated by an Automatic Speech Recogni-
tion System (ASR).  
Hypothesis evaluation and search 
Let a dialogue system have a belief which generates 
expectations B about conceptual structures.  
 
Expectation uncertainty is represented by a probability 
distribution P(B) which is non-zero for a set of concep-
tual structures expected at a given time. Thus for a gen-
eral concept structure ? and a description Y of the 
speech signal, one gets:   
 
)B,Y,(Pmax)B,Y,(P)Y,(P
)Y,(Pmaxarg)Y|(Pmaxarg*
BB
????=?
?=?=?
??  
 
{ })B,,W(P)W|Y(Pmaxarg*
)B,,W(P)W|Y(Pmax
)B,Y,,W(P)B,Y,(P
B,W,
W
W
???
?
???=?
?
  (1) 
 
)B(P)B|W(P)BW|(P)B,W,(P ?=?  
 
A general concept structure ? can be represented as a 
string of parenthesized terminals and non-terminals.  
 
These expressions can be decomposed into chunks. A 
sentence may contain only one or more chunks of an 
incomplete structure, Thus, a system should be able to 
generate interpretation hypotheses about parts of a con-
ceptual structure. In this case, symbol ? makes refer-
ence only to a set of components.  
 
Probability P(?|BW) can be simply set equal to 0 for a 
conceptual structure which cannot be inferred from W. 
If the conceptual structure is part of the expectations of 
system beliefs and can be inferred unambiguously from 
W, then P(?|BW) as in many practical applications in-
cluding the one considered in this paper, then P(?|BW). 
Otherwise, let [  be the sequence of con-
cept symbols corresponding to the preterminal symbols 
in ?. Probability P(?|BW) can be expressed as follows: 
]c....c...c1 ??
 
{ }?
==?
?
=? ???
??
2
111
1
BW]c...c[|cP)BW|c(P
)BW|]c....c...c([P)BW|(P
 (2) 
 
At least, for some values of ? the probability { }BW]c...c[|cP 11 ???  is one for a class of applica-
tions. 
 
Let ? be the set of conceptual components, chunks of 
them or conceptual structures known to the system. Ex-
pectations derived from the system belief can be 
grouped into a set B1. Let B2 the complement of B1 
w.r.t. ? and F be a filler structure representing all the 
conceptual structures not in the application or just ig-
nored by ignorance of the system knowledge. B1, B2 
and F are the possible values for B in the (1) and their 
probabilities P(B) can be established subjectively or by 
evaluating counts for user responses consistent with the 
belief, consistent with the application but not with the 
belief and inconsistent with the application knowledge. 
 
Probability P(W|B) is that of an LM which is adapted to 
the system belief. It can be obtained with an LM built in 
the following way.  
 
Each conceptual structure or part of it ? is represented 
by a finite-state network N(?). 
 
All the networks corresponding to structures in B1 are 
connected in parallel in a single structure with associ-
ated a probability P(B1). A similar structure is built for 
the automata corresponding to structures in B2. A filler 
F is also considered containing a network derived by a 
trigram LM. A network N(?) is obtained by the con-
catenation of finite-state automata C(?) inferred with 
the procedure described in the next section representing 
chunks of knowledge with fillers F. These automata 
output components of conceptual structures. 
 
A search is performed by finding the most likely com-
mon path in the network and in the automaton derived 
from a lattice of word hypotheses generated by the 
speech recognizer with the generic trigram LM. System 
belief make vary the topology of the network by dy-
namically changing the composition of  sets B1 and B2. 
Network recompilation can be avoided by just putting 
all the N(?) in parallel and dynamically assigning each 
network of B1 a probability : 
 
1B
)1B(P)](N[P =?    (3) 
where 1B  indicates the number of elements in B1.  
Probabilities of networks in B2 are assigned in a similar 
way. 
A word sequence W always corresponds to a path in F 
and may correspond to one or more conceptual struc-
tures represented by paths in networks in B1 and B2. In 
the second case, the likelihood of W in F will be much 
lower than the likelihood in B1 or B2 because phrases 
recognized by the chunk automata of the network are 
boosted as it will be shown later. Thus the best path for 
W, in this case, will go through a network whose auto-
mata produce as output the components of a conceptual 
structure.  
 
3 Knowledge inference 
Usually, when an application is developed, an even 
small training corpus is available. 
 
Semantic categories and functions are manually derived 
for an application. They can be modified when the ap-
plication is deployed in order to correct errors or add 
missing constituents.  
 
A number of words in the lexicon have lexical entries 
containing their syntactic category, syntactic constructs 
which can appear in the same sentence, semantic fea-
tures and constructs they can be part of. When one of 
these words is encountered in the training corpus, it is 
considered as a trigger for the semantic categories con-
tained in its lexical entry. The association between 
words and semantic features is part of the semantic 
knowledge of the system. 
 
The presence of a category in the sentence under analy-
sis can be verified manually or by deriving it from the 
parse tree of the sentence. As lexical entries, grammars 
and rules for deriving semantic structures from parse 
trees may be imprecise or incomplete, a single example 
can be carefully examined and validated manually.  
 
Once a single example is available with a detailed syn-
tactic and semantic analysis, it can be generalized. A 
sentence may contain a complete or partial semantic 
structure or just one component concept. Let ? represent 
such a semantic interpretation. Furthermore, each struc-
ture may correspond to a pattern made of phrases and 
fillers of the sentence represented by a sequence of 
words W. Semantic Classification Trees (SCT) pro-
posed in [1] can be used for automatically deriving sen-
tence patterns corresponding to conceptual structures.  
 
The purpose of learning is to build or modify a SFST 
that accepts a sequence of words and output a semantic 
interpretation ?. 
 
The initial analysis of an example starts by using a tag-
ger for replacing words with their preterminal syntactic 
categories.  
 
Then, semantic tags are automatically associated with 
sequences of syntactic tags manually or using the se-
mantic knowledge. A tag expression made of syntactic 
and semantic tags is obtained in this way as a represen-
tation for of ?. As a by-product, expressions for the 
constituents of and components of  ? are built and 
added to the semantic knowledge.  
 
Generalization of the example uses a phrase generator to 
produce sequences of words from the tag expression. 
These sequences of words enrich the finite state transla-
tor which has to map word sequences into the concep-
tual structure ?.  
 
Further generalization can be obtained by inferring 
synonyms with a WordNet. If generalization has pro-
vided erroneous sequences of words, these sequences 
can be removed  by manual inspection or when it is ob-
served that the system has made an interpretation error 
because of them. With a similar procedure, new se-
quences of words can be added to the automaton for ?.  
 
Once it has been found that a word (noun or verb) con-
tributed to hypothesize a concept in the semantic struc-
ture, the concept is added as semantic feature in the 
lexical entry of the word.  
 
In summary learning of semantic knowledge follows the 
following steps: 
 
1 Set the semantic categories for the application. 
 
2 Set the lexical entries for the words that are semanti-
cally relevant for the application. 
 
3 For every analyzed sentence  
? if semantic interpretation is correct   then do 
nothing, 
? if a phrase is misplaced in the representation of 
a semantic structure then remove it, 
? if a phrase is missed in the representation of a 
semantic structure, but the corresponding tag 
expressions is present in the semantic knowl-
edge, then the phrase is added to the corre-
sponding SFST, 
? if the tag expression does not exist in the se-
mantic knowledge, then it is built and se-
quences of words are generated from it with 
the above outlined generalization procedure. 
 
A set of SFST is built in this way. They are added to the 
LM to provide concept specific components and to pro-
duce semantic interpretations at the same time with a 
translation process.  
 
References 
 
[1] Kuhn R.  and De Mori R. (1995). The Application of 
Semantic Classification Trees to Natural Language Un-
derstanding. IEEE Trans. on Pattern Analysis and Ma-
chine Intelligence, 17 : 449-460.  
 
[2] Pieraccini R., Levin E., and Lee C.-H. (1991). Sto-
chastic Representation of Conceptual Structure in the 
ATIS Task. Proceedings of the, 1991 Speech and Natu-
ral Language Workshop, 121-124, Morgan Kaufmann 
publ, Los Altos, CA. 
 
[3] Vossen P. Diez-Orzas P. and Peters W., (1997) 
The multilingual design of EuroWordnet. 
Proc ACL/EACL workshop on automatic information 
extraction and building of lexical semantic resources for 
NLP applications, Madrid, 1997. 
 
Proceedings of the ACL-HLT 2011 System Demonstrations, pages 86?91,
Portland, Oregon, USA, 21 June 2011. c?2011 Association for Computational Linguistics
MACAON
An NLP Tool Suite for Processing Word Lattices
Alexis Nasr Fre?de?ric Be?chet Jean-Franc?ois Rey Beno??t Favre Joseph Le Roux?
Laboratoire d?Informatique Fondamentale de Marseille- CNRS - UMR 6166
Universite? Aix-Marseille
(alexis.nasr,frederic.bechet,jean-francois.rey,benoit.favre,joseph.le.roux)
@lif.univ-mrs.fr
Abstract
MACAON is a tool suite for standard NLP tasks
developed for French. MACAON has been de-
signed to process both human-produced text
and highly ambiguous word-lattices produced
by NLP tools. MACAON is made of several na-
tive modules for common tasks such as a tok-
enization, a part-of-speech tagging or syntac-
tic parsing, all communicating with each other
through XML files . In addition, exchange pro-
tocols with external tools are easily definable.
MACAON is a fast, modular and open tool, dis-
tributed under GNU Public License.
1 Introduction
The automatic processing of textual data generated
by NLP software, resulting from Machine Transla-
tion, Automatic Speech Recognition or Automatic
Text Summarization, raises new challenges for lan-
guage processing tools. Unlike native texts (texts
produced by humans), this new kind of texts is the
result of imperfect processors and they are made
of several hypotheses, usually weighted with con-
fidence measures. Automatic text production sys-
tems can produce these weighted hypotheses as n-
best lists, word lattices, or confusion networks. It is
crucial for this space of ambiguous solutions to be
kept for later processing since the ambiguities of the
lower levels can sometimes be resolved during high-
level processing stages. It is therefore important to
be able to represent this ambiguity.
?This work has been funded by the French Agence Nationale
pour la Recherche, through the projects SEQUOIA (ANR-08-
EMER-013) and DECODA (2009-CORD-005-01)
MACAON is a suite of tools developped to pro-
cess ambiguous input and extend inference of in-
put modules within a global scope. It con-
sists in several modules that perform classical
NLP tasks (tokenization, word recognition, part-of-
speech tagging, lemmatization, morphological anal-
ysis, partial or full parsing) on either native text
or word lattices. MACAON is distributed under
GNU public licence and can be downloaded from
http://www.macaon.lif.univ-mrs.fr/.
From a general point of view, a MACAON module
can be seen as an annotation device1 which adds a
new level of annotation to its input that generally de-
pends on annotations from preceding modules. The
modules communicate through XML files that allow
the representation different layers of annotation as
well as ambiguities at each layer. Moreover, the ini-
tial XML structuring of the processed files (logical
structuring of a document, information from the Au-
tomatic Speech Recognition module . . . ) remains
untouched by the processing stages.
As already mentioned, one of the main charac-
teristics of MACAON is the ability for each module
to accept ambiguous inputs and produce ambiguous
outputs, in such a way that ambiguities can be re-
solved at a later stage of processing. The compact
representation of ambiguous structures is at the heart
of the MACAON exchange format, described in sec-
tion 2. Furthermore every module can weight the
solutions it produces. such weights can be used to
rank solutions or limit their number for later stages
1Annotation must be taken here in a general sense which in-
cludes tagging, segmentation or the construction of more com-
plex objets as syntagmatic or dependencies trees.
86
of processing.
Several processing tools suites alread exist for
French among which SXPIPE (Sagot and Boullier,
2008), OUTILEX (Blanc et al, 2006), NOOJ2 or UNI-
TEX3. A general comparison of MACAON with these
tools is beyond the scope of this paper. Let us just
mention that MACAON shares with most of them the
use of finite state machines as core data represen-
tation. Some modules are implemented as standard
operations on finite state machines.
MACAON can also be compared to the numerous
development frameworks for developping process-
ing tools, such as GATE4, FREELING5, ELLOGON6
or LINGPIPE7 that are usually limited to the process-
ing of native texts.
The MACAON exchange format shares a cer-
tain number of features with linguistic annotation
scheme standards such as the Text Encoding Initia-
tive8, XCES9, or EAGLES10. They all aim at defining
standards for various types of corpus annotations.
The main difference between MACAON and these
approaches is that MACAON defines an exchange for-
mat between NLP modules and not an annotation
format. More precisely, this format is dedicated to
the compact representation of ambiguity: some in-
formation represented in the exchange format are
to be interpreted by MACAON modules and would
not be part of an annotation format. Moreover,
the MACAON exchange format was defined from the
bottom up, originating from the authors? need to use
several existing tools and adapt their input/output
formats in order for them to be compatible. This is in
contrast with a top down approach which is usually
chosen when specifying a standard. Still, MACAON
shares several characteristics with the LAF (Ide and
Romary, 2004) which aims at defining high level
standards for exchanging linguistic data.
2www.nooj4nlp.net/pages/nooj.html
3www-igm.univ-mlv.fr/?unitex
4gate.ac.uk
5garraf.epsevg.upc.es/freeling
6www.ellogon.org
7alias-i.com/lingpipe
8www.tei-c.org/P5
9www.xml-ces.org
10www.ilc.cnr.it/eagles/home.html
2 The MACAON exchange format
The MACAON exchange format is based on four con-
cepts: segment, attribute, annotation level and seg-
mentation.
A segment refers to a segment of the text or
speech signal that is to be processed, as a sentence,
a clause, a syntactic constituent, a lexical unit, a
named entity . . . A segment can be equipped with at-
tributes that describe some of its aspects. A syntac-
tic constituent, for example, will define the attribute
type which specifies its syntactic type (Noun Phrase,
Verb Phrase . . . ). A segment is made of one or more
smaller segments.
A sequence of segments covering a whole sen-
tence for written text, or a spoken utterance for oral
data, is called a segmentation. Such a sequence can
be weighted.
An annotation level groups together segments of
a same type, as well as segmentations defined on
these segments. Four levels are currently defined:
pre-lexical, lexical, morpho-syntactic and syntactic.
Two relations are defined on segments: the prece-
dence relation that organises linearly segments of a
given level into segmentations and the dominance
relation that describes how a segment is decomposed
in smaller segments either of the same level or of a
lower level.
We have represented in figure 2, a schematic rep-
resentation of the analysis of the reconstructed out-
put a speech recognizer would produce on the in-
put time flies like an arrow11. Three annotation lev-
els have been represented, lexical, morpho-syntactic
and syntactic. Each level is represented by a finite-
state automaton which models the precedence rela-
tion defined over the segments of this level. Seg-
ment time, for example, precedes segment flies. The
segments are implicitly represented by the labels of
the automaton?s arcs. This label should be seen as
a reference to a more complex objet, the actual seg-
ment. The dominance relations are represented with
dashed lines that link segments of different levels.
Segment time, for example, is dominated by seg-
ment NN of the morpho-syntactic level.
This example illustrates the different ambiguity
cases and the way they are represented.
11For readability reasons, we have used an English example,
MACAON, as mentioned above, currently exists for French.
87
thyme
time
flies like
liken
an arrow
a row
JJ IN
VB
DT NN
DT NN
VB
NN
NN
VBZ
VB VB
VP
VP
NP
NP
VP
NP
VP
VP
PP
NP
NP
Figure 1: Three annotation levels for a sample sentence.
Plain lines represent annotation hypotheses within a level
while dashed lines represent links between levels. Trian-
gles with the tip up are ?and? nodes and triangles with
the tip down are ?or? nodes. For instance, in the part-of-
speech layer, The first NN can either refer to ?time? or
?thyme?. In the chunking layer, segments that span mul-
tiple part-of-speech tags are linked to them through ?and?
nodes.
The most immediate ambiguity phenomenon is
the segmentation ambiguity: several segmentations
are possible at every level. This ambiguity is rep-
resented in a compact way through the factoring of
segments that participate in different segmentations,
by way of a finite state automaton.
The second ambiguity phenomenon is the dom-
inance ambiguity, where a segment can be decom-
posed in several ways into lower level segments.
Such a case appears in the preceding example, where
the NN segment appearing in one of the outgoing
transition of the initial state of the morpho-syntactic
level dominates both thyme and time segments of the
lexical level. The triangle with the tip down is an
?or? node, modeling the fact that NN corresponds to
time or thyme.
Triangles with the tip up are ?and? nodes. They
model the fact that the PP segment of the syntac-
tic level dominates segments IN, DT and NN of the
morpho-syntactic level.
2.1 XML representation
The MACAON exchange format is implemented in
XML. A segment is represented with the XML tag
<segment> which has four mandatory attributes:
? type indicates the type of the segment, four dif-
ferent types are currently defined: atome (pre-
lexical unit usually referred to as token in en-
glish), ulex (lexical unit), cat (part of speech)
and chunk (a non recursive syntactic unit).
? id associates to a segment a unique identifier in
the document, in order to be able to reference
it.
? start and end define the span of the segment.
These two attributes are numerical and repre-
sent either the index of the first and last char-
acter of the segment in the text string or the
beginning and ending time of the segment in
a speech signal.
A segment can define other attributes that can be
useful for a given description level. We often find
the stype attribute that defines subtypes of a given
type.
The dominance relation is represented through the
use of the <sequence> tag. The domination of the
three segments IN, DT and NN by a PP segment,
mentionned above is represented below, where p1,
p2 and p3 are respectively the ids of segments IN,
DT and NN.
<segment type="chunk" stype="PP" id="c1">
<sequence>
<elt segref="p1"/>
<elt segref="p2"/>
<elt segref="p3"/>
</sequence>
</segment>
The ambiguous case, described above where seg-
ment NN dominates segments time or thyme is rep-
resented below as a disjunction of sequences inside
a segment. The disjunction itself is not represented
as an XML tag. l1 and l2 are respectively the ids
of segments time and thyme.
<segment type="cat" stype="NN" id="c1">
<sequence>
<elt segref="l1" w="-3.37"/>
</sequence>
<sequence>
<elt segref="l2" w="-4.53"/>
</sequence>
</segment>
88
The dominance relation can be weighted, by way
of the attribute w. Such a weight represents in the
preceding example the conditional log-probability
of a lexical unit given a part of speech, as in a hidden
Markov model.
The precedence relation (i.e. the organization
of segments in segmentations), is represented as a
weighted finite state automaton. Automata are rep-
resented as a start state, accept states and a list of
transitions between states, as in the following exam-
ple that corresponds to the lexical level of our exam-
ple.
<fsm n="9">
<start n="0"/>
<accept n="6"/>
<ltrans>
<trans o="0" d="1" i="l1" w="-7.23"/>
<trans o="0" d="1" i="l2" w="-9.00"/>
<trans o="1" d="2" i="l3" w="-3.78"/>
<trans o="2" d="3" i="l4" w="-7.37"/>
<trans o="3" d="4" i="l5" w="-3.73"/>
<trans o="2" d="4" i="l6" w="-6.67"/>
<trans o="4" d="5" i="l7" w="-4.56"/>
<trans o="5" d="6" i="l8" w="-2.63"/>
<trans o="4" d="6" i="l9" w="-7.63"/>
</ltrans>
</fsm>
The <trans/> tag represents a transition, its
o,d,i and w features are respectively the origin, and
destination states, its label (the id of a segment) and
a weight.
An annotation level is represented by the
<section> tag which regroups two tags, the
<segments> tag that contains the different segment
tags defined at this annotation level and the <fsm>
tag that represents all the segmentations of this level.
3 The MACAON architecture
Three aspects have guided the architecture of
MACAON: openness, modularity, and speed. Open-
ness has been achieved by the definition of an ex-
change format which has been made as general as
possible, in such a way that mapping can be de-
fined from and to third party modules as ASR, MT
systems or parsers. Modularity has been achieved
by the definition of independent modules that com-
municate with each other through XML files using
standard UNIX pipes. A module can therefore be re-
placed easily. Speed has been obtained using effi-
cient algorithms and a representation especially de-
signed to load linguistic data and models in a fast
way.
MACAON is composed of libraries and compo-
nents. Libraries contain either linguistic data, mod-
els or API functions. Two kinds of components are
presented, the MACAON core components and third
party components for which mappings to and from
the MACAON exchange format have been defined.
3.1 Libraries
The main MACAON library is macaon common.
It defines a simple interface to the MACAON ex-
change format and functions to load XML MACAON
files into memory using efficient data structures.
Other libraries macaon lex, macaon code and
macaon tagger lib represent the lexicon, the
morphological data base and the tagger models in
memory.
MACAON only relies on two third-party libraries,
which are gfsm12, a finite state machine library and
libxml, an XML library13.
3.2 The MACAON core components
A brief description of several standard components
developed in the MACAON framework is given be-
low. They all comply with the exchange format de-
scribed above and add a <macaon stamp> to the
XML file that indicates the name of the component,
the date and the component version number, and rec-
ognizes a set of standard options.
maca select is a pre-processing component: it adds
a macaon tag under the target tags specified by
the user to the input XML file. The follow-
ing components will only process the document
parts enclosed in macaon tags.
maca segmenter segments a text into sentences by
examining the context of punctuation with a
regular grammar given as a finite state automa-
ton. It is disabled for automatic speech tran-
scriptions which do not typically include punc-
tuation signs and come with their own segmen-
tation.
12ling.uni-potsdam.de/?moocow/projects/
gfsm/
13xmlsoft.org
89
maca tokenizer tokenizes a sentence into pre-
lexical units. It is also based on regular gram-
mars that recognize simple tokens as well as a
predefined set of special tokens, such as time
expressions, numerical expressions, urls. . . .
maca lexer allows to regroup pre-lexical units into
lexical units. It is based on the lefff French lex-
icon (Sagot et al, 2006) which contains around
500,000 forms. It implements a dynamic pro-
gramming algorithm that builds all the possible
grouping of pre-lexical units into lexical units.
maca tagger associates to every lexical unit one or
more part-of-speech labels. It is based on a
trigram Hidden Markov Model trained on the
French Treebank (Abeille? et al, 2003). The es-
timation of the HMM parameters has been re-
alized by the SRILM toolkit (Stolcke, 2002).
maca anamorph produces the morphological anal-
ysis of lexical units associated to a part of
speech. The morphological information come
from the lefff lexicon.
maca chunker gathers sequences of part-of-speech
tags in non recursive syntactic units. This com-
ponent implements a cascade of finite state
transducers, as proposed by Abney (1996). It
adds some features to the initial Abney pro-
posal, like the possibility to define the head of
a chunk.
maca conv is a set of converters from and to the
MACAON exchange format. htk2macaon
and fsm2macaon convert word lattices from
the HTK format (Young, 1994) and ATT
FSM format (Mohri et al, 2000) to the
MACAON exchange format. macaon2txt and
txt2macaon convert from and to plain text
files. macaon2lorg and lorg2macaon
convert to and from the format of the LORG
parser (see section 3.3).
maca view is a graphical interface that allows to in-
spect MACAON XML files and run the compo-
nents.
3.3 Third party components
MACAON is an open architecture and provides a rich
exchange format which makes possible the repre-
sentation of many NLP tools input and output in the
MACAON format. MACAON has been interfaced with
the SPEERAL Automatic Speech Recognition Sys-
tem (Nocera et al, 2006). The word lattices pro-
duced by SPEERAL can be converted to pre-lexical
MACAON automata.
MACAON does not provide any native module for
parsing yet but it can be interfaced with any already
existing parser. For the purpose of this demonstra-
tion we have chosen the LORG parser developed at
NCLT, Dublin14. This parser is based on PCFGs
with latent annotations (Petrov et al, 2006), a for-
malism that showed state-of-the-art parsing accu-
racy for a wide range of languages. In addition it of-
fers a sophisticated handling of unknown words re-
lying on automatically learned morphological clues,
especially for French (Attia et al, 2010). Moreover,
this parser accepts input that can be tokenized, pos-
tagged or pre-bracketed. This possibility allows for
different settings when interfacing it with MACAON.
4 Applications
MACAON has been used in several projects, two of
which are briefly described here, the DEFINIENS
project and the LUNA project.
DEFINIENS (Barque et al, 2010) is a project that
aims at structuring the definitions of a large coverage
French lexicon, the Tre?sor de la langue franc?aise.
The lexicographic definitions have been processed
by MACAON in order to decompose the definitions
into complex semantico-syntactic units. The data
processed is therefore native text that possesses a
rich XML structure that has to be preserved during
processing.
LUNA15 is a European project that aims at extract-
ing information from oral data about hotel booking.
The word lattices produced by an ASR system have
been processed by MACAON up to a partial syntactic
level from which frames are built. More details can
be found in (Be?chet and Nasr, 2009). The key aspect
of the use of MACAON for the LUNA project is the
ability to perform the linguistic analyses on the mul-
tiple hypotheses produced by the ASR system. It is
therefore possible, for a given syntactic analysis, to
14www.computing.dcu.ie/?lorg. This software
should be freely available for academic research by the time
of the conference.
15www.ist-luna.eu
90
Figure 2: Screenshot of the MACAON visualization inter-
face (for French models). It allows to input a text and see
the n-best results of the annotation.
find all the word sequences that are compatible with
this analysis.
Figure 2 shows the interface that can be used to
see the output of the pipeline.
5 Conclusion
In this paper we have presented MACAON, an NLP
tool suite which allows to process native text as well
as several hypotheses automatically produced by an
ASR or an MT system. Several evolutions are cur-
rently under development, such as a named entity
recognizer component and an interface with a de-
pendency parser.
References
Anne Abeille?, Lionel Cle?ment, and Franc?ois Toussenel.
2003. Building a treebank for french. In Anne
Abeille?, editor, Treebanks. Kluwer, Dordrecht.
Steven Abney. 1996. Partial parsing via finite-state cas-
cades. In Workshop on Robust Parsing, 8th European
Summer School in Logic, Language and Information,
Prague, Czech Republic, pages 8?15.
M. Attia, J. Foster, D. Hogan, J. Le Roux, L. Tounsi, and
J. van Genabith. 2010. Handling Unknown Words in
Statistical Latent-Variable Parsing Models for Arabic,
English and French. In Proceedings of SPMRL.
Lucie Barque, Alexis Nasr, and Alain Polgue`re. 2010.
From the definitions of the tre?sor de la langue franc?aise
to a semantic database of the french language. In EU-
RALEX 2010, Leeuwarden, Pays Bas.
Fre?de?ric Be?chet and Alexis Nasr. 2009. Robust depen-
dency parsing for spoken language understanding of
spontaneous speech. In Interspeech, Brighton, United
Kingdom.
Olivier Blanc, Matthieu Constant, and Eric Laporte.
2006. Outilex, plate-forme logicielle de traitement de
textes e?crits. In TALN 2006, Leuven.
Nancy Ide and Laurent Romary. 2004. International
standard for a linguistic annotation framework. Nat-
ural language engineering, 10(3-4):211?225.
M. Mohri, F. Pereira, and M. Riley. 2000. The design
principles of a weighted finite-state transducer library.
Theoretical Computer Science, 231(1):17?32.
P. Nocera, G. Linares, D. Massonie?, and L. Lefort. 2006.
Phoneme lattice based A* search algorithm for speech
recognition. In Text, Speech and Dialogue, pages 83?
111. Springer.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning Accurate, Compact, and In-
terpretable Tree Annotation. In ACL.
Beno??t Sagot and Pierre Boullier. 2008. Sxpipe 2:
architecture pour le traitement pre?syntaxique de cor-
pus bruts. Traitement Automatique des Langues,
49(2):155?188.
Beno??t Sagot, Lionel Cle?ment, Eric Villemonte de la
Clergerie, and Pierre Boullier. 2006. The lefff 2 Syn-
tactic Lexicon for French: Architecture, Acquisition,
Use. In International Conference on Language Re-
sources and Evaluation, Genoa.
Andreas Stolcke. 2002. Srilm - an extensible language
modeling toolkit. In International Conference on Spo-
ken Language Processing, Denver, Colorado.
S.J. Young. 1994. The HTK Hidden Markov Model
Toolkit: Design and Philosophy. Entropic Cambridge
Research Laboratory, Ltd, 2:2?44.
91
Proceedings of the Workshop on Innovative Hybrid Approaches to the Processing of Textual Data (Hybrid2012), EACL 2012, pages 52?60,
Avignon, France, April 23 2012. c?2012 Association for Computational Linguistics
A Joint Named Entity Recognition and Entity Linking System
Rosa Stern,1,2 Beno??t Sagot1 and Fre?de?ric Be?chet3
1Alpage, INRIA & Univ. Paris Diderot, Sorbonne Paris Cite? / F-75013 Paris, France
2AFP-Medialab / F-75002 Paris, France
3Univ. Aix Marseille, LIF-CNRS / Marseille, France
Abstract
We present a joint system for named entity
recognition (NER) and entity linking (EL),
allowing for named entities mentions ex-
tracted from textual data to be matched to
uniquely identifiable entities. Our approach
relies on combined NER modules which
transfer the disambiguation step to the EL
component, where referential knowledge
about entities can be used to select a correct
entity reading. Hybridation is a main fea-
ture of our system, as we have performed
experiments combining two types of NER,
based respectively on symbolic and statis-
tical techniques. Furthermore, the statisti-
cal EL module relies on entity knowledge
acquired over a large news corpus using a
simple rule-base disambiguation tool. An
implementation of our system is described,
along with experiments and evaluation re-
sults on French news wires. Linking ac-
curacy reaches up to 87%, and the NER F-
score up to 83%.
1 Introduction
1.1 Textual and Referential Aspects of
Entities
In this work we present a system designed for the
extraction of entities from textual data. Named
entities (NEs), which include person, location,
company or organization names1 must therefore
be detected using named entity recognition (NER)
techniques. In addition to this detection based
on their surface forms, NEs can be identified by
mapping them to the actual entity they denote,
in order for these extractions to constitute use-
ful and complete information. However, because
1The set of possible named entities varies from restric-
tive, as in our case, to wide definitions; it can also include
dates, event names, historical periods, etc.
of name variation, which can be surfacic or en-
cyclopedic, an entity can be denoted by several
mentions (e.g., Bruce Springsteen, Springsteen,
the Boss); conversely, due to name ambiguity, a
single mention can denote several distinct entities
(Orange is the name of 22 locations in the world;
in French, M. Obama can denote both the US
president Barack Obama (M. is an abbreviation of
Monsieur ?Mr?) or his spouse Michelle Obama; in
this case ambiguity is caused by variation). Even
in the case of unambiguous mentions, a clear link
should be established between the surface men-
tion and a uniquely identifiable entity, which is
achieved by entity linking (EL) techniques.
1.2 Entity Approach and Related Work
In order to obtain referenced entities from raw
textual input, we introduce a system based on
the joint application of named entity recognition
(NER) and entity linking (EL), where the NER out-
put is given to the linking component as a set of
possible mentions, preserving a number of am-
biguous readings. The linking process must there-
after evaluate which readings are the most proba-
ble, based on the most likely entity matches in-
ferred from a similarity measure with the context.
NER has been widely addressed by symbolic,
statistical as well as hybrid approaches. Its major
part in information extraction (IE) and other NLP
applications has been stated and encouraged by
several editions of evaluation campaigns such
as MUC (Marsh and Perzanowski, 1998),
the CoNLL-2003 NER shared task
(Tjong Kim Sang and De Meulder, 2003) or
ACE (Doddington et al, 2004), where NER
systems show near-human performances for
the English language. Our system aims at
benefitting from both symbolic and statistical
NER techniques, which have proven efficient
52
but not necessarily over the same type of data
and with different precision/recall tradeoff. NER
considers the surface form of entities; some
type disambiguation and name normalization
can follow the detection to improve the result
precision but do not provide referential infor-
mation, which can be useful in IE applications.
EL achieves the association of NER results with
uniquely identified entities, by relying on an
entity repository, available to the extraction
system and defined beforehand in order to serve
as a target for mention linking. Knowledge about
entities is gathered in a dedicated knowledge base
(KB) to evaluate each entity?s similarity to a given
context. After the task of EL was initiated with
Wikipedia-based works on entity disambiguation,
in particular by Cucerzan (2007) and Bunescu
and Pasca (2006), numerous systems have been
developed, encouraged by the TAC 2009 KB
population task (McNamee and Dang, 2009).
Most often in EL, Wikipedia serves both as an
entity repository (the set of articles referring to
entities) and as a KB about entities (derived from
Wikipedia infoboxes and articles which contain
text, metadata such as categories and hyperlinks).
Zhang et al (2010) show how Wikipedia, by
providing a large annotated corpus of linked
ambiguous entity mentions, pertains efficiently
to the EL task. Evaluated EL systems at TAC
report a top accuracy rate of 0.80 on English data
(McNamee et al, 2010).
Entities that are unknown to the reference
database, called out-of-base entities, are also con-
sidered by EL, when a given mention refers to
an entity absent from the available Wikipedia ar-
ticles. This is addressed by various methods,
such as setting a threshold of minimal similarity
for an entity selection (Bunescu and Pasca, 2006),
or training a separate binary classifier to judge
whether the returned top candidate is the actual
denotation (Zheng et al, 2010). Our approach
of this issue is closely related to the method of
Dredze et al in (2010), where the out-of-base en-
tity is considered as another entry to rank.
Our task differs from EL configurations out-
lined previously, in that its target is entity extrac-
tion from raw news wires from the news agency
Agence France Presse (AFP), and not only link-
ing relying on gold NER annotations: the input
of the linking system is the result of an auto-
matic NER step, which will produce errors of var-
ious kinds. In particular, spans erroneously de-
tected as NEs will have to be discarded by our EL
system. This case, which we call not-an-entity,
contitute an additional type of special situations,
together with out-of-base entities but specific to
our setting. This issue, as well as others of our
task specificities, will be discussed in this paper.
In particular, we use resources partially based on
Wikipedia but not limited to it, and we experiment
on the building of a domain specific entity KB in-
stead of Wikipedia.
Section 2 presents the resources used through-
out our system, namely an entity repository and
an entity KB acquired over a large corpus of news
wires, used in the final linking step. Section 3
states the principles on which the NER compo-
nents of our system relies, and introduces the two
existing NER modules used in our joint architec-
ture. The EL component and the methodology ap-
plied are presented in section 4. Section 5 illus-
trates this methodology with a number of experi-
ments and evaluation results.
2 Entity Resources
Our system relies on two large-scale resources
which are very different in nature:
? the entity database Aleda, automatically
extracted from the French Wikipedia and
Geonames;
? a knowledge base extracted from a large cor-
pus of AFP news wires, with distributional
and contextual information about automati-
cally detected entites.
2.1 Aleda
The Aleda entity repository2 is the result of an ex-
traction process from freely available resources
(Sagot and Stern, 2012). We used the French
Aleda databased, extracted the French Wikipedia3
and Geonames4. In its current development, it pro-
vides a generic and wide coverage entity resource
accessible via a database. Each entity in Aleda is
associated with a range of attributes, either refer-
ential (e.g., the type of the entity among Person,
Location, Organization and Company, the popu-
lation for a location or the gender of a person, etc.)
2Aleda is part of the Alexina project and freely available
at https://gforge.inria.fr/projects/alexina/.
3
www.fr.wikipedia.org
4
www.geonames.org
53
or formal, like the entity?s URI from Wikipedia or
Geonames; this enables to uniquely identify each
entry as a Web resource.
Moreover, a range of possible variants (men-
tions when used in textual content) are associ-
ated to entities entries. Aleda?s variants include
each entity?s canonical name, Geonames location
labels, Wikipedia redirection and disambiguation
pages aliases, as well as dynamically computed
variants for person names, based in particular on
their first/middle/last name structure. The French
Aleda used in this work comprises 870,000 entity
references, associated with 1,885,000 variants.
The main informative attributes assigned to
each entity in Aleda are listed and illustrated by
examples of entries in Tab. 1. The popularity at-
tribute is given by an approximation based on the
length of the entity?s article or the entity?s popu-
lation, from Wikipedia and Geonames entries re-
spectively. Table 1 also details the structure of
Aleda?s variants entries, each of them associated
with one or several entities in the base.
Unlike most EL systems, Wikipedia is not the
entity base we use in the present work; rather,
we rely on the autonomous Aleda database. The
collect of knowledge about entities and their us-
age in context will also differ in that our target
data are news wires, for which the adaptability of
Wikipedia can be questioned.
2.2 Knowledge Acquisition over AFP news
The linking process relies on knowledge about en-
tities, which can be acquired from their usage in
context and stored in a dedicated KB. AFP news
wires, like Wikipedia articles, have their own
structure and formal metadata: while Wikipedia
articles each have a title referring to an entity, ob-
ject or notion, a set of categories, hyperlinks, etc.,
AFP news wires have a headline and are tagged
with a subject (such as Politics or Culture) and
several keywords (such as cinema, inflation or
G8), as well as information about the date, time
and location of production. Moreover, the distri-
bution of entities over news wires can be expected
to be significantly different from Wikipedia, in
particular w.r.t. uniformity, since a small set of
entities forms the majority of occurrences. Our
particular context can thus justify the need for a
domain specific KB.
As opposed to Wikipedia where entities are
identifiable by hyperlinks, AFP corpora provide
no such indications. Wikipedia is in fact a corpus
where entity mentions are clearly and uniquely
linked, whereas this is what we aim at achiev-
ing over AFP?s raw textual data. The acquisi-
tion of domain specific knowledge about enti-
ties from AFP corpora must circumvent this lack
of indications. In this perspective we use an
implementation of a naive linker described in
(Stern and Sagot, 2010). For the main part, this
system is based on heuristics favoring popular en-
tities in cases of ambiguities. An evaluation of
this system showed good accuracy of entity link-
ing (0.90) over the subset of correctly detected en-
tity mentions:5 on the evaluation data, the result-
ing NER reached a precision of 0.86 and a recall
of 0.80. Therefore we rely on the good accuracy
of this system to identify entities in our corpus,
bearing in mind that it will however include cases
of false detections, while knowledge will not be
available on missed entities. It can be observed
that by doing so, we aim at performing a form of
co-training of a new system, based on supervised
machine learning. In particular, we aim at pro-
viding a more portable and systematic method for
EL than the heuristics-based naive linker which
is highly dependent on a particular NER system,
SXPipe/NP, described later on in section 3.2.
The knowledge acquisition was conducted over
a large corpus of news wires (200,000 news items
of the years 2009, 2010 and part of 2011). For
each occurrence of an entity identified as such by
the naive linker, the following features are col-
lected, updated and stored in the KB at the en-
tity level: (i) entity total occurrences and occur-
rences with a particular mention; (ii) entity oc-
currence with a news item topics and keywords,
most salient words, date and location; (iii) entity
co-occurrence with other entity mentions in the
news item. These features are collected for both
entities identified by the naive linker as Aleda?s
entities and mentions recognized by NER pat-
tern based rules; the latter account for out-of-
base entities, approximated by a cluster of all
mentions whose normalization returns the same
string. For instance, if the mentions John Smith
and J. Smith were detected in a document but not
linked to an entity in Aleda, it would be assumed
5This subset is defined by a strict span and type correct
detection, and among the sole entities for which a match in
Aleda or outside of it was identified; the evaluation data is
presented in section 5.1.
54
Entities
ID Type CanonicalName Popularity URI
20013 Loc Kingdom of Spain 46M geon:2510769
10063 Per Michael Jordan 245 wp:Michael Jordan
20056 Loc Orange (California) 136K geon:5379513
10039 Comp Orange 90 wp:Orange (entreprise)
Variants
ID Variant FirstName MidName LastName
20013 Espagne ? ? ?
10063 Jordan ? ? Jordan
10029 George Walker Bush George Walker Bush
10039 Orange ? ? ?
20056 Orange ? ? ?
Table 1: Structure of Entities Entries and Variants in Aleda
that they co-refer to an entity whose normalized
name would be John Smith; this anonymous en-
tity would therefore be stored and identified via
this normalized name in the KB, along with its oc-
currence information.
3 NER Component
3.1 Principles
One challenging subtask of NER is the correct de-
tection of entity mentions spans among several
ambiguous readings of a segment. The other usual
subtask of NER consists in the labeling or classi-
fication of each identified mention with a type; in
our system, this functionality is used as an indica-
tion rather than a final attribute of the denoted en-
tity. The type assigned to each mention will in the
end be the one associated with the matching en-
tity. The segment Paris Hilton can for instance be
split in two consecutive entity mentions, Paris and
Hilton, or be read as a single one. Whether one
reading or the other is more likely can be inferred
from knowledge about entities possibly denoted
by each of these three mentions: depending on the
considered document?s topic, it can be more prob-
able for this segment to be read as the mention
Paris Hilton, denoting the celebrity, rather than
the sequence of two mentions denoting the cap-
ital of France and the hotel company. Based on
this consideration, our system relies on the ability
of the NER module to preserve multiple readings
in its output, in order to postpone to the linker the
appropriate decisions for ambiguous cases. Two
NER systems fitted with this ability are used in our
architecture.
Figure 1: Ambiguous NER output for the segment
Paris Hilton in SXPipe/NP
3.2 Symbolic NER: SXPipe/NP
NP is part of the SXPipe surface processing chain
(Sagot and Boullier, 2008). It is based on a se-
ries of recognition rules and on a large coverage
lexicon of possible entity variants, derived from
the Aleda entity repository presented in section
2.1. As an SXPipe component, NP formalizes the
text input in the form of directed acyclic graphs
(DAGs), in which each possible entity mention
is represented as a distinct transition, as illus-
trated in Figure 1. Possible mentions are labeled
with types among Person, Location, Organization
and Company, based on the information available
about the entity variant in Aleda and on the type
of the rule applied for the recognition.
Figure 1 also shows how an alternative transi-
tion is added to each mention reading of a seg-
ment, in order to account for a possible non-entity
reading (i.e., for a false match returned by the
NER module). When evaluating the adequacy of
each reading, the following EL module will in
fact consider a special not-an-entity candidate as
a possible match for each mention, and select it
as the most probable if competing entity readings
prove insufficiently adequate w.r.t. the considered
context.
55
3.3 Statistical NER: LIANE
The statistical NER system LIANE
(Bechet and Charton, 2010) is based on (i) a
generative HMM-based process used to predict
part-of-speech and semantic labels among Per-
son, Location, Organization and Product for each
input word6, and (ii) a discriminative CRF-based
process to determine the entity mentions? spans
and overall type. The HMM and CRF models
are learnt over the ESTER corpus, consisting in
several hundreds of hours of transcribed radio
broadcast (Galliano et al, 2009), annotated with
the BIO format (table 2). The output of LIANE
investiture NFS O
aujourd?hui ADV B-TIME
a` PREPADE O
Bamako LOC B-LOC
Mali LOC B-LOC
Table 2: BIO annotation for LIANE training
consists in an n-best lists of possible entity
mentions, along with a confidence score assigned
to each result. Therefore it also provides several
readings of some text segments, with alternatives
of entity mention readings.
As shown in (Bechet and Charton, 2010), the
learning model of LIANE makes it particularly
robust to difficult conditions such as non capital-
ization and allows for a good recall rate on various
types of data. This is in opposition with manually
handcrafted systems such as SXPipe/NP, which
can reach high precision rates over the develop-
ment data but prove less robust otherwise. These
considerations, as well as the benefits of a coop-
erations between these two types of systems are
explored in (Be?chet et al, 2011).
By coupling LIANE and SXPipe/NP to perform
the NER step of our architecture, we expect to
benefit from each system?s best predictions and
improving the precision and recall rates. This
is achieved by not enforcing disambiguation of
spans and types at the NER level but by transfer-
ring this possible source of errors to the linking
step, which will rely on entity knowledge rather
than mere surface forms to determine the best
readings, along with the association of mentions
with entity references.
6For the purpose of type consistency across both NER
modules, the NP type Company is merged with Organiza-
tion, and the LIANE mentions typed as Product are ignored
since they are not yet supported by the overall architecture.
Figure 2: Possible readings of the segment Paris
Hilton and ordered candidates
4 Linking Component
4.1 Methodology for Best Reading Selection
As previously outlined, the purpose of our joint
architecture is to infer best entity readings from
contextual similarity between entities and docu-
ments rather than at the surface level during NER.
The linking component will therefore process am-
biguous NER outputs in the following way, illus-
trated by Fig. 2.
1. For each mention returned by the NER mod-
ule, we aim at finding the best fitting entity
w.r.t. the context of the mention occurrence,
i.e., at the document level. This results in
a list of candidate entities associated with
each mention. This candidates set alays in-
cludes the not-an-entity candidate in order to
account for possible false matches returned
by the NER modules.
2. The list of candidates is ordered using a
pointwise ranking model, based on the max-
imum entropy classifier megam.7 The best
scored candidate is returned as a match for
the mention; it can be either an entity present
in Aleda, i.e., a known entity, or an anony-
mous entity, seen during the KB acquisition
but not resolved to a known reference and
identified by a normalized name, or the spe-
cial not-an-entity candidate, which discards
the given mention as an entity denotation.
3. Each reading is assigned a score depending
on the best candidates? scores in the reading.
The key steps of this process are the selection
of candidates for each mention, which must reach
a sufficient recall in order to ensure the reference
resolution, and the building of the feature vec-
tor for each mention/entity pair, which will be
evaluated by the candidate ranker to return the
7http://www.cs.utah.edu/
?
hal/megam/
56
most adequate entity as a match for the mention.
Throughout this process, the issues usually raised
by EL must be considered, in particular the ability
for the model to learn cases of out-of-base enti-
ties, which our system addresses by forming a set
of candidates not only from the entity reference
base (i.e., Aleda), but also from the dedicated KB
where anonymous entities are also collected. Fur-
thermore, unlike the general configuration of EL
tasks, such as the TAC KB population task (sec-
tion 1.2), our input data does not consist in men-
tions to be linked but in multiple possibilities of
mention readings, which adds to our particular
case the need to identify false matches among the
queries made to the linker module.
4.2 Candidates Selection
For each mention detected in the NER output, the
mention string or variant is sent as a query to
the Aleda database. Entity entries associated with
the given variant are returned as candidates. The
set of retrieved entities, possibly empty, consti-
tutes the candidate set for the mention. Because
the knowledge acquisition included the extraction
of unreferenced entities identified by normalized
names (section 2.2), we can send the normaliza-
tion of the mention as an additional query to our
KB. If a corresponding anonymous entity is re-
turned, we can create an anonymous candidate
and add it to the candidate set. Anonymous candi-
dates account for the possibility of an out-of-base
entity denoted by the given mention, with respec-
tively some and no information about the potential
entity they might stand for. Finally, the set is aug-
mented with the special not-an-entity candidate.
4.3 Features for Candidates Ranking
For each pair formed by the considered mention
and each entity from the candidate set, we com-
pute a feature vector which will be used by our
model for assessing the probability that it repre-
sents a correct mention/entity linking. The vec-
tor contains attributes pertaining to the mention,
the candidate and the document themselves, and
to the relations existing between them.
Entity attributes Entity attributes present in
Aleda and the KB are used as features: Aleda pro-
vides the entity type, a popularity indication and
the number of variants associated with the entity.
We retrieve from the KB the entity frequency over
the corpus used for knowledge acquisition.
Mention attributes At the mention level, the
feature set considers the absence or presence of
the mention as a variant in Aleda (for any en-
tity), its occurrence frequency in the document,
and whether similar variants, possibly indicating
name variation of the same entity, are present in
the document (similar variants can have a string
equal to the mention?s string, longer or shorter
than the mention?s string, included in the men-
tion?s string or including it). In the case of a
mention returned by LIANE, the associated con-
fidence score is also included in the feature set.
Entity/mention relation The comparison be-
tween the surface form of the entity?s canonical
name and the mention gives a similarity rate fea-
ture. Also considered as features are the relative
occurrence frequency of the entity w.r.t. the whole
candidate set, the existence of the mention as a
variant for the entity in Aleda, the presence of
the candidate?s type (retrieved from Aleda) in the
possible mention types provided by the NER. The
KB indicates frequency of its occurrences with the
considered mention, which adds another feature.
Document/entity similarity Document metadata
(in particular topics and keywords) are inherited
by the mention and can thus characterize the en-
tity/mention pair. Equivalent information was col-
lected for entities and stored in the KB, which al-
lows to compute a cosine similarity between the
document and the candidate. Moreover, the most
salient words of the document are compared to the
ones most frequently associated with the entity in
the KB. Several atomic and combined features are
derived from these similarity measures.
Other features pertain to the NER output con-
figuration, as well as possible false matches:
NER combined information One of the two
available NER modules is selected as the base
provider for entity mentions. For each mention
which is also returned by the second NER mod-
ule, a feature is instanciated accordingly.
Non-entity features In order to predict cases of
not-an-entity readings of a mention, we use a
generic lexicon of French forms (Sagot, 2010)
where we check for the existence of the mention?s
variant, both with and without capitalization. If
the mention?s variant is the first word of the sen-
tence, this information is added as a feature.
These features represent attributes of the en-
tity/mention pair which can either have a boolean
value (such as variant presence or absence in
57
Aleda) or range throughout numerical values
(e.g., entity frequencies vary from 0 to 201,599).
In the latter case, values are discretized. All fea-
tures in our model are therefore boolean.
4.4 Best Candidate Selection
Given the feature vector instanciated for an (can-
didate entity, mention) pair, our model assigns it a
score. All candidates in the subset are then ranked
accordingly and the first candidate is returned as
the match for the current mention/entity linking.
Anonymous and not-an-entity candidates, as de-
fined earlier and accounting respectively for po-
tential out-of-base entity linking and NER false
matches, are included in this ranking process.
4.5 Ranking of Readings
The last step of our task consists in the ranking
of multiple readings and has yet to be achieved in
order to obtain an output where entity mentions
are linked to adequate entities. In the case of a
reading consisting in a single transition, i.e., a sin-
gle mention, the score is equal to the best candi-
date?s score. In case of multiple transitions and
mentions, the score is the minimum among the
best candidates? scores, which makes a low entity
match probability in a mention sequence penaliz-
ing for the whole reading. Cases of false matches
returned by the NER module can therefore be dis-
carded as such in this step, if an overall non-entity
reading of the whole path receives a higher score
than the other entity predictions.
5 Experiments and Evaluation
5.1 Training and Evaluation Data
We use a gold corpus of 96 AFP news items in-
tended for both NER and EL purposes: the manual
annotation includes mention boundaries as well as
an entity identifier for each mention, correspond-
ing to an Aleda entry when present or the normal-
ized name of the entity otherwise. This allows for
the model learning to take into account cases of
out-of-base entities. This corpus contains 1,476
mentions, 437 distinct Aleda?s entries and 173 en-
tities absent from Aleda. All news items in this
corpus are dated May and June 2009.
In order for the model to learn from cases of
not-an-entity, the training examples were aug-
mented with false matches from the NER step, as-
sociated with this special candidate and the pos-
itive class prediction, while other possible candi-
dates were associated with the negative class. Us-
ing a 10-fold cross-validation, we used this corpus
for both training and evaluation of our joint NER
and EL system.
It should be observed that the learning step con-
cerns the ranking of candidates for a given men-
tion and context, while the final purpose of our
system is the ranking of multiple readings of sen-
tences, which takes place after the application of
our ranking model for mention candidates. Thus
our system is evaluated according to its ability to
choose the right reading, considering both NER re-
call and precision and EL accuracy, and not only
the latter.
5.2 Task Specificities
As outlined in section 1.2, the input for the stan-
dard EL task consists in sets of entity mentions
from a number of documents, sent as queries to a
linking system. Our current task differs in that we
aim at both the extraction and the linking of enti-
ties in our target corpus, which consists in unan-
notated news wires. Therefore, the results of our
system are comparable to previous work when
considering a setting where the NER output is in
fact the gold annotation of our evaluation data,
i.e., when all mention queries should be linked to
an entity. Without modifying the parameters of
our system (i.e., no deactivation of false matches
predictions), we obtain an accuracy of 0.76, in
comparison with a TAC top accuracy of 0.80 and
a median accuracy of 0.70 on English data.8
It is important to observe that our data con-
sists only in journalistic content, as opposed to the
TAC dataset which included various types of cor-
pora. This difference can lead to unequally diffi-
culty levels w.r.t. the EL task, since NER and EL
in journalistic texts, and in particular news wires,
tend to be easier than on other types of corpora.
This comes among other things from the fact that
a small number of popular entities constitute the
majority of NE mention occurrences.
In most systems, EL is performed over noisy
8As explained previously, these figures, as well as the
ones presented later on, cannot be compared with the 0.90
score obtained by the naive linker which we used for the en-
tity KB acquisition. This score is obtained only on mentions
identified by the SXPipe/NP system with the correct span and
type, whereas our system does not consider the mention type
as a contraint for the linking process, and on correct identifi-
cation of a match in or outside of Aleda.
58
Setting NER EL Joint NER+EL
Precision Recall f-measure Accuracy Precision Recall f-measure
SXPipe/NP 0.849 0.768 0.806 0.871 0.669 0.740 0.702
LIANE 0.786 0.891 0.835 0.820 0.730 0.645 0.685
SXPipe/NP- NL 0.775 0.726 0.750 0.875 0.635 0.678 0.656
LIANE- NL 0.782 0.886 0.831 0.818 0.725 0.640 0.680
SXPipe/NP & 2 0.812 0.747 0.778 0.869 0.649 0.705 0.676
LIANE & SXPipe/NP 0.803 0.776 0.789 0.859 0.667 0.689 0.678
Table 3: Joint NER and EL results. Each EL accuracy covers a different set of correctly detected mentions
NER output and participates to the final decisions
about NEs extractions. Therefore the ability of
our system to correctly detect entity mentions in
news content is estimated by computing its pre-
cision, recall and f-measure.9 The EL accuracy,
i.e., the rate of correctly linked mentions, is mea-
sured over the subset of mentions whose reading
was adequately selected by the final ranking. The
evaluation of our system has been conducted over
the corpus described previously with settings pre-
sented in the next section.
5.3 Settings and results
We used each of the two available NER modules
as a provider for entity mentions, either on its
own or together with the second system, used
as an indicator. For each of these settings, we
tried a modified setting in which the prediction
of the naive linker (NL) used to build the en-
tity KB (section 2.2) was added as a feature to
each mention/candidate pair (settings SXPipe/NP-
NL and LIANE-NL). These experiments? results
are reported in Table 3 and are given in terms of:
? NER precision, recall and f-measure;
? EL accuracy over correctly recognized enti-
ties; therefore, the different figures in col-
umn EL Accuracy are not directly compara-
ble to one another, as they are not obtained
over the same set of mentions;
? joint NER+EL precision, recall and f-
measure; the precision/recall is computed as
the product of the NER precision/recall by the
EL accuracy.
9Only mention boundaries are considered for NER evalu-
ation, while other settings require correct type identification
for validating a fully correct detection. In our case, NER is
not a final step, and entity typing is derived from the entity
linking result.
As expected, SXPipe/NP performs better as far
as NER precision is concerned, and LIANE per-
forms better as far as NER recall is concerned.
However, the way we implemented hybridation
at the NER level does not seem to bring improve-
ments. Using the output of the naive linker as a
feature leads to similar or slightly lower NER pre-
cision and recall. Finally, it is difficult to draw
clear-cut comparative conclusions at this stage
concerning the joint NER +EL task.
6 Conclusion and Future Work
We have described and evaluated various settings
for a joint NER and EL system which relies on the
NER systems SXPipe/NP and LIANE for the NER
step. The EL step relies on a hybrid model, i.e., a
statistical model trained on a manually annotated
corpus. It uses features extracted from a large cor-
pus automatically annotated and where entity dis-
ambiguations and matches were computed using
a basic heuristic tool. The results given in the pre-
vious section show that the joint model allows for
good NER results over French data. The impact of
the hybridation of the two NER modules over the
EL task should be further evaluated. In particu-
lar, we should investigate the situations where an
mention was incorrectly detected (e.g., the span is
not fully correct) although the EL module linked it
with the correct entity. Moreover, a detailed eval-
uation of out-of-base linkings vs. linking in Aleda
remains to be performed.
In the future, we aim at exploring various addi-
tional features in the EL system, in particular more
combinations of the current features. The adapta-
tion of our learning model to NER combinations
should also be improved. Finally, a larger set of
training data should be considered. This shall be-
come possible with the recent manual annotation
of a half-million word French journalistic corpus.
59
References
F. Bechet and E Charton. 2010. Unsupervised knowl-
edge acquisition for extracting named entities from
speech. In 2010 IEEE International Conference on
Acoustics, Speech and Signal Processing.
R. Bunescu and M. Pasca. 2006. Using encyclope-
dic knowledge for named entity disambiguation. In
Proceedings of EACL, volume 6, pages 9?16.
F. Be?chet, B. Sagot, and R. Stern. 2011.
Coope?ration de me?thodes statistiques et sym-
boliques pour l?adaptation non-supervise?e d?un
syste`me d?e?tiquetage en entite?s nomme?es. In Actes
de la Confe?rence TALN 2011, Montpellier, France.
S. Cucerzan. 2007. Large-scale named entity disam-
biguation based on wikipedia data. In Proceedings
of EMNLP-CoNLL, volume 2007, pages 708?716.
G. Doddington, A. Mitchell, M. Przybocki,
L. Ramshaw, S. Strassel, and R. Weischedel.
2004. The automatic content extraction (ace)
program-tasks, data, and evaluation. In Proceed-
ings of LREC - Volume 4, pages 837?840.
M. Dredze, P. McNamee, D. Rao, A. Gerber, and
T. Finin. 2010. Entity disambiguation for knowl-
edge base population. In Proceedings of the 23rd
International Conference on Computational Lin-
guistics, pages 277?285.
S. Galliano, G. Gravier, and L. Chaubard. 2009. The
Ester 2 Evaluation Campaign for the Rich Tran-
scription of French Radio Broadcasts. In Inter-
speech 2009.
E. Marsh and D. Perzanowski. 1998. Muc-7 eval-
uation of ie technology: Overview of results. In
Proceedings of the Seventh Message Understanding
Conference (MUC-7) - Volume 20.
P. McNamee and H.T. Dang. 2009. Overview of the
tac 2009 knowledge base population track. In Text
Analysis Conference (TAC).
P. McNamee, H.T. Dang, H. Simpson, P. Schone, and
S.M. Strassel. 2010. An evaluation of technologies
for knowledge base population. Proc. LREC2010.
B. Sagot and P. Boullier. 2008. SXPipe 2 : ar-
chitecture pour le traitement pre?syntaxique de cor-
pus bruts. Traitement Automatique des Langues
(T.A.L.), 49(2):155?188.
B. Sagot and R. Stern. 2012. Aleda, a free large-
scale entity database for French. In Proceedings of
LREC. To appear.
B. Sagot. 2010. The Lefff , a freely available and
large-coverage morphological and syntactic lexicon
for French. In Proceedings of the 7th Language
Resources and Evaluation Conference (LREC?10),
Vallette, Malta.
R. Stern and B. Sagot. 2010. De?tection et re?solution
d?entite?s nomme?es dans des de?pe?ches d?agence.
In Actes de la Confe?rence TALN 2010, Montre?al,
Canada.
E. F. Tjong Kim Sang and F. De Meulder. 2003. In-
troduction to the conll-2003 shared task: Language-
independent named entity recognition. In Proceed-
ings of CoNLL, pages 142?147, Edmonton, Canada.
W. Zhang, J. Su, C.L. Tan, and W.T. Wang. 2010. En-
tity linking leveraging: automatically generated an-
notation. In Proceedings of the 23rd International
Conference on Computational Linguistics, pages
1290?1298.
Z. Zheng, F. Li, M. Huang, and X. Zhu. 2010. Learn-
ing to link entities with knowledge base. In Human
Language Technologies: The 2010 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 483?491.
60
