Proceedings of the ACL-IJCNLP 2009 Student Research Workshop, pages 72?80,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
Data Cleaning for Word Alignment
Tsuyoshi Okita
CNGL / School of Computing
Dublin City University, Glasnevin, Dublin 9
tokita@computing.dcu.ie
Abstract
Parallel corpora are made by human be-
ings. However, as an MT system is an
aggregation of state-of-the-art NLP tech-
nologies without any intervention of hu-
man beings, it is unavoidable that quite a
few sentence pairs are beyond its analy-
sis and that will therefore not contribute
to the system. Furthermore, they in turn
may act against our objectives to make the
overall performance worse. Possible unfa-
vorable items are n : m mapping objects,
such as paraphrases, non-literal transla-
tions, and multiword expressions. This
paper presents a pre-processing method
which detects such unfavorable items be-
fore supplying them to the word aligner
under the assumption that their frequency
is low, such as below 5 percent. We show
an improvement of Bleu score from 28.0
to 31.4 in English-Spanish and from 16.9
to 22.1 in German-English.
1 Introduction
Phrase alignment (Marcu and Wong, 02) has re-
cently attracted researchers in its theory, although
it remains in infancy in its practice. However, a
phrase extraction heuristic such as grow-diag-final
(Koehn et al, 05; Och and Ney, 03), which is a sin-
gle difference between word-based SMT (Brown
et al, 93) and phrase-based SMT (Koehn et al,
03) where we construct word-based SMT by bi-
directional word alignment, is nowadays consid-
ered to be a key process which leads to an over-
all improvement of MT systems. However, tech-
nically, this phrase extraction process after word
alignment is known to have at least two limita-
tions: 1) the objectives of uni-directional word
alignment is limited only in 1 : n mappings and
2) an atomic unit of phrase pair used by phrase ex-
traction is thus basically restricted in 1 : n or n : 1
with small exceptions.
Firstly, the posterior-based approach (Liang,
06) looks at the posterior probability and partially
delays the alignment decision. However, this ap-
proach does not have any extension in its 1 : n
uni-directional mappings in its word alignment.
Secondly, the aforementioned phrase alignment
(Marcu and Wong, 02) considers the n : m map-
ping directly bilingually generated by some con-
cepts without word alignment. However, this ap-
proach has severe computational complexity prob-
lems. Thirdly, linguistic motivated phrases, such
as a tree aligner (Tinsley et al, 06), provides n : m
mappings using some information of parsing re-
sults. However, as the approach runs somewhat in
a reverse direction to ours, we omit it from the dis-
cussion. Hence, this paper will seek for the meth-
ods that are different from those approaches and
whose computational cost is cheap.
n : m mappings in our discussion include para-
phrases (Callison-Burch, 07; Lin and Pantel, 01),
non-literal translations (Imamura et al, 03), mul-
tiword expressions (Lambert and Banchs, 05), and
some other noise in one side of a translation pair
(from now on, we call these ?outliers?, meaning
that these are not systematic noise). One com-
mon characteristic of these n : m mappings is
that they tend to be so flexible that even an ex-
haustive list by human beings tends to be incom-
plete (Lin and Pantel, 01). There are two cases
which we should like to distinguish: when we use
external resources and when we do not. For ex-
ample, Quirk et al employ external resources by
drawing pairs of English sentences from a compa-
rable corpus (Quirk et al, 04), while Bannard and
Callison-Burch (Bannard and Callison-Burch, 05)
identified English paraphrases by pivoting through
phrases in another language. However, in this pa-
per our interest is rather the case when our re-
sources are limited within our parallel corpus.
72
Imamura et al (Imamura et al, 03), on the other
hand, do not use external resources and present a
method based on literalness measure called TCR
(Translation Correspondence Rate). Let us de-
fine literal translation as a word-to-word transla-
tion, and non-literal translation as a non word-to-
word translation. Literalness is defined as a de-
gree of literal translation. Literalness measure of
Imamura et al is trained from a parallel corpus
using word aligned results, and then sentences are
selected which should either be translated by a ?lit-
eral translation? decoder or by a ?non-literal trans-
lation? decoder based on this literalness measure.
Apparently, their definition of literalness measure
is designed to be high recall since this measure
incorporates all the possible correspondence pairs
(via realizability of lexical mappings) rather than
all the possible true positives (via realizability of
sentences). Adding to this, the notion of literal
translation may be broader than this. For exam-
ple, literal translation of ?C?est la vie.? in French
is ?That?s life.? or ?It is the life.? in English.
If literal translation can not convey the original
meaning correctly, non-literal translation can be
applied: ?This is just the way life is.?, ?That?s how
things happen.?, ?Love story.?, and so forth. Non-
literal translation preserves the original meaning1
as much as possible, ignoring the exact word-to-
word correspondence. As is indicated by this ex-
ample, the choice of literal translation or non-
literal translation seems rather a matter of trans-
lator preference.
This paper presents a pre-processing method us-
ing the alternative literalness score aiming for high
precision. We assume that the percentages of these
n : m mappings are relatively low. Finally, it
turned out that if we focus on outlier ratio, this
method becomes a well-known sentence cleaning
approach. We refer to this in Section 5.
This paper is organized as follows. Section 2
outlines the 1 : n characteristics of word align-
ment by IBM Model 4. Section 3 reviews an
atomic unit of phrase extraction. Section 4 ex-
plains our Good Points Algorithm. Experimen-
tal results are presented in Section 5. Section 6
discusses a sentence cleaning algorithm. Section
7 concludes and provides avenues for further re-
search.
1Dictionary goes as follows: something that you say when
something happens that you do not like but which you have
to accept because you cannot change it [Cambridge Idioms
Dictionary 2nd Edition, 06].
C
BA
D
Figure 1: Figures A and C show the results of
word alignment for DE-EN where outliers de-
tected by Algorithm 1 are shown in blue at the bot-
tom. We check all the alignment cept pairs in the
training corpus inspecting so-called A3 final files
by type of alignment from 1:1 to 1:13 (or NULL
alignment). It is noted that outliers are miniscule
in A and C because each count is only 3 percent.
Most of them are NULL alignment or 1:1 align-
ment, while there are small numbers of alignments
with 1:3 and 1:4 (up to 1:13 in the DE-EN direc-
tion in Figure A). In Figure C, 1:11 is the greatest.
Figure B and D show the ratio of outliers over all
the counts. Figure B shows that in the case of 1:10
alignments, 1/2 of the alignments are considered
to be outliers by Algorithm 1, while 100 percent
of alignment from 1:11 to 1:13 are considered to
be outliers (false negative). Figure D shows that in
the case of EN-DE, most of the outlier ratios are
less than 20 percent.
2 1 : n Word Alignment
Our discussion of uni-directional alignments of
word alignment is limited to IBM Model 4.
Definition 1 (Word alignment task) Let e
i
be
the i-th sentence in target language, e?
i,j
be the j-
th word in i-th sentence, and e?
i
be the i-th word in
parallel corpus (Similarly for f
i
,
?
f
i,j
, and ?f
i
). Let
|e
i
| be a sentence length of e
i
, and similarly for
|f
i
|. We are given a pair of sentence aligned bilin-
gual texts (f
1
, e
1
), . . . , (f
n
, e
n
) ? X ? Y , where
f
i
= (
?
f
i,1
, . . . ,
?
f
i,|f
i
|
) and e
i
= (e?
i,1
, . . . , e?
i,|e
i
|
).
It is noted that e
i
and f
i
may include more than
one sentence. The task of word alignment is to
find a lexical translation probability p
?
f
i
: e?
i
?
p
?
f
j
(e?
i
) such that ?p
?
f
j
(e?
i
) = 1 and ?e?
i
: 0 ?
p
?
f
j
(e?
i
) ? 1 (It is noted that some models such
73
to my regret i cannot go today .
i am sorry that i cannot visit today .
it is a pity that i cannot go today .
i am sorry that i cannot visit today .
it is a pity that i cannot go today .
sorry , today i will not be available
Source Language
GIZA++ alignment results for IBM Model 4
i NULL 0.667
cannot available 0.272
it am 1
is am 1
sorry go 0.667
, go 1
that regret 0.25
cannot regret 0.18
visit regret 1
regret not 1
be pity 1
available pity 1
cannot sorry 0.55
go sorry 0.667
am to 1
sorry to 0.33
to , 1
my , 1
will is 1
not is 1
a that 1
pity that 1
today . 1
. . 1
i cannot 0.33
that cannot 0.75
Target Language
to my regret i cannot go today .sorry , today i will not be available
Figure 2: Example shows an example alignment
of paraphrases in a monolingual case. Source and
target use the same set of sentences. Results show
that only the matching between the colon is cor-
rect3.
as IBM Model 3 and 4 have deficiency problems).
It is noted that there may be several words in
source language and target language which do not
map to any words, which are called unaligned (or
null aligned) words. Triples ( ?f
i
, e?
i
, p
?
f
i
(e?
1
)) (or
(
?
f
i
, e?
i
,? log
10
p
?
f
i
(e?
1
))) are called T-tables.
As the above definition shows, the purpose of
the word alignment task is to obtain a lexical
translation probability p( ?f
i
|e?
i
), which is a 1 : n
uni-directional word alignment. The initial idea
underlying the IBM Models, consisting of five
distinctive models, is that it introduces an align-
ment function a(j|i), or alternatively the distor-
tion function d(j|i) or d(j ??
i
), when the task is
viewed as a missing value problem, where i and j
denote the position of a cept in a sentence and ?
i
denotes the center of a cept. d(j|i) denotes a dis-
tortion of the absolute position, while d(j??
i
) de-
notes the distortion of relative position. Then this
missing value problem can be solved by EM algo-
rithms : E-step is to take expectation of all the pos-
sible alignments and M-step is to estimate maxi-
mum likelihood of parameters by maximizing the
expected likelihood obtained in the E-step. The
second idea of IBM Models is in the mechanism
of fertility and a NULL insertion, which makes the
performance of IBM Models competitive. Fertility
and a NULL insertion is used to adjust the length
3It is noted that there might be a criticism that this is not a
fair comparison because we do not have sufficient data. Un-
der a transductive setting (where we can access the test data),
we believe that our statement is valid. Considering the nature
of the 1 : n mapping, it would be quite lucky if we obtain
n : m mapping after phrase extraction (Our focus is not on
the incorrect probability, but rather on the incorrect match-
ing.)
n when the length of the source sentence is differ-
ent from this n. Fertility is a mechanism to aug-
ment one source word into several source words
or delete a source word, while a NULL insertion
is a mechanism of generating several words from
blank words. Fertility uses a conditional probabil-
ity depending only on the lexicon. For example,
the length of ?today? can be conditioned only on
the lexicon ?today?.
As is already mentioned, the resulting align-
ments are 1 : n (shown in the upper figure in
Figure 1). For DE-EN News Commentary cor-
pus, most of the alignments fall in either 1:1 map-
ping or NULL mappings whereas small numbers
are 1:2 mappings and miniscule numbers are from
1:3 to 1:13. However, this 1 : n nature of word
alignment will cause problems if we encounter
n : m mapping objects, such as a paraphrase, non-
literal translation, or multiword expression. Figure
2 shows such difficulties where we show a mono-
lingual paraphrase. Without loss of generality this
can be easily extended to bilingual paraphrases. In
this case, results of word alignment are completely
wrong, with the exception of the example consist-
ing of a colon. Although these paraphrases, non-
literal translations, and multiword expressions do
not always become outliers, they may face the
potential danger of producing the incorrect word
alignments with incorrect probabilities.
3 Phrase Extraction and Atomic Unit of
Phrases
The phrase extraction is a process to exploit
phrases for a given bi-directional word alignment
(Koehn et al, 05; Och and Ney, 03). If we focus on
its generative process, this would become as fol-
lows: 1) add intersection of two word alignments
as an alignment point, 2) add new alignment points
that exist in the union with the constraint that a
new alignment point connects at least one previ-
ously unaligned word, 3) check the unaligned row
(or column) as unaligned row (or column, respec-
tively), 4) if n alignment points are contiguous in
horizontal (or vertical) direction we consider that
this is a contiguous 1 : n (or n : 1) phrase pair
(let us call these type I phrase pairs), 5) if a neigh-
borhood of a contiguous 1 : n phrase pair is (an)
unaligned row(s) or (an) unaligned column(s) we
grow this region (with consistency constraint) (let
us call these type II phrase pair), and 6) we con-
sider all the diagonal combinations of type I and
74
type II phrase pairs generatively.
The atomic unit of type I phrase pairs is 1 : n
or n : 1, while that of type II phrase pairs is n : m
if unaligned row(s) and column(s) exist in neigh-
borhood. So, whether they form a n : m map-
ping or not depends on the existence of unaligned
row(s) and column(s). And at the same time, n or
m should be restricted to a small value. There is
a chance that a n : m phrase pair can be created
in this way. This is because around one third of
word alignments, which is quite a large figure, are
1 : 0 as is shown in Figure 1. Nevertheless, our
concern is if the results of word alignment is very
low quality, e.g. similar to the situation depicted
in Figure 2, this mechanism will not work. Fur-
thermore, this mechanism is only restricted in the
unaligned row(s) and column(s).
4 Our Approach: Good Points Approach
Our approach aims at removing outliers by the lit-
eralness score, which we defined in Section 1, be-
tween a pair of sentences. Sentence pairs with low
literalness score should be removed. Following
two propositions are the theory behind this. Let
a word-based MT system be M
WB
and a phrase-
based MT system be M
PB
. Then,
Proposition 1 Under an ideal MT system M
PB
, a
paraphrase is an inlier (or realizable), and
Proposition 2 Under an ideal MT system M
WB
,
a paraphrase is an outlier (or not realizable).
Based on these propositions, we could assume
that if we measure the literalness score under a
word-based MT M
WB
we will be able to deter-
mine the degree of outlier-ness whatever the mea-
sure we use for it. Hence, what we should do is,
initially, to score it under a word-based MT M
WB
using Bleu, for example. (Later we replace it with
a variant of Bleu, i.e. cumulative n-gram score).
However, despite Proposition 1, our MT system
at hand is unfortunately not ideal. What we can
currently do is the following: if we witness bad
sentence-based scores in word-based MT, we can
consider our MT system failing to incorporating a
n : m mapping object for those sentences. Later
in our revised version, we use both of word-based
MT and phrase-based MT. The summary of our
first approach becomes as follows: 1) employing
the mechanism of word-based MT trained on the
same parallel corpus, we measure the literalness
between a pair of sentences, 2) we use the variants
Figure 3: Left figure shows sentence-based Bleu
score of word-based SMT and right figure shows
that of phrase-based SMT. Each row shows the cu-
mulative n-gram score (n = 1,2,3,4) and we use
News Commentary parallel corpus (DE-EN).
Figure 4: Each row shows Bleu, NIST, and TER,
while each column shows different language pairs
(EN-ES, EN-DE and FR-DE). These figures show
the scores of all the training sentences by the
word-based SMT system. In the row for Bleu,
note that the area of rectangle shows the num-
ber of sentence pairs whose Bleu scores are zero.
(There are a lot of sentence pairs whose Bleu score
are zero: if we draw without en-folding the coor-
dinate, these heights reach to 25,000 to 30,000.)
There is a smooth probability distribution in the
middle, while there are two non-smoothed connec-
tions at 1.0 and 0.0. Notice there is a small num-
ber of sentences whose score is 1.0. In the middle
row for NIST score, similarly, there is a smooth
probability distribution in the middle and we have
a non-smoothed connection at 0.0. In the bottom
row for TER score, the 0.0 is the best score unlike
Bleu and NIST, and we omit scores more than 2.5
in these figures. (The maximum was 27.0.)
75
of Bleu score as the measure of literalness, and
3) based on this score, we reduce the sentences in
parallel corpus. Our algorithm is as follows:
Algorithm 1 Good Points Algorithm
Step 1: Train word-based MT.
Step 2: Translate all training sentences by the
above trained word-based MT decoder.
Step 3: Obtain the cumulative X-gram score for
each pair of sentences where X is 4, 3, 2, and 1.
Step 4: By the threshold described in Table 1,
we produce new reduced parallel corpus.
(Step 5: Do the whole procedure of phrase-
based SMT using the reduced parallel corpus
which we obtain from Step 1 to 4.)
conf A1 A2 A3 A4
Ours 0.05 0.05 0.1 0.2
1 0.1
2 0.1 0.2
3 0.1 0.2 0.3 0.5
4 0.05 0.1 0.2 0.4
5 0.22 0.3 0.4 0.6
6 0.25 0.4 0.5 0.7
7 0.2 0.4 0.5 0.8
8 0.6
Table 1: Table shows our threshold where A1, A2,
A3, and A4 correspond to the absolute cumulative
n-gram precision value (n=1,2,3,4 respectively).
In experiments, we compare ours with eight con-
figurations above in Table 6.
but this does not matter .
peu importe !
we may find ourselves there once again .
va-t-il en e?tre de me?me cette fois-ci ?
all for the good .
et c? est tant mieux !
but if the ceo is not accountable , who is ?
mais s? il n? est pas responsable , qui alors ?
Table 2: Sentences judged as outliers by Algo-
rithm 1 (ENFR News Commentary corpus).
We would like to mention our motivation for
choosing the variant of Bleu. In Step 3 we
need to set up a threshold in M
WB
to determine
outliers. Natural intuition is that this distribu-
tion takes some smooth distribution as Bleu takes
weighted geometric mean. However, as is shown
cumulative 4?gram scores
cumulative 1?gram scorescumulative 2?gram scores
cumulative 3?gram scores
4?gram scores
2?gram scores
3?gram scores 3?gram scores
1?gram scores
2?gram scores 1?gram scores
of MT_PB
of MT_PBof MT_PB
of MT_PB
of MT_WB
of MT_WB
of MT_WB
of MT_WB
4?gram scores
count count
count count
Figure 5: Four figures show the sentence-based
cumulative n-gram scores: x-axis is phrase-based
SMT and y-axis is word-based SMT. Focus is on
the worst point (0,0) where both scores are zero.
Many points reside in (0,0) in cumulative 4-gram
scores, while only small numbers of point reside
in (0,0) in cumulative 1-gram scores.
in the first row of Figure 4, typical distribution of
words in this space M
WB
is separated in two clus-
ters: one looks like a geometric distribution and
the other one contains a lot of points whose value
is zero. (Especially in the case of Bleu, if the sen-
tence length is less than 3 the Bleu score is zero.)
For this reason, we use the variants of Bleu score:
we decompose Bleu score in cumulative n-gram
score (n=1,2,3,4), which is shown in Figure 3. It is
noted that the following relation holds: S
4
(e, f) ?
S
3
(e, f) ? S
2
(e, f) ? S
1
(e, f) where e denotes
an English sentence, f denotes a foreign sentence,
and S
X
denotes cumulative X-gram scores. For 3-
gram scores, the tendency to separate in two clus-
ters is slightly decreased. Furthermore, for 1-gram
scores, the distribution approaches to normal dis-
tribution. We model P(outlier) taking care of the
quantity of S
2
(e, f), where we choose 0.1: other
configurations in Table 1 are used in experiments.
It is noted that although we choose the variants
of Bleu score, it is clear, in this context, that we
can replace Bleu with any other measure, such as
METEOR (Banerjee and Lavie, 05), NIST (Dod-
dington, 02), GTM (Melamed et al, 03), TER
(Snover et al, 06), labeled dependency approach
(Owczarzak et al, 07) and so forth (see Figure 4).
Table 2 shows outliers detected by Algorithm 1.
Finally, a revised algorithm which incorporates
sentence-based X-gram scores of phrase-based
MT is shown in Algorithm 2. Figure 5 tells us
76
that there are many sentence pair scores actually
improved in phrase-based MT even if word-based
score is zero.
Algorithm 2 Revised Good Points Algorithm
Step 1: Train word-based MT for full parallel
corpus. Translate all training sentences by the
above trained word-based MT decoder.
Step 2: Obtain the cumulative X-gram score
S
WB,X
for each pair of sentences where X is
4, 3, 2, and 1 for word-based MT decoder.
Step 3: Train phrase-based MT for full parallel
corpus. Note that we do not need to run a word
aligner again in here, but use the results of Step
1. Translate all training sentences by the above
trained phrase-based MT decoder.
Step 4: Obtain the cumulative X-gram score
S
PB,X
for each pair of sentences where X is
4, 3, 2, and 1 for phrase-based MT decoder.
Step 5: Remove sentences whose (S
WB,2
,
S
PB,2
) = (0, 0). We produce new reduced par-
allel corpus.
(Step 6: Do the whole procedure of phrase-
based SMT using the reduced parallel corpus
which we obtain from Step 1 to 5.)
5 Results
We evaluate our algorithm using the News Com-
mentary parallel corpus used in 2007 Statistical
Machine Translation Workshop shared task (cor-
pus size and average sentence length are shown in
Table 8). We use the devset and the evaluation set
alignment ENFR ESEN
grow-diag-final 0.058 0.115
union 0.205 0.116
intersection 0.164 0.116
Table 3: Performance of word-based MT system
in different alignment methods. The above is be-
tween ENFR and ESEN.
pair ENFR FREN
score 0.205 0.176
ENES ENDE DEEN
0.276 0.134 0.208
Table 4: Performance of word-based MT system
for different language pairs with union alignment
method.
provided by this workshop. We use Moses (Koehn
et al, 07) as the baseline system, with mgiza (Gao
and Vogel, 08) as its word alignment tool. We do
MERT in all the experiments below.
Step 1 of Algorithm 1 produces, for a given
parallel corpus, a word-based MT. We do this us-
ing Moses with option max-phrase-length set to 1,
alignment as union as we would like to extract the
bi-directional results of word alignment with high
recall. Although we have chosen union, other se-
lection options may be possible as Table 3 sug-
gests. Performance of this word-based MT system
is as shown in Table 4.
Step 2 is to obtain the cumulative n-gram score
for the entire training parallel corpus by using the
word-based MT system trained in Step 1. Table 5
shows the first two sentences of News Commen-
tary corpus. We score for all the sentence pairs.
c score = [0.4213,0.4629,0.5282,0.6275]
consider the number of clubs that have
qualified for the european champions ?
league top eight slots .
conside?rons le nombre de clubs qui se sont
qualifie?s parmi les huit meilleurs de la ligue
des champions europenne .
c score = [0.0000,0.0000,0.0000,0.3298]
estonia did not need to ponder long
about the options it faced .
l? estonie n? a pas eu besoin de longuement
rflchir sur les choix qui s? offraient a` elle .
Table 5: Four figures marked as score shows the
cumulative n-gram score from left to right. The
following EN and FR are the calculated sentences
used by word-based MT system trained on Step 1.
In Step 3, we obtain the cumulative n-gram
score (shown in Figure 3). As is already men-
tioned, there are a lot of sentence pairs whose cu-
mulative 4-gram score is zero. In the cumulative
3-gram score, this tendency is slightly decreased.
For 1-gram scores, the distribution approaches to
normal distribution. In Step 4, other than our con-
figuration we used 8 different configurations in Ta-
ble 6 to reduce our parallel corpus.
Now we obtain the reduced parallel corpus. In
Step 5, using this reduced parallel corpus we car-
ried out training of MT system from the begin-
ning: we again started from the word alignment,
followed by phrase extraction, and so forth. The
results corresponding to these configurations are
shown in Table 6. In Table 6, in the case of
77
ENES Bleu effective sent UNK
Base 0.280 99.30 % 1.60%
Ours 0.314 96.54% 1.61%
1 0.297 56.21% 2.21%
2 0.294 60.37% 2.09%
3 0.301 66.20% 1.97%
4 0.306 84.60% 1.71%
5 0.299 56.12% 2.20%
6 0.271 25.05% 2.40%
7 0.283 35.28% 2.26%
8 0.264 19.78% 4.22%
DEEN % ENFR %
Base 0.169 99.10% 0.180 91.81%
Ours 0.221 96.42% 0.192 96.38%
1 0.201 40.49% 0.187 49.37%
2 0.205 48.53% 0.188 55.03%
3 0.208 58.07% 0.187 61.22%
4 0.215 83.10% 0.190 81.57%
5 0.192 29.03% 0.180 31.52%
6 0.174 17.69% 0.162 29.97%
7 0.186 24.60% 0.179 30.52%
8 0.177 18.29% 0.167 17.11%
Table 6: Table shows Bleu score for ENES,
DEEN, and ENFR: 0.314, 0.221, and 0.192, re-
spectively. All of these are better than baseline.
Effective ratio can be considered to be the inlier
ratio, which is equivalent to 1 - (outlier ratio). The
details for the baseline system are shown in Table
8.
ENES Bleu effective sent
Base 0.280 99.30 %
Ours 0.317 97.80 %
DEEN Bleu effective sent
Base 0.169 99.10 %
Ours 0.218 97.14 %
Table 7: This table shows results for the revised
Good Points Algorithm.
English-Spanish our configuration discards 3.46
percent of sentences, and the performance reaches
0.314 which is the best among other configura-
tions. Similarly in the case of German-English our
configuration attains the best performance among
configurations. It is noted that results for the base-
line system are shown in Table 8 where we picked
up the score where n is 100. It is noted that the
baseline system as well as other configurations use
MERT. Similarly, results for a revised Good Points
Figure 6: Three figures in the left show the his-
togram of sentence length (main figures) and his-
togram of sentence length of outliers (at the bot-
tom). (As the numbers of outliers are less than
5 percent in each case, outliers are miniscule. In
the case of EN-ES, we can observe the blue small
distributions at the bottom from 2 to 16 sentence
length.) Three figures in the right show that if we
see this by ratio of outliers over all the counts, all
of three figures tend to be more than 20 to 30 per-
cent from 80 to 100 sentence length. The lower
two figures show that sentence length 1 to 4 tend
to be more than 10 percent.
Algorithm is shown in Table 7.
6 Discussion
In Section 1, we mentioned that if we aim at out-
lier ratio using the indirect feature sentence length,
this method reduces to a well-known sentence
cleaning approach shown below in Algorithm 3.
Algorithm 3 Sentence Cleaning Algorithm
Remove sentences with lengths greater than X
(or remove sentences with lengths smaller than
X in the case of short sentences).
This approach is popular although the reason
behind why this approach works is not well un-
derstood. Our explanation is shown in the right-
hand side of Figure 6 where outliers are shown at
the bottom (almost invisible) which are extracted
by Algorithm 1. The region that Algorithm 3 re-
moves via sentence length X is possibly the region
where the ratio of outliers is high.
This method is a high recall method. This
method does not check whether the removed sen-
tences are really sentences whose behavior is bad
or not. For example, look at Figure 6 for sen-
78
X ENFR FREN ESEN DEEN ENDE
10 0.167 0.088 0.143 0.097 0.079
20 0.087 0.195 0.246 0.138 0.127
30 0.145 0.229 0.279 0.157 0.137
40 0.175 0.242 0.295 0.168 0.142
50 0.229 0.250 0.297 0.170 0.145
60 0.178 0.253 0.297 0.171 0.146
70 0.179 0.251 0.298 0.170 0.146
80 0.181 0.252 0.301 0.169 0.147
90 0.180 0.252 0.297 0.171 0.147
100 0.180 0.251 0.302 0.169 0.146
# 51k 51k 51k 60k 60k
ave 21.0/23.8(EN/FR) 20.9/24.5(EN/ES)
len 20.6/21.6(EN/DE)
Table 8: Bleu score after cleaning of sen-
tences with length greater than X. The row
shows X, while the column shows the language
pair. Parallel corpus is News Commentary par-
allel corpus. It is noted that the default set-
ting of MAX SENTENCE LENTH ALLOWED
in GIZA++ is 101.
tence length 10 to 30 where there are considerably
many outliers in the region that a lot of inliers re-
side. However, this method cannot cope with such
outliers. Instead, the method cope with the region
that the outlier ratio is possibly high at both ends,
e.g. sentence length > 60 or sentence length < 5.
The advantage is that sentence length information
is immediately available from the sentence which
is easy to implement. The results of this algorithm
is shown in Table 8 where we varies X and lan-
guage pair. This table also suggests that we should
refrain from saying that X = 60 is best or X = 80
is best.
7 Conclusions and Further Work
This paper shows some preliminary results that
data cleaning may be a useful pre-processing tech-
nique for word alignment. At this moment, we ob-
serve two positive results, improvement of Bleu
score from 28.0 to 31.4 in English-Spanish and
16.9 to 22.1 in German-English which are shown
in Table 6. Our method checks the realizability of
target sentences in training sentences. If we wit-
ness bad cumulative X-gram scores we suspect
that this is due to some problems caused by the
n : m mapping objects during word alignment fol-
lowed by phrase extraction process.
Firstly, although we removed training sentences
whose n-gram scores are low, we can dupli-
cate such training sentences in word alignment.
This method is appealing, but unfortunately if we
use mgiza or GIZA++, our training process of-
ten ceased in the middle by unrecognized errors.
However, if we succeed in training, the results of-
ten seem comparable to our results. Although we
did not supply back removed sentences, it is pos-
sible to examine such sentences using the T-tables
to extract phrase pairs.
Secondly, it seems that one of the key matters
lies in the quantities of n : m mapping objects
which are difficult to learn by word-based MT (or
by phrase-based MT). It is possible that such quan-
tities are different depending on their language
pairs and on their corpora size. A rough estimation
is that this quantity may be somewhere less than
10 percent (in FR-EN Hansard corpus, recall and
precision reach around 90 percent (Moore, 05)),
or less than 5 percent (in News Commentary cor-
pus, the best Bleu scores by Algorithm 1 are when
this percentage is less than 5 percent ). As further
study, we intend to examine this issue further.
Thirdly, this method has other aspects that it
removes discontinuous points: such discontinu-
ous points may relate to the smoothness of opti-
mization surface. One of the assumptions of the
method such as Wang et al (Wang et al, 07) re-
lates to smoothness. Then, our method may im-
prove their results, which is our further study.
In addition, although our algorithm runs a word
aligner more than once, this process can be re-
duced since removed sentences are less than 5 per-
cent or so.
Finally, we did not compare our method with
TCR of Imamura. In our case, the focus was 2-
gram scores rather than other n-gram scores. We
intend to investigate this further.
8 Acknowledgements
This work is supported by Science Foundation
Ireland (Grant No. 07/CE/I1142). Thanks to
Yvette Graham and Sudip Naskar for proof read-
ing, Andy Way, Khalil Sima?an, Yanjun Ma, and
annonymous reviewers for comments, and Ma-
chine Translation Marathon.
References
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. ACL.
79
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An Automatic Metric for MT Evaluation With Im-
proved Correlation With Human Judgments. Work-
shop On Intrinsic and Extrinsic Evaluation Measures
for Machine Translation and/or Summarization.
Peter F. Brown, Vincent J.D. Pietra, Stephen A.D.
Pietra, and Robert L. Mercer. 1993. The Mathe-
matics of Statistical Machine Translation: Parame-
ter Estimation,? Computational Linguistics, Vol.19,
Issue 2.
Chris Callison-Burch. 2007. Paraphrasing and Trans-
lation. PhD Thesis, University of Edinburgh.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved Statistical Machine Transla-
tion Using Paraphrases. NAACL.
Chris Callison-Burch, Trevor Cohn, and Mirella La-
pala. 2008. ParaMetric: An Automatic Evaluation
Metric for Paraphrasing. COLING.
A.P. Dempster, N.M. Laird, and D.B. Rubin. 1977.
Maximum likelihood from Incomplete Data via the
EM algorithm. Journal of the Royal Statistical Soci-
ety.
Yonggang Deng and William Byrne. 2005. HMM
Word and Phrase Alignment for Statistical Machine
Translation. Proc. Human Language Technology
Conference and Empirical Methods in Natural Lan-
guage Processing.
George Doddington. 2002. Automatic Evaluation
of Machine Translation Quality Using N-gram Co-
Occurrence Statistics. HLT.
David A. Forsyth and Jean Ponce. 2003. Computer
Vision. Pearson Education.
Qin Gao and Stephan Vogel. 2008. Parallel Imple-
mentations of Word Alignment Tool. Software Engi-
neering, Testing, and Quality Assurance for Natural
Language Processing.
Kenji Imamura, Eiichiro Sumita, and Yuji Matsumoto.
2003. Automatic Construction of Machine Trans-
lation Knowledge Using Translation Literalness.
EACL.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation.
HLT/NAACL.
Philipp Koehn, Amittai Axelrod, Alexandra Birch,
Chris Callison-Burch, Miles Osborne, and David
Talbot. 2005. Edinburgh System Description for
the 2005 IWSLT Speech Translation Evaluation. In-
ternational Workshop on Spoken Language Transla-
tion.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
ACL.
Patrik Lambert and Rafael E. Banchs. 2005. Data
Inferred Multiword Expressions for Statistical Ma-
chine Translation. Machine Translation Summit X.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. HLT/NAACL.
Dekang Lin and Patrick Pantel. 1999. Induction of Se-
mantic Classes from Natural Language Text. In Pro-
ceedings of ACM Conference on Knowledge Dis-
covery and Data Mining (KDD-01).
Daniel Marcu and William Wong. 2002. A Phrase-
based, Joint Probability Model for Statistical Ma-
chine Translation. In Proceedings of Conference on
Empirical Methods in Natural Language Processing
(EMNLP).
I. Dan Melamed, Ryan Green, and Joseph Turian.
2003. Precision and Recall of Machine Translation.
NAACL/HLT 2003.
Robert C. Moore. 2005. A Discriminative Framework
for Bilingual Word Alignment. HLT/EMNLP.
Franz Josef Och and Hermann Ney. 2003. A Sys-
tematic Comparison of Various Statistical Align-
ment Models. Computational Linguistics, volume
20,number 1.
Karolina Owczarzak, Josef van Genabith, and Andy
Way. 2007. Evaluating Machine Translation with
LFG Dependencies. Machine Translation, Springer,
Volume 21, Number 2.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A Method For Automatic
Evaluation of Machine Translation ACL.
Chris Quirk, Chris Brockett, and William Dolan. 2004.
Monolingual machine translation for paraphrase
generation. EMNLP-2004.
Matthew Snover. Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study of
Translation Edit Rate with Targeted Human Anno-
tation. Association for Machine Translation in the
Americas.
John Tinsley, Ventsisiav Zhechev, Mary Hearne, and
Andy Way. 2006. Robust Language Pair-
Independent Sub-Tree Alignment. Translation Sum-
mit XI.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based Word Alignment in Statistical
Translation. COLING 96.
Zhuoran Wang, John Shawe-Taylor, and Sandor Szed-
mak. 2007. Kernel Regression Based Machine
Translation. Proceedings of NAACL-HLT 2007.
80
Proceedings of the 4th International Workshop on Cross Lingual Information Access at COLING 2010, pages 26?34,
Beijing, August 2010
Multi-Word Expression-Sensitive Word Alignment
Tsuyoshi Okita1, Alfredo Maldonado Guerra2, Yvette Graham3, Andy Way1
{CNGL1, NCLT3} / School of Computing / Dublin City University,
CNGL / School of Computer Science and Statistics / Trinity College Dublin2
{tokita,ygraham,away}@computing.dcu.ie, maldonaa@scss.tcd.ie
Abstract
This paper presents a new word align-
ment method which incorporates knowl-
edge about Bilingual Multi-Word Expres-
sions (BMWEs). Our method of word
alignment first extracts such BMWEs in
a bidirectional way for a given corpus and
then starts conventional word alignment,
considering the properties of BMWEs in
their grouping as well as their alignment
links. We give partial annotation of align-
ment links as prior knowledge to the word
alignment process; by replacing the max-
imum likelihood estimate in the M-step
of the IBM Models with the Maximum A
Posteriori (MAP) estimate, prior knowl-
edge about BMWEs is embedded in the
prior in this MAP estimate. In our exper-
iments, we saw an improvement of 0.77
Bleu points absolute in JP?EN. Except
for one case, our method gave better re-
sults than the method using only BMWEs
grouping. Even though this paper does
not directly address the issues in Cross-
Lingual Information Retrieval (CLIR), it
discusses an approach of direct relevance
to the field. This approach could be
viewed as the opposite of current trends
in CLIR on semantic space that incorpo-
rate a notion of order in the bag-of-words
model (e.g. co-occurences).
1 Introduction
Word alignment (Brown et al, 1993; Vogel et
al., 1996; Och and Ney, 2003a; Graca et al,
2007) remains key to providing high-quality trans-
lations as all subsequent training stages rely on its
performance. It alone does not effectively cap-
ture many-to-many word correspondences, but in-
stead relies on the ability of subsequent heuristic
phrase extraction algorithms, such as grow-diag-
final (Koehn et al, 2003), to resolve them.
Some aligned corpora include implicit partial
alignment annotation, while for other corpora a
partial alignment can be extracted by state-of-
the-art techniques. For example, implicit tags
such as reference number within the patent cor-
pus of Fujii et al (2010) provide (often many-to-
many) correspondences between source and tar-
get words, while statistical methods for extract-
ing a partial annotation, like Kupiec et al (1993),
extract terminology pairs using linguistically pre-
defined POS patterns. Gale and Church (1991)
extract pairs of anchor words, such as num-
bers, proper nouns (organization, person, title),
dates, and monetary information. Resnik and
Melamed (1997) automatically extract domain-
specific lexica. Moore (2003) extracts named-
entities. In Machine Translation, Lambert and
Banchs (2006) extract BMWEs from a phrase ta-
ble, which is an outcome of word alignment fol-
lowed by phrase extraction; this method does not
alter the word alignment process.
This paper introduces a new method of incorpo-
rating previously known many-to-many word cor-
respondences into word alignment. A well-known
method of incorporating such prior knowledge
in Machine Learning is to replace the likelihood
maximization in the M-step of the EM algorithm
with either the MAP estimate or the Maximum
Penalized Likelihood (MPL) estimate (McLach-
26
lan and Krishnan, 1997; Bishop, 2006). Then, the
MAP estimate allows us to incorporate the prior,
a probability used to reflect the degree of prior be-
lief about the occurrences of the events.
A small number of studies have been carried
out that use partial alignment annotation for word
alignment. Firstly, Graca et al (2007) introduce
a posterior regularization to employ the prior that
cannot be easily expressed over model parameters
such as stochastic constraints and agreement con-
straints. These constraints are set in the E-step to
discard intractable alignments contradicting these
constraints. This mechanism in the E-step is in a
similar spirit to that in GIZA++ for IBM Model
3 and 4 which only searches around neighbour-
ing alignments around the Viterbi alignment. For
this reason, this algorithm is not intended to be
used combined with IBM Models 3 and 4. Al-
though theoretically it is possible to incorporate
partial annotation with a small change in its code,
Graca et al do not mention it. Secondly, Tal-
bot (2005) introduces a constrained EM method
which constrains the E-step to incorporate par-
tial alignment into word alignment,1 which is in
a similar manner to Graca et al (2007). He con-
ducted experiments using partial alignment anno-
tation based on cognate relations, a bilingual dic-
tionary, domain-specific bilingual semantic anno-
tation, and numerical pattern matching. He did
not incorporate BMWEs. Thirdly, Callison-Burch
et al (2004) replace the likelihood maximization
in the M-step with mixed likelihood maximiza-
tion, which is a convex combination of negative
log likelihood of known links and unknown links.
The remainder of this paper is organized as fol-
lows: in Section 2 we define the anchor word
alignment problem. In Section 3 we include
a review of the EM algorithm with IBM Mod-
els 1-5, and the HMM Model. Section 4 de-
scribes our own algorithm based on the combina-
tion of BMWE extraction and the modified word
alignment which incorporates the groupings of
BMWEs and enforces their alignment links; we
explain the EM algorithm with MAP estimation
1Although the code may be similar in practice to our Prior
Model I, his explanation to modify the E-step will not be
applied to IBM Models 3 and 4. Our view is to modify the
M-step due to the same reason above, i.e. GIZA++ searches
only over the alignment space around the Viterbi alignment.
pair GIZA++(no prior) Ours(with prior)
EN-FR fin ini prior fin ini prior
is NULL 1 .25 0 0 .25 .25
rosy en 1 .5 0 0 .5 .2
that . 1 .25 0 0 .25 .25
life la 1 .25 0 0 .25 0
. c? 1 .25 0 0 .25 .25
that c? 0 .25 0 1 .25 .25
is est 0 .25 0 1 .25 .25
life vie 0 .5 0 1 .5 1
rosy rose 0 .25 0 1 .25 .2
Table 1: The benefit of prior knowledge of anchor
words.
with three kinds of priors. In Section 5 our exper-
imental results are presented, and we conclude in
Section 6.
2 Anchor Word Alignment Problem
The input to standard methods of word alignment
is simply the sentence-aligned corpus, whereas
our alignment method takes in additionally a par-
tial alignment. We assume, therefore, the avail-
ability of a partial alignment, for example via a
MWE extraction tool. Let e? denote an English
sentence, and e denote an English word, through-
out this paper. The anchor word alignment prob-
lem is defined as follows:
Definition 1 (Anchor Word Alignment Problem)
Let (e?, f?) = {(e?1, f?1), . . . , (e?n, f?n)} be a parallel
corpus. By prior knowledge we additionally
have knowledge of anchor words (e?, f?) =
{(senti, te1, tf1 , pose1, posf1 , lengthe, lengthf ),
. . ., (sentk, ten , tfn , posen , posfn , lengthe,
lengthf )} where senti denotes sentence ID,
posei denotes the position of tei in a sentence e?i,
and lengthe (and lengthf ) denotes the sentence
length of the original sentence which includes
ei. Under a given (e?, f?) and (e?, f?), our objective
is to obtain word alignments. It is noted that an
anchor word may include a phrase pair which
forms n-to-m mapping objects.
Table 1 shows two example phrase pairs for
French to English c?est la vie and that is life, and
la vie en rose and rosy life with the initial value
for the EM algorithm, the prior value and the fi-
27
Statistical MWE extraction method
97|||groupe socialiste|||socialist group|||26|||26
101|||monsieur poettering|||mr poettering|||1|||4
103|||monsieur poettering|||mr poettering|||1|||11
110|||monsieur poettering|||mr poettering|||1|||9
117|||explication de vote|||explanation of vote|||28|||26
Heuristic-based MWE extraction method
28|||the wheel 2|||?? ?||| 25||| 5
28|||the primary-side fixed armature 13|||? ? ? ?
? ?? ? ? ?||| 13||| 9
28|||the secondary-side rotary magnet 7|||? ? ? ?
? ????? ?||| 15||| 11
Table 2: Example of MWE pairs in Europarl cor-
pus (FR-EN) and NTCIR patent corpus (JP-EN).
There are 5 columns for each term: sentence num-
ber, source term, target term, source position, and
target position. The number appended to each
term from the patent corpus (lower half) is a ref-
erence number. In this corpus, all the important
technical terms have been identified and annotated
with reference numbers.
nal lexical translation probability for Giza++ IBM
Model 4 and that of our modified Giza++. Our
modified Giza++ achieves the correct result when
anchor words ?life? and ?vie? are used to assign a
value to the prior in our model.
3 Word Alignment
We review two models which address the prob-
lem of word alignment. The aim of word align-
ment is to obtain the model parameter t among
English and French words, ei and fj respectively.
We search for this model parameter under some
model M where M is chosen by IBM Models 1-
5 and the HMM model. We introduce the latent
variable a, which is an alignment function with
the hypothesis that each e and f correspond to this
latent variable. (e, f, a) is a complete data set, and
(e, f) is an incomplete data set.
3.1 EM Algorithm
We follow the description of the EM algorithm for
IBM Models of Brown et al (1993) but introduce
the parameter t explicitly. In this model, the pa-
rameter t represents the lexical translation proba-
bilities t(ei|fj). It is noted that we use e|f rather
than f |e following the notation of Koehn (2010).
One important remark is that the Viterbi align-
ment of the sentence pair (e?, f?) = (eJ1 , f I1 ), which
is obtained as in (1):
Eviterbi : a?J1 = argmaxaJ1
p??(f, a|e) (1)
provides the best alignment for a given log-
likelihood distribution p??(f, a|e). Instead of sum-
ming, this step simplifies the E-step. However, un-
der our modification of maximum likelihood esti-
mate with MAP estimate, this simplification is not
a correct approximation of the summation since
our surface in the E-step is greatly perturbed by
the prior. There is no guarantee that the Viterbi
alignment is within the proximity of the target
alignment (cf. Table 1).
Let z be the latent variable, t be the parameters,
and x be the observations. The EM algorithm is
an iterative procedure repeating the E-step and the
M-step as in (2):
EEXH : q(z;x) =p(z|x; ?) (2)
MMLE : t? = argmax
t
Q(t, told)
= argmax
t
?
x,z
q(z|x) log p(x, z; t)
In the E-step, our knowledge of the values of the
latent variables in a is given only by the poste-
rior distribution p(a|e, f, t). Hence, the (negative
log)-likelihood of complete data (e, f, a), which
we denote by ? log p(t|e, f, a), is obtained over
all possible alignments a. We use the current pa-
rameter values told to find the posterior distribu-
tion of the latent variables given by p(a|e, f, told).
We then use this posterior distribution to find the
expectation of the complete data log-likelihood
evaluated for parameter value t. This expectation
is given by
?
a p(a|e, f, told) log p(e, f, a|t).
In the M-step, we use a maximal likelihood es-
timation to minimize negative log-likelihood in
order to determine the parameter t; note that t is
a lexical translation probability. Instead of using
the log-likelihood log p(a, e, f |t), we use the ex-
pected complete data log-likelihood over all the
possible alignments a that we obtained in the E-
28
step, as in (3):
MMLE : t? = argmax
t
Q(t, told) (3)
= c(f |e; f, e)?
e c(f |e; f, e)
where an auxiliary function c(e|f ; e, f) for IBM
Model 1 introduced by Brown et al is defined as
c(f |e; f, e) =
?
a
p(a|e, f)
m?
j=1
?(f, fj)?(e, eaj )
and where the Kronecker-Delta function ?(x, y) is
1 if x = y and 0 otherwise. This auxiliary func-
tion is convenient since the normalization factor of
this count is also required. We note that if we use
the MAP estimate, the E-step remains the same as
in the maximum likelihood case, whereas in the
M-step the quantity to be minimized is given by
Q(t, told) + log p(t). Hence, we search for the
value of t which maximizes the following equa-
tion:
MMAP : t? = argmax
t
Q(t, told) + log p(t)
3.2 HMM
A first-order Hidden Markov Model (Vogel et al,
1996) uses the sentence length probability p(J |I),
the mixture alignment probability p(i|j, I), and
the translation probability, as in (4):
p(f |e) = p(J |I)
J?
j=1
p(fj|ei) (4)
Suppose we have a training set of R observation
sequences Xr, where r = 1, ? ? ? , R, each of which
is labelled according to its class m, where m =
1, ? ? ? ,M , as in (5):
p(i|j, I) = r(i? j
I
J )?I
i?=1 r(i? ? j IJ )
(5)
The HMM alignment probabilities p(i|i?, I) de-
pend only on the jump width (i ? i?). Using a set
of non-negative parameters s(i? i?), we have (6):
p(i|i?, I) = s(i ? i
?)
?I
l=1 s(l ? i?)
(6)
4 Our Approach
Algorithm 1 Overall Algorithm
Given: a parallel corpus,
1. Extract MWEs by Algorithm 2.
2. Based on the results of Step 1, specify a set
of anchor word alignment links in the format of
anchor word alignment problem (cf. Definition
1 and Table 2).
3. Group MWEs in source and target text.
4. Calculate the prior in order to embed knowl-
edge about anchor words.
5. Calculate lexical translation probabilities
with the prior.
6. Obtain alignment probabilities.
7. Ungroup of MWEs in source and target text.
Algorithm 1 consists of seven steps. We use the
Model I prior for the case where our prior knowl-
edge is sparse and evenly distributed throughout
the corpus, whereas we use the Model II prior
when our prior knowledge is dense in a partial
corpus. A typical example of the former case
is when we use partial alignment annotation ex-
tracted throughout a corpus for bilingual terminol-
ogy. A typical example of the latter case is when a
sample of only a few hundred lines from the cor-
pus have been hand-annotated.
4.1 MWE Extraction
Our algorithm of extracting MWEs is a statisti-
cal method which is a bidirectional version of Ku-
piec (1993). Firstly, Kupiec presents a method to
extract bilingual MWE pairs in a unidirectional
manner based on the knowledge about typical
POS patterns of noun phrases, which is language-
dependent but can be written down with some ease
by a linguistic expert. For example in French they
are N N, N prep N, and N Adj. Secondly, we take
the intersection (or union) of extracted bilingual
MWE pairs.2
2In word alignment, bidirectional word alignment by tak-
ing the intersection or union is a standard method which
improves its quality compared to unidirectional word align-
ment.
29
Algorithm 2 MWE Extraction Algorithm
Given: a parallel corpus and a set of anchor
word alignment links:
1. We use a POS tagger (Part-Of-Speech Tag-
ger) to tag a sentence on the SL side.
2. Based on the typical POS patterns for the SL,
extract noun phrases on the SL side.
3. Count n-gram statistics (typically n =
1, ? ? ? , 5 are used) on the TL side which jointly
occur with each source noun phrase extracted
in Step 2.
4. Obtain the maximum likelihood counts of
joint phrases, i.e. noun phrases on the SL side
and n-gram phrases on the TL side.
5. Repeat the same procedure from Step 1 to 4
reversing the SL and TL.
6. Intersect (or union) the results in both direc-
tions.
Let SL be the source language side and TL be
the target language side. The procedure is shown
in Algorithm 2. We informally evaluated the
MWE extraction tool following Kupiec (1993) by
manually inspecting the mapping of the 100 most
frequent terms. For example, we found that 93 of
the 100 most frequent English terms in the patent
corpus were correctly mapped to their Japanese
translation.
Depending on the corpus, we can use more
prior knowledge about implicit alignment links.
For example in some categories of patent and
technical documents corpora,3 we can use heuris-
tics to extract the ?noun phrase? + ?reference
number? from both sides. This is due to the fact
that terminology is often labelled with a unique
reference number, which is labelled on both the
SL and TL sides.
4.2 Prior Model I
Prior for Exhaustive Alignment Space IBM
Models 1 and 2 implement a prior for all possible
3Unlike other language pairs, the availability of
Japanese?English parallel corpora is quite limited: the NT-
CIR patent corpus (Fujii et al, 2010) of 3 million sentence
pairs (the latest NTCIR-8 version) for the patent domain and
JENAAD corpus (Utiyama and Isahara, 2003) of 150k sen-
tence pairs for the news domain. In this regard, the patent
domain is particularly important for this particular language
pair.
Algorithm 3 Prior Model I for IBM Model 1
Given: parallel corpus e?, f? ,
anchor words biTerm
initialize t(e|f ) uniformly
do until convergence
set count(e|f ) to 0 for all e,f
set total(f) to 0 for all f
for all sentence pairs (e?s,f?s)
prior(e|f)s = getPriorModelI(e?, f? , biT erm)
for all words e in e?s
totals(e) = 0
for all words f in f?s
totals(e) += t(e|f )
for all words e in e?s
for all words f in f?s
count(e|f )+=t(e|f)/totals(e)? prior(e|f)s
total(f) += t(e|f)/totals(e) ? prior(e|f)s
for all f
for all e
t(e|f ) = count(e|f)/total(f)
alignments exhaustively. Such a prior requires the
following two conditions. Firstly, partial knowl-
edge about the prior that we use in our context is
defined as follows. Let us denote a bilingual term
list T = {(s1, t1), . . . , (sm, tm)}. For example
with IBM Model 1: Let us define the following
prior p(e|f, e, f ;T ) from Equation (4):
p(e|f, e, f ;T ) =
?
?
?
1 (ei = si, fj = tj)
0 (ei = si, fj 6= tj)
0 (ei 6= si, fj = tj)
uniform (ei 6= si, fj 6= tj)
Secondly, this prior should be proper for the ex-
haustive case and non-proper for the sampled
alignment space where by proper we mean that the
probability is normalized to 1. Algorithm 3 shows
the pseudo-code for Prior Model I. Note that if
the prior is uniform in the MAP estimation, this is
equivalent to maximum likelihood estimation.
Prior for Sampled Alignment (Function) Space
Due to the exponential costs introduced by fertil-
ity, null token insertion, and distortion probability,
IBM Models 3 and 4 do not consider all (I + 1)J
alignments exhaustively, but rather a small subset
in the E-step. Each iteration only uses the sub-
set of all the alignment functions: this sampling
30
is not uniform, as it only includes the best possi-
ble alignment with all its neighbouring alignments
which differ from the best alignment by one word
(this can be corrected by a move operation) or two
words (this can be corrected by a swap operation).
If we consider the neighbouring alignment via
a move or a swap operation, two issues arise.
Firstly, the fact that these two neighbouring align-
ments are drawn from different underlying distri-
butions needs to be taken into account, and sec-
ondly, that the application of a move and a swap
operation alters a row or column of a prior ma-
trix (or indices of the prior) since either operation
involves the manipulation of links.
Algorithm 4 Pseudo-code for Prior Model II Ex-
haustive Alignment Space
def getPriorModelII(e?,f? ,biTerm):
for i in sentence:
for e in e?i:
allWordsi = length of sentence e?
for f in f?i:
if (e, f ) in biTerm:
n= num of anchor words in i
uni(e|f)i = allWordsi?nallWordsi
expSum(e|f) += uni(e|f)i ? n
else:
countSum(e|f)i += n
countSum(e|f) += count(e|f)i
for e in alle:
for f in allf :
prior(e|f) = expSum(e|f) + countSum(e|f)
return prior(e|f)
Prior for Jump Width i? One implementation
of HMM is to use the forward-backward algo-
rithm. A prior should be embedded within the
forward-backward algorithm. From Equation (6),
there are three cases which depend on whether
ai and its neighbouring alignment ai?1 are deter-
mined by our prior knowledge about anchor words
or not. When both ai and aj are determined, this
probability is expressed as in (7):
p(i? i?; I) =
?
?
?
0 (else) (7)
1 (ei = si, fj = tj for ai) and
(e?i = s?i, f ?j = t?j for aj)
When either ai or aj is determined, this probabil-
ity is expressed as in (8):4
p(i? i?; I) =
?
???
???
0 (condition 1) (8)
1 (condition 2)
1
(m?#eai?????#eai+m)
(else)
(uniform distribution)
When neither ai nor aj is determined, this proba-
bility is expressed as in (9): 5
p(i? i?; I) =
?
????
????
0 (condition 3) (9)
1 (condition 4)
m?i?
(m?#eai?????#eai+m)2
(else)
(Pascal?s triangle distribution)
4.3 Prior Model II
Prior Model II assumes that we have prior knowl-
edge only in some part of the training corpus. A
typical example is when a small part of the corpus
has a hand-crafted ?gold standard? annotation.
Prior for Exhaustive Alignment Space Prior
Model II is used to obtain the prior probability
p(e|f) over all possible combinations of e and f .
In contrast to Prior Model I, which computes the
prior probability p(e|f) for each sentence, Prior
Model II computes the prior probability globally
for all sentences in the corpus. Algorithm 4 shows
the pseudo-code for Prior Model II Exhaustive
Alignment Space.
4condition 1 is as follows:
((ei 6= si, fj 6= tj for ai) and (e?i = s?i, f ?j = t?j for aj)) or
((ei 6= si, fj 6= tj for ai) and (e?i = s?i, f ?j = t?j for aj)) or
((ei = si, fj = tj for ai) and (e?i 6= s?i, f ?j 6= t?j for aj)) or
((ei = si, fj = tj for ai) and (e?i 6= s?i, f ?j 6= t?j for aj))
?condition 2? is as follows:
((ei = si, fj 6= tj for ai) and (e?i = s?i, f ?j = t?j for aj)) or
((ei 6= si, fj = tj for ai) and (e?i = s?i, f ?j = t?j for aj)) or
((ei = si, fj = tj for ai) and (e?i 6= s?i, f ?j = t?j for aj)) or
((ei = si, fj = tj for ai) and (e?i = s?i, f ?j 6= t?j for aj))
5
?condition 3? is as follows:
((ei 6= si, fj 6= tj for ai) and (e?i 6= s?i, f ?j 6= t?j for aj))
?condition 4? is as follows:
((ei 6= si, fj 6= tj for ai) and (e?i 6= s?i, f ?j = t?j for aj)) or
((ei 6= si, fj 6= tj for ai) and (e?i = s?i, f ?j 6= t?j for aj)) or
((ei 6= si, fj = tj for ai) and (e?i 6= s?i, f ?j 6= t?j for aj)) or
((ei = si, fj 6= tj for ai) and (e?i 6= s?i, f ?j 6= t?j for aj))
31
Prior for Sampled Alignment (Function) Space
This is identical to that of the Prior Model II ex-
haustive alignment space with only a difference in
the normalization process.
Prior for Jump Width i? This categorization of
Prior Model II is the same as that of Prior Model I
for for Jump Width i? (see Section 4.2). Note that
Prior Model II requires more memory compared
to the Prior Model I.6
5 Experimental Settings
The baseline in our experiments is a standard
log-linear phrase-based MT system based on
Moses. The GIZA++ implementation (Och and
Ney, 2003a) of IBM Model 4 is used as the base-
line for word alignment, which we compare to
our modified GIZA++. Model 4 is incrementally
trained by performing 5 iterations of Model 1, 5
iterations of HMM, 5 iterations of Model 3, and
5 iterations of Model 4. For phrase extraction the
grow-diag-final heuristics are used to derive the
refined alignment from bidirectional alignments.
We then perform MERT while a 5-gram language
model is trained with SRILM. Our implementa-
tion is based on a modified version of GIZA++
(Och and Ney, 2003a). This modification is on the
function that reads a bilingual terminology file,
the function that calculates priors, the M-step in
IBM Models 1-5, and the forward-backward algo-
rithm in the HMM Model. Other related software
tools are written in Python and Perl: terminol-
ogy concatenation, terminology numbering, and
so forth.
6 Experimental Results
We conduct an experimental evaluation on the
NTCIR-8 corpus (Fujii et al, 2010) and on Eu-
roparl (Koehn, 2005). Firstly, MWEs are ex-
tracted from both corpora, as shown in Table 3.
In the second step, we apply our modified version
of GIZA++ in which we incorporate the results of
6This is because it needs to maintain potentially an ??m
matrix, where ? denotes the number of English tokens in the
corpus and m denotes the number of foreign tokens, even if
the matrix is sparse. Prior Model I only requires an ?? ? m?
matrix where ?? is the number of English tokens in a sentence
and m? is the number of foreign tokens in a sentence, which
is only needed until this information is incorporated in a pos-
terior probability during the iterative process.
corpus language size #unique #all
MWEs MWEs
statistical method
NTCIR EN-JP 200k 1,121 120,070
europarl EN-FR 200k 312 22,001
europarl EN-ES 200k 406 16,350
heuristic method
NTCIR EN-JP 200k 50,613 114,373
Table 3: Statistics of our MWE extraction method.
The numbers of MWEs are from 0.08 to 0.6 MWE
/ sentence pair in our statistical MWE extraction
methods.
MWE extraction. Secondly, in order to incorpo-
rate the extracted MWEs, they are reformatted as
shown in Table 2. Thirdly, we convert all MWEs
into a single token, i.e. we concatenate them with
an underscore character. We then run the modi-
fied version of GIZA++ and obtain a phrase and
reordering table. In the fourth step, we split the
concatenated MWEs embedded in the third step.
Finally, in the fifth step, we run MERT, and pro-
ceed with decoding before automatically evaluat-
ing the translations.
Table 4 shows the results where ?baseline? in-
dicates no BMWE grouping nor prior, and ?base-
line2? represents a BMWE grouping but without
the prior. Although ?baseline2? (BMWE group-
ing) shows a drop in performance in the JP?EN
/ EN?JP 50k sentence pair setting, Prior Model I
results in an increase in performance in the same
setting. Except for EN?ES 200k, our Prior Model
I was better than ?baseline2?. For EN?JP NT-
CIR using 200k sentence pairs, we obtained an
absolute improvement of 0.77 Bleu points com-
pared to the ?baseline?; for EN?JP using 50k sen-
tence pairs, 0.75 Bleu points; and for ES?EN Eu-
roparl corpus using 200k sentence pairs, 0.63 Bleu
points. In contrast, Prior Model II did not work
well. The possible reason for this is the misspec-
ification, i.e. the modelling by IBM Model 4 was
wrong in terms of the given data. One piece of ev-
idence for this is that most of the enforced align-
ments were found correct in a manual inspection.
For EN?JP NTCIR using the same corpus of
200k, although the number of unique MWEs ex-
32
size EN-JP Bleu JP-EN Bleu
50k baseline 16.33 baseline 22.01
50k baseline2 16.10 baseline2 21.71
50k prior I 17.08 prior I 22.11
50k prior II 16.02 prior II 20.02
200k baseline 23.42 baseline 21.68
200k baseline2 24.10 baseline2 22.32
200k prior I 24.22 prior I 22.45
200k prior II 23.22 prior II 21.00
size FR-EN Bleu EN-FR Bleu
50k baseline 17.68 baseline 17.80
50k baseline2 17.76 baseline2 18.00
50k prior I 17.81 prior I 18.02
50k prior II 17.01 prior II 17.30
200k baseline 18.40 baseline 18.20
200k baseline2 18.80 baseline2 18.50
200k prior I 18.99 prior I 18.60
200k prior II 18.20 prior II 17.50
size ES-EN Bleu EN-ES Bleu
50k baseline 16.21 baseline 15.17
50k baseline2 16.61 baseline2 15.60
50k prior I 16.91 prior I 15.87
50k prior II 16.15 prior II 14.60
200k baseline 16.87 baseline 17.62
200k baseline2 17.40 baseline2 18.21
200k prior I 17.50 prior I 18.20
200k prior II 16.50 prior II 17.10
Table 4: Results. Baseline is plain GIZA++ /
Moses (without BMWE grouping / prior), base-
line2 is with BMWE grouping, prior I / II are with
BMWE grouping and prior.
tracted by the statistical method and the heuris-
tic method varies significantly, the total number
of MWEs by each method becomes comparable.
The resulting Bleu score for the heuristic method
(24.24 / 22.48 Blue points for 200k EN?JP / JP?
EN) is slightly better than that of the statistical
method. The possible reason for this is related
to the way the heuristic method groups terms in-
cluding reference numbers, while the statistical
method does not. As a result, the complexity of
the alignment model simplifies slightly in the case
of the heuristic method.
7 Conclusion
This paper presents a new method of incorporat-
ing BMWEs into word alignment. We first de-
tect BMWEs in a bidirectional way and then use
this information to do groupings and to enforce
already known alignment links. For the latter pro-
cess, we replace the maximum likelihood estimate
in the M-step of the EM algorithm with the MAP
estimate; this replacement allows the incorpora-
tion of the prior in the M-step of the EM algo-
rithm. We include an experimental investigation
into incorporating extracted BMWEs into a word
aligner. Although there is some work which incor-
porates BMWEs in groupings, they do not enforce
alignment links.
There are several ways in which this work can
be extended. Firstly, although we assume that our
a priori partial annotation is reliable, if we extract
such MWEs automatically, we cannot avoid erro-
neous pairs. Secondly, we assume that the rea-
son why our Prior Model II did not work was due
to the misspecification (or wrong modelling). We
would like to check this by discriminative mod-
elling. Thirdly, although here we extract BMWEs,
we can extend this to extract paraphrases and non-
literal expressions.
8 Acknowledgments
This research is supported by the Science Foun-
dation Ireland (Grant 07/CE/I1142) as part of
the Centre for Next Generation Localisation
(http://www.cngl.ie) at Dublin City Uni-
versity and Trinity College Dublin. We would also
like to thank the Irish Centre for High-End Com-
puting.
References
Bishop, Christopher M. 2006. Pattern Recognition
and Machine Learning. Springer. Cambridge, UK
Brown, Peter F., Vincent .J.D Pietra, Stephen
A.D.Pietra, Robert L. Mercer. 1993. The Mathe-
matics of Statistical Machine Translation: Param-
eter Estimation. Computational Linguistics. 19(2),
pp. 263?311.
Callison-Burch, Chris, David Talbot and Miles Os-
borne. 2004. Statistical Machine Translation with
33
Word- and Sentence-Aligned Parallel Corpora. Pro-
ceedings of the 42nd Annual Meeting of the As-
sociation for Computational Linguistics (ACL?04),
Main Volume. Barcelona, Spain, pp. 175?182.
Fujii, Atsushi, Masao Utiyama, Mikio Yamamoto,
Takehito Utsuro, Terumasa Ehara, Hiroshi Echizen-
ya, Sayori Shimohata. 2010. Overview of the
Patent Translation Task at the NTCIR-8 Workshop.
Proceedings of the 8th NTCIR Workshop Meet-
ing on Evaluation of Information Access Technolo-
gies: Information Retrieval, Question Answering
and Cross-lingual Information Access, pp. 293?302.
Graca, Joao de Almeida Varelas, Kuzman Ganchev,
Ben Taskar. 2007. Expectation Maximization
and Posterior Constraints. In Neural Information
Processing Systems Conference (NIPS), Vancouver,
BC, Canada, pp. 569?576.
Gale, William, and Ken Church. 1991. A Program for
Aligning Sentences in Bilingual Corpora. In Pro-
ceedings of the 29th Annual Meeting of the Associ-
ation for Computational Linguistics. Berkeley CA,
pp. 177?184.
Koehn, Philipp, Franz Och, Daniel Marcu. 2003. Sta-
tistical Phrase-Based Translation. In Proceedings
of the 2003 Human Language Technology Confer-
ence of the North American Chapter of the Asso-
ciation for Computational Linguistics. Edmonton,
Canada. pp. 115?124.
Koehn, Philipp. 2005. Europarl: A Parallel Corpus
for Statistical Machine Translation. In Conference
Proceedings: the tenth Machine Translation Sum-
mit. Phuket, Thailand, pp.79-86.
Koehn, Philipp, H. Hoang, A. Birch, C. Callison-
Burch, M. Federico, N. Bertoldi, B. Cowan,
W. Shen, C. Moran, R. Zens, C. Dyer, O. Bojar,
A. Constantin, and E. Herbst, 2007. Moses: Open
source toolkit for Statistical Machine Translation.
Proceedings of the 45th Annual Meeting of the As-
sociation for Computational Linguistics Companion
Volume Proceedings of the Demo and Poster Ses-
sions, Prague, Czech Republic, pp. 177?180.
Koehn, Philipp. 2010. Statistical Machine Transla-
tion. Cambridge University Press. Cambridge, UK.
Kupiec, Julian. 1993. An Algorithm for finding Noun
Phrase Correspondences in Bilingual Corpora. In
Proceedings of the 31st Annual Meeting of Associa-
tion for Computational Linguistics. Columbus. OH.
pp. 17?22.
Lambert, Patrik and Rafael Banchs. 2006. Group-
ing Multi-word Expressions According to Part-Of-
Speech in Statistical Machine Translation. In Pro-
ceedings of the EACL Workshop on Multi-Word-
Expressions in a Multilingual Context. Trento, Italy,
pp. 9?16.
McLachlan, Geoffrey J. and Thriyambakam Krishnan,
1997. The EM Algorithm and Extensions. Wiley
Series in probability and statistics. New York, NY.
Moore, Robert C.. 2003. Learning Translations of
Named-Entity Phrases from Parallel Corpora. In
Proceedings of the 11th Conference of the European
Chapter of the Association for Computational Lin-
guistics. Budapest, Hungary. pp. 259?266.
Moore, Robert C.. 2004. On Log-Likelihood-Ratios
and the Significance of Rare Events. In Proceedings
of the 2004 Conference on Empirical Methods in
Natural Language Processing (EMNLP). Barcelona,
Spain, pp. 333?340.
Och, Franz and Herman Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Mod-
els. Computational Linguistics. 29(1), pp. 19?51.
Resnik, Philip and I. Dan Melamed, 1997. Semi-
Automatic Acquisition of Domain-Specific Transla-
tion Lexicons. Proceedings of the 5th Applied Nat-
ural Language Processing Conference. Washington,
DC., pp. 340?347.
Talbot, David. 2005. Constrained EM for parallel text
alignment, Natural Language Engineering, 11(3):
pp. 263?277.
Utiyama, Masao and Hitoshi Isahara. 2003. Reliable
Measures for Aligning Japanese-English News Arti-
cles and Sentences, In Proceedings of the 41st An-
nual Meeting of the Association for Computational
Linguistics. Sapporo, Japan, pp. 72?79.
Vogel, Stephan, Hermann Ney, Christoph Tillmann
1996. HMM-Based Word Alignment in Statisti-
cal Translation. In Proceedings of the 16th Inter-
national Conference on Computational Linguistics.
Copenhagen, Denmark, pp. 836?841.
34
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 177?184,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Shallow Semantically-Informed PBSMT and HPBSMT
Tsuyoshi Okita
Qun Liu
Josef van Genabith
Dublin City University
Glasnevin, Dublin 9, Ireland
{tokita,qliu,josef}@computing.dcu.ie
Abstract
This paper describes shallow
semantically-informed Hierarchical
Phrase-based SMT (HPBSMT) and
Phrase-Based SMT (PBSMT) systems
developed at Dublin City University
for participation in the translation task
between EN-ES and ES-EN at the Work-
shop on Statistical Machine Translation
(WMT 13). The system uses PBSMT
and HPBSMT decoders with multiple
LMs, but will run only one decoding
path decided before starting translation.
Therefore the paper does not present a
multi-engine system combination. We
investigate three types of shallow seman-
tics: (i) Quality Estimation (QE) score,
(ii) genre ID, and (iii) context ID derived
from context-dependent language models.
Our results show that the improvement is
0.8 points absolute (BLEU) for EN-ES
and 0.7 points for ES-EN compared to
the standard PBSMT system (single best
system). It is important to note that we
developed this method when the standard
(confusion network-based) system com-
bination is ineffective such as in the case
when the input is only two.
1 Introduction
This paper describes shallow semantically-
informed Hierarchical Phrase-based SMT
(HPBSMT) and Phrase-Based SMT (PBSMT)
systems developed at Dublin City University
for participation in the translation task between
EN-ES and ES-EN at WMT 13. Our objectives
are to incorporate several shallow semantics into
SMT systems. The first semantics is the QE score
for a given input sentence which can be used to
select the decoding path either of HPBSMT or
PBSMT. Although we call this a QE score, this
score is not quite a standard one which does not
have access to translation output information. The
second semantics is genre ID which is intended to
capture domain adaptation. The third semantics
is context ID: this context ID is used to adjust the
context for the local words. Context ID is used in
a continuous-space LM (Schwenk, 2007), but is
implicit since the context does not appear in the
construction of a continuous-space LM. Note that
our usage of the term semantics refers to meaning
constructed by a sentence or words. The QE
score works as a sentence level switch to select
HPBSMT or PBSMT, based on the semantics
of a sentence. The genre ID gives an indication
that the sentence is to be translated by genre ID-
sensitive MT systems, again based on semantics
on a sentence level. The context-dependent LM
can be interpreted as supplying the local context
to a word, capturing semantics on a word level.
The architecture presented in this paper is sub-
stantially different from multi-engine system com-
bination. Although the system has multiple paths,
only one path is chosen at decoding when process-
ing unseen data. Note that standard multi-engine
system combination using these three semantics
has been presented before (Okita et al, 2012b;
Okita et al, 2012a; Okita, 2012). This paper also
compares the two approaches.
The remainder of this paper is organized as fol-
lows. Section 2 describes the motivation for our
approach. In Section 3, we describe our proposed
systems, while in Section 4 we describe the exper-
imental results. We conclude in Section 5.
2 Motivation
Model Difference of PBSMT and HPBSMT
Our motivation is identical with a system combi-
nation strategy which would obtain a better trans-
lation if we can access more than two translations.
Even though we are limited in the type of MT sys-
177
tems, i.e. SMT systems, we can access at least
two systems, i.e. PBSMT and HPBSMT systems.
The merit that accrues from accessing these two
translation is shown in Figure 1. In this exam-
ple between EN-ES, the skirts of the distribution
shows that around 20% of the examples obtain the
same BLEU score, 37% are better under PBSMT,
and 42% under HPBSMT. Moreover, around 10%
of sentences show difference of 10 BLEU points.
Even a selection of outputs would improve the re-
sults. Unfortunately, some pitfall of system com-
bination (Rosti et al, 2007) impact on the process
when the number of available translation is only
two. If there are only two inputs, (1) the mismatch
of word order and word selection would yield a
bad combination since system combination relies
on monolingual word alignment (or TER-based
alignment) which seeks identical words, and (2)
Minimum Bayes Risk (MBR) decoding, which is
a first step, will not work effectively since it re-
lies on voting. (In fact, only selecting one of the
translation outputs is even effective: this method
is called system combination as well (Specia et al,
2010).) Hence, although the aim is similar, we do
not use a system combination strategy, but we de-
velop a semantically-informed SMT system.
Figure 1: Figure shows the difference of sentence-
based performance between PBSMT and HPB-
SMT systems.
Relation of Complexity of Source Sentence and
Performance of HPBSMT and PBSMT It is
interesting to note that PBSMT tends to be bet-
ter than HPBSMT for European language pairs
as the recent WMT workshop shows, while HPB-
SMT shows often better performance for distant
language pairs such as EN-JP (Okita et al, 2010b)
and EN-ZH in other workshops.
Under the assumption that we use the same
training corpus for training PBSMT and HPBSMT
systems, our hypothesis is that we may be able
to predict the quality of translation. Note that al-
though this is the analogy of quality estimation,
the setting is slightly different in that in test phase,
we will not be given a translation output, but only
a source sentence. Our aim is to predict whether
HPBSMT obtains better translation output than
PBSMT or not. Hence, our aim does not require
that the quality prediction here is very accurate
compared to the standard quality estimation task.
We use a feature set consisting of various charac-
teristics of input sentences.
3 Our Methods: Shallow Semantics
Our system accommodates PBSMT and HPBSMT
with multiple of LMs. A decoder which handles
shallow semantic information is shown in Table
3.1.
3.1 QE Score
Quality estimation aims to predict the quality of
translation outputs for unseen data (e.g. by build-
ing a regressor or a classifier) without access to
references: the inputs are translation outputs and
source sentences in a test phase, while in a training
phase the corresponding BLEU or HTER scores
are used. In this subsection, we try to build a re-
gressor with the similar settings but without sup-
plying the translation outputs. That is, we supply
only the input sentences. (Since our method is not
a quality estimation for a given translation output,
quality estimation may not be an entirely appro-
priate term. However, we borrow this term for this
paper.) If we can build such a regressor for PB-
SMT and HPBSMT systems, we would be able
to select a better translation output without actu-
ally translating them for a given input sentence.
Note that we translate the training set by PBSMT
and HPBSMT in a training phase only to supply
their BLEU scores to a regressor (since a regres-
sor is a supervised learning method). Then, we
use these regressors for a given unseen source sen-
tence (which has no translation output attached) to
predict their BLEU scores for PBSMT and HPB-
SMT.
Our motivation came from the comparison of
a sequential learning system and a parser-based
system. The typical decoder of the former is a
178
Viterbi decoder while that of the latter is a Cocke-
Younger-Kasami (CYK) decoder (Younger, 1967).
The capability of these two systems provides
an intuition about the difference of PBSMT and
HPBSMT: the CYK decoder-based system has
some capability to handle syntactic constructions
while the Viterbi decoder-based system has only
the capability of learning a sequence. For ex-
Input: Foreign sent f=f1,...,f1f , language model,
translation model, rule table.
Output: English translation e
ceScore = predictQEScore(fi)
if (ceScore == HPBSMTBetter)
for span length l=1 to 1f do
for start=0..1f -1 do
genreID = predictGenreID(fi)
end = start + 1
forall seq s of entries and words in span
[start,end] do
forall rules r do
if rule r applies to chart seq s then
create new chart entry c
with LM(genreID)
add chart entry c to chart
return e from best chart entry in span [0,1f ]
else:
genreID = predictGenreID(fi)
place empty hypothesis into stack 0
for all stacks 0...n-1 do
for all hypotheses in stack do
for all translation options do
if applicable then
create new hyp with LM(ID)
place in stack
recombine with existing hyp if
possible
prune stack if too big
return e
predictQEScore()
predictGenreID()
predictContextID(wordi, wordi?1)
Table 1: Decoding algorithm: the main algorithm
of PBSMT and HPBSMT are from (Koehn, 2010).
The modification is related to predictQEScore(),
predictGenreID(), and predictContextID().
ample, the (context-free) grammar-based system
has the capability of handling various difficul-
ties caused by inserted clauses, coordination, long
Multiword Expressions, and parentheses, while
the sequential learning system does not (This is
since this is what the aim of the context-free
grammar-based system is.) These difficulties are
manifest in input sentences.
0 50 100 150 200 250 300
sample ID
?1.0
?0.5
0.0
0.5
1.0
d
i
f
f
e
r
e
n
c
e
 
o
f
 
B
L
E
U
 
p
o
i
n
t
s
true BLEU difference of PBSMT and HPBSMT
predicted BLEU difference of PBSMT and HPBSMT
Figure 2: A blue line shows the true BLEU dif-
ference between PBSMT and HPBSMT (y-axis)
where x-axis is the sample IDs reordered in de-
scending order (blue), while green dots show the
BLEU absolute difference (y-axis) of the typical
samples where x-axis is shared with the above.
This example is sampled 300 points from new-
stest2013 (ES-EN). Even if the regressor does not
achieve a good performance, the bottom line of the
overall performance is already really high in this
tricky problem. Roughly, even if we plot randomly
we could achieve around 80 - 90% of correctness.
Around 50% of samples (middle of the curve) do
not care (since the true performance of PBSMT
and HPBSMT are even), there is a slope in the left
side of the curve where random plot around this
curve would achieve 15 - 20% among 25% of cor-
rectness (the performance of PBSMT is superior),
and there is another slope in the right side of the
curve where random plot would achieve again 15
- 20% among 25% (the performance of HPBSMT
is superior). In this case, accuracy is 86%.
If we assume that this is one major difference
between these two systems, the complexity of the
input sentence will correlate with the difference of
translation quality of these two systems. In this
subsection, we assume that this is one major dif-
ference of these two systems and that the complex-
ity of the input sentence will correlate with the dif-
ference of translation quality of these two systems.
Based on these assumptions, we build a regressor
179
for each system for a given input sentence where in
a training phase we supply the BLEU score mea-
sured using the training set. One remark is that the
BLEU score which we predict is only meaning-
ful in a relative manner since we actually generate
a translation output in preparation phase (there is
a dependency to the mean of BLEU score in the
training set). Nevertheless, this is still meaningful
as a relative value if we want to talk about their
difference, which is what we want in our settings
to predict which system, either PBSMT or HPB-
SMT, will generate a better output.
The main features used for training the regres-
sor are as follows: (1) number of / length of in-
serted clause / coordination / multiword expres-
sions, (2) number of long phrases (connection by
?of?; ordering of words), (3) number of OOV
words (which let it lower the prediction quality),
(4) number of / length of parenthesis, etc. We ob-
tained these features using parser (de Marneffe et
al., 2006) and multiword extractor (Okita et al,
2010a).
3.2 Genre ID
Genre IDs allow us to apply domain adaptation
technique according to the genre ID of the testset.
Among various methods of domain adaptation, we
investigate unsupervised clustering rather than al-
ready specified genres.
We used (unsupervised) classification via La-
tent Dirichlet Allocation (LDA) (Blei et al, 2003)
to obtain genre ID. LDA represents topics as
multinomial distributions over the W unique
word-types in the corpus and represents docu-
ments as a mixture of topics.
Let C be the number of unique labels in the
corpus. Each label c is represented by a W -
dimensional multinomial distribution ?c over the
vocabulary. For document d, we observe both the
words in the document w(d) as well as the docu-
ment labels c(d). Given the distribution over top-
ics ?d, the generation of words in the document is
captured by the following generative model.
1. For each label c ? {1, . . . C}, sample a distri-
bution over word-types ?c ? Dirichlet(?|?)
2. For each document d ? {1, . . . , D}
(a) Sample a distribution over its observed
labels ?d ? Dirichlet(?|?)
(b) For each word i ? {1, . . . , NWd }
i. Sample a label z(d)i ?
Multinomial(?d)
ii. Sample a word w(d)i ?
Multinomial(?c) from the la-
bel c = z(d)i
Using topic modeling (or LDA) as described
above, we perform the in-domain data partitioning
as follows, building LMs for each class, and run-
ning a decoding process for the development set,
which will obtain the best weights for cluster i.
1. Fix the number of clusters C, we explore val-
ues from small to big.1
2. Do unsupervised document classification (or
LDA) on the source side of the training, de-
velopment and test sets.
3. Separate each class of training sets and build
LM for each cluster i (1 ? i ? C).
4. Separate each class of development set (keep
the original index and new index in the allo-
cated separated dataset).
5. (Using the same class of development set):
Run the decoder on each class to obtain the
n-best lists, run a MERT process to obtain the
best weights based on the n-best lists, (Repeat
the decoding / MERT process several itera-
tions. Then, we obtain the best weights for a
particular class.)
For the test phase,
1. Separate each class of the test set (keep the
original index and new index in the allocated
separated dataset).
2. Suppose the test sentence belongs to cluster
i, run the decoder of cluster i.
3. Repeat the previous step until all the test sen-
tences are decoded.
3.3 Context ID
Context ID semantics is used through the re-
ranking of the n-best list in a MERT process
(Schwenk, 2007; Schwenk et al, 2012; Le et al,
2012). 2-layer ngram-HMM LM is a two layer
version of the 1-layer ngram-HMM LM (Blun-
som and Cohn, 2011) which is a nonparametric
1Currently, we do not have a definite recommendation on
this. It needs to be studied more deeply.
180
Bayesian method using hierarchical Pitman-Yor
prior. In the 2-layer LM, the hidden sequence of
the first layer becomes the input to the higher layer
of inputs. Note that such an architecture comes
from the Restricted Boltzmann Machine (Smolen-
sky, 1986) accumulating in multiple layers in or-
der to build deep belief networks (Taylor and Hin-
ton, 2009). Although a 2-layer ngram-HMM LM
is inferior in its performance compared with other
two LMs, the runtime cost is cheaper than these.
ht denotes the hidden word for the first layer, h?t
denotes the hidden word for the second layer, wi
denotes the word in output layer. The generative
model for this is shown below.
ht|h?t ? F (??st) (1)
wt|ht ? F (?st) (2)
wi|w1:i?1 ? PY(di, ?i, Gi) (3)
where ? is a concentration parameter, ? is a
strength parameter, and Gi is a base measure.
Note that these terms belong to the hierarchical
Pitman-Yor language model (Teh, 2006). We used
a blocked inference for inference. The perfor-
mance of 2-layer LM is shown in Table 3.
4 Experimental Settings
We used Moses (Koehn et al, 2007) for PBSMT
and HPBSMT systems in our experiments. The
GIZA++ implementation (Och and Ney, 2003) of
IBM Model 4 is used as the baseline for word
alignment: Model 4 is incrementally trained by
performing 5 iterations of Model 1, 5 iterations
of HMM, 3 iterations of Model 3, and 3 iter-
ations of Model 4. For phrase extraction the
grow-diag-final heuristics described in (Koehn et
al., 2003) is used to derive the refined alignment
from bidirectional alignments. We then perform
MERT process (Och, 2003) which optimizes the
BLEU metric, while a 5-gram language model is
derived with Kneser-Ney smoothing (Kneser and
Ney, 1995) trained with SRILM (Stolcke, 2002).
For the HPBSMT system, the chart-based decoder
of Moses (Koehn et al, 2007) is used. Most of the
procedures are identical with the PBSMT systems
except the rule extraction process (Chiang, 2005).
The procedures to handle three kinds of se-
mantics are implemented using the already men-
tioned algorithm. We use libSVM (Chang and Lin,
2011), and Mallet (McCallum, 2002) for Latent
Dirichlet Allocation (LDA) (Blei et al, 2003).
For the corpus, we used all the resources pro-
vided for the translation task at WMT13 for lan-
output layer
2?layer conditional RBM language model
ngram language model
1st RBM
2nd RBM
hidden layer
output layer
N
projection layer
discrete representation
N
P
neural network
probability estimation
continuous?space language
model [Schwenk, 2007]
1st hidden layer
2?layer ngram?HMM language model
2nd hidden layer
output layer
ngram language model
Figure 3: Figure shows the three kinds of context-
dependent LM. The upper-side shows continuous-
space language model (Schwenk, 2007). The
lower-left shows ours, i.e. the 2-layer ngram-
HMM LM. The lower-right shows the 2-layer con-
ditional Restricted Boltzmann Machine LM (Tay-
lor and Hinton, 2009).
guage model, that is parallel corpora (Europarl
V7 (Koehn, 2005), Common Crawl corpus, UN
corpus, and News Commentary) and monolingual
corpora (Europarl V7, News Commentary, and
News Crawl from 2007 to 2012).
Experimental results are shown in Table 2.
The left-most column (sem-inform) shows our re-
sults. The sem-inform made a improvement of 0.8
BLEU points absolute compared to the PBSMT
results in EN-ES, while the standard system com-
bination lost 0.1 BLEU points absolute compared
to the single worst. For ES-EN, the sem-inform
made an improvement of 0.7 BLEU points abso-
lute compared to the PBSMT results. These im-
provements over both of PBSMT and HPBSMT
are statistically significant by a paired bootstrap
test (Koehn, 2004).
5 Conclusion
This paper describes shallow semantically-
informed HPBSMT and PBSMT systems devel-
oped at Dublin City University for participation in
the translation task at the Workshop on Statistical
Machine Translation (WMT 13). Our system has
181
EN-ES sem-inform PBSMT HPBSMT syscomb aug-syscomb
BLEU 30.3 29.5 28.2 28.1 28.5
BLEU(11b) 30.3 29.5 28.2 28.1 28.5
BLEU-cased 29.0 28.4 27.1 27.0 27.5
BLEU-cased(11b) 29.0 28.4 27.1 27.0 27.5
NIST 7.91 7.74 7.35 7.35 7.36
Meteor 0.580 0.579 0.577 0.577 0.578
WER 53.7 55.4 59.3 59.2 58.9
PER 41.3 42.4 46.0 45.8 45.5
ES-EN sem-inform PBSMT HPBSMT syscomb aug-syscomb
BLEU 31.1 30.4 23.1? 28.8 29.9
BLEU(11b) 31.1 30.4 23.1? 28.8 29.9
BLEU-cased 29.7 29.1 22.3? 27.9 28.8
BLEU-cased(11b) 29.7 29.1 22.3? 27.9 28.8
NIST 7.87 7.79 6.67? 7.40 7.71
Meteor 0.615 0.612 0.533? 0.612 0.613
WER 54.8 55.4 62.5? 59.3 56.1
PER 41.3 41.8 48.3? 45.8 41.9
Table 2: Table shows the score where ?sem-inform? shows our system. Underlined figure shows the
official score. ?syscomb? denotes the confusion-network-based system combination using BLEU, while
?aug-syscomb? uses three shallow semantics described in QE score (Okita et al, 2012a), genre ID (Okita
et al, 2012b), and context ID (Okita, 2012). Note that the inputs for syscomb and aug-syscomb are the
output of HPBSMT and PBSMT. HPBSMT from ES to EN has marked with ?, which indicates that this
is trained only with Europarl V7.
2-layer ngram- SRI-
EN HMM LM LM
newstest12 130.4 140.3
newstest11 146.2 157.1
newstest10 156.4 166.8
newstest09 176.3 187.1
Table 3: Table shows the perplexity of context-
dependent language models, which is 2-layer
ngram HMM LM, and that of SRILM (Stolcke,
2002) in terms of newstest09 to 12.
PBSMT and HPBSMT decoders with multiple
LMs, but our system will execute only one path,
which is different from multi-engine system
combination. We consider investigate three types
of shallow semantic information: (i) a Quality
Estimate (QE) score, (ii) genre ID, and (iii) a
context ID through context-dependent language
models. Our experimental results show that the
improvement is 0.8 points absolute (BLEU) for
EN-ES and 0.7 points for ES-EN compared to
the standard PBSMT system (single best system).
We developed this method when the standard
(confusion network-based) system combination is
ineffective such as in the case when the input is
only two.
A further avenue would be the investigation of
other semantics such as linguistic semantics, in-
cluding co-reference resolution or anaphora reso-
lution, hyper-graph decoding, and text understand-
ing. Some of which are investigated in the context
of textual entailment task (Okita, 2013b) and we
would like to extend this to SMT task. Another
investigation would be the integration of genre ID
into the context-dependent LM. The preliminary
work shows that such integration would decrease
the overall perplexity (Okita, 2013a).
Acknowledgments
We thank Antonio Toral and Santiago Corte?s
Va??lo for providing parts of their processing
data. This research is supported by the Science
Foundation Ireland (Grant 07/CE/I1142) as part
of the Centre for Next Generation Localisation
(http://www.cngl.ie) at Dublin City Uni-
versity.
182
References
David Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. Journal of Ma-
chine Learning Research, 3:9931022.
Phil Blunsom and Trevor Cohn. 2011. A hierarchical
pitman-yor process hmm for unsupervised part of
speech induction. In Proceedings of Annual Meet-
ing of the Association for Computational Linguistics
(ACL11), pages 865?874.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technology,
2:27:1?27:27.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of the 43th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL-05), pages
263?270.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of the Sixth International Conference
on Language Resources and Evaluation (LREC-
2006), pages 449?454.
Reinhard Kneser and Hermann Ney. 1995. Im-
proved backing-off for n-gram language modeling.
In Proceedings of the IEEE International Confer-
ence on Acoustics, Speech and Signal Processing,
pages 181?184.
Philipp Koehn, Franz Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceed-
ings of the Human Language Technology Confer-
ence of the North American Chapter of the Associ-
ation for Computationa Linguistics (HLT / NAACL
2003), pages 115?124.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for Statistical Machine Translation. In
Proceedings of the 45th Annual Meeting of the As-
sociation for Computational Linguistics Companion
Volume Proceedings of the Demo and Poster Ses-
sions, pages 177?180.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP 2004), pages 388?395.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proceedings of the
Machine Translation Summit, pages 79?86.
Philipp Koehn. 2010. Statistical machine translation.
Cambridge University Press.
Hai-Son Le, Thomas Lavergne, Alexandre Allauzen,
Marianna Apidianaki, Li Gong, Aurelien Max,
Artem Sokolov, Guillaume Wisniewski, and Fran-
cois Yvon. 2012. Limsi at wmt12. In Proceed-
ings of the Seventh Workshop on Statistical Machine
Translation, pages 330?337.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Franz Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1):19?51.
Franz Och. 2003. Minimum Error Rate Training in
Statistical Machine Translation. In Proceedings of
the 41st Annual Meeting of the Association for Com-
putational Linguistics, pages 160?167.
Tsuyoshi Okita, Alfredo Maldonado Guerra, Yvette
Graham, and Andy Way. 2010a. Multi-Word Ex-
pression sensitive word alignment. In Proceed-
ings of the Fourth International Workshop On Cross
Lingual Information Access (CLIA2010, collocated
with COLING2010), Beijing, China., pages 1?8.
Tsuyoshi Okita, Jie Jiang, Rejwanul Haque, Hala Al-
Maghout, Jinhua Du, Sudip Kumar Naskar, and
Andy Way. 2010b. MaTrEx: the DCU MT System
for NTCIR-8. In Proceedings of the MII Test Col-
lection for IR Systems-8 Meeting (NTCIR-8), pages
377?383.
Tsuyoshi Okita, Raphae?l Rubino, and Josef van Gen-
abith. 2012a. Sentence-level quality estima-
tion for mt system combination. In Proceedings
of ML4HMT Workshop (collocated with COLING
2012), pages 55?64.
Tsuyoshi Okita, Antonio Toral, and Josef van Gen-
abith. 2012b. Topic modeling-based domain adap-
tation for system combination. In Proceedings
of ML4HMT Workshop (collocated with COLING
2012), pages 45?54.
Tsuyoshi Okita. 2012. Neural Probabilistic Language
Model for System Combination. In Proceedings
of ML4HMT Workshop (collocated with COLING
2012), pages 65?76.
Tsuyoshi Okita. 2013a. Joint space neural probabilis-
tic language model for statistical machine transla-
tion. Technical Report at arXiv, 1301(3614).
Tsuyoshi Okita. 2013b. Local graph matching with
active learning for recognizing inference in text at
ntcir-10. NTCIR 10 Conference, pages 499?506.
Antti-Veikko I. Rosti, Spyros Matsoukas, and Richard
Schwartz. 2007. Improved word-level system com-
bination for machine translation. In Proceedings of
the 45th Annual Meeting of the Association for Com-
putational Linguistics, pages 312?319.
183
Holger Schwenk, Anthony Rousseau, and Mohammed
Attik. 2012. Large, pruned or continuous space lan-
guage models on a gpu for statistical machine trans-
lation. In NAACL-HLT workshop on the Future of
Language Modeling for HLT, pages 11?19.
Holger Schwenk. 2007. Continuous space language
models. Computer Speech and Language, 21:492?
518.
Paul Smolensky. 1986. Chapter 6: Information pro-
cessing in dynamical systems: Foundations of har-
mony theory. In Rumelhart, David E.; McLel-
land, James L. Parallel Distributed Processing:
Explorations in the Microstructure of Cognition,
1:194281.
Lucia Specia, D. Raj, and Marco Turchi. 2010. Ma-
chine translation evaluation versus quality estima-
tion. Machine Translation, Springer, 24(1):39?50.
Andreas Stolcke. 2002. SRILM ? An extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Process-
ing, pages 901?904.
Graham Taylor and Geoffrey Hinton. 2009. Factored
conditional restricted boltzmann machines for mod-
eling motion style. In Proceedings of the 26th Inter-
national Conference on Machine Learning (ICML),
pages 1025?1032.
Yee Whye Teh. 2006. A hierarchical bayesian lan-
guage model based on pitman-yor processes. In
Proceedings of the 44th Annual Meeting of the As-
sociation for Computational Linguistics (ACL-06),
Prague, Czech Republic, pages 985?992.
Daniel H. Younger. 1967. Recognition and parsing of
context-free languages in time n3. Information and
Control, 10(2):189208.
184
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 215?220,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
DCU-Lingo24 Participation in WMT 2014 Hindi-English Translation task
Xiaofeng Wu, Rejwanul Haque*, Tsuyoshi Okita
Piyush Arora, Andy Way, Qun Liu
CNGL, Centre for Global Intelligent Content
School of Computing, Dublin City University
Dublin 9, Ireland
{xf.wu,tokita,parora,away,qliu}@computing.dcu.ie
*Lingo24, Edinburgh, UK
rejwanul.haque@lingo24.com
Abstract
This paper describes the DCU-Lingo24
submission to WMT 2014 for the Hindi-
English translation task. We exploit
miscellaneous methods in our system,
including: Context-Informed PB-SMT,
OOV Word Conversion (OWC), Multi-
Alignment Combination (MAC), Oper-
ation Sequence Model (OSM), Stem-
ming Align and Normal Phrase Extraction
(SANPE), and Language Model Interpola-
tion (LMI). We also describe various pre-
processing steps we tried for Hindi in this
task.
1 Introduction
This paper describes the DCU-Lingo24 submis-
sion to WMT 2014 for the Hindi-English transla-
tion task.
All our experiments on WMT 2014 are built
upon the Moses phrase-based model (PB-SMT)
(Koehn et al., 2007) and tuned with MERT
(Och, 2003). Starting from this baseline system,
we exploit various methods including Context-
Informed PB-SMT (CIPBSMT), zero-shot learn-
ing (Palatucci et al., 2009) using neural network-
based language modelling (Bengio et al., 2000;
Mikolov et al., 2013) for OOV word conversion,
various lexical reordering models (Axelrod et al.,
2005; Galley and Manning, 2008), various Mul-
tiple Alignment Combination (MAC) (Tu et al.,
2012), Operation Sequence Model (OSM) (Dur-
rani et al., 2011) and Language Model Interpola-
tion(LMI).
In the next section, the preprocessing steps are
explained. In Section 3 a detailed explanation of
the technique we exploit is provided. Then in Sec-
tion 4, we provide our experimental results and re-
sultant discussion.
2 Pre-processing Steps
We use all the training data provided for Hindi?
English translation. Following Bojar et al. (2010),
we apply a number of normalisation methods on
the Hindi corpus. The HindEnCorp parallel cor-
pus compiles several sources of parallel data. We
observe that the source-side (Hindi) of the TIDES
data source contains font-related noise, i.e. many
Hindi sentences are a mixture of two different en-
codings: UTF-8
1
and WX
2
notations. We pre-
pared a WX-to-UTF-8 font conversion script for
Hindi which converts all WX encoded characters
into UTF-8, thus removing all WX encoding ap-
pearing in the TIDES data.
We also observe that a portion of the English
training corpus contained the following bracket-
like sequences of characters: -LRB-, -LSB-, -
LCB-, -RRB-, -RSB-, and -RCB-.
3
For consis-
tency, those character sequences in the training
data were replaced by the corresponding brackets.
For English ? both monolingual and the target
side of the bilingual data ? we perform tokeniza-
tion, normalization of punctuation, and truecasing.
For parallel training data, we filter sentences pairs
containing more than 80 tokens on either side and
1
http://en.wikipedia.org/wiki/UTF-8
2
http://en.wikipedia.org/wiki/WX_notation
3
The acronyms stand for (Left|Right)
(Round|Square|Curly) Bracket.
215
sentence pairs with length difference larger than 3
times.
3 Techniques Deployed
3.1 Combination of Various Lexical
Reordering Model (LRM)
Clearly, Hindi and English have quite different
word orders, so we adopt three lexical reordering
models to address this problem. They are word-
based LRM and phrase-based LRM, which mainly
focus on local reordering phenomena, and hierar-
chical phrase-based LRM, which mainly focuses
on longer distance reordering (Galley and Man-
ning, 2008).
3.2 Operation Sequence Model
The Operation Sequence Model (OSM) of Dur-
rani et al. (2011) defines four translation opera-
tions: Generate(X,Y), Continue Source Concept,
Generate Source Only (X) and Generate Identical,
as well as three reordering operations: Insert Gap,
Jump Back(W) and Jump Forward.
The probability of an operation sequence O =
(o
1
o
2
? ? ? o
J
) is calculated as in (1):
p(O) =
J
?
j=1
p(o
j
|o
j?n+1
? ? ? o
j?1
) (1)
where n indicates the number of previous opera-
tions used.
We employ a 9-order OSM in our framework.
3.3 Language Model Interpolation (LMI)
We build a large language model by including data
from the English Gigaword fifth edition, the En-
glish side of the UN corpus, the English side of the
10
9
French?English corpus and the English side of
the Hindi?English parallel data provided by the or-
ganisers. We interpolate language models trained
using each dataset, with the monolingual data pro-
vided split into three parts (news 2007-2013, Eu-
roparl (?) and news commentary) and the weights
tuned to minimize perplexity on the target side of
the devset.
The language models in our systems are trained
with SRILM (Stolcke, 2002). We train a 5-gram
model with Kneser-Ney discounting (Chen and
Goodman, 1996).
3.4 Context-informed PB-SMT
Haque et al. (2011) express a context-dependent
phrase translation as a multi-class classification
problem, where a source phrase with given addi-
tional context information is classified into a dis-
tribution over possible target phrases. The size of
this distribution needs to be limited, and would
ideally omit irrelevant target phrase translations
that the standard PB-SMT (Koehn et al., 2003) ap-
proach would normally include. Following Haque
et al. (2011), we derive a context-informed feature
?
h
mbl
that is expressed as the conditional probabil-
ity of the target phrase e?
k
given the source phrase
?
f
k
and its context information (CI), as in (2):
?
h
mbl
= log P(e?
k
|
?
f
k
,CI(
?
f
k
)) (2)
Here, CI may include any feature that can pro-
vide useful information to disambiguate the given
source phrase. In our experiment, we use CCG su-
pertag (Steedman, 2000) as a contextual features.
CCG supertag expresses the specific syntactic be-
haviour of a word in terms of the arguments it
takes, and more generally the syntactic environ-
ment in which it appears.
We consider the CCG supertags of the context
words, as well as of the focus phrase itself. In our
model, the supertag of a multi-word focus phrase
is the concatenation of the supertags of the words
composing that phrase. We generate a window
of size 2l + 1 features (we set l:=2), including
the concatenated complex supertag of the focus
phrase. Accordingly, the supertag-based contex-
tual information (CI
st
) is described as in (3):
CI
st
(
?
f
k
) = {st(f
i
k
?l
), ..., st(f
i
k
?1
), st(
?
f
k
),
st(f
j
k
+1
), ..., st(f
j
k
+l
)}
(3)
For the Hindi-to-English translation task, we use
part-of-speech (PoS) tags
4
of the source phrase
and the neighbouring words as the contextual fea-
ture, owing to the fact that supertaggers are readily
available only for English.
We use a memory-based machine learning
(MBL) classifier (TRIBL: (Daelemans, 2005))
5
that is able to estimate P(e?
k
|
?
f
k
,CI(
?
f
k
)) by
similarity-based reasoning over memorized
nearest-neighbour examples of source?target
phrase translations. Thus, we derive the feature
?
h
mbl
defined in Equation (2). In addition to
?
h
mbl
,
4
In order to obtain PoS tags of Hindi words,
we used the LTRC shallow parser for Hindi from
http://ltrc.iiit.ac.in/analyzer/hindi/shallow-parser-hin-
4.0.fc8.tar.gz.
5
An implementation of TRIBL is freely available as part
of the TiMBL software package, which can be downloaded
from http://ilk.uvt.nl/timbl.
216
we derive a simple two-valued feature
?
h
best
,
defined in Equation (4):
?
h
best
=
{
1 if e?
k
maximizes P(e?
k
|
?
f
k
,CI(
?
f
k
))
u 0 otherwise
(4)
where
?
h
best
is set to 1 when e?
k
is one of the tar-
get phrases with highest probability according to
P(e?
k
|
?
f
k
,CI(
?
f
k
)) for each source phrase
?
f
k
; oth-
erwise
?
h
best
is set to 0.000001. We performed ex-
periments by integrating these two features
?
h
mbl
and
?
h
best
directly into the log-linear model of
Moses. Their weights are optimized using mini-
mum error-rate training (MERT)(Och, 2003) on a
held-out development set for each of the experi-
ments.
3.5 Morphological Segmentation
Haque et al. (2012) applied a morphological suffix
separation process in a Bengali-to-English trans-
lation task and showed that suffix separation sig-
nificantly reduces data sparseness in the Bengali
corpus. They also showed an SMT model trained
on the suffix-stripped training data significantly
outperforms the state-of-the-art PB-SMT baseline.
Like Bengali, Hindi is a morphologically very rich
and highly inflected Indian language. As done
previously for Bengali-to-English (Haque et al.,
2012), we employ a suffix-stripping method for
lemmatizing inflected Hindi words in the WMT
Hindi-to-English translation task. Following Das-
gupta and Ng (2006), we developed an unsu-
pervised morphological segmentation method for
Hindi. We also used a Hindi lightweight stem-
mer (Ramanathan and Rao, 2003) in order to pre-
pare a training corpus with only Hindi stems. We
prepared Hindi-to-English SMT systems on the
both types of training data (i.e. suffix-stripped and
stemmed).
6
3.6 Multi-Alignment Combination (MAC)
Word alignment is a critical component of MT
systems. Various methods for word alignment
have been proposed, and different models can pro-
duce signicantly different outputs. For example,
Tu et al. (2012) demonstrates that the alignment
agreement between the two best-known alignment
tools, namely Giza++(Och and Ney, 2003) and
6
Suffixes were separated and completely removed from
the training data.
the Berkeley aligner
7
(Liang et al., 2006), is be-
low 70%. Taking into consideration the small size
of the the corpus, in order to extract more ef-
fective phrase tables, we concatenate three align-
ments: Giza++ with grow-diag-final-and, Giza++
with intersection, and that derived from the Berke-
ley aligner.
3.7 Stemming Alignment and Normal Phrase
Extraction (SANPE)
The rich morphology of Hindi will cause word
alignment sparsity, which results in poor align-
ment quality. Furthermore, word stemming on
the Hindi side usually results in too many English
words being aligned to one stemmed Hindi word,
i.e. we encounter the problem of phrase over-
extraction. Therefore, we conduct word alignment
with the stemmed version of Hindi, and then at
the phrase extraction step, we replace the stemmed
form with the original Hindi form.
3.8 OOV Word Conversion Method
Our algorithm for OOV word conversion uses the
recently developed zero-shot learning (Palatucci
et al., 2009) using neural network language mod-
elling (Bengio et al., 2000; Mikolov et al., 2013).
The same technique is used in (Okita et al., 2014).
This method requires neither parallel nor compa-
rable corpora, but rather two monolingual corpora.
In our context, we prepare two monolingual cor-
pora on both sides, which are neither parallel nor
comparable, and a small amount of already known
correspondences between words on the source and
target sides (henceforth, we refer to this as the
?dictionary?). Then, we train both sides with the
neural network language model, and use a contin-
uous space representation to project words to each
other on the basis of a small amount of correspon-
dences in the dictionary. The following algorithm
shows the steps involved:
1. Prepare the monolingual source and target
sentences.
2. Prepare the dictionary which consists of U
entries of source and target sentences com-
prising non-stop-words.
3. Train the neural network language model on
the source side and obtain the real vectors of
X dimensions for each word.
7
http://code.google.com/p/berkeleyaligner/
217
4. Train the neural network language model on
the target side and obtain the real vectors of
X dimensions for each word.
5. Using the real vectors obtained in the above
steps, obtain the linear mapping between the
dictionary items in two continuous spaces us-
ing canonical component analysis (CCA).
In our experiments we use U the same as the en-
tries of Wiki corpus, which is provided among
WMT14 corpora, and X as 50. The resulted pro-
jection by this algorithm can be used as the OOV
word conversion which projects from the source
language which among OOV words into the tar-
get language. The overall algorithm which uses
the projection which we build in the above step is
shown in the following.
1. Collect unknown words in the translation out-
puts.
2. Do Hindi named-entity recognition (NER) to
detect noun phrases.
3. If they are noun phrases, do the projection
from each unknown word in the source side
into the target words (We use the projection
prepared in the above steps). If they are not
noun phrases, run the transliteration to con-
vert each of them.
We perform Hindi NER by training CRF++ (Kudo
et al., 2004) using the Hindi named entity corpus,
and use the Hindi shallow parser (Begum et al.,
2008) for preprocessing of the inputs.
4 Results and Discussion
4.1 Data
We conduct our experiments on the standard
datasets released in the WMT14 shared translation
task. We use HindEnCorp
8
(Bojar et al., 2014)
parallel corpus for MT system building. We also
used the CommonCrawl Hindi monolingual cor-
pus (Bojar et al., 2014) in order to build an addi-
tional language model for Hindi.
For the Hindi-to-English direction, we also em-
ployed monolingual English data used in the other
translation tasks for building the English language
model.
8
http://ufallab.ms.mff.cuni.cz/ bojar/hindencorp/
4.2 Moses Baseline
We employ a standard Moses PB-SMT model as
our baseline. The Hindi side is preprocessed but
unstemmed. We use Giza++ to perform word
alignment, the phrase table is extracted via the
grow-diag-final-and heuristic and the max-phrase-
length is set to 7.
4.3 Automatic Evaluation
Experiments BLEU
Moses Baseline 8.7
Context-Based 9.4
Context-Based + CommonCrawl LM 11.4
Table 1: BLEU scores of the English-to-Hindi MT
Systems on the WMT test set.
Experiments BLEU
Moses Baseline 10.1
Context-Based 10.1
Suffix-Stripped 10.0
OWC 11.2
OSM 10.3
Three LRMs 10.5
MAC 10.7
SANPE 10.6
LMI 10.9
LMI+SANPE+MAC+ThreeLRMs+OSM 11.7
Table 2: BLEU scores of the Hindi-to-English MT
Systems on the WMT test set.
We prepared a number of MT systems for both
English-to-Hindi and Hindi-to-English, and sub-
mitted their runs in the WMT 2014 Evaluation
Matrix. The BLEU scores of the different English-
to-Hindi MT systems (Moses Baseline, Context-
Based (CCG) MT system, and Context-Based
(CCG) MT system with an additional LM built
on the CommonCrawl Hindi monolingual corpus
(Bojar et al., 2014)) on the WMT 2014 English-
to-Hindi test set are reported in Table 1. As can
be seen from Table 1, Context-Based (CCG) MT
system produces 0.7 BLEU points improvement
(8.04% relative) over the Moses Baseline. When
we add an additional large LM built on the Com-
monCrawl data to the Context-Based (CCG) MT
system, we achieved a 2 BLEU-point improve-
ment (21.3% relative) (cf. last row in Table 1) over
218
the Context-Based (CCG) MT system.
9
The BLEU scores of the different Hindi-to-
English MT systems on the WMT 2014 Hindi-
to-English test set are reported in Table 2. The
first row of Table 2 shows the BLEU score for
the Baseline MT system. We note that the per-
formance of the Context-Based (PoS) MT system
obtains identical performance to the Moses base-
line (10.1 BLEU points) on the WMT 2014 Hindi-
to-English test set.
We employed a source language (Hindi) nor-
malisation technique, namely suffix separation,
but unfortunately this did not bring about any
improvement for the Hindi-to-English translation
task. The improvement gained by individually
employing OSM, three lexical reordering mod-
els, Multi-alignment Combination, Stem-align and
normal Phrase Extraction and Language Model In-
terpolation can be seen in Table 2. Our best sys-
tem is achieved by combining OSM, Three LMR,
MAC, SANPE and LMI, which results in a 1.6
BLEU point improvement over the Baseline.
5 Acknowledgments
This research is supported by the Science Foun-
dation Ireland (Grant 12/CE/I2267) as part of
the CNGL Centre for Global Intelligent Content
(www.cngl.ie) at Dublin City University.
References
Amittai Axelrod, Ra Birch Mayne, Chris Callison-
burch, Miles Osborne, and David Talbot. 2005. Ed-
inburgh system description for the 2005 iwslt speech
translation evaluation. In Proceedings of the Inter-
national Workshop on Spoken Language Translation
(IWSLT).
Rafiya Begum, Samar Husain, Arun Dhwaj,
Dipti Misra Sharma, Lakshmi Bai, and Rajeev
Sangal. 2008. Dependency annotation scheme for
indian languages. In Proceedings of The Third In-
ternational Joint Conference on Natural Language
Processing (IJCNLP).
Yoshua Bengio, Rejean Ducharme, and Pascal Vincent.
2000. A neural probabilistic language model. In
Proceedings of Neural Information Systems.
Ond Bojar, Pavel Stranak, and Daniel Zeman. 2010.
Data issues in english-to-hindi machine translation.
In LREC.
9
Please note that this is an unconstrained submission.
Ondrej Bojar, V. Diatka, Rychly P., Pavel Stranak,
A. Tamchyna, and Daniel Zeman. 2014. Hindi-
english and hindi-only corpus for machine transla-
tion. In LREC.
Stanley F. Chen and Joshua Goodman. 1996. An em-
pirical study of smoothing techniques for language
modeling. In Proceedings of the 34th Annual Meet-
ing on Association for Computational Linguistics,
ACL ?96, pages 310?318, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Walter Daelemans. 2005. Memory-based language
processing. Cambridge University Press.
Sajib Dasgupta and Vincent Ng. 2006. Unsupervised
morphological parsing of bengali. Language Re-
sources and Evaluation, 40(3-4):311?330.
Nadir Durrani, Helmut Schmid, and Alexander Fraser.
2011. A joint sequence translation model with in-
tegrated reordering. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies - Vol-
ume 1, HLT ?11, pages 1045?1054, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Process-
ing, pages 848?856, Honolulu, Hawaii, October. As-
sociation for Computational Linguistics.
Rejwanul Haque, Sudip Kumar Naskar, Antal van den
Bosch, and Andy Way. 2011. Integrating source-
language context into phrase-based statistical ma-
chine translation. Machine translation, 25(3):239?
285.
Rejwanul Haque, Sergio Penkale, Jie Jiang, and Andy
Way. 2012. Source-side suffix stripping for bengali-
to-english smt. In Asian Language Processing
(IALP), 2012 International Conference on, pages
193?196. IEEE.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 48?54. Association for Computa-
tional Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ond?rej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
ACL ?07, pages 177?180, Stroudsburg, PA, USA.
Association for Computational Linguistics.
219
Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.
2004. Appliying conditional random fields to
japanese morphological analysis. In Proceedings of
EMNLP.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of the main
conference on Human Language Technology Con-
ference of the North American Chapter of the As-
sociation of Computational Linguistics, pages 104?
111. Association for Computational Linguistics.
Tomas Mikolov, Quoc V. Le, and Ilya Sutskever. 2013.
Exploiting similarities among languages for ma-
chine translation. ArXiv.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics - Volume 1, ACL ?03, pages 160?
167, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Tsuyoshi Okita, Ali Hosseinzadeh Vahid, Andy Way,
and Qun Liu. 2014. Dcu terminology translation
system for medical query subtask at wmt14.
Mark Palatucci, Dean Pomerleau, Geoffrey Hinton,
and Tom Mitchell. 2009. Zero-shot learning with
semantic output codes. In Neural Information Pro-
cessing Systems (NIPS), December.
Ananthakrishnan Ramanathan and Durgesh D Rao.
2003. A lightweight stemmer for hindi. In the Pro-
ceedings of EACL.
Mark Steedman. 2000. The syntactic process, vol-
ume 35. MIT Press.
Andreas Stolcke. 2002. Srilm ? an extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference Spoken Language Processing,
pages 901?904, Denver, CO.
Zhaopeng Tu, Yang Liu, Yifan He, Josef van Genabith,
Qun Liu, and Shouxun Lin. 2012. Combining mul-
tiple alignments to improve machine translation. In
COLING (Posters), pages 1249?1260.
220
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 239?245,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
The DCU Terminology Translation System for the Medical Query Subtask
at WMT14
Tsuyoshi Okita, Ali Hosseinzadeh Vahid, Andy Way, Qun Liu
Dublin City University, School of Computing
Glasnevin, Dublin 9
Ireland
{tokita,avahid,away,qliu}@computing.dcu.ie
Abstract
This paper describes the Dublin City
University terminology translation system
used for our participation in the query
translation subtask in the medical trans-
lation task in the Workshop on Statisti-
cal Machine Translation (WMT14). We
deployed six different kinds of terminol-
ogy extraction methods, and participated
in three different tasks: FR?EN and EN?
FR query tasks, and the CLIR task. We
obtained 36.2 BLEU points absolute for
FR?EN and 28.8 BLEU points absolute
for EN?FR tasks where we obtained the
first place in both tasks. We obtained 51.8
BLEU points absolute for the CLIR task.
1 Introduction
This paper describes the terminology translation
system developed at Dublin City University for
our participation in the query translation subtask at
the Workshop on Statistical Machine Translation
(WMT14). We developed six kinds of terminol-
ogy extraction methods for the problem of medi-
cal terminology translation, especially where rare
and new words are considered. We have several
motivations which we address before providing a
description of the actual algorithms undeprinning
our work.
First, terminology translation cannot be seen
just as a simple extension of the translation process
if we use an analogy from human translation. Ter-
minology translation can be considered as more
important and a quite different task than transla-
tion per se, so we need a considerably different
way of solving this particular problem. Bilingual
terminology selection has been claimed to be the
touchstone in human translation, especially where
scientific and legal translation are concerned. Ter-
minology selection is often the hardest and most
time-consuming process in the translation work-
flow. Depending on the particular requirements of
the use-case (Way, 2013), users may not object to
disfluent translations, but will invariably be very
sensitive to the wrong selection of terminology,
even if the meaning of the chosen terms is correct.
This is especially true if this selected terminology
does not match with that preferred by the users
themselves, in which case users are likely to ex-
press some kind of complaint; it may even be that
the entire translation is rejected as sub-standard or
inappropriate on such grounds.
Second, we look at how to handle new and rare
words. If we inspect the process of human trans-
lation more closely, it is easy to identify several
differences compared to the methods used in sta-
tistical MT (SMT). Unless stipulated by the client,
the selection of bilingual terminology can be a
highly subjective process. Accordingly, it is not
necessarily the bilingual term-pair with the highest
probability that is chosen by the human translator.
It is often the case that statistical methods often
forget about or delete less frequent n-grams, but
rely on more frequent n-grams using maximum
likelihood or Maximum A Priori (MAP) meth-
ods. If some terminology is highly suitable, a
human translator can use it quite freely. Further-
more, there are a lot of new words in reality for
which new target equivalents have to be created by
the translators themselves, so the question arises
as to how human translators actually select ap-
propriate new terminology. Transliteration, which
is often supported by many Asian languages in-
cluding Hindi, Japanese, and Chinese, is perhaps
the easiest things to do under such circumstances.
Slight modifications of alphabets/accented charac-
ters can sometimes successfully create a valid new
term, even for European languages.
The remainder of this paper is organized as fol-
lows. Section 2 describes our algorithms. Our
decoding strategy in Section 3. Our experimen-
239
tal settings and results are presented in Section 4,
and we conclude in Section 5.
2 Our Methods
Apart from the conventional statistical approach to
extract bilingual terminology, this medical query
task reminds us of two frequently occurring prob-
lems which are often ignored: (i) ?Can we forget
about terminology which occurs only once in a
corpus??, and (ii) ?What can we do if the termi-
nology does not occur in a corpus?? These two
problems require computationally quite different
approaches than what is usually done in the stan-
dard statistical approach. Furthermore, the medi-
cal query task in WMT14 provides a wide range of
corpora: parallel and monolingual corpora, as well
as dictionaries. These two interesting aspects mo-
tivate our extraction methods which we present in
this section, including one relatively new Machine
Learning algorithm of zero-shot learning arising
from recent developments in the neural network
community (Bengio et al., 2000; Mikolov et al.,
2013b).
2.1 Translation Model
Word alignment (Brown et al., 1993) and phrase
extraction (Koehn et al., 2003) can capture bilin-
gual word- and phrase-pairs with a good deal of
accuracy. We omit further details of these stan-
dard methods which are freely available elsewhere
in the SMT literature (e.g. (Koehn, 2010)).
2.2 Extraction from Parallel Corpora
(Okita et al., 2010) addressed the problem of
capturing bilingual term-pairs from parallel data
which might otherwise not be detected by the
translation model. Hence, the requirement in
Okita et al. is not to use SMT/GIZA++ (Och and
Ney, 2003) to extract term-pairs, which are the
common focus in this medical query translation
task.
The classical algorithm of (Kupiec, 1993) used
in (Okita et al., 2010) counts the statistics of ter-
minology c(e
term
i
, f
term
j
|s
t
) on the source and
the target sides which jointly occur in a sentence
s
t
after detecting candidate terms via POS tag-
ging, which are then summed up over the entire
corpus
?
N
t=1
c(e
term
i
, f
term
j
|s
t
). Then, the al-
gorithm adjusts the length of e
term
i
and f
term
j
.
It can be said that this algorithm captures term-
pairs which occur rather frequently. However, this
apparent strength can also be seen in disadvanta-
geous terms since the search for terminology oc-
curs densely in each of the sentences which in-
creases the computational complexity of this algo-
rithm, and causes the method to take a consider-
able time to run. Furthermore, if we suppose that
most frequent term-pairs are to be extracted via a
standard translation model (as described briefly in
the previous section), our efforts to search among
frequent pairs is not likely to bring about further
gain.
It is possible to approach this in a reverse man-
ner: ?less frequent pairs can be outstanding term
candidates?. Accordingly, if our aim changes to
capture only those less frequent pairs, the situation
changes dramatically. The number of terms we
need to capture is considerably decreased. Many
sentences do not include any terminology at all,
and only a relatively small subset of sentences in-
cludes a few terms, such that term-pairs become
sparse with regard to sentences. Term-pairs can
be found rather easily if a candidate term-pair co-
occurs on the source and the target sides and on
the condition that the items in the term-pair actu-
ally correspond with one another.
This condition can be easily checked in various
ways. One way is to translate the source side of
the targeted pairs with the alignment option in the
Moses decoder (Koehn et al., 2007), which we did
in this evaluation campaign. Another way is to use
asupervised aligner, such as the Berkeley aligner
(Haghighi et al., 2009), to align the targeted pairs
and check whether they are actually aligned or not.
We assume two predefined sets of terms at
the outset, E
term
= {e
term
1
, . . . , e
term
n
} and
F
term
= {f
term
1
, . . . , f
term
n
}. We search for
possible alignment links between the term-pair
only when they co-occur in the same sentence.
One obvious advantage of this approach is the
computational complexity which is fairly low.
Note that the result of (Okita et al., 2010)
shows that the frequency-based approach of (Ku-
piec, 1993) worked well for NTCIR patent termi-
nology (Fujii et al., 2010), which otherwise would
have been difficult to capture via the traditional
SMT/GIZA++ method. In contrast, however, this
did not work well on the Europarl corpus (Koehn,
2005).
240
2.3 Terminology Dictionaries
Terminology dictionaries themselves are obvi-
ously among the most important resources for
bilingual term-pairs. In this medical query transla-
tion subtask, two corpora are provided for this pur-
pose: (i) Unified Medical Language System cor-
pus (UMLS corpus),
1
and (ii) Wiki entries.
2
2.4 Extraction from Terminology
Dictionaries: lower-order n-grams
Terminology dictionaries provide reliable higher-
order n-gram pairs. However, they do not often
provide the correspondences between the lower-
order n-grams contained therein. For example, the
UMLS corpus provides a term-pair of ?abdominal
compartment syndrome ||| syndrome du compar-
timent abdominal? (EN|||FR). However, such ter-
minology dictionaries often do not explicitly pro-
vide the correspondent pairs ?abdominal ||| ab-
dominal? (EN|||FR) or ?syndrome ||| syndrome?
(EN|||FR). Clearly, these terminology dictionaries
implicitly provide the correspondent pairs. Note
that UMLS and Wiki entries provide terminol-
ogy dictionaries. Hence, it is possible to obtain
some suggestion by higher order n-gram models if
we know their alignments between words on the
source and target sides. Algorithm 1 shows the
overall procedure.
Algorithm 1 Lower-order n-gram extraction algo-
rithm
1: Perform monolingual word alignment for
higher-order n-gram pairs.
2: Collect only the reliable alignment pairs (i.e.
discard unreliable alignment pairs).
3: Extract the lower-order word pairs of our in-
terest.
2.5 Extraction from Monolingual Corpora:
Transliteration and Abbreviation
Monolingual corpora can be used in various ways,
including:
1. Transliteration: Many languages support the
fundamental mechanism of between Euro-
pean and Asian languages. Japanese even
supports a special alphabet ? katakana ? for
this purpose. Chinese and Hindi also per-
mit transliteration using their own alphabets.
1
http://www.nlm.nih.gov/research/umls/.
2
http://www.wikipedia.org.
However, even among European languages,
this mechanism makes it possible to find
possible translation counterparts for a given
term. In this query task, we did this only
for the French-to-English direction and only
for words containing accented characters (by
rule-based conversion).
2. Abbreviation: It is often the case that abbre-
viations should be resolved in the same lan-
guage. If the translation includes some ab-
breviation, such as ?C. difficile?, this needs
to be investigated exhaustively in the same
language. However, in the specific domain
of medical terminology, it is quite likely that
possible phrase matches will be successfully
identified.
2.6 Extraction from Monolingual Corpora:
Zero-Shot Learning
Algorithm 2 Algorithm to connect two word em-
bedding space
1: Prepare the monolingual source and target
sentences.
2: Prepare the dictionary which consists of U
entries of source and target sentences among
non-stop-words.
3: Train the neural network language model on
the source side and obtain the continuous
space real vectors of X dimensions for each
word.
4: Train the neural network language model on
the target side and obtain the continuous space
real vectors of X dimensions for each word.
5: Using the real vectors obtained in the above
steps, obtain the linear mapping between the
dictionary in two continuous spaces using
canonical component analysis (CCA).
Another interesting terminology extraction
method requires neither parallel nor comparable
corpora, but rather just monolingual corpora on
both sides (possibly unrelated to each other) to-
gether with a small amount of dictionary entries
which provide already known correspondences
between words on the source and target sides
(henceforth, we refer to this as the ?dictionary?).
This method uses the recently developed zero-shot
learning (Palatucci et al., 2009) using neural net-
work language modelling (Bengio et al., 2000;
Mikolov et al., 2013b). Then, we train both sides
241
with the neural network language model, and use
a continuous space representation to project words
to each other on the basis of a small amount of
correspondences in the dictionary. If we assume
that each continuous space is linear (Mikolov et
al., 2013c), we can connect them via linear projec-
tion (Mikolov et al., 2013b). Algorithm 2 shows
this situation.
In our experiments we use U the same as the
entries of Wiki and X as 50. Algorithm 3 shows
the algorithm to extract the counterpart of OOV
words.
Algorithm 3 Algorithm to extract the counterpart
of OOV words.
1: Prepare the projection by Algorithm 2.
2: Detect unknown words in the translation out-
puts.
3: Do the projection of it (the source word) into
the target word using the trained linear map-
pings in the training step.
3 Decoding Strategy
We deploy six kinds of extraction methods: (1)
translation model, (2) extraction from parallel cor-
pora, (3) terminology dictionaries, (4) lower-order
n-grams, (5) transliteration and abbreviation, and
(6) zero-shot learning. Among these we deploy
four of them ? (2), (4), (5) and (6) ? in a limited
context, while the remaining two are used with-
out any context, mainly owing to time constraints;
only when we did not find the correspondent pairs
via (1) and (3), did we complement this by the
other methods.
The detected bilingual term-pairs using (1) and
(3) can be combined using various methods. One
way is to employ a method similar to (confu-
sion network-based) system combination (Okita
and van Genabith, 2011; Okita and van Genabith,
2012). First we make a lattice: if we regard one
candidate of (1) and two candidates in (3) as trans-
lation outputs where the words of two candidates
in (3) are connected using an underscore (i.e. one
word), we can make a lattice. Then, we can deploy
monotonic decoding over them. If we do this for
the devset and then apply it to the test set, we can
incorporate a possible preference learnt from the
development set, i.e. whether the query transla-
tor prefers method (1) or UMLS/Wiki translation.
MERT process and language model are applied in
a similar manner with (confusion network-based)
system combination (cf. (Okita and van Genabith,
2011)).
We note also that a lattice structure is useful for
handling grammatical coordination. Since queries
are formed by real users, reserved words for
database query such as ?AND? (or ?ET? (FR)) and
?OR? (or ?OU? (FR)) are frequently observed in
the test set. Furthermore, there is repeated use of
?and? more than twice, for example ?douleur ab-
nominal et Helicobacter pylori et cancer?, which
makes it very difficult to detect the correct coor-
dination boundaries. The lattice on the input side
can express such ambiguity at the cost of splitting
the source-side sentence in a different manner.
4 Experimental Results
The baseline is obtained in the following way. The
GIZA++ implementation (Och and Ney, 2003) of
IBM Model 4 is used as the baseline for word
alignment: Model 4 is incrementally trained by
performing 5 iterations of Model 1, 5 iterations of
HMM, 3 iterations of Model 3, and 3 iterations
of Model 4. For phrase extraction the grow-diag-
final heuristics described in (Koehn et al., 2003) is
used to derive the refined alignment from bidirec-
tional alignments. We then perform MERT (Och,
2003) which optimizes parameter settings using
the BLEUmetric (Papineni et al., 2002), while a 5-
gram language model is derived with Kneser-Ney
smoothing (Kneser and Ney, 1995) trained using
SRILM (Stolcke, 2002). We use the whole train-
ing corpora including the WMT14 translation task
corpora as well as medical domain data. UMLS
and Wikipedia are used just as training corpora for
the baseline.
For the extraction from parallel corpora (cf.
Section 2.2), we used Genia tagger (Tsuruoka and
Tsujii, 2005) and the Berkeley parser (Petrov and
Klein, 2007). For the zero-shot learning (cf. Sec-
tion 2.6) we used scikit learn (Pedregosa et al.,
2011), word2vec (Mikolov et al., 2013a), and a
recurrent neural network (Mikolov, 2012). Other
tools used are in-house software.
Table 2 shows the results for the FR?EN query
task. We obtained 36.2 BLEU points absolute,
which is an improvement of 6.3 BLEU point ab-
solute (21.1% relative) over the baseline. Table
3 shows the results for the EN?FR query task.
We obtained 28.8 BLEU points absolute, which
is an improvement of 8.7 BLEU points abso-
242
lute (43% relative) over the baseline. Our sys-
tem was the best system for both of these tasks.
These improvements over the baseline were sta-
tistically significant by a paired bootstrap test
(Koehn, 2004).
Query task FR?EN
Our method baseline
BLEU 36.2 29.9
BLEU cased 30.9 26.5
TER 0.340 0.443
Table 1: Results for FR?EN query task.
extraction LM MERT BLEU (cased)
(1) - (6) all Y 30.9
(1), (2), (3) all Y 30.3
(1), (3), (6) all Y 30.1
(1), (3), (4) all Y 29.1
(1), (3), (5) all Y 29.0
(1) and (3) all Y 29.0
(1) and (3) medical Y 27.5
(1) and (3) WMT Y 27.0
(1) and (3) medical N 25.1
(1) and (3) WMT N 24.3
(1) medical Y 25.9
(1) WMT Y 25.0
Table 2: Table shows the effects of extraction
methods, language model and MERT process. All
the measurements are by BLEU (cased). In this
table, ?medical? indicates a language model built
on all the medical corpora while ?WMT? indicates
a language model built on all the non-medical cor-
pora. Note that some sentence in testset can be
considered as non-medical domain. Extraction
methods (1) - (6) correspond to those described in
Section 2.1 - 2.6.
Table 4 shows the results for CLIR task. We
obtained 51.8 BLEU points absolute, which is an
improvement of 9.4 BLEU point absolute (22.2%
relative) over the baseline. Although CLIR task al-
lowed 10-best lists, our submission included only
1-best list. This resulted in the score of P@5 of
0.348 and P@10 of 0.346 which correspond to
the second place, despite a good result in terms
of BLEU. This is since unlike BLEU score P@5
and P@10 measure whether the whole elements
in reference and hypothesis are matched or not.
We noticed that our submission included a lot of
Query task EN?FR
Our method baseline
BLEU 28.8 20.1
BLEU cased 27.7 18.7
TER 0.483 0.582
Table 3: Results for EN?FR query task.
near miss sentences only in terms of capitaliza-
tion: ?abnominal pain and Helicobacter pylori and
cancer? (reference) and ?abnominal pain and heli-
cobacter pylori and cancer? (submission). These
are counted as incorrect in terms of P@5 and
P@10.
3
Noted that after submission we obtained
the revised score of P@5 of 0.560 and P@10 of
0.560 with the same method but with 2-best lists
which handles the capitalization varieties.
CLIR task FR?EN
Our method baseline
BLEU 51.8 42.2
BLEU cased 46.0 38.3
TER 0.364 0.398
P@5 0.348 (0.560
?
) ?
P@10 0.346 (0.560
?
) ?
NDCG@5 0.306 ?
NDCG@10 0.307 ?
MAP 0.2252 ?
Rprec 0.2358 ?
bpref 0.3659 ?
relRet 1524 ?
Table 4: Results for CLIR task.
5 Conclusion
This paper provides a description of the Dublin
City University terminology translation system for
our participation in the query translation subtask
in the medical translation task in the Workshop on
Statistical Machine Translation (WMT14). We de-
ployed six different kinds of terminology extrac-
tion methods. We obtained 36.2 BLEU points ab-
solute for FR?EN, and 28.8 BLEU points abso-
lute for EN?FR tasks, obtaining first place on both
tasks. We obtained 51.8 BLEU points absolute for
the CLIR task.
3
The method which incorporates variation in capitaliza-
tion in its n-best lists outperforms the best result in terms of
P@5 and P@10.
243
Acknowledgments
This research is supported by the Science Founda-
tion Ireland (Grant 07/CE/I1142) as part of CNGL
at Dublin City University.
References
Yoshua Bengio, Rejean Ducharme, and Pascal Vincent.
2000. A neural probabilistic language model. In
Proceedings of Neural Information Systems, pages
1137?1155.
Peter F. Brown, Vincent J.D Pietra, Stephen A.D.Pietra,
and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, Vol.19, Issue 2,
pages 263?311.
Atsushi Fujii, Masao Utiyama, Mikio Yamamoto,
Takehito Utsuro, Terumasa Ehara, Hiroshi Echizen-
ya, and Sayori Shimohata. 2010. Overview of the
patent translation task at the NTCIR-8 workshop.
In Proceedings of the 8th NTCIR Workshop Meet-
ing on Evaluation of Information Access Technolo-
gies: Information Retrieval, Question Answering
and Cross-lingual Information Access, pages 293?
302.
Aria Haghighi, John Blitzer, John DeNero, and Dan
Klein. 2009. Better word alignments with super-
vised itg models. In In Proceedings of the Confer-
ence of Association for Computational Linguistics,
pages 923?931.
Reinhard Kneser and Hermann Ney. 1995. Im-
proved backing-off for n-gram language modeling.
In Proceedings of the IEEE International Confer-
ence on Acoustics, Speech and Signal Processing,
pages 181?184.
Philipp Koehn, Franz Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceed-
ings of the Human Language Technology Confer-
ence of the North American Chapter of the Associ-
ation for Computationa Linguistics (HLT / NAACL
2003), pages 115?124.
Philipp Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit
for Statistical Machine Translation. In Proceedings
of the 45th Annual Meeting of the Association for
Computational Linguistics Companion Volume Pro-
ceedings of the Demo and Poster Sessions, pages
177?180.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP 2004), pages 388?395.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proceedings of the
Machine Translation Summit, pages 79?86.
Philipp Koehn. 2010. Statistical machine translation.
Cambridge University Press.
Julian. Kupiec. 1993. An algorithm for finding Noun
phrase correspondences in bilingual corpora. In
Proceedings of the 31st Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 17?22.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space. In Proceedings of Workshop
at International Conference on Learning Represen-
tations.
Tomas Mikolov, Quoc V. Le, and Ilya Sutskever.
2013b. Exploiting similarities among languages for
machine translation. ArXiv:1309.4168.
Tomas Mikolov, Wen tau Yih, and Geoffrey Zweig.
2013c. Linguistic regularities in continuous space
word representations. In Proceedings of the Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics / Human Lan-
guage Technology (NAACL/HLT 2005), pages 746?
751.
Tomas Mikolov. 2012. Statistical language models
based on neural networks. PhD thesis at Brno Uni-
versity of Technology.
Franz Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1):19?51.
Franz Och. 2003. Minimum Error Rate Training in
Statistical Machine Translation. In Proceedings of
the 41st Annual Meeting of the Association for Com-
putational Linguistics, pages 160?167.
Tsuyoshi Okita and Josef van Genabith. 2011. DCU
Confusion Network-based System Combination for
ML4HMT. Shared Task on Applying Machine
Learning techniques to optimising the division of
labour in Hybrid MT (ML4HMT-2011, collocated
with LIHMT-2011), pages 93?98.
Tsuyoshi Okita and Josef van Genabith. 2012. Mini-
mum Bayes Risk Decoding with Enlarged Hypoth-
esis Space in System Combination. In Proceed-
ings of 13th International Conference on Intelligent
Text Processing and Computational Linguistics (CI-
CLING 2012), pages 40?51.
Tsuyoshi Okita, Alfredo Maldonado Guerra, Yvette
Graham, and Andy Way. 2010. Multi-word
expression-sensitive word alignment. In Proceed-
ings of the Fourth International Workshop On Cross
Ling ual Information Access (CLIA2010, collocated
with COLING2010), pages 26?34.
244
Mark Palatucci, Dean Pomerleau, Geoffrey Hinton,
and Tom Mitchell. 2009. Zero-shot learning with
semantic output codes. In Neural Information Pro-
cessing Systems (NIPS), pages 1410?1418.
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2002.
BLEU: A Method For Automatic Evaluation of Ma-
chine Translation. In Proceedings of the 40th An-
nual Meeting of the Association for Computational
Linguistics (ACL-02).
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine learn-
ing in Python. Journal of Machine Learning Re-
search, 12:2825?2830.
Slav Petrov and Dan Klein. 2007. Learning and infer-
ence for hierarchically split PCFGs. In Proceedings
of AAAI (Nectar Track), pages 1663?1666.
Andreas Stolcke. 2002. SRILM ? An extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Process-
ing, pages 901?904.
Yoshimasa Tsuruoka and Jun?ichi Tsujii. 2005. Bidi-
rectional inference with the easiest-first strategy
for tagging sequence data. In Proceedings of the
Conference on Human Language Technology / Em-
pirical Methods on Natural Language Processing
(HLT/EMNLP 2005), pages 467?474.
Andy Way. 2013. Traditional and emerging use-cases
for machine translation. In Proceedings of Translat-
ing and the Computer 35.
245
