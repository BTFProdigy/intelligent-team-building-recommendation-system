Proceedings of the BioNLP Shared Task 2013 Workshop, pages 125?129,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
Identification of Genia Events using Multiple Classifiers
Roland Roller and Mark Stevenson
Department of Computer Science,
University of Sheffield
Regent Court, 211 Portobello
Sheffield, S1 4DP
United Kingdom
{R.Roller, M.Stevenson}@dcs.shef.ac.uk
Abstract
We describe our system to extract genia
events that was developed for the BioNLP
2013 Shared Task. Our system uses a su-
pervised information extraction platform
based on Support Vector Machines (SVM)
and separates the process of event clas-
sification into multiple stages. For each
event type the SVM parameters are ad-
justed and feature selection carried out.
We find that this optimisation improves
the performance of our approach. Overall
our system achieved the highest precision
score of all systems and was ranked 6th
of 10 participating systems on F-measure
(strict matching).
1 Introduction
The BioNLP 2013 Shared Task focuses on infor-
mation extraction in the biomedical domain and
comprises of a range of extraction tasks. Our sys-
tem was developed to participate within the Genia
Event Extraction task (GE), which focuses on the
detection of gene events and their regulation. The
task considers 13 different types of events which
can be divided into four groups: simple events,
bindings, protein modifications and regulations.
All events consist of a core event, which contains
a trigger word and a theme. With the exception of
regulation events, the theme always refer to a pro-
tein. A regulation event theme can either refer to
a protein or to another event. Binding events can
include up to two proteins as themes. In addition
to the core event, events may include additional
arguments such as ?cause? or ?to location?.
Figure 1 shows examples of events from the
BioNLP 2013 corpus. More details about the Ge-
nia Event task can be found in Kim et al (2011).
Previous editions of the BioNLP Shared Task
took place in 2009 (Kim et al, 2009) and 2011
Figure 1: Two events from the BioNLP 2013 GE
task: a phosphorylation event consisting of a trig-
ger and a protein and a positive-regulation event
consisting of a trigger, a theme referring to an
event and a cause argument.
(Kim et al, 2011). Promising approaches in the
most recent competition were event parsing (Mc-
Closky et al, 2011) and dual decomposition mod-
els (Riedel and McCallum, 2011). The winner of
the GE task 2011, FAUST (Riedel et al, 2011),
combined these two approaches by using result
from the event parser as an additional input fea-
ture for the dual decomposition.
The UTurku system of Bjo?rne et al (2009) was
the winner of the GE task in 2009. The system
was based on a pipeline containing three main
stages: trigger detection, argument detection and
post-processing. Bjo?rne and Salakoski (2011) im-
proved the performance of this system for BioNLP
2011, but was outperformed by FAUST.
Our approach to the BioNLP Shared Task re-
lies on separating the process of event classifica-
tion into multiple stages and creates separate clas-
sifiers for each event type. Our system begins by
pre-processing the input text, followed by multiple
classification stages and a post-processing stage.
The pre-processing applies tokenization, sentence
splitting and dictionary-based trigger detection,
similar to Bui and Sloot (2011). Classification is
based on a Support Vector Machine (SVM) and
uses three main stages: trigger-protein detection,
trigger-event detection and event-cause detection.
Post-processing is a combination of classification
and rule-based approaches. We train a separate
classifier for each event type, rather that relying on
a single classifier to recognise trigger-theme rela-
125
tionships for all event types. In addition, we also
optimise the SVM?s parameters and apply feature
selection for each event type.
Our system participated in subtask 1 of the
GE task, which involves the recognition of core
events, including identification of their ?cause?.
The remainder of this paper describes our sys-
tem in detail (Section 2), presents results from the
Genia Event Extraction task (Section 3) and draws
the conclusions of this work (Section 4).
2 System Description
2.1 Preprocessing
Our system begins by preprocessing the input text,
by applying the sentence splitter and biomedical
named entity tagger from LingPipe1. The sentence
splitter is trained on the MEDLINE data set. The
text is then tokenised. Tokens containing punc-
tuation marks are split, as are tokens containing
a protein or suffixes which could be utilised as
a trigger word. For instance the term ?Foxp3-
expression? will be split into ?Foxp3 - expression?,
since ?Foxp3? is as a protein and ?expression? a
suffix often used as trigger word. The tokens are
then stemmed using the Porter Stemmer from the
NLTK2 toolkit. The Stanford Parser3 is used to ex-
tract part-of-speech tags, syntax trees and depen-
dency trees.
2.1.1 Trigger Detection
The names of proteins in the text are provided in
the GE task, however the trigger words that form
part of the relation have to be identified. Our sys-
tem uses a dictionary-based approach to trigger
detection. The advantage of this approach is that it
is easy to implement and allows us to easily iden-
tify as many potential trigger words as possible.
However, it will also match many words which
are not true triggers. We rely on the classification
stage later in our approach to identify the true trig-
ger words.
A training corpus was created by combining the
training data from the 2013 Shared Task with all
of the data from the 2011 task. All words that are
used as a trigger in this corpus are extracted and
stored in a set of dictionaries. Separate dictionar-
ies are created for different event types (e.g. local-
ization, binding). Each type has its own dictionary,
1http://alias-i.com/lingpipe/index.html
2http://nltk.org/
3http://nlp.stanford.edu/software/lex-parser.shtml
with the exception of protein modification events
(protein modification, phosphorylation, ubiquiti-
nation, acetylation, deacetylation). The corpus did
not contain enough examples of trigger terms for
these events and consequently they are combined
into a single dictionary. The words in the dictio-
naries are stemmed and sorted by their frequency.
Irrelevant words (such as punctuations) are filtered
out.
Trigger detection is carried out by matching the
text against each of the trigger dictionaries, start-
ing with the trigger words with the highest fre-
quency. A word may be annotated as a trigger
word by different dictionaries. If a word is anno-
tated as a trigger word for a specific event then it
may not be annotated as being part of another trig-
ger word from the same dictionary. This restric-
tion prevents the generation of overlapping trigger
words for the same event as well as preventing too
many words being identified as potential triggers.
2.2 Classification
Classification of relations is based on SVM with
a polynomial kernel, using LibSVM (Chang and
Lin, 2011), and is carried out in three stages. The
first covers the core event, which consists of a trig-
ger and a theme referring to a protein. The second
takes all classified events and tries to detect regu-
lation events consisting of a trigger and a theme
that refers to one of these events (see positive-
regulation event in figure 1). In addition to a trig-
ger and theme, regulation and protein modification
events may also include a cause argument. The
third stage is responsible for identifying this addi-
tional argument for events detected in the previous
two stages.
Classification in each stage is always between
pairs of object: trigger-protein (stage 1), trigger-
event (stage 2), event-protein (stage 3) or event-
event (stage 3). At each stage the role of the clas-
sifier is to determine whether there is in fact a re-
lation between a given pair of objects. This ap-
proach is unable to identify binding events involv-
ing two themes. These are identified in a post-
processing step (see Section 2.3) which consid-
ers binding events involving the same trigger word
and decides whether they should be merged or not.
2.2.1 Feature Set
The classification process uses a wide range of
features constructed from words, stemmed words,
part of speech tags, NE tags and syntactic analysis.
126
Object Features: The classification process
always considers a pair of objects (e.g. trigger-
protein, trigger-event, event-protein). Object fea-
tures are derived from the tokens (words, stemmed
words etc.) which form the objects. We consider
the head of this object, extracted from the depen-
dency tree, as a feature and all other tokens within
that object as bag of word features. We also con-
sider the local context of each object and include
the three words preceding and following the ob-
jects as features.
Sentence Features: The tokens between the
two objects are also used to form features. A
bag of word is formed from the tokens between
the features and, in addition, the complete se-
quence of tokens is also used as a feature. Differ-
ent sentence features are formed from the words,
stemmed words, part of speech tags and NE tags .
Syntactic Features: A range of features are ex-
tracted from the dependency and phrase-structure
trees generated for each sentence. These fea-
tures are formed from the paths between the the
objects within dependency tree, collapsed depen-
dency tree and phrase-structure tree. The paths are
formed from tokens, stemmed tokens etc.
The features are organised into 57 groups for
use in the feature selection process described later.
For example all of the features relating to the bag
of words between the two objects in the depen-
dency tree are treated as a single group, as are all
of the features related to the POS tags in the three
word range around one of the objects.
2.2.2 Generation of Training and Test Data
Using the training data, a set of positive and neg-
ative examples were generated to train our classi-
fiers. Pairs of entities which occur in a specific
relation in the training data are used to generate
positive examples and all other pairs used to gen-
erate negative ones. Since we do not attempt to
resolve coreference, we only consider pairs of en-
tities that occur within the same sentence.
Due to the fact that we run a dictionary-based
trigger detection on a stemmed corpus we might
cover many trigger words, but unfortunately also
many false ones. To handle this situation our clas-
sifier should learn whether a word serves as a trig-
ger of an event or not. To generate sufficient nega-
tive examples we also run the trigger detection on
the training data set, which already contains the
right trigger words.
2.2.3 Classifier optimisation
Two optimisation steps were applied to the rela-
tion classifiers and found to improve their perfor-
mance.
SVM bias adjustment: The ratio of positive
and negative examples differs in the training data
generated for each relation. For instance the data
for the protein catabolism event contains 156 pos-
itive examples and 643 negatives ones while the
gene expression event has 3617 positive but 34544
negative examples. To identify the best configura-
tion for two SVM parameters (cost and gamma),
we ran a grid search for each classification step
using 5-fold cross validation on the training set.
Feature Selection: We also perform feature se-
lection for each event type. We remove each fea-
ture in turn and carry out 5-fold cross validation on
the training data to identify whether the F-measure
improves. If improvement is found then the fea-
ture that leads to the largest increase in F-measure
is removed from the feature set for that event type
and the process repeated. The process is continued
until no improvement in F-measure is observed
when any of the features are removed. The set of
features which remain are used as the final set for
the classifier.
The feature selection shows the more positive
training examples we have for an event type the
fewer features are removed. For example, gene
expression events have the highest amount of pos-
itive examples (3617) and achieve the best F-
measure score without removing any feature. On
the other hand, there are just 156 training exam-
ples for protein catabolism events and the best re-
sults are obtained when 39 features are removed.
On average we remove around 14 features for each
event classifier. We observed that sentence fea-
tures and those derived from the local context of
the object are those which are removed most of-
ten.
2.3 Post-Processing
The output from the classification stage is post-
processed in order to reduce errors. Two stages of
post-processing are applied: one of which is based
on a classifier and another which is rule based.
Binding Re-Ordering: As already mentioned
in Section 2.2, our classification is only capable
of detecting single trigger-protein bindings. How-
ever if two binding events share the same trig-
ger, they could be merged into a single binding
127
containing two themes. A classifier is trained to
decide whether to merge pairs of binding events.
The classifier is provided with the two themes that
share a trigger word and is constructed in the same
way as the classifiers that were used for relations.
We utilise the same feature set as in the other clas-
sification steps and run a grid search to adjust the
SVM parameter to decide whether to merge two
bindings or not.
Rule-Based Post-Processing: The second
stage of post-processing considers all the events
detected within a sentence and applies a set of
manually created rules designed to select the most
likely. Some of the most important rules include:
? Assume that the classifier has identified both
a simple event (e1) and regulation event (e2)
using the same trigger word and theme. If an-
other event uses a different trigger word with
e1 as its theme then e2 is removed.
? If transcription and gene expression events
are identified which use the same trigger and
theme then the gene expression event is re-
moved. This situation occurs since transcrip-
tion is a type of a gene expression and the
classifiers applied in Section 2.2 may identify
both types.
? Assume there are two events (e1 and e2) of
the same type (e.g. binding) that use the same
trigger word but refer to different proteins. If
the theme of a regulation event refers to e1
then a new regulation event referring to e2 is
introduced.
3 Results
Our approach achieved the highest precision score
(63.00) in the formal evaluation in terms of strict
matching in the GE task 1. The next highest preci-
sion scores were achieved by BioSEM (60.67) and
NCBI (56.72). We believe that the classifier opti-
misation (Section 2.2.3) for each event and the use
of manually created post-processing rules (Section
2.3) contributed to the high precision score. Our
system was ranked 6th place of 10 in terms of F-
measure with a score of 42.06.
Table 1 presents detailed results of our system
for the GE task. Our approach leads to high preci-
sion scores for many of the event types with a pre-
cision of 79.23 for all simple events and 92.68 for
protein modifications. Our system?s performance
is lower for regulation events than other types with
a precision of 52.69. Unlike other types of events,
the theme of a regulation event may refer to an-
other event. The detection of regulation events can
therefore be affected by errors in the detection of
simple events.
Results of our system are closer to the best re-
ported results when strict matching is used as the
evaluation metric. In this case the F-measure is
6.86 lower than the winning system (BioSEM).
However, when the approximate span & recursive
matching metric is used the results of our sys-
tem are 8.74 lower than the best result, which is
achieved by the EVEX system.
Event Class Recall Prec. Fscore
Gene expression 62.20 85.37 71.96
Transcription 33.66 45.33 38.64
Protein catabolism 57.14 53.33 55.17
Localization 23.23 85.19 36.51
SIMPLE ALL 54.02 79.23 64.24
Binding 31.53 46.88 37.70
Phosphorylation 47.50 92.68 62.81
PROT-MOD ALL 39.79 92.68 55.68
Regulation 11.46 42.86 18.08
Positive regulation 23.72 53.60 32.88
Negative regulation 20.91 54.19 30.18
REG. ALL 21.14 52.69 30.18
EVENT TOTAL 31.57 63.00 42.06
Table 1: Evaluation Results (strict matching)
4 Conclusion
Our approach to the BioNLP GE task 1 was to cre-
ate a separate SVM-based classifier for each event
type. We adjusted the SVM parameters and ap-
plied feature selection for each classifier. Our sys-
tem post-processed the outputs from these classi-
fiers using a further classifier (to decide whether
events should be merged) and manually created
rules (to select between conflicting events). Re-
sults show that our approach achieves the high-
est precision of all systems and was ranked 6th in
terms of F-measure when strict matching is used.
In the future we would like to improve the recall
of our approach and also aim to explore the use of
a wider range of features. We would also like to
experiment with post-processing based on a clas-
sifier and compare performance with the manually
created rules currently used.
128
References
Jari Bjo?rne and Tapio Salakoski. 2011. Generaliz-
ing biomedical event extraction. In Proceedings of
BioNLP Shared Task 2011 Workshop, pages 183?
191, Portland, Oregon, USA, June. Association for
Computational Linguistics.
Jari Bjo?rne, Juho Heimonen, Filip Ginter, Antti Airola,
Tapio Pahikkala, and Tapio Salakoski. 2009. Ex-
tracting complex biological events with rich graph-
based feature sets. In Proceedings of the BioNLP
2009 Workshop Companion Volume for Shared Task,
pages 10?18, Boulder, Colorado, June. Association
for Computational Linguistics.
Quoc-Chinh Bui and Peter. M.A. Sloot. 2011. Extract-
ing biological events from text using simple syntac-
tic patterns. In Proceedings of BioNLP Shared Task
2011 Workshop, pages 143?146, Portland, Oregon,
USA, June. Association for Computational Linguis-
tics.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technol-
ogy, 2:27:1?27:27. Software available at http://
www.csie.ntu.edu.tw/?cjlin/libsvm.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview of
bionlp?09 shared task on event extraction. In Pro-
ceedings of the BioNLP 2009 Workshop Companion
Volume for Shared Task, pages 1?9, Boulder, Col-
orado, June. Association for Computational Linguis-
tics.
Jin-Dong Kim, Yue Wang, Toshihisa Takagi, and Aki-
nori Yonezawa. 2011. Overview of genia event
task in bionlp shared task 2011. In Proceedings of
BioNLP Shared Task 2011 Workshop, pages 7?15,
Portland, Oregon, USA, June. Association for Com-
putational Linguistics.
David McClosky, Mihai Surdeanu, and Christopher
Manning. 2011. Event extraction as dependency
parsing for bionlp 2011. In Proceedings of BioNLP
Shared Task 2011 Workshop, pages 41?45, Portland,
Oregon, USA, June. Association for Computational
Linguistics.
Sebastian Riedel and Andrew McCallum. 2011. Ro-
bust biomedical event extraction with dual decom-
position and minimal domain adaptation. In Pro-
ceedings of BioNLP Shared Task 2011 Workshop,
pages 46?50, Portland, Oregon, USA, June. Asso-
ciation for Computational Linguistics.
Sebastian Riedel, David McClosky, Mihai Surdeanu,
Andrew McCallum, and Christopher D. Manning.
2011. Model combination for event extraction in
bionlp 2011. In Proceedings of BioNLP Shared
Task 2011 Workshop, pages 51?55, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
129
Proceedings of the 5th International Workshop on Health Text Mining and Information Analysis (Louhi) @ EACL 2014, pages 80?84,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Applying UMLS for Distantly Supervised Relation Detection
Roland Roller and Mark Stevenson
University of Sheffield
Regent Court, 211 Portobello
S1 4DP Sheffield, UK
{R.Roller,M.Stevenson}@dcs.shef.ac.uk
Abstract
This paper describes first results using
the Unified Medical Language System
(UMLS) for distantly supervised relation
extraction. UMLS is a large knowledge
base which contains information about
millions of medical concepts and relations
between them. Our approach is evaluated
using existing relation extraction data sets
that contain relations that are similar to
some of those in UMLS.
1 Introduction
Distant supervision has proved to be a popular ap-
proach to relation extraction (Craven and Kum-
lien, 1999; Mintz et al., 2009; Hoffmann et al.,
2010; Nguyen and Moschitti, 2011). It has the
advantage that it does not require manually anno-
tated training data. Distant supervision avoids this
by using information from a knowledge base to
automatically identify instances of a relation from
text and use them in order to generate training data
for a relation extraction system.
Distant supervision has already been applied
to the biomedical domain (Craven and Kumlien,
1999; Thomas et al., 2011). Craven and Kum-
lien (1999) were the first to apply distant supervi-
sion and used the Yeast Protein Database (YPD) to
detect sentences containing subcellar-localization
relations. Thomas et al. (2011) trained a clas-
sifier for protein-protein interactions (PPI) using
the knowledge base IntAct and evaluated their ap-
proach on different PPI corpora.
There have also been recent applications of dis-
tant supervision outside the biomedical domain.
The use of Freebase to train a classifier, e.g.
(Mintz et al., 2009; Riedel et al., 2010), has proved
popular. Other, such as Hoffmann et al. (2010),
use Wikipedia info-boxes as the knowledge base.
Applications of distant supervision face several
challenges. The main problem is ensuring the
quality of the automatically identified training in-
stances identified by the self-annotation. The use
of instances that have been incorrectly labelled as
positive can lower performance (Takamatsu et al.,
2012). Another problem arises when positive ex-
amples are included in the set of negative train-
ing instances, which can occur when information
is missing from the knowledge base (Min et al.,
2013; Ritter et al., 2013; Xu et al., 2013).
Evaluation of relation extraction systems that
use distant supervision represents a further chal-
lenge. In the ideal case an annotated evaluation set
is available. Others, such as Ritter et al. (2013) and
Hoffmann et al. (2011), use Freebase as knowl-
edge base and evaluate their classifier on an an-
notated New York Times corpus. However, if no
evaluation set is available leave-out can be used
where the data identified using distant supervision
used for both training and testing (Hoffmann et al.,
2010).
This paper makes use of the Unified Medical
Language System (UMLS) as a knowledge source
for distant supervision. It is widely used for
biomedical language processing and readily avail-
able. The advantage of UMLS is that it contains
information about a wide range of different types
of relations and therefore has the potential to gen-
erate a large number of relation classifiers. To our
knowledge, it has not been used as a knowledge
source to train relation extraction systems.
Evaluating such as wide range of relation clas-
sifiers is not straightforward due to the lack of
gold-standard data. As an alternative approach we
make use of existing annotated data sets and iden-
tify ones which contain relations that are similar to
those included in UMLS.
The next section provides a short description of
UMLS. We then describe how we acquire existing
data sets to evaluate certain relations. In section 4
we present our first results using UMLS for distant
supervision.
80
2 Unified Medical Language System
The Unified Medical Language System
1
is a set
of files and software which combines different
biomedical vocabularies, knowledge bases and
standards. The Metathesaurus is a database within
UMLS which contains several million biomedical
and health related names and concepts and rela-
tionships among them. All different names of a
concept are unified by the Concept Unique Identi-
fiers (CUI). MRREL is a subset of the Metathe-
saurus and involves different relationships be-
tween different medical concepts defined by a pair
of CUIs. Many of them are child-parent rela-
tionships, express a synonymy or are vaguely de-
fined as broader or narrower relation. Other re-
lations are more specific, such as has location or
drug contraindicated for. This work focuses on
more specific types of relations.
3 Acquiring Evaluation Data Sets
We examined a number of relation extraction data
sets in order to identify ones that could be used to
evaluate our system. The aim is to find a data set
that is annotated with relations that are similar to
some of those found in the UMLS. If an appropri-
ate relation can be identified then a relation extrac-
tion system can be trained using information from
the UMLS and evaluated using the data set.
To determine whether a data set is suitable we
used MetaMap (Aronson and Lang, 2010) to iden-
tify the CUIs for each related item. We then com-
pared each pair against the MRREL table to deter-
mine whether it is included as a relation. To in-
crease coverage we also included parent and child
nodes in the mapping process.
Table 1 shows the mappings obtained for two
of the data sets: the DDI 2011 data set (Segura-
Bedmar et al., 2011) and the data set described by
Rosario and Hearst (2004).
The DDI data set contains information about
drug-drug interactions and includes a single re-
lation (DDI). The relations it contained were
mapped onto 701 CUI pairs. 266 (37.9%) of these
mappings could be matched to the MRREL rela-
tion has contraindicated drug. Many of the CUI
pairs could also be mapped to the isa relationship
in MRREL, but this is a very general relationship
and the matches are caused by the large number of
these in UMLS rather than it being a reasonable
1
https://www.nlm.nih.gov/research/umls/
match for the DDI relation.
The data set described by Rosario and Hearst
(2004) focuses on different relationships between
treatments and diseases. The two most com-
mon relations TREAT FOR DIS (TREAT), denot-
ing the treatment for a particular disease, and PRE-
VENT (PREV), which indicates that a treatment
can be used to prevent a disease. The MRREL
isa relationship also matches many of these re-
lations, again due to its prevalence in MRREL.
Other MRREL relations (may be prevented by
and may be treated by) match fewer CUI pairs but
seem to be better matches for the TREAT and
PREV relations.
Relation MRREL
DDI (701) has contraindicated drug (266),
isa (185), may treat (57),
has contraindication (51)
PREV (41) isa (11), may be prevented by (5)
TREAT (741) isa (172), may be treated by (118)
Table 1: Relation mapping to MRREL
It is important to note that it is not always possi-
ble to find a CUI mapping for each entity and the
mapping process means that the mapping cannot
be guaranteed to be correct in all cases. High cov-
erage does not necessarily mean that a corpus is
very similar to a certain MRREL relation, just that
many of the CUI pairs which have been mapped
to the related entities in the corpus occur often to-
gether in a certain MRREL relation. However, in
the absence of any other suitable evaluation data
we assume that high coverage is an indicator that
the relations are strongly similar and use these two
data sets for evaluation.
4 Distant Supervision using UMLS
In this section we carry out two different dis-
tant supervised experiments using UMLS. The
first experiment will be evaluated on a subset
of the DDI 2011 training data set using the
MRREL relation has contraindicated drug and
has contraindication. The second experiment
uses the MRREL relations may be treated by and
may be prevented by and are evaluated on the
Rosario & Hearst data set.
We use 7,500,000 Medline abstracts annotated
with CUIs using MetaMap (choosing the best
mapping as annotation) as a corpus for distant su-
pervision. Our information extraction platform
based on a system developed for the BioNLP
81
Shared Task 2013 (Roller and Stevenson, 2013).
In contrast to our previous work, our classification
process relies on the Shallow Linguistic Kernel
(Giuliano et al., 2006) in combination with Lib-
SVM (Chang and Lin, 2011) taking the kernel as
input.
4.1 Experiment 1: DDI 2011
The DDI 2011 data set was split into training and
test sets for the experiments. Table 2 presents
results that place the distant supervision perfor-
mance in context. The naive classification ap-
proach predicts all candidate pairs as positive. The
supervised approach is trained on the training set,
using the same kernel method as our distant su-
pervised experiments and evaluated on the test set.
This represents the performance that can be ob-
tained using manually labelled training data and
can be considered as an upper bound for distant
supervision.
Method Prec. / Recall / F1
naive 0.098 / 1.000 / 0.178
supervised 0.428 / 0.702 / 0.532
Table 2: DDI 2011 baseline results
The distant supervision approach requires pairs
of positive and negative CUI to be identified.
These pairs are used to identify positive and nega-
tive examples of the target relation from a corpus.
Pairs which occur in our target MRREL relation
are used as positive CUI pairs. Negative pairs are
generated by selecting pairs of CUIs that are occur
in any other MRREL relation.
Sentences containing these CUI pairs are iden-
tified in the subset of the MetaMapped Medline.
In the basic setup (basic), sentences containing a
positive pair will be considered as a positive train-
ing example. There are many cases where just the
occurrence of a positive MRREL pair does not ex-
press the target relation. In an effort to remove
this noisy data we apply some simple heuristics.
The first discards all training instances with more
than five words (5w) between the two entities, an
approach similar to one applied by Takamatsu et
al. (2012). The second discards positive sentences
containing a comma between the related entities
(com). We found that commas often indicate a sen-
tence containing a list of items (e.g. genes or dis-
eases) and that these sentences do not form good
training examples due to the multiple relations that
are possible when there are several items. Finally
we also apply a combination of both techniques
(5w+com).
1000 positive examples were generated using
each approach and used for training. Although it
would be possible to generate more examples for
some approaches, for example basic, applying the
combination of techniques (5w+com) significantly
reduces the number of instances available.
Method has contraindication has contraindicated
(P./R./F1) drug (P./R./F1)
basic 0.146 / 0.371 / 0.210 0.158 / 0.598 / 0.250
5w 0.109 / 0.641 / 0.187 0.207 / 0.487 / 0.290
com 0.212 / 0.560 / 0.308 0.177 / 0.498 / 0.261
5w+com 0.207 / 0.487 / 0.291 0.214 / 0.471 / 0.294
Table 3: Evaluation with DDI 2011
Table 3 presents results of the experiments.
The results show that all applied techniques for
both MRREL relations outperform the naive ap-
proach. The best results in terms of F1 score
for the has contraindication MRREL relation
are obtained using the com selection technique.
Applying just 5w leads to worse results than
using the basic approach. The situation for
has contraindicated drug is different. The classi-
fier provides for all techniques a better F1 score
than the basic approach. The best results are
achieved by using 5w+com. It is interesting to see,
that both MRREL relations provide similar aver-
age classification results, even if both relations are
different from the target relation and cover com-
pletely different CUI pairs. It is also interest-
ing that the MRREL relation has contraindication
has a lower coverage to the DDI relation than
has contraindicated drug, but provides slightly
better results overall. A problem with the distant
supervised classification of these two MRREL re-
lations is their low occurrence in our Medline sub-
set. Using more training data will often lead to
better results. In our case, if we apply the com-
bined selection technique, there are fewer positive
training instances than are available to the super-
vised approach, making it difficult to outperform
the supervised approach.
4.2 Experiment 2: Rosario & Hearst
The second experiment addresses the prob-
lem of detecting the MRREL relations
may be prevented by and may be treated by.
Parts of the Rosario & Hearst data set are used
to evaluate this relation. This data set differs
in structure from the DDI data set. Instead of
82
annotating the entities in the sentence according
to its relation, the annotations in the data set
indicate whether a certain relation occurs in the
sentence. This data set does not contain any
negative examples. If a sentence contains two
entities, it will always describe a certain relation.
A supervised classifier is created by dividing the
data set into training and test sets. The test set
contains 253 different sentences (221 describe
a TREAT relation, 15 a PREV relation and 17
involve other relationships). Positive and negative
CUI pairs are selected in a different way to the
previous experiment. The two most frequent
relations in the data set are TREAT and PREV.
A classifier for a particular relation is trained
using sentences annotated with the corresponding
MRREL relation as positive instances. Negative
instances are identified using the other relation.
For example, the classifier for the TREAT relation
is trained using positive examples identified
using may be treated by with negative examples
generated using may be treated by.
Table 4 shows the baseline results on the data
set using a naive and a supervised approach on the
two original relations TREAT and PREV. Perfor-
mance of the naive approach for TREAT is very
high since the majority of sentences in the data set
are annotated with that relation.
Data Set Method Prec. / Recall / F1
TREAT
naive 0.874 / 1.000 / 0.933
supervised 0.944 / 0.923 / 0.934
PREV
naive 0.059 / 1.000 / 0.112
supervised 0.909 / 0.667 / 0.769
Table 4: Rosario & Hearst baseline results
Table 5 shows the results for the various dis-
tant supervision approaches. Again, 1000 positive
training examples were used to train the classifier.
Since the F-Score of the naive and the supervised
approaches of TREAT are very high, it is difficult
to compete with the may be treated by distant su-
pervised classifier. However, considering that just
15.9% of the TREAT instance pairs of the train-
ing set match the MRREL may be treated by re-
lation, the results are promising. Furthermore, the
precision of all may be treated by distant super-
vised experiments outperform the naive approach.
The best results are achieved using com as selec-
tion technique.
The experiments using the PREV relation for
evaluation are more interesting. Due to its low
occurrence in the test set it is more difficult to
detect this relation. The distant supervised clas-
sifier trained with the may be prevented by rela-
tion easily outperforms the naive approach. The
best overall F1 scoer results are achieved using
the 5w technique. As expected the distant super-
vised results are outperformed by the supervised
approach. However, the recall for all distantly su-
pervised approaches are at least as high as those
obtained using the supervised approach.
may be treated by may be prevented by
evaluated on TREAT evaluated on PREV
Method (P./R./F1) (P./R./F1)
basic 0.926 / 0.733 / 0.818 0.286 / 0.667 / 0.400
5w 0.925 / 0.783 / 0.848 0.407 / 0.733 / 0.524
com 0.928 / 0.819 / 0.870 0.222 / 0.800 / 0.348
5w+com 0.924 / 0.769 / 0.840 0.361 / 0.867 / 0.510
Table 5: Evaluation with Rosario & Hearst data
set
5 Conclusion and Discussion
In this paper we presented first results using
UMLS to train a distant supervised relational clas-
sifier. Evaluation was carried out using existing
evaluation data sets since no resources directly an-
notated with UMLS relations were available. We
showed that using a distantly supervised classifier
trained on MRREL relations similar to those found
in the evaluation data set provides promising re-
sults.
Overall, our system works with some compo-
nents which should be improved to achieve better
results. First, we rely on a cheap and fast anno-
tation using MetaMap, which might produce an-
notation errors. In addition, the use of noisy dis-
tant supervised training data decreases the classi-
fication quality. An improvement of the selection
process and an improvement of the classification
method, such as Chowdhury and Lavelli (2013),
could lead to better classification results. In future
we would also like to make further use of existing
data sets with similar relations to those of interest
to evaluate distant supervision approaches.
Acknowledgements
The authors are grateful to the Engineering
and Physical Sciences Research Council for
supporting the work described in this paper
(EP/J008427/1).
83
References
A. Aronson and F. Lang. 2010. An overview of
MetaMap: historical perspective and recent ad-
vances. Journal of the American Medical Associ-
ation, 17(3):229?236.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technology,
2:27:1?27:27.
Md. Faisal Mahbub Chowdhury and Alberto Lavelli.
2013. Fbk-irst : A multi-phase kernel based ap-
proach for drug-drug interaction detection and clas-
sification that exploits linguistic information. In
Second Joint Conference on Lexical and Computa-
tional Semantics (*SEM), Volume 2: Proceedings
of the Seventh International Workshop on Seman-
tic Evaluation (SemEval 2013), pages 351?355, At-
lanta, Georgia, USA, June. Association for Compu-
tational Linguistics.
Mark Craven and Johan Kumlien. 1999. Constructing
biological knowledge bases by extracting informa-
tion from text sources. In In Proceedings of the Sev-
enth International Conference on Intelligent Systems
for Molecular Biology (ISMB), pages 77?86. AAAI
Press.
Claudio Giuliano, Alberto Lavelli, and Lorenza Ro-
mano. 2006. Exploiting shallow linguistic infor-
mation for relation extraction from biomedical liter-
ature. In In Proc. EACL 2006.
Raphael Hoffmann, Congle Zhang, and Daniel S.
Weld. 2010. Learning 5000 relational extractors. In
Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics, ACL ?10,
pages 286?295, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Raphael Hoffmann, Congle Zhang, Xiao Ling,
Luke Zettlemoyer, and Daniel S. Weld. 2011.
Knowledge-based weak supervision for information
extraction of overlapping relations. In Proceedings
of the 49th Annual Meeting of the Association for
Computational Linguistics, ACL ?11, pages 541?
550.
Bonan Min, Ralph Grishman, Li Wan, Chang Wang,
and David Gondek. 2013. Distant supervision for
relation extraction with an incomplete knowledge
base. In Proceedings of the 2013 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 777?782, Atlanta, Georgia, June.
Association for Computational Linguistics.
Mike Mintz, Steven Bills, Rion Snow, and Dan Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP: Vol-
ume 2 - Volume 2, ACL ?09, pages 1003?1011,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Truc-Vien T. Nguyen and Alessandro Moschitti. 2011.
End-to-end relation extraction using distant super-
vision from external semantic repositories. In Pro-
ceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies: short papers - Volume 2, HLT
?11, pages 277?282, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions with-
out labeled text. In Proceedings of the European
Conference on Machine Learning and Knowledge
Discovery in Databases (ECML PKDD ?10).
Alan Ritter, Luke Zettlemoyer, Mausam, and Oren Et-
zioni. 2013. Modeling missing data in distant su-
pervision for information extraction. In Association
for Computational Linguistics Vol. 1 (TACL).
Roland Roller and Mark Stevenson. 2013. Identi-
fication of genia events using multiple classifiers.
In Proceedings of BioNLP Shared Task 2013 Work-
shop, Sofia, Bulgaria, August. Association for Com-
putational Linguistics.
Barbara Rosario and Marti A. Hearst. 2004. Classi-
fying semantic relations in bioscience texts. In Pro-
ceedings of the 42nd Annual Meeting on Association
for Computational Linguistics, ACL ?04, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Isabel Segura-Bedmar, Paloma Martnez, and Daniel
Snchez-Cisneros. 2011. The 1st ddi extraction-
2011 challenge task: Extraction of drug-drug inter-
actions from biomedical texts. In Proceedings of
DDI Extraction-2011 challenge task., pages 1?9.
Shingo Takamatsu, Issei Sato, and Hiroshi Nakagawa.
2012. Reducing wrong labels in distant supervi-
sion for relation extraction. In Proceedings of the
50th Annual Meeting of the Association for Compu-
tational Linguistics: Long Papers - Volume 1, ACL
?12, pages 721?729, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Philippe Thomas, Ill?es Solt, Roman Klinger, and Ulf
Leser. 2011. Learning protein protein interaction
extraction using distant supervision. In Proceedings
of Robust Unsupervised and Semi-Supervised Meth-
ods in Natural Language Processing, pages 34?41.
Wei Xu, Raphael Hoffmann, Le Zhao, and Ralph Gr-
ishman. 2013. Filling knowledge base gaps for dis-
tant supervision of relation extraction. In Proceed-
ings of the 51st Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Pa-
pers), pages 665?670, Sofia, Bulgaria, August. As-
sociation for Computational Linguistics.
84
