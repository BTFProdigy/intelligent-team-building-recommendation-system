Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 253?261,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Discovering the Discriminative Views: Measuring Term Weights for
Sentiment Analysis
Jungi Kim, Jin-Ji Li and Jong-Hyeok Lee
Division of Electrical and Computer Engineering
Pohang University of Science and Technology, Pohang, Republic of Korea
{yangpa,ljj,jhlee}@postech.ac.kr
Abstract
This paper describes an approach to uti-
lizing term weights for sentiment analysis
tasks and shows how various term weight-
ing schemes improve the performance of
sentiment analysis systems. Previously,
sentiment analysis was mostly studied un-
der data-driven and lexicon-based frame-
works. Such work generally exploits tex-
tual features for fact-based analysis tasks
or lexical indicators from a sentiment lexi-
con. We propose to model term weighting
into a sentiment analysis system utilizing
collection statistics, contextual and topic-
related characteristics as well as opinion-
related properties. Experiments carried
out on various datasets show that our
approach effectively improves previous
methods.
1 Introduction
With the explosion in the amount of commentaries
on current issues and personal views expressed in
weblogs on the Internet, the field of studying how
to analyze such remarks and sentiments has been
increasing as well. The field of opinion mining
and sentiment analysis involves extracting opin-
ionated pieces of text, determining the polarities
and strengths, and extracting holders and targets
of the opinions.
Much research has focused on creating testbeds
for sentiment analysis tasks. Most notable
and widely used are Multi-Perspective Question
Answering (MPQA) and Movie-review datasets.
MPQA is a collection of newspaper articles anno-
tated with opinions and private states at the sub-
sentence level (Wiebe et al, 2003). Movie-review
dataset consists of positive and negative reviews
from the Internet Movie Database (IMDb) archive
(Pang et al, 2002).
Evaluation workshops such as TREC and NT-
CIR have recently joined in this new trend of re-
search and organized a number of successful meet-
ings. At the TREC Blog Track meetings, re-
searchers have dealt with the problem of retriev-
ing topically-relevant blog posts and identifying
documents with opinionated contents (Ounis et
al., 2008). NTCIR Multilingual Opinion Analy-
sis Task (MOAT) shared a similar mission, where
participants are provided with a number of topics
and a set of relevant newspaper articles for each
topic, and asked to extract opinion-related proper-
ties from enclosed sentences (Seki et al, 2008).
Previous studies for sentiment analysis belong
to either the data-driven approach where an anno-
tated corpus is used to train a machine learning
(ML) classifier, or to the lexicon-based approach
where a pre-compiled list of sentiment terms is uti-
lized to build a sentiment score function.
This paper introduces an approach to the senti-
ment analysis tasks with an emphasis on how to
represent and evaluate the weights of sentiment
terms. We propose a number of characteristics of
good sentiment terms from the perspectives of in-
formativeness, prominence, topic?relevance, and
semantic aspects using collection statistics, con-
textual information, semantic associations as well
as opinion?related properties of terms. These term
weighting features constitute the sentiment analy-
sis model in our opinion retrieval system. We test
our opinion retrieval system with TREC and NT-
CIR datasets to validate the effectiveness of our
term weighting features. We also verify the ef-
fectiveness of the statistical features used in data-
driven approaches by evaluating an ML classifier
with labeled corpora.
2 Related Work
Representing text with salient features is an im-
portant part of a text processing task, and there ex-
ists many works that explore various features for
253
text analysis systems (Sebastiani, 2002; Forman,
2003). Sentiment analysis task have also been us-
ing various lexical, syntactic, and statistical fea-
tures (Pang and Lee, 2008). Pang et al (2002)
employed n-gram and POS features for ML meth-
ods to classify movie-review data. Also, syntac-
tic features such as the dependency relationship of
words and subtrees have been shown to effectively
improve the performances of sentiment analysis
(Kudo and Matsumoto, 2004; Gamon, 2004; Mat-
sumoto et al, 2005; Ng et al, 2006).
While these features are usually employed by
data-driven approaches, there are unsupervised ap-
proaches for sentiment analysis that make use of a
set of terms that are semantically oriented toward
expressing subjective statements (Yu and Hatzi-
vassiloglou, 2003). Accordingly, much research
has focused on recognizing terms? semantic ori-
entations and strength, and compiling sentiment
lexicons (Hatzivassiloglou and Mckeown, 1997;
Turney and Littman, 2003; Kamps et al, 2004;
Whitelaw et al, 2005; Esuli and Sebastiani, 2006).
Interestingly, there are conflicting conclusions
about the usefulness of the statistical features in
sentiment analysis tasks (Pang and Lee, 2008).
Pang et al (2002) presents empirical results in-
dicating that using term presence over term fre-
quency is more effective in a data-driven sentiment
classification task. Such a finding suggests that
sentiment analysis may exploit different types of
characteristics from the topical tasks, that, unlike
fact-based text analysis tasks, repetition of terms
does not imply a significance on the overall senti-
ment. On the other hand, Wiebe et al (2004) have
noted that hapax legomena (terms that only appear
once in a collection of texts) are good signs for
detecting subjectivity. Other works have also ex-
ploited rarely occurring terms for sentiment anal-
ysis tasks (Dave et al, 2003; Yang et al, 2006).
The opinion retrieval task is a relatively recent
issue that draws both the attention of IR and NLP
communities. Its task is to find relevant documents
that also contain sentiments about a given topic.
Generally, the opinion retrieval task has been ap-
proached as a two?stage task: first, retrieving top-
ically relevant documents, then reranking the doc-
uments by the opinion scores (Ounis et al, 2006).
This approach is also appropriate for evaluation
systems such as NTCIR MOAT that assumes that
the set of topically relevant documents are already
known in advance. On the other hand, there are
also some interesting works on modeling the topic
and sentiment of documents in a unified way (Mei
et al, 2007; Zhang and Ye, 2008).
3 Term Weighting and Sentiment
Analysis
In this section, we describe the characteristics of
terms that are useful in sentiment analysis, and
present our sentiment analysis model as part of
an opinion retrieval system and an ML sentiment
classifier.
3.1 Characteristics of Good Sentiment Terms
This section examines the qualities of useful terms
for sentiment analysis tasks and corresponding
features. For the sake of organization, we cate-
gorize the sources of features into either global or
local knowledge, and either topic-independent or
topic-dependent knowledge.
Topic-independently speaking, a good senti-
ment term is discriminative and prominent, such
that the appearance of the term imposes greater
influence on the judgment of the analysis system.
The rare occurrence of terms in document collec-
tions has been regarded as a very important feature
in IR methods, and effective IR models of today,
either explicitly or implicitly, accommodate this
feature as an Inverse Document Frequency (IDF)
heuristic (Fang et al, 2004). Similarly, promi-
nence of a term is recognized by the frequency of
the term in its local context, formulated as Term
Frequency (TF) in IR.
If a topic of the text is known, terms that are rel-
evant and descriptive of the subject should be re-
garded to be more useful than topically-irrelevant
and extraneous terms. One way of measuring this
is using associations between the query and terms.
Statistical measures of associations between terms
include estimations by the co-occurrence in the
whole collection, such as Point-wise Mutual In-
formation (PMI) and Latent Semantic Analysis
(LSA). Another method is to use proximal infor-
mation of the query and the word, using syntactic
structure such as dependency relations of words
that provide the graphical representation of the
text (Mullen and Collier, 2004). The minimum
spans of words in such graph may represent their
associations in the text. Also, the distance between
words in the local context or in the thesaurus-
like dictionaries such as WordNet may be approx-
imated as such measure.
254
3.2 Opinion Retrieval Model
The goal of an opinion retrieval system is to find a
set of opinionated documents that are relevant to a
given topic. We decompose the opinion retrieval
system into two tasks: the topical retrieval task
and the sentiment analysis task. This two-stage
approach for opinion retrieval has been taken by
many systems and has been shown to perform well
(Ounis et al, 2006). The topic and the sentiment
aspects of the opinion retrieval task are modeled
separately, and linearly combined together to pro-
duce a list of topically-relevant and opinionated
documents as below.
ScoreOpRet(D,Q) = ??Scorerel(D,Q)+(1??)?Scoreop(D,Q)
The topic-relevance model Scorerel may be sub-
stituted by any IR system that retrieves relevant
documents for the query Q. For tasks such as
NTCIR MOAT, relevant documents are already
known in advance and it becomes unnecessary to
estimate the relevance degree of the documents.
We focus on modeling the sentiment aspect of
the opinion retrieval task, assuming that the topic-
relevance of documents is provided in some way.
To assign documents with sentiment degrees,
we estimate the probability of a document D to
generate a query Q and to possess opinions as in-
dicated by a random variable Op.1 Assuming uni-
form prior probabilities of documentsD, queryQ,
and Op, and conditional independence between Q
and Op, the opinion score function reduces to es-
timating the generative probability of Q and Op
given D.
Scoreop(D,Q) ? p(D | Op,Q) ? p(Op,Q | D)
If we regard that the document D is represented
as a bag of words and that the words are uniformly
distributed, then
p(Op,Q | D) =
X
w?D
p(Op,Q | w) ? p(w | D)
=
X
w?D
p(Op | w) ? p(Q | w) ? p(w | D) (1)
Equation 1 consists of three factors: the proba-
bility of a word to be opinionated (P (Op|w)), the
likelihood of a query given a word (P (Q|w)), and
the probability of a document generating a word
(P (w|D)). Intuitively speaking, the probability of
a document embodying topically related opinion is
estimated by accumulating the probabilities of all
1Throughout this paper, Op indicates Op = 1.
words from the document to have sentiment mean-
ings and associations with the given query.
In the following sections, we assess the three
factors of the sentiment models from the perspec-
tives of term weighting.
3.2.1 Word Sentiment Model
Modeling the sentiment of a word has been a pop-
ular approach in sentiment analysis. There are
many publicly available lexicon resources. The
size, format, specificity, and reliability differ in all
these lexicons. For example, lexicon sizes range
from a few hundred to several hundred thousand.
Some lexicons assign real number scores to in-
dicate sentiment orientations and strengths (i.e.
probabilities of having positive and negative sen-
timents) (Esuli and Sebastiani, 2006) while other
lexicons assign discrete classes (weak/strong, pos-
itive/negative) (Wilson et al, 2005). There are
manually compiled lexicons (Stone et al, 1966)
while some are created semi-automatically by ex-
panding a set of seed terms (Esuli and Sebastiani,
2006).
The goal of this paper is not to create or choose
an appropriate sentiment lexicon, but rather it is
to discover useful term features other than the
sentiment properties. For this reason, one sen-
timent lexicon, namely SentiWordNet, is utilized
throughout the whole experiment.
SentiWordNet is an automatically generated
sentiment lexicon using a semi-supervised method
(Esuli and Sebastiani, 2006). It consists of Word-
Net synsets, where each synset is assigned three
probability scores that add up to 1: positive, nega-
tive, and objective.
These scores are assigned at sense level (synsets
in WordNet), and we use the following equations
to assess the sentiment scores at the word level.
p(Pos | w) = max
s?synset(w)
SWNPos(s)
p(Neg | w) = max
s?synset(w)
SWNNeg(s)
p(Op | w) = max (p(Pos | w), p(Neg | w))
where synset(w) is the set of synsets of w and
SWNPos(s), SWNNeg(s) are positive and neg-
ative scores of a synset in SentiWordNet. We as-
sess the subjective score of a word as the maxi-
mum value of the positive and the negative scores,
because a word has either a positive or a negative
sentiment in a given context.
The word sentiment model can also make use
of other types of sentiment lexicons. The sub-
255
jectivity lexicon used in OpinionFinder2 is com-
piled from several manually and automatically
built resources. Each word in the lexicon is tagged
with the strength (strong/weak) and polarity (Pos-
itive/Negative/Neutral). The word sentiment can
be modeled as below.
P (Pos|w) =
8
><
>:
1.0 if w is Positive and Strong
0.5 if w is Positive and Weak
0.0 otherwise
P (Op | w) = max (p(Pos | w), p(Neg | w))
3.2.2 Topic Association Model
If a topic is given in the sentiment analysis, terms
that are closely associated with the topic should
be assigned heavy weighting. For example, sen-
timent words such as scary and funny are more
likely to be associated with topic words such as
book and movie than grocery or refrigerator.
In the topic association model, p(Q | w) is es-
timated from the associations between the word w
and a set of query terms Q.
p(Q | w) =
P
q?Q Asc-Score(q, w)
| Q |
?
X
q?Q
Asc-Score(q, w)
Asc-Score(q, w) is the association score between
q and w, and | Q | is the number of query words.
To measure associations between words, we
employ statistical approaches using document col-
lections such as LSA and PMI, and local proximity
features using the distance in dependency trees or
texts.
Latent Semantic Analysis (LSA) (Landauer and
Dumais, 1997) creates a semantic space from a
collection of documents to measure the semantic
relatedness of words. Point-wise Mutual Informa-
tion (PMI) is a measure of associations used in in-
formation theory, where the association between
two words is evaluated with the joint and individ-
ual distributions of the two words. PMI-IR (Tur-
ney, 2001) uses an IR system and its search op-
erators to estimate the probabilities of two terms
and their conditional probabilities. Equations for
association scores using LSA and PMI are given
below.
Asc-ScoreLSA(w1, w2) =
1 + LSA(w1, w2)
2
Asc-ScorePMI(w1, w2) =
1 + PMI-IR(w1, w2)
2
2http://www.cs.pitt.edu/mpqa/
For the experimental purpose, we used publicly
available online demonstrations for LSA and PMI.
For LSA, we used the online demonstration mode
from the Latent Semantic Analysis page from the
University of Colorado at Boulder.3 For PMI, we
used the online API provided by the CogWorks
Lab at the Rensselaer Polytechnic Institute.4
Word associations between two terms may also
be evaluated in the local context where the terms
appear together. One way of measuring the prox-
imity of terms is using the syntactic structures.
Given the dependency tree of the text, we model
the association between two terms as below.
Asc-ScoreDTP (w1, w2) =
(
1.0 min. span in dep. tree ? Dsyn
0.5 otherwise
where, Dsyn is arbitrarily set to 3.
Another way is to use co-occurrence statistics
as below.
Asc-ScoreWP (w1, w2) =
(
1.0 if distance betweenw1andw2 ? K
0.5 otherwise
where K is the maximum window size for the
co-occurrence and is arbitrarily set to 3 in our ex-
periments.
The statistical approaches may suffer from data
sparseness problems especially for named entity
terms used in the query, and the proximal clues
cannot sufficiently cover all term?query associa-
tions. To avoid assigning zero probabilities, our
topic association models assign 0.5 to word pairs
with no association and 1.0 to words with perfect
association.
Note that proximal features using co-occurrence
and dependency relationships were used in pre-
vious work. For opinion retrieval tasks, Yang et
al. (2006) and Zhang and Ye (2008) used the co-
occurrence of a query word and a sentiment word
within a certain window size. Mullen and Collier
(2004) manually annotated named entities in their
dataset (i.e. title of the record and name of the
artist for music record reviews), and utilized pres-
ence and position features in their ML approach.
3.2.3 Word Generation Model
Our word generation model p(w | d) evaluates the
prominence and the discriminativeness of a word
3http://lsa.colorado.edu/, default parameter settings for
the semantic space (TASA, 1st year college level) and num-
ber of factors (300).
4http://cwl-projects.cogsci.rpi.edu/msr/, PMI-IR with the
Google Search Engine.
256
w in a document d. These issues correspond to the
core issues of traditional IR tasks. IR models, such
as Vector Space (VS), probabilistic models such
as BM25, and Language Modeling (LM), albeit in
different forms of approach and measure, employ
heuristics and formal modeling approaches to ef-
fectively evaluate the relevance of a term to a doc-
ument (Fang et al, 2004). Therefore, we estimate
the word generation model with popular IR mod-
els? the relevance scores of a document d given w
as a query.5
p(w | d) ? IR-SCORE(w, d)
In our experiments, we use the Vector Space
model with Pivoted Normalization (VS), Proba-
bilistic model (BM25), and Language modeling
with Dirichlet Smoothing (LM).
V SPN(w, d) =
1 + ln(1 + ln(c(w, d)))
(1? s) + s ?
| d |
avgdl
? ln
N + 1
df(w)
BM25(w, d) = ln
N ? df(w) + 0.5
df(w) + 0.5
?
(k1 + 1) ? c(w, d)
k1
?
(1? b) + b |d|avgdl
?
+ c(w, d)
LMDI(w, d) = ln
 
1 +
c(w, d)
? ? c(w,C)
!
+ ln
?
| d | +?
c(w, d) is the frequency of w in d, | d | is the
number of unique terms in d, avgdl is the average
| d | of all documents, N is the number of doc-
uments in the collection, df(w) is the number of
documents with w, C is the entire collection, and
k1 and b are constants 2.0 and 0.75.
3.3 Data-driven Approach
To verify the effectiveness of our term weight-
ing schemes in experimental settings of the data-
driven approach, we carry out a set of simple ex-
periments with ML classifiers. Specifically, we
explore the statistical term weighting features of
the word generation model with Support Vector
machine (SVM), faithfully reproducing previous
work as closely as possible (Pang et al, 2002).
Each instance of train and test data is repre-
sented as a vector of features. We test various
combinations of the term weighting schemes listed
below.
? PRESENCE: binary indicator for the pres-
ence of a term
? TF: term frequency
5With proper assumptions and derivations, p(w | d) can
be derived to language modeling approaches. Refer to (Zhai
and Lafferty, 2004).
? VS.TF: normalized tf as in VS
? BM25.TF: normalized tf as in BM25
? IDF: inverse document frequency
? VS.IDF: normalized idf as in VS
? BM25.IDF: normalized idf as in BM25
4 Experiment
Our experiments consist of an opinion retrieval
task and a sentiment classification task. We use
MPQA and movie-review corpora in our experi-
ments with an ML classifier. For the opinion re-
trieval task, we use the two datasets used by TREC
blog track and NTCIR MOAT evaluation work-
shops.
The opinion retrieval task at TREC Blog Track
consists of three subtasks: topic retrieval, opinion
retrieval, and polarity retrieval. Opinion and polar-
ity retrieval subtasks use the relevant documents
retrieved at the topic retrieval stage. On the other
hand, the NTCIR MOAT task aims to find opin-
ionated sentences given a set of documents that are
already hand-assessed to be relevant to the topic.
4.1 Opinion Retieval Task ? TREC Blog
Track
4.1.1 Experimental Setting
TREC Blog Track uses the TREC Blog06 corpus
(Macdonald and Ounis, 2006). It is a collection
of RSS feeds (38.6 GB), permalink documents
(88.8GB), and homepages (28.8GB) crawled on
the Internet over an eleven week period from De-
cember 2005 to February 2006.
Non-relevant content of blog posts such as
HTML tags, advertisement, site description, and
menu are removed with an effective internal spam
removal algorithm (Nam et al, 2009). While our
sentiment analysis model uses the entire relevant
portion of the blog posts, further stopword re-
moval and stemming is done for the blog retrieval
system.
For the relevance retrieval model, we faithfully
reproduce the passage-based language model with
pseudo-relevance feedback (Lee et al, 2008).
We use in total 100 topics from TREC 2007 and
2008 blog opinion retrieval tasks (07:901-950 and
08:1001-1050). We use the topics from Blog 07
to optimize the parameter for linearly combining
the retrieval and opinion models, and use Blog 08
topics as our test data. Topics are extracted only
from the Title field, using the Porter stemmer and
a stopword list.
257
Table 1: Performance of opinion retrieval models
using Blog 08 topics. The linear combination pa-
rameter ? is optimized on Blog 07 topics. ? indi-
cates statistical significance at the 1% level over
the baseline.
Model MAP R-prec P@10
TOPIC REL. 0.4052 0.4366 0.6440
BASELINE 0.4141 0.4534 0.6440
VS 0.4196 0.4542 0.6600
BM25 0.4235? 0.4579 0.6600
LM 0.4158 0.4520 0.6560
PMI 0.4177 0.4538 0.6620
LSA 0.4155 0.4526 0.6480
WP 0.4165 0.4533 0.6640
BM25?PMI 0.4238? 0.4575 0.6600
BM25?LSA 0.4237? 0.4578 0.6600
BM25?WP 0.4237? 0.4579 0.6600
BM25?PMI?WP 0.4242? 0.4574 0.6620
BM25?LSA?WP 0.4238? 0.4576 0.6580
4.1.2 Experimental Result
Retrieval performances using different combina-
tions of term weighting features are presented in
Table 1. Using only the word sentiment model is
set as our baseline.
First, each feature of the word generation and
topic association models are tested; all features of
the models improve over the baseline. We observe
that the features of our word generation model is
more effective than those of the topic association
model. Among the features of the word generation
model, the most improvement was achieved with
BM25, improving the MAP by 2.27%.
Features of the topic association model show
only moderate improvements over the baseline.
We observe that these features generally improve
P@10 performance, indicating that they increase
the accuracy of the sentiment analysis system.
PMI out-performed LSA for all evaluation mea-
sures. Among the topic association models, PMI
performs the best in MAP and R-prec, while WP
achieved the biggest improvement in P@10.
Since BM25 performs the best among the word
generation models, its combination with other fea-
tures was investigated. Combinations of BM25
with the topic association models all improve the
performance of the baseline and BM25. This
demonstrates that the word generation model and
the topic association model are complementary to
each other.
The best MAP was achieved with BM25, PMI,
and WP (+2.44% over the baseline). We observe
that PMI and WP also complement each other.
4.2 Sentiment Analysis Task ? NTCIR
MOAT
4.2.1 Experimental Setting
Another set of experiments for our opinion analy-
sis model was carried out on the NTCIR-7 MOAT
English corpus. The English opinion corpus
for NTCIR MOAT consists of newspaper articles
from the Mainichi Daily News, Korea Times, Xin-
hua News, Hong Kong Standard, and the Straits
Times. It is a collection of documents manu-
ally assessed for relevance to a set of queries
from NTCIR-7 Advanced Cross-lingual Informa-
tion Access (ACLIA) task. The corpus consists of
167 documents, or 4,711 sentences for 14 test top-
ics. Each sentence is manually tagged with opin-
ionatedness, polarity, and relevance to the topic by
three annotators from a pool of six annotators.
For preprocessing, no removal or stemming is
performed on the data. Each sentence was pro-
cessed with the Stanford English parser6 to pro-
duce a dependency parse tree. Only the Title fields
of the topics were used.
For performance evaluations of opinion and po-
larity detection, we use precision, recall, and F-
measure, the same measure used to report the offi-
cial results at the NTCIR MOAT workshop. There
are lenient and strict evaluations depending on the
agreement of the annotators; if two out of three an-
notators agreed upon an opinion or polarity anno-
tation then it is used during the lenient evaluation,
similarly three out of three agreements are used
during the strict evaluation. We present the perfor-
mances using the lenient evaluation only, for the
two evaluations generally do not show much dif-
ference in relative performance changes.
Since MOAT is a classification task, we use a
threshold parameter to draw a boundary between
opinionated and non-opinionated sentences. We
report the performance of our system using the
NTCIR-7 dataset, where the threshold parameter
is optimized using the NTCIR-6 dataset.
4.2.2 Experimental Result
We present the performance of our sentiment anal-
ysis system in Table 2. As in the experiments with
6http://nlp.stanford.edu/software/lex-parser.shtml
258
Table 2: Performance of the Sentiment Analy-
sis System on NTCIR7 dataset. System parame-
ters are optimized for F-measure using NTCIR6
dataset with lenient evaluations.
Opinionated
Model Precision Recall F-Measure
BASELINE 0.305 0.866 0.451
VS 0.331 0.807 0.470
BM25 0.327 0.795 0.464
LM 0.325 0.794 0.461
LSA 0.315 0.806 0.453
PMI 0.342 0.603 0.436
DTP 0.322 0.778 0.455
VS?LSA 0.335 0.769 0.466
VS?PMI 0.311 0.833 0.453
VS?DTP 0.342 0.745 0.469
VS?LSA?DTP 0.349 0.719 0.470
VS?PMI?DTP 0.328 0.773 0.461
the TREC dataset, using only the word sentiment
model is used as our baseline.
Similarly to the TREC experiments, the features
of the word generation model perform exception-
ally better than that of the topic association model.
The best performing feature of the word genera-
tion model is VS, achieving a 4.21% improvement
over the baseline?s f-measure. Interestingly, this is
the tied top performing f-measure over all combi-
nations of our features.
While LSA and DTP show mild improvements,
PMI performed worse than baseline, with higher
precision but a drop in recall. DTP was the best
performing topic association model.
When combining the best performing feature
of the word generation model (VS) with the fea-
tures of the topic association model, LSA, PMI
and DTP all performed worse than or as well as
the VS in f-measure evaluation. LSA and DTP im-
proves precision slightly, but with a drop in recall.
PMI shows the opposite tendency.
The best performing system was achieved using
VS, LSA and DTP at both precision and f-measure
evaluations.
4.3 Classification task ? SVM
4.3.1 Experimental Setting
To test our SVM classifier, we perform the classi-
fication task. Movie Review polarity dataset7 was
7http://www.cs.cornell.edu/people/pabo/movie-review-
data/
Table 3: Average ten-fold cross-validation accura-
cies of polarity classification task with SVM.
Accuracy
Features Movie-review MPQA
PRESENCE 82.6 76.8
TF 71.1 76.5
VS.TF 81.3 76.7
BM25.TF 81.4 77.9
IDF 61.6 61.8
VS.IDF 83.6 77.9
BM25.IDF 83.6 77.8
VS.TF?VS.IDF 83.8 77.9
BM25.TF?BM25.IDF 84.1 77.7
BM25.TF?VS.IDF 85.1 77.7
first introduced by Pang et al (2002) to test various
ML-based methods for sentiment classification. It
is a balanced dataset of 700 positive and 700 neg-
ative reviews, collected from the Internet Movie
Database (IMDb) archive. MPQA Corpus8 con-
tains 535 newspaper articles manually annotated
at sentence and subsentence level for opinions and
other private states (Wiebe et al, 2005).
To closely reproduce the experiment with the
best performance carried out in (Pang et al, 2002)
using SVM, we use unigram with the presence
feature. We test various combinations of our fea-
tures applicable to the task. For evaluation, we use
ten-fold cross-validation accuracy.
4.3.2 Experimental Result
We present the sentiment classification perfor-
mances in Table 3.
As observed by Pang et al (2002), using the raw
tf drops the accuracy of the sentiment classifica-
tion (-13.92%) of movie-review data. Using the
raw idf feature worsens the accuracy even more
(-25.42%). Normalized tf-variants show improve-
ments over tf but are worse than presence. Nor-
malized idf features produce slightly better accu-
racy results than the baseline. Finally, combining
any normalized tf and idf features improved the
baseline (high 83% ? low 85%). The best combi-
nation was BM25.TF?VS.IDF.
MPQA corpus reveals similar but somewhat un-
certain tendency.
8http://www.cs.pitt.edu/mpqa/databaserelease/
259
4.4 Discussion
Overall, the opinion retrieval and the sentiment
analysis models achieve improvements using our
proposed features. Especially, the features of the
word generation model improve the overall per-
formances drastically. Its effectiveness is also ver-
ified with a data-driven approach; the accuracy of
a sentiment classifier trained on a polarity dataset
was improved by various combinations of normal-
ized tf and idf statistics.
Differences in effectiveness of VS, BM25, and
LM come from parameter tuning and corpus dif-
ferences. For the TREC dataset, BM25 performed
better than the other models, and for the NTCIR
dataset, VS performed better.
Our features of the topic association model
show mild improvement over the baseline perfor-
mance in general. PMI and LSA, both modeling
the semantic associations between words, show
different behaviors on the datasets. For the NT-
CIR dataset, LSA performed better, while PMI
is more effective for the TREC dataset. We be-
lieve that the explanation lies in the differences
between the topics for each dataset. In general,
the NTCIR topics are general descriptive words
such as ?regenerative medicine?, ?American econ-
omy after the 911 terrorist attacks?, and ?law-
suit brought against Microsoft for monopolistic
practices.? The TREC topics are more named-
entity-like terms such as ?Carmax?, ?Wikipedia
primary source?, ?Jiffy Lube?, ?Starbucks?, and
?Windows Vista.? We have experimentally shown
that LSA is more suited to finding associations
between general terms because its training docu-
ments are from a general domain.9 Our PMI mea-
sure utilizes a web search engine, which covers a
variety of named entity terms.
Though the features of our topic association
model, WP and DTP, were evaluated on different
datasets, we try our best to conjecture the differ-
ences. WP on TREC dataset shows a small im-
provement of MAP compared to other topic asso-
ciation features, while the precision is improved
the most when this feature is used alone. The DTP
feature displays similar behavior with precision. It
also achieves the best f-measure over other topic
association features. DTP achieves higher rela-
tive improvement (3.99% F-measure verse 2.32%
MAP), and is more effective for improving the per-
formance in combination with LSA and PMI.
9TASA Corpus, http://lsa.colorado.edu/spaces.html
5 Conclusion
In this paper, we proposed various term weighting
schemes and how such features are modeled in the
sentiment analysis task. Our proposed features in-
clude corpus statistics, association measures using
semantic and local-context proximities. We have
empirically shown the effectiveness of the features
with our proposed opinion retrieval and sentiment
analysis models.
There exists much room for improvement with
further experiments with various term weighting
methods and datasets. Such methods include,
but by no means limited to, semantic similarities
between word pairs using lexical resources such
as WordNet (Miller, 1995) and data-driven meth-
ods with various topic-dependent term weighting
schemes on labeled corpus with topics such as
MPQA.
Acknowledgments
This work was supported in part by MKE & IITA
through IT Leading R&D Support Project and in
part by the BK 21 Project in 2009.
References
Kushal Dave, Steve Lawrence, and David M. Pennock. 2003.
Mining the peanut gallery: Opinion extraction and seman-
tic classification of product reviews. In Proceedings of
WWW, pages 519?528.
Andrea Esuli and Fabrizio Sebastiani. 2006. Sentiword-
net: A publicly available lexical resource for opinion min-
ing. In Proceedings of the 5th Conference on Language
Resources and Evaluation (LREC?06), pages 417?422,
Geneva, IT.
Hui Fang, Tao Tao, and ChengXiang Zhai. 2004. A formal
study of information retrieval heuristics. In SIGIR ?04:
Proceedings of the 27th annual international ACM SIGIR
conference on Research and development in information
retrieval, pages 49?56, New York, NY, USA. ACM.
George Forman. 2003. An extensive empirical study of fea-
ture selection metrics for text classification. Journal of
Machine Learning Research, 3:1289?1305.
Michael Gamon. 2004. Sentiment classification on customer
feedback data: noisy data, large feature vectors, and the
role of linguistic analysis. In Proceedings of the Inter-
national Conference on Computational Linguistics (COL-
ING).
Vasileios Hatzivassiloglou and Kathleen R. Mckeown. 1997.
Predicting the semantic orientation of adjectives. In Pro-
ceedings of the 35th Annual Meeting of the Association
for Computational Linguistics (ACL?97), pages 174?181,
madrid, ES.
Jaap Kamps, Maarten Marx, Robert J. Mokken, and
Maarten De Rijke. 2004. Using wordnet to measure se-
mantic orientation of adjectives. In Proceedings of the
4th International Conference on Language Resources and
Evaluation (LREC?04), pages 1115?1118, Lisbon, PT.
260
Taku Kudo and Yuji Matsumoto. 2004. A boosting algorithm
for classification of semi-structured text. In Proceedings
of the Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP).
Thomas K. Landauer and Susan T. Dumais. 1997. A solution
to plato?s problem: The latent semantic analysis theory of
acquisition, induction, and representation of knowledge.
Psychological Review, 104(2):211?240, April.
Yeha Lee, Seung-Hoon Na, Jungi Kim, Sang-Hyob Nam,
Hun young Jung, and Jong-Hyeok Lee. 2008. Kle at trec
2008 blog track: Blog post and feed retrieval. In Proceed-
ings of TREC-08.
Craig Macdonald and Iadh Ounis. 2006. The TREC Blogs06
collection: creating and analysing a blog test collection.
Technical Report TR-2006-224, Department of Computer
Science, University of Glasgow.
Shotaro Matsumoto, Hiroya Takamura, and Manabu Oku-
mura. 2005. Sentiment classification using word sub-
sequences and dependency sub-trees. In Proceedings of
PAKDD?05, the 9th Pacific-Asia Conference on Advances
in Knowledge Discovery and Data Mining.
Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su, and
ChengXiang Zhai. 2007. Topic sentiment mixture: Mod-
eling facets and opinions in weblogs. In Proceedings of
WWW, pages 171?180, New York, NY, USA. ACM Press.
George A. Miller. 1995. Wordnet: a lexical database for
english. Commun. ACM, 38(11):39?41.
Tony Mullen and Nigel Collier. 2004. Sentiment analysis
using support vector machines with diverse information
sources. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing (EMNLP),
pages 412?418, July. Poster paper.
Sang-Hyob Nam, Seung-Hoon Na, Yeha Lee, and Jong-
Hyeok Lee. 2009. Diffpost: Filtering non-relevant con-
tent based on content difference between two consecutive
blog posts. In ECIR.
Vincent Ng, Sajib Dasgupta, and S. M. Niaz Arifin. 2006.
Examining the role of linguistic knowledge sources in the
automatic identification and classification of reviews. In
Proceedings of the COLING/ACL Main Conference Poster
Sessions, pages 611?618, Sydney, Australia, July. Associ-
ation for Computational Linguistics.
I. Ounis, M. de Rijke, C. Macdonald, G. A. Mishne, and
I. Soboroff. 2006. Overview of the trec-2006 blog track.
In Proceedings of TREC-06, pages 15?27, November.
I. Ounis, C. Macdonald, and I. Soboroff. 2008. Overview
of the trec-2008 blog track. In Proceedings of TREC-08,
pages 15?27, November.
Bo Pang and Lillian Lee. 2008. Opinion mining and sen-
timent analysis. Foundations and Trends in Information
Retrieval, 2(1-2):1?135.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002.
Thumbs up? Sentiment classification using machine
learning techniques. In Proceedings of the Conference
on Empirical Methods in Natural Language Processing
(EMNLP), pages 79?86.
Fabrizio Sebastiani. 2002. Machine learning in automated
text categorization. ACM Computing Surveys, 34(1):1?47.
Yohei Seki, David Kirk Evans, Lun-Wei Ku, Le Sun, Hsin-
Hsi Chen, and Noriko Kando. 2008. Overview of mul-
tilingual opinion analysis task at ntcir-7. In Proceedings
of The 7th NTCIR Workshop (2007/2008) - Evaluation of
Information Access Technologies: Information Retrieval,
Question Answering and Cross-Lingual Information Ac-
cess.
Philip J. Stone, Dexter C. Dunphy, Marshall S. Smith, and
Daniel M. Ogilvie. 1966. The General Inquirer: A Com-
puter Approach to Content Analysis. MIT Press, Cam-
bridge, USA.
Peter D. Turney and Michael L. Littman. 2003. Measur-
ing praise and criticism: Inference of semantic orientation
from association. ACM Transactions on Information Sys-
tems, 21(4):315?346.
Peter D. Turney. 2001. Mining the web for synonyms: Pmi-
ir versus lsa on toefl. In EMCL ?01: Proceedings of the
12th European Conference on Machine Learning, pages
491?502, London, UK. Springer-Verlag.
Casey Whitelaw, Navendu Garg, and Shlomo Argamon.
2005. Using appraisal groups for sentiment analysis. In
Proceedings of the 14th ACM international conference
on Information and knowledge management (CIKM?05),
pages 625?631, Bremen, DE.
Janyce Wiebe, E. Breck, Christopher Buckley, Claire Cardie,
P. Davis, B. Fraser, Diane Litman, D. Pierce, Ellen Riloff,
Theresa Wilson, D. Day, and Mark Maybury. 2003. Rec-
ognizing and organizing opinions expressed in the world
press. In Proceedings of the 2003 AAAI Spring Sympo-
sium on New Directions in Question Answering.
Janyce M. Wiebe, Theresa Wilson, Rebecca Bruce, Matthew
Bell, and Melanie Martin. 2004. Learning subjec-
tive language. Computational Linguistics, 30(3):277?308,
September.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions in
language. Language Resources and Evaluation,
39(2/3):164?210.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005.
Recognizing contextual polarity in phrase-level sentiment
analysis. In Proceedings of the Conference on Human
Language Technology and Empirical Methods in Natural
Language Processing (HLT-EMNLP?05), pages 347?354,
Vancouver, CA.
Kiduk Yang, Ning Yu, Alejandro Valerio, and Hui Zhang.
2006. WIDIT in TREC-2006 Blog track. In Proceedings
of TREC.
Hong Yu and Vasileios Hatzivassiloglou. 2003. Towards an-
swering opinion questions: Separating facts from opinions
and identifying the polarity of opinion sentences. In Pro-
ceedings of 2003 Conference on the Empirical Methods in
Natural Language Processing (EMNLP?03), pages 129?
136, Sapporo, JP.
Chengxiang Zhai and John Lafferty. 2004. A study of
smoothing methods for language models applied to infor-
mation retrieval. ACM Trans. Inf. Syst., 22(2):179?214.
Min Zhang and Xingyao Ye. 2008. A generation model
to unify topic relevance and lexicon-based sentiment for
opinion retrieval. In SIGIR ?08: Proceedings of the 31st
annual international ACM SIGIR conference on Research
and development in information retrieval, pages 411?418,
New York, NY, USA. ACM.
261
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 190?196,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
Chinese Syntactic Reordering for Adequate Generation of Korean 
Verbal Phrases in Chinese-to-Korean SMT 
Jin-Ji Li, Jungi Kim, Dong-Il Kim*, and Jong-Hyeok Lee 
Department of Computer Science and Engineering,  
Electrical and Computer Engineering Division, 
Pohang University of Science and Technology (POSTECH), 
San 31 Hyoja Dong, Pohang, 790-784, R. of Korea 
E-mail: {ljj, yangpa, jhlee}@postech.ac.kr 
 
*Language Engineering Institute,  
Department of Computer, Electron and Telecommunication Engineering, 
Yanbian University of Science and Technology (YUST), 
Yanji, Jilin, 133-000, P.R. of China 
E-mail: {dongil}@ybust.edu.cn 
 
Abstract 
Chinese and Korean belong to different lan-
guage families in terms of word-order and 
morphological typology. Chinese is an SVO 
and morphologically poor language while Ko-
rean is an SOV and morphologically rich one. 
In Chinese-to-Korean SMT systems, systemat-
ic differences between the verbal systems of 
the two languages make the generation of Ko-
rean verbal phrases difficult. To resolve the 
difficulties, we address two issues in this paper. 
The first issue is that the verb position is dif-
ferent from the viewpoint of word-order ty-
pology. The second is the difficulty of com-
plex morphology generation of Korean verbs 
from the viewpoint of morphological typology. 
We propose a Chinese syntactic reordering 
that is better at generating Korean verbal 
phrases in Chinese-to-Korean SMT. Specifi-
cally, we consider reordering rules targeting 
Chinese verb phrases (VPs), preposition 
phrases (PPs), and modality-bearing words 
that are closely related to Korean verbal phras-
es. We verify our system with two corpora of 
different domains. Our proposed approach 
significantly improves the performance of our 
system over a baseline phrased-based SMT 
system. The relative improvements in the two 
corpora are +9.32% and +5.43%, respectively. 
1 Introduction 
Recently, there has been a lot of research on en-
coding syntactic information into statistical ma-
chine translation (SMT) systems in various forms 
and in different stages of translation processes. 
During preprocessing source language sen-
tences undergo reordering and morpho-syntactic 
reconstruction phases to generate more target 
language-like sentences. Also, fixing erroneous 
words, generating complex morphology, and re-
ranking translation results in post-processing 
phases may utilize syntactic information of both 
source and target languages. A syntax-based 
SMT system encodes the syntactic information in 
its translation model of the decoding step. 
A number of researchers have proposed syn-
tactic reordering as a preprocessing step (Xia and 
McCord, 2004; Collins et al, 2005; Wang et al, 
2007). In these syntactic reordering approaches, 
source sentences are first parsed and a series of 
reordering rules are applied to the parsed trees to 
reorder the source sentences into target language-
like word orders. Such an approach is an effec-
tive method for a phrase-based SMT system that 
employs a relatively simple distortion model in 
the decoding phase. 
This paper concentrates upon reordering 
source sentences in the preprocessing step of a 
Chinese-to-Korean phrase-based SMT system 
using syntactic information. Chinese-to-Korean 
SMT has more difficulties than the language 
pairs studied in previous research (French-
English, German-English, and Chinese-English). 
From the viewpoint of language typology, these 
language pairs are all SVO languages and they 
have relatively simpler morphological inflections. 
On the other hand, Korean is an SOV and agglu-
tinative language with relatively free word order 
and with complex and rich inflections. 
For the Chinese-to-Korean SMT, these syste-
matic differences of the two languages make the 
generation of Korean verbal phrases very diffi-
cult. Firstly, the difference in the verb position of 
the two languages may not be reflected in the 
simple distortion model of a phrase-based SMT 
system. Secondly, Morphology generation of 
190
Korean verbs is difficult because of its complexi-
ty and the translation direction from a low-
inflection language to a high-inflection language. 
In the following sections, we describe the cha-
racteristics of Korean verbal phrases and their 
corresponding Chinese verbal phrases, and 
present a set of hand-written syntactic reordering 
rules including Chinese verb phrases (VPs), pre-
position phrases (PPs), and modality-bearing 
words. In the latter sections, we empirically veri-
fy that our reordering rules effectively reposition 
source words to target language-like order and 
improve the translation results. 
2 Contrastive analysis of Chinese and 
Korean with a focus on Korean verbal 
phrase generations 
In the Chinese-to-Korean SMT, the basic transla-
tion units are morphemes. For Chinese, sentences 
are segmented into words. As a typical isolating 
language, each segmented Chinese word is a 
morpheme. Korean is a highly agglutinative lan-
guage and an eojeol refers to a fully inflected 
lexical form separated by a space in a sentence. 
Each eojeol in Korean consists of one or more 
base forms (stem morphemes or content mor-
phemes) and their inflections (function mor-
phemes). Inflections usually include postposi-
tions and verb endings (verb affixes) of verbs 
and adjectives. These base forms and inflections 
are grammatical units in Korean, and they are 
defined as morphemes. As for the translation unit, 
eojeol cause data sparseness problems hence we 
consider a morpheme as a translation unit for 
Korean.  
As briefly mentioned in the previous section, 
Chinese and Korean belong to different word-
order typologies. The difference of verb position 
causes the difficulty in generating correct Korean 
verbal phrases. Also, the complexity of verb af-
fixes in Korean verbs is problematic in SMT sys-
tems targeting Korean, especially if the source 
language is isolated. 
In the Dong-A newspaper corpus on which we 
carry out our experiments in Section 4, Korean 
function morphemes occupy 41.3% of all Korean 
morphemes. Verb endings consist of 40.3% of all 
Korean function words, and the average number 
of function morphemes inflected by a verb or an 
adjective is 1.94 while that of other content mor-
phemes is only 0.7.  
These statistics indicate that the morphological 
form of Korean verbal phrases (Korean verbs) 1 
are significantly complex. A verbal phrase in 
Korean consists of a series of verb affixes along 
with a verb stem. A verb stem cannot be used by 
itself but should take at least one affix to form a 
verbal complex. Verb affixes in Korean are or-
dered in a relative sequence within a verbal com-
plex (Lee, 1991) and express different modality 
information2: tense, aspect, mood, negation, and 
voice (Figure 1). These five grammatical catego-
ries are the major constituents of modal expres-
sion in Korean. 
 
K1: ?(stem) +?_?(aspect prt.) +  ?(aspect prt.)
+ ?(tense prt.) + ?(mood prt.) 
E1: had been eating 
K2. ?(stem) + ?(passive prt.) + ?(aspect prt.) + 
?_ ?_?(modality prt.)  + ?(mood prt.) 
E2: might have been caught 
Figure 1. Verbal phrases in Korean. Bold-faced 
content morphemes followed by functional ones 
with ?+? symbols. Prt. is an acronym for particle. 
 
The modality of Korean is expressed inten-
sively by verb affixes. However, Chinese ex-
presses modality using discontinuous morphemes 
scattered throughout a sentence (Figure 2). Also, 
the prominence of grammatical categories ex-
pressing modality information is different from 
language to language, and correlations of such 
categories in a language are also different. The 
differences between the two languages lead to 
difficulties in alignment and cause linking obscu-
rities. 
 
C3: ??(thief)/??(might)/?(passive prt.)/??
(police)/?(catch)/?(aspect prt.)/? 
 
K3: ??(thief)+?   ??(police)+??   
?(catch)+?(passive prt.)+?(aspect prt.)+?_?_?
(modality prt.)+?(mood prt.)+./ 
E3: The thief might have been caught by the police.
Figure 2. Underlined morphemes are modality-
bearing morphemes in Chinese and Korean sen-
tences. Chinese words are separated by a ?/? 
symbol and Korean eojeols by a space.  
                                                 
1 ?Korean verbal phrase? or ?Korean verbs? in this paper 
refer to Korean predicates (verbs or adjectives) in a sentence.  
2  Modality system refers to five grammatical categories: 
tense, aspect, mood (modality & mood), negation, and voice. 
The definition of these categories is described in detail in 
(Li et al, 2005). 
191
We consider two issues for generating ade-
quate Korean verbal phrases. First is the correct 
position of verbal phrases, and the second is the 
generation of verb affixes which convey modali-
ty information. 
3 Chinese syntactic reordering rules 
In this section, we describe a set of manually 
constructed Chinese syntactic reordering rules. 
Chinese sentences are first parsed by Stanford 
PCFG parser which uses Penn Chinese Treebank 
as the training corpus (Levy and Manning, 2003). 
Penn Chinese Treebank adopts 23 tags for phras-
es (Appendix A). We identified three categories 
in Chinese that need to be reordered: verb phras-
es (VPs), preposition phrases (PPs), and modali-
ty-bearing words.  
3.1 Verb phrases 
Korean is a verb-final language, and verb phrase 
modifiers and complements occur in the pre-
verbal positions. However, in Chinese, verb 
phrase modifiers occur in the pre-verbal or post-
verbal positions, and complements mostly occur 
in post-verbal positions. 
We move the verb phrase modifiers and com-
plements located before the verbal heads to the 
post-verbal position as demonstrated in the fol-
lowing examples. A verbal head consists of a 
verb (including verb compound) and an aspect 
sequence (Xue and Xia, 2000). Therefore, aspect 
markers such as ?? (perfective prt.)?, ??
(durative prt.)?, ??(experiential prt.)? positioned 
immediately after a verb should remain in the 
relatively same position with the preceding verb. 
The third one in the example reordering rules 
shows this case. Mid-sentence punctuations are 
also considered when constructing the reordering 
rules. 
 
Examples of reordering rules of VPs3: 
 
VV0 NP1 ? NP1 VV0 
VV0 IP1 ? IP1 VV0 
VV0 AS1 NP2 ? NP2 VV0 AS1 
VV0 PU1 IP2 ? IP2 PU1 VV0 
 
Original parse tree: 
VP 
      PP (P ?) 
          NP (NN ??) 
      PP (P ?) 
                                                 
3 VV: common verb; AS: aspect marker; P: preposi-
tion; PU: punctuation; PN: pronoun; 
          NP (PN ??) 
      VP (VV ??) 
          NP (NN ??) 
 
Reordered parse tree: 
VP 
      PP (P ?) 
          NP (NN ??) 
      PP (P ?) 
          NP (PN ??) 
      NP (NN ??) 
VP (VV ??) 
 
3.2 Preposition phrases 
Chinese prepositions originate from verbs, and 
they preserve the characteristics of verbs. Chi-
nese prepositions are translated into Korean 
verbs, other content words, or particles. We only 
consider the Chinese prepositions that translate 
into verbs and other content words. We swap the 
prepositions with their objects as demonstrated in 
the following examples.  
 
Examples of reordering rules of PPs: 
 
Case 1: translate into Korean verbs 
P(?)0 NP1 ? NP1 P(?)0 
P(??)0 IP1 ? IP1 P(??)0 
P(??)0 LCP1 ? LCP1 P(??)0 
 
Case 2: translate into other content words 
P(??)0 IP1 ? IP1 P(??)0 
P(??)0 NP1 ? NP1 P(??)0 
 
Original parse tree: 
VP 
      PP (P ?) 
          NP (NN ??) 
      PP (P ?) 
          NP (PN ??) 
      VP (VV ??) 
          NP (NN ??) 
 
Reordered parse tree: 
VP 
  NP (NN ??) 
PP (P ?)  
      PP (P ?) 
          NP (PN ??) 
      VP (VV ??) 
          NP (NN ??) 
 
192
3.3 Modality-bearing words 
Verb affixes in Korean verbal phrases indicate 
modality information such as tense, aspect, mood, 
negation, and voice. The corresponding modality 
information is implicitly or explicitly expressed 
in Chinese. It is important to figure out what fea-
tures are used to represent modality information. 
Li et al (2008) describes in detail the features in 
Chinese that express modality information. 
However, since only lexical features can be reor-
dered, we consider explicit modality features 
only. 
Modality-bearing words are scattered over an 
entire sentence. We move them near their verbal 
heads because their correspondences in Korean 
sentences are always placed right after their 
verbs. 
When constructing reordering rules, we con-
sider temporal adverbs, auxiliary verbs, negation 
particles, and aspect particles only. The follow-
ing example sentences show the results of a few 
of our reordering rules for modality-bearing 
words. 
 
Examples of reordering rules of modality-
bearing words: 
 
Original parse tree: 
VP 
      ADVP (AD ?)                   ? Temporal adverb 
          PP (P ?) 
              LCP 
                  NP (NN ??) (NN ??) (NN ??) 
                  (LC?) 
          VP (VV ??) 
              NP (NN ??) 
 
Reordered parse tree: 
VP 
      PP (P ?) 
          LCP  
              NP (NN ??) (NN ??) (NN ??) 
              (LC ?) 
      ADVP (AD ?)  
VP (VV ??) 
          NP (NN ??) 
 
Original parse tree: 
VP (VV ?)                                ? Auxiliary verb 
       VP 
           PP (P ?) 
               LCP 
                   NP (NN ??) (NN ?) 
                   (LC ?) 
           VP (VV ??) 
Reordered parse tree: 
VP 
        PP (P ?) 
             LCP 
                  NP (NN ??) (NN ?) 
                  (LC ?) 
      VP (VV ?) 
VP (VV ??) 
 
Original parse tree: 
VP 
ADVP (AD ?)                ? Negation particle 
   VP (VV ??)                       ? Auxiliary verb 
   VP 
        PP (P ?) 
           NP (NN ???) (NN ??) 
             VP (VV ??) 
 
Reordered parse tree: 
VP 
     PP (P ?) 
         NP (NN ???) (NN ??) 
ADVP (AD ?) 
VP (VV ??) 
          VP (VV ??) 
 
Generally speaking, Chinese does not have 
grammatical forms for voice. Although, voice is 
also a grammatical category expressing modality 
information, we have left it out of the current 
phase of our experiment since voice detection is 
another research issue and reordering rules for 
voice are unavoidably complicated. 
4 Experiment 
Our baseline system is a popular phrase-based 
SMT system, Moses (Koehn et al, 2007), with 
5-gram SRILM language model (Stolcke, 2002), 
tuned with Minimum Error Training (Och, 2003). 
We adopt NIST (NIST, 2002) and BLEU (Papi-
neni et al, 2001) as our evaluation metrics.  
Chinese sentences in training and test corpora 
are first parsed and are applied a series of syntac-
tic reordering rules. To evaluate the contribution 
of the three categories of syntactic reordering 
rules, we perform the experiments applying each 
category independently. Experiments of various 
combinations are also carried out. 
4.1 Corpus profile 
We automatically collected and constructed a 
sentence-aligned parallel corpus from the online 
193
Dong-A newspaper 4 . Strictly speaking, it is a 
non-literally translated Korean-to-Chinese cor-
pus. The other corpus is provided by MSRA 
(Microsoft Research Asia). It is a Chinese-
Korean-English trilingual corpus of technical 
manuals and a literally translated corpus. 
Chinese sentences are segmented by Stanford 
Chinese word segmenter (Tseng et al, 2005), 
and parsed by Stanford Chinese parser (Levy and 
Manning, 2003). Korean sentences are seg-
mented into morphemes by an in-house morpho-
logical analyzer. 
The detailed corpus profiles are displayed in 
Table 1 and 2. The Dong-A newspaper corpus is 
much longer than the MSRA technical manual 
corpus. In Korean, we report the length of con-
tent and function words. 
 
 Training (99,226 sentences) 
Chinese Korean Content Function
# of words 2,692,474 1,859,105 1,277,756 
# of singletons 78,326 67,070 514
avg. sen. length 27.13 18.74 12.88
 Development (500 sentences) 
Chinese Korean Content Function
# of words 14,485 9,863 6,875
# of singletons 4,029 4,166 163
avg. sen. length 28.97 19.73 13.75
 Test (500 sentences) 
Chinese Korean Content Function
# of words 14,657 10,049 6,980
# of singletons 4,027 4,217 164
avg. sen. length 29.31 20.10 13.96
Table 1. Corpus profile of Dong-A newspaper. 
 
 Training (29,754 sentences) 
Chinese Korean Content Function
# of words 425,023  316,289 207,909
# of singletons 5,746 4,689 197
avg. sen. length 14.29 10.63 6.99
 Development (500 sentences) 
Chinese Korean Content Function
# of words 6,380 4,853 3,214
# of singletons 1,174 975 93
avg. sen. length 12.76 9.71 6.43
 Test (500 sentences) 
Chinese Korean Content Function
                                                 
4 http://www.donga.com/news/ (Korean) and 
http://chinese.donga.com/gb/index.html (Chinese) 
# of words 7,451 5,336 3,548
# of singletons 1,182 964 99
avg. sen. length 14.90 10.67 7.10
Table 2. Corpus profile of MSRA technical ma-
nual. 
4.2 Result and discussion 
The experimental results are displayed in Table 3 
and 4. Besides assessing the effectiveness of 
each reordering category, we test various combi-
nations of the three categories. 
 
Method NIST BLEU 
Baseline 5.7801 20.49 
Reorder.VP 5.8402  22.12 (+7.96%) 
Reorder.PP 5.7773  20.10 (-1.90%) 
Reorder.Modality 5.7682  20.93 (+2.15%) 
Reorder.VP+PP 5.8176  21.96 (+7.17%) 
Reorder.VP+Modality 5.9198  22.24 (+8.54%) 
Reorder.All 5.9361 22.40 (+9.32%) 
Table 3. Experimental results on the Dong-A 
newspaper corpus. 
 
Method NIST BLEU 
Baseline 7.2596 44.03 
Reorder.VP 7.2238  44.57 (+1.23%) 
Reorder.PP 7.2793  44.22 (+0.43%) 
Reorder.Modality 7.3110  44.25 (+0.50%) 
Reorder.VP+PP 7.3401  45.28 (+2.84%) 
Reorder.VP+Modality 7.4246  46.42 (+5.43%) 
Reorder.All 7.3849  46.33 (+5.22%) 
Table 4. Experimental results on the MSRA 
technical manual corpus. 
 
From the experimental result of the Dong-A 
newspaper corpus, we find that the most effec-
tive category is the reordering rules of VPs. 
When the VP reordering rules are combined with 
the modality ones, the performance is even better. 
The gain of BLEU is not significant, but the gain 
of NIST is significant from 5.8402 to 5.9198. 
The PP reordering rules do not contribute to the 
performance when they are singly applied. How-
ever, when combined with the other two catego-
ries, they contribute to the performance. The best 
performance is achieved when all three catego-
ries? reordering rules are applied and the relative 
improvement is +9.32% over the baseline system. 
In the MSRA corpus, the performance of vari-
ous combinations of the three categories is better 
than those of the individual categories. The PP 
category shows improvement when it is com-
bined with the VP category. The combination of 
VP and modality category improves the perfor-
mance by +5.43% over the baseline. 
194
These results agree with our expectations: re-
solving the word order and modality expression 
differences of verbal phrases between Chinese 
and Korean is an effective approach.  
4.3 Error Analysis 
We adopt an error analysis method proposed by 
Vilar et al (2006). They presented a framework 
for classifying error types of SMT systems. (Ap-
pendix B.)  
Since our approach focuses on verbal phrase 
differences between Chinese and Korean, we 
carry out the error analysis only on the verbal 
heads. Three types of errors are considered:  
word order, missing words, and incorrect words. 
We further classify the incorrect words category 
into two sub-categories: wrong lexical 
choice/extra word, and incorrect form of modali-
ty information. 50 sentences are selected from 
each test corpus on which to perform the error 
analysis. For each corpus, we choose the best 
system: Reorder.All for the Dong-A corpus and 
Reorder.VP+modality for the MSRA corpus. 
 The most frequent error type is wrong word 
order in both corpora. When a verb without any 
modality information appears in a wrong position, 
we only count it as a wrong word order but not 
as a wrong modality. Therefore, the number of 
wrong modalities is not as frequent as it should 
be. 
Table 5 and 6 indicate that our proposed me-
thod helps improve the SMT system to reduce 
the number of error types related to verbal phras-
es. 
 
Error type Frequency Baseline Reorder.All
wrong word order  34 7
missing content word  18 5
wrong lexical choice/ 
extra word  6 1
wrong modality  10 6
Table 5. Error analysis of the Dong-A newspaper 
corpus. 
 
Error type 
Frequency 
Baseline Reorder. VP+Modality
wrong word order 19 11
missing content word 4 2
wrong lexical choice/ 
extra word 8 3
wrong modality  11 6
Table 6. Error analysis of the MSRA technical 
manual corpus. 
 
5 Conclusion and future work 
In this paper, we proposed a Chinese syntactic 
reordering more suitable to adequately generate 
Korean verbal phrases in Chinese-to-Korean 
SMT. Specifically, we considered reordering 
rules targeting Chinese VPs, PPs, and modality-
bearing words that are closely related to Korean 
verbal phrases. 
Through a contrastive analysis between the 
two languages, we first showed the difficulty of 
generating Korean verbal phrases when translat-
ing from a morphologically poor language, Chi-
nese. Then, we proposed a set of syntactic reor-
dering rules to reorder Chinese sentences into a 
more Korean like word order. 
We conducted several experiments to assess 
the contributions of our method. The reordering 
of VPs is the most effective, and improves the 
performance even more when combined with the 
reordering rules of modality-bearing words. Ap-
plied to the Dong-A newspaper corpus and the 
MSRA technical manual corpus, our proposed 
approach improved the baseline systems by 
9.32% and 5.43%, respectively. We also per-
formed error analysis with a focus on verbal 
phrases. Our approach effectively decreased the 
size of all errors.  
There remain several issues as possible future 
work. We only considered the explicit modality 
features and relocated them near the verbal heads. 
In the future, we may improve our system by 
extracting implicit modality features. 
In addition to generating verbal phrases, there 
is the more general issue of generating complex 
morphology in SMT systems targeting Korean, 
such as generating Korean case markers. There 
are several previous studies on this topic (Min-
kov et al, 2007; Toutanova et al, 2008). This 
issue will also be the focus of our future work in 
both the phrase- and syntax-based SMT frame-
works. 
 
Acknowledgments 
This work was supported in part by MKE & II-
TA through the IT Leading R&D Support Project 
and also in part by the BK 21 Project in 2009. 
References  
Charles N. Li, and Sandra A. Thompson 1996. Man-
darin Chinese: A functional reference grammar, 
University of California Press, USA. 
David Vilar, Jia Xu, Luis Fernando D?Haro, and 
Hermann Ney. 2006. Error Analysis of Statistical 
195
Machine Translation Output. In Proceedings of 
LREC. 
Einat Minkov, Kristina Toutanova, and Hisami Suzu-
ki. 2007. Generating Complex Morphology for 
Machine Translation. In Proceedings of ACL.  
Fei Xia and Michael McCord. 2004. Improving a sta-
tistical MT system with automatically learned re-
write patterns. In Proceedings of COLING. 
Huihsin Tseng, Pichuan Chang, Galen Andrew, Da-
niel Jurafsky and Christopher Manning. 2005. A 
Conditional Random Field Word Segmenter. In 
Fourth SIGHAN Workshop on Chinese Language 
Processing.  
HyoSang Lee 1991. Tense, aspect, and modality: A 
discourse-pragmatic analysis of verbal affixes in 
Korean from a typological perspective, PhD thesis, 
Univ. of California, Los Angeles. 
Jin-Ji Li, Ji-Eun Roh, Dong-Il Kimand Jong-Hyeok 
Lee. 2005. Contrastive Analysis and Feature Selec-
tion for Korean Modal Expression in Chinese-
Korean Machine Translation System. International 
Journal of Computer Processing of Oriental Lan-
guages, 18(3), 227--242. 
 Jin-Ji Li, Dong-Il Kim and Jong-Hyeok Lee. 2008. 
Annotation Guidelines for Chinese-Korean Word 
Alignment. In Proceedings of LREC. 
Kristina Toutanova, Hisami Suzuki, and Achim 
Puopp. 2008. Applying Morphology Generation 
Models to Machine Translation. In Proceedings of 
ACL. 
Nianwen Xue, and Fei Xia. 2000. The bracketing 
guidelines for the Penn Chinese Treebank (3.0). 
IRCS technical report, University of Pennsylvania. 
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha 
Palmer. 2005. The Penn Chinese Treebank: Phrase 
structure annotation of a large corpus. Natural 
Language Engineering, 11(2):207?238. 
Michael Collins, Philipp Koehn, and Ivona 
Ku?cerov?a. 2005. Clause restructuring for statis-
tical machine translation. In Proceedings of ACL, 
pages 531?540. 
NIST. 2002. Automatic evaluation of machine trans-
lation quality using n-gram co-occurrence statis-
tics. 
Och, F. J. 2003. Minimum error rate training in sta-
tistical machine translation. In Proceedings of 
ACL. 
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris-
Callison-Burch, Marcello Federico, Nicola Bertoldi, 
Brooke Cowan, Wade Shen, Christine Moran, Ri-
chard Zens, Chris Dyer, Ondrej Bojar, Alexandra 
Constrantin, and Evan Herbst. 2007. Moses: Open 
source toolkit for statistical machine translation. In 
Proceedings of ACL, Demonstration Session.  
Roger Levy and Christopher D. Manning. 2003. Is it 
harder to parse Chinese, or the Chinese Treebank? 
In Proceedings of ACL. 
Stolcke, A. 2002. SRILM - an extensible language 
modeling toolkit. In Proceedings of ICSLP, 2:901-
904. 
Appendix A. Tag for phrases in Penn 
Chinese Treebank. 
ADJP adjective phrase 
ADVP adverbial phrase headed by AD (adverb)
CLP  classifier phrase 
CP   clause headed by C (complementizer) 
DNP  phrase formed by ?XP+DEG? 
DP   determiner phrase 
DVP  phrase formed by ?XP+DEV? 
FRAG fragment 
IP   simple clause headed by I (INFL) 
LCP  phrase formed by ?XP+LC? 
LST  list marker 
NP   noun phrase 
PP   preposition phrase 
PRN  parenthetical 
QP   quantifier phrase 
UCP  unidentical coordination phrase 
VP   verb phrase 
Appendix B. Classification of translation 
errors proposed by Vilar et al (2006). 
 
196
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 595?603,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Evaluating Multilanguage-Comparability of Subjectivity Analysis
Systems
Jungi Kim, Jin-Ji Li and Jong-Hyeok Lee
Division of Electrical and Computer Engineering
Pohang University of Science and Technology, Pohang, Republic of Korea
{yangpa,ljj,jhlee}@postech.ac.kr
Abstract
Subjectivity analysis is a rapidly grow-
ing field of study. Along with its ap-
plications to various NLP tasks, much
work have put efforts into multilingual
subjectivity learning from existing re-
sources. Multilingual subjectivity analy-
sis requires language-independent crite-
ria for comparable outcomes across lan-
guages. This paper proposes to mea-
sure the multilanguage-comparability of
subjectivity analysis tools, and provides
meaningful comparisons of multilingual
subjectivity analysis from various points
of view.
1 Introduction
The field of NLP has seen a recent surge in the
amount of research on subjectivity analysis. Along
with its applications to various NLP tasks, there
have been efforts made to extend the resources
and tools created for the English language to other
languages. These endeavors have been success-
ful in constructing lexicons, annotated corpora,
and tools for subjectivity analysis in multiple lan-
guages.
There are multilingual subjectivity analysis sys-
tems available that have been built to monitor and
analyze various concerns and opinions on the In-
ternet; among the better known are OASYS from
the University of Maryland that analyzes opinions
on topics from news article searches in multiple
languages (Cesarano et al, 2007)1 and TextMap,
an entity search engine developed by Stony Brook
University for sentiment analysis along with other
functionalities (Bautin et al, 2008).2 Though these
systems currently rely on English analysis tools
and a machine translation (MT) technology to
1http://oasys.umiacs.umd.edu/oasysnew/
2http://www.textmap.com/
translate other languages into English, up-to-date
research provides various ways to analyze subjec-
tivity in multilingual environments.
Given sentiment analysis systems in differ-
ent languages, there are many situations when
the analysis outcomes need to be multilanguage-
comparable. For example, it has been common
these days for the Internet users across the world
to share their views and opinions on various top-
ics including music, books, movies, and global af-
fairs and incidents, and also multinational compa-
nies such as Apple and Samsung need to analyze
customer feedbacks for their products and services
from many countries in different languages. Gov-
ernments may also be interested in monitoring ter-
rorist web forums or its global reputation. Sur-
veying these opinions and sentiments in various
languages involves merging the analysis outcomes
into a single database, thereby objectively compar-
ing the result across languages.
If there exists an ideal subjectivity analy-
sis system for each language, evaluating the
multilanguage-comparability would be unneces-
sary because the analysis in each language would
correctly identify the exact meanings of all in-
put texts regardless of the language. However, this
requirement is not fulfilled with current technol-
ogy, thus the need for defining and measuring the
multilanguage-comparability of subjectivity anal-
ysis systems is evident.
This paper proposes to evaluate the
multilanguage-comparability of multilingual
subjectivity analysis systems. We build a number
of subjectivity classifiers that distinguishes sub-
jective texts from objective ones, and measure
the multilanguage-comparability according to our
proposed evaluation method. Since subjectivity
analysis tools in languages other than English are
not readily available, we focus our experiments on
comparing different methods to build multilingual
analysis systems from the resources and systems
595
created for English. These approaches enable us to
extend a monolingual system to many languages
with a number of freely available NLP resources
and tools.
2 Related Work
Much research have been put into developing
methods for multilingual subjectivity analysis re-
cently. With the high availability of subjectivity re-
sources and tools in English, an easy and straight-
forward approach would be to employ a machine
translation (MT) system to translate input texts
in target languages into English then carry out
the analyses using an existing subjectivity analy-
sis tool (Kim and Hovy, 2006; Bautin et al, 2008;
Banea et al, 2008). Mihalcea et al (2007) and
Banea et al (2008) proposed a number of ap-
proaches exploiting a bilingual dictionary, a paral-
lel corpus, and an MT system to port the resources
and systems available in English to languages with
limited resources.
For subjectivity lexicons translation, Mihalcea
et al (2007) and Wan (2008) used the first sense in
a bilingual dictionary, Kim and Hovy (2006) used
a parallel corpus and a word alignment tool to ex-
tract translation pairs, and Kim et al (2009) used
a dictionary to translate and a link analysis algo-
rithm to refine the matching intensity.
To overcome the shortcomings of available re-
sources and to take advantage of ensemble sys-
tems, Wan (2008) and Wan (2009) explored meth-
ods for developing a hybrid system for Chinese us-
ing English and Chinese sentiment analyzers. Ab-
basi et al (2008) and Boiy and Moens (2009) have
created manually annotated gold standards in tar-
get languages and studied various feature selec-
tion and learning techniques in machine learning
approaches to analyze sentiments in multilingual
web documents.
For learning multilingual subjectivity, the lit-
erature tentatively concludes that translating lex-
icon is less dependable in terms of preserving sub-
jectivity than corpus translation (Mihalcea et al,
2007; Wan, 2008), and though corpus translation
results in modest performance degradation, it pro-
vides a viable approach because no manual la-
bor is required (Banea et al, 2008; Brooke et al,
2009).
Based on the observation that the performances
of subjectivity analysis systems in comparable
experimental settings for two languages differ,
Texts with an identical negative sentiment:* The iPad could cannibalize the e-reader market. * ?????(iPad) ??? ???(e-reader market) 
???? ? ??(could cannibalize).
Texts with different strengths of positive sentiments:* Samsung cell phones have excellent battery life.* ??(Samsung) ????(cell phone) ????(battery) ????(somehow or other) ????(last long).
Figure 1: Examples of sentiments in multilingual
text
Banea et al (2008) have attributed the variations
in the difficulty level of subjectivity learning to
the differences in language construction. Bautin et
al. (2008)?s system analyzes the sentiment scores
of entities in multilingual news and blogs and ad-
justed the sentiment scores using entity sentiment
probabilities of languages.
3 Multilanguage-Comparability
3.1 Motivation
The quality of a subjectivity analysis tool is mea-
sured by its ability to distinguish subjectivity from
objectivity and/or positive sentiments from nega-
tive sentiments. Additionally, a multilingual sub-
jectivity analysis system is required to generate
unbiased analysis results across languages; the
system should base its outcome solely on the sub-
jective meanings of input texts irrespective of the
language, and the equalities and inequalities of
subjectivity labels and intensities must be useful
within and throughout the languages.
Let us consider two cases where the pairs of
multilingual inputs in English and Korean have
identical and different subjectivity meanings (Fig-
ure 1). The first pair of texts carry a negative sen-
timent about how the release of a new electronics
device might affect an emerging business market.
When a multilanguage-comparable system is in-
putted with such a pair, its output should appropri-
ately reflect the negative sentiment, and be identi-
cal for both texts. The second pair of texts share
a similar positive sentiment about a mobile de-
vice?s battery capacity but with different strengths.
A good multilingual system must be able to iden-
tify the positive sentiments and distinguish the dif-
ferences in their intensities.
However, these kinds of conditions cannot be
measured with performance evaluations indepen-
596
dently carried out on each language; A system
with a dissimilar ability to analyze subjective ex-
pressions from one language to another may de-
liver opposite labels or biased scores on texts with
an identical subjective meaning, and vice versa,
but still might produce similar performances on
the evaluation data.
Macro evaluations on individual languages can-
not provide any conclusions on the system?s
multilanguage-comparability capability. To mea-
sure how much of a system?s judgment principles
are preserved across languages, an evaluation from
a different perspective is necessary.
3.2 Evaluation Approach
An evaluation of multilanguage-comparability
may be done in two ways: measuring agreements
in the outcomes of a pair of multilingual texts with
an identical subjective meaning, or measuring the
consistencies in the label and/or accordance in the
order of intensity of a pair of texts with different
subjectivities.
There are advantages and disadvantages to each
approaches. The first approach requires multi-
lingual texts aligned at the level of specificity,
for instance, document, sentence and phrase, that
the subjectivity analysis system works. Text cor-
pora for MT evaluation such as newspapers,
books, technical manuals, and government offi-
cial records provide a wide variety of parallel
texts, typically at the sentence level. Annotating
these types of corpus can be efficient; as par-
allel texts must have identical semantic mean-
ings, subjectivity?related annotations for one lan-
guage can be projected into other languages with-
out much loss of accuracy.
The latter approach accepts any pair of multi-
lingual texts as long as they are annotated with la-
bels and/or intensity. In this case, evaluating the la-
bel consistency of a multilingual system is only as
difficult as evaluating that of a monolingual sys-
tem; we can produce all possible pairs of texts
from test corpora annotated with labels for each
language. Evaluating with intensity is not easy for
the latter approach; if test corpora already exist
with intensity annotations for both languages, nor-
malizing the intensity scores to a comparable scale
is necessary (yet is uncertain unless every pair is
checked manually), otherwise every pair of mul-
tilingual texts needs a manual annotation with its
relative order of intensity.
In this paper, we utilize the first approach be-
cause it provides a more rational means; we can
reasonably hypothesize that text translated into an-
other language by a skilled translator carries an
identical semantic meaning and thereby conveys
identical subjectivity. Therefore the required re-
source is more easily attained in relatively inex-
pensive ways.
For evaluation, we measure the consistency in
the subjectivity labels and the correlation of sub-
jectivity intensity scores of parallel texts. Section
5.1 describes the details of evaluation metrics.
4 Multilingual Subjectivity System
We create a number of multilingual systems con-
sisting of multiple subsystems each processing a
language, where one system analyzes English, and
the other systems analyze the Korean, Chinese,
and Japanese languages. We try to reproduce a set
of systems using diverse methods in order to com-
pare the systems and find out which methods are
more suitable for multilanguage-comparability.
4.1 Source Language System
We adopt the three systems described below as our
source language systems: a state-of-the-art sub-
jectivity classifier, a corpus-based, and a lexicon-
based systems. The resources needed for devel-
oping the systems or the system itself are readily
available for research purposes. In addition, these
systems cover the general spectrum of current ap-
proaches to subjectivity analysis.
State-of-the-art (S-SA): OpinionFinder is a
publicly-available NLP tool for subjectivity analy-
sis (Wiebe and Riloff, 2005; Wilson et al, 2005).3
The software and its resources have been widely
used in the field of subjectivity analysis, and it
has been the de facto standard system against
which new systems are validated. We use a high-
coverage classifier from the OpinionFinder?s two
sentence-level subjectivity classifiers. This Naive
Bayes classifier builds upon a corpus annotated by
a high-precision classifier with the bootstrapping
of the corpus and extraction patterns. The classi-
fier assesses a sentence?s subjectivity with a label
and a score for confidence in its judgment.
Corpus-based (S-CB): The MPQA opinion cor-
pus is a collection of 535 newspaper articles in En-
glish annotated with opinions and private states at
3http://www.cs.pitt.edu/mpqa/opinionfinderrelease/, ver-
sion 1.5
597
the sub-sentence level (Wiebe et al, 2003).4 We
retrieve the sentence level subjectivity labels for
11,111 sentences using the set of rules described
in (Wiebe and Riloff, 2005). The corpus provides
a relatively balanced corpus with 55% subjective
sentences. We train an ML-based classifier us-
ing the corpus. Previous studies have found that,
among several ML-based approaches, the SVM
classifier generally performs well in many subjec-
tivity analysis tasks (Pang et al, 2002; Banea et
al., 2008).
We use SVMLight with its default configura-
tions,5 inputted with a sentence represented as a
feature vector of word unigrams and their counts
in the sentence. An SVM score (a margin or the
distance from a learned decision boundary) with a
positive value predicts the input as being subjec-
tive, and negative value as objective.
Lexicon-based (S-LB): OpinionFinder contains a
list of English subjectivity clue words with in-
tensity labels (Wilson et al, 2005). The lexicon
is compiled from several manually and automati-
cally built resources and contains 6885 unique en-
tries.
Riloff and Wiebe (2003) constructed a high-
precision classifier for contiguous sentences us-
ing the number of strong and weak subjective
words in current and nearby sentences. Unlike pre-
vious work, we do not (or rather, cannot) main-
tain assumptions about the proximity of input text.
Using the lexicon, we build a simple and high-
coverage rule-based subjectivity classifier. Setting
the scores of strong and weak subjective words as
1.0 and 0.5, we evaluate the subjectivity of a given
sentence as the sum of subjectivity scores; above
a threshold, the input is subjective, and otherwise
objective. The threshold value is optimized for an
F-measure using the MPQA corpus, and is set to
1.0 throughout our experiments.
4.2 Target Language System
To construct a target language system leveraging
on available resources in the source language, we
consider three approaches from previous litera-
ture:
1. translating test sentences in target language
into source language and inputting them into
4http://www.cs.pitt.edu/mpqa/databaserelease/, version
1.2
5http://svmlight.joachims.org/, version 6.02
a source language system (Kim and Hovy,
2006; Bautin et al, 2008; Banea et al, 2008)
2. translating a source language training corpus
into target language and creating a corpus-
based system in target language (Banea et al,
2008)
3. translating a subjectivity lexicon from source
language to target language and creating a
lexicon-based system in target language (Mi-
halcea et al, 2007)
Each approach has its advantages and disadvan-
tages. The advantage of the first approach is its
simple architecture, clear separation of subjectiv-
ity and MT systems, and that it has only one sub-
jectivity system, and is thus easier to maintain.
Its disadvantage is that the time-consuming MT
has to be executed for each text input. In the sec-
ond and third approaches, a subjectivity system in
the target language is constructed sharing corpora,
rules, and/or features with the source language
system. Later on, it may also include its own set
of resources specifically engineered for the target
language as a performance improvement. How-
ever, keeping the systems up-to-date would require
as much effort as the number of languages. All
three approaches use MT, and would suffer sig-
nificantly if the translation results are poor.
Using the first approach, we can easily adopt all
three source language systems;
? Target input translated into source, analyzed
by source language system S-SA
? Target input translated into source, analyzed
by source language system S-CB
? Target input translated into source, analyzed
by source language system S-LB
The second and the third approaches are carried
out as follows:
Corpus-based (T-CB): We translate the MPQA
corpus into the target languages sentence by sen-
tence using a web-based service.6 Using the same
method for S-CB, we train an SVM model for
each language with the translated training corpora.
Lexicon-based (T-LB): This classifier is identi-
cal to S-LB, where the English lexicon is replaced
by one of the target languages. We automatically
translate the lexicon using free bilingual dictionar-
ies.7 First, the entries in the lexicon are looked
6Google Translate (http://translate.google.com/)
7quick english-korean, quick eng-zh CN, and JMDict
from StarDict (http://stardict.sourceforge.net/) licensed under
GPL and EDRDG.
598
Table 1: Agreement on subjectivity (S for subjec-
tive, O objective) of 859 sentence chunks in Ko-
rean between two annotators (An. 1 and An. 2).
An. 2
S O Total
A
n.
1 S 371 93 464
O 23 372 395
Total 394 465 859
up in the dictionary, if they are found, we se-
lect the first word in the first sense of the def-
inition. If the entry is not in the dictionary, we
lemmatize it,8 then repeat the search. Our sim-
ple approach produces moderate-sized lexicons
(3,808, 3,980, 3,027 for Korean, Chinese, and
Japanese) compared to Mihalcea et al (2007)?s
complicated translation approach (4,983 Roma-
nian words). The threshold values are optimized
using the MPQA corpus translated into each tar-
get language.9
5 Experiment
5.1 Experimental Setup
Test Corpus
Our evaluation corpus consists of 50 parallel
newspaper articles from the Donga Daily News
Website.10 The website provides news articles in
Korean and their human translations in English,
Japanese, and Chinese. We selected articles that
contain Editorial in its English title from a 30-
day period. Three human annotators who are flu-
ent in the two languages manually annotated N-
to-N sentence alignments for each language pairs
(KR-EN, KR-CH, KR-JP). By keeping only the
sentence chunks whose Korean chunk appears in
all language pairs, we were left with 859 sentence
chunk pairs.
The corpus was preprocessed with NLP tools
for each language,11 and the Korean, Chinese, and
Japanese texts were translated into English with
the same web-based service used to translate the
training corpus in Section 4.2.
Manual Annotation and Agreement Study
8JWI (http://projects.csail.mit.edu/jwi/)
9Korean 1.0, Chinese 1.0, and Japanese 0.5
10http://www.donga.com/
11Stanford POS Tagger 1.5.1 and Stanford Chinese Word
Segmenter 2008-05-21 (http://nlp.stanford.edu/software/),
Chasen 2.4.4 (http://chasen-legacy.sourceforge.jp/), Korean
Morphological Analyzer (KoMA) (http://kle.postech.ac.kr/)
Table 2: Agreement on projection of subjectivity
(S for subjective, O objective) from Korean (KR)
to English (EN) by one annotator.
EN
S O Total
K
R
S 458 6 464
O 12 383 395
Total 470 389 859
To assess the performance of our subjectiv-
ity analysis systems, the Korean sentence chunks
were manually annotated by two native speakers
of Korean with Subjective and Objective labels
(Table 1). A proportion agreement of 0.86 and a
kappa value of 0.73 indicate a substantial agree-
ment between the two annotators. We set aside
743 sentence chunks that both annotators agreed
on for the automatic evaluation of subjectivity
analysis systems, thereby removing the borderline
cases, which are difficult even for humans to as-
sess. The corresponding sentence chunks for other
languages were extracted and tagged with labels
equivalent to Korean chunks.
In addition, to verify how consistently the sub-
jectivity of the original texts is projected to the
translated, we carried out another manual annota-
tion and agreement study with Korean and English
sentence chunks (Table 2).
Note that our cross-lingual agreement study is
similar to the one carried out by Mihalcea et
al. (2007), where two annotators labeled the sen-
tence subjectivity of a parallel text in different lan-
guages. They reported that, similarly to monolin-
gual annotations, most cases of disagreements on
annotations are due to the differences in the anno-
tators? judgments on subjectivity, and the rest from
subjective meanings lost in the translation process
and figurative language such as irony.
To avoid the role played by annotators? pri-
vate views from disagreements, the subjectivity of
sentence chunks in English were manually anno-
tated by one of the annotators for the Korean text.
Judged by the same annotator, we speculate that
the disagreement in the annotation should account
only for the inconsistency in the subjectivity pro-
jection. By proportion, the agreement between the
annotation of Korean and English is 0.97, and the
kappa is 0.96, suggesting an almost perfect agree-
ment. Only a small number of sentence chunk
pairs have inconsistent labels; six chunks in Ko-
599
Texts swihanwseanwiadxcahh liwgcvm:giwc*nht*wsvnPoi???i???ubswgiwseazi?i???ulshx*csw-r:*xzi
????i??ubslansn:zkoi.vcha (iwgaiua vnves zilshx*csw-iu)awbaanifvmwgipvca*i*nliSvcwgipvca*zishibvchansn:ibswgiwseak
fanwseanwitvhwisniwc*nht*wsvnPoi???i??i??????uTnls*yhi#*w*i$vwvchzi%%&&????i???i???u%(%&&'lvtt*ci*mwvev)staiS*nvzi!"#uxcahanwalzi$%&i'(?ulcabi*wwanwsvnzkoiTnls*yhi#*w*i$vwvchig*hixcvlm aliwgai%(%&&'lvtt*cihm) vex* wiS*nvk
Figure 2: Excerpts from Donga Daily News with
differing sentiments between parallel texts
rean lost subjectivity in translation, and implied
subjective meanings in twelve chunks were ex-
pressed explicitly through interpretation. Excerpts
from our corpus show two such cases (Figure 2).
Evaluation Metrics
To evaluate the multilanguage-comparability of
subjectivity analysis systems, we measure 1) how
consistently the system assigns subjectivity labels
and 2) how closely numeric scores for systems?
confidences correlate with regard to parallel texts
in different languages.
In particular, we use Cohen?s kappa coefficient
for the first and Pearson?s correlation coefficient
for the latter. These widely used metrics provide
useful comparability measures for categorical and
quantitative data.
Both coefficients are scaled from ?1 to +1, in-
dicating negative to positive correlations. Kappa
measures are corrected for chance, thereby yield-
ing better measurements than agreement by pro-
portion. The characteristics of Pearson?s correla-
tion coefficient that it measures linear relation-
ships and is independent of change in origin, scale,
and unit comply with our experiments.
5.2 Subjectivity Classification
Our multilingual subjectivity analysis systems
were evaluated on the test corpora described in
Section 5.1 (Table 3).
Due to the difference in testbeds, the perfor-
mance of the state-of-the-art English system (S-
SA) on our corpus is lower by about 10% rela-
tively than the performance reported on the MPQA
corpus.12 However, it still performs sufficiently
12precision, recall, and F-measure of 79.4, 70.6, and 74.7.
well and provides the most balanced results among
the three source language systems; The corpus-
based system (S-CB) classifies with a high pre-
cision, and the lexicon-based (S-LB) with a high
recall. The source language systems (S-SA,-CB,-
LB) lose a small percentage in precision when in-
putted with translations, but the recalls are gener-
ally on a par or even higher in the target languages.
For the systems created from target language re-
sources, Corpus-based systems (T-CB) generally
perform better than the ones with source language
resource (S-CB), and lexicon-based systems (T-
LB) perform worse than (S-LB). Similarly to sys-
tems with source language resources, T-CB clas-
sifies with a high precision and T-LB with a high
recall, but the gap is less. Among the target lan-
guages, Korean tends to have a higher precision,
and Japanese a higher recall than other languages
in most systems.
Overall, S-SA provides easy accessibility when
analyzing both the source and the target languages,
with a balanced precision and recall performance.
Among the other approaches, only T-CB is bet-
ter in all measures than S-SA, and S-LB performs
best on F-measure evaluations.
5.3 Multilanguage-Comparability
The evaluation results on multilanguage-
comparability are presented in Table 4. The
subjectivity analysis systems are evaluated with
all language pairs with kappa and Pearson?s
correlation coefficients. Kappa and Pearson?s
correlation values are consistent with each other;
Pearson?s correlation between the two evaluation
measures is 0.91.
We observe a distinct contrast in performances
between corpus-based systems (S-CB and T-CB)
and lexicon-based systems (S-LB and T-LB); All
corpus-based systems show moderate agreements
while agreements on lexicon-based systems are
only fair.
Within corpus-based systems, S-CB performs
better with language pairs that include English,
and T-CB performs better with language pairs of
the target languages.
For lexicon-based systems, systems in the tar-
get languages (T-LB) performs the worst with
only slight to fair agreements between languages.
Lexicon-based systems and state-of-the-art sys-
tems in the source language (S-LB and S-SA) re-
sult in average performances.
600
Table 3: Performance of subjectivity analysis with precision (P), recall (R), and F-measure (F). S-SA,-
CB,-LB systems in Korean, Chinese, Japanese indicate English analysis systems inputted with transla-
tions of the target languages into English.
English Korean Chinese Japanese
P R F P R F P R F P R F
S-SA 71.1 63.5 67.1 70.7 61.1 65.6 67.3 68.8 68.0 69.1 67.5 68.3
S-CB 74.4 53.9 62.5 74.5 52.2 61.4 71.1 63.3 67.0 72.9 65.3 68.9
S-LB 62.5 87.7 73.0 62.9 87.7 73.3 59.9 91.5 72.4 61.8 94.1 74.6
T-CB 72.4 67.5 69.8 75.0 66.2 70.3 72.5 70.3 71.4
T-LB 59.4 71.0 64.7 58.4 82.3 68.2 56.9 92.4 70.4
Table 4: Performance of multilanguage-comparability: kappa coefficient (?) for measuring comparability
of classification labels and Pearson?s correlation coefficient (?) for classification scores for English (EN),
Korean (KR), Chinese (CH), and Japanese (JP). Evaluations of T-CB,-LB for language pairs including
English are carried out with results from S-CB,-LB for English and T-CB,-LB for target languages.
S-SA S-CB S-LB T-CB T-LB
? ? ? ? ? ? ? ? ? ?
EN & KR 0.41 0.55 0.45 0.60 0.37 0.59 0.42 0.60 0.25 0.41
EN & CH 0.39 0.54 0.41 0.62 0.33 0.52 0.39 0.57 0.22 0.38
EN & JP 0.39 0.53 0.43 0.65 0.30 0.59 0.40 0.59 0.15 0.33
KR & CH 0.36 0.54 0.39 0.59 0.28 0.57 0.46 0.64 0.23 0.37
KR & JP 0.37 0.60 0.44 0.69 0.50 0.69 0.63 0.76 0.18 0.38
CH & JP 0.37 0.53 0.49 0.66 0.29 0.57 0.46 0.63 0.22 0.46
Average 0.38 0.55 0.44 0.64 0.35 0.59 0.46 0.63 0.21 0.39
-100
-50
0
50
100
-100 -50 0 50 100
(a) S-SA
-4
-3
-2
-1
0
1
2
3
4
-4 -3 -2 -1 0 1 2 3 4
(b) S-CB
-10
-5
0
5
10
-10 -5 0 5 10
(c) S-LB
-4
-3
-2
-1
0
1
2
3
4
-4 -3 -2 -1 0 1 2 3 4
(d) T-CB
-10
-5
0
5
10
-10 -5 0 5 10
(e) T-LB
Figure 3: Scatter plots of English (x-axis) and Korean (y-axis) subjectivity scores from state-of-the-art
(S-SA), corpus-based (S-CB), and lexicon-based (S-LB) systems of the source language, and corpus-
based with translated corpora (T-CB), and lexicon-based with translated lexicon (T-LB) systems. Slanted
lines in figures are best-fit lines through the origins.
601
Figure 3 shows scatter plots of subjectivity
scores of our English and Korean test corpora eval-
uated on different systems; the data points on the
first and the third quadrants are occurrences of la-
bel agreements, and the second and the fourth are
disagreements. Linearly scattered data points are
more correlated regardless of the slope.
Figure 3a shows a moderate correlation for mul-
tilingual results from the state-of-the-art system
(S-SA). Agreements on objective instances are
clustered together while agreements on subjective
instances are diffused over a wide region.
Agreements between the source language
corpus-based system (S-CB) and the corpus-based
system trained with translated resources (T-CB)
are more distinctively correlated than the results
for other pairs of systems (Figures 3b and 3d). We
notice that S-CB seems to have a lower number of
outliers than T-CB, but slightly more diffusive.
Lexicon-based systems (S-LB, T-LB) gener-
ate noticeably uncorrelated scores (Figures 3c and
3e). We observe that the results from the English
system with translated inputs (S-LB) is more cor-
related than those from systems with translated
lexicons (T-LB), and that analysis results from
both systems are biased toward subjective scores.
6 Discussion
Which approach is most suitable for multilingual
subjectivity analysis?
In our experiments, the corpus-based sys-
tems trained on corpora translated from English
to the target languages (T-CB) perform well
for subjectivity classification and multilanguage-
comparability measures on the whole. However,
the methods we employed to expand the languages
were naively carried out without much considera-
tions for optimization. Further adjustments could
improve the other systems for both classification
and multilanguage-comparability performances.
Is there a correlation between classification per-
formance and multilanguage-comparability?
Lexicon-based systems in the source language
(S-LB) have good overall classification perfor-
mances, especially on recall and F-measures.
However, these systems performs worse on
multilanguage-comparability than other systems
with poorer classification performances. Intrigued
by the observation, we tried to measure which
criteria for classification performance influences
multilanguage-comparability. We again employed
Pearson?s correlation metrics to measure the corre-
lations of precision (P), recall (R), and F-measures
(F) to kappa (?) and Pearson?s correlation (?) val-
ues.
Specifically, we measure the correlations be-
tween the sums of P, the sums of R, and the
sums of F to ? and ? for all pairs of systems.13
The correlations of P with ? and ? are 0.78
and 0.68, R ?0.38 and ?0.28, and F ?0.20
and ?0.05. These numbers strongly suggest that
multilanguage-comparability correlates with the
precisions of classifiers.
However, we cannot always expect a high-
precision multilingual subjectivity classifier to be
multilanguage-comparable as well. For example,
the S-SA system has a much higher precision
than S-LB consistently over all languages, but
their multilanguage-comparability performances
differed only by small amounts.
7 Conclusion
Multilanguage-comparability is an analysis sys-
tem?s ability to retain its decision criteria across
different languages. We implemented a number of
previously proposed approaches to learning mul-
tilingual subjectivity, and evaluated the systems
on multilanguage-comparability as well as clas-
sification performance. Our experimental results
provide meaningful comparisons of the multilin-
gual subjectivity analysis systems across various
aspects.
Also, we developed a multilingual subjectivity
evaluation corpus from a parallel text, and studied
inter-annotator, inter-language agreements on sub-
jectivity, and observed persistent subjectivity pro-
jections from one language to another from a par-
allel text.
For future work, we aim extend this work to
constructing a multilingual sentiment analysis sys-
tem and evaluate it with multilingual datasets
such as product reviews collected from different
countries. We also plan to resolve the lexicon-
based classifiers? classification bias towards sub-
jective meanings with a list of objective words
(Esuli and Sebastiani, 2006) and their multilin-
gual expansion (Kim et al, 2009), and evaluate
the multilanguage-comparability of systems con-
structed with resources from different sources.
13Pairs of values such as 71.1 + 70.7 and 0.41 for preci-
sions and Kappa of S-SA for English and Korean.
602
Acknowledgement
We thank the anonymous reviewers for valuable
comments and helpful suggestions. This work is
supported in part by Basic Science Research Pro-
gram through the National Research Foundation
of Korea (NRF) funded by the Ministry of Edu-
cation, Science and Technology (MEST) (2009-
0075211), and in part by the BK 21 project in
2010.
References
Ahmed Abbasi, Hsinchun Chen, and Arab Salem.
2008. Sentiment analysis in multiple languages:
Feature selection for opinion classification in web
forums. ACM Transactions on Information Systems,
26(3):1?34.
Carmen Banea, Rada Mihalcea, Janyce Wiebe, and
Samer Hassan. 2008. Multilingual subjectivity
analysis using machine translation. In EMNLP ?08:
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages 127?
135, Morristown, NJ, USA.
Mikhail Bautin, Lohit Vijayarenu, and Steven Skiena.
2008. International sentiment analysis for news and
blogs. In Proceedings of the International Confer-
ence on Weblogs and Social Media (ICWSM).
Erik Boiy and Marie-Francine Moens. 2009. A
machine learning approach to sentiment analysis
in multlingual Web texts. Information Retrieval,
12:526?558.
Julian Brooke, Milan Tofiloski, and Maite Taboada.
2009. Cross-linguistic sentiment analysis: From en-
glish to spanish. In Proceedings of RANLP 2009,
Borovets, Bulgaria.
Carmine Cesarano, Antonio Picariello, Diego Refor-
giato, and V.S. Subrahmanian. 2007. The oasys 2.0
opinion analysis system: A demo. In Proceedings of
the International Conference on Weblogs and Social
Media (ICWSM).
Andrea Esuli and Fabrizio Sebastiani. 2006. Senti-
wordnet: A publicly available lexical resource for
opinion mining. In Proceedings of the 5th Con-
ference on Language Resources and Evaluation
(LREC?06), pages 417?422, Geneva, IT.
Soo-Min Kim and Eduard Hovy. 2006. Identifying
and analyzing judgment opinions. In Proceedings
of the Human Language Technology Conference of
the NAACL (HLT/NAACL?06), pages 200?207, New
York, USA.
Jungi Kim, Hun-Young Jung, Sang-Hyob Nam, Yeha
Lee, and Jong-Hyeok Lee. 2009. Found in trans-
lation: Conveying subjectivity of a lexicon of one
language into another using a bilingual dictionary
and a link analysis algorithm. In ICCPOL ?09: Pro-
ceedings of the 22nd International Conference on
Computer Processing of Oriental Languages. Lan-
guage Technology for the Knowledge-based Econ-
omy, pages 112?121, Berlin, Heidelberg.
Rada Mihalcea, Carmen Banea, and Janyce Wiebe.
2007. Learning multilingual subjective language via
cross-lingual projections. In Proceedings of the 45th
Annual Meeting of the Association of Computational
Linguistics (ACL?07), pages 976?983, Prague, CZ.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment classification using
machine learning techniques. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 79?86.
Ellen Riloff and Janyce Wiebe. 2003. Learning ex-
traction patterns for subjective expressions. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing (EMNLP).
Xiaojun Wan. 2008. Using bilingual knowledge and
ensemble techniques for unsupervised Chinese sen-
timent analysis. In Proceedings of the 2008 Con-
ference on Empirical Methods in Natural Language
Processing, pages 553?561, Honolulu, Hawaii, Oc-
tober. Association for Computational Linguistics.
Xiaojun Wan. 2009. Co-training for cross-lingual sen-
timent classification. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Nat-
ural Language Processing of the AFNLP, pages
235?243, Suntec, Singapore, August. Association
for Computational Linguistics.
Janyce Wiebe and Ellen Riloff. 2005. Creating subjec-
tive and objective sentence classifiers from unanno-
tated texts. In Proceedings of the 6th International
Conference on Intelligent Text Processing and Com-
putational Linguistics (CICLing-2005), pages 486?
497, Mexico City, Mexico.
Janyce Wiebe, E. Breck, Christopher Buckley, Claire
Cardie, P. Davis, B. Fraser, Diane Litman, D. Pierce,
Ellen Riloff, Theresa Wilson, D. Day, and Mark
Maybury. 2003. Recognizing and organizing opin-
ions expressed in the world press. In Proceedings
of the 2003 AAAI Spring Symposium on New Direc-
tions in Question Answering.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the Con-
ference on Human Language Technology and Em-
pirical Methods in Natural Language Processing
(HLT-EMNLP?05), pages 347?354, Vancouver, CA.
603
