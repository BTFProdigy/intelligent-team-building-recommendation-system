Semantic Role Labeling Using Dependency Trees  
Kadri Hacioglu 
 Center for Spoken Language Research  
University of Colorado at Boulder 
hacioglu@cslr.colorado.edu
 
Abstract 
In this paper, a novel semantic role labeler based 
on dependency trees is developed. This is ac-
complished by formulating the semantic role la-
beling as a classification problem of dependency 
relations into one of several semantic roles. A 
dependency tree is created from a constituency 
parse of an input sentence. The dependency tree 
is then linearized into a sequence of dependency 
relations. A number of features are extracted for 
each dependency relation using a predefined lin-
guistic context. Finally, the features are input to a 
set of one-versus-all support vector machine 
(SVM) classifiers to determine the corresponding 
semantic role label. We report results on 
CoNLL2004 shared task data using the represen-
tation and scoring scheme adopted for that task. 
1   Introduction 
In semantic role labeling (SRL) the goal is to group 
sequences of words together and classify them by 
using semantic labels. For semantic representation 
we select the predicate-argument structure that ex-
ists in most languages. In this structure a word is 
specified as a predicate and a number of word 
groups are considered as arguments accompanying 
the predicate.  Those arguments are assigned differ-
ent semantic categories depending on the roles that 
they play with respect to the predicate.  
We illustrate the predicate-argument structure in 
Figure 1 for the sentence ?We are prepared to pur-
sue aggressively completion of this transaction he 
says? taken from the PropBank corpus. The chosen 
predicate is the word pursue, and its arguments with 
their associated word groups are illustrated. Note 
that the word prepared is another predicate of the 
sentence possibly with different argument labels 
attached to the same or different word groups. For 
example, the word we is A1 of prepared. This proc-
ess of selecting a predicate in a sentence, grouping 
sequences of words and assigning the semantic roles 
they play with respect to the chosen predicate is of-
ten referred to as semantic role labeling. We believe 
that a highly accurate extraction of this structure is 
vital for high performance in many NLP tasks such 
as information extraction, question answering, 
summarization and machine translation. 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1: Predicate-argument structure of sample 
sentence. Argument labels are in PropBank-style. 
 
Semantic role labeling based on predicate-
argument structure was first explored in detail by 
(Gildea and Jurafsky, 2002). Since then several 
variants of the basic approach have been introduced 
using different features and different classifiers 
based on various machine learning techniques 
(Gildea and Palmer, 2002; Gildea and Hockenmaier, 
2003; Surdeanu et. al., 2003;  Chen and Rambow, 
2003; Fleischman and Hovy, 2003; Hacioglu and 
Ward, 2003; Thompson et. al., 2003; Pradhan et. al., 
2003b; Hacioglu, 2004). Large semantically anno-
tated databases, like FrameNet (Baker et.al, 1998) 
and PropBank (Kingsbury and Palmer, 2002) have 
been used to train and test the classifiers. Most of 
those approaches can be divided into one of the fol-
lowing three broad classes with respect to the type 
of tokens classified; namely, constituent-by-
constituent (C-by-C), phrase-by-phrase (P-by-P) and 
word-by-word (W-by-W) semantic role labelers.  
In C-by-C semantic role labeling, the syntactic 
tree representation of a sentence is linearized into a 
sequence of its syntactic constituents (non-
terminals). Then each constituent is classified into 
one of several semantic roles using a number of fea-
tures derived from the sentence structure or a lin-
guistic context defined for the constituent token. In 
the P-by-P and W-by-W methods (Hacioglu, 2004; 
Hacioglu and Ward, 2003) the problem is formu-
lated as a chunking task and the features are derived 
for each base phrase and word, respectively. The 
tokens were classified into one of the semantic la-
bels using an IOB (inside-outside-begin) representa-
tion and a bank of SVM classifiers; a one-versus-all 
classifier has been used for each class. 
Predicate:pursue 
A0  A1 AM-MNR
  we  completion of this transaction  aggressively 
   
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 2. Example of a dependency tree augmented with semantic roles. Semantic labels correspond to the 
predicate posted. The same tree with different semantic labels also exists in the corpus for predicate abated. 
 
In this paper, we introduce another approach that 
we refer to as the relation-by-relation (R-by-R) se-
mantic role labeling. The method is based on de-
pendency trees generated from constituency trees. 
Although the system currently does not use more 
information than C-by-C systems, the information is 
structured in a different manner and, consequently, 
the nature of some linguistic features is quite differ-
ent. We point out that this information restructuring 
is very useful in localizing the semantic roles asso-
ciated with the selected predicate, since the depend-
ency trees directly encode the argument structure of 
lexical units populated at their nodes through de-
pendency relations. 
A related work is reported in (Gildea and Hock-
enmaier, 2003). However, they use Combinatory 
Categorical Grammar (CCG) to derive the depend-
ency relations. In addition, our method differs in the 
selection of dependency relations for labeling, in the 
creation of features and in the implementation of the 
classifier.  
Recently, there has been some interest in develop-
ing a deterministic machine-learning based approach 
for dependency parsing (Yamada and Matsumato, 
2003). In addition to relatively easier portability to 
other domains and languages the deterministic de-
pendency parsing promises algorithms that are ro-
bust and efficient. Therefore, an SRL algorithm 
based on dependency structures is expected to bene-
fit from those properties.  
2   Dependency Bank (DepBank) 
In this section, we describe the corpus that we auto-
matically created using the syntactic annotations of 
the Penn TreeBank with the semantic annotations of 
the PropBank. Hereafter, we refer to this new corpus 
as DepBank. 
Firstly, we convert constituency trees into de-
pendency trees1. The functional tags are removed 
from constituency trees before the conversion, since 
the current state-of-the-art syntactic parsers do not 
exploit those tags. Secondly, we trace the depend-
ency trees to determine the word sequences covered 
by the dependency relation nodes. Finally, we aug-
ment those nodes with their semantic role labels that 
cover the same sequence of words. The relations 
that do not align with any semantic role are tagged 
using the label  ?O?. In Figure 2, we illustrate a 
sample dependency tree from the DepBank. It corre-
sponds to the predicate posted of the following sen-
tence (semantic roles are also indicated):  
 
[A0 The dollar] [V posted]  [A1 gains] [AM-LOC 
in quiet training] [AM-ADV as concerns about equi-
ties abated] 
 
We note that the other predicate in the sentence is 
abated and the same tree with different semantic 
labels is also instantiated in the DepBank for it. The 
dependency relation nodes are indicated by ?R:? in 
Figure 2. The lexical nodes are indicated by ?W:?. 
The dependency relation types are paired with the 
corresponding semantic role labels. The only excep-
tion is the node that belongs to the predicate; the 
semantic label V is used with the lemma of the 
predicate. The lexical nodes include the word itself 
and its part-of-speech (POS) tag.   
3   Semantic Role Labeling of Relations 
In the proposed approach, we first linearize the de-
pendency tree in a bottom-up left-to-right manner 
into a sequence of dependency relations. During this 
                                                        
1 engconst2dep, from the University of Maryland, is used. Spe-
cial thanks to R. Hwa, A. Lopez and M. Diab.  
process we filter out the dependency relations that 
are less likely to be an argument. The selection 
mechanism is based on simple heuristics derived 
from dependency trees. Then we extract a set of fea-
tures for each dependency relation. Finally, we input 
the features to a bank of SVM classifiers. A one-
versus-all SVM classifier is used for each semantic 
role.   
3.1 Dependency Relation Selection 
In dependency tree representations, we observe that 
the semantic roles are highly localized with respect 
to the chosen predicate.  We exploit this observation 
to devise a method for deciding whether a depend-
ency relation is likely to be a semantic role or not. 
We define a tree-structured family of a predicate as a 
measure of locality. It is a set of dependency relation 
nodes that consists of the predicate?s parent, chil-
dren, grandchildren, siblings, siblings? children and 
siblings? grandchildren with respect to its depend-
ency tree. Any relation that does not belong to this 
set is skipped while we linearize the dependency 
tree in a bottom-up left-to-right manner. Further se-
lection is performed on the family members that are 
located at the leaves of the tree. For example, a leaf 
member with det dependency relation is not consid-
ered for semantic labeling. Our selection mechanism 
reduces the data for semantic role labeling by ap-
proximately 3-4 fold with nearly 1% miss of seman-
tic labels, since a quite large number of nodes in the 
dependency trees are not associated with any seman-
tic role.  
3.2  Features 
For each candidate dependency relation we extract a 
set of features. In the following, we explain these 
features and give examples for their values referring 
to the dependency tree shown in Figure 1 (feature 
values for the relation node R:mod with the seman-
tic label [A0] is given in parentheses). The features 
that are specific to the dependency relation (i.e. to-
ken-level features) are  
 
Type: This feature indicates the type of the de-
pendency relation  (mod) 
Family membership:  This feature indicates how 
the dependency relation is related to the predicate in 
the family (child) 
Position:  This feature indicates the position of 
the headword of the dependency relation with re-
spect to the predicate position in the sentence (be-
fore) 
Headword: the modified (head) word in the rela-
tion (posted).    
Dependent word: the modifying word in the re-
lation (dollar) 
POS tag of  headword: (VBD)   
POS tag of dependent word: (NN)  
Path: the chain of relations from   relation node 
to predicate. (mod? *)  
 
and the features that are specific to the  predicate 
(i.e. sentence-level features): 
 
POS pattern of predicate?s children: This fea-
ture indicates the left-to-right chain of the POS tags 
of the immediate words that depend on the predi-
cate. (NN-NNS-IN-IN) 
Relation pattern of predicate?s children: This 
feature indicates the left-to-right chain of the rela-
tion labels of the predicate?s children (mod-obj-p-
obj) 
POS pattern of predicate?s siblings: This fea-
ture indicates the left-to-right chain of the POS tags 
of the headwords of the siblings of predicate. (-) 
Relation pattern of predicate?s siblings: This 
feature indicates the left-to-right chain of the rela-
tion labels of the predicate?s siblings. (-).  
3.3  Classifier 
We selected support vector machines (Vapnik, 
1995) to implement the semantic role classifiers. 
The motivation for this selection was the ability of 
SVMs to handle an extremely large number of inter-
acting or overlapping features with quite strong gen-
eralization properties. Support vector machines for 
SRL were first used in (Hacioglu and Ward, 2003) 
as word-by-word (W-by-W) classifiers. The system 
was then applied to the constituent-by-constituent 
(C-by-C) classification in (Hacioglu et. al., 2003) 
and phrase-by-phrase (P-by-P) classification in (Ha-
cioglu, 2004). Several extensions of the basic sys-
tem with state-of-the-art performance were reported 
in (Pradhan et.al, 2003; Pradhan et. al. 2004; Ha-
cioglu et. al. 2004). All  SVM classifiers for seman-
tic argument labeling were  realized using the 
TinySVM with a polynomial kernel of degree 2 and 
the general purpose SVM based chunker YamCha2. 
4   Experiments    
Experiments were carried out using a part of the 
February 2004 release of the PropBank.  Sections 15 
through 18 were used for training, Section 20 was 
used for developing and Section 21 was used for 
testing. This is exactly the same data used for 
CoNLL2004 shared task on SRL. Therefore, the 
results can be directly compared to the performance 
of the systems that used or that will use the same 
data. The system performance is evaluated by using 
precision, recall and F metrics. In the experiments, 
                                                        
2 http://cl.aist-nara.ac.jp/~taku-ku/software/ 
the gold standard constituency parses were used. 
Therefore, the results provide an upper bound on the 
performance with automatic parses. Table 1 presents 
the results on the DepBank development set.  The 
results on the CoNLL2004 development set are also 
illustrated. After we project the predicted semantic 
role labels in the DepBank dev set onto the 
CoNLL2004 dev set (directly created from the 
PropBank) we observe a sharp drop in the recall per-
formance. The drop is due to the loss of approxi-
mately 8% of semantic roles in the DepBank dev set 
during the conversion process; not all phrase nodes 
in constituency trees find an equivalent relation 
node in dependency trees. However, this mismatch 
is significantly less than the 23% mismatch reported 
in (Gildea and Hockenmaier, 2003) between the 
CCGBank and an earlier version of the PropBank. 
  
Dev  Set Precision Recall F1 
DepBank 85.6% 83.6%  84.6 
CoNLL 84.9% 75.2% 79.8 
Table 1: Results on DepBank and CoNLL04 sets. 
5   Conclusions 
We have automatically created a new corpus of de-
pendency trees augmented with semantic role labels. 
Using this corpus, we have developed and experi-
mented with a novel SRL system that classifies de-
pendency relations. This is quite different from 
previous research on semantic role labeling. We 
have presented encouraging intermediate results. 
Currently, we are investigating the reasons of mis-
match between PropBank and DepBank semantic 
annotations. We also plan to add new features, ex-
periment with automatic parses, and compare and 
combine the system with our state-of-the-art C-by-C 
system. 
Acknowledgements 
This research was supported in part by the ARDA 
AQUAINT Program via contract OCG4423B and by 
the NSF grant ISS-9978025. 
References  
Collin F. Baker, Charles J. Fillmore, and John B. 
Lowe 1998. The Berkley FrameNet Project. In 
Proc. of  CoLING-ACL?98. 
John Chen and Owen Rambow. 2003. Use of Deep 
Linguistic Features for the Recognition and La-
beling of Semantic Arguments. In Proc. of 
EMNLP-2003 
Daniel Gildea and Daniel Jurafsky. 2002. Automatic 
Labeling of Semantic Roles. Computational Lin-
guistics, 28:3, pages 245-288. 
Daniel Gildea  and Martha Palmer. 2002. The Ne-
cessity of Syntactic Parsing for Predicate Argu-
ment Recognition. In Proc. of ACL?02. 
Daniel Gildea and Julia Hockenmaier. 2003. Identi-
fying Semantic Roles Using Combinatory Cate-
gorical Grammar. In Proc. of EMNL?03, Japan. 
Micheal Fleischman and Eduard Hovy. 2003. A 
Maximum Entropy Approach to FrameNet Tag-
ging. In Proc. of  HLT/NAACL-03. 
Kadri Hacioglu and Wayne Ward. 2003. Target 
word detection and semantic role chunking using 
support vector machines. In  Proc.  of 
HLT/NAACL-03. 
Kadri Hacioglu, Sameer Pradhan, Wayne Ward, 
James H. Martin and Daniel Jurafsky. 2003. Shal-
low Semantic Parsing using Support Vector Ma-
chines. CSLR Technical Report, CSLR-TR-2003-1.  
Kadri Hacioglu. 2004. A Semantic Chunking Model 
Based on Tagging.  In Proc. of  HLT/NAACL-04. 
Kadri Hacioglu, Sameer Pradhan, Wayne Ward, 
James H. Martin and Daniel Jurafsky. 2004. Se-
mantic Role Labeling by Tagging Syntactic 
Chunks. CONLL-2004 Shared Task. 
Paul Kingsbury, Martha Palmer, 2002. From Tree-
Bank to PropBank. In Proc. of  LREC-2002. 
Sameer Pradhan, Kadri Hacioglu, Wayne Ward, 
James H. Martin, Dan Jurafsky. 2003. Semantic 
Role Parsing: Adding Semantic Structure to Un-
structured Text. In Proc. of ICDM 2003. 
Sameer Pradhan, Kadri Hacioglu, Wayne Ward, 
James H. Martin, Dan Jurafsky. 2004. Support 
Vector Learning for Semantic Argument Classifi-
cation. To appear in Journal of Machine Learn-
ing. 
Mihai Surdeanu, Sanda Harabagiu, John Williams, 
and Paul Aarseth. 2003. Using Predicate-
Argument Structure for  Information Extraction. 
In Proc. of ACL03. 
Cynthia A. Thompson, Roger Levy, and Christopher 
D. Manning. 2003. A Generative Model for Se-
mantic Role Labeling.  In Proc.of  ECML-03. 
Vladamir Vapnik 1995. The Nature of Statistical 
Learning Theory. Springer Verlag, New York, 
USA. 
Hiroyasu Yamada and Yuji Matsumoto. 2003. Sta-
tistical Dependency Analysis with Support Vector 
Machines.  In Proc. of  IWPT?03. 
On Combining Language Models :
Oracle Approach
Kadri Hacioglu and Wayne Ward
Center for Spoken Language Research
University of Colorado at Boulder
fhacioglu,whwg@cslr.colorado.edu
ABSTRACT
In this paper, we address the problem of combining several lan-
guage models (LMs). We find that simple interpolation methods,
like log-linear and linear interpolation, improve the performance
but fall short of the performance of an oracle. The oracle knows the
reference word string and selects the word string with the best per-
formance (typically, word or semantic error rate) from a list of word
strings, where each word string has been obtained by using a dif-
ferent LM. Actually, the oracle acts like a dynamic combiner with
hard decisions using the reference. We provide experimental results
that clearly show the need for a dynamic language model combina-
tion to improve the performance further. We suggest a method that
mimics the behavior of the oracle using a neural network or a de-
cision tree. The method amounts to tagging LMs with confidence
measures and picking the best hypothesis corresponding to the LM
with the best confidence.
1. INTRODUCTION
Statistical language models (LMs) are essential in speech recog-
nition and understanding systems for high word and semantic ac-
curacy, not to mention robustness and portability. Several language
models have been proposed and studied during the past two decades
[8]. Although it has turned out to be a rather difficult task to beat
the (almost) standard class/word n-grams (typically n = 2 or 3),
there has been a great deal of interest in grammar based language
models [1]. A promising approach for limited domain applications
is the use of semantically motivated phrase level stochastic context
free grammars (SCFGs) to parse a sentence into a sequence of se-
mantic tags which are further modeled using n-grams [2, 9, 10, 3].
The main motivation behind the grammar based LMs is the inabil-
ity of n-grams to model longer-distance constraints in a language.
With the advent of fairly fast computers and efficient parsing and
search schemes several researchers have focused on incorporating
relatively complex language models into speech recognition and
understanding systems at different levels. For example, in [3], we
The work is supported by DARPA through SPAWAR under grant
#N66001-00-2-8906.
report a significant perplexity improvement with a moderate in-
crease in word/semantic accuracy, at N -best list (rescoring) level,
using a dialog-context dependent, semantically motivated grammar
based language model.
Statistical language modeling is a ?learning from data? problem.
The generic steps to be followed for language modeling are
 preparation of training data
 selection of a model type
 specification of the model structure
 estimation of model parameters
The training data should consist of large amounts of text, which
is hardly satisfied in new applications. In those cases, complex
models fit to the training data. On the other hand, simple models
can not capture the actual structure. In the Bayes? (sequence) de-
cision framework of speech recognition/understanding we heavily
constrain the model structure to come up with a tractable and prac-
tical LM. For instance, in a class/word n-gram LM the dependency
of a word is often restricted to the class that it belongs and the de-
pendency of a class is limited to n-1 previous classes. The estima-
tion of the model parameters, which are commonly the probabili-
ties, is another important issue in language modeling. Besides data
sparseness, the estimation algorithms (e.g. EM algorithm) might be
responsible for the estimated probabilities to be far from optimal.
The aforementioned problems of learning have different effects
on different LM types. Therefore, it is wise to design LMs based on
different paradigms and combine them in some optimal sense. The
simplest combination method is the so called linear interpolation
[4]. Recently, the linear interpolation in the logarithmic domain
has been investigated in [6]. Perplexity results on a couple of tasks
have shown that the log-linear interpolation is better than the linear
interpolation. Theoretically, a far more powerful method for LM
combination is the maximum entropy approach [7]. However, it
has not been widely used in practice, since it is computationally
demanding.
In this research, we consider two LMs:
 class-based 3-gram LM (baseline).
 dialog dependent semantic grammar based 3-gram LM [3].
After N-best list rescoring experiments with linear and log-linear
interpolation, we realized that the performance in terms of word
and semantic accuracies fall considerably short of the performance
of an oracle. We explain the set-up for the oracle experiment and
point out that the oracle is a dynamic LM combiner. To fill the
performance gap, we suggest a method that can mimic the oracle.
concept 
         G S
C
W
dialog goal
generator
generator
sequence
word
speech
generator
waveform
phoneme
sequence
generator
P
A
dialog context
Figure 1: A speech production model
The paper is organized as follows. Section 2 presents the lan-
guage models considered in this study. In Section 3, we briefly
explain combining of LMs using linear and log-linear interpola-
tion. Section 4 explains the set up for the oracle experiment. Ex-
perimental results are reported in Section 5. The future work and
conclusions are given in the last section.
2. LANGUAGE MODELS
In language modeling, the goal is to find the probability distribu-
tion of word sequences, i.e. P (W ), where W = w
1
; w
2
:    ; w
L
.
We first describe a model for sentence generation in a dialog [5]
on which our grammar LM is based. The model is illustrated in
Figure 1. Here, the user has a specific goal that does not change
throughout the dialog. According to the goal and the dialog con-
text the user first picks a set of concepts with respective values and
then use phrase generators associated with concepts to generate the
word sequence. The word sequence is next mapped into a sequence
of phones and converted into a speech signal by the user?s vocal ap-
paratus which we finally observe as a sequence of acoustic feature
vectors.
Assuming that
 the dialog context S is given,
 W is independent of S but the concept sequence C, i.e.
P (W=C; S) = P (W=C),
 (W,C) pair is unique (possible with either Viterbi approxima-
tion or unambigious association between C and W),
one can easily show that P (W ) is given by
P (W ) = P (W=C)P (C=S) (1)
In (1) we identify two models:
 Concept model: P (C=S)
 Syntactic model : P (W=C)
<s> I WANT TO FLY FROM MIAMI FLORIDA TO SYDNEY AUS-
TRALIA ON OCTOBER FIFTH </s>
<s> [i want] [depart loc] [arrive loc] [date] </s>
<s> I DON?T TO FLY FROM MIAMI FLORIDA TO SYDNEY
AFTER AREA ON OCTOBER FIFTH </s>
<s> [Pronoun] [Contraction] [depart loc] [arrive loc] [after] [Noun] [date]
</s>
Figure 2: Examples of parsing into concepts and filler classes
The concept model is conditioned on the dialog context. Al-
though there are several ways to define a dialog context, we select
the last question prompted by the system as the dialog context. It is
simple and yet strongly predictive and constraining.
The concepts are classes of phrases with the same meaning. Put
differently, a concept class is a set of all phrases that may be used
to express that concept (e.g. [i want], [arrive loc]). Those concept
classes are augmented with single word, multiple word and a small
number of broad (and unambigious) part of speech (POS) classes.
In cases where the parser fails, we break the phrase into a sequence
of words and tag them using this set of ?filler? classes. Two exam-
ples in Figure 2 clearly illustrate the scheme.
The structure of the concept sequences is captured by an n-gram
LM. We train a seperate language model for each dialog context.
Given the context S and C = c
0
c
1
   c
K
; c
K+1
, the concept se-
quence probabilities are calculated as (for n = 3)
P (C=S) = P (c
1
= < s >; S)P (c
2
= < s >; c
1
; S)
K+1
Y
k=3
P (c
k
=c
k 2
; c
k 1
; S)
where c
0
and c
K+1
are for the sentence-begin and sentence-end
symbols, respectively.
Each concept class is written as a CFG and compiled into a
stochastic recursive transition network (SRTN). The production rules
define complete paths beginning from the start-node through the
end-node in these nets. The probability of a complete path tra-
versed through one or more SRTNs initiated by the top-level SRTN
associated with the concept is the probability of the phrase given
that concept. This probability is calculated as the multiplication of
all arc probabilities that defines the path. That is,
P (W=C) =
Q
K
i=1
P (s
i
=c
i
)
=
Q
K
i=1
Q
M
i
j=1
P (r
j
=c
i
)
where s
i
is a substring in W = w
1
; w
2
::w
L
= s
1
; ::s
2
; s
K
(K 
L) and r
1
; r
2
; :::r
M
i
are the production rules that construct s
i
. The
concept and rule sequences are assumed to be unique in the above
equations. The parser uses heuristics to comply with this assump-
tion.
SCFG and n-gram probabilities are learned from a text corpus
by simple counting and smoothing. Our semantic grammars have a
low degree of ambiguity and therefore do not require computation-
ally intensive stochastic training and parsing techniques.
The class based LM can be considered as a very special case
of our grammar based model. Concepts (or classes) are restricted
to those that represent a list of semantically similar words, like
[city name] , [day of week], [month day] and so forth. So, instead
of rule probabilities we have given the class the word probabilities,
P (w
i
=c
j
). For simplicity, each word belongs to at most one class.
reference
N-best list
grammar based
LM
class based
LM
dialog context
f   f   g  c  I
S
W
W
g  
c  
best Woracle
training data
Figure 3: The set up for oracle experiments
3. LINEAR AND LOG-LINEAR INTERPO-
LATION
Assuming that we have M language models, P
i
(W ); i = 1; 2;    ;M ,
the combined LM obtained using the linear interpolation (at sen-
tence level) is given by
P (W ) =
M
X
i=1

i
P
i
(W ) (2)
where 
i
are positive interpolation weights that sum up to unity.
The log-linear interpolation suggests an LM, again at sentence
level, given by
P (W ) =
1
Z()
M
Y
i=1
P
i
(W )

i (3)
where Z() is the normalization factor and it is a function of the
interpolation weights. The linearity in logarithmic domain is obvi-
ous if we take the logarithm of both sides. In the sequel, we omit
the normalization term, as its computation is very expensive. We
hope that its impact on the performance is not significant. Yet, it
prevents us from reporting perplexity results.
4. THE ORACLE APPROACH
The set-up for oracle experiments is illustrated in Figure 3. The
purpose of this set-up is twofold. First, we use it to evaluate the or-
acle performance. Second, we use it to prepare data for the training
of a stochastic decision model. For the sake of simplicity, we show
the set-up for two LMs and do experiments accordingly. Nonethe-
less, the set-up can be extended to an arbitrary number of LMs.
The language models are used for N-best list rescoring. The
N-best list is generated by a speech recognizer using a relatively
simpler LM (here, a class-based trigram LM) . The framework for
N-best list rescoring is the following MAP decision:
W

= argmax p
A
P (W=C
W
)P (C
W
=S) (4)
W 2 L
N
where p
A
is the acoustic probability from the first pass, C
W
is the
unique concept sequence associated with W , and L
N
denotes the
N-best list. Each rescoring module supplies the oracle with their
N-best list
grammar based
LM
class based
LM
dialog context
S
best W
I
Ig  
c  
W c  
W g  
f   c  
f   g  select  maxneural network
Figure 4: The LM combining system based on the oracle ap-
proach.
best hypothesis after rescoring. The oracle compares each hypoth-
esis to the reference and pick the one with the best word (or seman-
tic) accuracy.
For training purposes, we create the input feature vector by aug-
menting features from each rescoring module (f
g
; f
c
) and the dia-
log context (S). The output vector is the LM indicator I from the
oracle. The element that corresponds to the LM with the best final
hypothesis is unity and the rest are zeros. After training the oracle
combiner (here, we assume a neural network), we set our system
as shown in Figure 4. The input to the neural network (NN) is the
augmented feature vector. The output of the NN is the LM indica-
tor probably with fuzzy values. So, we first pick the max output,
and then, we select and output the respective word string.
5. EXPERIMENTAL RESULTS
The models were developed and tested in the context of the CU
Communicator dialog system which is used for telephone-based
flight, hotel and rental car reservations [11]. The text corpus was
divided into two parts as training and test sets with 15220 and 1220
sentences, respectively. The test set was further divided into two
parts. Each part, in turn, was used to optimize language and in-
terpolation weights to be used for the other part in a ?jacknife
paradigm?. The results were reported as the average of the two
results. The average sentence length of the corpus was 4 words
(end-of-sentence was treated as a word). We identified 20 dialog
contexts and labeled each sentence with the associated dialog con-
text.
We trained a dialog independent (DI) class based LM and dia-
log dependent (DD) grammar based LM. In all LMs n is set to 3.
It must be noted that the DI class-based LM served as the LM of
the baseline system with 921 unigrams including 19 classes. The
total number of the distinct words in the lexicon was 1681. The
grammar-based LM had 199 concept and filler classes that com-
pletely cover the lexicon. In rescoring experiments we set the N-
best list size to 10. We think that the choice of N = 10 is a reson-
able tradeoff between performance and complexity.
The perplexity results are presented in Table 1. The perplexity
of the grammar-based LM is 36.8% better than the baseline class-
based LM.
We did experiments using 10-best lists from the baseline recog-
nizer. We first determined the best possible performance in WER
Table 1: Perplexity results
LM Perplexity
DI class 3-gram 22.0
DD SCFG 3-gram 13.9
offered by 10-best lists. This is done by picking the hypothesis
with the lowest WER from each list. This gives an upperbound for
the performance gain possible from rescoring 10-best lists . The
rescoring results in terms of absolute and relative improvements in
WER and semantic error rate (SER) along with the best possible
improvement are reported in Table 2. It should be noted that the
optimizations are made using WER. The slight drop in SER with
interpolation might be due to that. Actually this is good for text
transcription but not for a dialog system. We believe that the re-
sults will reverse if we replace the optimization using WER with
the optimization using SER.
Table 2: The WER and SER results of the 10-best list rescoring
with different LMs: the baseline WER is 25.9% and SER is
23.7%
Method WER SER
Class based LM alone 0.0% 0.0%
Grammar based LM alone 1.4(5.4)% 1.4(5.9)%
Linear interpolation 1.6(6.2)% 1.3(5.5)%
Log-linear interpolation 1.7(6.6)% 1.2(5.1)%
Oracle 3.0(11.6)% 2.7(11.4) %
Best 6.4(24.1)% 5.5(23.2)%
The performance gap between the oracle and interpolation meth-
ods promotes the system in Figure 4. We expect that, based on the
universal approximation theory, a neural network with consistent
features, sufficiently large training data and proper training would
approximate fairly well the behavior of the oracle. On the other
hand, the performance gap between the oracle and the best possi-
ble performance from 10-best lists suggests the use of more than
two language models and dynamic combination with the acoustic
model.
6. CONCLUSIONS
We have presented our recent work on language model combin-
ing. We have shown that although a simple interpolation of LMs
improves the performance, it fails to reach the performance of an
oracle. We have proposed a method for LM combination that mim-
ics the behavior of the oracle. Although our work is not complete
without a neural network that mimics the oracle, we argue that
the universal approximation theory ensures the success of such a
method. However, extensive experiments are required to reach the
goal with the main focus on the selection of features. At the mo-
ment, the number of concepts, the number of filler classes and the
number of 3-gram hits in a sentence (all normalized by the length
of the sentence) and the behavior of n-grams in a context are the
features that we consider to use. Also, it has been observed that the
performance of the oracle is still far from the best possible perfor-
mance. This is partly due to the very small number of LMs used
in the rescoring, partly due to the oracle?s hard decision combining
strategy and partly due to the static combination with the acous-
tic model. The work is in progress towards the goal of filling the
performance gap.
7. REFERENCES
[1] J. K. Baker. Trainable grammars for speech recognition. In
Speech Communications for th 97th Meeting of the
Acoustical Society of America, pages 31?35, June 1979.
[2] J. Gillett and W. Ward. A language model combining
trigrams and stochastic context-free grammars. In 5-th
International Conference on Spoken Language Processing,
pages 2319?2322, Sydney, Australia, 1998.
[3] K. Hacioglu and W. Ward. Dialog-context dependent
language models combining n-grams and stochastic
context-free grammars. In submitted toInternational
Conference of Acoustics, Speech, and Signal Processing,
Salt-Lake, Utah,, 2001.
[4] F. Jelinek and R. Mercer. Interpolated estimation of markov
source parameters from sparse data. Pattern Recognition in
Practice, 23:381, 1980.
[5] A. Keller, B. Rueber, F. Seide, and B. Tran. PADIS - an
automatic telephone switchboard and directory information
system. Speech Communication, 23:95?111, 1997.
[6] D. Klakow. Log-linear interpolation of language models. In
5-th International Conference on Spoken Language
Processing, pages 1695?1699, Sydney, Australia, 1998.
[7] R. Rosenfeld. A maximum entropy approach to adaptive
language modeling. Computer Speech and Language,
(10):187?228, 1996.
[8] R. Rosenfeld. Two decades of statistical language modeling:
Where do we go from here? Proceedings of the IEEE,
88(8):1270?1278, August 2000.
[9] B. Souvignier, A. Keller, B. Rueber, H.Schramm, and
F. Seide. The thoughtful elephant: Strategies for spoken
dialog systems. IEEE Transactions on Speech and Audio
Processing, 8(1):51?62, January 2000.
[10] Y. Wang, M. Mahajan, and X.Huang. A unified context-free
grammar and n-gram model for spoken language processing.
In International Conference of Acoustics, Speech, and Signal
Processing, pages 1639?1642, Istanbul, Turkey, 2000.
[11] W. Ward and B. Pellom. The CU communicator system. In
IEEE Workshop on Automatic Speech Recognition and
Understanding, Keystone, Colorado, 1999.
University of Colorado Dialog Systems for 
Travel and Navigation 
B. Pellom, W. Ward, J. Hansen, R. Cole, K. Hacioglu, J. Zhang, X. Yu, S. Pradhan 
Center for Spoken Language Research, University of Colorado 
Boulder, Colorado 80303, USA 
{pellom, whw, jhlh, cole, hacioglu, zjp, xiu, spradhan}@cslr.colorado.edu 
 
ABSTRACT 
This paper presents recent improvements in the development of 
the University of Colorado ?CU Communicator? and ?CU-
Move? spoken dialog systems. First, we describe the CU 
Communicator system that integrates speech recognition, 
synthesis and natural language understanding technologies using 
the DARPA Hub Architecture. Users are able to converse with an 
automated travel agent over the phone to retrieve up-to-date 
travel information such as flight schedules, pricing, along with 
hotel and rental car availability.  The CU Communicator has 
been under development since April of 1999 and represents our 
test-bed system for developing robust human-computer 
interactions where reusability and dialogue system portability 
serve as two main goals of our work.  Next, we describe our more 
recent work on the CU Move dialog system for in-vehicle route 
planning and guidance.  This work is in joint collaboration with 
HRL and is sponsored as part of the DARPA Communicator 
program.  Specifically, we will provide an overview of the task, 
describe the data collection environment for in-vehicle systems 
development, and describe our initial dialog system constructed 
for route planning. 
1. CU COMMUNICATOR 
1.1 Overview  
The Travel Planning Task 
The CU Communicator system [1,2] is a Hub compliant 
implementation of the DARPA Communicator task [3].  The 
system combines continuous speech recognition, natural 
language understanding and flexible dialogue control to enable 
natural conversational interaction by telephone callers to access 
information from the Internet pertaining to airline flights, hotels 
and rental cars.  Specifically, users can describe a desired airline 
flight itinerary to the Communicator and use natural dialog to 
negotiate a flight plan.  Users can also inquire about hotel 
availability and pricing as well as obtain rental car reservation 
information.   
System Overview 
The dialog system is composed of a Hub and several servers as 
shown in Fig. 1.  The Hub is used as a centralized message router 
through which servers can communicate with one another [4].  
Frames containing keys and values are emitted by each server, 
routed by the hub, and received by a secondary server based on 
rules defined in a ?Hub script?.   
 
 
 
 
 
 
 
 
 
Figure 1.  Block diagram of the functional components that 
comprise the CU Communicator system1. 
1.2 Audio Server 
The audio server is responsible for answering the incoming call, 
playing prompts and recording user input.  Currently, our system 
uses the MIT/MITRE audio server that was provided to DARPA 
Communicator program participants.  The telephony hardware 
consists of an external serial modem device that connects to the 
microphone input and speaker output terminals on the host 
computer.  The record process is pipelined to the speech 
recognition server and the play process is pipelined the text-to-
speech server.  This audio server does not support barge-in. 
Recently we have developed a new audio server that supports 
barge-in using the Dialogic hardware platform.  The new audio 
server implements a Fast Normalized Least-Mean-Square (LMS) 
algorithm for software-based echo cancellation.  During 
operation, the echo from the system speech is actively cancelled 
from the recorded audio to allow the user to cut through while 
                                                          
1
 This work was supported by DARPA through SPAWAR under 
Grant No. N66001-002-8906.  The ?CU Move? system is 
supported in part through a joint collaboration with HRL 
Laboratories. 
 
 
 
Language
Generator
Language
enerator
Hub
Speech 
Recognizer
Speech 
Recognizer
Speech 
Synthesizer
Speech 
Synthesizer
Semantic
Parser
Se antic
Parser
Dialogue
Manager
Dialogue
anager
Data Base / 
Backend
Data Base / 
Backend
Confidence
Server
Confidence
Server
Audio ServerAudio Server
www
the system is speaking.  The new audio server operates in the 
Linux environment and is currently being field-tested at CSLR.  
Because the server implements software-based echo cancellation, 
it can work on virtually any low-cost Dialogic hardware 
platform.  This server will be made available to the research 
community as a resource in the near future. 
1.3 Speech Recognizer 
We are currently using the Carnegie Mellon University Sphinx-II 
system [5] in our speech recognition server. This is a semi-
continuous Hidden Markov Model recognizer with a class 
trigram language model. The recognition server receives the 
input vectors from the audio server. The recognition server 
produces a word lattice from which a single best hypothesis is 
picked and sent to the hub for processing by the dialog manager. 
Acoustic Modeling 
During dialog interaction with the user, the audio server sends 
the acoustic samples to three Sphinx-II speech recognizers.  
While the language model is the same for each decoder, the 
acoustic models consist of (i) speaker independent analog 
telephone, (ii) female adapted analog telephone, and (iii) cellular 
telephone adapted acoustic model sets.   Each decoder outputs a 
word string hypothesis along with a word-sequence probability 
for the best path.  An intermediate server is used to examine each 
hypothesis and pass the most likely word string onto the natural 
language understanding module.   
Language Modeling 
The Communicator system is designed for end users to get up-to-
date worldwide air travel, hotel and rental car information via the 
telephone. In the task there are word lists for countries, cities, 
states, airlines, etc.  To train a robust language model, names are 
clustered into different classes. An utterance with class tagging is 
shown in Fig.2.  In this example, city, hour_number, and am_pm 
are class names. 
Figure 2.  Examples of class-based and grammar-based 
language modeling  
Each commonly used word takes one class. The probability of 
word Wi given class Ci is estimated from training corpora. After 
the corpora are correctly tagged, a back-off class-based trigram 
language model can be computed from the tagged corpora.  We 
use the CMU-Cambridge Statistical Language Modeling Toolkit 
to compute our language models. 
More recently, we have developed a dialog context dependent 
language model (LM) combining stochastic context free 
grammars (SCFGs) and n-grams [6,7].  Based on a spoken 
language production model in which a user picks a set of 
concepts with respective values and constructs word sequences 
using phrase generators associated with each concept in 
accordance with the dialog context, this LM computes the 
probability of a word, P(W), as 
 
         P(W) = P(W/C) P(C/S)          (1) 
 
where W is the sequence of words, C is the sequence of concepts 
and S is the dialog context. Here, the assumptions are (i) S is 
given, (ii) W is independent of S but C, and (iii) W and C 
associations are unambiguous. This formulation can be 
considered as a general extension of the standard class word 
based statistical language model as seen in Fig. 2. 
 
The first term in (1) is modeled by SCFGs, one for each concept. 
The concepts are classes of phrases with the same meaning. Each 
SCFG is compiled into a stochastic recursive transition network 
(STRN). Our grammar is a semantic grammar since the 
nonterminals correspond to semantic concepts instead of 
syntactic constituents. The set of task specific concepts is 
augmented with a single word, multiple word and a small number 
of broad but unambigious part of speech (POS) classes to 
account for the phrases that are not covered by the grammar. 
These classes are considered as "filler" concepts within a unified 
framework. The second term in (1) is modeled as a pool of 
concept n-gram LMs. That is, we have a separate LM for each 
dialog context. At the moment, the dialog context is selected as 
the last question prompted by the system, as it is very simple and 
yet strongly predictive and constraining. SCFG and n-gram 
probabilities are learned by simple counting and smoothing. Our 
semantic grammars have a low degree of ambiguity and therefore 
do not require computationally intensive stochastic training and 
parsing techniques. 
 
Experimental results with N-best list rescoring were found 
promising (5-6% relative improvement in WER).  In addition, we 
have shown that a dynamic combining of our new LM and the 
standard class word n-gram (the LM currently in use in our 
system) should result in further improvements. At the present, we 
are interfacing the grammar LM to the speech recognizer using a 
word graph. 
1.4 Confidence Server 
Our prior work on confidence assessment has considered 
detection and rejection of word-level speech recognition errors 
and out-of-domain phrases using language model features [8].  
More recently [9], we have considered detection and rejection of 
misrecognized units at the concept level.  Because concepts are 
used to update the state of the dialog system, we believe that 
concept level confidence is vitally important to ensuring a 
graceful human-computer interaction.  Our current work on 
concept error detection has considered language model features 
(e.g., LM back-off behavior, language model score) as well as 
acoustic features from the speech recognizer (e.g., normalized 
acoustic score, lattice density, phone perplexity).  Confidence 
Original Utterance 
I want to go from Boston to Portland around nine a_m 
Class-Tagged Utterance 
I want to go from [city:Boston] to [city:Portland] 
around [hour_number:nine] [am_pm:a_m] 
Concept-Tagged Utterance 
[I_want: I want to go] [depart_loc: from Boston] 
[arrive_loc: to Portland] [time:around nine a_m] 
features are combined to compute word-level, concept-level, and 
utterance-level confidence scores.  
1.5 Language Understanding 
We use a modified version of the Phoenix [10] parser to map the 
speech recognizer output onto a sequence of semantic frames. A 
Phoenix frame is a named set of slots, where the slots represent 
related pieces of information. Each slot has an associated 
context-free semantic grammar that specifies word string patterns 
that can fill the slot. The grammars are compiled into Recursive 
Transition Networks, which are matched against the recognizer 
output to fill slots. Each filled slot contains a semantic parse tree 
with the slot name as root.  
Phoenix has been modified to also produce an extracted 
representation of the parse that maps directly onto the task 
concept structures. For example, the utterance  
?I want to go from Boston to Denver Tuesday morning?  
would produce the extracted parse: 
Flight_Constraint: Depart_Location.City.Boston 
Flight_Constraint: Arrive_Location.City.Denver 
Flight Constraints:[Date_Time].[Date].[Day_Name].tuesday 
                             [Time_Range].[Period_Of_Day].morning 
1.6 Dialog Management 
The Dialogue Manager controls the system?s interaction with the 
user and the application server. It is responsible for deciding 
what action the system will take at each step in the interaction. 
The Dialogue Manager has several functions. It resolves 
ambiguities in the current interpretation; Estimates confidence in 
the extracted information; Clarifies the interpretation with the 
user if required; Integrates new input with the dialogue context; 
Builds database queries (SQL); Sends information to NL 
generation for presentation to user; and prompts the user for 
missing information. 
We have developed a flexible, event driven dialogue manager in 
which the current context of the system is used to decide what to 
do next. The system does not use a dialogue network or a 
dialogue script, rather a general engine operates on the semantic 
representations and the current context to control the interaction 
flow.  The Dialogue Manager receives the extracted parse. It then 
integrates the parse into the current context. Context consists of a 
set of frames and a set of global variables. As new extracted 
information arrives, it is put into the context frames and 
sometimes used to set global variables. The system provides a 
general-purpose library of routines for manipulating frames. 
This ?event driven? architecture functions similar to a production 
system. An incoming parse causes a set of actions to fire which 
modify the current context. After the parse has been integrated 
into the current context, the DM examines the context to decide 
what action to take next. The DM attempts the following actions 
in the order listed: 
? Clarify if necessary  
? Sign off if all done  
? Retrieve data and present to user  
? Prompt user for required information  
The rules for deciding what to prompt for next are very 
straightforward. The frame in focus is set to be the frame 
produced in response to the user, or to the last system prompt.  
? If there are unfilled required slots in the focus frame, then 
prompt for the highest priority unfilled slot in the frame. 
? If there are no unfilled required slots in the focus frame, 
then prompt for the highest priority missing piece of 
information in the context.  
Our mechanism does not have separate ?user initiative? and 
?system initiative? modes. If the system has enough information 
to act on, then it does it. If it needs information, then it asks for 
it. The system does not require that the user respond to the 
prompt. The user can respond with anything and the system will 
parse the utterance and set the focus to the resulting frame. This 
allows the user to drive the dialog, but doesn?t require it. The 
system prompts are organized locally, at the frame level. The 
dialog manager or user puts a frame in focus, and the system tries 
to fill it. This representation is easy to author, there is no separate 
dialog control specification required. It is also robust in that it 
has a simple control that has no state to lose track of. 
An additional benefit of Dialog Manager mechanism is that it is 
very largely declarative. Most of the work done by a developer 
will be the creation of frames, forms and grammars. The system 
developer creates a task file that specifies the system ontology 
and templates for communicating about nodes in the hierarchy. 
The templates are filled in from the values in the frames to 
generate output in the desired language. This is the way we 
currently generate SQL queries and user prompts. An example 
task frame specification is: 
Frame:Air 
 [Depart_Loc]+ 
    Prompt: "where are you departing from" 
    [City_Name]* 
 Confirm: "You are departing from $([City_Name]).  
    Is that correct?" 
 Sql: "dep_$[leg_num] in (select airport_code from 
 airport_codes where city like '!%' $(and state_province 
like '[Depart_Loc].[State]' ) )" 
    [Airport_Code]* 
 
This example defines a frame with name Air and slot 
[Depart_Loc]. The child nodes of Depart_Loc are are 
[City_Name] and [Airport_Code]. The ?+? after [Depart_Loc] 
indicates that it is a mandatory field. The Prompt string is the 
template for prompting for the node information. The ?*? after 
[City_Name] and [Airport_Code] indicate that if either of them is 
filled, the parent node [Depart_Loc] is filled. The Confirm string 
is a template to prompt the user to confirm the values. The SQL 
string is the template to use the value in an SQL query to the 
database. 
The system will prompt for all mandatory nodes that have 
prompts. Users may specify information in any order, but the 
system will prompt for whatever information is missing until the 
frame is complete.   
1.7 Database & Internet Interface  
The back-end interface consists of an SQL database and domain-
specific Perl scripts for accessing information from the Internet.  
During operation, database requests are transmitted by the Dialog 
Manager to the database server via a formatted frame. 
The back-end consists of a static and dynamic information 
component.  Static tables contain data such as conversions 
between 3-letter airport codes and the city, state, and country of 
the airport (e.g., BOS for Boston Massachusetts).  There are over 
8000 airports in our database, 200 hotel chains, and 50 car rental 
companies.  The dynamic information content consists of 
database tables for car, hotel, and airline flights.   
When a database request is received, the Dialog Manager?s SQL 
command is used to select records in local memory.  If no 
records are found to match, the back-end can submit an HTTP-
based request for the information via the Internet.  Records 
returned from the Internet are then inserted as rows into the local 
SQL database and the SQL statement is once again applied.   
1.8 Language Generation 
The language generation module uses templates to generate text 
based on dialog speech acts.  Example dialog acts include 
?prompt? for prompting the user for needed information, 
?summarize? for summarization of flights, hotels, and rental cars, 
and ?clarify? for clarifying information such as departure and 
arrival cities that share the same name. 
1.9 Text-to-Speech Synthesis 
For audio output, we have developed a domain-dependent 
concatenative speech synthesizer.  Our concatenative synthesizer 
can adjoin units ranging from phonemes, to words, to phrases 
and sentences.   For domain modeling, we use a voice talent to 
record entire task-dependent utterances  (e.g., ?What are your 
travel plans??) as well as short phrases with carefully determined 
break points (e.g., ?United flight?, ?ten?, ?thirty two?, ?departs 
Anchorage at?).    Each utterance is orthographically transcribed 
and phonetically aligned using a HMM-based recognizer.   Our 
research efforts for data collection are currently focused on 
methods for reducing the audible distortion at segment 
boundaries, optimization schemes for prompt generation, as well 
as tools for rapidly correcting boundary misalignments.  In 
general, we find that some degree of hand-correction is always 
required in order to reduce distortions at concatenation points. 
During synthesis, the text is automatically divided into individual 
sentences that are then synthesized and pipelined to the audio 
server.  A text-to-phoneme conversion is applied using a 
phonetic dictionary.  Words that do not appear in the phonetic 
dictionary are automatically pronounced using a multi-layer 
perceptron based pronunciation module.  Here, a 5-letter context 
is extracted from the word to be pronounced.  The letter input is 
fed through the MLP and a phonetic symbol (or possibly epsilon) 
is output by the network.  By sliding the context window, we can 
extract the phonetic pronunciation of the word.   The MLP is 
trained using letter-context and symbol output pairs from a large 
phonetic dictionary. 
The selection of units to concatenate is determined using a hybrid 
search algorithm that operates at the word or phoneme level.  
During synthesis, sections of word-level text that have been 
recorded are automatically concatenated.  Unrecorded words or 
word sequences are synthesized using a Viterbi beam search 
across all available phonetic units.  The cost function includes 
information regarding phonetic context, pitch, duration, and 
signal amplitude.  Audio segments making up the best-path are 
then concatenated to generate the final sentence waveform.   
2. DATA COLLECTION & EVALUATION 
2.1 Data Collection Efforts 
Local Collection Effort 
The Center for Spoken Language Research maintains a dialup 
Communicator system for data collection1. Users wishing to use 
the dialogue system can register at our web site [1] and receive a 
PIN code and system telephone number. To date, our system has 
fielded over 1750 calls totaling over 25,000 utterances from 
nearly 400 registered users.  
NIST Multi-Site Data Collection 
During the months of June and July of 2000, The National 
Institute of Standards (NIST) conducted a multi-site data 
collection effort for the nine DARPA Communicator 
participants.  Participating sites included: AT&T, IBM, BBN, 
SRI, CMU, Colorado, MIT, Lucent, and MITRE.  In this data 
collection, a pool of potential users was selected from various 
parts of the United States by a market research firm.  The 
selected subjects were native speakers of American English who 
were possible frequent travelers.  Users were asked to perform 
nine tasks.  The first seven tasks consisted of fixed scenarios for 
one-way and round-trip flights both within and outside of the 
United States. The final two tasks consisted of users making 
open-ended business or vacation.   
2.2 System Evaluation 
Task Completion 
A total of 72 calls from NIST participants were received by the 
CU Communicator system.  Of these, 44 callers were female and 
28 were male.  Each scenario was inspected by hand and 
compared against the scenario provided by NIST to the subject. 
For the two open-ended tasks, judgment was made based on what 
the user asked for with that of the data provided to the user. In 
total, 53/72 (73.6%) of the tasks were completed successfully.   
A detailed error analysis can be found in [11]. 
Word Error Rate Analysis 
A total of 1327 utterances were recorded from the 72 NIST calls.  
Of these, 1264 contained user speech.  At the time of the June 
2000 NIST evaluation, the CU Communicator system did not 
implement voice-based barge-in.  We noticed that one source of 
error was due to users who spoke before the recording process 
was started.  Even though a tone was presented to the user to 
signify the time to speak, 6.9% of the utterances contained 
instances in which the user spoke before the tone.  Since all users 
were exposed to several other Communicator systems that 
                                                          
2
 The system can be accessed toll-free at 1-866-735-5189 
employed voice barge-in, there may be some effect from 
exposure to those systems. Table 3 summarizes the word error 
rates for the system utilizing the June 2000 NIST data as the test 
set.  Overall, the system had a word error rate (WER) of 26.0% 
when parallel gender-dependent decoders were utilized. Since 
June of 2000, we have collected an additional 15,000 task-
dependent utterances.  With the extra data, we were able to 
remove our dependence on the CMU Communicator training 
data [12].  When the language model was reestimated and 
language model weights reoptimized using only CU 
Communicator data, the WER dropped from 26.0% to 22.5%.  
This amounts to a 13.5% relative reduction in WER. 
Table 1: CU Communicator Word Error Rates for (A) 
Speaker Independent acoustic models and June 2000 
language model, (B) Gender-dependent parallel recognizers 
with June 2000 Language Model, and (C) Language Model 
retrained in December 2000. 
June 2000 NIST Evaluation Data, 1264 
utterances, 72 speakers 
Word Error 
Rate 
(A) Speaker Indep. HMMs (LM#1) 29.8% 
(B) Gender Dependent HMMs (LM#1) 26.0% 
(C) Gender Dependent HMMs (LM#2)  22.5% 
 
Core Metrics 
Sites in the DARPA Communicator program agreed to log a 
common set of metrics for their systems. The proposed set of 
metrics was: Task Completion, Time to Completion, Turns to 
Completion, User Words/Turn, System Words/Turn, User 
Concepts/Turn, Concept Efficiency, State of Itinerary, Error 
Messages, Help Messages, Response Latency, User Words to 
Completion, System Words to Completion, User Repeats, System 
Repeats/Reprompts, Word Error, Mean Length of System 
Utterance, and Mean Length of System Turn. 
Table 2: Dialogue system evaluation metrics 
Item Min Mean Max 
Time to Completion (secs) 120.9 260.3 537.2 
Total Turns to Completion 23 37.6 61 
Response Latency (secs) 1.5 1.9 2.4 
User Words to Task End 19 39.4 105 
System Words to End 173 331.9 914 
Number of Reprompts 0 2.4 15 
 
Table 2 summarizes results obtained from metrics derived 
automatically from the logged timing markers for the calls in 
which the user completed the task assigned to them.  The average 
time to task completion is 260.  During this period there are an 
average of 19 user turns and 19 computer turns (37.6 average 
total turns).  The average response latency was 1.86 seconds.  
The response latency also includes the time required to access the 
data live from the Internet travel information provider. 
3. CU MOVE 
3.1 Task Overview 
The ?CU Move? system represents our work towards achieving 
graceful human-computer interaction in automobile 
environments.  Initially, we have considered the task of vehicle 
route planning and navigation.  As our work progresses, we will 
expand our dialog system to new tasks such as information 
retrieval and summarization and multimedia access. 
The problem of voice dialog within vehicle environments offers 
some important speech research challenges. Speech recognition 
in car environments is in general fragile, with word-error-rates 
(WER) ranging from 30-65% depending on driving conditions. 
These changing environmental conditions include speaker 
changes (task stress, emotion, Lombard effect, etc.) as well as the 
acoustic environment (road/wind noise from windows, air 
conditioning, engine noise, exterior traffic, etc.).   
In developing the CU-Move system [13,14], there are a number 
of research challenges that must be overcome to achieve reliable 
and natural voice interaction within the car environment. Since 
the speaker is performing a task (driving the vehicle), the driver 
will experience a measured level of user task stress and therefore 
this should be included in the speaker-modeling phase. Previous 
studies have clearly shown that the effects of speaker stress and 
Lombard effect can cause speech recognition systems to fail 
rapidly. In addition, microphone type and placement for in-
vehicle speech collection can impact the level of acoustic 
background noise and speech recognition performance.    
3.2 Signal Processing  
Our research for robust recognition in automobile environments 
is concentrated on development of an intelligent microphone 
array.  Here, we employ a Gaussian Mixture Model (GMM) 
based environmental classification scheme to characterize the 
noise conditions in the automobile.  By integrating an 
environmental classification system into the microphone array 
design, decisions can be made as to how best to utilize a noise-
adaptive frequency-partitioned iterative enhancement algorithm 
[15,16] or model-based adaptation algorithms [17,18] during 
decoding to optimize speech recognition accuracy on the beam-
formed signal. 
3.3 Data Collection 
A five-channel microphone array was constructed using Knowles 
microphones and a multi-channel data recorder housing built 
(Fostex) for in-vehicle data collection. An additional reference 
microphone is situated behind the driver?s seat.  Fig. 3 shows the 
constructed microphone array and data recorder housing. 
      
Figure 3: Microphone array and reference microphone (left), 
Fostex multi-channel data recorder (right). 
As part of the CU-Move system formulation, a two phase data 
collection plan has been initiated. Phase I focuses on collecting 
acoustic noise and probe speech from a variety of cars and 
driving conditions. Phase II focuses on a extensive speaker 
collection across multiple U.S. sites. A total of eight vehicles 
have been selected for acoustic noise analysis. These include the 
following: a compact car, minivan, cargo van, sport utility 
vehicle (SUV), compact and full size trucks, sports car, full size 
luxury car.  A fixed 10 mile route through Boulder, CO was used 
for Phase I data collection. The route consisted of city (25 & 
45mph) and highway driving (45 & 65mph). The route included 
stop-and-go traffic, and prescribed locations where 
driver/passenger windows, turn signals, wiper blades, air 
conditioning were operated. Each data collection run per car 
lasted approximately 35-45 minutes.  A detailed acoustic analysis 
of Phase I data can be found in [13]. Our plan is to begin Phase 
II speech/dialogue data collection during spring 2001, which will 
include (i) phonetically balanced utterances, (ii) task-specific 
vocabularies, (iii) natural extemporaneous speech, and (iv) 
human-to-human and Wizard-of-Oz (WOZ) interaction with CU-
Communicator and CU-Move dialog systems. 
3.4 Prototype Dialog System 
Finally, we have developed a prototype dialog system for data 
collection in the car environment.  The dialog system is based on 
the MIT Galaxy-II Hub architecture with base system 
components derived from the CU Communicator system [1].  
Users interacting with the dialog system can enter their origin 
and destination address by voice. Currently, 1107 street names 
for Boulder, CO area are modeled.  The system can resolve street 
addresses by business name via interaction with an Internet 
telephone book.  This allows users to ask more natural route 
queries (e.g., ?I need an auto repair shop?, or ?I need to get to the 
Boulder Marriott?).  The dialog system automatically retrieves 
the driving instructions from the Internet using an online WWW 
route direction provider.  Once downloaded, the driving 
directions are queried locally from an SQL database.  During 
interaction, users mark their location on the route by providing 
spoken odometer readings.  Odometer readings are needed since 
GPS information has not yet been integrated into the prototype 
dialog system. Given the odometer reading of the vehicle as an 
estimate of position, route information such as turn descriptions, 
distances, and summaries can be queried during travel (e.g., 
"What's my next turn", "How far is it", etc.).  
The prototype system uses the CMU Sphinx-II speech recognizer 
with cellular telephone acoustic models along with the Phoenix 
Parser [10] for semantic parsing.  The dialog manager is mixed-
initiative and event driven.  For route guidance, the natural 
language generator formats the driving instructions before 
presentation to the user by the text-to-speech server.   For 
example, the direction,  "Park Ave W. becomes 22nd St." is 
reformatted to, "Park Avenue West becomes Twenty Second  
Street".  Here, knowledge of the task-domain can be used to 
significantly improve the quality of the output text.   For speech 
synthesis, we have developed a Hub-compliant server that 
interfaces to the AT&T NextGen speech synthesizer.   
3.5 Future Work 
We have developed a Hub compliant server that interfaces a 
Garmin GPS-III global positioning device to a mobile computer 
via a serial port link.  The GPS server reports vehicle velocity in 
the X,Y,Z directions as well as real-time updates of  vehicle 
position in latitude and longitude.  HRL Laboratories has 
developed a route server that interfaces to a major navigation 
content provider.  The HRL route server can take GPS 
coordinates as inputs and can describe route maneuvers in terms 
of GPS coordinates.  In the near-term, we will interface our GPS 
server to the HRL route server in order to provide real-time 
updating of vehicle position.  This will eliminate the need for 
periodic location update by the user and also will allow for more 
interesting dialogs to be established (e.g., the computer might 
proactively tell the user about upcoming points of interest, etc.). 
 
4. REFERENCES 
[1] http://communicator.colorado.edu 
[2] W. Ward, B. Pellom, "The CU Communicator System," IEEE 
Workshop on Automatic Speech Recognition and Understanding, 
Keystone Colorado, December, 1999. 
[3] http://fofoca.mitre.org 
[4] Seneff, S., Hurley, E., Lau, R., Pao, C., Schmid, P., Zue,  V., 
?Galaxy-II: A Reference Architecture for Conversational System 
Development,? Proc. ICSLP, Sydney Australia, Vol. 3, pp. 931-
934, 1998. 
[5] Ravishankar, M.K., ?Efficient Algorithms for Speech 
Recognition?. Unpublished Dissertation CMU-CS-96-138, 
Carnegie Mellon University, 1996 
[6] K. Hacioglu, W. Ward, "Dialog-Context Dependent Language 
Modeling Using N-Grams and Stochastic Context-Free Grammars", 
Proc. IEEE ICASSP, Salt Lake City, May 2001. 
[7] K. Hacioglu, W. Ward, "Combining Language Models : Oracle 
Approach", Proc. Human Language Technology Conference, San 
Diego, March 2001. 
[8] R. San-Segundo, B. Pellom, W. Ward, J. M. Pardo, "Confidence 
Measures for Dialogue Management in the CU Communicator 
System," Proc. IEEE ICASSP, Istanbul Turkey, June 2000. 
[9] R. San-Segundo, B. Pellom, K. Hacioglu, W. Ward, J.M. Pardo, 
"Confidence Measures for Dialogue Systems," Proc. IEEE ICASSP, 
Salt Lake City, May 2001.  
[10] Ward, W., ?Extracting Information From Spontaneous Speech?, 
Proc. ICSLP, September 1994. 
[11] B. Pellom, W. Ward, S. Pradhan, "The CU Communicator: An 
Architecture for Dialogue Systems", Proc. ICSLP, Beijing China, 
November 2000. 
[12] Eskenazi,  M., Rudnicky, A., Gregory, K., Constantinides, P.,  
Brennan, R., Bennett, K., Allen, J., ?Data Collection and 
Processing in the Carnegie Mellon Communicator,?   Proc. 
Eurospeech-99, Budapest, Hungary. 
[13] J.H.L. Hansen, J. Plucienkowski, S. Gallant, B.L. Pellom, W. Ward, 
"CU-Move: Robust Speech Processing for In-Vehicle Speech 
Systems," Proc. ICSLP, vol. 1, pp. 524-527, Beijing, China, Oct. 
2000. 
[14] http://cumove.colorado.edu/ 
[15] J.H.L. Hansen, M.A. Clements, ?Constrained Iterative Speech 
Enhancement with Application to Speech Recognition,? IEEE 
Trans. Signal Proc., 39(4):795-805, 1991. 
[16] B. Pellom, J.H.L. Hansen, ?An Improved Constrained Iterative 
Speech Enhancement Algorithm for Colored Noise Environments," 
IEEE Trans. Speech & Audio Proc., 6(6):573-79, 1998. 
[17] R. Sarikaya, J.H.L. Hansen, "Improved Jacobian Adaptation for 
Fast Acoustic Model Adaptation in Noisy Speech Recognition," 
Proc. ICSLP, vol. 3, pp. 702-705, Beijing, China, Oct. 2000. 
[18] R. Sarikaya, J.H.L. Hansen, "PCA-PMC: A novel use of a priori 
knowledge for fast model combination," Proc. ICASSP, vol. II, pp. 
1113-1116, Istanbul, Turkey, June 2000. 
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 379?386, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Detection of Entity Mentions Occurring in English and Chinese Text
Kadri Hacioglu, Benjamin Douglas and Ying Chen
Center for Spoken Language Research
University of Colorado at Boulder
{hacioglu,benjamin.douglas,yc}@colorado.edu
Abstract
In this paper, we describe an integrated
approach to entity mention detection that
yields a monolithic, almost language in-
dependent system. It is optimal in the
sense that all categorical constraints are si-
multaneously considered. The system is
compact and easy to develop and main-
tain, since only a single set of features and
classifiers are needed to be designed and
optimized. It is implemented using one-
versus-all support vector machine (SVM)
classifiers and a number of feature extrac-
tors at several linguistic levels. SVMs
are well known for their ability to han-
dle a large set of overlapping features with
theoretically sound generalization proper-
ties. Data sparsity might be an impor-
tant issue as a result of a large number
of classes and relatively moderate train-
ing data size. However, we report re-
sults that the integrated system performs
as good as a pipelined system that decom-
poses the problem into a few smaller sub-
tasks. We conduct all our experiments us-
ing ACE 2004 data, evaluate the systems
using ACE metrics and report competitive
performance.
1 Introduction
The entity-relation (ER) model (Chen, 1976) views
the physical world as a collection of entities with
complex relationships. Automatic extraction of
this model from raw text is important for creat-
ing a knowledge base (such as relational databases,
marked-up text etc.) that can be used to achieve bet-
ter end-to-end performances in several natural lan-
guage processing (NLP) applications including in-
formation retrieval, question answering and machine
translation. For example, in a typical QA system this
knowledge base can be used to facilitate extraction
of answers and retrieval of relevant documents.
Entities and relations in a document can be men-
tioned in several different ways. For example, a per-
son entity, e.g. Bill Clinton, can be expressed in
many different ways such as The President, Presi-
dent Clinton, Mr. Clinton, he, him etc. Similarly,
one can express a geo-political entity, e.g. United
States, as his country or another person entity, e.g.
Hillary Clinton, as his wife, and their relation to the
entity Bill Clinton as ?president-of? and ?family?,
respectively. It is clear that the detection of these
mentions is the first crucial step for the extraction of
the ER model to populate a database or an ontology.
Extraction of entities and their relationships is
usually done in a pipelined system that first iden-
tifies entity mentions, next resolves mentions into
unique entities (co-reference) and finally finds rela-
tions among them (Florian et al, 2004; Kambhatla,
2004). In that architecture, the errors in the first
stage propagate and reduce the performance of sub-
sequent stages; namely, co-reference resolver, that
clusters all different mentions of an entity into a
unique entity, and relation finder, that links entities
according to their relationships. In fact, the subtask
of entity mention detection itself is a very challeng-
379
Table 1: Categorical structure of entities in ACE program
Entity Mention
Entity Mention
Type Sub-Type Class Type Role
ing subtask since respective expressions can have
relatively complex syntactic and categorical (?se-
mantic?) structures. That is, entity mentions in a
body of text can occur in relatively complex embed-
ded constructs with many attributes. Table 1 illus-
trates the categorical structure of an entity mention
as specified in the Automatic Content Extraction
(ACE) program run by NIST (ACE, 2004). Com-
pared to the previous years the number of entity
types and subtypes is greater.
The following segment of a sentence provides a
typical example of the annotation:
[The [[Jordanian] military] spokesman] added ...
For simplicity, the entity mention attributes are
excluded. The annotation clearly shows the em-
bedded structure of entity mentions. We identify
three entity mentions as The Jordanian military
spokesman, Jordanian military and Jordanian.
Due to its complex nature, it is not uncommon that
the mention detection task itself is also divided into
a number of smaller sub-tasks. However, in this pa-
per, we adopt an integrated classification approach
to this problem that yields a monolithic structure.
This allows all attributes, which define the categori-
cal (?semantic?) structure of a mention, to be jointly
considered. The system has the ability to achieve
better performance in principle provided that there
is ?enough? data to train, is easier to maintain and
develop, and has a single set of features and classi-
fiers to be engineered. All possible class labels are
obtained by filling in the values of each attribute in
the label etype subtype class mtype role, where,
to avoid confusion, etype and mtype are used to de-
note entity and mention types, respectively.
Our data representation requires segmenting doc-
uments into sentences and then tokenizing sentences
into words and punctuation. Each word is then as-
signed a label depending on its role in the mention.
This data representation reduces the problem to a
tagging task. For each token in focus, we create a
number of features at lexical, syntactic and semantic
levels. Additionally, we augment those features us-
ing features from external resources (e.g. named en-
tity taggers, gazetteers, wordnet). We train a number
of one-versus-all classifiers (Allwein et. al, 2000)
using SVMs (Vapnik, 1995; Burges, 1998). During
testing, classification of each token is performed in
a greedy left-to-right manner using a finite-size slid-
ing context window centered at the token in focus
(Kudo and Matsumato, 2000).
This approach yields a large number of classes
and a large number of overlapping features. We used
a machine learning framework based on SVM clas-
sification since a large number of classes (in a one-
versus-all set-up) and a large number of overlapping
features can be easily handled with good general-
ization properties. We argue that data sparsity and
computational complexity is not as severe as it might
be expected in the other machine learning methods
that are based on maximum likelihood parameter es-
timation. In other words, we claim that the large set
of classification labels and training data sparseness
are not major drawbacks. To provide evidence for
this we also consider an approach that divides the
task into relatively simpler tasks with considerably
smaller numbers of labels. The approach yields a
pipelined structure in which the decisions in earlier
stages are used in later stages. We report results that
the integrated approach performs similar to, and in
some cases, even slightly better than the pipelined
structure.
We also implement a novel post-processing
scheme based on an entity base (EB) created from
the tagged test data. This is motivated by the fact
that an entity is identically referenced several times
in a document. However, depending on the capital-
ization information of the entity mention and context
in which it occurs, the entity can be missed at several
positions in the document. A simple postprocess-
ing algorithm that checks untagged tokens with low
confidence against the EB is implemented. In doing
so, it is highly likely that some of those missed en-
tities could be identified. This is expected to reduce
misses at the expense of false alarms. We report re-
sults that support our expectation.
The paper is organized as follows. Section 2 de-
scribes the ACE 2004 data used for training and
evaluation. In Section 3, the problem is explained
380
Table 2: ACE 2004 corpus statistics for English and Chinese
text.
Language Train Test
English ? 150K words ? 50K words
Chinese ? 150K words ? 50K words
and its data representation is introduced. Section 4
describes the general system architecture, that con-
sists of a number of feature extractors, a (machine-
learned) classifier and a simple post processor. In
section 5, the features used for both English and
Chinese systems are described. In section 6, we de-
scribe an alternative pipelined system. A novel post
processing algorithm is introduced in section 7. Sec-
tion 8 reports experimental results. Concluding re-
marks are made in the final section.
2 ACE Data
The ACE 2004 corpus consists of various text an-
notated for entities and relations. This corpus was
created by the Linguistic Data Consortiom (LDC)
in three languages: English, Chinese and Arabic
(with support from the ACE program that began in
1999). Resources for data are newswire reports and
broadcast news programs. Table 2 gives train and
test statistics of this corpus for English and Chinese
languages. Both languages have almost the same
amount of data for both training and evaluation.
3 Problem Description and Data
Representation
As shown in Table 1, an entity mention is charac-
terized along 5 dimensions; namely etype, sub-type
class, mtype and role. The ACE program speci-
fies seven entity types; person, organization, geo-
political, location, facility, vehicle, weapon. All en-
tity types except person are further divided into sev-
eral sub-types. For example, organization has gov-
ernment, commercial, educational, non-prot and
other as its sub-types. The class attribute describes
the kind of reference the entity mention makes to
the entity in the world by taking one of the values
{generic, specic, negative, under-specied} . En-
tity mentions are further characterized according to
linguistic types of references as name (proper noun),
nominal (common noun), pronominal (pronoun) and
premodifier. The role of entity mention applies only
to geo-political entities indicating the role of the en-
tity in the context of the mention as one of person,
location, organization and geo-political. For further
details the reader is referred to (ACE, 2004)
All entity mentions in the original data are
XML tagged with their respective attributes. In
addition to the full extent of mentions, mention
heads are also tagged. Referring to the previous
example, the entity mention ?The Jordanian military
spokesman? which refers to a PERSON has the
word ?spokesman? as its head. Similarly, the entity
mention ?Jordanian military? which refers to an
ORGANIZATION has the word ?military? as its
head. If one reduces the problem of entity mention
detection to the detection of its head, the nature
of the problem changes and the annotation of data
becomes flat;
The [GPE Jordanian] [ORG military] [PERspokesman] .....
This allows us to consider the problem as a
tagging/chunking problem and describe each word
as beginning (B) an entity mention, inside (I) an
entity mention or outside (O) an entity mention
(Ramhsaw and Marcus, 1995; Sang and Veenstra,
1999). However, we believe that the information
regarding the embedded structure in which the
heads of entities occur is also useful for subse-
quent stages of an IE system including inference
of relations among heads occurring in the same
embedded construct. So, in addition to the IOB tags
we introduce bracketing tags that might partially
recover the embedded structure surrounding the
heads. We refer to the following simple example
[Javier Trevino] was [the campaign manager for
[the [ruling party] candidate [Fox] beat ]].
to illustrate our tokenwise vertical representa-
tion:
#SNT BEG#
Javier B-PER NAM
Trevino I-PER NAM
was O
381
Lexical
Analysis
Syntactic
Analysis
Semantic
Analysis
External
Taggers Lookup
Resource 
Feature Combiner
Documents Preprocessor
SVM
Models
Tagged
Documents
PostprocessorClassifier
WordNet
Gazetteers
Figure 1: System Architecture
the (*
campaign *
manager B-PER NOM
for *
the (*
ruling *
party B-ORG NOM
candidate B-PER NOM
Fox B-PER NAM
beat *))
. O
#SNT END#
If one does not use the bracketing representation, all
non-head tokens will be labeled as ?Outside?. We
believe that it is useful to discriminate the tokens that
take part in mentions from those that do not occur in
mentions.
4 General System Architecture
The general system block diagram is illustrated in
Figure 1. It consists of a pre-processor, several fea-
ture extractors, a classifier and a post-processing
module. Although the architecture is language in-
dependent, there are some minor language specific
differences in some modules depending on the na-
ture of the language and availability of resources for
that language. In the following, we briefly describe
both English and Chinese systems and indicate dif-
ferences between them.
In the English system, the pre-processor segments
the documents into sentences. It also includes a
caser that restores the capitalization information of
text without case (e.g. broadcast news) and a to-
kenizer that separates contractions and punctuation
from words. Tokenized sentences are then processed
at different linguistic levels to create features. At
this stage, we employ a lexical pattern analyzer,
part-of-speech tagger, a base phrase chunker, a syn-
tactic parser, a dependency analyzer, look-up inter-
faces to external knowledge sources, and external
small scale named entity taggers trained on different
genres of text with different machine learning algo-
rithms. All features are combined and then input to
a classifier based on one-versus-all SVM classifiers.
Finally, we perform simple post-processing to make
sure that the final bracketing information is consis-
tent.
The POS tagger and BP chunker are trained
in-house using the Penn TreeBank. The syntac-
tic parser is the Charniak parser which has mod-
els trained on the Penn TreeBank. The depen-
dency analyzer performs dependency analysis using
a set of head rules. The software was generously
made available to us by the University of Maryland.
The look-up interface to external knowledge sources
such as WordNet or gazetteers is implemented using
simple pattern matching.
In the Chinese system, the pre-processor is
slightly different from that of the English system.
It (obviously) does not need a caser and consid-
ers single Chinese characters as the minimal units
of processing. It jointly segments a document
into sentences and words. Then, it passes both
word and sentence segmentation information to the
subsequent stages along with Chinese characters.
The SVM-based joint sentence/word segmenter is
trained using the Chinese TreeBank (CTB). Linguis-
382
tic analysis at different levels is performed in a man-
ner similar to the analysis in the English system.
In the Chinese system, the CTB is used to train
a SVM-based POS tagger and BP chunker. The
syntactic parser is trained on the CTB using Dan
Bikel?s parser. Dependency analysis is performed
as in the English system using a set of Chinese head
rules. Several in-house external taggers are trained
using SVMs and different corpora. We have used
only gazetteers for chinese as external knowledge
sources.
5 Features
The following features are used in the English sys-
tem:
? tokens: words in their original and all lower-
cased forms
? n-grams: token prefixes and suffixes of length
less than and equal to four
? lexical patterns: indicate case information
(all lower-case, mixed case, first letter capital,
all upper-case), is hyphen, type (numeral, al-
phanumeral, alpha, other)
? Part of Speech tags
? BP Positions: The position of a token in a BP
using the IOB representation (e.g. B-NP, I-NP,
O etc.)
? Clause tags: The tags that mark token posi-
tions in a sentence with respect to clauses. (e.g
*S)*S) marks a position that two clauses end)
? Named entities-1: The IOB tags of named en-
tities. There are four categories; LOC, ORG,
PERSON and MISC. A SVM-based tagger
which is trained on CoNLL 2003 shared task
data is used.
? Named entities-2: IOB tags of named enti-
ties found by the Identifinder (Bikel et. al,
1999); a HMM-based named entity tagger with
29 classes
? Named entities-3: IOB tags from a named en-
tity tagger trained on MUC-6 and MUC-7 data
using only the entity classes PERSON, LOCA-
TION and ORGANIZATION.
? Gazetteer labels: indicate the name of the list
to which the token belongs. Simple pattern
matching is employed here.
? WordNet categories: concepts or class names
in the WordNet 2.0 hypernym hierarchy rooted
at ?entity? concept. We trace hypernym hier-
archies of the two most frequent senses of to-
kens that are tagged as nouns (NN, NNS, NNP
etc.) to the top concepts. We count the num-
ber of concepts (that match to ACE entity types
and subtypes) that occur in the hypernym hier-
archy indicating that token is a (kind of) con-
cept. The concepts (i.e entity/types/subtypes)
with the maximum counts in the top two senses
are selected as features (can also be considered
as ?maybe? labels)
? Syntactic tags: patterns of non-terminals and
brackets that indicate the position of tokens in
syntactic trees.
? Head words: words that the tokens depend
? POS of Head words:
? main verb: the verb at which the dependency
parse tree is rooted.
? Relations: the grammatical and semantic rela-
tions between tokens and their heads.
? Head word flag: indicates whether the token
plays a role of head in the sentence.
The features used in the Chinese system are
? tokens: Chinese characters
? token positions: IOB tags that indicate posi-
tion of characters in words
? Part of Speech tags: POS tags of words to
which tokens (characters) belong
? BP Positions: The position of a token in a BP
using the IOB representation (e.g. B-NP, I-NP,
O etc.)
? Named entities-1: IOB tags of two type of en-
tities; location and person. A SVM based tag-
ger trained on part of the Sinica corpus from
Taiwan is used to generate these features.
383
? Named entities-2: IOB tags of named enti-
ties: person, location, organization etc. An-
other SVM based tagger trained on the People
Daily data from mainland of China.
? Gazetteer labels: indicate the name of the list
to which the token belongs. Simple pattern
matching is employed here. Examples are la-
bels that indicate Chinese last name, foreign
person last name, first name etc.
? Syntactic labels: base phrase chunk labels and
paths in syntactic trees
? Head words: as determined by Chinese depen-
dency analysis
? POS of Head words:
? Relations: the grammatical and semantic rela-
tions between tokens and their heads.
6 A Pipelined System
As mentioned earlier the structure of entity men-
tion categories is very complex. Considering all at-
tributes together yields a large number of classes.
One can argue that the large number of classes and
data sparsity is an important issue here that it might
have significant effect on performance. However,
several attempts to divide the task into simpler sub-
tasks have failed to yield a system with a better per-
formance than that of the integrated system. In this
section, we describe one such system.
The system consists of three stages in cascade: (i)
entity mention extent detector, , (ii) mention type de-
tector and (iii) entity type, subtype and mention role
detector. Referring to the earlier example, the data
representation in terms of class labels at each level
is as follows:
#SNT BEG#
Javier (* B-NAM PER
Trevino *) I-NAM PER
was O O O
the (* O O
campaign * O O
manager * B-NOM PER
for * O O
the (* O O
ruling (* O O
party *) B-NOM ORG
candidate *) B-NOM PER
Fox * B-NAM PER
beat *)) O O
. O O O
#SNT END#
where the second column is for the extent labels of
mentions in bracketed representation, the third col-
umn is for the mention type labels in IOB represen-
tation and the last column is for the type labels (sub-
type and role labels are omitted for the sake of sim-
plicity) of entity mentions in plain representation.
The pipelined system operates as follows. First it
detects embedding structure of mention extents. Us-
ing that information the second stage identifies the
type of mentions. In the final stage, the system iden-
tifies entity types, subtypes and mention roles using
information (as features within context) from previ-
ous stages. Finally we combine all information into
entity mention attributes and resolve inconsistencies
by simple postprocessing.
Here, we have not done any feature selection spe-
cific to each stage. Instead we used the same fea-
tures in all stages. One can argue that this is not the
optimal set up for a cascaded system; separate fea-
ture design and selection should be made for each
stage. Also we acknowledge that there are several
other ways of dividing the task into smaller, simpler
subtasks. Although we have not explored all pos-
sible pipelined architectures with all possible fea-
ture selections , we conjecture that the data sparsity
is not as big an issue in SVMs as expected to be
in the other machine learning algorithms based on
maximum likelihood parameter estimation such as
those based on maximum entropy (ME) or condi-
tional random fields (CRF) frameworks.
7 A Novel Post-Processing Method
In our experiments, we have consistently observed
that the identical mentions of a unique entity are
missed depending on the missing capitalization in-
formation, unseen context and errors in feature ex-
traction. For example, although the name mention
of person ?Eminem? is captured at several positions
in the document, the entity mention ?eminem? is
missed, probably, due to its missing capitalization.
384
Table 3: Statistics on ACE 2004 data.
Language Train Samples Test Samples # Joint Classes # Pipelined Classes
Extent MType EType-SubTypey-Role
English ? 167K ? 61K 384 24 9 93
Chinese ? 307K ? 105K 374 15 7 95
As a solution we propose a post-processing
method that is based on an entity base (EB) cre-
ated from the tagged text. We populate the EB with
all entity mentions (particularly with those that have
name values) identified in the text. After we create
the EB, we tag the text again by case insensitive pat-
tern matching. We determine all tagged tokens that
were initially left untagged or tagged with a differ-
ent label by the SVM classifier. Using the SVM out-
put (distance from separating hyperplane) as a confi-
dence measure, we accept or reject the new tag based
on a preselected threshold.
8 Experiments and Results
In this section, we describe the experiments con-
ducted and results obtained using the ACE 2004
data. The number of training and test examples,
which are words/punctuations in English and char-
acters in Chinese, are summarized in Table 3. The
number of classes in the joint task and in each
pipelined subtask are also included.
In the first set of experiments we evaluated our
integrated system and investigated the performance
with respect to broad classes of features introduced
in section 5, by adding one group of features at a
time. Grouping of features into broad classes were
done as follows:
? baseline features: tokens
? lexical features: POS, lexical patterns
? syntactic features: base phrase chunks, syntac-
tic tree features
? ?semantic? features: heads and grammatical re-
lations
? external features: features from external re-
sources; e.g. wordnet, gazetteers, other entity
taggers etc.
Table 4: English system performance with respect to broad
classes of features; lex: lexical features, syn: syntactic fea-
tures, sem: ?semantic? features, ext: external features, Fuw:unweighted F-score, Fw: weighted F-score, ACE: ACE value.
Feature class Fuw Fw ACE
baseline (tokens) 56.5 54.8 36.1
baseline+lex 76.8 86.7 75.6
baseline+lex+syn 76.9 87.4 76.8
baseline+lex+syn+sem 77.1 87.8 77.6
baseline+lex+syn+sem+ext 82.0 90.7 82.9
The results are summarized in Table 4 and Table
5 for both English and Chinese systems. Both un-
weighted and weighted F-scores, and also ACE val-
ues are reported. It is interesting to note that sig-
nificant gains were achieved by simple lexical and
external features when they are added. The degree
of improvement by using computationally intensive
syntactic and dependency analysis is marginal. This
might partly be due to the type of features derived
from parse trees and partly due to the mismatch of
the genre of text to the text on which the syntactic
chunker and parser is trained. Since the dependency
analysis is based on the syntactic analysis using a
set of head rules, the extracted dependency based
features might also be inaccurate. Although we
observed moderate improvement for English, those
features slightly hurt the performance of the Chinese
system. This is because of the fact that the Chinese
syntactic parser performs relatively worse than the
English syntactic parser.
Table 6 presents the integrated and pipelined sys-
tem performances using all features extracted for
English and Chinese. Post-processing results are
also included. It shows notable performance im-
provement with the recovery of many misses by
post-processing. It should be noted that, in the
pipelined architecture the post-processing is per-
formed twice; at both mention and entity levels.
385
Table 5: Chinese system performance with respect to broad
classes of features; lex: lexical features, syn: syntactic fea-
tures, sem: ?semantic? features, ext: external features, Fuw:unweighted F-score, Fw: weighted F-score, ACE: ACE value.
Feature class Fuw Fw ACE
baseline (tokens) 77.6 83.5 70.8
baseline+lex 78.3 85.2 73.4
baseline+lex+syn 76.1 83.7 70.8
baseline+lex+syn+sem 74.8 83.6 70.8
baseline+lex+syn+sem+ext 78.4 86.8 76.1
9 Conclusions
We have discussed the significance of the entity
mention detection in ER model extraction from
raw text and presented the complex syntactic and
categorical structure of the entity mentions speci-
fied in the ACE program. We have explored dif-
ferent ways of representing the problem and im-
plemented two architecturally different (supervised)
machine-learning based systems to accomplish the
task; namely, a monolithic system and a cascaded
system. We have described those systems in detail
and empirically compared them. Both systems have
achieved comparable performances on English text.
However, the integrated system has achived moder-
ately better performance on Chinese text. We have
argued that it is easier to develop and maintain the
monolithic system since it has a single set of features
and classifiers to be tuned. We believe that the per-
formance levels achieved at mid 80s (in ACE values)
for English and at upper 70s for Chinese, using only
the ACE data, are competitive. We have introduced
a post-processing algorithm based on an entity base
created during the testing. It has worked very well
for both languages to recover several missed en-
tity mentions and considerably improved the perfor-
mance.
10 Acknowledgement
We extend our special thanks to Wayne Ward,
Steven Bethard, James H. Martin and Dan Juraf-
sky for their useful feedback during this work. This
work is supported by the ARDA Aquaint II Program
via contract NBCHC040040.
Table 6: English and Chinese system performances with all
features and post-processing: Fuw: unweighted F-score, Fw:weighted F-score, ACE: ACE value.
English System Fuw Fw ACE
Integrated 82.0 90.7 82.9
Pipelined 82.1 90.8 83.1
Integrated+Post 82.2 91.5 84.3
Pipelined+Post 82.3 91.3 84.0
Chinese System
Integrated 78.4 86.8 76.1
Pipelined 76.9 85.7 74.1
Integrated+Post 79.6 87.7 77.5
Pipelined+Post 79.1 86.6 75.6
References
E. L. Allwein, R. E Schapire and Y. Singer. 2000. Re-
ducing multiclass to binary: A unifying approach formargin classifiers. Journal of Machine Learning Re-search, 1:113-141,
Dan M. Bikel, Robert L. Schwartz, and Ralph M.Weischedel. 1999 An algorithm that learns what?s
in a name. Machine Learning, Vol. 34, pp. 211-231.
Chiristopher J. C. Burges 1998. Tutorial on Support Vec-tor Machines for Pattern Recognition. Data Miningand Knowledge Discovery, 2(2), pages 1-47.
Peter P. Chen 1976. The Entity-Relationship Model:Toward a Unified View of Data. ACM Trans. onDatabase Systems, Vol. 1, No. 1, pages 1-36.
R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kamb-hatla, X. Luo, N. Nicolov, and S. Roukos. 2004. AStatistical Model for Multilingual Entity Detection and
Tracking. Proceedings of HLT-2004.
Nanda Kambhatla. 2004. Combining Lexical Syntactic
and Semantic Features with Maximum Entropy Mod-els for Extracting Relations. Proceedings of ACL-04.
Taku Kudo and Yuji Matsumato. 2000. Use of support
vector learning for chunk identification. Proc. of the4th Conference on Very Large corpora, pages 142-144.
Lance E. Ramhsaw and Mitchel P. Marcus. 1995.Text Chunking Using Transformation Based Learning.Proceedings of the 3rd ACL Workshop on Very LargeCorpora, pages 82-94.
Erik F. T. J. Sangand and Jorn Veenstra 1999. Repre-senting text chunks. Proceedings of EACL?99, pages
173-179.
The Automatic Content Extraction (ACE) Evaluation
Plan. 2004. www.nist.gov/speech/tests/ace/
Vladamir Vapnik. 1995. The Nature of Statistical Learn-ing Theory. Springer Verlag, New York, USA.
386






Shallow Semantic Parsing using Support Vector Machines?
Sameer Pradhan, Wayne Ward,
Kadri Hacioglu, James H. Martin
Center for Spoken Language Research,
University of Colorado, Boulder, CO 80303
{spradhan,whw,hacioglu,martin}@cslr.colorado.edu
Dan Jurafsky
Department of Linguistics
Stanford University
Stanford, CA 94305
jurafsky@stanford.edu
Abstract
In this paper, we propose a machine learning al-
gorithm for shallow semantic parsing, extend-
ing the work of Gildea and Jurafsky (2002),
Surdeanu et al (2003) and others. Our al-
gorithm is based on Support Vector Machines
which we show give an improvement in perfor-
mance over earlier classifiers. We show perfor-
mance improvements through a number of new
features and measure their ability to general-
ize to a new test set drawn from the AQUAINT
corpus.
1 Introduction
Automatic, accurate and wide-coverage techniques that
can annotate naturally occurring text with semantic argu-
ment structure can play a key role in NLP applications
such as Information Extraction, Question Answering and
Summarization. Shallow semantic parsing ? the process
of assigning a simple WHO did WHAT to WHOM, WHEN,
WHERE, WHY, HOW, etc. structure to sentences in text,
is the process of producing such a markup. When pre-
sented with a sentence, a parser should, for each predicate
in the sentence, identify and label the predicate?s seman-
tic arguments. This process entails identifying groups of
words in a sentence that represent these semantic argu-
ments and assigning specific labels to them.
In recent work, a number of researchers have cast this
problem as a tagging problem and have applied vari-
ous supervised machine learning techniques to it (Gildea
and Jurafsky (2000, 2002); Blaheta and Charniak (2000);
Gildea and Palmer (2002); Surdeanu et al (2003); Gildea
and Hockenmaier (2003); Chen and Rambow (2003);
Fleischman and Hovy (2003); Hacioglu and Ward (2003);
Thompson et al (2003); Pradhan et al (2003)). In this
?This research was partially supported by the ARDA
AQUAINT program via contract OCG4423B and by the NSF
via grant IS-9978025
paper, we report on a series of experiments exploring this
approach.
For the initial experiments, we adopted the approach
described by Gildea and Jurafsky (2002) (G&J) and eval-
uated a series of modifications to improve its perfor-
mance. In the experiments reported here, we first re-
placed their statistical classification algorithm with one
that uses Support Vector Machines and then added to the
existing feature set. We evaluate results using both hand-
corrected TreeBank syntactic parses, and actual parses
from the Charniak parser.
2 Semantic Annotation and Corpora
We will be reporting on results using PropBank1 (Kings-
bury et al, 2002), a 300k-word corpus in which predi-
cate argument relations are marked for part of the verbs
in the Wall Street Journal (WSJ) part of the Penn Tree-
Bank (Marcus et al, 1994). The arguments of a verb are
labeled ARG0 to ARG5, where ARG0 is the PROTO-
AGENT (usually the subject of a transitive verb) ARG1
is the PROTO-PATIENT (usually its direct object), etc.
PropBank attempts to treat semantically related verbs
consistently. In addition to these CORE ARGUMENTS,
additional ADJUNCTIVE ARGUMENTS, referred to as
ARGMs are also marked. Some examples are ARGM-
LOC, for locatives, and ARGM-TMP, for temporals. Fig-
ure 1 shows the syntax tree representation along with the
argument labels for an example structure extracted from
the PropBank corpus.
Most of the experiments in this paper, unless speci-
fied otherwise, are performed on the July 2002 release
of PropBank. A larger, cleaner, completely adjudicated
version of PropBank was made available in Feb 2004.
We will also report some final best performance numbers
on this corpus. PropBank was constructed by assigning
semantic arguments to constituents of the hand-corrected
TreeBank parses. The data comprise several sections of
the WSJ, and we follow the standard convention of using
1http://www.cis.upenn.edu/?ace/
Section-23 data as the test set. Section-02 to Section-
21 were used for training. In the July 2002 release, the
training set comprises about 51,000 sentences, instantiat-
ing about 132,000 arguments, and the test set comprises
2,700 sentences instantiating about 7,000 arguments. The
Feb 2004 release training set comprises about 85,000 sen-
tences instantiating about 250,000 arguments and the test
set comprises 5,000 sentences instantiating about 12,000
arguments.
[ARG0 He] [predicate talked] for [ARGM?TMP about
20 minutes].
S
hhhh
((((
NP
PRP
He
ARG0
VP
hhhh
((((
VBD
talked
predicate
PP
hhh
(((
IN
for
NULL
NP
hhhhh
(((((
about 20 minutes
ARGM ? TMP
Figure 1: Syntax tree for a sentence illustrating the Prop-
Bank tags.
3 Problem Description
The problem of shallow semantic parsing can be viewed
as three different tasks.
Argument Identification ? This is the process of identi-
fying parsed constituents in the sentence that represent
semantic arguments of a given predicate.
Argument Classification ? Given constituents known to
represent arguments of a predicate, assign the appropri-
ate argument labels to them.
Argument Identification and Classification ? A combina-
tion of the above two tasks.
Each node in the parse tree can be classified as either
one that represents a semantic argument (i.e., a NON-
NULL node) or one that does not represent any seman-
tic argument (i.e., a NULL node). The NON-NULL nodes
can then be further classified into the set of argument la-
bels. For example, in the tree of Figure 1, the node IN
that encompasses ?for? is a NULL node because it does
not correspond to a semantic argument. The node NP
that encompasses ?about 20 minutes? is a NON-NULL
node, since it does correspond to a semantic argument
? ARGM-TMP.
4 Baseline Features
Our baseline system uses the same set of features in-
troduced by G&J. Some of the features, viz., predicate,
voice and verb sub-categorization are shared by all the
nodes in the tree. All the others change with the con-
stituent under consideration.
? Predicate ? The predicate itself is used as a feature.
? Path ? The syntactic path through the parse tree
from the parse constituent to the predicate being
classified. For example, in Figure 1, the path from
ARG0 ? ?He? to the predicate talked, is represented
with the string NP?S?VP?VBD. ? and ? represent
upward and downward movement in the tree respec-
tively.
? Phrase Type ? This is the syntactic category (NP,
PP, S, etc.) of the phrase/constituent corresponding
to the semantic argument.
? Position ? This is a binary feature identifying
whether the phrase is before or after the predicate.
? Voice ? Whether the predicate is realized as an ac-
tive or passive construction.
? Head Word ? The syntactic head of the phrase. This
is calculated using a head word table described by
(Magerman, 1994) and modified by (Collins, 1999,
Appendix. A).
? Sub-categorization ? This is the phrase struc-
ture rule expanding the predicate?s parent node
in the parse tree. For example, in Figure 1, the
sub-categorization for the predicate talked is
VP?VBD-PP.
5 Classifier and Implementation
We formulate the parsing problem as a multi-class clas-
sification problem and use a Support Vector Machine
(SVM) classifier (Hacioglu et al, 2003; Pradhan et al
2003). Since SVMs are binary classifiers, we have to con-
vert the multi-class problem into a number of binary-class
problems. We use the ONE vs ALL (OVA) formalism,
which involves training n binary classifiers for a n-class
problem.
Since the training time taken by SVMs scales exponen-
tially with the number of examples, and about 80% of the
nodes in a syntactic tree have NULL argument labels, we
found it efficient to divide the training process into two
stages, while maintaining the same accuracy:
1. Filter out the nodes that have a very high probabil-
ity of being NULL. A binary NULL vs NON-NULL
classifier is trained on the entire dataset. A sigmoid
function is fitted to the raw scores to convert the
scores to probabilities as described by (Platt, 2000).
2. The remaining training data is used to train OVA
classifiers, one of which is the NULL-NON-NULL
classifier.
With this strategy only one classifier (NULL vs NON-
NULL) has to be trained on all of the data. The remaining
OVA classifiers are trained on the nodes passed by the
filter (approximately 20% of the total), resulting in a con-
siderable savings in training time.
In the testing stage, we do not perform any filtering
of NULL nodes. All the nodes are classified directly
as NULL or one of the arguments using the classifier
trained in step 2 above. We observe no significant per-
formance improvement even if we filter the most likely
NULL nodes in a first pass.
For our experiments, we used TinySVM2 along with
YamCha3 (Kudo and Matsumoto, 2000)
(Kudo and Matsumoto, 2001) as the SVM training and
test software. The system uses a polynomial kernel with
degree 2; the cost per unit violation of the margin, C=1;
and, tolerance of the termination criterion, e=0.001.
6 Baseline System Performance
Table 1 shows the baseline performance numbers on the
three tasks mentioned earlier; these results are based on
syntactic features computed from hand-corrected Tree-
Bank (hence LDC hand-corrected) parses.
For the argument identification and the combined iden-
tification and classification tasks, we report the precision
(P), recall (R) and the F14 scores, and for the argument
classification task we report the classification accuracy
(A). This test set and all test sets, unless noted otherwise
are Section-23 of PropBank.
Classes Task P R F1 A
(%) (%) (%)
ALL Id. 90.9 89.8 90.4
ARGs Classification - - - 87.9
Id. + Classification 83.3 78.5 80.8
CORE Id. 94.7 90.1 92.3
ARGs Classification - - - 91.4
Id. + Classification 88.4 84.1 86.2
Table 1: Baseline performance on all three tasks using
hand-corrected parses.
7 System Improvements
7.1 Disallowing Overlaps
The system as described above might label two con-
stituents NON-NULL even if they overlap in words. This
is a problem since overlapping arguments are not allowed
in PropBank. Among the overlapping constituents we re-
tain the one for which the SVM has the highest confi-
dence, and label the others NULL. The probabilities ob-
tained by applying the sigmoid function to the raw SVM
scores are used as the measure of confidence. Table 2
shows the performance of the parser on the task of iden-
tifying and labeling semantic arguments using the hand-
corrected parses. On all the system improvements, we
perform a ?2 test of significance at p = 0.05, and all the
2http://cl.aist-nara.ac.jp/?talus-Au/software/TinySVM/
3http://cl.aist-nara.ac.jp/?taku-Au/software/yamcha/
4F1 = 2PRP+R
significant improvements are marked with an ?. In this
system, the overlap-removal decisions are taken indepen-
dently of each other.
P R F1
(%) (%)
Baseline 83.3 78.5 80.8
No Overlaps 85.4 78.1 ?81.6
Table 2: Improvements on the task of argument identi-
fication and classification after disallowing overlapping
constituents.
7.2 New Features
We tested several new features. Two were obtained from
the literature ? named entities in constituents and head
word part of speech. Other are novel features.
1. Named Entities in Constituents ? Following
Surdeanu et al (2003), we tagged 7 named en-
tities (PERSON, ORGANIZATION, LOCATION,
PERCENT, MONEY, TIME, DATE) using Identi-
Finder (Bikel et al, 1999) and added them as 7
binary features.
2. Head Word POS ? Surdeanu et al (2003) showed
that using the part of speech (POS) of the head word
gave a significant performance boost to their system.
Following that, we experimented with the addition
of this feature to our system.
3. Verb Clustering ? Since our training data is rel-
atively limited, any real world test set will con-
tain predicates that have not been seen in training.
In these cases, we can benefit from some informa-
tion about the predicate by using predicate clus-
ter as a feature. The verbs were clustered into 64
classes using the probabilistic co-occurrence model
of Hofmann and Puzicha (1998). The clustering al-
gorithm uses a database of verb-direct-object rela-
tions extracted by Lin (1998). We then use the verb
class of the current predicate as a feature.
4. Partial Path ? For the argument identification task,
path is the most salient feature. However, it is also
the most data sparse feature. To overcome this prob-
lem, we tried generalizing the path by adding a new
feature that contains only the part of the path from
the constituent to the lowest common ancestor of the
predicate and the constituent, which we call ?Partial-
Path?.
5. Verb Sense Information ? The arguments that a
predicate can take depend on the word sense of the
predicate. Each predicate tagged in the PropBank
corpus is assigned a separate set of arguments de-
pending on the sense in which it is used. Table 3
illustrates the argument sets for the predicate talk.
Depending on the sense of the predicate talk, either
ARG1 or ARG2 can identify the hearer. Absence of
this information can be potentially confusing to the
learning mechanism.
Talk sense 1: speak sense 2: persuade/dissuade
Tag Description Tag Description
ARG0 Talker ARG0 Talker
ARG1 Subject ARG1 Talked to
ARG2 Hearer ARG2 Secondary action
Table 3: Argument labels associated with the two senses
of predicate talk in PropBank corpus.
We added the oracle sense information extracted
from PropBank, to our features by treating each
sense of a predicate as a distinct predicate.
6. Head Word of Prepositional Phrases ? Many ad-
junctive arguments, such as temporals and locatives,
occur as prepositional phrases in a sentence, and
it is often the case that the head words of those
phrases, which are always prepositions, are not very
discriminative, eg., ?in the city?, ?in a few minutes?,
both share the same head word ?in? and neither
contain a named entity, but the former is ARGM-
LOC, whereas the latter is ARGM-TMP. Therefore,
we tried replacing the head word of a prepositional
phrase, with that of the first noun phrase inside the
prepositional phrase. We retained the preposition in-
formation by appending it to the phrase type, eg.,
?PP-in? instead of ?PP?.
7. First and Last Word/POS in Constituent ? Some
arguments tend to contain discriminative first and
last words so we tried using them along with their
part of speech as four new features.
8. Ordinal constituent position ? In order to avoid
false positives of the type where constituents far
away from the predicate are spuriously identified as
arguments, we added this feature which is a concate-
nation of the constituent type and its ordinal position
from the predicate.
9. Constituent tree distance ? This is a finer way of
specifying the already present position feature.
10. Constituent relative features ? These are nine fea-
tures representing the phrase type, head word and
head word part of speech of the parent, and left and
right siblings of the constituent in focus. These were
added on the intuition that encoding the tree context
this way might add robustness and improve general-
ization.
11. Temporal cue words ? There are several temporal
cue words that are not captured by the named entity
tagger and were considered for addition as a binary
feature indicating their presence.
12. Dynamic class context ? In the task of argument
classification, these are dynamic features that repre-
sent the hypotheses of at most previous two nodes
belonging to the same tree as the node being classi-
fied.
8 Feature Performance
Table 4 shows the effect each feature has on the ar-
gument classification and argument identification tasks,
when added individually to the baseline. Addition of
named entities improves the F1 score for adjunctive ar-
guments ARGM-LOC from 59% to ?68% and ARGM-
TMP from 78.8% to ?83.4%. But, since these arguments
are small in number compared to the core arguments, the
overall accuracy does not show a significant improve-
ment. We found that adding this feature to the NULL vs
NON-NULL classifier degraded its performance. It also
shows the contribution of replacing the head word and the
head word POS separately in the feature where the head
of a prepositional phrase is replaced by the head word
of the noun phrase inside it. Apparently, a combination
of relative features seem to have a significant improve-
ment on either or both the classification and identification
tasks, and so do the first and last words in the constituent.
Features Class ARGUMENT ID
Acc.
P R F1
Baseline 87.9 93.7 88.9 91.3
+ Named entities 88.1 - - -
+ Head POS ?88.6 94.4 90.1 ?92.2
+ Verb cluster 88.1 94.1 89.0 91.5
+ Partial path 88.2 93.3 88.9 91.1
+ Verb sense 88.1 93.7 89.5 91.5
+ Noun head PP (only POS) ?88.6 94.4 90.0 ?92.2
+ Noun head PP (only head) ?89.8 94.0 89.4 91.7
+ Noun head PP (both) ?89.9 94.7 90.5 ?92.6
+ First word in constituent ?89.0 94.4 91.1 ?92.7
+ Last word in constituent ?89.4 93.8 89.4 91.6
+ First POS in constituent 88.4 94.4 90.6 ?92.5
+ Last POS in constituent 88.3 93.6 89.1 91.3
+ Ordinal const. pos. concat. 87.7 93.7 89.2 91.4
+ Const. tree distance 88.0 93.7 89.5 91.5
+ Parent constituent 87.9 94.2 90.2 ?92.2
+ Parent head 85.8 94.2 90.5 ?92.3
+ Parent head POS ?88.5 94.3 90.3 ?92.3
+ Right sibling constituent 87.9 94.0 89.9 91.9
+ Right sibling head 87.9 94.4 89.9 ?92.1
+ Right sibling head POS 88.1 94.1 89.9 92.0
+ Left sibling constituent ?88.6 93.6 89.6 91.6
+ Left sibling head 86.9 93.9 86.1 89.9
+ Left sibling head POS ?88.8 93.5 89.3 91.4
+ Temporal cue words ?88.6 - - -
+ Dynamic class context 88.4 - - -
Table 4: Effect of each feature on the argument identifi-
cation and classification tasks when added to the baseline
system.
We tried two other ways of generalizing the head word:
i) adding the head word cluster as a feature, and ii) replac-
ing the head word with a named entity if it belonged to
any of the seven named entities mentioned earlier. Nei-
ther method showed any improvement. We also tried gen-
eralizing the path feature by i) compressing sequences of
identical labels, and ii) removing the direction in the path,
but none showed any improvement on the baseline.
8.1 Argument Sequence Information
In order to improve the performance of their statistical ar-
gument tagger, G&J used the fact that a predicate is likely
to instantiate a certain set of arguments. We use a similar
strategy, with some additional constraints: i) argument
ordering information is retained, and ii) the predicate is
considered as an argument and is part of the sequence.
We achieve this by training a trigram language model on
the argument sequences, so unlike G&J, we can also es-
timate the probability of argument sets not seen in the
training data. We first convert the raw SVM scores to
probabilities using a sigmoid function. Then, for each
sentence being parsed, we generate an argument lattice
using the n-best hypotheses for each node in the syn-
tax tree. We then perform a Viterbi search through the
lattice using the probabilities assigned by the sigmoid
as the observation probabilities, along with the language
model probabilities, to find the maximum likelihood path
through the lattice, such that each node is either assigned
a value belonging to the PROPBANK ARGUMENTs, or
NULL.
CORE ARGs/ P R F1
Hand-corrected parses (%) (%)
Baseline w/o overlaps 90.0 86.1 88.0
Common predicate 90.8 86.3 88.5
Specific predicate lemma 90.5 87.4 ?88.9
Table 5: Improvements on the task of argument identifi-
cation and tagging after performing a search through the
argument lattice.
The search is constrained in such a way that no two
NON-NULL nodes overlap with each other. To simplify
the search, we allowed only NULL assignments to nodes
having a NULL likelihood above a threshold. While train-
ing the language model, we can either use the actual pred-
icate to estimate the transition probabilities in and out
of the predicate, or we can perform a joint estimation
over all the predicates. We implemented both cases con-
sidering two best hypotheses, which always includes a
NULL (we add NULL to the list if it is not among the
top two). On performing the search, we found that the
overall performance improvement was not much differ-
ent than that obtained by resolving overlaps as mentioned
earlier. However, we found that there was an improve-
ment in the CORE ARGUMENT accuracy on the combined
task of identifying and assigning semantic arguments,
given hand-corrected parses, whereas the accuracy of the
ADJUNCTIVE ARGUMENTS slightly deteriorated. This
seems to be logical considering the fact that the ADJUNC-
TIVE ARGUMENTS are not linguistically constrained in
any way as to their position in the sequence of argu-
ments, or even the quantity. We therefore decided to
use this strategy only for the CORE ARGUMENTS. Al-
though, there was an increase in F1 score when the lan-
guage model probabilities were jointly estimated over all
the predicates, this improvement is not statistically signif-
icant. However, estimating the same using specific predi-
cate lemmas, showed a significant improvement in accu-
racy. The performance improvement is shown in Table 5.
9 Best System Performance
The best system is trained by first filtering the most
likely nulls using the best NULL vs NON-NULL classi-
fier trained using all the features whose argument identi-
fication F1 score is marked in bold in Table 4, and then
training a ONE vs ALL classifier using the data remain-
ing after performing the filtering and using the features
that contribute positively to the classification task ? ones
whose accuracies are marked in bold in Table 4. Table 6
shows the performance of this system.
Classes Task Hand-corrected parses
P R F1 A
(%) (%) (%)
ALL Id. 95.2 92.5 93.8
ARGs Classification - - - 91.0
Id. + Classification 88.9 84.6 86.7
CORE Id. 96.2 93.0 94.6
ARGs Classification - - - 93.9
Id. + Classification 90.5 87.4 88.9
Table 6: Best system performance on all tasks using
hand-corrected parses.
10 Using Automatic Parses
Thus far, we have reported results using hand-corrected
parses. In real-word applications, the system will have
to extract features from an automatically generated
parse. To evaluate this scenario, we used the Charniak
parser (Chaniak, 2001) to generate parses for PropBank
training and test data. We lemmatized the predicate using
the XTAG morphology database5 (Daniel et al, 1992).
Table 7 shows the performance degradation when
automatically generated parses are used.
11 Using Latest PropBank Data
Owing to the Feb 2004 release of much more and com-
pletely adjudicated PropBank data, we have a chance to
5ftp://ftp.cis.upenn.edu/pub/xtag/morph-1.5/morph-
1.5.tar.gz
Classes Task Automatic parses
P R F1 A
(%) (%) (%)
ALL Id. 89.3 82.9 86.0
ARGs Classification - - - 90.0
Id. + Classification 84.0 75.3 79.4
CORE Id. 92.0 83.3 87.4
ARGs Classification - - - 90.5
Id. + Classification 86.4 78.4 82.2
Table 7: Performance degradation when using automatic
parses instead of hand-corrected ones.
report our performance numbers on this data set. Table 8
shows the same information as in previous Tables 6 and
7, but generated using the new data. Owing to time limi-
tations, we could not get the results on the argument iden-
tification task and the combined argument identification
and classification task using automatic parses.
ALL ARGs Task P R F1 A
(%) (%) (%)
HAND Id. 96.2 95.8 96.0
Classification - - - 93.0
Id. + Classification 89.9 89.0 89.4
AUTOMATIC Classification - - - 90.1
Table 8: Best system performance on all tasks using
hand-corrected parses using the latest PropBank data.
12 Feature Analysis
In analyzing the performance of the system, it is useful
to estimate the relative contribution of the various feature
sets used. Table 9 shows the argument classification ac-
curacies for combinations of features on the training and
test data, using hand-corrected parses, for all PropBank
arguments.
Features Accuracy
(%)
All 91.0
All except Path 90.8
All except Phrase Type 90.8
All except HW and HW -POS 90.7
All except All Phrases ?83.6
All except Predicate ?82.4
All except HW and FW and LW -POS ?75.1
Path, Predicate 74.4
Path, Phrase Type 47.2
Head Word 37.7
Path 28.0
Table 9: Performance of various feature combinations on
the task of argument classification.
In the upper part of Table 9 we see the degradation in
performance by leaving out one feature or a feature fam-
ily at a time. After the addition of all the new features,
it is the case that removal of no individual feature except
predicate degrades the classification performance signifi-
cantly, as there are some other features that provide com-
plimentary information. However, removal of predicate
information hurts performance significantly, so does the
removal of a family of features, eg., all phrase types, or
the head word (HW), first word (FW) and last word (LW)
information. The lower part of the table shows the per-
formance of some feature combinations by themselves.
Table 10 shows the feature salience on the task of ar-
gument identification. One important observation we can
make here is that the path feature is the most salient fea-
ture in the task of argument identification, whereas it is
the least salient in the task of argument classification. We
could not provide the numbers for argument identifica-
tion performance upon removal of the path feature since
that made the SVM training prohibitively slow, indicating
that the SVM had a very hard time separating the NULL
class from the NON-NULL class.
Features P R F1
(%) (%)
All 95.2 92.5 93.8
All except HW 95.1 92.3 93.7
All except Predicate 94.5 91.9 93.2
Table 10: Performance of various feature combinations
on the task of argument identification
13 Comparing Performance with Other
Systems
We compare our system against 4 other shallow semantic
parsers in the literature. In comparing systems, results are
reported for all the three types of tasks mentioned earlier.
13.1 Description of the Systems
The Gildea and Palmer (G&P) System.
The Gildea and Palmer (2002) system uses the same
features and the same classification mechanism used by
G&J. These results are reported on the December 2001
release of PropBank.
The Surdeanu et al System.
Surdeanu et al (2003) report results on two systems
using a decision tree classifier. One that uses exactly the
same features as the G&J system. We call this ?Surdeanu
System I.? They then show improved performance of an-
other system ? ?Surdeanu System II,? which uses some
additional features. These results are are reported on the
July 2002 release of PropBank.
The Gildea and Hockenmaier (G&H) System
The Gildea and Hockenmaier (2003) system uses fea-
tures extracted from Combinatory Categorial Grammar
(CCG) corresponding to the features that were used by
G&J and G&P systems. CCG is a form of dependency
grammar and is hoped to capture long distance relation-
ships better than a phrase structure grammar. The fea-
tures are combined using the same algorithm as in G&J
and G&P. They use a slightly newer ? November 2002 re-
lease of PropBank. We will refer to this as ?G&H System
I?.
The Chen and Rambow (C&R) System
Chen and Rambow report on two different systems,
also using a decision tree classifier. The first ?C&R Sys-
tem I? uses surface syntactic features much like the G&P
system. The second ?C&R System II? uses additional
syntactic and semantic representations that are extracted
from a Tree Adjoining Grammar (TAG) ? another gram-
mar formalism that better captures the syntactic proper-
ties of natural languages.
Classifier Accuracy
(%)
SVM 88
Decision Tree (Surdeanu et al, 2003) 79
Gildea and Palmer (2002) 77
Table 11: Argument classification using same features
but different classifiers.
13.2 Comparing Classifiers
Since two systems, in addition to ours, report results us-
ing the same set of features on the same data, we can
directly assess the influence of the classifiers. G&P sys-
tem estimates the posterior probabilities using several dif-
ferent feature sets and interpolate the estimates, while
Surdeanu et al (2003) use a decision tree classifier. Ta-
ble 11 shows a comparison between the three systems for
the task of argument classification.
13.3 Argument Identification (NULL vs NON-NULL)
Table 12 compares the results of the task of identify-
ing the parse constituents that represent semantic argu-
ments. As expected, the performance degrades consider-
ably when we extract features from an automatic parse as
opposed to a hand-corrected parse. This indicates that the
syntactic parser performance directly influences the argu-
ment boundary identification performance. This could be
attributed to the fact that the two features, viz., Path and
Head Word that have been seen to be good discriminators
of the semantically salient nodes in the syntax tree, are
derived from the syntax tree.
Classes System Hand Automatic
P R F1 P R F1
ALL SVM 95 92 94 89 83 86
ARGs Surdeanu System II - - 89 - - -
Surdeanu System I 85 84 85 - - -
Table 12: Argument identification
13.4 Argument Classification
Table 13 compares the argument classification accuracies
of various systems, and at various levels of classification
granularity, and parse accuracy. It can be seen that the
SVM System performs significantly better than all the
other systems on all PropBank arguments.
Classes System Hand Automatic
Accuracy Accuracy
ALL SVM 91 90
ARGs G&P 77 74
Surdeanu System II 84 -
Surdeanu System I 79 -
CORE SVM 93.9 90.5
ARGs C&R System II 93.5 -
C&R System I 92.4 -
Table 13: Argument classification
13.5 Argument Identification and Classification
Table 14 shows the results for the task where the system
first identifies candidate argument boundaries and then
labels them with the most likely argument. This is the
hardest of the three tasks outlined earlier. SVM does a
very good job of generalizing in both stages of process-
ing.
Classes System Hand Automatic
P R F1 P R F1
ALL SVM 89 85 87 84 75 79
ARGs G&H System I 76 68 72 71 63 67
G&P 71 64 67 58 50 54
CORE SVM System 90 87 89 86 78 82
ARGs G&H System I 82 79 80 76 73 75
C&R System II - - - 65 75 70
Table 14: Identification and classification
14 Generalization to a New Text Source
Thus far, in all experiments our unseen test data was
selected from the same source as the training data.
In order to see how well the features generalize to
texts drawn from a similar source, we used the classifier
trained on PropBank training data to test data drawn from
the AQUAINT corpus (LDC, 2002). We annotated 400
sentences from the AQUAINT corpus with PropBank
arguments. This is a collection of text from the New
York Times Inc., Associated Press Inc., and Xinhua
News Service (PropBank by comparison is drawn from
Wall Street Journal). The results are shown in Table 15.
Task P R F1 A
(%) (%) (%)
ALL Id. 75.8 71.4 73.5 -
ARGs Classification - - - 83.8
Id. + Classification 65.2 61.5 63.3 -
CORE Id. 88.4 74.4 80.8 -
ARGs Classification - - - 84.0
Id. + Classification 75.2 63.3 68.7 -
Table 15: Performance on the AQUAINT test set.
There is a significant drop in the precision and recall
numbers for the AQUAINT test set (compared to the pre-
cision and recall numbers for the PropBank test set which
were 84% and 75% respectively). One possible reason
for the drop in performance is relative coverage of the
features on the two test sets. The head word, path and
predicate features all have a large number of possible val-
ues and could contribute to lower coverage when moving
from one domain to another. Also, being more specific
they might not transfer well across domains.
Features Arguments non-Arguments
(%) (%)
Predicate, Path 87.60 2.91
Predicate, Head Word 48.90 26.55
Cluster, Path 96.31 4.99
Cluster, Head Word 83.85 60.14
Path 99.13 15.15
Head Word 93.02 90.59
Table 16: Feature Coverage on PropBank test set using
parser trained on PropBank training set.
Features Arguments non-Arguments
(%) (%)
Predicate, Path 62.11 4.66
Predicate, Head Word 30.26 17.41
Cluster, Path 87.19 10.68
Cluster, Head Word 65.82 45.43
Path 96.50 29.26
Head Word 84.65 83.54
Table 17: Coverage of features on AQUAINT test set us-
ing parser trained on PropBank training set.
Table 16 shows the coverage for features on the hand-
corrected PropBank test set. The tables show feature
coverage for constituents that were Arguments and con-
stituents that were NULL. About 99% of the predicates in
the AQUAINT test set were seen in the PropBank train-
ing set. Table 17 shows coverage for the same features on
the AQUAINT test set. We believe that the drop in cover-
age of the more predictive feature combinations explains
part of the drop in performance.
15 Conclusions
We have described an algorithm which significantly im-
proves the state-of-the-art in shallow semantic parsing.
Like previous work, our parser is based on a supervised
machine learning approach. Key aspects of our results
include significant improvement via an SVM classifier,
improvement from new features and a series of analytic
experiments on the contributions of the features. Adding
features that are generalizations of the more specific fea-
tures seemed to help. These features were named enti-
ties, head word part of speech and verb clusters. We also
analyzed the transferability of the features to a new text
source.
We would like to thank Ralph Weischedel and Scott Miller of
BBN Inc. for letting us use their named entity tagger ? Iden-
tiFinder; Martha Palmer for providing us with the PropBank
data, Valerie Krugler for tagging the AQUAINT test set with
PropBank arguments, and all the anonymous reviewers for their
helpful comments.
References
[Bikel et al1999] Daniel M. Bikel, Richard Schwartz, and Ralph M. Weischedel.
1999. An algorithm that learns what?s in a name. Machine Learning, 34:211?
231.
[Blaheta and Charniak2000] Don Blaheta and Eugene Charniak. 2000. Assigning
function tags to parsed text. In NAACL, pages 234?240.
[Chaniak2001] Eugene Chaniak. 2001. Immediate-head parsing for language
models. In ACL-01.
[Chen and Rambow2003] John Chen and Owen Rambow. 2003. Use of deep
linguistics features for the recognition and labeling of semantic arguments.
EMNLP-03.
[Collins1999] Michael John Collins. 1999. Head-driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania,
Philadelphia.
[Daniel et al1992] K. Daniel, Y. Schabes, M. Zaidel, and D. Egedi. 1992. A freely
available wide coverage morphological analyzer for English. In COLING-92.
[Fleischman and Hovy2003] Michael Fleischman and Eduard Hovy. 2003. A
maximum entropy approach to framenet tagging. In HLT-03.
[Gildea and Hockenmaier2003] Dan Gildea and Julia Hockenmaier. 2003. Identi-
fying semantic roles using combinatory categorial grammar. In EMNLP-03.
[Gildea and Jurafsky2000] Daniel Gildea and Daniel Jurafsky. 2000. Automatic
labeling of semantic roles. In ACL-00, pages 512?520.
[Gildea and Jurafsky2002] Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguistics, 28(3):245?288.
[Gildea and Palmer2002] Daniel Gildea and Martha Palmer. 2002. The necessity
of syntactic parsing for predicate argument recognition. In ACL-02.
[Hacioglu and Ward2003] Kadri Hacioglu and Wayne Ward. 2003. Target word
detection and semantic role chunking using support vector machines. In HLT-
03.
[Hacioglu et al2003] Kadri Hacioglu, Sameer Pradhan, Wayne Ward, James Mar-
tin, and Dan Jurafsky. 2003. Shallow semantic parsing using support vector
machines. Technical Report TR-CSLR-2003-1, Center for Spoken Language
Research, Boulder, Colorado.
[Hofmann and Puzicha1998] Thomas Hofmann and Jan Puzicha. 1998. Statistical
models for co-occurrence data. Memo, MIT AI Laboratory.
[Kingsbury et al2002] Paul Kingsbury, Martha Palmer, and Mitch Marcus. 2002.
Adding semantic annotation to the Penn Treebank. In HLT-02.
[Kudo and Matsumoto2000] Taku Kudo and Yuji Matsumoto. 2000. Use of sup-
port vector learning for chunk identification. In CoNLL-00.
[Kudo and Matsumoto2001] Taku Kudo and Yuji Matsumoto. 2001. Chunking
with support vector machines. In NAACL-01.
[LDC2002] LDC. 2002. The AQUAINT Corpus of English News Text, Catalog
no. LDC2002t31.
[Lin1998] Dekang Lin. 1998. Automatic retrieval and clustering of similar words.
In COLING-98.
[Magerman1994] David Magerman. 1994. Natural Language Parsing as Statisti-
cal Pattern Recognition. Ph.D. thesis, Stanford University, CA.
[Marcus et al1994] Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz,
Robert MacIntyre, Ann Bies, Mark Ferguson, Karen Katz, and Britta Schas-
berger. 1994. The Penn TreeBank: Annotating predicate argument structure.
[Platt2000] John Platt. 2000. Probabilities for support vector machines. In
A. Smola, P. Bartlett, B. Scolkopf, and D. Schuurmans, editors, Advances in
Large Margin Classifiers. MIT press.
[Pradhan et al2003] Sameer Pradhan, Kadri Hacioglu, Wayne Ward, James Mar-
tin, and Dan Jurafsky. 2003. Semantic role parsing: Adding semantic struc-
ture to unstructured text. In ICDM-03.
[Surdeanu et al2003] Mihai Surdeanu, Sanda Harabagiu, John Williams, and Paul
Aarseth. 2003. Using predicate-argument structures for information extrac-
tion. In ACL-03.
[Thompson et al2003] Cynthia A. Thompson, Roger Levy, and Christopher D.
Manning. 2003. A generative model for semantic role labeling. In ECML-03.
A Lightweight Semantic Chunking Model Based On Tagging 
 
 
Kadri Hacioglu 
Center for Spoken Language Research,  
University of Colorado, Boulder 
hacioglu@cslr.colorado.edu 
 
 
 
 
 
 
Abstract  
In this paper, a framework for the develop-
ment of a fast, accurate, and highly portable 
semantic chunker is introduced. The frame-
work is based on a non-overlapping, shallow 
tree-structured language. The derivation of the 
tree is considered as a sequence of tagging ac-
tions in a predefined linguistic context, and a 
novel semantic chunker is accordingly devel-
oped. It groups the phrase chunks into the ar-
guments of a given predicate in a bottom-up 
fashion. This is quite different from current 
approaches to semantic parsing or chunking 
that depend on full statistical syntactic parsers 
that require tree bank style annotation. We 
compare it with a recently proposed word-by-
word semantic chunker and present results 
that show that the phrase-by-phrase approach 
performs better than its word-by-word coun-
terpart.  
1 Introduction 
Semantic representation, and, obviously, its extraction 
from an input text, are very important for several natural 
language processing tasks; namely, information extrac-
tion, question answering, summarization, machine trans-
lation and dialog management. For example, in question 
answering systems, semantic representations can be 
used to understand the user?s question, expand the 
query, find relevant documents and present a summary 
of multiple documents as the answer.   
Semantic representations are often defined as a 
collection of frames with a number of slots for each 
frame to represent the task structure and domain objects. 
This frame-based semantic representation has been 
successfully used in many limited-domain tasks. For 
                                                          
  This work is supported by the ARDA AQUAINT pro-
gram via contract OCG4423B and by the NSF via grant ISS-
9978025 
fully used in many limited-domain tasks. For example, 
in a spoken dialog system designed for travel planning 
one might have an Air frame with slots Origin, Destina-
tion, Depart_date, Airline etc. The drawback of this 
domain specific representation is the high cost to 
achieve adequate coverage in a new domain. A new set 
of frames and slots are needed when the task is extended 
or changed. Authoring the patterns that instantiate those 
frames is time consuming and expensive. 
Domain independent semantic representations can 
overcome the poor portability of domain specific repre-
sentations. A natural candidate for this representation is 
the predicate-argument structure of a sentence that ex-
ists in most languages. In this structure, a word is speci-
fied as a predicate and a number of word groups are 
considered as arguments accompanying the predicate. 
Those arguments are assigned different semantic cate-
gories depending on the roles that they play with respect 
to the predicate. Researchers have used several different 
sets of argument labels. One possibility are the non-
mnemonic labels used in the PropBank corpus (Kings-
bury and Palmer, 2002): ARG0, ARG1, ?, ARGM-
LOC, etc. An alternative set are thematic roles similar 
to those proposed in (Gildea and Jurafsky, 2002): 
AGENT, ACTOR, BENEFICIARY, CAUSE, etc. 
Shallow semantic parsing with the goal of creating a 
domain independent meaning representation based on 
predicate/argument structure was first explored in detail 
by (Gildea and Jurafsky, 2002). Since then several vari-
ants of the basic approach have been introduced using 
different features and different classifiers based on vari-
ous machine-learning methods (Gildea and Palmer, 
2002;.Gildea and Hockenmaier, 2003; Surdeanu et. al., 
2003;  Chen and Rambow, 2003; Fleischman and Hovy, 
2003; Hacioglu and Ward, 2003; Thompson et. al., 2003 
; Pradhan et. al., 2003). Large semantically annotated 
databases, like FrameNet (Baker et.al, 1998) and Prop-
Bank (Kingsbury and Palmer, 2002) have been used to 
train and test the classifiers. Most of these approaches 
can be divided into two broad classes: Constituent-by-
Constituent  (C-by-C) or Word-by-Word (W-by-W) 
classifiers. In C-by-C classification, the syntactic tree 
 
 
Figure 1. Proposed non-overlapping, shallow lexicalized syntactic/semantic tree structure 
 
representation of a sentence is linearized into a sequence 
of its syntactic constituents (non-terminals). Then each 
constituent is classified into one of several arguments or 
semantic roles using a number of features derived from 
its respective context. In the W-by-W method (Hacioglu 
and Ward, 2003) the problem is formulated as a chunk-
ing task and the features are derived for each word (as-
suming part of speech tags and syntactic phrase chunks 
are available), and the word is classified into one of the 
semantic labels using an IOB2 representation. Among 
those methods, only the W-by-W method considered 
semantic classification with features created in a bot-
tom-up manner. The motivations for bottom-up analysis 
are  
? Full syntactic parsing is computationally expen-
sive 
? Taggers and chunkers are fast 
? Not all languages have full syntactic parsers 
? The annotation effort required for a full syntactic 
parser is larger than that required for taggers and 
chunkers. 
In this paper, we propose a non-overlapping shallow 
tree structure, at lexical, syntactic and semantic levels to 
represent the language. The goal is to improve the port-
ability of semantic processing to other applications, 
domains and languages. The new structure is complex 
enough to capture crucial (non-exclusive) semantic 
knowledge for intended applications and simple enough 
to allow flat, easier and fast annotation. The human ef-
fort required for flat labeling is significantly less than 
that required for creating tree bank style labels. We pre-
sent a particular derivation of the structure yielding a 
lightweight machine learned semantic chunker.  
2 Representation of Language 
We assume a flat, non-overlapping (or chunked) repre-
sentation of language at the lexical, syntactic and se-
mantic levels. In this representation a sentence is a 
sequence of base phrases at a syntactic level.  A base 
phrase is a phrase that does not dominate another 
phrase.  At a semantic level, the chosen predicate has a 
number of arguments attached to it. The arguments are 
filled by a sequence of base phrases that span sequences 
of words tagged with their part of speech. We propose 
to organize this flat structure in a lexicalized tree as il-
lustrated in Fig 1. The root is the standard non-terminal 
S lexicalized with the predicate. One level below, argu-
ments attached to the predicate are organized in a flat 
structure and lexicalized with headwords. The next level 
is organized in terms of the syntactic chunks spanned by 
each argument. The lower levels consist of the part of 
speech tags and the words. The lower level can also be 
extended to include flat morphological representations 
of words to deal with morphologically rich languages 
like Arabic, Korean and Turkish.  One can introduce a 
relatively deeper structure using a small set of rules at 
the phrasal level under each semantic non-terminal. For 
example, the application of simple rules in order on 
THEME?s chunks, such as (1) combine flat PP NP into 
a right branching PP and then  (2) combine flat NP with 
PP into a recursive NP, will result in a relatively deeper 
tree. Although the main focus of the paper is on the 
structure presented in Figure 1, we note that a deeper 
structure obtained by using a small number of simple 
hand-crafted rules on syntactic chunks (applied in a 
bottom-up manner) is worthy of further research. 
3 Model for Tree Decomposition 
The tree structure introduced in the preceding section 
can be generated as a unique sequence of derivation 
actions in many different ways. We propose a model 
that decomposes the tree into a sequence of tagging ac-
tions at the word, phrase and argument levels. In this 
model the procedure is a bottom up derivation of the 
tree that is accomplished in several steps. Each step 
consists of a number of actions. The first step is a se-
quence of actions to tag the words with their Part-Of-
Speech (POS). Then the words are tagged as inside a 
phrase (I), outside a phrase (O) or beginning of a phrase 
(B) (Ramhsaw and Marcus, 1995). For example, in Fig-
ure 1, the word For is tagged as B-PP, fiscal is tagged as 
B-NP, 1989 is tagged as I-NP, etc.  This step is followed 
by a sequence of join actions. A sequence that starts 
with a B-tag and continues with zero or more I-tags of 
the same type is joined into a single tag that represents 
the type of the phrase (e.g. NP, PP etc.). The next step 
tags phrases as inside an argument, outside an argument 
or beginning of an argument. Finally, we join IOB ar-
gument tags as we did for base phrases.  
4 Parsing Strategy 
The parse strategy based on the tagging actions consists 
of t  
inpu
men  
sem
pon  
inpu , 
and
 
that
fine
wor
wor
app
(SV
to l
stag
text
feat
tags
use
sim
the 
gro
sho
 
 
 
 
 
 
 
 
 
 
alon
bas
the 
phr
usin
dec
      
thei
with
increasing the context window, adding new sentence 
level and predicate dependent features, and introducing 
alternate organizations of the input. An alternative to 
our approach is the W-by-W approach proposed in (Ha-
cioglu and Ward, 2003).  We show it below:   
 
 
 
 
 
 
 
 
 
 
 
 
Here the labeling is carried out in a word-by-word 
basis. We note that the Phrase-by-Phrase (P-by-P) tag-
ging classifies larger units, ignores some of the words 
For       IN     B-PP B-TEMPORAL 
    fiscal     JJ      B-NP    I-TEMPORAL 
1989               CD    I-NP               ?? 
Mr.               NNP   B-NP 
McGovern     NNP   I-NP 
    received      VBD    B-VP 
a           DT       B-NP    
salary           ` NN  I-NP 
of                    PP     B-PP 
877,663          CD         B-NP 
 
context 
current  
decision hree components that are sequentially applied to the
t text for a chosen predicate to determine its argu-
ts. These components are POS, base phrase and
antic taggers/chunkers. In the following, each com-
ent will be described along the dimensions of its (i)
t, (ii) decision context, (ii) features, (iv) classifier
 (v) output. 
In the first stage, the input is the sequence of words
 are processed from left-to-right. The context is de-
d to be a fixed-size window centered around the 
d in focus. The features are derived from a set of 
d specific features and previous tag decisions that 
ear in the context. A Support Vector Machine 
M) (Vapnik, 1995) as a multi-class classifier is used 
abel words with their POS tags1.  In the second 
e, the input is the sequence of word/tag pairs. Con-
 is defined in the same way as in the first stage. The 
ures are the word/tag pairs and previous phrase IOB 
 that appear in the context. An SVM classifier is 
d to classify the base phrase IOB label. This is very 
ilar to the set up in (Kudo and Matsumato, 2000). In 
last stage (the major contribution of the paper) we 
up the input, context, features and decisions as 
wn below.  
The input is the base-phrase labels and headwords 
g with their part of speech tags and positions in the 
e phrase. The context is ?2/+2 window centered at 
base phrase in question. An SVM classifies the base 
ase into semantic role tags in an IOB representation 
g a context including the two previous semantic tag 
isions.  It is possible to enrich the set of features by 
                                                    
1 Although not limited to, SVMs are selected because of 
r ability to manage a large number of overlapping features 
 a good generalization performance.  
 
(modifiers), uses effectively a wider linguistic context 
for a given window size and performs tagging in a 
smaller number of steps.   
5 Experiments 
All experiments were carried out using sections 15-18  
of the PropBank data holding out Section-00 and Sec-
tion-23 for development and test, respectively. We used 
chunklink 2  to flatten syntactic trees. Then using the  
predicate argument annotation we obtained a new cor-
pus of  the tree structure introduced in Section 2.   
All SVM classifiers, for POS tagging, syntactic 
phrase chunking and semantic argument labeling, were 
realized using the TinySVM3 with the polynomial ker-
nel of degree 2 and the general purpose SVM based 
chunker YamCha4 . The results were evaluated using 
precision and recall numbers along with the F metric. 
Table 1 compares W-by-W and P-by-P approaches.  
The base features described in Section 4 along with two 
additional predicate specific features were used; the 
lemma of the predicate and a binary feature that indi-
cates the word is before or after the predicate.  
 
Table 1. Performance comparisons 
Method Precision Recall F1 
W-by-W 58% (60%) 49% (52%) 53% (56%) 
P-by-P 63% (66%) 56% (59%) 59% (62%) 
 
In these experiments the accuracy of the POS tagger 
was 95.5% and the F-metric of the phrase chunker was 
94.5%. The figures in parantheses are for gold standard 
                                                          
2 http://ilk.uvt.nl/~sabine/chunklink 
3 http://cl.aist-nara.ac.jp/~taku-ku/software/TinySVM 
4 http://cl.aist-nara.ac.jp/~taku-ku/software/yamcha 
current  
decision 
PP    For  IN          B-PP B-TEMPORAL 
NP   1989  CD      I-NP I-TEMPORAL 
NP   McGovern NNP I-NP                ??  
VP   received VBD B-VP  
NP   salary NN         I-NP 
PP    of  IN B-PP 
NP    877,663 CD B-NP       
  
context
(i.e. POS and phrase features are derived from hand-
annotated trees). The others show the performance of 
the sequential bottom-up tagging scheme that we have 
described in section 4.  We experimented with a reduced 
set of  PropBank arguments. The set contains the most 
frequent 19 arguments in the corpus.  
It is interesting to note that there is a huge drop in 
performance for ?chunked? semantic analysis as com-
pared with the performances at mid 90s for the syntactic 
and lexical analyses. This clearly shows that the extrac-
tion of even ?chunked? semantics of a text is a very 
difficult task and still a lot remains to be done to bridge 
the gap.  This is partly due to the difficulty of having 
consistent semantic annotations, partly due to the miss-
ing information/features for word senses and usages, 
partly due to the absence of world knowledge and partly 
due to the relatively small size of the training set. Our 
other experiments clearly show that with more training 
data and additional features it is possible to improve the 
performance by 10-15% absolute (Hacioglu et. al., 
2004). The feature engineering for semantic chunking is 
open-ended and the discussion of it is beyond the scope 
of the short paper.  Here, we have illustrated that the P-
by-P approach is a promising alternative to the recently 
proposed W-by-W approach (Hacioglu and Ward, 
2003).  
6 Conclusions 
We have developed a novel phrase-by-phrase semantic 
chunker based on a non-overlapping (or chunked) shal-
low language structure at lexical, syntactic and semantic 
levels. We have implemented a baseline system and 
compared it to a recently proposed word-by-word sys-
tem. We have shown better performance with the 
phrase-by-phrase approach. It has been also pointed out 
that the new method has several advantages; it classifies 
larger units, uses wider context, runs faster. Prior work 
has not considered this bottom-up strategy for semantic 
chunking, which we claim yields a lightweight, fast, and 
robust chunker at moderately high performance. Al-
though we have flattened the trees in the PropBank cor-
pus for our experiments, the proposed language 
structure supports flat annotation from scratch, which 
we believe is useful for porting the method to other do-
mains and languages. While our initial results have been 
encouraging, this work must be extended and enhanced 
to produce the quality of semantic parse produced by 
systems using a full syntactic parse.   
References 
Collin F. Baker, Charles J. Fillmore, and John B. Lowe 
1998. The Berkley FrameNet Project. Proceedings of  
Coling-ACL, pp. 86-90. 
John Chen and Owen Rambow. 2003. Use of Deep Lin-
guistic Features for the Recognition and Labeling of 
Semantic Arguments. In Proceedings of EMNLP-
2003, Sapporo, Japan.  
Daniel Gildea and Daniel Jurafsky. 2002. Automatic 
Labeling of Semantic Roles. Computational Linguis-
tics, 28:3, pages 245-288. 
Daniel Gildea  and Martha Palmer. 2002. The necessity 
of syntactic parsing for predicate argument recogni-
tion. In Proceedings of ACL?02. 
Daniel Gildea and Julia Hockenmaier. 2003. Identifying 
Semantic Roles Using Combinatory Categorical 
Grammar. In Proceedings of EMNL?03, Japan. 
Micheal Fleischman and Eduard Hovy. 2003. A Maxi-
mum Entropy Approach to FrameNet Tagging. Pro-
ceedings of  HLT/NAACL-03. 
Kadri Hacioglu and Wayne Ward. 2003. Target word 
Detection and semantic role chunking using support 
vector machines.  Proceedings of  HLT/NAACL-03. 
Kadri Hacioglu, Sameer Pradhan, Wayne Ward, James 
H. Martin and Daniel Jurafsky. 2004. Semantic Role 
Labeling by Tagging Syntactic Chunks. CONLL-
2004 Shared Task. 
Paul Kingsbury, Martha Palmer, 2002. From TreeBank 
to PropBank. Conference on Language Resources 
and Evaluation LREC-2002. 
Taku Kudo, Yuji Matsumato. 2000. Use of support vec-
tor learning for chunk identification. Proc. of the 4th 
Conference on Very Large Corpora, pp. 142-144. 
Sameer Pradhan, Kadri Hacioglu, Wayne Ward, James 
H. Martin, Dan Jurafsky.2003. Semantic Role Pars-
ing: Adding Semantic Structure to Unstructured Text. 
In Proceedings of ICDM 2003, Melbourne, Florida. 
Lance E. Ramhsaw and Mitchell P. Marcus. 1995. Text 
Chunking Using Transformation Based Learning.  
Proceedings of  the 3rd ACL  Workshop on  Very 
Large Corpora, pages 82-94. 
Mihai Surdeanu, Sanda Harabagiu, John Williams, and 
Paul Aarseth. 2003. Using Predicate-Argument 
Structure for Information Extraction. Proceedings of 
the 41th Annual Conference on the Association for 
Computational Linguistics (ACL-03). 
Cynthia A. Thompson, Roger Levy, and Christopher D. 
Manning. 2003. A Generative Model for Semantic 
Role Labeling. Proc. of the European Conference on 
Machine Learning (ECML-03). 
Vladamir Vapnik 1995. The Nature of Statistical Learn-
ing Theory. Springer Verlag, New York, USA. 
Automatic Tagging of Arabic Text: From Raw Text to Base Phrase Chunks  
Mona Diab
Linguistics Department
Stanford University
mdiab@stanford.edu
Kadri Hacioglu
Center for Spoken Language Research
University of Colorado, Boulder
hacioglu@colorado.edu
Daniel Jurafsky
Linguistics Department
Stanford University
jurafsky@stanford.edu
Abstract
To date, there are no fully automated systems
addressing the community?s need for funda-
mental language processing tools for Arabic
text. In this paper, we present a Support Vector
Machine (SVM) based approach to automati-
cally tokenize (segmenting off clitics), part-of-
speech (POS) tag and annotate base phrases
(BPs) in Arabic text. We adapt highly accu-
rate tools that have been developed for En-
glish text and apply them to Arabic text. Using
standard evaluation metrics, we report that the
SVM-TOK tokenizer achieves an

score
of 99.12, the SVM-POS tagger achieves an ac-
curacy of 95.49%, and the SVM-BP chunker
yields an


score of 92.08.
1 Introduction
Arabic is garnering attention in the NLP community due
to its socio-political importance and its linguistic differ-
ences from Indo-European languages. These linguistic
characteristics, especially dialect differences and com-
plex morphology present interesting challenges for NLP
researchers. But like most non-European languages, Ara-
bic is lacking in annotated resources and tools. Fully au-
tomated fundamental NLP tools such as Tokenizers, Part
Of Speech (POS) Taggers and Base Phrase (BP) Chun-
kers are still not available for Arabic. Meanwhile, these
tools are readily available and have achieved remarkable
accuracy and sophistication for the processing of many
European languages. With the release of the Arabic
Penn TreeBank 1 (v2.0),1 the story is about to
change.
In this paper, we propose solutions to the problems of
Tokenization, POS Tagging and BP Chunking of Arabic
text. By Tokenization we mean the process of segmenting
clitics from stems, since in Arabic, prepositions, conjunc-
tions, and some pronouns are cliticized (orthographically

This work was partially supported by the National
Science Foundation via a KDD Supplement to NSF
CISE/IRI/Interactive Systems Award IIS-9978025.
1http://www.ldc.upenn.edu/
and phonological fused) onto stems. Separating conjunc-
tions from the following noun, for example, is a key first
step in parsing. By POS Tagging, we mean the standard
problem of annotating these segmented words with parts
of speech drawn from the ?collapsed? Arabic Penn
TreeBank POS tagset. Base Phrase (BP) Chunking is
the process of creating non-recursive base phrases such
as noun phrases, adjectival phrases, verb phrases, prepo-
sition phrases, etc. For each of these tasks, we adopt
a supervised machine learning perspective using Sup-
port Vector Machines (SVMs) trained on the Arabic
TreeBank, leveraging off of already existing algorithms
for English. The results are comparable to state-of-the-art
results on English text when trained on similar sized data.
2 Arabic Language and Data
Arabic is a Semitic language with rich templatic mor-
phology. An Arabic word may be composed of a stem
(consisting of a consonantal root and a template), plus
affixes and clitics. The affixes include inflectional mark-
ers for tense, gender, and/or number. The clitics include
some (but not all) prepositions, conjunctions, determin-
ers, possessive pronouns and pronouns. Some are pro-
clitic ( attaching to the beginning of a stem) and some
enclitics (attaching to the end of a stem). The following
is an example of the different morphological segments
in the word 	
  which means and by their virtues.
Arabic is read from right to left hence the directional
switch in the English gloss.
enclitic affix stem proclitic proclitic
Arabic: Proceedings of the 43rd Annual Meeting of the ACL, pages 581?588,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Semantic Role Labeling Using Different Syntactic Views?
Sameer Pradhan, Wayne Ward,
Kadri Hacioglu, James H. Martin
Center for Spoken Language Research,
University of Colorado,
Boulder, CO 80303
{spradhan,whw,hacioglu,martin}@cslr.colorado.edu
Daniel Jurafsky
Department of Linguistics,
Stanford University,
Stanford, CA 94305
jurafsky@stanford.edu
Abstract
Semantic role labeling is the process of
annotating the predicate-argument struc-
ture in text with semantic labels. In this
paper we present a state-of-the-art base-
line semantic role labeling system based
on Support Vector Machine classifiers.
We show improvements on this system
by: i) adding new features including fea-
tures extracted from dependency parses,
ii) performing feature selection and cali-
bration and iii) combining parses obtained
from semantic parsers trained using dif-
ferent syntactic views. Error analysis of
the baseline system showed that approx-
imately half of the argument identifica-
tion errors resulted from parse errors in
which there was no syntactic constituent
that aligned with the correct argument. In
order to address this problem, we com-
bined semantic parses from a Minipar syn-
tactic parse and from a chunked syntac-
tic representation with our original base-
line system which was based on Charniak
parses. All of the reported techniques re-
sulted in performance improvements.
1 Introduction
Semantic Role Labeling is the process of annotat-
ing the predicate-argument structure in text with se-
?This research was partially supported by the ARDA
AQUAINT program via contract OCG4423B and by the NSF
via grants IS-9978025 and ITR/HCI 0086132
mantic labels (Gildea and Jurafsky, 2000; Gildea
and Jurafsky, 2002; Gildea and Palmer, 2002; Sur-
deanu et al, 2003; Hacioglu and Ward, 2003; Chen
and Rambow, 2003; Gildea and Hockenmaier, 2003;
Pradhan et al, 2004; Hacioglu, 2004). The architec-
ture underlying all of these systems introduces two
distinct sub-problems: the identification of syntactic
constituents that are semantic roles for a given pred-
icate, and the labeling of the those constituents with
the correct semantic role.
A detailed error analysis of our baseline system
indicates that the identification problem poses a sig-
nificant bottleneck to improving overall system per-
formance. The baseline system?s accuracy on the
task of labeling nodes known to represent semantic
arguments is 90%. On the other hand, the system?s
performance on the identification task is quite a bit
lower, achieving only 80% recall with 86% preci-
sion. There are two sources of these identification
errors: i) failures by the system to identify all and
only those constituents that correspond to semantic
roles, when those constituents are present in the syn-
tactic analysis, and ii) failures by the syntactic ana-
lyzer to provide the constituents that align with cor-
rect arguments. The work we present here is tailored
to address these two sources of error in the identifi-
cation problem.
The remainder of this paper is organized as fol-
lows. We first describe a baseline system based on
the best published techniques. We then report on
two sets of experiments using techniques that im-
prove performance on the problem of finding argu-
ments when they are present in the syntactic analy-
sis. In the first set of experiments we explore new
581
features, including features extracted from a parser
that provides a different syntactic view ? a Combi-
natory Categorial Grammar (CCG) parser (Hocken-
maier and Steedman, 2002). In the second set of
experiments, we explore approaches to identify opti-
mal subsets of features for each argument class, and
to calibrate the classifier probabilities.
We then report on experiments that address the
problem of arguments missing from a given syn-
tactic analysis. We investigate ways to combine
hypotheses generated from semantic role taggers
trained using different syntactic views ? one trained
using the Charniak parser (Charniak, 2000), another
on a rule-based dependency parser ? Minipar (Lin,
1998), and a third based on a flat, shallow syntactic
chunk representation (Hacioglu, 2004a). We show
that these three views complement each other to im-
prove performance.
2 Baseline System
For our experiments, we use Feb 2004 release of
PropBank1 (Kingsbury and Palmer, 2002; Palmer
et al, 2005), a corpus in which predicate argument
relations are marked for verbs in the Wall Street
Journal (WSJ) part of the Penn TreeBank (Marcus
et al, 1994). PropBank was constructed by as-
signing semantic arguments to constituents of hand-
corrected TreeBank parses. Arguments of a verb
are labeled ARG0 to ARG5, where ARG0 is the
PROTO-AGENT, ARG1 is the PROTO-PATIENT, etc.
In addition to these CORE ARGUMENTS, additional
ADJUNCTIVE ARGUMENTS, referred to as ARGMs
are also marked. Some examples are ARGM-LOC,
for locatives; ARGM-TMP, for temporals; ARGM-
MNR, for manner, etc. Figure 1 shows a syntax tree
along with the argument labels for an example ex-
tracted from PropBank. We use Sections 02-21 for
training, Section 00 for development and Section 23
for testing.
We formulate the semantic labeling problem as
a multi-class classification problem using Support
Vector Machine (SVM) classifier (Hacioglu et al,
2003; Pradhan et al, 2003; Pradhan et al, 2004)
TinySVM2 along with YamCha3 (Kudo and Mat-
1http://www.cis.upenn.edu/?ace/
2http://chasen.org/?taku/software/TinySVM/
3http://chasen.org/?taku/software/yamcha/
S
hhhh
((((
NP
hhhh
((((
The acquisition
ARG1
VP
```
   
VBD
was
NULL
VP
XXX
VBN
completed
predicate
PP
```
   
in September
ARGM?TMP
[ARG1 The acquisition] was [predicate completed] [ARGM?TMP in September].
Figure 1: Syntax tree for a sentence illustrating the
PropBank tags.
sumoto, 2000; Kudo and Matsumoto, 2001) are used
to implement the system. Using what is known as
the ONE VS ALL classification strategy, n binary
classifiers are trained, where n is number of seman-
tic classes including a NULL class.
The baseline feature set is a combination of fea-
tures introduced by Gildea and Jurafsky (2002) and
ones proposed in Pradhan et al, (2004), Surdeanu et
al., (2003) and the syntactic-frame feature proposed
in (Xue and Palmer, 2004). Table 1 lists the features
used.
PREDICATE LEMMA
PATH: Path from the constituent to the predicate in the parse tree.
POSITION: Whether the constituent is before or after the predicate.
VOICE
PREDICATE SUB-CATEGORIZATION
PREDICATE CLUSTER
HEAD WORD: Head word of the constituent.
HEAD WORD POS: POS of the head word
NAMED ENTITIES IN CONSTITUENTS: 7 named entities as 7 binary features.
PARTIAL PATH: Path from the constituent to the lowest common ancestor
of the predicate and the constituent.
VERB SENSE INFORMATION: Oracle verb sense information from PropBank
HEAD WORD OF PP: Head of PP replaced by head word of NP inside it,
and PP replaced by PP-preposition
FIRST AND LAST WORD/POS IN CONSTITUENT
ORDINAL CONSTITUENT POSITION
CONSTITUENT TREE DISTANCE
CONSTITUENT RELATIVE FEATURES: Nine features representing
the phrase type, head word and head word part of speech of the
parent, and left and right siblings of the constituent.
TEMPORAL CUE WORDS
DYNAMIC CLASS CONTEXT
SYNTACTIC FRAME
CONTENT WORD FEATURES: Content word, its POS and named entities
in the content word
Table 1: Features used in the Baseline system
As described in (Pradhan et al, 2004), we post-
process the n-best hypotheses using a trigram lan-
guage model of the argument sequence.
We analyze the performance on three tasks:
? Argument Identification ? This is the pro-
cess of identifying the parsed constituents in
the sentence that represent semantic arguments
of a given predicate.
582
? Argument Classification ? Given constituents
known to represent arguments of a predicate,
assign the appropriate argument labels to them.
? Argument Identification and Classification ?
A combination of the above two tasks.
ALL ARGs Task P R F1 A
(%) (%) (%)
HAND Id. 96.2 95.8 96.0
Classification - - - 93.0
Id. + Classification 89.9 89.0 89.4
AUTOMATIC Id. 86.8 80.0 83.3
Classification - - - 90.1
Id. + Classification 80.9 76.8 78.8
Table 2: Baseline system performance on all tasks
using hand-corrected parses and automatic parses on
PropBank data.
Table 2 shows the performance of the system us-
ing the hand corrected, TreeBank parses (HAND)
and using parses produced by a Charniak parser
(AUTOMATIC). Precision (P), Recall (R) and F1
scores are given for the identification and combined
tasks, and Classification Accuracy (A) for the clas-
sification task.
Classification performance using Charniak parses
is about 3% absolute worse than when using Tree-
Bank parses. On the other hand, argument identifi-
cation performance using Charniak parses is about
12.7% absolute worse. Half of these errors ? about
7% are due to missing constituents, and the other
half ? about 6% are due to mis-classifications.
Motivated by this severe degradation in argument
identification performance for automatic parses, we
examined a number of techniques for improving
argument identification. We made a number of
changes to the system which resulted in improved
performance. The changes fell into three categories:
i) new features, ii) feature selection and calibration,
and iii) combining parses from different syntactic
representations.
3 Additional Features
3.1 CCG Parse Features
While the Path feature has been identified to be very
important for the argument identification task, it is
one of the most sparse features and may be diffi-
cult to train or generalize (Pradhan et al, 2004; Xue
and Palmer, 2004). A dependency grammar should
generate shorter paths from the predicate to depen-
dent words in the sentence, and could be a more
robust complement to the phrase structure grammar
paths extracted from the Charniak parse tree. Gildea
and Hockenmaier (2003) report that using features
extracted from a Combinatory Categorial Grammar
(CCG) representation improves semantic labeling
performance on core arguments. We evaluated fea-
tures from a CCG parser combined with our baseline
feature set. We used three features that were intro-
duced by Gildea and Hockenmaier (2003):
? Phrase type ? This is the category of the max-
imal projection between the two words ? the
predicate and the dependent word.
? Categorial Path ? This is a feature formed by
concatenating the following three values: i) cat-
egory to which the dependent word belongs, ii)
the direction of dependence and iii) the slot in
the category filled by the dependent word.
? Tree Path ? This is the categorial analogue of
the path feature in the Charniak parse based
system, which traces the path from the depen-
dent word to the predicate through the binary
CCG tree.
Parallel to the hand-corrected TreeBank parses,
we also had access to correct CCG parses derived
from the TreeBank (Hockenmaier and Steedman,
2002a). We performed two sets of experiments.
One using the correct CCG parses, and the other us-
ing parses obtained using StatCCG4 parser (Hocken-
maier and Steedman, 2002). We incorporated these
features in the systems based on hand-corrected
TreeBank parses and Charniak parses respectively.
For each constituent in the Charniak parse tree, if
there was a dependency between the head word of
the constituent and the predicate, then the corre-
sponding CCG features for those words were added
to the features for that constituent. Table 3 shows the
performance of the system when these features were
added. The corresponding baseline performances
are mentioned in parentheses.
3.2 Other Features
We added several other features to the system. Po-
sition of the clause node (S, SBAR) seems to be
4Many thanks to Julia Hockenmaier for providing us with
the CCG bank as well as the StatCCG parser.
583
ALL ARGs Task P R F1
(%) (%)
HAND Id. 97.5 (96.2) 96.1 (95.8) 96.8 (96.0)
Id. + Class. 91.8 (89.9) 90.5 (89.0) 91.2 (89.4)
AUTOMATIC Id. 87.1 (86.8) 80.7 (80.0) 83.8 (83.3)
Id. + Class. 81.5 (80.9) 77.2 (76.8) 79.3 (78.8)
Table 3: Performance improvement upon adding
CCG features to the Baseline system.
an important feature in argument identification (Ha-
cioglu et al, 2004) therefore we experimented with
four clause-based path feature variations. We added
the predicate context to capture predicate sense vari-
ations. For some adjunctive arguments, punctuation
plays an important role, so we added some punctu-
ation features. All the new features are shown in
Table 4
CLAUSE-BASED PATH VARIATIONS:
I. Replacing all the nodes in a path other than clause nodes with an ?*?.
For example, the path NP?S?VP?SBAR?NP?VP?VBD
becomes NP?S?*S?*?*?VBD
II. Retaining only the clause nodes in the path, which for the above
example would produce NP?S?S?VBD,
III. Adding a binary feature that indicates whether the constituent
is in the same clause as the predicate,
IV. collapsing the nodes between S nodes which gives NP?S?NP?VP?VBD.
PATH N-GRAMS: This feature decomposes a path into a series of trigrams.
For example, the path NP?S?VP?SBAR?NP?VP?VBD becomes:
NP?S?VP, S?VP?SBAR, VP?SBAR?NP, SBAR?NP?VP, etc. We
used the first ten trigrams as ten features. Shorter paths were padded
with nulls.
SINGLE CHARACTER PHRASE TAGS: Each phrase category is clustered
to a category defined by the first character of the phrase label.
PREDICATE CONTEXT: Two words and two word POS around the
predicate and including the predicate were added as ten new features.
PUNCTUATION: Punctuation before and after the constituent were
added as two new features.
FEATURE CONTEXT: Features for argument bearing constituents
were added as features to the constituent being classified.
Table 4: Other Features
4 Feature Selection and Calibration
In the baseline system, we used the same set of fea-
tures for all the n binary ONE VS ALL classifiers.
Error analysis showed that some features specifi-
cally suited for one argument class, for example,
core arguments, tend to hurt performance on some
adjunctive arguments. Therefore, we thought that
selecting subsets of features for each argument class
might improve performance. To achieve this, we
performed a simple feature selection procedure. For
each argument, we started with the set of features in-
troduced by (Gildea and Jurafsky, 2002). We pruned
this set by training classifiers after leaving out one
feature at a time and checking its performance on
a development set. We used the ?2 significance
while making pruning decisions. Following that, we
added each of the other features one at a time to the
pruned baseline set of features and selected ones that
showed significantly improved performance. Since
the feature selection experiments were computation-
ally intensive, we performed them using 10k training
examples.
SVMs output distances not probabilities. These
distances may not be comparable across classifiers,
especially if different features are used to train each
binary classifier. In the baseline system, we used the
algorithm described by Platt (Platt, 2000) to convert
the SVM scores into probabilities by fitting to a sig-
moid. When all classifiers used the same set of fea-
tures, fitting all scores to a single sigmoid was found
to give the best performance. Since different fea-
ture sets are now used by the classifiers, we trained
a separate sigmoid for each classifier.
Raw Scores Probabilities
After lattice-rescoring
Uncalibrated Calibrated
(%) (%) (%)
Same Feat. same sigmoid 74.7 74.7 75.4
Selected Feat. diff. sigmoids 75.4 75.1 76.2
Table 5: Performance improvement on selecting fea-
tures per argument and calibrating the probabilities
on 10k training data.
Foster and Stine (2004) show that the pool-
adjacent-violators (PAV) algorithm (Barlow et al,
1972) provides a better method for converting raw
classifier scores to probabilities when Platt?s algo-
rithm fails. The probabilities resulting from either
conversions may not be properly calibrated. So, we
binned the probabilities and trained a warping func-
tion to calibrate them. For each argument classifier,
we used both the methods for converting raw SVM
scores into probabilities and calibrated them using
a development set. Then, we visually inspected
the calibrated plots for each classifier and chose the
method that showed better calibration as the calibra-
tion procedure for that classifier. Plots of the pre-
dicted probabilities versus true probabilities for the
ARGM-TMP VS ALL classifier, before and after cal-
ibration are shown in Figure 2. The performance im-
provement over a classifier that is trained using all
the features for all the classes is shown in Table 5.
Table 6 shows the performance of the system af-
ter adding the CCG features, additional features ex-
584
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Predicted Probability
Tr
ue
 P
ro
ba
bi
lity
Before Calibration
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Predicted Probability
Tr
ue
 P
ro
ba
bi
lity
After Calibration
Figure 2: Plots showing true probabilities versus predicted probabilities before and after calibration on the
test set for ARGM-TMP.
tracted from the Charniak parse tree, and performing
feature selection and calibration. Numbers in paren-
theses are the corresponding baseline performances.
TASK P R F1 A
(%) (%) (%)
Id. 86.9 (86.8) 84.2 (80.0) 85.5 (83.3)
Class. - - - 92.0 (90.1)
Id. + Class. 82.1 (80.9) 77.9 (76.8) 79.9 (78.8)
Table 6: Best system performance on all tasks using
automatically generated syntactic parses.
5 Alternative Syntactic Views
Adding new features can improve performance
when the syntactic representation being used for
classification contains the correct constituents. Ad-
ditional features can?t recover from the situation
where the parse tree being used for classification
doesn?t contain the correct constituent representing
an argument. Such parse errors account for about
7% absolute of the errors (or, about half of 12.7%)
for the Charniak parse based system. To address
these errors, we added two additional parse repre-
sentations: i) Minipar dependency parser, and ii)
chunking parser (Hacioglu et al, 2004). The hope is
that these parsers will produce different errors than
the Charniak parser since they represent different
syntactic views. The Charniak parser is trained on
the Penn TreeBank corpus. Minipar is a rule based
dependency parser. The chunking parser is trained
on PropBank and produces a flat syntactic represen-
tation that is very different from the full parse tree
produced by Charniak. A combination of the three
different parses could produce better results than any
single one.
5.1 Minipar-based Semantic Labeler
Minipar (Lin, 1998; Lin and Pantel, 2001) is a rule-
based dependency parser. It outputs dependencies
between a word called head and another called mod-
ifier. Each word can modify at most one word. The
dependency relationships form a dependency tree.
The set of words under each node in Minipar?s
dependency tree form a contiguous segment in the
original sentence and correspond to the constituent
in a constituent tree. We formulate the semantic la-
beling problem in the same way as in a constituent
structure parse, except we classify the nodes that
represent head words of constituents. A similar for-
mulation using dependency trees derived from Tree-
Bank was reported in Hacioglu (Hacioglu, 2004).
In that experiment, the dependency trees were de-
rived from hand-corrected TreeBank trees using
head word rules. Here, an SVM is trained to as-
sign PropBank argument labels to nodes in Minipar
dependency trees using the following features:
Table 8 shows the performance of the Minipar-
based semantic parser.
Minipar performance on the PropBank corpus is
substantially worse than the Charniak based system.
This is understandable from the fact that Minipar
is not designed to produce constituents that would
exactly match the constituent segmentation used in
TreeBank. In the test set, about 37% of the argu-
585
PREDICATE LEMMA
HEAD WORD: The word representing the node in the dependency tree.
HEAD WORD POS: Part of speech of the head word.
POS PATH: This is the path from the predicate to the head word through
the dependency tree connecting the part of speech of each node in the tree.
DEPENDENCY PATH: Each word that is connected to the head
word has a particular dependency relationship to the word. These
are represented as labels on the arc between the words. This
feature is the dependencies along the path that connects two words.
VOICE
POSITION
Table 7: Features used in the Baseline system using
Minipar parses.
Task P R F1
(%) (%)
Id. 73.5 43.8 54.6
Id. + Classification 66.2 36.7 47.2
Table 8: Baseline system performance on all tasks
using Minipar parses.
ments do not have corresponding constituents that
match its boundaries. In experiments reported by
Hacioglu (Hacioglu, 2004), a mismatch of about
8% was introduced in the transformation from hand-
corrected constituent trees to dependency trees. Us-
ing an errorful automatically generated tree, a still
higher mismatch would be expected. In case of
the CCG parses, as reported by Gildea and Hock-
enmaier (2003), the mismatch was about 23%. A
more realistic way to score the performance is to
score tags assigned to head words of constituents,
rather than considering the exact boundaries of the
constituents as reported by Gildea and Hocken-
maier (2003). The results for this system are shown
in Table 9.
Task P R F1
(%) (%)
CHARNIAK Id. 92.2 87.5 89.8
Id. + Classification 85.9 81.6 83.7
MINIPAR Id. 83.3 61.1 70.5
Id. + Classification 72.9 53.5 61.7
Table 9: Head-word based performance using Char-
niak and Minipar parses.
5.2 Chunk-based Semantic Labeler
Hacioglu has previously described a chunk based se-
mantic labeling method (Hacioglu et al, 2004). This
system uses SVM classifiers to first chunk input text
into flat chunks or base phrases, each labeled with
a syntactic tag. A second SVM is trained to assign
semantic labels to the chunks. The system is trained
on the PropBank training data.
WORDS
PREDICATE LEMMAS
PART OF SPEECH TAGS
BP POSITIONS: The position of a token in a BP using the IOB2
representation (e.g. B-NP, I-NP, O, etc.)
CLAUSE TAGS: The tags that mark token positions in a sentence
with respect to clauses.
NAMED ENTITIES: The IOB tags of named entities.
TOKEN POSITION: The position of the phrase with respect to
the predicate. It has three values as ?before?, ?after? and ?-? (for
the predicate)
PATH: It defines a flat path between the token and the predicate
CLAUSE BRACKET PATTERNS
CLAUSE POSITION: A binary feature that identifies whether the
token is inside or outside the clause containing the predicate
HEADWORD SUFFIXES: suffixes of headwords of length 2, 3 and 4.
DISTANCE: Distance of the token from the predicate as a number
of base phrases, and the distance as the number of VP chunks.
LENGTH: the number of words in a token.
PREDICATE POS TAG: the part of speech category of the predicate
PREDICATE FREQUENCY: Frequent or rare using a threshold of 3.
PREDICATE BP CONTEXT: The chain of BPs centered at the predicate
within a window of size -2/+2.
PREDICATE POS CONTEXT: POS tags of words immediately preceding
and following the predicate.
PREDICATE ARGUMENT FRAMES: Left and right core argument patterns
around the predicate.
NUMBER OF PREDICATES: This is the number of predicates in
the sentence.
Table 10: Features used by chunk based classifier.
Table 10 lists the features used by this classifier.
For each token (base phrase) to be tagged, a set of
features is created from a fixed size context that sur-
rounds each token. In addition to the above features,
it also uses previous semantic tags that have already
been assigned to the tokens contained in the linguis-
tic context. A 5-token sliding window is used for the
context.
P R F1
(%) (%)
Id. and Classification 72.6 66.9 69.6
Table 11: Semantic chunker performance on the
combined task of Id. and classification.
SVMs were trained for begin (B) and inside (I)
classes of all arguments and outside (O) class for a
total of 78 one-vs-all classifiers. Again, TinySVM5
along with YamCha6 (Kudo and Matsumoto, 2000;
Kudo and Matsumoto, 2001) are used as the SVM
training and test software.
Table 11 presents the system performances on the
PropBank test set for the chunk-based system.
5http://chasen.org/?taku/software/TinySVM/
6http://chasen.org/?taku/software/yamcha/
586
6 Combining Semantic Labelers
We combined the semantic parses as follows: i)
scores for arguments were converted to calibrated
probabilities, and arguments with scores below a
threshold value were deleted. Separate thresholds
were used for each parser. ii) For the remaining ar-
guments, the more probable ones among overlap-
ping ones were selected. In the chunked system,
an argument could consist of a sequence of chunks.
The probability assigned to the begin tag of an ar-
gument was used as the probability of the sequence
of chunks forming an argument. Table 12 shows
the performance improvement after the combina-
tion. Again, numbers in parentheses are respective
baseline performances.
TASK P R F1
(%) (%)
Id. 85.9 (86.8) 88.3 (80.0) 87.1 (83.3)
Id. + Class. 81.3 (80.9) 80.7 (76.8) 81.0 (78.8)
Table 12: Constituent-based best system perfor-
mance on argument identification and argument
identification and classification tasks after combin-
ing all three semantic parses.
The main contribution of combining both the
Minipar based and the Charniak-based parsers was
significantly improved performance on ARG1 in ad-
dition to slight improvements to some other argu-
ments. Table 13 shows the effect on selected argu-
ments on sentences that were altered during the the
combination of Charniak-based and Chunk-based
parses.
Number of Propositions 107
Percentage of perfect props before combination 0.00
Percentage of perfect props after combination 45.95
Before After
P R F1 P R F1
(%) (%) (%) (%)
Overall 94.8 53.4 68.3 80.9 73.8 77.2
ARG0 96.0 85.7 90.5 92.5 89.2 90.9
ARG1 71.4 13.5 22.7 59.4 59.4 59.4
ARG2 100.0 20.0 33.3 50.0 20.0 28.5
ARGM-DIS 100.0 40.0 57.1 100.0 100.0 100.0
Table 13: Performance improvement on parses
changed during pair-wise Charniak and Chunk com-
bination.
A marked increase in number of propositions for
which all the arguments were identified correctly
from 0% to about 46% can be seen. Relatively few
predicates, 107 out of 4500, were affected by this
combination.
To give an idea of what the potential improve-
ments of the combinations could be, we performed
an oracle experiment for a combined system that
tags head words instead of exact constituents as we
did in case of Minipar-based and Charniak-based se-
mantic parser earlier. In case of chunks, first word in
prepositional base phrases was selected as the head
word, and for all other chunks, the last word was se-
lected to be the head word. If the correct argument
was found present in either the Charniak, Minipar or
Chunk hypotheses then that was selected. The re-
sults for this are shown in Table 14. It can be seen
that the head word based performance almost ap-
proaches the constituent based performance reported
on the hand-corrected parses in Table 3 and there
seems to be considerable scope for improvement.
Task P R F1
(%) (%)
C Id. 92.2 87.5 89.8
Id. + Classification 85.9 81.6 83.7
C+M Id. 98.4 90.6 94.3
Id. + Classification 93.1 86.0 89.4
C+CH Id. 98.9 88.8 93.6
Id. + Classification 92.5 83.3 87.7
C+M+CH Id. 99.2 92.5 95.7
Id. + Classification 94.6 88.4 91.5
Table 14: Performance improvement on head word
based scoring after oracle combination. Charniak
(C), Minipar (M) and Chunker (CH).
Table 15 shows the performance improvement in
the actual system for pairwise combination of the
parsers and one using all three.
Task P R F1
(%) (%)
C Id. 92.2 87.5 89.8
Id. + Classification 85.9 81.6 83.7
C+M Id. 91.7 89.9 90.8
Id. + Classification 85.0 83.9 84.5
C+CH Id. 91.5 91.1 91.3
Id. + Classification 84.9 84.3 84.7
C+M+CH Id. 91.5 91.9 91.7
Id. + Classification 85.1 85.5 85.2
Table 15: Performance improvement on head word
based scoring after combination. Charniak (C),
Minipar (M) and Chunker (CH).
587
7 Conclusions
We described a state-of-the-art baseline semantic
role labeling system based on Support Vector Ma-
chine classifiers. Experiments were conducted to
evaluate three types of improvements to the sys-
tem: i) adding new features including features ex-
tracted from a Combinatory Categorial Grammar
parse, ii) performing feature selection and calibra-
tion and iii) combining parses obtained from seman-
tic parsers trained using different syntactic views.
We combined semantic parses from a Minipar syn-
tactic parse and from a chunked syntactic repre-
sentation with our original baseline system which
was based on Charniak parses. The belief was that
semantic parses based on different syntactic views
would make different errors and that the combina-
tion would be complimentary. A simple combina-
tion of these representations did lead to improved
performance.
8 Acknowledgements
This research was partially supported by the ARDA
AQUAINT program via contract OCG4423B and
by the NSF via grants IS-9978025 and ITR/HCI
0086132. Computer time was provided by NSF
ARI Grant #CDA-9601817, NSF MRI Grant #CNS-
0420873, NASA AIST grant #NAG2-1646, DOE
SciDAC grant #DE-FG02-04ER63870, NSF spon-
sorship of the National Center for Atmospheric Re-
search, and a grant from the IBM Shared University
Research (SUR) program.
We would like to thank Ralph Weischedel and
Scott Miller of BBN Inc. for letting us use their
named entity tagger ? IdentiFinder; Martha Palmer
for providing us with the PropBank data; Dan Gildea
and Julia Hockenmaier for providing the gold stan-
dard CCG parser information, and all the anony-
mous reviewers for their helpful comments.
References
R. E. Barlow, D. J. Bartholomew, J. M. Bremmer, and H. D. Brunk. 1972. Statis-
tical Inference under Order Restrictions. Wiley, New York.
Eugene Charniak. 2000. A maximum-entropy-inspired parser. In Proceedings of
NAACL, pages 132?139, Seattle, Washington.
John Chen and Owen Rambow. 2003. Use of deep linguistics features for
the recognition and labeling of semantic arguments. In Proceedings of the
EMNLP, Sapporo, Japan.
Dean P. Foster and Robert A. Stine. 2004. Variable selection in data mining:
building a predictive model for bankruptcy. Journal of American Statistical
Association, 99, pages 303?313.
Dan Gildea and Julia Hockenmaier. 2003. Identifying semantic roles using com-
binatory categorial grammar. In Proceedings of the EMNLP, Sapporo, Japan.
Daniel Gildea and Daniel Jurafsky. 2000. Automatic labeling of semantic roles.
In Proceedings of ACL, pages 512?520, Hong Kong, October.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic labeling of semantic roles.
Computational Linguistics, 28(3):245?288.
Daniel Gildea and Martha Palmer. 2002. The necessity of syntactic parsing for
predicate argument recognition. In Proceedings of ACL, Philadelphia, PA.
Kadri Hacioglu. 2004. Semantic role labeling using dependency trees. In Pro-
ceedings of COLING, Geneva, Switzerland.
Kadri Hacioglu and Wayne Ward. 2003. Target word detection and semantic role
chunking using support vector machines. In Proceedings of HLT/NAACL,
Edmonton, Canada.
Kadri Hacioglu, Sameer Pradhan, Wayne Ward, James Martin, and Dan Jurafsky.
2003. Shallow semantic parsing using support vector machines. Technical
Report TR-CSLR-2003-1, Center for Spoken Language Research, Boulder,
Colorado.
Kadri Hacioglu, Sameer Pradhan, Wayne Ward, James Martin, and Daniel Juraf-
sky. 2004. Semantic role labeling by tagging syntactic chunks. In Proceed-
ings of CoNLL-2004, Shared Task ? Semantic Role Labeling.
Kadri Hacioglu. 2004a. A lightweight semantic chunking model based on tag-
ging. In Proceedings of HLT/NAACL, Boston, MA.
Julia Hockenmaier and Mark Steedman. 2002. Generative models for statistical
parsing with combinatory grammars. In Proceedings of the ACL, pages 335?
342.
Julia Hockenmaier and Mark Steedman. 2002a. Acquiring compact lexicalized
grammars from a cleaner treebank. In Proceedings of the 3rd International
Conference on Language Resources and Evaluation (LREC-2002), Las Pal-
mas, Canary Islands, Spain.
Paul Kingsbury and Martha Palmer. 2002. From Treebank to PropBank. In
Proceedings of LREC, Las Palmas, Canary Islands, Spain.
Taku Kudo and Yuji Matsumoto. 2000. Use of support vector learning for chunk
identification. In Proceedings of CoNLL and LLL, pages 142?144.
Taku Kudo and Yuji Matsumoto. 2001. Chunking with support vector machines.
In Proceedings of the NAACL.
Dekang Lin and Patrick Pantel. 2001. Discovery of inference rules for question
answering. Natural Language Engineering, 7(4):343?360.
Dekang Lin. 1998. Dependency-based evaluation of MINIPAR. In In Workshop
on the Evaluation of Parsing Systems, Granada, Spain.
Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz, Robert MacIntyre, Ann
Bies, Mark Ferguson, Karen Katz, and Britta Schasberger. 1994. The Penn
Treebank: Annotating predicate argument structure.
Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005. The proposition bank:
An annotated corpus of semantic roles. To appear Computational Linguistics.
John Platt. 2000. Probabilities for support vector machines. In A. Smola,
P. Bartlett, B. Scholkopf, and D. Schuurmans, editors, Advances in Large
Margin Classifiers. MIT press, Cambridge, MA.
Sameer Pradhan, Kadri Hacioglu, Wayne Ward, James Martin, and Dan Jurafsky.
2003. Semantic role parsing: Adding semantic structure to unstructured text.
In Proceedings of ICDM, Melbourne, Florida.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James Martin, and Dan Jurafsky.
2004. Shallow semantic parsing using support vector machines. In Proceed-
ings of HLT/NAACL, Boston, MA.
Mihai Surdeanu, Sanda Harabagiu, John Williams, and Paul Aarseth. 2003. Us-
ing predicate-argument structures for information extraction. In Proceedings
of ACL, Sapporo, Japan.
Nianwen Xue and Martha Palmer. 2004. Calibrating features for semantic role
labeling. In Proceedings of EMNLP, Barcelona, Spain.
588
Semantic Role Labeling by Tagging Syntactic Chunks?
Kadri Hacioglu1, Sameer Pradhan1, Wayne Ward1, James H. Martin1, Daniel Jurafsky2
1University of Colorado at Boulder, 2Stanford University
{hacioglu,spradhan,whw}@cslr.colorado.edu, martin@cs.colorado.edu, jurafsky@stanford.edu
Abstract
In this paper, we present a semantic role la-
beler (or chunker) that groups syntactic chunks
(i.e. base phrases) into the arguments of a pred-
icate. This is accomplished by casting the se-
mantic labeling as the classification of syntactic
chunks (e.g. NP-chunk, PP-chunk) into one of
several classes such as the beginning of an ar-
gument (B-ARG), inside an argument (I-ARG)
and outside an argument (O). This amounts to
tagging syntactic chunks with semantic labels
using the IOB representation. The chunker is
realized using support vector machines as one-
versus-all classifiers. We describe the represen-
tation of data and information used to accom-
plish the task. We participate in the ?closed
challenge? of the CoNLL-2004 shared task and
report results on both development and test
sets.
1 Introduction
In semantic role labeling the goal is to group sequences
of words together and classify them by using semantic la-
bels. For meaning representation the predicate-argument
structure that exists in most languages is used. In this
structure a word (most frequently a verb) is specified as
a predicate, and a number of word groups are considered
as arguments accompanying the word (or predicate).
In this paper, we select support vector machines
(SVMs) (Vapnik, 1995; Burges, 1998) to implement
the semantic role classifiers, due to their ability to han-
dle an extremely large number of (overlapping) features
with quite strong generalization properties. Support vec-
tor machines for semantic role chunking were first used
?This research was partially supported by the ARDA
AQUAINT program via contract OCG4423B and by the NSF
via grant IIS-9978025
in (Hacioglu and Ward, 2003) as word-by-word (W-by-
W) classifiers. The system was then applied to the
constituent-by-constituent (C-by-C) classification in (Ha-
cioglu et al, 2003). In (Pradhan et al, 2003; Prad-
han et al, 2004), several extensions to the basic system
have been proposed, extensively studied and systemati-
cally compared to other systems. In this paper, we imple-
ment a system that classifies syntactic chunks (i.e. base
phrases) instead of words or the constituents derived from
syntactic trees. This system is referred to as the phrase-
by-phrase (P-by-P) semantic role classifier. We partici-
pate in the ?closed challenge? of the CoNLL-2004 shared
task and report results on both development and test sets.
A detailed description of the task, data and related work
can be found in (Carreras and Ma`rquez, 2004).
2 System Description
2.1 Data Representation
In this paper, we change the representation of the original
data as follows:
? Bracketed representation of roles is converted into
IOB2 representation (Ramhsaw and Marcus, 1995;
Sang and Veenstra, 1995)
? Word tokens are collapsed into base phrase (BP) to-
kens.
Since the semantic annotation in the PropBank corpus
does not have any embedded structure there is no loss of
information in the first change. However, this results in
a simpler representation with a reduced set of tagging la-
bels. In the second change, it is possible to miss some
information in cases where the semantic chunks do not
align with the sequence of BPs. However, in Section 3.2
we show that the loss in performance due to the misalign-
ment is much less than the gain in performance that can
be achieved by the change in representation.
from
million
251.2
$
to
declined VBD
CD
NN
IN
CD
$
TO
Sales
%
CD
10
$
278.7
$
CD
NNS
*A2)
*
*
*A4)
*
*
million CD I?NP
O
*
*
B?NP
B?VP
B?NP
I?NP
B?PP
B?NP
I?NP
I?NP
B?PP
B?NP
I?NP
*
(S*
*
*
*
*
*
*
*
*
*
decline
?
?
?
?
?
?
?
?
?
?
?
?
*A3)
(A3*
(A4*
(A2*
(V*V)
(A1*A1)
*S)
O
NP
VP
NP
PP
NP
PP
NP
Sales
declined
NNS
VBD
% NN
TOto
million
from
million
B?NP
CD
CD
O
I?NP
B?PP
I?NP
B?PP
I?NP
?
?
?
?
?
?
?
*
*
*
*
*
* B?V
B?A2
B?A1
B?A3
B?A4
O
O
O
B?VP
IN
*S)
(S*
(b)(a)
. .
. .
decline
Figure 1: Illustration of change in data representation; (a) original word-by-word data representation (b) phrase-by-
phrase data representation used in this paper. Words are collapsed into base phrase types retaining only headwords
with their respective features. Bracketed representation of semantic role labels is converted into IOB2 representation.
See text for details.
The new representation is illustrated in Figure 1 along
with the original representation. Comparing both we note
the following differences and advantages in the new rep-
resentation:
? BPs are being classified instead of words.
? Only the BP headwords (rightmost words) are re-
tained as word information.
? The number of tagging steps is smaller.
? A fixed context spans a larger segment of a sentence.
Therefore, the P-by-P semantic role chunker classifies
larger units, ignores some of the words, uses a relatively
larger context for a given window size and performs the
labeling faster.
2.2 Features
The following features, which we refer to as the base fea-
tures, are provided in the shared task data for each sen-
tence;
? Words
? Predicate lemmas
? Part of Speech tags
? BP Positions: The position of a token in a BP using
the IOB2 representation (e.g. B-NP, I-NP, O etc.)
? Clause tags: The tags that mark token positions in a
sentence with respect to clauses. (e.g *S)*S) marks
a position that two clauses end)
? Named entities: The IOB tags of named entities.
There are four categories; LOC, ORG, PERSON
and MISC.
Using available information we have created the fol-
lowing token level features:
? Token Position: The position of the phrase with re-
spect to the predicate. It has three values as ?be-
fore?, ?after? and ?-? for the predicate.
? Path: It defines a flat path between the token and
the predicate as a chain of base phrases. At both
ends, the chain is terminated with the POS tags of
the predicate and the headword of the token.
? Clause bracket patterns: We use two patterns of
clauses for each token. One is the clause bracket
chain between the token and the predicate, and the
other is from the token to sentence begin or end de-
pending on token?s position with respect to the pred-
icate.
? Clause Position: a binary feature that indicates the
token is inside or outside of the clause which con-
tains the predicate
? Headword suffixes: suffixes of headwords of length
2, 3 and 4.
? Distance: we have two notions of distance; the first
is the distance of the token from the predicate as a
number of base phrases, and the second is the same
distance as the number of VP chunks.
? Length: the number of words in a token.
We also use some sentence level features:
? Predicate POS tag: the part of speech category of
the predicate
? Predicate Frequency; this is a feature which indi-
cates whether the predicate is frequent or rare with
respect to the training set. The threshold on the
counts is currently set to 3.
? Predicate BP Context : The chain of BPs centered
at the predicate within a window of size -2/+2.
? Predicate POS Context : The POS tags of the
words that immediately precede and follow the pred-
icate. The POS tag of a preposition is replaced with
the preposition itself.
? Predicate Argument Frames: We used the left and
right patterns of the core arguments (A0 through A5)
for each predicate . We used the three most frequent
argument frames for both sides depending on the po-
sition of the token in focus with respect to the pred-
icate. (e.g. raise has A0 and A1 AO (A0 being the
most frequent) as its left argument frames, and A1,
A1 A2 and A2 as the three most frequent right argu-
ment frames)
? Number of predicates: This is the number of pred-
icates in the sentence.
For each token (base phrase) to be tagged, a set of or-
dered features is created from a fixed size context that
surrounds each token. In addition to the above features,
we also use previous semantic IOB tags that have already
been assigned to the tokens contained in the context. A
5-token sliding window is used for the context. A greedy
left-to-right tagging is performed.
All of the above features are designed to implicitly cap-
ture the patterns of sentence constructs with respect to
different word/predicate usages and senses. We acknowl-
edge that they significantly overlap and extensive exper-
iments are required to determine the impact of each fea-
ture on the performance.
2.3 Classifier
All SVM classifiers were realized using TinySVM1 with
a polynomial kernel of degree 2 and the general purpose
SVM based chunker YamCha 2. SVMs were trained for
begin (B) and inside (I) classes of all arguments and one
outside (O) class for a total of 78 one-vs-all classifiers
(some arguments do not have an I-tag).
1http://cl.aist-nara.ac.jp/taku-ku/software/TinySVM
2http://cl.aist-nara.ac.jp/taku-ku/software/yamcha
Table 1: Comparison of W-by-W and P-by-P methods.
Both systems use the base features provided (i.e. no fea-
ture engineering is done). Results are on dev set.
Method Precision Recall F?=1
P-by-P 69.04% 54.68% 61.02
W-by-W 68.34% 45.16% 54.39
Table 2: Number of sentences and unique training exam-
ples in each method.
Method Sentences Training Examples
P-by-P 19K 347K
W-by-W 19K 534K
3 Experimental Results
3.1 Data and Evaluation Metrics
The data provided for the shared task is a part of the
February 2004 release of the PropBank corpus. It con-
sists of sections from the Wall Street Journal part of the
Penn Treebank. All experiments were carried out using
Sections 15-18 for training Section-20 for development
and Section-21 for testing. The results were evaluated for
precision, recall and F?=1 numbers using the srl-eval.pl
script provided by the shared task organizers.
3.2 W-by-W and P-by-P Experiments
In these experiments we used only the base features to
compare the two approaches. Table 1 illustrates the over-
all performance on the dev set. Although both systems
were trained using the same number of sentences, the ac-
tual number of training examples in each case were quite
different. Those numbers are presented in Table 2. It is
clear that P-by-P method uses much less data for the same
number of sentences. Despite this we particularly note a
considerable improvement in recall. Actually, the data
reduction was not without a cost. Some arguments have
been missed as they do not align with the base phrase
chunks due to inconsistencies in semantic annotation and
due to errors in automatic base phrase chunking. The per-
centage of this misalignment was around 2.5% (over the
dev set). We observed that nearly 45% of the mismatches
were for the ?outside? chunks. Therefore, sequences of
words with outside tags were not collapsed.
3.3 Best System Results
In these experiments all of the features described earlier
were used with the P-by-P system. Table 3 presents our
best system performance on the development set. Ad-
ditional features have improved the performance from
61.02 to 71.72. The performance of the same system on
the test set is similarly illustrated in Table 4.
Table 3: System results on development set.
Precision Recall F?=1
Overall 74.17% 69.42% 71.72
A0 82.86% 78.50% 80.62
A1 72.82% 73.97% 73.39
A2 60.16% 56.18% 58.10
A3 59.66% 47.65% 52.99
A4 83.21% 74.15% 78.42
A5 100.00% 75.00% 85.71
AM-ADV 52.52% 41.48% 46.35
AM-CAU 61.11% 41.51% 49.44
AM-DIR 47.37% 15.00% 22.78
AM-DIS 76.47% 76.47% 76.47
AM-EXT 74.07% 40.82% 52.63
AM-LOC 51.21% 46.09% 48.51
AM-MNR 51.04% 36.83% 42.78
AM-MOD 99.47% 95.63% 97.51
AM-NEG 99.20% 94.66% 96.88
AM-PNC 70.00% 28.00% 40.00
AM-PRD 0.00% 0.00% 0.00
AM-REC 0.00% 0.00% 0.00
AM-TMP 69.33% 58.37% 63.38
R-A0 91.55% 80.25% 85.53
R-A1 72.46% 67.57% 69.93
R-A2 100.00% 52.94% 69.23
R-AM-LOC 100.00% 25.00% 40.00
R-AM-TMP 0.00% 0.00% 0.00
V 99.05% 99.05% 99.05
4 Conclusions
We have described a semantic role chunker using SVMs.
The chunking method has been based on a chunked sen-
tence structure at both syntactic and semantic levels. We
have jointly performed semantic chunk segmentation and
labeling using a set of one-vs-all SVM classifiers on a
phrase-by-phrase basis. It has been argued that the new
representation has several advantages as compared to the
original representation. It yields a semantic role labeler
that classifies larger units, exploits relatively larger con-
text, uses less data (possibly, redundant and noisy data
are filtered out), runs faster and performs better.
References
Xavier Carreras and Llu??s Ma`rquez. 2004. Introduction
to the CoNLL-2004 Shared Task: Semantic Role La-
beling in the same volume of Proc. of CoNLL?2004
Shared Task.
Christopher J. C. Burges. 1997. A Tutorial on Support
Vector Machines for Pattern Recognition. Data Min-
ing and Knowledge Discovery, 2(2), pages 1-47.
Kadri Hacioglu and Wayne Ward. 2003. Target word
Table 4: System results on test set.
Precision Recall F?=1
Overall 72.43% 66.77% 69.49
A0 82.93% 79.88% 81.37
A1 71.92% 71.33% 71.63
A2 49.37% 49.30% 49.33
A3 57.50% 46.00% 51.11
A4 87.10% 54.00% 66.67
A5 0.00% 0.00% 0.00
AM-ADV 53.36% 38.76% 44.91
AM-CAU 57.89% 22.45% 32.35
AM-DIR 37.84% 28.00% 32.18
AM-DIS 66.83% 62.44% 64.56
AM-EXT 70.00% 50.00% 58.33
AM-LOC 46.63% 36.40% 40.89
AM-MNR 50.31% 31.76% 38.94
AM-MOD 98.12% 92.88% 95.43
AM-NEG 91.11% 96.85% 93.89
AM-PNC 52.00% 15.29% 23.64
AM-PRD 0.00% 0.00% 0.00
AM-TMP 64.57% 50.74% 56.82
R-A0 90.21% 81.13% 85.43
R-A1 83.02% 62.86% 71.54
R-A2 100.00% 33.33% 50.00
R-A3 0.00% 0.00% 0.00
R-AM-LOC 0.00% 0.00% 0.00
R-AM-MNR 0.00% 0.00% 0.00
R-AM-PNC 0.00% 0.00% 0.00
R-AM-TMP 60.00% 21.43% 31.58
V 98.46% 98.46% 98.46
Detection and Semantic Role Chunking Using Support
Vector Machines. Proc. of the HLT-NAACL-03.
Kadri Hacioglu, Sameer Pradhan, Wayne Ward, James
Martin, and Dan Jurafsky. 2003. Shallow Semantic
Parsing Using Support Vector Machines. CSLR Tech.
Report, CSLR-TR-2003-1.
Sameer Pradhan, Kadri Hacioglu, Wayne Ward, James
Martin, and Dan Jurafsky. 2003. Semantic Role Pars-
ing: Adding Semantic Structure to Unstructured Text.
Proc. of Int. Conf. on Data Mining (ICDM03).
Sameer Pradhan, Kadri Hacioglu, Wayne Ward, James
Martin, and Dan Jurafsky. 2004. Support Vector
Learning for Semantic Argument Classification. to ap-
pear in Journal of Machine Learning.
Lance E. Ramhsaw and Mitchell P. Marcus. 1995.
Text Chunking Using Transformation Based Learning.
Proc. of the 3rd ACL Workshop on Very Large Cor-
pora, pages 82-94.
Erik F. T. J. Sang, John Veenstra. 1999. Representing
Text Chunks. Proc. of EACL?99, pages 173-179.
Vladamir Vapnik 1995. The Nature of Statistical Learn-
ing Theory. Springer Verlag, New York, USA.
Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),
pages 217?220, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Semantic Role Chunking Combining Complementary Syntactic Views
Sameer Pradhan, Kadri Hacioglu, Wayne Ward, James H. Martin and Daniel Jurafsky?
Center for Spoken Language Research, University of Colorado, Boulder, CO 80303
?Department of Linguistics, Stanford University, Stanford, CA 94305
{spradhan,hacioglu,whw,martin}@cslr.colorado.edu, jurafsky@stanford.edu
Abstract
This paper describes a semantic role la-
beling system that uses features derived
from different syntactic views, and com-
bines them within a phrase-based chunk-
ing paradigm. For an input sentence, syn-
tactic constituent structure parses are gen-
erated by a Charniak parser and a Collins
parser. Semantic role labels are assigned
to the constituents of each parse using
Support Vector Machine classifiers. The
resulting semantic role labels are con-
verted to an IOB representation. These
IOB representations are used as additional
features, along with flat syntactic chunks,
by a chunking SVM classifier that pro-
duces the final SRL output. This strategy
for combining features from three differ-
ent syntactic views gives a significant im-
provement in performance over roles pro-
duced by using any one of the syntactic
views individually.
1 Introduction
The task of Semantic Role Labeling (SRL) involves
tagging groups of words in a sentence with the se-
mantic roles that they play with respect to a particu-
lar predicate in that sentence. Our approach is to use
supervised machine learning classifiers to produce
the role labels based on features extracted from the
input. This approach is neutral to the particular set
of labels used, and will learn to tag input according
to the annotated data that it is trained on. The task
reported on here is to produce PropBank (Kingsbury
and Palmer, 2002) labels, given the features pro-
vided for the CoNLL-2005 closed task (Carreras and
Ma`rquez, 2005).
We have previously reported on using SVM clas-
sifiers for semantic role labeling. In this work, we
formulate the semantic labeling problem as a multi-
class classification problem using Support Vector
Machine (SVM) classifiers. Some of these systems
use features based on syntactic constituents pro-
duced by a Charniak parser (Pradhan et al, 2003;
Pradhan et al, 2004) and others use only a flat syn-
tactic representation produced by a syntactic chun-
ker (Hacioglu et al, 2003; Hacioglu and Ward,
2003; Hacioglu, 2004; Hacioglu et al, 2004). The
latter approach lacks the information provided by
the hierarchical syntactic structure, and the former
imposes a limitation that the possible candidate roles
should be one of the nodes already present in the
syntax tree. We found that, while the chunk based
systems are very efficient and robust, the systems
that use features based on full syntactic parses are
generally more accurate. Analysis of the source
of errors for the parse constituent based systems
showed that incorrect parses were a major source
of error. The syntactic parser did not produce any
constituent that corresponded to the correct segmen-
tation for the semantic argument. In Pradhan et al
(2005), we reported on a first attempt to overcome
this problem by combining semantic role labels pro-
duced from different syntactic parses. The hope is
that the syntactic parsers will make different errors,
and that combining their outputs will improve on
217
either system alone. This initial attempt used fea-
tures from a Charniak parser, a Minipar parser and a
chunk based parser. It did show some improvement
from the combination, but the method for combin-
ing the information was heuristic and sub-optimal.
In this paper, we report on what we believe is an im-
proved framework for combining information from
different syntactic views. Our goal is to preserve the
robustness and flexibility of the segmentation of the
phrase-based chunker, but to take advantage of fea-
tures from full syntactic parses. We also want to
combine features from different syntactic parses to
gain additional robustness. To this end, we use fea-
tures generated from a Charniak parser and a Collins
parser, as supplied for the CoNLL-2005 closed task.
2 System Description
We again formulate the semantic labeling problem
as a multi-class classification problem using Sup-
port Vector Machine (SVM) classifiers. TinySVM1
along with YamCha2 (Kudo and Matsumoto, 2000;
Kudo and Matsumoto, 2001) are used to implement
the system. Using what is known as the ONE VS
ALL classification strategy, n binary classifiers are
trained, where n is number of semantic classes in-
cluding a NULL class.
The general framework is to train separate seman-
tic role labeling systems for each of the parse tree
views, and then to use the role arguments output by
these systems as additional features in a semantic
role classifier using a flat syntactic view. The con-
stituent based classifiers walk a syntactic parse tree
and classify each node as NULL (no role) or as one
of the set of semantic roles. Chunk based systems
classify each base phrase as being the B(eginning)
of a semantic role, I(nside) a semantic role, or
O(utside) any semantic role (ie. NULL). This
is referred to as an IOB representation (Ramshaw
and Marcus, 1995). The constituent level roles are
mapped to the IOB representation used by the chun-
ker. The IOB tags are then used as features for a
separate base-phase semantic role labeler (chunker),
in addition to the standard set of features used by
the chunker. An n-fold cross-validation paradigm
is used to train the constituent based role classifiers
1http://chasen.org/?taku/software/TinySVM/
2http://chasen.org/?taku/software/yamcha/
and the chunk based classifier.
For the system reported here, two full syntactic
parsers were used, a Charniak parser and a Collins
parser. Features were extracted by first generating
the Collins and Charniak syntax trees from the word-
by-word decomposed trees in the CoNLL data. The
chunking system for combining all features was
trained using a 4-fold paradigm. In each fold, sepa-
rate SVM classifiers were trained for the Collins and
Charniak parses using 75% of the training data. That
is, one system assigned role labels to the nodes in
Charniak based trees and a separate system assigned
roles to nodes in Collins based trees. The other 25%
of the training data was then labeled by each of the
systems. Iterating this process 4 times created the
training set for the chunker. After the chunker was
trained, the Charniak and Collins based semantic la-
belers were then retrained using all of the training
data.
Two pieces of the system have problems scaling
to large training sets ? the final chunk based clas-
sifier and the NULL VS NON-NULL classifier for
the parse tree syntactic views. Two techniques were
used to reduce the amount of training data ? active
sampling and NULL filtering. The active sampling
process was performed as follows. We first train
a system using 10k seed examples from the train-
ing set. We then labeled an additional block of data
using this system. Any sentences containing an er-
ror were added to the seed training set. The sys-
tem was retrained and the procedure repeated until
there were no misclassified sentences remaining in
the training data. The set of examples produced by
this procedure was used to train the final NULL VS
NON-NULL classifier. The same procedure was car-
ried out for the chunking system. After both these
were trained, we tagged the training data using them
and removed all most likely NULLs from the data.
Table 1 lists the features used in the constituent
based systems. They are a combination of features
introduced by Gildea and Jurafsky (2002), ones pro-
posed in Pradhan et al (2004), Surdeanu et al
(2003) and the syntactic-frame feature proposed in
(Xue and Palmer, 2004). These features are ex-
tracted from the parse tree being labeled. In addition
to the features extracted from the parse tree being
labeled, five features were extracted from the other
parse tree (phrase, head word, head word POS, path
218
PREDICATE LEMMA
PATH: Path from the constituent to the predicate in the parse tree.
POSITION: Whether the constituent is before or after the predicate.
PREDICATE SUB-CATEGORIZATION
HEAD WORD: Head word of the constituent.
HEAD WORD POS: POS of the head word
NAMED ENTITIES IN CONSTITUENTS: Person, Organization, Location
and Miscellaneous.
PARTIAL PATH: Path from the constituent to the lowest common ancestor
of the predicate and the constituent.
HEAD WORD OF PP: Head of PP replaced by head word of NP inside it,
and PP replaced by PP-preposition
FIRST AND LAST WORD/POS IN CONSTITUENT
ORDINAL CONSTITUENT POSITION
CONSTITUENT TREE DISTANCE
CONSTITUENT RELATIVE FEATURES: Nine features representing
the phrase type, head word and head word part of speech of the
parent, and left and right siblings of the constituent.
SYNTACTIC FRAME
CONTENT WORD FEATURES: Content word, its POS and named entities
in the content word
CLAUSE-BASED PATH VARIATIONS:
I. Replacing all the nodes in a path other than clause nodes with an ?*?.
For example, the path NP?S?VP?SBAR?NP?VP?VBD
becomes NP?S?*S?*?*?VBD
II. Retaining only the clause nodes in the path, which for the above
example would produce NP?S?S?VBD,
III. Adding a binary feature that indicates whether the constituent
is in the same clause as the predicate,
IV. collapsing the nodes between S nodes which gives NP?S?NP?VP?VBD.
PATH N-GRAMS: This feature decomposes a path into a series of trigrams.
For example, the path NP?S?VP?SBAR?NP?VP?VBD becomes:
NP?S?VP, S?VP?SBAR, VP?SBAR?NP, SBAR?NP?VP, etc. We
used the first ten trigrams as ten features. Shorter paths were padded
with nulls.
SINGLE CHARACTER PHRASE TAGS: Each phrase category is clustered
to a category defined by the first character of the phrase label.
PREDICATE CONTEXT: Two words and two word POS around the
predicate and including the predicate were added as ten new features.
PUNCTUATION: Punctuation before and after the constituent were
added as two new features.
FEATURE CONTEXT: Features for argument bearing constituents
were added as features to the constituent being classified.
Table 1: Features used by the constituent-based sys-
tem
and predicate sub-categorization). So for example,
when assigning labels to constituents in a Charniak
parse, all of the features in Table 1 were extracted
from the Charniak tree, and in addition phrase, head
word, head word POS, path and sub-categorization
were extracted from the Collins tree. We have pre-
viously determined that using different sets of fea-
tures for each argument (role) achieves better results
than using the same set of features for all argument
classes. A simple feature selection was implemented
by adding features one by one to an initial set of
features and selecting those that contribute signifi-
cantly to the performance. As described in Pradhan
et al (2004), we post-process lattices of n-best de-
cision using a trigram language model of argument
sequences.
Table 2 lists the features used by the chunker.
These are the same set of features that were used
in the CoNLL-2004 semantic role labeling task by
Hacioglu, et al (2004) with the addition of the two
semantic argument (IOB) features. For each token
(base phrase) to be tagged, a set of features is created
from a fixed size context that surrounds each token.
In addition to the features in Table 2, it also uses pre-
vious semantic tags that have already been assigned
to the tokens contained in the linguistic context. A
5-token sliding window is used for the context.
SVMs were trained for begin (B) and inside (I)
classes of all arguments and an outside (O) class.
WORDS
PREDICATE LEMMAS
PART OF SPEECH TAGS
BP POSITIONS: The position of a token in a BP using the IOB2
representation (e.g. B-NP, I-NP, O, etc.)
CLAUSE TAGS: The tags that mark token positions in a sentence
with respect to clauses.
NAMED ENTITIES: The IOB tags of named entities.
TOKEN POSITION: The position of the phrase with respect to
the predicate. It has three values as ?before?, ?after? and ?-? (for
the predicate)
PATH: It defines a flat path between the token and the predicate
HIERARCHICAL PATH: Since we have the syntax tree for the sentences,
we also use the hierarchical path from the phrase being classified to the
base phrase containing the predicate.
CLAUSE BRACKET PATTERNS
CLAUSE POSITION: A binary feature that identifies whether the
token is inside or outside the clause containing the predicate
HEADWORD SUFFIXES: suffixes of headwords of length 2, 3 and 4.
DISTANCE: Distance of the token from the predicate as a number
of base phrases, and the distance as the number of VP chunks.
LENGTH: the number of words in a token.
PREDICATE POS TAG: the part of speech category of the predicate
PREDICATE FREQUENCY: Frequent or rare using a threshold of 3.
PREDICATE BP CONTEXT: The chain of BPs centered at the predicate
within a window of size -2/+2.
PREDICATE POS CONTEXT: POS tags of words immediately preceding
and following the predicate.
PREDICATE ARGUMENT FRAMES: Left and right core argument patterns
around the predicate.
DYNAMIC CLASS CONTEXT: Hypotheses generated for two preceeding
phrases.
NUMBER OF PREDICATES: This is the number of predicates in
the sentence.
CHARNIAK-BASED SEMANTIC IOB TAG: This is the IOB tag generated
using the tagger trained on Charniak trees
COLLINS-BASED SEMANTIC IOB TAG: This is the IOB tag generated
using the tagger trained on Collins? trees
Table 2: Features used by phrase-based chunker.
3 Experimental Results
Table 3 shows the results obtained on the WSJ de-
velopment set (Section 24), the WSJ test set (Section
23) and the Brown test set (Section ck/01-03)
4 Acknowledgments
This research was partially supported by the ARDA
AQUAINT program via contract OCG4423B and
by the NSF via grants IS-9978025 and ITR/HCI
219
Precision Recall F?=1
Development 80.90% 75.38% 78.04
Test WSJ 81.97% 73.27% 77.37
Test Brown 73.73% 61.51% 67.07
Test WSJ+Brown 80.93% 71.69% 76.03
Test WSJ Precision Recall F?=1
Overall 81.97% 73.27% 77.37
A0 91.39% 82.23% 86.57
A1 79.80% 76.23% 77.97
A2 68.61% 62.61% 65.47
A3 73.95% 50.87% 60.27
A4 78.65% 68.63% 73.30
A5 75.00% 60.00% 66.67
AM-ADV 61.64% 46.05% 52.71
AM-CAU 76.19% 43.84% 55.65
AM-DIR 53.33% 37.65% 44.14
AM-DIS 80.56% 63.44% 70.98
AM-EXT 100.00% 46.88% 63.83
AM-LOC 64.48% 51.52% 57.27
AM-MNR 62.90% 45.35% 52.70
AM-MOD 98.64% 92.38% 95.41
AM-NEG 98.21% 95.65% 96.92
AM-PNC 56.67% 44.35% 49.76
AM-PRD 0.00% 0.00% 0.00
AM-REC 0.00% 0.00% 0.00
AM-TMP 83.37% 71.94% 77.23
R-A0 94.29% 88.39% 91.24
R-A1 85.93% 74.36% 79.73
R-A2 100.00% 37.50% 54.55
R-A3 0.00% 0.00% 0.00
R-A4 0.00% 0.00% 0.00
R-AM-ADV 0.00% 0.00% 0.00
R-AM-CAU 0.00% 0.00% 0.00
R-AM-EXT 0.00% 0.00% 0.00
R-AM-LOC 90.00% 42.86% 58.06
R-AM-MNR 66.67% 33.33% 44.44
R-AM-TMP 75.00% 40.38% 52.50
V 98.86% 98.86% 98.86
Table 3: Overall results (top) and detailed results on
the WSJ test (bottom).
0086132. Computer time was provided by NSF
ARI Grant #CDA-9601817, NSF MRI Grant #CNS-
0420873, NASA AIST grant #NAG2-1646, DOE
SciDAC grant #DE-FG02-04ER63870, NSF spon-
sorship of the National Center for Atmospheric Re-
search, and a grant from the IBM Shared University
Research (SUR) program.
Special thanks to Matthew Woitaszek, Theron Vo-
ran and the other administrative team of the Hemi-
sphere and Occam Beowulf clusters. Without these
the training would never be possible.
References
Xavier Carreras and Llu??s Ma`rquez. 2005. n Introduction to the CoNLL-2005
Shared Task: Semantic Role Labeling. In Proceedings of CoNLL-2005.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic labeling of semantic roles.
Computational Linguistics, 28(3):245?288.
Kadri Hacioglu and Wayne Ward. 2003. Target word detection and semantic
role chunking using support vector machines. In Proceedings of the Human
Language Technology Conference, Edmonton, Canada.
Kadri Hacioglu, Sameer Pradhan, Wayne Ward, James Martin, and Dan Jurafsky.
2003. Shallow semantic parsing using support vector machines. Technical
Report TR-CSLR-2003-1, Center for Spoken Language Research, Boulder,
Colorado.
Kadri Hacioglu, Sameer Pradhan, Wayne Ward, James Martin, and Daniel Ju-
rafsky. 2004. Semantic role labeling by tagging syntactic chunks. In Pro-
ceedings of the 8th Conference on CoNLL-2004, Shared Task ? Semantic Role
Labeling.
Kadri Hacioglu. 2004. A lightweight semantic chunking model based on tagging.
In Proceedings of the Human Language Technology Conference /North Amer-
ican chapter of the Association of Computational Linguistics (HLT/NAACL),
Boston, MA.
Paul Kingsbury and Martha Palmer. 2002. From Treebank to PropBank. In
Proceedings of the 3rd International Conference on Language Resources and
Evaluation (LREC-2002), Las Palmas, Canary Islands, Spain.
Taku Kudo and Yuji Matsumoto. 2000. Use of support vector learning for chunk
identification. In Proceedings of the 4th Conference on CoNLL-2000 and
LLL-2000, pages 142?144.
Taku Kudo and Yuji Matsumoto. 2001. Chunking with support vector machines.
In Proceedings of the 2nd Meeting of the North American Chapter of the As-
sociation for Computational Linguistics (NAACL-2001).
Sameer Pradhan, Kadri Hacioglu, Wayne Ward, James Martin, and Dan Jurafsky.
2003. Semantic role parsing: Adding semantic structure to unstructured text.
In Proceedings of the International Conference on Data Mining (ICDM 2003),
Melbourne, Florida.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James Martin, and Dan Jurafsky.
2004. Shallow semantic parsing using support vector machines. In Proceed-
ings of the Human Language Technology Conference/North American chapter
of the Association of Computational Linguistics (HLT/NAACL), Boston, MA.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James Martin, and Dan Jurafsky.
2005. Semantic role labeling using different syntactic views. In Proceedings
of the Association for Computational Linguistics 43rd annual meeting (ACL-
2005), Ann Arbor, MI.
L. A. Ramshaw and M. P. Marcus. 1995. Text chunking using transformation-
based learning. In Proceedings of the Third Annual Workshop on Very Large
Corpora, pages 82?94. ACL.
Mihai Surdeanu, Sanda Harabagiu, John Williams, and Paul Aarseth. 2003. Us-
ing predicate-argument structures for information extraction. In Proceedings
of the 41st Annual Meeting of the Association for Computational Linguistics,
Sapporo, Japan.
Nianwen Xue and Martha Palmer. 2004. Calibrating features for semantic role
labeling. In Proceedings of the Conference on Empirical Methods in Natural
Language Processing, Barcelona, Spain.
220
Proceedings of NAACL HLT 2009: Short Papers, pages 77?80,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Generating Synthetic Children's Acoustic Models from Adult Models 
 
Andreas Hagen, Bryan Pellom, and Kadri Hacioglu 
Rosetta Stone Labs  
{ahagen, bpellom, khacioglu}@rosettastone.com 
 
 
Abstract 
This work focuses on generating children?s 
HMM-based acoustic models for speech rec-
ognition from adult acoustic models. Collect-
ing children?s speech data is more costly 
compared to adult?s speech. The patent-
pending method developed in this work re-
quires only adult data to estimate synthetic 
children?s acoustic models in any language 
and works as follows: For a new language 
where only adult data is available, an adult 
male and an adult female model is trained. A 
linear transformation from each male HMM 
mean vector to its closest female mean vector 
is estimated. This transform is then scaled to a 
certain power and applied to the female model 
to obtain a synthetic children?s model. In a 
pronunciation verification task the method 
yields 19% and 3.7% relative improvement on 
native English and Spanish children?s data, re-
spectively, compared to the best adult model. 
For Spanish data, the new model outperforms 
the available real children?s data based model 
by 13% relative. 
1 Introduction 
Language learning is becoming more and more 
important in the age of globalization. Depending 
on their work or cultural situation some people are 
confronted with various different languages on a 
daily basis. While it is very desirable to learn lan-
guages at any age, language learning, among other 
learning experiences, is comparably simpler for 
children than for adults and should therefore be 
encouraged at early ages. 
Even though the children?s language learning mar-
ket is highly important, comprising effective 
speech recognition tools for pronunciation assess-
ment is relatively hard due to the special characte-
ristics of children?s speech and the limited 
availability of children?s speech data in many lan-
guages in the speech research community. Adult 
speech data is usually easier to obtain. By under-
standing the characteristics of children?s speech the 
unconditional need for children?s speech data can 
be lessened by altering adult acoustic models such 
that they are suitable for children?s speech. 
Children?s speech has higher pitch and formants 
than female speech. Further, female speech has 
higher pitch and formants than male speech. Child-
ren?s speech is more variable than female speech, 
and, as research has shown, female speech is more 
variable than male speech (Lee et al, 1999). Given 
this transitive chain of argumentation, the trans-
formation from a male to a female acoustic model 
can be estimated for a language and applied (at a 
certain adjustable degree) to the female model. 
This process results in a synthetic children?s 
speech model designed on the basis of the female 
model. Therefore, for a new language an effective 
synthetic children?s acoustic model can be derived 
without the need of children?s data (Hagen et al, 
2008). 
2 Related Work  
Extensive research has been done in the field of 
children?s speech analysis and recognition in the 
past few years. A detailed overview of children?s 
speech characteristics can be found in (Lee et al, 
1999). The paper presents research results showing 
the higher variability in speech characteristics 
among children compared to adult speech. The 
properties of children?s speech that were re-
searched were duration of vowels and sentences, 
pitch, and formant locations. 
When designing acoustic models specially suited 
for children, properties as the formant locations 
and higher variability of children?s speech need to 
be accounted for. The best solution for building 
children?s speech models is to collect children?s 
speech data and to train models from scratch (Ha-
77
gen et al, 2003, Cosi et al 2005). Researchers 
have also tried to apply adult acoustic models us-
ing speaker normalization techniques to recognize 
children?s speech (Elenius et al, 2005, Potamianos 
et al 1997). Adult acoustic models were adapted 
towards children?s speech. A limited amount of 
children?s speech data was available for adapta-
tion. In (Gustafson et al, 2002) children?s voices 
were transformed before being sent to the recog-
nizer using adult acoustic models. In (Claes et al, 
1997) children?s acoustic models were built based 
on a VTL adaptation of cepstral parameters based 
on the third formant frequency. The method 
showed to be effective for building children?s 
speech models. 
3 Building Synthetic Children?s Models 
from Adult Models 
As mentioned in Section 1, research has shown 
that pitch and formants of children?s speech are 
higher than for female speech. Female speech has 
higher pitch and formants than male speech. In 
order to exploit these research results a transforma-
tion from a male acoustic model to a female acous-
tic model can be derived. This transformation will 
map a male model as close as possible to a female 
model. The transformation can be adjusted and 
applied to the female model. The resulting synthet-
ic model can be tested on children?s data. 
Parameters that are subject to transformation in 
this process are the mean vectors of the HMM 
states. The transformation can be represented as a 
square matrix in the dimension of the mean vec-
tors. The transformation chosen in this approach is 
therefore linear and is for example capable of 
representing a vocal tract length adaptation as it 
was shown in (Pitz et al, 2005). Linear transfor-
mations (i.e. matrices) are also chosen in adapta-
tion approaches as MAPLR and MLLR, whose 
benefit has been shown to be additive to the benefit 
of VTLN in speaker adaptation applications. A 
linear transform in the form of a matrix is therefore 
well suited due to its expressive power as well as 
its mathematical manageability. 
3.1 Transformation Matrix 
The transformation matrix used in this approach is 
estimated by mapping the male to the female 
acoustic model, such that each HMM state mean 
vector in the male model is assigned a correspond-
ing mean vector in the female model. Information 
used in the mapping process is the basic phoneme 
and context. The resulting mean vector pairs are 
used as source and target features in the training 
process of the transformation matrix. During train-
ing the matrix is initialized as the identity matrix 
and the estimate of the mapping is refined by gra-
dient descent. In a typical acoustic model there are 
several hundred, sometimes thousands, of these 
mean vector pairs to train the transformation ma-
trix. The expression that needs to be minimized is: 
2
),(
)(minarg yAxT
pairsyxA
?= ?   
where T is the error-minimizing transformation 
matrix; x is a male model?s source vector and y it 
corresponding female model?s target vector.  
In this optimization process the Matrix A is initia-
lized as the identity matrix. Each matrix entry ija is 
updated (to the new value 'ija ) in the following way 
by gradient descent: 
( ) jiiijij xyxAkaa ?+='  
where iA  is the i-th line of matrix A and k deter-
mines the descent step size (k<0 and incorporates 
the factor of 2 resulting from the differentiation). 
The gradient descent needs to be run multiple 
times over all vector pairs (x,y) for the matrix to 
converge to an acceptable approximation which is 
called the transformation matrix T. 
3.2 Synthetic Children?s Model Creation 
The transformation matrix can be applied to the 
female model in order to create a new synthetic 
acoustic model which should suit children?s speech 
better than adult acoustic models. It is unlikely that 
the transformation applied ?as is? will result in the 
best model possible, therefore the transformation 
can be altered (amplified or weakened) in order to 
yield the best results. An intuitive way to alter the 
impact of the transformation is taking the matrix T 
to a certain power p. Synthetic models can be 
created by applying pT  to the female model1, for 
various values p. If children?s data is available for 
evaluation purposes, the best value of p can be de-
termined. The power p is claimed to be language 
independent. It might vary in nuances, but experi-
                                                          
1
 Taking a matrix to the power of p is meant in the sense 
TT
pp
=
/1
, IdentityT =0 , TT =1  
78
ments have shown that a value around 0.25 is a 
reasonable choice. 
3.3 Transformation Algorithm 
The previous section presented the theoretical 
means necessary for the synthetic children?s model 
creation process. The precise, patent-pending algo-
rithm to create a synthetic children?s model in a 
new language is as follows (Hagen et al, 2008): 
 
1. Train a male and a female acoustic model 
2. Estimate the transform T from the male 
to the female model 
3. Determine the power p by which the 
transform T should be adjusted 
4. Apply pT  to the female acoustic model 
to create the synthetic children?s model 
 
Step 3, the determination of the power p, can be 
done in two different ways. If children?s test data 
in the relevant language is available, various mod-
els based on different p-values can be evaluated 
and the best one chosen. If there is no children?s 
data available in a new language, p can be esti-
mated by evaluations in a language where there is 
enough male, female, and children?s speech data 
available. The claim here is that the power p is rel-
atively language independent and estimating p in a 
different language is superior to a simple guess. 
4 Experiments 
The algorithm was tested on two languages: US 
English and Spanish. For both languages sufficient 
male, female, and children?s speech data was 
available (more than 20 hours) in order to train 
valid acoustic models and to have reference child-
ren?s acoustic models available. For English test 
data we used a corpus of 22 native speakers in the 
age range of 5 to 14. The number of utterances is 
2,182. For Spanish test data the corpus is com-
prised of 19 speakers in the age range of 8 to 13 
years. The number of utterances is 2,598. 
The transform from the male to the female model 
was estimated in English. The power of p was 
gradually increased and the transformation matrix 
was adjusted. With this adjusted matrix pT  a syn-
thetic children?s model was built. This synthetic 
children?s model was evaluated on children?s test 
data and the results were compared to the reference 
children?s model?s and the female model?s perfor-
mance. 
When speech is evaluated in a language learning 
system, the first step is utterance verification, 
meaning the task of evaluating if the user actually 
tried to produce the desired utterance. The Equal 
Error Rate (EER) on the utterance level is a means 
of evaluating this performance. For each utterance 
an in- and out-of-grammar likelihood score is de-
termined. The EER operating points, determined 
by the cutting point of the two distributions (in-
grammar and out-of-grammar), are reported as an 
error metric. Figure 1 shows the EER values of the 
synthetic model applied to children?s data. 
 
 
 
Figure 1: Synthetic model?s EER performance de-
pending on the power p used for model creation. 
 
It can be seen that the best performance is reached 
at about p=0.25. The overview of the results is 
given in Table 1. 
 
 Equal Error Rate  
Real Children?s Model 1.90% 
Male Model 4.07% 
Female Model 2.92% 
Synthetic Model 2.36% 
 
Table 1: EER numbers when using a real children?s 
model compared to a male, female, and synthetic 
model for children?s data evaluation. 
 
The results show that the synthetic children?s mod-
el yields good classification results when applied 
to children?s data. The gold standard, the real 
children?s model application, results in the best 
EER performance. 
If the same evaluation scenario is applied to Span-
ish, a very similar picture evolves. Figure 2 shows 
the EER results versus transformation power p for 
Spanish children?s data. 
 
79
  
Figure 2: Spanish synthetic model?s EER perfor-
mance depending on the power p used for model 
creation. 
 
In Figure 2 it can be seen that the optimal setting 
for p is about 0.27. This value is very similar to the 
one found for US English, which supports, but cer-
tainly does not prove, the language independence 
claim. Results for Spanish are given in Table 2. 
 
 Equal Error Rate 
Real Children?s model 2.40% 
Male model 5.62% 
Female model 2.17% 
Synthetic model 2.09% 
 
Table 2: EER numbers for Spanish when using a 
real children?s model compared to a male, female, 
and synthetic model for Spanish children?s data 
evaluation. 
 
Similar to English, the Spanish synthetic model 
performs better than the female model on child-
ren?s speech. Interestingly, the acoustic model 
purely trained on children?s data performs worse 
than the female and the synthetic model. It is not 
clear why the children?s model does not outper-
form the female and the synthetic model; an expla-
nation could be diverse and variable training data 
that hurts classification performance. 
It can be seen that for US English and Spanish the 
power p used to adjust the transformation is about 
0.25. Therefore, for a new language where only 
adult data is available, the transformation from the 
male to the female model can be estimated and 
applied to the female model (after being adjusted 
by p=0.25). The resulting synthetic model will 
work reasonably well and could be refined as soon 
as children?s data becomes available. 
 
5 Conclusion 
This work presented a new technique to create 
children?s acoustic models from adult acoustic 
models without the need for children?s training 
data when applied to a new language. While it can 
be assumed that the availability of children?s data 
would improve the resulting acoustic models, the 
approach is effective if children?s data is not avail-
able. It will be interesting to see how performance 
of this technique compares to adapting adult mod-
els by adaptation techniques, i.e. MLLR, when li-
mited amounts of children?s data are available. 
Two scenarios are possible: With increasing 
amount of children?s data speaker adaptation will 
draw even and/or be superior. The other possibility 
is that the presented technique yields better results 
regardless how much real children?s data is availa-
ble, due to the higher variability and noise-
pollution of children?s data. 
References  
Claes, T., Dologlou, I, ten Bosch, L., Van Compernolle, 
D. 1997. New Transformations of Cepstral Parame-
ters for Automatic Vocal Tract Length Normalization 
in Speech Recognition, 5th Europ. Conf. on Speech 
Comm. and Technology, Vol. 3: 1363-1366. 
Cosi, P., Pellom, B. 2005. Italian children's speech rec-
ognition for advanced interactive literacy tutors. 
Proceedings Interspeech, Lisbon, Portugal. 
Elenius, D. and Blomberg, M. 2005. Adaptation and 
Normalization Experiments in Speech Recognition 
for 4 to 8 Year old Children. Proceedings Inters-
peech, Lisbon, Portugal. 
Gustafson, J., Sj?lander, K. 2002. Voice transformations 
for improving children?s speech recognition in a pub-
licly available dialogue system. ICSLP, Denver. 
Hagen, A., Pellom, B., and Cole, R. 2003. Children's 
Speech Recognition with Application to Interactive 
Books and Tutors. Proceedings ASRU, USA. 
Lee, S., Potamianos, A., and Narayanan, S. 1999. 
Acoustics of children's speech: Developmental 
changes of temporal and spectral parameter. J. 
Acoust. Soc. Am., Vol. 105(3):1455-1468. 
Pitz, M., Ney, H. 2005. Vocal Tract Normalization 
Equals Linear Transformation in Cepstral Space. 
IEEE Trans. Speech & Audio Proc., 13(5): 930-944. 
Potamianos, A., Narayanan, S., and Lee, S. 1997. Auto-
matic Speech Recognition for Children. Proceedings 
Eurospeech, Rhodes, Greece. 
Hagen, A., Pellom, B., and Hacioglu, K. 2008. Method 
for Creating a Speech Model. US Patent Pending.  
 
80
