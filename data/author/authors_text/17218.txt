Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 860?865,
Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational Linguistics
___________________  
*Corresponding author 
Cross-lingual Opinion Analysis via Negative Transfer Detection  
Lin Gui1,2, Ruifeng Xu1*, Qin Lu2, Jun Xu1, Jian Xu2, Bin Liu1, Xiaolong Wang1 
1Key Laboratory of Network Oriented Intelligent Computation, Shenzhen Graduate School, 
Harbin Institute of Technology, Shenzhen 518055 
2Department Of Computing, the Hong Kong Polytechnic University 
guilin.nlp@gmail.com, xuruifeng@hitsz.edu.cn, csluqin@comp.polyu.edu.hk, xujun@hitsz.edu.cn, 
csjxu@comp.polyu.edu.hk,{bliu,wangxl}@insun.hit.edu.cn 
 
Abstract 
Transfer learning has been used in opin-
ion analysis to make use of available lan-
guage resources for other resource scarce 
languages. However, the cumulative 
class noise in transfer learning adversely 
affects performance when more training 
data is used. In this paper, we propose a 
novel method in transductive transfer 
learning to identify noises through the 
detection of negative transfers. Evalua-
tion on NLP&CC 2013 cross-lingual 
opinion analysis dataset shows that our 
approach outperforms the state-of-the-art 
systems. More significantly, our system 
shows a monotonic increase trend in per-
formance improvement when more train-
ing data are used.  
1 Introduction 
Mining opinions from text by identifying their 
positive and negative polarities is an important 
task and supervised learning methods have been 
quite successful. However, supervised methods 
require labeled samples for modeling and the 
lack of sufficient training data is the performance 
bottle-neck in opinion analysis especially for re-
source scarce languages. To solve this problem, 
the transfer leaning method (Arnold et al, 2007) 
have been used to make use of samples from a 
resource rich source language to a resource 
scarce target language, also known as cross lan-
guage opinion analysis (CLOA). 
In transductive transfer learning (TTL) where 
the source language has labeled data and the tar-
get language has only unlabeled data, an algo-
rithm needs to select samples from the unlabeled 
target language as the training data and assign 
them with class labels using some estimated con-
fidence. These labeled samples in the target lan-
guage, referred to as the transferred samples, also 
have a probability of being misclassified. During 
training iterations, the misclassification introduc-
es class noise which accumulates, resulting in a 
so called negative transfer that affects the classi-
fication performance.  
In this paper, we propose a novel method 
aimed at reducing class noise for TTL in CLOA. 
The basic idea is to utilize transferred samples 
with high quality to identify those negative trans-
fers and remove them as class noise to reduce 
noise accumulation in future training iterations. 
Evaluations on NLP&CC 2013 CLOA evalua-
tion data set show that our algorithm achieves the 
best result, outperforming the current state-of-
the-art systems. More significantly, our system 
shows a monotonic increasing trend in perfor-
mance when more training data are used beating 
the performance degradation curse of most trans-
fer learning methods when training data reaches 
certain size. 
The rest of the paper is organized as follows. 
Section 2 introduces related works in transfer 
learning, cross lingual opinion analysis, and class 
noise detection technology. Section 3 presents 
our algorithm. Section 4 gives performance eval-
uation. Section 5 concludes this paper. 
2 Related works 
TTL has been widely used before the formal 
concept and definition of TTL was given in (Ar-
nold, 2007). Wan introduced the co-training 
method into cross-lingual opinion analysis (Wan, 
2009; Zhou et al, 2011), and Aue et al intro-
duced transfer learning into cross domain analy-
sis (Aue, 2005) which solves similar problems. 
In this paper, we will use the terms source lan-
guage and target language to refer to all cross 
lingual/domain analysis. 
Traditionally, transfer learning methods focus 
on how to estimate the confidence score of trans-
ferred samples in the target language or domain 
(Blitzer et al 2006, Huang et al, 2007; Sugiya-
ma et al, 2008, Chen et al 2011, Lu et al, 2011). 
In some tasks, researchers utilize NLP tools such 
as alignment to reduce the bias towards that of 
860
 the source language in transfer learning (Meng et 
al., 2012). However, detecting misclassification 
in transferred samples (referred to as class noise) 
and reducing negative transfers are still an unre-
solved problem. 
There are two basic methods for class noise 
detection in machine learning. The first is the 
classification based method (Brodley and Friedl, 
1999; Zhu et al 2003; Zhu 2004; Sluban et al, 
2010) and the second is the graph based method 
(Zighed et al 2002; Muhlenbach et al 2004; 
Jiang and Zhou, 2004). Class noise detection can 
also be applied to semi-supervised learning be-
cause noise can accumulate in iterations too. Li 
employed Zighed?s cut edge weight statistic 
method in self-training (Li and Zhou, 2005) and 
co-training (Li and Zhou, 2011). Chao used Li?s 
method in tri-training (Chao et al 2008). (Fuku-
moto et al 2013) used the support vectors to de-
tect class noise in semi-supervised learning.  
In TTL, however, training and testing samples 
cannot be assumed to have the same distributions. 
Thus, noise detection methods used in semi-
supervised learning are not directly suited in 
TTL. Y. Cheng has tried to use semi-supervised 
method (Jiang and Zhou, 2004) in transfer learn-
ing (Cheng and Li, 2009). His experiment 
showed that their approach would work when the 
source domain and the target domain share simi-
lar distributions. How to reduce negative trans-
fers is still a problem in transfer learning. 
3 Our Approach 
In order to reduce negative transfers, we pro-
pose to incorporate class noise detection into 
TTL. The basic idea is to first select high quality 
labeled samples after certain iterations as indica-
tor to detect class noise in transferred samples. 
We then remove noisy samples that cause nega-
tive transfers from the current accumulated train-
ing set to retain an improved set of training data 
for the remainder of the training phase. This neg-
ative sample reduction process can be repeated 
several times during transfer learning. Two ques-
tions must be answered in this approach: (1) how 
to measure the quality of transferred samples, 
and (2) how to utilize high quality labeled sam-
ples to detect class noise in training data. 
3.1 Estimating Testing Error 
To determine the quality of the transferred 
samples that are added iteratively in the learning 
process, we cannot use training error to estimate 
true error because the training data and the test-
ing data have different distributions. In this work, 
we employ the Probably Approximately Correct 
(PAC) learning theory to estimate the error 
boundary. According to the PAC learning theory, 
the least error boundary ? is determined by the 
size of the training set m and the class noise rate 
?, bound by the following relation: 
  ?   (   )                      ( ) 
In TTL, m increases linearly, yet ? is multi-
plied in each iteration. This means the signifi-
cance of m to performance is higher at the begin-
ning of transfer learning and gradually slows 
down in later iterations. On the contrary, the in-
fluence of class noise increases. That is why per-
formance improves initially and gradually falls to 
negative transfer when noise accumulation out-
performs the learned information as shown in 
Fig.1. In TTL, transferred samples in both the 
training data and test data have the same distribu-
tion. This implies that we can apply the PAC 
theory to analyze the error boundary of the ma-
chine learning model using transferred data. 
 
Figure 1 Negative transfer in the learning process 
According to PAC theorem with an assumed 
fixed probability ? (Angluin and Laird, 1988), 
the least error boundary ? is given by:   
  ?   (   ? )  ( (   ) )       ( ) 
where N is a constant decided by the hypothesis 
space.  In any iteration during TTL, the hypothe-
sis space is the same and the probability ? is 
fixed. Thus the least error boundary is deter-
mined by the size of the transferred sample m 
and the class noise of transferred samples ?. Ac-
cording to (2), we apply a manifold assumption 
based method to estimate ?. Let T be the number 
of iterations to serve as one period. We then es-
timate the least error boundary before and after 
each T to measure the quality of transferred sam-
ples during each T. If the least error boundary is 
reduced, it means that transferred samples used 
in this period are of high quality and can improve 
the performance. Otherwise, the transfer learning 
algorithm should stop.  
861
 3.2 Estimating Class Noise 
For formula (2) to work, we need to know the 
class noise rate ? to calculate the error boundary. 
Obviously, we cannot use conditional probabili-
ties from the training data in the source language 
to estimate the noise rate ? of the transferred 
samples because the distribution of source lan-
guage is different from that of target language. 
Consider a KNN graph on the transferred 
samples using any similarity metric, for example, 
cosine similarity, for any two connected vertex 
(     )and (     ) in the graph from samples to 
classes, the edge weight is given by: 
       (     )                         ( ) 
Furthermore, a sign function for the two vertices 
(     )and (     ), is defined as: 
    {
          
          
                   ( ) 
According to the manifold assumption, the 
conditional probability  (  |  ) can be approxi-
mated by the frequency of  (     ) which is 
equal to  (     ). In opinion annotations, the 
agreement of two annotators is often no larger 
than 0.8. This means that for the best cases 
 (     )=0.2. Hence     follows a Bernoulli 
distribution with p=0.2 for the best cases in 
manual annotations.  
Let      (     )  be the vertices that are 
connected to the     vertex, the statistical magni-
tude of the     vertex can be defined as: 
   ?                                 ( )  
where j refers to the     vertex that is connected 
to the     vertex.  
From the theory of cut edge statics, we know 
that the expectation of    is: 
    (     )  ?                  ( )  
And the variance of    is: 
  
   (     ) (     )  ?    
 
 ( )  
By the Center Limit Theorem (CLT),    fol-
lows the normal distribution: 
(     )
  
  (   )                    ( )  
To detect the noise rate of a sample (     ) , 
we can use (8) as the null hypothesis to test the 
significant level. Let    denotes probability of 
the correct classification for a transferred sample. 
   should follow a normal distribution,  
   
 
?    
?  
 
(    )
 
   
   
  
           ( )  
Note that experiments (Li and Zhou, 2011; 
Cheng and Li, 2009; Brodley and Friedl, 1999) 
have shown that     is related to the error rate of 
the example (     ), but it does not reflect the 
ground-truth probability in statistics. Hence we 
assume the class noise rate of example (     ) is: 
                              (  ) 
 We take the general significant level of 0.05 
to reject the null hypothesis. It means that if    of 
(     ) is larger than 0.95, the sample will be 
considered as a class noisy sample. Furthermore, 
   can be used to estimate the average class noise 
rate of a transferred samples in (2). 
In our proposed approach, we establish the 
quality estimate period T to conduct class noise 
detection to estimate the class noise rate of trans-
ferred samples. Based on the average class noise 
we can get the least error boundary so as to tell if 
an added sample is of high quality. If the newly 
added samples are of high quality, they can be 
used to detect class noise in transferred training 
data. Otherwise, transfer learning should stop. 
The flow chart for negative transfer is in Fig.2. 
SLS(labeled)
TLS
(unlabeled)
Classifier
Top k
TS
 period 1
TS
period 2
TS
 period n
KNN 
graph
Estimate ?i and ?n 
?n ? ?n-1?
Output SLS and TS 
(period 1 to n-1)
No
Yes
Del te TS
 ?i? 0.95 
period 1 to n-1
Input 
Input 
T iterations per period
Transfer
process
Negative
transfer
detection
Figure 2 Flow charts of negative transfer detection 
In the above flow chart, SLS and TLS refer to 
the source and target language samples, respec-
tively. TS refers to the transferred samples. Let T 
denote quality estimate period T in terms of itera-
tion numbers. The transfer process select k sam-
ples in each iteration. When one period of trans-
fer process finishes, the negative transfer detec-
tion will estimate the quality by comparing and 
either select the new transferred samples or re-
move class noise accumulated up to this iteration. 
4 Experiment 
4.1 Experiment Setting 
The proposed approach is evaluated on the 
NLP&CC 2013 cross-lingual opinion analysis (in 
862
 short, NLP&CC) dataset 1 . In the training set, 
there are 12,000 labeled English Amazon.com 
products reviews, denoted by Train_ENG, and 
120 labeled Chinese product reviews, denoted as 
Train_CHN, from three categories, DVD, BOOK, 
MUSIC. 94,651 unlabeled Chinese products re-
views from corresponding categories are used as 
the development set, denoted as Dev_CHN. In 
the testing set, there are 12,000 Chinese product 
reviews (shown in Table.1). This dataset is de-
signed to evaluate the CLOA algorithm which 
uses Train_CHN, Train_ENG and Dev_CHN to 
train a classifier for Test_CHN. The performance 
is evaluated by the correct classification accuracy 
for each category in Test_CHN2:  
          
                                  
    
 
where c is either DVD, BOOK or MUSIC. 
Team DVD Book Music 
Train_CHN 40 40 40 
Train_ENG 4000 4000 4000 
Dev_CHN 17814 47071 29677 
Test_CHN 4000 4000 4000 
Table.1 The NLP&CC 2013 CLOA dataset 
In the experiment, the basic transfer learning 
algorithm is co-training. The Chinese word seg-
mentation tool is ICTCLAS (Zhang et al 2003) 
and Google Translator3 is the MT for the source 
language. The monolingual opinion classifier is 
SVMlight4, word unigram/bigram features are em-
ployed. 
4.2 CLOA Experiment Results 
Firstly, we evaluate the baseline systems 
which use the same monolingual opinion classi-
fier with three training dataset including 
Train_CHN, translated Train_ENG and their un-
ion, respectively.  
 DVD Book Music Accuracy 
Train_CHN 0.552 0.513 0.500 0.522 
Train_ENG 0.729 0.733 0.722 0.728 
Train_CHN 
+Train_ENG 
0.737 0.722 0.742 0.734 
Table.2 Baseline performances  
It can be seen that using the same method, the 
classifier trained by Train_CHN are on avergage 
20% worse than the English counter parts.The 
combined use of Train_CHN and translated 
Train_ENG, however, obtained similar 
                                                 
1http://tcci.ccf.org.cn/conference/2013/dldoc/evdata03.zip 
2http://tcci.ccf.org.cn/conference/2013/dldoc/evres03.pdf 
3https://translate.google.com 
4http://svmlight.joachims.org/ 
performance to the English counter parts. This 
means the predominant training comes from the 
English training data. 
In the second set of experiment, we compare  
our proposed approach to the official results in 
NLP&CC 2013 CLOA evaluation and the result 
is given in Table 3. Note that in Table 3, the top 
performer of NLP&CC 2013 CLOA evaluation 
is the HLT-HITSZ system(underscored in the 
table), which used the co-training method in 
transfer learning (Gui et al 2013), proving that 
co-training is quite effective for cross-lingual 
analysis. With the additional negative transfer 
detection, our proposed approach achieves the 
best performance on this dataset outperformed 
the top system (by HLT-HITSZ) by a 2.97% 
which translate to 13.1% error reduction im-
provement to this state-of-the-art system as 
shown in the last row of Table 3.     
Team DVD Book Music Accuracy 
BUAA 0.481 0.498 0.503 0.494 
BISTU 0.647 0.598 0.661 0.635 
HLT-HITSZ 0.777 0.785 0.751 0.771 
THUIR 0.739 0.742 0.733 0.738 
SJTU 0.772 0.724 0.745 0.747 
WHU 0.783 0.770 0.760 0.771 
Our approach 0.816 0.801 0.786 0.801 
Error 
Reduction 
0.152 0.072 0.110 0.131 
Table.3 Performance compares with NLP&CC 
2013 CLOA evaluation results 
To further investigate the effectiveness of our 
method, the third set of experiments evaluate the 
negative transfer detection (NTD) compared to 
co-training (CO) without negative transfer 
detection as shown in Table.4 and Fig.3 Here, we 
use the union of Train_CHN and Train_ENG as 
labeled data and Dev_CHN as unlabeled data to 
be transferred in the learning algorithms. 
 DVD Book Music Mean 
NTD 
Best case 0.816 0.801 0.786 0.801 
Best period 0.809 0.798 0.782 0.796 
Mean 0.805 0.795 0.781 0.794 
CO 
Best case 0.804 0.796 0.783 0.794 
Best period 0.803 0.794 0.781 0.792 
Mean 0.797 0.790 0.775 0.787 
Table.4 CLOA performances 
Taking all categories of data, our proposed 
method improves the overall average precision 
(the best cases) from 79.4% to 80.1% when 
compared to the state of the art system which 
translates to error reduction of 3.40% (p-
value?0.01 in Wilcoxon signed rank test). Alt-
hough the improvement does not seem large, our 
863
  
   
Figure 3 Performance of negative transfer detection vs. co-training 
algorithm shows a different behavior in that it 
can continue to make use of available training 
data to improve the system performance. In other 
words, we do not need to identify the tipping 
point where the performance degradation can 
occur when more training samples are used. Our 
approach has also shown the advantage of stable 
improvement.  
In the most practical tasks, co-training based 
approach has the difficulty to determine when to 
stop the training process because of the negative 
transfer. And thus, there is no sure way to obtain 
the above best average precision. On the contrary, 
the performance of our proposed approach keeps 
stable improvement with more iterations, i.e. our 
approach has a much better chance to ensure the 
best performance. Another experiment is con-
ducted to compare the performance of our pro-
posed transfer learning based approach with su-
pervised learning. Here, the achieved perfor-
mance of 3-folder cross validation are given in 
Table 5. 
 DVD Book Music Average 
Supervised 0.833 0.800 0.801 0.811 
Our approach 0.816 0.801 0.786 0.801 
Table.5 Comparison with supervised learning  
The accuracy of our approach is only 1.0% 
lower than the supervised learning using 2/3 of 
Test_CHN. In the BOOK subset, our approach 
achieves match result. Note that the performance 
gap in different subsets shows positive correla-
tion to the size of Dev_CHN. The more samples 
are given in Dev_CHN, a higher precision is 
achieved even though these samples are unla-
beled. According to the theorem of PAC, we 
know that the accuracy of a classifier training 
from a large enough training set with confined 
class noise rate will approximate the accuracy of 
classifier training from a non-class noise training 
set. This experiment shows that our proposed 
negative transfer detection controls the class 
noise rate in a very limited boundary. Theoreti-
cally speaking, it can catch up with the perfor-
mance of supervised learning if enough unla-
beled samples are available. In fact, such an ad-
vantage is the essence of our proposed approach.  
5 Conclusion 
In this paper, we propose a negative transfer 
detection approach for transfer learning method 
in order to handle cumulative class noise and 
reduce negative transfer in the process of transfer 
learning. The basic idea is to utilize high quality 
samples after transfer learning to detect class 
noise in transferred samples. We take cross lin-
gual opinion analysis as the data set to evaluate 
our method. Experiments show that our proposed 
approach obtains a more stable performance im-
provement by reducing negative transfers. Our 
approach reduced 13.1% errors than the top sys-
tem on the NLP&CC 2013 CLOA evaluation 
dataset. In BOOK category it even achieves bet-
ter result than the supervised learning. Experi-
mental results also show that our approach can 
obtain better performance when the transferred 
samples are added incrementally, which in pre-
vious works would decrease the system perfor-
mance. In future work, we plan to extend this 
method into other language/domain resources to 
identify more transferred samples.  
Acknowledgement 
This research is supported by NSFC 61203378, 
61300112, 61370165, Natural Science Founda-
tion of GuangDong S2013010014475, MOE 
Specialized Research Fund for the Doctoral Pro-
gram of Higher Education 20122302120070,  
Open Projects Program of National Laboratory 
of Pattern Recognition?Shenzhen Foundational 
Research Funding JCYJ20120613152557576, 
JC201005260118A, Shenzhen International Co-
operation Research Funding 
GJHZ20120613110641217 and Hong Kong Pol-
ytechnic University Project code Z0EP. 
DVD Book Music 
864
 Reference 
Angluin, D., Laird, P. 1988. Learning from Noisy 
Examples. Machine Learning, 2(4): 343-370. 
Arnold, A., Nallapati, R., Cohen, W. W. 2007. A 
Comparative Study of Methods for Transductive 
Transfer Learning. In Proc. 7th IEEE ICDM Work-
shops, pages 77-82. 
Aue, A., Gamon, M. 2005. Customizing Sentiment 
Classifiers to New Domains: a Case Study, In Proc. 
of t RANLP. 
Blitzer, J., McDonald, R., Pereira, F. 2006. Domain 
Adaptation with Structural Correspondence Learn-
ing. In Proc. EMNLP, 120-128. 
Brodley, C. E., Friedl, M. A. 1999. Identifying and 
Eliminating Mislabeled Training Instances. Journal 
of Artificial Intelligence Research, 11:131-167. 
Chao, D., Guo, M. Z., Liu, Y.,  Li, H. F. 2008.  Partic-
ipatory Learning based Semi-supervised Classifica-
tion. In Proc. of 4th ICNC, pages 207-216. 
Cheng, Y., Li, Q. Y. 2009. Transfer Learning with 
Data Edit. LNAI, pages 427?434. 
Chen, M., Weinberger, K. Q.,  Blitzer, J. C. 2011.  
Co-Training for Domain Adaptation. In Proc. of 
23th NIPS. 
Fukumoto, F., Suzuki, Y., Matsuyoshi, S. 2013. Text 
Classification from Positive and Unlabeled Data 
using Misclassified Data Correction. In Proc. of 
51st ACL, pages 474-478. 
Gui, L., Xu, R.,  Xu, J., et al 2013. A Mixed Model 
for Cross Lingual Opinion Analysis. In CCIS, 400, 
pages 93-104. 
Huang, J., Smola, A., Gretton, A., Borgwardt, K.M., 
Scholkopf, B. 2007. Correcting Sample Selection 
Bias by Unlabeled Data. In Proc. of 19th NIPS,  
pages 601-608. 
Jiang, Y., Zhou, Z. H. 2004. Editing Training Data for 
kNN Classifiers with Neural Network Ensemble. In 
LNCS, 3173,  pages 356-361. 
Li, M., Zhou, Z. H. 2005. SETRED: Self-Training 
with Editing. In Proc. of PAKDD, pages 611-621. 
Li, M., Zhou, Z. H. 2011.  COTRADE: Confident Co-
Training With Data Editing. IEEE Transactions on 
Systems, Man, and Cybernetics?Part B: Cyber-
netics, 41(6):1612-1627. 
Lu, B., Tang, C. H., Cardie, C., Tsou, B. K. 2011. 
Joint Bilingual Sentiment Classification with Un-
labeled Parallel Corpora. In Proc. of 49th ACL, 
pages 320-330. 
Meng, X. F., Wei, F. R., Liu, X. H., et al 2012. 
Cross-Lingual Mixture Model for Sentiment Clas-
sification. In Proc. of 50th ACL, pages 572-581. 
Muhlenbach, F., Lallich, S., Zighed, D. A. 2004. 
Identifying and Handling Mislabeled Instances.  
Journal of Intelligent Information System, 22(1): 
89-109. 
Pan, S. J., Yang, Q. 2010. A Survey on Transfer 
Learning, IEEE Transactions on Knowledge and 
Data Engineering, 22(10):1345-1360. 
Sindhwani, V., Rosenberg, D. S. 2008. An RKHS for 
Multi-view Learning and Manifold Co-
Regularization. In Proc. of 25th  ICML, pages 976?
983. 
Sluban, B., Gamberger, D., Lavra, N. 2010.  Advanc-
es in Class Noise Detection. In Proc.19th ECAI,  
pages 1105-1106. 
Sugiyama, M.,  Nakajima, S., Kashima, H., Buenau, 
P.V., Kawanabe, M. 2008. Direct Importance Es-
timation with Model Selection and its Application 
to Covariate Shift Adaptation. In Proc. 20th NIPS. 
Wan, X. 2009. Co-Training for Cross-Lingual Senti-
ment Classification, In Proc. of the 47th Annual 
Meeting of the ACL and the 4th IJCNLP of the 
AFNLP,  235?243. 
Zhang, H. P., Yu, H. K., Xiong, D. Y., and Liu., Q. 
2003. HHMM-based Chinese Lexical Analyzer 
ICTCLAS. In 2nd SIGHAN workshop affiliated 
with 41th ACL, pages 184-187. 
 Zhou, X., Wan X., Xiao, J. 2011. Cross-Language 
Opinion Target Extraction in Review Texts. In 
Proc. of IEEE 12th ICDM, pages 1200-1205. 
Zhu, X. Q., Wu, X. D., Chen, Q. J. 2003.  Eliminating 
Class Noise in Large Datasets. In Proc. of 12th 
ICML, pages 920-927. 
Zhu, X. Q. 2004. Cost-guided Class Noise Handling 
for Effective Cost-sensitive Learning In Proc. of 4th  
IEEE ICDM,  pages 297-304. 
Zighed, D. A., Lallich, S., Muhlenbach, F. 2002.  
Separability Index in Supervised Learning. In Proc. 
of PKDD, pages 475-487. 
865
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 524?528,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
PolyUCOMP: Combining Semantic Vectors with Skip bigrams for  
Semantic Textual Similarity 
 
Jian Xu Qin Lu Zhengzhong Liu 
The Hong Kong Polytechnic University 
Department of Computing 
Hung Hom, Kowloon, Hong Kong 
{csjxu, csluqin, hector.liu}@comp.polyu.edu.hk 
 
 
Abstract 
This paper presents the work of the Hong 
Kong Polytechnic University (PolyUCOMP) 
team which has participated in the Semantic 
Textual Similarity task of SemEval-2012. The 
PolyUCOMP system combines semantic vec-
tors with skip bigrams to determine sentence 
similarity. The semantic vector is used to 
compute similarities between sentence pairs 
using the lexical database WordNet and the 
Wikipedia corpus. The use of skip bigram is 
to introduce the order of words in measuring 
sentence similarity.  
1 Introduction 
Sentence similarity computation plays an im-
portant role in text summarization, classification, 
question answering and social network applica-
tions (Lin and Pantel, 2001; Erkan and Radev, 
2004; Ko et al, 2004; Ou et al, 2011).  The 
SemEval 2012 competition includes a task targeted 
at Semantic Textual Similarity (STS) between sen-
tence pairs (Eneko et al, 2012). Given a set of sen-
tence pairs, participants are required to assign to 
each sentence pair a similarity score. 
Because a sentence has only a limited amount of 
content words, it is not easy to determine sentence 
similarities because of the sparseness issue. 
Hatzivassiloglou et al (1999) proposed to use lin-
guistic features as indicators of text similarity to 
address the problem of sparse representation of 
sentences. Mihalcea et al (2006) measured sen-
tence similarity using component words in sen-
tences. Li et al (2006) proposed to incorporate the 
semantic vector and word order to calculate sen-
tence similarity.  
In our approach to the STS task, semantic vector 
is used and the semantic relatedness between 
words is derived from two sources: WordNet and 
Wikipedia. Because WordNet is limited in its cov-
erage, Wikipedia is used as a candidate for deter-
mining word similarity.  
Word order, however, is not considered in se-
mantic vector. As semantic information are coded 
in sentences according to its order of writing, and 
in our systems, content words may not be adjacent 
to each other, we proposed to use skip bigrams to 
represent the structure of sentences. Skip bigrams, 
generally speaking, are pairs of words in a sen-
tence order with arbitrary gap (Lin and Och, 
2004a). Different from the previous skip bigram 
statistics which compare sentence similarities 
through overlapping skip bigrams (Lin and Och, 
2004a), the skip bigrams we used are weighted by 
a decaying factor of the skipping gap in a sentence, 
giving higher scores to closer occurrences of skip 
bigrams. It is reasonable to assume that similar 
sentences should have more overlapping skip bi-
grams, and the gaps in their shared skip bigrams 
should also be similar.  
The rest of this paper is organized as followed. 
Section 2 describes sentence similarity using se-
mantic vectors and the order-sensitive skip bigrams. 
Section 3 gives the performance evaluation. Sec-
tion 4 is the conclusion.   
2 Similarity between Sentences 
Words are used to represent a sentence in the 
vector space model. Semantic vectors are con-
structed for sentence representations with each en-
try corresponding to a word. Since the semantic 
vector does not consider word order, we further 
proposed to use skip bigrams to represent sentence 
structure. Moreover, these skip bigrams are 
524
weighted by a decaying factor based on the so 
called skip distance in the sentence.  
2.1 Sentence similarity using Semantic 
Vector 
Given a sentence pair, S1 and S2, for example, 
S1: Chairman Michael Powell and FCC colleagues at 
the Wednesday hearing. 
S2: FCC chief Michael Powell presides over hearing 
Monday. 
The term set of the vector space is first formed 
by taking only the content words in both sentences, 
T={chairman, chief, colleagues, fcc, hearing, michael, 
monday, powell, presides, wednesday } 
Each entry of the semantic vector corresponds to 
a word in the joint word set (Li et al, 2006). Then, 
the vector for each sentence is formed in two steps: 
For a word both in the term set T and in the sen-
tence, the value for this word entry is set to 1. If a 
word is not in the sentence, the most similar word 
in the sentence will then be identified, and the cor-
responding path similarity value will be assigned 
to this entry. Let T be the term set with a sorted list 
of content words, T=(t1, t2,?, tn). Without loss of 
generality, let a sentence S=(w1 w2?wm) where wj 
is a content word and wj is a word in T. Let the 
vector space of the sentence S be VSs = (v1, v2, ?, 
vn). Then the value of vi is assigned as follows, 
 
where the similarity function SIM(ti, wj) is calcu-
lated according to the path measure (Pedersen et 
al., 2004) using the WordNet, formally defined as, 
),(1),( jiji wtdistwtSIM ?
 
where dist(ti, wj) is the shortest path from  ti, to 
wj by counting nodes in the WordNet taxonomy. 
Based on this, the semantic vectors for the two ex-
ample sentences will be,  
SVS1 = (1, 0.25, 1, 1, 1, 1, 0.33, 1, 0, 1) and 
SVS2 = (0.25, 1, 0, 1, 1, 1, 1, 1, 1, 0.33) 
Based on the two semantic vectors, the cosine 
metric is used to measure sentence similarity. In 
the WordNet, the entry chairman in the joint set is 
most similar to the word chief in sentence S2. In 
practice, however, this entry might be closer to the 
word presides than to the word chief. Therefore, 
we try to obtain the semantic relatedness using the 
Wikipedia for sentence T and find that the entry 
chairman is closest to the word presides. The Wik-
ipedia-based word relatedness utilizes the hyper-
link structure (Milne & Witten, 2008).  It first 
identifies the candidate articles, a and b, that dis-
cuss ti and wj respectively in this case and then 
compute relatedness between these articles, 
|))||,log(min(||)log(|
)log(|))||,log(max(|),( BAW
BABAbarel ?
???
 
where A and B are sets of articles that link to a 
and b. W is the set of all articles in the Wikipedia. 
Finally, two articles that represent ti and wj are se-
lected and their relatedness score is assigned to 
SIM(ti, wj).  
2.2 Sentence Similarity by Skip bigrams 
Skip bigrams are pairs of words in a sentence 
order with arbitrary gaps. They contain the order-
sensitive information between two words. The skip 
bigrams of a sentence are extracted as features 
which will be stacked in a vector space. Each skip 
bigram is weighted by a decaying factor with its 
skip distances in the sentence. To illustrate this, 
consider the following sentences S and T: 
S =  w1 w2 w1 w3 w4   and    T =  w2 w1 w4 w5 w4 
where w denotes a word. It can be used more 
than once in a sentence. Each sentence above has a 
C(5, 2) 1 = 10 skip bigrams. 
The sentence S has the following skip bigrams: 
?w1w2?, ?w1w1?, ?w1w3?, ?w1w4?, ?w2w1?, 
?w2w3? , ?w2w4? , ?w1w3?, ?w1w4?, ?w3w4? 
The sentence T has the following skip bigrams: 
? 2w1?, ?w2w4?, ?w2w5?, ?w2w4?, ?w1w4?, 
?w1w5? , ?w1w4? , ?w4w5?, ?w4w4?, ?w5w4? 
In the sentence S, we have two repeated skip bi-
grams ?w1w4? and ?w1w3?. In the sentence T, we 
have ?w2w4? and ?w1w4? repeated twice. In this 
case, the weight of the recurring skip bigrams will 
be increased. Hereafter, vectors for S and T will be 
                                                          
1 Combination: C(5,2)=5!/(2!*3!)=10. 
525
formulated with each entry corresponding to a dis-
tinctive skip bigram.  
VS = (?w1w2?, ?w1w1?, ?w1w3?, ?w1w4?, ?w2w1, 
?w2w3?, ?w2w4?, ?w3w4?)? 
VT = (?w2w1?, ?w2w4?, ?w2w5?, ?w1w4?, ?w1w5?, 
?w4w5?, ?w4w4?, ?w5w4?)? 
Now, the question remains how to weight the 
skip bigrams. Given?  as a finite word set, let 
S=w1w2?w|S| be a sentence, wi??and 1?i?|S|. 
A skip bigram of S, denoted by u, is defined by an 
index set I=(i1, i2) of S (1?i1<i2?|S| and u=S[I]). 
The skip distance of S[I] , denoted by du (I), is the 
skip distance of the first word and the second word 
of u, calculated by i2-i1+1. For example, if S is the 
sentence of w1w2w1w3w4 and u = w1w4, then there 
are two index sets, I1=[3,5] and I2=[1,5] such that 
u=S[3,5] and u=S[1,5], and the skip distances of 
S[3,5] and S[1,5] are 3 and 5. The weight of a skip 
bigram u for a sentence S with all its possible oc-
currences, denoted by ( )u S? , is defined as: 
( )
: [ ]
( ) ud Iu I u S IS? ??? ?
 
where ? is the decay factor which penalizes the 
longer skip distance of a skip bigram. By doing so, 
for the sentence S, the complete word set is ?={w1, 
w2, w3,w4}. The weights for the skip bigrams are 
listed in Table 1: 
u 
)(Su?
 u 
)(Su?
 
21ww   2?  12ww  2?  
11ww  3?  32ww  3?  
31ww  24 ?? ?  42ww  4?  
41ww  35 ?? ?  43ww  2?  
Table 1: Skip bigrams and their Weights in S 
In Table 1, if ? is set to 0.25, the weight of the 
skip bigram w1w2 in S is 0.25
2=0.0625, and w1w3 is 
0.254 +0.252=0.064. Similarly, the skip bigrams 
and weights in the sentence T can be obtained. 
With the skip bigram-based vectors, cosine metric 
is then used to compute similarity between S and T. 
3 Experiments 
In the STS task, three training datasets are avail-
able: MSR-Paraphrase, MSR-Video and 
SMTeuroparl (Eneko et al, 2012). The number of 
sentence pairs for three dataset is 750, 750 and 734.  
In the following experiments, Let SWN, SWIKI
 and 
SSKIP denote similarity measures of the vector space 
representation using WordNet, Wikipedia and skip 
bigrams, respectively. The three similarity 
measures are linearly combined as SCOMB: 
SKIPWIKIWNCOMB SSSS ???????? )1( ????  
where ? and ? are weight factors for SWN and 
SWIKI in the range [0,1].  If ? is set to 1, only the 
WordNet-based similarity measure is used; if ? is 0, 
the Wikipedia and skip bigram measures are used.  
Because each dataset has a different representa-
tion for sentences, the parameter configurations for 
them are different. For the word similarity using 
the lexical resource WordNet, the path measure is 
used in experiments. To get word relatedness from 
the English Wikipedia, the Wikipedia Miner tool2 
is used. When computing sentence similarity based 
on the skip bigrams, the decaying factor (DF) must 
be specified beforehand. Hence, parameter config-
urations for the three datasets are listed in Table 2: 
 
Table 2: Parameter Configurations 
In the testing phase, five testing dataset are pro-
vided. In addition to three test datasets drawn from 
the publicly available datasets used in the training 
phase, two surprise datasets are given. They are 
SMTnews and OnWN (Eneko et al, 2012). 
SMTnews has 399 pairs of sentences and OnWN 
contains 750 sentence pairs. The parameter config-
urations for these two surprise datasets are the 
same as those for the dataset MSR-Paraphrase. 
The official scoring is based on Pearson correla-
tion. If the system gives the similarity scores close 
to the reference answers, the system will attain a 
high correlation value. Besides, three other evalua-
tion metrics (ALL, ALLnrm, Mean) based on the 
Pearson correlation are used (Eneko et al, 2012).  
Among the 89 submitted systems, the results of 
our system are given in Table 3: 
Run ALL Rank ALLnrm RankNrm Mean RankMean
PolyUCOMP 0.6528 31 0.7642 59 0.5492 51 
Table 3: Performance using Different Metrics 
                                                          
2 http://wikipedia-miner.cms.waikato.ac.nz/ 
526
Using the ALL metric, our system ranks 31, but 
for ALLnrm and Mean metrics, our system ranking 
is decreased to 59 and 51. In terms of ALL metric, 
our system achieves a medium performance, im-
plying that our system correlates well with human 
assessments. In terms of ALLnrm and Mean met-
rics, our system performance degrades a lot, imply-
ing that our system is not well correlated with the 
reference answer when each dataset is normalized 
into the aggregated dataset using the least square 
error or the weighted mean across the datasets.  
To see how well each of the individual vector 
space models performed on the evaluation sets, we 
experiment on the five datasets using vectors based 
on WordNet, Wikipedia (Wiki), SkipBigram and 
PolyuCOMP (a combination of the three vectors). 
Table 4 gives detailed results of each dataset. 
 
Table 4: Pearson Correlation for each Dataset 
Table 4 shows that after combining three vector 
representations, each dataset obtains the best per-
formance. The WordNet-based approach gives a 
better performance than Wikipedia-based approach 
in MSRvid dataset. The two approaches, however, 
give similar performance in other four datasets. 
This is because the sentences in the MSRvid da-
taset are too short with limited amount of content 
words. It is difficult to capture the meaning of a 
sentence without distinguishing words in consecu-
tive positions. This is why the order-sensitive 
SkipBigram approach gives better performance 
than the other two approaches. For example, 
A woman is playing a game with a man. 
A man is playing piano. 
Using the semantic vectors, we will get high 
similarity scores, but the two sentences are dissimi-
lar. If the skip bigram approach is used, the simi-
larity score between sentences will be 0, which 
correlates with human judgment. In parameter con-
figurations for the MSRvid dataset, higher weight 
(1-0.123-0.01=0.867) is also given to skip bigrams. 
It is interesting to note that the decaying factor for 
this dataset is 1.4 and is not in the range from 0 to 
1 inclusive. This is because higher decaying factor 
helps to capture semantic meaning between words 
that span afar. For example, 
A man is playing a flute. 
A man is playing a bamboo flute. 
In this sentence pair, the second sentence is en-
tailed by the first one. The similarity can be cap-
tured by assigned larger decay factor to weigh the 
skip bigram ?playing flute? in two sentences. 
Hence, if the value of the decay factor is greater 
than 1, the two sentences will become much more 
similar. After careful investigation, these two sen-
tences are similar to a large extent. In this sense, a 
higher decaying factor would help capture the 
meaning between sentence pairs. This is quite dif-
ferent from the other four datasets which focus on 
shared skip bigrams with smaller decaying factor. 
4 Conclusions and Future Work 
In the Semantic Textual Similarity task of 
SemEval-2012, we proposed to combine the se-
mantic vector with the order-sensitive skip bigrams 
to capture the meaning between sentences. First, a 
semantic vector is derived from either the 
WordNet or Wikipedia. The WordNet simulates 
the common human knowledge about word con-
cepts. However, WordNet is limited in its word 
coverage. To remedy this, Wikipedia is used to 
obtain the semantic relatedness between words. 
Second, the proposed approach also considers the 
impact of word order in sentence similarity by us-
ing skip bigrams. Finally, the overall sentence sim-
ilarity is defined as a linear combination of the 
three similarity metrics. However, our system is 
limited in its approaches. In future work, we would 
like to apply machine learning approach in deter-
mining sentence similarity. 
 
527
References  
David Milne , Ian H. Witten. 2008. An Effective, Low-
cost Measure of Semantic Relatedness Obtained from 
Wikipedia Links. In Proceedings of the first AAAI 
Workshop on Wikipedia and Artificial Intelligence 
(WIKIAI'08), Chicago, I.L 
Dekang Lin and Patrick Pantel. 2001. Discovery of In-
ference Rules for Question Answering. Natural Lan-
guage Engineering, 7(4):343-360. 
Eneko Agirre, Daniel Cer, Mona Diab and Aitor Gonza-
lez-Agirre. 2012. SemEval-2012 Task 6: A Pilot on 
Semantic Textual Similarity. In Proceedings of the 
6th International Workshop on Semantic Evaluation 
(SemEval 2012), in conjunction with the First Joint 
Conference on Lexical and Computational Semantics 
(*SEM 2012). 
Gunes Erkan and Dragomir R. Radev. 2004. Lexrank: 
Graph-based Lexical Centrality as Salience in Text 
Summarization. Journal of Artificial Intelligence Re-
search, 22: 457?479. 
Lin, Chin-Yew and Franz Josef Och. 2004a. Automatic 
Evaluation of Machine Translation Quality Using 
Longest Common Subsequence and Skip bigram Sta-
tistics. In Proceedings of the 42nd Annual Meeting of 
the Association for Computational Linguistics (ACL 
2004), Barcelona, Spain. 
Ou Jin, Nathan Nan Liu, Yong Yu and Qiang Yang.  
2011. Transferring Topical Knowledge from Auxilia-
ry Long Text for Short Text Understanding. In: Pro-
ceedings of the 20th ACM Conference on 
Information and Knowledge Management (ACM 
CIKM 2011). Glasgow, UK. 
Rada Mihalcea and Courtney Corley. 2006. Corpus-
based and Knowledge-based Measures of Text Se-
mantic Similarity. In Proceeding of the Twenty-First 
National Conference on Artificial Intelligence and 
the Eighteenth Innovative Applications of Artificial 
Intelligence Conference. 
Ted Pedersen, Siddharth Patwardhan and Jason 
Michelizzi. 2004. WordNet::Similarity?Measuring 
the Relatedness of Concepts. In Proceedings of the 
19th National Conference on Artificial Intelligence 
(AAAI, San Jose, CA), pages 144?152. 
Vasileios Hatzivassiloglou, Judith L. Klavans , Eleazar 
Eskin. 1999. Detecting Text Similarity over Short 
Passages: Exploring Linguistic Feature Combinations 
via Machine Learning. In Proceeding of Empirical 
Methods in natural language processing and Very 
Large Corpora. 
Youngjoong Ko,  Jinwoo Park, and Jungyun Seo. 2004. 
Improving Text Categorization using the Importance 
of Sentences. Information Processingand Manage-
ment, 40(1): 65?79. 
Yuhua Li, David Mclean, Zuhair B, James D. O'shea 
and Keeley Crockett. 2006. Sentence Similarity 
Based on Semantic Nets and Corpus Statistics. IEEE 
Transactions on Knowledge and Data Engineering, 
18(8), 1138?1149. 
 
 
 
 
528
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 90?95, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
PolyUCOMP-CORE TYPED: Computing Semantic Textual Similarity
using Overlapped Senses
Jian Xu Qin Lu
The Hong Kong Polytechnic University
Department of Computing
Hung Hom, Kowloon, Hong Kong
{csjxu, csluqin}@comp.polyu.edu.hk
Abstract
The Semantic Textual Similarity (STS)
task aims to exam the degree of semantic
equivalence between sentences (Agirre et
al., 2012). This paper presents the work
of the Hong Kong Polytechnic University
(PolyUCOMP) team which has participated
in the STS core and typed tasks of SemEval-
2013. For the STS core task, the PolyUCOMP
system disambiguates words senses using
contexts and then determine sentence
similarity by counting the number of senses
they shared. For the STS typed task, the string
kernel (Lodhi et al, 2002) is used to compute
similarity between two entities to avoid string
variations in entities.
1 Introduction
Sentence similarity computation plays an important
role in text summarization and social network
applications (Erkan et al, 2004; Jin et al, 2011).
The SemEval 2012 competition initiated a task
targeted at Semantic Textual Similarity (STS)
between sentence pairs (Agirre et al, 2012). Given
a set of sentence pairs, participants are required to
assign to each sentence pair a similarity score.
Because a sentence has only a limited amount of
content words, it is difficult to determine sentence
similarities. To solve this problem, Hatzivassiloglou
et al (1999) proposed to use linguistic features
as indicators of text similarity to address the
problem of sparse representation of sentences.
Mihalcea et al (2006) measured sentence similarity
using component words in sentences. Li et al
(2006) proposed to incorporate the semantic vector
and word order to calculate sentence similarity.
Biemann et al (2012) applied the log-linear
regression model by combining the simple string
based measures, for example, word ngrams and
semantic similarity measures, for example, textual
entailment. Similarly, Saric et al (2012) used a
support vector regression model which incorporates
features computed from sentence pairs. The features
are knowledge- and corpus-based word similarity,
ngram overlaps, WordNet augmented word overlap,
syntactic features and so on. Xu et al (2012)
combined semantic vectors with skip bigrams to
determine sentence similarity, whereas the skip
bigrams take into the sequential order between
words.
In our approach to the STS task, words in
sentences are assigned with appropriate senses using
their contexts. Sentence similarity is computed by
calculating the number of shared senses in both
sentences since it is reasonable to assume that
similar sentences should have more overlapping
senses. For the STS-TYPED task, variations
might occur in author names, people involved,
time expression and location. Thus, string kernel
is applied to compute similarity between entities
because it can capture variations between entities.
Moreover, for the event similarity in STS-TYPED
task, semantic relatedness between verbs is derived
the WordNet.
The rest of this paper is structured as follows.
Section 2 describes sentence similarity using sense
overlapping and string kernel. Section 3 gives the
performance evaluation. Section 4 is the conclusion.
90
2 Similarity between Sentences
Words are used to convey meaning in a sentence.
They are tagged with appropriate senses initially and
then sentence similarity is calculated based on the
number of shared senses.
2.1 Sense Overlapping
When comparing word features, we did not compare
their surface equality, but we first conceptualize
these words and then calculate their similarities
based on the hierarchial structure in WordNet. For a
word in a sentence, it will be assigned a WordNet
sense. In this paper, we focus on the Word
Sense Disambiguation (WSD) algorithm taken by
Banerjee and Pederson (2003). They measured the
semantic relatedness between concepts by counting
the shared words in their WordNet glosses.
In WordNet, a word sense is represented by a
synset which has a gloss that defines the concept
that it represents. For example, the words walking,
afoot, ambulate constitute a single synset which has
gloss representations as follows,
walking: the act of traveling by foot
afoot: traveling by foot
ambulate: walk about
To lift the limitations of dictionary glosses which
are fairly short with insufficient vocabulary, we
utilize the glosses of related senses since we assume
that words co-occur in one sentence share related
senses and the more glosses two senses share, the
more similar they are. Therefore, we extract not
only glosses of target synset, but also the glosses
of the hypernym, hyponym, meronym, holonym and
troponym synsets of the target synset to form a
synset context. Finally, we compare the sentence
contexts with different synset contexts to determine
which sense should be assigned to the words.
To disambiguate word senses, a window of
contexts surrounding the the target word is specified
and a set of candidate word senses are extracted for
the content word (noun, verb, adjective) within that
window. Let the current target word index i = 0 that
is,w0, the window size be 2n+1 and?n ? i ? +n.
Let |wi| be the number of senses for word wi and the
jth sense of wi is si,j , where 1 ? j ? |wi|. Next is
to assign an appropriate sense k to the target word.
We achieve this by adding together the relatedness
scores calculated by comparing the senses of the
target word and senses of every non-target word
within the window of context. The sense score for
the current target word w0 is defined as,
Sensek =
n?
i=?n
|wi|?
j=1
relatedness(s0,k, si,j) (1)
The kth sense which has the biggest sense score
will be chosen as the right sense for the target word
w0. Now remains the question of how to define the
relatedness between two synsets. It is defined as,
relatedness(s0,k, si,j) =
score(gloss(s0,k), gloss(si,j))
+score(hype(s0,k), hype(si,j))
+score(hypo(s0,k), hypo(si,j))
+score(hype(s0,k), gloss(si,j))
+score(gloss(s0,k), hype(si,j))
(2)
In Equation 2, the score function counts the
number of overlapping words between two glosses.
However, if there is a phrasal n-word overlap, then
a score of n2 will be assigned, thus encouraging the
longer n-word overlap. Let V denote the set of n-
word overlaps shared between two glosses, the score
is defined as,
score =
?
w?V
?w?2 (3)
where ?w? refers to the number of words in w. In
so doing, we can have corresponding senses for the
sentence Castro celebrates 86th birthday Monday
as follows,
castro/10886929-n celebrate/02490877-v
birthday/15250178-n monday/15163979-n
To find the n-word overlap, we found that
contiguous words in two glosses lie in the diagonal
of a matrix, take the senses walk and afoot for
example, their glosses are,
walking: the act of traveling by foot
afoot: traveling by foot
91
Place the walking glosses in rows and afoot
glosses in columns, we get the matrix representation
in Figure 1,
Figure 1: n-word overlap representation
Figure 1 shows that travel by foot is a continuous
sequence of words shared by two glosses. Steps to
find n-word overlapping are:
(1) Construct a matrix for two sentences;
(2) Get continuous n-word overlapping, n is
greater than 1;
(3) Set the cell values to 0 if they are contained in
continuous n-word.
(4) Get the words (unigrams) which are shared by
two sentences.
Take a b c d and b c a d for example, we will have
the matrix as follows,
b c a d
a 0 0 1 0
b 1 0 0 0
c 0 1 0 0
d 0 0 0 1
Table 1: Matrix representation for two sentences
By the step 2, we will get the b c and its
corresponding cells cell(1,0) and cell(2,1). We then
set the two cells to zero, and obtain an updated
matrix as follows,
b c a d
a 0 0 1 0
b 0 0 0 0
c 0 0 0 0
d 0 0 0 1
Table 2: Updated matrix representation for two sentences
In Table 2, we found that cell(0,2) and cell(3,3)
have values greater than zero. Therefore, a and b
will be extracted the common terms.
This approach can also be applied to find common
n-word overlaps between sentences, for example,
s1: Olli Heinonen, the Head of the International
Atomic Energy Agency delegation to Iran, declared
yesterday that the agency has reached an agreement
with Tehran on the method of conducting the
negotiations pertaining to its nuclear program.
s2: leader of international atomic energy agency
delegation to iran , olli heinonen said yesterday ,
that the agency concluded a mutual understanding
with tehran on the way to manage talks depending
upon its atomic program .
We will have ngrams with n ranging from 1 to 7,
such as,
unigram: of, to, its, program, yesterday
bigram: olli heinonen
trigram: that the agency
four-gram: with tehran on the
seven-gram: international atomic energy agency
delegation to iran
Similarity between two sentences is calculated by
counting the number of overlapped n-words. The
similarity for s1 and s2 is, (1 + 1 + 1 + 1 + 1) +
(2)2 + (3)2 + (4)2 + (7)2 = 83.
2.2 String kernel
For the STS-TYPED task, when comparing whether
people or authors are similar or not, we found that
some entity mentions may have tiny variations, for
example,
E Vincent Harris and E.Vincent Harris
The difference between the entities lies in fact that
the second entity has one more dot. In this case,
string kernel would be a good choice in verifying
they are similar or not. If we consider n=2, we obtain
79-dimensional feature space where the two entities
are mapped in Table 3.
In Table 3, ? is the decay factor, in the range
of [0,1], that penalizes the longer distance of a
subsequence. Formally, string kernel is defined as,
Kn(s, t) =
?
u?
?n
??u(s) ? ?u(t)? (4)
92
ev ei en ? ? ? e. ? ? ? rs is
?(evincentharris) ?2 ?3 + ?13 ?2 + ?4 + ?7 ? ? ? 0 ? ? ? ?3 + ?4 ?2 + ?12
?(e.vincentharris) ?3 ?4 + ?14 ?2 + ?5 + ?8 ? ? ? ?2 ? ? ? ?3 + ?4 ?2 + ?12
Table 3: Feature mapping for two entities
TEAM headlines OnWN FNWN SMT mean rank
RUN1 0.5176 0.1517 0.2496 0.2914 0.3284 77
Table 4: Experimental results for STS-CORE
where
?n is the set of all possible subsequences
of length n. u indicates an item in the set, for
example, the subsequence ev in Table 3. ?u(s) is
the feature mapping of the subsequences in s. In
so doing, we can have similarity between entities in
Table 3 as follows:
Kn(s, t) = ?2? ?3 + (?3 + ?13)? (?4 + ?14) +
? ? ?+(?3+?4)?(?3+?4)+(?2+?12)?(?2+?12)
To avoid enumeration of all subsequences for
similarity measurement, dynamic programming,
similar to the method by Lodhi et al (2002) is used
here for similarity calculation.
3 Experiments
The STS-CORE task is to quantify how similar
two sentences are. We simply use the sense
overlapping approach to compute the similarity.
Since this approach needs to find appropriate senses
for each word based on its contexts. The number
of contextual words is set to 5. Experiments
are conducted on four datasets. They are:
headlines mined from news sources by European
Media, OnWN extracted from from WordNet and
OntoNotes, FNWN from WordNet and FrameNet
and SMT dataset from DARPA GALE HTER and
HyTER. The results of our system (PolyUCOMP-
RUN1) are given in Table 4 ,
Our system achieves rather lower performance
in the OnWN and FNWN datasets. This is because
it is difficult to use contextual terms to find the
correct senses for words in sentences of these two
datasets. Take the two sentences in OnWN dataset
for example,
s1: the act of choosing among alternatives
s2: the act of changing one thing for another
thing.
The valid concepts for the two sentences are:
c1: 06532095-n 05790944-n
c2: 00030358-n 00126264-v 00002452-n
00002452-n
c1 and c2 have no shared senses, resulting in a
zero similarity between s1 and s2. However, s1 and
s2 should have the same meaning. Moreover, in the
FNWN dataset, the sentence lengths are unbalanced,
for example,
s1: there exist a number of different possible
events that may happen in the future. in most cases,
there is an agent involved who has to consider which
of the possible events will or should occur. a salient
entity which is deeply involved in the event may also
be mentioned.
s2: doing as one pleases or chooses;
s1 has 48 tokens with punctuations being
excluded and s2 has only 6 tokens. This would affect
our system performance as well.
For the STS-TYPED task, data set is taken
from Europeana, which provides millions of books,
paintings, films, museum objects and archival
records that have been digitised throughout Europe.
Each item has one line per type, where the type
can be the title of a record, list of subject terms,
textual description of the record, creator of the
record and date of the record. Participating systems
are supposed to compute similarities between semi-
structured items. In this task, we take the strategies
in Table 5,
Jaccard denotes the Jaccard similarity measure.
Stringkernel + Jaccard means that two types
are similar if they share many terms, for example,
93
TEAM general author people time location event subject description mean rank
RUN1 0.4888 0.6940 0.3223 0.3820 0.3621 0.1625 0.3962 0.4816 0.4112 12
RUN2 0.4893 0.6940 0.3253 0.3777 0.3628 0.1968 0.3962 0.4816 0.4155 11
RUN3 0.4915 0.6940 0.3254 0.3737 0.3667 0.2207 0.3962 0.4816 0.4187 10
Table 6: Experimental results for STS-TYPED
Type Strategy
author String kernel
people String kernel + Jaccard
time String kernel + Jaccard
location String kernel + Jaccard
event WordNet + Jaccard
subject Sense overlapping
description Sense overlapping
Table 5: Strategies for computing similarity
location; and string kernel is used to determine
whether two locations are similar or not. For the
type of event, we extract verbs from records and
count the number of shared verbs between two
records. The verb similarity is obtained through
WordNet. The general similarity is equal to the
average of the 7 scores. Also, Stanford CoreNLP
tool1 is used to extract author, date, time, location
and handle part-of-speech tagging.
In this STS-TYPED task, we use string kernel and
WordNet to determine whether two terms are similar
and increase the number of counts if their similarity
exceeds a certain threshold. Therefore, we have
chosen 0.4, 0.5 and 0.6 in a heuristic manner and
obtained three different runs. Experimental results
are given in Table 6.
Since the types of author, subject and
description are not related to either string kernel
or WordNet, their performances remain unchanged
during three runs.
4 Conclusions and Future Work
In the Semantic Textual Similarity task of SemEval-
2013, to capture the meaning between sentences,
we proposed to disambiguate word senses using
contexts and then determine sentence similarity
by counting the senses they shared. First, word
senses are disambiguated by means of the contextual
1http://nlp.stanford.edu/software/corenlp.shtml
words. When determining similarity between two
senses (synsets), n-word overlapping approach is
used for counting the number of shared words
in two glosses. Besides, string kernel is used
to capture similarity between entities to avoid
variations between entities. Our approach is simple
and we will apply regression models to determine
sentence similarity on the basis of these features in
future work.
References
Daniel B., Chris Biemann, Iryna Gurevych and Torsten
Zesch. 2012. UKP: Computing Semantic Textual
Similarity by Combining Multiple Content Similarity
Measures. Proceedings of the 6th International
Workshop on Semantic Evaluation (SemEval 2012), in
conjunction with the First Joint Conference on Lexical
and Computational Semantics (*SEM 2012).
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonza-
lez-Agirre. 2012. SemEval-2012 Task 6: A Pilot
on Semantic Textual Similarity. Proceedings of the
6th International Workshop on Semantic Evaluation
(SemEval 2012), in conjunction with the First Joint
Conference on Lexical and Computational Semantics
(*SEM 2012).
Frane Saric, Goran Glavas, Mladen Karan, Jan Snajder
and Bojana Dalbelo Basia. 2012. TakeLab: Systems
for Measuring Semantic Text Similarity. Proceedings
of the 6th International Workshop on Semantic
Evaluation (SemEval 2012), in conjunction with the
First Joint Conference on Lexical and Computational
Semantics (*SEM 2012).
Gunes Erkan and Dragomir R. Radev. 2004. Lexrank:
Graph-based Lexical Centrality as Salience in Text
Summarization. Journal of Artificial Intelligence
Research, 22(2004):457?479.
Huma Lodhi, Craig Saunders, John Shawe-Taylor,
Nello Cristianini, and Chris Watkins. 2002. Text
Classification using String Kernels. The Journal of
Machine Learning Research, 2(2002):419?444.
Jian Xu, Qin Lu and Zhengzhong Liu. 2012.
PolyUCOMP: Combining Semantic Vectors with
Skip-bigrams for Semantic Textual Similarity.
94
Proceedings of the 6th International Workshop on
Semantic Evaluation (SemEval 2012), in conjunction
with the First Joint Conference on Lexical and
Computational Semantics (*SEM 2012).
Ou Jin, Nathan Nan Liu, Yong Yu and Qiang Yang 2011.
Transferring Topical Knowledge from Auxiliary Long
Text for Short Text Understanding. Proceedings of the
20th ACM Conference on Information and Knowledge
Management (ACM CIKM 2011).
Rada Mihalcea and Courtney Corley. 2006. Corpusbased
and Knowledge-based Measures of Text Semantic
Similarity. Proceeding of the Twenty-First National
Conference on Artificial Intelligence and the
Eighteenth Innovative Applications of Artificial
Intelligence Conference..
Satanjeev Banerjee and Ted Pedersen. 2003. Extended
Gloss Overlaps as a Measure of Semantic Relatedness.
Proceedings of the 18th International Joint
Conference on Artificial Intelligence.
Vasileios Hatzivassiloglou, Judith L. Klavans , Eleazar
Eskin. 1999. Detecting Text Similarity over Short
Passages: Exploring Linguistic Feature Combinations
via Machine Learning. Proceeding of Empirical
Methods in natural language processing and Very
Large Corpora.
Yuhua Li, David Mclean, Zuhair B, James D. O?shea
and Keeley Crockett. 2006. Sentence Similarity
Based on Semantic Nets and Corpus Statistics. IEEE
Transactions on Knowledge and Data Engineering,
18(8):1138?1149.
95
