Proceedings of the EACL 2009 Workshop on GEMS: GEometical Models of Natural Language Semantics, pages 91?95,
Athens, Greece, 31 March 2009. c?2009 Association for Computational Linguistics
A Graph-Theoretic Algorithm for Automatic Extension of Translation
Lexicons
Beate Dorow Florian Laws Lukas Michelbacher Christian Scheible Jason Utt
Institute for Natural Language Processing
Universita?t Stuttgart
{dorowbe,lawsfn,michells,scheibcn,uttjn}@ims.uni-stuttgart.de
Abstract
This paper presents a graph-theoretic
approach to the identification of yet-
unknown word translations. The proposed
algorithm is based on the recursive Sim-
Rank algorithm and relies on the intuition
that two words are similar if they estab-
lish similar grammatical relationships with
similar other words. We also present a for-
mulation of SimRank in matrix form and
extensions for edge weights, edge labels
and multiple graphs.
1 Introduction
This paper describes a cross-linguistic experiment
which attempts to extend a given translation dic-
tionary with translations of novel words.
In our experiment, we use an English and
a German text corpus and represent each cor-
pus as a graph whose nodes are words and
whose edges represent grammatical relationships
between words. The corpora need not be parallel.
Our intuition is that a node in the English and a
node in the German graph are similar (that is, are
likely to be translations of one another), if their
neighboring nodes are. Figure 1 shows part of the
English and the German word graph.
Many of the (first and higher order) neighbors
of food and Lebensmittel translate to one another
(marked by dotted lines), indicating that food and
Lebensmittel, too, are likely mutual translations.
Our hypothesis yields a recursive algorithm for
computing node similarities based on the simi-
larities of the nodes they are connected to. We
initialize the node similarities using an English-
German dictionary whose entries correspond to
known pairs of equivalent nodes (words). These
node equivalences constitute the ?seeds? from
which novel English-German node (word) corre-
spondences are bootstrapped.
We are not aware of any previous work using a
measure of similarity between nodes in graphs for
cross-lingual lexicon acquisition.
Our approach is appealing in that it is language
independent, easily implemented and visualized,
and readily generalized to other types of data.
Section 2 is dedicated to related research on
the automatic extension of translation lexicons. In
Section 3 we review SimRank (Jeh and Widom,
2002), an algorithm for computing similarities of
nodes in a graph, which forms the basis of our
work. We provide a formulation of SimRank in
terms of simple matrix operations which allows
an efficient implementation using optimized ma-
trix packages. We further present a generalization
of SimRank to edge-weighted and edge-labeled
graphs and to inter-graph node comparison.
Section 4 describes the process used for build-
ing the word graphs. Section 5 presents an experi-
ment for evaluating our approach to bilingual lex-
icon acquisition. Section 6 reports the results. We
present our conclusions and directions for future
research in Section 7.
2 Related Work on cross-lingual lexical
acquisition
The work by Rapp (1999) is driven by the idea
that a word and its translation to another lan-
guage are likely to co-occur with similar words.
Given a German and an English corpus, he com-
putes two word-by-word co-occurrence matrices,
one for each language, whose columns span a vec-
tor space representing the corresponding corpus.
In order to find the English translation of a Ger-
man word, he uses a base dictionary to translate
all known column labels to English. This yields
a new vector representation of the German word
in the English vector space. This mapped vector
is then compared to all English word vectors, the
most similar ones being candidate translations.
91
food Lebensmittel
receive erhalten
award Preis
provide liefern
evidence Beweis
buy kaufen
book Buch
publish verlegen
boat Haus
waste ablehnen
Figure 1: Likely translations based on neighboring nodes
Rapp reports an accuracy of 72% for a small
number of test words with well-defined meaning.
Diab and Finch (2000) first compute word sim-
ilarities within each language corpus separately
by comparing their co-occurrence vectors. Their
challenge then is to derive a mapping from one
language to the other (i.e. a translation lexicon)
which best preserves the intra-language word sim-
ilarities. The mapping is initialized with a few seed
?translations? (punctuation marks) which are as-
sumed to be common to both corpora.
They test their method on two corpora written
in the same language and report accuracy rates of
over 90% on this pseudo-translation task. The ap-
proach is attractive in that it does not require a
seed lexicon. A drawback is its high computational
cost.
Koehn and Knight (2002) use a (linear) com-
bination of clues for bootstrapping an English-
German noun translation dictionary. In addition to
similar assumptions as above, they consider words
to be likely translations of one another if they have
the same or similar spelling and/or occur with sim-
ilar frequencies. Koehn and Knight reach an accu-
racy of 39% on a test set consisting of the 1,000
most frequent English and German nouns. The
experiment excludes verbs whose semantics are
more complex than those of nouns.
Otero and Campos (2005) extract English-
Spanish pairs of lexico-syntactic patterns from a
small parallel corpus. They then construct con-
text vectors for all English and Spanish words by
recording their frequency of occurrence in each of
these patterns. English and Spanish vectors thus
reside in the same vector space and are readily
compared.
The approach reaches an accuracy of 89% on a
test set consisting of 100 randomly chosen words
from among those with a frequency of 100 or
higher. The authors do not report results for low-
frequency words.
3 The SimRank algorithm
An algorithm for computing similarities of nodes
in graphs is the SimRank algorithm (Jeh and
Widom, 2002). It was originally proposed for di-
rected unweighted graphs of web pages (nodes)
and hyperlinks (links).
The idea of SimRank is to recursively com-
pute node similarity scores based on the scores
of neighboring nodes. The similarity Sij of two
different nodes i and j in a graph is defined as
the normalized sum of the pairwise similarities of
their neighbors:
Sij =
c
|N(i)| |N(j)|
?
k?N(i),l?N(j)
Skl. (1)
N(i) and N(j) are the set of i?s and j?s neigh-
bors respectively, and c is a multiplicative factor
smaller than but close to 1 which demotes the con-
tribution of higher order neighbors. Sij is set to 1
if i and j are identical, which provides a basis for
the recursion.
3.1 Matrix formulation of SimRank
We derive a formulation of the SimRank similarity
updates which merely consists of matrix multipli-
cations as follows. In terms of the graph?s (binary)
adjacency matrix A, the SimRank recursion reads:
Sij =
c
|N(i)| |N(j)|
?
k?N(i),l?N(j)
Aik Ajl Skl
(2)
noting that AikAjl = 1, iff k is a neighbor of i
and l is a neighbor of j at the same time. This is
92
equivalent to
Sij = c
?
k,l
Aik
|N(i)|
Ajl
|N(j)| Skl (3)
= c
?
k,l
Aik
?
? Ai?
Ajl
?
? Aj?
Skl.
The Sij can be assembled in a square node sim-
ilarity matrix S, and it is easy to see that the indi-
vidual similarity updates can be summarized as:
Sk = c A? Sk?1A?T (4)
where A? is the row-normalized adjacency matrix
and k denotes the current level of recursion. A? is
obtained by dividing each entry of A by the sum of
the entries in its row. The SimRank iteration is ini-
tialized with S = I , and the diagonal of S, which
contains the node self-similarities, is reset to ones
after each iteration.
This representation of SimRank in closed ma-
trix form allows the use of optimized off-the-shelf
sparse matrix packages for the implementation of
the algorithm. This rendered the pruning strate-
gies proposed in the original paper unnecessary.
We also note that the Bipartite SimRank algorithm
introduced in (Jeh and Widom, 2002) is just a spe-
cial case of Equation 4.
3.2 Extension with weights and link types
The SimRank algorithm assumes an unweighted
graph, i.e. a binary adjacency matrix A. Equa-
tion 4 can equally be used to compute similarities
in a weighted graph by letting A? be the graph?s
row-normalized weighted adjacency matrix. The
entries of A? then represent transition probabili-
ties between nodes rather than hard (binary) adja-
cency. The proof of the existence and uniqueness
of a solution to this more general recursion pro-
ceeds in analogy to the proof given in the original
paper.
Furthermore, we allow the links in the graph to
be of different types and define the following gen-
eralized SimRank recursion, where T is the set of
link types and Nt(i) denotes the set of nodes con-
nected to node i via a link of type t.
Sij =
c
|T |
?
t?T
1
|Nt(i)| |Nt(j)|
?
k?Nt(i),l?Nt(j)
Skl.
(5)
In matrix formulation:
Sk =
c
|T |
?
t?T
A?t Sk?1A?t
T (6)
where At is the adjacency matrix associated with
link type t and, again, may be weighted.
3.3 SimRank across graphs
SimRank was originally designed for the com-
parison of nodes within a single graph. However,
SimRank is readily and accordingly applied to
the comparison of nodes of two different graphs.
The original SimRank algorithm starts off with the
nodes? self-similarities which propagate to other
non-identical pairs of nodes. In the case of two dif-
ferent graphs A and B, we can instead initialize the
algorithm with a set of initially known node-node
correspondences.
The original SimRank equation (2) then be-
comes
Sij =
c
|N(i)| |N(j)|
?
k,l
Aik Bjl Skl, (7)
which is equivalent to
Sk = c A? Sk?1 B?T , (8)
or, if links are typed,
Sk =
c
|T |
?
t?T
A?t Sk?1 B?t
T . (9)
The similarity matrix S is now a rectangular
matrix containing the similarities between nodes
in A and nodes in B. Those entries of S which
correspond to known node-node correspondences
are reset to 1 after each iteration.
4 The graph model
The grammatical relationships were extracted
from the British National Corpus (BNC) (100 mil-
lion words), and the Huge German Corpus (HGC)
(180 million words of newspaper text). We com-
piled a list of English verb-object (V-O) pairs
based on the verb-argument information extracted
by (Schulte im Walde, 1998) from the BNC. The
German V-O pairs were extracted from a syntactic
analysis of the HGC carried out using the BitPar
parser (Schmid, 2004).
We used only V-O pairs because they consti-
tute far more sense-discriminative contexts than,
for example, verb-subject pairs, but we plan to ex-
amine these and other grammatical relationships
in future work.
We reduced English compound nouns to their
heads and lemmatized all data. In English phrasal
93
English German
Low Mid High Low Mid High
N V N V N V N V N V N V
0.313 0.228 0.253 0.288 0.253 0.255 0.232 0.247 0.205 0.237 0.211 0.205
Table 1: The 12 categories of test words, with mean relative ranks of test words
verbs, we attach the particles to the verbs to dis-
tinguish them from the original verb (e.g put off
vs. put). Both the English and German V-O pairs
were filtered using stop lists consisting of modal
and auxiliary verbs as well as pronouns. To reduce
noise, we decided to keep only those relationships
which occurred at least three times in the respec-
tive corpus.
The English and German data alike are then rep-
resented as a bipartite graph whose nodes divide
into two sets, verbs and nouns, and whose edges
are the V-O relationships which connect verbs to
nouns (cf. Figure 1). The edges of the graph are
weighted by frequency of occurrence.
We ?prune? both the English and German graph
by recursively removing all leaf nodes (nodes with
a single neighbor). As these correspond to words
which appear only in a single relationship, there is
only limited evidence of their meaning.
After pruning, there are 4,926 nodes (3,365
nouns, 1,561 verbs) and 43,762 links in the En-
glish, and 3,074 nodes (2,207 nouns, 867 verbs)
and 15,386 links in the German word graph.
5 Evaluation experiment
The aim of our evaluation experiment is to test
the extended SimRank algorithm for its ability to
identify novel word translations given the English
and German word graph of the previous section
and an English-German seed lexicon. We use the
dict.cc English-German dictionary 1.
Our evaluation strategy is as follows. We se-
lect a set of test words at random from among the
words listed in the dictionary, and remove their en-
tries from the dictionary. We run six iterations of
SimRank using the remaining dictionary entries
as the seed translations (the known node equiv-
alences), and record the similarities of each test
word to its known translations. As in the original
SimRank paper, c is set to 0.8.
We include both English and German test words
and let them vary in frequency: high- (> 100),
1http://www.dict.cc/ (May 5th 2008)
mid- (> 20 and ? 100), and low- (? 20) fre-
quent as well as word class (noun, verb). Thus, we
obtain 12 categories of test words (summarized in
Table 1), each of which is filled with 50 randomly
selected words, giving a total of 600 test words.
SimRank returns a matrix of English-German
node-node similarities. Given a test word, we ex-
tract its row from the similarity matrix and sort the
corresponding words by their similarities to the
test word. We then scan this sorted list of words
and their similarities for the test word?s reference
translations (those listed in the original dictionary)
and record their positions (i.e. ranks) in this list.
We then replace absolute ranks with relative ranks
by dividing by the total number of candidate trans-
lations.
6 Results
Table 1 lists the mean relative rank of the reference
translations for each of the test categories. The
values of around 0.2-0.3 clearly indicate that our
approach ranks the reference translations much
higher than a random process would.
Relative rank
Fr
eq
ue
nc
y
0.0 0.2 0.4 0.6 0.8 1.0
0
5
15
25
Figure 2: Distribution of the relative ranks of the
reference translations in the English-High-N test
set.
Exemplary of all test sets, Figure 2 shows the
distribution of the relative ranks of the reference
translations for the test words in English-High-N.
The bulk of the distribution lies below 0.3, i.e. in
the top 30% of the candidate list.
In order to give the reader an idea of the results,
we present some examples of test words and their
94
Test word Top 10 predicted translations Ranks
sanction Ausgangssperre Wirtschaftssanktion
Ausnahmezustand Embargo Moratorium
Sanktion Todesurteil Geldstrafe Bu?geld
Anmeldung
Sanktion(6)
Ma?nahme(1407)
delay anfechten revidieren zuru?ckstellen
fu?llen verku?nden quittieren vertagen
verschieben aufheben respektieren
verzo?gern(78)
aufhalten(712)
Kosten hallmark trouser blouse makup uniform
armour robe testimony witness jumper
cost(285)
o?ffnen unlock lock usher step peer shut guard
hurry slam close
open(12)
undo(481)
Table 2: Some examples of test words, their pre-
dicted translations, and the ranks of their true
translations.
predicted translations in Table 2.
Most of the 10 top-ranked candidate transla-
tions of sanction are hyponyms of the correct
translations. This is mainly due to insufficient
noun compound analysis. Both the English and
German nouns in our graph model are single
words. Whereas the English nouns consist only of
head nouns, the German nouns include many com-
pounds (as they are written without spaces), and
thus tend to be more specific.
Some of the top candidate translations of de-
lay are correct (verschieben) or at least acceptable
(vertagen), but do not count as such as they are
missing in the gold standard dictionary.
The mistranslation of the German noun Kosten
is due to semantic ambiguity. Kosten co-occurs of-
ten with the verb tragen as in to bear costs. The
verb tragen however is ambiguous and may as
well be translated as to wear which is strongly as-
sociated with clothes.
We find several antonyms of o?ffnen among its
top predicted translations. Verb-object relation-
ships alone do not suffice to distinguish synonyms
from antonyms. Similarly, it is extremely difficult
to differentiate between the members of closed
categories (e.g. the days of the week, months of
the year, mass and time units) using only syntactic
relationships.
7 Conclusions and Future Research
The matrix formulation of the SimRank algorithm
given in this paper allows an implementation using
efficient off-the-shelf software libraries for matrix
computation.
We presented an extension of the SimRank
algorithm to edge-weighted and edge-labeled
graphs. We further generalized the SimRank equa-
tions to permit the comparison of nodes from two
different graphs, and proposed an application to
bilingual lexicon induction.
Our system is not yet accurate enough to be
used for actual compilation of translation dictio-
naries. We further need to address the problem of
data sparsity. In particular, we need to remove the
bias towards low-degree words whose similarities
to other words are unduly high.
In order to solve the problem of ambiguity, we
intend to apply SimRank to the incidence repre-
sentation of the word graphs, which is constructed
by putting a node on each link. The proposed al-
gorithm will then naturally return similarities be-
tween the more sense-discriminative links (syn-
tactic relationships) in addition to similarities be-
tween the often ambiguous nodes (isolated words).
References
M. Diab and S. Finch. 2000. A statistical word-
level translation model for comparable corpora. In
In Proceedings of the Conference on Content-Based
Multimedia Information Access (RIAO).
G. Jeh and J. Widom. 2002. Simrank: A measure of
structural-context similarity. In KDD ?02: Proceed-
ings of the eighth ACM SIGKDD International Con-
ference on Knowledge Discovery and Data Mining,
pages 538?543.
P. Koehn and K. Knight. 2002. Learning a translation
lexicon from monolingual corpora. In Proceedings
of the ACL-02 Workshop on Unsupervised Lexical
Acquisition, pages 9?16.
P. Gamallo Otero and J. Ramon Pichel Campos. 2005.
An approach to acquire word translations from non-
parallel texts. In EPIA, pages 600?610.
R. Rapp. 1999. Automatic identification of word trans-
lations from unrelated English and German corpora.
In Proceedings of the 37th Annual Meeting of the
Association for Computational Linguistics on Com-
putational Linguistics, pages 519?526.
Helmut Schmid. 2004. Efficient parsing of highly am-
biguous context-free grammars with bit vectors. In
COLING ?04: Proceedings of the 20th International
Conference on Computational Linguistics, page 162.
Sabine Schulte im Walde. 1998. Automatic Se-
mantic Classification of Verbs According to Their
Alternation Behaviour. Master?s thesis, Insti-
tut fu?r Maschinelle Sprachverarbeitung, Universita?t
Stuttgart.
95
Coling 2010: Poster Volume, pages 614?622,
Beijing, August 2010
A Linguistically Grounded Graph Model for Bilingual Lexicon
Extraction
Florian Laws, Lukas Michelbacher, Beate Dorow, Christian Scheible,
Ulrich Heid, Hinrich Schu?tze
Institute for Natural Language Processing
Universita?t Stuttgart
{lawsfn,michells,dorowbe}@ims.uni-stuttgart.de
Abstract
We present a new method, based on
graph theory, for bilingual lexicon ex-
traction without relying on resources with
limited availability like parallel corpora.
The graphs we use represent linguis-
tic relations between words such as ad-
jectival modification. We experiment
with a number of ways of combining
different linguistic relations and present
a novel method, multi-edge extraction
(MEE), that is both modular and scalable.
We evaluate MEE on adjectives, verbs
and nouns and show that it is superior
to cooccurrence-based extraction (which
does not use linguistic analysis). Finally,
we publish a reproducible baseline to es-
tablish an evaluation benchmark for bilin-
gual lexicon extraction.
1 Introduction
Machine-readable translation dictionaries are an
important resource for bilingual tasks like ma-
chine translation and cross-language information
retrieval. A common approach to obtaining bilin-
gual translation dictionaries is bilingual lexicon
extraction from corpora. Most work has used
parallel text for this task. However, parallel cor-
pora are only available for few language pairs and
for a small selection of domains (e.g., politics).
For other language pairs and domains, monolin-
gual comparable corpora and monolingual lan-
guage processing tools may be more easily avail-
able. This has prompted researchers to investigate
bilingual lexicon extraction based on monolingual
corpora (see Section 2) .
In this paper, we present a new graph-theoretic
method for bilingual lexicon extraction. Two
monolingual graphs are constructed based on syn-
tactic analysis, with words as nodes and relations
(such as adjectival modification) as edges. Each
relation acts as a similarity source for the node
types involved. All available similarity sources
interact to produce one final similarity value for
each pair of nodes. Using a seed lexicon, nodes
from the two graphs can be compared to find a
translation.
Our main contributions in this paper are: (i) we
present a new method, based on graph theory,
for bilingual lexicon extraction without relying
on resources with limited availability like paral-
lel corpora; (ii) we show that with this graph-
theoretic framework, information obtained by lin-
guistic analysis is superior to cooccurrence data
obtained without linguistic analysis; (iii) we ex-
periment with a number of ways of combining dif-
ferent linguistic relations in extraction and present
a novel method, multi-edge extraction, which is
both modular and scalable; (iv) progress in bilin-
gual lexicon extraction has been hampered by the
lack of a common benchmark; we therefore pub-
lish a benchmark and the performance of MEE as
a baseline for future research.
The paper discusses related work in Section 2.
We then describe our translation model (Sec-
tion 3) and multi-edge extraction (Section 4). The
benchmark we publish as part of this paper is de-
scribed in Section 5. Section 6 presents our ex-
perimental results and Section 7 analyzes and dis-
cusses them. Section 8 summarizes.
2 Related Work
Rapp (1999) uses word cooccurrence in a vector
space model for bilingual lexicon extraction. De-
tails are given in Section 5.
Fung and Yee (1998) also use a vector space
approach, but use TF/IDF values in the vector
components and experiment with different vec-
tor similarity measures for ranking the translation
candidates. Koehn and Knight (2002) combine
614
a vector-space approach with other clues such as
orthographic similarity and frequency. They re-
port an accuracy of .39 on the 1000 most frequent
English-German noun translation pairs.
Garera et al (2009) use a vector space model
with dependency links as dimensions instead of
cooccurring words. They report outperforming
a cooccurrence vector model by 16 percentage
points accuracy on English-Spanish.
Haghighi et al (2008) use a probabilistic model
over word feature vectors containing cooccur-
rence and orthographic features. They then use
canonical correlation analysis to find matchings
between words in a common latent space. They
evaluate on multiple languages and report high
precision even without a seed lexicon.
Most previous work has used vector spaces and
(except for Garera et al (2009)) cooccurrence
data. Our approach uses linguistic relations like
subcategorization, modification and coordination
in a graph-based model. Further, we evaluate our
approach on different parts of speech, whereas
some previous work only evaluates on nouns.
3 Translation Model
Our model has two components: (i) a graph repre-
senting words and the relationships between them
and (ii) a measure of similarity between words
based on these relationships. Translation is re-
garded as cross-lingual word similarity. We rank
words according to their similarity and choose the
top word as the translation.
We employ undirected graphs with typed nodes
and edges. Node types represent parts of speech
(POS); edge types represent different kinds of re-
lations. We use a modified version of SimRank
(Jeh and Widom, 2002) as a similarity measure
for our experiments (see Section 4 for details).
SimRank is based on the idea that two nodes
are similar if their neighbors are similar. We ap-
ply this notion of similarity across two graphs. We
think of two words as translations if they appear
in the same relations with other words that are
translations of each other. Figure 1 illustrates this
idea with verbs and nouns in the direct object rela-
tion. Double lines indicate seed translations, i.e.,
known translations from a dictionary (see Sec-
tion 5). The nodes buy and kaufen have the same
house
magazine
book
thought
buy
read
Haus
Zeitschrift
Buch
Gedanke
kaufen
lesen
Figure 1: Similarity through seed translations
objects in the two languages; one of these (maga-
zine ? Zeitschrift) is a seed translation. This re-
lationship contributes to the similarity of buy ?
kaufen. Furthermore, book and Buch are similar
(because of read ? lesen) and this similarity will
be added to buy ? kaufen in a later iteration. By
repeatedly applying the algorithm, the initial sim-
ilarity introduced by seeds spreads to all nodes.
To incorporate more detailed linguistic infor-
mation, we introduce typed edges in addition to
typed nodes. Each edge type represents a linguis-
tic relation such as verb subcategorization or ad-
jectival modification. By designing a model that
combines multiple edge types, we can compute
the similarity between two words based on mul-
tiple sources of similarity. We superimpose dif-
ferent sets of edges on a fixed set of nodes; a node
is not necessarily part of every relation.
The graph model can accommodate any kind of
nodes and relations. In this paper we use nodes
to represent content words (i.e., non-function
words): adjectives (a), nouns (n) and verbs (v).
We extracted three types of syntactic relations
from a corpus: see Table 1.
Nouns participate in two bipartite relations
(amod, dobj) and one unipartite relation (ncrd).
This means that the computation of noun similar-
ities will benefit from three different sources.
Figure 2 depicts a sample graph with all node
and edge types. For the sake of simplicity, a
monolingual example is shown. There are four
nouns in the sample graph all of which are (i)
modified by the adjectives interesting and polit-
ical and (ii) direct objects of the verbs like and
615
relation entities description example
used in this paper
amod a, n adjectival modification a fast car
dobj v, n object subcategorization drive a car
ncrd n, n noun coordination cars and busses
other possible relations
vsub v, n subject subcategorization a man sleeps
poss n, n possessive the child?s toy
acrd a, a adjective coordination red or blue car
Table 1: Relations used in this paper (top) and
possible extensions (bottom).
dobj
amod
ncrd
verb
adjective
noun
like promote
idea
article book
magazine
interesting political
Figure 2: Graph snippet with typed edges
promote. Based on amod and dobj, the four nouns
are equally similar to each other. However, the
greater similarity of article, book, and magazine
to each other can be deduced from the fact that
these three nouns also occur in the relation ncrd.
We exploit this information in the MEE method.
Data and Preprocessing. Our corpus in this
paper is the Wikipedia. We parse all German
and English articles with BitPar (Schmid, 2004)
to extract verb-argument relations. We extract
adjective-noun modification and noun coordina-
tions with part-of-speech patterns based on a
version of the corpus tagged with TreeTagger
(Schmid, 1994). We use lemmas instead of sur-
face forms. Because we perform the SimRank
matrix multiplications in memory, we need to fil-
ter out rare words and relations; otherwise, run-
ning SimRank to convergence would not be feasi-
ble. For adjective-noun pairs, we apply a filter on
pair frequency (? 3). We process noun pairs by
applying a frequency threshold on words (? 100)
and pairs (? 3). Verb-object pairs (the smallest
data set) were not frequency-filtered. Based on
the resulting frequency counts, we calculate asso-
ciation scores for all relationships using the log-
likelihood measure (Dunning, 1993). For noun
pairs, we discard all pairs with an association
score < 3.84 (significance at ? = .05). For all
three relations, we discard pairs whose observed
frequency was smaller than their expected fre-
quency (Evert, 2004, p. 76). As a last step,
we further reduce noise by removing nodes of de-
gree 1. Key statistics for the resulting graphs are
given in Table 2.
We have found that accuracy of extraction is
poor if unweighted edges are used. Using the
log-likelihood score directly as edge weight gives
too much weight to ?semantically weak? high-
frequency words like put and take. We there-
fore use the logarithms of the log-likelihood score
as edge weights in all SimRank computations re-
ported in this paper.
nodes n a v
de 34,545 10,067 2,828
en 22,257 12,878 4,866
edges ncrd amod dobj
de 65,299 417,151 143,906
en 288,889 686,073 510,351
Table 2: Node and edge statistics
4 SimRank
Our work is based on the SimRank graph similar-
ity algorithm (Jeh and Widom, 2002). In (Dorow
et al, 2009), we proposed a formulation of Sim-
Rank in terms of matrix operations, which can be
applied to (i) weighted graphs and (ii) bilingual
problems. We now briefly review SimRank and
its bilingual extension. For more details we refer
to (Dorow et al, 2009).
The basic idea of SimRank is to consider two
nodes as similar if they have similar neighbor-
hoods. Node similarity scores are recursively
computed from the scores of neighboring nodes:
the similarity Sij of two nodes i and j is computed
616
as the normalized sum of the pairwise similarities
of their neighbors:
Sij =
c
|N(i)| |N(j)|
?
k?N(i),l?N(j)
Skl.
where N(i) and N(j) are the sets of i?s and j?s
neighbors. As the basis of the recursion, Sij is set
to 1 if i and j are identical (self-similarity). The
constant c (0 < c < 1) dampens the contribution
of nodes further away. Following Jeh and Widom
(2002), we use c = 0.8. This calculation is re-
peated until, after a few iterations, the similarity
values converge.
For bilingual problems, we adapt SimRank for
comparison of nodes across two graphs A and B.
In this case, i is a node in A and j is a node in B,
and the recursion basis is changed to S(i, j) = 1 if
i and j are a pair in a predefined set of node-node
equivalences (seed translation pairs).
Sij =
c
|NA(i)| |NB(j)|
?
k?NA(i),l?NB(j)
Skl.
Multi-edge Extraction (MEE) Algorithm To
combine different information sources, corre-
sponding to edges of different types, in one Sim-
Rank computation, we use multi-edge extrac-
tion (MEE), a variant of SimRank (Dorow et al,
2009). It computes an aggregate similarity matrix
after each iteration by taking the average similar-
ity value over all edge types T :
Sij =
c
|T |
?
t?T
1
f(|NA,t(i)|)f(|NB,t(j)|)
?
k?NA,t(i),
l?NB,t(j)
Skl.
f is a normalization function (either f = g,
g(n) = n as before or the normalization discussed
in the next section).
While we have only reviewed the case of un-
weighted graphs, the extended SimRank can also
be applied to weighted graphs. (See (Dorow et
al., 2009) for details.) In what follows, all graph
computations are weighted.
Square Root Normalization Preliminary ex-
periments showed that SimRank gave too much
influence to words with few neighbors. We there-
fore modified the normalization function g(n) =
n. To favor words with more neighbors, we want
f to grow sublinearly with the number of neigh-
bors. On the other hand, it is important that,
even for nodes with a large number of neigh-
bors, the normalization term is not much smaller
than |N(i)|, otherwise the similarity computation
does not converge. We use the function h(n) =?n?
?
maxk(|N(k)|). h grows quickly for small
node degrees, while returning values close to the
linear term for large node degrees. This guaran-
tees that nodes with small degrees have less influ-
ence on final similarity scores. In all experiments
reported in this paper, the matrices A?, B? are nor-
malized with f = h (rather than using the stan-
dard normalization f = g). In one experiment,
accuracy of the top-ranked candidate (acc@1) was
.52 for h and .03 for g, demonstrating that the
standard normalization does not work in our ap-
plication.
Threshold Sieving For larger experiments,
there is a limit to scalability, as the similarity ma-
trix fills up with many small entries, which take up
a large amount of memory. Since these small en-
tries contribute little to the final result, Lizorkin et
al. (2008) proposed threshold sieving: an approxi-
mation of SimRank using less space by deleting
all similarity values that are below a threshold.
The quality of the approximation is set by a pa-
rameter ? that specifies maximum acceptable dif-
ference of threshold-sieved similarity and the ex-
act solution. We adapted this to the matrix formu-
lation by integrating the thresholding step into a
standard sparse matrix multiplication algorithm.
We verified that this approximation yields use-
ful results by comparing the ranks of exact and ap-
proximate solutions. We found that for the high-
ranked words that are of interest in our task, siev-
ing with a suitable threshold does not negatively
affect results.
5 Benchmark Data Set
Rapp?s (1999) original experiment was carried out
on newswire corpora and a proprietary Collins
dictionary. We use the free German (280M to-
kens) and English (850M tokens) Wikipedias as
source and target corpora. Reinhard Rapp has
generously provided us with his 100 word test set
617
n a v
training set .61 .31 .08
TS100 .65 .28 .07
TS1000 .66 .14 .20
Table 3: Percentages of POS in test and training
(TS100) and given us permission to redistribute
it. Additionally, we constructed a larger test set
(TS1000) consisting of the 1000 most frequent
words from the English Wikipedia. Unlike the
noun-only test sets used in other studies, (e.g.,
Koehn and Knight (2002), Haghighi et al (2008)),
TS1000 also contains adjectives and verbs. As
seed translations, we use a subset of the dict.cc
online dictionary. For the creation of the sub-
set we took raw word frequencies from Wikipedia
as a basis. We extracted all verb, noun and ad-
jective translation pairs from the original dictio-
nary and kept the pairs whose components were
among the 5,000 most frequent nouns, the 3,500
most frequent adjectives and the 500 most fre-
quent verbs for each language. These numbers are
based on percentages of the different node types
in the graphs. The resulting dictionary contains
12,630 pairs: 7,767 noun, 3,913 adjective and 950
verb pairs. Table 3 shows the POS composition of
the training set and the two test sets. For experi-
ments evaluated on TS100 (resp. TS1000), the set
of 100 (resp. 1000) English words it contains and
all their German translations are removed from the
seed dictionary.
Baseline. Our baseline is a reimplementation
of the vector-space method of Rapp (1999). Each
word in the source corpus is represented as a word
vector, the dimensions of which are words of seed
translation pairs. The same is done for corpus
words in the target language, using the translated
seed words as dimensions. The value of each di-
mension is determined by association statistics of
word cooccurrence. For a test word, a vector is
constructed in the same way. The labels on the
dimensions are then translated, yielding an input
vector in the target language vector space. We
then find the closest corpus word vector in the tar-
get language vector space using the city block dis-
tance measure. This word is taken as the transla-
tion of the test word.
We went to great lengths to implement Rapp?s
method, but omit the details for reasons of space.
Using the Wikipedia/dict.cc-based data set, we
achieve 50% acc@1 when translating words from
English to German. While this is somewhat lower
than the performance reported by Rapp, we be-
lieve this is due to Wikipedia being more hetero-
geneous and less comparable than news corpora
from identical time periods used by Rapp.
Publication. In conjunction with this paper we
publish the benchmark for bilingual lexicon ex-
traction described. It consists of (i) two Wikipedia
dumps from October 2008 and the linguistic re-
lations extracted from them, (ii) scripts to recre-
ate the training and test sets from the dict.cc
data base, (iii) the TS100 and TS1000 test sets,
and (iv) performance numbers of Rapp?s system
and MEE. These can serve as baselines for fu-
ture work. Note that (ii)?(iv) can be used in-
dependently of (i) ? but in that case the effect
of the corpus on performance would not be con-
trolled. The data and scripts are available at
http://ifnlp.org/wiki/extern/WordGraph
6 Results
In addition to the vector space baseline experi-
ment described above, we conducted experiments
with the SimRank model. Because TS100 only
contains one translation per word, but words can
have more than one valid translation, we manu-
ally extended the test set with other translations,
which we verified using dict.cc and leo.org. We
report the results separately for the original test set
(?strict?) and the extended test set in Table 4. We
also experimented with single-edge models con-
sisting of three separate runs on each relation.
The accuracy columns report the percentage of
test cases where the correct translation was found
among the top 1 (acc@1) or top 10 (acc@10)
candidate words found by the translation mod-
els. Some test words are not present in the data at
all; we count these as 0s when computing acc@1
and acc@10. The acc@10 measure is more use-
ful for indicating topical similarity while acc@1
measures translation accuracy.
MRR is Mean Reciprocal Rank of correct trans-
lations: 1n
?n
i
1
ranki (Voorhees and Tice, 1999).
MRR is a more fine-grained measure than acc@n,
618
TS100, strict TS100, extended TS1000
acc@1 acc@10 MRR acc@1 acc@10 MRR acc@1 acc@10 MRR
baseline .50 .67 .56 .54 .70 .60 .33 .56 .41
single .44 .67 .52 .49 .68 .56 .40? .70? .50
MEE .52 .79? .62 .58 .82? .68 .48? .76? .58
Table 4: Results compared to baseline?
e.g., it will distinguish ranks 2 and 10. All MRR
numbers reported in this paper are consistent with
acc@1/acc@10 and support our conclusions.
The results for acc@1, the measure that most
directly corresponds to utility in lexicon extrac-
tion, show that the SimRank-based models out-
perform the vector space baseline ? only slightly
on TS100, but significantly on TS1000. Using the
various relations separately (single) already yields
a significant improvement compared to the base-
line. Using all relations in the integrated MEE
model further improves accuracy. With an acc@1
score of 0.48, MEE outperforms the baseline by
.15 compared to TS1000. This shows that a com-
bination of several sources of information is very
valuable for finding the correct translation.
MEE outperforms the baseline on TS1000 for
all parts of speech, but performs especially well
compared to the baseline for adjectives and verbs
(see Table 5). It has been suggested that vector
space models perform best for nouns and poorly
for other parts of speech. Our experiments seem to
confirm this. In contrast, MEE exhibits good per-
formance for nouns and adjectives and a marked
improvement for verbs.
On acc@10, MEE is consistently better than the
baseline, on both TS100 and TS1000. All three
differences are statistically significant.
6.1 Relation Comparison
Table 5 compares baseline, single-edge and MEE
accuracy for the three parts of speech covered.
Each single-edge experiment can compute noun
similarity; for adjectives and verbs, only amod,
dobj and MEE can be used.
Performance for nouns varies greatly depend-
ing on the relation used in the model. ncrd per-
?We indicate statistical significance at the ? = 0.05 (?)
and 0.01 level (?) when compared to the baseline. We did
not calculate significance for MRR.
forms best, while dobj shows the worst perfor-
mance. We hypothesize that dobj performs badly
because (i) many verbs are semantically non-
restrictive with respect to their arguments, (e.g.,
use, contain or include) and as a result seman-
tically unrelated nouns become similar because
they share the same verb as a neighbor; (ii) light
verb constructions (e.g., take a walk or give an ac-
count) dilute the extracted relations; and (iii) dobj
is the only relation we extracted with a syntac-
tic parser. The parser was trained on newswire
text, a genre that is very different from Wikipedia.
Hence, parsing is less robust than the relatively
straightforward POS patterns used for the other
relations.
Similarly, many semantically non-restrictive
adjectives such as first and new can modify vir-
tually any noun, diluting the quality of the amod
source. We conjecture that ncrd exhibits the best
performance because there are fewer semantically
non-restrictive nouns than non-restrictive adjec-
tives and verbs.
MEE performance for nouns (.45) is signifi-
cantly better than that of the single-edge models.
The information about nouns that is contained in
the verb-object and adjective-noun data is inte-
grated in the model and helps select better trans-
lations. This, however, is only true for the noun
noun adj verb all
TS100 baseline .55 .43 .29 .50
amod .15 .71 - .30
ncrd .34 - - .22
dobj .02 - .43 .04
MEE .45 .71 .43 .52
TS1000 baseline .42 .26 .18 .33
MEE .53 .55 .27 .48
Table 5: Relation comparison, acc@1
619
source acc@1 acc@10
dobj .02 .10
amod .15 .37
amod+dobj .22 .43
ncrd+dobj .32 .65
ncrd .34 .60
ncrd+amod .49 .74
MEE .45 .77
Table 6: Accuracy of sources for nouns
node type, the ?pivot? node type that takes part in
edges of all three types. For adjectives and verbs,
the performance of MEE is the same as that of the
corresponding single-edge model.
We ran three additional experiments each of
which combines only two of the three possible
sources for noun similarity, namely ncrd+amod,
ncrd+dobj and amod+dobj and performed strict
evaluation (see Table 6). We found that in gen-
eral combination increases performance except
for ncrd+dobj vs. ncrd. We attribute this to the
lack of robustness of dobj mentioned above.
6.2 Comparison MEE vs. All-in-one
An alternative to MEE is to use untyped edges in
one large graph. In this all-in-one model (AIO),
we connect two nodes with an edge if they are
linked by any of the different linguistic relations.
While MEE consists of small adjacency matrices
for each type, the two adjacency matrices for AIO
are much larger. This leads to a much denser sim-
ilarity matrix taking up considerably more mem-
ory. One reason for this is that AIO contains simi-
larity entries between words of different parts of
speech that are 0 (and require no memory in a
sparse matrix representation) in MEE.
Since AIO requires more memory, we had to
filter the data much more strictly than before to be
able to run an experiment. We applied the follow-
ing stricter thresholds on relationships to obtain
a small graph: 5 instead of 3 for adjective-noun
MEEsmall AIOsmall
acc@1 .51 .52
acc@10 .72 .75
MRR .62 .59
Table 7: MEE vs. AIO
pairs, and 3 instead of 0 for verb-object pairs,
thereby reducing the total number of edges from
2.1M to 1.4M. We also applied threshold sieving
(see Section 4) with ? = 10?10 for AIO. The re-
sults on TS100 (strict evaluation) are reported in
Table 7. For comparison, MEE was also run on
the smaller graph. Performance of the two models
is very similar, with AIO being slightly better (not
significant). The slight improvement does not jus-
tify the increased memory requirements. MEE is
able to scale to more nodes and edge types, which
allows for better coverage and performance.
7 Analysis and Discussion
Error analysis. We examined the cases where a
reference translation was not at the top of the sug-
gested list of translation candidates. There are a
number of elements in the translation process that
can cause or contribute to this behavior.
Our method sometimes picks a cohyponym of
the correct translation. In many of these cases, the
correct translation is in the top 10 (together with
other words from the same semantic field). For
example, the correct translation of moon, Mond, is
second in a list of words belonging to the semantic
field of celestial phenomena: Komet (comet), Mond
(moon), Planet (planet), Asteroid (asteroid), Stern (star),
Galaxis (galaxy), Sonne (sun), . . . While this behavior
is undesirable for strict lexicon extraction, it can
be exploited for other tasks, e.g. cross-lingual se-
mantic relatedness (Michelbacher et al, 2010).
Similarly, the method sometimes puts the
antonym of the correct translation in first place.
For example, the translation for swift (schnell) is
in second place behind langsam (slow). Based
on the syntactic relations we use, it is difficult to
discriminate between antonyms and semantically
similar words if their syntactic distributions are
similar.
Ambiguous source words also pose a problem
for the system. The correct translation of square
(the geometric shape) is Quadrat. However, 8 out
of its top 10 translation candidates are related to
the location sense of square. The other two are ge-
ometric shapes, Quadrat being listed second. This
is only a concern for strict evaluation, since cor-
rect translations of a different sense were included
in the extended test set.
620
bed is also ambiguous (piece of furniture vs.
river bed). This introduces translation candidates
from the geographical domain. As an additional
source of errors, a number of bed?s neighbors
from the furniture sense have the German transla-
tion Bank which is ambiguous between the furni-
ture sense and the financial sense. This ambiguity
in the target language German introduces spurious
translation candidates from the financial domain.
Discussion. The error analysis demonstrates
that most of the erroneous translations are words
that are incorrect, but that are related, in some ob-
vious way, to the correct translation, e.g. by co-
hyponymy or antonymy. This suggests another
application for bilingual lexicon extraction. One
of the main challenges facing statistical machine
translation (SMT) today is that it is difficult to
distinguish between minor errors (e.g., incorrect
word order) and major errors that are completely
implausible and undermine the users? confidence
in the machine translation system. For example,
at some point Google translated ?sarkozy sarkozy
sarkozy? into ?Blair defends Bush?. Since bilin-
gual lexicon extraction, when it makes mistakes,
extracts closely related words that a human user
can understand, automatically extracted lexicons
could be used to discriminate smaller errors from
grave errors in SMT.
As we discussed earlier, parallel text is not
available in sufficient quantity or for all impor-
tant genres for many language pairs. The method
we have described here can be used in such cases,
provided that large monolingual corpora and ba-
sic linguistic processing tools (e.g. POS tagging)
are available. The availability of parsers is a more
stringent constraint, but our results suggest that
more basic NLP methods may be sufficient for
bilingual lexicon extraction.
In this work, we have used a set of seed trans-
lations (unlike e.g., Haghighi et al (2008)). We
believe that in most real-world scenarios, when
accuracy and reliability are important, seed lexica
will be available. In fact, seed translations can be
easily found for many language pairs on the web.
Although a purely unsupervised approach is per-
haps more interesting from an algorithmic point
of view, the semisupervised approach taken in this
paper may be more realistic for applications.
In this paper, we have attempted to reimplement
Rapp?s system as a baseline, but have otherwise
refrained from detailed comparison with previous
work as far as the accuracy of results is concerned.
The reason is that none of the results published so
far are easily reproducible. While previous publi-
cations have tried to infer from differences in per-
formance numbers that one system is better than
another, these comparisons have to be viewed with
caution since neither the corpora nor the gold stan-
dard translations are the same. For example, the
paper by Haghighi et al (2008) (which demon-
strates how orthography and contextual informa-
tion can be successfully used) reports 61.7% ac-
curacy on the 186 most confident predictions of
nouns. But since the evaluation data sets are not
publicly available it is difficult to compare other
work (including our own) with this baseline. We
simply do not know how methods published so far
stack up against each other.
For this reason, we believe that a benchmark
is necessary to make progress in the area of bilin-
gual lexicon extraction; and that our publication of
such a benchmark as part of the research reported
here is an important contribution, in addition to
the linguistically grounded extraction and the new
graph-theoretical method we present.
8 Summary
We have presented a new method, based on graph
theory, for bilingual lexicon extraction without re-
lying on resources with limited availability like
parallel corpora. We have shown that with this
graph-theoretic framework, information obtained
by linguistic analysis is superior to cooccurrence
data obtained without linguistic analysis. We have
presented multi-edge extraction (MEE), a scalable
graph algorithm that combines different linguis-
tic relations in a modular way. Finally, progress
in bilingual lexicon extraction has been hampered
by the lack of a common benchmark. We publish
such a benchmark with this paper and the perfor-
mance of MEE as a baseline for future research.
9 Acknowledgement
This research was funded by the German Re-
search Foundation (DFG) within the project A
graph-theoretic approach to lexicon acquisition.
621
References
Dorow, Beate, Florian Laws, Lukas Michelbacher,
Christian Scheible, and Jason Utt. 2009. A graph-
theoretic algorithm for automatic extension of trans-
lation lexicons. In EACL 2009 Workshop on Geo-
metrical Models of Natural Language Semantics.
Dunning, Ted. 1993. Accurate methods for the statis-
tics of surprise and coincidence. Computational
Linguistics, 19(1):61?74.
Evert, Stefan. 2004. The Statistics of Word Cooccur-
rences - Word Pairs and Collocations. Ph.D. thesis,
Institut fu?r maschinelle Sprachverarbeitung (IMS),
Universita?t Stuttgart.
Fung, Pascale and Lo Yuen Yee. 1998. An IR ap-
proach for translating new words from nonparallel,
comparable texts. In COLING-ACL, pages 414?
420.
Garera, Nikesh, Chris Callison-Burch, and David
Yarowsky. 2009. Improving translation lexicon
induction from monolingual corpora via depen-
dency contexts and part-of-speech equivalences. In
CoNLL ?09: Proceedings of the Thirteenth Confer-
ence on Computational Natural Language Learn-
ing, pages 129?137, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics.
Haghighi, Aria, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In Proceedings of ACL-
08: HLT, pages 771?779, Columbus, Ohio, June.
Association for Computational Linguistics.
Jeh, Glen and Jennifer Widom. 2002. Simrank: A
measure of structural-context similarity. In KDD
?02, pages 538?543.
Koehn, Philipp and Kevin Knight. 2002. Learning a
translation lexicon from monolingual corpora. In
Proceedings of the ACL-02 Workshop on Unsuper-
vised Lexical Acquisition, pages 9?16.
Lizorkin, Dmitry, Pavel Velikhov, Maxim N. Grinev,
and Denis Turdakov. 2008. Accuracy estimate and
optimization techniques for simrank computation.
PVLDB, 1(1):422?433.
Michelbacher, Lukas, Florian Laws, Beate Dorow, Ul-
rich Heid, and Hinrich Schu?tze. 2010. Building
a cross-lingual relatedness thesaurus using a graph
similarity measure. In Proceedings of the Seventh
conference on International Language Resources
and Evaluation (LREC?10), Valletta, Malta, may.
Rapp, Reinhard. 1999. Automatic identification of
word translations from unrelated English and Ger-
man corpora. In COLING 1999.
Schmid, Helmut. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
International Conference on New Methods in Lan-
guage Processing, pages 44?49.
Schmid, Helmut. 2004. Efficient parsing of highly
ambiguous context-free grammars with bit vectors.
In COLING ?04, page 162.
Voorhees, Ellen M. and Dawn M. Tice. 1999. The
TREC-8 question answering track evaluation. In
Proceedings of the 8th Text Retrieval Conference.
622
Coling 2010: Poster Volume, pages 1104?1112,
Beijing, August 2010
Sentiment Translation through Multi-Edge Graphs
Christian Scheible, Florian Laws, Lukas Michelbacher, and Hinrich Schu?tze
Institute for Natural Language Processing
University of Stuttgart
{scheibcn, lawsfn, michells}@ims.uni-stuttgart.de
Abstract
Sentiment analysis systems can benefit
from the translation of sentiment informa-
tion. We present a novel, graph-based ap-
proach using SimRank, a well-established
graph-theoretic algorithm, to transfer sen-
timent information from a source lan-
guage to a target language. We evaluate
this method in comparison with semantic
orientation using pointwise mutual infor-
mation (SO-PMI), an established unsuper-
vised method for learning the sentiment of
phrases.
1 Introduction
Sentiment analysis is an important topic in com-
putational linguistics that is of theoretical interest
but is also useful in many practical applications.
Usually, two aspects are of importance in senti-
ment analysis. The first is the detection of sub-
jectivity, i.e., whether a text or an expression is
meant to express sentiment at all; the second is the
determination of sentiment orientation, i.e., what
sentiment is to be expressed in a structure that is
considered subjective.
Work on sentiment analysis most often cov-
ers resources or analysis methods in a single lan-
guage, usually English. However, the transfer
of sentiment analysis between languages can be
advantageous by making use of resources for a
source language to improve the analysis of the tar-
get language.
This paper presents an approach to the transfer
of sentiment information between two languages
that does not rely on resources with limited avail-
ability like parallel corpora. It is built around Sim-
Rank, a graph similarity algorithm that has suc-
cessfully been applied to the acquisition of bilin-
gual lexicons (Laws et al, 2010) and semantic
similarity (Michelbacher et al, 2010). It uses
linguistic relations extracted from two monolin-
gual corpora to determine the similarity of words
in different languages. One of the main benefits
of our method is its ability to handle sparse data
about the relations between the languages well
(i.e., a small seed lexicon). Further, we experi-
ment with combining multiple types of linguistic
relations for graph-based translation. Our exper-
iments are carried out using English as a source
language and German as a target language. We
evaluate our method using a hand-annotated set of
German adjectives which we intend to publish.
In the following section, related work is dis-
cussed. Section 3.1 gives an introduction to Sim-
Rank and its application to lexicon induction,
while section 3.2 reviews SO-PMI (Turney, 2002),
an unsupervised baseline method for the genera-
tion of sentiment lexicons. In section 4, we define
our sentiment transfer method which we apply in
experiments in section 5.
2 Related Work
Mihalcea et al (2007) propose two methods for
translating sentiment lexicons. The first method
simply uses bilingual dictionaries to translate an
English sentiment lexicon. A sentence-based clas-
sifier built with this list achieved high precision,
but low recall on a small Romanian test set. The
second method is based on parallel corpora. The
source language in the corpus is annotated with
sentiment information, and the information is then
projected to the target language. Problems arise
due to mistranslations.
Banea et al (2008) use machine translation for
multilingual sentiment analysis. Given a corpus
annotated with sentiment information in one lan-
guage, machine translation is used to produce an
annotated corpus in the target language, by pre-
serving the annotations. The original annotations
1104
can be produced either manually or automatically.
Wan (2009) constructs a multilingual classi-
fier using co-training. In co-training, one classi-
fier produces additional training data for a second
classifier. In this case, an English classifier assists
in training a Chinese classifier.
The induction of a sentiment lexicon is the sub-
ject of early work by Hatzivassiloglou and McK-
eown (1997). They construct graphs from coordi-
nation data from large corpora based on the intu-
ition that adjectives with the same sentiment ori-
entation are likely to be coordinated. For example,
fresh and delicious is more likely than rotten and
delicious. They then apply a graph clustering al-
gorithm to find groups of adjectives with the same
orientation. Finally, they assign the same label to
all adjectives that belong to the same cluster.
Corpus work and bilingual dictionaries are
promising resources for translating sentiment. In
contrast to previous approaches, the work pre-
sented in this paper uses corpora that are not an-
notated with sentiment.
Turney (2002) suggests a corpus-based extrac-
tion method based on his pointwise mutual infor-
mation (PMI) synonymy measure. He assumes
that the sentiment orientation of a phrase can be
determined by comparing its pointwise mutual in-
formation with a positive (excellent) and a nega-
tive phrase (poor). An introduction to this method
is given in Section 3.2.
3 Background
3.1 Lexicon Induction via SimRank
We use the extension of the SimRank (Jeh and
Widom, 2002) node similarity algorithm proposed
by Dorow et al (2009). Given two graphs A and
B, the similarity between two nodes a in A and b
in B is computed in each iteration as:
S(a, b) = c
|NA(a)||NB(b)|
?
k?NA(a),l?NB(b)
S(k, l).
NX(x) is the neighborhood of node x in graph
X . To compute similarities between two graphs,
some initial links between these graphs have to be
given, called seed links. These form the recursion
basis which sets S(a, b) = 1 if there is a seed
link between a and b. At the beginning of each
iteration, all known equivalences between nodes
are reset to 1.
Multi-Edge Extraction (MEE). MEE is an ex-
tension of SimRank that, in each iteration, com-
putes the average node-node similarity of several
different SimRank matrices. In our case, we use
two different SimRank matrices, one for coordi-
nations and one for adjective modification. See
(Dorow et al, 2009) for details. We also used
the node degree normalization function h(n) =?n ??maxk(|N(k)|) (where n is the node de-
gree, and N(k) the degree of node k) to decrease
the harmful effect of high-degree nodes on final
similarity values. See (Laws et al, 2010) for de-
tails.
3.2 SO-PMI
Semantic orientation using pointwise mutual in-
formation (SO-PMI) (Turney, 2002) is an algo-
rithm for the unsupervised learning of semantic
orientation of words or phrases. A word has pos-
itive (resp. negative) orientation if it is associ-
ated with positive (resp. negative) terms more
frequently than with negative (resp. positive)
terms. Association of terms is measured using
their pointwise mutual information (PMI) which
is defined for two words w1 and w2 as follows:
PMI(w1, w2) = log
( p(w1, w2)
p(w1)p(w2)
)
Using PMI, Turney defines SO-PMI for a word
w as
SO-PMI(w) =
log
?
p?P hits(word NEAR p)?
?
n?N hits(n)?
n?N hits(word NEAR n)?
?
p?P hits(p)
hits is a function that returns the number of hits
in a search engine given the query. P is a set of
known positive words, N a set of known negative
words, and NEAR an operator of a search engine
that returns documents in which the operands oc-
cur within a close range of each other.
1105
4 Sentiment Translation
Unsupervised methods like SO-PMI are suitable
to acquire basic sentiment information in a lan-
guage. However, since hand-annotated resources
for sentiment analysis exist in other languages,
it seems plausible to use automatic translation of
sentiment information to leverage these resources.
In order to translate sentiment, we will use multi-
ple sources of information that we represent in a
MEE graph as given in Section 3.1.
In our first experiments (Scheible, 2010), coor-
dinated adjectives were used as the sole training
source. Two adjectives are coordinated if they are
linked with a conjunction like and or but. The
intuition behind using coordinations ? based on
work by Hatzivassiloglou and McKeown (1997)
and Widdows and Dorow (2002) ? was that words
which are coordinated share properties. In partic-
ular, coordinated adjectives usually express sim-
ilar sentiments even though there are exceptions
(e.g., ?The movie was both good and bad?).
In this paper, we focus on using multiple edge
types for sentiment translation. In particular, the
graph we will use contains two types of relations,
coordinations and adjective-noun modification. In
the sentence ?The movie was enjoyable and fun?,
enjoyable and fun are coordinated. In This is an
enjoyable movie, the adjective enjoyable modifies
the noun movie.
We selected these two relation types for two
reasons. First, the two types provide clues for
sentiment analysis. Coordination information is
an established source for sentiment similarity (e.g.
Hatzivassiloglou and McKeown (1997)) while
adjective-noun relations provide a different type
of information on sentiment. For example, nouns
with positive associations (vacation) tend to occur
with positive adjectives and nouns with negative
associations (pain) tend to occur with negative ad-
jectives. Second, we have successfully used these
two types for a similar acquisition task, the acqui-
sition of word-to-word translation pairs (Laws et
al., 2010).
In the resulting graph, adjectives and nouns are
represented as nodes, each containing a word and
its part of speech, and relations are represented as
links which are distinguished by their edge types.
Two graphs, one in the source language and one in
the target language, are needed to translate words
between those languages. Figure 1 shows an ex-
ample for such a setup. Black links in this graph
are coordinations, grey links are seed relations.
In order to calculate sentiment for all nodes in
the target language, we apply the SimRank algo-
rithm to the graphs which gives us similarities be-
tween all nodes in the source graph and all nodes
in the target graph. Using the similarity S(ns, nt)
between a node ns in the source language graph
S and a node nt in the target language graph T ,
the sentiment score (sent(nt)) is the similarity-
weighted average of all sentiment scores in the
target language:
sent(nt) =
?
ns?S
simnorm(ns, nt) sent(ns)
We assume that sentiment scores in the source
language are expressed on a numeric scale. The
normalized similarity simnorm is defined as
simnorm(ns, nt) = S(ns, nt)?
ns?S S(ns, nt)
.
The normalization assures that all resulting sen-
timent values are within [?1, 1], with ?1 being
the most negative sentiment and 1 the most posi-
tive.
5 Experiments
5.1 Data Acquisition
For our experiments, we needed coordination data
to build weighted graphs and a bilingual lexi-
con to define seed relations between those graphs.
Coordinations were extracted from the English
and German versions of Wikipedia1 by applying
pattern-based search using the Corpus Query Pro-
cessor (CQP) (Christ et al, 1999). We annotated
both corpora with parts of speech using the Tree
Tagger (Schmid, 1994). A total of 477,291 En-
glish coordinations and 112,738 German coordi-
nations were collected. A sample of this data is
given in Figure 2. We restrict these experiments
to the use of and/und since other coordinations
1http://www.wikipedia.org/ (01/19/2009)
1106
affordable
delicious
nutritiousjuicy
tasty
healthylovely
schmackhaft
gesundstrange
frisch
wertvoll
nahrhaft angesehen
ertragreich
Figure 1: A German and an English graph with coordinated adjectives including seed links
affordable
delicious
diverse
popularnutritious
inexpensive
original
varied
melodious
rare
strange
juicy
tasty
exotic healthy
tempting
lovely
hearty fragrant
dangerous
beautiful
charming authentic
Figure 2: English sample coordinations (adjectives)
1107
behave differently and might even express dissim-
ilarity (e.g. Was the weather good or bad?).
The seed lexicon was constructed from the
dict.cc dictionary2. While the complete dictionary
contains 30,551 adjective pairs, we reduced the
number of pairs used in the experiments to 1,576.
To produce a smaller seed lexicon which still
makes sense from a semantic point of view, we
used the General Service List (GSL) (West, 1953)
which contains about 2000 words the author con-
sidered central to the English language. More
specifically, a revised list was used3.
SO-PMI needs a larger amount of training data.
Since Wikipedia does not satisfy this need, we
collected additional coordination data from the
web using search result counts from Google. In
Turney?s original paper, he uses the NEAR oper-
ator, which returns documents that contain two
search terms that are within a certain distance of
each other, to collect collocations. Unfortunately,
Google does not support this operator, so instead,
we searched for coordinations using the queries
+ "w and s" and
+ "w und s"
for English and German, respectively. We added
the quotes and the + operator to make sure that
both spelling correction and synonym replace-
ments were disabled.
The original experiments were made for En-
glish, so we had to construct our own set of
seed words. For German, we chose gut (good),
nett (nice), richtig (right), scho?n (beautiful), or-
dentlich (neat), angenehm (pleasant), aufrichtig
(honest), gewissenhaft (faithful), and hervorra-
gend (excellent) as positive seed words, and
schlecht (bad), teuer (expensive), falsch (wrong),
bo?se (evil), feindlich (hostile), verhasst (invidi-
ous), widerlich (disgusting), fehlerhaft (faulty),
and mangelhaft (flawed) as negative ones.
5.2 Sentiment Lexicon
For our experiments, we used two different polar-
ity lexicons. The lexicon of Wilson et al (2005)
contains sentiment annotations for 8,221 words
2http://www.dict.cc
3http://jbauman.com/aboutgsl.html
annotation value
positive 1.0
weakpos 0.5
neutral 0.0
weakneg ?0.5
negative ?1.0
Table 1: Assigned values for Wilson et al set
which are tagged as positive, neutral, or nega-
tive. A few words are tagged as weakneg, imply-
ing weak negativity. These categorial annotations
are mapped to the range [-1,1] using the assign-
ment scheme given in Table 1.
5.3 Human Ratings
In order to manually annotate a test set, we
chose 200 German adjectives that occurred in the
Wikipedia corpus and that were part of a coor-
dination. From these words, we removed those
which we deemed uncommon, too complicated,
or which were mislabeled as adjectives by the tag-
ger. The test set contained 150 adjectives of which
seven were excluded after annotators discarded
them.
We asked 9 native speakers of German to anno-
tate the adjectives. Possible annotations were very
positive, slightly positive, neutral, slightly nega-
tive, or very negative. These categories are the
same as the ones used in the training data.
In order to capture the general sentiment, i.e.,
sentiment that is not related to a specific context,
the judges were asked to stay objective and not
let their personal opinions influence the annota-
tion. However, some words with strong political
implications were annotated by some judges as
non-neutral which led to disagreement beyond the
usual level. Nuklear (nuclear) is an example for
such a word. We measured the agreement of the
judges with Kendall?s coefficient of concordance
(W ) with tie correction (Legendre, 2005), yield-
ing W = 0.674 with a high level of significance
(p < .001); thus, inter-annotator agreement was
high (Landis and Koch, 1977).
5.4 Experimental Setup
Given the relations extracted from Wikipedia, we
built a German and an English graph by setting
1108
Method r
MEE 0.63
MEE-GSL 0.47
SR 0.63
SR-GSL 0.48
SO-PMI 0.58
Table 2: Correlation with human ratings
the weight of each link to the log-likelihood ra-
tio of the two words it connects according to the
corpus frequencies. There are two properties of
the graph transfer algorithm that we intend to in-
vestigate. First, we are interested in the merits of
applying multi edge extraction (MEE) for senti-
ment transfer. Second, we are interested in how
the transfer quality changes when the seed lexi-
con is reduced in size. This way, a sparse data
situation is simulated where large dictionaries are
unavailable. Having these two properties in mind,
four possible setups are evaluated: (i) using the
full seed lexicon with all 30,551 entries, but using
only coordination data (SR), (ii) reducing the seed
lexicon to 1,576 entries from the General Service
List (SR-GSL), (iii) applying MEE by adding ad-
jective modification data (MEE), and (iv) using
MEE with a reduced seed lexicon (MEE-GSL).
SimRank was run for 6 iterations in all experi-
ments. All experiments use the weight function
h as described above. We show that this function
improves similarities and thus lexicon induction
in Laws et al (2010).
Correlation. First, we will examine the correla-
tion between the automatic methods (SO-PMI and
the aforementioned SimRank variations) and the
gold standard as done by Turney in his evaluation.
For this purpose, the human ratings are mapped
to float values following Table 1 and the aver-
age rating over all judges for each word is used.
The correlation coefficients r are given in Table 2.
Judging from these results, the ordering of SR and
MEE matches the human ratings better than SO-
PMI, however it decreases when using any of the
GSL variations instead which can be attributed to
using less data.
Classification. The correct identification of the
classes positive, neutral, and negative is more im-
portant than the correct assignment of values on
a scale since the rank ordering is debatable ? this
becomes apparent when measuring the agreement
of human annotators. Since the assignments made
by the human judges are not unanimous in most
cases, the averages are distributed across the in-
terval [-1,1]; this means that the borders between
the three distinct categories are not clear. Since
there is no standard evaluation for this particu-
lar problem, we need to devise a way to make
the range of the neutral category dynamic. In or-
der to find possible borders, we first assume that
sentiment is distributed symmetrically around 0.
We then define a threshold x which assumes the
values x ? { i20 |0 ? i ? 20}, covering the in-terval [0,0.5]. Since 0.5 is slightly positive, we
do not believe that values above it are plausible.
Then, each word w is positive if its human rating
scoreh(w) ? x, negative if scoreh(w) ? ?x, and
neutral if ?x < scoreh(w) < x. The result of
this process is a gold standard for the three cate-
gories for each of the values for x. The percentiles
of the sizes of those categories are mapped to the
values produced by the automatic methods. For
example, if x = 0.35 means that the top 21% of
all adjectives are in the positive class, the top 21%
of all adjectives as assigned by SO-PMI and the
SimRank varieties are positive as well.
The size of the neutral category increases the
larger x becomes. Thus, high values for x are
unlikely to produce a correct partitioning of the
data. Since slightly positive was defined as 0.5,
we expect the highest plausible value for x to be
below that. The size of the neutral category for
each value of x is given in Table 3. (Recall that
the total size of the set is 143.)
We can then compute the assignment accu-
racy on the positive, neutral, and negative classes,
as well macro- and micro-averages over these
classes.
5.5 Results and Discussion
Figures 3 and 4 show the macro- and micro-
averaged accuracies over the positive, negative,
and neutral class for each automatic method, re-
spectively. Overall, the SimRank variations per-
form better for x in the interval [0, 0.3]. In partic-
ular, MEE has a slightly higher accuracy than SR,
1109
x 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.45 0.50
# neutral 0 13 35 46 56 64 74 82 92 99 99
Table 3: Size of neutral category given x
word (translation) humans SO MEE MEE-GSL SR SR-GSL
chemisch (chemical) 0.00 -20.20 0.185 0.185 0.186 0.184
auferstanden (resurrected) 0.39 -10.96 -0.075 -0.577 -0.057 -0.493
intelligent (intelligent) 0.94 46.59 0.915 0.939 0.834 0.876
versiert (skilled) 0.67 -5.26 0.953 0.447 0.902 0.404
mean -0.04 -9.58 0.003 0.146 0.010 0.142
median 0.00 -15.60 0.110 0.157 0.114 0.157
Table 4: Example adjectives including translation, and their scores
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.1  0.2  0.3  0.4  0.5
A
c
c
u
r
a
c
y
x
SO-PMI (macro)MEE (macro)MEE-GSL (macro)SR (macro)SR-GSL (macro)
Figure 3: Macro-averaged Accuracy
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.1  0.2  0.3  0.4  0.5
A
c
c
u
r
a
c
y
x
SO-PMI (micro)MEE (micro)MEE-GSL (micro)SR (micro)SR-GSL (micro)
Figure 4: Micro-averaged Accuracy
1110
however, not significantly.
Table 4 shows selected example words with
their scores. These values can be understood bet-
ter together with the means and medians of the
respective methods which are given in the table as
well. These values give us an idea of where we
might expect the neutral point of a particular dis-
tribution of polarities.
Chemisch (chemical) is misclassified by SO-
PMI since it occurs in negative contexts on the
web. SimRank in turn was able to recognize
that most words similar to chemisch are neutral,
the most similar one being its literal translation,
chemical. Auferstanden (resurrected) is an exam-
ple for misclassification by SimRank which hap-
pens because the word is usually coordinated with
words that have negative sentiment, e.g. gestor-
ben (deceased) and gekreuzigt (crucified). This
problem could not be fixed by including adjective-
noun modification data since the coordinations
produced high log-likelihood values which lead to
dead being the most similar word to auferstanden.
Intelligent receives a score close to neutral with
the original (coordination-only) training method,
which could be corrected by applying MEE sim-
ply because the ordering of similar words changes
through the new weighting method. Nouns modi-
fied by intelligent include Leben (life) and Wesen
(being) whose translations are modified by pos-
itive adjectives. Many words, such as versiert
(skilled) are classified more accurately due to the
new weighting method when compared to our pre-
vious experiments (Scheible, 2010) where it re-
ceived a SimRank polarity of only 0.224.
The inclusion of adjective modifications does
not improve the classification results as often as
we had hoped. For some cases (cf. intelligent
mentioned above), the scores do improve, but the
overall impact is limited.
6 Conclusion and Outlook
We were able to show that sentiment translation
with SimRank is able to classify adjectives more
accurately than SO-PMI, an unsupervised base-
line method. We demonstrated that SO-PMI is
outperformed by SimRank when choosing a rea-
sonable region of neutral adjectives. In addition,
we showed that the improvements of SimRank
lead to better accuracy in sentiment translation in
some cases. In future work, we will apply a senti-
ment lexicon generated with SimRank in a senti-
ment classification task for reviews.
The algorithms we compared are different in
their purpose of application. While SO-PMI is
applicable when large corpora are available for a
language, it fails when used in a sparse-data situ-
ation, as noted by Turney (2002). We showed that
despite reducing the seed lexicon for SimRank to
a small fraction of its original size, it still performs
better than SO-PMI.
Currently, our experiments are limited by the
choice of using adjectives for our test set. While
the examination of adjectives is highly important
for sentiment analysis (as shown by Pang et al
(2002) who were able to achieve high accuracy
even when using only adjectives), the application
of our algorithms to a broader set of linguistic
units is an important goal for future work.
Acknowledgments. We are grateful to
Deutsche Forschungsgemeinschaft for fund-
ing this research as part of the WordGraph
project.
References
Banea, Carmen, Rada Mihalcea, Janyce Wiebe, and
Samer Hassan. 2008. Multilingual subjectivity
analysis using machine translation. In Empirical
Methods in Natural Language Processing, pages
127?135.
Christ, O., B.M. Schulze, A. Hofmann, and E. Koenig.
1999. The IMS Corpus Workbench: Corpus Query
Processor (CQP): User?s Manual. University of
Stuttgart, March, 8:1999.
Dorow, Beate, Florian Laws, Lukas Michelbacher,
Christian Scheible, and Jason Utt. 2009. A graph-
theoretic algorithm for automatic extension of trans-
lation lexicons. In Workshop on Geometrical Mod-
els of Natural Language Semantics, pages 91?95.
Hatzivassiloglou, Vasileios and Kathleen R. McKe-
own. 1997. Predicting the semantic orientation of
adjectives. In Proceedings of the 35th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 174?181.
Jeh, Glen and Jennifer Widom. 2002. Simrank: A
measure of structural-context similarity. In Pro-
ceedings of the Eighth ACM SIGKDD Interna-
1111
tional Conference on Knowledge Discovery and
Data Mining, pages 538?543.
Landis, J.R. and G.G. Koch. 1977. The measurement
of observer agreement for categorical data. Biomet-
rics, 33(1):159?174.
Laws, Florian, Lukas Michelbacher, Beate Dorow,
Christian Scheible, Ulrich Heid, and Hinrich
Schu?tze. 2010. A linguistically grounded graph
model for bilingual lexicon extraction. In Proceed-
ings of the 23nd International Conference on Com-
putational Linguistics.
Legendre, P. 2005. Species associations: the Kendall
coefficient of concordance revisited. Journal of
Agricultural Biological and Environment Statistics,
10(2):226?245.
Michelbacher, Lukas, Florian Laws, Beate Dorow, Ul-
rich Heid, and Hinrich Schu?tze. 2010. Building
a cross-lingual relatedness thesaurus using a graph
similarity measure. In Proceedings of the Seventh
Conference on International Language Resources
and Evaluation.
Mihalcea, Rada, Carmen Banea, and Janyce Wiebe.
2007. Learning multilingual subjective language
via cross-lingual projections. In Proceedings of the
45th Annual Meeting of the Association of Compu-
tational Linguistics, pages 976?983.
Pang, Bo, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment classification using
machine learning techniques. In Proceedings of the
2002 Conference on Empirical Methods in Natural
Language Processing, pages 79?86.
Scheible, Christian. 2010. Sentiment translation
through lexicon induction. In Proceedings of the
ACL 2010 Student Research Workshop, Uppsala,
Sweden. Association for Computational Linguis-
tics.
Schmid, Helmut. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
International Conference on New Methods in Lan-
guage Processing.
Turney, Peter. 2002. Thumbs up or thumbs down?
semantic orientation applied to unsupervised classi-
fication of reviews. In Proceedings of 40th Annual
Meeting of the Association for Computational Lin-
guistics, pages 417?424.
Wan, Xiaojun. 2009. Co-training for cross-lingual
sentiment classification. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natu-
ral Language Processing of the AFNLP, pages 235?
243, Suntec, Singapore, August. Association for
Computational Linguistics.
West, Michael. 1953. A general service list of english
words.
Widdows, Dominic and Beate Dorow. 2002. A graph
model for unsupervised lexical acquisition. InCOL-
ING.
Wilson, Theresa, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of Human
Language Technology Conference and Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 347?354, October.
1112
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 793?803,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
A Cascaded Classification Approach to Semantic Head Recognition
Lukas Michelbacher Alok Kothari Martin Forst?
Christina Lioma Hinrich Schu?tze
Institute for NLP
University of Stuttgart
{michells,kotharak,liomaca}@ims.uni-stuttgart.de
?Microsoft
martin.forst@microsoft.com
Abstract
Most NLP systems use tokenization as part
of preprocessing. Generally, tokenizers are
based on simple heuristics and do not recog-
nize multi-word units (MWUs) like hot dog
or black hole unless a precompiled list of
MWUs is available. In this paper, we propose
a new cascaded model for detecting MWUs
of arbitrary length for tokenization, focusing
on noun phrases in the physics domain. We
adopt a classification approach because ? un-
like other work on MWUs ? tokenization re-
quires a completely automatic approach. We
achieve an accuracy of 68% for recognizing
non-compositional MWUs and show that our
MWU recognizer improves retrieval perfor-
mance when used as part of an information re-
trieval system.
1 Introduction
Most NLP systems use tokenization as part of pre-
processing. Generally, tokenizers are based on sim-
ple heuristics and do not recognize multi-word units
(MWUs) like hot dog or black hole. Our long-term
goal is to build MWU-aware tokenizers that are used
as part of the standard toolkit for NLP preprocessing
alongside part-of-speech and named-entity tagging.
We define an MWU as a sequence of words that
has properties that cannot be inferred from the com-
ponent words (cf. e.g. Manning and Schu?tze (1999,
Ch. 5), Sag et al (2002)). The most important
of these properties is non-compositionality, the fact
that the meaning of a phrase cannot be predicted
from the meanings of its component words. For ex-
ample, a hot dog is not a hot animal but a sausage in
a bun and a black hole in astrophysics is a region of
space with special properties, not a dark cavity.
The correct recognition of MWUs is an important
building block of many NLP tasks. For example, in
information retrieval (IR) the query hot dog should
not retrieve documents that only contain the words
hot and dog individually, outside of the phrase hot
dog.
In this study, we focus on noun phrases in the
physics domain. For specialized domains such as
physics, adaptable and reliable MWU recognition
is of particular importance because comprehensive
and up-to-date lists of MWUs are not available
and would have to be created by hand. We chose
noun phrases because domain-specific terminology
is commonly encoded in noun phrase MWUs; other
types of phrases ? e.g., verb constructions ? rarely
give rise to fixed domain-specific multi-word se-
quences that should be treated as a unit.
We cast the task of MWU tokenization as seman-
tic head recognition in this paper. The importance of
syntactic heads for many NLP tasks is generally ac-
cepted. For example, in coreference resolution iden-
tity of syntactic heads is predictive of coreference;
in parse disambiguation, the syntactic head of a noun
phrase is a powerful feature for resolving attachment
ambiguities. However, in all of these cases, the syn-
tactic head is only an approximation of the informa-
tion that is really needed; the underlying assumption
made when using the syntactic head as a substitute
for the entire phrase is that the syntactic head is rep-
resentative of the phrase. This is not the case when
the phrase is non-compositional.
We define the semantic head of a noun phrase as
the non-compositional part of a phrase. Semantic
heads would serve most NLP tasks better than syn-
tactic heads. For example, a coreference resolution
system is misled if it looks at syntactic heads to de-
793
termine possible coreference of a hot dog . . . the dog
in I first ate a hot dog and then fed the dog. This is
not the case for a system that makes the decision
based on the semantic heads hot dog of a hot dog
and dog of the dog.
The specific NLP application we evaluate in this
paper is information retrieval. We will show that se-
mantic head recognition improves the performance
of an information retrieval system.
We introduce a cascaded classification framework
for recognizing semantic heads that allows us to treat
noun phrases of arbitrary length. We use a number
of previously proposed features for recognizing non-
compositionality and semantic heads. In addition,
we compare three features that measure contextual
similarity.
Our main contributions in this paper are as fol-
lows. First, we introduce the notion of semantic
head, in analogy to syntactic head, and propose se-
mantic head recognition as a new component of NLP
preprocessing. Second, we develop a cascaded clas-
sification framework for semantic head recognition.
Third, we investigate the utility of contextual simi-
larity for detecting non-compositionality and show
that it significantly enhances a baseline semantic
head recognizer. However, we also identify a num-
ber of challenges of using contextual similarity in
high-confidence semantic head recognition. Fourth,
we show that our approach to semantic head recog-
nition improves the performance of an IR system.
Section 2 discusses previous work. In Section 3
we introduce semantic heads and present our cas-
caded model for semantic head recognition. In Sec-
tion 4, we describe our data and three different mea-
sures of contextual similarity. Section 5 introduces
the classifier and its features. Section 6 presents
classification results and discussion. Section 7 de-
scribes the information retrieval experiments. In
Section 8 we present our conclusions.
2 Related Work
While there is a large number of publications on
MWUs and collocation extraction, the general prob-
lem of automatic MWU detection for the specific
purpose of tokenization has not been investigated
before to our knowledge.
The classic approach to identifying collocations
and MWUs is to apply statistical association mea-
sures (AMs) to n-grams extracted from a corpus
? often combined with various linguistic heuris-
tics and other filters, resulting in candidate lists.
Choueka (1988) and the XTRACT system (Smadja,
1993) are well-known examples of this approach.
More recent approaches such as Pecina (2010)
and Ramisch et al (2010) combine classifiers with
association measures. Although our approach is
classification-based as well, our data set has a more
realistic size than Pecina (2010)?s (1 billion words
vs 1.5 million words) and we work on noun phrases
of arbitrary length (instead of just bigrams). The
mwetoolkit1 by Ramisch et al (2010) aims to
be a software package for lexicographers and its
features are limited to a small set of association
measures that do not consider marginal frequencies.
Neither of these two studies includes evaluation in
the context of an application.
Lin (1999) defines a decision criterion for non-
compositional phrases based on the change in the
mutual information of a phrase when substituting
one word for a similar one based on an automatically
constructed thesaurus. The method reaches 15.7%
precision and 13.7% recall.
In terms of the extraction of domain-specific
MWUs, cross-language methods have been pro-
posed that make use of the fact that an MWU in one
language might be expressed as a single word in an-
other. Caseli et al (2009) utilize word alignments
in a parallel corpus; Attia et al (2010) exploit the
links between article names of different-language
Wikipedias to search for many-to-one translations.
We did not pursue a cross-language approach be-
cause we strive for a self-contained method of MWU
recognition that operates on a single textual re-
source.
Non-compositionality and distributional se-
mantics. In recent years, a number of studies have
investigated the relationship between distributional
semantics and non-compositionality. These studies
compute the similarity between words and phrases
represented as semantic vectors in a word space
model. A semantic vector of a word is the accumu-
lation of the particular contexts in which the word
1http://sourceforge.net/projects/
mwetoolkit/
794
appears. The underlying idea is similar to Lin?s:
the meaning of a non-compositional phrase some-
how deviates from what one would expect given the
semantic vectors of parts of the phrase. The stan-
dard measure to compare semantic vectors is cosine
similarity. The questions that arise are (i) which
vectors to compare, (ii) how to combine the vectors
of the parts and (iii) from what point on a certain
dissimilarity indicates non-compositionality. To our
knowledge, there are no generally accepted answers
to these questions.
Regarding (i), Schone and Jurafsky (2001) com-
pare the semantic vector of a phrase p and the vec-
tors of its component words in two ways: one in-
cludes the contexts of p in the construction of the
semantic vectors of the parts and one does not. Re-
garding (ii), they suggest weighted or unweighted
sums of the semantic vectors of the parts.
Baldwin et al (2003) investigate semantic decom-
posability of noun-noun compounds and verb con-
structions. They address (i) by comparing the se-
mantic vectors of phrases with the vectors of their
parts individually to detect meaning changes; e.g.,
they compare vice president to vice and president.
We propose a new method that compares phrases
with their alternative phrases, in the spirit of Lin
(1999)?s substitution approach (see Section 4.3).
Our rationale is that context features should be
based on contexts that are syntactically similar to the
phrase in question.
With respect to (iii), the above-mentioned studies
use ad hoc thresholds to separate compositional and
non-compositional phrases but do not offer a princi-
pled decision criterion.2 In contrast, we train a sta-
tistical classifier to learn a decision criterion.
There is a larger body of work concerning non-
compositionality which revolves around the prob-
lem of literal (compositional) vs. non-literal (non-
compositional) usage of idiomatic verb construc-
tions like to break the ice or to spill the beans.
Some studies approach the problem with semantic
vector comparisons in the style of Schone and Ju-
rafsky (2001), e.g Katz and Giesbrecht (2006) and
Cook et al (2007). Other approaches use word-
alignment (e.g. Moiro?n and Tiedemann (2006)) or
2Lin (1999) uses a well-defined criterion but his approach is
not based on vector similarity.
a combination of heuristic and linguistic features
(e.g. Diab and Bhutada (2009), Li and Sporleder
(2010)). Even though there is some methodologi-
cal overlap between our approach and some of the
verb-oriented studies, we believe that verb construc-
tions have properties that are quite different from
noun phrases. For example, our definition of alter-
native vector relies on the fact that most noun phrase
MWUs are fixed and exhibit no syntactic variability.
In contrast, verb constructions are often discontinu-
ous.
The motivation for most work on MWU detec-
tion is lexicography, terminology extraction or the
creation of machine-readable dictionaries. Our mo-
tivation ? tokenization in a preprocessing setting ? is
different from this earlier work.
3 Semantic Heads and Cascaded Model
We cast the task of MWU tokenization as seman-
tic head recognition in this paper. We define the
semantic head of a noun phrase as the largest non-
compositional part of the phrase that contains the
syntactic head. For example, black hole is the se-
mantic head of unusual black hole and afterglow is
the semantic head of bright optical afterglow; in the
latter case syntactic and semantic heads coincide.
Semantic heads would serve most NLP tasks bet-
ter than syntactic heads. The attachment ambiguity
of the last noun phrase in he bought the hot dogs in a
packet can be easily resolved for the semantic head
hot dogs (food is often in a packet), but not as easily
for the syntactic head dogs (dogs are usually not in
packets). Indeed, we will show in Section 7 that se-
mantic head recognition improves the performance
of an IR system.
The semantic head is either a single noun or a non-
compositional noun phrase. In the latter case, the
modifier(s) introduce(s) a non-compositional, un-
predictable shift of meaning; hot shifts the mean-
ing of dog from live animal to food. In contrast,
the compositional meaning shift caused by small
in small dog is transparent. The semantic head al-
ways contains the syntactic head; for compositional
phrases, syntactic head and semantic head are iden-
tical.
To determine the semantic head of a phrase, we
use a cascaded classification approach. The cascade
795
(1) neutron star
(2) unusual black hole
(3) bright optical afterglow
(4) small moment of inertia
Figure 1: Example phrases with modifiers. Peripheral
elements are set in italics, syntactic heads in bold.
comes into play in all aspects of our study: the rat-
ing experiments with human subjects, data extrac-
tion, feature design and classification itself.
We need a cascade because we want to recog-
nize the semantic head in noun phrases of arbitrary
length. The starting point is a phrase of length n:
p = w1 . . . wn. We distinguish between the syntac-
tic head of a phrase and the remaining words, the
modifiers. Figure 1 shows phrases of varying syn-
tactic complexity. The syntactic head is marked in
bold. The model accommodates pre-nominal modi-
fiers as in examples (1) through (3) and post-nominal
modifiers like PPs in example (4).
Among the modifiers, there is a distinguished ele-
ment, the peripheral element u (italicized in the ex-
amples). The remaining words are called the rest
v. We can now represent any phrase p as p = uv.3
The element u is always the outermost modifier. of -
PPs are treated as a single modifier and they take
precedence over pre-nominal modification because
this analysis is dominant in our gold standard data.
This means that in the phrase small moment of iner-
tia, small (and not of inertia) is the peripheral ele-
ment u.
Cascaded classification then operates as shown in
Figure 2. In each iteration, the classifier decides
whether the relation between the current peripheral
element u and the rest v is compositional (C) or non-
compositional (NC). If the relation is NC, process-
ing stops and uv is returned as the semantic head
of p. If the relation is compositional, u is discarded
and classification continues with v as the new input
phrase, which again is represented in the form u?v?.
In case there is no more peripheral element u, i.e.
the new v is a single word, it is returned as the se-
mantic head of p.
Table 1 shows two examples. For the fully com-
positional phrase bright optical afterglow, the pro-
3We use the abstract representation p = uv even though u
can appear after v in the surface form of p.
function recognize semantic head(p)
u? peripheral(p)
v ? rest(p)
while decision(u, v) 6= NC do
u? peripheral(v)
if u = ? then
return v
v ? rest(v)
return uv
Figure 2: Cascaded classification of p
step u v decision
1 bright optical afterglow C
2 optical afterglow C
3 ? afterglow
1 small moment of inertia C
2 of inertia moment NC
Table 1: Cascaded decision processes
cess runs all the way down to the syntactic head af-
terglow which is also the semantic head. In the sec-
ond case, the process stops earlier, in step 2, because
the classifier finds that the relation between moment
and of inertia is NC. This means that the semantic
head of small moment of inertia is moment of iner-
tia.
4 Corpus and Feature Definitions
4.1 Candidate phrases
As our corpus, we use the iSearch collection, a
one billion word collection of documents from the
physics domain (Lykke et al, 2010). We tokenized
the collection by splitting on white space and adding
sentence boundaries and part-of-speech tags to the
output. With part-of-speech information, the iden-
tification of MWU candidates is easy, fast and reli-
able.
We extracted all noun phrases from the collection
that consist of a head noun with up to four modifiers
? almost all domain-specific terminology in our col-
lection is captured by this pattern. The pre-nominal
modifiers can be nouns, proper nouns, adjectives or
cardinal numbers.
The baseline accuracy of a classifier that always
chooses compositionality is very high (> 90%) for
796
V = v V 6= v
U = u O11 O12 = R1
U 6= u O21 O22 = R2
= C1 = C2 = N
Table 2: 2-by-2 contingency tables with observed and
marginal frequencies
phrases of the type [noun] of the/a [noun] (sg.)
(e.g. rest of the paper) and [noun] of [noun] (pl.)
(e.g. series of papers). We therefore restrict post-
nominal modifiers to prepositional phrases with the
word of followed by a non-modified, indefinite, sin-
gular noun, e.g., speed of light or moment of inertia.
Out of all phrases extracted with part-of-speech
patterns, we keep only the ones that appear more of-
ten than 50 times because it is hard to compute re-
liable features for less frequent phrases. All experi-
ments were carried out with lemmatized word forms.
We refer to lemmas as words if not noted otherwise.
4.2 Association measures
Statistical association measures are frequently used
for MWU detection and collocation extraction (e.g.
Schone and Jurafsky (2001), Evert and Krenn
(2001), Pecina (2010)).
We use all measures used by Schone and Jurafsky
(2001) that can be derived from a phrase?s contin-
gency table. These measures are Student?s t-score,
z-score, ?2, pointwise mutual information (MI),
Dice coefficient, frequency, log-likelihood (G2) and
symmetric conditional probability.
We define the AMs in Table 3 based on the no-
tation for the contingency table shown in Table 2
(cf. Evert (2004)). Oij is observed frequency and
Eij = RiCjN expected frequency.
The AMs are designed to deal with two random
variables U and V that traditionally represent single
words. In our model, we use U to represent periph-
eral elements u and V for rests v.
association measure formula
student?s t-score (amt) O11?E11?O11
z-score (amz) O11?E11?E11
chi-square (am?2)
?
i,j
(Oij?Eij)2
Eij
pointwise mutual infor-
mation (amMI ) log
O11
E11
Dice coefficient (amD) 2O11R1+C1
frequency (amf ) O11
log-likelihood (amG2) 2
?
i,j
Oij log OijEij
symmetric conditional
probability (amscp)
O112
R1C1
Table 3: Association measures
4.3 Word space model
As our baseline, we use two methods of compar-
ing semantic vectors: sj1 and sj2, both introduced
by Schone and Jurafsky (2001). They experimented
with variants of sj1 and sj2, but found no large differ-
ences. In addition, we introduce our own approach
alt.
Method sj1 compares the semantic vector of a
phrase p with the sum of the vectors of its parts.
Method sj2 is like sj1, except the contexts of p are
not part of the semantic vectors of the parts. Method
alt compares the semantic vector of a phrase with its
alternative vector. In the definitions below, s repre-
sents a vector similarity measure, w(p) a general se-
mantic vector of a phrase p and w?(wi) the semantic
vector of a partwi of a phrase p that does not include
the contexts of occurrences of wi that were part of p
itself.
sj1 s(w(black hole), w(black) + w(hole))
sj2 s(w(black hole), w?(black) + w?(hole))
alt s(w(black hole),?
u
w(u, hole)); u 6= black
For the third comparison, we build the alternative
vector as follows. For a phrase p = uv with pe-
ripheral element u and rest v, we call the phrase
797
p? = u?v an alternative phrase if the rest v is the
same and u? 6= u. E.g., giant star is an alternative
phrase of neutron star and isolated neutron star is
an alternative of young neutron star. The alterna-
tive vector of p is then the semantic vector that is
computed from the contexts of all of p?s alternative
phrases. The alternative vector is a representation
of the contexts of v except for those modified by u.
This technique bears resemblance to the substitution
approach of Lin (1999). The difference is that he
relies on a similarity thesaurus for substitution and
monitors the change in mutual information for each
substitution individually whereas we substitute with
general alternative modifiers and combine the alter-
native contexts into one vector for comparison.
Previous work has compared the semantic vector
of a phrase with the vectors of its components. Our
approach is more ?head-centric? and only compares
phrases in the same syntactic configuration. Our
question is: Is the typical context of the head hole
if it occurs with a modifier that is not black different
from when it occurs with the modifier black?
We used a bag-of-words model and a window of
?10 words for contexts to create semantic vectors.
We only kept the content words in the window which
we defined as words that are tagged as either a noun,
verb, adjective or adverb. To add information about
the variability of syntactic contexts in which phrases
occur, we add the words immediately before and af-
ter the phrase with positional markers (?1 and +1,
respectively) to the vector. These words were not
subject to the content-word filter. The dimension-
ality of the vectors is then 3V where V is the size
of the vocabulary: V dimensions each for bag-of-
words, left and right syntactic contexts. We did not
include vectors for the stop word of for sj1 and sj2.
4.4 Non-compositionality judgments
Since the domain of the corpus is physics, highly
specialized vocabulary had to be judged. We em-
ployed domain experts as raters (one engineering
and two physics graduate students).
In line with the cascaded model, the raters where
asked to identify the semantic head of each candi-
date phrase. If at least two raters agreed on a seman-
tic head of a phrase we made this choice the seman-
tic head in the gold standard. The final gold standard
comprises 1560 phrases.
We computed raw agreement of each rater with
the gold standard as the percentage of correctly rec-
ognized semantic heads ? this is the task that the
classifier addresses. Agreement is quite high at
86.5%, 88.3% and 88.5% for the three raters. In
addition, we calculated chance-corrected agreement
with Cohen?s ? on the first decision task against the
gold standard (see Section 6). As expected, agree-
ment decreases, but is still substantial at 74.0%,
78.2% and 71.8% for the three raters.
5 Classifier
We use the Stanford maximum entropy classifier for
our experiment.4 We randomly split the data into a
training set of 1300 and a held-out test set of 260
pairs.
We use the eight AMs and the cosine similari-
ties simsj1, simsj2 and simalt described in Sec-
tion 4.3 as features for the classifier. Cosine similar-
ity should be small if a phrase is non-compositional
and large if it is compositional. In other words, if the
contexts of the candidate phrase are too dissimilar to
the contexts of the sum of its parts or to the alterna-
tive phrases, then we suspect non-compositionality.
Feature values are binned into 5 bins. We ap-
plied a log transformation to the four AMs with large
values: amf , amG2 , am?2 and amz . For our ap-
plication there is little difference between statistical
significance at p < .001 and p < .00001. The
log transformation reduces the large gap in magni-
tude between high significance and very high signif-
icance. If co-occurrence of u and v in uv is below
chance, then we set the association scores to 0 since
this is an indication of compositionality (even if it is
highly significant).
Since AMs have been shown to be correlated (e.g.
Pecina (2010)), we first perform feature selection on
the AM features. We tested accuracy of all 2r ? 1
non-empty combinations of the r = 8 AM features
on the task of deciding whether the first decision
during the classification of a phrase was C or NC.
We then selected those AM features that were part
of at least one top 10 result in each fold. Those fea-
tures were amt, amf and amscp.
The main experiment combines these three se-
4http://nlp.stanford.edu/software/
classifier.shtml
798
lected AM features with all possible subsets of con-
text features. We train on the 1300-element training
set and test on the 260-element test set.
6 Results and Discussion
We ran three evaluation modes: dec-1st, dec-all, and
semh. Mode dec-1st only evaluates the first deci-
sion for each phrase; the baseline in this case is .554
since 55.4% of the first decisions are C. In mode
dec-all, we evaluate all decisions that were made in
the course of recognizing the semantic head. This
mode emphasizes the correct recognition of seman-
tic heads in phrases where multiple correct decisions
in a row are necessary. We define the confidence
for multi-decision classification as the product of
the confidence values of all intermediate decisions.
There is no obvious baseline for dec-all because the
number of decisions depends on the classifier ? a
classifier whose first decision on a four-word phrase
is NC makes one decision, another one may make
three. The mode semh evaluates how many semantic
heads were recognized correctly. This mode directly
evaluates the task of semantic head recognition. The
baseline for semh is the tokenizer that always returns
the syntactic head; this baseline is .488.5 Table 4
shows 8? 3 runs, corresponding to the three modes
tested on the AM features (amt, amf , and amscp)
and the eight possible subsets of the three context
features.
For all modes, the best result is achieved with base
AMs combined with the simalt feature; the accura-
cies are .692, .703 and .680. The improvements over
the baselines (for dec-1st and semh) are statistically
significant at p < .01 (binomial test, n = 260).
For semh, accuracy without any context features
is .603; this is significantly better than the .488 base-
line (p < .01). Performance with only the base AM
features is significantly lower than the best context
feature experiment (.680) at p < .01 and signifi-
cantly lower than the worst context feature exper-
iment (.653) at p < .1. However, the differences
between the context feature runs are not significant.
When the semantic head recognizer processes a
phrase, there are four possible results. Result rsemh:
5The baseline could be improved with simple heuristics, e.g.
?uv contains capital letter?? NC. However, this feature only
results in a 2% improvement compared to the baseline.
type freq definition
rsemh 92 sem. head correct (6= synt. head)
rsynth 85 sem. head correct (= synt. head)
r+ 48 sem. head too long
r? 35 sem. head too short
all 260
Table 5: Distribution of result types
the semantic head is correctly recognized and it is
distinct from the syntactic head. Result rsynth: the
semantic head is correctly recognized and it is iden-
tical to the syntactic head. Result r+: the semantic
head is not correctly recognized because the cascade
was stopped too early, i.e., a compositional modifier
that should have been removed was kept. Result r?:
the semantic head is not correctly recognized be-
cause the cascade was stopped too late, i.e., a modi-
fier causing a non-compositional meaning shift was
removed. Table 5 shows the distribution of result
types. It shows that r+ is the more common error:
the classifier more often regards compositional rela-
tions as non-compositional than vice versa.
Table 6 shows the top 20 classifications where
the semantic head was not the same as the syntac-
tic head sorted by confidence in descending order.
In the third column ?phrase . . . ? we list the candi-
dates with semantic heads in bold. The columns to
the right show the predicted semantic head and the
feature values. All five errors in the list are of type
r+.
Two r+ phrases are schematic view and many oth-
ers. The two phrases are clearly compositional and
the classifier failed even though the context feature
points in the direction of compositionality with a
value greater than .5. It can be argued that many oth-
ers is a trivial example that does not require complex
machinery to be identified as compositional, e.g. by
using a stop list. We included it in the analysis since
we want to be able to process arbitrary phrases with-
out additional hand-crafted resources.
Another incorrect classification occurs with the
phrase massive star birth6 for which star birth was
annotated as the semantic head. Here we have a case
where the peripheral element massive does not mod-
6i.e. the birth of a massive star, a certain type of star with
very high mass
799
mode baseline context feature context feature subsets
simalt - ? ? ? ? - - -
simsj1 - - ? ? ? ? - ?
simsj2 - - - ? ? - ? ?
dec-1st .554 .604 .692 .669 .685 .677 .654 .654 .662
dec-all - .615 .703 .681 .696 .688 .666 .669 .675
semh .488 .603 .680 .657 .673 .665 .653 .653 .661
Table 4: Performance for base AM features plus context feature subsets. A ??? indicates the use of the corresponding
context feature.
ify the syntactic head birth but massive star is itself
a complex modifier. In the test set, 5% of the phrases
exhibit structural ambiguities of this type. Our sys-
tem cannot currently deal with this phenomenon.
The remaining r+ phrases are peculiar velocity
and local group. However, Wikipedia lists both
phrases with an individual entry defining the former
as the true velocity of an object, relative to a rest
frame7 and the latter as the group of galaxies that
includes Earth?s galaxy, the Milky Way8. Both def-
initions provide evidence for non-compositionality
since the velocity is not peculiar (as in strange) and
the scope of local is not clear without further knowl-
edge. Arguably, in these cases our method chose a
justifiable semantic head, but the raters disagreed.9
For NLP preprocessing, it is acceptable to sacri-
fice recall and only make high-confidence decisions
on semantic heads. A tokenizer that reliably detects
a subset of MWUs is better than one that recognizes
none. However, our attempts to use the simalt rec-
ognizer (bold in Table 4) in this way were not suc-
cessful. Precision is .680 for confidence > .7 and
does not exceed .770 for higher confidence values.
To understand this effect, we analyzed the distri-
bution of simalt scores. Surprisingly, moderate sim-
ilarity between .4 and .6 is a more reliable indicator
for NC than low similarity < .3. Our intuition for
using distributional semantics in Section 2 was that
low similarity indicates non-compositionality. This
7http://en.wikipedia.org/wiki/Peculiar_
velocity
8http://en.wikipedia.org/wiki/Local_
group
9Further evidence that local group is non-compositional is
the fact that one of the domain experts annotated the phrase as
non-compositional but was overruled by the other two.
does not seem to hold for the lowest similarity val-
ues possibly because they are often extreme cases
in terms of distribution and frequency and then give
rise to unreliable decisions. This means that the con-
text features enhance the overall performance of the
classifier, but they are unreliable and do not support
the high-confidence decisions we need in NLP pre-
processing.
For comparison, the classifier that only uses AM
features achieves 90% precision at 14% recall with
confidence > .7 ? although it has lower overall ac-
curacy than the simalt recognizer. We are still in
the process of analyzing these results and decided to
use the AM-only recognizer for the IR experiment
because it has more predictable performance.
In summary, the results show that, for the recogni-
tion of semantic heads, basic AMs offer a significant
improvement over the baseline. We have shown that
some wrong decisions are defensible even though
the gold standard data suggests otherwise. Context
features further increase performance significantly,
but surprisingly, they are not of clear benefit for
a high-confidence classifier that is targeted towards
recognizing a smaller subset of semantic heads with
high confidence.
7 Information Retrieval Experiment
Typically, IR systems do not process non-
compositional phrases as one semantic entity,
missing out on potentially important information
captured by non-compositionality. This section
illustrates one way of adjusting the retrieval process
so that non-compositional phrases are processed as
semantic entities that may enhance retrieval perfor-
mance. The underlying hypothesis is that, given
800
c. type phrase (semantic head in bold) predicted semantic head amt amf amcp simalt
.99 rsemh ellipsoidal figure of equilibrium ellipsoidal figure of equilibrium 18.03 325 6.23e-01 .219
.99 rsemh point spread function point spread function 95.03 9056 2.33e-01 .529
.99 r+ massive star birth massive star birth 19.99 402 4.81e-03 .134
.98 rsemh high angular resolution imaging high angular resolution imaging 13.07 179 1.27e-03 .173
.98 rsemh integral field spectrograph integral field spectrograph 24.20 586 4.12e-02 .279
.98 r+ local group local group 153.54 24759 8.73e-03 .650
.98 rsemh neutral kaon system neutral kaon system 1.38 108 4.17e-03 .171
.97 rsemh IRAF task IRAF task 49.07 2411 2.96e-02 .517
.92 rsemh easy axis easy axis 44.66 2019 2.79e-03 .599
.89 r+ schematic view schematic view 40.56 1651 8.06e-03 .612
.87 rsemh differential resistance differential resistance 31.71 1034 6.38e-04 .548
.86 rsemh TiO band TiO band 36.84 1372 2.21e-03 .581
.86 r+ many others many others 97.76 9806 6.54e-03 .708
.86 rsemh VLBA observation VLBA observation 43.95 2004 9.35e-04 .648
.85 r+ peculiar velocity peculiar velocity 167.63 28689 2.37e-02 .800
.84 rsemh computation time computation time 43.80 1967 1.35e-03 .657
.83 rsemh Land factor Land factor 21.15 453 6.30e-04 .360
.83 rsemh interference filter interference filter 31.44 1002 1.27e-03 .574
.83 rsemh line formation calculations line formation calculations 14.20 203 1.96e-03 .381
.82 rsemh Wess-Zumino-Witten term Wess-Zumino-Witten term 9.60 94 8.12e-05 .291
Table 6: The 20 most confident classifications where the prediction is semantic head 6= syntactic head. ?c.? = confi-
dence
a query that contains a non-compositional phrase,
boosting the retrieval weight of documents that
contain this phrase will improve overall retrieval
performance.
We do this boosting using Indri?s10 combination
of the language modeling and inference network
approaches (Metzler and Croft, 2004), which al-
lows assigning different degrees of belief to differ-
ent parts of the query. This belief can be drawn from
any suitable external evidence of relevance. In our
case, this source of evidence is the knowledge that
certain query terms constitute a non-compositional
phrase. Under this approach, and using the #weight
and #combine operators for combining beliefs, the
relevance of a documentD to a queryQ is computed
as the probability that D generates Q, P (Q|D):
P (Q|D) =
?
t?Q
P (t|D)
wt
W (W =
?
t?Q
wt) (1)
where t is a term and wt is the belief weight as-
signed to t. The higher wt is, the higher the rank
of documents containing t. In this work, we dis-
10http://www.lemurproject.org/
tinguish between two types of query terms: terms
occurring in non-compositional phrases (Qnc), and
the remaining query terms (Qc). Terms t ? Qnc
receive belief weight wnc and terms t ? Qc belief
weight wc, (wnc + wc = 1 and wnc, wc ? [0, 1]).
To boost the ranking of documents containing non-
compositional phrases, we increase wnc at the ex-
pense of wc. We estimate P (t|D) in Eq. 1 using
Dirichlet smoothing (Zhai and Lafferty, 2002).
We use Indri for indexing and retrieval without
removing stopwords or stemming. This choice is
motivated by two reasons: (i) We do not have a
domain-specific stopword list or stemmer. (ii) Base-
line performance is higher when keeping stopwords
and without stemming, rather than without stop-
words and with stemming.
We use the iSearch collection discussed in Sec-
tion 4. It comprises 453,254 documents and a
set of 65 queries with relevance assessments. To
match documents to queries without any treat-
ment of non-compositionality (baseline run), we
use the Kullback-Leibler language model with
Dirichlet smoothing (KL-Dir) (Zhai and Lafferty,
2002). We applied the preprocessing described
801
run MAP REC P20
baseline 0.0663 770 0.1385
real NC 0.0718 844 0.1538
pseudo NC1 0.0664 788 0.1385
pseudo NC2 0.0658 782 0.1462
pseudo NC3 0.0671 777 0.1477
pseudo NC4 0.0681 807 0.1462
pseudo NC5 0.0670 783 0.1423
Table 7: IR performance without considering non-
compositionality (baseline), versus boosting real and
pseudo non-compositionality (real NC, pseudo NCi).
in Section 4 to the queries and identified non-
compositional phrases with the base AM classifier
from Section 5. Our approach for boosting the
weight of these non-compositional phrases uses
the same retrieval model enhanced with belief
weights as described in Eq. 1 (real NC run). In
addition, we include five runs that boost the weight
of pseudo non-compositional phrases that were
created randomly from the query text (pseudo NC
runs). These pseudo non-compositional phrases
have exactly the same length as the observed non-
compositional phrases for each query. We measure
retrieval performance in terms of mean average
precision (MAP), precision at 20 (P20), and recall
(REC, number of relevant documents retrieved
? total is 2878). For each evaluation measure
separately, we tune the following parameters and
report the best performance: (i) the smoothing
parameter ? of the KL-Dir retrieval model (? ?
{100, 500, 800, 1000, 2000, 3000, 4000, 5000, 8000,
10000}, following Zhai and Lafferty (2002)); (ii)
the belief weights wnc, wc ? {0.1, . . . , 0.9} in steps
of 0.1 while preserving wnc + wc = 1 at all times.
Table 7 displays retrieval performance of our
approach against the baseline and five runs with
pseudo non-compositional phrases. We see a 9.61%
improvement in the number of relevant retrieved
documents over the baseline. MAP and P20 also
show improvements. Our approach is better than
any of the 5 random runs on all three metrics ? the
probability of getting such a good result by chance
is 125 < .05, and thus the improvements are statis-tically significant. On doing a query-wise analysis
of MAP scores, we find that large improvements
over the baseline occur when a non-compositional
phrase aligns with what the user is looking for. The
system seems to retrieve more relevant documents
in that case. E.g., the improvement in MAP is
0.0977 for query #19. The user was looking for
?articles . . . on making tunable vertical cavity sur-
face emitting laser diodes? and laser diodes was
one of the non-compositional phrases recognized.
On the other hand, a decrease in MAP occurs for
non-compositional phrases unrelated to the infor-
mation need. In query #4 the user is looking for
?protein-protein interaction, the surface charge dis-
tribution of these proteins and how this has been in-
vestigated with Electrostatic Force Microscopy? and
though non-compositional phrases such as Force Mi-
croscopy are recognized, these do not reflect the core
information need ?The proteins of interest are the
Avidin-Biotin and IgG-anti-IgG systems?.
8 Conclusion
We have presented an approach to improving to-
kenization in NLP preprocessing that is based on
the notion of semantic head. Semantic heads are
? in analogy to syntactic heads ? the core meaning
units of phrases that cannot be further semantically
decomposed. To perform semantic head recogni-
tion for tokenization, we defined a novel cascaded
model and implemented it as a statistical classifier
that used previously proposed and new context fea-
tures. We have shown that the classifier significantly
outperforms the baseline and that context features
increase performance. We reached an accuracy of
68% and argued that even a semantic head recog-
nizer restricted to high-confidence decisions is use-
ful ? because reliably recognizing a subset of se-
mantic heads is better than recognizing none. We
showed that context features increase the accuracy
of the classifier, but undermine the confidence as-
sessments of the classifier, a result we are still ana-
lyzing. Finally, we showed that even in its prelim-
inary current form the semantic head recognizer is
able to improve the performance of an IR system.
Acknowledgments
This work was funded by DFG projects SFB 732 and
WordGraph. We also thank the anonymous review-
ers for their comments.
802
References
Mohammed Attia, Antonio Toral, Lamia Tounsi, Pavel
Pecina, and Josef van Genabith. 2010. Automatic ex-
traction of arabic multiword expressions. In Proceed-
ings of the 2010 Workshop on Multiword Expressions,
pages 19?27, Beijing, China. Coling 2010 Organizing
Committee.
Timothy Baldwin, Colin Bannard, Takaaki Tanaka, and
Dominic Widdows. 2003. An empirical model of
multiword expression decomposability. In Proceed-
ings of the ACL 2003 Workshop on Multiword Expres-
sions, pages 89?96, Sapporo, Japan. Association for
Computational Linguistics.
Helena Caseli, Aline Villavicencio, Andre? Machado,
and Maria Jose? Finatto. 2009. Statistically-driven
alignment-based multiword expression identification
for technical domains. In Proceedings of the 2009
Workshop on Multiword Expressions, pages 1?8, Sin-
gapore. Association for Computational Linguistics.
Yaacov Choueka. 1988. Looking for needles in a
haystack. In Proceedings of RIAO88, pages 609?623.
Paul Cook, Afsaneh Fazly, and Suzanne Stevenson.
2007. Pulling their weight: Exploiting syntactic forms
for the automatic identification of idiomatic expres-
sions in context. In Proceedings of the 2007 on Mul-
tiword Expressions, pages 41?48, Prague, Czech Re-
public. Association for Computational Linguistics.
Mona Diab and Pravin Bhutada. 2009. Verb noun con-
struction mwe token classification. In Proceedings of
the 2009 Workshop on Multiword Expressions, pages
17?22, Singapore. Association for Computational Lin-
guistics.
Stefan Evert and Brigitte Krenn. 2001. Methods for the
qualitative evaluation of lexical association measures.
In Proceedings of the 39th Annual Meeting on Associ-
ation for Computational Linguistics, pages 188?195.
Association for Computational Linguistics.
Stefan Evert. 2004. The Statistics of Word Cooccur-
rences: Word Pairs and Collocations. Ph.D. thesis, In-
stitut fu?r maschinelle Sprachverarbeitung (IMS), Uni-
versita?t Stuttgart.
Graham Katz and Eugenie Giesbrecht. 2006. Auto-
matic identification of non-compositional multi-word
expressions using latent semantic analysis. In Pro-
ceedings of the 2006 Workshop on Multiword Expres-
sions, pages 12?19, Sydney, Australia. Association for
Computational Linguistics.
Linlin Li and Caroline Sporleder. 2010. Linguistic cues
for distinguishing literal and non-literal usages. In
Coling 2010: Posters, pages 683?691, Beijing, China.
Coling 2010 Organizing Committee.
Dekang Lin. 1999. Automatic identification of non-
compositional phrases. In Proceedings of the 37th
Annual Meeting of the Association for Computational
Linguistics, pages 317?324, College Park, Maryland,
USA. Association for Computational Linguistics.
Marianne Lykke, Birger Larsen, Haakon Lund, and Pe-
ter Ingwersen. 2010. Developing a test collection for
the evaluation of integrated search. In Advances in In-
formation Retrieval, 32nd European Conference on IR
Research, ECIR 2010, Milton Keynes, UK, March 28-
31, 2010. Proceedings, pages 627?630.
Christopher D. Manning and Hinrich Schu?tze. 1999.
Foundations of Statistical Natural Language Process-
ing. The MIT Press, Cambridge, MA.
Donald Metzler and W. Bruce Croft. 2004. Combining
the language model and inference network approaches
to retrieval. Inf. Process. Manage., 40(5):735?750.
B.V. Moiro?n and Jo?rg Tiedemann. 2006. Identify-
ing Idiomatic Expressions Using Automatic Word-
Alignment. In Multi-Word-Expressions in a Multilin-
gual Context, page 33.
Pavel Pecina. 2010. Lexical association measures and
collocation extraction. Language Resources and Eval-
uation, 44(1-2):138?158.
Carlos Ramisch, Aline Villavicencio, and Christian
Boitet. 2010. mwetoolkit: a framework for multiword
expression identification. In Proceedings of the Sev-
enth conference on International Language Resources
and Evaluation (LREC?10), Valletta, Malta.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multiword ex-
pressions: A pain in the neck for nlp. In Proceedings
of the 3rd International Conference on Intelligent Text
Processing and Computational Linguistics, pages 1?
15, Mexico City.
Patrick Schone and Daniel Jurafsky. 2001. Is
knowledge-free induction of multiword unit dictionary
headwords a solved problem? In Proceedings of
the 2001 Conference on Empirical Methods in Natu-
ral Language Processing, pages 100?108, Pittsburgh,
Pennsylvania, USA. Association for Computational
Linguistics.
Frank Smadja. 1993. Retrieving collocations from text:
Xtract. Computational linguistics, 19(1):143?177.
ChengXiang Zhai and John D. Lafferty. 2002. Two-stage
language models for information retrieval. In SIGIR,
pages 49?56. ACM.
803
