Proceedings of the EACL 2009 Workshop on Computational Linguistic Aspects of Grammatical Inference, pages 33?40,
Athens, Greece, 30 March 2009. c?2009 Association for Computational Linguistics
A note on contextual binary feature grammars
Alexander Clark
Department of Computer Science
Royal Holloway, University of London
alexc@cs.rhul.ac.uk
Re?mi Eyraud and Amaury Habrard
Laboratoire d?Informatique Fondamentale
de Marseille, CNRS,
Aix-Marseille Universite?, France
remi.eyraud,amaury.habrard@lif.univ-mrs.fr
Abstract
Contextual Binary Feature Grammars
were recently proposed by (Clark et al,
2008) as a learnable representation for
richly structured context-free and con-
text sensitive languages. In this pa-
per we examine the representational
power of the formalism, its relationship
to other standard formalisms and lan-
guage classes, and its appropriateness
for modelling natural language.
1 Introduction
An important issue that concerns both natu-
ral language processing and machine learning
is the ability to learn suitable structures of a
language from a finite sample. There are two
major points that have to be taken into ac-
count in order to define a learning method use-
ful for the two fields: first the method should
rely on intrinsic properties of the language it-
self, rather than syntactic properties of the
representation. Secondly, it must be possible
to associate some semantics to the structural
elements in a natural way.
Grammatical inference is clearly an impor-
tant technology for NLP as it will provide a
foundation for theoretically well-founded un-
supervised learning of syntax, and thus avoid
the annotation bottleneck and the limitations
of working with small hand-labelled treebanks.
Recent advances in context-free grammati-
cal inference have established that there are
large learnable classes of context-free lan-
guages. In this paper, we focus on the ba-
sic representation used by the recent approach
proposed in (Clark et al, 2008). The authors
consider a formalism called Contextual Binary
Feature Grammars (CBFG) which defines a
class of grammars using contexts as features
instead of classical non terminals. The use of
features is interesting from an NLP point of
view because we can associate some semantics
to them, and because we can represent com-
plex, structured syntactic categories. The no-
tion of contexts is relevant from a grammatical
inference standpoint since they are easily ob-
servable from a finite sample. In this paper
we establish some basic language theoretic re-
sults about the class of exact Contextual Bi-
nary Feature Grammars (defined in Section 3),
in particular their relationship to the Chomsky
hierarchy: exact CBFGs are those where the
contextual features are associated to all the
possible strings that can appear in the corre-
sponding contexts of the language defined by
the grammar.
The main results of this paper are proofs
that the class of exact CBFGs:
? properly includes the regular languages
(Section 5),
? does not include some context-free lan-
guages (Section 6),
? and does include some non context-free
languages (Section 7).
Thus, this class of exact CBFGs is orthog-
onal to the classic Chomsky hierarchy but
can represent a very large class of languages.
Moreover, it has been shown that this class
is efficiently learnable. This class is therefore
an interesting candidate for modeling natural
language and deserves further investigation.
2 Basic Notation
We consider a finite alphabet ?, and ?? the
free monoid generated by ?. ? is the empty
string, and a language is a subset of ??. We
will write the concatenation of u and v as uv,
and similarly for sets of strings. u ? ?? is a
substring of v ? ?? if there are strings l, r ? ??
such that v = lur.
33
A context is an element of ?? ? ??. For a
string u and a context f = (l, r) we write f 
u = lur; the insertion or wrapping operation.
We extend this to sets of strings and contexts
in the natural way. A context is also known in
structuralist linguistics as an environment.
The set of contexts, or distribution, of a
string u of a language L is, CL(u) = {(l, r) ?
?? ? ??|lur ? L}. We will often drop the
subscript where there is no ambiguity. We
define the syntactic congruence as u ?L v iff
CL(u) = CL(v). The equivalence classes un-
der this relation are the congruence classes of
the language. In general we will assume that
? is not a member of any language.
3 Contextual Binary Feature
Grammars
Most definitions and lemmas of this section
were first introduced in (Clark et al, 2008).
3.1 Definition
Before the presentation of the formalism, we
give some results about contexts to help to
give an intuition of the representation. The
basic insight behind CBFGs is that there is a
relation between the contexts of a string w and
the contexts of its substrings. This is given by
the following trivial lemma:
Lemma 1. For any language L and for any
strings u, u?, v, v? if C(u) = C(u?) and C(v) =
C(v?), then C(uv) = C(u?v?).
We can also consider a slightly stronger result:
Lemma 2. For any language L and for any
strings u, u?, v, v? if C(u) ? C(u?) and C(v) ?
C(v?), then C(uv) ? C(u?v?).
C(u) ? C(u?) means that we can replace
any occurrence of u in a sentence, with a u?,
without affecting the grammaticality, but not
necessarily vice versa. Note that none of these
strings need to correspond to non-terminals:
this is valid for any fragment of a sentence.
We will give a simplified example from En-
glish syntax: the pronoun it can occur every-
where that the pronoun him can, but not vice
versa1. Thus given a sentence ?I gave him
away?, we can substitute it for him, to get the
1This example does not account for a number of syn-
tactic and semantic phenomena, particularly the distri-
bution of reflexive anaphors.
grammatical sentence I gave it away, but we
cannot reverse the process. For example, given
the sentence it is raining, we cannot substi-
tute him for it, as we will get the ungrammat-
ical sentence him is raining. Thus we observe
C(him) ( C(it).
Looking at Lemma 2 we can also say that,
if we have some finite set of strings K, where
we know the contexts, then:
Corollary 1.
C(w) ?
?
u?,v?:
u?v?=w
?
u?K:
C(u)?C(u?)
?
v?K:
C(v)?C(v?)
C(uv)
This is the basis of the representation: a
word w is characterised by its set of contexts.
We can compute the representation of w, from
the representation of its parts u?, v?, by looking
at all of the other matching strings u and v
where we understand how they combine (with
subset inclusion). In order to illustrate this
concept, we give here a simple example.
Consider the language {anbn|n > 0} and
the set K = {aabb, ab, abb, aab, a, b}. Suppose
we want to compute the set of contexts of
aaabbb, Since C(abb) ? C(aabbb), and vacu-
ously C(a) ? C(a), we know that C(aabb) ?
C(aaabbb). More generally, the contexts of ab
can represent anbn, those of aab the strings
an+1bn and the ones of abb the strings anbn+1.
The key relationships are given by context
set inclusion. Contextual binary feature gram-
mars allow a proper definition of the combina-
tion of context inclusion:
Definition 1. A Contextual Binary Feature
Grammar (CBFG) G is a tuple ?F, P, PL,??.
F is a finite set of contexts, called features,
where we write C = 2F for the power set of F
defining the categories of the grammar, P ?
C ? C ? C is a finite set of productions that
we write x ? yz where x, y, z ? C and PL ?
C ? ? is a set of lexical rules, written x ? a.
Normally PL contains exactly one production
for each letter in the alphabet (the lexicon).
A CBFG G defines recursively a map fG
34
from ?? ? C as follows:
fG(?) = ? (1)
fG(w) =
?
(c?w)?PL
c iff |w| = 1
(2)
fG(w) =
?
u,v:uv=w
?
x?yz?P :
y?fG(u)?
z?fG(v)
x iff |w| > 1.
(3)
We give here more explanation about the
map fG. It defines in fact the analysis of a
string by a CBFG. A rule z ? xy is applied
to analyse a string w if there is a cut uv = w
s.t. x ? fG(u) and y ? fG(v), recall that x
and y are sets of contexts. Intuitively, the re-
lation given by the production rule is linked
with Lemma 2: z is included in the set of fea-
tures of w = uv. From this relationship, for
any (l, r) ? z we have lwr ? L(G).
The complete computation of fG is then jus-
tified by Corollary 1: fG(w) defines all the
possible features associated by G to w with all
the possible cuts uv = w (i.e. all the possible
derivations).
Finally, the natural way to define the mem-
bership of a string w in L(G) is to have the
context (?, ?) ? fG(w) which implies that
?u? = u ? L(G).
Definition 2. The language defined by a
CBFG G is the set of all strings that are as-
signed the empty context: L(G) = {u|(?, ?) ?
fG(u)}.
As we saw before, we are interested in cases
where there is a correspondence between the
language theoretic interpretation of a context,
and the occurrence of that context as a feature
in the grammar. From the basic definition of
a CBFG, we do not require any specific con-
dition on the features of the grammar, except
that a feature is associated to a string if the
string appears in the context defined by the
feature. However, we can also require that fG
defines exactly all the possible features that
can be associated to a given string according
to the underlying language.
Definition 3. Given a finite set of contexts
F = {(l1, r1), . . . , (ln, rn)} and a language L
we can define the context feature map FL :
?? ? 2F which is just the map u 7? {(l, r) ?
F |lur ? L} = CL(u) ? F .
Using this definition, we now need a cor-
respondence between the language theoretic
context feature map FL and the representa-
tion in the CBFG fG.
Definition 4. A CBFG G is exact if for all
u ? ??, fG(u) = FL(G)(u).
Exact CBFGs are a more limited formalism
than CBFGs themselves; without any limits
on the interpretation of the features, we can
define a class of formalisms that is equal to
the class of Conjunctive Grammars (see Sec-
tion 4). However, exactness is an important
notion because it allows to associate intrinsic
components of a language to strings. Contexts
are easily observable from a sample and more-
over it is only when the features correspond to
the contexts that distributional learning algo-
rithms can infer the structure of the language.
A basic example of such a learning algorithm
is given in (Clark et al, 2008).
3.2 A Parsing Example
To clarify the relationship with CFG
parsing, we will give a simple worked
example. Consider the CBFG G =
?{(?, ?), (aab, ?), (?, b), (?, abb), (a, ?)(aab, ?)},
P, PL, {a, b}? with PL =
{{(?, b), (?, abb)} ? a, {(a, ?), (aab, ?)} ? b}
and P =
{{(?, ?)} ? {(?, b)}{(aab, ?)},
{(?, ?)} ? {(?, abb)}{(a, ?)},
{(?, b)} ? {(?, abb)}{(?, ?)},
{(a, ?)} ? {(?, ?)}{(aab, ?)}}.
If we want to parse the string w = aabb the
usual way is to have a bottom-up approach.
This means that we recursively compute the
fG map on the substrings of w in order to
check whether (?, ?) belongs to fG(w).
The Figure 1 graphically gives the main
steps of the computation of fG(aabb). Ba-
sically there are two ways to split aabb that
allow the derivation of the empty context:
aab|b and a|abb. The first one correspond
to the top part of the figure while the sec-
ond one is drawn at the bottom. We can
see for instance that the empty context be-
longs to fG(ab) thanks to the rule {(?, ?)} ?
{(?, abb)}{(a, ?)}: {(?, abb)} ? fG(a) and
{(a, ?)} ? fG(b). But for symmetrical reasons
35
the result can also be obtained using the rule
{(?, ?)} ? {(?, b)}{(aab, ?)}.
As we trivially have fG(aa) = fG(bb) = ?,
since no right-hand side contains the concate-
nation of the same two features, an induction
proof can be written to show that (?, ?) ?
fG(w) ? w ? {anbn : n > 0}.
a         a         b         bfG
{(?,b),(?,abb)} {(?,b),(?,abb)} {(a,?),(aab,?)} {(a,?),(aab,?)}
fG fG fG
Rule: (?,?) ? (?,b) (aab,?)
fG(ab)  ? {(?,?)}
Rule: (a,?) ? (?,?) (aab,?)
fG(abb)  ? {(a,?)}
Rule: (?,?) ? (?,abb) (a,?)
fG(aabb)  ? {(?,?)}
f G
{(?,b),(?,abb)} {(?,b),(?,abb)} {(a,?),(aab,?)} {(a,?),(aab,?)}
f G f G f G
Rule: (?,?) ? (?,abb) (a,?)
fG(ab)  ? {(?,?)}
Rule: (?,b) ? (?,abb) (?,?)
fG(aab)  ? {(?,b)}
Rule: (?,?) ? (?,b) (aab,?)
fG(aabb)  ? {(?,?)}
Figure 1: The two derivations to obtain (?, ?)
in fG(aabb) in the grammar G.
This is a simple example that illustrates
the parsing of a string given a CBFG. This
example does not characterize the power of
CBFG since no right handside part is com-
posed of more than one context. A more inter-
esting, example with a context-sensitive lan-
guage, will be presented in Section 7.
4 Non exact CBFGs
The aim here is to study the expressive power
of CBFG compare to other formalism recently
introduced. Though the inference can be done
only for exact CBFG, where features are di-
rectly linked with observable contexts, it is
still worth having a look at the more general
characteristics of CBFG. For instance, it is in-
teresting to note that several formalisms in-
troduced with the aim of representing natural
languages share strong links with CBFG.
Range Concatenation Grammars
Range Concatenation Grammars are a very
powerful formalism (Boullier, 2000), that is a
current area of research in NLP.
Lemma 3. For every CBFG G, there is
a non-erasing positive range concatenation
grammar of arity one, in 2-var form that de-
fines the same language.
Proof. Suppose G = ?F, P, PL,??. Define
a RCG with a set of predicates equal to F
and the following clauses, and the two vari-
ables U, V . For each production x ? yz in
P , for each f ? x, where y = {g1, . . . gi},
z = {h1, . . . hj} add clauses
f(UV ) ? g1(U), . . . gi(U), h1(V ), . . . hj(V ).
For each lexical production {f1 . . . fk} ? a
add clauses fi(a) ? . It is straightforward
to verify that f(w) `  iff f ? fG(w).
Conjunctive Grammar
A more exact correspondence is to the class of
Conjunctive Grammars (Okhotin, 2001), in-
vented independently of RCGs. For every ev-
ery language L generated by a conjunctive
grammar there is a CBFG representing L#
(where the special character # is not included
in the original alphabet).
Suppose we have a conjunctive grammar
G = ??, N, P, S? in binary normal form (as
defined in (Okhotin, 2003)). We construct the
equivalent CBFG G? = ?F, P ?, PL,?? as fol-
lowed:
? For every letter a we add a context (la, ra)
to F such that laara ? L;
? For every rules X ? a in P , we create a
rule {(la, ra)} ? a in PL.
? For every non terminal X ? N , for every
rule X ? P1Q1& . . .&PnQn we add dis-
tinct contexts {(lPiQi , rPiQi)} to F, such
that for all i it exists ui, lPiQiuirPiQi ? L
and PiQi
?
?G ui;
? Let FX,j = {(lPiQi , rPiQi) : ?i} the
set of contexts corresponding to the
jth rule applicable to X. For all
36
(lPiQi , rPiQi) ? FX,j , we add to P
? the
rules (lPiQi , rPiQi) ? FPi,kFQi,l (?k, l).
? We add a new context (w, ?) to F such
that S
?
?G w and (w, ?) ? # to PL;
? For all j, we add to P ? the rule (?, ?) ?
FS,j{(w, ?)}.
It can be shown that this construction gives
an equivalent CBFG.
5 Regular Languages
Any regular language can be defined by an ex-
act CBFG. In order to show this we will pro-
pose an approach defining a canonical form for
representing any regular language.
Suppose we have a regular language L, we
consider the left and right residual languages:
u?1L = {w|uw ? L} (4)
Lu?1 = {w|wu ? L} (5)
They define two congruencies: if l, l? ? u?1L
(resp. r, r? ? Lu?1) then for all w ? ??, lw ?
L iff l?w ? L (resp. wr ? L iff wr? ? L).
For any u ? ??, let lmin(u) be the lexico-
graphically shortest element such that l?1minL =
u?1L. The number of such lmin is finite by
the Myhil-Nerode theorem, we denote by Lmin
this set, i.e. {lmin(u)|u ? ??}. We de-
fine symmetrically Rmin for the right residuals
(Lr?1min = Lu
?1).
We define the set of contexts as:
F (L) = Lmin ?Rmin. (6)
F (L) is clearly finite by construction.
If we consider the regular language de-
fined by the deterministic finite automata
of Figure 2, we obtain Lmin = {?, a, b}
and Rmin = {?, b, ab} and thus F (L) =
{(?, ?), (a, ?), (b, ?), (?, b), (a, b), (b, b), (?, ab),
(a, ab), (b, ab)}.
By considering this set of features, we
can prove (using arguments about congruence
classes) that for any strings u, v such that
FL(u) ? FL(v), then CL(u) ? CL(v). This
means the set of feature F is sufficient to rep-
resent context inclusion, we call this property
the fiduciality.
Note that the number of congruence classes
of a regular language is finite. Each congru-
ence class is represented by a set of contexts
Figure 2: Example of a DFA. The left residuals
are defined by ??1L, a?1L, b?1L and the right
ones by L??1, Lb?1, Lab?1 (note here that
La?1 = L??1).
FL(u). Let KL be finite set of strings formed
by taking the lexicographically shortest string
from each congruence class. The final gram-
mar can be obtained by combining elements
of KL. For every pair of strings u, v ? KL, we
define a rule
FL(uv) ? FL(u), FL(v) (7)
and we add lexical productions of the form
FL(a) ? a, a ? ?.
Lemma 4. For all w ? ??, fG(w) = FL(w).
Proof. (Sketch) Proof in two steps: ?w ?
??, FL(w) ? fG(w) and fG(w) ? FL(w). Each
step is made by induction on the length of w
and uses the rules created to build the gram-
mar, the derivation process of a CBFG and
the fiduciality for the second step. The key
point rely on the fact that when a string w is
parsed by a CBFG G, there exists a cut of w
in uv = w (u, v ? ??) and a rule z ? xy in G
such that x ? fG(u) and y ? fG(v). The rule
z ? xy is also obtained from a substring from
the set used to build the grammar using the
FL map. By inductive hypothesis you obtain
inclusion between fG and FL on u and v.
For the language of Figure 2, the following
set is sufficient to build an exact CBGF:
{a, b, aa, ab, ba, aab, bb, bba} (this corresponds
to all the substrings of aab and bba). We have:
FL(a) = F (L)\{(?, ?), (a, ?)} ? a
FL(b) = F (L) ? b
FL(aa) = FL(a) ? FL(a), FL(a)
FL(ab) = F (L) ? FL(a), FL(b) = FL(a), F (L)
FL(ba) = F (L) ? FL(b), FL(a) = F (L), FL(a)
FL(bb) = F (L) ? FL(b), FL(b) = F (L), F (L)
37
FL(aab) = FL(bba) = FL(ab) = FL(ba)
The approach presented here gives a canon-
ical form for representing a regular language
by an exact CBFG. Moreover, this is is com-
plete in the sense that every context of every
substring will be represented by some element
of F (L): this CBFG will completely model the
relation between contexts and substrings.
6 Context-Free Languages
We now consider the relationship between
CFGs and CBFGs.
Definition 5. A context-free grammar (CFG)
is a quadruple G = (?, V, P, S). ? is a fi-
nite alphabet, V is a set of non terminals
(? ? V = ?), P ? V ? (V ? ?)+ is a finite
set of productions, S ? V is the start symbol.
In the following, we will suppose that a CFG
is represented in Chomsky Normal Form, i.e.
every production is in the form N ? UW with
N,U,W ? V or N ? a with a ? ?.
We will write uNv ?G u?v if there is a pro-
duction N ? ? ? P .
?
?G is the reflexive tran-
sitive closure of ?G. The language defined by
a CFG G is L(G) = {w ? ??|S
?
?G w}.
6.1 A Simple Characterization
A simple approach to try to represent a CFG
by a CBFG is to define a bijection between the
set of non terminals and the set of context fea-
tures. Informally we define each non terminal
by a single context and rewrite the productions
of the grammar in the CBFG form.
To build the set of contexts F , it is sufficient
to choose |V | contexts such that a bijection bC
can be defined between V and F with bC(N) =
(l, r) implies that S
?
? lNr. Note that we fix
bT (S) = (?, ?).
Then, we can define a CBFG
?F, P ?, P ?L,??, where P
? = {bT (N) ?
bT (U)bT (W )|N ? UW ? P} and
P ?L = {bT (N) ? a|N ? a ? P, a ? ?}.
A similar proof showing that this construction
produces an equivalent CBFG can be found
in (Clark et al, 2008).
If this approach allows a simple syntactical
convertion of a CFG into a CBFG, it is not
relevant from an NLP point of view. Though
we associate a non-terminal to a context, this
may not correspond to the intrinsic property
of the underlying language. A context could
be associated with many non-terminals and we
choose only one. For example, the context
(He is, ?) allows both noun phrases and ad-
jective phrases. In formal terms, the resulting
CBFG is not exact. Then, with the bijection
we introduced before, we are not able to char-
acterize the non-terminals by the contexts in
which they could appear. This is clearly what
we don?t want here and we are more interested
in the relationship with exact CBFG.
6.2 Not all CFLs have an exact CBFG
We will show here that the class of context-
free grammars is not strictly included in the
class of exact CBFGs. First, the grammar
defined in Section 3.2 is an exact CBFG for
the context-free and non regular language
{anbn|n > 0}, showing the class of exact
CBFG has some elements in the class of CFGs.
We give now a context-free language L that
can not be defined by an exact CBFG:
L = {anb|n > 0} ? {amcn|n > m > 0}.
Suppose that there exists an exact CBFG that
recognizes it and let N be the length of the
biggest feature (i.e. the longuest left part of
the feature). For any sufficiently large k >
N , the sequences ck and ck+1 share the same
features: FL(ck) = FL(ck+1). Since the CBFG
is exact we have FL(b) ? FL(ck). Thus any
derivation of ak+1b could be a derivation of
ak+1ck which does not belong to the language.
However, this restriction does not mean that
the class of exact CBFG is too restrictive for
modelling natural languages. Indeed, the ex-
ample we have given is highly unnatural and
such phenomena appear not to occur in at-
tested natural languages.
7 Context-Sensitive Languages
We now show that there are some exact
CBFGs that are not context-free. In particu-
lar, we define a language closely related to the
MIX language (consisting of strings with an
equal number of a?s, b?s and c?s in any order)
which is known to be non context-free, and
indeed is conjectured to be outside the class
of indexed grammars (Boullier, 2003).
38
Let M = {(a, b, c)?}, we consider the language
L = Labc?Lab?Lac?{a?a, b?b, c?c, dd?, ee?, ff ?}:
Lab = {wd|w ? M, |w|a = |w|b},
Lac = {we|w ? M, |w|a = |w|c},
Labc = {wf |w ? M, |w|a = |w|b = |w|c}.
In order to define a CBFG recognizing L, we
have to select features (contexts) that can rep-
resent exactly the intrinsic components of the
languages composing L. We propose to use the
following set of features for each sublanguages:
? For Lab: (?, d) and (?, ad), (?, bd).
? For Lac: (?, e) and (?, ae), (?, ce).
? For Labc: (?, f).
? For the letters a?, b?, c?, a, b, c we add:
(?, a), (?, b), (?, c), (a?, ?), (b?, ?), (c?, ?).
? For the letters d, e, f, d?, e?, f ? we add;
(?, d?), (?, e?), (?, f ?), (d, ?), (e, ?), (f, ?).
Here, Lab will be represented by (?, d), but we
will use (?, ad), (?, bd) to define the internal
derivations of elements of Lab. The same idea
holds for Lac with (?, e) and (?, ae), (?, ce).
For the lexical rules and in order to have an
exact CBFG, note the special case for a, b, c:
{(?, bd), (?, ce), (a?, ?)} ? a
{(?, ad), (b?, ?)} ? b
{(?, ad), (?, ae), (c?, ?)} ? c
For the nine other letters, each one is defined
with only one context like {(?, d?)} ? d.
For the production rules, the most impor-
tant one is: (?, ?) ? {(?, d), (?, e)}, {(?, f ?)}.
Indeed, this rule, with the presence of two
contexts in one of categories, means that an
element of the language has to be derived
so that it has a prefix u such that fG(u) ?
{(?, d), (?, e)}. This means u is both an ele-
ment of Lab and Lac. This rule represents the
language Labc since {(?, f ?)} can only repre-
sent the letter f .
The other parts of the language will be
defined by the following rules:
(?, ?) ? {(?, d)}, {(?, d?)},
(?, ?) ? {(?, e)}, {(?, e?)},
(?, ?) ? {(?, a)}, {(?, bd), (?, ce), (a?, ?)},
(?, ?) ? {(?, b)}, {(?, ad), (b?, ?)},
(?, ?) ? {(?, c)}, {(?, ad), (?, ae), (c?, ?)},
(?, ?) ? {(?, d?)}, {(d, ?)},
(?, ?) ? {(?, e?)}, {(e, ?)},
(?, ?) ? {(?, f ?)}, {(f, ?)}.
This set of rules is incomplete, since for rep-
resenting Lab, the grammar must contain the
rules ensuring to have the same number of a?s
and b?s, and similarly for Lac. To lighten the
presentation here, the complete grammar is
presented in Annex.
We claim this is an exact CBFG for a
context-sensitive language. L is not context-
free since if we intersect L with the regular
language {??d}, we get an instance of the
non context-free MIX language (with d ap-
pended). The exactness comes from the fact
that we chose the contexts in order to ensure
that strings belonging to a sublanguage can
not belong to another one and that the deriva-
tion of a substring will provide all the possible
correct features with the help of the union of
all the possible derivations.
Note that the Mix language on its own is
probably not definable by an exact CBFG: it
is only when other parts of the language can
distributionally define the appropriate partial
structures that we can get context sensitive
languages. Far from being a limitation of this
formalism (a bug), we argue this is a feature:
it is only in rather exceptional circumstances
that we will get properly context sensitive lan-
guages. This formalism thus potentially ac-
counts not just for the existence of non context
free natural language but also for their rarity.
8 Conclusion
The chart in Figure 3 summarises the different
relationship shown in this paper. The substi-
tutable languages (Clark and Eyraud, 2007)
and the very simple ones (Yokomori, 2003)
form two different learnable class of languages.
There is an interesting relationship with Mar-
cus External Contextual Grammars (Mitrana,
2005): if we defined the language of a CBFG
to be the set {fG(u)  u : u ? ??} we would
be taking some steps towards contextual gram-
mars.
In this paper we have discussed the weak
generative power of Exact Contextual Binary
Feature Grammars; we conjecture that the
class of natural language stringsets lie in this
class. ECBFGs are efficiently learnable (see
(Clark et al, 2008) for details) which is a com-
39
Context-free                                                   
Regular                        
Context sensitive
very simple
substi-tutable
Range Concatenation                         
                          Conjunctive = CBFG                                Exact CBFG
                   
Figure 3: The relationship between CBFG and
other classes of languages.
pelling technical advantage of this formalism
over other more traditional formalisms such as
CFGs or TAGs.
References
Pierre Boullier. 2000. A Cubic Time Extension
of Context-Free Grammars. Grammars, 3:111?
131.
Pierre Boullier. 2003. Counting with range con-
catenation grammars. Theoretical Computer
Science, 293(2):391?416.
Alexander Clark and Re?mi Eyraud. 2007. Polyno-
mial identification in the limit of substitutable
context-free languages. Journal of Machine
Learning Research, 8:1725?1745, Aug.
Alexander Clark, Re?mi Eyraud, and Amaury
Habrard. 2008. A polynomial algorithm for the
inference of context free languages. In Proceed-
ings of International Colloquium on Grammati-
cal Inference, pages 29?42. Springer, September.
V. Mitrana. 2005. Marcus external contextual
grammars: From one to many dimensions. Fun-
damenta Informaticae, 54:307?316.
Alexander Okhotin. 2001. Conjunctive grammars.
J. Autom. Lang. Comb., 6(4):519?535.
Alexander Okhotin. 2003. An overview of con-
junctive grammars. Formal Language Theory
Column, bulletin of the EATCS, 79:145?163.
Takashi Yokomori. 2003. Polynomial-time iden-
tification of very simple grammars from pos-
itive data. Theoretical Computer Science,
298(1):179?206.
Annex
(?, ?) ? {(?, d), (?, e)}, {(?, f ?)}
(?, ?) ? {(?, d)}, {(?, d?)}
(?, ?) ? {(?, e)}, {(?, e?)}
(?, ?) ? {(?, a)}, {(?, bd), (?, ce), (a?, ?)}
(?, ?) ? {(?, b)}, {(?, ad), (b?, ?)}
(?, ?) ? {(?, c)}, {(?, ad), (?, ae), (c?, ?)}
(?, ?) ? {(?, d?)}, {(d, ?)}
(?, ?) ? {(?, e?)}, {(e, ?)}
(?, ?) ? {(?, f ?)}, {(f, ?)}
(?, d) ? {(?, d)}, {(?, d)}
(?, d) ? {(?, ad)}, {(?, bd)}
(?, d) ? {(?, bd)}, {(?, ad)}
(?, d) ? {(?, d)}, {(?, ad), (?, ae), (c?, ?)}
(?, d) ? {(?, ad), (?, ae), (c?, ?)}, {(?, d)}
(?, ad) ? {(?, ad), (?, ae), (c?, ?)}, {(?, ad)}
(?, ad) ? {(?, ad)}, {(?, ad), (?, ae), (c?, ?)}
(?, ad) ? {(?, ad), (b?, ?)}, {(?, d)}
(?, ad) ? {(?, d)}, {(?, ad), (b?, ?)}
(?, bd) ? {(?, ad), (?, ae), (c?, ?)}, {(?, bd)}
(?, bd) ? {(?, bd)}, {(?, ad), (?, ae), (c?, ?)}
(?, bd) ? {(?, bd), (?, ce), (a?, ?)}, {(?, d)}
(?, bd) ? {(?, d)}, {(?, bd), (?, ce), (a?, ?)}
(?, e) ? {(?, e)}, {(?, e)}
(?, e) ? {(?, ae)}, {(?, ce)}
(?, e) ? {(?, ce)}, {(?, ae)}
(?, e) ? {(?, e)}, {(?, ad), (b?, ?)}
(?, e) ? {(?, ad), (b?, ?)}, {(?, e)}
(?, ae) ? {(?, ad), (b?, ?)}, {(?, ae)}
(?, ae) ? {(?, ae)}, {(?, ad), (b?, ?)}
(?, ae) ? {(?, ad), (?, ae), (c?, ?)}, {(?, e)}
(?, ae) ? {(?, e)}, {(?, ad), (?, ae), (c?, ?)}
(?, ce) ? {(?, ad), (b?, ?)}, {(?, ce)}
(?, ce) ? {(?, ce)}, {(?, ad), (b?, ?)}
(?, ce) ? {(?, bd), (?, ce), (a?, ?)}, {(?, e)}
(?, ce) ? {(?, e)}, {(?, bd), (?, ce), (a?, ?)}
{(?, bd), (?, ce), (a?, ?)} ? a
{(?, ad), (b?, ?)} ? b
{(?, ad), (?, ae), (c?, ?)} ? c
{(?, d?)} ? d
{(?, e?)} ? e
{(?, f ?)} ? f
{(?, a)} ? a?
{(?, b)} ? b?
{(?, c)} ? c?
{(d, ?)} ? d?
{(e, ?)} ? e?
{(f, ?)} ? f ?
40
Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),
pages 125?132, New York City, June 2006. c?2006 Association for Computational Linguistics
Learning Auxiliary Fronting with Grammatical Inference
Alexander Clark
Department of Computer Science
Royal Holloway University of London
Egham, Surrey TW20 0EX
alexc@cs.rhul.ac.uk
Re?mi Eyraud
EURISE
23, rue du Docteur Paul Michelon
42023 Saint- ?Etienne Cedex 2
France
remi.eyraud@univ-st-etienne.fr
Abstract
We present a simple context-free gram-
matical inference algorithm, and prove
that it is capable of learning an inter-
esting subclass of context-free languages.
We also demonstrate that an implementa-
tion of this algorithm is capable of learn-
ing auxiliary fronting in polar interroga-
tives (AFIPI) in English. This has been
one of the most important test cases in
language acquisition over the last few
decades. We demonstrate that learning
can proceed even in the complete absence
of examples of particular constructions,
and thus that debates about the frequency
of occurrence of such constructions are ir-
relevant. We discuss the implications of
this on the type of innate learning biases
that must be hypothesized to explain first
language acquisition.
1 Introduction
For some years, a particular set of examples has
been used to provide support for nativist theories
of first language acquisition (FLA). These exam-
ples, which hinge around auxiliary inversion in the
formation of questions in English, have been con-
sidered to provide a strong argument in favour of
the nativist claim: that FLA proceeds primarily
through innately specified domain specific mecha-
nisms or knowledge, rather than through the oper-
ation of general-purpose cognitive mechanisms. A
key point of empirical debate is the frequency of oc-
currence of the forms in question. If these are van-
ishingly rare, or non-existent in the primary linguis-
tic data, and yet children acquire the construction in
question, then the hypothesis that they have innate
knowledge would be supported. But this rests on the
assumption that examples of that specific construc-
tion are necessary for learning to proceed. In this
paper we show that this assumption is false: that this
particular construction can be learned without the
learner being exposed to any examples of that par-
ticular type. Our demonstration is primarily mathe-
matical/computational: we present a simple experi-
ment that demonstrates the applicability of this ap-
proach to this particular problem neatly, but the data
we use is not intended to be a realistic representation
of the primary linguistic data, nor is the particular
algorithm we use suitable for large scale grammar
induction.
We present a general purpose context-free gram-
matical algorithm that is provably correct under a
certain learning criterion. This algorithm incorpo-
rates no domain specific knowledge: it has no spe-
cific information about language; no knowledge of
X-bar schemas, no hidden sources of information to
reveal the structure. It operates purely on unanno-
tated strings of raw text. Obviously, as all learn-
ing algorithms do, it has an implicit learning bias.
This very simple algorithm has a particularly clear
bias, with a simple mathematical description, that al-
lows a remarkably simple characterisation of the set
of languages that it can learn. This algorithm does
not use a statistical learning paradigm that has to be
tested on large quantities of data. Rather it uses a
125
symbolic learning paradigm, that works efficiently
with very small quantities of data, while being very
sensitive to noise. We discuss this choice in some
depth below.
For reasons that were first pointed out by Chom-
sky (Chomsky, 1975, pages 129?137), algorithms
of this type are not capable of learning all of nat-
ural language. It turns out, however, that algorithms
based on this approach are sufficiently strong to
learn some key properties of language, such as the
correct rule for forming polar questions.
In the next section we shall describe the dispute
briefly; in the subsequent sections we will describe
the algorithm we use, and the experiments we have
performed.
2 The Dispute
We will present the dispute in traditional terms,
though later we shall analyse some of the assump-
tions implicit in this description. In English, po-
lar interrogatives (yes/no questions) are formed by
fronting an auxiliary, and adding a dummy auxiliary
?do? if the main verb is not an auxiliary. For exam-
ple,
Example 1a The man is hungry.
Example 1b Is the man hungry?
When the subject NP has a relative clause that also
contains an auxiliary, the auxiliary that is moved is
not the auxiliary in the relative clause, but the one in
the main (matrix) clause.
Example 2a The man who is eating is hungry.
Example 2b Is the man who is eating hungry?
An alternative rule would be to move the first oc-
curring auxiliary, i.e. the one in the relative clause,
which would produce the form
Example 2c Is the man who eating is hungry?
In some sense, there is no reason that children
should favour the correct rule, rather than the in-
correct one, since they are both of similar com-
plexity and so on. Yet children do in fact, when
provided with the appropriate context, produce sen-
tences of the form of Example 2b, and rarely if ever
produce errors of the form Example 2c (Crain and
Nakayama, 1987). The problem is how to account
for this phenomenon.
Chomsky claimed first, that sentences of the type
in Example 2b are vanishingly rare in the linguis-
tic environment that children are exposed to, yet
when tested they unfailingly produce the correct
form rather than the incorrect Example 2c. This is
put forward as strong evidence in favour of innately
specified language specific knowledge: we shall re-
fer to this view as linguistic nativism.
In a special volume of the Linguistic Review, Pul-
lum and Scholz (Pullum and Scholz, 2002), showed
that in fact sentences of this type are not rare at all.
Much discussion ensued on this empirical question
and the consequences of this in the context of ar-
guments for linguistic nativism. These debates re-
volved around both the methodology employed in
the study, and also the consequences of such claims
for nativist theories. It is fair to say that in spite
of the strength of Pullum and Scholz?s arguments,
nativists remained completely unconvinced by the
overall argument.
(Reali and Christiansen, 2004) present a possible
solution to this problem. They claim that local statis-
tics, effectively n-grams, can be sufficient to indi-
cate to the learner which alternative should be pre-
ferred. However this argument has been carefully re-
butted by (Kam et al, 2005), who show that this ar-
gument relies purely on a phonological coincidence
in English. This is unsurprising since it is implausi-
ble that a flat, finite-state model should be powerful
enough to model a phenomenon that is clearly struc-
ture dependent in this way.
In this paper we argue that the discussion about
the rarity of sentences that exhibit this particular
structure is irrelevant: we show that simple gram-
matical inference algorithms can learn this property
even in the complete absence of sentences of this
particular type. Thus the issue as to how frequently
an infant child will see them is a moot point.
3 Algorithm
Context-free grammatical inference algorithms are
explored in two different communities: in gram-
matical inference and in NLP. The task in NLP is
normally taken to be one of recovering appropri-
ate annotations (Smith and Eisner, 2005) that nor-
mally represent constituent structure (strong learn-
ing), while in grammatical inference, researchers
126
are more interested in merely identifying the lan-
guage (weak learning). In both communities, the
best performing algorithms that learn from raw posi-
tive data only 1, generally rely on some combination
of three heuristics: frequency, information theoretic
measures of constituency, and finally substitutabil-
ity. 2 The first rests on the observation that strings
of words generated by constituents are likely to oc-
cur more frequently than by chance. The second
heuristic looks for information theoretic measures
that may predict boundaries, such as drops in condi-
tional entropy. The third method which is the foun-
dation of the algorithm we use, is based on the distri-
butional analysis of Harris (Harris, 1954). This prin-
ciple has been appealed to by many researchers in
the field of grammatical inference, but these appeals
have normally been informal and heuristic (van Za-
anen, 2000).
In its crudest form we can define it as follows:
given two sentences ?I saw a cat over there?, and ?I
saw a dog over there? the learner will hypothesize
that ?cat? and ?dog? are similar, since they appear
in the same context ?I saw a __ there?. Pairs of
sentences of this form can be taken as evidence that
two words, or strings of words are substitutable.
3.1 Preliminaries
We briefly define some notation.
An alphabet ? is a finite nonempty set of sym-
bols called letters. A string w over ? is a finite se-
quence w = a1a2 . . . an of letters. Let |w| denote
the length of w. In the following, letters will be in-
dicated by a, b, c, . . ., strings by u, v, . . . , z, and the
empty string by ?. Let ?? be the set of all strings,
the free monoid generated by ?. By a language we
mean any subset L ? ??. The set of all substrings
of a language L is denoted Sub(L) = {u ? ?+ :
?l, r, lur ? L} (notice that the empty word does not
belong to Sub(L)). We shall assume an order ? or
 on ? which we shall extend to ?? in the normal
way by saying that u ? v if |u| < |v| or |u| = |v|
and u is lexicographically before v.
A grammar is a quadruple G = ?V, ?, P, S?
where ? is a finite alphabet of terminal symbols, V
1We do not consider in this paper the complex and con-
tentious issues around negative data.
2For completeness we should include lexical dependencies
or attraction.
is a finite alphabet of variables or non-terminals, P
is a finite set of production rules, and S ? V is a
start symbol.
If P ? V ? (??V )+ then the grammar is said to
be context-free (CF), and we will write the produc-
tions as T ? w.
We will write uTv ? uwv when T ? w ? P .
?? is the reflexive and transitive closure of ?.
In general, the definition of a class L relies on
a class R of abstract machines, here called rep-
resentations, together with a function L from rep-
resentations to languages, that characterize all and
only the languages of L: (1) ?R ? R,L(R) ? L
and (2) ?L ? L, ?R ? R such that L(R) = L.
Two representations R1 and R2 are equivalent iff
L(R1) = L(R2).
3.2 Learning
We now define our learning criterion. This is identi-
fication in the limit from positive text (Gold, 1967),
with polynomial bounds on data and computation,
but not on errors of prediction (de la Higuera, 1997).
A learning algorithm A for a class of represen-
tations R, is an algorithm that computes a function
from a finite sequence of strings s1, . . . , sn to R. We
define a presentation of a language L to be an infinite
sequence of elements of L such that every element
of L occurs at least once. Given a presentation, we
can consider the sequence of hypotheses that the al-
gorithm produces, writing Rn = A(s1, . . . sn) for
the nth such hypothesis.
The algorithm A is said to identify the class R in
the limit if for every R ? R, for every presentation
of L(R), there is an N such that for all n > N ,
Rn = RN and L(R) = L(RN ).
We further require that the algorithm needs only
polynomially bounded amounts of data and compu-
tation. We use the slightly weaker notion defined by
de la Higuera (de la Higuera, 1997).
Definition A representation class R is identifiable
in the limit from positive data with polynomial time
and data iff there exist two polynomials p(), q() and
an algorithm A such that S ? L(R)
1. Given a positive sample S of size m A returns
a representation R ? R in time p(m), such that
2. For each representation R of size n there exists
127
a characteristic set CS of size less than q(n)
such that if CS ? S, A returns a representation
R? such that L(R) = L(R?).
3.3 Distributional learning
The key to the Harris approach for learning a lan-
guage L, is to look at pairs of strings u and v and to
see whether they occur in the same contexts; that is
to say, to look for pairs of strings of the form lur and
lvr that are both in L. This can be taken as evidence
that there is a non-terminal symbol that generates
both strings. In the informal descriptions of this that
appear in Harris?s work, there is an ambiguity be-
tween two ideas. The first is that they should appear
in all the same contexts; and the second is that they
should appear in some of the same contexts. We can
write the first criterion as follows:
?l, r lur ? L if and only if lvr ? L (1)
This has also been known in language theory by the
name syntactic congruence, and can be written u ?L
v.
The second, weaker, criterion is
?l, r lur ? L and lvr ? L (2)
We call this weak substitutability and write it as
u .=L v. Clearly u ?L v implies u .=L v when u is
a substring of the language. Any two strings that do
not occur as substrings of the language are obviously
syntactically congruent but not weakly substitutable.
First of all, observe that syntactic congruence is a
purely language theoretic notion that makes no ref-
erence to the grammatical representation of the lan-
guage, but only to the set of strings that occur in
it. However there is an obvious problem: syntac-
tic congruence tells us something very useful about
the language, but all we can observe is weak substi-
tutability.
When working within a Gold-style identification
in the limit (IIL) paradigm, we cannot rely on statis-
tical properties of the input sample, since they will
in general not be generated by random draws from a
fixed distribution. This, as is well known, severely
limits the class of languages that can be learned un-
der this paradigm. However, the comparative sim-
plicity of the IIL paradigm in the form when there
are polynomial constraints on size of characteristic
sets and computation(de la Higuera, 1997) makes it
a suitable starting point for analysis.
Given these restrictions, one solution to this prob-
lem is simply to define a class of languages where
substitutability implies congruence. We call these
the substitutable languages: A language L is substi-
tutable if and only if for every pair of strings u, v,
u .=L v implies u ?L v. This rather radical so-
lution clearly rules out the syntax of natural lan-
guages, at least if we consider them as strings of
raw words, rather than as strings of lexical or syn-
tactic categories. Lexical ambiguity alone violates
this requirement: consider the sentences ?The rose
died?, ?The cat died? and ?The cat rose from its bas-
ket?. A more serious problem is pairs of sentences
like ?John is hungry? and ?John is running?, where
it is not ambiguity in the syntactic category of the
word that causes the problem, but rather ambigu-
ity in the context. Using this assumption, whether
it is true or false, we can then construct a simple
algorithm for grammatical inference, based purely
on the idea that whenever we find a pair of strings
that are weakly substitutable, we can generalise the
hypothesized language so that they are syntactically
congruent.
The algorithm proceeds by constructing a graph
where every substring in the sample defines a node.
An arc is drawn between two nodes if and only if
the two nodes are weakly substitutable with respect
to the sample, i.e. there is an arc between u and v if
and only if we have observed in the sample strings
of the form lur and lvr. Clearly all of the strings in
the sample will form a clique in this graph (consider
when l and r are both empty strings). The connected
components of this graph can be computed in time
polynomial in the total size of the sample. If the
language is substitutable then each of these compo-
nents will correspond to a congruence class of the
language.
There are two ways of doing this: one way, which
is perhaps the purest involves defining a reduction
system or semi-Thue system which directly captures
this generalisation process. The second way, which
we present here, will be more familiar to computa-
tional linguists, and involves constructing a gram-
mar.
128
3.4 Grammar construction
Simply knowing the syntactic congruence might not
appear to be enough to learn a context-free gram-
mar, but in fact it is. In fact given the syntactic con-
gruence, and a sample of the language, we can sim-
ply write down a grammar in Chomsky normal form,
and under quite weak assumptions this grammar will
converge to a correct grammar for the language.
This construction relies on a simple property of
the syntactic congruence, namely that is in fact a
congruence: i.e.,
u ?L v implies ?l, r lur ?L lvr
We define the syntactic monoid to be the quo-
tient of the monoid ??/ ?L. The monoid operation
[u][v] = [uv] is well defined since if u ?L u? and
v ?L v? then uv ?L u?v?.
We can construct a grammar in the following triv-
ial way, from a sample of strings where we are given
the syntactic congruence.
? The non-terminals of the grammar are iden-
tified with the congruence classes of the lan-
guage.
? For any string w = uv , we add a production
[w] ? [u][v].
? For all strings a of length one (i.e. letters of ?),
we add productions of the form [a] ? a.
? The start symbol is the congruence class which
contains all the strings of the language.
This defines a grammar in CNF. At first sight, this
construction might appear to be completely vacu-
ous, and not to define any strings beyond those in
the sample. The situation where it generalises is
when two different strings are congruent: if uv =
w ? w? = u?v? then we will have two different rules
[w] ? [u][v] and [w] ? [u?][v?], since [w] is the
same non-terminal as [w?].
A striking feature of this algorithm is that it makes
no attempt to identify which of these congruence
classes correspond to non-terminals in the target
grammar. Indeed that is to some extent an ill-posed
question. There are many different ways of assign-
ing constituent structure to sentences, and indeed
some reputable theories of syntax, such as depen-
dency grammars, dispense with the notion of con-
stituent structure all together. De facto standards,
such as the Penn treebank annotations are a some-
what arbitrary compromise among many different
possible analyses. This algorithm instead relies on
the syntactic monoid, which expresses the combina-
torial structure of the language in its purest form.
3.5 Proof
We will now present our main result, with an outline
proof. For a full proof the reader is referred to (Clark
and Eyraud, 2005).
Theorem 1 This algorithm polynomially identi-
fies in the limit the class of substitutable context-free
languages.
Proof (Sketch) We can assume without loss of
generality that the target grammar is in Chomsky
normal form. We first define a characteristic set, that
is to say a set of strings such that whenever the sam-
ple includes the characteristic set, the algorithm will
output a correct grammar.
We define w(?) ? ?? to be the smallest word,
according to ?, generated by ? ? (? ? V )+. For
each non-terminal N ? V define c(N) to be the
smallest pair of terminal strings (l, r) (extending ?
from ?? to ?? ? ??, in some way), such that S ??
lNr.
We can now define the characteristic set CS =
{lwr|(N ? ?) ? P, (l, r) = c(N), w = w(?)}.
The cardinality of this set is at most |P | which
is clearly polynomially bounded. We observe that
the computations involved can all be polynomially
bounded in the total size of the sample.
We next show that whenever the algorithm en-
counters a sample that includes this characteristic
set, it outputs the right grammar. We write G? for
the learned grammar. Suppose [u] ??G? v. Then
we can see that u ?L v by induction on the max-
imum length of the derivation of v. At each step
we must use some rule [u?] ? [v?][w?]. It is easy
to see that every rule of this type preserves the syn-
tactic congruence of the left and right sides of the
rules. Intuitively, the algorithm will never generate
too large a language, since the languages are sub-
stitutable. Conversely, if we have a derivation of a
string u with respect to the target grammar G, by
129
construction of the characteristic set, we will have,
for every production L ? MN in the target gram-
mar, a production in the hypothesized grammar of
the form [w(L)] ? [w(M)][w(N)], and for every
production of the form L ? a we have a produc-
tion [w(L)] ? a. A simple recursive argument
shows that the hypothesized grammar will generate
all the strings in the target language. Thus the gram-
mar will generate all and only the strings required
(QED).
3.6 Related work
This is the first provably correct and efficient gram-
matical inference algorithm for a linguistically in-
teresting class of context-free grammars (but see for
example (Yokomori, 2003) on the class of very sim-
ple grammars). It can also be compared to An-
gluin?s famous work on reversible grammars (An-
gluin, 1982) which inspired a similar paper(Pilato
and Berwick, 1985).
4 Experiments
We decided to see whether this algorithm without
modification could shed some light on the debate
discussed above. The experiments we present here
are not intended to be an exhaustive test of the learn-
ability of natural language. The focus is on deter-
mining whether learning can proceed in the absence
of positive samples, and given only a very weak gen-
eral purpose bias.
4.1 Implementation
We have implemented the algorithm described
above. There are a number of algorithmic issues
that were addressed. First, in order to find which
pairs of strings are substitutable, the naive approach
would be to compare strings pairwise which would
be quadratic in the number of sentences. A more
efficient approach maintains a hashtable mapping
from contexts to congruence classes. Caching hash-
codes, and using a union-find algorithm for merging
classes allows an algorithm that is effectively linear
in the number of sentences.
In order to handle large data sets with thousands
of sentences, it was necessary to modify the al-
gorithm in various ways which slightly altered its
formal properties. However for the experiments
reported here we used a version which performs
the man who is hungry died .
the man ordered dinner .
the man died .
the man is hungry .
is the man hungry ?
the man is ordering dinner .
is the man who is hungry ordering dinner ?
?is the man who hungry is ordering dinner ?
Table 1: Auxiliary fronting data set. Examples
above the line were presented to the algorithm dur-
ing the training phase, and it was tested on examples
below the line.
exactly in line with the mathematical description
above.
4.2 Data
For clarity of exposition, we have used extremely
small artificial data-sets, consisting only of sen-
tences of types that would indubitably occur in the
linguistic experience of a child.
Our first experiments were intended to determine
whether the algorithm could determine the correct
form of a polar question when the noun phrase had a
relative clause, even when the algorithm was not ex-
posed to any examples of that sort of sentence. We
accordingly prepared a small data set shown in Ta-
ble 1. Above the line is the training data that the
algorithm was trained on. It was then tested on all of
the sentences, including the ones below the line. By
construction the algorithm would generate all sen-
tences it has already seen, so it scores correctly on
those. The learned grammar also correctly generated
the correct form and did not generate the final form.
We can see how this happens quite easily since the
simple nature of the algorithm allows a straightfor-
ward analysis. We can see that in the learned gram-
mar ?the man? will be congruent to ?the man who
is hungry?, since there is a pair of sentences which
differ only by this. Similarly, ?hungry? will be con-
gruent to ?ordering dinner?. Thus the sentence ?is
the man hungry ?? which is in the language, will be
congruent to the correct sentence.
One of the derivations for this sentence would be:
[is the man hungry ?] ? [is the man hungry] [?] ?
[is the man] [hungry] [?] ? [is] [the man] [hungry]
[?] ? [is] [the man][who is hungry] [hungry] [?] ?
130
it rains
it may rain
it may have rained
it may be raining
it has rained
it has been raining
it is raining
it may have been raining
?it may have been rained
?it may been have rain
?it may have been rain
Table 2: English auxiliary data. Training data above
the line, and testing data below.
[is] [the man][who is hungry] [ordering dinner] [?].
Our second data set is shown in Table 2, and is a
fragment of the English auxiliary system. This has
also been claimed to be evidence in favour of na-
tivism. This was discussed in some detail by (Pilato
and Berwick, 1985). Again the algorithm correctly
learns.
5 Discussion
Chomsky was among the first to point out the limi-
tations of Harris?s approach, and it is certainly true
that the grammars produced from these toy exam-
ples overgenerate radically. On more realistic lan-
guage samples this algorithm would eventually start
to generate even the incorrect forms of polar ques-
tions.
Given the solution we propose it is worth look-
ing again and examining why nativists have felt that
AFIPI was such an important issue. It appears that
there are several different areas. First, the debate
has always focussed on how to construct the inter-
rogative from the declarative form. The problem
has been cast as finding which auxilary should be
?moved?. Implicit in this is the assumption that the
interrogative structure must be defined with refer-
ence to the declarative, one of the central assump-
tions of traditional transformational grammar. Now,
of course, given our knowledge of many differ-
ent formalisms which can correctly generate these
forms without movement we can see that this as-
sumption is false. There is of course a relation be-
tween these two sentences, a semantic one, but this
does not imply that there need be any particular syn-
tactic relation, and certainly not a ?generative? rela-
tion.
Secondly, the view of learning algorithms is very
narrow. It is considered that only sentences of that
exact type could be relevant. We have demonstrated,
if nothing else, that that view is false. The distinction
can be learnt from a set of data that does not include
any example of the exact piece of data required: as
long as the various parts can be learned separately,
the combination will function in the natural way.
A more interesting question is the extent to which
the biases implicit in the learning algorithm are do-
main specific. Clearly the algorithm has a strong
bias. It overgeneralises massively. One of the advan-
tages of the algorithm for the purposes of this paper
is that its triviality allows a remarkably clear and ex-
plicit statement of its bias. But is this bias specific to
the domain of language? It in no way refers to any-
thing specific to the field of language, still less spe-
cific to human language ? no references to parts of
speech, or phrases, or even hierarchical phrase struc-
ture. It is now widely recognised that this sort of re-
cursive structure is domain-general (Jackendoff and
Pinker, 2005).
We have selected for this demonstration an algo-
rithm from grammatical inference. A number of sta-
tistical models have been proposed over the last few
years by researchers such as (Klein and Manning,
2002; Klein and Manning, 2004) and (Solan et al,
2005). These models impressively manage to ex-
tract significant structure from raw data. However,
for our purposes, neither of these models is suitable.
Klein and Manning?s model uses a variety of differ-
ent cues, which combine with some specific initial-
isation and smoothing, and an explicit constraint to
produce binary branching trees. Though very im-
pressive, the model is replete with domain-specific
biases and assumptions. Moreover, it does not learn
a language in the strict sense (a subset of the set of
all strings), though it would be a simple modification
to make it perform such a task. The model by Solan
et al would be more suitable for this task, but again
the complexity of the algorithm, which has numer-
ous components and heuristics, and the lack of a the-
oretical justification for these heuristics again makes
the task of identifying exactly what these biases are,
and more importantly how domain specific they are,
131
a very significant problem.
In this model, the bias of the algorithm is com-
pletely encapsulated in the assumption u .= v im-
plies u ? v. It is worth pointing out that this does
not even need hierarchical structure ? the model
could be implemented purely as a reduction system
or semi-Thue system. The disadvantage of using
that approach is that it is possible to construct some
bizarre examples where the number of reductions
can be exponential.
Using statistical properties of the set of strings,
it is possible to extend these learnability results to
a more substantial class of context free languages,
though it is unlikely that these methods could be ex-
tended to a class that properly contains all natural
languages.
6 Conclusion
We have presented an analysis of the argument that
the acquisition of auxiliary fronting in polar inter-
rogatives supports linguistic nativism. Using a very
simple algorithm based on the ideas of Zellig Har-
ris, with a simple domain-general heuristic, we show
that the empirical question as to the frequency of oc-
currence of polar questions of a certain type in child-
directed speech is a moot point, since the distinction
in question can be learned even when no such sen-
tences occur.
Acknowledgements This work has been partially
supported by the EU funded PASCAL Network of
Excellence on Pattern Analysis, Statistical Mod-
elling and Computational Learning.
References
D. Angluin. 1982. Inference of reversible languages.
Communications of the ACM, 29:741?765.
Noam Chomsky. 1975. The Logical Structure of Lin-
guistic Theory. University of Chicago Press.
Alexander Clark and Remi Eyraud. 2005. Identification
in the limit of substitutable context free languages. In
Sanjay Jain, Hans Ulrich Simon, and Etsuji Tomita,
editors, Proceedings of The 16th International Confer-
ence on Algorithmic Learning Theory, pages 283?296.
Springer-Verlag.
S. Crain and M. Nakayama. 1987. Structure dependence
in grammar formation. Language, 63(522-543).
C. de la Higuera. 1997. Characteristic sets for poly-
nomial grammatical inference. Machine Learning,
(27):125?138. Kluwer Academic Publishers. Manu-
factured in Netherland.
E. M. Gold. 1967. Language indentification in the limit.
Information and control, 10(5):447 ? 474.
Zellig Harris. 1954. Distributional structure. Word,
10(2-3):146?62.
Ray Jackendoff and Steven Pinker. 2005. The nature of
the language faculty and its implications for the evolu-
tion of language. Cognition, 97:211?225.
X. N. C. Kam, I. Stoyneshka, L. Tornyova, J. D. Fodor,
and W. G. Sakas. 2005. Non-robustness of syntax
acquisition from n-grams: A cross-linguistic perspec-
tive. In The 18th Annual CUNY Sentence Processing
Conference, April.
Dan Klein and Christopher D. Manning. 2002. A gener-
ative constituent-context model for improved grammar
induction. In Proceedings of the 40th Annual Meeting
of the ACL.
Dan Klein and Chris Manning. 2004. Corpus-based in-
duction of syntactic structure: Models of dependency
and constituency. In Proceedings of the 42nd Annual
Meeting of the ACL.
Samuel F. Pilato and Robert C. Berwick. 1985. Re-
versible automata and induction of the english auxil-
iary system. In Proceedings of the ACL, pages 70?75.
Geoffrey K. Pullum and Barbara C. Scholz. 2002. Em-
pirical assessment of stimulus poverty arguments. The
Linguistic Review, 19(1-2):9?50.
Florencia Reali and Morten H. Christiansen. 2004.
Structure dependence in language acquisition: Uncov-
ering the statistical richness of the stimulus. In Pro-
ceedings of the 26th Annual Conference of the Cogni-
tive Science Society, Mahwah, NJ. Lawrence Erlbaum.
Noah A. Smith and Jason Eisner. 2005. Contrastive esti-
mation: Training log-linear models on unlabeled data.
In Proceedings of the 43rd Annual Meeting of the As-
sociation for Computational Linguistics, pages 354?
362, Ann Arbor, Michigan, June.
Zach Solan, David Horn, Eytan Ruppin, and Shimon
Edelman. 2005. Unsupervised learning of natural lan-
guages. Proc. Natl. Acad. Sci., 102:11629?11634.
Menno van Zaanen. 2000. ABL: Alignment-based learn-
ing. In COLING 2000 - Proceedings of the 18th Inter-
national Conference on Computational Linguistics.
Takashi Yokomori. 2003. Polynomial-time identification
of very simple grammars from positive data. Theoret-
ical Computer Science, 298(1):179?206.
132
