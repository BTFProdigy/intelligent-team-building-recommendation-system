Designing a Common POS-Tagset Framework for Indian Languages 
Sankaran Baskaran, Microsoft Research India. Bangalore. baskaran@microsoft.com 
Kalika Bali, Microsoft Research India. Bangalore. kalikab@microsoft.com 
Tanmoy Bhattacharya, Delhi University, Delhi. tanmoy1@gmail.com 
Pushpak Bhattacharyya, IIT-Bombay, Mumbai. pb@cse.iitb.ac.in 
Girish Nath Jha, Jawaharlal Nehru University, Delhi. girishj@mail.jnu.ac.in 
Rajendran S, Tamil University, Thanjavur. raj_ushush@yahoo.com 
Saravanan K, Microsoft Research India, Bangalore. v-sarak@microsoft.com 
Sobha L, AU-KBC Research Centre, Chennai. sobha@au-kbc.org 
Subbarao K V. Delhi. kvs2811@yahoo.com
 
 
Abstract 
Research in Parts-of-Speech (POS) tagset 
design for European and East Asian lan-
guages started with a mere listing of impor-
tant morphosyntactic features in one lan-
guage and has matured in later years to-
wards hierarchical tagsets, decomposable 
tags, common framework for multiple lan-
guages (EAGLES) etc. Several tagsets 
have been developed in these languages 
along with large amount of annotated data 
for furthering research. Indian Languages 
(ILs) present a contrasting picture with 
very little research in tagset design issues. 
We present our work in designing a com-
mon POS-tagset framework for ILs, which 
is the result of in-depth analysis of eight 
languages from two major families, viz. 
Indo-Aryan and Dravidian. Our framework 
follows hierarchical tagset layout similar to 
the EAGLES guidelines, but with signifi-
cant changes as needed for the ILs. 
1 Introduction 
A POS tagset design should take into consideration 
all possible morphosyntactic categories that can 
occur in a particular language or group of languag-
es (Hardie, 2004). Some effort has been made in 
the past, including the EAGLES guidelines for 
morphosyntactic annotation (Leech and Wilson, 
1996) to define guidelines for a common tagset 
across multiple languages with an aim to capture 
more detailed morphosyntactic features of these 
languages.  
However, most of the tagsets for ILs are lan-
guage specific and cannot be used for tagging data 
in other language. This disparity in tagsets hinders 
interoperability and reusability of annotated corpo-
ra. This further affects NLP research in resource 
poor ILs where non-availability of data, especially 
tagged data, remains a critical issue for researchers. 
Moreover, these tagsets capture the morphosyntac-
tic features only at a shallow level and miss out the 
richer information that is characteristic of these 
languages. 
The work presented in this paper focuses on de-
signing a common tagset framework for Indian 
languages using the EAGLES guidelines as a mod-
el. Though Indian languages belong to (mainly) 
four distinct families, the two largest being Indo-
Aryan and Dravidian, as languages that have been 
in contact for a long period of time, they share sig-
nificant similarities in morphology and syntax. 
This makes it desirable to design a common tagset 
framework that can exploit this similarity to facili-
tate the mapping of different tagsets to each other. 
This would not only allow corpora tagged with 
different tagsets for the same language to be reused 
but also achieve cross-linguistic compatibility be-
tween different language corpora. Most important-
ly, it will ensure that common categories of differ-
ent languages are annotated in the same way. 
In the next section we will discuss the impor-
tance of a common standard vis-?-vis the currently 
available tagsets for Indian languages. Section 3 
will provide the details of the design principles 
The 6th Workshop on Asian Languae Resources, 2008
89
behind the framework presented in this paper. Ex-
amples of tag categories in the common framework 
will be presented in Section 4. Section 5 will dis-
cuss the current status of the paper and future steps 
envisaged.  
2 Common Standard for POS Tagsets 
Some of the earlier POS tagsets were designed 
for English (Greene and Rubin, 1981; Garside, 
1987; Santorini, 1990) in the broader context of 
automatic parsing of English text. These tagsets 
popular even today, though designed for the same 
language differ significantly from each other mak-
ing the corpora tagged by one incompatible with 
the other. Moreover, as these are highly language 
specific tagsets they cannot be reused for any other 
language without substantial changes this requires 
standardization of POS tagsets (Hardie 2004).  
Leech and Wilson (1999) put forth a strong argu-
ment for the need to standardize POS tagset for 
reusability of annotated corpora and interopera-
bility across corpora in different languages. 
EAGLES guidelines (Leech and Wilson 1996) 
were a result of such an initiative to create stan-
dards that are common across languages that share 
morphosyntactic features. 
Several POS tagsets have been designed by a 
number of research groups working on Indian 
Languages though very few are available publicly 
(IIIT-tagset, Tamil tagset). However, as each of 
these tagsets have been motivated by specific re-
search agenda, they differ considerably in terms of 
morphosyntactic categories and features, tag defi-
nitions, level of granularity, annotation guidelines 
etc. Moreover, some of the tagsets (Tamil tagset) 
are language specific and do not scale across other 
Indian languages. This has led to a situation where 
despite strong commonalities between the lan-
guages addressed resources cannot be shared due 
to incompatibility of tasgets. This is detrimental to 
the development of language technology for Indian 
languages which already suffer from a lack of ade-
quate resources in terms of data and tools. 
In this paper, we present a common framework 
for all Indian languages where an attempt is made 
to treat equivalent morphosyntactic phenomena 
consistently across all languages. The hierarchical 
design, discussed in detail in the next section, also 
allows for a systematic method to annotate lan-
guage particular categories without disregarding 
the shared traits of the Indian languages.  
3 Design Principles 
Whilst several large projects have been concerned 
with tagset development very few have touched 
upon the design principles behind them. Leech 
(1997), Cloeren (1999) and Hardie (2004) are 
some important examples presenting universal 
principles for tagset design. 
In this section we restrict the discussion to the 
principles behind our tagset framework. Important-
ly, we diverge from some of the universal prin-
ciples but broadly follow them in a consistent way.  
Tagset structure: Flat tagsets just list down the 
categories applicable for a particular language 
without any provision for modularity or feature 
reusability. Hierarchical tagsets on the other hand 
are structured relative to one another and offer a 
well-defined mechanism for creating a common 
tagset framework for multiple languages while 
providing flexibility for customization according to 
the language and/ or application. 
Decomposability in a tagset alows different fea-
tures to be encoded in a tag by separate sub-stings. 
Decomposable tags help in better corpus analysis 
(Leech 1997) by allowing to search with an un-
derspecified search string. 
In our present framework, we have adopted the 
hierarchical layout as well as decomposable tags 
for designing the tagset. The framework will have 
three levels in the hierarchy with categories, types 
(subcategories) and features occupying the top, 
medium and the bottom layers. 
What to encode? One thumb rule for the POS 
tagging is to consider only the aspects of morpho-
syntax for annotation and not that of syntax, se-
mantics or discourse. We follow this throughout 
and focus only on the morphosyntactic aspects of 
the ILs for encoding in the framework. 
Morphology and Granularity: Indian languag-
es have complex morphology with varying degree 
of richness. Some of the languages such as those of 
the Dravidian family also display agglutination as 
an important characteristic. This entails that mor-
phological analysis is a desirable pre-process for 
the POS tagging to achieve better results in auto-
matic tagging. We encode all possible morphosyn-
tactic features in our framework assuming the exis-
The 6th Workshop on Asian Languae Resources, 2008
90
tence of morphological analysers and leave the 
choice of granularity to users. 
As pointed out by Leech (1997) some of the 
linguistically desirable distinctions may not be 
feasible computationally. Therefore, we ignore 
certain features that may not be computationally 
feasible at POS tagging level. 
Multi-words: We treat the constituents of Mul-
ti-word expressions (MWEs) like Indian Space 
Research Organization as individual words and tag 
them separately rather than giving a single tag to 
the entire word sequence. This is done because: 
Firstly, this is in accordance with the standard 
practice followed in earlier tagsets. Secondly, 
grouping MWEs into a single unit should ideally 
be handled in chunking. 
Form vs. function: We try to adopt a balance 
between form and function in a systematic and 
consistent way through deep analysis. Based on 
our analysis we propose to consider the form in 
normal circumstances and the function for words 
that are derived from other words. More details on 
this will be provided in the framework document 
(Baskaran et al2007) 
Theoretical neutrality: As Leech (1997) points 
out the annotation scheme should be theoretically 
neutral to make it clearly understandable to a larger 
group and for wider applicability. 
Diverse Language families: As mentioned ear-
lier, we consider eight languages coming from two 
major language families of India, viz. Indo-Aryan 
and Dravidian. Despite the distinct characteristics 
of these two families, it is however striking to note 
the typological parallels between them, especially 
in syntax. For example, both families follow SOV 
pattern. Also, several Indo-Aryan languages such 
as Marathi, Bangla etc. exhibit some agglutination, 
though not to the same extent of Dravidian. Given 
the strong commonalities between the two families 
we decided to use a single framework for them 
4 POS Tagset Framework for Indian lan-
guages 
The tagset framework is laid out at the following 
four levels similar to EAGLES. 
I. Obligatory attributes or values are generally 
universal for all languages and hence must be 
included in any morphosyntactic tagset. The 
major POS categories are included here. 
II. Recommended attributes or values are recog-
nised to be important sub-categories and fea-
tures common to a majority of languages.  
III. Special extensions1 
a. Generic attributes or values 
b. Language-specific attributes or values are 
the attributes that are relevant only for few lan-
guages and do not apply to most languages. 
All the tags were discussed and debated in detail 
by a group of linguists and computer scien-
tists/NLP experts for eight Indian languages viz. 
Bengali, Hindi, Kannada, Malayalam, Marathi, 
Sanskrit, Tamil and Telugu.  
Now, because of space constraints we present 
only the partial tagset framework. This is just to 
illustrate the nature of the framework and the com-
plete version as well as the rationale for different 
categories/features in the framework can be found 
in Baskaran et al (2007).2 
In the top level the following 12 categories are 
identified as universal categories for all ILs and 
hence these are obligatory for any tagset. 
 
1. [N] Nouns 7.   [PP] Postpositions  
2. [V] Verbs  8.   [DM] Demonstratives 
3. [PR] Pronouns  9.   [QT] Quantifiers 
4. [JJ] Adjectives  10. [RP] Particles  
5. [RB] Adverbs  11. [PU] Punctuations  
6. [PL] Participles  12. [RD] Residual3 
 
The partial tagset illustrated in Figure 1 high-
lights entries in recommended and optional catego-
ries for verbs and participles marked for three le-
vels.4 The features take the form of attribute-value 
pairs with values in italics and in some cases (such 
as case-markers for participles) not all the values 
are fully listed in the figure. 
5 Current Status and Future Work 
In the preceding sections we presented a common 
framework being designed for POS tagsets for In-
dian Languages. This hierarchical framework has 
                                                 
1
 We do not have many features defined under the special 
extensions and this is mainly retained for any future needs. 
2 Currently this is just the draft version and the final version 
will be made available soon 
3 For words or segments in the text occurring outside the gam-
bit of grammatical categories like foreign words, symbols,etc.   
4  These are not finalised as yet and there might be some 
changes in the final version of the framework. 
The 6th Workshop on Asian Languae Resources, 2008
91
three levels to permit flexibility and interoperabili-
ty between languages. We are currently involved in 
a thorough review of the present framework by 
using it to design the tagset for specific Indian lan-
guages. The issues that come up during this 
process will help refine and consolidate the 
framework further.  In the future, annotation guide-
lines with some recommendations for handling 
ambiguous categories will also be defined.  With 
the common framework in place, it is hoped that 
researchers working with Indian Languages would 
be able to not only reuse data annotated by each 
other but also share tools across projects and lan-
guages. 
References 
Baskaran S. et al 2007. Framework for a Common 
     Parts-of-Speech Tagset for Indic Languages. (Draft) 
    http://research.microsoft.com/~baskaran/POSTagset/ 
  
Cloeren, J. 1999. Tagsets. In Syntactic Wordclass Tagging, 
ed. Hans van Halteren, Dordrecht.: Kluwer Academic. 
Hardie, A . 2004. The Computational Analysis of Morpho-
syntactic Categories in Urdu. PhD thesis submitted to 
Lancaster University. 
Greene, B.B. and Rubin, G.M. 1981. Automatic grammati-
cal tagging of English. Providence, R.I.: Department of 
Linguistics, Brown University 
Garside, R. 1987 The CLAWS word-tagging system. In 
The Computational Analysis of English, ed. Garside, 
Leech and Sampson, London: Longman. 
Leech, G and Wilson, A. 1996. Recommendations for the 
Morphosyntactic Annotation of Corpora. EAGLES Re-
port EAG-TCWG-MAC/R. 
Leech, G. 1997. Grammatical Tagging. In Corpus Annota-
tion: Linguistic Information from Computer Text Cor-
pora, ed: Garside, Leech and McEnery, London: Long-
man  
Leech, G and Wilson, A. 1999. Standards for Tag-sets. In 
Syntactic Wordclass Tagging, ed. Hans van Halteren, 
Dordrecht: Kluwer Academic. 
Santorini, B. 1990. Part-of-speech tagging guidelines for 
the Penn Treebank Project. Technical report MS-CIS-
90-47, Department of Computer and Information 
Science, University of Pennsylvania 
IIIT-tagset. A Parts-of-Speech tagset for Indian languages. 
http://shiva.iiit.ac.in/SPSAL2007/iiit_tagset_guidelines.
pdf 
Tamil tagset. AU-KBC Parts-of-Speech tagset for Tamil. 
http://nrcfosshelpline.in/smedia/images/downloads/Tam
il_Tagset-opensource.odt 
Aspect 
 Perfect 
 Imperfect 
 Progressive 
Mood 
 Declarative 
 Subjunctative/   
        Hortative 
 Conditional 
 Imperative 
 Presumptive 
Level - 3 
Nouns 
Verbs 
Pronouns 
Adjectives 
Adverbs 
Postpositions 
Demonstratives 
Quantifiers 
Particles 
Punctuations 
Residual Participles 
Level - 1 
Type 
 Finite 
 Auxiliary 
 Infinitive 
 Non-finite 
 Nominal 
Gender 
 Masculine 
 Feminine 
 Neuter 
Number 
 Singular 
 Plural/Hon. 
 Dual 
 Honourific 
Person 
 First 
 Second 
 Third 
Tense 
 Past 
 Present 
 Future 
Negative 
Type 
 General 
 Adjectival 
 Verbal 
 Nominal 
Gender 
 As in verbs 
Number 
 Singular 
 Plural 
 Dual 
Case 
 Direct 
 Oblique 
Case-markers 
 Ergative 
 Accusative 
 etc. 
Tense 
 As in verbs 
Negative 
Level - 2 
Fig-1. Tagset framework - partial representation 
The 6th Workshop on Asian Languae Resources, 2008
92
Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 10?18,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Complex Linguistic Annotation ? No Easy Way Out!  A Case from Bangla and Hindi POS Labeling Tasks  Sandipan Dandapat1         Priyanka Biswas1       Monojit Choudhury  Kalika Bali Dublin City University       LDCIL         Microsoft Research Labs India Ireland                   CIIL-Mysore, India               Bangalore, India E-mail:  sdandapat@computing.dcu.ie, biswas.priyanka@gmail.com, monojitc@microsoft.com, kalikab@microsoft.com   Abstract 
Alternative paths to linguistic annotation, such as those utilizing games or exploiting the web users, are becoming popular in recent times owing to their very high benefit-to-cost ratios. In this paper, however, we report a case study on POS annotation for Bangla and Hindi, where we observe that reliable linguis-tic annotation requires not only expert anno-tators, but also a great deal of supervision. For our hierarchical POS annotation scheme, we find that close supervision and training is necessary at every level of the hierarchy, or equivalently, complexity of the tagset. Never-theless, an intelligent annotation tool can sig-nificantly accelerate the annotation process and increase the inter-annotator agreement for both expert and non-expert annotators. These findings lead us to believe that reliable annotation requiring deep linguistic knowl-edge (e.g., POS, chunking, Treebank, seman-tic role labeling) requires expertise and su-pervision. The focus, therefore, should be on design and development of appropriate anno-tation tools equipped with machine learning based predictive modules that can signifi-cantly boost the productivity of the annota-tors.1  1 Introduction Access to reliable annotated data is the first hur-dle encountered in most NLP tasks be it at the level of Parts-of-Speech (POS) tagging or a more complex discourse level annotation. The per-formance of the machine learning approaches which have become de rigueur for most NLP tasks are dependent on accurately annotated large datasets. Creation of such databases is, hence, a highly resource intensive task both in terms of time and expertise.                                                  1 This work has been done during the authors? internship at Microsoft Research Lab India. 
While the cost of an annotation task can be characterized by the number of man-hours and the level of expertise required, the productivity or the benefit can be measured in terms of the reliability and usability of the end-product, i.e., the annotated dataset. It is thus no surprise that considerable effort has gone into developing techniques and tools that can effectively boost the benefit-to-cost ratio of the annotation proc-ess. These include, but are not limited to: (a) exploiting the reach of the web to reduce the effort required for annotation (see, e.g., Snow et al (2008) and references therein) (b) smartly designed User Interfaces for aiding the annotators (see, e.g., Eryigit (2007); Koutsis et al (2007); Reidsma et al (2004)) (c) using supervised learning to bootstrap a small annotated dataset to automatically la-bel a larger corpus and getting it corrected by human annotators (see, e.g., Tomanek et al (2007); Wu et al (2007)) (d) Active Learning (Ringger et al 2007) where only those data-points which are directly re-levant for training are presented for manual annotation. Methods exploiting the web-users for linguis-tic annotation are particularly popular these days, presumably because of the success of the ESP-Game (von Ahn and Dabbish, 2004) and its suc-cessors in image annotation. A more recent study by (Snow et al, 2008) shows that annotated data obtained from non-expert anonymous web-users is as good as those obtained from experts. How-ever, unlike the game model, here the task is dis-tributed among non-experts through an Internet portal such as Amazon Mechanical Turk, and the users are paid for their annotations.  This might lead to an impression that the ex-pert knowledge is dispensable for NLP annota-tion tasks. However, while these approaches may work for more simple tasks like those described in (Snow et al, 2008), most NLP related annota-tion tasks such as POS tagging, chunking, se-mantic role labeling, Treebank annotation and 
10
discourse level tagging, require expertise in the relevant linguistic area. In this work, we present a case study of POS annotation in Bangla and Hindi using a hierarchical tagset, where we ob-serve that reliable linguistic annotation requires not only expert annotators, but also a great deal of supervision. A generic user interface for facili-tating the task of hierarchical word level linguis-tic annotation was designed and experiments conducted to measure the inter-annotator agreement (IA) and annotation time. It is ob-served that the tool can significantly accelerate the annotation process and increase the IA. The productivity of the annotation process is further enhanced through bootstrapping, whereby a little amount of manually annotated data is used to train an automatic POS tagger. The annotators are then asked to edit the data already tagged by the automatic tagger using an appropriate user interface.      However, the most significant observation to emerge from these experiments is that irrespec-tive of the complexity of the annotation task (see Sec. 2 for definition), language, design of the user interface and the accuracy of the automatic POS tagger used during bootstrapping, the pro-ductivity and reliability of the expert annotators working under close supervision of the dataset designer is higher than that of non-experts or those working without expert-supervision. This leads us to believe that among the four aforemen-tioned approaches for improving the benefit-to-cost ratio of the annotation tasks, solution (a) does not seem to be the right choice for involved linguistic annotations; rather, approaches (b), (c) and (d) show more promise. The paper is organized as follows: Section 2 provides a brief introduction to IL-POST ? a hi-erarchical POS Tag framework for Indian Lan-guages which is used for defining the specific annotation tasks used for the experiments. The design and features of the data annotation tool are described in Section 3. Section 4 presents the experiments conducted for POS labeling task of Bangla and Hindi while the results of these ex-periments are discussed in Section 5. The con-clusions are presented in Section 6. 2 IL-POST IL-POST is a POS-tagset framework for Indian Languages, which has been designed to cover the morphosyntactic details of Indian Languages (Baskaran et al 2008).  It  supports  a  three-level 
 Figure 1: A schematic of IL-POST framework  hierarchy of Categories, Types  and  Attributes that provides a systematic method to annotate language specific categories without disregarding the shared traits of the Indian languages. This allows the framework to offer flexibility, cross-linguistic compatibility and reusability across several languages and applications. An important consequence of its hierarchical structure and decomposable tags is that it allows users to spec-ify the morpho-syntactic information applicable at the desired granularity according to the spe-cific language and task. The complete framework supports 11 categories at the top level with 32 types at the second level to represent the main POS categories and their sub-types. Further, 18 morphological attributes or features are associ-ated with the types. The framework can thus, be used to derive a flat tagset of only 11 categories or a complex three level tagset of several thou-sand tags depending on the language and/or ap-plication. Figure 1 shows a schematic of the IL-POST framework. The current framework has been used to derive maximally specified tagsets for Bangla and Hindi (see Baskaran et al (2008) for the descriptions of the tagsets), which have been used to design the experiments presented in this paper. 3 Annotation Tool  Though a number of POS annotation tools are available none are readily suitable for hierarchi-cal tagging. The tools from other domains (like discourse annotation, for example) that use hier-archical tagsets require considerable customiza-tion for the task described here. Thus, in order to facilitate the task of word-level linguistic annota-tion for complex tagsets we developed a generic annotation tool. The annotation tool can be cus-tomized to work for any tagset that has up to 
11
 Figure 2: The basic Interface Window and Controls. See the text for details.  three levels of hierarchy and for any word level linguistic annotation task, such as Named Entity annotation and Chunk boundary labeling. In this section we describe the design of the user inter-face and other features of the annotation tool. 3.1 Interface Design Principles The annotation scheme followed for linguistic data creation is heavily dependent on the end-application the data will cater to. Moreover, an-notations are often performed by trained linguists who, in the Indian context, are either novice or intermittent users of computer. These observa-tions led us to adopt the following principles: (1) customizability of the interface to any word level annotation task; (2) mouse driven selection of tags for faster and less erroneous annotation; and (3) display of all possible choices at every stage of the task to reduce memorization overload.   3.2 Basic Interface Figure 2 depicts the basic interface of the annota-tion tool 3.2.1 Automatic Handling Apart from the surface controls, the interface also supports automatic selection facility that highlights the next unlabeled word that needs to be annotated. After loading the task (i.e., a sen-tence) it automatically highlights the first unla-beled word. Once a tag is assigned to the high-lighted word, the next unlabeled word is auto-matically selected. However, the automatic se-lection module can be stopped by selecting a par-ticular word through a mouse click. 3.2.2 Handling Hierarchical Annotation The first two levels of the IL-POST hierarchy are displayed (on a right mouse click) as a two level 
context menu. This is illustrated in Fig. 3(a). On selection of the category and type by left clicks, a window is dynamically generated for the as-signment of the attribute values, i.e., the third level of the hierarchy. A drop down box is asso-ciated with each attribute for selecting the appro-priate values. This is shown in Fig. 3(b). The default values for each of the attributes are set based on the frequency of occurrence of the val-ues in a general corpus. This further reduces the time of tag assignment. When the user clicks ?OK? on the attribute assignment window, the system automatically generates the tag as per the user?s selection and displays it in the Text-box just after the selected word.  3.3 Edit Mode Annotation While performing the annotation task, human annotators need to label every word of a sen-tence. Instead of annotating every word from scratch, we incorporate machine intelligence to automatically label every word in a sentence. Suppose that we have an automatic POS tag pre-diction module that does a fairly accurate job. In that case, the task of annotation would mean ed-iting the pre-assigned tags to the words. We hy-pothesize that such an editing based annotation task that incorporates some intelligence in the form of a tagger will be much faster than purely manual annotation, provided that the pre-assigned tags are ?sufficiently accurate?. Thus, human annotators only need to edit a particular word whenever machine assigns an incorrect tag making the process faster. We also make certain changes to the basic interface for facilitating easy editing.  In particular, when the corpus is loaded using the interface, the predicted tags are shown for each word and the first category-type is high-lighted automatically. The user can navigate 
12
                                                       (a)                                 (b)  Figure 3: Annotation at a) Category-Type level, b) Attribute level  to the next or pervious editable positions (Cate-gory-Type or Attributes) by using the Shift and the Ctrl keys respectively. The user may edit a particular pre-assigned tag by making a right mouse click and choosing from the usual context menus or attribute editing window.  The user also has the provision to choose an editable loca-tion by left mouse-click. 3.3.1 Automatic POS Tagger We developed a statistical POS tagger based on Cyclic Dependency Network (Toutanova et al, 2003) as an initial annotator for the Edit mode annotation. The tagger was trained for Bangla and Hindi on the data that was created during the first phase of annotation (i.e. annotation from scratch). We developed taggers for both Cate-gory+Type level (CT) and Category+Type+ At-tribute level (CTA). We also developed two ver-sions of the same tagger with high and low accu-racies for each level of the annotation by control-ling the amount of training data.  As we shall see in Sec. 4 and 5, the different versions of the tag-ger at various levels of the hierarchy and accu-racy will help us to understand the relation be-tween the Edit mode annotation, and the com-plexity of the tagset and the accuracy of the tag-ger used for initial annotation. The taggers were trained on 1457 sentences (approximately 20,000 words) for Bangla and 2366 sentences (approxi-mately 45,000 words) for Hindi. The taggers were tested on 256 sentences (~ 3,500 words) for Bangla and 591 sentences for Hindi, which are disjoint from the training corpus. The evaluation of a hierarchical tagset is non-trivial because the error in the machine tagged data with respect to the gold standard should take into account the level of the hierarchy where the mismatch be-tween the two takes place. Clearly, mismatch at the category or type level should incur a higher 
penalty than one at the level of the attributes. If for a word, there is a mismatch between the type assigned by the machine and that present in the gold standard, then it is assumed to be a full error (equivalent to 1 unit). On the other hand, if the type assigned is correct, then the error is 0.5 times the fraction of attributes that do not agree with the gold standard.  Table 1 reports the accuracies of the various taggers. Note that the attributes in IL-POST cor-respond to morphological features. Unlike Bangla, we do not have access to a morphologi-cal analyzer for Hindi to predict the attributes during the POS tagging at the CTA level. There-fore, the tagging accuracy in the CTA level for Hindi is lower than that of Bangla even though the amount of training data used in Hindi is much higher than that in Bangla.  4 Experiments The objective of the current work is to study the cognitive load associated with the task of linguis-tic annotation, more specifically, POS annota-tion. Cognitive load relates to the higher level of processing required by the working memory of an annotator when more learning is to be done in a shorter time. Hence, a higher cognitive load implies more time required for annotation and higher error rates. The time required for annota-tion can be readily measured by keeping track of the time taken by the annotators while tagging a sentence. The timer facility provided with the annotation tool helps us keep track of the annota-tion time. Measuring the error rate is slightly trickier as we do not have any ground truth (gold standard) against which we can measure the ac-curacy of the manual annotators. Therefore, we measure the IA, which should be high if the error rate is low. Details of the evaluation metrics are discussed in the next section. 
13
 Table 1: Tagging accuracy in % for Bangla and Hindi  The cognitive load of the annotation task is de-pendent on the complexity of the tagset, (un)availability of an appropriate annotation tool and bootstrapping facility. Therefore, in order to quantify the effect of these factors on the annota-tion task, annotation experiments are conducted under eight different settings. Four experiments are done for annotation at the Category+Type (CT) level. These are: ? CT-AT: without using annotation tool, i.e., using any standard text editor2. ? CT+AT: with the help of the basic anno-tation tool.  ? CT+ATL: with the help of the annota-tion tool in the edit mode, where the POS tagger used has low accuracy. ? CT+ATH: in the edit mode where the POS tagger used has a high accuracy. Similarly, four experiments are conducted at the Category+Type+Attribute (CTA) level, which are named following the same convention: CTA-AT, CTA+AT, CTA+ATL, CTA+ATH.  4.1 Subjects The reliability of annotation is dependent on the expertise of the annotators. In order to analyze the effect of annotator expertise, we chose sub-jects with various levels of expertise and pro-vided them different amount of training and su-pervision during the annotation experiments.  The experiments for Bangla have been con-ducted with 4 users (henceforth referred to as B1, B2, B3 and B4), all of whom are trained linguists having at least a post-graduate degree in linguis-tics. Two of them, namely B1 and B2, were pro-vided rigorous training in-house before the anno-tation task. During the training phase the tagset and the annotation guidelines were explained to them in detail. This was followed by 3-4 rounds of trial annotation tasks, during which the anno-                                                2 The experiments without the tool were also conducted using the basic interface, where the annotator has to type in the tag strings; the function of the tool here is limited to loading the corpus and the timer.  
tators were asked to annotate a set of 10-15 sen-tences and they were given feedback regarding the correctness of their annotations as judged by other human experts. For B1 and B2, the experi-ments were conducted in-house and under close supervision of the designers of the tagset and the tool, as well as a senior research linguist.   The other two annotators, B3 and B4, were provided with the data, the required annotation tools and the experimental setup, annotation guidelines and the tool usage guidelines, and the task were described in another document. Thus, the annotators were self-trained as far as the tool usage and the annotation scheme were con-cerned. They were asked to return the annotated data (and the time logs that are automatically generated during the annotation) at the end of all the experiments. This situation is similar to that of linguistic annotation using the Internet users, where the annotators are self-trained and work under no supervision. However, unlike ordinary Internet users, our subjects are trained linguists.    Experiments in Hindi were conducted with two users (henceforth referred to as H1 and H2), both of whom are trained linguists. As in the case of B1 and B2, the experiments were conducted under close supervision of a senior linguist, but H1 and H2 were self-trained in the use of the tool.   The tasks were randomized to minimize the effect of familiarity with the task as well as the tool. 4.2 Data The annotators were asked to annotate approxi-mately 2000 words for CT+AT and CTA+AT experiments and around 1000 words for CT-AT and CTA-AT experiments. The edit mode ex-periments (CT+ATL, CT+ATH, CTA+ATL and CTA+ATH) have been conducted on approxi-mately 1000 words. The amount of data was de-cided based primarily on the time constraints for the experiments. For all the experiments in a par-ticular language, 25-35% of the data was com-mon between every pair of annotators. These common sets have been used to measure the IA. However, there was no single complete set common to all the annotators. In order to meas-ure the influence of the pre-assigned labels on the judgment of the annotators, some amount of data was kept common between CTA+AT and CTA+ATL/H experiments for every annotator. 
CT CTA Language High Low High Low Bangla 81.43 66.73 76.98 64.52 Hindi 87.66 67.85 69.53 57.90 
14
  
 (a)        (b) Figure 4: Mean annotation time (in sec per word) for different users at (a) CT and (b) CTA levels  Mean Time (in Sec) Level -AT +AT +ATL +ATH CT 6.3 5.0 (20.7) 2.6 (59.4) 2.5 (59.8) CTA 15.2 10.9 (28.1) 5.2 (66.0) 4.8 (68.3) Table 2: Mean annotation time for Bangla ex-periments (%reduction in time with respect to ?AT is given within parentheses). 5 Analysis of Results In this section we report the observations from our annotation experiments and analyze those to identify trends and their underlying reasons.  5.1 Mean Annotation Time We measure the mean annotation time by com-puting the average time required to annotate a word for a sentence and then average it over all sentences for a given experiment by a specific annotator. Fig. 4 shows the mean annotation time (in seconds per word) for the different experi-ments by the different annotators. It is evident that complex annotation task (i.e., CTA level) takes much more time compared to a simple one (i.e., CT level). We also note that the tool effec-tively reduces the annotation time for most of the subjects. There is some variation in time (for ex-ample, B3) where the subject took longer to get accustomed to the annotation tool. As expected, the annotation process is accelerated by boot-strapping. In fact, the higher the accuracy of the automatic tagger, the faster is the annotation. Table 2 presents the mean time averaged over the six subjects for the 8 experiments in Bangla along with the %reduction in the time with re-spect to the case when no tool is present (i.e., ?-AT?). We observe that (a) the tool is more effec-tive for complex annotation, (b) on average, an-notation at the CTA level take twice the time of their CT level counterparts, and (c) bootstrapping   
IA (in %) Level -AT +AT +ATL +ATH CT 68.9 79.2 (15.0) 77.2 (12.2) 89.9 (30.6) CTA 51.4 72.5 (41.0) 79.3 (54.2) 83.4 (62.1) Table 3: Average IA for Bangla experiments (%increase in IA with respect to ?AT is given within parentheses).  can significantly accelerate the annotation proc-ess.  We also note that experts working under close supervision (B1 and B2) are in general faster than self-trained annotators (B3 and B4). 5.2 Inter-annotator Agreement Inter-annotator agreement (IA) is a very good indicator of the reliability of an annotated data. A high IA denotes that at least two annotators agree on the annotation and therefore, the probability that the annotation is erroneous is very small. There are various ways to quantify the IA rang-ing from a very simple percentage agreement to more complex measures such as the kappa statis-tics (Cohen, 1960; Geertzen and Bunt, 2006). For a hierarchical tagset the measurement of IA is non-trivial because the extent of disagreement should take into account the level of the hierar-chy where the mismatch between two annotators takes place. Here we use percentage agreement which takes into consideration the level of hier-archy where the disagreement between the two annotators takes place. For example, the differ-ence in IA at the category level between say, a Noun and a Nominal Modifier, versus the differ-ence at the number attribute level between singu-lar and plural. The extent of agreement for each of the tags is computed in the same way as we have evaluated our POS tagger (Sec.3.2.1). We have also measured the Cohen?s Kappa (Cohen, 1960) for the CT level experiments. Its behavior is similar to that of percentage agreement. 
15
 (a)      (b) Figure 5: Pair-wise IA (in %) at (a) CT and (b) CTA levels  Fig. 5 shows the pair-wise percentage IA for the eight experiments and Table 3 summarizes the %increase in IA due to the use of tool/bootstrapping with respect to the ?-AT? ex-periments at CT and CTA levels. We observe the following basic trends: (a) IA is consistently lower for a complex annotation (CTA) task than a simpler one (CT),  (b)  use  of  annotation  tool  helps in improvement of the IA, more so for the CTA level experiments, (c) bootstrapping helps in further improvement in IA, especially when the POS tagger used has high accuracy, and (d) IA between the trained subjects (B1 and B2) is always higher than the other pairs. IA is dependent on several factors such as the ambiguity in the tagset, inherently ambiguous cases, underspecified or ambiguously specified annotation guidelines, and errors due to careless-ness of the annotator. However, manual inspec-tion reveals that the factor which results in very low IA in ?-AT? case that the tool helps improve significantly is the typographical errors made by the annotators while using a standard text editor for annotation (e.g., NC mistyped as MC). This is more prominent in the CTA level experiments, where typing the string of attributes in the wrong order or missing out on some attributes, which are very common when annotation tool is not used, lead to a very low IA. Thus, memorization has a huge overload during the annotation proc-ess, especially for complex annotation schemes, which the annotation tool can effectively handle. In fact, more than 50% errors in CTA level are due to the above phenomenon. The analysis of other factors that lower the IA is discussed in Sec. 5.4. We would like to emphasize the fact that al-though the absolute time difference between the trained and un-trained users reduces when the  tool and/or bootstrapping is used, the IA does not decrease significantly in case of the untrained users for the complex annotation task.   
 Subjects Level Tagger B1 B2 B3 B4 Low 89.6 89.8 74.2 81.8 CT High 90.8 90.1 64.8 77.8 Low 85.4 85.1 68.2 76.1 CTA High 86.4 85.4 59.1 73.4 Table 4: Percentage agreement between the edit and the normal mode annotations (for Bangla).   5.3 Machine Influence We have seen that the IA increases in the edit mode experiments. This apparent positive result might be an unacceptable artifact of machine influence, which is to say that the annotators, whenever in confusion, might blindly agree with the pre-assigned labels.  In order to understand the influence of the pre-assigned labels on the annotators, we calculate the percentage agree-ment for a subject between the data annotated from scratch using the tool (+AT) and that in the edit mode (+ATL and +ATH). The results are summarized in Table 4.  The low agreement between the data anno-tated under the two modes for the untrained an-notators (B3 and B4) shows that there is a strong influence of pre-assigned labels for these users. Untrained annotators have lower agreement while using a high accuracy initial POS tagger compared to the case when a low accuracy POS tagger is used. This is because the high accuracy tagger assigns an erroneous label mainly for the highly ambiguous cases where a larger context is required to disambiguate. These cases are also difficult for human annotators to verify and un-trained annotators tend to miss these cases during edit mode experiments. The trained annotators show a consistent performance. Nevertheless, there is still some influence of the pre-assigned labels. 
16
5.4 Error Patterns In order to understand the reasons of disagree-ment between the annotators, we analyze the confusion matrix for different pairs of users for the various experimental scenarios. We observe that the causes of disagreement are primarily of three kinds: (1) unspecified and/or ambiguous guidelines, (2) ignorance about the guidelines, and (3) inherent ambiguities present in the sen-tences. We have found that a large number of the errors are due to type (1). For example, in attrib-ute level annotation, for every attribute two spe-cial values are ?0? (denotes ?not applicable for the particular lexical item?) and ?x? (denotes ?undecided or doubtful to the annotator?). How-ever, we find that both trained and untrained an-notators have their own distinct patterns of as-signing ?0? or ?x?. Later we made this point clearer with examples and enumerated possible cases of ?0? and ?x? tags. This was very helpful in improving the IA.   A major portion of the errors made by the un-trained users are due to type (2).  For example, it was clearly mentioned in the annotation guide-lines that if a borrowed/foreign word is written in the native script, then it has to be tagged accord-ing to its normal morpho-syntactic function in the sentence. However, if a word is typed in for-eign script, then it has to be tagged as a foreign word.  However, none of the untrained annota-tors adhered to these rules strictly.  Finally, there are instances which are inher-ently ambiguous. For example, in noun-noun compounds, a common confusion is whether the first noun is to be tagged as a nouns or an adjec-tive. These kinds of confusions are evenly dis-tributed over all the users and at every level of annotation. One important fact that we arrive at through the analysis of the confusion matrices is that the trained annotators working under close supervi-sion have few and consistent error patterns over all the experiments, whereas the untrained anno-tators exhibit no consistent and clearly definable error patterns. This is not surprising because the training helps the annotators to understand the task and the annotation scheme clearly; on the other hand, constant supervision helps clarifying doubts arising during annotation.  6 Conclusion In this paper we reported our observations for POS annotation experiments for Bangla and Hindi using the IL-POST annotation scheme un-
der various scenarios. Experiments in Tamil and Sanskrit are planned in the future. We argue that the observations from the various experiments make a case for the need of training and supervision for the annotators as well as the use of appropriate annotation interfaces and techniques such as bootstrapping. The results are indicative in nature and need to be validated with larger number of annotators. We summarize our salient contributions/conclusions: ? The generic tool described here for complex and hierarchical word level annotation is ef-fective in accelerating the annotation task as well as improving the IA. Thus, the tool helps reducing the cognitive load associated with annotation. ? Bootstrapping, whereby POS tags are pre-assigned by an automatic tagger and human annotators are required to edit the incorrect labels, further accelerates the task, at the risk of slight influence of the pre-assigned labels. ? Although with the help of the tool and tech-niques such as bootstrapping we are able to bring down the time required by untrained annotators to the level of their trained coun-terparts, the IA, and hence the reliability of the annotated data for the former is always poorer. Hence, training and supervision is very important for reliable linguistic annota-tion. We would like to emphasize the last point be-cause recently it is being argued that Internet and other game based techniques can be effectively used for gathering annotated data for NLP. While this may be suitable for certain types of annota-tions, such as word sense, lexical similarity or affect (see Snow et al (2008) for details), we argue that many mainstream linguistic annotation tasks such as POS, chunk, semantic roles and Treebank annotations call for expertise, training and close supervision. We believe that there is no easy way out to this kind of complex linguistic annotations, though smartly designed annotation interfaces and methods such as bootstrapping and active learning can significantly improve the productivity and reliability, and therefore, should be explored and exploited in future. Acknowledgements We would like to thank the annotators Dripta Piplai, Anumitra Ghosh Dastidar, Narayan Choudhary and Maansi Sharma. We would also like to thank Prof. Girish Nath Jha for his help in conducting the experiments. 
17
References  S. Baskaran, K. Bali, M. Choudhury, T. Bhattacharya, P. Bhattacharyya, G. N. Jha, S. Rajendran, K. Sa-ravanan, L. Sobha and K.V. Subbarao. 2008. A Common Parts-of-Speech Tagset Framework for Indian Languages. In Proc. of LREC 2008. J.  Cohen. 1960. A coefficient of agreement for nomi-nal scales. Educational and Psychological Meas-urement, 20 (1):37-46 J. Geertzen, and H. Bunt. 2006.  Measuring annotator agreement in a complex hierarchical dialogue act annotation scheme. In Proc. of the Workshop on Discourse and Dialogue, ACL 2006, pp. 126-133. G. Eryigit. 2007. ITU Treebank annotation tool. In Proc. of Linguistic Annotation Workshop, ACL 2007, pp. 117-120. I. Koutsis, G. Markopoulos, and G. Mikros. 2007. Episimiotis: A Multilingual Tool for Hierarchical Annotation of Texts. In Corpus Linguistics, 2007. D. Reidsma, N. Jovanovi, and D. Hofs. 2004. Design-ing annotation tools based on the properties of an-notation problems. Report, Centre for Telematics and Information Technology, 2004. E. Ringger, P. McClanahan, R. Haertel, G. Busby, M. Carmen, J. Carroll, K. Seppi, and D. Lonsdale. 2007. Active Learning for Part-of-Speech Tagging: Accelerating Corpus Annotation. In Proc. of Lin-guistic Annotation Workshop, ACL 2007, pp.  101-108. R. Snow, B. O?Connor, D. Jurafsky and A. Y. Ng. 2008. Cheap and Fast ? But is it Good? Evaluat-ing Non-Expert Annotations for Natural Language Tasks. In Proc of EMNLP-08 K. Tomanek, J. Wermter, and U. Hahn. 2007. Effi-cient annotation with the Jena ANotation Environ-ment (JANE). In Proc. of Linguistic Annotation Workshop, ACL 2007, pp.  9-16. L. von Ahn and L. Dabbish. 2004. Labeling Images with a Computer Game. In ACM Conference on Human Factors in Computing Systems, CHI 2004. Y. Wu, P. Jin, T. Guo and S. Yu. 2007. Building Chi-nese sense annotated corpus with the help of soft-ware tools. In Proc. of Linguistic Annotation Workshop, ACL 2007, pp. 125-131.  
18
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 974?979,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
POS Tagging of English-Hindi Code-Mixed Social Media Content
Yogarshi Vyas
?
University of Maryland
yogarshi@cs.umd.edu
Spandana Gella
?
Xerox Research Centre Europe
spandanagella@gmail.com
Jatin Sharma Kalika Bali Monojit Choudhury
Microsoft Research India
{jatin.sharma,kalikab,monojitc}@microsoft.com
Abstract
Code-mixing is frequently observed in
user generated content on social media,
especially from multilingual users. The
linguistic complexity of such content is
compounded by presence of spelling vari-
ations, transliteration and non-adherance
to formal grammar. We describe our
initial efforts to create a multi-level an-
notated corpus of Hindi-English code-
mixed text collated from Facebook fo-
rums, and explore language identifica-
tion, back-transliteration, normalization
and POS tagging of this data. Our re-
sults show that language identification and
transliteration for Hindi are two major
challenges that impact POS tagging accu-
racy.
1 Introduction
Code-Switching and Code-Mixing are typical
and well-studied phenomena of multilingual so-
cieties (Gumperz, 1964; Auer, 1984; Myers-
Scotton, 1993; Danet and Herring, 2007;
Cardenas-Claros and Isharyanti, 2009). Lin-
guists differentiate between the two, where
Code-Switching is juxtaposition within the same
speech exchange of passages of speech be-
longing to two different grammatical systems
or sub-systems (Gumperz, 1982), and Code-
Mixing (CM) refers to the embedding of linguis-
tic units such as phrases, words and morphemes
of one language into an utterance of another lan-
guage (Myers-Scotton, 1993). The first exam-
ple in Fig. 1 features CM where English words
are embedded in a Hindi sentence, whereas the
second example shows codeswitching. Here, we
will use CM to imply both. Work on computa-
?
This work was done during authors? internship at Mi-
crosoft Research India.
tional models of CM have been few and far be-
tween (Solorio and Liu, 2008a; Solorio and Liu,
2008b; Nguyen and Dogruoz, 2013), primarily
due to the paucity of CM data in conventional
text-corpora which makes data-intensive methods
hard to apply. Solorio and Liu (2008a) in their
work on English-Spanish CM use models built on
smaller datasets to predict valid switching points
to synthetically generate data from monolingual
corpora, and in another work (2008b) describe
parts-of-speech (POS) tagging of CM text.
CM though typically observed in spoken lan-
guage is now increasingly more common in text,
thanks to the proliferation of the Computer Me-
diated Communication channels, especially so-
cial media like Twitter and Facebook (Crys-
tal, 2001; Herring, 2003; Danet and Herring,
2007; Cardenas-Claros and Isharyanti, 2009).
Social media content is tremendously important
for studying trends, reviews, events, human-
behaviour as well as linguistic analysis, and there-
fore in recent times has spurred a lot of interest
in automatic processing of such data. Neverthe-
less, CM on social media has not been studied
from a computational aspect. Moreover, social
media content presents additional challenges due
to contractions, non-standard spellings and non-
grammatical constructions. Furthermore, for lan-
guages written in scripts other than Roman, like
Hindi, Bangla, Japanese, Chinese and Arabic, Ro-
man transliterations are typically used for repre-
senting the words (Sowmya et al., 2010). This can
prove a challenge for language identification and
segregation of the two languages.
In this paper, we describe our initial efforts to
POS tag social media content from English-Hindi
(henceforth En-Hi) bilinguals while trying to ad-
dress the challenges of CM, transliteration and
non-standard spelling, as well as lack of anno-
tated data. POS tagging is one of the fundamen-
tal pre-processing steps for NLP, and while there
974
have been works on POS tagging of social media
data (Gimpel et al., 2011; Owoputi et al., 2013)
and of CM (Solorio and Liu, 2008b), but we do
not know of any work on POS tagging of CM
text from social media that involves transliteration.
The salient contributions of this work are in for-
malizing the problem and related challenges for
processing of En-Hi social media data, creation
of an annotated dataset and some initial experi-
ments for language identification, transliteration,
normalization and POS tagging of this data.
2 Corpus Creation
For this study, we collected data from Face-
book public pages of three celebrities: Amitabh
Bachchan, Shahrukh Khan, Narendra Modi, and
the BBC Hindi news page. All these pages are
very popular with 1.8 to 15.5 million ?likes?. A to-
tal of 40 posts were manually selected from these
pages, which were published between 22nd ? 28th
October 2013. The posts having a long thread of
comments (50+) were preferred, because CM and
non-standard usage of language is more common
in the comments. We shall use the term post to re-
fer to either a post or a comment. The corpus thus
created has 6,983 posts and 113,578 words. The
data was semi-automatically cleaned and format-
ted. The user names were removed for anonymity,
but the names appearing in comments, which are
mostly of celebrities, were retained.
2.1 Annotation
There are various interesting linguistic as well as
socio-pragmatic features (e.g., user demograph-
ics, presence of sarcasm or humor, polarity) for
which this corpus could be annotated because CM
is influenced by both linguistic as well as extra-
linguistic features. However, initial attempts at
such detailed and layered annotation soon revealed
the resource-intensiveness of the task. We, thus,
scaled down the annotation to the following four
layers:
Matrix: The posts are split into contiguous
fragments of words such that each fragment has
a unique matrix language (either En or Hi). The
matrix language is defined as the language which
governs the grammatical relation between the con-
stituents of the utterance. Any other language
words that are nested into the matrix constitute the
embedded language(s). Usually, matrix language
can be assigned to clauses or sentences.
Word origin: Every word is marked for its ori-
gin or source language, En or Hi, depending on
whether it is an English or Hindi word. Words that
are of neither Hindi nor English origin are marked
as Ot or Other. Here, we assume that code-mixing
does not happen at sublexical levels, as it is un-
common in this data; Hi and En have a sim-
pler inflectional morphology and thus, sub-lexical
mixing though present (e.g., computeron has
a En root - computer and a Hi plural marker
on) is relatively less common. In languages with
richer morphology and agglutination, like Bangla
and most Dravidian languages, more frequent sub-
lexical mixing may be observed. Also note that
words are borrowed extensively between Hi and
En such that certain English words (e.g., bus,
party, vote etc) are no longer perceived as English
words by the Hindi speakers. However, here we
will not distinguish between CM and borrowing,
and such borrowed English words have also been
labeled as En words.
Normalization/Transliteration: Whenever the
word is in a transliterated form, which is often the
case for the Hi words, it is labeled with the in-
tended word in the native script (e.g., Devanagari
for Hi). If the word is in native script, but uses
a non-standard spelling, it is labeled with the cor-
rect standard spelling. We call this the spelling
normalization layer.
Parts-of-Speech (POS): Finally, each word is
also labeled with its POS. We use the Universal
POS tagset proposed by Petrov et al. (2011) which
has 12 POS tags that are applicable to both En
and Hi. The POS labels are decided based on the
function of a word in the context, rather than a
decontextualized lexical category. This is an im-
portant notion, especially for CM text, because of-
ten the original lexical category of an embedded
word is lost in the context of the matrix language,
and it plays the role of a different lexical category.
Though the Universal POS tagset does not pre-
scribe a separate tag for Named Entities, we felt
the necessity of marking three different kinds of
NEs - people, location and organization, because
almost every comment has one or more NEs and
strictly speaking word origin does not make sense
for these words.
Annotation Scheme: Fig. 1 illustrates the an-
notation scheme through two examples. Each
post is enclosed within <s></s> tags. The
matrices within a post are separated by the
<matrix></matrix> tags which take the matrix
language as an argument. Each word is anno-
975
Figure 1: Two example annotations.
tated for POS, and the language (/E or /H for En
or Hi respectively) only if it is different from the
language of the matrix. In case of non-standard
spelling in English, the correct spelling is ap-
pended as ?sol NOUN=soul?, while for the
Hindi words, the correct Devanagari translitera-
tion is appended. The NEs are marked with the
tags P (person), L (location) or O (organization)
and multiword NEs are enclosed within square
brackets ?[]?.
A random subsample of 1062 posts consisting
of 10171 words were annotated by a linguist who
is a native speaker of Hi and proficient in En. The
annotations were reviewed and corrected by two
experts linguists. During this phase, it was also
observed that a large number of comments were
very short, typically an eulogism of their favorite
celebrity and hence were not interesting from a
linguistic point of view. For our experiments, we
removed all posts that had fewer than 5 words.
The resulting corpus had 381 comments/posts and
4135 words.
2.2 CM Distribution
Most of the posts (93.17%) are in Roman script,
and only 2.93% were in Devanagari. Around 3.5%
of the posts contain words in both the scripts (typ-
ically a post in Devanagari with hashtags or urls in
Roman script), and a very small fraction of the text
(0.4% of comments/posts and 0.6% words) was in
some other script. The fraction of words present
in Roman and Devanagri scripts are 80.76% and
15.32% respectively, which shows that the De-
vanagari posts are relatively longer than the Ro-
man posts. Due to their relative rarity, the posts
containing words in Devanagari or any other script
were not considered for annotation.
In the annotated data, 1102 sentences are in a
single matrix (398 Hi, 698 En and 6 Ot) and in
45 posts there is at least one switch of matrix
(mostly between Hi and En. Thus, 4.2% of the
data shows code-switching. This is a strict defi-
nition of code-switching; if we consider a change
in matrix within a conversation thread as a code-
switch, then in this data all the threads exhibit
code-switching. However, out of the 398 com-
ments in Hi-matrix, 23.37% feature CM (i.e., they
have at least one or more non-Hi (or rather, al-
most always En) words embedded. On the other
hand, only 7.34% En-matrix comments feature
CM (again almost always with Hi). Thus, a total
of 17.2% comments/posts, which contains a quar-
ter of all the words in the annotated corpus, fea-
ture either CM or code-switching or both. We also
note that more than 40% words in the corpus are
in Hi or other Indian languages, but written in Ro-
man script; hence, they are in transliterated form.
See (Bali et al., 2014) for an in-depth discussion
on the characteristics of the CM data.
This analysis demonstrates the necessity of CM
and transliterated text processing in the context of
Indian user-generated social media content. Per-
haps, the numbers are not too different for such
content generated by the users of any other bilin-
gual and multilingual societies.
3 Models and Experiments
POS tagging of En-Hi code-mixed data requires
language identification at both word and matrix
level as well back-transliteration of the text into
976
Actual Predicted Label Recall
Label Hi En
Hi 1057 515 0.672
En 45 2023 0.978
Precision 0.959 0.797
Table 1: Confusion matrix, precision and recall of
the language identification module.
the native script. Additionaly, since we are work-
ing with content from social media, the usage of
non-standard spelling is rampant and thus, nor-
malization of text into some standard form is re-
quired. Ideally, these tasks should be performed
jointly since they are interdependent. However,
due to lack of resources, we implement a pipelined
approach in which the tasks - language identifica-
tion, text normalization and POS tagging - are per-
formed sequentially, in that order. This pipelined
approach also allows us to use various off-the-
shelf tools for solving these subtasks and quickly
create a baseline system. The baseline results can
also provide useful insight into the inherent hard-
ness of POS tagging of code-mixed social media
text. In this section, we first describe our approach
to solve these three tasks, and then discuss the ex-
periments and results.
3.1 Language identification
Langauge identification is a well studied prob-
lem (King and Abney, 2013; Carter et al., 2013;
Goldszmidt et al., 2013; Nguyen and Dogruoz,
2013), though for CM text, especially those in-
volving transliterations and orthographic varia-
tion, this is far from a solved problem (Nguyen and
Dogruoz, 2013). There was a shared task in FIRE
2013 (Saha Roy et al., 2013) on language iden-
tification and back transliteration for En mixed
with Hi, Bangla and Gujarati. Along the lines
of Gella et al (Gella et al., 2013), which was the
best performing system in this shared task, we
used the word-level logistic regression classifier
built by King and Abney (2013). This system pro-
vides a source language with a confidence prob-
ability for each word in the test set. We trained
the classifier on 3201 English words extracted
from the SMS corpus developed by Choudhury
et al (2007), while the Hindi data was obtained
by sampling 3218 Hindi transliterations out of the
En-Hi transliteration pairs developed by Sowmya
et al. (Sowmya et al., 2010). Ideally, the context of
a token is important for identifying the language.
Again, following (Gella et al., 2013) we incorpo-
rate context information through a code-switching
probability, P
s
. A higher value of P
s
implies a
lower probability of code-switching, i.e., adjacent
words are more likely to be in the same language.
Table 1 shows the token (word) level confusion
matrix for the language identification task on our
dataset. The language labels of 84.6% of the to-
kens were correctly predicted by the system. As
can be seen from the Table, the precision for pre-
dicting Hi is high, whereas that for En is low. This
is mainly due to the presence of a large number of
contracted and distorted Hi words in the dataset,
e.g. h for hai (Fig. 1), which were tagged as
En by our system because the training examples
had no contracted Hi words, but short and non-
conventional spellings were in plenty in the En
training examples as those were extracted from the
SMS corpus.
3.2 Normalization
In our dataset, if a word is identified as Hi, then
it must be back-transliterated to Devanagari script
so that any off-the-shelf Hindi POS tagger can be
used. We used the system by Gella et al. (Gella
et al., 2013) for this task, which is part rule-based
and part statistical. The system was trained on the
35000 unique transliteration pairs extracted from
Hindi song lyrics (Gupta et al., 2012). This corpus
has a reasonably wide coverage of Hindi words,
and past researchers have also shown that translit-
eration does not require a very large amount of
training data. Normalization of the En text was
not needed because the POS tagger (Owoputi et
al., 2013) could handle unnormalized text.
3.3 POS tagging
Solorio and Liu (2008b) describes a few ap-
proaches to POS-tagging of code-switched Span-
glish text, all of which primarily relies on two
monolingual taggers and certain heuristics to com-
bine the output from the two. One of the sim-
pler heuristics is based on language identification,
where the POS tag of a word is the output of the
monolingual tagger of the language in which the
word is. In this initial study, we apply this ba-
sic idea for POS tagging of CM data. We divide
the text (which is already sentence-separated) into
contiguous maximal chunks of words which are in
the same language. Then we apply a Hi POS tag-
ger to the Hi chunks, and an En POS tagger to the
En chunks.
977
Model LI HN Tagger Hi Acc. En Acc. Total Acc. Hi CA En CA Total CA
1a K K Standard 75.14 81.91 79.02 27.34 39.67 34.05
1b K K Twitter 75.14 82.66 79.02 27.34 35.74 31.91
2 K NK Twitter 65.61 81.73 74.87 17.58 33.77 26.38
3 NK NK Twitter 44.74 80.68 65.39 40.00 13.17 25.00
Table 2: POS Tagging accuracies for the different models. K=Known, NK = Not Known. LI = Language
labels, HN = Hindi normalized forms, Acc. = Token level accuracy, CA = Chunk level accuracy.
We use a CRF++ based POS tagger for Hi,
which is freely available from http://nltr.
org/snltr-software/. For En, we use the
Twitter POS tagger (Owoputi et al., 2013). It
also has an inbuilt tokenizer and can work di-
rectly on unnormalized text. This tagger has been
chosen because Facebook posts and comments
are more Twitter-like. We also use the Stanford
POS Tagger (Toutanova et al., 2003) which, un-
like the Twitter POS Tagger, has not been tuned
for Twitter-like text. These taggers use different
tagsets - the ILPOST for Hi (Sankaran et al., 2008)
and Penn-TreeBank for En (Marcus et al., 1993).
The output tags are appropriately mapped to the
smaller Universal tagset (Petrov et al., 2011).
3.4 Experiments and Results
We conducted three different experiments as fol-
lows. In the first experiment, we assume that
we know the language identities and normal-
ized/transliterated forms of the words, and only do
the POS tagging. This experiment gives us an idea
of the accuracy of POS tagging task, if normal-
ization, transliteration and language identification
could be done perfectly. We conduct this exper-
iments with two different En POS taggers: the
Stanford POS tagger which is trained on formal
English text (Model 1a) and the Twitter POS tag-
ger (Model 1b). In the next experiment (Model
2), we assume that only the language identity of
the words are known, but for Hindi we apply our
model to generate the back transliterations. For
English, we apply the Twitter POS tagger directly
because it can handle unnormalized social media
text. The third experiment (Model 3) assumes that
nothing is known. So language identifier is first
applied, and based on the language detected, we
apply the Hi translitertaion module, and Hi POS
tagger, or the En tagger. This is the most chal-
lenging and realistic setting. Note that the matrix
information is not used in any of our experiments,
though it could be potentially useful for POS tag-
ging and could be explored in future.
Table 2 gives a summary of the four models
along with the POS tagging accuracies (in %). It
shows token level as well as chunk leve accuracies
(CA), i.e., what percentage of chunks have been
correctly POS tagged. As can be seen, Hi POS
tagging has relatively low accuracies than En POS
tagging at word level for all cases. This is primar-
ily due to the errors of the transliteration module,
which in turn, is because the transliteration does
not address spelling contractions. This is also re-
flected in the drop in the accuracies for the case
where LI is unknown. The very low CA for En
for model 3 is primarily because some of the Hi
chunks are incorrectly identified as En by the lan-
guage identification module (see Table 1). How-
ever, the gradual drop of token and chunk level
accuracies from model 1 to model 3 clearly shows
the effect of gradual error accumulation from each
of the modules. We observe that Nouns were
usually confused most with Verbs and vice versa,
while the Adj were mostly confused with Nouns,
Pronouns with Determiners, and Adpositions with
Conjunctions.
4 Conclusion
This is a work in progress. We have identified
normalization and transliteration as two very chal-
lenging problems for En-Hi CM text. Joint mod-
elling of language identification, normalization,
transliteration as well as POS tagging is expected
to yield better results. We plan to continue our
work in that direction, specifically for conversa-
tional text in social media in a multilingual con-
text. CM is a common phenomenon found in all
bilingual and multilingual societies. The issue of
transliteration exist for most of the South Asian
languages as well as many other languages such as
Arabic and Greek, which use a non-Roman based
script (Gupta et al., 2014). The challenges and is-
sues identified in this study are likely to hold for
many other languages as well, which makes this a
very important and globally prevalent problem.
978
References
Peter Auer. 1984. The Pragmatics of Code-Switching:
A Sequential Approach. Cambridge University
Press.
Kalika Bali, Yogarshi Vyas, Jatin Sharma, and Monojit
Choudhury. 2014. ??i am borrowing ya mixing?? an
analysis of English-Hindi code mixing in Facebook.
In Proceedings of the First Workshop on Computa-
tional Approaches to Code Switching, EMNLP.
M?onica Stella Cardenas-Claros and Neny Isharyanti.
2009. Code-switching and code-mixing in internet
chatting: Between yes, ya, and si a case study. In
The JALT CALL Journal, 5.
Simon Carter, Wouter Weerkamp, and Manos
Tsagkias. 2013. Microblog language identification:
Overcoming the limitations of short, unedited and
idiomatic text. Language Resources and Evaluation
Journal, 47:195?215.
Monojit Choudhury, Rahul Saraf, Vijit Jain, Animesh
Mukherjee, Sudeshna Sarkar, and Anupam Basu.
2007. Investigation and modeling of the structure
of texting language. IJDAR, 10(3-4):157?174.
David Crystal. 2001. Language and the Internet.
Cambridge University Press.
Brenda Danet and Susan Herring. 2007. The Multilin-
gual Internet: Language, Culture, and Communica-
tion Online. Oxford University Press., New York.
Spandana Gella, Jatin Sharma, and Kalika Bali. 2013.
Query word labeling and back transliteration for in-
dian languages: Shared task system description. In
FIRE Working Notes.
Kevin Gimpel, N. Schneider, B. O?Connor, D. Das,
D. Mills, J. Eisenstein, M. Heilman, D. Yogatama,
J. Flanigan, and N. A. Smith. 2011. Part-of-speech
tagging for twitter: Annotation, features, and exper-
iments. In Proceedings of ACL.
Moises Goldszmidt, Marc Najork, and Stelios Papari-
zos. 2013. Boot-strapping language identifiers for
short colloquial postings. In Machine Learning and
Knowledge Discovery in Databases, volume 8189 of
Lecture Notes in Computer Science, pages 95?111.
John J. Gumperz. 1964. Hindi-punjabi code-switching
in Delhi. In Proceedings of the Ninth International
Congress of Linguistics. Mouton:The Hague.
John J. Gumperz. 1982. Discourse Strategies. Oxford
University Press.
Kanika Gupta, Monojit Choudhury, and Kalika Bali.
2012. Mining Hindi-English transliteration pairs
from online Hindi lyrics. In Proceedings of LREC.
Parth Gupta, Kalika Bali, Rafael E. Banchs, Monojit
Choudhury, and Paolo Rosso. 2014. Query ex-
pansion for mixed-script information retrieval. In
Proc. of SIGIR, pages 677?686. ACM Association
for Computing Machinery.
Susan Herring, editor. 2003. Media and Language
Change. Special issue of Journal of Historical Prag-
matics 4:1.
Ben King and Steven Abney. 2013. Labeling the lan-
guages of words in mixed-language documents us-
ing weakly supervised methods. In Proceedings of
NAACL-HLT, pages 1110?1119.
Mitchell P Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of english: The penn treebank. Compu-
tational linguistics, 19(2):313?330.
Carol Myers-Scotton. 1993. Dueling Languages:
Grammatical Structure in Code-Switching. Clare-
don, Oxford.
Dong Nguyen and A. Seza Dogruoz. 2013. Word
level language identification in online multilingual
communication. In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Language
Processing, pages 857?862.
Olutobi Owoputi, Brendan OConnor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A.
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Proceedings of NAACL.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2011.
A universal part-of-speech tagset. arXiv preprint
arXiv:1104.2086.
Rishiraj Saha Roy, Monojit Choudhury, Prasenjit Ma-
jumder, and Komal Agarwal. 2013. Overview and
datasets of fire 2013 track on transliterated search.
In FIRE Working Notes.
Bhaskaran Sankaran, Kalika Bali, Monojit Choudhury,
Tanmoy Bhattacharya, Pushpak Bhattacharyya,
Girish Nath Jha, S. Rajendran, K. Saravanan,
L. Sobha, and K. V. Subbarao. 2008. A com-
mon parts-of-speech tagset framework for indian
languages. In Proceedings of LREC.
Thamar Solorio and Yang Liu. 2008a. Learning to
predict code-switching points. In Proceedings of the
Empirical Methods in natural Language Processing.
Thamar Solorio and Yang Liu. 2008b. Parts-of-speech
tagging for English-Spanish code-switched text. In
Proceedings of the Empirical Methods in natural
Language Processing.
V. B. Sowmya, Monojit Choudhury, Kalika Bali,
Tirthankar Dasgupta, and Anupam Basu. 2010. Re-
source creation for training and testing of translitera-
tion systems for indian languages. In Proceedings of
the Language Resource and Evaluation Conference
(LREC).
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of HLT-NAACL.
979
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1713?1722,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Crowd Prefers the Middle Path: A New IAA Metric for Crowdsourcing
Reveals Turker Biases in Query Segmentation
Rohan Ramanath?
R. V. College of Engineering
Bangalore, India
ronramanath@gmail.com
Monojit Choudhury
Microsoft Research Lab India
Bangalore, India
monojitc@microsoft.com
Kalika Bali
Microsoft Research Lab India
Bangalore, India
kalikab@microsoft.com
Rishiraj Saha Roy?
Indian Institute of Technology Kharagpur
Kharagpur, India
rishiraj@cse.iitkgp.ernet.in
Abstract
Query segmentation, like text chunking,
is the first step towards query understand-
ing. In this study, we explore the effec-
tiveness of crowdsourcing for this task.
Through carefully designed control ex-
periments and Inter Annotator Agreement
metrics for analysis of experimental data,
we show that crowdsourcing may not be a
suitable approach for query segmentation
because the crowd seems to have a very
strong bias towards dividing the query into
roughly equal (often only two) parts. Sim-
ilarly, in the case of hierarchical or nested
segmentation, turkers have a strong prefer-
ence towards balanced binary trees.
1 Introduction
Text chunking of Natural Language (NL) sentences
is a well studied problem that is an essential pre-
processing step for many NLP applications (Ab-
ney, 1991; Abney, 1995). In the context of Web
search queries, query segmentation is similarly the
first step towards analysis and understanding of
queries (Hagen et al, 2011). The task in both the
cases is to divide the sentence or the query into
contiguous segments or chunks of words such that
the words from a segment are related to each other
more strongly than words from different segments
(Bendersky et al, 2009). It is typically assumed
that the segments are structurally and semantically
coherent and, therefore, the information contained
in them can be processed holistically.
?The work was done during author?s internship at Mi-
crosoft Research Lab India.
? This author was supported by Microsoft Corporation
and Microsoft Research India under the Microsoft Research
India PhD Fellowship Award.
f Pipe representation Boundary var.
4 apply | first aid course | on line 1 0 0 1 0
3 apply first aid course | on line 0 0 0 1 0
2 apply first aid | course on line 0 0 1 0 0
1 apply | first aid | course | on line 1 0 1 1 0
Table 1: Example of flat segmentation by Turkers.
f is the frequency of annotations; segment bound-
aries are represented by |.
f Bracket representation Boundary var.
4 ((apply first) ((aid course) (on line))) 0 2 0 1 0
2 (((apply (first aid)) course) (on line)) 1 0 2 3 0
2 ((apply ((first aid) course)) (on line)) 2 0 1 3 0
1 (apply (((first aid) course) (on line))) 3 0 1 2 0
1 ((apply (first aid)) (course (on line))) 1 0 2 1 0
Table 2: Example of nested segmentation by Turk-
ers. f is the frequency of annotations.
A majority of work on query segmentation re-
lies on manually segmented queries by human ex-
perts for training and evaluation of segmentation
algorithms. These are typically small datasets and
even with detailed annotation guidelines and/or
close supervision, low Inter Annotator Agreement
(IAA) remains an issue. For instance, Table 1 il-
lustrates the variation in flat segmentation by 10
annotators. This confusion is mainly because the
definition of a segment in a query is ambiguous
and of an unspecified granularity. This is fur-
ther compounded by the fact that other than eas-
ily recognizable and agreed upon segments such as
Named Entities or Multi-Word Expressions, there
is no established notion of linguistic grouping such
as phrases and clauses in a query.
Although there is little work on the use of
crowdsourcing for query segmentation (Hagen et
al., 2011; Hagen et al, 2012), the idea that the
1713
crowd could be a potential (and cheaper) source
for reliable segmentation seems a reasonable as-
sumption. The need for larger datasets makes this
an attractive proposition. Also, a larger number
of annotations could be appropriately distilled to
obtain better quality segmentations.
In this paper we explore crowdsourcing as an
option for query segmentation through experi-
ments designed using Amazon Mechanical Turk
(AMT)1. We compare the results against gold
datasets created by trained annotators. We ad-
dress the issues pertaining to disagreements due to
both ambiguity and granularity and attempt to ob-
jectively quantify their role in IAA. To this end,
we also conduct similar annotation experiments
for NL sentences and randomly generated queries.
While queries are not as structured as NL sen-
tences they are not simply a set of random words.
Thus, it is necessary to compare query segmenta-
tion to the u?ber-structure of NL sentences as well
as the unter-structure of random n-grams. This has
important implications for understanding any in-
herent biases annotators may have as a result of
the apparent lack of structure of the queries.
To quantify the effect of granularity on segmen-
tation, we also ask annotators to provide hierar-
chical or nested segmentations for real and ran-
dom queries, as well as sentences. Following
Abney?s (1992) proposal for hierarchical chunk-
ing of NL, we ask the annotators to group ex-
actly two words or segments at a time to recur-
sively form bigger segments. The concept is illus-
trated in Fig. 1. Table 2 shows annotations from
10 Turkers. It is important to constrain the join-
ing of exactly two segments or words at a time
to avoid the issue of fuzziness in granularity. We
shall refer to this style of annotation as Nested
segmentation, whereas the non-hierarchical non-
constrained chunking will be referred to as Flat
segmentation.
Through statistical analysis of the experimen-
tal data we show that crowdsourcing may not be
the best practice for query segmentation, not only
because of ambiguity and granularity issues, but
because there exist very strong biases amongst an-
notators to divide a query into two roughly equal
parts that result in misleadingly high agreements.
As a part of our analysis framework, we introduce
a new IAA metric for comparison across flat and
nested segmentations. This versatile metric can be
1https://www.mturk.com/mturk/welcome
3
2
1
apply 0
first aid
course
0
on line
Figure 1: Nested Segmentation: Illustration.
readily adapted for measuring IAA for other lin-
guistic annotation tasks, especially when done us-
ing crowdsourcing.
The rest of the paper is organized as follows.
Sec 2 provides a brief overview of related work.
Sec 3 describes the experiment design and proce-
dure. In Sec 4, we introduce a new metric for IAA,
that could be uniformly applied across flat and
nested segmentations. Results of the annotation
experiments are reported in Sec 5. In Sec 6, we an-
alyze the possible statistical and linguistic biases
in annotation. Sec 7 concludes the paper by sum-
marizing the work and discussing future research
directions. All the annotated datasets used in this
research are freely available for non-commercial
research purposes2.
2 Related Work
Query segmentation was introduced by Risvik et.
al. (2003) as a possible means to improve Informa-
tion Retrieval. Since then there has been a signif-
icant amount of research exploring various algo-
rithms for this task and its use in IR (see Hagen et.
al. (2011) for a survey). Most of the research and
evaluation considers query segmentation as a pro-
cess analogous to identification of phrases within
a query which when put within double-quotes (im-
plying exact matching of the quoted phrase in the
document) leads to better IR performance. How-
ever, this is a very restricted view of the process
and does not take into account the full potential of
query segmentation.
A more generic notion of segments leads to di-
verse and ambiguous definitions, making its eval-
uation a hard problem (see Saha Roy et. al. (2012)
for a discussion on issues with evaluation). Most
automatic segmentation techniques (Bergsma and
Wang, 2007; Tan and Peng, 2008; Zhang et al,
2Related datasets and supplementary material can be ac-
cessed from http://bit.ly/161Gkk9 or can be ob-
tained by directly emailing the authors.
1714
2009; Brenes et al, 2010; Hagen et al, 2011; Li et
al., 2011) have so far been evaluated only against
a small set of human-annotated queries (Bergsma
and Wang, 2007). The reported low IAA for such
datasets casts serious doubts on the reliability of
annotation and the performance of the algorithms
evaluated on them (Hagen et al, 2011; Saha Roy
et al, 2012).
To address the problem of data scarcity, Ha-
gen et. al. (2011) have created larger annotated
datasets through crowdsourcing3. However, in
their approach the crowd is provided with a few
(four) possible segmentations of a query to choose
from (known through a personal communication
with a authors). Thus, it presupposes an automatic
process that can generate the correct segmentation
of a query within top few options. It is far from
obvious how to generate these initial segmenta-
tions in a reliable manner. This may also result
in an over-optimistic IAA. An ideal segmentation
should be based on the annotators? own interpreta-
tion of the query. Nevertheless, if large scale data
has to be procured, crowdsourcing seems to be the
only efficient and effective model for this task, and
has been proven to be so for other IR and linguistic
annotations; see Carvalho et al (2011) for exam-
ples of crowdsourcing for IR resources and (Snow
et al, 2008; Callison-Burch, 2009) for language
resources.
In the context of NL text, segmentation has
been traditionally referred to as chunking and is
a well-studied problem. Abney (1991; 1992;
1995) defines a chunk as a sub-tree within a
syntactic phrase structure tree corresponding to
Noun, Prepositional, Adjectival, Adverbial and
Verb Phrases. Similarly, Bharati et al(1995) de-
fines it as Noun Group and Verb Group based only
on local surface information. However, cognitive
and annotation experiments for chunking of En-
glish (Abney, 1992) and other language text (Bali
et al, 2009) have shown that native speakers agree
on major clause and phrase boundaries, but may
not do so on more fine-grained chunks. One im-
portant implication of this is that annotators are
expected to agree more on the higher level bound-
aries for nested segmentation than the lower ones.
We note that hierarchical query segmentation was
proposed for the first time by Huang et al (2010),
where the authors recursively split a query (or its
fragment) into exactly two parts and evaluate the
3http://www.webis.de/research/corpora
final output against human annotations.
3 Experiments
The annotation experiments have been designed to
systematically study the various aspects of query
segmentation. In order to verify the effective-
ness and reliability of crowdsourcing, we designed
an AMT experiment for flat segmentation of Web
search queries. As a baseline, we would like to
compare these annotations with those from hu-
man experts trained for the task. We shall refer
to this baseline as the Gold annotation set. Since
we believe that the issue of granularity could be
the prime reason for previously reported low IAA
for segmentation, we also designed AMT-based
nested segmentation experiments for the same set
of queries, and obtained the corresponding gold
annotations.
Finally, to estimate the role of ambiguity inher-
ent in the structure of Web search queries on IAA,
we conducted two more control experiments, both
through crowdsourcing. First, flat and nested seg-
mentation of well-formed English, i.e., NL sen-
tences of similar length distribution; and second,
flat and nested segmentation of randomly gener-
ated queries. Higher IAA for NL sentences would
lead us to conclude that ambiguity and lack of
structure in queries is the main reason for low
agreements. On the other hand high or comparable
IAA for random queries would mean that annota-
tions have strong biases.
Thus, we have the following four pairs of anno-
tation experiments: flat and nested segmentation
of queries from crowdsourcing, corresponding flat
and nested gold annotations, flat and nested seg-
mentation of English sentences from crowdsourc-
ing, and flat and nested segmentations for ran-
domly generated queries through crowdsourcing.
3.1 Dataset
For our experiments, we need a set of Web search
queries and well-formed English sentences. Fur-
thermore, for generating the random queries, we
will use search query logs to learn n-gram mod-
els. In particular, we use the following datasets:
Q500, QG500: Saha Roy et al (2012) re-
leased a dataset of 500 queries, 5 to 8 words long,
for evaluation of various segmentation algorithms.
This dataset has flat segmentations from three an-
notators obtained under controlled experimental
settings, and can be considered as Gold annota-
1715
Figure 2: Length distribution of datasets.
tions. Hence, we select this set for our experiments
as well. We procured the corresponding nested
segmentation for these queries from two human
experts, who are regular search engine users, be-
tween 20 and 30 years old, and familiar with var-
ious linguistic annotation tasks. They annotated
the data under supervision. They were trained and
paid for the task. We shall refer to the set of flat
and nested gold annotations as QG500, whereas
Q500 will be reserved for AMT experiments.
Q700: Since 500 queries may not be enough
for reliable conclusion and since the queries may
not have been chosen specifically for the purpose
of annotation experiments, we expanded the set
with another 700 queries sampled from a slice of
the query logs of Bing Australia4 containing 16.7
million queries issued over a period of one month
(May 2010). We picked, uniformly at random,
queries that are 4 to 8 words long, have only En-
glish letters and numerals, and a high click entropy
because ?a query with a larger click entropy value
is more likely to be an informational or ambiguous
query? (Dou et al, 2008). Q500 consists of tail-
ish queries with frequency between 5 and 15 that
have at least one multiword named entity; but un-
like the case of Q700, click-entropy was not con-
sidered during sampling. As we shall see, this dif-
ference is clearly reflected in the results.
S300: We randomly selected 300 English sen-
tences from a collection of full texts of public do-
main books5 that were 5 to 15 words long, and
checked them for well-formedness. This set will
be referred to as S300.
QRand: Instead of generating search queries
by throwing in words randomly, we thought it
will be more interesting to explore annotation of
4http://www.bing.com/?cc=au
5http://www.gutenberg.org
Parameter Flat Details Nested Details
Time needed: actual (allotted) 49 sec (10 min) 1 min 52 sec (15 min)
Reward per HIT $0.02 $0.06
Instruction video duration 26 sec 1 min 40 sec
Turker qualification Completion rate >100 tasks
Turker approval rate Acceptance rate >60 %
Turker location United States of America
Table 3: Specifics of the HITs for AMT.
queries generated using n-gram models for n =
1, 2, 3. We estimated the models from the Bing
Australia log of 16.7 million queries. We gener-
ated 250 queries each of desired length distribu-
tion using the 1, 2 and 3-gram models. We shall
refer to these as U250, B250, T250 (for Uni, Bi
and Trigram) respectively, and the whole dataset
as QRand. Fig. 2 shows the query and sentence
length distribution for the various sets.
3.2 Crowdsourcing Experiments
We used AMT to get our annotations through
crowdsourcing. Pilot experiments were carried out
to test the instruction set and examples presented.
Based on the feedback, the precise instructions for
the final experiments were designed.
Two separate AMT Human Intelligence Tasks
(HITs) were designed for flat and nested query
segmentation. Also, the experiments for queries
(Q500+Q700) were conducted separately from
S300 and QRand. Thus, we had six HITs in
all. The concept of flat and nested segmentation
was introduced to the Turkers with the help of ex-
amples presented in two short videos6. When in
doubt regarding the meaning of a query, the Turk-
ers were advised to issue the query on a search
engine of their choice and find out its possible
interpretation(s). Note that we intentionally kept
definitions of flat and nested segmentation fuzzy
because (a) it would require very long instruction
manuals to cover all possible cases and (b) Turkers
do not tend to read verbose and complex instruc-
tions. Table 3 summarizes other specifics of HITs.
Honey pots or trap questions whose answers are
known a priori are often included in a HIT to iden-
tify turkers who are unable to solve the task ap-
propriately leading to incorrect annotations. How-
ever, this trick cannot be employed in our case be-
cause there is no notion of an absolutely correct
segmentation. We observe that even with unam-
biguous queries, even expert annotators may dis-
6Flat: http://youtu.be/eMeLjJIvIh0, Nested:
http://youtu.be/xE3rwANbFvU
1716
agree on some of the segment boundaries. Hence,
we decided to include annotations from all the
turkers, except for those that were syntactically ill-
formed (e.g., non-binary nested segmentation).
4 Inter Annotator Agreement
Inter Annotator Agreement is the only way to
judge the reliability of annotated data in absence
of an end application. Therefore, before we can
venture into analysis of the experimental data, we
need to formalize the notion of IAA for flat and
nested queries. The task is non-trivial for two
reasons. First, traditional IAA measures are de-
fined for a fixed set of annotators. However, for
crowdsourcing based annotations, different anno-
tators might have annotated different parts of the
dataset. For instance, we observed that a total
of 128 turkers have provided the flat annotations
for Q700, when we had only asked for 10 anno-
tations per query. Thus, on average, a turker has
annotated only 7.81% of the 700 queries. In fact,
we found that 31 turkers had annotated less than
5 queries. Hence, measures such as Cohen?s ?
(1960) cannot be directly applied in this context
because for crowdsourced annotations, we cannot
meaningfully compute annotator-specific distribu-
tion of the labels and biases.
Second, most of the standard annotation metrics
do not generalize for flat segmentation and trees.
Artstein and Poesio (2008) provides a comprehen-
sive survey of the IAA metrics and their usage in
NLP. They note that all the metrics assume that
a fixed set of labels are used for items. There-
fore, it is far from obvious how to compare chunk-
ing or segmentation that covers the whole text or
that might have overlapping units as in the case of
nested segmentation. Furthermore, we would like
to compare the reliability of flat and nested seg-
mentation, and therefore, ideally we would like to
have an IAA metric that can be meaningfully ap-
plied to both of these cases.
After considering various measures, we decided
to appropriately generalize one of the most versa-
tile and effective IAA metrics proposed till date,
the Kripendorff?s ? (2004). To be consistent with
prior work, we will stick to the notation used
in Artstein and Poesio (2008) and redefine the
? in the context of flat and nested segmentation.
Note that though the notations introduced here will
be from the perspective of queries, it is equally
applicable to sentences and the generalization is
straightforward.
4.1 Notations and Definitions
Let Q be the set of all queries with cardinality q.
A query q ? Q can be represented as a sequence of
|q| words: w1w2 . . . w|q|. We introduce |q?1| ran-
dom variables, b1, b2, . . . b|q|?1, such that bi rep-
resents the boundary between the words wi and
wi+1. A flat or nested segmentation of q, repre-
sented by qj , j varying from 1 to total number of
annotations c, is a particular instantiation of these
boundary variables as described below.
Definition. A flat segmentation, qj can be
uniquely defined by a binary assignment of the
boundary variables bj,i, where bj,i = 1 iff wi and
wi+1 belong to two different flat segments. Oth-
erwise, bj,i = 0. Thus, q has 2|q|?1 possible flat
segmentations.
Definition. A nested segmentation qj can also
be uniquely defined by assigning non-negative in-
tegers to the boundary variables such that bj,i = 0
iff words wi and wi+1 form an atomic segment
(i.e., they are grouped together), else bj,i = 1 +
max(lefti, righti), where lefti and righti are
the heights of the largest subtrees ending at wi and
beginning at wi+1 respectively.
This numbering scheme for nested segmenta-
tion can be understood through Fig. 1. Every in-
ternal node of the binary tree corresponding to the
nested segmentation is numbered according to its
height. The lowest internal nodes, both of whose
children are query words, are assigned a value of
0. Other internal nodes get a value of one greater
than the height of its higher child. Since every in-
ternal node corresponds to a boundary, we assign
the height of the node to the corresponding bound-
aries. The number of unique nested segmentations
of a query of length |q| is its corresponding Cata-
lan number7.
Boundary variables for flat and nested segmen-
tation are illustrated with an example of each kind
in Tables 1 and 2 (last column).
4.2 Krippendorff ?s ? for Segmentation
Krippendorff ?s ? (Krippendorff, 2004) is an ex-
tremely versatile agreement coefficient, which is
based on the assumption that the expected agree-
ment is calculated by looking at the overall distri-
bution of judgments without regard to which anno-
tator produced them (Artstein and Poesio, 2008).
7http://goo.gl/vKQvK
1717
Hence, it is appropriate for crowdsourced annota-
tion, where the judgments come from a large num-
ber of unrelated annotators. Moreover, it allows
for different magnitudes of disagreement, which
is a useful feature as we might want to differen-
tially penalize disagreements at various levels of
the tree for nested segmentation.
? is defined as
? = 1? DoDe
= 1? s
2
within
s2total
(1)
where Do and De are, respectively, the observed
and expected disagreements that are measured by
s2within ? the variance within the annotation of an
item and s2total ? variance across annotations of
all items. We adapt the equations presented in
pp.565-566 of Artstein and Poesio (2008) for mea-
suring these quantities for queries:
s2within =
1
2qc(c? 1)
?
q?Q
c?
m=1
c?
n=1
d(qm, qn)
(2)
s2total =
1
2qc(qc? 1)
?
q?Q
c?
m=1
?
q??Q
c?
n=1
d(qm, q?n)
(3)
where, d(qm, q?n) is a distance metric for the agree-
ment between annotations qm and q?n.
We define two different distance metrics d1 and
d2 that are applicable to flat and nested segmenta-
tion. We shall first define these metrics for com-
paring queries with equal length (i.e., |q| = |q?|):
d1(qm, q?n) =
1
|q| ? 1
|q|?1?
i=1
|bm,i ? b?n,i| (4)
d2(qm, q?n) =
1
|q| ? 1
|q|?1?
i=1
|b2m,i ? (b?n,i)2| (5)
While d1 penalizes all disagreements equally, d2
penalizes disagreements higher up the tree more.
d2 might be a desirable metric for nested seg-
mentation, because research on sentence chunk-
ing shows that annotators agree more on clause or
major phrase boundaries, even though they may
not always agree on intra-clausal or intra-phrasal
boundaries (Bali et al, 2009). Note that for flat
segmentation, d1 and d2 are identical, and hence
we will denote them as d.
We propose the following extension to these
metrics for queries of unequal lengths. Without
loss of generality, let us assume that |q| < |q?|. k
is 1 or 2; r = |q?| ? |q|+ 1.
dk(qm, q?n) =
1
r(|q| ? 1)
r?1?
a=0
|q|?1?
i=1
|bkm,i ? (b?n,i+a)k| (6)
4.3 IAA under Random Bias Assumption
Krippendorff?s ? uses the cross-item variance as
an estimate of chance agreement, which is reli-
able in general. However, this might result in mis-
leadingly low values of IAA, especially when the
items in the set are indeed expected to have sim-
ilar annotations. To resolve this, we also com-
pute the chance agreement under a random bias
model. The random model assumes that all the
structural annotations of q are equiprobable. For
flat segmentation, it boils down to the fact that
all the 2|q|?1 annotations are equally likely, which
is equivalent to the assumption that any boundary
variable bi has 0.5 probability of being 0 and 0.5
for 1.
Analytical computation of the expected proba-
bility distributions of d1(qm, qn) and d2(qm, qn)
is harder for nested segmentation. Therefore, we
programmatically generate all possible trees for q,
which is again dependent only on |q| and com-
pute d1 and d2 between all pairs of trees, from
which the expected distributions can be readily
estimated. Let us denote this expected cumula-
tive probability distribution for flat segmentation
as Pd(x; |q|) = the probability that for a pair
of randomly chosen flat segmentations of q, qm
and qn, d(qm, qn) ? x. Likewise, let Pd1(x; |q|)
and Pd2(x; |q|) be the respective probabilities that
for any two nested segmentations qm and qn of
q, the following holds: d1(qm, qn) ? x and
d2(qm, qn) ? x.
We define the IAA under random bias model as
(k is 1, 2 or null):
S = 1qc2
?
q?Q
c?
m=1
c?
n=1
Pdk(dk(qm, qn); |q|) (7)
Thus, S is the expected probability of observing a
similar or worse agreement by random chance, av-
eraged over all pairs of annotations for all queries,
and not a chance corrected IAA metric such as
?. Thus, S = 1 implies that the observed agree-
ment is almost always better than that by random
chance and S = 0.5 and 0 respectively imply that
the observed agreement is as good as and almost
always worse than that by random chance. We
1718
Dataset Flat Nested
d1 d1 d2
Q700 0.21(0.59) 0.21(0.89) 0.16(0.68)
Q500 0.22(0.62) 0.15(0.70) 0.15(0.44)
QG500 0.61(0.88) 0.66(0.88) 0.67(0.80)
S300 0.27(0.74) 0.18(0.94) 0.14(0.75)
U250 0.23(0.89) 0.42(0.90) 0.30(0.78)
B250 0.22(0.86) 0.34(0.88) 0.22(0.71)
T250 0.20(0.86) 0.44(0.89) 0.34(0.76)
Table 4: Agreement Statistics: ?(S).
also note that a high value of S and low value
of ? indicate that though the annotators agree on
the judgment of individual items, they also tend to
agree on judgments of two different items, which
in turn, could be due to strong annotator biases or
due to lack of variability of the dataset.
In the supplementary material, computations of
? and S have been explained in further details
through worked out examples. Tables for the ex-
pected distributions of d, d1 and d2 under the ran-
dom annotation assumption are also available.
5 Results
Table 4 reports the values of ? and S for flat
and nested segmentation on the various datasets.
For nested segmentation, the values were com-
puted for two different distance metrics d1 and
d2. As expected, the highest value of ? for both
flat and nested segmentation is observed for gold
annotations. An ? > 0.6 indicates quite good
IAA, and thus, reliable annotations. Higher ? for
nested segmentation QG500 than flat further vali-
dates our initial postulate that nested segmentation
may reduce disagreement from granularity issues
inherent in the definition of flat segmentation.
Opposite trends are observed for Q700, Q500
and S300, where ? for flat is the highest, followed
by that for nested using d1, and then d2. More-
over, except for flat segmentation of sentences, ?
lies between 0.14 and 0.22, which is quite low.
This clearly shows that segmentation, either flat
or nested, cannot be reliably procured through
crowdsourcing. Lower ? for d2 than d1 further
indicates that annotators disagree more for higher
levels of the trees, contrary to what we had ex-
pected. However, nearly equal IAA for sentences
and queries implies that low agreement may not be
an outcome of inherent ambiguity in the structure
of queries. Slightly higher ? for flat segmentation
and a much higher ? for nested segmentation of
QRand reinforce the fact that low IAA is not due
to a lack of structure in queries.
It is interesting to note that ? for nested segmen-
tation of S300 and all segmentations of QRand
are low or medium despite the fact that S is very
high in all these cases. Thus, it is clear that an-
notators have a strong bias towards certain struc-
tures across queries. In the next section, we will
analyze some of these biases. We also computed
the IAA between QG500 and Q500, and found
? = 0.27. This is much lower than ? for QG500,
though slightly higher than that for Q500. We did
not observe any significant variation in agreement
with respect to the length of the queries.
6 Biases in Annotation
The IAA statistics clearly show that there are cer-
tain strong biases in both flat and nested query
segmentation, especially those obtained through
crowdsourcing. To identify these biases, we went
through the annotations and came up with possi-
ble hypotheses, which we tried to verify through
statistical analysis of the data. Here, we report the
most prominent biases that were thus discovered.
Bias 1: During flat segmentation, annotators pre-
fer dividing the query into two segments of roughly
equal length.
As discussed earlier, one of the major problems
of flat segmentation is the fuzziness in granularity.
In our experiments, we intentionally left the de-
cision of whether to go for fine or coarse-grained
segmentation to the annotator. However, it is sur-
prising to observe that annotators typically divide
the query into two segments (see Fig. 3, plots A1
and A2), and at times three, but hardly ever more
than three. This bias is observed across queries,
sentences and random queries, where the percent-
age of annotations with 2 or 3 segments are greater
than 83%, 91% and 96% respectively. This bias
is most strongly visible for QRand because the
lack of syntactic or semantic cohesion between the
words provides no clue for segmentation.
Furthermore, we observe that typically seg-
ments tend to be of equal length. For this, we com-
puted standard deviations (sd) of segment lengths
for all annotations having 2 or 3 segments; the dis-
tribution of sd is shown in Fig. 3, plots B1 and B2.
We observe that for all datasets, sd lies mainly be-
tween 0.5 and 1 (for perspective, consider a query
1719
Figure 3: Analysis of annotation biases: A1, A2 ? number of segments per flat segmentation vs. length;
B1, B2 ? standard deviation of segment length for flat segmentation; C1, C2 ? distribution of the tree
heights in nested segmentation.
Length Expected Q500 QG500 Q700 S300 QRand
5 2.57 2.00 2.02 2.08 2.02 2.01
6 3.24 2.26 2.23 2.23 2.24 2.02
7 3.88 2.70 2.71 2.67 2.55 2.62
8 4.47 2.89 2.68 2.72 2.72 2.35
Table 5: Average height for nested segmentation.
with 7 words; with two segments of length 3 and
4 the sd is 0.5, and for 2 and 5, the sd is 1.5), im-
plying that segments are roughly of equal length.
It is likely that due to this bias, the S or observed
agreement is moderately high for queries and very
high for sentences, but then it also leads to high
agreement across different queries and sentences
(i.e., high s2total) especially when they are of equal
length, which in turn brings down the value of ? ?
the true agreement after bias correction.
Bias 2: During nested segmentation, annotators
prefer balanced binary trees.
Quite analogous to bias 1, for nested segmen-
tation we observe that annotators tend to prefer
more balanced binary trees. Fig. 3 plots C1 and C2
show the distribution of the tree heights for various
cases and Table 5 reports the corresponding aver-
age height of the trees for queries and sentences
of various lengths and the the expected value of
the height if all trees were equally likely. The ob-
served heights are much lower than the expected
values clearly implying the preference of the an-
notators for more balanced trees.
Thus, the crowd seems to choose the middle
path, avoiding extremes and hence may not be a
reliable source of annotation for query segmen-
tation. It can be argued that similar biases are
also observed for gold annotations, and therefore,
probably it is the inherent structure of the queries
and sentences that lead to such biased distribution
of segmentation patterns. However, note that ? for
QG500 is much higher than all other cases, which
shows that the true agreement between gold anno-
tators is immune to such biases or skewed distri-
butions in the datasets. Furthermore, high values
of ? for QRand despite the very strong biases in
annotation shows that there perhaps is very little
choice that the annotators have while segmenting
randomly generated queries. On the other hand,
the textual coherence of the real queries and sen-
tences provide many different choices for segmen-
tation and the Turker typically gets carried away
by these biases, leading to low ?.
Bias 3: Phrase structure drives segmentation only
when reconcilable with Bias 1. Whenever the sen-
tence or query has a verb phrase (VP) spanning
roughly half of it, annotators seem to chunk be-
fore the VP as one would expect, quite as of-
ten as just after the verb, which is quite unex-
pected. For instance, the sentence A gentle
sarcasm ruffled her anger. gathers as
many as eight flat annotations with a boundary be-
tween sarcasm and ruffled, and four with
a boundary between ruffled and her. How-
ever, if the VP is very short consisting of a single
1720
Position Q500 QG500 Q700 S300 QRand
Both 2.24 0.37 2.78 2.08 0.63
None 50.34 56.85 35.74 35.84 39.81
Right 23.86 21.50 19.02 12.52 15.23
Left 18.08 15.97 40.59 45.96 21.21
Table 6: Percentages of positions of segment
boundaries with respect to prepositions. Prepo-
sitions occurring in the beginning or end of a
query/sentence have been excluded from the anal-
ysis; hence, numbers in a column do not total 100.
verb, as in A fleeting and furtive air
of triumph erupted., annotators seem to
attempt for a balanced annotation due to Bias 1.
As a clear middle boundary is not present in such
sentences, the annotations show a lot more varia-
tion and disagreement. For instance, only 1 out of
10 annotations had a boundary before erupted
in the above example. In fact, at least one anno-
tation had a boundary after each word in the sen-
tence, with no clear majority.
Bias 4: Prepositions influence segment bound-
aries differently for queries and sentences. We
automatically labeled all the prepositions in the
flat annotations and classified them according to
the criterion of whether a boundary was placed
immediately before or after it, or on both sides
or neither side. The statistics, reported in Ta-
ble 6, show that for NL sentences a majority
of the boundaries are present before the prepo-
sition, marking the beginning of a prepositional
phrase. However, for queries, a much richer pat-
tern emerges depending on the specific preposi-
tion. For instance, to, of and for are often
chunked with the previous word (e.g., how to |
choose a bike size, birthday party
ideas for | one year old). We believe
that this difference is because in sentences due
to the presence of a verb, the PP has a well-
defined head, lack of which leads to preposition
in queries getting chunked with words that form
more commonly seen patterns (e.g., flights
to and tickets for).
Bias 3 and 4 present the complex interpretation
of the structure of queries by the annotators which
could be due to some emerging cognitive model of
queries among the search engine users. This is a
fascinating and unexplored aspect of query struc-
tures that demands deeper investigation through
cognitive and psycholinguistic experiments.
7 Conclusion
We have studied various aspects of query segmen-
tation through crowdsourcing by designing and
conducting suitable experiments. Analysis of ex-
perimental data leads us to conclude the follow-
ing: (a) crowdsoucing may not be a very effective
way to collect judgments for query segmentation;
(b) addressing fuzziness of granularity for flat seg-
mentation by introducing strict binary nested seg-
ments does not lead to better agreement in crowd-
sourced annotations, though it definitely improves
the IAA for gold standard segmentations, imply-
ing that low IAA in flat segmentation among ex-
perts is primarily an effect of unspecified granular-
ity of segments; (c) low IAA is not due to the in-
herent structural ambiguity in queries as this holds
true for sentences as well; (d) there are strong bi-
ases in crowdsourced annotations, mostly because
turkers prefer more balanced segment structures;
and (e) while annotators are by and large guided
by linguistic principles, application of these prin-
ciples differ between query and NL sentences and
also closely interact with other biases.
One of the important contributions of this work
is the formulation of a new IAA metric for com-
paring across flat and nested segmentations, espe-
cially for crowdsourcing based annotations. Since
trees are commonly used across various linguistic
annotations, this metric can have wide applicabil-
ity. The metric, moreover, can be easily adapted
to other annotation schemes as well by defining an
appropriate distance metric between annotations.
Since large scale data for query segmentation is
very useful, it would be interesting to see if the
problem can be rephrased to the Turkers in a way
so as to obtain more reliable judgments. Yet a
deeper question is regarding the theoretical status
of query structure, which though in an emergent
state is definitely an operating model for the anno-
tators. Our future work in this area would specifi-
cally target understanding and formalization of the
theoretical model underpinning a query.
Acknowledgments
We thank Ed Cutrell and Andrew Cross, Microsoft
Research Lab India, for their help in setting up the
AMT experiments. We would also like to thank
Anusha Suresh, IIT Kharagpur, India, for helping
us with data preparation.
1721
References
Steven P. Abney. 1991. Parsing By Chunks. Kluwer
Academic Publishers.
Steven P. Abney. 1992. Prosodic Structure, Perfor-
mance Structure And Phrase Structure. In Proceed-
ings 5th DARPA Workshop on Speech and Natural
Language, pages 425?428. Morgan Kaufmann.
Steven P. Abney. 1995. Chunks and dependencies:
Bringing processing evidence to bear on syntax.
Computational Linguistics and the Foundations of
Linguistic Theory, pages 145?164.
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Computa-
tional Linguistics, 34(4):555?596.
Kalika Bali, Monojit Choudhury, Diptesh Chatterjee,
Sankalan Prasad, and Arpit Maheswari. 2009. Cor-
relates between Performance, Prosodic and Phrase
Structures in Bangla and Hindi: Insights from a Psy-
cholinguistic Experiment. In Proceedings of Inter-
national Conference on Natural Language Process-
ing, pages 101 ? 110.
Michael Bendersky, W. B. Croft, and David A. Smith.
2009. Two-stage query segmentation for informa-
tion retrieval. In Proceedings of the 32nd interna-
tional ACM Special Interest Group on Information
Retrieval (SIGIR) Conference on Research and De-
velopment in Information Retrieval, pages 810?811.
ACM.
Shane Bergsma and Qin Iris Wang. 2007. Learning
Noun Phrase Query Segmentation. In Proceedings
of Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 819?826.
Akshar Bharati, Vineet Chaitanya, Rajeev Sangal, and
KV Ramakrishnamacharyulu. 1995. Natural lan-
guage processing: a Paninian perspective. Prentice-
Hall of India New Delhi.
David J. Brenes, Daniel Gayo-Avello, and Rodrigo
Garcia. 2010. On the fly query segmentation using
snippets. In CERI ?10, pages 259?266.
Chris Callison-Burch. 2009. Fast, cheap, and cre-
ative: evaluating translation quality using amazon?s
mechanical turk. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP ?09, pages 286?295. Associa-
tion for Computational Linguistics.
Vitor R Carvalho, Matthew Lease, and Emine Yilmaz.
2011. Crowdsourcing for search evaluation. ACM
Sigir forum, 44(2):17?22.
Jacob Cohen. 1960. A Coefficient of Agreement for
Nominal Scales. Educational and Psychological
Measurement, 20(1):37?46.
Zhicheng Dou, Ruihua Song, Xiaojie Yuan, and Ji-
Rong Wen. 2008. Are Click-through Data Adequate
for Learning Web Search Rankings? In Proceed-
ings of the 17th ACM Conference on Information
and Knowledge Management, pages 73?82. ACM.
Matthias Hagen, Martin Potthast, Benno Stein, and
Christof Bra?utigam. 2011. Query Segmentation
Revisited. In Proceedings of the 20th Interna-
tional Conference on World Wide Web, pages 97?
106. ACM.
Matthias Hagen, Martin Potthast, Anna Beyer, and
Benno Stein. 2012. Towards Optimum Query Seg-
mentation: In Doubt Without. In Proceedings of the
Conference on Information and Knowledge Man-
agement, pages 1015?1024.
Jian Huang, Jianfeng Gao, Jiangbo Miao, Xiaolong
Li, Kuansan Wang, Fritz Behr, and C. Lee Giles.
2010. Exploring web scale language models for
search query processing. In Proceedings of the 19th
international conference on World wide web, WWW
?10, pages 451?460, New York, NY, USA. ACM.
Klaus Krippendorff. 2004. Content Analysis: An
Introduction to its Methodology. Sage,Thousand
Oaks, CA.
Yanen Li, Bo-Jun Paul Hsu, ChengXiang Zhai, and
Kuansan Wang. 2011. Unsupervised query segmen-
tation using clickthrough for information retrieval.
In SIGIR ?11, pages 285?294. ACM.
Knut Magne Risvik, Tomasz Mikolajewski, and Peter
Boros. 2003. Query segmentation for web search.
In WWW (Posters).
Rishiraj Saha Roy, Niloy Ganguly, Monojit Choud-
hury, and Srivatsan Laxman. 2012. An IR-based
Evaluation Framework for Web Search Query Seg-
mentation. In Proceedings of the International ACM
Special Interest Group on Information Retrieval (SI-
GIR) Conference on Research and Development in
Information Retrieval, pages 881?890. ACM.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast?but is it
good?: evaluating non-expert annotations for natural
language tasks. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP ?08, pages 254?263, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Bin Tan and Fuchun Peng. 2008. Unsupervised Query
Segmentation Using Generative Language Models
and Wikipedia. In Proceedings of the 17th Inter-
national Conference on World Wide Web (WWW),
pages 347?356. ACM.
Chao Zhang, Nan Sun, Xia Hu, Tingzhu Huang, and
Tat-Seng Chua. 2009. Query segmentation based on
eigenspace similarity. In Proceedings of the ACL-
IJCNLP 2009 Conference Short Papers, ACLShort
?09, pages 185?188, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
1722
Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 42?50,
Sofia, Bulgaria, August 8-9, 2013. c?2013 Association for Computational Linguistics
Entailment: An Effective Metric for Comparing and Evaluating
Hierarchical and Non-hierarchical Annotation Schemes
Rohan Ramanath?
R. V. College of Engineering, India
ronramanath@gmail.com
Monojit Choudhury Kalika Bali
Microsoft Research Lab India
{monojitc, kalikab}@microsoft.com
Abstract
Hierarchical or nested annotation of lin-
guistic data often co-exists with simpler
non-hierarchical or flat counterparts, a
classic example being that of annotations
used for parsing and chunking. In this
work, we propose a general strategy for
comparing across these two schemes of
annotation using the concept of entailment
that formalizes a correspondence between
them. We use crowdsourcing to obtain
query and sentence chunking and show
that entailment can not only be used as
an effective evaluation metric to assess the
quality of annotations, but it can also be
employed to filter out noisy annotations.
1 Introduction
Linguistic annotations at all levels of linguistic or-
ganization ? phonological, morpho-syntactic, se-
mantic, discourse and pragmatic, are often hierar-
chical or nested in nature. For instance, syntac-
tic dependencies are annotated as phrase structure
or dependency trees (Jurafsky and Martin, 2000).
Nevertheless, the inherent cognitive load associ-
ated with nested segmentation and the sufficiency
of simpler annotation schemes for building NLP
applications have often lead researchers to define
non-hierarchical or flat annotation schemes. The
flat annotation, in essence, is a ?flattened? ver-
sion of the tree. For instance, chunking of Natu-
ral Language (NL) text, which is often considered
an essential preprocessing step for many NLP ap-
plications (Abney, 1991; Abney, 1995), is, loosely
speaking, a flattened version of the phrase struc-
ture tree. The closely related task of Query Seg-
mentation is of special interest to us here, as it is
?The work was done during author?s internship at Mi-
crosoft Research Lab India.
f Pipe representation Boundary var.
3 barbie dress up | games 0 0 1
3 barbie dress | up games 0 1 0
2 barbie | dress up | games 1 0 1
2 barbie | dress up games 1 0 0
Table 1: Example of flat segmentations from 10
Turkers. f is the frequency of annotations; seg-
ment boundaries are represented by |.
the first step in further analysis and understanding
of Web search queries (Hagen et al, 2011).
The task in both query and sentence chunking is
to divide the string of words into contiguous sub-
strings of words (commonly refered to as segments
or chunks) such that the words from a segment
are related to each other more strongly than words
from different segments. It is typically assumed
that the segments are syntactically and semanti-
cally coherent. Table 1 illustrates the concept of
segmentation of a query. The crowdsourced an-
notations for this data were obtained from 10 an-
notators, the experimental details of which will be
described in Sec. 5. We shall refer to this style of
text chunking as flat segmentation.
Nested segmentation of a query or a sentence,
on the other hand, is a recursive application of flat
segmentation, whereby the longer flat segments
are further divided into smaller chunks recursively.
The process stops when a segment consists of less
than three words or is a multiword entity that can-
not be segmented further. This style of segmenta-
tion can be represented through nested parenthe-
sization of the text, as illustrated in Table 2. These
annotations were also obtained through the same
crowdsourcing experiment (Sec. 5). Fig. 1 shows
an alternative visualization of a nested segmenta-
tion in the form of a tree.
An important problem that arises in the con-
text of flat segmentation is the issue of granular-
42
f Bracket representation Boundary var.
4 ((barbie dress)( up games)) 0 1 0
3 (barbie ((dress up) games)) 2 0 1
2 (barbie (dress (up games))) 2 1 0
1 ((barbie (dress up)) games) 1 0 2
Table 2: Example of nested segmentation from 10
Turkers. f is the frequency of annotations.
2
barbie 1
0
dress up
games
Figure 1: Tree representation of the nested seg-
mentation: (barbie ((dress up) games))
ity. For instance, in the case of NL chunking, it
is not clear whether the chunk boundaries should
correspond to the innermost parentheses in the
nested segmentation marking very short chunks,
or should one annotate the larger chunks corre-
sponding to clausal boundaries. For this reason,
Inter-Annotator Agreement (IAA) for flat annota-
tion tasks is often poor (Bali et al, 2009; Hagen
et al, 2011; Saha Roy et al, 2012). However, low
IAA does not necessarily imply low quality anno-
tation, and could as well be due to the inherent am-
biguity in the task definition with respect to gran-
ularity. Although we have illustrated the concept
and problems of flat and nested annotations using
the examples of sentence and query segmentation,
these issues are generic and typical of any flat an-
notation scheme which tries to flatten or approx-
imate an underlying hierarchical structure. There
are three important research questions pertaining
to the linguistic annotations of this kind:
? How to measure the true IAA and the quality
of the flat annotations?
? How to compare the agreement between the
flat and the nested annotations?
? How can we identify or construct the opti-
mal or error-free flat annotations from a noisy
mixture of nested and flat annotations?
In this paper, we introduce the concept of ?en-
tailment of a flat annotation by a nested annota-
tion?. For a given linguistic unit (a query or a sen-
tence, for example), a nested annotation is said to
entail a flat annotation if the structure of the lat-
ter does not contradict the more specific structure
represented by the former. Based on this simple
notion, which will be formalized in Sec. 3, we
develop effective techniques for comparing across
and evaluating the quality of flat and nested an-
notations, and identifying the optimal flat annota-
tion. We validate our theoretical framework on the
tasks of query and sentence segmentation. In par-
ticular, we conduct crowdsourcing based flat and
nested segmentation experiments for Web search
queries and sentences using Amazon Mechanical
Turk (AMT)1. We also obtain annotations for the
same datasets by trained experts which are ex-
pected to be of better quality than the AMT-based
annotations. Various statistical analyses of the an-
notated data bring out the effectiveness of entail-
ment as a metric for comparison and evaluation of
flat and nested annotations.
The rest of the paper is organized as fol-
lows. Sec. 2 provides some background on the
annotation tasks and related work on IAA. In
Sec. 3, we introduce the notion of entailment
and develop theoretical models and related
strategies for assessing the quality of annotation.
In Sec. 4, we introduce some strategies based
on entailment for the identification of error-free
annotations from a given set of noisy annotations.
Sec. 5 describes the annotation experiments
and results. Sec. 6 concludes the paper by
summarizing the work and discussing future
research directions. All the annotated datasets
used in this research can be obtained freely from
http://research.microsoft.com/
apps/pubs/default.aspx?id=192002
and used for non-commercial research purposes.
2 Background
Segmentation or chunking of NL text is a well-
studied problem. Abney (1991; 1992; 1995)
defines a chunk as a sub-tree within a syntac-
tic phrase structure tree corresponding to Noun,
Prepositional, Adjectival, Adverbial and Verb
Phrases. Similarly, Bharati et al(1995) define it
as Noun Group and Verb Group based only on lo-
cal surface information. Chunking is an important
preprocessing step towards parsing.
Like chunking, query segmentation is an im-
portant step towards query understanding and is
generally believed to be useful for Web search
1https://www.mturk.com/mturk/welcome
43
(see Hagen et al (2011) for a survey). Auto-
matic query segmentation algorithms are typically
evaluated against a small set of human-annotated
queries (Bergsma and Wang, 2007). The reported
low IAA for such datasets casts serious doubts on
the reliability of annotation and the performance
of the algorithms evaluated on them (Hagen et al,
2011; Saha Roy et al, 2012). To address the is-
sue of data scarcity, Hagen et al (2011) created
a large set of manually segmented queries through
crowdsourcing2. However, their approach has cer-
tain limitations because the crowd is already pro-
vided with a few possible segmentations of a query
to choose from. Nevertheless, if large scale data
has to be procured crowdsourcing seems to be the
only efficient and effective model for the task, and
has been proven to be so for other IR and lin-
guistic annotations (see Lease et al (2011) for
examples). It should be noted that almost all the
work on query segmentation, except (Huang et al,
2010), has considered only flat segments.
An important problem that arises in the context
of flat annotations is the issue of granularity. In the
absence of a set of guidelines that explicitly state
the granularity expected, Inter-Annotator Agree-
ment (IAA) for flat annotation tasks are often poor.
Bali et al (2009) showed that for NL chunking,
annotators typically agree on major (i.e., clausal)
boundaries but do not agree on minor (i.e., phrasal
or intra-phrasal) boundaries. Similarly, for query
segmentation, low IAA remains an issue (Hagen
et al, 2011; Saha Roy et al, 2012).
The issue of granularity is effectively addressed
in nested annotation, because the annotator is ex-
pected to mark the most atomic segments (such
as named entities and multiword expressions) and
then recursively combine them to obtain larger
segments. Certain amount of ambiguity, that may
arise because of lack of specific guidelines on the
number of valid segments at the last level (i.e., top-
most level of the nested segmentation tree), can
also be resolved by forcing the annotator to recur-
sively divide the sentence/query always into ex-
actly two parts (Abney, 1992; Bali et al, 2009).
The present study is an extension of our recent
work (Ramanath et al, 2013) on analysis of the
effectiveness of crowdsourcing for query and sen-
tence segmentation. We introduced a novel IAA
metric based on Kripendorff?s ?, and showed that
while the apparent agreement between the annota-
2http://www.webis.de/research/corpora
tors in a crowdsourced experiment might be high,
the chance corrected agreement is actually low for
both flat and nested segmentations (as compared
to gold annotations obtained from three experts).
The reason for the apparently high agreement is
due to an inherent bias of the crowd to divide
a piece of text in roughly two equal parts. The
present study extends this work by introducing a
metric to compare across flat and nested segmen-
tations that enables us to further analyze the relia-
bility of the crowdsourced annotations. This met-
ric is then employed to identify the optimal flat
segmentation(s) from a set of noisy annotations.
The study uses the same experimental setup and
annotated datasets as described in (Ramanath et
al., 2013). Nevertheless, for the sake of readability
and self-containedness, the relevant details will be
mentioned here again.
We do not know of any previous work that com-
pares flat and nested schemes of annotation. In
fact, Artstein and Poesio (2008), in a detailed sur-
vey of IAA metrics and their usage in NLP, men-
tion that defining IAA metrics for trees (hierarchi-
cal annotations) is a difficult problem due to the
existence of overlapping annotations. Vadas and
Curran (2011) and Brants (2000) discuss measur-
ing IAA of nested segmentations employing the
concepts of precision, recall, and f-score. How-
ever, neither of these studies apply statistical cor-
rection for chance agreement.
3 Entailment: Definition and Modeling
In this section, we shall introduce certain notations
and use them to formalize the notion of entail-
ment, which in turn, is used for the computation of
agreement between flat and nested segmentations.
Although we shall develop the whole framework
in the context of queries, it is applicable to sen-
tence segmentation and, in fact, more generally to
any flat and nested annotations.
3.1 Basic Definitions
Let Q be the set of all queries. A query q ? Q
can be represented as a sequence of |q| words:
w1w2 . . . w|q|. We introduce |q| ? 1 random vari-
ables, b1, b2, . . . b|q|?1, such that bi represents the
boundary between the words wi and wi+1. A flat
and nested segmentation of q, represented by F jq
and N jq respectively, j varying from 1 to total
number of annotations, c, is a particular instan-
tiation of these boundary variables as follows.
44
Definition. Flat Segmentation: A flat segmen-
tation, F jq , can be uniquely defined by a binary
assignment of the boundary variables bji , where
bji = 1 iff wi and wi+1 belong to two different flat
segments. Otherwise, bji = 0. Thus, q has 2
|q|?1
possible flat segmentations.
Definition. Nested Segmentation: A nested seg-
mentation, N jq , is defined as an assignment of
non-negative integers to the boundary variables
such that bji = 0 iff words wi and wi+1 form an
atomic segment (i.e., they are grouped together),
else bji = 1 + max(lefti, righti), where lefti
and righti are the heights of the largest subtrees
ending at wi and beginning at wi+1 respectively.
This numbering scheme can be understood
through Fig. 1. Every internal node of the binary
tree corresponding to the nested segmentation is
numbered according to its height. The lowest in-
ternal nodes, both of whose children are query
words, are assigned a value of 0. Other internal
nodes get a value of one greater than the height
of its higher child. Since every internal node cor-
responds to a boundary, we assign the height of
the node to the corresponding boundary variables.
The number of unique nested segmentations of q
is the corresponding Catalan number3 C|q|?1.
Note that, following Abney?s (1992) suggestion
for nested chunking, we define nested segmenta-
tion as a strict binary tree or binary bracketing of
the query. This is not only helpful for theoretical
analysis, but also necessary to ensure that there
is no ambiguity related to the granularity of seg-
ments.
3.2 Entailment
Given a nested segmentation N jq , there are several
possible ways to ?flatten? it. Flat segmentations of
q, where bi = 0 for all i (i.e., the whole query is
one segment) and bi = 1 for all i (i.e., all words are
in different segments) are trivially obtainable from
N jq , and therefore, are not neither informative nor
interesting. Intuitively, any flat segmentation, F kq ,
can be said to agree with N jq if for every flat seg-
ment in F kq there is a corresponding internal node
in N jq , such that the subgraph rooted at that node
spans (contains) all and only those words present
in the flat segment (Abney, 1991).
Let us take the examples of flat and nested
segmentations shown in Tables 1 and 2 to illus-
3http://en.wikipedia.org/wiki/Catalan\
_number
trate this notion. Consider two nested segmenta-
tions, N1q = ((barbie (dress up)) games), N
2
q =
(barbie ((dress up) games)) and three flat seg-
mentations, F 1q = barbie | dress up | games,
F 2q = barbie | dress up games, F
3
q =
barbie dress | up games. Figure 2 diagram-
matically compares the two nested segmentations
(the two rows) with the three flat segmentations
(columns A, B and C). There are three flat seg-
ments in F 1q , of which the two single word
segments barbie and games trivially coincide
with the corresponding leaf nodes. The segment
dressup coincides exactly with the words spanned
by the node marked 0 of N1q (Fig. 2, top row, col-
umn A). Hence, F 1q can be said to be in agree-
ment withN1q . On the other hand, there is no node
in N1q , which exactly coincides with the segment
dressupgames of F 2q (Fig. 2, top row, column B).
Hence, we say that N1q does not agree with F
2
q .
We formalize this notion of agreement in terms
of entailment, which is defined as follows.
Definition: Entailment. A nested segmentation,
N jq is said to entail a flat segmentation, F
k
q , (or
equivalently, F kq is entailed by N
j
q ) if and only if
for every multiword segment wi+1, wi+2, ..., wi+l
in F kq , the corresponding boundary variables in
N jq follows the constraint: bi > bi+m and bi+l >
bi+m for all 1 ? m < l.
It can be proved that this definition of entail-
ment is equivalent to the intuitive description pro-
vided earlier. Yet another equivalent definition of
entailment is presented in the form of Algorithm 1.
Due to paucity of space, the proofs of equivalence
are omitted.
Definition: Average Observed Entailment. For
the set of queries Q, and corresponding sets of
c flat and nested segmentations, there are |Q|c2
pairs of flat and nested segmentations that can be
compared for entailment. We define the average
observed entailment for this annotation set as the
fraction of these |Q|c2 annotation pairs for which
the flat segmentation is entailed by the correspond-
ing nested segmentation. We shall express this
fraction as percentage.
3.3 Entailment by Random Chance
Average observed Entailment can be considered
as a measure of the IAA, and hence, an indica-
tor of the quality of the annotations. However,
in order to interpret the significance of this value,
we need an estimate of the average entailment that
45
Figure 2: Every node of the tree represent boundary values, nested(flat). Column A: F 1q is entailed by
both N1q and N
2
q , Column B: F
2
q is entailed by N
2
q but not N
1
q , Column C: F
3
q is entailed by neither
N1q nor N
2
q . The nodes (or equivalently the boundaries) violating the entailment constraint are marked a
cross, and those agreeing are marked with ticks.
Algorithm 1 Algorithm: isEntail
1: procedure ISENTAIL(flat, nested) . flat,
nested are lists containing boundary values
2: if len(nested) ? 1 or len(flat) ? 1 then
3: return True
4: end if
5: h? largest element in nested
6: i? index of h
7: if flat[i] = 1 then
8: if ! isEntail(flat[: i], nested[: i]) or
! isEntail(flat[i+1 :], nested[i+1 :]) then
9: return False
10: else
11: return True
12: end if
13: else
14: while h 6= 0 do
15: nested[i]? ?nested[i]
16: h? largest element in nested
17: i? index of h
18: if flat[i] = 1 then
19: return False
20: end if
21: end while
22: return True
23: end if
24: end procedure
one would expect if the annotations, both flat and
nested, were drawn uniformly at random from the
set of all possible annotations. From our exper-
iments we observe that trivial flat segmentations
are, in fact, extremely rare, and a very large frac-
tion of the flat annotations have two or three seg-
ments. Therefore, for computing the chance en-
tailment, we assume that the number of segments
in the flat segmentation is known and fixed, which
is either 2 or 3, but all segmentations with these
many segments are equally likely to be chosen.
We also assume that all nested segmentations are
equally likely.
When there are 2 segments: For a query q, the
number of flat segmentations with two segments,
i.e., one boundary, is
(|q|?1
1
)
= |q| ? 1. Note
that for any nested segmentation N jq , all flat seg-
mentations that have at least one boundary and is
entailed by it must have a boundary between wi?
and wi?+1, where bi? has the highest value in N jq .
In other words, bi? is the boundary corresponding
to the root of the nested tree (the proof is intu-
itive and is omitted). Therefore, there is exactly
one ?flat segmentation with one boundary? that is
entailed by a given N jq . Therefore, the random
chance that a nested segmentation N jq will entail
a flat segmentation with one boundary is given by
(|q| ? 1)?1 (for |q| > 1).
When there are 3 segments: Number of flat
segmentations with two boundaries is
(|q|?1
2
)
. The
flat segmentation(s) entailed by N jq can be gener-
ated as follows. As argued above, every flat seg-
mentation entailed by N jq must have a boundary
46
at position i?. The second boundary can be either
in the left or right of i?. But in either case, the
choice of the boundary is unique which will corre-
spond to the highest node in the left or right sub-
tree of the root node. Thus, every nested segmen-
tation entails at most 2 flat segmentations. How-
ever, if i? = 1 or |q| ? 1 for a N jq , then, respec-
tively, the left or right subtrees do not exist. In
such cases, there is only one flat segmentation en-
tailed by N jq . Note that there are exactly C|q|?2
nested segmentations for which the i? = 1, and
similarly another C|q|?2 for which i
? = |q| ? 1.
Therefore, out of C|q|?1 ?
(|q|?1
2
)
pairs, exactly
2C|q|?1?2C|q|?2 pairs satisfy the entailment con-
ditions. Thus, the expected probability of entail-
ment by random chance when there are exactly
two boundaries in the flat segmentation of q is:
2(C|q|?1 ? C|q|?2)
C|q|?1
(|q|?1
2
) = 2
(
|q| ? 1
2
)?1
(1?
C|q|?2
C|q|?1
)
The values of the probability of observing a ran-
dom nested segmentation entailing a flat segmen-
tation with exactly two boundaries for |q| =
3, 4, 5, 6, 7 and 8 are 1, 0.4, 0.213. 0.133, 0.091
and 0.049 respectively.
3.4 Other IAA Metrics
Although entailment can be used as a measure of
agreement between flat and nested segmentations,
IAA within flat or within nested segmentations
cannot be computed using this notion. In (Ra-
manath et al, 2013), we have extensively dealt
with the issue of computing IAA for these cases.
Krippendorff?s ? (Krippendorff, 2004), which is
an extremely versatile agreement coefficient, has
been appropriately modified to be applicable to a
crowdsourced annotation scenario. ? = 1 im-
plies perfect agreement, ? = 0 implies that the
observed agreement is just as good as that by ran-
dom chance, whereas ? < 0 implies that the ob-
served agreement is less than that one would ex-
pect by random chance. Due to paucity of space
we omit any further discussion on this and refer
the reader to (Ramanath et al, 2013). Here, we
will use the ? values as an alternative indicator of
IAA and therefore, the quality of annotation.
4 Optimal Segmentation
Suppose that we have a large number of flat and
nested annotations coming from a noisy source
such as crowdsourcing; is it possible to employ
the notion of entailment to identify the annota-
tions which are most likely to be correct? Here,
we describe two such strategies to obtain the opti-
mal (error-free) flat segmentation.
Flat Entailed by Most Nested (FEMN): The
intuition behind this approach is that if a flat seg-
mentation F kq is entailed by most of the nested
segmentations of q, then it is very likely that F kq
is correct. Therefore, for each flat segmentations
of q, we count the number of nested segmentations
of q that entail it, and the one with highest count is
declared as the optimal FEMN segmentation. It is
interesting to note that while computing the opti-
mal FEMN segmentation, we never encountered a
tie between two flat segmentations. The trivial flat
segmentations (i.e., if the whole query is one seg-
ment or every word is in different segments) are
filtered as a preprocessing step.
Iterative Voting (IV): FEMN assumes that the
nested segmentations are relatively noise-free. If
most of the nested segmentations are erroneous,
FEMN would select an erroneous optimal flat seg-
mentation. To circumvent this issue, we propose a
more sophisticated iterative voting process, where
we count the number of flat segmentations entailed
by each nested segmentation of q, and similarly,
number of nested segmentations that entail each
flat segmentation. The flat and nested segmenta-
tions with the least scores are then removed from
the dataset. Then we recursively apply the IV pro-
cess on the reduced set of annotations until we are
left with a single flat segmentation.
5 Experiments and Results
We obtained nested and flat segmentation of Web
search queries through crowdsourcing as well as
from trained experts. Furthermore, we also con-
ducted similar crowdsourcing experiments for NL
sentences, which helped us understand the specific
challenges in annotating queries because of their
apparent lack of a well-defined syntactic structure.
In this section, we first describe the experimen-
tal setup and datasets, and then present the obser-
vations and results.
5.1 Crowdsourcing Experiment
In this study we use the same set of crowd-
sourced annotations as described in (Ramanath
et al, 2013). For the sake of completeness, we
briefly describe the annotation procedure here as
47
well. We used Amazon Mechanical Turk for the
crowdsourcing experiments. Two separate Hu-
man Intelligence Tasks were designed for flat and
nested segmentation. The concept of flat and
nested segmentation was introduced to the Turk-
ers with the help of two short videos4.
When in doubt regarding the meaning of a
query, the Turkers were advised to issue the query
on a search engine of their choice and find out its
possible interpretation(s). Only Turkers who had
completed more than 100 tasks at an acceptance
rate of ? 60% were allowed to participate in the
task and were paid $0.02 for a flat and $0.06 for a
nested segmentation. Every query was annotated
by 10 different annotators.
5.2 Dataset
The following sets of queries and sentences were
used for annotations:
Q500, QG500: Saha Roy et al (2012) re-
leased a dataset of 500 queries, 5 to 8 words long,
for the evaluation of various segmentation algo-
rithms. This dataset has flat segmentations from
three annotators obtained under controlled exper-
imental settings, and could be considered as Gold
annotation. Hence, we selected this set for our ex-
periments as well. We procured the correspond-
ing nested segmentation for these queries from
two human experts who are regular search engine
users. They annotated the data under supervision
and were trained and paid for the task. We shall
refer to the set of flat and nested gold annotations
as QG500, whereas Q500 will be reserved for the
dataset procured through the AMT experiments.
Q700: As 500 queries are not enough for mak-
ing reliable conclusions and also, since the queries
may not have been chosen specifically for the pur-
pose of annotation experiments, we expanded the
set with another 700 queries sampled from the
logs of a popular commercial search engine. We
picked, uniformly at random, queries that were 4
to 8 words long.
S300: We randomly selected 300 English sen-
tences from a collection of full texts of public do-
main books5 that were 5 to 15 words long, and
manually checked them for well-formedness.
4Flat: http://youtu.be/eMeLjJIvIh0, Nested:
http://youtu.be/xE3rwANbFvU
5http://www.gutenberg.org
5.3 Entailment Statistics
Table 3 reports two statistics ? the values of
Kripendorff?s ? and the average observed entail-
ment (expressed as %) for flat and nested segmen-
tations along with the corresponding expected val-
ues for entailment by chance. For nested segmen-
tation, the ? values were computed for two differ-
ent distance metrics6 d1 and d2.
As expected, the highest value of ? for both
flat and nested segmentation is observed for the
gold annotations. An ? > 0.6 indicates a rea-
sonably good7 IAA, and thus, reliable annota-
tions. We note that the entailment statistics fol-
low a very similar trend as ?, and for all the cases,
the observed average entailment is much higher
than what we would expect by random chance.
These two observations clearly point to the fact
that entailment is indeed a good indicator of the
agreement between the nested and flat segmenta-
tions, and consequently, the reliability of the an-
notations. We also observe that the average en-
tailment for S300 is in the same ballpark as for
the queries. This indicates that the apparent lack
of structure in queries does not specifically influ-
ence the annotations. Along the same lines, one
can also argue that the length of a text, which
is higher for sentences than queries, does not af-
fect the crowdsourced annotations. In fact, in our
previous study (Ramanath et al, 2013), we show
that it is the bias of the Turkers to divide a text
in approximately two segments of equal size (ir-
respective of other factors, like syntactic structure
or length), that leads to very similar IAA across
different types of texts. Our current study on en-
tailment further strengthens this fact.
Figure 3 plots the distribution of the entailment
values for the three datasets. The distributions are
normal-like implying that entailment is a robust
metric and its average value is a usable statistic.
In order to analyze the agreement between the
Turkers and the experts, we computed the av-
erage entailment between Q500 flat annotations
(from AMT) with QG500 nested annotations, and
similarly, Q500 nested annotations with QG500
6Intuitively, for d1 disagreements between segment
boundaries are equally penalized at all the levels of nested
tree, whereas for d2 disagreements higher up the tree (i.e.,
close to the root) are penalized more than those at lower lev-
els.
7It should be noted that there is no consensus on what is
a good value of ? for linguistic annotations, partly because
it is dependent on the nature of the annotation task and the
demand of the end applications that use the annotated data.
48
Dataset Krippendorff?s ? Entailment Statistics
Flat Nested Observed Chance
d1 d1 d2
Q700 0.21 0.21 0.16 49.68 12.63
Q500 0.22 0.15 0.15 56.69 19.08
QG500 0.61 0.66 0.67 87.07 11.91
S300 0.27 0.18 0.14 52.86 19.12
Table 3: ? and Average Entailment Statistics
Figure 3: Distribution of the entailment values (x-
axis) plotted as the % of comparable flat-nested
annotation pairs.
Figure 4: Distribution of percentage of entailed
pairs using QG500 as reference.
flat annotations, which turned out to be 70.42%
and 63.24% respectively. The corresponding dis-
tributions are shown as Nested and Flat in Fig.
4. Thus, the flat segmentations from the Turkers
seem to be more accurate than their nested seg-
mentations, a fact also supported by the ? values.
This could be due to the much higher cognitive
load associated with nested segmentation that de-
mands more time and concentration that an ordi-
nary Turker may not be willing to invest.
5.4 Optimal Segmentation Results
In order to evaluate the optimal flat segmentation
selection strategies, FEMN and IV, we computed
the percentage of queries in Q500 for which the
optimal flat segmentation (as obtained by apply-
ing these strategies on AMT annotations) is en-
tailed by the corresponding nested segmentations
in QG500. The average entailment values for
FEMN and IV turns out to be 79.60% and 82.80%
respectively. This shows that the strategies are in-
deed able to pull out the more accurate flat seg-
mentations from the set, though, as one would ex-
pect, IV performs better than FEMN, and its cho-
sen segmentations are almost as good as that by
expert annotators.
Another experiment was conducted to precisely
characterize the effectiveness of these strategies
whereby we mixed the annotations from the Q500
and QG500, and then applied FEMN and IV to
pull out the optimal flat segmentations. We ob-
served that for 63.71% and 91.44% of the queries,
the optimal segmentation chosen by FEMN and IV
respectively was indeed one of the three gold flat
annotations in QG500. This reinforces our con-
clusion that IV can effectively identify the optimal
flat segmentation of a query from a noisy set of flat
and nested segmentations.
6 Conclusion
In this paper, we proposed entailment as a theo-
retical model for comparing hierarchical and non-
hierarchical annotations. We present a formaliza-
tion of the notion of entailment and use it for de-
vising two strategies, FEMN and IV, for identify-
ing the optimal flat segmentation in a noisy set of
annotations. One of the main contributions of this
work resides in our following experimental find-
ing: Even though annotations obtained through
crowdsourcing for a difficult task like query seg-
mentation might be very noisy, a small fraction of
the annotations are nevertheless correct; it is pos-
sible to filter out these correct annotations using
the Iterative Voting strategy when both hierarchi-
cal and non-hierarchical segmentations are avail-
able from the crowd.
The proposed model is generic and we be-
lieve that the experimental findings extend beyond
query and sentence segmentation to other kinds of
linguistic annotations where hierarchical and non-
hierarchical schemes co-exist.
Acknowledgment
Thanks to Rishiraj Saha Roy, IIT Kharagpur, for
his valuable inputs during this work.
49
References
Steven P. Abney. 1991. Parsing By Chunks. Kluwer
Academic Publishers.
Steven P. Abney. 1992. Prosodic Structure, Perfor-
mance Structure And Phrase Structure. In Proceed-
ings 5th Darpa Workshop on Speech and Natural
Language, pages 425?428. Morgan Kaufmann.
Steven P. Abney. 1995. Chunks and dependencies:
Bringing processing evidence to bear on syntax.
Computational Linguistics and the Foundations of
Linguistic Theory, pages 145?164.
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Computa-
tional Linguistics, 34(4):555?596.
Kalika Bali, Monojit Choudhury, Diptesh Chatterjee,
Sankalan Prasad, and Arpit Maheswari. 2009. Cor-
relates between Performance, Prosodic and Phrase
Structures in Bangla and Hindi: Insights from a Psy-
cholinguistic Experiment. In ICON ?09, pages 101
? 110.
Shane Bergsma and Qin Iris Wang. 2007. Learn-
ing Noun Phrase Query Segmentation. In EMNLP-
CoNLL ?07, pages 819?826.
Akshar Bharati, Vineet Chaitanya, and Rajeev Sangal.
1995. Natural Language Processing: A Paninian
Perspective. Prentice.
Thorsten Brants. 2000. Inter-annotator agreement for
a German newspaper corpus. In In Proceedings of
Second International Conference on Language Re-
sources and Evaluation LREC-2000.
Matthias Hagen, Martin Potthast, Benno Stein, and
Christof Bra?utigam. 2011. Query segmentation re-
visited. In WWW ?11, pages 97?106.
Jian Huang, Jianfeng Gao, Jiangbo Miao, Xiaolong Li,
Kuansan Wang, Fritz Behr, and C. Lee Giles. 2010.
Exploring Web Scale Language Models for Search
Query Processing. In WWW ?10, pages 451?460.
Dan Jurafsky and James H Martin. 2000. Speech &
Language Processing. Pearson Education India.
Klaus Krippendorff. 2004. Content Analysis: An
Introduction to Its Methodology. Sage,Thousand
Oaks, CA.
Matthew Lease, Vaughn Hester, Alexander Sorokin,
and Emine Yilmaz, editors. 2011. Proceedings of
the ACM SIGIR 2011 Workshop on Crowdsourcing
for Information Retrieval (CIR 2011).
Rohan Ramanath, Monojit Choudhury, Kalika Bali,
and Rishiraj Saha Roy. 2013. Crowd Prefers the
Middle Path: A New IAA Metric for Crowdsourc-
ing Reveals Turker Biases in Query Segmentation.
In Proceedings of ACL. ACL.
Rishiraj Saha Roy, Niloy Ganguly, Monojit Choud-
hury, and Srivatsan Laxman. 2012. An IR-based
Evaluation Framework for Web Search Query Seg-
mentation. In SIGIR ?12, pages 881?890. ACM.
David Vadas and James R. Curran. 2011. Parsing
Noun Phrases in the Penn Treebank. Comput. Lin-
guist., 37(4):753?809, December.
50
Proceedings of The First Workshop on Computational Approaches to Code Switching, pages 73?79,
October 25, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Word-level Language Identification using CRF: Code-switching Shared
Task Report of MSR India System
Gokul Chittaranjan
Microsoft Research India
t-gochit@microsoft.com
Yogarshi Vyas
?
University of Maryland
yogarshi@cs.umd.edu
Kalika Bali Monojit Choudhury
Microsoft Research India
{kalikab, monojitc}@microsoft.com
Abstract
We describe a CRF based system for
word-level language identification of
code-mixed text. Our method uses lexical,
contextual, character n-gram, and special
character features, and therefore, can
easily be replicated across languages. Its
performance is benchmarked against the
test sets provided by the shared task on
code-mixing (Solorio et al., 2014) for
four language pairs, namely, English-
Spanish (En-Es), English-Nepali (En-Ne),
English-Mandarin (En-Cn), and Standard
Arabic-Arabic (Ar-Ar) Dialects. The
experimental results show a consistent
performance across the language pairs.
1 Introduction
Code-mixing and code-switching in conversations
has been an extensively studied topic for sev-
eral years; it has been analyzed from structural,
psycholinguistic, and sociolinguistic perspec-
tives (Muysken, 2001; Poplack, 2004; Senaratne,
2009; Boztepe, 2005). Although bilingualism
is very common in many countries, it has sel-
dom been studied in detail in computer-mediated-
communication, and more particularly in social
media. A large portion of related work (Androut-
sopoulos, 2013; Paolillo, 2011; Dabrowska, 2013;
Halim and Maros, 2014), does not explicitly deal
with computational modeling of this phenomena.
Therefore, identifying code-mixing in social me-
dia conversations and the web is a very relevant
topic today. It has garnered interest recently, in
the context of basic NLP tasks (Solorio and Liu,
2008b; Solorio and Liu, 2008a), IR (Roy et al.,
2013) and social media analysis (Lignos and Mar-
cus, 2013). It should also be noted that the identi-
?
The author contributed to this work during his intern-
ship at Microsoft Research India
fication of languages due to code-switching is dif-
ferent from identifying multiple languages in doc-
uments (Nguyen and Dogruz, 2013), as the dif-
ferent languages contained in a single document
might not necessarily be due to instances of code
switching.
In this paper, we present a system built with
off-the-shelf tools that utilize several character and
word-level features to solve the EMNLP Code-
Switching shared task (Solorio et al., 2014) of
labeling a sequence of words with six tags viz.
lang1, lang2, mixed, ne, ambiguous, and others.
Here, lang1 and lang2 refer to the two languages
that are mixed in the text, which could be English-
Spanish, English-Nepali, English-Mandarin or
Standard Arabic-dialectal Arabic. mixed refers
to tokens with morphemes from both, lang1 and
lang2, ne are named entities, a word whose label
cannot be determined with certainty in the given
context is labeled ambiguous, and everything else
is tagged other (Smileys, punctuations, etc.).
The report is organized as follows. In Sec. 2,
we present an overview of the system and detail
out the features. Sec. 3 describes the training ex-
periments to fine tune the system. The shared task
results on test data provided by the organizers is
reported and discussed in Sec. 4. In Sec. 5 we con-
clude with some pointers to future work.
2 System overview
The task can be viewed as a sequence labeling
problem, where, like POS tagging, each token in a
sentence needs to be labeled with one of the 6 tags.
Conditional Random Fields (CRF) are a reason-
able choice for such sequence labeling tasks (Laf-
ferty et al., 2001); previous work (King and Ab-
ney, 2013) has shown that it provides good perfor-
mance for the language identification task as well.
Therefore, in our work, we explored various token
level and contextual features to build an optimal
CRF using the provided training data. The features
73
Lang. Given Ids Available Available (%)
Train Test Train Test Train Test
Es 11,400 3,014 11,400 1,672 100% 54.5%
Ne 9,999 3,018 9,296 2,874 93% 95.2%
Cn 999 316 995 313 99.6% 99.1%
Ar 5,839 2,363 5,839 2,363 100% 100%
Ar 2 - 1,777 - 1,777 - 100%
Table 2: Number of tweets retrieved for the vari-
ous datasets.
used can be broadly grouped as described below:
Capitalization Features: They capture if let-
ter(s) in a token has been capitalized or not. The
reason for using this feature is that in several lan-
guages, capital Roman letters are used to denote
proper nouns which could correspond to named
entities. This feature is meaningful only for lan-
guages which make case distinction (e.g., Roman,
Greek and Cyrillic scripts).
Contextual Features: They constitute the cur-
rent and surrounding tokens and the length of the
current token. Code-switching points are context
sensitive and depend on various structural restric-
tions (Muysken, 2001; Poplack, 1980).
Special Character Features: They capture the
existence of special characters and numbers in the
token. Tweets contain various entities like hash-
tags, mentions, links, smileys, etc., which are sig-
naled by #, @ and other special characters.
Lexicon Features: These features indicate the
existence of a token in lexicons. Common words
in a language and named entities can be curated
into finite, manageable lexicons and were there-
fore used for cases where such data was available.
Character n-gram features: Following King
and Abney (2013), we also used charagter n-grams
for n=1 to 5. However, instead of directly using
the n-grams as features in the CRF, we trained
two binary maximum entropy classifiers to identify
words of lang1 and lang2. The classifiers returned
the probability that a word is of lang1 (or lang2),
which were then binned into 10 equal buckets and
used as features.
The features are listed in Table 1.
3 Experiments
3.1 Data extraction and pre-processing
The ruby script provided by the shared task orga-
nizers was used to retrieve tweets for each of the
language pairs. Tweets that could not be down-
loaded either because they were deleted or pro-
Source Language For
instance types en.nt.bz2
1
English NE
instance types es.nt.bz2
1
Spanish NE
eng wikipedia 2010 1M-text.tar.gz
2
English FW
spa wikipedia 2011 1M-text.tar.gz
2
Spanish FW
Table 3: External resources used in the task.
1
http://wiki.dbpedia.org/Download,
2
http://corpora.uni-
leipzig.de/download.html; NE:Named entities, FW:Word fre-
quency list
tected were excluded from the training set. Ta-
ble 2 shows the number of tweets that we were
able to retrieve for the released datasets. Further,
we found a few rare cases of tokenization errors,
as evident from the occurrence of spaces within
tokens. These were not removed from the training
set and instead, the spaces in these tokens were re-
placed by an underscore.
3.2 Feature extraction and labeling
Named entities for English and Spanish were
obtained from DBPedia instance types, namely,
Agent, Award, Device, Holiday, Language, Mean-
sOfTransportation, Name, PersonFunction, Place,
and Work. Frequency lists for these languages
were obtained from the Leipzig Copora Collec-
tion(Quasthoff et al., 2006); words containing spe-
cial characters and numbers were removed from
the list. The files used are listed in table 3. The
character n-gram classifiers were implemented
using the MaxEnt classifier provided in MAL-
LET (McCallum, 2002). The classifiers were
trained on 6,000 positive examples randomly sam-
pled from the training set and negative examples
sampled from both, the training set and from word
lists of multiple languages from (Quasthoff et al.,
2006); the number of examples used for each of
these classifiers is given in Table 4.
We used CRF++ (Kudo, 2014) for labeling the
tweets. For all language pairs, CRF++ was run
under its default settings.
3.3 Model selection
For each language pair, we experimented with var-
ious feature combinations using 3-fold cross vali-
dation on the released training sets. Table 5 reports
the token-level labeling accuracies for the various
models, based on which the optimal feature sets
for each language pairs were chosen. These opti-
mal features are reported in Table 1, and the cor-
responding performance for 3-fold cross valida-
tion in Table 5. The final runs submitted for the
74
ID Feature Description Type Features used in the final submission (Optimal set)
En-Es En-Ne En-Cn Ar-Ar
Capitalization Features
CAP1 Is first letter capitalized? True/False 3 3 3 NA
CAP2 Is any character capitalized? True/False 3 3 3 NA
CAP3 Are all characters capitalized? True/False 3 3 3 NA
Contextual Features
CON1 Current Token String 3 3 3 3
CON2 Previous 3 and next 3 tokens Array (Strings) 3 3 3
CON3 Word length String 3 3 3 3
Special Character Features
CHR0 Is English alphabet word? True/False 3 NA
CHR1 Contains @ in locations 2-end True/False 3 3 3 3
CHR2 Contains # in locations 2-end True/False 3 3 3 3
CHR3 Contains ? in locations 2-end True/False 3 3 3 3
CHR4 Contains / in locations 2-end True/False 3 3 3 3
CHR5 Contains number in locations 2-end True/False 3 3 3 3
CHR6 Contains punctuation in locations 2-
end
True/False 3 3 3 3
CHR7 Starts with @ True/False 3 3 3 3
CHR8 Starts with # True/False 3 3 3 3
CHR9 Starts with ? True/False 3 3 3 3
CHR10 Starts with / True/False 3 3 3 3
CHR11 Starts with number True/False 3 3 3 3
CHR12 Starts with punctuation True/False 3 3 3 3
CHR13 Token is a number? True/False 3 3 3 3
CHR14 Token is a punctuation? True/False 3 3 3 3
CHR15 Token contains a number? True/False 3 3 3 3
Lexicon Features
LEX1 In lang1 dictionary of most frequent
words?
True/False 3 3 3 NA
LEX2 In lang2 dictionary of most frequent
words?
True/False 3 NA NA
LEX3 Is NE? True/False 3 3 NA NA
LEX4 Is Acronym True/False 3 3 NA NA
Character n-gram Features
CNG0 Output of two MaxEnt classifiers
that classify lang1 vs. others and
lang2 vs. others. This gives 2 prob-
ability values binned into 10 bns,
two from each classifier, for the two
classes.
Array (binned
probability)
3 3 NA NA
CRF Feature Type U U U B
Table 1: A description of features used. NA refers to features that were either not applicable to the
language pair or were not available. B/U implies that the CRF has/does not have access to the features
of the previous token.
75
Classifier Languages used (And # words)
English-Spanish Language Pair
Spanish vs Others [es (6000)], [en (4000), fr (500), hi (500), it (500), po (500)]
English vs Others [en (6000)], [es (4000), fr (500), hi (500), it (500), po (500)]
English-Nepali Language Pair
Nepali vs Others [ne (6000)], [en (3500), fr (500), hi (500), it (500), po (500)]
English vs Others [en (6000)], [ne (3500), fr (500), hi (500), it (500), po (500)]
Standard Arabic vs. Arabic Dialects
Std vs. Dialect [lang1 (9000)], [lang2 (3256)]
Table 4: Data to train character n-gram classifiers.
shared task, including those for the surprise test
sets, use the corresponding optimal feature sets for
each language pair.
Feature Context Language Pair
En-
Es
En-
Ne
?
En-
Cn
Ar-
Ar
Ar-
Ar
(2)
Development Set
All B 92.8 94.3 93.1 85.5 -
- CON2 B 93.8 95.6 94.9 81.2 -
- CHR* B 92.3 93.5 91.0 85.3 -
- CAP* B 92.7 94.2 90.1 - -
- CON2 U 93.0 94.3 93.1 85.6 -
- CNG0 B 92.7 94.2 - - -
- LEX* B 92.7 94.1 - - -
Optimal - 95.0 95.6 95.0 85.5 -
Results on Test data for the optimal feature sets
Regular 85.0 95.2 90.4 90.1 53.6
Surprise 91.8 80.8 - 65.0 -
Table 5: The overall token labeling accuracies (in
%) for all language pairs on the training and test
datasets. ?-? indicates the removal of the given
feature. ?*? is used to indicate a group of features.
Refer tab. 1) for the feature Ids and the optimal
set. B and U stand for bigram and unigram respec-
tively, where the former refers to the case when the
CRF had access to features of the current and pre-
vious tokens, and the latter to the case where the
CRF had access only to the features of the current
token. ?: Lexical resources available for En only.
4 Results and Observations
4.1 Overall token labeling accuracy
The overall token labeling accuracies for the regu-
lar and surpise test sets (wherever applicable) and
a second set of dialectal and standard Arabic are
reported in the last two rows of Table 5. The same
table also reports the results of the 3-fold cross val-
idation on the training datasets. Several important
observations can be made from these accuracy val-
ues.
Firstly, accuracies observed during the training
phase was quite high (? 95%) and exactly simi-
lar for En-Es, En-Ne and En-Cn data; but for Ar-
Ar dataset our method could achieve only up to
85% accuracy. We believe that this is due to un-
availability of any of the lexicon features, which
in turn was because we did not have access to any
lexicon for dialectal Arabic. While complete set
of lexical features were not available for En-Cn as
well, we did have English lexicon; also, we no-
ticed that in the En-Cn dataset, almost always the
En words were written in Roman script and the Cn
words were written in the Chinese script. Hence,
in this case, script itself is a very effective feature
for classification, which has been indirectly mod-
eled by the CHR0 feature. On the other hand, in
the Ar-Ar datasets, both the dialects are written us-
ing the same script (Arabic). Further, we found
that using the CNG0 feature that is obtained by
training a character n-gram classifier for the lan-
guage pairs resulted in the drop of performance.
Since we are not familiar with arabic scripts, we
are not sure how effective the character n-gram
based features are in differentiating between the
standard and the dialectal Arabic. Based on our
experiment with CNG0, we hypothesize that the
dialects may not show a drastic difference in their
character n-gram distributions and therefore may
not contribute to the performance of our system.
Secondly, we observe that effectiveness of the
different feature sets vary across language pairs.
Using all the features of the previous words (con-
text = B) seems to hurt the performance, though
just looking at the previous 3 and next 3 tokens
was useful. On the other hand, in Ar-Ar the re-
verse has been observed. Apart from lexicons,
76
character n-grams seems to be a very useful fea-
ture in En-Es classification. As discussed above,
CHR* features are effective for En-Cn because,
among other things, one of these features also cap-
tures whether the word is in Roman script. For En-
Ne, we do not see any particular feature or sets of
features that strongly influence the classification.
The overall token labeling accuracy of the
shared task runs, at least in some cases, differ quite
significantly from our 3-fold cross validation re-
sults. On the regular test sets, the results for En-
Ne is very similar to, and En-Cn and Ar-Ar are
within expected range of the training set results.
However, we observe a 10% drop in En-Es. We
observe an even bigger drop in the accuracy of the
second Ar-Ar test set. We will discuss the possible
reason for this in the next subsection. The accura-
cies on the surprise sets do not show any specific
trend. While for En-Es the accuracy is higher by
5% for the surprise set than the regular set, En-Ne
and Ar-Ar show the reverse, and a more expected
trend. The rather drastic drops in the accuracy for
these two pairs on the surprise sets makes error
analysis and comparative analysis of the training,
test and surprise datasets imperative.
4.2 Error Analysis
Table 6 reports the F-scores for the six labels, i.e.,
classes, and also an overall tweet/post level accu-
racy. The latter is defined as the percentage of in-
put units (which could be either a tweet or a post or
just a sentence depending on the dataset) that are
correctly identified as either code-mixed or mono-
lingual; an input unit is considered code-mixed if
there is at least one word labeled as lang1 and one
as lang2.
For all the language pairs other than Arabic, the
F-score for NE is much lower than that for lang1
and lang2. Thus, the performance of the system
can be significantly improved by identifying NEs
better. Currently, we have used lexicons for only
English and Spanish. This information was not
available for the other languages, namely, Nepali,
Mandarin, and Arabic. The problem of NE detec-
tion is further compounded by the informal nature
of sentences, because of which they may not al-
ways be capitalized or spelt properly. Better de-
tection of NEs in code-mixed and informal text is
an interesting research challenge that we plan to
tackle in the future.
Note that the ambiguous and mixed classes can
be ignored because their combined occurrence is
less than 0.5% in all the datasets, and hence they
have practically no effect on the final labeling ac-
curacy. In fact, their rarity (especially in the train-
ing set) is also the reason behind the very poor F-
scores for these classes. In En-Cn, we also observe
a low F-score for other.
In the Ar-Ar training data as well as the test set,
there are fewer words of lang2, i.e., dialectal Ara-
bic. Since our system was trained primarily on the
context and word features (and not lexicon or char-
acter n-grams), there was not enough examples in
the training set for lang2 to learn a reliable model
for identifying lang2. Moreover, due to the dis-
tributional skew, the system learnt to label the to-
kens as lang1 with very high probability. The high
accuracy in the Ar-Ar original test set is because
81.5% of the tokens were indeed of type lang1
in the test data while only 0.26% were labeled as
lang2. This is also reflected by the fact that though
the F-score for lang2 in Ar-Ar test set is 0.158, the
overall accuracy is still 90.1% because F-score for
lang1 is 94.2%.
As shown in Table 7, the distribution of the
classes in the second Ar-Ar test set and the sur-
prise set is much less skewed and thus, very differ-
ent from that of the training and original test sets.
In fact, words of lang2 occur more frequently in
these sets than those of lang1. This difference in
class distributions, we believe, is the primary rea-
son behind the poorer performance of the system
on some of the Ar-Ar test sets.
We also observe a significant drop in accuracy
for En-Ne surprise data, as compared to the accu-
racy on the regular En-Ne test and training data.
We suspect that it could be either due to the dif-
ference in the class distribution or the genre/style
of the two datasets, or both. An analysis of the
surprise test set reveals that a good fraction of
the data consist of long song titles or part of the
lyrics of various Nepali songs. Many of these
words were labeled as lang2 (i.e., Nepali) by our
system, but were actually labeled as NEs in the
gold annotations
1
While song titles can certainly
be considered as NEs, it is very difficult to iden-
tify them without appropriate resources. It should
however be noted that the En-Ne surprise set has
only 1087 tokens, which is too small to base any
strong claims or conclusions on.
1
Confirmed by the shared task organizers over email com-
munication.
77
Language Pair F-measure (Token-level) Accuracy of
Ambiguous lang1 lang2 mixed NE Other Comment/Post
En-Es 0.000 0.856 0.879 0.000 0.156 0.856 82.1
En-Ne - 0.948 0.969 0.000 0.454 0.972 95.3
En-Cn - 0.980 0.762 0.000 0.664 0.344 81.8
Ar-Ar 0.000 0.942 0.158 - 0.577 0.911 94.7
Ar-Ar (2) 0.015 0.587 0.505 0.000 0.424 0.438 71.4
En-Es Surprise 0.000 0.845 0.864 0.000 0.148 0.837 81.5
En-Ne Surprise - 0.785 0.874 - 0.370 0.808 71.6
Ar-Ar Surprise 0.000 0.563 0.698 0.000 0.332 0.966 84.8
Table 6: Class-wise F-scores and comment/post level accuracy of the submitted runs.
Dataset Percentage of
Amb. lang1 lang2 mixed NE Other
Training 0.89 66.36 13.60 0.01 11.83 7.30
Test-1 0.02 81.54 0.26 0.00 10.97 7.21
Test-2 0.37 32.04 45.34 0.01 13.24 9.01
Surprise 0.91 22.36 57.67 0.03 9.13 9.90
Table 7: Distribution (in %) of the classes in the
training and the three test sets for Ar-Ar.
5 Conclusion
In this paper, we have described a CRF based word
labeling system for word-level language identifi-
cation of code-mixed text. The system relies on
annotated data for supervised training and also
lexicons of the languages, if available. Character
n-grams of the words were also used in a MaxEnt
classifier to detect the language of a word. This
feature has been found to be useful for some lan-
guage pairs. Since none of the techniques or con-
cepts used here is language specific, we believe
that this approach is applicable for word labeling
for code-mixed text between any two (or more)
languages as long as annotated data is available.
This is demonstrated by the fact that the sys-
tem performs more or less consistently with accu-
racies ranging from 80% - 95% across four lan-
guage pairs (except for the case of Ar-Ar second
test set and the surprise set which is due to stark
distributional differences between the training and
test sets). NE detection is one of the most chal-
lenging problems, improving which will definitely
improve the overall performance of our system. It
will be interesting to explore semi-supervised and
unsupervised techniques for solving this task be-
cause creating annotated datasets is expensive and
effort-intensive.
References
Jannis Androutsopoulos. 2013. Code-switching in
computer-mediated communication. In Pragmatics
of Computer-mediated Communication, pages 667?
694. Berlin/Boston: de Gruyter Mouton.
Erman Boztepe. 2005. Issues in code-switching:
competing theories and models. Teachers College,
Columbia University Working Papers in TESOL &
Applied Linguistics, 3.2.
Marta Dabrowska. 2013. Functions of code-switching
in polish and hindi facebook users? post. Studia
Linguistica Universitatis Lagellonicae Cracovien-
sis, 130:63?84.
Nur Syazwani Halim and Marlyana Maros. 2014.
The functions of code-switching in facebook inter-
actions. In Proceedings of the International Con-
ference on Knowledge-Innovation-Excellence: Syn-
ergy in Language Research and Practice; Social and
Behavioural Sciences, volume 118, pages 126?133.
Ben King and Steven Abney. 2013. Labeling the lan-
guages of words in mixed-language documents us-
ing weakly supervised methods. In Proceedings of
NAACL-HLT, pages 1110?1119.
Taku Kudo. 2014. Crf++: Yet another crf
toolkit. http://crfpp.googlecode.com/
svn/trunk/doc/index.html?source=
navbar#links, Retrieved 11.09.2014.
John Lafferty, Andrew McCallum, and Fernando CN
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of the International
Conference on Machine Learning (ICML), pages
282?289.
Constantine Lignos and Mitch Marcus. 2013. Toward
web-scale analysis of codeswitching. In 87th An-
nual Meeting of the Linguistic Society of America.
Andrew Kachites McCallum. 2002. Mallet: A ma-
chine learning for language toolkit. http://
mallet.cs.umass.edu.
Pieter Muysken. 2001. The study of code-mixing. In
Bilingual Speech: A typology of Code-Mixing. Cam-
bridge University Press.
78
Dong Nguyen and A. Seza Dogruz. 2013. Word level
language identification in online multilingual com-
munication. In Proceedings of the 2013 Conference
on Empirical Methods in natural Language Process-
ing, pages 857?862.
John C. Paolillo. 2011. Conversational codeswitch-
ing on usenet and internet relay chat. Lan-
guage@Internet, 8.
Shana Poplack. 1980. Sometimes i?ll start a sentence
in Spanish y termino en espanol: Toward a typology
of code-switching. Linguistics, 18:581?618.
Shana Poplack. 2004. Code-switching. In U. Am-
mon, N. Dittmar, K.K. Mattheier, and P. Turdgill,
editors, Soziolinguistik. An international handbook
of the science of language. Walter de Gruyter.
U. Quasthoff, M. Richter, and C. Biemann. 2006. Cor-
pus portal for search in monolingual corpora. In
Proceedings of the fifth International Conference on
Language Resource and Evaluation, pages 1799?
1802.
Rishiraj Saha Roy, Monojit Choudhury, Prasenjit Ma-
jumder, and Komal Agarwal. 2013. Overview and
datasets of fire 2013 track on transliterated search.
In Proceedings of the FIRE 2013 Shared Task on
Transliterated Search.
Chamindi Dilkushi Senaratne, 2009. Sinhala-English
code-mixing in Sri Lanka: A sociolinguistic study,
chapter Code-mixing as a research topic. LOT Pub-
lications.
Thamar Solorio and Yang Liu. 2008a. Learning to pre-
dict code-switching points. In Proceedings of the
Empirical Methods on Natural Language Process-
ing (EMNLP), pages 973?981.
Thamar Solorio and Yang Liu. 2008b. Part-of-speech
tagging for English-Spanish code-switched text. In
Proceedings of the Empirical Methods on Natural
Language Processing (EMNLP), pages 1051?1060.
Thamar Solorio, Elizabeth Blair, Suraj Maharjan, Steve
Bethard, Mona Diab, Mahmoud Gonheim, Abdelati
Hawwari, Fahad AlGhamdi, Julia Hirshberg, Alison
Chang, and Pascale Fung. 2014. Overview for the
first shared task on language identifiation in code-
switched data. In Proceedings of the First Workshop
on Computational Approaches to Code-Switching.
Conferencfe on Empirical Methods in Natural Lan-
guage Processing.
79
Proceedings of The First Workshop on Computational Approaches to Code Switching, pages 116?126,
October 25, 2014, Doha, Qatar. c?2014 Association for Computational Linguistics
?I am borrowing ya mixing ?? 
An Analysis of English-Hindi Code Mixing in Facebook 
 
 
Kalika Bali          Jatin Sharma           Monojit Choudhury 
Microsoft Research Lab India 
{kalikab,jatin.sharma,monojitc}@microsoft.com 
 
Yogarshi Vyas* 
University of Maryland  
yogarshi@cs.umd.edu 
 
Abstract1 
Code-Mixing is a frequently observed 
phenomenon in social media content gen-
erated by multi-lingual users. The pro-
cessing of such data for linguistic analysis 
as well as computational modelling is 
challenging due to the linguistic complex-
ity resulting from the nature of the mixing 
as well as the presence of non-standard 
variations in spellings and grammar, and 
transliteration. Our analysis shows the ex-
tent of Code-Mixing in English-Hindi 
data. The classification of Code-Mixed 
words based on frequency and linguistic 
typology underline the fact that while 
there are easily identifiable cases of bor-
rowing and mixing at the two ends, a large 
majority of the words form a continuum in 
the middle, emphasizing the need to han-
dle these at different levels for automatic 
processing of the data. 
1 Introduction 
The past decade has seen an explosion of Com-
puter Mediated Communication (CMC) world-
wide (Herring 2003). CMC provides users with 
multiple options, both asynchronous and synchro-
nous, like email, chat, and more recently, social 
media like Facebook and Twitter (Isharayanti et al 
2009, Paolillo 2011). This form of communica-
tion raises interesting questions on language use 
across these media. Language use in CMC lies 
somewhere in between spoken and written forms 
                                                 
1 This work was done during the author?s internship at Mi-
crosoft Research Lab India.  
of a language, and tend to use simple shorter con-
structions, contractions, and phrasal repetitions 
typical of speech (Dannett and Herring 2007) 
Such conversations, especially in social-media are 
also multi-party and multilingual, with switching 
between, and mixing of two or more languages, 
the choice of language-use being highly influ-
enced by the speakers and their communicative 
goals (Crystal 2001). 
Code-Switching and Code-Mixing are stable and 
well-studied linguistic phenomena of multilingual 
speech communities. Code-Switching is ?juxta-
position within the same speech exchange of pas-
sages of speech belonging to two different gram-
matical systems or sub-systems? (Gumperz 1982), 
and Code-Mixing refers to the embedding of lin-
guistic units such as phrases, words and mor-
phemes of one language into an utterance of an-
other language (Myers-Scotton 1993, 2002). 
Thus, Code-Switching is usually inter-sentences 
while Code-Mixing (CM) is an intra-sentential 
phenomenon. Linguists believe that there exists a 
continuum in the manner in which a lexical item 
transfers from one to another of two languages in 
contact (Myers-Scotton 2002, Thomason 2003). 
Example (1) below illustrates the phenomenon of 
Code-Switching, while (2) shows Code-Mixing. 
 
(1) I was going for a movie yesterday. raaste 
men mujhe Sudha mil gayi.  
Gloss: [I was going for a movie yesterday.] 
way in I Sudha meet went 
Translation: I was going for a movie yester-
day; I met Sudha on the way. 
116
 (2) Main kal movie dekhne jaa rahi thi and 
raaste me I met Sudha. 
Gloss: I yesterday [movie] to-see go Contin-
uous-marker was [and] way in [I met] Sudha.  
Translation: I was going for a movie yester-
day and on the way I met Sudha. 
 
The main view held by linguists being that a lexi-
cal item goes from being used as a foreign word 
to a valid loanword indistinguishable from the na-
tive vocabulary by virtue of repeated use and 
adoption of morpho-syntactic features of the re-
cipient language (Auer 1984). However, in the 
case of single words, most scholars agree that it is 
difficult to determine whether or not a word is a 
?bona fide loanword/borrowing? or an instance of 
nonce borrowing2 or CM (Alex 2008, Bentahila 
and Davies, 1991, Field 2002, Myers-Scotton 
2002, Winford 2003). In this study, we only con-
sider Code-mixing examples, i.e., intra-sentential 
embedding of a language in another language. 
Processing such language data is challenging 
from the perspective of linguistic understanding 
vis-?-vis discourse and conversational analysis, as 
well as computational modelling and applications 
to Machine Translation, Information Retrieval 
and Natural Interfaces. Especially, in the case of 
social-media content where there are added com-
plications due to contractions, non-standard spell-
ings, and ungrammatical constructions as well as 
mixing of scripts. Many languages that use non-
Roman scripts, like Hindi, Bangla, Chinese, Ara-
bic etc., are often represented using Roman trans-
literations (Virga and Khudanpur 2003, Sowmya 
et al 2010). This poses additional challenges of ac-
curately identifying and separating the two lan-
guages. Further, it is often difficult to disambigu-
ate a borrowing as a valid native vocabulary from 
a mixing of a second language when dealing with 
single words. An understanding of the nature of 
mixing in such data is one of the first steps to-
wards processing this data and hence, making a 
more natural interaction in CMC a real possibility. 
                                                 
2 Nonce-borrowings are typically borrowings that do 
not necessarily follow any phonological, morpho-syn-
tactic or sociolinguistic constraints on their assimila-
tion into the host language (Poplack et al 1988). How-
ever, it is not clear if this is always a defining feature 
In this paper, we analyze social media content 
from English-Hindi (En-Hin) bilingual users to 
better understand CM in such data. We look at the 
extent of CM in both Hindi embedding in English, 
as well as English in Hindi. Our analysis of the 
type of CM in this context based on frequency of 
use and linguistic typology helps further an under-
standing of the different kinds of CM employed 
by users and emphasizes the need to tackle these 
at different levels. 
Facebook 
Page 
No. of 
likes 
No. of 
posts col-
lected 
No. of 
comments 
collected 
Amitabh 
Bachchan 
12,674,509 5 3364 
BBC 
Hindi 
1,876,306 18 240 
Narendra 
Modi 
15,150,669 15 2779 
Shahrukh 
Khan 
8,699,146 2 600 
Total  40 6983 
 
Table 1: Facebook Data Source  
2 Corpus Creation and Annotation 
For the creation of corpus for studying En-Hin 
CM, data from public Facebook pages in which 
En-Hin bilinguals are highly active was consid-
ered appropriate. Hence, we chose the Facebook 
pages of three Indian public figures, two promi-
nent Bollywood stars viz, Amitabh Bachchan and 
Shahrukh Khan, and the then-PM-elect Narendra 
Modi. We also collected data from the BBC Hindi 
News page. The assumption was that Bollywood, 
politics and news being three very popular areas 
of interest for Indians, we would see a lot of activ-
ity from the community on these pages. A total of 
40 posts from Oct 22- 28, 2013 were manually 
collected and preference was given to posts hav-
ing a long (50+) thread of comments. This is be-
cause CM and non-standard use of language is 
more frequent in comments. In the rest of the pa-
per, we shall use the term posts to cover both com-
ments and posts. The data was semi-automatically 
cleaned and formatted, removing user names for 
privacy. The names of public figures in the posts 
were retained. The final corpus consisted of 6983 
between established loanwords and nonce-borrowing, 
the line between them being extremely tenuous 
(Sankoff et al, 1990) 
117
posts and 113,578 words. Table 1 shows the data 
source statistics.  
While a number of posts were in the Devanagari 
script, the largest representation was that of Ro-
man script. A small number of posts were found 
in the script of other Indian languages like Bangla, 
Telugu etc. Tables 2 (a) and (b) show the distribu-
tion of posts and words by script 
 
 
Facebook 
Page 
Deva-
nagari 
Roman Mixed 
Script 
Other 
Script 
Amitabh 
Bachhcan 
73 3168 112 16 
BBC 
Hindi 
56 175 27 0 
Narendra 
Modi 
77 2633 84 11 
Shahrukh 
Khan 
0 578 23 1 
 
Table 2 (a): Script used for Posts 
 
 
Facebook 
Page 
Deva-
nagari 
Roman Other 
Script 
Symbols 
Amitabh 
Bachhcan 
2661 38144 439 1768 
BBC 
Hindi 
5225 4265 23 160 
Narendra 
Modi 
9509 43,804 217 1470 
Shahrukh 
Khan 
0 5,514 105 274 
 
Table 2(b): Script used for Words 
2.1 Annotation 
As a first step towards analysis, it is imperative 
that an annotation scheme be arrived at that cap-
tures the richness, diversity and uniqueness of the 
data. Any analysis of code-mixed CMC language-
use requires inputs at social, contextual, and dif-
ferent linguistic and meta-linguistic levels that op-
erate on various sub-parts of the conversation. 
This would help label not only the structural lin-
guistics phenomena such as POS tagging, 
Chunks, Phrases, Semantic Roles etc.  but also the 
various socio-pragmatic contexts (User de-
mographics, Communicative intent, Polarity etc.). 
However, an initial attempt at such a rich, layered 
annotation proved the task to be immensely re-
source intensive. Hence, for the initial analysis the 
annotation scheme was scaled down to four la-
bels: 
Matrix: Myers Scotton?s (1993) framework, CM 
occurs where one language provides the morpho-
syntactic frame into which a second language in-
serts words and phrases. The former is termed as 
the Matrix while the latter is called Embedding. 
Usually, matrix language can be assigned to 
clauses and sentences. 
Following this framework, the annotator was 
asked to split all posts into contiguous fragments 
of words such that each fragment has a unique ma-
trix language (En or Hin) 
Word Origin: Every embedded word is marked 
for its origin (En or Hin) depending on whether 
the source language was English or Hindi. A word 
from a language other than English or Hindi was 
marked as Other (Ot). It was assumed that the un-
marked words within a matrix language origi-
nated in that language. In our data we did not find 
examples of sub-lexical CM. For example an Eng-
lish word with Hindi inflection like computeron 
(???????????) were the English word ?computer? is 
inflected by the Hindi plural marker ?on. How-
ever, this can be a possible occurrence in En-Hin 
CM and needs to be marked as such. 
Normalization: Whenever a word in its native 
script uses a non-standard spelling (including con-
tractions) it is marked with its correct spellings. 
For transliterations of Hindi in Roman script, the 
word is marked with the correct spelling in Deva-
nagari script.  
POS tagging: Each word is labelled with its POS 
tag following the Universal Tagset proposed by 
Petrov et al (2011). This tagset uses 12 high-level 
tags for main POS classes. While, this tagset is not 
good at capturing granularity at a deeper level, we 
chose this because of a) its applicability to both 
English and Hindi doing away with the need for 
any mapping of labels between the two languages, 
and b) the small size of the corpus posed serious 
doubts on the usefulness of a more granular tagset 
for any analysis. 
The POS tags were decided on the basis of the 
function of the word in a context rather than a de-
contextualized absolute word class. This was done 
because often in the case of embedded words, the 
lexical category of the original language is com-
pletely lost and it is the function of the word in the 
matrix language that applies and assumes im-
portance. 
Named Entities: Named Entities (NE) are per-
haps the most common and amongst the first to 
form the borrowed or mixed vocabulary in CM. 
As the Universal Tagset did not have a separate 
118
category for NEs, we chose to label and classify 
them as people, locations and organizations. It is 
important to remember that while NEs are perhaps 
the most frequent ?borrowings? the notion of 
Word Origin in the context of CM is debatable. 
However, these need to be analyzed and processed 
separately for any NLP application. 
1062 posts consisting of 1071 words were ran-
domly selected and annotated by a linguist who is 
a native speaker of Hindi and proficient in Eng-
lish. Non-overlapping subsets of the annotations 
were then reviewed and corrected by two expert 
linguists.  
The two annotated examples from the corpus of 
En in Hin Matrix and Hin in En Matrix are given 
below: 
 
 
<s> 
 <matrix name="Hindi"> 
love_NOUN/E affection_NOUN/E le-
kar_VERB/??? ??  ?
salose_NOUN=saalon/?????? ??  ?
sunday_NOUN/E ke_ADP/???  ?
din_NOUN/????  ?chali_VERB/ ????  ?aar-
ahi_VERB/?? ???  ?divine_ADJ/E param-
para_NOUN/???????? ko_ADP/ ???  ?
age_NOUN=aage/?????badhha_VERB/????  ?
rahe_VERB/????  ?ho_VERB/???  ?
 </matrix> 
</s> 
Translation: The divine tradition that (you) have 
been carrying forward every Sunday with love 
and affection.  
 
<s> 
<matrix name="English"> 
 sir_NOUN u_PRON=you r_VERB=are 
blessed_VERB by_ADP entire_ADJ brah-
mand_NOUN/H???????????  ?
 </matrix> 
</s> 
Translation: Sir, you are blessed by the entire 
Universe. 
 
It was observed that a large chunk of data con-
sisted of short posts typically a greeting or a eu-
logy from a fan of the public figures and were un-
interesting from a structural linguistic analysis of 
CM. Thus, all such posts (consisting of 5 or less 
words) were deleted from the corpus and the re-
maining corpus of 381 posts and 4135 words was 
used for further analysis. 
3 An Analysis of Code Mixed Data 
The annotated data consists of 398 Hin sentences, 
698 En and 6 Ot in a single language. 45 posts 
show at least one switch in matrix between En and 
Hin. Thus, at least 4.2% of the data is Code-
Switched. It should be noted however that this is 
matrix switching within an utterance. If we con-
sider Code-Switching at a global level to include 
switching from one language to another within a 
conversation thread then all the threads in the data 
show code-switching as they contain utterances 
from both English and Hindi.  
Looking at the 398 Hindi matrices, we find that 
23.7% of them show at least one En embedding as 
compared to only 7.2% of the En matrices with 
Hin embedding. In total 17.2% of all posts which 
consist of nearly a quarter of all words in the data 
show some amount of CM. 
 If we look at the number of points in a single ma-
trix where embedding happens, we find that in 
86% of  the En matrices, Hin embeddings appear 
only once or twice. En embeddings in Hin matrix 
is not only twice as more frequent, but can occur 
more often in a single matrix (more than 3 times 
in at least 10% of the cases). Table 3 shows the 
distribution of CM points for both the cases. 
 
# of points Hin in En En in Hin 
1 11 (36.66%) 19 (31.15%) 
2 15 (50%) 28 (45.9%) 
3 2   (6.67%) 2   (3.28%) 
4 2   (6.67%) 9   (5.49%) 
5 0 2   (3.28%) 
6 0 1   (1.64%) 
Total 30 61 
 
Table 3: Distribution of CM points 
 
 
 
Table 4: Distribution of NE by Type 
 
As expected, NEs are common in the corpus and 
there are a total of 233 NEs in 406 matrices (322 
of 4134 words). The distribution of NEs by sub-
classes is given in Table 4. 
Table 5 shows the distribution of the various POS 
in the entire corpus, as well as for the embedded 
words. Nouns do form the largest class of words 
NE Type Person 159 
NE Type Location 39 
NE Type Organization 35 
Total NE 233 
119
overall as well as for Hin as well as En embed-
ding. In fact, for Hin in English matrix, there are 
only two instances of words which are not Nouns. 
Table 5 shows the distribution of POS for Hin in 
En matrix, and En in Hin matrix 
Looking at these top-level distributions we can 
observe that though there are some similarities be-
tween the patterns of CM for Hin in English and 
En in Hindi matrices (the high frequency of 
nouns, for instance), they both exhibit distinct pat-
terns in terms of how often CM occurs as well as 
in the prevalence of POS other than Nouns. In 
Section 3.1 and 3.2 we will look at both these L1 
embedding in L2 matrix individually in more de-
tail. 
3.1 Hindi words in English matrix 
As mentioned above, most of the Hin embedding 
in En (32 out of 33) matrices are Nouns. The ex-
ception is variation of the particle ?ji? used as an 
honorific marker in Hindi. The particle is used to 
denote respect and occurs in formulaic expression 
of the kind <(name/address form)> ji as in: 
 
?Amit ji, I am your fan and have seen all your 
movies? 
 
A closer look at the embedded Hin Nouns shows 
that a large number of them are actually part of 
multi-word Named Entities which do not fall un-
der the categories defined in the annotation guide-
lines. Almost all of them also function as regular 
Nouns or Verbs in Hindi. For example, the word 
?hunkaar? (a roar) is not an NE, however its use 
in the following sentence, where it is used to de 
note the name of a particular rally (event) can be 
viewed as an NE. 
 
?hunkar rally will be held tomorrow? 
 
Similarly, the word ?yaatraa? in Hindi means 
journey whereas its use in the phrase ?Kerala 
yaatraa? is specific to a tour of Kerala. 
 
There are some instances of nonce-borrowing or 
CM where Hindi Nouns are not used as a part of a 
potential NE or formulaic expressions. For exam-
ple, in the following sentence: 
 
??and the party workers (will) come with me 
without virodh? 
 
The Hindi word ?virodh? is used instead of the 
English alternative ?protest? or ?objection?. It can 
only be assumed that the user did this for sociolin-
guistic or pragmatic reasons to emphasize or hu-
mour. 
 
Kinship terms form another domain of frequent 
embedding of Hin in En. Hindi has a more com-
plex system of kinship terms where not only are 
there finer distinctions maintained between mater-
nal and paternal relations but also kinship terms 
are used to address older (and hence) respectable 
people. Thus, we find the use of ?chacha? (fa-
ther?s younger brother), ?bhaiya? (elder brother) 
as well as ?baapu? (father) used frequently in the 
data as address forms. 
3.2 English words in Hindi matrix 
There is a far greater use of English words in 
Hindi matrices both as single words as well as 
multi-word expressions. A total of 116 unique 
Hindi words are found embedded in En matrices 
of which 76 are single word embedding and the 
rest are a part of 16 multi-word expressions. 
While Nouns continue to dominate the POS class 
of the Hindi embedding as well, there is far more 
variations in the type of CM that seems to be hap-
pening in this case. 
3.2.1 Single Word Embedding 
As in the case of English embedding (3.1) we find 
a number of Hindi Noun embedding to be of kin-
ship terms, greetings and other address form. 
POS 
Tag 
Over-
all 
En in Hin 
matrix* 
Hin in  En 
matrix* 
NOUN 1260 77 32 
VERB 856 8  
PRON 499 4  
ADP 445 0  
ADJ 302 16  
PRT 241 4 1 
DET 141 2  
. 125 NA  
ADV 104 3  
CNJ 98 2  
NUM 46 0  
X 18 0  
Total 4135   
Table 5: POS distribution for the Annotated 
Corpus.  
* Overall distribution is given at token level 
whereas the embedding En in Hin matrix, and 
Hin in E matrix are at Unique Word level. 
 
120
Words like, ?sir?, ?uncle?, ?hello?, ?good morn-
ing? etc are used frequently to start or end a par-
ticular turn.  
A fraction of Nouns are genuine borrowings into 
the language is no Hindi equivalent for that 
word/concept. Common examples are words like 
?goal? and ?bomb? which may be considered a 
part of the Hindi vocabulary. What is interesting 
is that users? variations in spellings these words 
either in English (?goal?, ?bomb?) or in equiva-
lent Hindi transliteration (?gol?, ?bam?). This 
may be taken as an indication that the user is not 
actively conscious of using an English word. 
However, there are a fairly large number of Nouns 
as single words where this is not applicable as in: 
 
?agar aap BJP ke follower hain to is page ko like 
karen? 
 
(If you are a BJP follower then like this page) 
 
where there are frequently used Hindi equivalents 
but the user seems to be following certain conven-
tions on Facebook (?page? and ?like?) or is mix-
ing for other purposes (?follower?) 
 
Single adjectives are not as common and when 
used are mostly intensifiers such as ?very? or 
?best? etc. There are some instances of adjectives 
as nonce-borrowings such as in the following ex-
ample: 
 
??divine paramparaa ko aage?? 
(?(taking the) divine tradition forward?) 
 
Single verb embedding of En words are always of 
the form V + kar in the data. The verb karnaa (?to 
do?) in Hindi is used to form conjunctives in 
Hindi. Thus, we have a number of Hindi phrases 
of the type: kaam karnaa  ? work to do? (to work), 
and a closer look at the English Verbs embedded 
in Hindi shows that most of these are actually in 
their nominalized form, such as ? driving kar-
naa?, or as a V + V conjunct such as ?admit kar-
naa?. 
There are fewer instances of other POS classes, 
however, one interesting case is the use of con-
juncts like ?but? and ?and? to join two Hindi 
clauses as in: 
 
?main to gayi thi but wo wahaan nahi thaa? 
(I had gone but he wasn?t there) 
 
3.2.2 Multi Word Embedding 
Multi word expressions in English used in a Hindi 
matrix range from standard formulaic expressions 
to clause or phrase insertion. Other than standard 
greetings, these formulaic (or frozen) expression 
may work as Named Entities or Nominal com-
pounds as in the case of  ?Film star?, ?Cricket 
player?, ?Health minister?, ?Educational Insti-
tutes? and ?Participation Certificate?. There are 
also other expressions that border on formulaic in 
English but which nevertheless have an ambigu-
ous status within Hindi, such as, ?love and affec-
tion?. Another example of such a case of MW em-
bedding is: 
 
?Befitting reply to mere papa ne maaraa? 
 
(my father gave a befitting reply) 
 
Here, while ?befitting reply? is not really a formu-
laic expression in Hindi, the user is clearly using 
it as such with the use of  the  emphatic to and the 
use of the verb maaraa (?hit?) instead of  diyaa 
(?gave?) 
 
Clause or phrase level mixing, though less fre-
quent can also be found in the data. For example,  
 
?Those who support the opposition kabhi Mu-
zaffarnagar aa kar dekho? 
 
(Those who support the opposition should come 
to Muzaffarnagar and see (for themselves)) 
 
This is a classic case of CM where both the 
phrases retain the grammatical structure of the 
language concerned. 
 
As can be seen from the analysis of the annotated 
corpus above, Code-Mixing if understood as the 
insertion of words from a language into the gram-
matical structure of another, can show a wide var-
iation in its structural linguistic manifestation.  
4 Borrowing ya Mixing? 
In linguistic literature on ?other language embed-
ding? there has been a long-standing debate on 
what is true Code-mixing, what is nonce-word 
borrowing, and what are  ?loanwords? that 
are integrated into the native vocabulary and 
grammatical structure (Bentahila and Davies,  
1991, Field 2002, Myers-Scotton 2002, Winford 
2003, Poplack and Dion 2012). Many linguists be-
lieve that loan-words start out as a CM or Nonce-
121
borrowing but by repeated use and diffusion 
across the language they gradually convert to na-
tive vocabulary and acquire the characteristics of 
the ?borrowing? language (see Alex (2008) for a 
discussion). Normally, they look at spoken forms 
to see phonological convergence and inflections 
for morpho-syntactic convergence. However, as 
pointed out by Poplack and Dion (2012) the prob-
lem with this is that in many cases a native ?ac-
cent? might be mistaken for phonological conver-
gence, and a morpho-syntactic marking might not 
be readily visible. For example, most Hindi speak-
ers of English would pronounce an English alve-
olar /d/ as a retroflex because an alveolar plosive 
is not a part of the Hindi phonology. However, 
this does not imply that the said English word has 
become a part of the native vocabulary. Similarly, 
if we look at the two sentences: 
 
?sab artists ko bulayaa hai?  
(all artists have been called), 
 
and 
 
?sab artist kal aayenge? 
(all artists will come tomorrow) 
 
In the first sentence the English inflection ?s on 
the word artist marks it as plural but in the second 
case, the plural is marked on the Hindi Verb. Does 
this imply that in the first case it is CM and in the 
second a case of borrowing given that both the 
forms and the structures are equally acceptable 
and common in Hindi?  
Many studies (Mysken 2000, Gardner-Chloros. 
2009, Poplack and Dion 2012 etc.) thus point out 
that it is not easy to decide these categories espe-
cially for single words without looking at dia-
chronic data and the inherent fuzziness of the dis-
tinction itself. In general, it is believed that there 
exists a sort of continuum between CM and loan 
vocabulary where the edges might be clearly dis-
tinguishable but it is difficult to disambiguate the 
vast majority in the middle especially for single 
words.  
As we have seen in the preceding Section CM of 
Hin in English matrix mainly follows a very dis-
tinct pattern of using NEs (and functional NEs) 
and formulaic expressions. However, in the case 
of En in Hindi CM, there is a far wider variation 
and it could be difficult in many instances to de-
cide by just looking at the data whether a certain 
embedding is a borrowing or CM. 
One way to make a distinction between a borrow-
ing and CM could be to look at the diffusion of the 
word in the native language. Borrowed words of-
ten appear in monolingual usage long before dic-
tionaries and lexicons adopt them as native vocab-
ulary. Thus, to judge the diffusion of an English 
word one would have to look at the frequency of 
its use in suitable monolingual context such as 
news wire data, chat logs or telephone conversa-
tions.  
For a further analysis of En embedding in Hin 
matrix in our data, we decided to check their fre-
quency based diffusion in a monolingual new cor-
pus of Hindi. For this purpose we took a corpus of 
51,277,891words from Dainik Jagaran 
(http://www.jagran.com/), a popular daily news-
paper in Hindi, and created a frequency count of 
the 230,116 unique words in it. News corpora are 
a reasonable choice for monolingual frequencies 
as code-mixing is relatively rare and frowned 
upon in news unless it refers to a named entity or 
is a part of a direct quote. We then mapped com-
mon Hindi equivalents of all the English words 
used in the corpora. Finally, we checked the fre-
quency of both the English embedding as well as 
their corresponding Hindi equivalents. As men-
tioned before, a number of English words do not 
have Hindi equivalents and for these words we ex-
pect the English words themselves to have a high 
frequency count in the corpus. 
An analysis of the results thus obtained shows 
that the English words do indeed fall into two dis-
tinct buckets at the edges. Thus, for words such as 
?party? (as in ?political party?), ?vote?, ?team? 
we find that not only are the word counts quite 
high (over 67K for ?party? and over 18k for 
?vote? and ?team?) but the counts for the equiva-
lent Hindi forms are relatively low. Similarly, 
words like ?affection?, ?driving?, ?easily? etc. 
were not found in the corpus, while their Hindi 
equivalents had relatively medium to high counts. 
However, there is a large number of words in the 
middle where both the English and the Hindi 
equivalents have a comparative count or the dif-
ference is not significant. For these words it is dif-
ficult to decide whether they ought to be classified 
as borrowing or CM.  
Let us denote the frequency of an En word as fe 
and that of its Hin synonym as fh. Let ? be an ar-
bitrary margin > 0. The aforementioned intuition 
about the nature of CM and borrowing can be for-
malized as follows:  
? If for a given word log(fh/fe)> ?, we call it 
CM  
122
?  If for a given word log(fh/fe) < -?, we call 
it a borrowing. 
? If -? ? log(fh/fe) ? ?, it is not possible to 
decide between the two cases, and hence 
we call the word ambiguous. 
Figure 1 shows the scatter plot of the frequency of 
all the En words that occur within Hin matrix (119 
in total) in the Dainik Jagaran data (x-axis) against 
the frequency of its Hindi synonym (y-axis) in the 
same corpus. Since frequency follows Zipfs law, 
the axes are in log-scale. The words, which are 
represented by dots in Figure 1, are scattered all 
over the plot without any discernable pattern. This 
indicates that there are no distinct classes of words 
that can be called borrowings or mixing; rather, it 
is a continuum. If we assume ? to be 1, an arbi-
trary value, we can divide the plot into three zones 
using the three rules proposed above. These 
zones, bounded by the blue lines are shown in Fig-
ure 1: Mixing ? words that are code-mixed (top-
left triangle), borrowings (bottom-right triangle) 
and ambiguous (the narrow zone running diago-
nally between the two with a width of 2?. 
However, we observe that some En words which 
has very high frequency in our corpus (e.g., vote, 
party, team), are classified as ambiguous because 
their Hin synonyms have a comparable high fre-
quency as well. To a native speaker of Hindi, 
these words are clearly borrowings and used even 
in formal Hin text. In fact, it seems reasonable to 
declare an En word as a borrowing solely on the 
basis of its very high frequency in the monolin-
gual corpus. We could choose another arbitrary 
threshold ? = 1000, such that a word is declared 
as a borrowing if the following two conditions are 
satisfied: 
? -? ? log(fh/fe) ? ?  
? fe > ? 
Note that the choice of ? should also depend on 
the size of the corpus. Table 6 reports the number 
of CM in the data with and without applying the 
large frequency rule. We see that the number of 
CM words is the highest followed by ambiguous 
words. This clearly indicates that CM is a very 
common phenomenon on social media. Appendix 
A lists all the En words and their classes.  
 Using arbitrary thresholds, ? and ?, to classify the 
words into three distinct set is a convenient tool to 
deal with code-mixing; but it ignores the fact that 
in reality it is not possible to classify words into a 
few distinct categories. There is always a contin-
uum between borrowing and mixing. Figure 1 
shows a more appropriate gradient based visuali-
zation of the space. Words falling on the darker 
regions of this plot are more likely to be borrow-
ing. The gradients reflect the two equations dis-
cussed above. The darkness linearly increases 
with log(fe) and decreases with log(fh/fe). The 
overall darkness is a simple linear combination of 
these two independent factors. Note that this for-
mulation is only for a visualization purpose, and 
should not be interpreted as some formal proba-
bility or measure of ?borrowing-ness? of a word. 
 
 
 
Figure 1: Plot of the frequencies of En words em-
bedded in Hin matrix (x-axis) and their Hin syno-
nyms (y-axis) in the Dainik Jagaran corpus. 
 
 CM Ambigu-
ous 
Borrowing 
w/o ?-Rule 69 39 11 
w/ ?-Rule 69 31 19 
 
Table 6: Classification of embedded En words 
into three classes for ? =1. 
 
A note on synonym selection: Which syno-
nym(s) of an En word should be considered for 
CM vs. borrowing analysis is a difficult question. 
First, a word can have many senses. E.g., the word 
party can mean a political party, a group of peo-
ple, or a social gathering, and also a verb ? to par-
ticipate in a social gathering. Each of these senses 
can be translated in, often more than one ways. 
E.g., dala in the sense of political party, 
anusThANa or dAwata in the sense of social gath-
ering, etc. To complicate the situation further, 
these Hindi words can have many senses as well 
(e.g., the word dala can mean a sports team, or a 
political party or group of people or animals).  
Thus, when we compare synonyms without 
context, we cannot be sure in which sense the 
123
words are used and therefore, the frequency 
counts maybe misleading. A second problem arise 
with phrase embedding. While an entire phrase 
can be borrowed, its words may not be (e.g., clean 
chit  -Indian version of the English expression 
?clean sheet?- is a borrowed expression in Hindi, 
but clean is not). However, we had access to only 
the wordlist and word frequencies, which made it 
impossible to disentangle such effects. Compar-
ing contexts automatically deciphering word 
sense is a complex problem in itself. For this 
work, we used an En to Hin lexicon 
(http://shabdkosh.raftaar.in/) to find out the syno-
nyms, and for every synonym extracted the fre-
quency from the wordlist, and deemed the highest 
frequency as the fh for the word. A more thorough 
synonym selection using context and phrase level 
analysis would be an interesting extension of this 
work. 
4.1 Ambiguous Words 
The words classified as ambiguous pose a prob-
lem as we do not know whether these words are 
in the process of being borrowed, or are working 
as near-synonym of the Hindi equivalent, or are 
CMs where the intention of the user is the motiva-
tion for the ?other language? use.  
Poplack and Dion (2012) are of the view that there 
does not exist a continuum between CM, Nonce-
borrowing and loanwords. In their diachronic 
study on En-French CM, the authors show that the 
frequency of all three categories remain stable. 
According to them, a user is always aware 
whether they are using an ?other language? word 
as a CM (for socio-linguistic purposes) or as a so-
cio-linguistically unmarked borrowing. Our data 
does not capture diachronic statistics neither does 
our monolingual corpus is at the scale at which 
language changes occur. However, we interpret 
our results to indicate that there is indeed a fuzzy 
boundary between CM and borrowing. Neverthe-
less, this distinction may not be readily observable 
through word classification or even diffusion 
and/or other structural linguistic features. The no-
tion of ?social acceptance? of a particular word in 
that language community may play a big role. 
Further, the perception of a word as either CM, 
or borrowing could depend on a large number of 
meta- and extra-linguistic factors that may include 
including the fluency of the user in English, famil-
iarity with the word, and the pragmatic/dis-
course/socio-linguistics reasons for using them. 
Thus, for a true bilingual, fluent in both lan-
guages, an adverb like ?easily? might be more sta-
ble and almost a borrowing, but for someone with 
less familiarity with English, it might be a mixing.  
Similarly, whether or not a person is consciously 
using the English word to make a point can matter. 
A frequent example of this in our data is the use 
of swear words and expletives which are often ac-
companied by a switch in language. These words 
thus are difficult to disambiguate without more in-
formation and data, and an analysis that takes into 
account the non-structural linguistic motivations.  
5 Conclusion 
In this paper, we present an analysis of data from 
Facebook generated by En-Hin bilingual users. 
Our analysis shows that a significant amount of 
this data shows Code Mixing in the form of En in 
Hindi matrix as well as Hin in English matrix. 
While the embedding of Hindi words in English 
mostly follows formulaic patterns of Nouns and 
Particles, the mixing of English in Hindi is clearly 
happening at different levels, and is of different 
types. This can range from single words to multi-
word phrases ranging from frozen expressions to 
clauses. Considering monolingual corpus fre-
quency counts clearly shows that the words them-
selves fall into three categories of clear CM, clear 
Borrowings and Ambiguous where the distinction 
becomes fuzzy. The problem is amplified because 
in transliterated text, even the borrowings are 
mostly in English spellings and sometimes Hindi 
spellings (goal vs gol), and will be identified as 
English words. From an NLP perspective, all 
these have to be handled differently. Some are 
easier to handle (?party? would be in a Hindi lex-
icon, for example, and NEs) and some are more 
difficult for example where Adverbials or clauses 
are involved. 
The insights from this analysis indicate that any 
future work on CM in social media content would 
have to involve a deeper analysis at the intersec-
tion of structural and discourse linguistics. We 
plan to continue our work in this area in the future 
with focus on larger data sets, richer annotations 
which take into account not only structural lin-
guistics annotation but also discourse and prag-
matic level annotations. We believe that an under-
standing of the interaction between morpho-syn-
tax and discourse, and a deeper look at sociolin-
guistic context of the interaction in the future will 
help us to better define and understand this phe-
nomenon and hence, implement suitable NLP 
techniques for processing such data. 
 
 
 
124
Appendix A 
List of English words embedded in Hindi matrix 
found in our data, classified into three classes for 
? = 1 and ? = 1000. 
Code-mixed words: health, public, army, India, 
affection, divine, pm, drama, clean, anti, 
young, follower, page, like, request, easily, In-
dian, uncle, comment, reply, sun, bomb, means, 
game, month, spokesperson, actor, I, word, ad-
mit, good, afternoon, time, look, please, help, 
husband, artists, very, sad, but, higher, plan-
ning, mad, keep, failure, well, strike, sorry, 
girlfriend, those, who, support, opposition, 
and, profile, right, good, men, driving, lady, 
leader, singer, shift, culture, only, with, befit-
ting, reply 
Ambiguous words: blast, daily, love, sir, bloody, 
cheapo, chit, hello, it, football, style, pant, hi, 
commonwealth, participation, certificates, ed-
ucation, robot, Bollywood, player, big, bee, the, 
agency, women, line, trolling, ODI, tiger, com-
edy 
Borrowings: CBI, goal, rally, match, police, film, 
cricket, appeal, Italian, fan, best, vote, party, 
power, minister, team, you, photo, star 
Reference 
Beatrice Alex. 2008.  Automatic Detection of Eng-
lish Inclusions in Mixed-lingual Data with an 
Application to Parsing, Doctor of Philosophy 
Thesis, School of Informatics, University of 
Edinburgh, Edinburgh, UK. 
Celso Alvarez-C?ccamo. 2011. "Rethinking con-
versational code-switching: codes, speech vari-
eties, and contextualization." Proceedings of 
the Annual Meeting of the Berkeley Linguistics 
Society. Vol. 16. 
Peter Auer. 1984. The Pragmatics of Code-
Switching: A Sequential Approach. Cambridge 
University Press. 
Abdelali Bentahila and Eirlys E. Davies. 1991. 
"Constraints on code-switching: A look beyond 
grammar. Papers for the symposium on code-
switching in blingual studies: Theory, signifi-
cance and perspectives. Strasbourg: European 
Science Foundation.  
MS Cardenas-Claros and N Isharyanti. 2009. 
Code-switching and code-mixing in internet 
chatting: Between yes, ya, and si- a case study. 
In The JALT CALL Journal, 5 
David Crystal. 2001. Language and the Internet. 
Cambridge University Press. 
B. Danet and S. Herring. 2007. The Multilingual 
Internet: Language, Culture, and Communica-
tion Online. Oxford University Press, New 
York. 
Frederic Field. 2002. Linguistic borrowing in bi-
lingual contexts. Amsterdam: Benjamins. 
Penelope Gardner-Chloros. 2009. Code-Switch-
ing. Cambrudge University Press 
J. Gumperz. 1964. Hindi-Punjabi code-switching 
in Delhi. In Proceedings of the Ninth Interna-
tional Congress of Linguistics, Mouton: The 
Hague. 
J. Gumperz. 1982. Discourse Strategies. Oxford 
University Press. 
S. Herring. 2003. Media and Language Change: 
Special Issue. 
Jeff MacSwan. 2012." Code-Switching and 
Grammatical Theory." In The Handbook of Bi-
lingualism and Multilingualism (2012). 323. 
Carol Myers-Scotton. 1993. Duelling Languages: 
Grammatical Structure in Code-switching. 
Claredon. Oxford. 
Carol Myers-Scotton. 2002. Contact linguistics: 
Bilingual encounters and grammatical out-
comes. Oxford University Press. 
Pieter Muysken. 2000. Bilingual speech: A typol-
ogy of code-mixing. Cambridge University 
Press. 
John C. Paolillo. 2011. ?Conversational? 
codeswitching on Usenet and Internet Relay 
Chat. In Language@Internet, 8, article 3. 
Slav Petrov, Dipanjan Das, and Ryan McDonald. 
2011. A universal part-of-speech tagset. arXiv 
preprint arXiv:1104.2086  
Shana Poplack, D. Sankoff, and C. Miller. 1988. 
The social correlates and linguistic processes of 
lexical borrowing and assimilation. Linguistics 
26:47-104. 
Shana Poplack and Nathalie Dion. 2012. "Myths 
and facts about loanword development." in 
Language Variation and Change 24, 3. 
David Sankoff, Shana Poplack, and Swathi 
Vanniarajan. 1990. The case of the nonce loan 
in Tamil. Language Variation and Change, 2 
(1990), 71-101. Cambridge University Press. 
125
V.B. Sowmya, M. Choudhury, K. Bali, T. Das-
gupta, and A. Basu. 2010. Resource creation for 
training and transliteration systems for Indian 
languages. In Proceedings of Language Re-
source and Evaluations Conference (LREC 
2010). 
Sarah G. Thomason. 2003. Contact as a Source of 
Language Change. In R.D. Janda & B. D. Jo-
seph (eds), A handbook of historical linguistics, 
Oxford: Blackwell. 
Paola Virga and Sanjeev Khudanpur. 2003. 
Transliteration of proper names in cross-lingual 
information retrieval. Proceedings of the ACL 
2003 workshop on Multilingual and mixed-lan-
guage named entity recognition-Volume 15. 
Association for Computational Linguistics. 
Donald Winford. 2003. An Introduction to Con-
tact Linguistics. Malden, MA: Blackwell. 
 
 
 
 
 
 
126
