  
Bilingual-Dictionary Adaptation to Domains 
Hiroyuki Kaji 
Central Research Laboratory, Hitachi, Ltd. 
1-280 Higashi-Koigakubo, Kokubunji-shi, Tokyo 185-8601, Japan 
kaji@crl.hitachi.co.jp 
 
Abstract 
Two methods using comparable corpora to se-
lect translation equivalents appropriate to a do-
main were devised and evaluated. The first 
method ranks translation equivalents of a target 
word according to similarity of their contexts to 
that of the target word. The second method 
ranks translation equivalents according to the 
ratio of associated words that suggest them. An 
experiment using the EDR bilingual dictionary 
together with Wall Street Journal and Nihon 
Keizai Shimbun corpora proved that the method 
using the ratio of associated words outperforms 
the method based on contextual similarity. 
Namely, in a quantitative evaluation using 
pseudo words, the maximum F-measure of the 
former method was 86%, while that of the latter 
method was 82%. 
1 Introduction 
It is well known that appropriate translations for a 
word vary with domains, and bilingual-dictionary 
adaptation to domains is an effective way to improve 
the performance of, for example, machine translation 
and cross-language information retrieval. However, 
bilingual dictionaries have commonly been adapted 
to domains on the basis of lexicographers? intuition. 
It is thus desirable to develop an automated method 
for bilingual-dictionary adaptation. 
Technologies for extracting pairs of translation 
equivalents from parallel corpora have been estab-
lished (Gale and Church 1991; Dagan, et al 1993; 
Fung 1995; Kitamura and Matsumoto 1996; 
Melamed 1997). They can, naturally, be used to 
adapt a bilingual dictionary to domains, that is, to 
select corpus-relevant translation equivalents from 
among those provided by an existing bilingual dic-
tionary. However, their applicability is limited be-
cause of the limited availability of large parallel 
corpora. Methods of bilingual-dictionary adaptation 
using weakly comparable corpora, i.e., a pair of two 
language corpora of the same domain, are therefore 
required. 
There are a number of previous works related to 
bilingual-dictionary adaptation using comparable 
corpora. Tanaka and Iwasaki?s (1996) optimization 
method for a translation-probability matrix mainly 
aims at adapting a bilingual dictionary to domains. 
However, it is hampered by a huge amount of com-
putation, and was only demonstrated in a small-scale 
experiment. Several researchers have developed a 
contextual-similarity-based method for extracting 
pairs of translation equivalents (Kaji and Aizono 
1996; Fung and McKeown 1997; Fung and Yee 
1998; Rapp 1999). It is computationally efficient 
compared to Tanaka and Iwasaki?s method, but the 
precision of extracted translation equivalents is still 
not acceptable. 
In the light of these works, the author proposes 
two methods for bilingual-dictionary adaptation. The 
first one is a variant of the contex-
tual-similarity-based method for extracting pairs of 
translation equivalents; it focuses on selecting cor-
pus-relevant translation equivalents from among 
those provided by a bilingual dictionary. This select-
ing may be easier than finding new pairs of transla-
tion equivalents. The second one is a newly devised 
method using the ratio of associated words that sug-
gest each translation equivalent; it was inspired by a 
research on word-sense disambiguation using bilin-
gual comparable corpora (Kaji and Morimoto 2002). 
The two methods were evaluated and compared by 
using the EDR (Japan Electronic Dictionary Re-
search Institute) bilingual dictionary together with 
Wall Street Journal and Nihon Keizai Shimbun cor-
pora. 
2 Method based on contextual similarity 
This method is based on the assumption that a 
word in a language and its translation equivalent in 
another language occur in similar contexts, albeit 
their contexts are represented by words in their re-
spective languages. In the case of the present task 
(i.e., bilingual-dictionary adaptation), a bilingual 
dictionary provides a set of candidate translation 
equivalents for each target word1. The contextual 
similarity of each of the candidate translation 
equivalents to the target word is thus evaluated with 
the assistance of the bilingual dictionary, and a pre-
determined number of translation equivalents are 
selected in descending order of contextual similarity. 
Note that it is difficult to preset a threshold for con-
textual similarity since the distribution of contextual 
similarity values varies with target words. 
                                                           
1 In this paper, ?target word? is used to indicate the word for 
which translation equivalents are to be selected. 
  
A flow diagram of the proposed method is shown 
in Figure 1. The essential issues regarding this 
method are described in the following. 
Word associations are extracted by setting a 
threshold for mutual information between words in 
the same language. The mutual information of a pair 
of words is defined in terms of their co-occurrence 
frequency and respective occurrence frequencies 
(Church and Hanks 1990). A medium-sized window, 
i.e., a window including a few-dozen words, is used 
to count co-occurrence frequencies. Only word asso-
ciations consisting of content words are extracted. 
This is because function words neither have do-
main-dependent translation equivalents nor represent 
contexts. 
Both a target word and each of its candidate 
translation equivalents are characterized by context 
vectors. A context vector consists of associated 
words weighted with mutual information. 
Similarity of a candidate translation equivalent to 
a target word is defined as the cosine coefficient be-
tween the context vector characterizing the target 
word and the translated context vector characterizing 
the candidate translation equivalent as follows. Un-
der the assumption that target word x and candidate 
translation equivalent y are characterized by 
first-language context vector a(x) = (a1(x), a2(x), ?, 
am(x)) and second-language context vector b(y) = 
(b1(y), b2(y), ?, bn(y)), respectively, b(y) is translated 
into a first-language vector denoted as a'(y) = (a'1(y), 
a'2(y), ?, a'm(y)). That is, 
m),1,2,()()( ,n,1,2, LL =?= = iybmaxy'a jjiji ? , 
where ?i,j=1 if the j-th element of b(y) is a translation 
of the i-th element of a(x); otherwise, ?i,j=0. Ele-
ments of b(y) that cannot be translated into elements 
of a'(y) constitute a residual second-language vector, 
denoted as b'(y) = (b'1(y), b'2(y), ?, b'n(y)). That is, 
.jybyb'
m
i
jijj n),1,2,(
otherwise0
0)()( 1
, L
L
L =
??
??
? == ?= ?  
The similarity of candidate translation equivalent y 
to target word x is then defined as 
))()(),(()( yyxcosy,xSim b'a'a += . 
Note that a'(y)+b'(y) is a concatenation of a'(y) and 
b'(y) since they have no elements in common. 
3 Method using the ratio of associated 
words 
3.1 Outline 
This method is based on the assumption that each 
word associated with a target word suggests a spe-
cific sense of the target word, in other words, spe-
cific translation equivalents of the target word. It is 
also assumed that dominance of a translation 
equivalent in a domain correlates with how many 
associated words suggesting it occur in a corpus of 
the domain. It is thus necessary to identify which 
associated words suggest which translation equiva-
lents. This can be done by using the sense-vs.-clue 
correlation algorithm that the author developed for 
unsupervised word-sense disambiguation (Kaji and 
Morimoto 2002). The algorithm works with a set of 
senses of a target word, each of which is defined as a 
set of synonymous translation equivalents, and it 
results in a correlation matrix of senses vs. clues (i.e., 
associated words). It is used here with a set of trans-
lation equivalents instead of a set of senses, resulting 
in a correlation matrix of translation equivalents vs. 
associated words. 
The proposed method consists of the following 
steps (as shown in Figure 2). 
First, word associations are extracted from a cor-
pus of each language. The first step is the same as 
that of the contextual-similarity-based method de-
scribed in Section 2. 
Second, word associations are aligned translin-
gually by consulting a bilingual dictionary, and 
pairwise correlation between translation equivalents 
of a target word and its associated words is calcu-
lated iteratively. A detailed description of this step is 
given in the following subsection. 
Third, each associated word is assigned to the 
translation equivalent having the highest correlation 
with it. This procedure may be problematic, since an 
associated word often suggests two or more transla-
tion equivalents that represent the same sense. How-
ever, it is difficult to separate translation equivalents 
suggested by an associated word from others. Each 
associated word is therefore assigned to the 
translation equivalent it suggests most strongly. 
Finally, a translation equivalent is selected when 
1st-language corpus 
Extract word associationsExtract word associations 
1st-language word associations 2nd-language word associations
2nd-language corpus 
Construct context vector Construct context vectors Original bilingual dictionary 
Translate context vectors 
Translated context vectors 
Calculate contextual similarity of candidate translation equivalents to target word 
Select N most-similar translation equivalents 
Adapted bilingual dictionary 
Context vectors characterizing candidate translation equivalents Context vector characterizing target word  
 
Figure 1: Bilingual-dictionary adaptation 
based on contextual similarity 
  
the ratio of associated words assigned to it exceeds a 
certain threshold. In addition, representative associ-
ated words are selected for each selected translation 
equivalent. A representativeness measure was de-
vised under the assumption that representative asso-
ciated words are near the centroid of a cluster con-
sisting of associated words assigned to a translation 
equivalent. The representative associated words help 
lexicographers validate the selected translation 
equivalents. 
3.2 Calculation of correlation between translation 
equivalents and associated words 
The iterative algorithm described below has two 
main features. First, it overcomes the problem of 
failure in word-association alignment due to incom-
pleteness of the bilingual dictionary and disparity in 
topical coverage between the corpora of the two 
languages. Second, it overcomes the problem of am-
biguity in word-association alignment. 
3.2.1 Alignment of word associations 
For a first-language word association (x, 
x?(j))?where a target word is given as x and its j-th 
associated word is given as x?(j)?a set consisting of 
second-language word associations alignable with it, 
denoted as Y(x, x?(j)), is constructed. That is, 
Y(x, x?(j))  
= {(y, y?) | (y, y?)?R2, (x, y)?D, (x?(j), y?)?D}, 
where R2 is the collection of word associations ex-
tracted from a corpus of the second language, and D 
is a bilingual dictionary to be adapted. 
Each first-language word association (x, x?(j)) is 
characterized by a set consisting of accompanying 
associated words, denoted as Z(x, x?(j)). An accom-
panying associated word is a word that is associated 
with both words making up the word association in 
question. That is, 
Z(x, x?(j)) = {x? | (x, x?)?R1, (x?(j), x?)?R1}, 
where R1 is the collection of word associations ex-
tracted from a corpus of the first language. 
In addition, alignment of a first-language word 
association (x, x?(j)) with a second-language word 
association (y, y?) (?Y(x, x?(j))) is characterized by a 
set consisting of translingually alignable accompa-
nying associated words, denoted as W((x, x?(j)), (y, 
y?)). A translingually alignable accompanying asso-
ciated word is a word that is an accompanying asso-
ciated word of the first-language word association 
making up the alignment in question and, at the same 
time, is alignable with an accompanying associated 
word of the second-language word association mak-
ing up the alignment in question. That is, 
W((x, x?(j)), (y, y?))  
= Z(x, x?(j)) ? {x? | ? y?(?V(y, y?)) (x?, y?)?D}, 
where V(y, y?) = {y? | (y, y?)?R2, (y?, y?)?R2}. 
3.2.2 Iterative calculation of correlation 
The correlation between the i-th translation 
equivalent of target word x, denoted as y(i), and the 
j-th associated word x?(j) is defined as 
( ) ( ) ( )( ) ,jx',kyPLmax
jx',iyPLjx',xMIjx',iyC
k
)()(
)()()()()( ?=  
where MI(x, x?(j)) is the mutual information between 
x and x?(j), and PL(y(i), x?(j)) is the plausibility fac-
tor for y(i) given by x?(j). The mutual information 
between the target word and the associated word is 
the base of the correlation between each translation 
equivalent of the target word and the associated 
word; it is multiplied by the normalized plausibility 
factor. The plausibility factor is defined as the 
weighted sum of two component plausibility factors. 
That is, 
( ) ( ) ( ),jx',iyPLjx',iyPLjx',iyPL )()(?)()()()( 21 ?+=  
where ? is a parameter adjusting the relative weights 
of the component plausibility factors. 
The first component plausibility factor, PL1, is de-
fined as the sum of correlations between the transla-
tion equivalent and the accompanying associated 
words. That is, 
( ) ( ).x",iyCjx',iyPL
jx',xx"
?
?
=
))(Z(
1 )()()(  
This is based on the assumption that an associated 
Target word 
Extract word associations 
1st-language corpus 
Original bilingual dictionary 
2nd-language corpus
Extract word associations 
2nd-language  word associations 1st-language  word associations 
Calculate correlation between translation equivalents and associated words 
Assign each associated word to the translation equivalent having the highest correlation with it 
Binary matrix of translation equivalents vs. associated words
Representative associated words 
Aligned word associations 
Correlation matrix of translation equivalents vs. associated words
Align word associations  
Candidate translation
equivalents 
Adapted bilingual dictionary 
Select representative associated words 
Select translation equivalents to which more than a certain percentage of associated words are assigned
 
Figure 2: Bilingual-dictionary adaptation using 
the ratio of associated words 
  
word usually correlates closely with the translation 
equivalent that correlates closely with a majority of 
its accompanying associated words. 
The second component plausibility factor, PL2, is 
defined as the maximum plausibility of alignment 
involving the translation equivalent, where the plau-
sibility of alignment of a first-language word asso-
ciation with a second-language word association is 
defined as the mutual information of the sec-
ond-language word association multiplied by the 
sum of correlations between the translation equiva-
lent and the translingually alignable accompanying 
associated words. That is, 
( )
( ) .x"iyCy'iyMImax
jx'iyPL
y'iyjx'xWx"jx'xYy'iy
???
?
???
?
?= ?
?? ))),(()),(,(())(,()),((
2
),()),((
)(),(
 
This is based on the assumption that correct align-
ment of word associations is usually accompanied by 
many associated words that are alignable with each 
other as well as the assumption that alignment with a 
strong word association is preferable to alignment 
with a weak word association. 
The above definition of the correlations between 
translation equivalents and associated words is re-
cursive, so they can be calculated iteratively. Initial 
values are set as 
C0(y(i), x?(j)) = MI(x, x?(j)). 
That is, the mutual information between the target 
word and an associated word is used as the initial 
value for the correlations between all translation 
equivalents of the target word and the associated 
word. 
It was proved experimentally that the algorithm 
works well for a wide range of values of parameter ? 
and that the correlation values converge rapidly. Pa-
rameter ? and the number of iterations were set to 
five and six, respectively, in the experiments de-
scribed in Section 4. 
4 Experiments 
4.1 Material and preparation 
The experiment focused on nouns, whose appro-
priate translations often vary with domains. A 
wide-coverage bilingual noun dictionary was con-
structed by collecting pairs of nouns from the EDR 
English-to-Japanese and Japanese-to-English dic-
tionaries. The resulting dictionary consists of 
633,000 pairs of 269,000 English nouns and 276,000 
Japanese nouns. 
An English corpus consisting of Wall Street Jour-
nal articles (July 1994 to December 1995; 189MB) 
and a Japanese corpus consisting of Nihon Keizai 
Shimbun articles (December 1993 to November 
1994; 275MB) were used as the comparable corpora. 
English nouns occurring 10 or more times in the 
English corpus were selected as the target words. 
The total number of selected target words was 
12,848. For each target word, initial candidate trans-
lation equivalents were selected from the bilingual 
dictionary in descending order of frequency in the 
Japanese corpus; the maximum number of candi-
dates was set at 20, and the minimum frequency was 
set at 10. The average number of candidate transla-
tion equivalents per target word was 3.3, and 1,251 
target words had 10 or more candidate translation 
equivalents. 
Extraction of word associations, which is the first 
step common to the method based on contextual 
similarity (abbreviated as the CS method hereinafter) 
and the method using the ratio of associated words 
(abbreviated as the RAW method hereinafter), was 
done as follows. Co-occurrence frequencies of noun 
pairs were counted by using a window of 13 words, 
excluding function words, and then noun pairs hav-
ing mutual information larger than zero were ex-
tracted. 
Table 1: Example translation equivalents selected by 
the method based on contextual similarity 
Target word 
[Freq.] # Translation equivalent
*) [Freq.] Similarity 
1 ???? (administration organ) [137] 0.127 
2 ?? (reign) [32] 0.119 
3 ?? (direction of domestic affairs) [2366] 0.116 
4 ?? (political power) [2370] 0.111 
admini-
stration 
[2027] 
5 ?? (operation) [453] 0.111 
1 ???? (election campaign) [71] 0.067 
2 ?? (competition) [2608] 0.050 
3 ?????? (aggressive activities) [561] 0.049 
4 ?? (movement) [947] 0.049 
campaign
[1656] 
5 ???? (military activities) [89] 0.040 
1 ?? (management) [4810] 0.116 
2 ?? (enterprise) [8735] 0.091 
3 ?? (conduct) [1431] 0.076 
4 ?? (tactics) [528] 0.074 
operation
[3469] 
5 ?? (function) [2721] 0.074 
1 ????? (energy) [913] 0.103 
2 ? (force) [6276] 0.101 
3 ?? (majority) [1036] 0.101 
4 ?? (electric power) [1208] 0.079 
power 
[2826] 
5 ?? (ability) [1254] 0.074 
1 ? (husk) [135] 0.082 
2 ? (ball) [137] 0.070 
3 ?? (cannonball) [32] 0.070 
4 ? (ball) [1370] 0.062 
shell 
[102] 
6 ??? (case) [4851] 0.060 
1 ? (voice) [13536] 0.103 
2 ?? (target) [4676] 0.096 
3 ?? (business) [7163] 0.087 
4 ?? (indication) [215] 0.087 
sign 
[4064] 
5 ??? (mark) [297] 0.084 
*) English translations other than target words are given in parentheses. 
  
4.2 Experimental results 
Results of the CS and RAW methods for six target 
words are listed in Tables 1 and 2, respectively. Table 
1 lists the top-five translation equivalents in de-
scending order of contextual similarity. Table 2 lists 
translation equivalents with a ratio of associated 
words larger than 4% along with their top-four rep-
resentative associated words. In these tables, the oc-
currence frequencies in the test corpora are appended 
to both the target words and the translation equiva-
lents. These indicate the weak comparability be-
tween the Wall Street Journal and Nihon Keizai 
Shimbun corpora. Moreover, it is clear that neither 
the CS method nor the RAW method relies on the 
occurrence frequencies of words. 
Tables 1 and 2 clearly show that the two methods 
produce significantly different lists of translation 
equivalents. It is difficult to judge the appropriate-
ness of the results of the CS method without exam-
ining the comparable corpora. However, it seems 
that inappropriate translation equivalents were often 
ranked high by the CS method. In contrast, referring 
to the representative associated words enables the 
results of the RAW method to be judged as appropri-
ate or inappropriate. More than 90% of the selected 
translation equivalents were judged as definitely ap-
propriate. 
Table 2 also includes the orders of translation 
equivalents determined by a conventional bilingual 
dictionary (remarks column). They are quite differ-
ent from the orders determined by the RAW method. 
This shows the necessity and effectiveness of rank-
ing translation equivalents according to relevancy to 
a domain. 
Processing times were measured by separating 
both the CS and RAW methods into two parts. The 
processing time of the first part shared by the two 
methods, i.e., extracting word associations from 
corpora, is roughly proportional to the corpus size. 
For example, it took 2.80 hours on a Windows PC 
(CPU clock: 2.40 GHz; memory: 1 GB) to extract 
word associations from the 275 MB Japanese corpus. 
The second part, i.e., selecting translation equiva-
lents for target words, is specific to each method, and 
the processing time of it is proportional to the num-
ber of target words. It took 11.5 minutes and 2.40 
hours on another Windows PC (CPU clock: 2.40 
GHz; memory: 512 MB) for the CS and RAW 
methods, respectively, to process the 12,848 target 
words. It was thus proved that both the CS and RAW 
methods are computationally feasible. 
4.3 Quantitative evaluation using pseudo target 
words 
4.3.1 Evaluation method 
A method for bilingual-dictionary adaptation us-
ing comparable corpora should be evaluated by us-
Table 2: Example translation equivalents selected by the method using the ratio of associated words 
Target word 
[Freq.] # Translation equivalent
*) [Freq.] Ratio Representative associated words Remarks **) 
1 ?? (cabinet) [1067] 0.419 House, Clinton, White House, Republican 3a 
2 ?? (political power) [2370] 0.236 U.S. official, Haiti, Haitian, Clinton admini-stration 
3a 
3 ?? (operation) [453] 0.147 GATT, fast-track, trade pact, Trade 4a 
administration 
[2027] 
4 ?? (control) [84] 0.058 China, U.S., import, Japan - 
1 ???? (election campaign) [71] 0.612 Republican, candidate, GOP, Democrat 2a campaign 
[1656] 2 ?????? (aggressive activities) [561] 0.371 ad, advertise, brand, advertising 
2a 
1 ?? (management) [4810] 0.788 Stock Exchange, last year, profit, loss 2b operation 
[3469] 2 ?? (enterprise) [8735] 0.144 quarter, net, income, plant - 
1 ?? (electric power) [1208] 0.434 electricity, power plant, utility, megawatt 8b 
2 ?? (influence) [826] 0.425 military, leader, President, Haiti 3 power [2826] 3 ?? (authority) [909] 0.062 reform, law, Ukraine, amendment 5a 
1 ?? (cannonball) [32] 0.560 Serb, U.N., Sarajevo, NATO 4a 
2 ? (shellfish) [100] 0.168 crab, fish, hermit crab, Mr. Soifer 1a 
3 ? (ball) [137] 0.112 rupture, bacterium, implant, brain - 
shell 
[102] 
4 ?? (external appearance) [267] 0.064 tape, camera, video, building 3a 
1 ?? (indication) [215] 0.568 inflation, interest rate, rate, economy 4a 
2 ?? (signboard) [566] 0.099 tourist, billboard,  airport, exit 3b 
3 ?? (target) [4676] 0.086 accord, agreement, pact, treaty - 
4 ?? (indication) [2396] 0.086 last year, month, demand, order - 
sign 
[4064] 
5 ?? (signal) [231] 0.062 driver, accident, highway, motorist 2a 
*) English translations other than target words are given in parentheses. 
**) This column shows the orders of translation equivalents determined by a conventional dictionary ?Kenkyusha?s New Collegiate English-Japanese 
Dictionary, 5th edition.? For example, ?3a? indicates that a translation equivalent belongs to the subgroup ?a? in the third group of translations. A hy-
phen indicates that a translation equivalent is not contained in the dictionary. 
  
ing recall and precision measures defined as 
,T
TSecisionPrandS
TScallRe ?=?=  
where S is a set consisting of pairs of translation 
equivalents contained in the test comparable corpora, 
and T is a set consisting of pairs of translation 
equivalents selected by the method. To calculate 
these measures, it is necessary to know all pairs of 
translation equivalents contained in the test corpora. 
This is almost impossible in the case that the test 
corpora are large. 
To avoid this difficulty, an automated evaluation 
scheme using pseudo target words was devised. A 
pseudo word is formed by three real words, and it 
has three distinctive pseudo senses corresponding to 
the three constituent words. Translation equivalents 
of a constituent word are regarded as candidate 
translation equivalents of the pseudo word that rep-
resent the pseudo sense corresponding to the con-
stituent word. For example, a pseudo word ?ac-
tion/address/application? has three pseudo senses 
corresponding to ?action,? ?address,? and ?applica-
tion.? It has candidate translation equivalents such as 
???<SOSHOU>? and ???<KETSUGI>? originating 
from ?action,? ??? <ENZETSU>? and ???
<SEIGAN>? originating from ?address,? and ???
<OUYOU>? and ???<OUBO>? originating from 
?application.? Furthermore, pseudo word associa-
tions are produced by combining a pseudo word with 
each of the associated words of the first two con-
stituent words. It is thus assumed that first two 
pseudo senses occur in the corpora but the third one 
does not. For example, the pseudo word ?ac-
tion/address/application? has associated words 
including ?court? and ?vote,? which are associated 
with ?action,? as well as ?President? and ?legisla-
tion,? which are associated with ?address.? 
Using the pseudo word associations, a bilin-
gual-dictionary-adaptation method selects translation 
equivalents for the pseudo target word. On the one 
hand, when at least one of the translation equivalents 
originating from the first (second) constituent word 
is selected, it means that the first (second) pseudo 
sense is successfully selected. For example, when 
? ?? <SOSHOU>? is selected as a translation 
equivalent for the pseudo target word ?ac-
tion/address/application,? it means that the pseudo 
sense corresponding to ?action? is successfully se-
lected. On the other hand, when at least one of 
translation equivalents originating from the third 
constituent word is selected, it means that the third 
pseudo sense is erroneously selected. For example, 
when ???<OUYOU>? is selected as a translation 
equivalent for the pseudo target word ?ac-
tion/address/application,? it means that the pseudo 
sense corresponding to ?application? is erroneously 
selected. The method is thus evaluated by recall and 
precision of selecting pseudo senses. That is, 
,'T
'T'SecisionPrand'S
'T'ScallRe ?=?=  
where S? is a set consisting of pseudo senses corre-
sponding to the first two constituent words, and T? is 
a set consisting of pseudo senses relevant to transla-
tion equivalents selected by the method. 
4.3.2 Evaluation results 
A total of 1,000 pseudo target words were 
formed by using randomly selected words that oc-
cur more than 100 times in the Wall Street Journal 
corpus. Using these pseudo target words, both the 
CS and RAW methods were evaluated. As for the 
CS method, the recall and precision of selecting 
pseudo senses were calculated in the case that N 
most-similar translation equivalents are selected 
(N=2, 3,?). As for the RAW method, the recall and 
precision of selecting pseudo senses were calcu-
lated in the case that the threshold for the ratio of 
associated words is set from 20% down to 1% in 
1% intervals. 
Recall vs. precision curves for the two methods 
are shown in Figure 3. These curves clearly show 
that the RAW method outperforms the CS method. 
The RAW method maximizes the F-measure, i.e., 
harmonic means of recall and precision, when the 
threshold for the ratio of associated words is set at 
4%; the recall, precision, and F-measure are 92%, 
80%, and 86%, respectively. In contrast, the CS 
method maximizes the F-measure when N is set at 
nine; the recall, precision, and F-measure are 96%, 
72%, and 82%, respectively. 
It should be mentioned that the above evaluation 
was done under strict conditions. That is, two out of 
three pseudo senses of each pseudo target word were 
assumed to occur in the corpus, while many real tar-
get words have only one sense in a specific domain. 
Target words with only one sense occurring in a 
corpus are generally easier to cope with than those 
with multiple senses occurring in a corpus. Accord-
ingly, recall and precision for real target words 
would be higher than the above ones for the pseudo 
target words. 
0.5
0.6
0.7
0.8
0.9
1.0
0.5 0.6 0.7 0.8 0.9 1.0
Recall
Pr
ec
isi
on
CS RAW  
Figure 3: Recall and precision of 
selecting pseudo senses 
  
5 Discussion 
The reasons for the superior performance of the 
RAW method to the CS method are discussed in the 
following. 
? The RAW method overcomes both the sparseness 
of word-association data and the topical disparity 
between corpora of two languages. This is due to 
the smoothing effects of the iterative algorithm for 
calculating correlation between translation equiva-
lents and associated words; namely, associated 
words are correlated with translation equivalents 
even if they fail to be aligned with their counter-
part. In contrast, the CS method is much affected 
by the above-mentioned difficulties. All low values 
of contextual similarity (see Table 1) support this 
fact. 
? The RAW method assumes that a target word has 
more than one sense, and, therefore, it is effective 
for polysemous target words. In contrast, contex-
tual similarity is ineffective for a target word with 
two or more senses occurring in a corpus. The 
context vector characterizing such a word is a 
composite of context vectors characterizing re-
spective senses; therefore, the context vector char-
acterizing any candidate translation equivalent 
does not show very high similarity.  
? The RAW method can select an appropriate 
number of translation equivalents for each target 
word by setting a threshold for the ratio of associ-
ated words. In contrast, the CS method is forced to 
select a fixed number of translation equivalents for 
all target words; it is difficult to predetermine a 
threshold for the contextual similarity, since the 
range of its values varies with target words (see 
Table 1). 
Finally, from a practical point of view, advantages 
of the RAW method are discussed in the following. 
? The RAW method selects translation equivalents 
contained in the comparable corpora of a domain 
together with evidence, i.e., representative associ-
ated words that suggest the selected translation 
equivalents. Accordingly, it allows lexicographers 
to check the appropriateness of selected translation 
equivalents efficiently. 
? The ratio of associated words can be regarded as 
a rough approximation of a translation probability. 
Accordingly, a translation equivalent can be fixed 
for a word, when the particular translation equiva-
lent has an exceedingly large ratio of associated 
words. A sophisticated procedure for word-sense 
disambiguation or translation-word selection needs 
to be applied only to words whose two or more 
translation equivalents have significant ratios of 
associated words. 
6 Conclusion 
The method using the ratio of associated words 
was proved to be effective, while the method based 
on contextual similarity was not. The former method 
has the following features that make it practical. First, 
is uses weakly comparable corpora, which are avail-
able in many domains. Second, it selects translation 
equivalents together with representative associated 
words that suggest them, enabling the translation 
equivalents to be validated. The method will be ap-
plied to several domains, and its effect on the per-
formance of application systems will be evaluated. 
7 Acknowledgments 
This research was supported by the New Energy 
and Industrial Technology Development Organiza-
tion of Japan (NEDO). 
References 
Church, Kenneth W. and Patrick Hanks. 1990. Word 
association norms, mutual information, and lexicog-
raphy. Computational Linguistics, 16(1): 22-29. 
Dagan, Ido, Kenneth W. Church, and William A. Gale. 
1993. Robust bilingual word alignment for machine 
aided translation. In Proc. Workshop on Very Large 
Corpora, pages 1-8. 
Fung, Pascale. 1995. A pattern matching method for 
finding noun and proper noun translations from 
noisy parallel corpora. In Proc. 33rd Annual Meeting 
of the ACL, pages 236-243. 
Fung, Pascale and Kathleen McKeown. 1997. Finding 
terminology translations from non-parallel corpora. 
In Proc. 5th Annual Workshop on Very Large Cor-
pora, pages 192-202. 
Fung, Pascale and Lo Yuen Yee. 1998. An IR approach 
for translating new words from nonparallel, compa-
rable texts. In Proc. 36th Annual Meeting of the ACL 
/ 17th COLING, pages 414-420. 
Gale, William A. and Kenneth W. Church. 1991. Iden-
tifying word correspondences in parallel texts. In 
Proc. 4th DARPA Speech and Natural Language 
Workshop, pages 152-157. 
Kaji, Hiroyuki and Toshiko Aizono. 1996. Extracting 
word correspondences from bilingual corpora based 
on word co-occurrence information. In Proc. 16th 
COLING, pages 23-28. 
Kaji, Hiroyuki and Yasutsugu Morimoto. 2002. Unsu-
pervised word sense disambiguation using bilingual 
comparable corpora. In Proc. 19th COLING, pages 
411-417. 
Kitamura, Mihoko and Yuji Matsumoto. 1996. Auto-
matic extraction of word sequence correspondences 
in parallel corpora, In Proc. 4th Workshop on Very 
Large Corpora, pages 79-87. 
Melamed, I. Dan. 1997. A word-for-word model of 
translational equivalence. In Proc. 35th Annual 
Meeting of the ACL / 8th Conference of the EACL, 
pages 490-497. 
Rapp, Reinhard. 1999. Automatic identification of 
word translations from unrelated English and Ger-
man corpora. In Proc. 37th Annual Meeting of the 
ACL, pages 320-322. 
Tanaka, Kumiko and Hideya Iwasaki. 1996. Extraction 
of lexical translations from non-aligned corpora, In 
Proc. 16th COLING, pages 580-585. 
Corpus-dependent Association Thesauri for Information Retrieval 
Hiroyuki Kaji "~, Yasutsugu Morimoto "~, Toshiko Aizono "l, and Noriyuki Yamasaki "2 
"~ Central Research Laboratory, Hitachi, Ltd. ,2 Software Division, Hitachi, Ltd. 
1-280 Higashi-Koigakubo, Kokubunji-shi 549-6 Shinano-cho, Totsuka-kt,, Yokohanla-shi 
Tokyo 185-8601, Japan Kanagawa 244-0801, Japan 
{kaji, morimoto, aizono}@crl.hitachi.co.jp, yamasa n@soft.hitachi.co.jp 
Abstract 
This paper presents a method for automati- 
cally generating an association thesaurus 
from a text corpus, and demonstrates it ap- 
plication to information retrieval. The the- 
saurus generation method .consists of ex- 
tracting tenns and co-occurrence data from a 
corpus and analyzing the correlation between 
terms statistically. A new method for dis- 
ambiguating the structure of compound 
nouns, which is a key component for term 
extraction, is also proposed. The automati- 
cally generated thesaurus i  effectively used 
as a tool for exploring infonnation. A the- 
saurus navigator having novel functions uch 
as term clustering, thesaurus overview, and 
zooming-in is proposed. 
1 Introduction 
A thesaurus plays essential roles in information 
retrieval systems. In particular, a domain- 
specific thesaurus greatly improves the effective- 
ness of information retrieval. However, we are 
confronted with the difficult problem of how to 
construct and maintain a domain-specific thesau- 
rus. The goal of our present research is to es- 
tablish a method for autolnatically generating a 
thesaurus from a text corpus of a domain and 
demonstrate its application to information re- 
trieval. 
Thesauri are classified 
into taxonomy-type thesauri 
and association thesauri. 
There has been various 
research on the extraction of 
taxonomic information |'io111 
a corpus, including extrac- 
tion of hyponyms by using 
linguistic patterns (Hearst 
1992) and extraction of synonyms based on the 
similarity of sets of co-occurring words (Ruge 
1991; Grefenstette 1992). However, the perfor- 
mance of these methods is limited, and they 
should be considered as aids to augment hand- 
made thesauri. In contrast, an association the- 
saurus, that is a collection of pairs of semanti- 
cally associated terms, can be possibly generated 
from a corpus entirely automatically. Word 
association orms based on co-occurrence infor- 
mation have been proposed by (Church and 
Hanks 1990). Here we focus on the automatic 
generation of an association thesanrus. 
Association thesauri are as useful as taxon? 
omy-type thesauri in information retrieval. The 
improvement of retrieval effectiveness by using 
an association thesaurus has been reported by a 
number of papers (Jing and Croft 1994; Schutze 
and Pedersen 1994). We propose to use a coro 
pus-dependent association thesaurus interac- 
tively. 
2 Automat ic  Generat ion of an Associa- 
tion Thesaurus  f rom a Corpus 
2.1 Outline of the thesaurus generation 
method 
The proposed thcsaurus generation method con- 
sists of term extraction, co-occurrence data ex- 
traction, and correlation analysis, as shown in Fig. 
1. 
~? Term F 
U Co-occ.rrence 
\[ data extraction r l pmrs and their 
223___ 
C o r r e l a t ! m ~  
Fig. 1. Automatic generation of a thesaurus from a corpus. 
404 
2.1.1 Ternt extraction 
A thesaurus hould consist of terms, each repre- 
senting a domain-specific concept. Most of tile 
terms representing important concepts are nol, lllS, 
simple or compound, that frequently occur in the 
corpus. ThereR)re, we extract both simple 
nonns and compound nouns whose occurrence 
frequencies exceed a predetermined threshold. 
We also use a list of stop words since frequently 
occurring nouns are not always terms. 
Compound nouns are identified by a pattern 
matching method using a part-of-speech sequen- 
ce pattern. Naturally, the pattern is language 
specific. The following is a pattern for Japanese 
compound nouns: 
COMP NOUN := { PREFIX } NOUN + 
SUFFIX } { PREFIX } NOUN + 
A problem in extracting compound nouns is 
that a word sequence matched to the above pat- 
tern, which actually defines just the type of noun 
phrase, is not always a term. We filter out some 
kind of non-term noun phrases by using a list of 
stop words for the first and last elements of com- 
pound nouns. Stop words for the first element 
of compound nouns include referential nouns (e.g. 
jouki (above-mentioned)) and determiner nouns 
(e.g. kaku (each)). Stop words for the last ele- 
ment of compound nouns include time/place 
nouns (e.g. nai (inside)) and relational nouns (e.g. 
koyuu (peculiar)). 
Another importmat problem we are confronted 
with in term extraction is the structural ambiguity 
of compound nouns. For our purpose, we need 
to extract non-maximal compound nouns as well 
sts lnaxin la l  comp()und nouns. Here a non-  
maximal compound noun means one that occurs 
as a part o f  a larger con lpound noun, and a 
n lax imal  compound nonn means  one thstt occurs  
not as a part of a larger compound noun. We 
must disambiguate tile structure of compound 
nouns to correctly extract non-maximal com- 
pound nouns. We have developed a statistical 
disambiguation method, the detail and evaluation 
of which are described in 2.2. 
2.1.2 Co-occurrence data evtraction 
Our purpose is to collect pairs of semantically or 
contextually associated terms, no matter what 
kind of association. So we extract co- 
occurrence in a window. That is, every pair of 
terms occurring together within a window is 
extracted as the window is moved through a text. 
The window size can be specified rather arbitrar- 
ily. Considering our purpose, the window 
should accommodate a few sentences. At tile 
same time, the window size should not be too 
large from the viewpoint of computational load. 
Therefore, 20 to 50 words, excluding function 
words,  seems to be an appropriate value. 
Note that we filter out any pair of words co- 
occurring within a compound noun. If such 
pairs were included in co-occurrence data, they 
would show high correlation. However, they 
would be redundant because compound nouns are 
treated as entities in our thesaurus. 
2.1.3 Correlation analysis 
As a correlation measure between terms, we use 
mutual information (Church and Hanks 1990). 
The mutual inlbrmation between terms t~ and t i is 
defined by the following formula: 
g(ti, tj)/~' g(t,, t,) 
Ml(ti, tj) = log_, /i.i , 
{t'(t0/ i ~ f(t0}' { f(tj)//j~ f(ti, 
where f(t~) is the occurrence frequency of term t~, 
and g(ti,ti ) is the co-occurrence frequency of 
terms t~ and tj. A rnaxinmm nunrber of associat-. 
ed terms for each term is predetermined as well 
as a threshold for tile mutual information, and 
associated terms are selected based on the de-o 
scending order of mutual information. 
Mutual infornaation involves a problem in 
that it is overestimated for low-frequency terms 
(I)unning 1993). Therefore, we determine 
whether two terms are related to each other by a 
log-likelihood ratio test, and we filter out pairs of 
terms that do not pass the test. 
2.2 Disambiguation of compound noun 
structure 
2.2.1 Disantbiguation based on coitus statistics 
Our disanabiguation method is described below 
for tile case of a compound noun consisting of 
three elelnents. A compound noun W~W2W 3 
has two possible structures: WI(W~W3) and 
(W~W,)W 3. We deternfine its structure based 
on the occurrence t}equencies of maxilnal com- 
pound nouns as follows: If tile maximal com- 
pound noun W~W3 occurs more frequently than 
tile inaxinml compound noun W~W,, then tile 
405 
Table 1 
(a) Examples 
Global-statistics-based disambiguation vs. local-statistics-based disambiguation. 
Maximal compound noun 
Deta shori shisutemu 
(data processing system) 
Deta tsuskin seiL:vo souchi 
(Data communication 
controller) 
Kaisen seigyo purosessa 
(Line control processor) 
F req .  Global-statistics-based 
Structure Freq. 
478 (Deta shori) shisutemu 478 
94 Deta (tsuskin seigvo 94 
souchi) 
54 Kaisen (seign'o 54 
purosessa) 
Local-statistics-based 
Structure Freq. 
(Deta shori) shisutemu 368 
Deta (skori shisutemu) 110 
(Deta tsushin) seigg'o souchi 40 
Deta (tsuskin seig:vo souchi) 54 
(Kaisen seign.,o) purosessa 54 
(b) Summary of disambiguation results 
Global-statistics-based 
Correct structure 12,565 words (62.0%) 
Incorrect structure 7,688 words (38.0%) 
Total 20,253 words (100%) 
(Note: Numbers of words are occurrence-based.) 
structure Wl(W2W3) is preferred. On the con? 
trary, if the maximal COlnpound noun W~W2 
occurs more frequently than the maximal com- 
pound noun W2W3, then the structure (W~W2) W3 
is preferred. 
The generalized isambiguation rnle is as 
follows: If a compound noun CN includes two 
compound noun candidates CN~ and CN2, which 
are incompatible with each other, and the maxi- 
mal compound noun CN~ occurs more frequently 
than the maximal compound noun CN> then a 
structure of CN including CN~ is preferred to a 
structure of CN including CN,. 
We have two alternatives regarding the range 
where we count occurrence frequencies of maxi- 
mal compound nouns. One is global-statistics 
which means that frequencies are counted in the 
whole corpus and they are used to disambiguate 
all compound nouns in the corpus. The other is 
local-statistics which means that frequencies are 
counted in each document in the corpus and they 
are used to disambiguate compound nouns in the 
corresponding document. 
2.2.2 Evaluation" Global-statistics vs. local- 
statistics 
We evaluated both the global-statistics-based 
disambiguation and the local-statistics-based 
disambiguation by using a 23.7-M Byte corpus 
consisting of 800 patent documents. Table l(a) 
shows comparative xamples of these methods. 
Evah, ation results for the 200 highest-frequency 
maximal COlnpound nouns consisting of three or 
Local-statistics-based 
more  words are summa ~ 
rized in Table l(b). 
14,921 words (73.7%) They prove that the 
local-statistics-based 5,332 words (26.3%) 
disambiguation method 
20,253 words (100%) is superior to the global- 
statistics-based disam- 
biguation method. 
Note that in the local-statistics-based disam- 
biguation method, we resorted to the global- 
statistics when local-statistics were not available. 
The percentage of cases the local-statistics were 
not available was 25.1 percent. 
(Kobayasi et al 1994) proposed a disam- 
biguation method using collocation information 
and semantic categories, and reported that the 
structure of compound nouns was disambiguated 
at 83% accuracy. Note that their accuracy was 
calculated for compound nouns including unam- 
biguous compound nouns, i.e. those consisting of 
only two words. If it were calculated for com-. 
pound nouns consisting three or more words, it 
would be less than that of our method. Thus, we 
can conclude that our local-statistics-based 
method compares quite well with rather sophisti- 
cated previous methods. 
2.3 Prototype and an experiment 
We implemented a prototype thesaurus generator 
in which the local-statistics-based method was 
used to disambiguate the structure of compound 
nouns. Using this thesaurus generator, we got a 
thesaurus consisting of 38,995 terms froln a 61- 
M Byte corpus consisting of almost 48,000 arti- 
cles in the financial pages of a Japanese newspa- 
per. In this experiment, the threshold for occur- 
fence frequencies of terms in the term extraction 
step was set to 10, and the window size in the co- 
occurrence data extraction step was set to 25. 
406 
The abow+" rtm took 5.4 hours on a HP9000 
C200 workstation. The tlarouglaput is tolerable 
from a practical point of view. We should also 
note that a thesaurus can be updated as efficiently 
as it can be initially generated. Because \ve can 
run the first two steps (extraction of terms and 
extraction of co-occurrence data) in accumulative 
fashion, and we only need to run the third step 
over again when a considerable amount of terms 
and co-occurrence data are accunmlated. 
3 Navigation in an Association Thesau-  
FUS 
3.1 Purpose and outline of the proposed 
thesaurus navigator 
A big problem with toclafs information re- 
trieval systems based on search techniques i that 
they require users, who may not know exactly 
what thcy arc looking for, to explicitly describe 
their information needs. Another problem is 
that mismatched vocabularies between users and 
the corpus would bring poor retrieval results. 
To solve these problems, we propose a corptts-~ 
dependent association-thesaurus navigator en- 
abling users to efficiently explore information 
through a corpus. 
Users' requirements are summarized as fob 
lows: 
They want to grasp the overall information 
structure of a domain.  
They want to know what topics or sub?. 
domains arc contained in the corpus. 
- They want to know terms that appropriately 
describe their w~gue information eeds. 
To meet the above requirements, our pro,- 
posed thesaurus navigator has novel functions 
such as clustering of related terms, generation of 
a thesaurus overview, and zoom-in on a sub-- 
domain of interest. A conceptual image of 
thesaurus navigation using these ftmctions is 
shown in Fig. 2. A typical informatio,> 
exploration session proceeds as follows. 
At the beginning, the system displays an 
overview of a corpus-dependent thesaurus o that 
users can easily enter the information space of 
the corpus. The overview is a kind of summary 
of the corpus, it consists of clusters of generic 
terms of the domain, and makes it easy to under- 
stand what topics or sub-domains are contained 
in the corpus. Looking at the thesaurus over- 
Thesaurtts 
overview 
' . .  . . . .  
O Generic term 
Zoom-in Zoom-in 
. . . .  
// 
? Specific term 
Fig. 2. Conceptual image of thesaurus 
navigation. 
view, the users can select one or a few term 
clusters they have interest in, and the screen will 
zoom in on the cluster(s). The zoomed view 
consists of a nulnber of clusters, each including 
more specific terms than those in the overview. 
Users can repeat his zoom-in operation until they 
reach term clusters representing sufficiently 
specific topics. 
3.2 Functions of the thesaurus navigator 
3.2.1 Clustering of  related terms 
We made a preliminary experiment o evaluate 
standard agglomerative clustering algorithms 
including the single-linkage method, the con> 
pletedinkage method, and the group-average- 
linkage method (Eldqamdouchi and Willett 1989)~ 
Among them, the group--average-linkage m thod 
resulted in the best results, ttowever, several 
potential clusters tended to merge into a large one 
when we repeated the merge operation until a 
predetermined number of clusters were obtained. 
Accordingly, we use the group-average-linkage 
method with an upper limit on the size of a clus- 
ter. 
3.2.2 Generation era thesaurus overview 
Our method tot generating a thesaurus overview 
consists of major-term extraction and term clus- 
tering. The m~0or-term extracting algorithm, 
which is carried out beforehand in batch mode, is 
described below. See 3.2.1 for the term clus- 
tering algorithnl. 
An overview of the thesaurus hould consist 
of generic terms included in the corpus, flow- 
ever, we do not have a definite criterion for get> 
eric terms. So we collect m~oor terms from the 
corpus as follows. The number of m~!jor terms, 
407 
denoted by M below, was set to 300 in the pro- 
totype. 
i) Determine a characteristic term set for each 
doculnent. 
Calculate the weight w~j of term tj for 
document 4 according to the tf-idf (term fre- 
quency - inverse document frequency) for- 
mula. Then select the first re(i) terms in the 
descending order of u, u for each document d,, 
where re(i), the number of characteristic terms 
for document 4, is set to 20% of the total 
number of distinct erms in 4. It is also lim- 
ited to between 5 and 50. 
ii) Select major terms in the corpus. 
Select the first M terms in the descending 
order of the frequency of being contained in 
the characteristic term sets? 
3.2.3 Zoom-in on a term cluster of  interest 
Our method for zooming in on a term cluster 
consists of term-set expansion and term cluster~ 
ing. The term-set expanding algorithm is de~ 
scribed below. See 3.2.1 for the term clustering 
algorithm. 
A user-specified term set To = {t~, 6. . . . .  t,,,} is 
expanded into a term set T,. consisting of M terms 
as follows. M was set to 300 in the prototype. 
i) Set the initial value of 7",. to 7",,. 
ii) Whi le  IT,.I< M for i = 1, 2 . . . .  do; 
While IE, I < Mfor j  = 1,2, ..., m do; 
Add the tenn having the i-th highest 
correlation with tj to T,,; 
end; 
end; 
The reason why the above-described proce- 
dure implements the zoom-in is that generic 
terms tend to have higher correlation with semi- 
generic terms than with specific terms. As- 
suming that high-frequency terms are generic and 
low-frequency terms are specific, we examined 
the distribution of terms by the distance from the 
major terms and the average occurrence frequen- 
cy of terms for each distance. Here the distance 
is the length of the shortest path in a graph that is 
obtained by connecting every pair of associated 
terms with an edge. Table 2 shows the results 
for the example thesaurus mentioned in 2.3. 
According to it, the average occurrence frequen- 
cy decreases with the distance from the major 
terms. Therefore, starting from an overview, 
our method is likely to produce more and more 
specific views. 
3.3 Prototype and an exper iment 
We developed a prototype as a client/server sys- 
tem. The thesaurus navigator is available on 
WWW browsers. It also has an interface to 
text-retrieval engines, through which a term 
cluster is transferred as a query. 
Test use was made with the example thesau- 
rus mentioned in 2.3. The response time for the 
zoom-in operation during the navigation sessions 
was about 8 seconds. This is acceptable given 
the rich information provided by the clustered 
view. Note that the response time is almost 
independent of the size of the thesaurus or corpus, 
because the number of temls to be clustered is 
always constant, as described in 3.2.2 and 3.2.3~ 
An example from navigation sessions is 
shown in Fig. 3. It demonstrates the useflflness 
of the corpus-dependent thesaurus navigation as a 
front-end for text retrieval. The effectiveness of
our thesaurus navigator is summarized as folo 
lows. 
- Improved accessibility to text retrieval systems: 
Users are not necessarily required to input 
terms to describe their information need. 
They need only select from among terms pre~ 
sented on the screen. This makes text re-. 
trieval systems accessible ven for those hav- 
ing vague information eeds, or those unfa- 
miliar with the domain. 
- Improved navigation efficiency: 
The unit of users' cognition is a topic rather 
than a term. That is, they can recognize a 
topic from a cluster of terms at a glance. 
Therefi)re, they can efficiently navigate 
through an information space. 
Table 2 Distribution of terms by distance from major terms. 
Distance flom major terms 0 1 2 
Number of terms 245 4278 23832 
Average occurrence frequency 2642.1 318.6 61.9 
3 4 5 
10408 149 l 82 
10.6 7.7 9.0 - 
408 
I :~#-k ,  99 Y,# -" 
'" " '=' / I \ [ 'C~? 
:::Z ::::::::::::::::::::::::::: : /  ' : " : '  
(a) Thesaurus overview 
1"~ 9~_/.x 9D 27,9 -" 
2'~I~\]J!c?x 72-'_ fAY \ ] "Z : -E .~ : iLtF'kM?Z 
J?,ff ,'ib'.. 9,ft.) v'~t/t!. 7,5_I 
r a~,:?: :,5 ,&5;: i!17.ii8 :~!~. I~f i  ~< ~qT. >g .'JJ/g i~;? E '~ 
3:i~ / ' l ' . , ' i tR ;C,'i:: i" "_Z tA :  
ki~':\[ ' :  ~LL.rJ.=\] 2>EU i::,.;~:'\]:::~: ;?~-:,':::,: i~L i~.  .+ <?.. ~ /: .:. r- ~;" +. 
't :f" Z ?0  ~t ? 
?~le'~ ' ' ~ I ' *~a ,-m/I':" ~ "~I o . . . . . . . . . . . . . . .  : ' " "~7:  . . . . . . . . . . . .  . . . . . . . . . . . .  
(b) Zoom-in 
F ~-)Ea?':~A rivaL752 ?~ o E o ~  ~ aLW.eE 
tlffa ~ Y_22_rD j tm~~ 
f~ 7ai~.NL~ ;r_u'2-?.,- in 97  
,_J 
(c) Further zoom-in 
An overview of the thesaurus was di.sTJko,ed. 
Then the user selected the.fi/ih and seventh 
cluste#w hich he was interested in: {China, 
col!\[L'rence, Asia, cooperation, meeting, 
Vietnam, region, development, technology, 
environment}, and/economy export, aid, 
toward, summit, Soviet Union, debt, Russia, 
reconstruction}. This means that the user 
was interested in "development assistance 
to developing countries or areas ". 
The Jil?h aml seventh cluste~w J)'om (a) were 
shown close up, and clustelw indicating 
more .Vwc~/ic domains were presented. 
The user couM undelwtand which topics the 
respective clustelw suggested: "Economic 
assis'tance for the development ofthe Asia- 
Pat(lie region ", "Global environmental 
problems ", "bTternational debt problems ", 
"'Mattetw on China ", "Energy resource 
development", and so on. Since he wax 
e.vwcially interested in "International debt 
problems ", he selected the third cluster 
{debt, Egypt, Paris Club, creditor nation, 
q\[licial debt, de/'erred, Poland, .fin'eign debt, 
reimbursement, Paris, club, pro,merit, 
.fi, reign}. 
The third cluster fi'om (I 0 was shown close 
up. The resulting screen gave the user a 
choice el'many sT~ecific terms relevant o 
"htternational debt problems ", although 
not all oJ'the clustel:v indicated spec!/ic 
topics. The user was able to retrieve 
documents by simply selectin? terms o/ 
interest./i'om those displayed on the screen. 
Fig. 3. Example of thesaurus navigation. 
409 
4 Comparison with related work 
Let us make a briefcolnparison with related work. 
Both scatter/gather document clustering (Cutting 
et al 1992; Hearst and Pedersen 1996) and Ko- 
honen's self-organizing map (Lin et al 1991; 
Lagus et al 1996; Kohonen 1998) enable explo- 
ration through a corpus. While they treat a 
corpus as a collection of documents, we treat it as 
a collection of terms. Therefore our method can 
elicit finer information structure than these previ- 
ous methods, and moreover, it can be applied to a 
corpus that includes multi-topic doculnents. 
Our method compares quite well with the previ- 
otis methods for throughput and response time. 
5 Conclusion 
We demonstrated the feasibility of automatic 
generation of an association thesaurus from a 
corpus. The proposed thesaurus generation 
method consists of extracting terms and co- 
occurrence data from a corpus and analyzing the 
correlation between terms statistically. As a 
component technology for thesaurus generation, 
a method for disambiguating the structure of 
compound nouns based on corpus statistics was 
developed and evaluated. 
We also demonstrated the information re- 
trieval application of an automatically generated 
association thesaurus. A thesaurus navigator 
having novel functions such as term clustering, 
thesaurus overview, and zooming-in was devel- 
oped. An experiment with an association the- 
saurus generated from a newspaper article corpus 
proved that the thesaurus navigator allows us to 
efficiently explore information through a text 
corpus even when our information needs are 
vague. 
Acknowledgements: This research was st, p- 
ported in part by the Next-generation Digital 
Library System R&D Project of MITI (Ministry 
of International Trade and Industry), IPA (Inlbr- 
mation-technology Promotion Agency), and 
JIPDEC (Japan Information Processing Devel- 
opment Center). We thank Mainichi Newspa- 
pers, Ltd. for permitting us to use the CD-ROMs 
of the '91, '92, '93, '94 and '95 Mainichi Shim- 
bun tbr the experiment. 
References 
Church, K. W., and P. ttanks. 1990. Word association 
norms, mutual information, and lexicography. Con> 
putational Linguistics, 16( 1 ): 22-29. 
Cutting, D. R., D. R. Karger, J. O. Pedersen, and J. W. 
Tukey. 1992. Scatter/gather: A cluster-based ap- 
proach to browsing large doctnnent collections. Proc. 
ACM SIG1R '92, pp. 318-329. 
Dunning, T. 1993. Accurate methods for the statistics 
of surprise and coincidence. Computational Lin- 
guistics, 19( 1 ): 61-74. 
El-Hamdouchi, A., and P. Willett. 1989. Comparison 
of hierarchical agglomerative clustering methods for 
document retrieval. The Computer Journal, 32(3): 
220-227. 
Grefenstette, G. 1992. Use of syntactic context to 
produce term association lists for text retrieval. Proc. 
ACM SIGIR '92, pp. 89-97. 
Hearst, M. A. 1992. Automatic acquisition ofhypo- 
nyms from large text corpora. Proc. COLING '92, 
pp. 539-545. 
Hearst, M. A., and J. O. Pedersen. 1996. Reexamining 
the cluster hypothesis: Scatter/gather on retrieval 
results. Proc. ACM S1GIR '96, pp. 76-84. 
Jing, Y., and W. B. Croft. 1994.. An association the-. 
saurus for information retrieval. Proc. R1AO '94, 
Cont. on Intelligent Text and linage Handling, pp. 
146-160. 
Kobayasi, Y., T. Tokunaga, and tf. Tanaka. 1994. 
Analysis of Japanese compound nouns using colloo 
cational information. Proc. COLING '94, pp. 865- 
869. 
Kohonen, T. 1998. Self-organization of very large 
document collections: State of the art. Proc. 8th lnt'l 
Cone on Artificial Neural Networks, vol. 1, pp. 65- 
74. 
Lagus, K., T. Honkela, S. Kaski, and T. Kohonen. 
1996. Self organizing maps of document collections: 
a new approach to interactive xploration. Proc. 2nd 
lnt'l Cone on Knowledge Discovery and Data Min- 
ing, pp. 238-243. 
Lin, X., D. Soergel, and G. Marchionini. 1991. A self- 
organizing semantic map for information retrieval. 
Proc. ACM SIGIR '91, pp. 262-269. 
Ruge, G. 1991. Experiments on linguistically based 
term associations. Proc. RIAO '91, Conf. on Intelli- 
gent Text and hnage Handling, pp. 528-545. 
Schutze, tt., and J. O. Pedersen. 1994. A cooccur- 
rence-based thesaurus and two applications to in- 
formation retriewd. Proc. RIAO '94, Conf. on Intel- 
ligent Text and hnage ttandling, pp. 266-274. 
410 
Unsupervised Word Sense Disambiguation
Using Bilingual Comparable Corpora
Hiroyuki Kaji and Yasutsugu Morimoto
Central Research Laboratory, Hitachi, Ltd.
1-280 Higashi-Koigakubo, Kokubunji-shi, Tokyo 185-8601, Japan
{kaji, y-morimo}@crl.hitachi.co.jp
Abstract
An unsupervised method for word sense disam-
biguation using a bilingual comparable corpus was
developed.  First, it extracts statistically significant
pairs of related words from the corpus of each lan-
guage.  Then, aligning pairs of related words
translingually, it calculates the correlation between
the senses of a first-language polysemous word and
the words related to the polysemous word, which
can be regarded as clues for determining the most
suitable sense.  Finally, for each instance of the
polysemous word, it selects the sense that maxi-
mizes the score, i.e., the sum of the correlations
between each sense and the clues appearing in the
context of the instance.  To overcome both the
problem of ambiguity in the translingual alignment
of pairs of related words and that of disparity of
topical coverage between corpora of different lan-
guages, an algorithm for calculating the correlation
between senses and clues iteratively was devised.
An experiment using Wall Street Journal and Ni-
hon Keizai Shimbun corpora showed that the new
method has promising performance; namely, the
applicability and precision of its sense selection are
88.5% and 77.7%, respectively, averaged over 60
test polysemous words.
1 Introduction
Word sense disambiguation (WSD) is an ?intermedi-
ate? task that is necessary for accomplishing most
natural language processing tasks, especially machine
translation and information retrieval.  A variety of
WSD methods have been proposed over the last dec-
ade; however, such methods are still immature.  In
response to this situation, we have developed an unsu-
pervised WSD method using bilingual comparable
corpora.
With the growing amount of texts available in elec-
tronic form, data-driven or corpus-based WSD has
become popular.  The knowledge useful for WSD can
be learned from corpora.  However, supervised
learning methods suffer from the high cost of manually
tagging the sense onto each instance of a polysemous
word in a training corpus.  A number of bootstrapping
methods have been proposed to reduce the sense-
tagging cost (Hearst 1991; Basili 1997).  A variety of
unsupervised WSD methods, which use a machine-
readable dictionary or thesaurus in addition to a corpus,
have also been proposed (Yarowsky 1992; Yarowsky
1995; Karov and Edelman 1998).  Bilingual parallel
corpora, in which the senses of words in the text of one
language are indicated by their counterparts in the text
of another language, have also been used in order to
avoid manually sense-tagging training data (Brown, et
al. 1991).
Unlike the previous methods using bilingual cor-
pora, our method does not require parallel corpora.
The availability of large parallel corpora is extremely
limited.  In contrast, comparable corpora are available
in many domains.  The comparability required by our
method is very weak: any combination of corpora of
different languages in the same domain is acceptable as
a comparable corpus.
Several types of information are useful for WSD
(Ide and Veronis 1998).  Three major types are the
grammatical characteristics of the polysemous word to
be disambiguated, words that are syntactically related
to the polysemous word, and words that are topically
related to the polysemous word.  Among these types,
use of grammatical characteristics, which are language-
dependent, is not compatible with the approach using
bilingual corpora.  On the other hand, since a topical
relation is language-independent, use of topically relat-
ed words is most compatible with the approach using
bilingual corpora.  Accordingly, we focused on using
topically related words as clues for determining the
most suitable sense of a polysemous word.
2 Approach
2.1 Framework
A comparable corpus consists of a first-language cor-
pus and a second-language corpus of the same domain.
Unlike a parallel corpus, we cannot align sentences or
instances of words translingually.  Therefore, we ex-
tract a collection of statistically significant pairs of
related words from each language corpus indepen-
dently of the other language, and then align the pairs of
related words translingually with the assistance of a
bilingual dictionary.  The underlying assumption is
that translations of words that are related in one lan-
guage are also related in the other language (Rapp
1995).
Translingual alignment of pairs of related words
enables us to acquire knowledge useful for WSD (i.e.,
sense-clue pair).  For example, the alignment of (tank,
gasoline) with (???<TANKU>, ????<GASORIN>)
implies that ?gasoline? is a clue for selecting the ?con-
tainer? sense of ?tank?, which is translated as ????
<TANKU>?, and the alignment of (tank, soldier) with (?
?<SENSYA>, ??<HEISI>) implies that ?soldier? is a
clue for selecting the ?military vehicle? sense of ?tank?,
which is translated as ???<SENSYA>?.
Figure 1 shows an overview of our proposed
method for acquiring knowledge for WSD.  In the
framework of translingually aligning pairs of related
words, we encounter two major problems: the ambi-
guity in alignment, and the disparity of topical cover-
age between the corpora of the two languages.  The
following sections discuss how to overcome these
problems.
2.2 Coping with ambiguity in alignment
Matching of pairs of related words via a bilingual dic-
tionary often suggests that a pair in one language can
be aligned with two or more pairs in the other language.
For example, an English pair (tank, troop) can be
aligned with Japanese pairs (??<SUISOU>, ??
<MURE>), (?<SOU>, ??<TASUU>), (??<SENSYA>,
?<GUN>), (??<SENSYA>, ??<TASUU>), and (??
<SENSYA>, ?<TAI>). We resolve this ambiguity on the
assumption that correct alignments are accompanied by
a lot of common related words that can be aligned with
each other.  In the above example, a lot of words
related to both ?tank? and ?troop? can be aligned with
words related to both ???<SENSYA>? and ??<TAI>?
(see Figure 2(b5)).
The plausibility of alignment is evaluated ac-
cording to the set of first-language common related
words that can be aligned with second-language
common related words.  Then, using the plausi-
bility of alignment, the correlation between the
senses of a polysemous word and the clues for se-
lecting the most suitable sense is calculated.  To
precisely evaluate the plausibility of alignment, we
define it as the sum of the correlations between the
sense suggested by the alignment and the common
related words accompanying the alignment.
2.3 Coping with disparity between corpora
Given the disparity of topical coverage between the
corpora of two languages as well as the insufficient
coverage of the bilingual dictionary, the method de-
scribed in the preceding section seems too strict.  As
exemplified in Figure 2, even for a correct alignment of
a first-language pair of related words with a second-
language pair of related words, only a small part of the
first-language common related words can be aligned
with second-language common related words.  To
improve the robustness of the method, instead of the
set of first-language common related words that can be
aligned with second-language common related words,
we use a weighted set consisting of all the first-
language common related words, where those aligned
with second-language common related words are given
 
Comparable corpus 
1st language corpus 2nd language corpus 
Align pairs of related words translingually 
Extract co-occurrence 
data and calculate 
mutual information  
Calculate correlation between 
senses and clues iteratively 
Bilingual 
dictionary 
Senses defined 
by sets of 
translations 
Sense-vs.-clue correlation 
Alignments of pairs of related words accompanied 
by a set of common related words 
Collection of pairs 
of related words 
Collection of pairs 
of related words 
Extract co-occurrence 
data and calculate 
mutual information  
Fig. 1 Overview of the proposed method for
acquiring knowledge for WSD
(a) Common related words of (tank, troop)
Army, Bosnian, Bosnian government, Chechen,
Chechnya, Force, Grozny, Israel, Moscow, Mr. Yelt-
sin, Mr. Yeltsin's, NATO, Pentagon, Republican,
Russia, Russian, Secretary, Serb, U.N., Yeltsin, Yelt-
sin's, air, area, army, assault, battle, bomb, carry, ci-
vilian, commander, control, defense, fight, fire, force,
government, helicopter, military, missile, rebel, sol-
dier, weapon
(b1) Common related words of (tank, troop) that can
be aligned with common related words of (??
<SUISOU>, ??<MURE>)
air, area, fire, government
(b2) Common related words of (tank, troop) that can
be aligned with common related words of (?
<SOU>, ??<TASUU>)
area, army, control, force
(b3) Common related words of (tank, troop) that can
be aligned with common related words of (??
<SENSYA>, ?<GUN>)
area, army, battle, commander, force, government
(b4) Common related words of (tank, troop) that can
be aligned with common related words of (??
<SENSYA>, ??<TASUU>)
Serb, area, army, battle, force, government
(b5) Common related words of (tank, troop) that can
be aligned with common related words of (??
<SENSYA>, ?<TAI>)
Russia, Serb, air, area, army, battle, commander, de-
fense, fight, fire, force, government, helicopter, sol-
dier
Fig. 2 Example of common related words
larger weights than the others.
The disparity of topical coverage between the cor-
pora of two languages and the insufficient coverage of
the bilingual dictionary also cause a lot of pairs of re-
lated words not to be aligned with any pair of related
words.  To recover the failure in alignment, we intro-
duce a ?wild card? pair, with which every first-
language pair of related words is aligned compulsorily.
The alignment with the wild-card pair suggests all
senses of the first-language polysemous word, and it is
accompanied by a set consisting of the first-language
common related words with the same weight.
3 Proposed method
3.1 Defining word senses
We define each sense of a polysemous word x of the
first language by a synonym set consisting of x itself
and one or more of its translations y1, y2, ... into the
second language.  The synonym set is similar to that
in WordNet (Miller 1990) except that it is bilingual, not
monolingual.  Examples of some sets are given be-
low.
{tank, ???<TANKU>, ??<SUISOU>, ?<SOU>}
{tank, ??<SENSYA>}
These synonym sets define the ?container? sense and
the ?military vehicle? sense of ?tank? respectively.
Translations that preserve the ambiguity are prefer-
ably eliminated from the synonym sets defining senses
because they are useless for distinguishing the senses.
An example is given below.
{title, ???<KATAGAKI>, ??<SYOUGOU>, ????
<TAITORU>, ??<KEISYOU>}
{title, ??<DAIMEI>, ??<DAIMOKU>, ??
<HYOUDAI>, ??<SYOMEI>, ????<TAITORU>}
{title, ????<TAITORU>, ???<SENSYUKEN>}
These synonym sets define the ?person?s rank or pro-
fession? sense, the ?name of a book or play? sense, and
the ?championship? sense of ?title?.  A Japanese word
?????<TAITORU>?, which represents all these senses,
is preferably eliminated from all these synonym sets.
3.2 Extraction of pairs of related words
The corpus of each language is statistically processed
in order to extract a collection of pairs of related words
in the language (Kaji et al 2000).  First, we extract
words from the corpus and count the occurrence fre-
quencies of each word.  We reject words whose fre-
quencies are less than a certain threshold.  We also
extract pairs of words co-occurring in a window and
count the co-occurrence frequency of each pair of
words.  In the present implementation, the words are
restricted to nouns and unknown words, which are
probably nouns, and the window size is set to 25 words
excluding function words.
Next, we calculate mutual information MI(x, x?)
between each pair of words x and x?.  MI(x, x?) is de-
fined by the following formula:
)'xPr()xPr(
)'x,xPr(
log)'x,x(MI
?
= ,
where Pr(x) is the occurrence probability of x, and Pr(x,
x?) is the co-occurrence probability of x and x?.  Fi-
nally, we select pairs of words whose mutual informa-
tion value is larger than a certain threshold and at the
same time whose relation is judged to be statistically
significant through a log-likelihood ratio test.
3.3 Alignment of pairs of related words
In this section, RX and RY denote the collections of pairs
of related words extracted from the corpora of the first
language and the second language, respectively.  D
denotes a bilingual dictionary, that is, a collection of
pairs consisting of a first-language word and a second-
language word that are translations of each other.
Let X(x) be the set of clues for determining the sen-
se of a first-language polysemous word x, i.e.,
X(x)={x?|(x, x?)?RX}.
Henceforth, the j-th clue for determining the sense of x
is denoted as x?(j).
Let Y(x, x?(j)) be the set of counterparts of a pair of
first-language related words (x, x?(j)), i.e.,
Y(x, x?(j))=
  {(y, y?) | (y, y?)?RY, (x, y)?D, (x?(j), y?)?D}.
(1) Each pair of first-language related words (x, x?(j)) is
aligned with each counterpart (y, y?) (?Y(x, x?(j))),
and a weighted set of common related words Z((x,
x?(j)), (y, y? )) is constructed as follows:
Z((x, x?(j)), (y, y? )) =
  {x? / w(x?) | (x, x?)?RX, (x?(j), x?)?RX},
where w(x?), which denotes the weight of x?, is set
as follows:
- w(x?) = 1+??MI(y, y?) when ?y? (x?, y?)?D,
(y, y?)?RY, and (y?, y?)?RY .
- w(x?) = 1 otherwise.
The mutual information of the counterpart, MI(y, y?),
was incorporated into the weight according to the as-
sumption that alignments with pairs of strongly relat-
ed words are more plausible than those with pairs of
weakly related words.  The coefficient ? was set
to 5 experimentally.
(2) Each pair of first-language related words (x, x?(j)) is
aligned with the wild-card pair (y0, y0?), and a weight-
ed set of common related words Z((x, x?(j)), (y0, y0?))
is constructed as follows:
Z((x, x?(j)), (y0, y0?)) =
  {x? / w(x?) | (x, x?)?RX, (x?(j), x?)?RX},
where w(x?) = 1 for all x?.
3.4?Calculation of correlation between senses and
clues
We define the correlation between the i-th sense S(i)
and the j-th clue x?(j) of a polysemous word x as fol-
lows:
( ) ( )
( )
( )
,
)k(S),'y,y()),j('x,x(Amaxmax
)i(S),'y,y()),j('x,x(Amax
)j('x,xMI)j('x),i(SC
}y{)k(Sy
)}'y,y{(
))j('x,x(Y)'y,y(k
}y{)i(Sy
)}'y,y{(
))j('x,x(Y)'y,y(
0
00
0
00
??
??
?
??
??
?
?=
??
?
?
??
?
?
where A((x, x?(j)), (y, y), S(i)) denotes the plausibility of
alignment of (x, x?(j)) with (y, y) suggesting S(i).
The first factor in the above formula, i.e., the mutu-
al information between the polysemous word and the j-
th clue, is the base of the correlation.  The numerator
of the second factor is the maximum plausibility of
alignments that suggest the i-th sense of the polyse-
mous word.  The denominator of the second factor
has been introduced to normalize the plausibility.
We define the plausibility of alignment suggesting a
sense as the weighted sum of the correlations between
the sense and the common related words, i.e.,
( )
( )."x),i(SC)"x(w
)i(S),'y,y()),j('x,x(A
))'y,y()),j('x,x((Z"x
?
?
?
=
As the definition of the correlation between senses
and clues is recursive, we calculate it iteratively with
the following initial values: C0(S(i), x?(j))=MI(x, x?(j)).
The number of iteration was set at 6 experimen-
tally.
Figure 3 shows how the correlation values converge.
?Troop? demonstrates a typical pattern of convergence;
namely, while the correlation with the relevant sen-
se is kept constant, that with the irrelevant sense
decreases as the iteration proceeds.  ?Ozone? de-
monstrates the effect of the wild-card pair.  Note
that the correlation values due to an alignment with
the wild-card pair begin to diverge in the second
cycle of iteration.  The alignment with the wild-
card pair, which is shared by all senses, does not
produce any distinction among the senses in the
first cycle of iteration; the divergence is caused by
the difference in correlation values between the
senses and the common related words.
3.5 Selection of the sense of a polysemous word
Consulting sense-vs.-clue correlation data acquired by
the method described in the preceding sections, we
select a sense for each instance of a polysemous word x
in a text.  The score of each sense of the polysemous
word is defined as the sum of the correlations between
the sense and clues appearing in the context, i.e.,
( ) ( )?
?
=
)x(Context)j('x
)j('x),i(SC)i(SScore .
A window of 51 words (25 words before the polyse-
mous word and 25 words after it) is used as the context.
Scores of all senses of a polysemous word are calcu-
lated, and the sense whose score is largest is selected as
the sense of the instance of the polysemous word.
When all scores are zero, no sense can be selected; the
case is called ?inapplicable?.
4 Experiment
4.1 Experimental method
We evaluated our method through an experiment using
corpora of English and Japanese newspaper articles.
The first language was English and the second lan-
guage was Japanese.  A Wall Street Journal corpus
(July, 1994 to Dec., 1995; 189 Mbytes) and a Nihon
Keizai Shimbun corpus (Dec., 1993 to Nov., 1994; 275
Mbytes) were used as the training comparable corpus.
EDR (Japan Electronic Dictionary Research Institute)
English-to-Japanese and Japanese-to-English diction-
aries were merged for the experiment.  The resulting
dictionary included 269,000 English nouns and
276,000 Japanese nouns.  Pairs of related words were
extracted from the corpus of each language under the
following parameter settings:
- threshold for occurrence frequencies of words: 10
- threshold for mutual information: 0.0
These settings were common to the English and Ja-
panese corpora.
We selected 60 English polysemous nouns as the
test words.  Words whose different senses appear in
newspapers were preferred.  The frequencies of the
test words in the training corpus ranged from 39,140
(?share?, the third noun in descending order of fre-
quency) to 106 (?appreciation?, the 2,914th noun).
0
0.5
1
1.5
2
2.5
3
0 1 2 3 4 5 6 7 8 9 10
Iteration
Co
rre
lat
ion
C({tank, ???<TANKU>, ??<SUISOU>, ?<SOU>}, troop)
C({tank, ??<SENSYA>}, troop)
C({tank, ???<TANKU>, ??<SUISOU>, ?<SOU>}, ozone)
C({tank, ??<SENSYA>}, ozone)
C({tank, ???<TANKU>, ??<SUISOU>, ?<SOU>}, safety)
C({tank, ??<SENSYA>}, safety)
Fig. 3 Convergence of correlation between
senses and clues
We defined the senses of each test word.  The number
of senses per test word ranged from 2 to 8, and the
average was 3.4.  For each test word, sense-vs.-clue
correlation data were acquired by the method described
in Sections 3.2 through 3.4.  175 clues on average
were acquired for each test word.
For evaluation, we selected 100 test passages per
test word from a Wall Street Journal corpus (Jan., 1996
to Dec. 1996) whose publishing period was different
from that of the training corpus.  The instances of test
words positioned in the center of each test passage
were disambiguated by the method described in Sec-
tion 3.5, and the results were compared with the manu-
ally selected senses.
4.2 Results and evaluation
We used two measurements, applicability and precision
(Dagan and Itai 1994), to evaluate the performance of
our method.  The applicability is the proportion of
instances of the test word(s) that the method could
disambiguate.  The precision is the proportion of dis-
ambiguated instances of the test word(s) that the
method disambiguated correctly.  The applicability
and precision of the proposed method, averaged over
the 60 test polysemous words, were 88.5% and 77.7%,
respectively.
The performance of our method on six out of the 60
test words is summarized in Table 1.  That is, the in-
stances are classified according to the correct sense and
the sense selected by our method.  These results show
that the performance varies according to the test words,
that our method is better in the case of frequent senses,
but worse in the case of infrequent senses, and that our
method can easily distinguish topic-specific senses, but
not generic senses.
We consider the reason for the poor performance
concerning ?measure? [Table 1(a)] and ?race? [Table
1(c)] as follows.  The second sense of ?measure?,
{measure, ? ? <TAISAKU>, ? ? <SYUDAN>, ? ?
<SYOTI>}, is a very generic sense; therefore effective
clues for identifying the sense could not be acquired.
The first sense of ?race?, {race, ???<REESU>, ??
<KYOUSOU>, ?? <KYOUSOU>, ?? <ARASOI>, ?
<SEN>}, is specific to the ?race for the presidency?
topic and the second sense of ?race?, {race, ??
<ZINSYU>, ?? <MINZOKU>, ?? <SYUZOKU>}, is
specific to the ?racial discrimination? topic; however,
both topics are related to ?politics? and, therefore,
many clues were shared by these two senses.
Comparison with a baseline method, which selects
the most frequent sense of each polysemous word
independently of contexts, was also done.  Since large
sense-tagged corpora were not available, we simulated
the baseline method with a modified version of the
proposed method; namely, for each polysemous word,
the sense that maximizes the sum of correlations with
all clues was selected as the most frequent sense.  The
applicability of the baseline method is 100%, while that
of the proposed method is less than 100%.  To com-
pare with the baseline method, the proposed method
was substituted with the proposed method + baseline
method; namely, the baseline method was applied
when the proposed method was inapplicable.
The average precisions of the baseline method and
the proposed method + baseline method, both of which
attained 100% applicability, were 62.8% and 73.4%
respectively.  Figure 4 visualizes the superiority of the
proposed method + baseline method; the 60 test poly-
semous words are scattered on a plane whose horizon-
tal and vertical coordinates represent the precision of
the baseline method and that of the proposed method +
baseline method, respectively.
5 Discussion
Although it has produced promising results, the devel-
oped WSD method has a few problems.  These limi-
tations, along with future extensions, are discussed
below.
(1) Multilingual distinction of senses
The developed method is based on the premise that
the senses of a polysemous word in a language are
lexicalized differently in another language.  However,
the premise is not always true; that is, the ambiguity of
a word may be preserved by its translations.  As de-
scribed in Section 3.1, we preferably use translations
that do not preserve the ambiguity.  However, doing
so is useless unless such translations are frequently
used words.  An essential approach to solving this
problem is to use two or more second languages (Res-
nik and Yarowsky 2000).
(2) Use of syntactic relations
The developed method extracts clues for WSD ac-
cording to co-occurrence in a window.  However, it is
obvious that doing this is not suitable for all polyse-
mous words.  Syntactic co-occurrence is more useful
for disambiguating some sorts of polysemous words.
It is an important and interesting research issue to ex-
tend our method so that it can acquire clues according
to syntactic co-occurrence.  This extended method
does not replace the present method; however, we
0
0.2
0.4
0.6
0.8
1
0 0.2 0.4 0.6 0.8 1
Baseline method
Pr
op
os
ed
 m
eth
od
 + 
ba
se
lin
e
me
tho
d
Fig. 4 Precision of sense selection
should combine both methods or use the one suitable
for each polysemous word.  It should be noted that
this extension also enables disambiguation of polyse-
mous verbs.
The framework of the method is compatible with
syntactic co-occurrence.  Basically, we only have to
incorporate a parser into the step of extracting pairs of
related words.  A parser of the first language is indis-
pensable, but a parser of the second language is not.
As for the second language, we can use co-occurrence
in a small-sized window instead of syntactic co-
occurrence.
6 Comparison with other methods
While our method aligns pairs of related words that are
statistically extracted, WSD using parallel corpora
aligns instances of words (Brown, et al 1991).  Both
alignment techniques are quite different.  Actually,
from the technological viewpoint, our method is close
to WSD using a second-language monolingual corpus
Table 1 Results of sense selection for six polysemous words
(a) Polysemous word ?measure? (applicability=91.0%; precision=48.4%)
Results
Correct sense S1 S2 S3 ? Total
S1={measure, ?? , ??? , ? , ?? ,
??, ??, ??, ??, ??}
20 0 13 4 37
S2={measure, ??, ??, ??} 4 0 29 5 38
S3={measure, ??, ??, ??} 1 0 24 0 25
Total 25 0 66 9 100
[Note]
S1: a system or instrument for calculating
amount, size, weight, etc.
S2: an action taken to gain a certain end
S3: a law suggested in Parliament
(b) Polysemous word ?promotion? (applicability=96.0%; precision=89.6%)
Results
Correct sense S1 S2 S3 ? Total
S1={promotion, ??, ????, ????, ???????} 71 1 0 1 73
S2={promotion, ??, ??, ??, ??, ??, ??} 6 15 0 3 24
S3={promotion, ??, ??, ??, ??, ??} 2 1 0 0 3
Total 79 17 0 4 100
[Note]
S1: an activity intended to
help sell a product
S2: advancement in rank or
position
S3: action to help something
develop or succeed
(c) Polysemous word ?race? (applicability=79.0%; precision=57.0%)
Results
Correct sense S1 S2 S3 ? Total
S1={race, ???, ??, ??, ??, ?} 28 33 0 15 76
S2={race, ??, ??, ??} 1 17 0 6 24
S3={race, ??, ??} 0 0 0 0 0
Total 29 50 0 21 100
[Note]
S1: any competition, or a contest of speed
S2: one of the groups that humans can be divid-
ed into according to physical features, his-
tory, language, etc.
S3: a channel for a current of water
(d) Polysemous word ?tank? (applicability=89.0%; precision=89.9%)
Results
Correct sense S1 S2 ? Total
S1={tank, ???, ??, ?} 57 1 6 64
S2={tank, ??} 8 23 5 36
Total 65 24 11 100
[Note]
S1: a large container for storing liquid or gas
S2: an enclosed heavily armed, armored vehicle
(e) Polysemous word ?title? (applicability=92.0%; precision=81.5%)
Results
Correct sense S1 S2 S3 S4 ? Total
S1={title, ???, ??, ??} 43 1 0 0 2 46
S2={title, ??, ??, ??, ??} 6 26 0 1 5 38
S3={title, ??, ??, ???} 1 1 0 1 1 4
S4={title, ???} 3 3 0 6 0 12
Total 53 31 0 8 8 100
[Note]
S1: a word or name given to a person to
be used before his/her name as a sign
rank, profession, etc.
S2: a name given to a book, play, etc.
S3: the legal right to own something
S4: the position of being the winner of an
sports competition
(f) Polysemous word ?trial? (applicability=92.0%; precision=92.4%)
Results
Correct sense S1 S2 S3 S4 S5 ? Total
S1={trial, ??, ??, ??} 62 3 0 0 0 5 70
S2={trial, ??, ??, ??, ??, ??} 4 23 0 0 0 2 29
S3={trial, ??} 0 0 0 0 0 1 1
S4={trial, ??, ???} 0 0 0 0 0 0 0
S5={trial, ??, ??, ??} 0 0 0 0 0 0 0
Total 66 26 0 0 0 8 100
[Note]
S1: a legal process in which a court
examines a case
S2: a process of testing to determine
quality, value, usefulness, etc.
S3: a sports competition that tests a
player?s ability
S4: annoying thing or person
S5: difficulties and troubles
(Dagan and Itai 1994; Kikui 1998), where instances of
co-occurrence in a first-language text are aligned with
co-occurrences statistically extracted from the second-
language corpus.  A comparison of our method with
WSD using a second-language monolingual corpus is
given below.
First, our method performs alignment during the
acquisition phase, and transforms word-word correla-
tion data into sense-clue correlation data, which is far
more informative than the original word-word correla-
tion data.  In contrast, a method using a second-
language monolingual corpus uses original word-word
correlation data during the disambiguation phase.
This difference results in a difference in the perfor-
mance of WSD, particularly in a poor-context situation
(e.g., query translation).
Second, our method can acquire sense-clue correla-
tion even from a pair of related words for which align-
ment results in failure [e.g., C({tank, ???<TANKU>,
??<SUISOU>, ?<SOU>}, ozone) in Figure 3].  On
the contrary, a conventional WSD method using a sec-
ond-language monolingual corpus uses only pairs of
related words for which alignment results in success.
Thus, our method can elicit more information than the
conventional method.
Tanaka and Iwasaki (1996) exploited the idea of
translingually aligning word co-occurrences to extract
pairs consisting of a word and its translation form a
non-aligned (comparable) corpus.  The essence of
their method is to obtain a translation matrix that
maximizes the distance between the co-occurrence
matrix of the first language and that of the second lan-
guage.  Their method is useful for extracting corpus-
dependent translations; however, it does not extract
knowledge for WSD, i.e., which co-occurring word
suggests which sense or translation.
7 Conclusion
A method for word sense disambiguation using a bilin-
gual comparable corpus together with sense definitions
by translations into another language was developed.
In this method, knowledge for WSD, i.e., sense-vs.-
clue correlation, is acquired in an unsupervised fashion
as follows.  First, statistically significant pairs of relat-
ed words are extracted from the corpus of each lan-
guage.  Then, aligning pairs of related words translin-
gually, the correlation between the senses of a polyse-
mous word and the clues, i.e., the words related to the
polysemous word, is calculated.  In order to overcome
both the problem of ambiguity in the translingual
alignment of pairs of related words and that of disparity
of topical coverage between corpora of different lan-
guages, an iterative algorithm for calculating the cor-
relation was developed.
WSD for each instance of the polysemous word is
done by selecting the sense that maximizes the score,
i.e., the sum of the correlations between each sense and
the clues appearing in the context of the instance.  An
experiment using corpora of English and Japanese
newspaper articles showed that the performance of the
new method is promising: the applicability and preci-
sion of sense selection were 88.5% and 77.7%, respec-
tively, averaged over 60 test polysemous words.
Acknowledgments: This research was sponsored in
part by the Telecommunications Advancement Organi-
zation of Japan.
References
Basili, Roberto, Michelangelo Della Rocca, and Maria
Tereza Pazienza. 1997. Towards a bootstrapping frame-
work for corpus semantic tagging. In Proceedings of the
ACL-SIGLEX Workshop ?Tagging Text with Lexical Se-
mantics: Why, What, and How?? pages 66-73.
Brown, Peter F., Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer.  1991. Word-sense disam-
biguation using statistical methods. In Proceedings of the
29th Annual Meeting of the ACL, pages 264-270.
Dagan, Ido and Alon Itai. 1994. Word sense disambiguation
using a second language monolingual corpus. Computa-
tional Linguistics, 20(4): 563-596.
Hearst, Marti A. 1991. Noun homograph disambiguation
using local context in large corpora. In Proceedings of the
7th Annual Conference of the Centre for the New OED and
Text Research: Using Corpora, pages 1-22.
Ide, Nancy and Jean Veronis. 1998. Introduction to the spe-
cial issue on word sense disambiguation: the state of the art.
Computational Linguistics, 24(1): 1-40.
Kaji, Hiroyuki, Yasutsugu Morimoto, Toshiko Aizono,
and Noriyuki Yamasaki. 2000. Corpus-dependent
association thesaurus for information retrieval, In
Proceedings of the 18th International Conference on
Computational Linguistics, pages 404-410.
Karov, Yael and Shimon Edelman. 1998. Similarity-based
word sense disambiguation. Computational Linguistics,
24(1): 41-59.
Kikui, Genichiro. 1998. Term-list translation using mono-
lingual word co-occurrence vectors. In Proceedings of the
17th International Conference on Computational Linguis-
tics, pages 670-674.
Miller, George A. 1990. WordNet: an on-line lexical data-
base. International Journal of Lexicography, 3(4): 235-
312.
Rapp, Reinhard. 1995. Identifying word translations in non-
parallel texts. In Proceedings of the 33rd Annual Meeting
of the ACL, pages 320-322.
Resnik, Philip and David Yarowsky. 2000. Distinguishing
systems and distinguishing senses: new evaluation
methods for word sense disambiguation. Natural Lan-
guage Engineering, 5(2): 113-133.
Tanaka, Kumiko and Hideya Iwasaki. 1996. Extraction of
lexical translations from non-aligned corpora, In Proceed-
ings of the 16th International Conference on Computation-
al Linguistics, pages 580-585.
Yarowsky, David. 1992. Word sense disambiguation using
statistical models of Roget's categories trained on large cor-
pora. In Proceedings of the 14th International Conference
on Computational Linguistics, pages 454-460.
Yarowsky, David. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. In Proceedings of
the 33rd Annual Meeting of the ACL, pages 189-196.
 Word Sense Acquisition from Bilingual Comparable Corpora 
 
Hiroyuki Kaji 
Central Research Laboratory, Hitachi, Ltd. 
1-280 Higashi-Koigakubo, Kokubunji-shi, Tokyo 185-8601, Japan 
kaji@crl.hitachi.co.jp 
 
 
 
 
 
Abstract 
Manually constructing an inventory of word 
senses has suffered from problems including 
high cost, arbitrary assignment of meaning to 
words, and mismatch to domains. To over-
come these problems, we propose a method 
to assign word meaning from a bilingual 
comparable corpus and a bilingual dictionary. 
It clusters second-language translation 
equivalents of a first-language target word on 
the basis of their translingually aligned dis-
tribution patterns. Thus it produces a hierar-
chy of corpus-relevant meanings of the target 
word, each of which is defined with a set of 
translation equivalents. The effectiveness of 
the method has been demonstrated through an 
experiment using a comparable corpus con-
sisting of Wall Street Journal and Nihon Kei-
zai Shimbun corpora together with the EDR 
bilingual dictionary. 
1  Introduction 
Word Sense Disambiguation (WSD) is an important 
subtask that is necessary for accomplishing most natu-
ral language processing tasks including machine 
translation and information retrieval. A great deal of 
research on WSD has been done over the past decade 
(Ide and Veronis, 1998). In contrast, word sense acqui-
sition has been a human activity; inventories of word 
senses have been constructed by lexicographers based 
on their intuition. Manually constructing an inventory 
of word senses has suffered from problems such as 
high cost, arbitrary division of word senses, and mis-
match to application domains. 
We address the problem of word sense acquisition 
along the lines of the WSD where word senses are 
defined with sets of translation equivalents in another 
language. Bilingual corpora or second-language cor-
pora enable unsupervised WSD (Brown, et al, 1991; 
Dagan and Itai, 1994). However, the correspondence 
between senses of a word and its translations is not 
one-to-one, and therefore we need to prepare an in-
ventory of word senses, each of which is defined with 
a set of synonymous translation equivalents. Although 
conventional bilingual dictionaries usually group 
translations according to their senses, the grouping 
differs by dictionary. In addition, senses specific to a 
domain are often missing while many senses irrelevant 
to the domain or rare senses are included. To over-
come these problems, we propose a method for pro-
ducing a hierarchy of clusters of translation equiva-
lents from a bilingual corpus and a bilingual diction-
ary. 
To the best of our knowledge, there are two pre-
ceding research papers on word sense acquisition (Fu-
kumoto and Tsujii, 1994; Pantel and Lin, 2002). Both 
proposed distributional word clustering algorithms that 
are characterized by their capabilities to produce 
overlapping clusters. According to their algorithms, a 
polysemous word is assigned to multiple clusters, each 
of which represents one of its senses. These and our 
approach differ in how to define the word sense, i.e., a 
set of synonyms in the same language versus a set of 
translation equivalents in another language. Schuetze 
(1998) proposed a method for dividing occurrences of 
a word into classes, each of which consists of contex-
tually similar occurrences. However, it does not pro-
duce definitions of senses such as sets of synonyms 
and sets of translation equivalents. 
2  Basic Idea 
2.1  Clustering of translation equivalents 
Most work on automatic extraction of synonyms from 
text corpora rests on the idea that synonyms have 
                                                               Edmonton, May-June 2003
                                                               Main Papers , pp. 32-39
                                                         Proceedings of HLT-NAACL 2003
 similar distribution patterns (Hindle, 1990; Peraira, et 
al., 1993; Grefenstette, 1994). This idea is also useful 
for our task, i.e., extracting sets of synonymous trans-
lation equivalents, and we adopt the approach to dis-
tributional word clustering. 
We need to mention that the singularity of our task 
makes the problem easier. First, we do not have to 
cluster all words of a language, but we only have to 
cluster a small number of translation equivalents for 
each target word, whose senses are to be extracted, 
separately. As a result, the problem of computational 
efficiency becomes less serious. Second, even if a 
translation equivalent itself is polysemous, it is not 
necessary to consider senses that are irrelevant to the 
target word. A translation equivalent usually represents 
one and only one sense of the target word, at least in 
case the language-pair is those with different origins 
like English and Japanese. Therefore, a 
non-overlapping clustering algorithm, which is far 
simpler than overlapping clustering algorithms, is suf-
ficient. 
2.2  Translingual distributional word clustering 
In conventional distributional word clustering, a word 
is characterized by a vector or weighted set consisting 
of words in the same language as that of the word it-
self. In contrast, we propose a translingual distribu-
tional word clustering method, whereby a word is 
characterized by a vector or weighted set consisting of 
words in another language. It is based on the 
sense-vs.-clue correlation matrix calculation method 
we originally developed for unsupervised WSD (Kaji 
and Morimoto, 2002). That method presupposes that 
each sense of a target word x is defined with a syno-
nym set consisting of the target word itself and one or 
more translation equivalents which represent the sense. 
It calculates correlations between the senses of x and 
the words statistically related to x, which act as clues 
for determining the sense of x, on the basis of 
translingual alignment of pairs of related words. Rows 
of the resultant correlation matrix are regarded as 
translingual distribution patterns characterizing trans-
lation equivalents.  
Sense-vs.-clue correlation matrix calculation 
method *) 
1) Alignment of pairs of related words 
                                                   
*) A description of the wild-card pair of related words, which plays 
an essential role in recovering alignment failure, has been omitted 
for simplicity. 
Let X(x) be the set of clues for determining the sen-
se of a first-language target word x. That is, 
X(x)={x?|(x, x?)?RX}, 
where RX denotes the collection of pairs of related 
words extracted from a corpus of the first language. 
Henceforth, the j-th clue for determining the sense of x 
will be denoted as x?(j). Furthermore, let Y(x, x?(j)) be 
the set consisting of all second-language counterparts 
of a first-language pair of related words x and x?(j). 
That is, 
Y(x, x?(j)) = {(y, y?) | (y, y?)?RY, (x, y)?D, 
(x?(j), y?)?D}, 
where RY denotes the collection of pairs of related 
words extracted from a corpus of the second language, 
and D denotes a bilingual dictionary, i.e., a collection 
of pairs consisting of a first-language word and a 
second-language word that are translations of one an-
other. 
Then, for each alignment, i.e., pair of (x, x?(j)) and 
(y, y?) (?Y(x, x?(j))), a weighted set of common re-
lated words Z((x, x?(j)), (y, y? )) is constructed as fol-
lows: 
Z((x, x?(j)), (y, y? )) = {x? / w(x?) | (x, x?)?RX, 
(x?(j), x?)?RX}. 
The weight of x?, denoted as w(x?), is determined as 
follows: 
- w(x?) = 1+??MI(y, y?) when ?y? (x?, y?)?D, 
(y, y?)?RY, and (y?, y?)?RY . 
- w(x?) = 1 otherwise. 
This is where MI(y, y?) is the mutual information of y 
and y?. The coefficient ? was set to 5 in the experiment 
described in Section 4. 
2) Calculation of correlation between senses and clues 
The correlation between the i-th sense S(x, i) and 
the j-th clue x?(j) is defined as: 
( ) ( )
( ) ( )( )
( ) ( )( )
.
),(,,,)(,maxmax
),(,,,)(,max
)(,)(),,(
),(
)),(,(Y),(
),(
)),(,(),(
??
??
?
??
??
?
?=
?
?
?
?
kxSy'yjx'xA
ixSy'yjx'xA
jx'xMIjx'ixSC
kxSy
jx'xy'yk
ixSy
jx'xYy'y
 
This is where MI(x, x?(j)) is the mutual information of 
x and x?(j), and A((x, x?(j)), (y, y?), S(x,i)), the plausi-
bility of alignment of (x, x?(j)) with (y, y?) suggesting 
S(x, i), is defined as the weighted sum of the correla-
tions between the sense and the common related words, 
i.e., 
( )
( ).),,()w(
),(),,()),(,(
)),()),(,((
?
?
?
=
y'yjx'xZx"
x"ixSCx"
ixSy'yjx'xA
 
The correlations between senses and clues are cal-
 culated iteratively with the following initial values: 
C0(S(x, i), x?(j))=MI(x, x?(j)). The number of iterations 
was set to 6 in the experiment. Figure 1 shows how the 
correlation values converge. 
Advantages of using translingually aligned 
distribution patterns 
Translingual distributional word clustering has advan-
tages over conventional monolingual distributional 
word clustering, when they are used to cluster transla-
tion equivalents of a target word. First, it avoids clus-
ters being degraded by polysemous translation equiva-
lents. Let ?race? be the target word. One of its 
translation equivalents, ????<REESU>?, is a poly-
semous word representing ?lace? as well as ?race?. 
According to monolingual distributional word cluster-
ing, ????<REESU>? is characterized by a mixture of 
the distribution pattern for ????<REESU>? repre-
senting ?race? and that for ????<REESU>? repre-
senting ?lace?, which often results in degraded clusters. 
In contrast, according to translingual distributional 
word clustering, ????<REESU>? is characterized by 
the distribution pattern for the sense of ?race? that 
means ?competition?. 
Second, translingual distributional word clustering 
can exclude from the clusters translation equivalents 
irrelevant to the corpus. For example, a bilingual dic-
tionary renders ???<TOKUCHOU>? (?feature?) as a 
translation of ?race?, but that sense of ?race? is used 
infrequently. If it is the case in a given domain, ???
<TOKUCHOU>? has low correlation with most words 
related to ?race?, and can therefore be excluded from 
any clusters. 
We should also mention the data-sparseness prob-
lem that hampers distributional word clustering. Gen-
erally speaking, the problem becomes more difficult in 
translingual distributional word clustering, since the 
sparseness of data in two languages is multiplied. 
However, the sense-vs.-clue correlation matrix calcu-
lation method overcomes this difficulty; it calculates 
the correlations between senses and clues iteratively to 
smooth out the sparse data. 
Translingual distributional word clustering can also 
be implemented on the basis of word-for-word align-
ment of a parallel corpus. However, availability of 
large parallel corpora is extremely limited. In contrast, 
the sense-vs.-clue correlation calculation method ac-
cepts comparable corpora which are available in many 
domains. 
2.3  Similarity based on subordinate distribu-
tion pattern 
Naive translingual distributional word clustering based 
on the sense-vs.-clue correlation matrix calculation 
method is outlined in the following steps: 
1) Define the sense of a target word by using each 
translation equivalent. 
2) Calculate the sense-vs.-clue correlation matrix for 
the set of senses resulting from step 1). 
3) Calculate similarities between senses on the basis 
of distribution patterns shown by the sense-vs.-clue 
correlation matrix. 
4) Cluster senses by using a hierarchical agglomera-
tive clustering method, e.g., the group-average 
method. 
However, this naive method is not effective be-
cause some senses usually have duplicated definitions 
in step 1) despite the fact that the sense-vs.-clue corre-
lation matrix calculation algorithm presupposes a set 
of senses without duplicated definitions. The algo-
rithm is based on the ?one sense per collocation? hy-
pothesis, and it results in each clue having a high cor-
relation with one and only one sense. A clue can never 
have high correlations with two or more senses, even 
when they are actually the same sense. Consequently, 
synonymous translation equivalents do not necessarily 
have high similarity. 
Figure 2(a) shows parts of distribution patterns for 
0.0
0.5
1.0
1.5
2.0
2.5
0 1 2 3 4 5 6 7 8 9 10
Iteration
Co
rre
lat
ion
C(S1, brand) C(S2, brand)
C(S3, brand) C(S1, woman)
C(S2, woman) C(S3, woman)
 
S1={promotion, ?? <SENDEN>, ? ? ??? ? ?
<PUROMOUSHON>,  ????<URIKOMI>, ?} 
 (?an activity intended to help sell a product?) 
S2={promotion, ??<SHOUKAKU>, ??<SHOUSHIN>, 
??<TOUYOU>, ?} 
 (?advancement in rank or position?) 
S3={promotion, ??<SHOUREI>, ??<SHINKOU>, ?
?<JOCHOU>,?} 
 (?action to help something develop or succeed?) 
 
Figure 1. Convergence of correlation between 
senses and clues. 
 {promotion, ??<SENDEN>}, {promotion, ????
???<PUROMOUSHON>}, and {promotion, ????
<URIKOMI>} all of which define the ?sales activity? 
sense of ?promotion?. We see that most clues for se-
lecting that sense have higher correlation with {pro-
motion, ??<SENDEN>} than with {promotion, ??
?????<PUROMOUSHON>} and {promotion, ??
??<URIKOMI>}. This is because ???<SENDEN>? 
is the most dominant translation equivalent of ?promo-
tion? in the corpus. 
To resolve the above problem, we calculated the 
sense-vs.-clue correlation matrix not only for the full 
set of senses but also for the set of senses excluding 
one of these senses. Excluding a definition of the sense, 
which includes the most dominant translation equiva-
lent, allows most clues for selecting the sense to have 
the highest correlations with another definition of the 
same sense, which includes the second most dominant 
translation equivalent. Figure 2(b) shows parts of dis-
tribution patterns for {promotion, ???????
<PUROMOUSHON>} and {promotion, ? ? ? ?
<URIKOMI>} shown by the sense-vs.-clue correlation 
matrix for the set of senses excluding {promotion, ?
?<SENDEN>}. We see that most clues for selecting 
the ?sales activity? sense have higher correlations with 
{promotion, ???????<PUROMOUSHON>} than 
with {promotion, ????<URIKOMI>}. This is be-
cause ????????<PUROMOUSHON>? is the sec-
ond most dominant translation equivalent in the corpus. 
We also see that the distribution pattern for {promo-
tion, ???????<PUROMOUSHON>} in Fig. 2(b) is 
more similar to that for {promotion, ??<SENDEN>} 
in Fig. 2(a) than that for {promotion, ???????
<PUROMOUSHON>} in Fig. 2(a). 
We call the distribution pattern for sense S2, result-
ing from the sense-vs.-clue correlation matrix for the 
set of senses excluding sense S1, the distribution pat-
tern for S2 subordinate to S1, while we call the distri-
bution pattern for sense S2, resulting from the 
sense-vs.-clue correlation matrix for the full set of 
senses, simply the distribution pattern for S2. We de-
fine the similarity of S2 to S1 as the similarity of the 
distribution pattern for S2 subordinate to S1 to the dis-
tribution pattern for S1. 
Calculating the sense-vs.-clue correlation matrix 
for a set of senses excluding one sense is of course 
insufficient since three or more translation equivalents 
may represent the same sense of the target word. We 
should calculate the sense-vs.-clue correlation matrices 
both for the full set of senses and for the set of senses 
excluding one of these senses again, after merging 
similar senses into one. Repeating these procedures 
enables corpus-relevant but less dominant translation 
equivalents to be drawn up, while corpus-irrelevant 
ones are never drawn up. Thus, a hierarchy of cor-
pus-relevant senses or clusters of corpus-relevant 
translation equivalents is produced. 
3  Proposed Method 
3.1  Outline 
As shown in Fig. 3, our method repeats the following 
three steps: 
1) Calculate sense-vs.-clue correlation matrices both 
for the full set of senses and for a set of senses ex-
cluding each of these senses. 
2) Calculate similarities between senses on the basis 
of distribution patterns and subordinate distribution 
patterns. 
3) Merge each pair of senses with high similarity 
into one. 
The initial set of senses is given as ?(x)={{x, y1}, {x, 
y2}, ?, {x, yN}} where x is a target word in the first 
language, and y1, y2, ?, and yN are translation equiva-
lents of x in the second-language.  Translation 
equivalents that occur less frequently in the sec-
ond-language corpus can be excluded from the initial 
0
1
2
3
4
5
6
7
Ac
cla
im
ad
 ca
mp
aig
n
aff
irm
ati
ve
an
aly
st 
say Au
di
Ba
tm
an
bra
nd
Bu
rge
r K
ing
car
eer
cer
eal
Co
ca-
Co
la
Co
nra
il
Co
ors
 Li
gh
t
Cy
rk
dis
cri
mi
na
tio
n
em
plo
ye
e
fil
m
Ge
ne
ral
Hi
spa
nic
ind
ust
ry job lab
el
las
t y
ear
ma
na
ge
me
nt
Clue
Co
rre
lat
ion
{promotion, ??<SENDEN>}
{promotion, ???????<PUROMOUSHON>}
{promotion, ????<URIKOMI>}  
(a) Distribution patterns 
0
1
2
3
4
5
6
7
Clue
Co
rre
lat
ion
 
(b) Distribution patterns subordinate to 
 {promotion, ??<SENDEN>} 
Figure 2. Distribution Patterns for Some Senses 
of ?promotion?. 
 set to shorten the processing time. The details of the 
steps are described in the following sections. 
3.2  Calculation of sense-vs.-clue correlation 
matrices 
First, a sense-vs.-clue correlation matrix is calculated 
for the full set of senses. The resulting correlation ma-
trix is denoted as C. That is, C(i, j) is the correlation 
between the i-th sense S(x,i) of a target word x and its 
j-th clue x?(j). 
Then a set of active senses, ?A(x), is determined. A 
sense is regarded active if and only if the ratio of clues 
with which it has the highest correlation exceeds a 
predetermined threshold ? (In the experiment in Sec-
tion 4, ? was set to 0.05). That is,  
{ }?))(()()( >= ix,SR|ix,Sx?A , 
where R(S(x, i)) denotes the ratio of clues having the 
highest correlation with S(x, i), i.e., 
})({)},(max),()({)),(( jx'jkCjiC|jx'ixSR
k
== . 
Thus ?A(x) consists of senses of the target word x that 
are relevant to the corpus. 
Finally, a sense-vs.-clue correlation matrix is cal-
culated for the set of senses excluding each of the ac-
tive senses. The correlation matrix calculated for the 
set of senses excluding the k-th sense is denoted as C-k. 
That is, C-k(i, j) (i?k) is the correlation between the 
i-th sense and the j-th clue that is calculated excluding 
the k-th sense. C-k(k, j) (j=1, 2, ...) are set to zero. This 
redundant k-th row is included to maintain the same 
correspondence between rows and senses as in C. 
3.3  Calculation of sense similarity matrix 
Similarity of the i-th sense S(x, i) to the j-th sense S(x, 
j), Sim(S(x, i), S(x, j)), is defined as the similarity of 
the distribution pattern for S(x, i) subordinate to S(x, j) 
to the distribution pattern of S(x, j). Note that this 
similarity is asymmetric and reflects which sense is 
more dominant in the corpus. It is probable that 
Sim(S(x, i), S(x, j)) is large but Sim(S(x, j), S(x, i)) is 
not when S(x, j) is more dominant than S(x, i). 
According to the sense-vs.-clue correlation matrix, 
each sense is characterized by a weighted set of clues. 
Therefore, we used the weighted Jaccard coefficient as 
the similarity measure. That is, 
{ }
{ }?
?
=
k
j-
k
j-
kj,Cki,C
kj,Cki,C
jxSixSSim )(),(max
)(),(min
)),(),,((  
when S(x, j)??A(x). 
0)),(),,(( =jxSixSSim  otherwise. 
It should be noted that a sense is characterized by dif-
ferent weighted sets of clues depending on which 
sense the similarity is calculated. Note also that inac-
tive senses are neglected because they are not reliable. 
3.4  Merging similar senses 
The set of senses is updated by merging every pair of 
mutually most-similar senses into one. That is, 
?(x) ? ?(x) ? {S(x, i), S(x, j)} + {S(x, i)?S(x, j)} 
if {maxmax)),(),,((
j'
jxSixSSim =  
))}},(),',(()),,(),,(({ ixSjxSSimj'xSixSSim ,  
{maxmax)),(),,((
i'
jxSixSSim =  
))}},(),,(()),,(),,(({ i'xSjxSSimjxSi'xSSim ,  
and ?>)),(),,(( jxSixSSim . 
The ? is a predetermined threshold for similarity, 
which is introduced to avoid noisy pairs of senses be-
ing merged. In the experiment in Section 4, ? was set 
to 0.25. 
If at least one pair of senses are merged, the whole 
procedure, i.e., the calculation of sense-vs.-clue ma-
trices through the merger of similar senses, is repeated 
for the updated set of senses. Otherwise, the clustering 
procedure terminates. 
Agglomerative clustering methods usually suffer 
from the problem of when to terminate merging. In our 
method described above, the similarity of senses that 
are merged into one does not necessarily decrease 
Initial set of senses 
Sense-vs.-clue correlation matrices 
Sense similarity matrix 
Updated set of senses 
Comparable 
corpus 
Bilingual 
dictionary 
Calculate similarities 
between distribution patterns 
Calculate correlations 
between senses and clues 
Merge similar senses 
 
Figure 3. Flow Diagram of Proposed Method. 
 monotonically, which makes the problem more diffi-
cult. At present, we are forced to output a dendrogram 
that represents the history of mergers and leave the 
final decision to humans. The dendrogram consists of 
translation equivalents that are included in active 
senses in the final cycle. Other translation equivalents 
are rejected as they are irrelevant to the corpus. 
4  Experimental Evaluation 
4.1  Experimental settings 
Our method was evaluated through an experiment us-
ing a Wall Street Journal corpus (189 Mbytes) and a 
Nihon Keizai Shimbun corpus (275 Mbytes). 
First, collected pairs of related words, which we 
restricted to nouns and unknown words, were obtained 
from each corpus by extracting pairs of words 
co-occurring in a window, calculating mutual informa-
tion of each pair of words, and selecting pairs with 
mutual information larger than the threshold. The size 
of the window was 25 words excluding function words, 
and the threshold for mutual information was set to 
zero. Second, a bilingual dictionary was prepared by 
collecting pairs of nouns that were translations of one 
another from the Japan Electronic Dictionary Research 
Institute (EDR) English-to-Japanese and Japa-
nese-to-English dictionaries. The resulting dictionary 
includes 633,000 pairs of 269,000 English nouns and 
276,000 Japanese nouns. 
Evaluating the performance of word sense acquisi-
tion methods is not a trivial task. First, we do not have 
a gold-standard sense inventory. Even if we have one, 
we have difficulty mapping acquired senses onto those 
in it. Second, there is no way to establish the complete 
set of senses appearing in a large corpus. Therefore, 
we evaluated our method on a limited number of target 
words as follows. 
We prepared a standard sense inventory by select-
ing 60 English target words and defining an average of 
3.4 senses per target word manually. The senses were 
rather coarse-grained; i.e., they nearly corresponded to 
groups of translation equivalents within the entries of 
everyday English-Japanese dictionaries. We then sam-
pled 100 instances per target word from the Wall 
Street Journal corpus, and we sense-tagged them 
manually. Thus, we estimated the ratios of the senses 
in the training corpus for each target word. 
We defined two evaluative measures, recall of 
senses and accuracy of sense definitions. The recall of 
senses is the proportion of senses with ratios not less 
than a threshold that are successfully extracted, and it 
varies with change of the threshold. We judged that a 
sense was extracted, when it shared at least one trans-
lation equivalent with some active sense in the final 
cycle. 
To evaluate the accuracy of sense definitions while 
avoiding mapping acquired senses onto those in the 
standard sense inventory, we regard a set of senses as a 
set of pairs of synonymous translation equivalents. Let 
TS be a set consisting of pairs of translation equiva-
lents belonging to the same sense in the standard sense 
inventory. Likewise, let T(k) be a set consisting of 
pairs of translation equivalents belonging to the same 
active sense in the k-th cycle. Further, let U be a set of 
pairs of translation equivalents that are included in 
active senses in the final cycle. Recall and precision of 
pairs of synonymous translation equivalents in the k-th 
cycle are defined as: 
UT
kTTkR
S
S
?
?
=
)()( . 
)(
)()( kT
kTTkP S ?= . 
Further, F-measure of pairs of synonymous translation 
equivalents in the k-th cycle is defined as: 
)()(
)()(2)( kPkR
kPkRkF
+
??
= . 
The F-measure indicates how well the set of active 
senses coincides with the set of sense definitions in the 
standard senses inventory. Although the current 
method cannot determine the optimum cycle, humans 
can identify the set of appropriate senses from a hier-
archy of senses at a glance. Therefore, we define the 
accuracy of sense definitions as the maximum 
F-measure in all cycles. 
4.2  Experimental results 
To simplify the evaluation procedure, we clustered 
translation equivalents that were used to define the 
senses of each target word in the standard sense in-
ventory, rather than clustering translation equivalents 
rendered by the EDR bilingual dictionary. The recall 
of senses for totally 201 senses of the 60 target words 
was: 
96% for senses with ratios not less than 25%, 
87% for senses with ratios not less than 5%, and 
78% for senses with ratios not less than 1%. 
The accuracy of sense definitions, averaged over the 
60 target words, was 77%. 
The computational efficiency of our method 
proved to be acceptable. It took 13 minutes per target 
word on a HP9000 C200 workstation (CPU clock: 200 
MHz, memory: 32 MB) to produce a hierarchy of 
clusters of translation equivalents. 
 Some clustering results are shown in Fig. 4. These 
demonstrate that our proposed method shows a great 
deal of promise. At the same time, evaluating the re-
sults revealed its deficiencies. The first of these lies in 
the crucial role of the bilingual dictionary. It is obvious 
that a sense is never extracted if the translation equiva-
lents representing it are not included in it. An 
exhaustive bilingual dictionary is therefore required. 
From this point of view, the EDR bilingual dictionary 
is fairly good. The second deficiency lies in the fact 
that it performs badly for low-frequency or non-topical 
senses. For example, the sense of ?bar? as the ?legal 
profession? was clearly extracted, but its sense as a 
?piece of solid material? was not extracted. 
We also compared our method with two alterna-
tives: monolingual distributional clustering mentioned 
in Section 2.2 and naive translingual clustering men-
tioned in Section 2.3. Figures 5(a), (b), and (c) show 
respective examples of clustering obtained by our 
method, the monolingual method, and the naive 
translingual method. Comparing (a) with (b) reveals 
the superiority of the translingual approach to the 
monolingual approach, and comparing (a) with (c) 
reveals the effectiveness of the subordinate distribu-
tion pattern introduced in Section 2.3. Note that delet-
ing the corpus-irrelevant translation equivalents from 
the dendrograms in both (b) and (c) would not result in 
appropriate ones. 
5  Discussion 
Our method has several practical advantages. One of 
these is that it produces a corpus-dependent inventory 
of word senses. That is, the resulting inventory covers 
most senses relevant to a domain, while it excludes 
senses irrelevant to the domain. 
Second, our method unifies word sense acquisition 
with word sense disambiguation. The sense-vs.-clue 
correlation matrix is originally used for word sense 
disambiguation. Therefore, our method guarantees that 
acquired senses can be distinguished by machines, and 
further it demonstrates the possibility of automatically 
optimizing the granularity of word senses. 
Some limitations of the present methods are dis-
cussed in the following with possible future extensions. 
First, our method produces a hierarchy of clusters but 
cannot produce a set of disjoint clusters. It is very im-
portant to terminate merging senses autonomously 
during an appropriate cycle. Comparing distribution 
patterns (not subordinate ones) may be useful to ter-
minate merging; senses characterized by complemen-
tary distribution patterns should not be merged. 
Second, the present method assumes that each 
translation equivalent represents one and only one sense of the target word, but this is not always the case. 
[Target word] 
Resulting dendrogram 
(English equivalent 
other than target 
word) 
[association] 
???????<KANKEI> 
??????????<KOUSAI> 
?          ????<TEIKEI> 
??????????<KANREN> 
??????????<KYOUDOU> 
?????????<RENGOU> 
????????<KUMIAI> 
???????<KYOUKAI> 
??????<KAI> 
?????<DANTAI> 
 
(relation) 
(friendship) 
(cooperation) 
(relation) 
(cooperation) 
(federation) 
(society) 
(society) 
(society) 
(organization) 
[bar] 
????????<URIBA> 
??????????<KAUNTAA> 
?  ????????<BAA> 
?    ???????<SHOUHEKI> 
??????????<KOUSHI> 
??????????<HOUSOU> 
?????????<BENGOSHI> 
????????<HOUTEI> 
 
(shop) 
(counter) 
(saloon) 
(obstacle) 
(lattice) 
(legal profession)
(lawyer) 
(law court) 
[discipline] 
????<KUNREN> 
??????<GAKKA> 
??  ????<GAKUMON> 
???????<KYOUKA> 
???????<CHITSUJO> 
??  ????<KISEI> 
??????<CHOUBATSU> 
??????<TOUSEI> 
?????<KIRITSU> 
 
(training) 
(subject of study) 
(learning) 
(subject of study) 
(order) 
(regulation) 
(punishment) 
(control) 
(order) 
[measure] 
?????<SHAKUDO> 
?????????<RYOU> 
?      ?????<SHISUU> 
?  ???????<SHUDAN> 
?????????<TAISAKU> 
??  ??????<KIJUN> 
????????<HOUREI> 
??????<GIAN> 
?????<HOUAN> 
 
(gauge) 
(quantity) 
(index) 
(means) 
(counter plan) 
(standard) 
(law) 
(bill) 
(bill) 
[promotion] 
????????<TOUYOU> 
??????????<SHOUSHIN> 
??????????<URIKOMI> 
??????????? 
??          <PUROMOUSHON> 
?????????<SENDEN> 
 
(elevation) 
(advancement) 
(sale) 
(advertising 
campaign) 
(advertisement) 
[traffic] 
?????<SHOUGYOU> 
??????<TORIHIKI> 
??????<BAIBAI> 
??????<TSUUKOU> 
??????<KOUTSUU> 
?????<UNYU> 
 
(commerce) 
(trade) 
(bargain) 
(passage) 
(transport) 
(transport) 
Figure 4. Examples of Clustering. 
 A Japanese Katakana word resulting from translitera-
tion of an English word sometimes represents multiple 
senses of the English word. It is necessary to detect 
and split translation equivalents representing more 
than one sense of the target word. 
Third, not only are acquired senses rather 
coarse-grained but also generic senses are difficult to 
acquire. One of the reasons for this may be that we 
rely on co-occurrence in the window. The fact that 
most distributional word clustering methods use syn-
tactic co-occurrence suggests that it is the most effec-
tive tool for extracting pairs of related words. 
6  Conclusion 
We presented a translingual distributional word clus-
tering method enabling word senses, exactly a hierar-
chy of clusters of translation equivalents, to be ac-
quired from a comparable corpus and a bilingual dic-
tionary. Its effectiveness was demonstrated through an 
experiment using Wall Street Journal and Nihon Kei-
zai Shimbun corpora and the EDR bilingual dictionary. 
The recall of senses was 87% for senses whose ratios 
in the corpus were not less than 5%, and the accuracy 
of sense definitions was 77%. 
Acknowledgments: This research was supported by 
the New Energy and Industrial Technology Develop-
ment Organization of Japan. 
References 
Brown, Peter F., Stephen A. Della Pietra, Vincent J. Della 
Pietra, and Robert L. Mercer. 1991. Word-sense disam-
biguation using statistical methods. In Proceedings of 
the 29th Annual Meeting of the Association for Com-
putational Linguistics, pages 264-270.  
Dagan, Ido and Alon Itai. 1994. Word sense disambigua-
tion using a second language monolingual corpus. 
Computational Linguistics, 20(4): 563-596. 
Fukumoto, Fumiyo and Junichi Tsujii. 1994. Automatic 
recognition of verbal polysemy. In Proceedings of the 
15th International Conference on Computational Lin-
guistics, pages 762-768. 
Grefenstette, Gregory. 1994. Explorations in Automatic 
Thesaurus Discovery. Kluwer Academic Publishers, 
Boston. 
Hindle, Donald. 1990. Noun classification from predi-
cate-argument structures. In Proceedings of the 28th 
Annual Meeting of the Association for Computational 
Linguistics, pages 268-275. 
Ide, Nancy and Jean Veronis. 1998. Introduction to the 
special issue on word sense disambiguation: The state 
of the art. Computational Linguistics, 24(1): 1-40. 
Kaji, Hiroyuki and Yasutsugu Morimoto. 2002. Unsuper-
vised word sense disambiguation using bilingual com-
parable corpora. In Proceedings of the 19th Interna-
tional Conference on Computational Linguistics, pages 
411-417. 
Pantel, Patrick and Dekang Lin. 2002. Discovering word 
senses from text. In Proceedings of the 8th ACM 
SIGKDD International Conference on Knowledge Dis-
covery and Data Mining, pages 613-619. 
Pereira, Fernando, Naftali Tishby, and Lillian Lee. 1993. 
Distributional clustering of English words. In Proceed-
ings of the 31st Annual Meeting of the Association for 
Computational Linguistics, pages 183-190. 
Schuetze, Hinrich. 1998. Automatic word sense dis-
crimination. Computational Linguistics, 24(1): 97-124. 
[race] 
   ??????<KEIRIN> 
 ???????<KEIBA> 
 ???????<REESU> 
 ???????<KOKUMIN> 
 ???????<MINZOKU> 
   ??????<JINSHU> 
 
(cycle race) 
(horse race) 
(competition) 
(nation) 
(ethnic) 
(human race) 
(a) Proposed method 
[race] 
???????????<HI> 
????????????????<KYOUSOU>
???                    ?????<REESU> 
???  ????????????????<KEIBA> 
?  ?          ????????????<JINSHU> 
?  ???????    ?????????<SHISSOU>
?              ????????????<SERIAI> 
?                  ??????????<KOKUMIN> 
?          ??????????????<HINKAKU> 
????????????????????<KEIRIN> 
??                        ??????<TOKUCHOU>
??          ?????????????<TOKUSEI> 
?      ???????????????<HINSHU> 
?  ???            ????????<FUUMI> 
???  ???????????????<SUIRO> 
?                  ???????<MINZOKU>
?????????????????<YOUSUI> 
 
 
(match) 
(competition) 
(competition) 
(horse race) 
(human race) 
(scamper) 
(competition) 
(nation) 
(dignity) 
(cycle race) 
(feature) 
(character) 
(kind) 
(flavor) 
(waterway) 
(ethnic) 
(water for 
irrigation) 
(b) Monolingual distributional clustering 
[race] 
?????????????????<KEIBA> 
???????????????????<REESU> 
??      ???????????????<JINSHU> 
????????????????????<MINZOKU>
?                          ??????<SERIAI> 
?                        ???????<SHISSOU> 
?                ???????????<HINKAKU> 
?                ?      ???????<HI> 
?          ????    ????????<SUIRO> 
?          ?    ???????????<TOKUSEI> 
?      ???    ???????????<KYOUSOU> 
?      ?  ?      ??????????<TOKUCHOU>
?  ???  ??????????????<FUUMI> 
?  ?  ?      ????????????<KEIRIN> 
???  ?  ??????????????<HINSHU> 
?  ????????????????<YOUSUI> 
? 
??????????????????<KOKUMIN>
 
(horse race) 
(competition) 
(human race) 
(ethnic) 
(competition) 
(scamper) 
(dignity) 
(match) 
(waterway) 
(character) 
(competition) 
(feature) 
(flavor) 
(cycle race) 
(kind) 
(water for 
irrigation) 
(nation) 
(c) Naive translingual distributional clustering 
Figure 5. Comparison with Alternatives. 
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1260?1268, Dublin, Ireland, August 23-29 2014.
 Enriching Wikipedia?s Intra-language Links  
by their Cross-language Transfer 
 
 
Takashi Tsunakawa  Makoto Araya  Hiroyuki Kaji 
Graduate School of Informatics, Shizuoka University 
3-5-1 Johoku, Naka-ku, Hamamatsu, Shizuoka 432-8011, Japan 
 
{tuna, araya, kaji}@inf.shizuoka.ac.jp 
 
  
 
Abstract 
Although hyperlinks enhance the utility of Wikipedia, embedding them in articles imposes a burden on 
contributors. To alleviate this burden as well as enrich hyperlinks in Wikipedia articles, we propose a 
method for transferring intra-language links between different-language articles linked via an inter-
language link. The method avoids anchor selection and disambiguation problems by which usual wikifi-
cation methods are affected, by exploiting the analogy between different language editions of Wikipedia. 
The effectiveness of the method was demonstrated through an experiment of transferring intra-language 
links from English to Japanese. It increased the number of intra-language links in Japanese articles by 
40.9%, and the accuracy of anchors selected was estimated to be 96.3%. 
1 Introduction 
Wikipedia is a Web-based encyclopedia constructed collaboratively by many contributors and contin-
ues to enlarge and improve daily. Because of its overwhelming scale, improved quality, and multilin-
gual nature, it has acquired a huge number of readers worldwide. One of the distinguishing features of 
Wikipedia is that it is a hypertext, which greatly enhances its usefulness and usability. That is, an arti-
cle is linked to its related articles in the same language via intra-language links as well as to its coun-
terpart articles in different languages via inter-language links (ILLs), and readers can navigate within 
millions of articles. 
Editing Wikipedia articles naturally includes linking them to their related articles, which imposes an 
additional burden on contributors. As a result, Wikipedia articles may remain incomplete; they some-
times lack important links as well as contain incorrect links. Thus, it is desirable to automate link-
related editorial tasks such as embedding links in new articles and verifying links in existing articles. 
Linking a plain text, usually non-Wikipedia articles, to Wikipedia articles is called wikification, and 
much effort has been devoted to developing a variety of wikification methods over the past decade 
(Mihalcea and Csomai, 2007; Milne and Witten, 2008a; Fogarolli, 2009; Ratinov et al., 2011). How-
ever, wikification methods are still immature and affected by two hard problems; anchor selection, 
which involves keyword extraction or term recognition, and destination-article determination, which is 
a kind of word sense disambiguation (WSD). 
We focused on the comparability of intra-language links between different language editions of 
Wikipedia, and developed a method for transferring intra-language links in one language edition to 
another language edition. Although the method is not applicable to texts other than Wikipedia articles, 
it avoids the problems of anchor selection and destination-article disambiguation by using analogy 
with different language editions. It does not require any language resources other than Wikipedia itself. 
When the target language is a morphologically rich one, a morphological analyzer is also required. 
Although the method is applicable to any language pairs, we evaluated its effectiveness through an 
This work is licensed under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer 
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 
 
1260
experiment of transferring intra-
language links from English to Japa-
nese. 
2 Basic Idea 
In Wikipedia, an article in one lan-
guage is often linked to another article 
in another language via an ILL. These 
two articles, which describe the same 
entity, concept or topic, are comparable. 
Note that this comparability holds not 
only for texts in articles but also for 
intra-language links, each of which 
links an anchor or an important term 
within an article to another same-
language article describing the entity, 
concept, or topic denoted by the an-
chor term. Figure 1 gives an example 
pair of ILL-linked articles; an English 
article ?Tata Motors? and a Japanese article ?????????.? The former has an intra-language 
link from an anchor ?Jaguar? to the English article ?Jaguar Cars,? while the latter has an intra-
language link from an anchor ?????? to the Japanese article ????? (???).? These two 
intra-language links are comparable: namely, the anchors are translations of one another and the desti-
nation articles are linked via an ILL.  
The above fact inspired us to develop a method for transferring intra-language links between ILL-
linked articles to enrich the intra-language links in each article. Suppose an extreme case in which an 
article ? in one language, which is linked to its counterpart ? in another language via an ILL, has no 
intra-language links. An intra-language link can be transferred from ? to ? as follows. First, following 
an intra-language link (? to ??) and then the ILL (?? to ??), the final destination article ?? is identi-
fied as that to be linked from ?. Second, the text of ? is searched for possible anchors for the destina-
tion article ??, which are learned from the entire Wikipedia beforehand. If two or more possible an-
chors are found, the most appropriate one will be selected according to a certain criterion. For example, 
suppose all intra-language links are missing from the Japanese article ?????????? in Figure 
1. The intra-language link from the English article ?Tata Motors? to ?Jaguar Cars? and the ILL from 
?Jaguar Cars? to ????? (???)? suggest that the Japanese article ?????????? should 
have an intra-language link to ????? (???).? The possible anchors for ????? (???),? 
which have been learned from all the Wikipedia articles linked to it, include ????? (???),? 
?????,? and others. Since the text of ?????????? contains ?????,? it is selected as 
the anchor for the destination article ????? (???).? 
It should be noted that our proposed method avoids the two hard problems in wikification, anchor 
selection and disambiguation, by exploiting the intra-language links provided by Wikipedia in another 
language. Resulting anchors are certainly important terms within ?, since their counterparts have been 
selected as anchors by the author of counterpart ? in another language.  Even if an anchor were an am-
biguous term, i.e., had two or more possible destination articles, it would be certainly linked to the ap-
propriate one due to the ?one sense per discourse? hypothesis (Gale et al., 1992). The hypothesis is 
extended to a pair of ILL-linked articles, ? and ?, as follows. A pair of corresponding anchors should 
be regarded as a single term and express the same sense in a discourse shared by ? and ?. In other 
words, they should be linked to articles that are linked via an ILL. Since the proposed method relies on 
this extended hypothesis, it will select correct destination articles for anchors in ? as long as anchors in 
? have been linked to their correct destination articles. 
It should also be noted that the proposed method first determines the destination articles then the 
anchors for them, while usual wikification methods first select anchors then determine their destina-
tion articles. The main reason for this is convenience of implementation; cross-language mapping of 
Figure 1. Transferring intra-language link. 
1261
destination articles is one-to-one (or one-to-zero), while that of anchors can be one-to-many. Deter-
mining destination articles prior to anchors, however, results in an additional advantage that allows a 
destination article to be proposed without an anchor for it. Since the pair ? and ? is not parallel but just 
comparable, the counterpart of an anchor in ? is not always found in ?. This is often the case when ? 
is incomplete, under construction, or written in a different style from that of ?. In such a case, our 
method proposes a destination article ?? without an anchor, and ? will be linked to ?? once ? is en-
larged to contain a term appropriate as the anchor for ??. 
3 Proposed Method 
The proposed method is divided into two steps; the preprocessing step for collecting possible anchors 
for all Wikipedia articles in a target language as well as estimating probabilities required in the suc-
ceeding step and the main step for transferring intra-language links in a source-language article ? to 
the target-language article ? linked to ? via an ILL. In this section, a triplet (?, ?, ??) denotes an intra-
language link from anchor ? in article ? to destination article ?? and, likewise, a triplet (?, ?, ??) does. 
Note that although an article can have two or more intra-language links from the same anchor at dif-
ferent positions in the text to the same destination article, they are treated as a single link. 
3.1 Preprocessing Step 
Collecting Possible Anchors for Wikipedia Articles 
The title of a Wikipedia article can be used as an anchor for the article. However, a title is often ac-
companied by a parenthesized note indicating the domain of the article to discriminate from other arti-
cles with the same title. The title ????? (???)? of an article that describes a car named Jaguar 
is an example; the parenthesized note ?(???)? discriminates the article from another article ???
???, which describes an animal belonging to the cat family. Such a title accompanied by a parenthe-
sized note rarely occurs in usual texts, and the title with the parenthesized note deleted is often marked 
as an anchor. Accordingly, we also regard a title with a parenthesized note deleted (e.g., ??????) 
as a possible anchor. Other terms, typically synonyms of the article title, are often used as anchors. 
Therefore, we collect terms that are actually used as anchors for each article from the entire Wikipedia. 
Finally, we threshold possible anchors by their keyphraseness to eliminate general words. The 
keyphraseness ?(?) of a term ? is defined as the probability that ? is used as an anchor in Wikipedia 
articles (Mihalcea and Csomai, 2007), i.e., 
?(?) =
|{?|??? . (?, ?, ??) ? ??}|
df(?)
, 
where ?? is a set consisting of all intra-language links in the target-language Wikipedia and df(?) is 
the number of Wikipedia articles in which ? occurs. 
In summary, a set of possible anchors A(??) are constructed for a target-language destination article 
?? as follows: 
A(??) = ({?????(??), ??????(??)} ? {?|??. (?, ?, ??) ? ??}) ? {?|?(?) ? ?}, 
where ?????(??) and  ??????(??) are ???s title with and without the parenthesized note, respectively, 
and ? is the threshold for the keyphraseness. 
Estimating Probabilities 
The following probabilities, which will be used to select one from among possible anchors for a desti-
nation article, are estimated from the entire Wikipedia. 
? The probability that the target-language anchor is ? on the condition that its source-language 
counterpart is ?, i.e., 
P(?|?) =
count(?, ?)
? count(?, ??)??
, 
1262
count(?, ?) = |{((?, ?), (?, ?))| 
??? . ??? . (?, ?, ??) ? ?? ? (?, ?, ??) ? ??
? (?, ?) ? ??? ? (?? , ??) ? ???
}| , 
where ?? is a set consisting of all intra-language links in the source-language Wikipedia, and 
??? is a set of all pairs of ILL-linked articles. 
? The probability that the anchor is ? on condition that the destination article is ??, i.e., 
P(?|??) =
|{?|(?, ?, ??) ? ??}|
|{?|???. (?, ??, ??) ? ??}|
 
? The probability that the destination article is ?? on condition that the anchor is ?, i.e., 
P(??|?) =
|{?| (?, ?, ??) ? ??}|
|{?|???
? . (?, ?, ??
? ) ? ??}|
 
3.2 Main Step 
Let ? and ? be source-language and target-language articles that are linked via an ILL, respectively. 
Intra-language links in ? are transferred to ? as follows: 
(i) For each source-language intra-language link (?, ?, ??), do (ii) to (v). 
(ii) If ?? has an ILL to an article in the target language, let ?? be the destination article of the ILL 
from ??. Otherwise, output ?NOT TRANSFERRED? and move to the next intra-language link. 
(iii) If A(??) is empty, output the transferred intra-language link (?, NULL, ??), which means that 
? should be linked to ?? but does not contain a term appropriate as the anchor, and move to 
the next intra-language link. 
(iv) For each possible anchor ? ? A(??), search the text of ? for ?. If found, let pos(?, ?) denote 
the position of its first occurrence in the text; otherwise, let pos(?, ?) = ?1. 
(v) If at least one possible anchor is found, choose the most appropriate one ??? according to an an-
chor priority score Score(?), i.e., 
??? = argmax
? s.t.  ??A(??)?pos(?,?)?0
Score(?). 
and output the transferred intra-language link (?, ???, ??). Otherwise, output the transferred in-
tra-language link (?, NULL, ??). 
We have the following five alternative anchor priority scores in step (v) above. 
? Anchor translation probability: Score1(?) = P(?|?). 
This score favors the anchor that occurs most frequently as counterpart to the source-language 
anchor. 
? Anchor probability: Score2(?) = P(?|??). 
This score favors the anchor by which the destination article is pointed most frequently. 
? Destination article likelihood: Score3(?) = P(??|?). 
This score favors the anchor that is most likely to point the destination article. 
? Spelling:  Score4(?) = 1 ? dist(?, ?????
?(??)) max{len(?), len(?????
?(??))}? , 
where dist(?, ??) is the Levenshtein distance between character strings ? and ?? (Levenshtein, 
1966), and len(?) is the length of character string ?. 
This score favors the anchor with the highest similarity to the article?s title without a parenthe-
sized note, which is the most representative term denoting the entity, concept, or topic described 
in the article. 
? Position: Score5(?) = 1 pos(?, ?)? . 
Note that in a Wikipedia article, among two or more occurrences of an important term, the first 
one tends to be marked as an anchor. 
1263
4 Experiment 
4.1 Experimental Settings 
We conducted an experiment on transferring 
intra-language links from the English edition 
to the Japanese edition of Wikipedia. 
Input Data 
The English edition of Wikipedia (2013-04-03 
dump), consisting of 4,241,324 articles, and 
the Japanese edition of Wikipedia (2013-03-28 
dump), consisting of 951,411 articles1, were 
used for the experiment. Intra-language links 
were extracted from each dump file, and ILLs 
were obtained from Wikidata (2013-03-28 
dump). Redirect pages were resolved prelimi-
narily, i.e., if the destination of an intra-
language link or ILL was a redirect page, the 
destination was replaced with an article pointed by the redirect page. 
From among a total of 366,358 pairs of English and Japanese articles linked by ILLs, 3,595 pairs 
were randomly selected as a test set. The remaining pairs were used as training data for constructing 
English and Japanese intra-language link sets, ?? and ??. The English articles in the test set contained 
179,963 intra-language links in total; these were input to the algorithm of the proposed method. 
Keyphraseness Threshold 
Limiting possible anchors to meaningful ones and gaining many links are in a trade-off relation ad-
justable by the keyphraseness threshold ?. In the experiment, ? was set to 0, 0.01, 0.05, and 0.1. 
Keyphraseness values of several anchors are listed in Table 1. Technical words (e.g., ?????
????????? ? Bayesian network) and uncommon proper names (e.g., ??????? ? Dic-
tionnaire Infernal) tend to have high keyphraseness values. Common words (e.g., ???? ? devil and 
???? ? conflict) and proper names (e.g., ???? ? Paris and ?????? ? Nicholas), especially 
identical to a general noun, have middle or low values according to their commonness. Although some 
functional words (e.g., ???? ? from) may be included in possible anchors for the Wikipedia articles 
of their homographic content words (e.g., ???? ? Yori (kana)), they naturally have extremely low 
values. By setting ? to a value slightly greater than zero, functional words could be removed from pos-
sible anchors.  
Comparison of Anchor Priority Scores 
To determine the most effective anchor priority score, the accuracy of anchors selected according to 
each score was evaluated, assuming the existing intra-language links in the original Japanese articles 
as gold standard. That is, anchor accuracy Acc is defined as the percentage of originally pointed desti-
nation articles for which correct anchors were selected, i.e.,  
Acc =
|?????? ? ???????|
|{(?, ?, ??) ? ??????|???. (?, ??, ??) ? ???????}|
 , 
where ?????? is a set consisting of all transferred intra-language links and ??????? is the gold stand-
ard intra-language link set. Table 2 lists the anchor accuracies for each anchor priority score and each 
?. Anchor translation probability exhibited the best results and, therefore, we adopted anchor transla-
tion probability as the anchor priority score. 
                                                 
1 Redirect pages and articles with no intra-language links were not included in these counts. 
Anchor English 
translation 
Keyphrase-
ness 
??????
?????? 
Bayesian 
network 
1 
????? 
Dictionnaire 
Infernal 
0.810 
??? demonology 0.678 
?? Paris 0.574 
???? occult 0.304 
?? devil 0.135 
???? Nicholas 0.039 
?? conflict 0.001 
?? half 7.8 ? 10?5  
?? Yori (kana) 4.4 ? 10?6 
Table 1. Example of keyphraseness values. 
1264
4.2 Experimental Results 
We inputted 179,963 English intra-language links to the algorithm of the proposed method and classi-
fied them into the following five classes. Examples of each class, except class B, are given in Figure 2, 
which is an excerpt from the results for the pair of English article ?Jacques Collin de Plancy? and Jap-
anese article ????????????.?  
A. Transferred to a Japanese intra-language link in the gold standard (bold underline in Figure 2) 
B. Transferred to a Japanese intra-language link whose anchor is not the same as the gold standard 
link to the same destination article 
C. Transferred to a Japanese intra-language link not in the gold standard (double underline in Fig-
ure 2) 
D. Transferred to a Japanese intra-language link without anchor (wavy underline in Figure 2) 
E. Not transferred to a Japanese intra-language link  (dashed underline in Figure 2) 
Table 3 lists the numbers of English intra-language links per class. The proposed method added 
many new intra-language links to Wikipedia articles. Since the total number of existing Japanese intra-
language links in the test-set articles was ? = 161,940, the increase rate of Japanese intra-language 
links was 100(? + ?) ?? = 100(13,916 + 52,275) 161,940? = 40.9%  (? = 0 ). When new links 
without anchors were excluded, the increase rate was 100? ?? = 100 ? 13,916 161,940? = 8.6% 
(? = 0). 
The anchor accuracy of existing links was 100? (? + ?) =? 100 ? 31,770/(31,770 + 1,219) =
96.3% (? = 0). Anchor accuracy of new intra-language links could not be calculated because of the 
unavailability of gold standard data. However, the proposed method specifies the anchor ? for destina-
tion article ?? only when possible anchors for it is found in the target-language article ?. The specified 
anchor ? is likely to be the counterpart of source-language anchor ? pointing to ?? that is the source-
language counterpart of ??, regardless of whether ? already points to ?? or not. Thus, the anchor accu-
racy of new links should be similar to that of existing links. 
Among the ? = 179,963 input English intra-language links, 100? ?? = 100 ? 52,275 179,963? =
29.0% (? = 0) were transferred to Japanese intra-language links with the anchor unspecified. This 
was because different language articles contain different contents even though they are linked via an 
ILL. The anchor-unspecified links are put in the ?????? sections (?See also? sections) of target-
language articles, and Wikipedia authors are expected to enlarge or revise the articles so that these an-
chor-unspecified links can be converted to anchor-specified links. Additionally, among the ? =
179,963  input English intra-language links, 100? ?? = 100 ? 80,783 179,963? = 44.9%  were not 
transferred to Japanese intra-language links. We assumed this was mainly due to missing Japanese 
articles. Note that the total number of Japanese articles is less than one-fourth that of English articles. 
The percentage of not-transferred links will decrease with the growing number of Japanese articles. 
Anchor priority score Anchor accuracy (%) 
? = 0 ? = 0.01 ? = 0.05 ? = 0.1 
Anchor translation probability 96.3 93.9 93.0 92.0 
Anchor probability 95.6 93.3 92.4 91.4 
Destination article likelihood 90.7 90.8 91.5 91.3 
Spelling 95.1 93.1 92.5 91.8 
Position 88.2 87.3 87.9 87.6 
Table 2. Anchor accuracy. 
1265
4.3 Additional Comments on Experimental Results 
Among alternative anchor priority scores, anchor translation probability seems most effective because 
this is a posterior probability of the target-language counterpart to the source-language anchor. Anchor 
probability is also useful because this is a posterior probability of the anchor for the destination. High-
er accuracy with spelling score indicates that Wikipedia editors tend to use the title of the destination 
as an anchor. This may be caused by manually specifying the anchor and destination independently. 
Contrary to expectations that the first occurrence likely becomes an anchor, position score exhibited 
the worst results. More detailed analysis of the context in which a term tends to be selected as an an-
chor is necessary. 
Table 2 shows that the anchor probability, unexpectedly, decreases with a rise of the keyphraseness 
threshold. It was caused by articles that have only one possible anchor with keyphraseness value be-
 Transfer result Number (percentage) 
Desti-
nation 
Anchor ? = 0 ? = 0.01 ? = 0.05 ? = 0.1 
A Existing Correct 31,770 (17.7%) 30,951 (17.2%) 30,661 (17.0%) 30,298 (16.8%) 
B Incorrect 1,219   (0.7%) 2,025   (1.1%) 2,298   (1.3%) 2,625   (1.5%) 
C New Found 13,916   (7.7%) 12,812   (7.1%) 11,421   (6.3%) 10,335   (5.7%) 
D Not found 52,275 (29.0%) 53,392 (29.7%) 54,800 (30.5%) 55,922 (31.1%) 
E Not transferred 80,783 (44.9%) 
Table 3. English intra-language links classified according to results. 
Figure 2. Example results of transferring intra-language links. 
Jacques Collin de Plancy 
 
Jacques Albin Simon Collin de Plancy 
(Plancy-l'Abbaye, 28 January 1793 ?Paris, 
1881) was a French occultist, demonologist 
and writer; he published several works on 
occultism and demonology.[1][2] 
 
He was born Jacques Albin Simon Collin on 
28 (in some sources 30) January 1793 in 
Plancy (presently Plancy-l'Abbaye) son of 
Edme-Aubin Collin and Marie-Anne Danton, 
sister of Georges-Jacques Danton who was 
executed the year after Jacques was born.[3] 
He later added the aristocratic "de Plancy" 
himself - an addition which would later cause 
accusations against his son in his career as a 
diplomat. He was a free-thinker influenced by 
Voltaire. He worked as a printer and publish-
er in Plancy-l'Abbaye and Paris. Between 
1830 and 1837, he resided in Brussels, and 
then in the Netherlands, before he returned to 
France after having converted to the Catholic 
religion. 
? 
In 1818 his best known work, Dictionnaire 
Infernal, was published. 
? 
??????????? 
 
????????????J. Collin de 
Plancy, 1794?????? 1793?? ? 
1881????? 1887???[1]????
19???????????????? 
? 
?????????????????? 
     ?Paris? 
???????1818?????????
????????????????? 
          ?Dictionnaire Infernal? 
??????????????? ??
??????????????????
?????????????????? 
                         ?occult? 
???? 3,799??????????  
? 
??????????????????
?????????????????? 
       ?demonology? 
?????????? 
? 
????  ?See also? 
??????   ?Brussels? 
??????   ?Voltaire? 
1266
low the threshold (e.g., ??? ? station). When the threshold was set high, the possible anchor set for 
such an article became empty and, as a results, the algorithm failed to reproduce the existing links to it. 
In this experiment, we transferred English links onto Japanese articles. Since the English edition of 
Wikipedia is richer than Japanese, it has been assumed that an English-to-Japanese direction is more 
effective than the inverse. However, among the 179,963 links in English and 161,940 links in Japanese 
extracted from the test set of English-Japanese article pairs, only 32,989 links are paired with their 
counterparts and others do not have counterparts. This fact indicates that a Japanese-to-English trans-
fer of links is also useful for enriching English articles. It also leads a low anchor recall, which is the 
percentage of correct links among existing links: 100? ?? = 100 ? 31,770 161,940? = 19.6% (? =
0). Combining usual wikification techniques should help improve the anchor recall. 
5 Discussion 
We now discuss two future directions, an extension to multiple language combination and a variation 
for inappropriate intra-language link detection. 
   The proposed method can be straightforwardly extended to three or more language combinations: 
Even if two source articles in two different languages are handled separately, the target article would 
be more enriched with the union of two transferred link sets. While this contributes to increasing the 
coverage of links, the reliability of links can also be improved by taking the intersection of the two 
transferred link sets. A more sophisticated combination of multiple source languages is a further prob-
lem. 
In the experiment, existing links were used as the gold standard for evaluation, despite the fact that 
they are not always appropriate because they are manually created by unspecified contributors. For 
example, there is a biology-related article containing an anchor ?translation? linking to an article 
?Translation? describing language translation, not to another article ?Translation (biology).? Such an 
incorrect intra-language link may be detected using a similar method as the proposed one. In the above 
example, suppose the Japanese counterpart article contains an anchor ???? linking to an article ??
? (???).? Two anchors ?translation? and ???? correspond to each other but their destination 
articles are not linked via an ILL. This inconsistency may be evidence for an inappropriate intra-
language link. Note that which of the English and Japanese links is inappropriate cannot be easily de-
termined. How to estimate the appropriateness of intra-language links is a problem to be solved. 
6 Related Work 
Wikification, which aims at linking mainly non-Wikipedia articles to Wikipedia articles, can be natu-
rally applied to linking between Wikipedia articles. There has been much research on wikification, 
most of which focused on disambiguation of destination articles (Milne and Witten, 2008a; Fogarolli, 
2009; Ratinov et al., 2011). Determining an appropriate destination article for an anchor term is a spe-
cial case of WSD. Although a variety of ideas for WSD have been adapted to wikification, their per-
formance is not satisfactory and there is room for further improvement. Another important issue with 
wikification is anchor selection, although most literature on wikification avoids the issue by selecting 
every term that is used as an anchor in any Wikipedia article. Anchor selection is a keyword extraction 
problem, which has been tackled using syntactic, statistical, and/or machine learning techniques but 
remains room for further improvement (Jacquemin and Bourigault, 2003). It should be added that our 
proposed method avoids both disambiguation and anchor selection problems by exploiting link infor-
mation in another language edition of Wikipedia. 
Adafre and de Rijke (2005) proposed a method for finding ?missing intra-language links? in a Wik-
ipedia article by assuming that an intra-language link represents the relatedness between concepts de-
scribed by the linked articles. Their method adds intra-language links to an article by using articles 
with similar link structures as that of the article in question. Similar methods that use the Wikipedia?s 
link structures as a semantic network have been proposed for entity linking (Milne and Witten, 2008b; 
Fogarolli, 2009; Ratinov et al., 2011). These still remain monolingual methods; the availability of oth-
er language editions cannot be assumed. 
A bilingual approach to improving quality of Wikipedia articles has also been studied. Sorg and 
Cimiano (2008) proposed a method for finding new ILLs by using a classifier whose features include 
1267
the number of ILLs between articles pointed by an article in one language and those pointed by an ar-
ticle in another language. Wang et al. (2013) improved the classifier by extending the intra-language 
links to increase the number of features. Both methods and our proposed method exploit the compara-
bility between intra-language links in different language editions. However, while the former find new 
ILLs, the latter finds new intra-language links. 
7 Conclusion 
We proposed a method for enriching intra-language links in Wikipedia articles. It transfers intra-
language links between a pair of different language articles linked by an inter-language link through 
the following two steps: first, determine destination articles to which the target-language article should 
be linked by following a source-language intra-language link and an ILL successively from each of the 
anchors in the source-language article; second, determine an anchor for each of the destination articles 
by searching the target-language article for possible anchors and selecting the most appropriate one 
according to the anchor translation probability criterion if two or more possible anchors are found. Un-
like usual wikification methods, our method avoids anchor selection and disambiguation problems by 
exploiting the comparability of intra-language links between different language editions of Wikipedia. 
   We conducted an experiment of transferring intra-language links from the English edition to the Jap-
anese edition to evaluate the effectiveness of our method. The method increased the number of intra-
language links in Japanese articles by 40.9%, and the accuracy of anchors selected was estimated to be 
96.3%. Future work includes an extension to multiple language combination and a variation for inap-
propriate intra-language link detection.  
References 
Adafre, Sisay Fissaha and Maarten de Rijke. 2005. Discovering missing links in Wikipedia. In Proceedings of 
the 3rd International Workshop on Link Discovery: Issues, Approaches and Applications, pages 90?97. 
Fogarolli, Angela. 2009. Word sense disambiguation based on Wikipedia link structure. In Proceedings of 2009 
IEEE International Conference on Semantic Computing, pages 77?82. 
Gale, William A., Kenneth W. Church, and David Yarowsky. 1992. One sense per discourse. In Proceedings of 
HLT ?91 Workshop on Speech and Natural Language, pages 233?237. 
Jacquemin, Christian and Didier Bourigault. 2003. Term extraction and automatic indexing. In Ruslan Mitkov 
(Ed.), The Oxford Handbook of Computational Linguistics, pages 599?615. Oxford University Press. 
Levenshtein, Vladimir I. 1966. Binary codes capable of correcting deletions, insertions, and reversals. 
Cybernetics and Control Theory, 10(8):707?710. 
Mihalcea, Rada and Andras Csomai. 2007. Wikify!: linking documents to encyclopedic knowledge. In 
Proceedings of the 16th ACM Conference on Information and Knowledge Management, pages 233?242. 
Milne, David and Ian H. Witten. 2008a. Learning to link with Wikipedia. In Proceedings of the 17th ACM 
Conference on Information and Knowledge Management, pages 509?518. 
Milne, David and Ian H. Witten. 2008b. An effective, low-cost measure of semantic relatedness obtained from 
Wikipedia links. In Proceedings of the Wikipedia and AI Workshop of AAAI, pages 25?30. 
Ratinov, Lev, Dan Roth, Doug Downey, and Mike Anderson. 2011. Local and global algorithms for 
disambiguation to Wikipedia. In Proceedings of the 49th Annual Meeting of the Association for 
Computational Linguistics: Human Language Technologies, pages 1375?1384. 
Sorg, Philipp and Philipp Cimiano. 2008. Enriching the crosslingual link structure of Wikipedia - a 
classification-based approach. In Proceedings of the AAAI 2008 Workshop on Wikipedia and Artificial 
Intelligence. 
Wang, Zhichun, Juanzi Li, and Jie Tang. 2013. Boosting cross-lingual knowledge linking via concept annotation. 
In Proceedings of the 23rd International Joint Conference on Artificial Intelligence, pages 2733?2739. 
 
1268
Proceedings of the 8th Workshop on Asian Language Resources, pages 30?37,
Beijing, China, 21-22 August 2010. c?2010 Asian Federation for Natural Language Processing
Augmenting a Bilingual Lexicon with Information       
for Word Translation Disambiguation
Takashi Tsunakawa
Faculty of Informatics
Shizuoka University
tuna@inf.shizuoka.ac.jp
Hiroyuki Kaji
Faculty of Informatics
Shizuoka University
kaji@inf.shizuoka.ac.jp
Abstract
We describe a method for augmenting
a bilingual lexicon with additional in-
formation for selecting an appropriate 
translation word. For each word in the 
source language, we calculate a corre-
lation matrix of its association words 
versus its translation candidates. We 
estimate the degree of correlation by
using comparable corpora based on 
these assumptions: ?parallel word as-
sociations? and ?one sense per word 
association.? In our word translation 
disambiguation experiment, the results 
show that our method achieved 42% 
recall and 49% precision for Japa-
nese-English newspaper texts, and 45% 
recall and 76% precision for Chi-
nese-Japanese technical documents.
1 Introduction
The bilingual lexicon, or bilingual dictionary, 
is a fundamental linguistic resource for multi-
lingual natural language processing (NLP). For 
each word, multiword, or expression in the 
source language, the bilingual lexicon provides 
translation candidates representing the original 
meaning in the target language.
Selecting the right words for translation is a 
serious problem in almost all of multilingual 
NLP. One word in the source language almost 
always has two or more translation candidates 
in the target language by looking up them in 
the bilingual lexicon. Because each translation
candidate has a distinct meaning and property, 
we must be careful in selecting the appropriate 
translation candidate that has the same sense as 
the word inputted. This task is often called 
word translation disambiguation.
In this paper, we describe a method for add-
ing information for word translation disam-
biguation into the bilingual lexicon. Compara-
ble corpora can be used to determine which 
word associations suggest which translations 
of the word (Kaji and Morimoto, 2002). First, 
we extract word associations in each language 
corpus and align them by using a bilingual dic-
tionary. Then, we construct a word correlation 
matrix for each word in the source language. 
This correlation matrix works as information 
for word translation disambiguation.
We carried out word translation experiments 
on two settings: English-to-Japanese and Chi-
nese-to-Japanese. In the experiments, we tested 
Dice/Jaccard coefficients, pointwise mutual 
information, log-likelihood ratio, and Student?s
t-score as the association measures for extract-
ing word associations.
2 Constructing word correlation ma-
trices for word translation disam-
biguation
2.1 Outline of our method
In this section, we describe the method for 
calculating a word correlation matrix for each 
word in the source language. The correlation 
matrix for a word f consists of its association 
words and its translation candidates. Among 
the translation candidates, we choose the most 
acceptable one that is strongly suggested by its 
association words occurring around f.
We use two assumptions for this framework:
(i)  Parallel word associations:
Translations of words associated with 
each other in a language are also asso-
ciated with each other in another language 
30
(Rapp, 1995). For example, two English 
words ?tank? and ?soldier? are associated 
with each other and their Japanese trans-
lations ??? (sensha)? and ??? (hei-
shi)? are also associated with each other.
(ii)  One sense per word association:
A polysemous word exhibits only one 
sense of a word per word association 
(Yarowsky, 1993). For example, a poly-
semous word ?tank? exhibits the ?military
vehicle? sense of a word when it is asso-
ciated with ?soldier,? while it exhibits the 
?container for liquid or gas? sense when it 
is associated with ?gasoline.?
Under these assumptions, we determine which 
of the words associated with an input word 
suggests which of its translations by aligning 
word associations by using a bilingual dictio-
nary. Consider the associated English words 
(tank, soldier) and their Japanese translations 
(?? (sensha), ?? (heishi)). When we 
translate the word ?tank? into Japanese, the 
associated word ?soldier? helps us to translate 
it into ??? (sensha)?, not to translate it into 
???? (tanku)? which means ?a storage 
tank.?
This naive method seems to suffer from the 
following difficulties:
 A disparity in topical coverage between 
two corpora in two languages
 A shortage in the bilingual dictionary
 The existence of polysemous associated 
words that cannot determine the correct 
sense of the input word
For these difficulties, we use the tendency that 
the two words associated with a third word are 
likely to suggest the same sense of the third 
word when they are also associated with each 
other. For example, consider an English asso-
ciated word pair (tank, troop). The word ?troop?
cannot distinguish the different meanings be-
cause it can co-occur with the word ?tank? in 
both senses of the word. The third word ?sol-
dier,? which is associated with both ?tank? and 
?troop,? can suggest the translation ???
(sensha).?
The overview of our method is shown in
Figure 1. We first extract associated word pairs 
in the source and target languages from com-
parable corpora. Using a bilingual dictionary,
we obtain alignments of these word associa-
tions. Then, we iteratively calculate a correla-
tion matrix for each word in the source lan-
guage. Finally, we select the translation with 
the highest correlation from the translation 
candidates of the input word and the 
co-occurring words.
For each input word in the source language, 
we calculate correlation values between their 
translation candidates and their association 
words. The algorithm is shown in Figure 2.
In Algorithm 1, the initialization of correla-
tion values is based on word associations, 
where D is a set of word pairs in the bilingual 
dictionary, and Af and Ae are the sets of asso-
ciated word pairs. First, we retain associated 
words f?(i) when its translation e? exists and 
when e? is associated with e. In the iteration, 
the correlation values of associated words f?(i)
that suggest e(j) increase relatively by using 
association scores ((), ) and
Figure 1. Overview of our method.
31
((), ). In our experiments, we set the 
number of iterations Nr to 10.
2.2 Alternative association measures for 
extracting word associations
We extract co-occurring word pairs and calcu-
late their association scores. In this paper, we 
focus on some frequently used metrics for 
finding word associations based on their oc-
currence/co-occurrence frequencies.
Suppose that words x and y frequently 
co-occur. Let n1 and n2 be the occurrence fre-
quencies of x and y respectively, and let m be 
the frequency that x and y co-occur between w
content words. The parameter w is a window 
size that adjusts the range of co-occurrences.
Let N and M be the sum of occur-
rences/co-occurrences of all words/word pairs, 
respectively. The frequencies are summarized 
in Table 1.
The word association scores (, ) are de-
fined as follows:
 Dice coefficient (Smadja, 1993)
Dice(, ) =
2
	
 + 	
(1)
 Jaccard coefficient (Smadja et al, 1996)
Jaccard(, ) =

	
 + 	  
(2)
 Pointwise mutual information (pMI) 
(Church and Hanks, 1990)
pMI(, ) = log
/
(	
  )(	  )
(3)
 Log-likelihood ratio (LLR) (Dunning, 
1993)
LLR(, )
= 2logL(, 	
, )
+ logL(	  ,   	
, )
 logL(, 	
, 
)
 logL(	  ,   	
, ); (4)
logL(, 	, ) =
 log  + (	  ) log(1  ), (5)

 =

	

,  =
	  
  	

,  =
	

(6)
 Student?s t-score (TScore) (Church et al, 
1991)
TScore(, ) =
  	
	 

(7)
We calculate association scores for all pairs 
of words when their occurrence frequencies 
are not less than a threshold Tf and when their
x
occur
x not 
occur
Total
y
occur
m n2 ? m n2
y not 
occur
n1 ? m M ? n1
? n2 + m
N ? n2
Total n1 N ? n1 N
Table 1. Contingency matrix of occurrence 
frequencies.
Figure 2. Algorithm for calculating correlation 
matrices.
?
 ((), )  (, 