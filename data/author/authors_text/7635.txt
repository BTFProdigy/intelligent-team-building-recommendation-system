Best Analysis Selection in Inflectional Languages
Ales? Hora?k and Pavel Smrz?
Faculty of Informatics, Masaryk University Brno
Botanicka? 68a, 602 00 Brno, Czech Republic
E-mail: {hales,smrz}@fi.muni.cz
Abstract
Ambiguity is the fundamental property of
natural language. Perhaps, the most bur-
densome case of ambiguity manifests itself
on the syntactic level of analysis. In order
to face up to the high number of obtained
derivation trees, this paper describes several
techniques for evaluation of the figures of
merit, which define a sort order on parsing
trees. The presented methods are based on
language specific features of synthetical lan-
guages and they improve the results of sim-
ple stochastic approaches.
1 Introduction
Ambiguity on all levels of representation is
an inherent property of natural languages
and it also forms a central problem of natu-
ral language parsing. A consequence of the
natural language ambiguity is a high num-
ber of possible outputs of a parser that are
usually represented by labeled trees. The av-
erage number of parsing trees per input sen-
tence strongly depends on the background
grammar and thence on the language. There
are natural language grammars producing
at most hundreds or thousands of parsing
trees but also highly ambiguous grammar
systems producing enormous number of re-
sults. For example, a grammar extracted
from the Penn Treebank and tested on a
set of sentences randomly generated from a
probabilistic version of the grammar has on
average 7.2?1027 parses per sentence accord-
ing to Moore?s work (Moore, 2000). Such a
mammoth extent of result is also no excep-
tion in parsing of Czech (Smrz? and Hora?k,
2000) (see Fig. 1) due to free word order and
Figure 1: The dependence of number of re-
sulting analysis on the number of words in
the input sentence
rich morphology of word forms whose gram-
matical case cannot often be unambiguously
determined.
A traditional solution for these problems
is presented by probabilistic parsing tech-
niques (Bunt and Nijholt, 2000) aiming at
finding the most probable parse of a given
input sentence. This methodology is usually
based on the relative frequencies of occur-
rences of the possible relations in a repre-
sentative corpus. ?Best? trees are judged by
a probabilistic figure of merit (FOM).
The term ?figure of merit? is usually used
to refer to a function that prunes implausi-
ble partial analyses during parsing. In this
paper, we rather take figure of merit as a
measure bounding the true probabilities of
the complete parses.
S

HH
H
NP1
 HH
AP


HH
HH
ADJ and ADJ
N1
V NP4
 HH
ADJ NP4
 HH
N4 N2
??
selected trigrams: [ADJ,and,ADJ]
[ADJ,N1,V]
[N1,V,N4]
[V,ADJ,N4]
[ADJ,N4,N2]
Figure 2: Lexical heads as n-gram?s elements.
The standard methods of the best analy-
sis selection (Caraballo and Charniak, 1998)
usually use simple stochastic functions inde-
pendent on the peculiarities of the underly-
ing language. This approach seems to work
satisfactorily in case of analytical languages.
On the other hand, the obstacles brought
by the synthetical languages in relationship
with those simple statistical techniques are
indispensable.
Therefore, we try to improve the standard
FOMs taking into consideration specific fea-
tures of free word order languages. The fol-
lowing text discusses the assets of three fig-
ures of merit that reflect selected phenomena
of the Czech language.
2 Figures of Merit
The overall figure of merit of the syntactic
analysis results is determined as a combina-
tion of several contributory FOMs that re-
flect particular language features such as
? frequency of syntactic constructs repre-
sented by pre-computed rule probabili-
ties
? augmented n-gram model based on the
occurrence of adjacent lexical heads
standing for the corresponding subtrees
? affinity between constituents modeled
by valency frames of verbs, adjectives
and nouns
The selected FOMs participate on the de-
termination of the most probable analysis.
A straightforward approach lies in the linear
combination of FOMs:
? = ?1 ? ?1 + ?2 ? ?2 + ?3 ? ?3
where ?i are the FOMs? contributions and
?i are empirically assigned weights (usually
taken as normalizing coefficients). However,
our experiments showed that the weights ?i
need to reflect the behaviour of particular
lexical items, their categories or even anal-
ysed constituents. We thus need to handle
the ?i variables as functions of various pa-
rameters.
? = ?1( ) ? ?1 + ?2( ) ? ?2 + ?3( ) ? ?3
The following sections deal with the figures
of merit that play a crucial role in the search
for the best output analysis.
2.1 Rule-tied Actions and ?1 FOM
A key question is then what the good can-
didates for FOMs are. The use of proba-
bilistic context-free grammars (PCFGs) in-
volves simple CF rule probabilities to form
a FOM (Chitrao and Grishman, 1990; Bo-
brow, 1991).
The evaluation of the first FOM is based
on the mechanism of contextual actions built
into the metagrammar conception (Smrz? and
Hora?k, 2000). It distinguishes four kinds of
contextual actions, tests or constraints:
1. rule-tied actions
2. agreement fulfilment constraints
3. post-processing actions
4. actions based on derivation tree
The rule-based probability estimations are
solved on the first level by the rule-tied ac-
tions, which also serve as rule parameteriza-
tion modifiers.
Agreement fulfilment constraints are used
in generating the expanded grammar (Smrz?
and Hora?k, 1999) or they serve also as
chart pruning actions. In terms of (Maxwell
III and Kaplan, 1991), the agreement ful-
filment constraints represent the functional
constraints, whose processing can be inter-
leaved with that of phrasal constraints.
The post-processing actions are not trig-
gered until the chart is already completed.
The main part of FOM computation for a
particular input sentence is driven by ac-
tions on this level. Some figures of merit
(e.g. verb valency FOM, see Section 2.3) de-
mand exponential resources for computation
over the whole chart structure. This prob-
lem is solved by splitting the calculation pro-
cess into the pruning part (run on the level
of post-processing actions) and the reorder-
ing part, that is postponed until the actions
based on derivation tree.
The actions that do not need to work with
the whole chart structure are run after the
best or n most probable derivation trees are
selected. These actions are used, for exam-
ple, for determination of possible verb va-
lencies within the input sentence, which can
produce a new ordering of the selected trees.
2.2 Augmented n-grams and ?2 FOM
The ?1 FOM is based on rule frequencies and
is not capable of describing the contextual
information in the input. A popular tech-
nique for capturing the relations between
sentence constituents is the n-gram method,
which takes advantage of a fast and efficient
evaluation algorithm.
For instance, (Caraballo and Charniak,
1998) presents and evaluate different figures
of merit in the context of best-first chart
parsing. They recommend boundary trigram
estimate that has achieved the best perfor-
mance on two testing grammars. This tech-
nique, as well as stochastic POS tagging
based on n-gram statistics, achieves satis-
factory results for analytical languages (like
English). However, in case of free word or-
der languages, current studies suggest that
these simple stochastic techniques consider-
ably suffer from the data sparseness problem
and require a huge amount of training data.
The reduction of the number of possible
training schemata, which correctly keeps the
correspondence with the syntactic tree struc-
ture, is achieved by elaborate selection of
n-gram candidates. While the standard n-
gram techniques work on the surface level,
this approach allows us to move up to the
syntactic tree level. We advantageously use
the ability of lexical heads to represent the
key features of the subtree formed by its de-
pendants (see Figure 2). The principle of
lexical heads has shown to be fruitfully ex-
ploited in the analysis of free word order
languages. The obtained cut-down of the
amount of training data may be also crucial
to the usability of this stochastic technique.
2.3 Verb Valencies and ?3 FOM
Our experiments have shown that, in case of
a really free word order language, the FOMs
?1 and ?2 are not always able to discover
the correct reordering of analyses. So as
to cope with the above mentioned difficul-
ties in Slavonic languages (namely Czech),
we propose to exploit the language specific
features. Preliminary results indicate that
the most advantageous approach is the one
based upon valencies of the verb phrase ? a
crucial concept in traditional linguistics.
The part of the system dedicated to ex-
ploitation of information obtained from a list
of verb valencies (Pala and S?evec?ek, 1997)
is necessary for solving the prepositional at-
tachment problem in particular. During the
analysis of noun groups and prepositional
noun groups in the role of verb valencies
in a given input sentence one needs to be
able to distinguish free adjuncts or modi-
fiers from obligatory valencies. We are test-
ing a set of heuristic rules that determine
With Charles Peter angered at the last meeting
Na Karla? ?? ?
<HUMAN>
se Petr rozhne?val na posledn?? sch?uzi? ?? ?
<ACTIVITY>
about the lost advance for payroll
kv?uli ztracene? za?loze na mzdu.? ?? ?
<RECOMPENSE>
Figure 3: Free adjuncts identification by means of lexico-semantic constraints.
whether a found noun group typically serves
as a free adjunct. The heuristics are based
on the lexico-semantic constraints (Smrz? and
Hora?k, 1999).
An example of the application of the heuris-
tics is depicted in Figure 3. In the presented
Czech sentence, the expression na Karla
(with Charles) is denoted as a verb argument
by the valency list of the verb rozhne?vat se
(anger), while the prepositional noun phrase
na schu?zi (at the meeting) is classified as
a free adjunct by the rule specifying that
the preposition na (at) in combination with
an <ACTIVITY> class member (in locative)
forms a location expression. The remaining
constituent na mzdu (for payroll) is finally
recommended as a modifier of the preceding
noun phrase za?loze ([about the] advance).
Certainly, we also need to discharge the
dependence on the surface order. Therefore,
before the system confronts the actual verb
valencies from the input sentence with the
list of valency frames found in the lexicon,
all the valency expressions are reordered. By
using the standard ordering of participants,
the valency frames can be handled as pure
sets independent on the current position of
verb arguments.
2.4 Preferred Word Order
In analytical languages, the word order is
usually taken as rather fixed and that is why
it can be employed in parsing tree prun-
ing algorithms. However, in case of inflec-
tional languages, the approaches to word or-
der analysis are diverse. The most influen-
tial theory works with the topic-focus artic-
ulation (Sgall et al, 1986). Although nearly
all rules that could limit the order of con-
stituents in Czech sentences can be fully re-
laxed, a standard order of participants can
be defined. A corpus analysis of general
texts affirms that this preferred word order
is often followed and that it can be advanta-
geously used as an arbiter for best analysis
selection.
Cases where the ?i FOMs do not unam-
biguously elect the best candidates can be
routed by the preferred word order in the
form of functional weights ?i( ) with appro-
priate parameters.
3 Results
This section presents results of experiments
with the stated figures of merit for the best
analysis selection algorithm. First, the ac-
quisition of training data set derived by ex-
ploitation of a standard dependency tree
bank for Czech is described. Then, we step
to a comparison of parser running times with
that of another available parser.
3.1 The Training Set Acquisition
A common approach to acquiring the sta-
tistical data for analysis of syntax employs
learning the values from a fully tagged tree
bank training corpus. Building of such cor-
pora is a tedious and expensive work and
it requires a team cooperation of linguists
and computer scientists. At present the only
source of Czech tree bank data is the Prague
Dependency Tree Bank (PDTB) (Hajic?,
1998), which includes dependency analyses
of about 100 000 Czech sentences.
First, in order to be able to exploit the
data from PDTB, we have supplemented our
grammar with the dependency specification
precision on sentences percentage
of 1-10 words 86.9%
of 11-20 words 78.2%
of more than 20 words 63.1%
overall precision 79.3%
number of sentences with 8.0%
mistakes in input
Table 1: Precision estimate
for constituents. Thus the output of the
analysis can be presented in the form of pure
dependency tree. In the same time we unify
classes of derivation trees that correspond to
one dependency structure. We then define a
canonical form of the derivation to select one
representative of the class that is used for as-
signing the edge probabilities.
This technique enables us to relate the
output of our parser to the PDTB data.
However, the profit of exploitation of the
information from the dependency structures
can be higher than that and can run in an
automatically controlled environment. For
this purpose, we use the mechanism of prun-
ing constraints. A set of strict limitations is
given to the syntactic analyser, which passes
on just the compliant parses. The con-
straints can be either supplied manually for
particular sentence by linguists, or obtained
from the transformed dependency tree in
PDTB.
The Table 1 summarizes the precision es-
timates counted on real corpus data. These
measurements presented here may discount
the actual benefits of our approach due to
the estimated 8% of mistakes in the input
corpus.
3.2 Running Time Comparison
The effectivity comparison of different
parsers and parsing techniques brings a
strong impulse to improving the actual im-
plementations. Since there is no other gen-
erally applicable and available NL parser for
Czech, we have compared the running times
of our syntactic analyser on the data pro-
vided at http://www.cogs.susx.ac.uk/
lab/nlp/carroll/cfg-resources/.
These WWW pages resulted from discus-
sions at the Efficiency in Large Scale Parsing
Systems Workshop at COLING?2000, where
one of the main conclusions was the need for
a bank of data for standardization of parser
benchmarking. The best results reported
on standard data sets (ATIS and PT gram-
mars) until today are the comparison data
by Robert C. Moore (Moore, 2000). In the
package, only the testing grammars with in-
put sentences are at the disposal, the release
of referential implementation of the parser is
currently being prepared (Moore, personal
communication).
ATIS grammar, Moore?s LC3 + UTF 11.6
ATIS grammar, our system 7.2
PT grammar, Moore?s LC3 + UTF 41.8
PT grammar, our system 57.2
Table 2: Running times comparison (in sec-
onds)
Since we could not run the referential im-
plementation of Moore?s parser on the same
machine, the above mentioned times are not
fully comparable (we assume that our tests
were run on a slightly faster machine than
that of Moore?s tests). We prepare a de-
tailed comparison, which will try to explain
the differences of results when parsing with
grammars of varying ambiguity level.
4 Conclusions
The methods of the best analysis selection
algorithm described in this paper show that
the parsing of inflectional languages calls for
sensitive approaches to the evaluation of the
appropriate figures of merit. The case study
of Czech suggests that the use of language
specific features can improve the results of
simple stochastic techniques on annotated
corpus data.
Future directions of our research lead to
improvements of the quality of training data
set so that it would cover all the most fre-
quent language phenomena. Our investiga-
tions indicate that, in addition to verbs, the
best analysis selection algorithms could also
take advantage of valency frames of other
POS categories (nouns, adjectives).
References
R. J. Bobrow. 1991. Statistical agenda
parsing. In Proceedings of the February
1991 DARPA Speech and Natural Lan-
guage Workshop, pages 222?224. San Ma-
teo: Morgan Kaufmann.
H. Bunt and A. Nijholt, editors. 2000. Ad-
vances in Probabilistic and Other Parsing
Technologies. Kluwer Academic Publish-
ers.
S. Caraballo and E. Charniak. 1998. New
figures of merit for best-first probabilistic
chart parsing. Computational Linguistics,
24(2):275?298.
M. Chitrao and R. Grishman. 1990. Statisti-
cal parsing of messages. In Proceedings of
the Speech and Natural Language Work-
shop, pages 263?266, Hidden Valley, PA.
J. Hajic?. 1998. Building a syntactically an-
notated corpus: The Prague Dependency
Treebank. In Issues of Valency and Mean-
ing, pages 106?132, Prague. Karolinum.
J. T. Maxwell III and R. M. Kaplan. 1991.
The interface between phrasal and func-
tional constraints. In M. Rosner, C. J.
Rupp, and R. Johnson, editors, Proceed-
ings of the Workshop on Constraint Prop-
agation, Linguistic Description, and Com-
putation, pages 105?120. Instituto Dalle
Molle IDSIA, Lugano. Also in Computa-
tional Linguistics, Vol. 19, No. 4, 571?590,
1994.
R. C. Moore. 2000. Improved left-corner
chart parsing for large context-free gram-
mars. In Proceedings of the 6th IWPT,
pages 171?182, Trento, Italy.
K. Pala and P. S?evec?ek. 1997. Valencies of
Czech verbs. In Proceedings of Works of
Philosophical Faculty at the University of
Brno, pages 41?54. Brno. (in Czech).
P. Sgall, E. Hajic?ova?, and J. Panevova?.
1986. The Meaning of the Sentence
and Its Semantic and Pragmatic As-
pects. Academia/Reidel Publishing Com-
pany, Prague, Czech Republic/Dordrecht,
Netherlands.
P. Smrz? and A. Hora?k. 1999. Implementa-
tion of efficient and portable parser for
Czech. In Text, Speech and Dialogue:
Proceedings of the Second International
Workshop TSD?1999, Pilsen, Czech Re-
public. Springer Verlag, Lecture Notes in
Computer Science, Volume 1692.
Pavel Smrz? and Ales? Hora?k. 2000. Large
scale parsing of Czech. In Proceedings of
Efficiency in Large-Scale Parsing Systems
Workshop, COLING?2000, pages 43?50,
Saarbrucken: Universitaet des Saarlandes.
Integrating Natural Language Processing into E-learning ?
A Case of Czech
Pavel Smrz?
Faculty of Informatics, Masaryk University Brno
Botanicka? 68a, 602 00 Brno, Czech Republic
E-mail: smrz@fi.muni.cz
Abstract
The paper deals with the application of NLP
technology in e-learning. We report our re-
search on intelligent platforms for computer-
mediated education. Some of the methods de-
scribed in the paper have already taken part in
the end-user applications that are in everyday
use, others still wait for their implementation in
the form of software products. The main mes-
sage of the paper is that the language technol-
ogy, even in the imperfect form of the current
state of the art, can significantly enhance to-
day?s computer-mediated teaching and learning
activities. It is true especially for languages dif-
ferent from English, where the adopted learning
management systems often do not support even
the basic functionality of a language-oriented
search and retrieval of learning objects. As a
case study, this paper demonstrates the appli-
cation of the given ideas for e-learning materials
in Czech.
1 Introduction
Contemporary projects aiming at launching
learning management systems (LMS) often fo-
cus on the introduction of an existing software
tool, rather than on an innovation by means
of the modern information technologies. In ef-
fect, there is almost no original research di-
rected to the complex integration of e-learning
systems with the relevant IT such as assistive
technologies (dialogue systems, speech recog-
nition and synthesis . . . ), knowledge acquisi-
tion and knowledge management systems, etc.
Among others, the current LMS do not inte-
grate the emerging natural language processing
(NLP) applications. The adopted learning man-
agement systems often do not support even the
basic functionality of a language-oriented search
and retrieval of learning objects.
Futhermore, the present-day LMS are not di-
rectly linked to the wealth of relevant infor-
mation and knowledge sources. In the case of
the higher education institutions, these sources
could comprise standard libraries that provide
at least an electronic catalogue of their sources
nowadays, local digital libraries that are usu-
ally freely available for academics from particu-
lar institution, and the access to comprehensive
electronic archives or digital libraries that are
provided by many publishers and other organi-
zations on a commercial basis. Companies often
neglect valuable knowledge sources too. For ex-
ample, they should consider the integration of
their knowledge bases in the form of recorded
questions and answers from the call centers.
The current e-learning systems do not exploit
the potential of available high-level personaliza-
tion techniques and adaptability of the form,
the content and the access to the education.
Most of them cannot play the role of a show-
case for the modern teaching methods.
This paper surveys several areas, where NLP
techniques and technologies can enhance educa-
tional systems and applications. Some of them
exist in the form of prototypes only and have
not been applied in an end-user system yet.
Others find their place in software tools that
have been implemented by our team. They will
be briefly introduced in the paper.
The range of LMS used or tested at Masaryk
University, Brno, Czech Republic (MU) is
rather broad. The most important ones are IL-
IAS (http://www.ilias.uni-koeln.de/) and
MOODLE (http://moodle.org). Both sys-
tems are developed and distributed under the
term of the GNU General Public License and
provide open platform appropriate for the inte-
gration on NLP solutions. The actual project at
MU aims at unification of the used e-learning
platforms and their integration with the ad-
ministrative information server of the univer-
sity (Pavlovic et al, 2003). Even though sep-
arate systems would be more modular, easily
maintainable and extendable, we opt for the in-
tegrated solution that will benefit from the per-
manent technical support and personal assis-
tence of the administrative information server
team. We strongly believe that NLP techniques
as a part of the e-learning system can help to
open doors to those faculties and departments
that have not discovered the world of computer-
mediated education yet.
The paper discusses also the incorporation of
language resources to support the learner during
his/her interaction with an educational system
and to provide personalized learning. We also
tackle the use of NLP technologies and resources
to support the automatic assessment of learn-
ers? answers, especially those which are in free
text or restricted free text form. Such assess-
ment is useful to learners for controlling their
learning progress (self-regulation), to teachers
for gathering information about learners and
to systems for personalizing interaction. Con-
cept mapping is a knowledge elicitation tech-
nique, which stimulates learners to articulate
and synthesize their actual states of knowledge
during the learning process. We propose the
use of NLP in concept mapping systems in or-
der to interactively support learners, who build
concept maps and automate the process of the
assessment of concept maps. The availabil-
ity of wordnet-like semantic networks result-
ing from several projects such as EuroWord-
Net (Vossen, 1998), BalkaNet (BWN, 2004),
RussNet (Azarova, 2004), or broad-coverage on-
tologies such as SUMO (SUMO, 2003) provide
a reasonable starting point for such an effort.
The NLP applications in the area of e-
learning can be divided according to various cri-
teria. They can be specific for synchronous or
asynchronous mode of the course. The main
focus of the methods can be stressed to ad-
dress e.g. enhancements of the teaching ma-
terial accessibility or the adaptability of LMS.
Also the complexity of the needed NLP tech-
niques can make the distinctions, whether the
methods are already available and prepared to
integration into LMS or they need further de-
velopment. The availability of language re-
sources or language technology (lingware) for
the particular language can make the difference
too. A related issue can be the portability of
a solution for other languages or other subject
area, where subject-specific information cannot
be obtained fully automatically. One can also
divide the NLP applications in e-learning ac-
cording to the NLP modules that are integrated,
e.g. a language-specific morphological module
or named-entity analyzer could play a crucial
role. As the educational process has two faces
? learning and teaching, the boarder-line can
also be drawn between the tools focusing on the
students? side and those intended for the course
authors and teachers.
The last mentioned aspect has been taken
into account in this paper. It is organized as
follows: The next section discusses NLP tech-
niques aimed at enhancements for the end-users
of e-learning systems ? students looking for an
appropriate e-learning material or those who al-
ready enrolled in a course. The third section
tackles the support of authors and providers of
the e-learning facilities that can take the ad-
vantage of the language and text technology
too. Of course, the boundary between those two
cases is not strict at all, so there are NLP tools
that can help both types of LMS users. The
fourth section then covers supplementary in-
formation technologies such as multimedia and
audio- or video- recording of courses that can-
not be classified as NLP per se but are strongly
related and, as our experience already shows,
their integration should be at least coordinated
with the employment language technology solu-
tions. The paper concludes with future direc-
tions of our research.
2 NLP Support of LMS End-users
2.1 Basic Methods
Several standard NLP tools have been de-
veloped recently in Natural Language Pro-
cessing Laboratory, Faculty of Informatics
(NLPlab FI MU) that are beneficial for many
areas. E-learning is no exception in this respect.
The most important module is morphological
analyzer AJKA (Sedlacek and Smrz, 2001) that
serves as a base for various related modules.
The importance of the morphology for Czech
can be demonstrated by the history of the web
search engines in the Czech Republic. Czech is
a representative of Slavonic languages charac-
terized by abundance of inflective morphologi-
cal processes. For example, nouns, adjectives,
pronouns and numerals can be present in one of
7 grammatical cases in Czech (nominative, gen-
itive, dative, accusative, vocative, locative and
instrumental), three grammatical numbers (sin-
gular, plural and dual), and three genders (mas-
culine, feminine and neuter), in which masculine
exists in two forms - animate and inanimate.
The most popular Czech web portals started
with the search engine provided by Google, but
after a short time they replaced it by special-
ized language-aware systems developed usually
in cooperation with Czech universities.
The story shows that even generally recog-
nized implementations can be outrivaled, if they
ignore language-specific features. It holds not
only for web environment but also for the e-
learning. Projects implementing LMS need to
integrate strong language support and go be-
yond simple localization.
A lot of e-learning material has been pro-
duced in last years and the number grows as
many new courses are prepared in an electronic
form nowadays. One of the most important ap-
plications of AJKA is therefore the system that
enables search in the available e-learning ma-
terials. As the document source language and
the encoding are not always specified explicitly,
a text technology module ? language guesser
based on the language samples in a particular
encoding ? has been also implemented and in-
corporated into the e-learning search engine.
Another language resource that proved to be
very useful in our experiments is the query ex-
pansion module based on information from the
Czech WordNet (Pala and Smrz, 2004). The
crucial point here is the improvement of the user
interface. Users are able to search for all word
forms, synonyms, hypernyms and other seman-
tically related words, exclude co-hyponyms, etc.
They can also choose between orthographic, re-
gional, style or register variants, words derived
from the term searched, verbs related by the
aspect, etc.
Of course, even a stemmer or a simple lem-
matizer would suit the purpose of the search
engine. However, the strength of the full mor-
phological module becomes apparent when the
incorporation of deeper analyses of texts is the
issue. We currently work on a module that will
be able to summarize the content of related doc-
uments. The immediate target of the tool is the
summarization of messages in course discussion
groups. Sometimes, the students are very ac-
tive (the discussion is occasionally off-topic) and
even a simple browsing through the discussion
threads from previous runs of a course could
present a tedious work. The automatic extrac-
tion of the most important points can signifi-
cantly help newcomers.
Recently, we have added automatic classifica-
tion of e-learning materials into given categories
or at least clustering if there are no pre-defined
classes. Both ? learners and authors can take
advantage of these options but the role of self-
learning is stressed in the current version. The
function that proved to be most useful for e-
learning content is searching for documents sim-
ilar to a given course or an additional learn-
ing material. Inspired by the biggest dictionary
publishing servers, we will provide ?a new doc-
ument for this day? function and allow course
authors to specify what should be the pool of
searched documents. As there is currently an
e-learning project covering all our university,
we strongly believe that such functionality will
draw a broad attention to the new methods of
teaching and learning.
All the enhancements of the search for the
additional e-learning material mentioned above
are also applicable in the search for appropri-
ate courses. The method based on the statis-
tical tests known from the computational lin-
guistics is provided which enables the automatic
extraction of keywords that can be added to
those specified by course authors. The available
named-entity and multiword-expression analyz-
ers for Czech have found their place in this task.
The summarization module is applicable here
too as not all authors provide enough metadata
to facilitate searching courses or relevant parts
of them. An automatically generated index as
well as glossary or a small encyclopedia from a
comprehensive course enables non-linear pass-
ing through the learning material, which be-
comes a norm.
Another technique that enables students to
concentrate on the selected parts of a course
is the matching of the course content to the
student?s knowledge of a given topic. The
method applied to the matching is derived from
the standard assessment procedure for language
learners. As the task for the first assignment,
students are asked to write an essay covering
their current knowledge about the subject of the
course. The content of the document is com-
pared with the sections (lectures) of the given
course and the parts that are not covered suffi-
ciently are presented for further study.
The described approach can serve as the
launching point for the LMS personalization.
Some e-learning courses already contain special
mechanisms to offer a path through the teach-
ing material adapted to the needs of particular
user. Such an enhancement can improve the
acceptance of the content by users that often
face learning materials that are too in-depth in
some parts and too sketchy in others. Also,
the speed of presentation of the teaching ma-
terial can vary according to user needs. Besides
the above-mentioned assignment of essays, the
simple statistical technique has been adopted
that automatically evaluates the match of stu-
dents? answers and determines knowledge lev-
els of the students. The solution is now ready
to be applied generally to all e-learning courses
provided by MU. It can be generalized to allow
semi-automatic generation of tests for each part
of the subject matter that would ?send? stu-
dents (back) to the parts they should go through
again.
The presented adaptive components, which
adjust the system to particular users, are not
the only ones that we thought about and that
are covered in our e-learning environment. As
FI MU hosts centre Teiresias, which is responsi-
ble for helping students with special needs from
all departments of MU (http://www.muni.cz/
teiresias/), we focused also on the adaptation
of LMS for disabled. The activity of the center
is not limited to the electronic form of learning
but the context of the computer-mediated edu-
cation fits its pursuance perfectly. We currently
plan the first e-learning course that would be
fully available both in Czech and in Brail for
visually impaired people. The content different
from a plain text still presents a problem as it is
sometimes very difficult to find Brail equivalents
for mathematical expressions or various symbols
and it is always an extremely time-consuming
work.
We should also mention the role of the speech
synthesizer developed by our team that is avail-
able for the needs of visually impaired partici-
pants of the e-learning courses. The users are
able to help other students providing the pro-
nunciation of the words, which is not correctly
derived from its orthographical representation
by our automatic engine.
2.2 Question Answering and Exercise
Generation
The automatic question answering (QA) is an-
other task where the morphological as well as
surface syntactic analyses (Smrz and Horak,
2000) play the crucial role. We gain from our ex-
perience with ?Encyclopedia Expert? (Svoboda,
2002), which is able to answer free questions
based on the information extracted from an
available Czech encyclopedia. A set of semantic
frames and corresponding syntactic structures
is defined that enables analysis of the most fre-
quent question types. The rest of queries are
answered by the full-text search and the iden-
tification of the relevant part of the document
containing the answer. Both cases are based on
the question-type analysis module determining
what information should be looked for.
The employment of the same strategy in
the context of e-learning material is straight-
forward. The current system is able to ap-
ply the mechanisms described above to answer
questions based on the content of a particular
course. The information about the part of the
course, which contains the answer, can be also
returned. The evaluation of the QA module
showed that the precision on queries that can be
answered just on the content of a given course
is pretty high. Even for information not cov-
ered by the pre-defined frames (answered by the
full-text search) it was about 87 percent (errors
are mainly due to the incorrect or incomplete
analysis by the morphological module ? un-
known words). On the other hand, practical
assessment of the module showed that the de-
scribed functionality has only a limited use for
the course participants. Many students would
need and appreciate a search for answer in a
much broader range of relevant e-learning ma-
terials. Therefore, we are going to provide such
an option. Of course, the number of errors can
increase in such a setting and it will be the re-
sponsibility of the course author to check the
function of the QA module on the sources he
or she identified and to improve the results by
means of additional linguistic resources needed
in the analysis phase.
The promising results of automatic QA led
us to the idea to engage similar NLP methods
the other way around and automatically gen-
erate questions and perhaps whole tests based
on the content of particular e-learning courses.
It is usually easy to extract ?what is? ques-
tions or ask about a particular fact explicitly
stated in the text. Sometimes, the structure
of the documents itself helps to identify the
important knowledge; sometimes, the above-
mentioned keyword extraction algorithm can be
employed.
The tricky part of the process is therefore
to create a procedure that would be reliable
enough for the automatic checking of answers.
Again, the basic QA module is satisfactory for
the questions that expect factoid as the an-
swer. However, it is much more difficult to
automatically evaluate more elaborate answers.
Although we are not able to present final re-
sults for this part yet, the preliminary ones show
that the application of the same method as for
the comparison student essays with the content
could provide at least a good approximation of
the assessment that would be given by human
teachers.
The weak point of the described automatic
generator of exercises is its focus on the fac-
tography and impossibility to verify that stu-
dents really understand the content, that they
got the heart of the matter and are able to ap-
ply the obtained knowledge. Obviously, it is still
far from today, when computers will be able to
substitute human teachers in this respect. An
interesting step, that at least simulates such a
function and that is investigated by our current
experiments, is the employment of standard on-
tologies for this purpose. The interest in on-
tologies increased with the recognition of their
importance for the Semantic Web. The emerg-
ing information systems need the definition of
common understanding for their application do-
mains. Even though ontologies are meant as a
means providing human knowledge to machines,
they can be very useful for e-learning too. The
formal specification of the concepts and rela-
tions between them takes usually advantage of
the new XML-family standards, RDF (Beckett,
2003) and OWL (van Harmelen et al, 2003).
The latter serves as a base for our recent re-
search on the possibility to automatically gen-
erate exercise questions asking for relations be-
tween concepts given by an ontology. As the
number of available ontologies for various sub-
ject domains will surely increase this direction
of research has significant potential.
2.3 Language Learning
In our work we pay a special attention to NLP
methods applied in the area of language learning
and teaching. The role of empirical data in the
form of corpora ? large collections of written or
spoken language in the digital form ? is gener-
ally recognized in the computational linguistics.
Corpora are crucial for language learning too.
For example, the English courses taught at our
faculty bear on BNC (the British National Cor-
pus) and other available English corpora (e. g.
Times Corpus). The standard form of the vo-
cabulary test is automatically generated which
lists concordances of a word that is deleted from
all the presented occurrences and students have
to fill the gap.
Corpora are beneficial not only for generation
of queries. Students often use them as the pri-
mary source to learn about the usage of a word.
What complicates the process is the presence of
words students are not familiar with. Another
direction of our research that is currently un-
der development is the effort to sort the concor-
dance list according to the estimated complexity
of words in them. To be able to compute such
a measure efficiently even for extensive concor-
dance lists, the evaluation is based on heuristics
that take into account frequencies of the words
in the contexts.
New approach that will find its role in the
modern computer-mediated language learning
is the employment of the word sketch engine
described recently in (Kilgarriff et al, 2004).
Word sketches are brief automatic corpus-based
summaries of a word?s grammatical and colloca-
tional behavior. The sketch engine is a corpus
tool developed by our team which takes as input
a corpus of any language and a corresponding
grammar patterns and generates word sketches
for the words of that language as its outputs.
The most helpful feature of the system from
the language e-learning point of view are not
word sketches per se but the ability to auto-
matically compare sketches generated for dif-
ferent words. It is crucial especially for seman-
tically similar words, e.g. for near synonyms
or co-hyponyms. Students are able to com-
pare collocates and grammatical relations of the
words and the system can also automatically
generate tests checking whether learners know
the difference between semantically close words.
The word sketch engine also generates a the-
saurus that can be directly used in inspecting
the knowledge of a particular semantic field.
Correct answers are usually required to en-
ter next levels of a course. However, errors can
play a significant role too. A special attention
has been paid to the errors produced by stu-
dents in project Czenglish. It is a join project of
NLPlab FI MU and the Department of English
at the Faculty of Arts. The e-learning course is
based on popular book ?English or Czenglish?
which is intended for students of English at the
advanced or the professional levels. Students
produce translations of sentences presented by
the system. If a translation does not match a
stored one, the correct answers are displayed.
At the same time, the actual answer is com-
pared with the examples of listed incorrect ones.
If a match is found the explanation of the error
is presented. Students may also indicate, that
they still believe their translation is possible.
The message is sent to the teacher that has to
decide whether it should be added to the list of
correct answers. Such a decision automatically
affects the assessment of the student?s test.
3 NLP Techniques for Authors and
Teachers
3.1 Course Preparation
The quality of LMS raised enormously in last
decades. Students are able to work with e-
learning applications that are much more user-
friendly than some years ago. However, simulta-
neously with the quality increased the complex-
ity of the systems for content providers. To pre-
pare a good course, teachers need to learn how
to use the authoring system, how to determine
the possible passes through the material etc.
The authoring systems are usually linked to par-
ticular LMS. At least a basic understanding of
the technical stuff and e-learning standards be-
hind the particular LMS is often required. Our
experience clearly shows that text and language
technology can significantly help the e-learning
especially in the phase of course preparation and
is extremely beneficial for teachers.
The context analysis and the evaluation of
the similarity between a new e-learning con-
tent and the existing courses can be employed
to speed-up the process of course preparation.
We currently prepare an expert system that will
serve as an assistant for authors. The system
compares the provided content with the stored
courses and uses found similarities to propose
standard links (to additional materials, on-line
encyclopedias etc.) It also groups the possible
actions to enable authors to perform many nec-
essary actions at one-click. It is advantageous
especially for providing metadata for the learn-
ing objects that are understood as a must for
all the future e-learning content.
A course with a dense web of hyperlinks can
turn to be a nightmare for maintainers in a dy-
namic environment. To keep track of all the
links and relations between the e-learning con-
tent, a new system called DEB has been de-
signed and implemented by our team (Smrz
and Povolny, 2003). It is a client-server ap-
plication that enables efficient storage and re-
trieval of XML documents. Also complex struc-
tural and content queries can be defined. For
example, it is the main platform to host the
above-mentioned Czenglish project. It guaran-
tees that the whole content will stay consistent
when linked information is changed. The consis-
tency checks are defined as XSLT sheets (Clark,
1999) and the system reports any violation of
the predefined constraints.
The implemented mechanisms of XSLT trans-
formations are powerful enough to be inte-
grated in another important task of the cur-
rent LMS ? the support of open e-learning
standards. All new learning objects developed
at our faculty should be developed in such
a way to facilitate sharing and interchange.
As the field of LMS is rather hot and many
commercial as well as public-domain systems
emerged recently, the newly produced learning
object should also reflect the possibility of a
switch between platforms without an informa-
tion loss. The currently developed authoring
module defines XSLT transformations of the
primary document (DocBook-like (Walsh and
Muellner, 1999)) format into LOM/SCORM
compliant forms. It could help to put the plat-
form independent standards into wide use.
3.2 Course Run
The support to teachers during the run of
courses is at least equally important as in the
phase of the content preparation. It is true es-
pecially if a course has already run several times
and the feed-back from the previous runs should
be reflected in the actual one. The current sys-
tems usually ignore the possibility to reflect the
experience from the past years. The students?
answers, either correct or incorrect, are often
thrown away or used just for elementary statisti-
cal profiles. One of the most valuable resources
is frequently neglected.
The results of our recent research reveal
that the detailed analysis of the students? out-
puts offers an extremely useful material di-
rectly applicable in the optimization of the e-
learning content. For example, special tools
have been developed for tagging and categoriz-
ing the grammatical and stylistic errors in stu-
dent essays (Pala et al, 2003). Teachers mark
the errors in the electronic documents. Students
have to correct them in the new version but also
to record the original (incorrect) form and the
error type. The files in the source format (usu-
ally MS Word or LaTeX) are transformed into
an appropriate XML format. A special form of
a learner corpus has been developed that serves
as a base for focusing the teaching in a spe-
cial course. As a side effect, the created re-
source is used to develop the Czech grammar
checker (Smrz and Horak, 1999) which will be
the first one based on the large empirical data.
The attempts to make the assessment of stu-
dent knowledge as objective as possible and to
reduce the teacher?s work led to the spread
of multi-choice tests in last decades. It is a
well-known fact that this form of testing has
many disadvantages too and that the focus
on them can easily produce test-experts rather
than people understanding the subject. The
current technology offers various means to im-
plement intelligent tests and to escape the trap
of multi-choice tests. Our current research con-
centrates on the integration of NLP techniques
into the evaluation module of LMS. The expe-
rience shows that it is relatively easy to provide
such functionality for short answers in the form
of phrases described by simple grammatical pat-
terns. The results of the first limited experi-
ment are very promising as the method reduced
the number of answers that needed to be pro-
cessed manually significantly (only 31 % of the
cases remained for the manual check). However,
there are also many open questions concerning
scaling-up the method of the answer patterns.
The longer the answers are, the less matches be-
tween the pre-defined applications are found. A
more general solution of the analyzing mecha-
nism in the form of a general grammar is ob-
viously needed. However, the available gram-
mar for Czech is developed for a robust ana-
lyzer so the ambiguity of the analysis tends to
be rather high. Also it is much more difficult
for authors that do not specialize in the com-
putational linguistics to define the acceptable
forms of answers. The assessment of open ques-
tions is definitively one of the actual topics of
our research.
Although the current state-of-the-art in NLP
does not allow full computerization of the as-
sessment of general tests, there are areas, where
the technology can already take over the role
traditionally falling upon the teacher. We have
recently implemented the automatic evaluation
of programs in the Java programming course at
FI MU and also the phrase-structure grammars
developed by students of the ?Introduction to
the computational linguistics?. Students are
obliged to provide their assignments in the des-
ignated form. Pre-defined tests are applied to
verify the correctness of the provided solutions.
We are to provide a similar function for a gram-
matical pre-checking of students? essays. Even
though only a limited number of error types can
be identified automatically, the method can still
reduce a significant portion of the tedious teach-
ers? work.
3.3 Additional Functions
The previous section discussed how the stan-
dard NLP methods can help students to search
in the e-learning data. The same technique
can be integrated into the LMS authoring sub-
system. Especially, the tedious linking of the
additional course sources can be facilitated by
NLP. We have designed and implemented a sys-
tem which automatically finds relevant articles
and papers available for the students of our fac-
ulty or the university (digital libraries provided
by ACM and Springer-Verlag), freely accessi-
ble on the web (www.arxiv.gov, ResearchIndex)
or that are at readers? disposal at the faculty
libraries. The experience shows that the per-
sonalization of the application interface is per-
haps even more important for authors than for
students. Too complex environment can scare
off and discourage some authors, others call for
more functions and more possibilities to deter-
mine the behavior of the search engine.
Many e-learning courses have been recently
created all over the world; many of them are
available for the general public via Internet now.
Most of the resources are written in English. It
is sometimes impossible to take directly the for-
eign version, at least a part of the content needs
to be translated. The NLP technology known
as translation memory finds its place here. The
motivation of the application of the translation
memories is the same as in the common local-
ization of software. If the content of the e-
course will be modified and a new version be-
comes available, the ?memory? of the tool will
help to translate parts that remain the same or
changed in a limited extent only. Our experi-
ence of the localization based on DejaVu soft-
ware (Dej, 2003) is rather positive as it enabled
to ?upgrade? our English-Czech translation in
just a week.
The last piece of software that will be men-
tioned here is the experimental plagiarism iden-
tifier. Very simple methods comparing word n-
grams showed to be efficient and precise enough
to identify plagiarism in cases where both the
original and the derived document are in one
language (Czech). However, teachers often find
essay that are word-for-word translations of an
original text. A reliable automatic identification
of such cases is difficult as the n-gram methods
could not provide reasonable precision due to
the difference of the syntactical structures be-
tween Czech and English. This kind of checking
forms another direction of our future research.
4 Web services and Multimedia
Support
4.1 Service-Oriented Architecture for
E-learning
A typical LMS is constructed as a monolithic
piece of software or a set of tools fully integrated
into another system such as administrative in-
formation system. The first really open systems
able to communicate with the environment via
platform-independent channels emerged in the
area of LMS only recently. The long-term goal
of our research is to provide NLP solutions for
LMS via open technologies in the heterogeneous
systems. The form of web services seems to be
appropriate for the task.
Generally speaking, our view is based on the
idea of the service oriented architecture. This
emerging approach assumes that software ap-
plications implement only functions specific for
their tasks, while more general functions are im-
plemented as services available on the net.
If a method can be provided as a web ser-
vice, it immediately evokes the idea of ?out-
sourcing?. A standard task in the prepara-
tion of learning courses is the search for addi-
tional material, its indexing and linking to the
primary course texts. As has been presented
above, NLP can offer supplementary functions,
such as automatic question answering. How-
ever, as has been also shown, the preparation of
necessary resources used in the analysis needs
detailed knowledge about the way a particu-
lar NLP techniques work. One reason for the
outsourcing lies therefore in the effort to en-
able users to focus on the content of e-learning
courses only and to provide other services exter-
nally (free of charge or on a commercial basis).
To motivate the other reason, let us share the
experience of the processing large corpora. The
above-mentioned word sketch engine needs sev-
eral hours to compute the necessary statistics on
a 100,000-word corpus (BNC). The implemen-
tation takes advantage of the available powerful
PC workstations. To process a 1-billion cor-
pus in the same time, we would need a super-
computer to perform the task. However, such a
computer would be idle almost all the time in
our laboratory. It is much easier and cheaper
to ?hire? the computing power needed for the
processing.
Many modern NLP methods are based on
large language corpora (hundreds thousands of
words) and lexical databases (e.g. Princeton
WordNet (Miller et al, 1990) contains more
than 200,000 lexical units and their relations).
High-speed networks and Grid systems are very
important in this context as they enable to
transfer the processing of resource-demanding
tasks such as the creation of indices for large
collections of data to the available powerful sys-
tems. Such systems can moreover benefit from
the analysis of various corpora. Such tasks can
be fully outsourced without an allocation of own
capacity ? in terms of efficiency as well as lan-
guage resources.
The described approach is one of the most
actual directions of the integration of NLP and
the Semantic Web. It is studied at FI MU espe-
cially as a part of the recently proposed project
combining e-learning and the Grid technologies.
We propose to integrate and evaluate services
based on the new specification WSRF (Web Ser-
vice Resource Framework) that defines a unify-
ing view at the web and Grid services.
4.2 E-learning and Multimedia Support
The modern e-learning courses are often supple-
mented by a multimedia data. It can range of
a simple recorded audio from the lecture to the
full video recording and its streaming with in-
tegrated switching between the shots of the lec-
turer and his/her presentation. A recent topic
we are working on deals with the software tools
that can facilitate the preparation of such an
e-learning material. One of the most impor-
tant current issues is the research on the col-
laboration between LMS and the development
platforms for multimedia applications (Author-
ware (Aut, 2004) in our case).
Processing and indexing the multimedia con-
tent is another large issue. We currently pre-
pare a new methodology to link the presenta-
tion with the lecture recording. We also per-
formed the first experiments aiming at the eval-
uation of automatic linking of the presentation
content to the recording by means of automatic
speech recognition. The obtained unsatisfac-
tory results influence the changes in the record-
ing setting with the aim to improve the acoustic
quality of the recording. We would like to define
a universal methodology applicable also outside
the university.
The next priority of our research is the shared
teaching. A pilot course with shared parts of se-
lected lectures transmitted between two distant
sites (Prague and Brno) will run in the next
semester. The experience with the teleconfer-
encing will surely be a helpful for this task, espe-
cially the verified camera tracking mechanisms.
FI MU is also active in the field of streaming the
lecture recordings via the high-speed networks.
5 Conclusions
The broader context of all the research de-
scribed in the paper is given by the common e-
learning strategy built by several leading univer-
sities in the Czech Republic. The initiative in-
cludes the support for choosing LMS platform,
study of the standards in the area, publishing
good practices for e-learning materials and the
new forms of education, etc. The incorporation
of NLP techniques to extend the functionality
of the current LMS becomes one of the primary
action lines in the program.
The experience gained by some faculties at
MU as well as of other cooperating institu-
tions demonstrates that preparation and pro-
viding of the e-learning courses can generate
significant profit. It holds not only for commer-
cial providers but also for the universities (dis-
tant learning courses). Moreover, the idea of
computer-mediated education is in accord with
the long-term aim to internationalize the uni-
versities in the Czech Republic. The described
NLP techniques and other modern approaches
help to open the institutions to students from
all over the world.
Acknowledgements
This work was supported by Ministry of Edu-
cation of the Czech Republic Research Intents
MSM6383917201 and CEZ:J07/98:143300003,
Grant Agency of the Czech Republic Grant
GACR 405/03/0913 and by EU project Balka-
Net IST-2000-29388.
References
2004. Macromedia Authorware 7, http://www.
macromedia.com/software/authorware/.
Irina V. Azarova. 2004. RussNet ? word-
net for Russian. http://www.phil.pu.ru/
depts/12/RN/.
2004. Balkanet project website, http://www.
ceid.upatras.gr/Balkanet/.
Dave Beckett. 2003. Rdf/xml syntax spec-
ification. http://www.w3.org/TR/2003/
WD-rdf-syntax-grammar-20030123/.
James Clark. 1999. XSL Transformations
(XSLT) Version 1.0. (http://www.w3.org/
TR/xslt).
2003. DejaVu translation memory and produc-
tivity system, http://www.atril.com/.
Adam Kilgarriff, Pavel Rychly, Pavel Smrz, and
David Tugwell. 2004. The sketch engine. In
Proceedings of Euralex 2004. (to be pub-
lished).
George Miller, Richard Beckwith, Christiane
Fellbaum, Derek Gross, and Katherine Miller.
1990. Five papers on wordnet. Technical Re-
port CSL Report 43, Cognitive Science Lab-
oratory, Princeton University.
Karel Pala and Pavel Smrz. 2004. Building
Czech wordnet. Romanian Journal of Infor-
mation Science and Technology, Special Issue
on BalkaNet. (to be published).
Karel Pala, Pavel Rychly, and Pavel Smrz.
2003. Text corpus with errors. In Proceedings
of TSD 2003, Berlin. Springer-Verlag. Lec-
ture Notes in Artificial Intelligence.
Jan Pavlovic, Tomas Pitner, Pavel Smrz, and
Jiri Verner. 2003. Customization of ILIAS
and its integration with the university infor-
mation system. In ILIAS 2003, Cologne, Ger-
many.
Radek Sedlacek and Pavel Smrz. 2001. A new
Czech morphological analyser ajka. In Pro-
ceedings of the TSD 2001, pages 100?107,
Czech Republic.
Pavel Smrz and Ales Horak. 1999. Implementa-
tion of efficient and portable parser for Czech.
In Proceedings of TSD?99, pages 105?108,
Berlin. Springer-Verlag. Lecture Notes in Ar-
tificial Intelligence 1692.
Pavel Smrz and Ales Horak. 2000. Large scale
parsing of Czech. In Proceedings of Efficiency
in Large-Scale Parsing Systems Workshop,
COLING 2000, pages 43?50, Saarbrucken:
Universitaet des Saarlandes.
Pavel Smrz and Martin Povolny. 2003. DEB -
Dictionary Editing and Browsing. In Proceed-
ings of the EACL03 Workshop on Language
Technology and the Semantic Web: The 3rd
Workshop on NLP and XML (NLPXML-
2003), pages 49?55, Budapest, Hungary.
2003. SUMO ? Suggested Upper Merged
Ontology, http://ontology.teknowledge.
com/.
Zdenek Svoboda. 2002. Znalec encyklope-
die (encyclopedia expert). Master?s thesis,
Faculty of Informatics, Masaryk University,
Brno.
Frank van Harmelen, Jim Hendler Ian Hor-
rocks, Deborah L. McGuinness, Peter F.
Patel-Schneider, and Lynn Andrea Stein.
2003. OWL web ontology language reference.
http://www.w3.org/TR/owl-ref/.
Piek Vossen, editor. 1998. EuroWordNet: A
Multilingual Database with Lexical Semantic
Networks. Kluwer Academic Publishers, Dor-
drecht.
Norman Walsh and Leonard Muellner. 1999.
DocBook: The Definitive Guide. O?Reilly.
Event Detection and Summarization in Weblogs with Temporal Collocations 
Chun-Yuan Teng and Hsin-Hsi Chen 
Department of Computer Science and Information Engineering 
National Taiwan University 
Taipei, Taiwan 
{r93019, hhchen}@csie.ntu.edu.tw 
Abstract 
 
This paper deals with the relationship between weblog content and time. With the proposed temporal mutual information, we analyze 
the collocations in time dimension, and the interesting collocations related to special events. The temporal mutual information is 
employed to observe the strength of term-to-term associations over time. An event detection algorithm identifies the collocations that 
may cause an event in a specific timestamp. An event summarization algorithm retrieves a set of collocations which describe an event. 
We compare our approach with the approach without considering the time interval. The experimental results demonstrate that the 
temporal collocations capture the real world semantics and real world events over time. 
 
1. 
2. 
Introduction 
Compared with traditional media such as online news 
and enterprise websites, weblogs have several unique 
characteristics, e.g., containing abundant life experiences 
and public opinions toward different topics, highly 
sensitive to the events occurring in the real world, and 
associated with the personal information of bloggers. 
Some works have been proposed to leverage these 
characteristics, e.g., the study of the relationship between 
the content and bloggers? profiles (Adamic & Glance, 
2005; Burger & Henderson, 2006; Teng & Chen, 2006), 
and content and real events (Glance, Hurst & Tornkiyo, 
2004; Kim, 2005; Thelwall, 2006; Thompson, 2003). 
In this paper, we will use temporal collocation to 
model the term-to-term association over time.  In the past, 
some useful collocation models (Manning & Sch?tze, 
1999) have been proposed such as mean and variance, 
hypothesis test, mutual information, etc. Some works 
analyze the weblogs from the aspect of time like the 
dynamics of weblogs in time and location (Mei, et al, 
2006), the weblog posting behavior (Doran, Griffith & 
Henderson, 2006; Hurst, 2006), the topic extraction (Oka, 
Abe & Kato, 2006), etc. The impacts of events on social 
media are also discussed, e.g., the change of weblogs after 
London attack (Thelwall, 2006), the relationship between 
the warblog and weblogs (Kim, 2005; Thompson, 2003), 
etc. 
This paper is organized as follows. Section 2 defines 
temporal collocation to model the strength of term-to-term 
associations over time.  Section 3 introduces an event 
detection algorithm to detect the events in weblogs, and 
an event summarization algorithm to extract the 
description of an event in a specific time with temporal 
collocations. Section 4 shows and discusses the 
experimental results.  Section 5 concludes the remarks. 
Temporal Collocations 
We derive the temporal collocations from Shannon?s 
mutual information (Manning & Sch?tze, 1999) which is 
defined as follows (Definition 1). 
Definition 1 (Mutual Information) The mutual 
information of two terms x and y is defined as: 
)()(
),(log),(),(
yPxP
yxPyxPyxI =  
where P(x,y) is the co-occurrence probability of x and y, 
and P(x) and P(y) denote the occurrence probability of x 
and y, respectively. 
Following the definition of mutual information, we 
derive the temporal mutual information modeling the 
term-to-term association over time, and the definition is 
given as follows.  
 Definition 2 (Temporal Mutual Information) Given 
a timestamp t and a pair of terms x and y, the temporal 
mutual information of x and y in t is defined as: 
)|()|(
)|,(log)|,()|,(
tyPtxP
tyxPtyxPtyxI =
where P(x,y|t) is the probability of co-occurrence of terms 
x and y in timestamp t, P(x|t) and P(y|t) denote the 
probability of occurrences of x and y in timestamp t, 
respectively. 
To measure the change of mutual information in time 
dimension, we define the change of temporal mutual 
information as follows. 
Definition 3 (Change of Temporal Mutual 
Information) Given time interval [t1, t2], the change of 
temporal mutual information is defined as: 
12
12
21
)|,()|,(),,,(
tt
tyxItyxIttyxC ?
?=  
where C(x,y,t1,t2) is the change of temporal mutual 
information of terms x and y in time interval [t1, t2], I(x,y| 
t1) and I(x,y| t2) are the temporal mutual information in 
time t1 and t2, respectively. 
3. Event Detection 
Event detection aims to identify the collocations 
resulting in events and then retrieve the description of 
events. Figure 1 sketches an example of event detection. 
The weblog is parsed into a set of collocations. All 
collocations are processed and monitored to identify the 
plausible events.  Here, a regular event ?Mother?s day? 
and an irregular event ?Typhoon Chanchu? are detected.  
The event ?Typhoon Chanchu? is described by the words  
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1: An Example of Event Detection
?Typhoon?, ?Chanchu?, ?2k?, ?Eye?, ?Path? and 
?chinaphillippine?.  
The architecture of an event detection system includes 
a preprocessing phase for parsing the weblogs and 
retrieving the collocations; an event detection phase 
detecting the unusual peak of the change of temporal 
mutual information and identifying the set of collocations 
which may result in an event in a specific time duration; 
and an event summarization phase extracting the 
collocations related to the seed collocations found in a 
specific time duration. 
The most important part in the preprocessing phase is 
collocation extraction. We retrieve the collocations from 
the sentences in blog posts. The candidates are two terms 
within a window size. Due to the size of candidates, we 
have to identify the set of tracking terms for further 
analysis. In this paper, those candidates containing 
stopwords or with low change of temporal mutual 
information are removed. 
In the event detection phase, we detect events by 
using the peak of temporal mutual information in time 
dimension.  However, the regular pattern of temporal 
mutual information may cause problems to our detection. 
Therefore, we remove the regular pattern by seasonal 
index, and then detect the plausible events by measuring 
the unusual peak of temporal mutual information. 
If a topic is suddenly discussed, the relationship 
between the related terms will become higher. Two 
alternatives including change of temporal mutual 
information and relative change of temporal mutual 
information are employed to detect unusual events. Given 
timestamps t1 and t2 with temporal mutual information 
MI1 and MI2, the change of temporal mutual information 
is calculated by (MI2-MI1). The relative change of 
temporal mutual information is calculated by (MI2-
MI1)/MI1. 
For each plausible event, there is a seed collocation, 
e.g., ?Typhoon Chanchu?. In the event description 
retrieval phase, we try to select the collocations with the 
highest mutual information with the word w in a seed 
collocation. They will form a collocation network for the 
event.  Initially, the seed collocation is placed into the 
network.  When a new collocation is added, we compute 
the mutual information of the multiword collocations by 
the following formula, where n is the number of 
collocations in the network up to now. 
?= n iMInInformatioMutualMultiwo  
If the multiword mutual information is lower than a 
threshold, the algorithm stops and returns the words in the 
collocation network as a description of the event.  Figure 
2 sketches an example.  The collocations ?Chanchu?s 
path?, ?Typhoon eye?, and ?Chanchu affects? are added 
into the network in sequence based on their MI. 
We have two alternatives to add the collocations to 
the event description. The first method adds the 
collocations which have the highest mutual information 
as discussed above. In contrast, the second method adds 
the collocations which have the highest product of mutual 
information and change of temporal mutual information. 
 
 
 
 
 
 
Figure 2: An Example of Collocation network 
4. 
4.1. 
Experiments and Discussions 
Temporal Mutual Information versus 
Mutual Information 
In the experiments, we adopt the ICWSM weblog data 
set (Teng & Chen, 2007; ICWSM, 2007). This data set 
collected from May 1, 2006 through May 20, 2006 is 
about 20 GB. Without loss of generality, we use the 
English weblog of 2,734,518 articles for analysis. 
To evaluate the effectiveness of time information, we 
made the experiments based on mutual information 
(Definition 1) and temporal mutual information 
(Definition 2). The former called the incremental 
approach measures the mutual information at each time 
point based on all available temporal information at that 
time. The latter called the interval-based approach 
considers the temporal mutual information in different 
time stamps.  Figures 3 and 4 show the comparisons 
between interval-based approach and incremental 
approach, respectively, in the event of Da Vinci Code.   
We find that ?Tom Hanks? has higher change of 
temporal mutual information compared to ?Da Vinci 
Code?. Compared to the incremental approach in Figure 4, 
the interval-based approach can reflect the exact release 
date of ?Da Vinci Code.? 
 rd
=i 1 4.2. Evaluation of Event Detection 
We consider the events of May 2006 listed in 
wikipedia1 as gold standard. On the one hand, the events 
posted in wikipedia are not always complete, so that we 
adopt recall rate as our evaluation metric.  On the other 
hand, the events specified in wikipedia are not always 
discussed in weblogs.  Thus, we search the contents of 
blog post to verify if the events were touched on in our 
blog corpus. Before evaluation, we remove the events 
listed in wikipedia, but not referenced in the weblogs. 
 
 
 
 
 
 
 
 
 
 
 
Figure 3: Interval-based Approach in Da Vinci Code  
 
 
 
 
 
 
 
 
Figure 4: Incremental Approach in Da Vinci Code 
gure 5 sketches the idea of evaluation.  The left side 
of t s figure shows the collocations detected by our event 
dete tion system, and the right side shows the events 
liste  in wikipedia.  After matching these two lists, we 
can find that the first three listed events were correctly 
identified by our system.  Only the event ?Nepal Civil 
War? was listed, but not found. Thus, the recall rate is 
75% in this case. 
 
 
 
 
 
 
 
Figure 5: Evaluation of Event Detection Phase 
As discussed in Section 3, we adopt change of 
temporal mutual information, and relative change of 
temporal mutual information to detect the peak. In Figure 
6, we compare the two methods to detect the events in 
weblogs. The relative change of temporal mutual 
information achieves better performance than the change 
of temporal mutual information. 
                                                     
1 http://en.wikipedia.org/wiki/May_2006 
Table 1 and Table 2 list the top 20 collocations based 
on these two approaches, respectively. The results of the 
first approach show that some collocations are related to 
the feelings such as ?fell left? and time such as ?Saturday 
night?. In contrast, the results of the second approach 
show more interesting collocations related to the news 
events at that time, such as terrorists ?zacarias 
moussaoui? and ?paramod mahajan.? These two persons 
were killed in May 3. Besides, ?Geena Davis? got the 
golden award in May 3. That explains why the 
collocations detected by relative change of temporal 
mutual information are better than those detected by 
change of temporal mutual information. 
-20
-15
-10
-5
0
5
10
1 3 5 7 9 11 13 15 17 19
Time (day)
M
ut
ua
l i
nf
or
m
at
io
n
Da-Vinci Tom Hanks
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 6: Performance of Event Detection Phase 
-15
-10
-5
0
5
10
1 3 5 7 9 11 13 15 17 19
Time (day)
M
ut
ua
l i
nf
or
m
at
io
n
Da-Vinci Tom Hanks
Collocations CMI Collocations CMI 
May 03 9276.08 Current music 1842.67
Illegal immigrants 5833.17 Hate studying 1722.32
Feel left 5411.57 Stephen Colbert 1709.59
Saturday night 4155.29 Thursday night 1678.78
Past weekend 2405.32 Can?t believe 1533.33
White house 2208.89 Feel asleep 1428.18
Red sox 2208.43 Ice cream 1373.23
Album tool 2120.30 Oh god 1369.52
Sunday morning 2006.78 Illegalimmigration 1368.12
16.56
f 
CMI
32.50
31.63
29.09
28.45
28.34
28.13Sunday night 1992.37 Pretty cool 13
Table 1: Top 20 collocations with highest change o
temporal mutual information 
Collocations CMI Collocations 
casinos online 618.36 Diet sodas 
zacarias moussaoui 154.68 Ving rhames 
Tsunami warning 107.93 Stock picks 
Conspirator zacarias 71.62 Happy hump 
Artist formerly 57.04 Wong kan 
Federal  
Jury 
41.78 Sixapartcom 
movabletype Wed 3 39.20 Aaron echolls 27.48
Pramod mahajan 35.41 Phnom penh 25.78
BBC  
Version 
35.21 Livejournal 
sixapartcom 
23.83  Fi
hi
c
dGeena davis 33.64 George yeo 20.34
Table 2: Top 20 collocations with highest relative change 
of mutual information 
4.3. Evaluation of Event Summarization 
As discussed in Section 3, we have two methods to 
include collocations to the event description. Method 1 
employs the highest mutual information, and Method 2 
utilizes the highest product of mutual information and 
change of temporal mutual information. Figure 7 shows 
the performance of Method 1 and Method 2. We can see 
that the performance of Method 2 is better than that of 
Method 1 in most cases. 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 7: Overall Performance of Event Summarization 
The results of event summarization by Method 2 are 
shown in Figure 8. Typhoon Chanchu appeared in the 
Pacific Ocean on May 10, 2006, passed through 
Philippine and China and resulted in disasters in these 
areas on May 13 and 18, 2006.  The appearance of the 
typhoon Chanchu cannot be found from the events listed 
in wikipedia on May 10.  However, we can identify the 
appearance of typhoon Chanchu from the description of 
the typhoon appearance such as ?typhoon named? and 
?Typhoon eye.  In addition, the typhoon Chanchu?s path 
can also be inferred from the retrieved collocations such 
as ?Philippine China? and ?near China?. The response of 
bloggers such as ?unexpected typhoon? and ?8 typhoons? 
is also extracted.   
 
 
 
 
 
 
 
 
 
 
Figure 8: Event Summarization for Typhoon Chanchu 
5. Concluding Remarks 
This paper introduces temporal mutual information to 
capture term-term association over time in weblogs. The 
extracted collocation with unusual peak which is in terms 
of relative change of temporal mutual information is 
selected to represent an event.  We collect those 
collocations with the highest product of mutual 
information and change of temporal mutual information 
to summarize the specific event.  The experiments on 
ICWSM weblog data set and evaluation with wikipedia 
event lists at the same period as weblogs demonstrate the 
feasibility of the proposed temporal collocation model 
and event detection algorithms. 
Currently, we do not consider user groups and 
locations. This methodology will be extended to model 
the collocations over time and location, and the 
relationship between the user-preferred usage of 
collocations and the profile of users. 
Acknowledgments 
Research of this paper was partially supported by 
National Science Council, Taiwan (NSC96-2628-E-002-
240-MY3) and Excellent Research Projects of National 
Taiwan University (96R0062-AE00-02). 
References 
Adamic, L.A., Glance, N. (2005). The Political 
Blogosphere and the 2004 U.S. Election: Divided 
They Blog. In: Proceedings of the 3rd International 
Workshop on Link Discovery, pp. 36--43. 
Burger, J.D., Henderson J.C. (2006). An Exploration of 
Observable Features Related to Blogger Age. In: 
Proceedings of AAAI 2006 Spring Symposium on 
Computational Approaches to Analysing Weblogs, pp. 
15--20. 
Doran, C., Griffith, J., Henderson, J. (2006). Highlights 
from 12 Months of Blogs. In: Proceedings of AAAI 
2006 Spring Symposium on Computational 
Approaches to Analysing Weblogs, pp. 30--33. 
Glance, N., Hurst, M., Tornkiyo, T. (2004). Blogpulse: 
Automated Trend Discovery for Weblogs. In: 
Proceedings of WWW 2004 Workshop on the 
Weblogging Ecosystem: Aggregation, Analysis, and 
Dynamics. 
Hurst, M. (2006). 24 Hours in the Blogosphere. In: 
Proceedings of AAAI 2006 Spring Symposium on 
Computational Approaches to Analysing Weblogs, pp. 
73--77. 
ICWSM (2007). http://www.icwsm.org/data.html 
Kim, J.H. (2005). Blog as an Oppositional Medium? A 
Semantic Network Analysis on the Iraq War Blogs. In: 
Internet Research 6.0: Internet Generations. 
 
Manning, C.D., Sch?tze, H. (1999). Foundations of 
Statistical Natural Language Processing, The MIT 
Press, London England. 
Mei, Q., Liu, C., Su, H., Zhai, C. (2006). A Probabilistic 
Approach to Spatiotemporal Theme Pattern Mining on 
Weblogs. In: Proceedings of the 15th International 
Conference on World Wide Web, Edinburgh, Scotland, 
pp. 533--542. 
Oka, M., Abe, H., Kato, K. (2006). Extracting Topics 
from Weblogs Through Frequency Segments. In: 
Proceedings of WWW 2006 Annual Workshop on the 
Weblogging Ecosystem: Aggregation, Analysis, and 
Dynamics. 
Teng, C.Y., Chen, H.H. (2006). Detection of Bloggers? 
Interest: Using Textual, Temporal, and Interactive 
Features. In: Proceeding of IEEE/WIC/ACM 
International Conference on Web Intelligence, pp. 
366--369. 
Teng, C.Y., Chen, H.H. (2007). Analyzing Temporal 
Collocations in Weblogs. In: Proceeding of 
International Conference on Weblogs and Social 
Media, 303--304. 
Thelwall, M. (2006). Blogs During the London Attacks: 
Top Information Sources and Topics. In: Proceedings 
of 3rd Annual Workshop on the Weblogging 
Ecosystem: Aggregation, Analysis and Dynamics. 
Thompson, G. (2003). Weblogs, Warblogs, the Public 
Sphere, and Bubbles. Transformations, 7(2). 
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 119?123, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
BUT-TYPED: Using domain knowledge for computing typed similarity
Lubomir Otrusina
Brno University of Technology
Faculty of Information Technology
IT4Innovations Centre of Excellence
Bozetechova 2, 612 66 Brno
Czech Republic
iotrusina@fit.vutbr.cz
Pavel Smrz
Brno University of Technology
Faculty of Information Technology
IT4Innovations Centre of Excellence
Bozetechova 2, 612 66 Brno
Czech Republic
smrz@fit.vutbr.cz
Abstract
This paper deals with knowledge-based text
processing which aims at an intuitive notion
of textual similarity. Entities and relations rel-
evant for a particular domain are identified and
disambiguated by means of semi-supervised
machine learning techniques and resulting an-
notations are applied for computing typed-
similarity of individual texts.
The work described in this paper particularly
shows effects of the mentioned processes in
the context of the *SEM 2013 pilot task on
typed-similarity, a part of the Semantic Tex-
tual Similarity shared task. The goal is to
evaluate the degree of semantic similarity be-
tween semi-structured records. As the evalu-
ation dataset has been taken from Europeana
? a collection of records on European cultural
heritage objects ? we focus on computing a se-
mantic distance on field author which has the
highest potential to benefit from the domain
knowledge.
Specific features that are employed in our sys-
tem BUT-TYPED are briefly introduced to-
gether with a discussion on their efficient ac-
quisition. Support Vector Regression is then
used to combine the features and to provide a
final similarity score. The system ranked third
on the attribute author among 15 submitted
runs in the typed-similarity task.
1 Introduction
The goal of the pilot typed-similarity task lied in
measuring a degree of semantic similarity between
semi-structured records. The data came from the
Europeana digital library1 collecting millions of
records on paintings, books, films, and other mu-
seum and archival objects that have been digitized
throughout Europe. More than 2,000 cultural and
scientific institutions across Europe have contributed
to Europeana. There are many metadata fields at-
tached to each item in the library, but only fields
title, subject, description, creator, date and source
were used in the task.
Having this collection, it is natural to expect that
domain knowledge on relevant cultural heritage en-
tities and their inter-relations will help to measure
semantic closeness between particular items. When
focusing on similarities in a particular field (a se-
mantic type) that clearly covers a domain-specific
aspect (such as field author/creator in our case), the
significance of the domain knowledge should be the
highest.
Intuitively, the semantic similarity among authors
of two artworks corresponds to strengths of links
that can be identified among the two (groups of)
authors. As the gold standard for the task resulted
from a Mechanical Turk experiment (Paolacci et al,
2010), it could be expected that close fields corre-
spond to authors that are well known to represent
the same style, worked in the same time or the same
art branch (e. g., Gabrie?l Metsu and Johannes Ver-
meer), come from the same region (often guessed
from the names), dealt with related topics (not nec-
essarily in the artwork described by the record in
question), etc. In addition to necessary evaluation of
the intersection and the union of two author fields
(leading naturally to the Jaccard similarity coeffi-
1http://www.europeana.eu/
119
cient on normalized name records ? see below), it
is therefore crucial to integrate means measuring the
above-mentioned semantic links between identified
authors.
Unfortunately, there is a lot of noise in the data
used in the task. Since Europeana does not precisely
define meaning and purpose of each particular field
in the database, many mistakes come directly from
the unmanaged importing process realized by par-
ticipating institutions. Fields often mix content of
various semantic nature and, occasionally, they are
completely misinterpreted (e. g., field creator stands
for the author, but, in many cases, it contains only
the institution the data comes from). Moreover, the
data in records is rather sparse ? many fields are left
empty even though the information to be filled in is
included in original museum records (e. g., the au-
thor of an artwork is known but not entered).
The low quality of underlying data can be also
responsible for results reported in related studies.
For example, Aletras et al (2012) evaluate semantic
similarity between semi-structured items from Euro-
peana. They use several measures including a sim-
ple normalized textual overlap, the extended Lesk
measure, the cosine similarity, a Wikipedia-based
model and the LDA (Latent Dirichlet Allocation).
The study, restricted to fields title, subject and de-
scription, shows that the best score is obtained by
the normalized overlap applied only to the title field.
Any other combination of the fields decreased the
performance. Similarly, sophisticated methods did
not bring any improvement.
The particular gold standard (training/test data)
used in the typed-similarity task is also problematic.
For example, it provides estimates of location-based
similarity even though it makes no sense for partic-
ular two records ? no field mentions a location and
it cannot be inferred from other parts). A through-
out analysis of the task data showed that creator is
the only field we could reasonably use in our exper-
iments (although many issues discussed in previous
paragraphs apply for the field as well). That is why
we focus on similarities between author fields in this
study.
While a plenty of measures for computing tex-
tual similarity have been proposed (Lin, 1998; Lan-
dauer et al, 1998; Sahlgren, 2005; Gabrilovich and
Markovitch, 2007) and there is an active research
in the fields of Textual Entailment (Negri et al,
2012), Paraphrase Identification (Lintean and Rus,
2010) and, recently, the Semantic Textual Similar-
ity (Agirre et al, 2012), the semi-structured record
similarity is a relatively new area of research. Even
though we focus on a particular domain-specific
field in this study, our work builds on previous re-
sults (Croce et al, 2012; Annesi et al, 2012) to
pre-compute semantic closeness of authors based on
available biographies and other related texts.
The rest of the paper is organized as follows: The
next section introduces the key domain-knowledge
processing step of our system which aims at recog-
nizing and disambiguating entities relevant for the
cultural heritage domain. The realized system and
its results are described in Section 3. Finally, Sec-
tion 4 briefly summarizes the achievements.
2 Entity Recognition and Disambiguation
A fundamental step in processing text in particu-
lar fields lies in identifying named entities relevant
for similarity measuring. There is a need for a
named entity recognition tool (NER) which identi-
fies names and classifies referred entities into pre-
defined categories. We take advantage of such a
tool developed by our team within the DECIPHER
project2.
The DECIPHER NER is able to recognize artists
relevant for the cultural heritage domain and, for
most of them, to identify the branch of the arts they
were primarily focused on (such as painter, sculp-
tors, etc.). It also recognizes names of artworks,
genres, art periods and movements and geograph-
ical features. In total, there are 1,880,985 recog-
nizable entities from the art domain and more than
3,000,000 place names. Cultural-heritage entities
come from various sources; the most productive
ones are given in Table 1. The list of place names
is populated from the Geo-Names database3.
The tool takes lists of entities and constructs a fi-
nite state automaton to scan and annotate input texts.
It is extremely fast (50,000 words per second) and
has a relatively small memory footprint (less than
90 MB for all the data).
Additional information attached to entities is
2http://decipher-research.eu/
3http://www.geonames.org/
120
Source # of entities
Freebase4 1,288,192
Getty ULAN5 528,921
VADS6 31,587
Arthermitage7 4,259
Artcyclopedia8 3,966
Table 1: Number of art-related entities from various
sources
stored in the automaton too. A normalized form of a
name and its semantic type is returned for each en-
tity. Normalized forms enable identifying equivalent
entities expressed differently in texts, e. g., Gabrie?l
Metsu refers to the same person as Gabriel Metsu,
US can stand for the United States (of America), etc.
Type-specific information is also stored. It includes
a detailed type (e. g., architect, sculptor, etc.), na-
tionality, relevant periods or movements, and years
of birth and death for authors. Types of geographical
features (city, river), coordinates and the GeoNames
database identifiers are stored for locations.
The tool is also able to disambiguate entities
based on a textual context in which they appeared.
Semantic types and simple rules preferring longer
matches provide a primary means for this. For ex-
ample, a text containing Bobigny ? Pablo Picasso,
refers probably to a station of the Paris Metro and
does not necessarily deal with the famous Spanish
artist. A higher level of disambiguation takes form
of classification engines constructed for every am-
biguous name from Wikipedia. A set of most spe-
cific terms characterizing each particular entity with
a shared name is stored together with an entity iden-
tifier and used for disambiguation during the text
processing phase. Disambiguation of geographical
names is performed in a similar manner.
3 System Description and Results
To compute semantic similarity of two non-empty
author fields, normalized textual content is com-
pared by an exact match first. As there is no unified
form defined for author names entered to the field,
the next step applies the NER tool discussed in the
previous section to the field text and tries to identify
all mentioned entities. Table 2 shows examples of
texts from author fields and their respective annota-
tions (in the typewriter font).
Dates and places of birth and death as well as few
specific keywords are put together and used in the
following processing separately. To correctly anno-
tate expressions that most probably refer to names of
people not covered by the DECIPHER NER tool, we
employ the Stanford NER9 that is trained to identify
names based on typical textual contexts.
The final similarity score for a pair of author fields
is computed by means of the SVR combining spe-
cific features characterizing various aspects of the
similarity. Simple Jaccard coefficient on recognized
person names, normalized word overlap of the re-
maining text and its edit distance (to deal with typos)
are used as basic features.
Places of births and deaths, author?s nationality
(e. g., Irish painter) and places of work (active in
Spain and France) provide data to estimate location-
based similarity of authors. Coordinates of each lo-
cation are used to compute an average location for
the author field. The distance between the average
coordinates is then applied as a feature. Since types
of locations (city, state, etc.) are also available, the
number of unique location types for each item and
the overlap between corresponding sets are also em-
ployed as features.
Explicitly mentioned dates as well as information
provided by the DECIPHER NER are compared too.
The time-similarity feature takes into account time
overlap of the dates and time distance of an earlier
and a later event.
Other features reflect an overlap between visual
art branches represented by artists in question (Pho-
tographer, Architect, etc.), an overlap between their
styles, genres and all other information available
from external sources. We also employ a matrix of
artistic influences that has been derived from a large
collection of domain texts by means of relation ex-
traction methods.
Finally, general relatedness of artists is pre-
computed from the above-mentioned collection by
means of Random Indexing (RI), Explicit Seman-
tic Analysis (ESA) and Latent Dirichlet Allocation
(LDA) methods, stored in sparse matrices and en-
tered as a final set of features to the SVR process.
The system is implemented in Python and takes
9http://nlp.stanford.edu/software/CRF-NER.shtml
121
Eginton, Francis; West, Benjamin
<author name="Francis Eginton" url="http://www.freebase.com/m/0by1w5n">
Eginton, Francis</author>; <author name="Benjamin West"
url="http://www.freebase.com/m/01z6r6">West, Benjamin</author>
Yossef Zaritsky Israeli, born Ukraine, 1891-1985
<author name="Joseph Zaritsky" url="http://www.freebase.com/m/0bh71xw"
nationality="Israel" place of birth="Ukraine" date of birth="1891"
date of death="1985">Yossef Zaritsky Israeli, born Ukraine,
1891-1985</author>
Man Ray (Emmanuel Radnitzky) 1890, Philadelphia ? 1976, Paris
<author name="Man Ray" alternate name="Emmanuel Radnitzky"
url="http://www.freebase.com/m/0gskj" date of birth="1890"
place of birth="Philadelphia" date of death="1976" place of death="Paris">
Man Ray (Emmanuel Radnitzky) 1890, Philadelphia - 1976, Paris</author>
Table 2: Examples of texts in the author field and their annotations
advantage of several existing modules such as gen-
sim10 for RI, ESA and other text-representation
methods, numpy11 for Support Vector Regression
(SVR) with RBF kernels, PyVowpal12 for an effi-
cient implementation of the LDA, and nltk13 for gen-
eral text pre-processing.
The resulting system was trained and tested on the
data provided by the task organizers. The train and
test sets consisted each of 750 pairs of cultural her-
itage records from Europeana along with the gold
standard for the training set. The BUT-TYPED sys-
tem reached score 0.7592 in the author field (cross-
validated results, Pearson correlation) on the train-
ing set where 80 % were used for training whereas
20 % for testing. The score for the field on the test-
ing set was 0.7468, while the baseline was 0.4278.
4 Conclusions
Despite issues related to the low quality of the
gold standard data, the attention paid to the sim-
ilarity computation on the chosen field showed to
bear fruit. The realized system ranked third among
14 others in the criterion we focused on. Domain
knowledge proved to significantly help in measuring
semantic closeness between authors and the results
correspond to an intuitive understanding of the sim-
10http://radimrehurek.com/gensim/
11http://www.numpy.org/
12https://github.com/shilad/PyVowpal
13http://nltk.org/
ilarity between artists.
Acknowledgments
This work was partially supported by the EC?s
Seventh Framework Programme (FP7/2007-
2013) under grant agreement No. 270001,
and by the Centrum excellence IT4Innovations
(ED1.1.00/02.0070).
References
Agirre, E., Diab, M., Cer, D., and Gonzalez-Agirre,
A. (2012). Semeval-2012 task 6: A pilot on se-
mantic textual similarity. In Proceedings of the
First Joint Conference on Lexical and Computa-
tional Semantics-Volume 1: Proceedings of the
main conference and the shared task, and Vol-
ume 2: Proceedings of the Sixth International
Workshop on Semantic Evaluation, pages 385?
393. Association for Computational Linguistics.
Aletras, N., Stevenson, M., and Clough, P. (2012).
Computing similarity between items in a digital
library of cultural heritage. Journal on Computing
and Cultural Heritage (JOCCH), 5(4):16.
Annesi, P., Storch, V., and Basili, R. (2012). Space
projections as distributional models for seman-
tic composition. In Computational Linguistics
and Intelligent Text Processing, pages 323?335.
Springer.
122
Croce, D., Annesi, P., Storch, V., and Basili, R.
(2012). Unitor: combining semantic text similar-
ity functions through sv regression. In Proceed-
ings of the First Joint Conference on Lexical and
Computational Semantics-Volume 1: Proceedings
of the main conference and the shared task, and
Volume 2: Proceedings of the Sixth International
Workshop on Semantic Evaluation, pages 597?
602. Association for Computational Linguistics.
Gabrilovich, E. and Markovitch, S. (2007). Comput-
ing semantic relatedness using wikipedia-based
explicit semantic analysis. In Proceedings of the
20th International Joint Conference on Artificial
Intelligence, pages 6?12.
Landauer, T., Foltz, P., and Laham, D. (1998). An in-
troduction to latent semantic analysis. Discourse
processes, 25(2):259?284.
Lin, D. (1998). An information-theoretic definition
of similarity. In Proceedings of the 15th Inter-
national Conference on Machine Learning, vol-
ume 1, pages 296?304. Citeseer.
Lintean, M. C. and Rus, V. (2010). Paraphrase iden-
tification using weighted dependencies and word
semantics. Informatica: An International Journal
of Computing and Informatics, 34(1):19?28.
Negri, M., Marchetti, A., Mehdad, Y., Bentivogli,
L., and Giampiccolo, D. (2012). semeval-2012
task 8: Cross-lingual textual entailment for con-
tent synchronization. In Proceedings of the First
Joint Conference on Lexical and Computational
Semantics-Volume 1: Proceedings of the main
conference and the shared task, and Volume 2:
Proceedings of the Sixth International Workshop
on Semantic Evaluation, pages 399?407. Associ-
ation for Computational Linguistics.
Paolacci, G., Chandler, J., and Ipeirotis, P. (2010).
Running experiments on amazon mechanical
turk. Judgment and Decision Making, 5(5):411?
419.
Sahlgren, M. (2005). An introduction to random in-
dexing. In Methods and Applications of Seman-
tic Indexing Workshop at the 7th International
Conference on Terminology and Knowledge En-
gineering, TKE 2005. Citeseer.
123
Zock/Rapp/Huang (eds.): Proceedings of the 4th Workshop on Cognitive Aspects of the Lexicon, pages 22?30,
Dublin, Ireland, August 23, 2014.
Deep Learning from Web-Scale Corpora for Better Dictionary Interfaces
Pavel Smrz
Brno University of Technology
Faculty of Information Technology
Bozetechova 2, 612 66 Brno
Czech Republic
smrz@fit.vutbr.cz
Lubomir Otrusina
Brno University of Technology
Faculty of Information Technology
Bozetechova 2, 612 66 Brno
Czech Republic
iotrusina@fit.vutbr.cz
Abstract
This paper explores advanced learning mechanisms ? neural networks trained by the Word2Vec
method ? for predicting word associations. We discuss how the approach can be built into dic-
tionary interfaces to help tip-of-the-tongue searches. We also describe our contribution to the
CogALex 2014 shared task. We argue that the reverse response-stimulus word associations cho-
sen for the shared task are only mildly related to the motivation idea of the lexical access support
system. The methods employed in our contribution are briefly introduced. We present results
of experiments with various parameter settings and show what improvement can be expected if
more than one answer is allowed. The paper concludes with a proposal for a new collective effort
to assemble real tip-of-the-tongue situation records for future, more-realistic evaluations.
1 Introduction
Human memory is fundamentally associative. To focus just on lexical access issues, it is often the case
that people cannot immediately recall a word expressing a specific concept but they can give one or
more words referring to concepts associated with the desired one in their minds. The failure to retrieve a
word from memory, combined with partial recall and the feeling that retrieval is imminent, is generally
referred to as the tip-of-the-tongue phenomenon (TOT), sometimes called presque vu (Brown, 1991).
Before one starts to think about automatic means supporting the lexical access, it is important to dis-
tinguish various situations in which TOT appears. First, the personal state of the language producer
(writer/speaker) plays a crucial role. Fatigue or lack of attention can increase frequency of TOT situ-
ations. Specific problems come with mild cognitive impairments (incipient dementia) which is more
frequent in elders. The communication mode (written or spoken language) also needs to be taken into
account ? it often helps to recollect an intended word if one just says associated words aloud. Conse-
quently, people can prefer expressing the hesitation over a TOT word as a question to a family member,
a friend or an automatic assistant. The spoken communication generally brings longer, more specific
and detailed clues that can potentially lead to better identification of the word to be reminded. The
language (mother tongue v. foreign language) and producer?s familiarity and proficiency also need to
be considered. Language learners would frequently associate a word with others that sound similar but
are not related semantically, they could combine clues in their native language and the target one, mis-
spell/mispronounce words, etc. Although the search across languages is not typically considered as a
kind of the TOT phenomenon, we include this situation in the considered scenario.
Research prototypes of automatic assistants have to consider the above-mentioned settings and clearly
identify in what types of TOT they can help. The primary decision a tool designer needs to make relates
to the appropriate interface. The ultimate goal of the work described in this paper consists in integrat-
ing a TOT-aware assistants into natural user interfaces. Rather than on a desktop or tablet computer
with a standard keyboard or (hand)written input, we focus on smart-phones or even wearable interfaces
(smart watches, glasses), intelligent home/office infrastructure components, or robotic companions that
can communicate in natural (spoken) language and that help users in their language producing tasks.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organizers. Licence details: http://creativecommons.org/licenses/by/4.0/
22
Although the current research deals strictly with explicitly expressed requests for a TOT-situation help,
there is a possibility of automatic detection of TOT-related hesitations and immediate generation of word
suggestions. In any case, the state of the art on this topic is at the beginning and these types of automatic
assistants are mostly research prototypes.
To be able to evaluate the ongoing development work, the first author of this paper started to docu-
ment and collect real TOT events. This includes personal experiences but also cases appearing during
his communication with colleagues, family members, etc. In a relatively short time of three months,
19 documented cases were recorded. This shows that a collective effort in this area could easily lead to
a new reasonably-large resource that would help to direct future research (see the concluding section).
As we aim at a general TOT setting, the collected data include full descriptions of the clues, not just
keyword-based TOT searches. For written-only interfaces, we provide a list of extracted keywords too.
Thus, there would the full sentence: It is like racism but on women (the correct answer ? discrimination)
and the set of two keywords ? racism, women ? for the written case.
Although its current limited size does not allow deriving statistically-significant results (only 3 out
of 19 TOT cases can be correctly retrieved by our method if we allow 4 suggestions), the resource can
be used to demonstrate crucial differences between the task of TOT- and reverse-association predictions
(see the next section).
In addition to this discussion, the paper presents methods used for the stimulus-response association
prediction submitted as our contribution to the CogALex 2014 shared task and their results. Section 3
introduces the methods, while Section 4 summarizes results under varying parameters. We conclude with
future directions of our research and a proposal for a joint TOT-related activity.
2 Related Work
There is a long-term interest in intelligent dictionary interfaces that reflect natural lexical-access needs.
Yet, advanced mechanisms of the access by meaning are rarely implemented as their integration presents
significant challenges. Zock and Bilac (2004) discuss lookup mechanisms on the basis of word asso-
ciations. Sinopalnikova and Smrz (2004) introduce lexical access-supporting dictionary enhancements
based on various language resources ? corpora, Wordnets, explanatory dictionaries and word association
norms.
Free-word associations are frequently used as testing data for word relatedness experiments. Church
and Hanks (1990) estimate word associations by a corpus-based association ratio. Word association the-
sauri or norms, representing a collection of empirical data obtained through large-scale psycholinguistic
free-association tests, often define a gold standard. In particular, Zesch and Gurevych (2007) employ the
University of South Florida word association, rhyme, and word fragment norms (Nelson et al., 1998) to
compare characteristics of its graph representation to that of Wikipedia. Rapp (2008) experiments with
associative responses to multiword stimuli on the Edinburgh Associative Thesaurus.
The CogALex 2014 shared task is very close to the experimental setting discussed in (Rapp, 2013)
which also aims at computing a stimulus word leading to responses given in EAT. A fixed-window
size to count word co-occurrences is used first. Log-likelihood ratios are employed to rank candidate
words and products of the ranks then define the winner. Providing 7 responses (as compared to 5 in
the CogALex 2014 shared task), the stimulus word is predicted with 54 % accuracy. However, only a
specific subset (Kent and Rosanoff, 1910) of EAT is used which comprises 100 words. It is also not fully
clear from which set of potential words target answers are chosen. This is a crucial aspect that influences
accuracy. For example, Rapp (2014) took into account only primary associative responses from EAT,
i. e,. only 2,792 words. Obviously, it is far simpler to choose the correct answer from a limited set than
from all existing words.
Other word association resources are also frequently used as test data. In addition to the Wordnet itself,
they include TOEFL (Landauer and Dumais, 1997) and ESL synonym questions (Turney, 2001), RG-
65 (Rubenstein and Goodenough, 1965) and WordSimilarity-353 (Finkelstein et al., 2001) test collections
for degree of synonymy or SAT analogy questions (Turney et al., 2003) for the relational similarity.
23
Additionaly, Heath et al. (2013) evaluate their association model in word guessing games (games with a
purpose).
3 Free word associations v. TOT ? similarities and differences
The CogALex 2014 shared task was motivated by natural lexical access but it was defined as computing
reversed free-word associations. Participating automatic systems were employed to determine the most
probable stimulus leading to given five most frequent responses from a free association test. For example,
given words circus, funny, nose, fool, and fun, participating systems were supposed to compute word
clown as the answer.
Training and test datasets came from the Edinburgh Associative Thesaurus (EAT)
1
(Kiss et al., 1972).
EAT comprises about 100 associative responses given by British students for each of 8,400 stimuli. Items
containing multi-word units and non-alphabetical characters were filtered out from the CogALex 2014
experimental data.
Although it has been shown that free word association norms and thesauri provide a valuable source
of information for TOT-assisting (Sinopalnikova and Smrz, 2006), the two corresponding phenomena
are not identical. Indeed, available data and experience clearly point out similarities but also significant
differences.
Both, individual free associations as well as TOT can be full of idiosyncrasies. However, while as-
sociation norms and thesauri try to present prototypical, generalized, most frequent associations, TOT
assistants need to cope with personal specificity. Ideally, a system should be able to help its user remind
a word given the clue it was mentioned by Mary during our yesterday?s conversation.
Both the phenomena are also strongly culturally-dependent. Among others, this can make some re-
sources such as large-scale corpora for particular language variants unusable. For example, let us con-
sider the very first item from the CogALex test set ? word capable is to be guessed as the stimulus for
responses able, incapable, brown, clever, good. Putting aside the first two response words sharing their
roots with the stimulus for a while (see the related discussion below), we come to word brown. This refers
to Lancelot Brown, more commonly known as Capability Brown ? an 18th century English landscape
architect. This association is specific for the U.K. and it is hardly known to Americans. For example,
the two words never collocate in the 450 million Corpus of Contemporary American English (COCA)
2
,
while Capability Brown is mentioned 36 times in the 100 million British National Corpus (BNC)
3
.
This observation led us to the question what is the overlap between two distinct word association the-
sauri/norms. To explore this, we compared EAT to the University of South Florida Word Association
Norm (SFWAN)
4
(Nelson et al., 1998). SFWAN consists of 5,019 normed words and their 72,176 re-
sponses. EAT and SFWAN have 3,545 stimulus terms in common. There are 11,788 words used as one
or more responses in both the sets. Despite the substantial overlap of the stimulus and response sets,
responses for same stimulus words in SFWAN rarely correspond to those given in EAT. Using a simple
algorithm of the highest overlap among response sets, only 106 stimuli from the CogALex test set (out
of 2,000) can be correctly determined from SFWAN. It can be partially explained by the cultural differ-
ences between the U.K. and the U.S.A., but also by relatively distant times of collecting/publishing the
resources (1972 v. 1998), slightly different settings of the experiments and non-uniform presentation of
the results. In any case, this finding casts doubts upon suitability of EAT for the shared task if no avail-
able (large) corpus data reflects the time and the setting of corresponding word association experiments
(reflecting the background of students in 1972).
It can be also argued that observed associations corresponding to TOT clue words are of different
nature than (reversed) free-word associations. Definitely, numbers of given clues vary, sometimes, there
are two or three words only, sometimes, there are full sentences giving more than 5 keywords to associate
1
http://www.eat.rl.ac.uk/
2
http://corpus.byu.edu/coca/
3
http://www.natcorp.ox.ac.uk/
4
http://web.usf.edu/FreeAssociation/
24
with. Spoken clues also frequently explicitly state the kind of relation of the search word to a clue (e.g.,
it is an opposite to. . . , it is used for. . . ).
Subjects are usually instructed to give the first response in their mind to the stimulus in free word
association tests. On the other hand, TOT clues are usually related to the searched word in much more
subtle way. At least, it is usually enough to mention any word of the same root/stem as a candidate
and the subject finds the word in TOT situations. Thus, testing free associations such as choler-cholera,
capable-incapable, misuse-abuse, actor-actress is completely irrelevant for vocabulary access problems.
Native speakers have usually no problem to retrieve a word from memory if it forms a part of an idiom
and the other part of the idiom is suggested. Thus, predicting either word of tooth a nail is not relevant for
TOT situations (in any language that lexicalizes Latin dentibus et vnguibus). Considering lexical access
in a foreign language, the reason for the same conclusion can be opposite ? an idiom can be unknown to
a learner so that it is not probable that a part will be given as a clue.
In languages naturally conceptualizing different parts of speech, writers or speakers always know
what word category they search for. The collected data as well as intuition also suggest that TOT clues
would not mix various senses of a word to be recalled. Consequently, free associations such as stage ?
theatre/coach or March ? April/Hare have also nothing to do with TOT.
4 Methods
This section introduces methods used to compute multi-word reversed associations in our experiments.
The primary method applied in the submitted system takes advantage of deep learning from web-scale
corpora. To be sure that computed word associations automatically derived from large textual data can-
not be matched by those resulting from a manually created resource, associations predicted by various
Wordnet-based measures were also considered.
The Word2Vec technique
5
available in Python package GenSim
6
(
?
Reh?u?rek and Sojka, 2010) was pri-
marily utilized to predict a stimulus word from a list of most frequent responses. Word2Vec defines an ef-
ficient way to work with continuous bag-of-word (CBOW) and skip-gram (SG) architectures computing
vector representations from very large data sets (Mikolov et al., 2013). The CBOW and SG approaches
are both based on distributed representations of words learned by neural networks. The CBOW architec-
ture predicts a current word based on contexts, while the SG algorithm predicts surrounding words given
a current word. Mikolov et al. (2013) showed that the SG algorithm achieves better accuracies in tested
cases. We have therefore applied only this architecture in our experiments. Various parameters of the
training model need to be set ? the dimensionality of feature vectors, the maximum distance between a
current and a predicted word within a sentence or the initial learning rate. Consequently, we built various
instances of the stimulus predictor varying values of the parameters. Their detailed evaluation is given in
the next section.
The CogALex 2014 shared task was divided into two categories. Unrestricted systems could use any
kind of data to compute results, while restricted systems were allowed only to draw on the freely available
UKWaC corpus (Ferraresi et al., 2008) in order to predict word associations. We implemented systems
for both the categories. Our unrestricted system employs the ClueWeb12 corpus.
7
UKWaC comprises
about 2 billion words and has size of about 30 GB (including annotations). The ClueWeb12 dataset
consists of more than 733 million English web pages (collected between February and May 2012). The
size of the complete ClueWeb12 data is 1.95 TB. To speed-up the process of training, only a fraction of
the ClueWeb12 dataset was used to compute the Word2Vec models. It consists of about 8.7 billion words
and has size of 131 GB. The ClueWeb12 data was pre-processed by removing web-page boilerplates
and content duplication. The original UKWaC dataset already contains POS and lemma annotations.
TreeTagger
8
was used to produce the same input for the ClueWeb12 dataset. Some models were created
from identified lemmata rather than individual tokens.
5
https://code.google.com/p/Word2Vec/
6
http://radimrehurek.com/gensim/
7
http://lemurproject.org/clueweb12/
8
http://www.cis.uni-muenchen.de/
?
schmid/tools/TreeTagger/
25
We took advantage of the nltk
9
toolkit to experiment with Wordnet-based measures. A candidate list of
all possible Wordnet-related words that could be considered as potential stimuli was computed for each
of five given responses first. The word with the highest sum of similarities to all five response words was
returned as the best stimulus candidate.
To populate the set of all possibly related words, standard Wordnet relations (Fellbaum, 1998)
were considered ? hypernyms/hyponyms, instances, holonyms/meronyms (including members and sub-
stances), attributes, entailments, causes, verb groups, see-also and similar-to relations. As the similarity
measures, we used the path similarity based on the shortest path that connects the senses in the is-a
(hypernym/hyponym) taxonomy, Wu-Palmer?s similarity (Wu and Palmer, 1994) based on the depth of
word/sense pairs in the taxonomy and that of their Least Common Subsumer, Leacock-Chodorow?s sim-
ilarity (Leacock and Chodorow, 1998) based on the shortest path that connects the senses (as above)
and the maximum depth of the taxonomy in which the senses occur, Resnik?s similarity (Resnik, 1995)
based on the Information Content (IC) of the least common subsumer, Jiang-Conrath? similarity (Jiang
and Conrath, 1997) based on the Information Content (IC) of the least common subsumer and that of
the two input synsets and Lin?s Similarity (Lin, 1998) based on the Information Content (IC) of the least
common subsumer and that of the two input synsets.
5 Evaluation
5.1 Word2Vec approach
There are various parameters to tune up when creating the models for the SG algorithm. We experi-
mented with three of them ? values 100, 300 and 500 were tested as dimensionalities of feature vectors,
values 3, 5 and 7 for maximum distances between current and predicted words within a sentence, and
the lemmatization was switched on or off. Value word means that original lowercased tokens were used
for the computation of models, whereas value lemma means we used lowercased lemmata correspond-
ing to original words. Resulting models are named accordingly: size-window-token (e.g., 100-3-lemma,
500-3-word), where size denotes the dimensionality of the feature vectors, window the maximum dis-
tance between the current and a predicted word within a sentence and token determines whether original
words or lemmata were used for a given model. We restricted the parameters to these values mainly to
cope with computational requirements. Although the Word2Vec toolkit supports multi-threaded compu-
tation, it took significant time to build all the models. For example, 60 hours in 8 threads were needed to
compute the 500-7-lemma model for the ClueWeb12 data. Although higher values for size and window
parameters would probably bring better accuracies, they were not tested due to time constraints. Results
for various combinations of parameters are summarized in Table 1.
EAT sometimes gives inflectional variants of words (e.g., plurals) as stimuli or responses. A strict
evaluation comparing exact strings can then harm systems that do not try to match particular wordforms.
To quantify the effect we compared results of our system on two versions of the test sets expanded target
word lists which allow all wordforms for each target word
10
and the original lists. Results are given
in Table 1 in columns denoted inflectional in the case of the expanded lists and non-inflectional for the
original data.
As can be seen, model 500-5-lemma reaches the best accuracy for the unrestricted task and models
300-7-word and 500-5-word win in the restricted task. As only one set of results was allowed to be
submitted for each task, we employed the 500-5-word model in our submission.
Although, the CogALex 2014 shared task was defined as to predict exactly one stimulus word for five
given responses, lexical access helpers can easily accommodate more suggestions. This can be evaluated
by checking how frequently a gold standard stimulus appears among top n predicted words. Figures 1
and 2 compare results of our unrestricted and restricted systems, respectively, for up to 10 suggestions
(the inflectional case). As expected, the accuracy increases with the number of candidate words taken
into account. The best value of 0.4865 for the unrestricted system is reached using model 500-5-lemma,
while the best accuracy of 0.4575 for the restricted system comes from model 500-7-word.
9
http://www.nltk.org/
10
Provided by Michael Flor and Beata Beigman Klebanov (ETS Princeton).
26
model
non-inflectional inflectional
unrestricted restricted unrestricted restricted
100-3-lemma 0.11 0.083 0.1215 0.0865
100-3-word 0.1005 0.098 0.11 0.1015
100-5-lemma 0.1055 0.1 0.1165 0.1045
100-5-word 0.1115 0.1165 0.1195 0.1225
100-7-lemma 0.1235 0.1035 0.1345 0.108
100-7-word 0.112 0.1265 0.1235 0.137
300-3-lemma 0.178 0.1395 0.1945 0.1475
300-3-word 0.1605 0.157 0.1705 0.163
300-5-lemma 0.179 0.1525 0.196 0.161
300-5-word 0.175 0.183 0.19 0.193
300-7-lemma 0.1875 0.158 0.206 0.167
300-7-word 0.17 0.195 0.1885 0.207
500-3-lemma 0.188 0.1395 0.203 0.1465
500-3-word 0.174 0.176 0.1845 0.1845
500-5-lemma 0.1975 0.161 0.219 0.1685
500-5-word 0.1795 0.195 0.1955 0.2065
500-7-lemma 0.193 0.169 0.2075 0.1795
500-7-word 0.191 0.194 0.209 0.2085
Table 1: Accuracies of Word2Vec-based methods with varying parameters
Together with their original Word2Vec implementation, Mikolov et al. (2013) made available also
word vectors resulting from training on a part of the Google News dataset (consisting of 100 billion
words). The model contains 300-dimensional vectors for 3 millions of words and phrases (no lemmati-
zation was performed). We repeated CogALex 2014 shared task experiments with this pre-trained model
as well. The resulting accuracy was 0.1375. The lower value is probably caused by the fact that the
model is trained on the specific dataset with different pre-processing.
5.2 Wordnet-based measures
The Wordnet-based approach was evaluated in the same way as the Word2Vec one. Result for all six
similarity measures are listed in Table 2. As in the previous case, accuracies for top n (1 ? n ? 10)
predicted responses are considered. The best performing Wordnet similarity measure for the task showed
to be Lin?s similarity based on the Information Content of the least common subsumer and that of the two
input synsets. Yet, the best values are far from accuracies of the Word2Vec-based methods, especially
when only few predicted responses are allowed. This confirms our hypothesis that approaches deriving
their lexical knowledge from large textual corpora overcome those based only on Wordnet.
6 Conclusions and future directions
The CogALex 2014 shared task focused on computing reversed multi-word response-stimulus relations
extracted from the Edinburgh Association Thesaurus. We showed that this setting is only weakly related
to computer-aided lexical access problems, namely to the tip-of-the-tongue phenomenon.
The submitted results were obtained with a system based on the Word2Vec distributional similarity
model. Best of the implemented systems reaches accuracy of 0.1975 when trained on a subset of the
ClueWeb12 dataset. Unfortunately, in time of writing this paper, official results of other teams are not
published. Hence, no comparison with other participants could be included.
Section 2 also mentions our experience in collecting real TOT data. We believe that a collective effort
could lead to a much larger resource better reflecting nature of the TOT phenomenon. We propose to
establish a task force aiming at this goal. During discussions at the workshop, we could focus on actual
27
Figure 1: Accuracies growing with the number of suggestions for the unrestricted system
Figure 2: Accuracies growing with the number of suggestions for the restricted system
28
sim.
top n predicted responses are considered
1 2 3 4 5 6 7 8 9 10
path 0.0085 0.0175 0.024 0.0325 0.0395 0.044 0.0525 0.057 0.063 0.066
wup 0.012 0.03 0.0455 0.057 0.068 0.0745 0.0875 0.095 0.1025 0.1075
lch 0.003 0.0085 0.015 0.0195 0.0215 0.028 0.034 0.0375 0.04 0.042
res 0.008 0.0205 0.031 0.0435 0.048 0.061 0.0715 0.0785 0.0845 0.09
jcn 0.0135 0.029 0.042 0.0575 0.065 0.0825 0.098 0.1105 0.12 0.1295
lin 0.022 0.044 0.068 0.091 0.1045 0.1265 0.1485 0.1675 0.1805 0.1955
Table 2: Results of Wordnet-based methods (path stands for the path similarity, wup for Wu-Palmer?s
similarity, lch for Leacock-Chodorow?s similarity, res for the Resnik?s similarity, jcn for the Jiang-
Conrath?s similarity and lin for Lin?s similarity).
procedures and technical support means (through a web-based system) to build the resource within the
next year. The collected dataset could then be used for future shared tasks in the domain.
Acknowledgements
The research leading to these results has received support from the IT4Innovations Centre of Excellence
project, Reg. No. CZ.1.05/1.1.00/02.0070, supported by Operational Programme ?Research and Devel-
opment for Innovations? funded by Structural Funds of the European Union and the state budget of the
Czech Republic.
References
Alan S. Brown. 1991. A review of the tip-of-the-tongue experience. Psychological Bulletin, 109(2):204?223,
March.
Kenneth Ward Church and Patrick Hanks. 1990. Word association norms, mutual information, and lexicography.
Computational Linguistics, 16(1):22?29, March.
Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database. The MIT Press, Cambridge, MA; London,
May. ISBN 978-0-262-06197-1.
Adriano Ferraresi, Eros Zanchetta, Marco Baroni, and Silvia Bernardini. 2008. Introducing and evaluating ukwac,
a very large web-derived corpus of english. In Proceedings of the 4th Web as Corpus Workshop (WAC-4) Can
we beat Google, pages 47?54.
Lev Finkelstein, Gabrilovich Evgenly, Matias Yossi, Rivlin Ehud, Solan Zach, Wolfman Gadi, and Ruppin Eytan.
2001. Placing search in context: the concept revisited. In Proceedings of the Tenth International World Wide
Web Conference.
Derrall Heath, David Norton, Eric K. Ringger, and Dan Ventura. 2013. Semantic models as a combination of free
association norms and corpus-based correlations. In ICSC, pages 48?55. IEEE.
J.J. Jiang and D.W. Conrath. 1997. Semantic similarity based on corpus statistics and lexical taxonomy. Arxiv
preprint cmp-lg/9709008.
Grace Helen Kent and Aaron Joshua Rosanoff. 1910. A study of association in insanity. American Journal of
Insanity.
GR Kiss, Christine A Armstrong, and R Milroy. 1972. An associative thesaurus of English. Medical Research
Council, Speech and Communication Unit, University of Edinburgh, Scotland.
T. K. Landauer and S. T. Dumais. 1997. A solution to Plato?s problem: The latent semantic analysis theory of
acquisition, induction, and representation of knowledge. Psychological review, 104(2):211?240.
C. Leacock and M. Chodorow. 1998. Combining local context and WordNet similarity for word sense identifica-
tion. WordNet: An electronic lexical database, 49(2):265?283.
29
D. Lin. 1998. An information-theoretic definition of similarity. In Proceedings of the 15th International Confer-
ence on Machine Learning, volume 1, pages 296?304. Citeseer.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of
words and phrases and their compositionality. In Advances in Neural Information Processing Systems, pages
3111?3119.
D. L. Nelson, McEvoy, C. L., and T. A. Schreiber. 1998. The University of South Florida word association, rhyme,
and word fragment norms. http://w3.usf.edu/FreeAssociation/.
Reinhard Rapp. 2008. The computation of associative responses to multiword stimuli. In Proceedings of the
Workshop on Cognitive Aspects of the Lexicon, COGALEX ?08, pages 102?109. Association for Computational
Linguistics.
Reinhard Rapp. 2013. From stimulus to associations and back. Natural Language Processing and Cognitive
Science, page 78.
Reinhard Rapp. 2014. Using word association norms to measure corpus representativeness. In Computational
Linguistics and Intelligent Text Processing, pages 1?13. Springer.
Radim
?
Reh?u?rek and Petr Sojka. 2010. Software Framework for Topic Modelling with Large Corpora. In Proceed-
ings of the LREC 2010 Workshop on New Challenges for NLP Frameworks, pages 45?50, Valletta, Malta, May.
ELRA. http://is.muni.cz/publication/884893/en.
P. Resnik. 1995. Using information content to evaluate semantic similarity in a taxonomy. Arxiv preprint cmp-
lg/9511007.
Herbert Rubenstein and John B. Goodenough. 1965. Contextual correlates of synonymy. Communications of the
ACM, 8(10):627?633.
Anna Sinopalnikova and Pavel Smrz. 2004. Word association norms as a unique supplement of traditional lan-
guage resources. In Proceedings of the 4th International Conference on Language Resources and Evaluation
(LREC), pages 1557?1561, Lisbon. European Language Resources Association.
Anna Sinopalnikova and Pavel Smrz. 2006. Knowing a word vs. accessing a word: Wordnet and word association
norms as interfaces to electronic dictionaries. In Proceedings of the Third International WordNet Conference,
pages 265?272.
Peter D. Turney, Michael L. Littman, Jeffrey Bigham, and Victor Shnayder. 2003. Combining independent mod-
ules to solve multiple-choice synonym and analogy problems. CoRR, cs.CL/0309035.
Peter D. Turney. 2001. Mining the web for synonyms: Pmi-ir versus lsa on toefl. In Proceedings of the 12th
European Conference on Machine Learning, EMCL ?01, pages 491?502. Springer-Verlag.
Z. Wu and M. Palmer. 1994. Verbs semantics and lexical selection. In Proceedings of the 32nd annual meeting
on Association for Computational Linguistics, pages 133?138. Association for Computational Linguistics.
Torsten Zesch and Iryna Gurevych. 2007. Analysis of the wikipedia category graph for nlp applications. In
Proceedings of the TextGraphs-2 Workshop (NAACL-HLT).
Michael Zock and Slaven Bilac. 2004. Word lookup on the basis of associations: From an idea to a roadmap. In
Proceedings of the Workshop on Enhancing and Using Electronic Dictionaries, ElectricDict ?04, pages 29?35.
Association for Computational Linguistics.
30
