Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 233?237,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Modeling of term-distance and term-occurrence information for im-
proving n-gram language model performance 
 
Tze Yuang Chong1,2, Rafael E. Banchs3, Eng Siong Chng1,2, Haizhou Li1,2,3 
1Temasek Laboratory, Nanyang Technological University, Singapore 639798 
2School of Computer Engineering, Nanyang Technological University, Singapore 639798 
3Institute for Infocomm Research, Singapore 138632 
tychong@ntu.edu.sg, rembanchs@i2r.a-star.edu.sg, 
aseschng@ntu.edu.sg, hli@i2r.a-star.edu.sg 
  
 
Abstract 
In this paper, we explore the use of distance 
and co-occurrence information of word-pairs 
for language modeling. We attempt to extract 
this information from history-contexts of up to 
ten words in size, and found it complements 
well the n-gram model, which inherently suf-
fers from data scarcity in learning long histo-
ry-contexts. Evaluated on the WSJ corpus, bi-
gram and trigram model perplexity were re-
duced up to 23.5% and 14.0%, respectively. 
Compared to the distant bigram, we show that 
word-pairs can be more effectively modeled in 
terms of both distance and occurrence. 
1 Introduction 
Language models have been extensively studied 
in natural language processing. The role of a lan-
guage model is to measure how probably a (tar-
get) word would occur based on some given evi-
dence extracted from the history-context. The 
commonly used n-gram model (Bahl et al 1983) 
takes the immediately preceding history-word 
sequence, of length   1 , as the evidence for 
prediction. Although n-gram models are simple 
and effective, modeling long history-contexts 
lead to severe data scarcity problems. Hence, the 
context length is commonly limited to as short as 
three, i.e. the trigram model, and any useful in-
formation beyond this window is neglected. 
In this work, we explore the possibility of 
modeling the presence of a history-word in terms 
of: (1) the distance and (2) the co-occurrence, 
with a target-word. These two attributes will be 
exploited and modeled independently from each 
other, i.e. the distance is described regardless the 
actual frequency of the history-word, while the 
co-occurrence is described regardless the actual 
position of the history-word. We refer to these 
two attributes as the term-distance (TD) and the 
term-occurrence (TO) components, respectively. 
The rest of this paper is structured as follows. 
The following section presents the most relevant 
related works. Section 3 introduces and moti-
vates our proposed approach. Section 4 presents 
in detail the derivation of both TD and TO model 
components. Section 5 presents some perplexity 
evaluation results. Finally, section 6 presents our 
conclusions and proposed future work. 
2 Related Work 
The distant bigram model (Huang et.al 1993, 
Simon et al 1997, Brun et al 2007) disassembles 
the n-gram into (n?1) word-pairs, such that each 
pair is modeled by a distance-k bigram model, 
where 1      1 . Each distance-k bigram 
model predicts the target-word based on the oc-
currence of a history-word located k positions 
behind.  
Zhou & Lua (1998) enhanced the effective-
ness of the model by filtering out those word-
pairs exhibiting low correlation, so that only the 
well associated distant bigrams are retained. This 
approach is referred to as the distance-dependent 
trigger model, and is similar to the earlier pro-
posed trigger model (Lau et al 1993, Rosenfeld 
1996) that relies on the bigrams of arbitrary dis-
tance, i.e. distance-independent. 
Latent-semantic language model approaches 
(Bellegarda 1998, Coccaro 2005) weight word 
counts with TFIDF to highlight their semantic 
importance towards the prediction. In this type of 
approach, count statistics are accumulated from 
long contexts, typically beyond ten to twenty 
words. In order to confine the complexity intro-
duced by such long contexts, word ordering is 
ignored (i.e. bag-of-words paradigm). 
Other approaches such as the class-based lan-
guage model (Brown 1992, Kneser & Ney 1993) 
233
use POS or POS-like classes of the history-words 
for prediction. The structured language model 
(Chelba & Jelinek 2000) determines the ?heads? 
in the history-context by using a parsing tree. 
There are also works on skipping irrelevant his-
tory-words in order to reveal more informative n-
grams (Siu & Ostendorf 2000, Guthrie et al 
2006). Cache language models exploit temporal 
word frequencies in the history (Kuhn & Mori 
1990, Clarkson & Robinson 1997). 
3 Motivation of the Proposed Approach 
The attributes of distance and co-occurrence are 
exploited and modeled differently in each lan-
guage modeling approach. In the n-gram model, 
for example, these two attributes are jointly taken 
into account in the ordered word-sequence. Con-
sequently, the n-gram model can only be effec-
tively implemented within a short history-context 
(e.g. of size of three or four). 
Both, the conventional trigger model and the 
latent-semantic model capture the co-occurrence 
information while ignoring the distance informa-
tion. It is reasonable to assume that distance in-
formation at far contexts is less likely to be in-
formative and, hence, can be discarded. Howev-
er, intermediate distances beyond the n-gram 
model limits can be very useful and should not 
be discarded. 
On the other hand, distant-bigram models and 
distance-dependent trigger models make use of 
both, distance and co-occurrence, information up 
to window sizes of ten to twenty. They achieve 
this by compromising inter-dependencies among 
history-words (i.e. the context is represented as 
separated word-pairs). However, similarly to n-
gram models, distance and co-occurrence infor-
mation are implicitly tied within the word-pairs. 
In our proposed approach, we attempt to ex-
ploit the TD and TO attributes, separately, to in-
corporate distant context information into the n-
gram, as a remedy to the data scarcity problem 
when learning the far context. 
4 Language Modeling with TD and TO 
A language model estimates word probabilities 
given their history, i.e.  	 
| 	 
 , 
where  denotes the target word and  denotes its 
corresponding history. Let the word located at ith 
position, 
 , be the target-word and its preceding 
word-sequence 
 	 
?

  of 
length   1, be its history-context. Also, in or-
der to alleviate the data scarcity problem, we as-
sume the occurrences of the history-words to be 
independent from each other, conditioned to the 
occurrence of the target-word 
 , i.e.  
 
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 30?37,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
An Empirical Evaluation of Stop Word Removal                                  
in Statistical Machine Translation 
Chong Tze Yuang 
School of Computer Engi-
neering, Nanyang Technolo-
gical University, 639798 
Singapore 
tychong@ntu.edu.sg 
Rafael E. Banchs 
Institute for Infocomm Re-
search, A*STAR, 138632, 
Singapore 
rembanchs@i2r.a-
star.edu.sg 
Chng Eng Siong 
School of Computer Engi-
neering Nanyang Technolo-
gical University, 639798 
Singapore. 
aseschng@ntu.edu.sg 
 
 
 
 
 
Abstract 
In this paper we evaluate the possibility of 
improving the performance of a statistical 
machine translation system by relaxing the 
complexity of the translation task by remov-
ing the most frequent and predictable terms 
from the target language vocabulary. After-
wards, the removed terms are inserted back 
in the relaxed output by using an n-gram 
based word predictor. Empirically, we have 
found that when these words are omitted 
from the text, the perplexity of the text de-
creases, which may imply the reduction of 
confusion in the text. We conducted some 
machine translation experiments to see if 
this perplexity reduction produced a better 
translation output. While the word predic-
tion results exhibits 77% accuracy in pre-
dicting 40% of the most frequent words in 
the text, the perplexity reduction did not 
help to produce better translations. 
1 Introduction 
It is a characteristic of natural language that a 
large proportion of running words in a corpus 
corresponds to a very small fraction of the voca-
bulary. An analysis of the Brown Corpus has 
shown that the hundred most frequent words 
account for 42% of the corpus, while only 0.1% 
in the vocabulary. On the other hand, words oc-
curring only once account merely 5.7% in the 
corpus but 58% in the vocabulary (Bell et al 
1990). This phenomenon can be explained in 
terms of Zipf?s Law, which states that the prod-
uct of word ranks and their frequencies approx-
imates a constant, i.e. word-frequency plot is 
close to a hyperbolic function, and hence the few 
top ranked words would account for a great por-
tion of the corpus. Also, it appears that the top 
ranked words are mainly function words. For 
instance, the eight most frequent words in the 
Brown Corpus are the, of, and, to, a, in, that and 
is (Bell et al 1990). 
It is a common practice in Information Re-
trieval (IR) to filter the most frequent words out 
from processed documents (which are referred to 
as stop words), as these function words are se-
mantically non-informative and constitute weak 
indexing terms. By removing this great amount 
of stop words, not only space and time complexi-
ties can be reduced, but document content can be 
better discriminated by the remaining content 
words (Fox, 1989; Rijsbergen, 1979; Zou et al, 
2006; Dolamic & Savoy 2009). 
Inspired by the concept of stop word removal 
in Information Retrieval, in this work we study 
the feasibility of stop word removal in Statistical 
Machine Translation (SMT). Different from In-
formation Retrieval, that ranks or classifies doc-
uments; SMT hypothesizes sentences in target 
language. Therefore, without explicitly removing 
frequent words from the documents, we proposed 
to ignore such words in the target language vo-
cabulary, i.e. by replacing those words with a 
null token. We term this process as ?relaxation? 
and the omitted words as ?relaxed words?.  
Relaxed SMT here refers to a translation task 
in which target vocabulary words are intentional-
ly omitted from the training dataset for reducing 
translation complexity. Since the most frequent 
words are targeted to be relaxed, as a result, there 
will be vast amount of null tokens in the output 
text, which later shall be recovered in a post 
processing stage. The idea of relaxation in SMT 
is motivated by one of our experimental findings, 
in which the perplexity measured over a test set 
decreases when most frequent words are relaxed. 
For instance, a 15% of perplexity reduction is 
observed when the twenty most frequent words 
are relaxed in the English EPPS dataset. The 
reduction of perplexity allows us to conjecture 
30
about the decrease of confusion in the text, from 
which a SMT system might be benefited. 
After applying relaxed SMT, the resulting 
null tokens in the translated sentences have to be 
replaced by the corresponding words from the set 
of relaxed words. As relaxed words are chosen 
from the top ranked words, which possess high 
occurrences in the corpus, their n-gram probabili-
ties could be reliably trained to serve for word 
prediction. Also, these words are mainly function 
words and, from the human perspective, function 
words are usually much easier to predict from 
their neighbor context than content words. Con-
sider for instance the sentence the house of the 
president is very nice. Function words like the, 
of, and is, are certainly easier to be predicted than 
content words such as house, president, and nice. 
The rest of the paper is organized into four 
sections. In section 2, we discuss the relaxation 
strategy implemented for a SMT system, which 
generates translation outputs that contain null to-
kens. In section 3, we present the word predic-
tion mechanism used to recover the null tokens 
occurring in the relaxed translation outputs. In 
section 4, we present and discuss the experimen-
tal results. Finally, in section 5 we present the 
most relevant conclusion of this work. 
2 Relaxation for Machine Translation 
In this paper, relaxation refers to the replacement 
of the most frequent words in text by a null to-
ken. In the practice, a set of frequent words is 
defined and the cardinality of such set is referred 
to as the relaxation order. For example, lets the 
relaxation order be two and the two words on the 
top rank are the and is. By relaxing the sample 
sentence previously presented in the introduc-
tion, the following relaxed sentence will be ob-
tained: NULL house of NULL President NULL 
very beautiful. 
From some of our preliminary experimental 
results with the EPPS dataset, we did observe 
that a relaxation order of twenty leaded to a per-
plexity reduction of about a 15%. To see whether 
this contributes to improving the translation per-
formance, we trained a translation system by 
relaxing the top ranked words in the vocabulary 
of the target language. In this way, there will be a 
large number of words in the source language 
that will be translated to a null token. For exam-
ple: la (the in Spanish) and es (is in Spanish) will 
be both translated to a null token in English. 
This relaxation of terms is only applied to the 
target language vocabulary, and it is conducted 
after the word alignment process but before the 
extraction of translation units and the computa-
tion of model probabilities. The main objective 
of this relaxation procedure is twofold: on the 
one hand, it attempts to reduce the complexity of 
the translation task by reducing the size of the 
target vocabulary while affecting a large propor-
tion of the running words in the text; on the other 
hand, it should also help to reduce model sparse-
ness and improve model probability estimates. 
Of course, different from the Information Re-
trieval case, in which stop words are not used at 
all along the search process, in the considered 
machine translation scenario, the removed words 
need to be recovered after decoding in order to 
produce an acceptable translation. The relaxed 
word replacement procedure, which is based on 
an n-gram based predictor, is implemented as a 
post-processing step and applied to the relaxed 
machine translation output in order to produce 
the final translation result.  
Our bet here is that the gain in the translation 
step, which is derived from the relaxation strate-
gy, should be enough to compensate the error 
rate of the word prediction step, producing, in 
overall, a significant improvement in translation 
quality with respect to the non-relaxed baseline 
procedure. 
The next section describes the implemented 
word prediction model in detail. It constitutes a 
fundamental element of our proposed relaxed 
SMT approach. 
3 Frequent Word Prediction 
Word prediction has been widely studied and 
used in several different tasks such as, for exam-
ple, augmented and alternative communication 
(Wandmacher and Antoine, 2007) and spelling 
correction (Thiele et al, 2000). In addition to the 
commonly used word n-gram, various language 
modeling techniques have been applied, such as 
the semantic model (Lu?s and Rosa, 2002; Wand-
macher and Antoine, 2007) and the class-based 
model (Thiele et al, 2000; Zohar and Roth, 
2000; Ruch et al, 2001). 
The role of such a word predictor in our con-
sidered problem is to recover the null tokens in 
the translation output by replacing them with the 
words that best fit the sentence. This task is es-
sentially a classification problem, in which the 
most suitable relaxed word for recovering a giv-
en null token must be selected. In other words, 
  max	
 sentence??, where 
sentence?  is the probabilistic model, e.g. n-
31
gram, that estimates the likelihood of a sentence 
when a null token is recovered with word  , 
drawn from the set of relaxed words . The car-
dinality || is referred to as the relaxation order, 
e.g. ||  5 implies that the five most frequent 
words have been relaxed and are candidates to be 
recovered. 
Notice that the word prediction problem in 
this task is quite different from other works in the 
literature. This is basically because the relaxed 
words to be predicted in this case are mainly 
function words. Firstly, it may not be effective to 
predict a function word semantically. For exam-
ple, we are more certain in predicting equity than 
for given the occurrence of share in the sentence. 
Secondly, although class-based modeling is com-
monly used for prediction, its original intention 
is to tackle the sparseness problem, whereas our 
task focuses only on the most frequent words. 
In this preliminary work, our predicting me-
chanism is based on an n-gram model. It predicts 
the word that yields the maximum a posteriori 
probability, conditioned on its predecessors. For 
the case of the trigram model, it can be expressed 
as follows: 
 
  max	
 |  (1) 
 
Often, there are cases in which more than one 
null token occur consecutively. In such cases 
predicting a null token is conditioned on the pre-
vious recovered null tokens. To prevent a predic-
tion error from being propagated, one possibility 
is to consider the marginal probability (summed 
over the relaxed word set) over the words that 
were previously null tokens. For example, if  
is a relaxed word, which has been recovered 
from earlier predictions, then the prediction of  
should no longer be conditioned by  . This 
can be computed as follows: 
 
  max	
 |