Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 91?101,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Measuring Ideological Proportions in Political Speeches
Yanchuan Sim? Brice D. L. Acree?
?Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{ysim,nasmith}@cs.cmu.edu
Justin H. Gross? Noah A. Smith?
?Department of Political Science
University of North Carolina at Chapel Hill
Chapel Hill, NC 27599, USA
{brice.acree,jhgross}@unc.edu
Abstract
We seek to measure political candidates? ideo-
logical positioning from their speeches. To ac-
complish this, we infer ideological cues from
a corpus of political writings annotated with
known ideologies. We then represent the
speeches of U.S. Presidential candidates as se-
quences of cues and lags (filler distinguished
only by its length in words). We apply a
domain-informed Bayesian HMM to infer the
proportions of ideologies each candidate uses
in each campaign. The results are validated
against a set of preregistered, domain expert-
authored hypotheses.
1 Introduction
The artful use of language is central to politics, and
the language of politicians has attracted consider-
able interest among scholars of political commu-
nication and rhetoric (Charteris-Black, 2005; Hart,
2009; Deirmeier et al, 2012; Hart et al, 2013)
and computational linguistics (Thomas et al, 2006;
Fader et al, 2007; Gerrish and Blei, 2011, in-
ter alia). In American politics, candidates for of-
fice give speeches and write books and manifestos
expounding their ideas. Every political season,
however, there are accusations of candidates ?flip-
flopping? on issues, with opinion shows, late-night
comedies, and talk radio hosts replaying clips of
candidates contradicting earlier statements. Pres-
idential candidate Mitt Romney?s own aide infa-
mously proclaimed in 2012: ?I think you hit a reset
button for the fall campaign [i.e., the general elec-
tion]. Everything changes. It?s almost like an Etch-
a-Sketch. You can kind of shake it up and we start
all over again.?
A more general observation, often stated but not
yet, to our knowledge, tested empirically, is that
successful primary candidates ?move to the cen-
ter? before a general election. The expectation fol-
lows directly from long-standing and widely influen-
tial theories of political competition that are collec-
tively referred to in their simplest form as the ?me-
dian voter theorem? (Hotelling, 1929; Black, 1948;
Downs, 1957). Thus it is to be expected that when
a set of voters that are more ideologically concen-
trated are replaced by a set who are more widely
dispersed across the ideological spectrum, as occurs
in the transition between the United States primary
and general elections, that candidates will present
themselves as more moderate in an effort to capture
enough votes to win.
Do political candidates in fact stray ideologically
at opportune moments? More specifically, can we
measure candidates? ideological positions from their
prose at different times? Following much work
on classifying the political ideology expressed by a
piece of text (Laver et al, 2003; Monroe and Maeda,
2004; Hillard et al, 2008), we start from the as-
sumption that a candidate?s choice of words and
phrases reflects a deliberate attempt to signal com-
mon cause with a target audience, and as a broader
strategy, to respond to political competitors. Our
central hypothesis is that, despite candidates? in-
tentional vagueness, differences in position?among
candidates or over time?can be automatically de-
tected and described as proportions of ideologies ex-
pressed in a speech.
In this work, we operationalize ideologies in a
novel empirical way, exploiting political writings
published in explicitly ideological books and mag-
azines (?2).1 The corpus then serves as evidence for
1We consider general positions in terms of broad ideolog-
ical groups that are widely discussed in current political dis-
course (e.g., ?Far Right,? ?Religious Right,? ?Libertarian,??
91
BACKGROUND
LEFT RIGHT
CENTER
PROGRESSIVE
RELIGIOUS LEFT
FAR LEFT
RELIGIOUS RIGHT
CENTER-LEFT
FAR RIGHT
CENTER-RIGHT
LIBERTARIAN
POPULIST
Figure 1: Ideology tree showing the labels for the ide-
ological corpus in ?2.1 (excluding BACKGROUND) and
corresponding to states in the HMM (?3.3).
a probabilistic model that allows us to automatically
infer compact, human-interpretable lexicons of cues
strongly associated with each ideology.
These lexicons are used, in turn, to create a low-
dimensional representation of political speeches: a
speech is a sequence of cues interspersed with lags.
Lags correspond to the lengths of sequences of non-
cue words, which are treated as irrelevant to the in-
ference problem at hand. In other words, a speech is
represented as a series alternating between cues sig-
naling ideological positions and uninteresting filler.
Our main contribution is a probabilistic technique
for inferring proportions of ideologies expressed by
a candidate (?3). The inputs to the model are the
cue-lag representation of a speech and a domain-
specific topology relating ideologies to each other.
The topology tree (shown in Figure 1) encoding
the closeness of different ideologies and, by exten-
sion, the odds of transitioning between them within a
speech. Bayesian inference is used to manage uncer-
tainty about the associations between cues and ide-
ologies, probabilities of traversing each of the tree?s
edges, and other parameters.
We demonstrate the usefulness of the measure-
ment model by showing that it accurately recov-
ers pre-registered beliefs regarding narratives widely
accepted?but not yet tested empirically?about the
2008 and 2012 U.S. Presidential elections (?4).
2 First Stage: Cue Extraction
We first present a data-driven technique for automat-
ically constructing ?cue lexicons? from texts labeled
with ideologies by domain experts.
etc.). Analysis of positions on specific issues is left for future
work.
Total tokens 32,835,190
Total types 138,235
Avg. tokens per book 77,628
Avg. tokens per mag. issue 31,713
Breakdown by ideology: Documents Tokens
LEFT 0 0
FAR LEFT 112 3,334,601
CENTER-LEFT 196 7,396,264
PROGRESSIVE LEFT 138 7,257,723
RELIGIOUS LEFT 7 487,844
CENTER 5 429,480
RIGHT 97 3,282,744
FAR RIGHT 211 7,392,163
LIBERTARIAN RIGHT 88 1,703,343
CENTER-RIGHT 9 702,444
POPULIST RIGHT 5 407,054
RELIGIOUS RIGHT 6 441,530
Table 1: Ideology corpus statistics. Note that some docu-
ments are not labeled with finer-grained ideologies.
2.1 Ideological Corpus
We start with a collection of contemporary political
writings whose authors are perceived as represen-
tative of one particular ideology. Our corpus con-
sists of two types of documents: books and maga-
zines. Books are usually written by a single author,
while each magazine consists of regularly published
issues with collections of articles written by several
authors. A political science domain expert who is
a co-author of this work manually labeled each ele-
ment in a collection of 112 books and 10 magazine
titles2 with one of three coarse ideologies: LEFT,
RIGHT, or CENTER. Documents that were labeled
LEFT and RIGHT were further broken down into
more fine-grained ideologies, shown in Fig. 1.3 Ta-
ble 1 summarizes key details about the ideological
corpus.
In addition to ideology labels, individual chapters
within the books were manually tagged with topics
that the chapter was about. For instance, in Barack
Obama?s book The Audacity of Hope, his chapter
2There are 765 magazine issues, which are published bi-
weekly to quarterly, depending on the magazine. All of a mag-
azine?s issues are labeled with the same ideology.
3We cannot claim that these texts are ?pure? examples of
the ideologies they are labeled with (i.e., they may contain parts
that do not match the label). By finding relatively few terms
strongly associated with texts sharing a label, our model should
be somewhat robust to impurities, focusing on those terms that
are indicative of whatever drew the expert to identify them as
(mostly) sharing an ideology.
92
titled ?Faith? is labeled as RELIGIOUS. Not all
chapters have clearly defined topics, and as such,
these chapters are simply labeled MISC. Maga-
zines are not labeled with topics because each issue
of a magazine generally touches on multiple top-
ics. There are a total of 61 topics; the full list can
be found in the supplementary materials, along with
a table summarizing key details about the corpus,
which contains 32.8 million tokens.
2.2 Cue Discovery Model
We use the ideological corpus to infer ideological
cues: terms that are strongly associated with an ide-
ology. Because our ideologies are organized hierar-
chically, we required a technique that can account
for multiple effects within a single text. We further
require that the sets of cue terms be small, so that
they can be inspected by domain experts. We there-
fore turn to the sparse additive generative (SAGE)
models introduced by Eisenstein et al (2011).
Like other probabilistic language models, SAGE
assigns probability to a text as if it were a bag of
terms. It differs from most language models in pa-
rameterizing the distribution using a generalized lin-
ear model, so that different effects on the log-odds
of terms are additive. In our case, we define the
probability of a term w conditioned on attributes of
the text in which it occurs. These attributes include
both the ideology and its coarsened version (e.g., a
FAR RIGHT book also has the attribute RIGHT).
For simplicity, let A(d) denote the set of attributes
of document d and A =
?
dA(d). The parametric
form of the distribution is given, for term w in doc-
ument d, by:
p(w | A(d);?) =
exp
(
?0w +
?
a?A(d) ?
a
w
)
Z(A(d),?)
Each of the ? weights can be a positive or negative
value influencing the probability of the word, condi-
tioned on various properties of the document. When
we stack an attribute a?s weights into a vector across
all words, we get an ?a vector, understood as an ef-
fect on the term distribution. (We use ? to refer to
the collection of all of these vectors.) The effects in
our model, described in terms of attributes, are:
? ?0, the background (log) frequencies of words,
fixed to the empirical frequencies in the corpus.
Hence the other effects can be understood as de-
viations from this background distribution.
? ?ic , the coarse ideology effect, which takes differ-
ent values for LEFT, RIGHT, and CENTER.
? ?if , the fine ideology effect, which takes different
values for the fine-grained ideologies correspond-
ing to the leaves in Fig. 1.
? ?t, the topic effect, taking different values for
each of the 61 manually assigned topics. We fur-
ther include one effect for each magazine series
(of which there are 10) to account for each maga-
zine?s idiosyncrasies (topical or otherwise).
? ?d, a document-specific effect, which captures id-
iosyncratic usage within a single document.
Note that the effects above are not mutually exclu-
sive, although some effects never appear together
due to constraints imposed by their semantics (e.g.,
no book is labeled both LEFT and RIGHT).
When estimating the parameters of the model (the
? vectors), we impose a sparsity-inducing `1 prior
that forces many weights to zero. The objective is:
max
?
?
d
?
w?d
log p(w | A(d);?)?
?
a?A
?a??a?1
This objective function is convex but requires spe-
cial treatment due to non-differentiability when any
elements are zero; we use the OWL-QN algorithm
to solve it (Andrew and Gao, 2007). To reduce the
complexity of the hyperparameter space (the possi-
ble values of all ?a) and to encourage similar levels
of sparsity across the different effect vectors, we let,
for each ideology attribute a,
?a = ? ? |V(a)| /maxa??A |V(a?)|
where V(a) is the set of term types appearing in
the data with attribute a (i.e., its vocabulary) , and
? is a hyperparameter we can adjust to control the
amount of sparsity in the SAGE vectors. For the
non-ideology effects, we fix ?a = 10 (not tuned).
2.3 Bigram and Trigram Lexicons
After estimating parameters, we are left with sparse
?a for each attribute. We are only interested, how-
ever, in the ideological attributes I ? A. For an
ideological attribute i ? I, we take the terms with
positive elements of this vector to be the cues for
ideology i; call this set L(i) and let L =
?
i?IL(i).
93
Because political texts use a fair amount of multi-
word jargon, we initially represented each document
as a bag of unigrams, bigrams, and trigrams, ignor-
ing the fact that these ?overlap? with each other.4
While this would be inappropriate in language mod-
eling and is inconsistent with our model?s indepen-
dence assumptions among words, it is sensible since
our goal is to identify cues that are statistically asso-
ciated with attributes like ideologies.
Preliminary trials revealed that unigrams tend to
dominate in such a model, since their frequency
counts are so much higher. Further, domain ex-
perts found them harder to interpret out of context
compared to bigrams and trigrams. We therefore in-
cluded only bigrams and trigrams as terms in our cue
discovery model.
2.4 Validation
The term selection method we have described can
be understood as a form of feature selection that
reasons globally about the data and tries to con-
trol for some effects that are not of interest (topic
or document idiosyncrasies). We compared the
approach to two classic, simple methods for fea-
ture selection: ranking based on pointwise mu-
tual information (PMI) and weighted average PMI
(WAPMI) (Schneider, 2005; Cover and Thomas,
2012). Selected features were used to classify the
ideologies of held-out documents from our cor-
pus.5 We evaluated these feature selection methods
within na??ve Bayes classification in a 5-fold cross-
validation setup. We vary ? for the SAGE model
and compare the results to equal-sized sets of terms
selected by PMI and WAPMI. We consider SAGE
with and without topic effects.
Figure 2 visualizes accuracy against the num-
ber of features for each method. Bigrams and
trigrams consistently outperform unigrams (McNe-
mar?s, p < 0.05). Otherwise, there are no sig-
nificant differences in performance except WAPMI
4Generative models that produce the same evidence more
than once are sometimes called ?deficient,? but model defi-
ciency does not necessarily imply that the model is ineffective.
Some of the IBM models for statistical machine translation pro-
vide a classic example (Brown et al, 1993).
5The text was tokenized and stopwords removed. Punctu-
ation, numbers, and web addresses were normalized. Tokens
appearing less than 20 times in training data, or in fewer than 5
documents were removed.
26 27 28 29 210 211 212 213 214 ?0.5
0.55
0.6
0.65
0.7
PMIWAPMI SAGESAGE w/ topics
Figure 2: Plot of average classification accuracy for
5-fold cross validation against the number of features.
Dashed lines refer to using only unigram features, while
solid lines refer to using bigram and trigram features.
with bigrams/trigrams at its highest point. SAGE
with topics is slightly (but not significantly) bet-
ter than without. We conclude that SAGE is a
competitive choice for cue discovery, noting that a
principled way of controlling for topical and doc-
ument effects?offered by SAGE but not the other
methods?may be even more relevant to our task
than classification accuracy.
2.5 Cue Lexicon
We ran SAGE on the the full ideological book cor-
pus, including topic effects, and setting ? = 30, ob-
tained a set of |L| = 8, 483 cue terms. The supple-
mentary materials include top cue terms associated
with various ideologies and a heatmap of similarities
among SAGE vectors.
We conducted a small, relatively informal study
in which seven subjects (including four scholars of
American politics) were asked to match brief de-
scriptions of the classes, including prominent proto-
typical individuals exemplifying each, to cue terms.
About 70% of ideologies were correctly matched
by experts, with relatively few confusions between
LEFT and RIGHT. More details are given in sup-
plementary materials.
3 Second Stage: Cue-Lag Ideological
Proportions
The main contribution of this paper is a technique
for measuring ideology proportions in the prose of
political candidates. We adopt a Bayesian approach
that manages our uncertainty about the cue lexi-
94
con L, the tendencies of political speakers to ?flip-
flop? among ideological types, and the relative ?dis-
tances? among different ideologies. The representa-
tion of a candidate?s ideology as a mixture among
discrete, hierarchically related categories can be dis-
tinguished from continuous representations (?scal-
ing? or ?spatial? models) often used in political sci-
ence, especially to infer positions from Congres-
sional roll-call voting patterns (Poole and Rosen-
thal, 1985; Poole and Rosenthal, 2000; Clinton
et al, 2004). Moreover, the ability to draw in-
ferences about individual policy-makers? ideologies
from their votes on proposed legislation is severely
limited by institutional constraints on the types of
legislation that is actually subject to recorded votes.
3.1 Political Speeches Corpus
We gathered transcribed speeches given by candi-
dates of the two main parties (Democrats and Re-
publicans) during the 2008 and 2012 Presidential
election seasons. Each election season is comprised
of two stages: (i) the primary elections, where can-
didates seek the support of their respective parties to
be nominated as the party?s Presidential candidate,
and (ii) the general elections where the parties? cho-
sen candidates travel across the states to garner sup-
port from all citizens. Each candidate?s speeches are
partitioned into epochs for each election; e.g., those
that occur before the candidate has secured enough
pledged delegates to win the party nomination are
?from the primary.? Table 2 presents a breakdown
of the candidates and speeches in our corpus.
3.2 Cue-Lag Representation
Our measurement model only considers ideological
cues; other terms are treated as filler. We therefore
transform each speech into a cue-lag representation.
The representation is a sequence of alternating
cues (elements from the ideological lexicon L) and
integer ?lags? (counts of non-cue terms falling be-
tween two cues). This will allow us to capture the in-
tuition that a candidate may use longer lags between
evocations of different ideologies, while nearby cues
are likely to be from similar ideologies.
To map a speech into the cue-lag representation,
we simply match all elements of L in the speech and
replace sequences of other words by their lengths.
When a trigram cue strictly includes a bigram cue,
Party Pri?08 Gen?08 Pri?12 Gen?12
Democrats? 167 - - -
Republicans? 50 - 49 -
Obama (D) 78 81 - 99
McCain (R) 9 159 - -
Romney (R) 8 ?(13) 19 19
?Democrats in our corpus are: Joe Biden, Hillary Clinton, John
Edwards, and Bill Richardson in 2008 and Barack Obama in
both 2008 and 2012.
?Republicans in our corpus are: Rudy Giuliani, Mike Huck-
abee, John McCain, and Fred Thompson in 2008, Michelle
Bachmann, Herman Cain, Newt Gingrich, Jon Huntsman, Rick
Perry, and Rick Santorum in 2012, and Ron Paul and Mitt Rom-
ney in both 2008 and 2012.
?For Romney, we have 13 speeches which he gave in the period
2008-2011 (between his withdrawal from the 2008 elections
and before the commencement of the 2012 elections). While
these speeches are not technically part of the regular Presiden-
tial election campaign, they can be seen as his preparation to-
wards the 2012 elections, which is particularly interesting as
Romney has been accused of having inconsistent viewpoints.
Table 2: Breakdown of number of speeches in our polit-
ical speech corpus by epoch. On average, 2,998 tokens,
and 95 cue terms are found in each speech document.
we take only the trigram. When two cues partially
overlap, we treat them as consecutive cue terms and
set the lag to 0. Figure 3 shows an example of our
cue-lag representation.
3.3 CLIP: An Ideology HMM
The model we use to infer ideologies, cue-lag ide-
ological proportions (CLIP), is a hidden Markov
model. Each state corresponds to an ideology
(Fig. 1) or BACKGROUND. The emission from a state
consists of (i) a cue from L and (ii) a lag value. The
high-level generative story for a single speech with
T cue-lag pairs is as follows:
1. Parameters are drawn from conjugate priors
(details in ?3.3.3).
2. Let the initial state be the BACKGROUND
state.
3. For t ? {1, 2, . . . , T}:6
(a) Transition to state St based on the
transition distribution, discussed in ?3.3.1.
This transition is conditioned on the previ-
ous state St?1 and the lag at timestep t?1,
denoted by Lt?1.
6The length of the sequence is assumed to be exogenous, so
that no stop state needs to be defined.
95
Original sentence Just compare this President?s record with Ronald Reagan?s first term. President Reagan also faced
an economic crisis. In fact, in 1982, the unemployment rate peaked at nearly 11 percent. But in the
two years that followed, he delivered a true recovery economic growth and job creation were three
times higher than in the Obama Economy.
Cue-lag representation . . .
6
?? ronald reagan
2
?? presid reagan
3
?? econom crisi
5
?? unemploy rate
17
?? econom growth
1
??
job creation
9
?? . . .
Figure 3: Example of the cue-lag representation.
(b) Emit cue term Wt from the lexicon L
and lag Lt based on the emission distribu-
tion, discussed in ?3.3.2.
We turn next to the transitions and emissions.
3.3.1 Ideology Topology and Transition
Parameterization
CLIP assumes that each cue term uttered by a
politician is generated from a hidden state corre-
sponding to an ideology. The ideologies are orga-
nized into a tree based on their hierarchical relation-
ships; see Fig. 1. In this study, the tree is fixed ac-
cording to our domain knowledge of current Ameri-
can politics; in future work it might be enriched with
greater detail or its structure learned automatically.
The ideology tree is used in defining the transition
distribution in the HMM, but not to directly define
the topology of the HMM. Importantly, each state
may transition to any other state, but the transition
distribution is defined using the graph, so that ide-
ologies that are closer to each other will tend to be
more likely to transition to each other. To transition
between two states si and sj , a walk must be taken
in the tree from vertex si to vertex sj . We emphasize
that the walk corresponds to a single transition?
the speaker does not emit anything from the states
passed through along the path.
A simplified version of our transition distribution,
for exposition, is given as follows:
ptree(sj | si; ?,?)
=
(?
?u,v??Path(si,sj)
(1? ?u)?u,v
)
?sj
Path(si, sj) refers to the sequence of edges in the
tree along the unique path from si to sj . Each of
these edges ?u, v? must be traversed, and the prob-
ability of doing so, conditioned on having already
reached u, is (1??u)?i.e., not stopping in u?times
?u,v?i.e., selecting vertex v from among those that
share an edge with u. Eventually, sj is reached, and
the walk ends, incurring probability ?sj .
In order to capture the intuition that a longer lag
after a cue term should increase the entropy over the
next ideology state, we introduce a restart probabil-
ity, which is conditioned on the length of the most
recent lag, `. The probability of restarting the walk
from the BACKGROUND state is a noisy-OR model
with parameter ?. This gives the transition distribu-
tion:
p(sj | si, `; ?,?, ?) = (1? ?)`+1ptree(sj | si; ?,?)
+ (1? (1? ?)`+1)ptree(sj | sBACKGROUND ; ?,?)
Note that, if ? = 1, there is no Markovian depen-
dency between states (i.e., there is always a restart),
so CLIP reverts to a mixture model.
This approach allows us to parameterize the full
set of |I|2 transitions with O(|I|) parameters.7 Since
the graph is a tree and the walks are not allowed
to backtrack, the only ambiguity in the transition
is due to the restart probability; this distinguishes
CLIP from other algorithms based on random walks
(Brin and Page, 1998; Mihalcea, 2005; Toutanova et
al., 2004; Collins-Thompson and Callan, 2005).
3.3.2 Emission Parameterization
Recall that, at time step t, CLIP emits a cue from
the lexicon L and an integer-valued lag. For each
state s, we let the probability of emitting cue w
be denoted by ?s,w; ?s is a multinomial distribu-
tion over the entire lexicon L. This allows our ap-
proach to handle ambiguous cues that can associate
with more than one ideology, and also to associate a
cue with a different ideology than our cue discovery
method proposed, if the signal from the data is suffi-
ciently strong. We assume each lag to be generated
by a Poisson distribution with global parameter ?.
7More precisely, there are |I| edges (since there are |I| + 1
vertices including BACKGROUND), each with a ?-parameter in
each direction. For a vertex with degree d, however, there are
only d?1 degrees of freedom, so that there are 2|I|?(|I|+1) =
|I|?1 degrees of freedom for ?. There are |I| ?-parameters and
a single ?, for a total of 2|I| degrees of freedom.
96
3.3.3 Inference and Learning
Above we described CLIP?s transitions and emis-
sions. Because our interest is in measuring
proportions?and, as we will see, in comparing
those proportions across speakers and campaign
periods?we require a way to allow variation in pa-
rameters across different conditions. Specifically,
we seek to measure differences in time spent in each
ideology state. This can be captured by allowing
each speaker to have a different ? and ? in each stage
of the campaign. On the other hand, we expect that a
speaker draws from his ideological lexicon similarly
across different epochs?there is a single ? shared
between different epochs.
In order to manage uncertainty about the param-
eters of CLIP, to incorporate prior beliefs based on
our ideology-specific cue lexicons {L(i)}i, and to
allow sharing of statistical strength across condi-
tions, we adopt a Bayesian approach to inference.
This will allow principled exploration of the poste-
rior distribution over the proportions of interest.
We place a symmetric Dirichlet prior on the tree
walk probabilities ?; its parameter is ?. For the
cue emission distribution associated with ideology
i, ?si , we use an informed Dirichlet prior with two
different values, ?cue for cues in L(i), and a smaller
?def for those in L \ L(i).8
Learning proceeds by collapsed Gibbs sampling
for the hidden states and slice sampling (with vague
priors) for the hyperparameters (?, ?, ?, and ?). De-
tails of the sampler are given in the supplementary
materials. At each Gibbs step, we resample the ide-
ology state and restart indicator variable for every
cue term in every speech.
We ran our Gibbs sampler for 75,000 iterations,
discarding the first 25,000 iterations for burn-in, and
collected samples at every 10 iterations. Further, we
perform the slice sampling step at every 5,000 itera-
tions. For each candidate, we collected 5,000 poste-
rior samples which we use to infer his/her ideologi-
cal proportions.
In order to determine the amount of time a candi-
date spends in each ideology, we denote the unit of
time in terms of half the lag before and after each cue
8This implies that a term can, in the posterior distribution,
be associated with an ideology i of whose L(i) it was not a
member. In fact, this occurred frequently in our runs of the
model.
term, i.e., when a candidate draws a cue term from
ideology i during timestep t, we say that he spends
1
2(Lt?1 + Lt) amount of time in ideology i. Aver-
aging over all the samples returned by our sampler
and normalizing it by the length of the documents in
each epoch, we obtain a candidate?s expected ideo-
logical proportions within the epoch.
4 Pre-registered Hypotheses
The traditional way to evaluate a text analysis model
in NLP is, of course, to evaluate its output against
gold-standard judgements by humans. In the case
of recent political speeches, however, we are doubt-
ful that such judgments can be made objectively at
a fine-grained level. While we are confident about
gross categorization of books and magazines in our
ideological corpus (?2.1), many of which are overtly
marked by their ideological assocations, we believe
that human estimates of ideological proportions, or
even association of particular tokens with ideologies
they may evoke, may be overly clouded by the vari-
ation in annotator ideology and domain expertise.
We therefore adopt a different method for evalua-
tion. Before running our model, we identified a set
of hypotheses, which we pre-registered as expec-
tations. These are categorized into groups based on
their strength and relevance to judging the validity of
the model. Strong hypotheses are those that consti-
tute the lowest bar for face validity; if violated, they
suggest a flaw in the model. Moderate hypotheses
are those that match the intuition of domain experts
conducting the research, or extant theory. Violations
suggest more examination is required, and may raise
the possibility that further testing might be pursued
to demonstrate the hypothesis is false. Our 13 prin-
cipal hypotheses are enumerated in Table 3.
5 Evaluation
We compare the posterior proportions inferred by
CLIP with several baselines:
? HMM: rather than ?3.3.1, a fully connected, tra-
ditional transition matrix is used.
? MIX: a mixture model; at each timestep, we al-
ways restart (? = 1). This eliminates Marko-
vian dependencies between ideologies at nearby
timesteps, but still uses the ideology tree in defin-
ing the probabilities of each state through ?.
97
Hypotheses CLIP HMM MIX NORES
Sanity checks (strong):
S1. Republican primary candidates should tend to draw more from RIGHT than
from LEFT.
*12/12 10/13 13/13 12/13
S2. Democratic primary candidates should tend to draw more from LEFT than
from RIGHT.
4/5 5/5 5/5 5/5
S3. In general elections, Democrats should draw more from the LEFT than the
Republicans and vice versa for the RIGHT.
4/4 4/4 3/4 0/4
S total 20/21 19/22 21/22 17/22
Primary hypotheses (strong):
P1. Romney, McCain and other Republicans should almost never draw from FAR
LEFT, and extremely rarely from PROGRESSIVE.
29/32 *21/31 27/32 29/32
P2. Romney should draw more heavily from the RIGHT than Obama in both stages
of the 2012 campaign.
2/2 2/2 1/2 1/2
Primary hypotheses (moderate):
P3. Romney should draw more heavily on words from the LIBERTARIAN,
POPULIST, RELIGIOUS RIGHT, and FAR RIGHT in the primary com-
pared to the general election. In the general election, Romney should draw
more heavily on CENTER, CENTER-RIGHT and LEFT vocabularies.
2/2 2/2 0/2 2/2
P4. Obama should draw more heavily on words from the PROGRESSIVE in the
2008 primary than in the 2008 general election.
0/1 0/1 0/1 1/1
P5. In the 2008 general election, Obama should draw more heavily on the
CENTER, CENTER-LEFT, and RIGHT vocabularies than in the 2008 primary.
1/1 1/1 1/1 1/1
P6. In the 2012 general election, Obama should sample more from the LEFT than
from the RIGHT, and should sample more from the LEFT vocabularies than
Romney.
2/2 2/2 0/2 0/2
P7. McCain should draw more heavily from the FAR RIGHT, POPULIST, and
LIBERTARIAN in the 2008 primary than in the 2008 general election.
0/1 1/1 1/1 1/1
P8. In the general 2008, McCain should draw more heavily from the CENTER,
CENTER-RIGHT, and LEFT vocabularies than in the 2008 primary.
1/1 1/1 1/1 1/1
P9. McCain should draw more heavily from the RIGHT than Obama in both stages
of the campaign.
2/2 2/2 2/2 1/2
P10.Obama and other Democrats should very rarely draw from FAR RIGHT. 6/7 5/7 7/7 4/7
P total 45/51 37/50 40/51 41/51
Table 3: Pre-registered hypotheses used to validate the measurement model; number of statements evaluated correctly
by different models. *Some differences were not significant at p = 0.05 and are not included in the results.
? NORES, where we never restart (? = 0). This
strengthens the Markovian dependencies.
In MIX, there are no temporal effects between cue
terms, although the structure of our ideology tree
encourages the speaker to draw from coarse-grained
ideologies over fine-grained ideologies. On the other
hand, the strong Markovian dependency between
states in NORES would encourage the model to stay
local within the ideology tree. In our experiments,
we will see how that the ideology tree and the ran-
dom treatment of restarting both contribute to our
model?s inferences.
Table 3 presents a summary of which hypothe-
ses the models? inferences are in accordance with.
CLIP is not consistently outperformed by any of the
competing baselines.
Sanity checks (S1?3) CLIP correctly identifies
sixteen LEFT/RIGHT alignments of primary candi-
dates (S1, S2), but is unable to determine one can-
didate?s orientation; it finds Jon Huntsman to spend
roughly equal proportions of speech-time drawing
on LEFT and RIGHT cue terms. Interestingly,
Huntsman, who had served as U.S. Ambassador to
China under Obama, was considered the one mod-
erate in the 2012 Republican field. MIX correctly
identifies all thirteen Republicans, while NORES
places McCain from the 2008 primaries as mostly
LEFT-leaning and HMM misses three of thirteen,
including Perry and Gingrich, who might be deeply
98
disturbed to find that they are misclassified as LEFT-
leaning. As for the Democratic primary candidates
(S2), CLIP?s one questionable finding is that John
Edwards spoke slightly more from the RIGHT than
the LEFT. For the general elections (S3), CLIP and
HMM correctly identify the relative amount of time
spent in LEFT/RIGHT between Obama and his Re-
publican competitors. NORES had the most trou-
ble, missing all four. CLIP finds Obama spend-
ing slightly more time on the RIGHT than on the
LEFT in the 2008 general elections but nevertheless,
Obama is still found to spend more time engaging in
LEFT-speak than McCain.
Name interference When we looked at the cue
terms actually used in the speeches, we found one
systematic issue: the inclusion of candidates? names
as cue terms. Terms mentioning John McCain are
associated with the RIGHT, so that Obama?s men-
tions of his opponent are taken as evidence for
rightward positioning; in total, mentions of McCain
contributed 4% absolute to Obama?s RIGHT ide-
ological proportion. Similarly, barack obama and
presid obama are LEFT cues (though senat obama
is a RIGHT cue). In future work, we believe filtering
candidate names in the first stage will be beneficial.
Strong hypotheses P1 and P2 CLIP and the vari-
ants making use of the ideology tree were in agree-
ment on most of the strong primary hypotheses.
Most of these involved our expectation that the
Republican candidates would rarely draw on FAR
LEFT and PROGRESSIVE LEFT. Our qualitative
hypotheses were not specific about how to quantify
?rare? or ?almost never.? We chose to find a result
inconsistent with a P1 hypothesis any time a Repub-
lican had proportions greater than 5% for either ide-
ology. The notable deviations for CLIP were Fred
Thompson (13% from the PROGRESSIVE LEFT
during the 2008 primary) and Mitt Romney (12%
from the PROGRESSIVE LEFT between the 2008
and 2012 elections, 13% from the FAR LEFT dur-
ing the 2012 general election). This model did no
worse than other variants here and much better than
one: HMM had 10 inconsistencies out of 32 oppor-
tunities, suggesting the importance of the ideology
tree.
M
cC
ai
n
Primaries 2008 General 2008Far LeftReligious (L)Center-Left
Center-Right
Libertarian (R)
Religious (R)
Progressive (L)Left
CenterRight
Populist (R)Far Right
R
om
ne
y
Primaries 2008 2008-2011 Primaries 2012 General 2012Far LeftReligious (L)
CenterCenter-Right
Libertarian (R)Religious (R)
Progressive (L)Left
Center-Left
Right
Populist (R)Far Right
O
ba
m
a
Primaries 2008 General 2008 General 2012Far Left
Religious (L)Left
Center-LeftCenter-Right
Libertarian (R)Populist (R)Religious (R)
Progressive (L)
Center
Right
Far Right
Figure 4: Proportion of time spent in each ideology by
McCain, Romney, and Obama during the 2008 and 2012
Presidential election seasons.
?Etch-a-Sketch? hypotheses Hypotheses P3, P4,
P5, P7, and P8 are all concerned with differences
between the primary and general elections: success-
ful primary candidates are expected to ?move to the
center.? A visualization of CLIP?s proportions for
McCain, Romney, and Obama is shown in Figure 4,
with their speeches grouped together by different
epochs. The model is in agreement with most of
these hypotheses. It did not confirm P4?Obama
appears to CLIP to be more PROGRESSIVE in the
2008 general election than in the primary, though the
difference is small (3%) and may be within the mar-
gin of error. Likewise, in P7, the difference between
McCain drawing from FAR RIGHT, POPULIST
and LIBERTARIAN between the 2008 primary and
general elections is only 2% and highly uncertain,
with a 95% credible interval of 44?50% during the
primary (vs. 47?50% in the general election).
Fine-grained ideologies Fine-grained ideologies
are expected to account for smaller proportions, so
that making predictions about them is quite difficult.
This is especially true for primary elections, where a
broader palette of ideologies is expected to be drawn
from, but we have fewer speeches from each candi-
99
date. CLIP?s inconsistency with P10, for example,
comes from assigning 5.4% of Obama?s 2008 pri-
mary cues to FAR RIGHT.
CLIP?s inferences on the corpus of political
speeches can be browsed at http://www.ark.
cs.cmu.edu/CLIP. We emphasize that CLIP
and its variants are intended to quantify the ideo-
logical content candidates express in speeches, not
necessarily their beliefs (which may not be perfectly
reflected in their words), or even how they are de-
scribed by pundits and analysts (who draw on far
more information than is expressed in speeches).
CLIP?s deviations from the hypotheses are sug-
gestive of potential improvements to cue extraction
(?2), but also of incorrect hypotheses. We expect
future research to explore a richer set of linguistic
cues and attributes beyond ideology (e.g., topics and
framing on various issues). We plan to use CLIP
as a text analysis method to support substantive in-
quiry in political science, such as following trends
in expressed ideology over time.
6 Related Work
As early as the 1960s, there has been research on
modeling ideological beliefs using automated sys-
tems (Abelson and Carroll, 1965; Carbonell, 1978;
Sack, 1994). These early works model ideology at a
sophisticated level, involving the actors, actions and
goals; they require manually constructed knowledge
bases. Poole and Rosenthal (1985) used congres-
sional roll call data to demonstrate the ideological
divide in Congress, and provided a methodology for
measuring ideological positions. Gerrish and Blei
(2011; 2012) augmented the methodology with text
from congressional bills using probabilistic models
to uncover lawmakers? positions on specific polit-
ical issues, putting them on a left-right spectrum,
while Thomas et al (2006) made use of floor de-
bate speeches to predict votes. Likewise, taking ad-
vantage of the proliferation of text today, numer-
ous techniques have been developed to identify top-
ics and perspectives in the media (Gentzkow and
Shapiro, 2005; Lin et al, 2008; Fortuna et al, 2009;
Gentzkow and Shapiro, 2010); determine the polit-
ical leanings of a document or author (Laver et al,
2003; Efron, 2004; Mullen and Malouf, 2006; Fader
et al, 2007); or recognize stances in debates (So-
masundaran and Wiebe, 2009; Anand et al, 2011).
Going beyong lexical indicators, Greene and Resnik
(2009) investigated syntactic features to identify per-
spectives or implicit sentiment.
7 Conclusions
We introduced CLIP, a domain-informed, Bayesian
model of ideological proportions in political lan-
guage. We showed how ideological cues could be
discovered from a lightly labeled corpus of ideolog-
ical writings, then incorporated into CLIP. The re-
sulting inferences are largely consistent with a set
of preregistered hypotheses about candidates in the
2008 and 2012 Presidential elections.
Acknowledgments
For thoughtful feedback on this research, the authors
thank: several anonymous reviewers, Amber Boydstun,
Philip Resnik, members of the ARK group at CMU, and
participants in Princeton University?s Political Methodol-
ogy Colloquium and PolMeth XXX hosted by The Uni-
versity of Virginia. This work was supported in part by an
A?STAR fellowship to Y. Sim, NSF grants IIS-1211201
and IIS-1211277, and Google?s support of the Reading is
Believing project at CMU.
References
Robert P. Abelson and J. Douglas Carroll. 1965. Com-
puter simulation of individual belief systems. Ameri-
can Behavioral Scientist, 8(9):24?30.
Pranav Anand, Marilyn Walker, Rob Abbott, Jean E. Fox
Tree, Robeson Bowmani, and Michael Minor. 2011.
Cats rule and dogs drool!: Classifying stance in online
debate. In Proceedings of the Second Workshop on
Computational Approaches to Subjectivity and Senti-
ment Analysis.
Galen Andrew and Jianfeng Gao. 2007. Scalable train-
ing of l1-regularized log-linear models. In Proceed-
ings of ICML.
Duncan Black. 1948. On the rationale of group decision-
making. The Journal of Political Economy, 56(1):23?
34.
Sergey Brin and Lawrence Page. 1998. The anatomy of a
large-scale hypertextual web search engine. Computer
Networks and ISDN Systems, 30(1):107?117.
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: parameter esti-
mation. Computational Linguistics, 19(2):263?311.
100
Jaime G. Carbonell. 1978. Politics: Automated ideolog-
ical reasoning. Cognitive Science, 2(1):27?51.
Jonathan Charteris-Black. 2005. Politicians and
Rhetoric: The Persuasive Power of Metaphor.
Palgrave-MacMillan.
Joshua Clinton, Simon Jackman, and Douglas Rivers.
2004. The statistical analysis of roll call data. Ameri-
can Political Science Review, 98(2):355?370.
Kevyn Collins-Thompson and Jamie Callan. 2005.
Query expansion using random walk models. In Pro-
ceedings of CIKM.
Thomas M. Cover and Joy A. Thomas. 2012. Elements
of Information Theory. Wiley-Interscience.
Daniel Deirmeier, Jean-Francois Godbout, Bei Yu, and
Stefan Kaufmann. 2012. Language and ideology
in congress. British Journal of Political Science,
42(1):31?55.
Anthony Downs. 1957. An Economic Theory of Democ-
racy. Harper, New York.
Miles Efron. 2004. Cultural orientation: Classifying
subjective documents by cociation analysis. In AAAI
Fall Symposium on Style and Meaning in Language,
Art, and Music.
Jacob Eisenstein, Amr Ahmed, and Eric P Xing. 2011.
Sparse additive generative models of text. In Proceed-
ings of ICML.
Anthony Fader, Dragomir R. Radev, Michael H. Crespin,
Burt L. Monroe, Kevin M. Quinn, and Michael Co-
laresi. 2007. MavenRank: Identifying influential
members of the US senate using lexical centrality. In
Proceedings of EMNLP-CoNLL.
Blaz Fortuna, Carolina Galleguillos, and Nello Cristian-
ini. 2009. Detecting the bias in media with statistical
learning methods. In Ashok N. Srivastava and Mehran
Sahami, editors, Text Mining: Classification, Cluster-
ing, and Applications, chapter 2, pages 27?50. Chap-
man & Hall/CRC.
Matthew Gentzkow and Jesse Shapiro. 2005. Media bias
and reputation. Technical report, National Bureau of
Economic Research.
Matthew Gentzkow and Jesse M. Shapiro. 2010. What
drives media slant? evidence from u.s. daily newspa-
pers. Econometrica, 78(1):35?71.
Sean M. Gerrish and David M. Blei. 2011. Predict-
ing legislative roll calls from text. In Proceedings of
ICML.
Sean M. Gerrish and David M. Blei. 2012. How they
vote: Issue-adjusted models of legislative behavior. In
Advances in NIPS 25.
Stephan Greene and Philip Resnik. 2009. More than
words: syntactic packaging and implicit sentiment. In
Proceedings of NAACL.
Roderick P. Hart, Jay P. Childers, and Colene J. Lind.
2013. Political Tone: How Leaders Talk and Why.
University of Chicago Press.
Roderick P. Hart. 2009. Campaign talk: Why elections
are good for us. Princeton University Press.
Dustin Hillard, Stephen Purpura, and John Wilker-
son. 2008. Computer-assisted topic classification for
mixed-methods social science research. Journal of In-
formation Technology & Politics, 4(4):31?46.
Harold Hotelling. 1929. Stability in competition. The
Economic Journal, 39(153):41?57.
Michael Laver, Kenneth Benoit, and John Garry. 2003.
Extracting policy positions from political texts using
words as data. The American Political Science Review,
97(2):311?331.
Wei-Hao Lin, Eric Xing, and Alexander Hauptmann.
2008. A joint topic and perspective model for ideo-
logical discourse. In Proceedings of ECML-PKDD.
Rada Mihalcea. 2005. Unsupervised large-vocabulary
word sense disambiguation with graph-based algo-
rithms for sequence data labeling. In Proceedings of
EMNLP.
Burt L. Monroe and Ko Maeda. 2004. Talk?s cheap:
Text-based estimation of rhetorical ideal-points. Pre-
sented at the Annual Meeting of the Society for Politi-
cal Methodology.
Tony Mullen and Robert Malouf. 2006. A preliminary
investigation into sentiment analysis of informal polit-
ical discourse. In AAAI Symposium on Computational
Approaches to Analysing Weblogs.
Keith T. Poole and Howard Rosenthal. 1985. A spatial
model for legislative roll call analysis. American Jour-
nal of Political Science, 29(2):357?384.
Keith T. Poole and Howard Rosenthal. 2000. Congress:
A Political-Economic History of Roll Call Voting. Ox-
ford University Press.
Warren Sack. 1994. Actor-role analysis: ideology, point
of view, and the news. Master?s thesis, Massachusetts
Institute of Technology, Cambridge, MA.
Karl-Michael Schneider. 2005. Weighted average point-
wise mutual information for feature selection in text
categorization. In Proceedings of PKDD.
Swapna Somasundaran and Janyce Wiebe. 2009. Rec-
ognizing stances in online debates. In Proceedings of
ACL.
Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get out
the vote: determining support or opposition from con-
gressional floor-debate transcripts. In Proceedings of
EMNLP.
Kristina Toutanova, Christopher D. Manning, and An-
drew Y. Ng. 2004. Learning random walk models for
inducing word dependency distributions. In Proceed-
ings of ICML.
101
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1858?1868,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Learning Topics and Positions from Debatepedia
Swapna Gottipati? Minghui Qiu? Yanchuan Sim? Jing Jiang? Noah A. Smith?
?School of Information Systems, Singapore Management University, Singapore
?Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA 15213, USA
?{swapnag.2010,minghui.qiu.2010,jingjiang}@smu.edu.sg
?{ysim,nasmith}@cs.cmu.edu
Abstract
We explore Debatepedia, a community-
authored encyclopedia of sociopolitical de-
bates, as evidence for inferring a low-
dimensional, human-interpretable representa-
tion in the domain of issues and positions. We
introduce a generative model positing latent
topics and cross-cutting positions that gives
special treatment to person mentions and opin-
ion words. We evaluate the resulting repre-
sentation?s usefulness in attaching opinionated
documents to arguments and its consistency
with human judgments about positions.
1 Introduction
The social web has evolved into a forum for large
portions of the population to discuss and debate
complex issues of societal importance. Websites like
Debatepedia,1 an online, community-authored ency-
clopedia of debates (?2), seek to organize some of
this exchange into structured information resources
that summarize arguments and link externally to
texts (editorials, blog posts, etc.) that express and
evoke them. Empirical NLP, we propose, has a
role to play in creating a more compact and easily-
interpretable way to understand the opinion space.
In particular, we envision applications to computa-
tional journalism, where there is high demand for
transformation of and pattern discovery in unman-
ageable, unstructured, evolving data (including text)
to inform the public (Cohen et al, 2011).
In this paper, we develop a generative model for
discovering such a representation (?3), using De-
batepedia as a corpus of evidence. We draw in-
spiration from Lin et al (2008) and Ahmed and
1http://dbp.idebate.org
Xing (2010), who used generative models to infer
topics?distributions over words?and other word-
associated variables representing perspectives or
ideologies. We view topics as lexicons, and propose
that grounding a topic model with evidence beyond
bags of words can lead to more lexicon-like repre-
sentations. Specifically, our generative topic model
grounds topics using the hierarchical organization
of arguments within Debatepedia. Further, we use
named entity recognition as a preprocessing step, an
existing sentiment lexicon to construct an informed
prior, and we incorporate a latent, discrete position
variable that cuts across debates.2
We evaluate the model informally and formally
(?4). Subjectively, the model identifies reasonable
topic and perspective terms, and it associates topics
sensibly with important public figures. In quanti-
tative evaluations, we find the model?s representa-
tion superior to topics from vanilla latent Dirichlet
allocation (Blei et al, 2003) and the joint sentiment
topic model (Lin and He, 2009) in matching external
texts to debates. Further, the position variables can
be used to infer the side of an argument within a de-
bate; our model performs with an accuracy of 86%
on position prediction of the debate argument. The
cross-cutting position variable is not especially con-
sistent with human judgments, suggesting that fur-
ther knowledge sources may be required to improve
interpretability across issues.
2 Data
Debatepedia, like Wikipedia, is constructed by vol-
unteer contributors and has a system of community
2This variable might serve to cluster debate sides according
to ?abstract beliefs commonly shared by a group of people,?
sometimes called ideologies (Van Dijk, 1998). We do not claim
that our model infers ideologies (see ?4).
1858
Debate: Gun control; should laws be passed to limit gun ownership further?
Question: Self-defense ? Is self-defense a good reason for gun ownership?
Side: Yes Side: No
Argument: A citizen has a ?right? to guns as a means
to self-defense: Many groups argue that a citizen
should have the ?right? to defend themselves, and that
a gun is frequently the . . .
Argument: The protection of property is not a good
justification for yielding a lethal weapon. While peo-
ple have a right to their property, this should not justify
wielding a lethal . . .
Argument: Gun restrictions and bans disadvantage cit-
izens against armed criminals. Citizens that are not al-
lowed to carry guns are disadvantaged against lawless
criminals that . . .
Argument: Robert F. Drinan, Former Democratic US
Congressman, ?Gun Control: The Good Outweighs
the Evil?, 1976 ? ?These graphic examples of individ-
ual instances of . . .
Question: Economic benefits ? Is gun control economically beneficial?
Side: Yes Side: No
Argument: Lax gun control laws are economically
costly. The Coalition for Gun Control claims that, ?in
Canada, the costs of firearms death and injury alone
have been estimated at . . .
Argument: Gun sports have economic benefits. Field
sports bring money into poor rural economies and pro-
vide a motivation for landowners to value environmen-
tal protection.
Table 1: An example of a Debatepedia debate on the topic ?Gun control.?
moderation. Many of the debate issues covered are
controversial and salient in current public discourse.
Because it is primarily expressed as text, Debatepe-
dia is a corpus of debate topics, but it is organized
hierarchically, with multiple issues in each debate
topic, questions within each issue, and arguments on
two sides of each question. An important feature of
the corpus is the widespread quotation and linking to
external articles on the web, including news stories,
blog postings, wiki pages, and social media forums;
here we use these external articles in evaluation (?4).
Table 1 shows excerpts from a debate page3 from
Debatepedia. Each debate contains ?questions,?
which reflect the different aspects of a debate. In this
particular debate, there are 13 questions (2 shown),
ranging from economic benefits to enforceability to
social impacts. For each question, there are two dis-
tinct sides, each with its own set of supporting argu-
ments. Many of these arguments also contains links
to online articles where the quotes are extracted from
(not shown in Table 1). For example, in the second
argument on the ?No? side, there is an inline link to
the article written by Congressman Drinan.4
Within a debate topic, the sides cut across differ-
ent questions, aligning arguments together. In gen-
3http://dbp.idebate.org/en/index.php/
Debate:_Gun_control
4http://www.saf.org/LawReviews/Drinan1.
html
Debates 1,303
Arguments 33,556
Articles linked by exactly one argument 3,352
Tokens 1,710,814
Types (excluding NE mentions) 59,601
Person named entity mentions 9,496
Table 2: Debatepedia corpus statistics. Types and tokens
include unigrams, bigrams and person named entities.
eral, the questions are phrased so that a consistent
?pro? and ?con? structure is apparent throughout
each debate, aligned to a high-level question (i.e.,
the ?Yes? sides of all the questions are consistent
with the same side of the larger debate). The ex-
ample of Table 1 deviates from this pattern, with the
self-defense ?Yes? arguing ?no? to the high-level de-
bate question?Should laws be passed to limit gun
ownership further??and the economic ?Yes? argu-
ing ?yes? to the high-level question.
Table 2 presents statistics of our corpus.
2.1 Preprocessing
We scraped the Debatepedia website and extracted
the debate, question, argument, and side structure
of the debate topics. We crawled the external
web articles that were linked from the Debatepe-
dia arguments. For the web articles, we extracted
the main text content (ignoring boilerplate elements
such as navigation and advertisments) using Boil-
1859
erpipe (Kohlschu?tter et al, 2010).5 We tokenized
the text and filtered stopwords.6 We considered both
unigrams and bigrams in our model, keeping all uni-
grams and removing bigram types that appeared less
than 5 times in the corpus. Although our modeling
approach ultimately treats texts as bags of terms (un-
igrams and bigrams), one important preprocessing
step was taken to further improve the interpretabil-
ity of the inferred representation: named entity men-
tions of persons. We identified these mentions of
persons using Stanford NER (Finkel et al, 2005)
and treated each person mention as a single token. In
our qualitative analysis of the model (?4.2), we will
show how this special treatment of person mentions
enables the association of well-known individuals
with debate topics. Though not part of our exper-
imental evaluation in this paper, such associations
are, we believe, an interesting direction for future
applications of the model.
3 Model
Our model defines a probability distribution over
terms7 that are observed in the corpus. Each term
occurs in a context defined by the tuple ?d, q, s, a?
(respectively, a debate, a question within the debate,
a side within the debate, and an argument). At each
level of the hierarchy is a different latent variable:
? Each question q within debate d is associated
with a distribution over topics, denoted ?d,q.8
? Each side s of the debate d is associated with a
position, denoted id,s and we posit a global dis-
tribution ? that cuts across different questions
and arguments. In our experiments, there are
two positions, and the two sides of a debate
are constrained to associate with opposing po-
sitions. As illustrated by Table 1, this assump-
5http://code.google.com/p/boilerpipe
6www.ranks.nl/resources/stopwords.html
7Recall that our model includes bigrams. We treat each un-
igram and bigram token (after filtering discussed in ?2.1) as a
separate term.
8In future work, more sharing across questions within a
debate, or more differentiation among the topic distributions
for arguments under a question, might be explored. Wallach
(2006) describes suitable techniques using hierarchical Dirich-
let draws, and Eisenstein et al (2011) suggests the use of sparse
shocks to log-odds at different levels. Here we work on the
assumption that Debatepedia?s questions are the most topically
coherent level, and work with a single topic mixture at this level.
wz y
Nd,q,s,a
Ad,q,s
?
?
Qd
?
?
?tt?
o
i,t?ii ?et
K TKT
?b
?i ?o ?t ?e
?b
i
?
?
Sd
D
Figure 1: Plate diagram. K is the number of positions,
and T is number of topics. The shaded variables are ob-
served and dashed variables are marginalized. ?,?,?
and all ? are fixed hyperparameters (?3.1).
tion is not always correct, though it tends to
hold most of the time.
? Each term wd,q,s,a,n (n is the position index
of the term within an argument) is associated
with one of five functional term types, denoted
yd,q,s,a,n. This variable is latent, except when it
takes the value ?entity? (e) for terms marked as
named entity mentions. When it is not an en-
tity, it takes one of the other four values: ?gen-
eral position? (i), ?topic-specific position? (o),
?topic? (t), or ?background? (b). Thus, every
term w is drawn from one of these 5 types of
bags, and y acts as a switching variable to se-
lect the type of bag.
? For some term types (the ones where y ?
{o, t}), each term wd,q,s,a,n is associated with
one of T discrete topics, as indexed by
zd,q,s,a,n.
Figure 1 illustrates the plate diagram for the
graphical model underlying our approach. The gen-
erative story is given in Figure 2.
3.1 Priors
Typical probabilistic topic models assume a sym-
metric Dirichlet prior over its term distributions or
1860
1. ? topics t, draw topic-term distribution ?tt ? Dirichlet(?t) and topic-entity distribution ?et ? Dirichlet(?e).
2. ? positions i, draw position-term distribution ?ii ? Dirichlet(?i).
3. ? topics t, ? positions i, draw topic-position term distribution ?oi,t ? Dirichlet(?o).
4. Draw background term distribution ?b ? Dirichlet(?b).
5. Draw functional term type distribution ? ? Dirichlet(?).
6. Draw position distribution ? ? Dirichlet(?).
7. ? debates d:
a. Draw id,1, id,2 ? Multinomial(?), assigning each of the two sides to a position.
b. ? questions q in d:
i. Draw topic mixture proportions ?d,q ? Dirichlet(?).
ii. ? arguments a under question q and term positions n in a:
A. Draw topic label zd,q,s,a ? Multinomial(?d,q).
B. Draw functional term type yd,q,s,a ? Multinomial(?).
C. Draw term wd,q,s,a ? Multinomial (?yd,q,s,a | id,1, id,2, zd,q,s,a).
Figure 2: Generative story for our model of Debatepedia.
apply empirical Bayesian techniques to estimate the
hyperparameters. Motivated by past efforts to ex-
ploit prior knowledge (Zhao et al, 2010; Lin and
He, 2009), we use the OpinionFinder sentiment lex-
icon9 (Wilson et al, 2005) to construct ?i and ?o.
Specifically, terms w in the lexicon were given pa-
rameters ?iw = ?ow = 0.01, and other terms were
given ?iw = ?ow = 0.001, capturing our prior belief
that opinion-expressing terms are likely to be used
in expressing positions. 5,451 types were given a
?boost? through this prior.
Information retrieval has long exploited the ob-
servation that a term?s document frequency (i.e., the
number of documents a term occurs in) is inversely
related its usefulness in retrieval (Jones, 1972). We
encode this in ?b, the prior over the background
term distribution, by setting each value to the log-
arithm of the term?s argument frequency.
The other priors were set to be symmetric: ?e =
0.01 (entity topics), ?t = 0.001 (topics), ? =
50/T = 1.25 (topic mixture coefficients), ? = 0.01
(positions), and ? = 0.01 (functional term types).
Preliminary tests showed that final topics are rela-
tively insensitive to the values of the hyperparame-
ters.
3.2 Inference and Parameter Estimation
Exact inference under this model, like most latent-
variable topic models, is intractable. We apply col-
lapsed Gibbs sampling, a standard approach for such
9http://mpqa.cs.pitt.edu/lexicons/subj_
lexicon/
models (Griffiths and Steyvers, 2004).10 The no-
table deviations from typical uses of collapsed Gibbs
sampling are: (i) we jointly sample id,1 and id,2 to
respect the constraint that they differ; and (ii) we
fix the priors, in some cases to be asymmetric, as
discussed in ?3.1. We perform Gibbs sampling for
2,000 iterations over the dataset, discarding the first
500 iterations for burn-in, and averaging over every
10th iteration thereafter to get estimates for our term
distributions.
3.3 T andK
In all experiments, we use T = 40 topics andK = 2
positions. We did not extensively explore different
values for T and K; preliminary exploration sug-
gested that interpretability, gauged informally by the
authors, degraded for higher values of either.
4 Evaluation
Recall that the aim of this work is to infer a low-
dimensional representation of debate text. We esti-
mated our model on the Debatepedia debates (not in-
cluding hyperlinked articles), and conducted several
evaluations of the model, each considering a differ-
ent aspect of the goal. We exploit external articles
hyperlinked from Debatepedia described in ?2 as
supporting texts for arguments, treating each one?s
association to an argument as variable to be pre-
dicted. Firstly, we evaluate our model on the article
associating task. Secondly, we evaluate our model
on the position prediction task. Then, we compare
10Because this technique is well known in NLP, details are
relegated to supplementary material.
1861
 0
 200
 400
 600
 800
 1000
 1200
 1400
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
N
o 
of
 A
rti
cle
s
JS Divergence
LDA
JST
Our Model
Figure 3: The distribution over Jensen-Shannon diver-
gences between a hyperlinked article and the correspond-
ing Debatepedia argument, n = 3, 352.
our model?s positional assignment of arguments to
human annotated clusterings. Finally, we present
qualitative discussion.
4.1 Quantitative Evaluation
4.1.1 Topics
As described in ?2, our corpus includes 3,352 ar-
ticles hyperlinked by Debatepedia arguments.11 Our
model can be used to infer the posterior over top-
ics associated with such an article, and we compare
that distribution to that of the Debatepedia article
that links to it. Calculating the similarity of these
distributions, we get an estimate of how closely our
model can associate text related to a debate with the
specific argument that linked to it. We compare with
LDA (Blei et al, 2003), which ignores sentiment,
and the joint sentiment topic (JST) model (Lin and
He, 2009), an unsupervised model that jointly cap-
tures sentiment and topic.12 Using Jensen-Shannon
divergence, we find that our approach embeds these
pairs significantly closer than LDA and JST (also
trained with 40 topics), under a Wilcoxon signed
rank test (p < 0.001). Figure 3 shows the histogram
of divergences between our model, JST, and LDA.
Associating external articles. More challenging,
of course, is selecting the argument to which an
external article should be associated. We used the
Jensen-Shannon divergence between topic distribu-
tions of articles and arguments to rank the latter,
for each article. The mean reciprocal rank scores
(Voorhees, 1999) for LDA, JST, and our model were
11We consider only those articles linked by a single Debate-
pedia argument.
12JST multiplies topics out by the set of sentiment labels, as-
signing each token to both a topic and a sentment. We use the
OpinionFinder lexicon in JST?s prior in the same way it is used
in our model.
 0.08
 0.1
 0.12
 0.14
 0.16
 0.18
5 10 15 20 25 inf
M
R
R
K
LDA
JST
Our Model
Figure 4: Mean reciprocal ranks for the association task.
0.1272, 0.1421, and 0.1507, respectively; the differ-
ence is significant (Wilcoxon signed rank test, p <
0.001). We found the same pattern for MRR@k,
k ? {5, 10, 15, 20, 25,?}, as shown in Figure 4.
It is likely possible to engineer more accurate
models for attaching articles to arguments, but the
attachment task is our aim only insofar as it con-
tributes to an overall assessment of an inferred rep-
resentation?s quality.
4.1.2 Positions
Positional distance by topic. We next consider
the JS divergences of position term distributions by
topic; for each topic t, we consider the divergence
between inferred values for ?o1,t and ?o2,t. Figure 5
shows these measurements sorted from most to least
different; these might be taken as evidence for which
issue areas? arguments are more lexically distin-
guishable by side, perhaps indicating less common
ground in discourse or (more speculatively) greater
controversy. For example, our model suggests that
debates relating to topics like presidential politics,
foreign policy, teachers, women?s health, religion,
and Israel/Palestine are more heated (within the De-
batepedia community at the time the debates took
place) than those about the minimum wage, Iran as
a nuclear threat, or immigration.
Predicting positions for arguments. We tested
our model?s ability to infer the positions of argu-
ments. In this experiment (only), we held out 3,000
arguments during parameter estimation. The held-
out arguments were selected so that every debate
side maintained at least one argument whose in-
ferred side could serve as the correct answer for the
held-out argument. We then inferred i for each held-
out argument from debate d and side s, given the
parameters, and compared it with the value of id,s
inferred during parameter estimation. The model
achieved 86% accuracy (Table 3 shows the confu-
1862
sion matrix). Note that JST does not provide a base-
line for comparison, since it does not capture debate
sides.
i = 1 i = 2
i? = 1 1,272 216
i? = 2 199 1,313
Table 3: Confusion matrix for position prediction on
held-out arguments.
Predicting positions for external articles. We
can also use the model to predict the position
adopted in an external text. For articles linked from
within Debatepedia, we have a gold standard: from
which side of a debate was it linked? After using
the model to infer a position variable for such a text,
we can check whether the inferred position variable
matches that of the argument that links to it. Table 4
shows that our model does not successfully com-
plete this task, assigning about 60% of both kinds
of articles i = 1.
i = 1 i = 2
i? = 1 1,042 623
i? = 2 1,043 644
Table 4: Confusion matrix for position prediction on hy-
perlinked articles.
Genre. We manually labeled 500 of these articles
into six genre categories. We had two annotators for
this task (Cohen?s ? = 0.856). These categories,
in increasing order of average Jensen-Shannon di-
vergence, are: blogs, editorials, wiki pages, news,
other, and government. Figure 6 shows the results.
While the only difference between the first and last
groups are surprising by chance, we are encouraged
by our model?s suggestion that blogs and editori-
als may be more ?Debatepedia argument-like? than
news and government articles.
Note that our model is learned only from text
within Debatepedia; it does not observe the text of
external linked articles. Future work might incorpo-
rate this text as additional evidence in order to cap-
ture effects on language stemming from the interac-
tion of position and genre.
0.1 0.2 0.3 0.4 0.5comment, minimum, wage, poverty, capitalism
nuclear, weapons, iran, states, threatparty, vote, republican, political, voters
energy, gas, power, fuel, windtax, economic, trade, cost, percent
immigration, cameras, police, immigrants, crimepeople, dont, time, lot, make
food, consumers, products, calorie, informationdeath, crime, punishment, penalty, justice
marijuana, drug, drugs, alcohol, agemarriage, gay, mars, space, moon
rights, law, people, individual, amendmentsouth, kosovo, independence, state, republic
human, rights, animals, life, animalchildren, child, sex, parents, sexual
school, schools, students, education, publicchina, tibet, chinese, people, tibetan
global, emissions, climate, carbon, warminginternational, court, war, crimes, icc
english, language, violence, people, videoorleans, euthanasia, city, suicide, priests
speech, corporations, corporate, public, moneyhealth, care, insurance, public, private
circumcision, men, sexual, circumcised, foreskininformation, torture, science, evidence, wikipedia
companies, market, industry, business, bailoutlaw, workers, union, rights, legal
college, cloning, game, football, incesttimes, york, ban, june, january
countries, eu, european, international, statesoil, water, production, ethanol, environmental
military, war, iraq, forces, marcheconomy, financial, spending, economic, government
government, social, governments, state, programsisrael, gaza, hamas, israeli, palestinian
women, religious, abortion, god, lifeteachers, pay, test, left, merit
peace, state, west, united, actionunited, states, president, administration, foreign
president, washington, obama, american, america
Figure 5: Jensen-Shannon divergences between topic-
specific positional term distributions, for each topic. Top-
ics are labeled by their most frequent terms from ?t.
4.1.3 Comparison to Human Judgments of
Positions
We compared our model?s inferred positions to
human judgments. For each of the 11 topics in Ta-
ble 8, we selected two associated debates with more
arguments than average (24.99). The debates were
provided to each of three human annotators,13 who
13All were native English-speaking American graduate stu-
dents not otherwise involved in this research. Each is known
by the authors to have basic literacy with issues and debates in
 0.4
 0.45
 0.5
 0.55
 0.6
 0.65
 0.7
blog(12) edit(14) wiki(11) news(33) other(18) gov(12)
JS
 D
ive
rge
nc
e S
co
re
Article type(% of articles)
Figure 6: Position prediction on 500 hyperlinked articles
by genre.
1863
?Israel-Palestine? ?Same-sex marriage? ?Drugs? ?Healthcare? ?Death penalty? ?Abortion?
i1
pre emptive same sex hands free single payer anti death pro choice
israeli palestinian long term performance enhancing so called non violent pro life
open and shut second class in depth self sustaining african american non muslim
i2
two state opposite sex long term government run semi automatic would be
long term well intentioned high speed government approved high profile full time
self destructive day time short term high risk hate crime late term
a. Our model: topic-specific position bigrams associated with six selected topics.
?
war large illegal support death power
assault possibility abuse force penalty limit
disproportionate problems high threat murder civil
+
peace civil disease care power care
independence rights nature universal clean suicide
self-determination affirmative potential uninsured waste death
b. JST: sentiments associated with six selected topics manually aligned to our model?s topics.
Table 6: Terms associated with selected topics. The labels and alignments between the two models? topics were
assigned manually. (a.) Our model: topic-specific position bigrams which are ranked by comparing the log odds
conditioned on the position and topic: log ?oi1,t,w? log ?oi2,t,w. We show the top three terms for each position (b.) JST:we show the top three terms for each sentiment (negative and positive).
A1 (11) A2 (5) A3 (16)
Model (2) 3.21 2.58 3.45
A1 (11) 2.15 2.15
A2 (5) 2.63
Table 5: Variation of information scores for each pairing
of annotators and model.
were instructed to group the 44 sides of the debates.
The instructions stated:
Our goal is to see what you think about how
the different sides of different debates can be
lined up. You might find it convenient to
think of these in terms of political philoso-
phies, contemporary political party platforms,
or something else. Any of these is fine; we
want you to tell us the grouping you find most
reasonable.
All three annotators (hereafter denoted A1, A2, and
A3) used fairly involved labeling schemes; the an-
notators used 37, 30, and 16 unique labels, respec-
tively.14 A1 used keyword lists to label items; we
coarsened his labels manually by removing or merg-
ing less common keywords (resulting in: Republi-
can, Democrat, science/environment, nanny, politi-
cal reform, fiscal liberal, fiscal conservative, liber-
tarian, Israel, Palestine, and one unlabeled side).
A2 provided a coarse annotation along with each
American politics.
14In a small number of cases, an annotator declined to label
a side. Each unlabeled item received its own cluster.
fine-grained one (liberal, conservative, ?, and two
unlabeled sides). We used 100 samples from our
Gibbs sampler to estimate posteriors for each id,s;
these were always 99% or more in agreement, so we
mapped each debate side into its single most proba-
ble cluster. Recall that the two sides of each debate
must be in different clusters.
Table 5 shows the variation of information mea-
sure (Meila, 2003) for each pairing among the three
annotators and our model. The model agrees with
A2?s coarse clustering most closely, and in fact is
closer to A2?s clustering than A2 is to A3?s; it also
agrees with A2?s coarse clustering better than A2?s
coarse and fine clusterings agree (3.36, not shown
in the table). This is promising, but we do not
have confidence that the positional dimension is be-
ing captured especially well in this model; for those
debate-sides labeled liberal or conservative by A2,
the best match of our two positions was still only in
agreement only about 60% of the time, and agree-
ment with each human annotator is within the inter-
val of what would be expected if each debate?s sides
were assigned uniformly at random to positions.15
Remarks. Within debates and within topics, the
model uses the position variable to distinguish sides
well. For external text, the model performs well
on articles such as blogs and editorials but on oth-
ers the positional categories do not seem meaning-
15This was determined using a Monte Carlo simulation with
1,000 samples.
1864
Topic i = 1 i = 2
None (?i) vice president, c sections, twenty four, cross pressures,
pre dates, anti ballistic, cost effectiveness, anti land-
mine, court appointed, child poverty
cross examination, under runs, hand outs, half million,
non christians, break down, counter argument, seventy
five, co workers, run up
?Israel-
Palestine?
pre emptive, israeli palestinian, open and shut, first
time, hamas controlled, democratically elected
two state, long term, self destructive, secretary general,
right wing, all out, near daily, short term
?Same-sex
marriage?
same sex, long term, second class, blankenhorn rauch,
wrong headed, self denial, left handed
opposite sex, well intentioned, day time, planet wide,
day night, child rearing, low earth, one way, one third
?Drugs? hands free, performance enhancing, in depth, hand
held, best kept, non pharmaceutical, anti marijuana
long term, high speed, short term, peer reviewed, alco-
hol related, mind altering, inner city, long lasting
?Healthcare? single payer, so called, self sustaining, public private,
for profit, long run, high cost, multi payer
government run, government approved, high risk, two
tier, government appointed, low cost, set up
?Death
penalty?
anti death, non violent, african american, self help, cut
and cover, heavy handed, dp equivalent
semi automatic, high profile, hate crime, assault
weapons, military style, high dollar, self protective
?Abortion? pro choice, pro life, non muslim, well educated, anti
abortion, much needed, church state, birth control
would be, full time, late term, judeo christian, life
style, day to day, non christian, child bearing
Table 7: General position (first row) and topic-specific position bigrams associated with six selected topics.
Topic Terms Person entity mentions
?Israel-
Palestine?
israel, gaza, hamas, israeli, pales-
tinian
Benjamin Netanyahu, Al Jazeera, Mavi Marmara, Nicholas Kristoff,
Steven R. David
?Same-sex
marriage?
marriage, gay, mars, space, moon Buzz Aldrin, Andrew Sullivan, Moon Base, Scott Bidstrup, Ted Olson
?Drugs? marijuana, drug, drugs, alcohol, age Four Loko, Evo Morales, Toni Meyer, Sean Flynn, Robert Hahn
?Healthcare? health, care, insurance, public, pri-
vate
Kent Conrad, Paul Hsieh, Paul Krugman, Ezra Klein, Jacob Hacker
?Death
penalty?
death, crime, punishment, penalty,
justice
Adam Bedau, Thomas R. Eddlem, Jeff Jacoby, John Baer, Peter Bronson
?Abortion? women, religious, abortion, god, life Ronald Reagan, John Paul II, Sara Malkani, Mother Teresa, Marcella
Alsan
Table 8: For 6 selected topics (labels assigned manually), top terms (?t) and person entities (?e). Bigrams were
included but did not rank in the top five for these topics. The model has conflated debates relating to same-sex
marriage with the space program.
ful, perhaps due to the less argumentative nature
of other kinds of articles. Noting the vast litera-
ture focusing on ideological positions expressed in
text, we believe this failure suggests (i) that broad-
based positions that hold across many topics may
require richer textual representations (see, e.g., the
?syntactic priming? of Greene and Resnik, 2009),
or (ii) that an alternative representation of positions,
such as the spatial models favored by political sci-
entists (Poole and Rosenthal, 1991), may be more
discoverable. Aside from those issues, a stronger
theory of positions may be required. Such a the-
ory could be encoded in a more informative prior or
weaker independence assumptions across debates.
Finally, exploiting explicitly ideological texts along-
side the moderated arguments of Debatepedia might
also help to identify textual associations with gen-
eral positions (Sim et al, 2013). We leave these di-
rections to future work.
4.2 Qualitative Analysis
Of the T = 40 topics our model inferred, we subjec-
tively judged 37 to be coherent; a glimpse of each is
given in Figure 5. We manually selected six of the
most interpretable topics for further evaluation.
As a generative modeling approach, our model
was designed for the purpose of reducing the dimen-
sionality of the sociopolitical debate space, as evi-
denced by Debatepedia. It is like other topic models
in this regard, but we believe that some effects of our
design choices are noteworthy. Table 6 compares the
positional bigrams of our model to the sentiments in-
ferred by JST. We observe the benefit of our model
in identifying terms associated with positions on so-
cial issues, while JST selects more general sentiment
terms.
1865
Table 7 shows bigrams most strongly associated
with general position distributions ?i and selected
topic-position distributions ?o.16 We see the poten-
tial benefit of multiword expressions. Although we
have used frequent bigrams as a poor man?s approx-
imation to multiword expression analysis, we find
the topic-specific positions terms to be subjectively
evocative. While somewhat internally coherent, we
do not observe consistent alignment across topics,
and the general distributions ?i are not suggestive.
The separation of personal name mentions into
their own distributions, shown for some topics in
Table 8, gives a distinctive characterization of top-
ics based on relevant personalities. Subjectively, the
top individuals are relevant to the subject matter as-
sociated with each topic (though the topics are not
always pure; same-sex marriage and the space pro-
gram are merged, for example).
5 Related Work
Insofar as debates are subjective, our study is related
to opinion mining. Subjective text classification
(Wiebe and Riloff, 2005) leads to opinion mining
tasks such as opinion extraction (Dave et al, 2003),
positive and negative polarity classification (Pang et
al., 2002), sentiment target detection (Hu and Liu,
2004; Ganapathibhotla and Liu, 2008), and feature-
opinion extraction (Wu et al, 2009). The above
studies are conducted mostly on product reviews, a
domain with a simpler opinion landscape and more
concrete rationales for those opinions, compared to
sociopolitical debates.
Generative topic models have been successfully
implemented in opinion mining tasks such as feature
identification (Titov and McDonald, 2008), entity-
topic extraction (Newman et al, 2006), mining con-
tentious expressions and interactions (Mukherjee
and Liu, 2012) and specific aspect-opinion word ex-
traction from labeled data (Zhao et al, 2010). Most
relevant to this research is work on feature-sentiment
extraction (Lin and He, 2009; Mei et al, 2007). Mei
et al (2007) built on PLSI, which is problematic
for generalizing beyond the training sample. The
JST model of Lin and He (2009) is an LDA-based
topic model in which each word token is assigned
both a sentiment and a topic; they exploited a sen-
16For more topics, please refer to the supplementary notes.
timent lexicon in the prior distribution. Our model
is closely related, but introduces a switching vari-
able that assigns some tokens to positions, some to
topics, and some to both. Unlike Lin and He?s senti-
ments, our model?s positions are associated with the
two sides of a debate, and we incorporate topics at
the level of questions within debates.
Some studies have specifically analyzed con-
trastive viewpoints or stances in general discussion
text.Agrawal et al (2003) used graph mining based
method to classify authors in to opposite camps for
a given topic. Paul et al (2010) developed an unsu-
pervised method for summarizing contrastive opin-
ions from customer reviews. Abu-Jbara et al (2012)
and Dasigi et al (2012) developed techniques to ad-
dress the problem of automatically detecting sub-
groups of people holding similar stances in a dis-
cussion thread.
Several prior studies have considered debates.
Cabrio and Villata (2012) developed a system based
on argumentation theory which recognizes the en-
tailment and contradiction relationships between
two texts. Awadallah et al (2011) used a debate
corpus as a seed for extracting person-opinion-topic
tuples from news and other web documents and in
later work classified the quotations to specific top-
ics and polarity using language models (Awadal-
lah et al, 2012). Somasundaran and Wiebe (2009)
and Anand et al (2011) were interested in ideolog-
ical content in debates, relying on discourse struc-
ture and leveraging sentiment lexicons to recognize
stances.
Closer to the methodology we describe, Lin et
al. (2008) presented a statistical model for politi-
cal discourse that incorporates both topics and ide-
ologies; they used debates on the Israeli-Palestinian
conflict. Fortuna et al (2009) showed that it is pos-
sible to isolate a subset of terms from media content
that are informative of a news organization?s bias to-
wards a particular issue. Ahmed and Xing (2010) in-
troduced multi-level latent Dirichlet alocation, and
Eisenstein et al (2011) introduced sparse additive
generative models, both conceived as extensions to
well-established probabilistic modeling techniques
(Blei et al, 2003); these were applied to debates
and political blog datasets. Our approach builds on
these models (especially the switching variables of
Ahmed and Xing). We go farther in jointly modeling
1866
text across many debates evidenced by the structure
of Debatepedia, thus grounding our models more
solidly in familiar sociopolitical issues, and in mak-
ing extensive use of existing NLP resources.
6 Conclusion
Using text from Debatepedia, we inferred topics and
position term lexicons in the domain of sociopoliti-
cal debates. Our approach brings together tools from
information extraction and sentiment analysis into a
latent-variable topic model and exploits the hierar-
chical structure of the dataset. Our qualitative and
quantitative evaluations show the model?s strengths
and weaknesses.
Acknowledgments
The authors thank several anonymous reviewers,
Justin Gross, David Kaufer, and members of the
ARK group at CMU for helpful feedback on this
work and gratefully acknowledge the assistance of
the annotators. This research is supported by the
Singapore National Research Foundation under its
International Research Centre@Singapore Funding
Initiative and administered by the IDM Programme
Office, by an A?STAR fellowship to Y.S., and by
Google?s support of the Reading is Believing project
at CMU.
References
Amjad Abu-Jbara, Mona Diab, Pradeep Dasigi, and
Dragomir Radev. 2012. Subgroup detection in ide-
ological discussions. In Proceedings of ACL.
Rakesh Agrawal, Sridhar Rajagopalan, Ramakrishnan
Srikant, and Yirong Xu. 2003. Mining newsgroups
using networks arising from social behavior. In WWW
?03.
Amr Ahmed and Eric P. Xing. 2010. Staying in-
formed: supervised and semi-supervised multi-view
topical analysis of ideological perspective. In Pro-
ceedings of EMNLP.
Pranav Anand, Marilyn Walker, Rob Abbott, Jean E. Fox
Tree, Robeson Bowmani, and Michael Minor. 2011.
Cats rule and dogs drool!: classifying stance in online
debate. In Proceedings of the Second Workshop on
Computational Approaches to Subjectivity and Senti-
ment Analysis.
Rawia Awadallah, Maya Ramanath, and Gerhard
Weikum. 2011. OpinioNetIt: Understanding the
opinions-people network for politically controversial
topics. In Proceedings of CIKM.
Rawia Awadallah, Maya Ramanath, and Gerhard
Weikum. 2012. PolariCQ: Polarity classification of
political quotations. In Proceedings of CIKM.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet alocation. Journal of Machine
Learning Research, 3:993?1022.
Elena Cabrio and Serena Villata. 2012. Combining
textual entailment and argumentation theory for sup-
porting online debates interactions. In Proceedings of
ACL.
Sarah Cohen, James T. Hamilton, and Fred Turner. 2011.
Computational journalism. Communications of the
ACM, 54(10):66?71.
Pradeep Dasigi, Weiwei Guo, and Mona Diab. 2012.
Genre independent subgroup detection in online dis-
cussion threads: a pilot study of implicit attitude using
latent textual semantics. In Proceedings of ACL.
Kushal Dave, Steve Lawrence, and David M. Pennock.
2003. Mining the peanut gallery: opinion extraction
and semantic classification of product reviews. In Pro-
ceedings of WWW.
Jacob Eisenstein, Amr Ahmed, and Eric P Xing. 2011.
Sparse additive generative models of text. In Proceed-
ings of ICML.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local information
into information extraction systems by gibbs sampling.
In Proceedings of ACL.
Blaz Fortuna, Carolina Galleguillos, and Nello Cristian-
ini. 2009. Detecting the bias in media with statis-
tical learning methods. In Ashok N . Srivastava and
Mehran Sahami, editors, Text Mining: Classification,
Clustering, and Applications, pages 27?50. Chapman
& Hall/CRC.
Murthy Ganapathibhotla and Bing Liu. 2008. Mining
opinions in comparative sentences. In Proceedings of
COLING.
Stephan Greene and Philip Resnik. 2009. More than
words: Syntactic packaging and implicit sentiment. In
Proceedings of HLT-NAACL.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences, 101(Suppl. 1):5228?5235.
Minqing Hu and Bing Liu. 2004. Mining and summariz-
ing customer reviews. In Proceedings of CIKM.
Karen Sparck Jones. 1972. A statistical interpretation of
term specificity and its application in retrieval. Jour-
nal of documentation, 28(1):11?21.
Christian Kohlschu?tter, Peter Fankhauser, and Wolfgang
Nejdl. 2010. Boilerplate detection using shallow text
features. In Proceedings of WSDM.
1867
Chenghua Lin and Yulan He. 2009. Joint sentiment/topic
model for sentiment analysis. In Proceedings of
CIKM.
Wei-Hao Lin, Eric Xing, and Alexander Hauptmann.
2008. A joint topic and perspective model for ideo-
logical discourse. In Proceedings of ECML-PKDD.
Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su, and
ChengXiang Zhai. 2007. Topic sentiment mixture:
modeling facets and opinions in weblogs. In Proceed-
ings of WWW.
Marina Meila. 2003. Comparing clusterings by the vari-
ation of information. In Bernhard Scho?lkopf and Man-
fred K. Warmuth, editors, Learning Theory and Kernel
Machines, volume 2777 of Lecture Notes in Computer
Science, pages 173?187. Springer.
Arjun Mukherjee and Bing Liu. 2012. Mining con-
tentions from discussions and debates. In Proceedings
of KDD.
David Newman, Chaitanya Chemudugunta, and Padhraic
Smyth. 2006. Statistical entity-topic models. In Pro-
ceedings of KDD.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using ma-
chine learning techniques. In Proceedings of EMNLP.
Michael J. Paul, ChengXiang Zhai, and Roxana Girju.
2010. Summarizing contrastive viewpoints in opin-
ionated text. In Proceedings of EMNLP.
Keith Poole and Howard Rosenthal. 1991. Patterns of
congressional voting. American Journal of Political
Science, pages 118?178.
Yanchuan Sim, Brice Acree, Justin H. Gross, and
Noah A. Smith. 2013. Measuring ideological propor-
tions in political speeches. In Proceedings of EMNLP.
Swapna Somasundaran and Janyce Wiebe. 2009. Rec-
ognizing stances in online debates. In Proceedings of
ACL.
Ivan Titov and Ryan McDonald. 2008. Modeling online
reviews with multi-grain topic models. In Proceedings
of WWW.
Teun A. Van Dijk. 1998. Ideology: A Multidisciplinary
Approach. Sage Publications Limited.
Ellen M. Voorhees. 1999. The trec-8 question answering
track report. In Proceedings of TREC.
Hanna M. Wallach. 2006. Topic modeling: beyond bag-
of-words. In Proceedings of ICML.
Janyce Wiebe and Ellen Riloff. 2005. Creating sub-
jective and objective sentence classifiers from unan-
notated texts. In Proceedings of CICLing.
Theresa Wilson, Paul Hoffmann, Swapna Somasun-
daran, Jason Kessler, Janyce Wiebe, Yejin Choi, Claire
Cardie, Ellen Riloff, and Siddharth Patwardhan. 2005.
Opinionfinder: a system for subjectivity analysis. In
Proceedings of HLT-EMNLP.
Yuanbin Wu, Qi Zhang, Xuanjing Huang, and Lide Wu.
2009. Phrase dependency parsing for opinion mining.
In Proceedings of EMNLP.
Wayne Xin Zhao, Jing Jiang, Hongfei Yan, and Xiaoming
Li. 2010. Jointly modeling aspects and opinions with
a maxent-lda hybrid. In Proceedings of EMNLP.
1868
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 685?693,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
A Probabilistic Model for Canonicalizing Named Entity Mentions
Dani Yogatama Yanchuan Sim Noah A. Smith
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{dyogatama,ysim,nasmith}@cs.cmu.edu
Abstract
We present a statistical model for canonicalizing
named entity mentions into a table whose rows rep-
resent entities and whose columns are attributes (or
parts of attributes). The model is novel in that it
incorporates entity context, surface features, first-
order dependencies among attribute-parts, and a no-
tion of noise. Transductive learning from a few
seeds and a collection of mention tokens combines
Bayesian inference and conditional estimation. We
evaluate our model and its components on two
datasets collected from political blogs and sports
news, finding that it outperforms a simple agglom-
erative clustering approach and previous work.
1 Introduction
Proper handling of mentions in text of real-world
entities?identifying and resolving them?is a cen-
tral part of many NLP applications. We seek an al-
gorithm that infers a set of real-world entities from
mentions in a text, mapping each entity mention to-
ken to an entity, and discovers general categories of
words used in names (e.g., titles and last names).
Here, we use a probabilistic model to infer a struc-
tured representation of canonical forms of entity at-
tributes through transductive learning from named
entity mentions with a small number of seeds (see
Table 1). The input is a collection of mentions found
by a named entity recognizer, along with their con-
texts, and, following Eisenstein et al (2011), the
output is a table in which entities are rows (the num-
ber of which is not pre-specified) and attribute words
are organized into columns.
This paper contributes a model that builds on the
approach of Eisenstein et al (2011), but also:
? incorporates context of the mention to help with
disambiguation and to allow mentions that do not
share words to be merged liberally;
? conditions against shape features, which improve
the assignment of words to columns;
? is designed to explicitly handle some noise; and
? is learned using elements of Bayesian inference
with conditional estimation (see ?2).
We experiment with variations of our model,
comparing it to a baseline clustering method and the
model of Eisenstein et al (2011), on two datasets,
demonstrating improved performance over both at
recovering a gold standard table. In a political
blogs dataset, the mentions refer to political fig-
ures in the United States (e.g., Mrs. Obama and
Michelle Obama). As a result, the model discov-
ers parts of names??Mrs., Michelle, Obama??
while simultaneously performing coreference res-
olution for named entity mentions. In the sports
news dataset, the model is provided with named en-
tity mentions of heterogenous types, and success
here consists of identifying the correct team for ev-
ery player (e.g., Kobe Bryant and Los Angeles Lak-
ers). In this scenario, given a few seed examples,
the model begins to identify simple relations among
named entities (in addition to discovering attribute
structures), since attributes are expressed as named
entities across multiple mentions. We believe this
adaptability is important, as the salience of different
kinds of names and their usages vary considerably
across domains.
Bill Clinton Mr.
George Bush Mr. W.
Barack Obama Sen. Hussein
Hillary Clinton Mrs. Sen.
Bristol Palin Ms.
Emil Jones Jr.
Kay Hutchison Bailey
Ben Roethlisberger Steelers
Bryant Los Angeles
Derek Jeter New York
Table 1: Seeds for politics (above) and sports (below).
685
 x ?
 
1
1
f
w c
r
s
?
 
?
?
M
L
T
?
C
Figure 1: Graphical representation of our model. Top,
the generation of the table: C is the number of at-
tributes/columns, the number of rows is infinite, ? is a
vector of concentration parameters, ? is a multinomial
distribution over strings, and x is a word in a table cell.
Lower left, for choosing entities to be mentioned: ? deter-
mines the stick lengths and ? is the distribution over en-
tities to be selected for mention. Middle right, for choos-
ing attributes to use in a mention: f is the feature vector,
and ? is the weight vector drawn from a Laplace distri-
bution with mean zero and variance ?. Center, for gen-
erating mentions: M is the number of mentions in the
data, w is a word token set from an entity/row r and at-
tribute/column c. Lower right, for generating contexts: s
is a context word, drawn from a multinomial distribution
? with a Dirichlet prior ?. Variables that are known or
fixed are shaded; variables that are optimized are double
circled. Others are latent; dashed lines imply collapsing.
2 Model
We begin by assuming as input a set of mention to-
kens, each one or more words. In our experiments
these are obtained by running a named entity recog-
nizer. The output is a table in which rows are un-
derstood to correspond to entities (types, not men-
tion tokens) and columns are fields, each associated
with an attribute or a part of it. Our approach is
based on a probabilistic graphical model that gener-
ates the mentions, which are observed, and the table,
which is mostly unobserved, similar to Eisenstein et
al. (2011). Our learning procedure is a hybrid of
Bayesian inference and conditional estimation. The
generative story, depicted in Figure 1, is:
? For each column j ? {1, . . . , C}:
? Draw a multinomial distribution ?j over the
vocabulary from a Dirichlet process: ?j ?
DP(?j , G0). This is the lexicon for field j.
? Generate table entries. For each row i (of which
there are infinitely many), draw an entry xi,j
for cell i, j from ?j . A few of these entries (the
seeds) are observed; we denote those x?.
? Draw weights ?j that associate shape and po-
sitional features with columns from a 0-mean,
?-variance Laplace distribution.
? Generate the distribution over entities to be men-
tioned in general text: ? ? GEM(?) (?stick-
breaking? distribution).
? Generate context distributions. For each row r:
? Draw a multinomial over the context vocabu-
lary (distinct from mention vocabulary) from a
Dirichlet distribution, ?r ? Dir(?).
? For each mention token m:
? Draw an entity/row r ? ?.
? For each word in the mention w, given some of
its features f (assumed observed):
. Choose a column c ? 1Z exp(?
>
c f). This
uses a log-linear distribution with partition
function Z. In one variation of our model,
first-order dependencies among the columns
are enabled; these introduce a dynamic char-
acter to the graphical model that is not shown
in Figure 1.
. With probability 1 ? , set the text wm` to
be xrc. Otherwise, generate any word from a
unigram-noise distribution.
? Generate mention context. For each of the T =
10 context positions (five before and five after
the mention), draw the word s from ?r.
Our choices of prior distributions reflect our be-
liefs about the shapes of the various distributions.
We expect field lexicons ?j and the distributions
over mentioned entities ? to be ?Zipfian? and so use
tools from nonparametric statistics to model them.
We expect column-feature weights ? to be mostly
zero, so a sparsity-inducing Laplace prior is used
(Tibshirani, 1996).
Our goal is to maximize the conditional likeli-
hood of most of the evidence (mentions, contexts,
and seeds), p(w, s, x? | ?,?, ?, ?, ?, ,f) =
?
r
?
c
?
x\x?
?
d?
?
d?
?
d?
p(w, s, r, c, x, ?, ?, ? | ?,?, ?, ?, ?, ,f)
686
with respect to ? and ? . We fix ? (see ?3.3 for the
values of ? for each dataset), ? = 2 (equivalent to
add-one smoothing), ? = 2 ? 10?8,  = 10?10,
and each mention word?s f . Fixing ?, ?, and ? is
essentially just ?being Bayesian,? or fixing a hyper-
parameter based on prior beliefs. Fixing f is quite
different; it is conditioning our model on some ob-
servable features of the data, in this case word shape
features. We do this to avoid integrating over fea-
ture vector values. These choices highlight that the
design of a probabilistic model can draw from both
Bayesian and discriminative tools. Observing some
of x as seeds (x?) renders this approach transductive.
Exact inference in this model is intractable, so we
resort to an approximate inference technique based
on Markov Chain Monte Carlo simulation. The opti-
mization of ? can be described as ?contrastive? esti-
mation (Smith and Eisner, 2005), in which some as-
pects of the data are conditioned against for compu-
tational convenience. The optimization of ? can be
described as ?empirical Bayesian? estimation (Mor-
ris, 1983) in which the parameters of a prior are
fit to data. Our overall learning procedure is a
Monte Carlo Expectation Maximization algorithm
(Wei and Tanner, 1990).
3 Learning and Inference
Our learning procedure is an iterative algorithm con-
sisting of two steps. In the E-step, we perform col-
lapsed Gibbs sampling to obtain distributions over
row and column indices for every mention, given the
current value of the hyperparamaters. In the M-step,
we obtain estimates for the hyperparameters, given
the current posterior distributions.
3.1 E-step
For the mth mention, we sample row index r, then
for each word wm`, we sample column index c.
3.1.1 Sampling Rows
Similar to Eisenstein et al (2011), when we sam-
ple the row for a mention, we use Bayes? rule and
marginalize the columns. We further incorporate
context information and a notion of noise.
p(rm = r | . . .) ? p(rm = r | r?m, ?)
(
?
`
?
c p(wm` | x, rm = r, cm` = c))
(
?
t p(smt | rm = r))
We consider each quantity in turn.
Prior. The probability of drawing a row index fol-
lows a stick breaking distribution. This allows us
to have an unbounded number of rows and let the
model infer the optimal value from data. A standard
marginalization of ? gives us:
p(rm = r | r?m, ?) =
{
N?mr
N+? if N
?m
r > 0
?
N+? otherwise,
where N is the number of mentions, Nr is the num-
ber of mentions assigned to row r, and N?mr is the
number of mentions assigned to row r, excludingm.
Mention likelihood. In order to compute the likeli-
hood of observing mentions in the dataset, we have
to consider a few cases. If a cell in a table has al-
ready generated a word, it can only generate that
word. This hard constraint was a key factor in the
inference algorithm of Eisenstein et al (2011); we
speculate that softening it may reduce MCMC mix-
ing time, so introduce a notion of noise. With proba-
bility  = 10?10, the cell can generate any word. If a
cell has not generated any word, its probability still
depends on other elements of the table. With base
distribution G0,1 and marginalizing ?, we have:
p(wm` | x, rm = r, cm` = c, ?c) = (1)
?
????
????
1?  if xrc = wm`
 if xrc 6? {wm`,?}
N?m`cw
N?m`c +?c
if xrc = ? and Ncw > 0
G0(wm`)
?c
N?m`c +?c
if xrc = ? and Ncw = 0
where N?m`c is the number of cells in column c that
are not empty and N?m`cw is the number of cells in
column c that are set to the word wm`; both counts
excluding the current word under consideration.
Context likelihood. It is important to be able to
use context information to determine which row
a mention should go into. As a novel extension,
our model also uses surrounding words of a men-
tion as its ?context??similar context words can en-
courage two mentions that do not share any words
to be merged. We choose a Dirichlet-multinomial
distribution for our context distribution. For every
row in the table, we have a multinomial distribution
over context vocabulary ?r from a Dirichlet prior ?.
1We let G0 be a uniform distribution over the vocabulary.
687
Therefore, the probability of observing the tth con-
text word for mention m is p(smt | rm = r, ?)
=
{
N?mtrs +?s?1
N?mtr +
P
v ?v?V
if N?mtr > 0
?s?1P
v ?v?V
otherwise,
whereN?mtr is the number of context words of men-
tions assigned to row r, N?mtrs is the number of con-
text words of mentions assigned to row r that are
smt, both excluding the current context word, and v
ranges over the context vocabulary of size V .
3.1.2 Sampling Columns
Our column sampling procedure is novel to this
work and substantially differs from that of Eisen-
stein et al (2011). First, we note that when we sam-
ple column indices for each word in a mention, the
row index for the mention r has already been sam-
pled. Also, our model has interdependencies among
column indices of a mention.2 Standard Gibbs sam-
pling procedure breaks down these dependencies.
For faster mixing, we experiment with first-order
dependencies between columns when sampling col-
umn indices. This idea was suggested by Eisenstein
et al (2011, footnote 1) as a way to learn structure
in name conventions. We suppressed this aspect of
the model in Figure 1 for clarity.
We sample the column index c1 for the first word
in the mention, marginalizing out probabilities of
other words in the mention. After we sample the
column index for the first word, we sample the col-
umn index c2 for the second word, fixing the pre-
vious word to be in column c1, and marginalizing
out probabilities of c3, . . . , cL as before. We repeat
the above procedure until we reach the last word
in the mention. In practice, this can be done effi-
ciently using backward probabilities computed via
dynamic programming. This kind of blocked Gibbs
sampling was proposed by Jensen et al (1995) and
used in NLP by Mochihashi et al (2009). We have:
p(cm` = c | . . .) ?
p(cm` = c | fm`, ?)p(cm` = c | cm`? = c?)
(?
c+ pb(cm` = c | cm`+ = c+)
)
p(wm` | x, rm = r, cm` = c, ?c),
2As shown in Figure 1, column indices in a mention form
?v-structures? with the row index r. Since everyw` is observed,
there is an active path that goes through all these nodes.
where `? is the preceding word and c? is its sam-
pled index, `+ is the following word and c+ is its
possible index, and pb(?) are backward probabilities.
Alternatively, we can perform standard Gibbs sam-
pling and drop the dependencies between columns,
which makes the model rely more heavily on the fea-
tures. For completeness, we detail the computations.
Featurized log linear distribution. Our model can
use arbitrary features to choose a column index.
These features are incorporated as a log-linear dis-
tribution, p(cm` = c | fm`,?) =
exp(?>c fm`)P
c? exp(?
>
c?
fm`)
.
The list of features used in our experiments is:
1{w is the first word in the mention}; 1{w ends
with a period}; 1{w is the last word in the men-
tion}; 1{w is a Roman numeral}; 1{w starts with
an upper-case letter}; 1{w is an Arabic number};
1{w ? {mr,mrs,ms,miss, dr,mdm} }; 1{w con-
tains ? 1 punctuation symbol}; 1{w ? {jr, sr}};
1{w ? {is, in, of, for}}; 1{w is a person entity};
1{w is an organization entity}.
Forward and backward probabilities. Since
we introduce first-order dependencies between
columns, we have forward and backward probabili-
ties, as in HMMs. However, we always sample from
left to right, so we do not need to marginalize ran-
dom variables to the left of the current variable be-
cause their values are already sampled. Our transi-
tion probabilities are as follows:
p(cm` = c | cm`? = c?) =
N?mc?,c
P
c??
N?m
c??,c
,
whereN?mc?,c is the number of times we observe tran-
sitions from column c? to c, excluding mention m.
The forward and backward equations are simple (we
omit them for space).
Mention likelihood. Mention likelihood p(wm` |
x, rm = r, cm` = c, ?c) is identical to when we
sample the row index (Eq. 1).
3.2 M-step
In the M-step, we use gradient-based optimization
routines, L-BFGS (Liu and Nocedal, 1989) and
OWL-QN (Andrew and Gao, 2007) respectively, to
maximize with respect to ? and ?.
688
3.3 Implementation Details
We ran Gibbs sampling for 500 iterations,3 discard-
ing the first 200 for burn-in and averaging counts
over every 10th sample to reduce autocorrelation.
For each word in a mention w, we introduced 12
binary features f for our featurized log-linear distri-
bution (?3.1.2).
We then downcased all words in mentions for the
purpose of defining the table and the mention words
w. Ten context words (5 each to the left and right)
define s for each mention token.
For non-convex optimization problems like ours,
initialization is important. To guide the model to
reach a good local optimum without many restarts,
we manually initialized feature weights and put a
prior on transition probabilities to reflect phenom-
ena observed in the initial seeds. The initializer was
constructed once and not tuned across experiments.4
The M-step was performed every 50 Gibbs sampling
iterations. After inference, we filled each cell with
the word that occurred at least 80% of the time in the
averaged counts for the cell, if such a word existed.
4 Experiments
We compare several variations of our model to
Eisenstein et al (2011) (the authors provided their
implementation to us) and a clustering baseline.
4.1 Datasets
We collected named entity mentions from two cor-
pora: political blogs and sports news. The political
blogs corpus is a collection of blog posts about poli-
tics in the United States (Eisenstein and Xing, 2010),
and the sports news corpus contains news summaries
of major league sports games (National Basketball
3On our moderate-sized datasets (see ?4.1), each iteration
takes approximately three minutes on a 2.2GHz CPU.
4For the politics dataset, we set C = 6, ? =
?1.0, 1.0, 10?12, 10?15, 10?12, 10?8?, initialized ? = 1, and
used a Dirichlet prior on transition counts such that before ob-
serving any data: N0,1 = 10, N0,5 = 5, N2,0 = 10, N2,1 =
10, N2,3 = 10, N2,4 = 5, N3,0 = 10, N3,1 = 10, N5,1 = 15
(others are set to zero). For the sports dataset, we set C = 5,
? = ?1.0, 1.0, 10?15, 10?6, 10?6?, initialized ? = 1, and
used a Dirichlet prior on transition counts N0,1 = 10, N2,3 =
20, N3,4 = 10 (others are set to zero). We also manually initial-
ized the weights of some features? for both datasets. These val-
ues were obtained from preliminary experiments on a smaller
sample of the datasets, and updated on the first EM iteration.
Politics Sports
# source documents 3,000 700
# mentions 10,647 13,813
# unique mentions 528 884
size of mention vocabulary 666 1,177
size of context vocabulary 2,934 2,844
Table 2: Descriptive statistics about the datasets.
Association, National Football League, and Major
League Baseball) in 2009. Due to the large size of
the corpora, we uniformly sampled a subset of doc-
uments for each corpus and ran the Stanford NER
tagger (Finkel et al, 2005), which tagged named en-
tities mentions as person, location, and organization.
We used named entity of type person from the po-
litical blogs corpus, while we are interested in per-
son and organization entities for the sports news cor-
pus. Mentions that appear less than five times are
discarded. Table 2 summarizes statistics for both
datasets of named entity mentions.
Reference tables. We use Eisenstein et al?s man-
ually built 125-entity (282 vocabulary items) refer-
ence table for the politics dataset. Each entity in the
table is represented by the set of all tokens that app-
pear in its references, and the tokens are placed in its
correct column. For the sports data, we obtained a
roster of all NBA, NFL, and MLB players in 2009.
We built our sports reference table using the play-
ers? names, teams and locations, to get 3,642 play-
ers and 15,932 vocabulary items. The gold standard
table for the politics dataset is incomplete, whereas
it is complete for the sports dataset.
Seeds. Table 1 shows the seeds for both datasets.
4.2 Evaluation Scores
We propose both a row evaluation to determine
how well a model disambiguates entities and merges
mentions of the same entity and a column evaluation
to measure how well the model relates words used in
different mentions. Both scores are new for this task.
The first step in evaluation is to find a maximum
score bipartite matching between rows in the re-
sponse and reference table.5 Given the response and
5Treating each row as a set of words, we can optimize the
matching using the Jonker and Volgenant (1987) algorithm.
The column evaluation is identical, except that sets of words
that are matched are defined by columns. We use the Jaccard
similarity?for two sets A and B, |A?B||A?B|?for our similarity
function, Sim(i, j).
689
reference tables, xres and xref , we can compute:
Sres = 1|xres |
?
i?xres ,j?xref :Match(i,j)=1
Sim(i, j)
Sref = 1
|
xref |
?
i?xres ,j?xref :Match(i,j)=1
Sim(i, j)
where i and j denote rows, Match(i, j) is one if i and
j are matched to each other in the optimal matching
or zero otherwise. Sres is a precision-like score, and
Sref is a recall-like score.6 Column evaluation is the
same, but compares columns instead.
4.3 Baselines
Our simple baseline is an agglomerative clustering
algorithm that clusters mentions into entities using
the single-linkage criterion. Initially, each unique
mention forms its own cluster that we incremen-
tally merge together to form rows in the table. This
method requires a similarity score between two clus-
ters. For the politics dataset, we follow Eisenstein et
al. (2011) and use the string edit distance between
mention strings in each cluster to define the score.
For the sports dataset, since mentions contain per-
son and organization named entity types, our score
for clustering uses the Jaccard distance between con-
text words of the mentions. However, such cluster-
ings do not produce columns. Therefore, we first
match words in mentions of type person against
a regular expression to recognize entity attributes
from a fixed set of titles and suffixes, and the first,
middle and last names. We treat words in mentions
of type organization as a single attribute.7 As we
merge clusters together, we arrange words such that
6Eisenstein et al (2011) used precision and recall for their
similarity function. Precision prefers a more compact response
row (or column), which unfairly penalizes situations like those
of our sports dataset, where rows are heterogeneous (e.g., in-
cluding people and organizations). Consider a response ta-
ble made up of two rows: ?Kobe, Bryant? and ?Los, Ange-
les, Lakers?, and a reference table containing all NBA play-
ers and their team names, e.g., ?Kobe, Bryant, Los, Angeles,
Lakers?. Evaluating with the precision similarity function, we
will have perfect precision by matching the first row to the ref-
erence row for Kobe Bryant and the latter row to any Lakers
player. The system is not rewarded for merging the two rows
together, even if they are describing the same entity. Our eval-
uation scores, however, reward the system for accurately filling
in more cells.
7Note that the baseline system uses NER tags, list of titles
and suffixes; which are also provided to our model through the
features in ?3.1.2.
all words within a column belong to the same at-
tribute, creating columns as necessary to accomo-
date multiple similar attributes as a result of merg-
ing. We can evaluate the tables produced by each
step of the clustering to obtain the entire sequence
of response-reference scores.
As a strong baseline, we also compare our ap-
proach with the original implementation of the
model of Eisenstein et al (2011), denoted by EEA.
4.4 Results
For both the politics and sports dataset, we run the
procedure in ?3.3 three times and report the results.
Politics. The results for the politics dataset are
shown in Figure 2. Our model consistently outper-
formed both baselines. We also analyze how much
each of our four main extensions (shape features,
context information, noise, and first-order column
dependencies) to EEA contributes to overall per-
formance by ablating each in turn (also shown in
Fig. 2). The best-performing complete model has
415 rows, of which 113 were matched to the ref-
erence table. Shape features are useful: remov-
ing them was detrimental, and they work even bet-
ter without column dependencies. Indeed, the best
model did not have column dependencies. Remov-
ing context features had a strong negative effect,
though perhaps this could be overcome with a more
carefully tuned initializer.
In row evaluation, the baseline system can achieve
a high reference score by creating one entity row per
unique string, but as it merges strings, the clusters
encompass more word tokens, improving response
score at the expense of reference score. With fewer
clusters, there are fewer entities in the response ta-
ble for matching and the response score suffers. Al-
though we use the same seed table in both exper-
iments, the results from EEA are below the base-
line curve because it has the additional complexity
of inferring the number of columns from data. Our
model is simpler in this regard since it assumes that
the number of columns is known (C = 6). In col-
umn evaluation, our method without column depen-
dencies was best.
Sports. The results for the sports dataset are shown
in Figure 3. Our best-performing complete model?s
response table contains 599 rows, of which 561
were matched to the reference table. In row eval-
690
 0.2 0.2
1
 0.2
2
 0.2
3
 0.2
4
 0.2
5  0
.1 
0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
response score
ref
ere
nce
 sc
ore
 0.3 0.3
5 0.4
 0
 0.0
5 0.1 0.1
5 0.2 0.2
5 0.3
 0.1
 0.1
5
 0.2
 0.2
5
 0.3
 0.3
5
response score
ref
ere
nce
 sc
ore
bas
elin
e
EE
A
com
ple
te
-de
pen
den
cie
s
-no
ise
-co
nte
xt
-fe
atu
res
Figure 2: Row (left) and column (right) scores for the politics dataset. For all but ?baseline? (clustering), each point
denotes a unique sampling run. Note the change in scale in the left plot at y = 0.25. For the clustering baseline, points
correspond to iterations.
 0.2
5 0.3 0.3
5 0.4
 0
 0.0
2 
0.0
4 
0.0
6 
0.0
8
 0.1
response score
ref
ere
nce
 sc
ore
 0
 0.0
5 0.1 0.1
5 0.2 0.2
5  0
 0.0
5
 0.1
 0.1
5
 0.2
 0.2
5
response score
ref
ere
nce
 sc
ore
bas
elin
e
EE
A
com
ple
te
-de
pen
den
cie
s
-no
ise
-co
nte
xt
-fe
atu
res
Figure 3: Row (left) and column (right) scores for the sports dataset. Each point denotes a unique sampling run. The
reference score is low since the reference set includes all entities in the NBA, NFL, and MLB, but most of them were
not mentioned in our dataset.
uation, our model lies above the baseline response-
reference score curve, demonstrating its ability to
correctly identify and combine player mentions with
their team names. Similar to the previous dataset,
our model is also substantially better in column eval-
uation, indicating that it mapped mention words into
a coherent set of five columns.
4.5 Discussion
The two datasets illustrate that our model adapts to
somewhat different tasks, depending on its input.
Furthermore, fixing C (unlike EEA) does appear to
have benefits.
In the politics dataset, most of the mentions con-
tain information about people. Therefore, besides
canonicalizing named entities, the model also re-
solves within-document and cross-document coref-
erence, since it assigned a row index for every men-
tion. For example, our model learned that most men-
tions of John McCain, Sen. John McCain, Sen. Mc-
Cain, and Mr. McCain refer to the same entity. Ta-
ble 3 shows a few noteworthy entities from our com-
plete model?s output table.
Barack Obama Mr. Sen. Hussein
Michelle Obama Mrs.
Norm Coleman Sen.
Sarah Palin Ms.
John McCain Mr. Sen. Hussein
Table 3: A small segment of the output table for the poli-
tics dataset, showing a few noteworthy correct (blue) and
incorrect (red) examples. Black indicates seeds. Though
Ms. is technically correct, there is variation in prefer-
ences and conventions. Our data include eight instances
of Ms. Palin and none of Mrs. Palin or Mrs. Sarah
Palin.
The first entity is an easy example since it only
had to complete information provided in the seed ta-
ble. The model found the correct gender-specific ti-
tle for Barack Obama, Mr.. The rest of the examples
were fully inferred from the data. The model was es-
sentially correct for the second, third, and fourth en-
tities. The last row illustrates a partially erroneous
example, in which the model confused the middle
name of John McCain, possibly because of a com-
bination of a strong prior to reuse this row and the
691
Derek Jeter New York
Ben Roethlisberger Pittsburgh Steelers
Alex Rodriguez New York Yankees
Michael Vick Philadelphia Eagles
Kevin Garnett Los Angeles Lakers
Dave Toub The Bears
Table 4: A small segment of the output table for the sports
dataset, showing a few noteworthy correct (blue) and in-
correct (red) examples. Black indicates seed examples.
introduction of a notion of noise.
In the sports dataset, persons and organizations
are mentioned. Recall that success here consists of
identifying the correct team for every player. The
EEA model is not designed for this and performed
poorly. Our model can do better, since it makes use
of context information and features, and it can put a
person and an organization in one row even though
they do not share common words. Table 4 shows a
few noteworthy entities from our complete model?s
output.
Surprisingly, the model failed to infer that Derek
Jeter plays for New York Yankees, although men-
tions of the organization New York Yankees can be
found in our dataset. This is because the model did
not see enough evidence to put them in the same row.
However, it successfully inferred the missing infor-
mation for Ben Roethlisberger. The next two rows
show cases where our model successfully matched
players with their teams and put each word token to
its respective column. The most frequent error, by
far, is illustrated in the fifth row, where a player is
matched with a wrong team. Kevin Garnett plays for
the Boston Celtics, not the Los Angeles Lakers. It
shows that in some cases context information is not
adequate, and a possible improvement might be ob-
tained by providing more context to the model. The
sixth row is interesting because Dave Toub is indeed
affiliated with the Chicago Bears. However, when
the model saw a mention token The Bears, it did not
have any other columns to put the word token The,
and decided to put it in the fourth column although it
is not a location. If we added more columns, deter-
miners could become another attribute of the entities
that might go into one of these new columns.
5 Related Work
There has been work that attempts to fill predefined
templates using Bayesian nonparametrics (Haghighi
and Klein, 2010) and automatically learns template
structures using agglomerative clustering (Cham-
bers and Jurafsky, 2011). Charniak (2001) and El-
sner et al (2009) focused specifically on names and
discovering their structure, which is a part of the
problem we consider here. More similar to our
work, Eisenstein et al (2011) introduced a non-
parametric Bayesian approach to extract structured
databases of entities. A fundamental difference of
our approach from any of the previous work is it
maximizes conditional likelihood and thus allows
beneficial incorporation of arbitrary features.
Our model is focused on the problem of canoni-
calizing mention strings into their parts, though its r
variables (which map mentions to rows) could be in-
terpreted as (within-document and cross-document)
coreference resolution, which has been tackled us-
ing a range of probabilistic models (Li et al, 2004;
Haghighi and Klein, 2007; Poon and Domingos,
2008; Singh et al, 2011). We have not evaluated it
as such, believing that further work should be done
to integrate appropriate linguistic cues before such
an application.
6 Conclusions
We presented an improved probabilistic model for
canonicalizing named entities into a table. We
showed that the model adapts to different tasks de-
pending on its input and seeds, and that it improves
over state-of-the-art performance on two corpora.
Acknowledgements
The authors thank Jacob Eisenstein and Tae Yano for
helpful discussions and providing us with the implemen-
tation of their model, Tim Hawes for helpful discussions,
Naomi Saphra for assistance in developing the gold stan-
dard for the politics dataset, and three anonymous review-
ers for comments on an earlier draft of this paper. This re-
search was supported in part by the U.S. Army Research
Office, Google?s sponsorship of the Worldly Knowledge
project at CMU, and A?STAR (fellowship to Y. Sim); the
contents of this paper do not necessarily reflect the posi-
tion or the policy of the sponsors, and no official endorse-
ment should be inferred.
692
References
G. Andrew and J. Gao. 2007. Scalable training of L1-
regularized log-linear models. In Proc. of ICML.
N. Chambers and D. Jurafsky. 2011. Template-based
information extraction without the templates. In Proc.
of ACL-HLT.
E. Charniak. 2001. Unsupervised learning of name
structure from coreference data. In Proc. of NAACL.
J. Eisenstein and E. P. Xing. 2010. The CMU 2008 po-
litical blog corpus. Technical report, Carnegie Mellon
University.
J. Eisenstein, T. Yano, W. W. Cohen, N. A. Smith, and
E. P. Xing. 2011. Structured databases of named
entities from Bayesian nonparametrics. In Proc. of
EMNLP Workshop on Unsupervised Learning in NLP.
M. Elsner, E. Charniak, and M. Johnson. 2009. Struc-
tured generative models for unsupervised named-
entity clustering. In Proc. of NAACL-HLT.
J. R. Finkel, T. Grenager, and C. Manning. 2005. In-
corporating non-local information into information ex-
traction systems by Gibbs sampling. In Proc. of ACL.
A. Haghighi and D. Klein. 2007. Unsupervised coref-
erence resolution in a nonparametric Bayesian model.
In Proc. of ACL.
A. Haghighi and D. Klein. 2010. An entity-level ap-
proach to information extraction. In Proc. of ACL
Short Papers.
C. S. Jensen, U. Kjaerulff, and A. Kong. 1995. Blocking
Gibbs sampling in very large probabilistic expert sys-
tem. International Journal of Human-Computer Stud-
ies, 42(6):647?666.
R. Jonker and A. Volgenant. 1987. A shortest augment-
ing path algorithm for dense and sparse linear assign-
ment problems. Computing, 38(4):325?340.
X. Li, P. Morie, and D. Roth. 2004. Identification and
tracing of ambiguous names: discriminative and gen-
erative approaches. In Proc. of AAAI.
D. C. Liu and J. Nocedal. 1989. On the limited memory
BFGS method for large scale optimization. Mathemat-
ical Programming B, 45(3):503?528.
D. Mochihashi, T. Yamada, and N. Ueda. 2009.
Bayesian unsupervised word segmentation with nested
Pitman-Yor language modeling. In Proc. of ACL-
IJCNLP.
C. Morris. 1983. Parametric empirical Bayes inference:
Theory and applications. Journal of the American Sta-
tistical Association, 78(381):47?65.
H. Poon and P. Domingos. 2008. Joint unsupervised
coreference resolution with Markov logic. In Proc. of
EMNLP.
S. Singh, A. Subramanya, F. Pereira, and A. McCallum.
2011. Large-scale cross-document coreference using
distributed inference and hierarchical models. In Proc.
of ACL-HLT.
N. A. Smith and J. Eisner. 2005. Contrastive estimation:
training log-linear models on unlabeled data. In Proc.
of ACL.
R. Tibshirani. 1996. Regression shrinkage and selection
via the lasso. Journal of Royal Statistical Society B,
58(1):267?288.
G. C. G. Wei and M. A. Tanner. 1990. A Monte Carlo
implementation of the EM algorithm and the poor
man?s data augmentation algorithms. Journal of the
American Statistical Association, 85(411):699?704.
693
Proceedings of the ACL-2012 Special Workshop on Rediscovering 50 Years of Discoveries, pages 22?32,
Jeju, Republic of Korea, 10 July 2012. c?2012 Association for Computational Linguistics
Discovering Factions in the Computational Linguistics Community
Yanchuan Sim Noah A. Smith
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{ysim,nasmith}@cs.cmu.edu
David A. Smith
Department of Computer Science
University of Massachusetts
Amherst, MA 01003, USA
dasmith@cs.umass.edu
Abstract
We present a joint probabilistic model of who
cites whom in computational linguistics, and
also of the words they use to do the citing. The
model reveals latent factions, or groups of in-
dividuals whom we expect to collaborate more
closely within their faction, cite within the fac-
tion using language distinct from citation out-
side the faction, and be largely understandable
through the language used when cited from
without. We conduct an exploratory data anal-
ysis on the ACL Anthology. We extend the
model to reveal changes in some authors? fac-
tion memberships over time.
1 Introduction
The ACL Anthology presents an excellent dataset
for studying both the language and the social con-
nections in our evolving research field. Extensive
studies using techniques from the field of biblio-
metrics have been applied to this dataset (Radev et
al., 2009a), quantifying the importance and impact
factor of both authors and articles in the commu-
nity. Moreover, recent work has leveraged the avail-
ability of digitized publications to study trends and
influences within the ACL community (Hall et al,
2008; Gerrish and Blei, 2010; Yogatama et al, 2011)
and to analyze academic collaborations (Johri et al,
2011).
To the best of our knowledge, however, existing
work has mainly pursued ?macroscopic? investiga-
tions of the interaction of authors in collaboration,
citation networks, or the textual content of whole
papers. We seek to complement these results with a
?microscopic? investigation of authors? interactions
by considering the individual sentences authors use
to cite each other.
In this paper, we present a joint model of who
cites whom in computational linguistics, and also of
how they do the citing. Central to this model is the
idea of factions, or groups of individuals whom we
expect to (i) collaborate more closely within their
faction, (ii) cite within the faction using language
distinct from citation outside the faction, (iii) be
largely understandable through the language used
when cited from without, and (iv) evolve over time.
1
Factions can be thought of as ?communities,? which
are loosely defined in the literature on networks
as subgraphs where internal connections are denser
than external ones (Radicchi et al, 2004). The dis-
tinction here is that the strength of connections de-
pends on a latent language model estimated from ci-
tation contexts.
This paper is an exploratory data analysis using a
Bayesian generative model. We aim both to discover
meaningful factions in the ACL community and also
to illustrate the use of a probabilistic model for such
discovery. As such, we do not present any objective
evaluation of the model or make any claims that the
factions optimally explain the research community.
Indeed, we suspect that reaching a broad consensus
among community members about factions (i.e., a
?gold standard?) would be quite difficult, as any so-
cial community?s factions are likely perceived very
1
Our factions are computational abstractions?clusters of
authors?discovered entirely from the corpus. We do not claim
that factions are especially contentious, any more than ?sub-
communities? in social networks are especially collegial.
22
subjectively. It is for this reason that a probabilistic
generative model, in which all assumptions are made
plain, is appropriate for the task. We hope this analy-
sis will prove useful in future empirical research on
social communities (including scientific ones) and
their use of language.
2 Model
In this paper, our approach is a probabilistic model
over (i) coauthorship relations and (ii) the words
in sentences containing citations. The words are
assumed to be generated by a distribution that de-
pends on the (latent) faction memberships of the cit-
ing authors, the cited authors, and whether the au-
thors have coauthored before. To model these dif-
ferent effects on language, we use a sparse additive
generative (SAGE) model (Eisenstein et al, 2011).
In contrast to the popular Dirichlet-multinomial for
topic modeling, which directly models lexical prob-
abilities associated with each (latent) topic, SAGE
models the deviation in log frequencies from a back-
ground lexical distribution. Imposing a sparsity-
inducing prior on the deviation vectors limits the
number of terms whose probabilities diverge from
the background lexical frequencies, thereby increas-
ing robustness to limited training data. SAGE can be
used with or without latent topics; our model does
not include topics. Figure 1 shows the plate diagram
for our model.
We describe the generative process:
? Generate the multinomial distribution over fac-
tion memberships from a Dirichlet distribution:
? ? Dir(?).
? Generate the binomial distribution for whether
two authors coauthor, given that they are in the
same faction, from a Beta distribution: ?
same
?
Beta(?same0 , ?
same
1 ). Generate the analogous bi-
nomial, given that they are in different factions:
?
diff
? Beta(?diff0 , ?
diff
1 ).
? For each author i, draw a faction indicator
ai ? Multinomial(?).
? For all ordered pairs of factions (g, h), draw a
deviation vector ?
(g,h)
? Laplace(0, ?). This
vector, which will be sparse, corresponds to the
?
?
a
(i)
a
(j)
z
(i,j)
?
same
?
same
?
diff
?
diff
w
(i,j)
N
(i,j)
A?A
m
?
(g,h)
?
G?G
Figure 1: Plate diagram for our graphical model. A and
G are the fixed numbers of authors and factions, respec-
tively. m is the background word distribution, ?, ? , ?
are hyperparameters, a are latent author factions, z and
w are the observed coauthorship relations and observed
words in citation sentences between authors, respectively.
Each of the a
(i)
, denoting author i?s faction alignment,
are sampled once every iteration conditioned on all the
other a
(j)
. If i and j are coauthors or i cited j in some
publication, a
(i)
and a
(j)
will not be conditionally inde-
pendent due to the v-structure. ?
same
and ?
diff
are bino-
mial distributions over whether two authors have collab-
orated together before, given that they are assigned to the
same/different factions. Dashed variables are collapsed
out in the Gibbs sampler, while double bordered variables
are optimized in the M-step.
deviations in word log-frequencies when fac-
tion g is citing faction h.
? For each word v in the vocabulary, let the uni-
gram probability that an author in faction g uses
to cite an author in faction h be
?
(g,h)
v =
exp(?(g,h)v +mv)
?
v? exp(?
(g,h)
v? +mv?)
.
? For each ordered pair of authors (i, j),
? For each word that i uses to cite j, draw
w
(i,j)
k ? Multinomial(?
(a(i),a(j))).
? If the authors are from the same faction,
i.e., a
(i) = a(j), draw coauthorship indi-
23
cator z
(i,j)
? Binomial(?same); else, draw
z
(i,j)
? Binomial(?diff).
Thus, our goal is to maximize the conditional like-
lihood of the observed data
p(w, z | ?,?, ?,m,?) =
?
?
?
?
?
a
p(w, z,?,?,a | ?,?, ?,m,?)
with respect to ? and ?. We fix ? and ?, which are
hyperparameters that encode our prior beliefs, and
m, which we assume to be a fixed background word
distribution.
Exact inference in this model is intractable, so we
resort to an approximate inference technique based
on Markov Chain Monte Carlo simulation. We per-
form Bayesian inference over the latent author fac-
tions while using maximum a posteriori estimates
of ? because Bayesian inference of ? is problematic
due to the logistic transformation. We refer the in-
terested reader to Eisenstein et al (2011). We take
an empirical Bayes approach to setting the hyper-
parameter ?. Our overall learning procedure is a
Monte Carlo Expectation Maximization algorithm
(Wei and Tanner, 1990).
3 Learning and Inference
Our learning algorithm is a two-step iterative pro-
cedure. During the E-step, we perform collapsed
Gibbs sampling to obtain distributions over factions
for each author, given the current setting of the hy-
perparameters. In the M-step, we obtain point es-
timates for the hyperparameters ? and ? given the
current posterior distributions for the author fac-
tions.
3.1 E-step
As the Dirichlet and Beta distributions are conjugate
priors to the multinomial and binomial respectively,
we can integrate out the latent variables ?, ?
(same)
and ?
(diff)
. For an author i, we sample his faction
alignment a
(i)
conditioned on faction assignments
to all other authors and citation words between i and
other authors (in both directions). Denoting a
?i
as
the current faction assignments for all the authors
except i,
p(a(i) = g | a(?i),w,?,?,?)
? p(a(i) = g,a(?i),w | ?,?,?)
? (Ng + ?g)
A?
j
?

z +N

z
?

0 + ?

1 +N

0 +N

1
p(w(i) | ?)
where Ng is the number of authors (except i) who
are assigned to faction g, ij = ?same? if g = a(j)
and ij = ?diff? otherwise, and N 1, N

0 denotes
the number of author pairs that have/have not coau-
thored before respectively, given the status of their
factions . We elide the subscripts of  and super-
script of z for notational simplicity and abuse nota-
tion to let w
(i)
refer to all author i?s citation words,
both incoming and outgoing. Using SAGE, the fac-
tor for an author?s words is
p(w(i) | ?) =
?
j
?
v
(
?
(g,a(j))
v
)w(i,j)v (
?
(a(j),g)
v
)w(j,i)v
where w
(i,j)
v is the observed count of the number of
times word v has been used when author i cites j; j
ranges over the A authors.
We sample each author?s faction in turn and do so
several times during the E-step, collecting samples
to estimate our posterior distribution over a.
3.2 M-step
In the M-step, we optimize all ?
(g,h)
and ? given
the posterior distribution over author factions.
Optimizing ?. Eisenstein et al (2011) postu-
lated that the components of ? are drawn from
a compound model
?
N (?;?, ?)E(?; ?)d?, where
E(?; ?) indicates the Exponential distribution. They
fit a variational distribution Q(?) and optimized the
log-likelihood of the data by iteratively fitting the
parameters ? using a Newton optimization step and
maximizing the variational bound.
The compound model described is equivalent to
the Laplace distribution L(?;?, ?) (Lange and Sin-
sheimer, 1993; Figueiredo, 2003). Moreover, a zero
mean Laplace prior has the same effect as placing an
L1 regularizer on ?. Therefore, we can equivalently
24
maximize the regularized likelihood
?c
(g,h)
?
T
?
(g,h)
? ?C
(g,h)
? log
?
v
exp(?(g,h)v +mv)
? ?
?
?
??
(g,h)
?
?
?
1
with respect to ?
(g,h)
. ?c
(g,h)
? is a vector of expected
count of the words that faction g used when citing
faction h, ?c
(g,h)
? =
?
v ?c
(g,h)
v ? and ? is the regu-
larization constant. The regularization constant and
Laplace variance are related by ? = ??1 (Tibshirani,
1996).
We use the gradient-based optimization routine
OWL-QN (Andrew and Gao, 2007) to maximize the
above objective function with respect to ?
(g,h)
for
each pair of factions g and h.
Optimizing ?. As in the empirical Bayes ap-
proach, we learn the hyperparameter setting of ?
from the data by maximizing the log likelihood
with respect to ?. By treating ? as the parame-
ter of a Dirichlet-multinomial compound distribu-
tion, we can directly use the samples of author fac-
tions produced by our Gibbs sampler to estimate
?. Minka (2009) describes in detail several itera-
tive approaches to estimate ?; we use the linear-
time Newton-Raphson iterative update to estimate
the components of ?.
4 Data Analysis
4.1 Dataset
We used the ACL Anthology Network Corpus
(Radev et al, 2009b), which currently contains
18,041 papers written by 12,777 authors. These pa-
pers are published in the field of computational lin-
guistics between 1965 and 2011.
2
Furthermore, the
corpus provides bibliographic data such as authors
of the papers and bibliographic references between
each paper in the corpus. We extracted sentences
containing citations using regular expressions and
linked them between authors with the help of meta-
data provided in the corpus.
We tokenized the extracted sentences and down-
cased them. Words that are numeric, appear less
2
For a list of the journals, conferences and workshops
archived by the ACL anthology, please visit http://
aclweb.org/anthology-new.
than 20 times, or are in a stop word list are dis-
carded. For papers with multiple authors, we divided
the word counts by the number of pairings between
authors in both papers, assigning each word to each
author-pair (i.e., a count of
1
nn? if a paper with n au-
thors cites a paper with n
?
authors).
Due to the large number of authors, we only used
the 500 most cited authors (within the corpus) who
have published at least 5 papers. Papers with no au-
thors left are removed from the dataset. As a re-
sult, we have 8,144 papers containing 80,776 cita-
tion sentences (31,659 citation pairs). After text pro-
cessing, there are 391,711 tokens and 3,037 word
types.
In each iteration of the EM algorithm, we run the
E-step Gibbs sampler for 300 iterations, discarding
the first 100 samples for burn-in and collecting sam-
ples at every 3rd iteration to avoid autocorrelation.
At the M-step, we update our ? and ? using the
samples collected. We run the model for 100 EM
iterations.
We fixed ? = 5, ?same = (0.5, 1) and ?diff =
(1, 0.5). Our setting of ? reflects our prior beliefs
that coauthors tend to be from the same faction.
4.2 Factions in ACL (1965?2011)
We ran the model withG = 30 factions and selected
the most probable faction for each author from the
posterior distribution of the author-faction alignment
obtained in the final E step. Only 26 factions were
selected as most probable for some author.
3
Table 1
presents members of selected factions, along with
citation words that have the largest positive log fre-
quency deviation from the background distribution.
4
Table 2 shows a list of the top three authors associ-
ated with factions not shown in Table 1. Incoming
(outgoing) citation words are found by summing the
log deviation vectors ? across citing (cited) factions.
The author factions are manually labeled.
We see from Table 1, the model has selected key-
words that are arguably significant in certain sub-
fields in computational linguistics. Incoming cita-
tions are generally indicative of the subject areas in
3
In future work, nonparametric priors might be employed to
automate the selection of G.
4
We found it quite difficult to make sense of terms with neg-
ative log frequency deviations. This suggests exploring a model
allowing only positive deviations; we leave that for future work.
25
Formalisms (31) Fernando Pereira, Jason M. Eisner, Stuart M. Shieber, Walter Daelemans, Hitoshi Isa-
hara
Self cites: parsing
In cites: parsing, semiring, grammars, tags, grammar, tag, lexicalized, dependency
Out cites: tagger, regular, dependency, transformationbased, tagging, stochastic, grammars, sense
Evaluation (17) Salim Roukos, Eduard Hovy, Marti A. Hearst, Chin-Yew Lin, Dekang Lin
Self cites: automatic, bleu, linguistics, evaluation, computational, text, proceedings
In cites: automatic, bleu, segmentation, method, proceedings, dependency, parses, text
Out cites: paraphrases, cohesion, agreement, hierarchical, entropy, phrasebased, evaluation, tree-
bank
Semantics (26) Martha Palmer, Daniel Jurafsky, Mihai Surdeanu, David Weir, German Rigau
Self cites: sense, semantic, wordnet
In cites: framenet, sense, semantic, task, wordnet, word, project, question
Out cites: sense, wordnet, moses, preferences, distributional, semantic, focus, supersense
Machine Translation
(MT1) (9)
Kevin Knight, Michel Galley, Jonathan Graehl, Wei Wang, Sanjeev P. Khudanpur
Self cites: inference, scalable, model
In cites: scalable, inference, machine, training, generation, translation, model, syntaxbased
Out cites: phrasebased, hierarchical, inversion, forest, transduction, translation, ibm, discourse
Word Sense Disam-
biguation (WSD) (42)
David Yarowsky, Rada Mihalcea, Eneko Agirre, Ted Pedersen, Yorick Wilks
Self cites: sense, word
In cites: sense, preferences, wordnet, acquired, semcor, word, semantic, calle
Out cites: sense, subcategorization, acquisition, automatic, corpora, lexical, processing, wordnet
Parsing (20) Michael John Collins, Eugene Charniak, Mark Johnson, Stephen Clark, Massimiliano
Ciaramita
Self cites: parser, parsing, model, perceptron, parsers, dependency
In cites: parser, perceptron, supersense, parsing, dependency, results, hmm, models
Out cites: parsing, forest, treebank, model, coreference, stochastic, grammar, task
Discourse (29) Daniel Marcu, Aravind K. Joshi, Barbara J. Grosz, Marilyn A. Walker, Bonnie Lynn
Webber
Self cites: discourse, structure, centering
In cites: discourse, phrasebased, centering, tag, focus, rhetorical, tags, lexicalized
Out cites: discourse, rhetorical, framenet, realizer, tags, resolution, grammars, synonyms
Machine Translation
(MT2) (9)
Franz Josef Och, Hermann Ney, Mitchell P. Marcus, David Chiang, Dekai Wu
Self cites: training, error
In cites: error, giza, rate, alignment, training, minimum, translation, phrasebased
Out cites: forest, subcategorization, arabic, model, translation, machine, models, heuristic
Table 1: Key authors and citation words associated with some factions. For each faction, we show the 5 authors with
highest expected incoming citations (i.e p(faction | author) ? citations). Factions are labeled manually, referring to
key sub-fields in computational linguistics. Faction sizes are in parenthesis following the labels. The citation words
with the strongest positive weights in the deviation vectors are shown.
which the faction holds recognized expertise. For
instance, the faction labeled ?semantics? has cita-
tion terms commonly associated with propositional
semantics: sense, framenet, wordnet. On the other
hand, outgoing citations hint at the related work that
a faction builds on; discourse might require building
on components involving framenet, grammars, syn-
onyms, while word sense disambiguation involves
solving problems like acquisition and modeling sub-
categorization.
4.3 Sensitivity
Given the same initial parameters, we found our
model to be fairly stable across iterations of Monte
26
Adam Lopez, Paul S. Jacobs (2)
Regina Barzilay, Judith L. Klavans, Robert T. Kasper (3)
Lauri Karttunen, Kemal Oflazer, Kimmo Koskenniemi (3)
John Carroll, Ted Briscoe, Scott Miller (7)
Vincent J. Della Pietra, Stephen A. Della Pietra, Robert L.
Mercer (25)
Thorsten Brants, Liang Huang, Anoop Sarkar (9)
Christoph Tillmann, Kenji Yamada, Sharon Goldwater (7)
Alex Waibel, Keh-Jiann Chen, Katrin Kirchhoff (3)
Lynette Hirschman, Claire Cardie, Vincent Ng (26)
Erik F. Tjong Kim Sang, Ido Dagan, Marius Pas?ca (21)
Yuji Matsumoto, Dragomir R. Radev, Chew Lim Tan (18)
Christopher D. Manning, Owen Rambow, Ellen Riloff (19)
Richard Zens, Hieu Hoang, Nicola Bertoldi (9)
Dan Klein, Jun?ichi Tsujii, Yusuke Miyao (6)
Janyce Wiebe, Mirella Lapata, Kathleen R. McKeown (50)
I. Dan Melamed, Ryan McDonald, Joakim Nivre (10)
Philipp Koehn, Lillian Lee, Chris Callison-Burch (80)
Kenneth Ward Church, Eric Brill, Richard M. Schwartz
(19)
Table 2: Top 3 authors of the remaining 18 factions not
displayed in Table 1.
Carlo EM. We found that when G was too small
(e.g., 10), groups were more mixed and the ? vectors
could not capture variation among them well. When
G was larger, the factions were subjectively cleaner,
but fields like translation split into many factions (as
is visible in the G = 30 case illustrated in Tables 1
and 2. Strengthening the L1 penalty made ? more
sparse, of course, but gave less freedom in fitting the
data and therefore more grouping of authors into a
fewer effective factions.
4.4 Inter-Faction Relationships
By using the most probable a posteriori faction for
each author, we can compute the number of cita-
tions between factions. We define the average inter-
faction citations by:
IFC(g, h) =
?(g ? h) + ?(h? g)
Ng +Nh
(1)
where ?(g ? h) is the total number of papers writ-
ten by authors in g that cite papers written by authors
in h.
Figure 2 presents a graph of selected factions
and how these factions talk about each other. As
we would expect, the machine translation faction is
quite strongly connected to formalisms and parsing
factions, reflecting the heavy use of grammars and
Av
era
ge 
ou
t-c
ita
tio
n c
ou
nts
Formalisms
Evaluation
MT 1
Parsing
MT 2
Semantics
WSD
Discourse
Formalisms
Evaluation
MT 1
Parsing
MT 2
Semantics
WSD
Discourse
Figure 3: Heat map showing citation rates across selected
factions. Factions on the horizontal axis are being cited;
factions on the vertical axis are citing. Darker shades de-
note higher average
?(g?h)
Ng
.
parsing algorithms in translation. Moreover, we can
observe that ?deeper? linguistics research, such as
semantics and discourse, are less likely to be cited
by the other factions. This is reflected in Figure 3,
where the statistical MT and parsing factions in the
bottom left exhibit higher citation activity amongst
each other. In addition, we note that factions tend to
self-cite more often than out of their own factions;
this is unsurprising given the prior we selected.
The IFC between discourse and MT2 (as shown
by the edge thickness in figure 2) is higher than ex-
pected, given our prior knowledge of the computa-
tional linguistics community. Further investigation
revealed that, Daniel Marcu, posited by our model
to be a member of the discourse faction, has coau-
thored numerous highly cited papers in MT in re-
cent years (Marcu and Wong, 2002). However, the
model split the translation field, which fragmented
the counts of MT related citation words. Thus,
assigning Daniel Marcu to the discourse faction,
which also has a less diverse citation vocabulary, is
more probable than assigning him to one of the MT
factions. In ?4.6, we consider a model of factions
over time to mitigate this problem.
4.5 Comparison to Graph Clustering
Work in the field of bibliometrics has largely fo-
cused on using the link structure of citation net-
works to study higher level structures. See Osareh
(1996) for a review. Popular methods include bib-
liographic coupling (Kessler, 1963), and co-citation
27
Discourse
Formalisms
MT 2
Parsing
Semantics
Word Sense
Disambiguation
?
p
a
r
s
e
,
p
a
r
s
i
n
g
,
t
r
a
i
n
i
n
g
?
m
o
d
e
l
,
a
l
g
o
r
i
t
h
m
s
,
g
r
a
m
m
a
r
?
a
l
i
g
n
m
e
n
t
,
g
i
z
a
,
u
s
i
n
g
,
m
o
d
e
l
?
p
a
r
s
i
n
g
,
p
a
r
s
e
r
,
p
e
r
c
e
p
-
t
r
o
n
,
h
m
m
,
d
e
p
e
n
d
e
n
c
y
?alignment, giza, training
?phrase, model, joint,
translation, probability
?parsing
?parsing
?
p
r
e
f
e
r
e
n
c
e
s
,
s
e
n
s
e
,
w
o
r
d
-
n
e
t
,
a
c
q
u
i
r
e
d
,
s
e
m
c
o
r
?
s
e
n
s
e
,
s
e
m
a
n
t
i
c
,
l
e
x
i
c
a
l
,
w
o
r
d
n
e
t
,
d
i
s
a
m
b
i
g
u
a
t
i
o
n
?using, alignment,
giza, translation, model
?memory, judges,
voice, allow, sequences
?
t
a
g
s
,
l
e
x
i
c
a
l
i
z
e
d
,
g
r
a
m
-
m
a
r
s
,
a
d
j
o
i
n
i
n
g
,
t
r
e
e
s
?
t
a
g
s
,
g
r
a
m
m
a
r
s
,
l
e
x
i
c
a
l
-
i
z
e
d
,
s
y
n
c
h
r
o
n
o
u
s
,
f
o
r
m
a
l
i
s
m
?
s
u
p
e
r
s
e
n
s
e
,
r
e
s
u
l
t
s
,
w
o
r
d
n
e
t
,
p
a
r
s
i
n
g
,
p
e
r
c
e
p
t
r
o
n
?
t
a
s
k
,
i
n
f
o
r
m
a
t
i
o
n
Figure 2: Citations among some factions. The size of a node is relative to the faction size and edge thickness is relative
to the average number of inter-faction citations (equation 1). The words on the edges are the highest weighted words
from the deviation vectors ?, with the arrow denoting the direction of the citation. Edges with below average IFC
scores are represented as dashed lines, and their citations words are not shown to preserve readability.
analysis (Small, 1973). By using authors as an unit
of analysis in co-citation pairs, author co-citations
have been presented as a technique to analyze their
subject specialties (White and Griffith, 1981). Using
standard graph clustering algorithms on these author
co-citation networks, one can obtain a semblance of
author factions. Hence, we performed graph clus-
tering on both collaboration and citation graphs
5
of
authors in our dataset using Graclus
6
, a graph clus-
tering implementation based on normalized cuts and
ratio associations (Dhillon et al, 2004).
In Table 3, we compare, for selected authors,
how their faction-mates obtained by our model and
graph clustering differ. When clustering on the au-
thor collaboration network, we obtained some clus-
ters easily identified with research labs (e.g., Daniel
Marcu at the Information Sciences Institute). The
co-citation graph leads to groupings dominated by
5
We converted the directed citation graph into a symmetric
graph by performing bibliometric symmetrization described in
Satuluri and Parthasarathy (2011, section 3.3).
6http://www.cs.utexas.edu/users/dml/
Software/graclus.html
heavily co-cited papers in major research areas.
While we do not have an objective measurement
of quality or usefulness, we believe that the fac-
tions identified by our model align somewhat bet-
ter with familiar technical themes around which
sub-communities naturally form than major research
problems or institutions.
4.6 Factions over Time
Faction alignments may be dynamic; we expect that,
over time, individual researchers may move from
one faction to another as their interests evolve. We
consider a slightly modified model whereby authors
are split into different copies of themselves during a
non-overlapping set of discrete time periods. Given
a set of disjoint time periods T , we denote each
author-faction node by {a
(i,t)
| (i, t) ? A? T}. As
we treat each ?incarnation? of an author as a distinct
individual, we can simply use the same inference al-
gorithm described in ?2. (In future work we might
impose an expectation of gradual changes along a
more continuous representation of time.)
28
Our Model Collaboration Network Co-citation Network
Franz Josef Och
Franz Josef Och, Hermann Ney,
Mitchell P. Marcus, David Chiang,
Dekai Wu
Franz Josef Och, Hermann Ney, Richard
Zens, Stephan Vogel, Nicola Ueffing
Franz Josef Och, Hermann Ney, Vincent
J. Della Pietra, Daniel Marcu, Robert L.
Mercer
error, giza, rate, alignment, training giza, mert, popovic, moses, alignments giza, bleu, phrasebased, alignment, mert
Daniel Marcu
Daniel Marcu, Aravind K. Joshi, Bar-
bara J. Grosz, Marilyn A. Walker, Bon-
nie Lynn Webber
Daniel Marcu, Kevin Knight, Daniel
Gildea, David Chiang, Liang Huang
Franz Josef Och, Hermann Ney, Vincent
J. Della Pietra, Daniel Marcu, Robert L.
Mercer
discourse, phrasebased, centering, tag,
focus
phrasebased, forest, cube, spmt, hiero giza, bleu, phrasebased, alignment, mert
Michael John Collins
Eugene Charniak, Michael John Collins,
Mark Johnson, Stephen Clark, Massim-
iliano Ciaramita
Michael John Collins, Joakim Nivre,
Llu??s M?arquez, Xavier Carreras, Jan
Haji?c
Michael John Collins, Christopher D.
Manning, Dan Klein, Eugene Charniak,
Mark Johnson
parser, perceptron, supersense, parsing,
dependency
pseudoprojective, maltparser, percep-
tron, malt, averaged
tnt, prototypedriven, perceptron,
coarsetofine, pcfg
Kathleen R. McKeown
Mirella Lapata, Janyce Wiebe, Kathleen
R. McKeown, Dan Roth, Ralph Grish-
man
Kathleen R. McKeown, Regina Barzi-
lay, Owen Rambow, Marilyn A. Walker,
Srinivas Bangalore
Kenneth Ward Church, David
Yarowsky, Eduard Hovy, Kathleen
R. McKeown, Lillian Lee
semantic, work, learning, corpus, model centering, arabic, pyramid, realpro, cue rouge, minipar, nltk, alignment, mon-
treal
Table 3: Comparing selected factions between our model and graph clustering algorithms. Authors with highest
incoming citations are shown. For our model, we show the largest weighted words in the SAGE vector of incoming
citations for the faction, while for graph clustering, we show words with the highest tf-idf weight.
We split the same data as the earlier sections into
four disjoint time periods, 1965?1989, 1990?1999,
2000?2005 and 2006?2011. The split across time
is unequal due to the number of papers published in
each period: these four periods include 1,917, 3,874,
3,786, and 8,105 papers, respectively. Here we used
G = 20 factions for faster runtime, leading to di-
minished interpretability, though the sparsity of the
deviation vectors mitigates this problem somewhat.
Figure 4 shows graphical plots of selected authors
and their faction membership posteriors over time
(drawn from the final E-step).
With a simple extension of the original model,
we can learn shifts in the subject area the author is
publishing about. Consider Eugene Charniak: the
model observed a major change in faction align-
ment around 2000, when one of the popular Char-
niak parsers (Charniak, 2000) was released; this is
somewhat later than Charniak?s interests shifted, and
the earlier faction?s words are not clearly an ac-
curate description of his work at that time. More
fine-grained modeling of time and also accounting
for the death and birth of factions might ameliorate
these inconsistencies with our background knowl-
edge about Charniak. The model finds that Ar-
avind Joshi was associated with the tagging/parsing
faction in the 1990s and in recent years moved
back towards discourse (Prasad et al, 2008). David
Yarowsky, known for his early work on word sense
disambiguation, has since focused on applying word
sense disambiguation techniques in a multilingual
context (Garera et al, 2009; Bergsma et al, 2011).
As mentioned in the previous section, we observe
that the extended model is able to capture Daniel
Marcu?s shift from discourse-related work to MT
with his work in phrase-based statistical MT (Marcu
and Wong, 2002).
5 Related Work
A number of algorithms use topic modeling to an-
alyze the text in the articles. Topic models such
as latent Dirichlet alocation (Blei et al, 2003) and
its variations have been increasingly used to study
trends in scientific literature (McCallum et al, 2006;
Dietz et al, 2007; Hall et al, 2008; Gerrish and Blei,
2010), predict citation information (McNee et al,
29
 0
 0.2
 0.4
 0.6
 0.8
 1
1970-1989 1990-1999 2000-2005 2006-2011
year
Eugene Charniak
 0
 0.2
 0.4
 0.6
 0.8
 1
1970-1989 1990-1999 2000-2005 2006-2011
year
Aravind K. Joshi
 0
 0.2
 0.4
 0.6
 0.8
 1
1990-1999 2000-2005 2006-2011
year
Daniel Marcu
 0
 0.2
 0.4
 0.6
 0.8
 1
1990-1999 2000-2005 2006-2011
year
David Y owsky
 0
 0.2
 0.4
 0.6
 0.8
 1
1970-1989 1990-1999 2000-2005 2006-2011
year
Kathleen R. McKeown
 0
 0.2
 0.4
 0.6
 0.8
 1
1990-1999 2000-2005 2006-2011
year
Michael J. Collins
 0
 0.2
 0.4
 0.6
 0.8
 1
1970-1989 1990-1999 2000-2005 2006-2011
year
Martha Palmer
 0
 0.2
 0.4
 0.6
 0.8
 1
1990-1999 2000-2005 2006-2011
year
Daniel Jurafsky
parser, parsing, stylistic, treebank, reduction
sense, npcomplete, inducing, wsd, unsupervised
building, annotated, discourse, treebank, kappa
cotraining, scalable, moses, open, implemen
framenet, roles, variation, semantic, propbank
moses, meteor, open, bbn, discovery
bleu, automatic, method, rouge, eval
pcfg, temporal, logic, linguistic, noun
bengston, shallow, conll, learning, kernel
multitext, linking, alignment, competitive, bilingu
phrasebased, forest, joint, hierarchical, kbest
whats, moses, open, rule, source, syntaxbased
human, metric, spade, evaluation, metrics
distributional, rasp, similarity, clustering, deep
tagger, pos, entropy, partofspeech, mathematics
propbank, labelled, dependency, lfg, correlation
vari, perceptron, ccg, counts, connectives
dependency, parser, proc, parse, parsing
contrastive, minimize, synchron, anneal, logist
giza, lins, minipar, error, alignment
Figure 4: Posterior probability of faction alignment over time periods for eight researchers with significant publication
records in at least three periods. The key for each entry contains the five highest weighted words in the deviation
vectors for the faction?s incoming citations. For each author, we show factions with which he or she is associated with
probability > 0.1 in at least one time period.
2002; Ib?a?nez et al, 2009; Nallapati et al, 2008) and
analyze authorship (Rosen-Zvi et al, 2004; Johri et
al., 2011).
Assigning author factions can be seen as network
classification problem, where the goal is to label
nodes in a network such that there is (i) a corre-
lation between a node?s label and its observed at-
tributes and (ii) a correlation between labels of in-
terconnected nodes (Sen et al, 2008). Such collec-
tive network-based approaches have been used on
scientific literature to classify papers/web pages into
its subject categories (Kubica et al, 2002; Getoor,
2005; Angelova and Weikum, 2006). If we knew
the word distributions between factions beforehand,
learning the author factions in our model would be
equivalent to the network classification task, where
our edge weights are proportional to the probability
of coauthorship multiplied by the probability of ob-
serving the citation words given the author?s faction
labels.
6 Conclusion
In this work, we have defined factions in terms of
how authors talk about each other?s work, going be-
yond co-authorship and citation graph representa-
tions of a research community. We take a first step
toward computationally modeling faction formation
by using a latent author faction model and applied
it to the ACL community, revealing both factions
and how they cite each other. We also extended the
model to capture authors? faction changes over time.
30
Acknowledgments
The authors thank members of the ARK group and the
anonymous reviewers for helpful feedback. We gratefully
acknowledge technical assistance from Matthew Fiorillo.
This research was supported in part by an A
?
STAR fel-
lowship to Y. Sim, NSF grant IIS-0915187 to N. Smith,
and the Center for Intelligent Information Retrieval and
NSF grant IIS-0910884 for D. Smith.
References
G. Andrew and J. Gao. 2007. Scalable training of L
1
-
regularized log-linear models. In Proc. of ICML.
R. Angelova and G. Weikum. 2006. Graph-based text
classification: learn from your neighbors. In Proc. of
SIGIR.
S. Bergsma, D. Yarowsky, and K. Church. 2011. Using
large monolingual and bilingual corpora to improve
coordination disambiguation. In Proc. of ACL.
D. M. Blei, A. Y. Ng, and M. I. Jordan. 2003. Latent
Dirichlet alocation. The Journal of Machine Learning
Research, 3:993?1022.
E. Charniak. 2000. A maximum-entropy-inspired parser.
In Proc. of NAACL.
I. S. Dhillon, Y. Guan, and B. Kulis. 2004. Kernel k-
means: spectral clustering and normalized cuts. In
Proc. of KDD.
L. Dietz, S. Bickel, and T. Scheffer. 2007. Unsupervised
prediction of citation influences. In Proc. of ICML.
J. Eisenstein, A. Ahmed, and E. P. Xing. 2011. Sparse
additive generative models of text. In Proc. of ICML.
M. A. T. Figueiredo. 2003. Adaptive sparseness for
supervised learning. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 25(9):1150?1159.
N. Garera, C. Callison-Burch, and D. Yarowsky. 2009.
Improving translation lexicon induction from mono-
lingual corpora via dependency contexts and part-of-
speech equivalences. In Proc. of CoNLL.
S. Gerrish and D. M. Blei. 2010. A language-based
approach to measuring scholarly impact. In Proc. of
ICML.
L. Getoor. 2005. Link-based classification. In Ad-
vanced Methods for Knowledge Discovery from Com-
plex Data, pages 189?207. Springer.
D. Hall, D. Jurafsky, and C. D. Manning. 2008. Studying
the history of ideas using topic models. In Proc. of
EMNLP.
A. Ib?a?nez, P. Larra?naga, and C. Bielza. 2009. Predict-
ing citation count of bioinformatics papers within four
years of publication. Bioinformatics, 25(24):3303?
3309.
N. Johri, D. Ramage, D. A. McFarland, and D. Juraf-
sky. 2011. A study of academic collaborations in
computational linguistics using a latent mixture of au-
thors model. In Proc. of the ACL Workshop on Lan-
guage Technology for Cultural Heritage, Social Sci-
ences, and Humanities.
M. M. Kessler. 1963. Bibliographic coupling between
scientific papers. American documentation, 14(1):10?
25.
J. Kubica, A. Moore, J. Schneider, and Y. Yang. 2002.
Stochastic link and group detection. In Proc. of AAAI.
K. Lange and J. S. Sinsheimer. 1993. Nor-
mal/independent distributions and their applications
in robust regression. Journal of Computational and
Graphical Statistics, 2(2):175?198.
D. Marcu and W. Wong. 2002. A phrase-based, joint
probability model for statistical machine translation.
In Proc. of EMNLP.
A. McCallum, G. S. Mann, and D. Mimno. 2006. Biblio-
metric impact measures leveraging topic analysis. In
Proc. of JCDL.
S. M. McNee, I. Albert, D. Cosley, P. Gopalkrishnan,
S. K. Lam, A. M. Rashid, J. A. Konstan, and J. Riedl.
2002. On the recommending of citations for research
papers. In Proc. of CSCW.
T. P. Minka. 2009. Estimating a Dirich-
let distribution. Available online at http:
//research.microsoft.com/en-us/
um/people/minka/papers/dirichlet/
minka-dirichlet.pdf.
R. M. Nallapati, A. Ahmed, E. P. Xing, and W. W. Cohen.
2008. Joint latent topic models for text and citations.
In Proc. of KDD.
F. Osareh. 1996. Bibliometrics, citation analysis and
co-citation analysis: A review of literature I. Libri,
46(3):149?158.
R. Prasad, N. Dinesh, A. Lee, E. Miltsakaki, L. Robaldo,
A. Joshi, and B. Webber. 2008. The Penn discourse
treebank 2.0. In Proc. of LREC.
D. R. Radev, M. T. Joseph, B. Gibson, and P. Muthukrish-
nan. 2009a. A bibliometric and network analysis of
the field of computational linguistics. Journal of the
American Society for Information Science and Tech-
nology.
D. R. Radev, P. Muthukrishnan, and V. Qazvinian. 2009b.
The ACL Anthology Network corpus. In Proceed-
ings of the Workshop on Text and Citation Analysis for
Scholarly Digital Libraries.
F. Radicchi, C. Castellano, F. Cecconi, V. Loreto,
D. Parisi, and G. Parisi. 2004. Defining and iden-
tifying communities in networks. Proceedings of the
National Academy of Sciences of the United States of
America, 101(9):2658?2663.
31
M. Rosen-Zvi, T. Griffiths, M. Steyvers, and P. Smyth.
2004. The author-topic model for authors and docu-
ments. In Proc. of UAI.
V. Satuluri and S. Parthasarathy. 2011. Symmetrizations
for clustering directed graphs. In Proc. of Interna-
tional Conference on Extending Database Technology.
P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher,
and T. Eliassi-Rad. 2008. Collective classification in
network data. AI magazine, 29(3):93.
H. Small. 1973. Co-citation in the scientific literature:
A new measure of the relationship between two docu-
ments. Journal of the American Society for informa-
tion Science, 24(4):265?269.
R. Tibshirani. 1996. Regression shrinkage and selection
via the lasso. Journal of the Royal Statistical Society.
Series B (Methodological), 58(1):267?288.
G. C. G. Wei and M. A. Tanner. 1990. A Monte Carlo
implementation of the EM algorithm and the poor
man?s data augmentation algorithms. Journal of the
American Statistical Association, 85(411):699?704.
H. D. White and B. C. Griffith. 1981. Author cocitation:
A literature measure of intellectual structure. Jour-
nal of the American Society for Information Science,
32(3):163?171.
D. Yogatama, M. Heilman, B. O?Connor, C.Dyer, B. R.
Routledge, and N. A. Smith. 2011. Predicting a sci-
entific community?s response to an article. In Proc. of
EMNLP.
32
