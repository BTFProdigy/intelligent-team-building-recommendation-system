Statistical Machine Translation Models for Personalized Search
Rohini U ?
AOL India R& D
Bangalore, India
Rohini.uppuluri@corp.aol.com
Vamshi Ambati
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, USA
vamshi@cs.cmu.edu
Vasudeva Varma
LTRC, IIIT Hyd
Hyderabad, India
vv@iiit.ac.in
Abstract
Web search personalization has been well
studied in the recent few years. Relevance
feedback has been used in various ways to
improve relevance of search results. In this
paper, we propose a novel usage of rele-
vance feedback to effectively model the pro-
cess of query formulation and better char-
acterize how a user relates his query to the
document that he intends to retrieve using
a noisy channel model. We model a user
profile as the probabilities of translation of
query to document in this noisy channel us-
ing the relevance feedback obtained from the
user. The user profile thus learnt is applied
in a re-ranking phase to rescore the search
results retrieved using an underlying search
engine. We evaluate our approach by con-
ducting experiments using relevance feed-
back data collected from users using a pop-
ular search engine. The results have shown
improvement over baseline, proving that our
approach can be applied to personalization
of web search. The experiments have also
resulted in some valuable observations that
learning these user profiles using snippets
surrounding the results for a query gives bet-
ter performance than learning from entire
document collection.
1 Introduction
Most existing text retrieval systems, including the
web search engines, suffer from the problem of ?one
?This work was done when the first and second authors were
at IIIT Hyderabad, India.
size fits all?: the decision of which documents to re-
trieve is made based only on the query posed, with-
out consideration of a particular user?s preferences
and search context. When a query (e.g. ?jaguar?) is
ambiguous, the search results are inevitably mixed
in content (e.g. containing documents on the jaguar
cat and on the jaguar car), which is certainly non-
optimal for a given user, who is burdened by having
to sift through the mixed results. In order to opti-
mize retrieval accuracy, we clearly need to model the
user appropriately and personalize search according
to each individual user. The major goal of person-
alized search is to accurately model a user?s infor-
mation need and store it in the user profile and then
re-rank the results to suit to the user?s interests using
the user profile. However, understanding a user?s in-
formation need is, unfortunately, a very difficult task
partly because it is difficult to model the search pro-
cess which is a cognitive process and partly because
it is difficult to characterize a user and his prefer-
ences and goals. Indeed, this has been recognized as
a major challenge in information retrieval research
(et. al, 2003).
In order to address the problem of personalization
one needs to clearly understand the actual process of
search. First the user has an information need that
he would like to fulfill. He is the only entity in the
process that knows the exact information he needs
and also has a vague notion of the document that
can full fill his specific information need. A query
based search engine is at his disposal for identifying
this particular document or set of documents from
among a vast repository of them. He then formu-
lates a query that he thinks is congruent to the doc-
ument he imagines to fulfill his need and poses it to
the search engine. The search engine now returns
521
a list of results that it calculates as relevant accord-
ing to its ranking algorithm. Every user is different
and has a different information need, perhaps over-
lapping sometimes. The way a user conceives an
ideal document that fulfills his need also varies. It is
our hypothesis that if one can learn the variations of
each user in this direction, effective personalization
can be done.
Most approaches to personalization have tried
to model the user?s interests by requesting explicit
feedback from the user during the search process
and observing these relevance judgments to model
the user?s interests. This is called relevance feed-
back, and personalization techniques using it have
been proven to be quite effective for improving re-
trieval accuracy (Salton and Buckley, 1990; Roc-
chio, 1971). These approaches to personalization
have considered, user profile to be a collection of
words, ontology, a matrix etc.
We use relevance feedback for personalization in
our approach. However we propose a novel usage of
relevance feedback to effectively model the process
of query formulation and better characterize how a
user relates his query to the document that he in-
tends to retrieve as discussed in the web search pro-
cess above. A user profile learnt from the relevance
feedback that captures the query generation process
is used as a guide to understand user?s interests over
time and personalize his web search results.
Interestingly, a new paradigm has been proposed
for retrieval rooted from statistical language mod-
eling recently that views the query generation pro-
cess through a Noisy channel model (Berger and
Lafferty, 1999) . It was assumed that the docu-
ment and query are from different languages and
the query generation process was viewed as a trans-
lation from the document language which is more
verbose to the language of the query which is more
compact and brief. The noisy channel model pro-
posed by Berger and Lafferty (Berger and Lafferty,
1999) inherently captures the dependencies between
the query and document words by learning a trans-
lation model between them. As we intend to achieve
personalized search by personalizing the query for-
mulation process, we also perceive the user profile
learning through a Noisy Channel Model. In the
model, when a user has an information need, he also
has an ideal document in mind that fulfills his need.
The user tries to in a way translate the notion of
the ideal document into a query that is more com-
pact but congruent to the document. He then poses
this query to the search engine and retrieves the re-
sults. By observing this above process over time,
we can capture how the user is generating a query
from his ideal document. By learning this model of a
user, we can predict which document best describes
his information need for the query he poses. This
is the motive of personalization. In our approach,
we learn a user model which is probabilistic model
for the noisy channel using statistical translation ap-
proaches and from the past queries and their corre-
sponding relevant documents provided as feedback
by the user.
The rest of the paper is organized as follows.
We first describe the related work on personalized
search then we provide the background and the
framework that our approach is based upon. we
discuss the modeling of a user profile as a transla-
tion model. after which we describe applying it to
personalized search. we describe our experimental
results followed by conclusions with directions to
some future work.
2 Related Work
There has been a growing literature available with
regard to personalization of search results. In this
section, we briefly overview some of the available
literature.
(Pretschner and Gauch, 1999) used ontology to
model users interests, which are studied from users
browsed web pages. (Speretta and Gauch, 2004)
used users search history to construct user profiles.
(Liu et al, 2002) performed personalized web search
by mapping a query to a set of categories using a
user profile and a general profile learned from the
user?s search history and a category hierarchy re-
spectively. (Hatano and Yoshikawa., 2004) consid-
ered the unseen factors of the relationship between
the web users behaviors and information needs and
constructs user profiles through a memory-based
collaborative filtering approach.
To our knowledge, there has been a very little
work has been done that explicitly uses language
models to personalization of search results. (Croft
et al, 2001) discuss about relevance feedback and
522
query expansion using language modeling. (Shen et
al., 2005) use language modeling for short term per-
sonalization by expanding queries.
Earlier approaches to personalization have con-
sidered, user profile to be a collection of words, on-
tology, language model etc. We perceive the user
profile learning through a Noisy Channel Model. In
the model, when a user has an information need, he
also has a vague notion of what is the ideal document
that he would like to retrieve. The user then creates
a compact query that he thinks would retrieve the
document. He then poses the query to the search en-
gine. By observing this above process over time, we
learn a user profile as the probabilities of translation
for the noisy channel that converts his document to
the query. We then use this profile in re-ranking the
results of a search engine to provide personalized re-
sults.
3 Background
In this section, we describe the statistical language
modeling and the translation model framework for
information retrieval that form a basis for our re-
search.
The basic approach for language modeling for IR
was proposed by Ponte and Croft (Ponte and Croft,
1998). It assumes that the user has a reasonable idea
of the terms that are likely to appear in the ideal doc-
ument that can satisfy his/her information need, and
that the query terms the user chooses can distinguish
the ideal document from the rest of the collection.
The query is thus generated as the piece of text rep-
resentative of the ideal document. The task of the
system is then to estimate, for each of the documents
in the collection, which is most likely to be the ideal
document.
argmax
D
P (D|Q) = argmax
D
P (Q|D)P (D)
where Q is a query and D is a document. The prior
probability P (D) is usually assumed to be uniform
and a language model P (Q|D) is estimated for ev-
ery document. In other words, they estimate a prob-
ability distribution over words for each document
and calculate the probability that the query is a sam-
ple from that distribution. Documents are ranked
according to this probability. The basic model has
been extended in a variety of ways. Modeling doc-
uments as in terms of a noisy channel model by
Berger & Lafferty (Berger and Lafferty, 1999), mix-
ture of topics, and phrases are considered (Song and
Croft., 1999), (Lavrenko and Croft, 2001) explicitly
models relevance, and a risk minimization frame-
work based on Bayesian decision theory has been
developed (Zhai and Lafferty, 2001).
The noisy channel by Berger and Lafferty (Berger
and Lafferty, 1999) view a query as a distilla-
tion or translation from a document describing the
query generation process in terms of a noisy channel
model. In formulating a query to a retrieval system,
a user begins with an information need. This infor-
mation need is then represented as a fragment of an
?ideal document?, a portion of the type of document
that the user hopes to receive from the system. The
user then translates or ?distills? this ideal document
fragment into a succinct query, selecting key terms
and replacing some terms with related terms.
To determine the relevance of a document to a
query, their model estimates the probability that the
query would have been generated as a translation
of that document. Documents are then ranked ac-
cording to these probabilities. More specifically,
the mapping from a document term w to a query
term qi is achieved by estimating translation mod-
els P (q|w). Using translation models, the retrieval
model becomes
P (Q|D) =
?
qi?Q
?P (qi|GE)+(1??)
?
w?D
P (qi|w)P (w|D)
where P (qi|GE) is the smoothed or general
probability obtained from a large general corpus.
P (qi|w) is an entry in the translation model. It repre-
sents the probability of generation of the query word
qi for a word w in the document. P (w|D) is the
probability of the word w in the document and ? is
a weighting parameter which lies between 0 and 1.
4 User Profile as a Translation Model
We perceive the user profile learning as learning
the channel probabilities of a Noisy Channel Model
that generates the query from the document. In the
model, when a user has an information need, he also
has a vague notion of what is the ideal document
that he would like to retrieve. The user then creates
523
a compact query that he thinks would retrieve the
document. He then poses the query to the search en-
gine. By observing this above process over time, we
can learn how the user is generating a query from
his notion of an ideal document. By learning this,
we can predict which document best describes his
information need. The learnt model, called a user
profile, is thus capable of personalizing results for
that particular user. Hence, the user profile here is
a translation model learnt from explicit feedback of
the user using statistical translation approaches. Ex-
plicit feedback consists of the past queries and their
corresponding relevant documents provided as feed-
back by the user. A translation model is a proba-
bilistic model consisting of the triples, the source
word, the target word and the probability of trans-
lation. The translation model here is between doc-
ument words and queries words. Therefore the user
profile as a translation model in our approach will
consist of triples of a document word, a query word
and the probability of the document word generating
the query word.
5 Personalized Search
In this section, we describe how we perform person-
alized search using the proposed translation model
based user profile. First, a user profile is learnt using
the translation model process then the re-ranking is
done using the learnt user profile.
5.1 Learning user profile
In our approach, a user profile consists of a statisti-
cal translation model. A translation model is a prob-
abilistic model consisting of the triples, the source
word, the target word and the probability of trans-
lation. Our user profiles consists of the following
triples, a document word, a query word and the prob-
ability of the document word generating the query
word.
Consider a user u, let { {Qi, Di}, i = 1, 2, ..., N}
represent the past history of the user u. where Qi
is the query and Di is the concatenation of all the
relevant documents for the query Qi and let Di =
{w1, w2, ..., wn} be the words in it. The user profile
learnt from the past history of user consists of the
following triples of the form (q, wi, p(q|wi)) where
q is a word in the query Qi and wi is a word in the
document Di.
Translation model is typically learnt from paral-
lel texts i.e a set of translation pairs consisting of
source and target language sentences. In learning
the user profile, we first extract parallel texts from
the past history of the user and then learn the trans-
lation model which is essentially the user profile. In
the subsections below, we describe the process in de-
tail.
5.1.1 Extracting Parallel Texts
By viewing documents as samples of a verbose
language and the queries as samples of a concise
language, we can treat each document-query pair as
a translation pair, i.e. a pair of texts written in the
verbose language and the concise language respec-
tively. The extracted parallel texts consists of pairs
of the form {Qi, Drel} where Drel is the concatena-
tion of contexts extracted from all relevant document
for the query Qi.
We believe that short snippets extracted in the
context of the query would be better candidates for
Drel than using the whole document. This is be-
cause there can be a lot of noisy terms which need
not right in the context of the query. We believe a
short snippet usually N (we considered 15) words
to the left and right of the query words, similar to a
short snippet displayed by search engines can bet-
ter capture the context of the query. In deed we
experimented with different context sizes for Drel.
The first is using the whole document i.e., consider-
ing the query and concatenation of all the relevant
documents as a pair in the parallel texts extracted
which is called Ddocuments The second is using just
a short text snippet from the document in the con-
text of query instead of the whole document which
is called Dsnippets Details are described in the ex-
periments section.
5.1.2 Learning Translation Model
According to the standard statistical translation
model (Brown et al, 1993), we can find the optimal
model M? by maximizing the probability of gener-
ating queries from documents or
M? = argmax
M
N?
i=1
P (Qi|Di,M)
524
qw dw P(qw|dw,u)
journal kdd 0.0176
journal conference 0.0123
journal journal 0.0176
journal sigkdd 0.0088
journal discovery 0.0211
journal mining 0.0017
journal acm 0.0088
music music 0.0375
music purchase 0.0090
music mp3 0.0090
music listen 0.0180
music mp3.com 0.0450
music free 0.0008
Table 1: Sample user profile
To find the optimal word translation probabilities
P (qw|dw,M?), we can use the EM algorithm. The
details of the algorithm can be found in the literature
for statistical translation models, such as (Brown et
al., 1993).
IBM Model1 (Brown et al, 1993) is a simplistic
model which takes no account of the subtler aspects
of language translation including the way word or-
der tends to differ across languages. Similar to ear-
lier work (Berger and Lafferty, 1999), we use IBM
Model1 because we believe it is more suited for IR
because the subtler aspects of language used for ma-
chine translation can be ignored for IR. GIZA++
(Och and Ney, 2003), an open source tool which im-
plements the IBM Models which we have used in
our work for computing the translation probabilities.
A sample user profile learned is shown in Table 1.
5.2 Re-ranking
Re-ranking is a phase in personalized search where
the set of documents matching the query retrieved
by a general search engine are re-scored using the
user profile and then re-ranked in descending order
of rank of the document. We follow a similar ap-
proach in our work.
Let D be set of all the documents returned by the
search engine. The rank of each document D re-
turned for a query Q for user u is computing using
his user profile as shown in Equation 1.
P (Q|D,u) =
?
qi?Q
?P (qi|GE)+(1??)
?
w?D
P (qi|w, u)P (w|D)
(1)
where P (qi|GE) is the smoothed or general
probability obtained from a large general corpus.
P (qi|w, u) is an entry in the translation model of the
user. It represents the probability of generation of
the query word qi for a word w in the document.
P (w|D) is the probability of the word w in the doc-
ument and ? is a weighting parameter which lies be-
tween 0 and 1.
6 Experiments
We performed experiments evaluating our approach
on data set consisting of 7 users. Each user submit-
ted a number of queries to a search engine (Google).
For each query, the user examined the top 10 docu-
ments and identified the set of relevant documents.
Table 2 gives the statistics of the data sets. There is
no repetition of query for any user though repetition
of some words in the query exists (see Table 2). The
document collection consists of top 20 documents
from google which is actually the set of documents
seen by the user while accessing the relevance of the
documents. In all, the total size of the document
collection was 3,469 documents. We did not include
documents of type doc and pdf files.
To evaluate our approach, we use the 10-fold
cross-validation strategy (Mitchell, 1997). We di-
vide the data of each user into 10 sets each hav-
ing (approximately) equal number of search queries
(For example, for user1 had 37 queries in total, we
divided this into 10 sets with 4 queries each approx-
imately). Learning of user profile is done 10 times,
each time leaving out one of the sets from training,
but using only the omitted subset for testing. Per-
formance is computed in the testing phase for each
time and average of the 10 times is taken. In the
testing phase, we take each query and re rank the
results using the proposed approach using his pro-
file learned from nine other sets. For measuring
performance for each query, we compute Precision
@10 (P@10), a widely used metric for evaluating
personalized search algorithms. It is defined as the
proportion of relevant documents among the top 10
results for the given ranking of documents. P@10
is computed by comparing with the relevant docu-
ments present in the data. All the values presented
in the tables are average values which are averaged
over all queries for each user, unless otherwise spec-
ified. We used Lucene1, an open source search en-
gine as the general search engine to first retrieve a
1http://lucene.apache.org
525
User No. Q % of Unique Total Rel Avg. Rel
words in Q
1 37 89 236 6.378
2 50 68.42 178 3.56
3 61 82.63 298 4.885
4 26 86.95 101 3.884
5 33 80.76 134 4.06
6 29 78.08 98 3.379
7 29 88.31 115 3.965
Table 2: Statistics of the data set of 7 users
set of results matching the query.
6.0.1 Comparison with Contextless Ranking
We test the effectiveness of our user profile by
comparing with a contextless ranking algorithm. We
used a generative language modeling for IR as the
context less ranking algorithm (Query Likelihood
model (Ponte and Croft, 1998; Song and Croft.,
1999)). This is actually the simplest version of the
model described in Equation 1. Each word w can be
translated only as itself that is the translation proba-
bilities (see Equation 1) are ?diagonal?.
P (qi|w, u) =
{
1 if q = w
0 Otherwise
This serves as a good baseline for us to see how
well the translation model actually captured the user
information. For fair testing similar to our approach,
for each query, we first retrieve results matching
a query using a general search engine (Lucene).
Then we rank the results using the formula shown
in Equation 2.
P (Q|D) =
?
qi?Q
?P (qi|GE)+(1??)P (qi|D) (2)
We used IBM Model1 for learning the translation
model (i.e., the user profile). The general English
probabilities are computed from all the documents in
the lucene?s index. Similar to earlier works (Berger
and Lafferty, 1999), we simply set the value of ? to
be 0.05. The values reported are P@10 values aver-
age over all 10 sets and the queries for the respec-
tive user. Table 3 clearly shows the improvement
brought in by the user profile.
6.0.2 Experiments with Different Models
We performed an experiment to see if different
training models for learning the user profile affected
Set Contextless Proposed
User1 0.1433 0.1421
User2 0.1426 0.2445
User3 0.1016 0.1216
User4 0.0557 0.1541
User5 0.1877 0.3933
User6 0.1566 0.3941
User7 0.1 0.1833
Avg 0.1268 0.2332
Table 3: Precision @10 results for 7 users
Training Model Document Test Snippet Test
IBM Model1
Document Train 0.2062 0.2028
Snippet Train 0.2333 0.2488
GIZA++
Document Train 0.1799 0.1834
Snippet Train 0.2075 0.2034
Table 4: Summary of Comparison of different Mod-
els and Contexts for learning user profile
the performance. We experimented with two mod-
els. The first is a basic model and used in ear-
lier work, IBM Model1. The second is using the
GIZA++ default parameters. We observed that user
profile learned using IBM Model1 outperformed
that using GIZA++ default parameters. We believe
this is because, IBM Model1 is more suited for IR
because the subtler aspects of language used for ma-
chine translation (which are used in GIZA++ default
parameters) can be ignored for IR. We obtained an
average P@10 value of 0.2333 for IBM Model1 and
0.2075 for GIZA++.
6.0.3 Snippet Vs Document
In extracting parallel texts consists of pairs of the
form {Qi, Drel} where Drel is the concatenation of
contexts extracted from all relevant document for
the queryQi we experimented with different context
sizes for Drel.
We believe that a short snippet extracted in the
context of the query would be better candidate for
Drel than using the whole document. This is be-
cause there can be a lot of noisy terms which need
not useful in the context of the query. We believe
a short snippet usually N (we considered 15) words
to the left and right of the query words, similar to a
short snippet displayed by search engines can better
526
Figure 1: Comparison of Snippet Vs Document
Training using IBM Model1 for training.
IBM Model1 : I - Document Training and Document Testing,
IBM Model1 : II - Document Training and Snippet Testing,
IBM Model1 : III - Snippet Training and Document Testing,
IBM Model1 : IV - Snippet Training and Snippet Testing
capture the context of the query.
We experimented with two context sizes. The
first is using the whole document i.e., considering
the query and concatenation of all the relevant doc-
uments as a pair in the parallel texts extracted which
is calledDdocuments. The second is using just a short
text snippet from the document in the context of
query instead of the whole document which is called
Dsnippets. The user profile learning from pairs of
parallel texts {Q,Ddocuments} is called Document
Train. The user profile learning from pairs of paral-
lel texts {Q,Dsnippets} is called Snippet Train. The
user profiles are trained using both IBM Model1 and
GIZA++ and comparison of the two is shown in Ta-
ble 4.
We also experimented with the size of the context
used for testing. Using the document for re-ranking
as shown in Equation 1 (called Document Test) 2 and
using just a short snippet extracted from the docu-
ment for testing (called Snippet Test). Table 4 shows
the average P@10 over the 10 sets and all queries
and users.
We observed that, not only did the model used
for training affected P@10, but also the data used in
training and testing, whether it was a snippet or doc-
ument, showed a large variation in the performance.
Training using IBM Model1 using the snippet and
2It is to be noted that Snippet Train and Document Test and
training using IBM Model1 is the default configuration used for
all the reported results unless explicitly specified.
Figure 2: Comparison of Snippet Vs Document
Training using GIZA++ Default parameters for
training.
GIZA++:I - Document Training and Document Testing,
GIZA++:II - Document Training and Snippet Testing,
GIZA++:III - Snippet Training and Document Testing,
GIZA++:IV - Snippet Training and Snippet Testing
testing using snippet achieved the best results. This
is in agreement with the discussion that the snip-
pet surrounding the query captures the context of
the query better than a document which may con-
tain many words that could possibly be unrelated to
the query, therefore diluting the strength of the mod-
els learnt. The detailed results for all the users are
shown in Figure 1 and Figure 2.
7 Conclusions and Future Work
Relevance feedback from the user has been used in
various ways to improve the relevance of the re-
sults for the user. In this paper we have proposed
a novel usage of relevance feedback to effectively
model the process of query formulation and better
characterize how a user relates his query to the doc-
ument that he intends to retrieve. We applied a noisy
channel model approach for the query and the doc-
uments in a retrieval process. The user profile was
modeled using the relevance feedback obtained from
the user as the probabilities of translation of query
to document in this noisy channel. The user pro-
file thus learnt was applied in a re-ranking phase to
rescore the search results retrieved using general in-
formation retrieval models. We evaluate the usage of
our approach by conducting experiments using rele-
vance feedback data collected from users of a popu-
lar search engine. Our experiments have resulted in
527
some valuable observations that learning these user
profiles using snippets surrounding the results for
a query show better performance than when learn-
ing from entire documents. In this paper, we have
only evaluated explicit relevance feedback gathered
from a user and performed our experiments. As part
of future work, we would like to evaluate our ap-
proach on implicit feedback gathered probably as
click-through data in a search engine, or on the client
side using customized browsers.
References
Adam Berger and John D. Lafferty. 1999. Information
retrieval as statistical translation. In Research and De-
velopment in Information Retrieval, pages 222?229.
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: parameter esti-
mation. Comput. Linguist., 19(2):263?311.
W. Bruce Croft, Stephen Cronen-Townsend, and Victor
Larvrenko. 2001. Relevance feedback and person-
alization: A language modeling perspective. In DE-
LOS Workshop: Personalisation and Recommender
Systems in Digital Libraries.
Jamie Allan et. al. 2003. Challenges in information re-
trieval language modeling. In SIGIR Forum, volume
37 Number 1.
K. Sugiyama K. Hatano and M. Yoshikawa. 2004. Adap-
tive web search based on user profile constructed with-
out any effort from users. In Proceedings of WWW
2004, page 675 684.
Victor Lavrenko and W. Bruce Croft. 2001. Relevance-
based language models. In Research and Development
in Information Retrieval, pages 120?127.
F. Liu, C. Yu, and W. Meng. 2002. Personalized web
search by mapping user queries to categories. In Pro-
ceedings of the eleventh international conference on
Information and knowledge management, ACM Press,
pages 558?565.
Tom Mitchell. 1997. Machine Learning. McGrawHill.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Jay M. Ponte and W. Bruce Croft. 1998. A lan-
guage modeling approach to information retrieval. In
Research and Development in Information Retrieval,
pages 275?281.
A. Pretschner and S. Gauch. 1999. Ontology based per-
sonalized search. In ICTAI., pages 391?398.
J. J. Rocchio. 1971. Relevance feedback in information
retrieval, the smart retrieval system. Experiments in
Automatic Document Processing, pages 313?323.
G. Salton and C. Buckley. 1990. Improving retrieval per-
formance by relevance feedback. Journal of the Amer-
ican Society of Information Science, 41:288?297.
Xuehua Shen, Bin Tan, and Chengxiang Zhai. 2005. Im-
plicit user modeling for personalized search. In Pro-
ceedings of CIKM 2005.
F. Song and W. B. Croft. 1999. A general language
model for information retrieval. In Proceedings on
the 22nd annual international ACM SIGIR conference,
page 279280.
Micro Speretta and Susan Gauch. 2004. Personalizing
search based on user search histories. In Thirteenth
International Conference on Information and Knowl-
edge Management (CIKM 2004).
Chengxiang Zhai and John Lafferty. 2001. A study of
smoothing methods for language models applied to ad
hoc information retrieval. In Proceedings of ACM SI-
GIR?01, pages 334?342.
528
Proceedings of the IJCNLP-08 Workshop on NER for South and South East Asian Languages, pages 67?74,
Hyderabad, India, January 2008. c?2008 Asian Federation of Natural Language Processing
A Character n-gram Based Approach for Improved Recall
in Indian Language NER
Praneeth M Shishtla
praneethms
@students.iiit.ac.in
Prasad Pingali
pvvpr
@iiit.ac.in
Vasudeva Varma
vv@iiit.ac.in
Language Technologies Research Centre
International Institute of Information Technology
Hyderabad, India
Abstract
Named Entity Recognition (NER) is the
task of identifying and classifying all proper
nouns in a document as person names, or-
ganization names, location names, date &
time expressions and miscellaneous. Previ-
ous work (Cucerzan and Yarowsky, 1999)
was done using the complete words as fea-
tures which suffers from a low recall prob-
lem. Character n-gram based approach
(Klein et al, 2003) using generative mod-
els, was experimented on English language
and it proved to be useful over the word
based models. Applying the same technique
on Indian Languages, we experimented with
Conditional Random Fields (CRFs), a dis-
criminative model, and evaluated our sys-
tem on two Indian Languages Telugu and
Hindi. The character n-gram based models
showed considerable improvement over the
word based models. This paper describes the
features used and experiments to increase
the recall of Named Entity Recognition Sys-
tems which is also language independent.
1 Introduction
The objective of NER is to classify all tokens in a
text document into predefined classes such as per-
son, organization, location, miscellaneous. NER is
a precursor to many language processing tasks. The
creation of a subtask for NER in Message Under-
standing Conference (MUC) (Chinchor, 1997) re-
flects the importance of NER in Information Extrac-
tion (IE). NER also finds aplication in question an-
swering systems (Toral et al, 2005; Molla et al,
2006), and machine translation (Babych and Hart-
ley, 2003). NER is an essential subtask in organizing
and retrieving biomedical information (Tsai, 2006).
NER can be treated as a two step process
? identification of proper nouns.
? classification of these identified proper nouns.
Challenges in named entity recognition.
Many named entities (NEs) occur rarely in corpus
if at all.
Ambiguity of NEs. Ex Washington can be a per-
son?s name or location.
There are many ways of mentioning the same
NE. Ex: Mahatma Gandhi, M.K.Gandhi, Mohandas
Karamchand Gandhi, Gandhi all refer to the same
person. New Jersey, NJ both refer to the same loca-
tion.
In English, the problem of identifying NEs is solved
to some extent by using the capitalization feature.
Most of the named entities begin with a capital let-
ter which is a discriminating feature for classifying a
token as named entity. In addition to the above chal-
lenges, the complexity of Indian Languages pose
few more problems. In case of Indian languages
there is no concept of capitalization. Ex: The per-
son name Y.S.R (in english) is represented as ysr in
the Indian Languages.
Agglutinative property of the Indian Languages
makes the identification more difficult. For exam-
ple: hyderabad, hyderabad ki, hyderabadki, hyder-
abadlo, hyderabad ni, hyderabad ko etc .. all refer
to the place Hyderabad. where lo, ki, ni are all post-
postion markers in Telugu and ko is a postposition
67
marker in Hindi.
There are many ways of representing acronyms.
The letters in acronyms could be the English alpha-
bet or the native alphabet. Ex: B.J.P and BaJaPa
both are acronyms of Bharatiya Janata Party. In-
dian Languages lack particular standard for forming
acronyms.
Due to these wide variations and the agglutina-
tive nature of Indian languages, probabilistic graph-
ical models result in very less recall. If we are able
to identify the presence of a named entity with a
fairly good amount of accuracy, classification then
can be done efficiently. But, when the machine fails
to identify the presence of named entities, there is
no chance of entity classification because we miss
many of the named entities (less recall which results
in less F-measure,F?=1). So we focus mainly on the
ways to improve the recall of the system. Also, In-
dian Languages have a relatively free word order,
i.e. the words (named entities) can occupy any place
in the sentence. This change in the word position is
compensated using case markers.
2 Related Work & Our Contributions
The state-of-art techniques for Indic lan-
guages(Telugu and Hindi) use word based models
which suffer from low recall, use gazetteers and
are language dependent. As such there is no
NER system for Telugu. Previously (Klein et al,
2003) experimented with character-level models
for English using character based HMM which is
a generative model. We experimented using the
discriminative model for English, Hindi and Telugu.
? We propose an approach that increases the re-
call of Indic languages (even the agglutinative
languages).
? The model is language independent as none of
the language resources is needed.
3 Problem Statement
3.1 NER as sequence labelling task
Named entity recognition (NER) can be modelled
as a sequence labelling task (Lafferty et al, 2001).
Given an input sequence of words W n1 = w1w2w3
...wn, the NER task is to construct a label sequence
Ln1 = l1l2l3 ...ln , where label li either belongs to
the set of predefined classes for named entities or
is none (representing words which are not proper
nouns). The general label sequence ln1 has the high-
est probability of occuring for the word sequence
W n1 among all possible label sequences, that is
?Ln1 = argmax {Pr (Ln1 | W n1 ) }
3.2 Tagging Scheme
We followed the IOB tagging scheme (Ramshaw
and Marcus, 1995) for all the three languages (En-
glish, Hindi and Telugu). In this scheme each line
contains a word at the beginning followed by its
tag. The tag encodes the type of named entity
and whether the word is in the beginning or inside
the NE. Empty lines represent sentence (document)
boundaries. An example of the IOB tagging scheme
is given in Table 1.
Words tagged with O are outside of named entities
Token Named Entity Tag
Dr. B-PER
Talcott I-PER
led O
a O
team O
of O
researchers O
from O
the O
National B-ORG
Cancer I-ORG
Institute I-ORG
Table 1: IOB tagging scheme.
and the I-XXX tag is used for words inside a named
entity of type XXX. Whenever two entities of type
XXX are immediately next to each other, the first
word of the second entity will be tagged B-XXX in
order to show that it starts another entity. This tag-
ging scheme is the IOB scheme originally put for-
ward by Ramshaw and Marcus (Ramshaw and Mar-
cus, 1995).
4 Conditional Random Fields
Conditional Random Fields (CRFs) (Wallach, 2004)
are undirected graphical models used to calculate
68
the conditional probability of values on designated
output nodes given the values assigned to other des-
ignated input nodes. In the special case in which
the output nodes of the graphical model are linked
by edges in a linear chain, CRFs make a first-order
Markov independence assumption, and thus can be
understood as conditionally-trained Finite State Ma-
chines (FSMs).
Let o = ? O1,O2,...OT ? be some observed input
data sequence, such as a sequence of words in text
in a document, (the values on n input nodes of the
graphical model). Let S be a set of Finite State Ma-
chine (FSM) states, each of which is associated with
a label, l ? L .
Let s = ? s1,s2,... sT ,? be some sequence of states,(the
values on T output nodes). By the Hammersley-
Clifford theorem CRFs define the conditional prob-
ability of a state sequence given an input sequence
to be
P(s|o) = 1
Zo
? exp(
T
?
t=1
?
k
?k fk (st?1,st ,o, t))
where Zo is a normalization factor over all state
sequences, is an arbitrary feature function over its ar-
guments, and ?k is a learned weight for each feature
function. A feature function may, for example, be
defined to have value 0 or 1. Higher ? weights make
their corresponding FSM transitions more likely.
CRFs define the conditional probability of a la-
bel sequence based on total probability over the state
sequences, P(l|o) = ?s:l(s)=l P(s|o) where l(s) is the
sequence of labels corresponding to the labels of the
states in sequence s. Note that the normalization fac-
tor, Zo, (also known in statistical physics as the parti-
tion function) is the sum of the scores of all possible
state sequences,
Zo = ?
s?ST
?exp(
T
?
t=1
?
k
?k fk (st?1,st ,o, t))
and that the number of state sequences is expo-
nential in the input sequence length, T. In arbitrarily-
structured CRFs, calculating the partition function in
closed form is intractable, and approximation meth-
ods such as Gibbs sampling, or loopy belief propa-
gation must be used.
5 Features
There are many types of features used in NER sys-
tems.
Many systems use binary features i.e. the
word-internal features, which indicate the presence
or absence of particular property in the word.
(Mikheev, 1997; Wacholder et al, 1997; Bikel et
al., 1997). Following are examples of commonly
used binary features: All-Caps (IBM), internal
capitalization (eBay), initial capital (Abdul Kalam),
uncapitalized word (can), 2-digit number (83, 73),
4-digit number (1983, 2007), all digits (8, 28, 1273)
etc. The features that correspond to the capitaliza-
tion are not applicable to Indian languages. Also,
we have not used any of the binary features in any
of our models.
Dictionaries: Dictionaries are used to check if a
part of the named entity is present in the dictionary.
These dictionaries are called as gazetteers. The
problem with the Indian languages is that there are
no proper gazetteers in Indian languages.
Lexical features like a sliding window
[w?2,w?1,wo,w1,w2] are used to create a lexi-
cal history view. Prefix and suffix tries were also
used previously (Cucerzan and Yarowsky, 1999).
Linguistics features like Part Of Speech, Chunk,
etc are also used. In our approach we don?t use any
of these language specific (linguistic) information.
5.1 Our Features
In our experiments, we considered and character n-
grams (ASCII characters) as tokens.
For example for the word Vivekananda, the 4-gram
model would result in 8 tokens namely Vive, ivek,
veka, ekan, kana, anan, nand and anda. If our cur-
rent token (w0) is kana
Feature Example
current token: w0 kana
previous 3 tokens: w?3,w?2,w?1 ivek,veka,ekan
next 3 tokens: w1,w2,w3 anan,nand,anda
compound feature: w0 w1 kanaanan
compound feature: w?1 w0 ekankana
In Indian Languages suffixes and other inflections
get attached to the words increasing the length of the
word and reducing the number of occurences of that
word in the entire corpus. The character n-grams
69
can capture these variations. The compound features
also help in capturing such variations. The sliding
window feature helps in guessing the class of the en-
tity using the context. In total 9 features were used
in training and testing. All the features are languge
independent and no binary features are used.
6 Experimental Setup
6.1 Corpus
We conducted the experiments on three languages
namely Telugu, Hindi and English. We collected the
Telugu corpus from Eenadu, a telugu daily news-
paper. The topics included politics, health and
medicine, sports, education, general issues etc. The
annotated corpus had 45714 tokens, out of which
4709 were named entities. We collected the English
corpus from the Wall Street Journal (WSJ) news ar-
ticles. The corpus had 45870 tokens out of which
4287 were named entities. And we collected the
hindi corpus from various sources. The topics in the
corpus included social sciences, biological sciences,
financial articles, religion, etc. The hindi corpus is
not a news corpus. The corpus had 45380 tokens out
of which 3140 were named entities. We evaluated
the hand-annotated corpus once to check for any er-
rors.
6.2 Experiments
We conducted various experiments on Telugu and
Hindi. Also, to verify the correctness of our model
for other languages, we have conducted some ex-
periments on English data also. In this section we
describe the various experiments conducted on the
Telugu, Hindi and English data sets.
We show the average performance of the system
in terms of precision, recall and F-measure for Tel-
ugu, Hindi and English in Table 6 and then for the
impact of training data size on performance of the
system in Table 7 (Telugu), Table 8 (English) and
Table 9 (Hindi). Here, precision measures the num-
ber of correct Named Entities (NEs) in the machine
tagged file over the total number of NEs in the ma-
chine tagged file and the recall measures the number
of correct NEs in the machine tagged file over the to-
tal number of NEs in the golden standard file while
F-measure is the weighted harmonic mean of preci-
sion and recall:
F =
(? 2 + 1) RP
? 2R + P
with
? 2 = 1
where P is Precision, R is Recall and F is F-measure.
Precision Recall F?=1
words 89.66% 29.21% 44.07
n=2 77.36% 46.07% 57.75
n=3 85.45% 52.81% 65.28
n=4 79.63% 48.31% 60.14
n=5 74.47% 39.33% 51.47
n=6 76.32% 32.58% 45.67
Table 2: Precision,Recall and F?=1 measure for Date
& Time expressions in Telugu.
Precision Recall F?=1
words 83.65% 28.71% 42.75
n=2 80.29% 36.30% 50
n=3 78.26% 35.64% 48.98
n=4 81.03% 31.02% 44.87
n=5 75.42% 29.37% 42.28
n=6 53.21% 27.39% 36.17
Table 3: Precision,Recall & F?=1 measure values for
location names in Telugu.
Precision Recall F?=1
words 51.11% 18.70% 27.38
n=2 53.41% 38.21% 44.55
n=3 69.35% 34.96% 46.49
n=4 69.35% 34.96% 46.49
n=5 55.00% 26.83% 36.07
n=6 50.98% 21.14% 29.89
Table 4: Precision,Recall and F?=1 measure values
for organisation names in Telugu.
Table:6 shows the average precison(P),recall(R)
and F-measure(F) values for NEs in Telugu.
Tables 2 to 5 show the P,R,F values for the indi-
vidual categories of NEs in Telugu. Interestingly,
70
Precision Recall F?=1
words 57.32% 18.65% 28.14
n=2 55.77% 34.52% 42.65
n=3 61.04% 37.30% 46.31
n=4 56.92% 29.37% 38.74
n=5 60.50% 28.57% 38.81
n=6 54.21% 23.02% 32.31
Table 5: Precision,Recall and F?=1 measure values
for Person names in Telugu.
though we have not used any of the features per-
taining to years and numbers we have acheived an
appreciable F-measure of 65.28 for date & time ex-
pressions.
In each table the model with the highest F-
measure is higlighted in bold. And, the tri-gram
model performed best in most of the cases except
with locations where bi-gram model performed well.
But, even the tri-gram model (F?=1=48.98) per-
formed close to the bi-gram model ((F?=1=50).
For Hindi, the recall of the n-gram models(Table
6) is more than the word based models but the
amount of increase in recall and F-measure is less.
On examining, we found that the average number of
named entities in the Hindi data were quite less. This
is because the articles for hindi were taken from gen-
eral articles. Whereas in case of English and Telugu,
the corpus was collected from news articles, which
had more probability of having new and more named
entities, which can occur in a similar repeating pat-
tern.
The character n-gram approach showed consider-
able improvement in recall and F-measure (with a
drop in precision) in Telugu and Hindi, which are
agglutinative in nature. In Telugu, there is a differ-
ence of 14.19 and 14.02 in recall and F-measure re-
spectively between the word based model and the
best performing n-gram model (n=3) of size 3. In
Hindi, there is a difference of 2.34 and 2.33 in re-
call and F-measure respectively between the word
based model and the best performing n-gram model
(n=5). Even in case of non-agglutinative language
like English there is a considerable improvement of
1.48 and 1.91 in recall and F-measure respectively
between the word based model and best performing
n-gram model (n=2) of size 2.
In almost all the cases the character based models
performed better in terms of recall and F-measure
than the word based models.
We also experimented changing the training data
size keeping the testing data size unchanged for Tel-
ugu(Table 7) and English(Table 8) and Hindi(Table
9). From Table 7:All the models (words,character
n-gram models) are able to learn as we increase the
training data size. And the recall of the character
n-gram models is considerably more than recall of
the word based model. Also the 3-gram model per-
formed well in almost all the runs. The rate of learn-
ing is more in case of 30K.
From Table 8, in all the runs, the bi-gram char-
acter model constantly performed the best. Also
interestingly the model is able to achieve a least
F-measure of 44.75 with just 10K words of train-
ing data. But, in case of Telugu,(Table 7) an F-
measure of 44+ was reached with training data of
size 35K i.e the learning rate for english is more for
less amount of data. This is due to the reason that
Telugu (Entropy=15.625 bits per character) (Bharati
et al, 1998) is comparitively a high entropy lan-
guage than English (Brown and Pietra, 1992). How-
ever for Hindi, the relative jump in the performance
(compared to Telugu and English)is less. Even the
entropy of Hindi (Entorpy=11.088) (Bharati et al,
1998) is more than English. This is also observed
from the table (Table 10). The numbers in the sec-
ond, third and fourth columns are the number of fea-
tures for English,Telugu and Hindi respectively.
English Telugu Hindi
words 29145 320260 685032
n=2 27707 267340 647109
n=3 45580 680720 1403352
n=4 64284 1162320 1830438
n=5 65248 1359980 1735614
n=6 57297 1278790 1433322
Table 10: Number of features calculated in the word
based model for English,Telugu and Hindi.
7 Conclusion & Future Work
The character based n-gram approach worked bet-
ter than the word based approach even with agglu-
tinative languages. A considerably good NER for
71
Language English Telugu Hindi
Precision Recall F?=1 Precision Recall F?=1 Precision Recall F?=1
Words 92.42% 47.29% 62.56 70.38% 23.83% 35.6 51.66% 36.45% 42.74
n=2 81.21% 68.77% 74.47 65.67% 37.11% 47.42 37.30% 36.06% 36.67
n=3 88.37% 62.45% 73.18 71.39% 38.02% 49.62 54.89% 37.23% 44.37
n=4 93.17% 59.19% 72.39 70.17% 33.07% 44.96 54.67% 37.62% 44.57
n=5 90.71% 58.30% 70.98 66.57% 29.82% 41.19 53.78% 38.79% 45.07
n=6 91.03% 56.14% 69.45 55.68% 25.52% 35 51.79% 36.65% 42.92
Table 6: Average Precision, Recall and F?=1 measure for English, Telugu and Hindi ?n? indicates the number
of n-gram characters
Size 10K 20K 30K 35K
Model P(%) R(%) F?=1 P(%) R(%) F?=1 P(%) R(%) F?=1 P(%) R(%) F?=1
words 58.04 8.46 14.77 56.54 14.06 22.52 67.90 21.48 32.64 71.03 23.31 35.1
n=2 53.81 13.80 21.97 60.31 25.52 35.86 63.68 31.51 42.16 65.16 35.55 46
n=3 68.07 14.71 24.2 64.71 24.35 35.38 70.22 32.55 44.48 71.79 37.11 48.93
n=4 71.23 13.54 22.76 63.42 21.22 31.8 68.14 28.12 39.82 68.16 31.77 43.34
n=4 69.92 11.20 19.3 61.20 19.92 30.06 63.90 26.04 37 66.96 29.30 40.76
n=6 52.38 8.59 14.77 52.70 16.54 25.17 56.13 22.66 32.28 55.16 24.35 33.79
Table 7: Effect of training data size on Average Precision,Recall and F?=1 measure for Telugu.
Size 10K 20K 30K 35K
Model P(%) R(%) F?=1 P(%) R(%) F?=1 P(%) R(%) F?=1 P(%) R(%) F?=1
words 81.84 30.79 44.75 86.54 40.93 55.57 89.04 45.95 60.62 89.80 46.35 61.14
n=2 71.49 42.00 52.92 74.80 58.40 65.59 75.46 61.03 67.49 76.63 61.87 68.46
n=3 76.09 28.85 41.84 81.15 50.03 61.9 81.31 54.28 65.11 82.18 56.84 67.2
n=4 83.42 25.75 39.36 83.35 42.93 56.67 88.01 48.70 62.7 87.40 50.25 63.81
n=5 81.95 25.64 39.06 84.48 41.00 55.21 86.81 44.47 58.81 88.07 47.43 61.66
n=6 79.24 26.89 40.16 83.31 38.18 52.36 89.34 42.88 57.95 87.71 44.32 58.88
Table 8: Effect of training data size on Average Precision,Recall and F?=1 measure for English.
Size 10K 20K 30K 35K
Model P(%) R(%) F?=1 P(%) R(%) F?=1 P(%) R(%) F?=1 P(%) R(%) F?=1
words 43.13 30.60 35.80 47.97 34.50 40.14 48.67 35.67 41.17 51.92 36.84 43.10
n=2 39.29 30.41 34.29 40.73 34.70 37.47 37.58 36.26 36.90 37.91 36.06 36.96
n=3 48.17 33.33 39.40 50.56 35.28 41.56 47.72 36.65 41.46 50.68 36.06 42.14
n=4 49.18 35.09 40.96 49.21 36.26 41.75 52.14 35.67 42.36 54.87 38.40 45.18
n=5 41.08 34.11 37.27 41.93 33.92 37.50 48.72 37.23 42.21 53.12 39.77 45.48
n=6 41.43 31.58 35.84 44.59 33.72 38.40 46.35 35.87 40.44 50.67 36.84 42.66
Table 9: Effect of training data size on Average Precision,Recall and F?=1 measure for Hindi.
English can be built with less amount of data when
we use character based models and for high entropy
languages large amount of training data is necessary
to build a considerably good NER. We are able to
achieve an F-measure (49.62 for Telugu and 45.07
for Hindi) even without any extra features like regu-
72
lar expressions and gazetteer information. The char-
acter based n-gram models have worked well even
with the discriminative models. A total of 9 features
were used in training and testing. We have not used
any of the language dependent resources and any bi-
nary features. To improve the efficiency of the sys-
tem we plan to experiment with language specific
resources like Part Of Speech (POS) Taggers, Chun-
kers, Morphological analyzers.. etc and also include
some regular expressions and gazetteer information.
References
Bogdan Babych and Anthony Hartley. 2003. Improv-
ing machine translation quality with automatic named
entity recognition.
Akshar Bharati, Prakash Rao K, Rajeev Sangal, and
S.M.Bendre. 1998. Basic statistical analysis of cor-
pus and cross comparison among corpora. Tech-
nical report, International Institute of Information
Technology-Hyderabad(IIIT-H).
D. Bikel, S. Miller, R. Schwartz, and R. Weischedel.
1997. Nymble: a high-performance learning name-
finder.
Peter E Brown and Vincent J. Della Pietra. 1992. An
estimate of an upper bound for the entropy of english.
Nancy Chinchor. 1997. Muc-7 named entity task defini-
tion. Technical Report Version 3.5, Science Applica-
tions International Corporation, Fairfax, Virginia.
S. Cucerzan and D. Yarowsky. 1999. Language indepen-
dent named entity recognition combining morphologi-
cal and contextual evidence.
D. Klein, J. Smarr, H. Nguyen, and C. Manning. 2003.
Named entity recognition with character-level models.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In Proc.
18th International Conf. on Machine Learning, pages
282?289. Morgan Kaufmann, San Francisco, CA.
Andrei Mikheev. 1997. Automatic rule induction
for unknown-word guessing. Comput. Linguist.,
23(3):405?423.
Diego Molla, Menno van Zaanen, and Daniel Smith.
2006. Named entity recognition for question answer-
ing. In Proceedings of Australasian Language Tech-
nology Workshop 2006, Sydney, Australia.
Lance Ramshaw and Mitch Marcus. 1995. Text chunk-
ing using transformation-based learning. In David
Yarovsky and Kenneth Church, editors, Proceedings
of the Third Workshop on Very Large Corpora, pages
82?94, Somerset, New Jersey. Association for Compu-
tational Linguistics.
Antonio Toral, Elisa Noguera, Fernando Llopis, and
Rafael Mun?oz. 2005. Improving question answering
using named entity recognition. In Proceedings of the
10th NLDB congress, Lecture notes in Computer Sci-
ence, Alicante, Spain. Springer-Verlag.
Richard Tzong-Han Tsai. 2006. A hybrid approach to
biomedical named entity recognition and semantic role
labeling. In Proceedings of the 2006 Conference of the
North American Chapter of the Association for Com-
putational Linguistics on Human Language Technol-
ogy, pages 243?246, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
N. Wacholder, Y. Ravin, and M. Choi. 1997. Disam-
biguation of proper names in text.
Hanna M. Wallach. 2004. Conditional random fields: An
introduction. Technical Report MS-CIS-04-21, Uni-
versity of Pennsylvania, Department of Computer and
Information Science, University of Pennsylvania.
73
74
Proceedings of the IJCNLP-08 Workshop on NER for South and South East Asian Languages, pages 105?110,
Hyderabad, India, January 2008. c?2008 Asian Federation of Natural Language Processing
Experiments in Telugu NER: A Conditional Random Field Approach
Praneeth M Shishtla, Karthik Gali, Prasad Pingali and Vasudeva Varma
{praneethms,karthikg}@students.iiit.ac.in,{pvvpr,vv}@iiit.ac.in
Language Technologies Research Centre
International Institute of Information Technology
Hyderabad, India
Abstract
Named Entity Recognition(NER) is the task
of identifying and classifying tokens in a
text document into predefined set of classes.
In this paper we show our experiments
with various feature combinations for Tel-
ugu NER. We also observed that the prefix
and suffix information helps a lot in find-
ing the class of the token. We also show
the effect of the training data on the perfor-
mance of the system. The best performing
model gave an F?=1 measure of 44.91. The
language independent features gave an F?=1
measure of 44.89 which is close to F?=1
measure obtained even by including the lan-
guage dependent features.
1 Introduction
The objective of NER is to identify and classify all
tokens in a text document into predefined classes
such as person, organization, location, miscella-
neous. The Named Entity information in a document
is used in many of the language processing tasks.
NER was created as a subtask in Message Under-
standing Conference (MUC) (Chinchor, 1997). This
reflects the importance of NER in the area of Infor-
mation Extraction (IE). NER has many applications
in the areas of Natural Language Processing, Infor-
mation Extraction, Information Retrieval and speech
processing. NER is also used in question answer-
ing systems (Toral et al, 2005; Molla et al, 2006),
and machine translation systems (Babych and Hart-
ley, 2003). It is also a subtask in organizing and re-
trieving biomedical information (Tsai, 2006).
The process of NER consists of two steps
? identification of boundaries of proper nouns.
? classification of these identified proper nouns.
The Named Entities(NEs) should be correctly iden-
tified for their boundaries and later correctly classi-
fied into their class. Recognizing NEs in an English
document can be done easily with a good amount
of accuracy(using the capitalization feature). Indian
Languages are very much different from the English
like languages.
Some challenges in named entity recognition that
are found across various languages are: Many
named entities(NEs) occur rarely in the corpus i.e
they belong to the open class of nouns. Ambiguity
of NEs. Ex Washington can be a person?s name or a
place name. There are many ways of mentioning the
same Named Entity(NE). In case of person names,
Ex: Abdul Kalam, A.P.J.Kalam, Kalam refer to the
same person. And, in case of place names Waran-
gal, WGL both refer to the same location. Named
Entities mostly have initial capital letters. This dis-
criminating feature of NEs can be used to solve the
problem to some extent in English.
Indian Languages have some additional chal-
lenges: We discuss the challenges that are specific
to Telugu. Absence of capitalization. Ex: The con-
densed form of the person name S.R.Shastry is writ-
ten as S.R.S in English and is represented as srs in
Telugu. Agglutinative property of the Indian Lan-
guages makes the identification more difficult. Ag-
glutinative languages such as Turkish or Finnish,
Telugu etc. differ from languages like English in
105
the way lexical forms are generated. Words are
formed by productive affixations of derivational and
inflectional suffixes to roots or stems. For example:
warangal, warangal ki, warangalki, warangallo,
warangal ni etc .. all refer to the place Waran-
gal. where lo, ki, ni are all postpostion markers
in Telugu. All the postpositions get added to the
stem hyderabad. There are many ways of represent-
ing acronyms. The letters in acronyms could be the
English alphabet or the native alphabet. Ex: B.J.P
and BaJaPa both are acronyms of Bharatiya Janata
Party. Telugu has a relatively free word order when
compared with English. The morpohology of Tel-
ugu is very complex. The Named Entity Recogni-
tion algorithm must be able handle most of these
above variations which otherwise are not found in
languages like English. There are not rich and robust
tools for the Indian Languages. For Telugu, though
a Part Of Speech(POS) Tagger for Telugu, is avail-
able, the accuracy is less when compared to English
and Hindi.
2 Problem Statement
NER as sequence labelling task
Named entity recognition (NER) can be modelled
as a sequence labelling task (Lafferty et al, 2001).
Given an input sequence of words W n1 = w1w2w3
...wn, the NER task is to construct a label sequence
Ln1 = l1l2l3 ...ln , where label li either belongs to
the set of predefined classes for named entities or
is none(representing words which are not proper
nouns). The general label sequence ln1 has the high-
est probability of occuring for the word sequence
W n1 among all possible label sequences, that is
L?n1 = argmax {Pr (L
n
1 |W
n
1 ) }
3 Conditional Random Fields
Conditional Random Fields (CRFs) (Wallach, 2004)
are undirected graphical models used to calculate the
conditional probability of values on designated out-
put nodes given values assigned to other designated
input nodes. In the special case in which the output
nodes of the graphical model are linked by edges in a
linear chain, CRFs make a first-order Markov inde-
pendence assumption, and thus can be understood as
conditionally-trained finite state machines(FSMs).
Let o = ? O1,O2,...OT ? be some observed input
data sequence, such as a sequence of words in text
in a document,(the values on n input nodes of the
graphical model). Let S be a set of FSM states, each
of which is associated with a label, l ?L .
Let s = ? s1,s2,... sT ,? be some sequence of states,(the
values on T output nodes). By the Hammersley-
Clifford theorem CRFs define the conditional prob-
ability of a state sequence given an input sequence
to be
P(s|o) =
1
Zo
? exp(
T
?
t=1
?
k
?k fk (st?1,st ,o, t))
where Zo is a normalization factor over all state
sequences, is an arbitrary feature function over its ar-
guments, and ?k is a learned weight for each feature
function. A feature function may, for example, be
defined to have value 0 or 1. Higher ? weights make
their corresponding FSM transitions more likely.
CRFs define the conditional probability of a la-
bel sequence based on total probability over the state
sequences, P(l|o) = ?s:l(s)=l P(s|o) where l(s) is the
sequence of labels corresponding to the labels of the
states in sequence s. Note that the normalization fac-
tor, Zo, (also known in statistical physics as the parti-
tion function) is the sum of the scores of all possible
state sequences,
Zo = ?
s?ST
?exp(
T
?
t=1
?
k
?k fk (st?1,st ,o, t))
and that the number of state sequences is expo-
nential in the input sequence length,T. In arbitrarily-
structure CRFs, calculating the partition function in
closed form is intractable, and approximation meth-
ods such as Gibbs sampling, or loopy belief propa-
gation must be used.
4 Features
There are many types of features used in general
NER systems. Many systems use binary features
i.e. the word-internal features, which indicate the
presence or absence of particular property in the
word. (Mikheev, 1997; Wacholder et al, 1997;
Bikel et al, 1997). Following are examples of
binary features commonly used. All-Caps (IBM),
Internal capitalization (eBay), initial capital (Abdul
Kalam), uncapitalized word (can), 2-digit number
106
(83, 28), 4-digit number (1273, 1984), all digits (8,
31, 1228) etc. The features that correspond to the
capitalization are not applicable to Telugu. We have
not used any binary features in our experiments.
Gazetteers are used to check if a part of the
named entity is present in the gazetteers. We don?t
have proper gazetteers for Telugu.
Lexical features like a sliding window
[w?2,w?1,wo,w1,w2] are used to create a lexi-
cal history view. Prefix and suffix tries were also
used previously(Cucerzan and Yarowsky,1999).
Linguistics features like Part Of Speech, Chunk,
etc are also used.
4.1 Our Features
We donot have a highly accurate Part Of
Speech(POS) tagger. In order to obtain some
POS and chunk information, we ran a POS Tagger
and chunker for telugu (PVS and G, 2007) on the
data. And from that, we used the following features
in our experiments.
Language Independent Features
current token: w0
previous 3 tokens: w?3,w?2,w?1
next 3 tokens: w1,w2,w3
compound feature:w0 w1
compound feature:w?1 w0
prefixes (len=1,2,3,4) of w0: pre0
suffixes (len=1,2,3,4) of w0: su f0
Language Dependent Features
POS of current word: POS0
Chunk of current word: Chunk0
Each feature is capable of providing some infor-
mation about the NE.
The word window helps in using the context in-
formation while guessing the tag of the token. The
prefix and suffix feature to some extent help in cap-
turing the variations that may occur due to aggluti-
nation.
The POS tag feature gives a hint whether the word
is a proper noun. When this is a proper noun it has
a chance of being a NE. The chunk feature helps in
finding the boundary of the NE.
In Indian Languages suffixes and other inflections
get attached to the words increasing the length of the
word and reducing the number of occurences of that
word in the entire corpus. The character n-grams can
capture these variations.
5 Experimental Setup
5.1 Corpus
We conducted the experiments on the developement
data released as a part of NER for South and South-
East Asian Languages (NERSSEAL) Competetion.
The corpus in total consisted of 64026 tokens out
of which 10894 were Named Entities(NEs). We di-
vided the corpus into training and testing sets. The
training set consisted of 46068 tokens out of which
8485 were NEs. The testing set consisted of 17951
tokens out of which 2407 were NEs. The tagset as
mentioned in the release, was based on AUKBC?s
ENAMEX,TIMEX and NAMEX, has the follow-
ing tags: NEP (Person), NED (Designation), NEO
(Organization), NEA (Abbreviation), NEB (Brand),
NETP (Title-Person), NETO (Title-Object), NEL
(Location), NETI (Time), NEN (Number), NEM
(Measure) & NETE (Terms).
5.2 Tagging Scheme
The corpus is tagged using the IOB tagging scheme
(Ramshaw and Marcus, 1995). In this scheme each
line contains a word at the beginning followed by
its tag. The tag encodes the type of named entity
and whether the word is in the beginning or inside
the NE. Empty lines represent sentence(document)
boundaries. An example is given in table 1.
Words tagged with O are outside of named en-
tities and the I-XXX tag is used for words inside a
named entity of type XXX. Whenever two entities
of type XXX are immediately next to each other,
the first word of the second entity will be tagged B-
XXX in order to show that it starts another entity.
This tagging scheme is the IOB scheme originally
put forward by Ramshaw and Marcus (1995).
5.3 Experiments
To evaluate the performance of our Named Entity
Recognizer, we used three standard metrics namely
precision, recall and f-measure. Precision measures
the number of correct Named Entities(NEs) in the
107
Token Named Entity Tag
Swami B-NEP
Vivekananda I-NEP
was O
born O
on O
January B-NETI
, I-NETI
12 I-NETI
in O
Calcutta B-NEL
. O
Table 1: IOB tagging scheme.
machine tagged file over the total number of NEs in
the machine tagged file and the recall measures the
number of correct NEs in the machine tagged file
over the total number of NEs in the golden standard
file while F-measure is the weighted harmonic mean
of precision and recall:
F =
(
? 2+1
)
RP
? 2R+P
with
? = 1
where P is Precision, R is Recall and F is F-measure.
W?n+n: A word window :w?n, w?n+1, .., w?1, w0,
w1, .., wn?1, wn.
POSn: POS nth token.
Chn: Chunk of nth token.
pren: Prefix information of nth token. (prefix
length=1,2,3,4)
su fn: Suffix information of nth token. (suffix
length=1,2,3,4)
The more the features, the better is the perfor-
mance. The inclusion of the word window, prefix
and suffix features have increased the F?=1 mea-
sure significantly. Whenever the suffix feature is
included, the performance of the system increased.
This shows that the system is able to caputure those
agglutinative language variations. We also have ex-
perimented changing the training data size. While
varying the training data size, we have tested the
performance on the same amount of testing data of
17951 tokens.
6 Conclusion & Future Work
The inclusion of prefix and suffix feature helps in
improving the F?=1 measure (also recall) of the sys-
tem. As the size of the training data is increased,
the F?=1 measure is increased. Even without the
language specific information the system is able to
perform well. The suffix feature helped improve the
recall. This is due to the fact that the POS tagger
also uses the same features in predicting the POS
tags. Prefix, suffix and word are three non-linguistic
features that resulted in good performance. We plan
to experiment with the character n-gram approach
(Klein et al, 2003) and include gazetteer informa-
tion.
References
Bogdan Babych and Anthony Hartley. 2003. Improv-
ing machine translation quality with automatic named
entity recognition. In Proceedings of Seventh Inter-
national EAMT Workshop on MT and other language
technology tools, Budapest, Hungary.
Daniel M. Bikel, Scott Miller, Richard Schwartz, and
Ralph Weischedel. 1997. Nymble: a high-
performance learning name-finder. In Proceedings of
the fifth conference on Applied natural language pro-
cessing, pages 194?201, San Francisco, CA, USA.
Morgan Kaufmann Publishers Inc.
Nancy Chinchor. 1997. Muc-7 named entity task defini-
tion. Technical Report Version 3.5, Science Applica-
tions International Corporation, Fairfax, Virginia.
Dan Klein, Joseph Smarr, Huy Nguyen, and Christo-
pher D. Manning. 2003. Named entity recognition
with character-level models. In Proceedings of the
seventh conference on Natural language learning at
HLT-NAACL 2003, pages 180?183, Morristown, NJ,
USA. Association for Computational Linguistics.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In Proc.
18th International Conf. on Machine Learning, pages
282?289. Morgan Kaufmann, San Francisco, CA.
Andrei Mikheev. 1997. Automatic rule induction
for unknown-word guessing. Comput. Linguist.,
23(3):405?423.
108
Features Precision Recall F?=1
Ch0 51.41% 9.19% 15.59
POS0 46.32% 9.52% 15.80
POS0.Ch0 46.63% 9.69% 16.05
W?3+3.Ch0 59.08% 19.50% 29.32
W?3+3.POS0 58.43% 19.61% 29.36
Ch0.pren 53.97% 24.76% 33.95
POS0.pren 53.94% 24.93% 34.10
POS0.Ch0.pren 53.94% 25.32% 34.46
POS0.su fn 47.51% 29.36% 36.29
POS0.Ch0.su fn 48.02% 29.24% 36.35
Ch0.su fn 48.55% 29.13% 36.41
W?3+3.POS0.pren 62.98% 27.45% 38.24
W?3+3.POS0.Ch0.pren 62.95% 27.51% 38.28
W?3+3.Ch0.pren 62.88% 27.62% 38.38
W?3+3.POS0.su fn 60.09% 30.53% 40.49
W?3+3.POS0.Ch0.su fn 59.93% 30.59% 40.50
W?3+3.Ch0.su fn 61.18% 30.81% 40.98
POS0.Ch0.pren.su fn 57.83% 34.57% 43.27
POS0.pren.su fn 57.41% 34.73% 43.28
Ch0.pren.su fn 57.80% 34.68% 43.35
W?3+3.Ch0.pren.su fn 64.12% 34.34% 44.73
W?3+3.POS0.pren.su fn 64.56% 34.29% 44.79
W?3+3.POS0.Ch0.pren.su fn 64.07% 34.57% 44.91
Table 2: Average Precision,Recall and F?=1 measure for different language dependent feature combinations.
Features Precision Recall F?=1
w 57.05% 20.62% 30.29
pre 53.65% 23.87% 33.04
suf 47.75% 29.19% 36.23
w.pre 63.08% 27.56% 38.36
w.suf 60.93% 30.76% 40.88
pre.suf 57.94% 34.96% 43.61
w.pre.suf 64.80% 34.34% 44.89
Table 3: Average Precision,Recall and F?=1 measure for different language independent feature combina-
tions.
Diego Molla, Menno van Zaanen, and Daniel Smith.
2006. Named entity recognition for question answer-
ing. In Proceedings of Australasian Language Tech-
nology Workshop 2006, Sydney, Australia.
Avinesh PVS and Karthik G. 2007. Part-of-speech tag-
ging and chunking using conditional random fields and
transformation based learning. In In Proceedings of
SPSAL-2007 Workshop.
Lance Ramshaw and Mitch Marcus. 1995. Text chunk-
ing using transformation-based learning. In David
Yarovsky and Kenneth Church, editors, Proceedings
of the Third Workshop on Very Large Corpora, pages
82?94, Somerset, New Jersey. Association for Compu-
tational Linguistics.
Antonio Toral, Elisa Noguera, Fernando Llopis, and
Rafael Mun?oz. 2005. Improving question answering
using named entity recognition. In Proceedings of the
10th NLDB congress, Lecture notes in Computer Sci-
ence, Alicante, Spain. Springer-Verlag.
109
Number of Words Precision Recall F?=1
2500 51.37% 9.47% 15.99
5000 64.74% 11.93% 20.15
7500 61.32% 13.50% 22.13
10000 66.88% 23.31% 34.57
12500 63.42% 27.39% 38.26
15000 63.55% 31.26% 41.91
17500 60.58% 30.64% 40.70
20000 58.32% 30.03% 39.64
22500 57.72% 29.75% 39.26
25000 59.33% 29.92% 39.78
27500 60.91% 30.03% 40.23
30000 62.77% 30.42% 40.98
32500 62.66% 30.64% 41.16
35000 62.08% 30.81% 41.18
37500 61.02% 30.87% 41.00
40000 61.60% 31.09% 41.33
42500 62.12% 32.44% 42.62
45000 62.70% 32.77% 43.05
47500 63.20% 32.72% 43.12
50000 64.29% 34.29% 44.72
Table 4: The effect of training data size on the performance of the NER.
Richard Tzong-Han Tsai. 2006. A hybrid approach to
biomedical named entity recognition and semantic role
labeling. In Proceedings of the 2006 Conference of the
North American Chapter of the Association for Com-
putational Linguistics on Human Language Technol-
ogy, pages 243?246, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
Nina Wacholder, Yael Ravin, and Misook Choi. 1997.
Disambiguation of proper names in text. In Proceed-
ings of the fifth conference on Applied natural lan-
guage processing, pages 202?208, San Francisco, CA,
USA. Morgan Kaufmann Publishers Inc.
HannaM.Wallach. 2004. Conditional random fields: An
introduction. Technical Report MS-CIS-04-21, Uni-
versity of Pennsylvania, Department of Computer and
Information Science, University of Pennsylvania.
110
Statistical Transliteration for Cross Langauge Information Retrieval using
HMM alignment and CRF
Surya Ganesh, Sree Harsha
LTRC, IIIT
Hyderabad, India
suryag,sreeharshay@students.iiit.net
Prasad Pingali, Vasudeva Varma
LTRC, IIIT
Hyderabad, India
pvvpr,vv@iiit.net
Abstract
In this paper we present a statistical translit-
eration technique that is language indepen-
dent. This technique uses Hidden Markov
Model (HMM) alignment and Conditional
Random Fields (CRF), a discriminative
model. HMM alignment maximizes the
probability of the observed (source, target)
word pairs using the expectation maximiza-
tion algorithm and then the character level
alignments (n-gram) are set to maximum
posterior predictions of the model. CRF
has efficient training and decoding processes
which is conditioned on both source and
target languages and produces globally op-
timal solutions. We apply this technique
for Hindi-English transliteration task. The
results show that our technique perfoms
better than the existing transliteration sys-
tem which uses HMM alignment and con-
ditional probabilities derived from counting
the alignments.
1 Introduction
In cross language information retrieval (CLIR)
a user issues a query in one language to search
a document collection in a different language.
Out of Vocabulary (OOV) words are problematic
in CLIR. These words are a common source of
errors in CLIR. Most of the query terms are OOV
words like named entities, numbers, acronyms and
technical terms. These words are seldom found in
Bilingual dictionaries used for translation. These
words can be the most important words in the query.
These words need to be transcribed into document
language when query and document languages
do not share common alphabet. The practice of
transcribing a word or text written in one language
into another language is called transliteration.
A source language word can have more than
one valid transliteration in target language. For
example for the Hindi word below four different
transliterations are possible .
- gautam, gautham, gowtam, gowtham
Therefore, in a CLIR context, it becomes im-
portant to generate all possible transliterations to
retrieve documents containing any of the given
forms.
Most current transliteration systems use a gen-
erative model for transliteration such as freely
available GIZA++1 (Och and Ney , 2000),an im-
plementation of the IBM alignment models (Brown
et al, 1993). These systems use GIZA++ (which
uses HMM alignment) to get character level
alignments (n-gram) from word aligned data. The
transliteration system was built by counting up the
alignments and converting the counts to conditional
probabilities. The readers are strongly encouraged
to refer to (Nasreen and Larkey , 2003) to have a
detailed understanding of this technique.
In this paper, we present a simple statistical
technique for transliteration. This technique
uses HMM alignment and Conditional Random
Fields (Hanna , 2004) a discriminative model.
Based on this technique desired number of translit-
erations are generated for a given source language
word. We also describe the Hindi-English transliter-
ation system built by us. However there is nothing
particular to both these languages in the system.
We evaluate the transliteration system on a test
set of proper names from Hindi-English parallel
transliterated word lists. We compare the efficiency
of this system with the system that was developed
using HMMs (Hidden Markov Models) only.
1http://www.fjoch.com/GIZA++.html
2 Previous work
Earlier work in the field of Hindi CLIR was done
by Jaleel and Larkey (Larkey et al, 2003). They did
this based on their work in English-Arabic transliter-
ation for cross language Information retrieval (Nas-
reen and Larkey , 2003). Their approach was
based on HMM using GIZA++ (Och and Ney ,
2000). Prior work in Arabic-English translitera-
tion for machine translation purpose was done by
Arababi (Arbabi et al, 1994). They developed a hy-
brid neural network and knowledge-based system to
generate multiple English spellings for Arabic per-
son names. Knight and Graehl (Knight and Graehl
, 1997) developed a five stage statistical model to
do back transliteration, that is, recover the original
English name from its transliteration into Japanese
Katakana. Stalls and Knight (Stalls and Knight ,
1998) adapted this approach for back translitera-
tion from Arabic to English of English names. Al-
Onaizan and Knight (Onaizan and Knight , 2002)
have produced a simpler Arabic/English translitera-
tor and evaluates how well their system can match a
source spelling. Their work includes an evaluation
of the transliterations in terms of their reasonable-
ness according to human judges. None of these stud-
ies measures their performance on a retrieval task or
on other NLP tasks. Fujii and Ishikawa (Fujii and
Ishikawa , 2001) describe a transliteration system
for English-Japanese cross language IR that requires
some linguistic knowledge. They evaluate the ef-
fectiveness of their system on an English-Japanese
cross language IR task.
3 Problem Description
The problem can be stated formally as a se-
quence labelling problem from one language al-
phabet to other. Consider a source language word
x1x2..xi..xN where each xi is treated as a word
in the observation sequence. Let the equivalent
target language orthography of the same word be
y1y2..yi..yN where each yi is treated as a label in
the label sequence. The task here is to generate a
valid target language word (label suquence) for the
source language word (observation sequence).
x1 ?????? y1
x2 ?????? y2
. ??????- .
. ??????- .
. ??????- .
xN ?????? yN
Here the valid target language alphabet(yi) for a
source language alphabet(xi) in the input source
language word may depend on various factors like
1. The source language alphabet in the input
word.
2. The context(alphabets) surrounding source lan-
guage alphabet(xi) in the input word.
3. The context(alphabets) surrounding target lan-
guage alphabet(yi) in the desired output word.
4 Transliteration using HMM alignment
and CRF
Our approach for transliteration is divided into
two phases. The first phase induces character
alignments over a word-aligned bilingual corpus,
and the second phase uses some statistics over the
alignments to transliterate the source language word
and generate the desired number of target language
words.
The selected statistical model for translitera-
tion is based on HMM alignment and CRF. HMM
alignment maximizes the probability of the observed
(source, target) word pairs using the expectation
maximization algorithm. After the maximization
process is complete, the character level alignments
(n-gram) are set to maximum posterior predictions
of the model. This alignment is used to get char-
acter level alignment (n-gram) of source and target
language words. From the character level alignment
obtained we compare each source language charac-
ter (n-gram) to a word and its corresponding target
language character (n-gram) to a label. Conditional
random fields (CRFs) are a probabilistic framework
for labeling and segmenting sequential data. We use
CRFs to generate target language word (similar to
label sequence) from source language word (similar
to observation sequence).
CRFs are undirected graphical models which
define a conditional distribution over a label
sequence given an observation sequence. We
define CRFs as conditional probability distributions
P (Y |X) of target language words given source
language words. The probability of a particular
target language word Y given source language word
X is the normalized product of potential functions
each of the form
e(
?
j
?jtj(Yi?1,Yi,X,i))+(
?
k
?ksk(Yi,X,i))
where tj(Yi?1, Yi, X, i) is a transition feature
function of the entire source language word and the
target language characters (n-gram) at positions i
and i? 1 in the target language word; sk(Yi, X, i) is
a state feature function of the target language word
at position i and the source language word; and ?j
and ?k are parameters to be estimated from training
data.
Fj(Y,X) =
n?
i=1
fj(Yi?1, Yi, X, i)
where each fj(Yi?1, Yi, X, i) is either a state
function s(Yi?1, Yi, X, i) or a transition function
t(Yi?1, Yi, X, i). This allows the probability of a tar-
get language word Y given a source language word
X to be written as
P (Y |X,?) = (
1
Z(X)
)e(
?
?jFj(Y,X))
Z(X) is a normalization factor.
The parameters of the CRF are usually estimated
from a fully observed training data {(x(k), y(k))}.
The product of the above equation over all training
words, as a function of the parameters ?, is known
as the likelihood, denoted by p({y(k)}|{x(k)}, ?).
Maximum likelihood training chooses parameter
values such that the logarithm of the likelihood,
known as the log-likelihood, is maximized. For a
CRF, the log-likelihood is given by
L(?) =
?
k
[log
1
Z(x(k))
+
?
j
?jFj(y
(k), x(k))]
This function is concave, guaranteeing con-
vergence to the global maximum. Maximum
likelihood parameters must be identified using
an iterative technique such as iterative scal-
ing (Berger , 1997) (Darroch and Ratcliff, 1972)
or gradient-based methods (Wallach , 2002).
Finally after training the model using CRF we gen-
erate desired number of transliterations for a given
source language word.
5 Hindi - English Transliteration system
The whole model has three important phases. Two
of them are off-line processes and the other is a run
time process. The two off-line phases are prepro-
cessing the parallel corpora and training the model
using CRF++2. CRF++ is a simple, customizable,
and open source implementation of Conditional
Random Fields (CRFs) for segmenting/labeling se-
quential data. The on-line phase involves generat-
ing desired number of transliterations for the given
Hindi word (UTF-8 encoded).
5.1 Preprocessing
The training file is converted into a format required
by CRF++. The sequence of steps in preprocessing
are
1. Both Hindi and English words were prefixed
with a begin symbol B and suffixed with an end
symbol E which correspond to start and end
states. English words were converted to lower
case.
2. The training words were segmented in to uni-
grams and the English-Hindi word pairs were
aligned using GIZA++, with English as the
source language and Hindi as target language.
3. The instances in which GIZA++ aligned a se-
quence of English characters to a single Hindi
unicode character were counted. The 50 most
frequent of these character sequences were
added to English symbol inventory. There were
hardly any instances in which a sequence of
Hindi unicode characters were aligned to a sin-
gle English character. So, in our model we con-
sider Hindi unicode characters, NULL, En-
glish unigrams and English n-grams.
4. The English training words were re segmented
based on the new symbol inventory, i.e., if
2http://crfpp.sourceforge.net/
a character was a part of an n-gram, it was
grouped with the other characters in the n-
gram. If not, it was rendered separately.
GIZA++ was used to align the above Hindi
and English training word pairs, with Hindi
as source language and English as target lan-
guage.
These four steps are performed to get the char-
acter level alignment (n-grams) for each source
and target language training words.
5. The alignment file from the GIZA++ output
is used to generate training file as required by
CRF++ to work. In the training file a Hindi uni-
code character aligned to a English uni-gram or
n-gram is called a token. Each token must be
represented in one line, with the columns sepa-
rated by white space (spaces or tabular charac-
ters).Each token should have equal number of
columns.
5.2 Training Phase
The preprocessing phase converts the corpus into
CRF++ input file format. This file is used to
train the CRF model. The training requires a tem-
plate file which specifies the features to be selected
by the model. The training is done using Lim-
ited memory Broyden-Fletcher-Goldfarb-Shannon
method(LBFGS) (Liu and Nocedal, 1989) which
uses quasi-newton algorithm for large scale numer-
ical optimization problem. We used Hindi unicode
characters as features for our model and a window
size of 5.
5.3 Transliteration
The list of Hindi words that need to be translit-
erated is taken. These words are converted into
CRF++ test file format and transliterated using the
trained model which gives the top n probable En-
glish words. CRF++ uses forward Viterbi and back-
ward A* search whose combination produce the ex-
act n-best results.
6 Evaluation
We evaluate the two transliteration systems for
Hindi - English that use HMM alignment and CRF
with the system that uses HMM only in two ways. In
first evaluation method we compare transliteration
accuracies of the two systems using in-corpus (train-
ing data) and out of corpus words. In second method
we compare CLIR performance of the two systems
using Cross Language Evaluation Forum (CLEF)
2007 ad-hoc bilingual track (Hindi-English) docu-
ments in English language and 50 topics in Hindi
Language. The evaluation document set consists of
news articles and reports from Los Angeles Times
of 2002. A set of 50 topics representing the informa-
tion need were given in Hindi. A set of human rele-
vance judgements for these topics were generated by
assessors at CLEF. These relevance judgements are
binary relevance judgements and are decided by a
human assessor after reviewing a set of pooled doc-
uments using the relevant document pooling tech-
nique. The system evaluation framework is similar
to the Craneld style system evaluations and the mea-
sures are similar to those used in TREC3.
6.1 Transliteration accuracy
We trained the model on 30,000 words containing
Indian city names, Indian family names, Male first
names and last names, Female first names and last
names. We compare this model with the HMM
model trained on same training data. We tested both
the models using in-corpus (training data) and out
of corpus words. The out of corpus words consist of
both Indian and foreign place names, person names.
We evaluate both the models by considering top 5,
10, 15 and 20 transliterations. Accuracy was calcu-
lated using the following equation below
Accuracy =
C
N
? 100
C - Number of test words with the correct transliter-
ation appeared in the desired number (5, 10, 15, 20,
25) of transliterations.
N - Total number of test words.
The results for 30,000 in-corpus words and 1,000
out of corpus words are shown in the table 1
and table 2 respectively. In below tables 1 & 2
HMM model refers to the system developed using
HMM alignment and conditional probabilities de-
rived from counting the alignments, HMM & CRF
model refers to the system developed using HMM
3Text Retrieval Conferences, http://trec.nist.gov
Model Top 5 Top 10 Top 15 Top 20 Top 25
HMM 74.2 78.7 81.1 82.1 83.0
HMM & CRF 76.5 83.6 86.5 88.9 89.7
Table 1: Transliteration accuracy of the two systems for in-corpus words.
Model Top 5 Top 10 Top 15 Top 20 Top 25
HMM 69.3 74.3 77.8 80.5 81.3
HMM & CRF 72.1 79.9 83.5 85.6 86.5
Table 2: Transliteration accuracy of the two systems for out of corpus words.
alignment and CRF for generating top n translitera-
tions.
CRF models for Named entity recognition, POS
tagging etc. have efficiency in high nineties when
tested on training data. Here the efficiency (Table 1)
is low due to the use of HMM alignment in GIZA++.
We observe that there is a good improvement in
the efficiency of the system with the increase in the
number of transliterations up to some extent(20) and
after that there is no significant improvement in the
efficiency with the increase in the number of translit-
erations.
During testing, the efficiency was calculated by con-
sidering only one of the correct transliterations pos-
sible for a given Hindi word. If we consider all the
correct transliterations the efficiency will be much
more.
The results clearly show that CRF model per-
forms better than HMM model for Hindi to English
transliteration.
6.2 CLIR Evaluation
In this section we evaluate the transliterations pro-
duced by the two systems in CLIR task, the task for
which these transliteration systems were developed.
We tested the systems on the CLEF 2007 documents
and 50 topics. The topics which contain named enti-
ties are few in number; there were around 15 topics
with them. These topics were used for evaluation of
both the systems.
We developed a basic CLIR system which per-
forms the following steps
1. Tokenizes the Hindi query and removes stop
words.
2. Performs query translation; each Hindi word is
looked up in a Hindi - English dictionary and
all the English meanings for the Hindi word
were added to the translated query and for the
words which were not found in the dictionary,
top 20 transliterations generated by one of the
systems are added to the query.
3. Retrieves relevant documents by giving trans-
lated query to CLEF documents.
We present standard IR evaluation metrics such as
precision, mean average precision(MAP) etc.. in the
table 3 below for the two systems.
The above results show a small improvement in
different IR metrics for the system developed using
HMM alignment and CRF when compared to the
other system. The difference in metrics between the
systems is low because the number of topics tested
and the number of named entities in the tested topics
is low.
7 Future Work
The selected statistical model for transliteration is
based on HMM alignment and CRF. This alignment
model is used to get character level alignment (n-
gram) of source and target language words. The
alignment model uses IBM models, such as Model
4, that resort to heuristic search techniques to ap-
proximate forward-backward and Viterbi inference,
which sacrifice optimality for tractability. So, we
plan to use discriminative model CRF for character
level alignment (Phil and Trevor , 2006) of source
and target language words. The behaviour of the
other discrminative models such as Maximum En-
tropy models etc., towards the transliteration task
Model P10 tot rel tot rel ret MAP bpref
HMM 0.3308 13000 3493 0.1347 0.2687
HMM & CRF 0.4154 13000 3687 0.1499 0.2836
Table 3: IR Evaluation of the two systems.
also needs to be verified.
8 Conclusion
We demonstrated a statistical transliteration sys-
tem using HMM alignment and CRF for CLIR that
works better than using HMMs alone. The following
are our important observations.
1. With the increase in number of output target
language words for a given source language
word the efficiency of the system increases.
2. The difference between efficiencies for top n
and n-5 where n > 5; is decreasing on increas-
ing the n value.
References
A. L. Berger. 1997. The improved iterative scaling algo-
rithm: A gentle introduction.
Al-Onaizan Y, Knight K. 2002. Machine transla-
tion of names in Arabic text. Proceedings of the ACL
conference workshop on computational approaches to
Semitic languages.
Arababi Mansur, Scott M. Fischthal, Vincent C. Cheng,
and Elizabeth Bar. 1994. Algorithms for Arabic name
transliteration. IBM Journal of research and Develop-
ment.
D. C. Liu and J. Nocedal. 1989. On the limited mem-
ory BFGS method for large-scale optimization, Math.
Programming 45 (1989), pp. 503?528.
Fujii Atsushi and Tetsuya Ishikawa. 2001.
Japanese/English Cross-Language Information
Retrieval: Exploration of Query Translation and
Transliteration. Computers and the Humanities,
Vol.35, No.4, pp.389-420.
H. M. Wallach. 2002. Efficient training of condi-
tional random fields. Masters thesis, University of Ed-
inburgh.
Hanna M. Wallach. 2004. Conditional Random Fields:
An Introduction.
J. Darroch and D. Ratcliff. 1972. Generalized iterative
scaling for log-linear models. The Annals of Mathe-
matical Statistics, 43:14701480.
Knight Kevin and Graehl Jonathan. 1997. Machine
transliteration. In Proceedings of the 35th Annual
Meeting of the Association for Computational Linguis-
tics, pp. 128-135. Morgan Kaufmann.
Larkey, Connell,AbdulJaleel. 2003. Hindi CLIR in
Thirty Days.
Nasreen Abdul Jaleel and Leah S. Larkey. 2003. Sta-
tistical Transliteration for English-Arabic Cross Lan-
guage Information Retrieval.
Och Franz Josef and Hermann Ney. 2000. Improved
Statistical Alignment Models. Proc. of the 38th Annual
Meeting of the Association for Computational Linguis-
tics, pp. 440-447, Hong Kong, China.
P. F. Brown, S. A. Della Pietra, and R. L. Mercer.
1993. The mathematics of statistical machine trans-
lation: Parameter estimation. Computational Linguis-
tics, 19(2):263-311.
Phil Blunsom and Trevor Cohn. 2006. Discriminative
Word Alignment with Conditional Random Fields.
Stalls Bonnie Glover and Kevin Knight. 1998. Translat-
ing names and technical terms in Arabic text.
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 105?108,
Suntec, Singapore, 4 August 2009.
c?2009 ACL and AFNLP
Query-Focused Summaries or Query-Biased Summaries ?
Rahul Katragadda
Language Technologies Research Center
IIIT Hyderabad
rahul k@research.iiit.ac.in
Vasudeva Varma
Language Technologies Research Center
IIIT Hyderabad
vv@iiit.ac.in
Abstract
In the context of the Document Understand-
ing Conferences, the task of Query-Focused
Multi-Document Summarization is intended to
improve agreement in content among human-
generated model summaries. Query-focus also
aids the automated summarizers in directing
the summary at specific topics, which may re-
sult in better agreement with these model sum-
maries. However, while query focus corre-
lates with performance, we show that high-
performing automatic systems produce sum-
maries with disproportionally higher query
term density than human summarizers do. Ex-
perimental evidence suggests that automatic
systems heavily rely on query term occurrence
and repetition to achieve good performance.
1 Introduction
The problem of automatically summarizing text doc-
uments has received a lot of attention since the early
work by Luhn (Luhn, 1958). Most of the current auto-
matic summarization systems rely on a sentence extrac-
tive paradigm, where key sentences in the original text
are selected to form the summary based on the clues (or
heuristics), or learning based approaches.
Common approaches for identifying key sentences
include: training a binary classifier (Kupiec et al,
1995), training a Markov model or CRF (Conroy et al,
2004; Shen et al, 2007) or directly assigning weights
to sentences based on a variety of features and heuris-
tically determined feature weights (Toutanova et al,
2007). But, the question of which components and fea-
tures of automatic summarizers contribute most to their
performance has largely remained unanswered (Marcu
and Gerber, 2001), until Nenkova et al (Nenkova et
al., 2006) explored the contribution of frequency based
measures. In this paper, we examine the role a query
plays in automated multi-document summarization of
newswire.
One of the issues studied since the inception of auto-
matic summarization is that of human agreement: dif-
ferent people choose different content for their sum-
maries (Rath et al, 1961; van Halteren and Teufel,
2003; Nenkova et al, 2007). Later, it was as-
sumed (Dang, 2005) that having a question/query to
provide focus would improve agreement between any
two human-generated model summaries, as well as be-
tween a model summary and an automated summary.
Starting in 2005 until 2007, a query-focused multi-
document summarization task was conducted as part of
the annual Document Understanding Conference. This
task models a real-world complex question answering
scenario, where systems need to synthesize from a set
of 25 documents, a brief (250 words), well organized
fluent answer to an information need.
Query-focused summarization is a topic of ongoing
importance within the summarization and question an-
swering communities. Most of the work in this area
has been conducted under the guise of ?query-focused
multi-document summarization?, ?descriptive question
answering?, or even ?complex question answering?.
In this paper, based on structured empirical evalu-
ations, we show that most of the systems participat-
ing in DUC?s Query-Focused Multi-Document Sum-
marization (QF-MDS) task have been query-biased in
building extractive summaries. Throughout our discus-
sion, the term ?query-bias?, with respect to a sentence,
is precisely defined to mean that the sentence has at
least one query term within it. The term ?query-focus?
is less precisely defined, but is related to the cognitive
task of focusing a summary on the query, which we as-
sume humans do naturally. In other words, the human
generated model summaries are assumed to be query-
focused.
Here we first discuss query-biased content in Sum-
mary Content Units (SCUs) in Section 2 and then in
Section 3 by building formal models on query-bias we
discuss why/how automated systems are query-biased
rather than being query-focused.
2 Query-biased content in
Summary Content Units (SCUs)
Summary content units, referred as SCUs hereafter, are
semantically motivated subsentential units that are vari-
able in length but not bigger than a sentential clause.
SCUs are constructed from annotation of a collection
of human summaries on a given document collection.
They are identified by noting information that is re-
peated across summaries. The repetition is as small
as a modifier of a noun phrase or as large as a clause.
The evaluation method that is based on overlapping
SCUs in human and automatic summaries is called the
105
Figure 1: SCU annotation of a source document.
pyramid method (Nenkova et al, 2007).
The University of Ottawa has organized the pyramid
annotation data such that for some of the sentences in
the original document collection, a list of correspond-
ing content units is known (Copeck et al, 2006). A
sample of an SCU mapping from topic D0701A of
the DUC 2007 QF-MDS corpus is shown in Figure 1.
Three sentences are seen in the figure among which
two have been annotated with system IDs and SCU
weights wherever applicable. The first sentence has not
been picked by any of the summarizers participating in
Pyramid Evaluations, hence it is unknown if the sen-
tence would have contributed to any SCU. The second
sentence was picked by 8 summarizers and that sen-
tence contributed to an SCU of weight 3. The third
sentence in the example was picked by one summa-
rizer, however, it did not contribute to any SCU. This
example shows all the three types of sentences avail-
able in the corpus: unknown samples, positive samples
and negative samples.
We extracted the positive and negative samples in the
source documents from these annotations; types of sec-
ond and third sentences shown in Figure 1. A total
of 14.8% sentences were annotated to be either posi-
tive or negative. When we analyzed the positive set,
we found that 84.63% sentences in this set were query-
biased. Also, on the negative sample set, we found that
69.12% sentences were query-biased. That is, on an
average, 76.67% of the sentences picked by any au-
tomated summarizer are query-biased. On the other
hand, for human summaries only 58% sentences were
query-biased. All the above numbers are based on the
DUC 2007 dataset shown in boldface in Table 1
1
.
There is one caveat: The annotated sentences come
only from the summaries of systems that participated in
the pyramid evaluations. Since only 13 among a total
32 participating systems were evaluated using pyramid
evaluations, the dataset is limited. However, despite
this small issue, it is very clear that at least those sys-
tems that participated in pyramid evaluations have been
biased towards query-terms, or at least, they have been
better at correctly identifying important sentences from
the query-biased sentences than from query-unbiased
sentences.
1
We used DUC 2007 dataset for all experiments reported.
3 Formalizing query-bias
Our search for a formal method to capture the relation
between occurrence of query-biased sentences in the
input and in summaries resulted in building binomial
and multinomial model distributions. The distributions
estimated were then used to obtain the likelihood of a
query-biased sentence being emitted into a summary by
each system.
For the DUC 2007 data, there were 45 summaries
for each of the 32 systems (labeled 1-32) among which
2 were baselines (labeled 1 and 2), and 18 summaries
from each of 10 human summarizers (labeled A-J). We
computed the log-likelihood, log(L[summary;p(C
i
)]),
of all human and machine summaries from DUC?07
query focused multi-document summarization task,
based on both distributions described below (see Sec-
tions 3.1, 3.2).
3.1 The binomial model
We represent the set of sentences as a binomial distribu-
tion over type of sentences. Let C
0
and C
1
denote the
sets of sentences without and with query-bias respec-
tively. Let p(C
i
) be the probability of emitting a sen-
tence from a specified set. It is also obvious that query-
biased sentences will be assigned lower emission prob-
abilities, because the occurrence of query-biased sen-
tences in the input is less likely. On average each topic
has 549 sentences, among which 196 contain a query
term; which means only 35.6% sentences in the input
were query-biased. Hence, the likelihood function here
denotes the likelihood of a summary to contain non
query-biased sentences. Humans? and systems? sum-
maries must now constitute low likelihood to show that
they rely on query-bias.
The likelihood of a summary then is :
L[summary; p(C
i
)] =
N !
n
0
!n
1
!
p(C
0
)
n
0
p(C
1
)
n
1
(1)
Where N is the number of sentences in the sum-
mary, and n
0
+ n
1
= N; n
0
and n
1
are the cardinali-
ties of C
0
and C
1
in the summary. Table 2 shows var-
ious systems with their ranks based on ROUGE-2 and
the average log-likelihood scores. The ROUGE (Lin,
2004) suite of metrics are n-gram overlap based met-
rics that have been shown to highly correlate with hu-
man evaluations on content responsiveness. ROUGE-2
and ROUGE-SU4 are the official ROUGE metrics for
evaluating query-focused multi-document summariza-
tion task since DUC 2005.
3.2 The multinomial model
In the previous section (Section 3.1), we described
the binomial model where we classified each sentence
as being query-biased or not. However, if we were
to quantify the amount of query-bias in a sentence,
we associate each sentence to one among k possible
classes leading to a multinomial distribution. Let C
i
?
106
Dataset total positive biased positive negative biased negative % bias in positive % bias in negative
DUC 2005 24831 1480 1127 1912 1063 76.15 55.60
DUC 2006 14747 1047 902 1407 908 86.15 71.64
DUC 2007 12832 924 782 975 674 84.63 69.12
Table 1: Statistical information on counts of query-biased sentences.
ID rank LL ROUGE-2 ID rank LL ROUGE-2 ID rank LL ROUGE-2
1 31 -1.9842 0.06039 J -3.9465 0.13904 24 4 -5.8451 0.11793
C -2.1387 0.15055 E -3.9485 0.13850 9 12 -5.9049 0.10370
16 32 -2.2906 0.03813 10 28 -4.0723 0.07908 14 14 -5.9860 0.10277
27 30 -2.4012 0.06238 21 22 -4.2460 0.08989 5 23 -6.0464 0.08784
6 29 -2.5536 0.07135 G -4.3143 0.13390 4 3 -6.2347 0.11887
12 25 -2.9415 0.08505 25 27 -4.4542 0.08039 20 6 -6.3923 0.10879
I -3.0196 0.13621 B -4.4655 0.13992 29 2 -6.4076 0.12028
11 24 -3.0495 0.08678 19 26 -4.6785 0.08453 3 9 -7.1720 0.10660
28 16 -3.1932 0.09858 26 21 -4.7658 0.08989 8 11 -7.4125 0.10408
2 18 -3.2058 0.09382 23 7 -5.3418 0.10810 17 15 -7.4458 0.10212
D -3.2357 0.17528 30 10 -5.4039 0.10614 13 5 -7.7504 0.11172
H -3.4494 0.13001 7 8 -5.6291 0.10795 32 17 -8.0117 0.09750
A -3.6481 0.13254 18 19 -5.6397 0.09170 22 13 -8.9843 0.10329
F -3.8316 0.13395 15 1 -5.7938 0.12448 31 20 -9.0806 0.09126
Table 2: Rank, Averaged log-likelihood score based on binomial model, true ROUGE-2 score for the summaries
of various systems in DUC?07 query-focused multi-document summarization task.
ID rank LL ROUGE-2 ID rank LL ROUGE-2 ID rank LL ROUGE-2
1 31 -4.6770 0.06039 10 28 -8.5004 0.07908 5 23 -14.3259 0.08784
16 32 -4.7390 0.03813 G -9.5593 0.13390 9 12 -14.4732 0.10370
6 29 -5.4809 0.07135 E -9.6831 0.13850 22 13 -14.8557 0.10329
27 30 -5.5110 0.06238 26 21 -9.7163 0.08989 4 3 -14.9307 0.11887
I -6.7662 0.13621 J -9.8386 0.13904 18 19 -15.0114 0.09170
12 25 -6.8631 0.08505 19 26 -10.3226 0.08453 14 14 -15.4863 0.10277
2 18 -6.9363 0.09382 B -10.4152 0.13992 20 6 -15.8697 0.10879
C -7.2497 0.15055 25 27 -10.7693 0.08039 32 17 -15.9318 0.09750
H -7.6657 0.13001 29 2 -12.7595 0.12028 7 8 -15.9927 0.10795
11 24 -7.8048 0.08678 21 22 -13.1686 0.08989 17 15 -17.3737 0.10212
A -7.8690 0.13254 24 4 -13.2842 0.11793 8 11 -17.4454 0.10408
D -8.0266 0.17528 30 10 -13.3632 0.10614 31 20 -17.5615 0.09126
28 16 -8.0307 0.09858 23 7 -13.7781 0.10810 3 9 -19.0495 0.10660
F -8.2633 0.13395 15 1 -14.2832 0.12448 13 5 -19.3089 0.11172
Table 3: Rank, Averaged log-likelihood score based on multinomial model, true ROUGE-2 score for the sum-
maries of various systems in DUC?07 query-focused multi-document summarization task.
{C
0
, C
1
, C
2
, . . . , C
k
} denote the k levels of query-
bias. C
i
is the set of sentences, each having i query
terms.
The number of sentences participating in each class
varies highly, with C
0
bagging a high percentage of
sentences (64.4%) and the rest {C
1
, C
2
, . . . , C
k
} dis-
tributing among themselves the rest 35.6% sentences.
Since the distribution is highly-skewed, distinguish-
ing systems based on log-likelihood scores using this
model is easier and perhaps more accurate. Like be-
fore, Humans? and systems? summaries must now con-
stitute low likelihood to show that they rely on query-
bias.
The likelihood of a summary then is :
L[summary; p(C
i
)] =
N !
n
0
!n
1
! ? ? ?n
k
!
p(C
0
)
n
0
p(C
1
)
n
1
? ? ? p(C
k
)
n
k
(2)
Where N is the number of sentences in the sum-
mary, and n
0
+ n
1
+ ? ? ? + n
k
= N; n
0
, n
1
,? ? ? ,n
k
are respectively the cardinalities of C
0
, C
1
, ? ? ? ,C
k
,
in the summary. Table 3 shows various systems with
their ranks based on ROUGE-2 and the average log-
likelihood scores.
3.3 Correlation of ROUGE and log-likelihood
scores
Tables 2 and 3 display log-likelihood scores of vari-
ous systems in the descending order of log-likelihood
scores along with their respective ROUGE-2 scores.
We computed the pearson correlation coefficient (?) of
?ROUGE-2 and log-likelihood? and ?ROUGE-SU4 and
log-likelihood?. This was computed for systems (ID: 1-
32) (r1) and for humans (ID: A-J) (r2) separately, and
for both distributions.
For the binomial model, r1 = -0.66 and r2 = 0.39 was
obtained. This clearly indicates that there is a strong
negative correlation between likelihood of occurrence
of a non-query-term and ROUGE-2 score. That is, a
strong positive correlation between likelihood of occur-
107
rence of a query-term and ROUGE-2 score. Similarly,
for human summarizers there is a weak negative cor-
relation between likelihood of occurrence of a query-
term and ROUGE-2 score. The same correlation anal-
ysis applies to ROUGE-SU4 scores: r1 = -0.66 and r2
= 0.38.
Similar analysis with the multinomial model have
been reported in Tables 4 and 5. Tables 4 and 5 show
the correlation among ROUGE-2 and log-likelihood
scores for systems
2
and humans
3
.
? ROUGE-2 ROUGE-SU4
binomial -0.66 -0.66
multinomial -0.73 -0.73
Table 4: Correlation of ROUGE measures with log-
likelihood scores for automated systems
? ROUGE-2 ROUGE-SU4
binomial 0.39 0.38
multinomial 0.15 0.09
Table 5: Correlation of ROUGE measures with log-
likelihood scores for humans
4 Conclusions and Discussion
Our results underscore the differences between human
and machine generated summaries. Based on Sum-
mary Content Unit (SCU) level analysis of query-bias
we argue that most systems are better at finding impor-
tant sentences only from query-biased sentences. More
importantly, we show that on an average, 76.67% of
the sentences picked by any automated summarizer are
query-biased. When asked to produce query-focused
summaries, humans do not rely to the same extent on
the repetition of query terms.
We further confirm based on the likelihood of emit-
ting non query-biased sentence, that there is a strong
(negative) correlation among systems? likelihood score
and ROUGE score, which suggests that systems are
trying to improve performance based on ROUGE met-
rics by being biased towards the query terms. On the
other hand, humans do not rely on query-bias, though
we do not have statistically significant evidence to sug-
gest it. We have also speculated that the multinomial
model helps in better capturing the variance across the
systems since it distinguishes among query-biased sen-
tences by quantifying the amount of query-bias.
From our point of view, most of the extractive sum-
marization algorithms are formalized based on a bag-
of-words query model. The innovation with individ-
ual approaches has been in formulating the actual algo-
rithm on top of the query model. We speculate that
2
All the results in Table 4 are statistically significant with
p-value (p < 0.00004, N=32)
3
None of the results in Table 5 are statistically significant
with p-value (p > 0.265, N=10)
the real difference in human summarizers and auto-
mated summarizers could be in the way a query (or rel-
evance) is represented. Traditional query models from
IR literature have been used in summarization research
thus far, and though some previous work (Amini and
Usunier, 2007) tries to address this issue using con-
textual query expansion, new models to represent the
query is perhaps one way to induce topic-focus on the
summary. IR-like query models, which are designed
to handle ?short keyword queries?, are perhaps not ca-
pable of handling ?an elaborate query? in case of sum-
marization. Since the notion of query-focus is appar-
ently missing in any or all of the algorithms, the future
summarization algorithms must try to incorporate this
while designing new algorithms.
Acknowledgements
We thank Dr Charles L A Clarke at the University of
Waterloo for his deep reviews and discussions on ear-
lier versions of the paper. We are also grateful to all the
anonymous reviewers for their valuable comments.
References
Massih R. Amini and Nicolas Usunier. 2007. A contextual query expansion
approach by term clustering for robust text summarization. In the proceed-
ings of Document Understanding Conference.
John M. Conroy, Judith D. Schlesinger, Jade Goldstein, and Dianne P. O?leary.
2004. Left-brain/right-brain multi-document summarization. In the pro-
ceedings of Document Understanding Conference (DUC) 2004.
Terry Copeck, D Inkpen, Anna Kazantseva, A Kennedy, D Kipp, Vivi Nastase,
and Stan Szpakowicz. 2006. Leveraging duc. In proceedings of DUC
2006.
Hoa Trang Dang. 2005. Overview of duc 2005. In proceedings of Document
Understanding Conference.
Julian Kupiec, Jan Pedersen, and Francine Chen. 1995. A trainable document
summarizer. In the proceedings of ACM SIGIR?95, pages 68?73. ACM.
Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of sum-
maries. In the proceedings of ACL Workshop on Text Summarization
Branches Out. ACL.
H.P. Luhn. 1958. The automatic creation of literature abstracts. In IBM Jour-
nal of Research and Development, Vol. 2, No. 2, pp. 159-165, April 1958.
Daniel Marcu and Laurie Gerber. 2001. An inquiry into the nature of mul-
tidocument abstracts, extracts, and their evaluation. In Proceedings of the
NAACL-2001 Workshop on Automatic Summarization.
Ani Nenkova, Lucy Vanderwende, and Kathleen McKeown. 2006. A compo-
sitional context sensitive multi-document summarizer: exploring the fac-
tors that influence summarization. In SIGIR ?06: Proceedings of the 29th
annual international ACM SIGIR conference on Research and development
in information retrieval, pages 573?580, New York, NY, USA. ACM.
Ani Nenkova, Rebecca Passonneau, and Kathleen McKeown. 2007. The
pyramid method: Incorporating human content selection variation in sum-
marization evaluation. In ACM Trans. Speech Lang. Process., volume 4,
New York, NY, USA. ACM.
G.J. Rath, A. Resnick, and R. Savage. 1961. The formation of abstracts by the
selection of sentences: Part 1: Sentence selection by man and machines. In
Journal of American Documentation., pages 139?208.
Dou Shen, Jian-Tao Sun, Hua Li, Qiang Yang, and Zheng Chen. 2007. Doc-
ument summarization using conditional random fields. In the proceedings
of IJCAI ?07., pages 2862?2867. IJCAI.
Kristina Toutanova, Chris Brockett, Michael Gamon, Jagadeesh Jagarlamundi,
Hisami Suzuki, and Lucy Vanderwende. 2007. The pythy summarization
system: Microsoft research at duc 2007. In the proceedings of Document
Understanding Conference.
Hans van Halteren and Simone Teufel. 2003. Examining the consensus be-
tween human summaries: initial experiments with factoid analysis. In
HLT-NAACL 03 Text summarization workshop, pages 57?64, Morristown,
NJ, USA. Association for Computational Linguistics.
108
Proceedings of CLIAWS3, Third International Cross Lingual Information Access Workshop, pages 46?52,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Sentence Position revisited:
A robust light-weight Update Summarization ?baseline? Algorithm
Rahul Katragadda
rahul k@research.iiit.ac.in
Prasad Pingali
pvvpr@iiit.ac.in
Language Technologies Research Center
IIIT Hyderabad
Vasudeva Varma
vv@iiit.ac.in
Abstract
In this paper, we describe a sentence po-
sition based summarizer that is built based
on a sentence position policy, created from
the evaluation testbed of recent summariza-
tion tasks at Document Understanding Con-
ferences (DUC). We show that the summa-
rizer thus built is able to outperform most sys-
tems participating in task focused summariza-
tion evaluations at Text Analysis Conferences
(TAC) 2008. Our experiments also show that
such a method would perform better at pro-
ducing short summaries (upto 100 words) than
longer summaries. Further, we discuss the
baselines traditionally used for summarization
evaluation and suggest the revival of an old
baseline to suit the current summarization task
at TAC: the Update Summarization task.
1 Introduction
Document summarization received a lot of atten-
tion since an early work by Luhn (1958). Statis-
tical information derived from word frequency and
distribution was used by the machine to compute
a relative measure of significance, first for individ-
ual words and then for sentences. Later, Edmund-
son (1969) introduced four clues for identifying sig-
nificant words (topics) in a text. Among them title
and location are related to position methods, while
the other two are presence of cue words and high
frequency content words. Edmundson assigned pos-
itive weights to sentences according to their ordinal
position in the text, giving more weight to the first
sentence in the first paragraph and last sentence in
the last paragraph.
Position of a sentence in a document or the po-
sition of a word in a sentence give good clues to-
wards importance of the sentence or word respec-
tively. Such features are called locational features,
and a sentence position feature deals with presence
of key sentences at specific locations in the text.
Sentence Position has been well studied in summa-
rization research since its inception, early in Ed-
mundson?s work (1969). Earlier, Baxendale (1958)
investigated a sample of 200 paragraphs to deter-
mine where the important words are most likely to
be found. He concluded that in 85% of the para-
graphs, the first sentence was a topic sentence and in
7% of the paragraphs, the final one.
Recent advances in machine learning have been
adapted to summarization problem through the years
and locational features have been consistently used
to identify salience of a sentence. Some represen-
tative work in ?learning? sentence extraction would
include training a binary classifier (Kupiec et al,
1995), training a Markov model (Conroy et al,
2004), training a CRF (Shen et al, 2007), and learn-
ing pairwise-ranking of sentences (Toutanova et al,
2007).
In recent years, at the Document Understand-
ing Conferences (DUC1), Text Summarization re-
search evolved through task focused evaluations
ranging from ?generic single-document summariza-
tion? to ?query-focused multi-document summariza-
tion (QFMDS)?. The QFMDS task models the real-
world complex question answering task wherein,
given a topic and a set of 25 relevant documents, the
1http://duc.nist.gov/
46
task is to synthesize a fluent, well-organized 250-
word summary of the documents that answers the
question(s) in the topic statement. Recent focus
in the community has been towards query-focused
update-summarization task at DUC and the Text
Analysis Conference (TAC2). The update task was to
produce short (~100 words) multi-document update
summaries of newswire articles under the assump-
tion that the user has already read a set of earlier
articles. The purpose of each update summary will
be to inform the reader of new information about a
particular topic.
The rest of the paper is organized as follows. In
Section 2, we describe a Sub-optimal Position Pol-
icy (SPP) based on Pyramid Annotated Data, then
we derive a simple algorithm for summarization
based on the SPP in Section 3, and show evaluation
results. Next, in Section 4, we explain the current
baselines and evaluation for Multi-Document Sum-
marization and finally in Section 5, we discuss the
need for an older baseline in the current context of
the short summary task of update summarization.
2 Sub-Optimal Sentence Position Policy
Given a large text collection and a way to approxi-
mate the relevance for a reasonably large subset of
sentences, we could identify significant positional
attributes for the genre of the collection. Our ex-
periments are based on the work described in (Lin
and Hovy, 1997), whose experiments using the Ziff-
Davis corpus gave great insights on the selective
power of the position method.
2.1 Sentence Position Yield and Optimal
Position Policy (OPP)
Lin and Hovy (1997) provide an empirical validation
for the position hypothesis. They describe a method
of deriving an Optimal Position Policy for a collec-
tion of texts within a genre, as long as a small set
of topic keywords is defined for each text. They de-
fined sentence yield (strength of relevance) of a sen-
tence based on the mention of topic keywords in the
sentence.
The positional yield is defined as the average sen-
tence yield for that position in the document. They
2http://www.nist.gov/tac/
computed the yield of each sentence position in each
document by counting the number of different key-
words contained in the respective sentence in each
document, and averaging over all documents. An
Optimal Position Policy (OPP) is derived based on
the decreasing values of positional yield.
Their experiments grounded on the assumption
that abstract is an ideal representation of central
topic(s) of a text. For their evaluations, they used
the abstract to compare whether the sentences found
based on their Optimal Position Policy are indeed a
good selection. They used precision-recall measures
to establish those findings.
At our disposal we had data from pyramid eval-
uations that provided sentences and their mapping
to any content units in the gold standard summaries.
The annotations in the data provide a unique prop-
erty that each sentence can derive for itself a score
for relevance.
2.2 Documents
There are a wide variety of document types across
genre. In our case of newswire collection we have
identified two primary types of documents: small
document and large document. This distinction is
made based on the total sentences in the document.
All documents that have the number of sentences
above a threshold should be considered large. We
experimented on thresholds varying from 10 to 35
sentences and figured out that documents? distribu-
tion into the two categories was acceptable when
threshold-ed at 20 sentences. This decision is also
well supported by the fact that the last sentences of
a document were more important than the others in
the middle (Baxendale, 1958).
Sentence Position Yield (SPY) is obtained sep-
arately for both types of documents. For a small
document, sentence positions have values from 1
through 20. Meanwhile, for a large document we
compute SPY for position 1 through 20, then the last
15 sentences labeled 136 through 150 and ?any other
sentence? is labeled 100. It can be seen in figure 3
that sentences that do not come from leading or trail-
ing part of large documents do not contribute much
content to the summaries.
47
Figure 1: A sample mapping of SCU annotation to source document sentences. An excerpt from mapping of topic
D0701A of DUC 2007 QF-MDS task.
Figure 2: Sentence Position Yield for small documents.
2.3 Pyramid Data
Summary content units, referred as SCUs hereafter,
are semantically motivated, sub-sentential units that
are variable in length but not bigger than a sentential
clause. SCUs emerge from annotation of a collec-
tion of human summaries for the same input. They
are identified by noting information that is repeated
across summaries, whether the repetition is as small
as a modifier of a noun phrase or as large as a clause.
The weight an SCU obtains is directly proportional
to the number of reference summaries that support
that piece of information. The evaluation method
that is based on overlapping SCUs in human and
automatic summaries is described in the Pyramid
method (Nenkova et al, 2007).
The University of Ottawa has organized the pyra-
mid annotation data such that for some of the sen-
tences in the original document collection (those
that were picked by systems participating in pyra-
mid evaluation), a list of corresponding content units
is known (Copeck et al, 2006). We used this data to
identify locations in a document from where most
sentences were being picked, and which of those lo-
cations were being most content responsive to the
query.
A sample of SCU mapping is shown in figure 1.
Three sentences are seen in the figure among which
two have been annotated with system IDs and SCU
weights wherever applicable. The first sentence has
not been picked by any of the summarizers partici-
pating in Pyramid Evaluations, hence it is unknown
if the sentence would have contributed to any SCU.
The second sentence was picked by 8 summarizers
and that sentence contributed to an SCU of weight
3. The third sentence in the example was picked
by one summarizer, however, it did not contribute
to any SCU. This example shows all the three types
of sentences available in the corpus: unknown sam-
ples, positive samples and negative samples.
For each SCU, a weight is associated in pyramid
annotations. Thus a sentential score could be de-
fined as sum of weights of all the contributing SCUs
of the sentence. For an unknown sample and a neg-
ative sample, sentential score is 0. For example, in
the second sentence in figure 1 the score is 3, con-
tributed by a single SCU.While the same for the first
and third sentences is 0.
For each sentence position the sentential score is
averaged over all documents, which we call Sen-
tence Position Yield. SPY for small and large doc-
uments is shown in figures 2 and 3. Based on these
values for various positions, a simple Position Pol-
48
Figure 3: Sentence Position Yield for large documents
icy was framed as shown below. A position policy is
an ordered set consisting of elements in the order of
most importance. Within a subset, each sub-element
is equally important and treated likewise.
{s1, S1, {s2, S2, s3} , {S3, s4, s5, s6, s7, s8, s20} ,
{S4, s9} . . . }
In the above position policy, sentences from small
documents and large documents are represented by
si and Sj respectively.
The position policy described above provides an
ordering of ranked sentence positions based on a
very accurate ?relevance? annotations on sentences.
However, there is a large subset of sentences that are
not annotated with either positive or negative rele-
vance judgment. Hence, the policy derived is based
on a high-precision low-recall corpus3 for sentence
relevance. If all the sentences were annotated with
such judgements, the policy could have been differ-
ent. For this reason we call the above derived policy,
a Sub-optimal Position Policy (SPP).
3 SPP as an algorithm
The goal of creating a position policy was to identify
its effectiveness as a summarization algorithm. The
3DUC 2005 and 2006 data has been used for learning the
SPP. In further experiments in section 3, DUC 2007 and TAC
2008 data have been used as test data.
above simple heuristic was easily incorporated as an
algorithm based on simple scoring for each distinct
set in the policy. For instance, based on the policy
above, all s1 get the highest weight followed by next
best weight to all S1 and so on.
As it can be observed, only the first sentence of
each document could end up comprising the sum-
mary. This is okay, till we don?t get redundant infor-
mation in the summary. Hence we also used a sim-
ple unigram match based redundancy measure that
doesn?t allow a sentence if it matches any of the al-
ready selected sentences in at least 40% of content
words in it. We also dis-allow sentences greater than
25 content words.
We applied the above algorithm to generate multi-
document summaries for various tasks. We have ap-
plied it to Query-Focused Multi-Document Summa-
rization (QF-MDS) task of DUC 2007 and Query-
Focused Update Summarization task of TAC 2008.
3.1 Query-Focused Multi-Document
Summarization
The query-focused multi-document summarization
task at DUC models the real world complex ques-
tion answering task. Given a topic and a set of 25
relevant documents, this task is to synthesize a flu-
ent, well-organized 250 word summary of the docu-
ments that answers the question(s) in the topic state-
49
ment/narration.
The summaries from the above algorithm for the
QF-MDS were evaluated based on ROUGE met-
rics (Lin, 2004). The average4 recall scores are re-
ported for ROUGE-2 and ROUGE-SU4 in Table 1.
Also reported are the performance of the top per-
forming system and the official baseline(s). This al-
gorithm performed worse than most systems partic-
ipating in the task that year and performed better5
than only the ?first x words? baseline and 3 other sys-
tems.
system ROUGE-2 ROUGE-SU4
?first x words? baseline 0.06039 0.10507
?generic? baseline 0.09382 0.14641
SPP algorithm 0.06913 0.12492
system 15 (top system) 0.12448 0.17711
Table 1: ROUGE 2, SU4 Recall scores for two base-
lines, the SPP algorithm and a top performing system
at Query-Focused Multi-Document Summarization task,
DUC 2007.
3.2 Update Summarization Task
The update summarization task is to produce short
(~100 words) multi-document update summaries of
newswire articles under the assumption that the user
has already read a set of earlier articles. The initial
document set is called cluster A and the next set of
articles are called cluster B. For cluster A, a query-
focused multi-document summary is expected. The
purpose of each ?update summary? (summary of
cluster B) will be to inform the reader of new in-
formation about a particular topic. Summaries from
the above algorithm for the Query Focused Up-
date Summarization task were evaluated based on
ROUGE metrics. This algorithm performed surpris-
ingly better at this task when compared to QF-MDS.
The rouge scores suggest that this algorithm is well
above the median for cluster A and among the top 5
systems for cluster B.
It must be noted that consistent performance
across clusters (both A and B) shows the robustness
of the ?SPP algorithm? at the update summarization
task. Also, it is evident that such an algorithm is
computationally simple and light-weight.
4Averaged over all the 45 topics of DUC 2007 dataset.
5Better in a statistical sense, based on 95% confidence inter-
vals of the two systems? evaluation based on ROUGE-2.
These surprisingly high scores on ROUGE met-
rics prompted us to evaluate the summaries based on
Pyramid Evaluation (Nenkova et al, 2007). Pyramid
evaluation provides a more semantic approach to
evaluation of content based on SCUs as discussed in
Section 2.3. The average6 modified pyramid scores
of cluster A and cluster B summaries is shown in
Table 2, along with the average recall scores for
ROUGE-2, ROUGE-SU4 scores. The pyramid eval-
uation7 suggests that this algorithm performs better
than all other automated systems at TAC 2008. Ta-
ble 3 shows the average performance (across clus-
ters) of ?first x words? baseline, SPP algorithm and
two top performing systems (System ID=43 and
ID=11). System 43 was adjudged best system based
on ROUGE metrics, and system 11 was top per-
former based on pyramid evaluations at TAC 2008.
ROUGE-2 ROUGE-SU4 pyramid
cluster A 0.08987 0.1213 0.3432
cluster B 0.09319 0.1283 0.3576
Table 2: Cluster wise ROUGE 2, SU4 Recall scores and
modified Pyramid Scores for SPP algorithm at the Update
Summarization task.
3.3 Discussion
It is interesting to observe that the algorithm that
performs very poorly at QF-MDS, does very well
in the Update Summarization task. A possible ex-
planation for such behavior could be based on sum-
mary length. For a 250 word summary in the QF-
MDS task, human summaries might provide a de-
scriptive answer to the query that includes informa-
tion nuggets accompanied by background informa-
tion. Indeed, it has been earlier reported that humans
appreciate receiving more information than just the
answer to the query, whenever possible (Lin et al,
2003; Bosma, 2005).
Whereas, in the case of Update Summarization
task the summary length is only 100 words. In such
a short length humans need to trade-off between an-
swer sentences and supporting sentences, and usu-
ally answers are preferred. And since our method
6Averaged over all the 48 topics of TAC 2008 dataset.
7Pyramid Annotation were done by a volunteer who also
volunteered for annotations during DUC 2007.
50
system ROUGE-2 ROUGE-SU4 pyramid
?first x words? baseline 0.05896 0.09327 0.166
SPP algorithm 0.09153 0.1245 0.3504
System 43 (top in ROUGE) 0.10395 0.13646 0.289
System 11 (top in pyramid) 0.08858 0.12484 0.336
Table 3: Average ROUGE 2, SU4 Recall scores and modified Pyramid Scores for baseline, SPP algorithm and two top
performing systems at TAC 2008.
identifies sentences that are known to be contribut-
ing towards the needed answers, it performs better
at the shorter version of the task.
Another possible explanation is that as a shorter
summary length is required, the task of choosing the
most important information becomes more difficult
and no approach works well consistently. Also, it
has often been noted that this baseline is indeed quite
strong for this genre, due to the journalistic conven-
tion for putting the most important part of an article
in the initial paragraphs.
4 Baselines in Summarization Tasks
Over the years, as summarization research followed
trends from generic single-document summariza-
tion, to generic multi-document summarization, to
focused multi-document summarization there were
two major baselines that stayed throughout the eval-
uations. Those two baselines are:
1. First N words of the document (or of the most re-
cent document).
2. First sentence from each document in chronological
order until the length requirement is reached.
The first baseline was in place ever since the first
evaluation of generic single document summariza-
tion took place in DUC 2001. For multi-document
summarization, first N words of the most recent
document (chronologically) was chosen as the base-
line 1. In the recent summarization evaluations at
Text Analysis Conference (TAC 2008), where up-
date summarization was evaluated; baseline 1 still
persists. This baseline performs pretty poorly at con-
tent evaluations based on all manual and automatic
metrics. However, since it doesn?t disturb the orig-
inal flow and ordering of a document, linguistically
these summaries are the best. Indeed it outperforms
all the automated systems based on linguistic quality
evaluations.
The second baseline had been used occasionally
with multi-document summarization from 2001 to
2004 with both generic multi-document summariza-
tion and focused multi-document summarization. In
2001 only one system significantly outperformed the
baseline 2 (Nenkova, 2005). In 2003 QF-MDS how-
ever, only one system outperformed the baseline 2
above, while in 2004 at the same task, no system
significantly outperforms the baseline. This baseline
as can be seen, over the years has been pretty much
untouched by systems based on content evaluation.
However, the linguistic aspects of summary quality
would be compromised in such a summary.
Currently, for the Update Summarization task at
TAC 2008, NIST?s baseline is the baseline 1 (?first x
words? baseline). And all systems (except one) per-
form better than the baseline in all forms of content
evaluation. Since the task is to generate 100 word
summaries (short summaries), based on past experi-
ences, there is no doubt that baseline 2 would per-
form well.
It is interesting to observe that baseline 2 is a close
approximation to the ?SPP algorithm? described in
this paper. There are two main differences that we
draw between ?baseline 2? and SPP algorithm. First,
?baseline 2? picks only the first sentence in each
document, while ?SPP algorithm? could pick other
sentences in an order described by the position pol-
icy. Second, ?baseline 2? puts no restriction on re-
dundancy, thus due to journalistic conventions entire
summary might be comprised of the same ?informa-
tion nuggets?, wasting the minimal real-estate avail-
able (~100 words). On the other hand, in our ?SPP
algorithm? we consider a simple unigram-overlap
measure to identify redundant information in sen-
tence pairs that avoids redundant nuggets in the final
summary.
51
5 Discussion and Conclusion
Baselines 1 and 2 mentioned above, could together
act as a balancing mechanism to compare for lin-
guistic quality and responsive content in the sum-
mary. The availability of a stronger content respon-
sive summary as a baseline would enable steady
progress in the field. While all the linguistically
motivated systems would compare themselves with
baseline 1, the summary content motivated systems
would compare with the stronger baseline 2 and get
better than it.
Over the years to come, the usage of ?baseline 1?
doesn?t help in understanding whether there has
been significant improvement in the field. This is be-
cause almost every simple algorithm beats the base-
line performance. Having a better baseline, like the
one based on the position hypothesis, would raise
the bar for systems participating in coming years,
and tracking progress of the field over the years is
easier.In this paper, we derived a method to identify a
?sub-optimal position policy? based on pyramid an-
notation data, that were previously unavailable. We
also distinguish small and large documents to obtain
the position policy. We described the Sub-optimal
Sentence Position Policy (SPP) based on pyramid
annotation data and implemented the SPP as an al-
gorithm to show that a position policy thus formed
is a good representative of the genre and thus per-
forms way above median performance. We further
describe the baselines used in summarization evalu-
ation and discuss the need to bring back baseline 2
(or the ?SPP algorithm?) as an official baseline for
update summarization task.
Ultimately, as Lin and Hovy (1997) suggest, the
position method can only take us certain distance. It
has a limited power of resolution (the sentence) and
its limited method of identification (the position in a
text). Which is why we intend to use it as a baseline.
Currently, as we can see the algorithm generates a
generic summary, it doesn?t consider the topic or
query to generate a query-focused summary. In fu-
ture we plan to extend the SPP algorithm with some
basic method for bringing in relevance.
References
P. B. Baxendale. 1958. Machine-made index for tech-
nical literature ? an experiment. IBM Journal of Re-
search and Development, 2(Non-topical Issue).
Wauter Bosma. 2005. Extending answers using dis-
course structures. In Horacio Saggion and J. L. Minel,
editors, RANLP workshop on Crossing Barriers in Text
summarization Research, pages 2?9. Incoma Ltd.
John M. Conroy, Judith D. Schlesinger, Jade Goldstein,
and Dianne P. O?leary. 2004. Left-brain/right-brain
multi-document summarization. In the proceedings of
Document Understanding Conference (DUC) 2004.
Terry Copeck, D Inkpen, Anna Kazantseva, A Kennedy,
D Kipp, Vivi Nastase, and Stan Szpakowicz. 2006.
Leveraging duc. In proceedings of DUC 2006.
H. P. Edmundson. 1969. New methods in automatic ex-
tracting. In Journal of the ACM, volume 16, pages
264?285. ACM.
Julian Kupiec, Jan Pedersen, and Francine Chen. 1995.
A trainable document summarizer. In the proceedings
of ACM SIGIR?95, pages 68?73. ACM.
Chin-Yew Lin and Eduard Hovy. 1997. Identifying top-
ics by position. In Proceedings of the fifth conference
on Applied natural language processing, pages 283?
290. ACL.
Jimmy Lin, Dennis Quan, Vineet Sinha, Karun Bakshi,
David Huynh, Boris Katz, and David R. Karger. 2003.
The role of context in question answering systems. In
the proceedings of CHI?04. ACM.
Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. In the proceedings of ACL
Workshop on Text Summarization Branches Out. ACL.
H.P. Luhn. 1958. The automatic creation of literature ab-
stracts. In IBM Journal of Research and Development,
Vol. 2, No. 2, pp. 159-165, April 1958.
Ani Nenkova, Rebecca Passonneau, and Kathleen McK-
eown. 2007. The pyramid method: Incorporating hu-
man content selection variation in summarization eval-
uation. In ACM Trans. Speech Lang. Process., vol-
ume 4, New York, NY, USA. ACM.
Ani Nenkova. 2005. Automatic text summarization of
newswire: Lessons learned from the document under-
standing conference. In Manuela M. Veloso and Sub-
barao Kambhampati, editors, AAAI, pages 1436?1441.
AAAI Press / The MIT Press.
Dou Shen, Jian-Tao Sun, Hua Li, Qiang Yang, and Zheng
Chen. 2007. Document summarization using condi-
tional random fields. In the proceedings of IJCAI ?07.,
pages 2862?2867. IJCAI.
Kristina Toutanova, Chris Brockett, Michael Gamon, Ja-
gadeesh Jagarlamundi, Hisami Suzuki, and Lucy Van-
derwende. 2007. The pythy summarization system:
Microsoft research at duc 2007. In the proceedings of
Document Understanding Conference 2007.
52
Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 40?43,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
A Language-Independent Transliteration Schema Using Character
Aligned Models At NEWS 2009
Praneeth Shishtla, Surya Ganesh V, Sethuramalingam Subramaniam, Vasudeva Varma
Language Technologies Research Centre,
IIIT-Hyderabad, India
praneethms@students.iiit.ac.in
{suryag,sethu}@research.iiit.ac.in, vv@iiit.ac.in
Abstract
In this paper we present a statistical
transliteration technique that is language
independent. This technique uses statis-
tical alignment models and Conditional
Random Fields (CRF). Statistical align-
ment models maximizes the probability of
the observed (source, target) word pairs
using the expectation maximization algo-
rithm and then the character level align-
ments are set to maximum posterior pre-
dictions of the model. CRF has efficient
training and decoding processes which is
conditioned on both source and target lan-
guages and produces globally optimal so-
lution.
1 Introduction
A significant portion of out-of-vocabulary (OOV)
words in machine translation systems, information
extraction and cross language retrieval models are
named entities (NEs). If the languages are written
in different scripts, these named entities must be
transliterated. Transliteration is defined as the pro-
cess of obtaining the phonetic translation of names
across languages. A source language word can
have more than one valid transliteration in the tar-
get language. In areas like Cross Language Infor-
mation Retrieval (CLIR), it is important to gener-
ate all possible transliterations of a Named Entity.
Most current transliteration systems use a gen-
erative model for transliteration such as freely
available GIZA++1 (Och and Ney , 2000), an
implementation of the IBM alignment mod-
els (Brown et al, 1993) and HMM alignment
model. These systems use GIZA++ to get charac-
ter level alignments from word aligned data. The
1http://www.fjoch.com/GIZA++.html
transliteration system (Nasreen and Larkey , 2003)
is built by counting up the alignments and convert-
ing the counts to conditional probabilities.
In this paper, we describe our participation
in NEWS 2009 Machine Transliteration Shared
Task (Li et al, 2009). We present a simple statis-
tical, language independent technique which uses
statistical alignment models and Conditional Ran-
dom Fields (CRFs) (Hanna , 2004). Using this
technique a desired number of transliterations are
generated for a given word.
2 Previous work
One of the works on Transliteration is done by
Arababi et al (Arababi et. al., 1994). They
model forward transliteration through a combina-
tion of neural net and expert systems. Work in
the field of Indian Language CLIR was done by
Jaleel and Larkey (Larkey et al, 2003). They
did this based on their work in English-Arabic
transliteration for CLIR (Nasreen and Larkey ,
2003). Their approach was based on HMM us-
ing GIZA++ (Och and Ney , 2000). Prior work in
Arabic-English transliteration for machine trans-
lation purpose was done by Arababi (Arbabi et al,
1994). They developed a hybrid neural network
and knowledge-based system to generate multi-
ple English spellings for Arabic person names.
Knight and Graehl (Knight and Graehl , 1997) de-
veloped a five stage statistical model to do back
transliteration, that is, recover the original En-
glish name from its transliteration into Japanese
Katakana. Stalls and Knight (Stalls and Knight ,
1998) adapted this approach for back translitera-
tion from Arabic to English of English names. Al-
Onaizan and Knight (Onaizan and Knight , 2002)
have produced a simpler Arabic/English translit-
erator and evaluates how well their system can
match a source spelling. Their work includes an
40
evaluation of the transliterations in terms of their
reasonableness according to human judges. None
of these studies measures their performance on a
retrieval task or on other NLP tasks. Fujii and
Ishikawa (Fujii and Ishikawa , 2001) describe a
transliteration system for English-Japanese CLIR
that requires some linguistic knowledge. They
evaluate the effectiveness of their system on an
English-Japanese CLIR task.
3 Problem Description
The problem can be stated formally as a se-
quence labeling problem from one language al-
phabet to other. Consider a source language word
x1x2..xi..xN where each xi is treated as a word
in the observation sequence. Let the equivalent
target language orthography of the same word be
y1y2..yi..yN where each yi is treated as a label in
the label sequence. The task here is to generate a
valid target language word (label sequence) for the
source language word (observation sequence).
x1 ?????? y1
x2 ?????? y2
. ??????- .
. ??????- .
. ??????- .
xN ?????? yN
Here the valid target language alphabet (yi) for a
source language alphabet (xi) in the input source
language word may depend on various factors like
1. The source language alphabet in the input
word.
2. The context (alphabets) surrounding source
language alphabet (xi) in the input word.
3. The context (alphabets) surrounding target
language alphabet (yi) in the desired output
word.
4 Transliteration using alignment models
and CRF
Our approach for transliteration is divided
into two phases. The first phase induces
character alignments over a word-aligned
bilingual corpus, and the second phase uses
some statistics over the alignments to translit-
erate the source language word and generate
the desired number of target language words.
The selected statistical model for transliteration
is based on a combination of statistical alignment
models and CRF. The alignment models maximize
the probability of the observed (source, target)
word pairs using the expectation maximization
algorithm. After the maximization process is
complete, the character level alignments are
set to maximum posterior predictions of the
model. This alignment is used to get character
level alignment of source and target language
words. From the character level alignment
obtained we compare each source language
character to a word and its corresponding tar-
get language character to a label. Conditional
random fields (CRFs) are a probabilistic frame-
work for labeling and segmenting sequential
data. We use CRF to generate target language
word (similar to label sequence) from source
language word (similar to observation sequence).
CRFs are undirected graphical models which
define a conditional distribution over a label se-
quence given an observation sequence. We define
CRFs as conditional probability distributions
P (Y |X) of target language words given source
language words. The probability of a particular
target language word Y given source language
word X is the normalized product of potential
functions each of the form
e(
?
j
?jtj(Yi?1,Yi,X,i))+(
?
k
?ksk(Yi,X,i))
where tj(Yi?1, Yi, X, i) is a transition feature
function of the entire source language word and
the target language characters at positions i and
i? 1 in the target language word; sk(Yi, X, i) is a
state feature function of the target language word
at position i and the source language word; and ?j
and ?k are parameters to be estimated from train-
ing data.
Fj(Y,X) =
n?
i=1
fj(Yi?1, Yi, X, i)
where each fj(Yi?1, Yi, X, i) is either a state
function s(Yi?1, Yi, X, i) or a transition function
t(Yi?1, Yi, X, i). This allows the probability of a
target language word Y given a source language
word X to be written as
P (Y |X,?) = (
1
Z(X)
)e(
?
?jFj(Y,X))
Z(X) is a normalization factor.
41
5 Our Transliteration system
The whole model has three important phases. Two
of them are off-line processes and the other is a on-
line process. The two off-line phases are prepro-
cessing the parallel corpora and training the model
using CRF++2 (Lafferty et al, 2001). CRF++ is a
simple, customizable, and open source implemen-
tation of Conditional Random Fields (CRFs) for
segmenting/labeling sequential data. The on-line
phase involves generating desired number of target
language transliterations (UTF-8 encoded) for the
given English input word. In our case, the source
is always an English word. The same system is
used for every language pair which makes it a lan-
guage independent. The target languages consist
of Chinese, Hindi, Kannada Tamil and Russian
words.
5.1 Preprocessing
The training file is converted into a format re-
quired by CRF++. The sequence of steps in pre-
processing are
1. Both source and target language words were
prefixed with a begin symbol B and suffixed
with an end symbol E which correspond to
start and end states. English words were con-
verted to lower case.
2. The training words were segmented in to
unigrams and the source-target word pairs
were aligned using GIZA++ (IBM model1,
HMM alignment model, IBM model3 and
IBM model4).
3. The alignment consist of NULLs on source
language i.e., a target language unigram is
aligned to NULL on the source language.
These NULLs are problematic during on-
line phase (as positions of NULLs are un-
known). So, these NULLs are removed by
appending the target language unigram to the
unigram of its previous alignment. For exam-
ple, the following alignment,
k ? K
NULL ? A
transforms to -
k ? KA
2http://crfpp.sourceforge.net/
So, in the final alignment, the source side al-
ways contains unigrams and the target side
might contain ngrams which depends on al-
phabet size of the languages. These three
steps are performed to get the character level
alignment for each source and target lan-
guage training words.
4. This final alignment is transformed to train-
ing format as required by CRF++ to work.
In the training format, a source language un-
igram aligned to a target language ngram is
called a token. Each token must be repre-
sented in one line, with the columns sepa-
rated by white space (spaces or tabular char-
acters). Each token should have equal num-
ber of columns.
5.2 Training Phase
The preprocessing phase converts the corpus into
CRF++ input file format. This file is used to
train the CRF model. The training requires a tem-
plate file which specifies the features to be selected
by the model. The training is done using Lim-
ited memory Broyden-Fletcher-Goldfarb-Shannon
method (L-BFGS) (Liu and Nocedal, 1989) which
uses quasi-newton algorithm for large scale nu-
merical optimization problem. We used English
characters as features for our model and a window
size of 5.
5.3 Transliteration
For a language pair, the list of English words that
need to be transliterated is taken. These words are
converted into CRF++ test file format and translit-
erated using the trained model which gives the top
n probable English words. CRF++ uses forward
Viterbi and backward A* search whose combina-
tion produces the exact n-best results. This process
is repeated for all the five language pairs.
6 Results
In this section, we present the results of our par-
ticipation in the NEWS-2009 shared task. We
conducted our experiments on five language pairs
namely English-Chinese (Li et al, 2004), English-
{Hindi, Kannada, Tamil, Russian} (Kumaran and
Kellner , 2007). As specified in NEWS 2009 Ma-
chine Transliteration Shared Task (Li et al, 2009),
we submitted our standard runs on all the five lan-
guage pairs. Table 1 shows the results of our sys-
tem.
42
Language Pair Accuracy in top-1 Mean F-score MRR MAPref MAP10 MAPsys
English-Tamil 0.406 0.894 0.542 0.399 0.193 0.193
English-Hindi 0.407 0.877 0.544 0.402 0.195 0.195
English-Russian 0.548 0.916 0.640 0.548 0.210 0.210
English-Chinese 0.493 0.804 0.600 0.493 0.192 0.192
English-Kannada 0.350 0.864 0.482 0.344 0.175 0.175
Table 1: Transliteration results for the language pairs
7 Conclusion
In this paper, we have described our translitera-
tion system build on a discriminative model using
CRF and statistical alignment models. As men-
tioned earlier, our system is language independent
and works on any language pair provided parallel
word lists are available for training in the particu-
lar language pair. The main advantage of our sys-
tem is that we use no language-specific heuristics
in any of our modules and hence it is extensible to
any language-pair with least effort.
References
A. Kumaran, Tobias Kellner. 2007. A generic frame-
work for machine transliteration, Proc. of the 30th
SIGIR.
A. L. Berger. 1997. The improved iterative scaling
algorithm: A gentle introduction.
Arbabi, M. and Fischthal, S. M. and Cheng, V. C. and
Bart, E. 1994. Algorithms for Arabic name translit-
eration, IBM Journal of Research And Development.
Al-Onaizan Y, Knight K. 2002. Machine translation of
names in Arabic text. Proceedings of the ACL con-
ference workshop on computational approaches to
Semitic languages.
Arababi Mansur, Scott M. Fischthal, Vincent C. Cheng,
and Elizabeth Bar. 1994. Algorithms for Arabic
name transliteration. IBM Journal of research and
Development.
D. C. Liu and J. Nocedal. 1989. On the limited memory
BFGS method for large-scale optimization, Math.
Programming 45 (1989), pp. 503?528.
Fujii Atsushi and Tetsuya Ishikawa. 2001.
Japanese/English Cross-Language Information
Retrieval: Exploration of Query Translation and
Transliteration. Computers and the Humanities,
Vol.35, No.4, pp.389-420.
H. M. Wallach. 2002. Efficient training of conditional
random fields. Masters thesis, University of Edin-
burgh.
HannaM.Wallach. 2004. Conditional Random Fields:
An Introduction.
Haizhou Li, A Kumaran, Min Zhang, Vladimir Pervou-
chine. 2009. Whitepaper of NEWS 2009 Machine
Transliteration Shared Task. Proceedings of ACL-
IJCNLP 2009 Named Entities Workshop (NEWS
2009), Singapore.
Haizhou Li, A Kumaran, Vladimir Pervouchine, Min
Zhang. 2009. Report on NEWS 2009 Machine
Transliteration Shared Task. Proceedings of ACL-
IJCNLP 2009 Named Entities Workshop (NEWS
2009), Singapore.
Haizhou Li, Min Zhang, Jian Su. 2004. A joint source
channel model for machine transliteration. Proc. of
the 42nd ACL.
J. Darroch and D. Ratcliff. 1972. Generalized iterative
scaling for log-linear models. The Annals of Mathe-
matical Statistics, 43:14701480.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Condi-
tional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. of
ICML, pp.282-289.
Knight Kevin and Graehl Jonathan. 1997. Machine
transliteration. In Proceedings of the 35th Annual
Meeting of the Association for Computational Lin-
guistics, pp. 128-135. Morgan Kaufmann.
Larkey, Connell,AbdulJaleel. 2003. Hindi CLIR in
Thirty Days.
Nasreen Abdul Jaleel and Leah S. Larkey. 2003.
Statistical Transliteration for English-Arabic Cross
Language Information Retrieval.
Och Franz Josef and Hermann Ney. 2000. Improved
Statistical Alignment Models. Proc. of the 38th An-
nual Meeting of the Association for Computational
Linguistics, pp. 440-447, Hong Kong, China.
P. F. Brown, S. A. Della Pietra, and R. L. Mercer.
1993. The mathematics of statistical machine trans-
lation: Parameter estimation. Computational Lin-
guistics, 19(2):263-311.
Phil Blunsom and Trevor Cohn. 2006. Discriminative
Word Alignment with Conditional Random Fields.
Stalls Bonnie Glover and Kevin Knight. 1998. Trans-
lating names and technical terms in Arabic text.
43
Coling 2010: Poster Volume, pages 597?604,
Beijing, August 2010
Generating Simulated Relevance Feedback: A Prognostic Search
approach
Nithin Kumar M and Vasudeva Varma
Search and Information Extraction Lab,
International Institute of Information Technology Hyderabad,
nithin m@research.iiit.ac.in and vv@iiit.ac.in
Abstract
Implicit relevance feedback has proved
to be a important resource in improv-
ing search accuracy and personalization.
However, researchers who rely on feed-
back data for testing their algorithms or
other personalization related problems are
loomed with problems like unavailabil-
ity of data, staling up of data and so
on. Given these problems, we are mo-
tivated towards creating a synthetic user
relevance feedback data, based on insights
from query log analysis. We call this sim-
ulated feedback. We believe that simu-
lated feedback can be immensely benefi-
cial to web search engine and personaliza-
tion research communities by greatly re-
ducing efforts involved in collecting user
feedback. The benefits from ?Simulated
feedback? are - it is easy to obtain and
also the process of obtaining the feed-
back data is repeatable, customizable and
does not need the interactions of the user.
In this paper, we describe a simple yet
effective approach for creating simulated
feedback. We have evaluated our system
using the clickthrough data of the users
and achieved 77% accuracy in generating
click-through data.
1 Introduction
Implicit relevance feedback serves as a great
source of information about user behaviour and
search context. A lot of research went through
in the recent past in making use of this great pool
of information. Relevance feedback is proven to
significantly improve retrieval performance (Har-
man, 1992; Salton and Buckley, 1990). It has also
been successfully used to improve searching rank-
ing, query expansion, personalization, user pro-
filing et cetera (Steve Fox et al,, 2005; Rocchio,
1999; Xuehua et al, 2005).
Clickthrough data is the most prevalent form of
implicit feedback used by researchers for person-
alization purposes. Click log data provides valu-
able information about the interests, preferences
and semantic search intent of the user (Daniel and
Levinson, 2004; Kelly and Belkin, 2001). Unlike
explicit feedback, clicks logs do not require any
special effort from the user (Rocchio, 1999). It is
collected in the background while the user inter-
acts with the search engine to quench his informa-
tion need. Hence, it is easy and feasible to collect
large amounts of clickthrough data.
However, using clickthrough data has its own
share of problems. Firstly, it is not available for
public or even research communities at large for
reasons like being a potential threat to privacy of
web users. Secondly, it only contains the URLs
of the results that the user clicked and does not
contain the documents that the user has chosen.
Given the dynamic nature of the web, content of
many of the urls is prone to change and in some
cases it might not exist. In other cases, even if
the old expected results remain good resources,
search engines might not retrieve them in response
to queries. It will return near-duplicate pages that
have equivalent content but different URLs. Thus
feedback data may rapidly become stale with new
pages replacing old ones as more approporiate re-
sources. And also, given the rapidly changing
ranking algorithms of web search engines, feed-
597
back data collected from the users becomes out-
dated. Hence researchers who rely on feedback
data either for testing their algorithms or other
personalization related problems are faced with
the problems of non-availability of user feedback
data.
In this paper, we strive to address the above
problems by generating simulated relevance feed-
back using prognostic search techniques. Prog-
nostic search is a process of simulating user?s
search process and emulating their actions,
through preferences captured in their profile. Such
generated feedback can be used for research in
personalization techniques and analyzing person-
alization algorithms and search ranking func-
tions(Harman, 1988). The main advantage with
this system is that we can create data on the fly and
hence not fear of it becoming stale. Since it does
not involve user?s actions, it is feasible to generate
large amounts of data in this way.
2 Contributions and Organization
In this paper, we propose a novel way of creat-
ing simulated feedback. The data thus produced
can be used for evaluating/training personaliza-
tion systems. Using our proposed method, given
a user?s training data, we can produce synthetic
implicit feedback data - simulated feedback data
on the fly. We also propose a novel user browsing
model which extends the high performing cascade
model of (Craswell et al, 2008). Our Patience pa-
rameter can be used to build more complex user
browsing models to bring the whole process of
generating implicit feedback data a step nearer to
the real world mechanisms.
In section 3, we describe our approach to gener-
ate simulated feedback data. In sections 3.2.3 and
3.2.4, we describe the process of browsing results
and generating clicks which form the crux of our
approach. We evaluate our system and prove the
usefulness of it in section 4. Section 5 and 6 give
an account of our experiments and the study of
works related to ours already present in the litera-
ture. We conclude that our proposed approach can
be highly useful in personalization research and
give an account of our future directions in section
7.
Figure 1: System architecture
3 Proposed Approach
Simulated feedback is a new type of feedback
similar to implicit and explicit relevance feedback.
Simulated feedback is created by observing and
analyzing real world search log data. We propose
a two phase process to create simulated relevance
feedback as follows: In phase 1, we process real
world click-through data of a search engine and
build user profiles using the data. In phase 2, we
simulate a user?s search process and emulate their
actions based on their profile. We call this process
as ?Prognostic Search?.
3.1 Creating Profiles
After closely examining and analyzing the seman-
tics of the query log, we have chosen the following
parameters to characterize a user: an anonymous
user-id, perceived relevance threshold, patience,
previous queries issued and search history of the
user.
A user-id is used to distinguish and uniquely
identify each and every user. Perceived relevance
is the relevance estimate of the result according
to the user on examining the title, snippet and the
url of the result. And Perceived relevance thresh-
old is the threshold limit of perceived relevance
of a result for the user to click it. Patience of the
user is the trait which determines the number of
clicks and the depth to which the user examines
the results. We explain the process of comput-
ing a user?s patience parameter in detail in section
3.2.3. We stored the previous queries and clicks
of the user to capture the preferences of the user.
To make use of the search history, we used
the previous queries issued and previous results
clicked by the user. We store the titles and snip-
pets of those results to capture the interests of the
598
user. Here, our aim is to generate implicit rel-
evance feedback which is very close to the real
world data. To generate synthetic relevance feed-
back, we instantiate these parameters with appro-
priate values using real world data.
3.2 Prognostic Search
Prognostic search is simulation of a user?s search
process and emulating their actions based on
their interests and preferences captured in their
user profile. Simulating search process involves
four steps viz., i)Query formulation, ii)Searching,
iii)Browsing results and iv)Generating Clicks.
Each of these processes are explained below.
3.2.1 Query Formulation
Query formulation involves cognitive process
of the user and requires background knowledge
about the user like their interests, preferences and
their knowledge base. It is highly impossible to
capture the cognitive thought process of a user and
emulate their method of generating a query. To
solve this problem, we randomly select a search
session from a user?s history and send all the
queries in it sequentially to the search engine.
This helps us to preserve the inter query rela-
tions that naturally exist between the subsequent
queries in a session.
3.2.2 Searching
This step involves retrieving documents rele-
vant to the query generated in the previous step.
We used yahoo search engine which is very much
similar to the search engine from which the train-
ing data is collected.
3.2.3 Browsing results
In this step, we simulate the manner in which a
user browses the results in the real world. Based
on the observations in (Granka et al, 2004; Filip
and Joachims, 2005), we assume that the user
in the real world follows the browsing model
explained in Algorithm 1. In real world, a user
may follow more complex browsing models,
but presently we have considered this browsing
model to simplify things.
Accordingly, to simulate the browsing process
of the user explained in algorithm 1, we followed
Algorithm 1 User browsing model in real world
Step1: Start browsing with the top-most result.
Step2: Examine title, snippet and URL of the re-
sult.
Step3: Click if the result looks promising.
Step4: If(user has patience) go to step 5, else go
to step 6.
Step5: Select next result and go to step 2.
Step6: Start examining the clicked results.
Step7: If(information need satisfied) end the pro-
cess, else go to step 8.
Step8: Reformulate the query and go to step 1.
the Algorithm 2.
Algorithm 2 Simulated User browsing model
Step 1: Determine the number of results to be
browsed based on patience parameter.
Step 2: Browse the results in increasing order of
their ranks and examine them.
Step 3: Compute the perceived relevance score of
the results.
Step 4: In the same order, generate clicks based
on the perceived relevance scores of the results.
Step 5: If(session has more queries) go to step 6,
else end the process.
Step 6: Select next query in the session and go to
step 1.
Thus based on the patience parameter, we
determine the number of results that the user
browses. In our analysis of query log parame-
ters, we learned that the patience value of a user
can be characterized by the following parameters:
number of clicks per session, maximum rank of
the result clicked in a session, time spent in a ses-
sion, the number of queries issued per second and
the average semantic relevance of the top ten re-
sults of that session to the user. We found out
that the patience of the user is directly propor-
tional to the maximum rank of the result he has
clicked in a session. We also found out that the
number of clicks a user generates is inversely pro-
portional to the number of queries he issues per
second and directly proportional to the amount of
time he spends per session. Thus, a user with
599
more patience tends to examine more search re-
sults and thus generate more clicks based on their
relevance. We explain these dependencies in de-
tail in the experiments section. So in order to learn
the Patience parameter of the user, we devised the
following formula:
Patience = ?? (MR? T ? C ? Sqi)Q (1)
Here MR denotes the average of maximum rank
of the results clicked by the user in a session, T
denotes the average time spent in a session, C is
the average number of clicks in a session and Q
denotes the average number of queries issued per
session and Sqi is the average semantic distance
of the top ten results of the query ?q?i. Here, ???? is
an equalization constant.
3.2.4 Generating clicks
This is the most important step in our simula-
tion process. Typically, a user observes the visual
information viz., title, snippet and the URL of a
result(Joachims et al, 2005). Then based on their
interests, they choose the results relevant to them.
Similarly, we closely examine the results selected
in the previous step and then score them according
to their relevance to the user. We consider the title,
snippet and the page-rank of the result and deter-
mine its relevance to the user known as perceived
relevance score.
We first compute the semantic distance between
the title and snippet of the present result from the
titles and snippets of previously clicked results of
the user. The results already clicked by the user
serve as a knowledge base of the interests and
preferences of the user. Thus, the semantic dis-
tance between the present result and the previous
result gives us an account of the relevance that the
present result carries to the user.
We used latent semantic analysis (LSA) to com-
pute the semantic distance between the results.
LSA does not take the dictionary meaning of the
words as input; it rather extracts the contextual
meaning of the word with respect to all other
words in semantic space(Landauer et al, 2007).
This property of LSA is very much useful in the
present context. A particular word may have a
lot of meanings but we are concerned about only
those meanings of the word which the user inter-
prets, which are captured in the sentences present
in the user?s click history. Hence, we used LSA
to compute the semantic distance between the re-
sults.
We also consider the page-rank of the result,
which has proven to be an important factor in
making the decision of a click. In our study,
we found that for about 89% of the queries with
clicks, the top ranked document has been clicked
and for 56% of the queries second ranked docu-
ment has been clicked. In Figure 3, we show the
click ratio for each of the top ten ranked docu-
ments1. Thereby, we derive that the rank of the
result is also a very important factor in deciding
whether a result has to be clicked or not. We also
consider the distance of the present result from
the previous click of the user. In (Joachims et
al., 2005), it is shown that the user is more bi-
ased to click the result that immediately follows
the result he previously clicked. In our simula-
tion process, if this distance for any result exceeds
10, then we terminate the browsing process and
reformulate the query. We believe that when this
distance exceeds 10, it signifies that the quality of
the results is low and hence can be ignored.
We used the bayesian probabilistic techniques
to calculate the probability of the user clicking a
result based on the above discussed factors. Hence
Click being a Bernoulli variable, we have
P (c/R, q, u) = ?cR,q,u (1? ?R,q,u)1?c (2)
Where ?R,q,u is the probability that user ?u?
clicks the result ?R? for a query ?q?. We model
the probability of a click, P (c/R, q, u) as a joint
probability of P(c,r,Rel,D) where ?r? denotes the
rank of the result, ?Rel? denotes the semantic rel-
evance score of the result to the user ? precisely
to his previous clicks ? and ?D? denotes the dis-
tance of the previous click of the user. We use this
probability of the result as the Perceived relevance
score of the result. Thus, we have:
1In figure 3, we have normalized the clicks statistics with
the number of clicks for top ranked document. So, the click-
ratio for the top ranked document will be 1.
600
Figure 2: Graph showing Precision and Recall of
generating clicks for a particular user
Perceived relevance = P (c/R, q, u) =
P (c/r,Rel,D) ? ln [P (r/c)] +
ln [P (Rel/c)] + ln [P (D/c)] + ln [P (ci+1)] (3)
Here, ?r? denotes the rank of the result, ?Rel?
denotes the perceived relevance of the result to
the user and ?D? denotes the distance of the re-
sult from the user?s previous clicked result. Prior
probablities of each of these factors are calculated
from the data stored in the user profile. We used
Laplace smoothing techniques to deal with zero
probability entries. P (ci+1) is the probability that
the user may click a result after clicking ?i? re-
sults. We also believe that the behaviour of the
user changes with each click he generates in a ses-
sion. Hence we used the factor P (ci+1) in de-
termining the probability of the click2. Then, we
compare this score with the Perceived relevance
threshold of the user and generate the clicks ac-
cordingly.
Computing Perceived Relevance Threshold: Us-
ing the above formula, we generated clicks for
different values of Perceived Relevance Threshold
for a user. Figure 2 show the precision and recall
values of generating clicks for different values of
Perceived Relevance Threshold of a user. Thus,
we plot the accuracy of our system for different
values of Patience Relevance Threshold and ac-
cordingly set the threshold selecting the best val-
ues for precision and recall of the system.
4 Experiments
Clickthrough data is a valuable source of user
information. In our statistical analysis of click-
2We used laplace smoothing technique to negate the ef-
fect of zero probability instances.
Figure 3: Ranks Vs Clicks-ratio
through data, we have found that the page-rank of
a result can highly influence the user to make a
click which can be seen in figure 3.
In our definition of Patience, we termed it as
parameter to denote the depth to which the user
examines the results and the number of clicks he
generates. In equation 1, we show that the pa-
tience value is inversely proportinal to the number
of queries the user issues in a session. To prove
this fact, we made a statistical analysis on the real
world querlogs3. From the graphs shown in fig-
ure 4, it can be clearly seen that the Patience of the
user is inversely proportional to the user?s number
of Queries/sec. These graphs show the influence
of the factor Queries/sec on the number of clicks
the user generates for a query and the maximum
rank clicked by the user in a session. We drew the
graphs averaging the different queries/sec value of
a user in a session for each value of MR and num-
ber of clicks respectively. It is evident that both
the graphs are weakly decreasing functions. Since
maximum rank clicked and the number of clicks
per session directly affect the Patience parameter,
we can say that Queries/sec is inversely propor-
tional to the Patience of the user.
Both the graphs show occasional phases of in-
creasing behaviour which can be attributed to a
variety of reasons. While plotting the graphs, for
a given value of MR/number of clicks, we take
observations from numerous sessions of the user
and average the queries/sec value. Thus, presence
of some outlier values may affect the overall out-
3We performed these experiments on the query log data
of a popular commerical search engine. The data consists of
21 million web queries collected from 650,000 users. The
query log data consists of anonymous id given to the user,
query, the time at which the query was posed, rank of the
clicked URL (if any) and the URL of the document clicked
by the user (if any).
601
Figure 4: Clicks Vs Queries/sec and MR Vs
Queries/sec
put of the graph. It can also be attributed to the
low quality of results that the search engine might
have returned due to various reasons.
5 Related Work
Although simulation-based methods have been
used to test query modification techniques (Har-
man, 1988) or to detect shifts in the interests
of computer users (Mostafa et al, 2003), to our
knowledge not much research went into creat-
ing relevance feedback for web search based on
search simulations.
Searcher simulations were created by White et
al (Mostafa et al, 2003; White et al, 2005), for
evaluating implicit feedback models. The simula-
tions assume the role of a searcher, browsing the
results of a retrieval. It is assumed that the ac-
tual relevant and irrelevant documents for a query
are given. The system creates simulations of
searchers by simulating relevance paths i.e., how
the user would traverse results of retrieval. Dif-
ferent strategies were experimented like, the users
only view relevant/non-relevant information, i.e.,
follow relevant paths from only relevant or only
non-relevant documents, or they view all rele-
vant or all non-relevant information, i.e., follow
all relevance paths from top-ranked relevant doc-
uments or top-ranked non-relevant documents etc.
Their research tries to model only certain phases
of the search process like clicking the results and
to some extent the process of looking and identi-
fying the results to click. It also does not consider
modeling the nature of the searcher in context and
also does not calculate the relevance of a docu-
ment for a user. The search process is not com-
plete without discussing or characterizing the user
that participates in the search and computing the
relevance of a document for a user.
In (Agichtein et al,, 2006), they show that
clickthrough data and other implicit data of a
user can be used to build user models to effec-
tively personalize the search results. Craswell et
al (Craswell et al, 2008) have also done some
good work in this area. They try to model the re-
sults browsing pattern of the user. (Craswell et
al., 2008) brings out the position bias in the user?s
click-decision making process. It provides some
interesting browsing models which can be used in
our prognostic search process. We used the cas-
cade model ? best performing model ? proposed
by them to compare the effectiveness of our ap-
proach.
In our approach, we address some of these is-
sues to improve the reliability of the simulated
feedback and the scalability of the simulations.
We first identify certain parameters that are nat-
ural to the search process on the whole and are
generic to hold well across search engines and
users. Wherever applicable we try to characterize
these parameters as probabilistic distributions, us-
ing large volumes of data from existing search en-
gine clickthrough logs. We then instantiate these
parameters by drawing values from these proba-
bilistic distributions. This ensures that the simu-
lated feedback resembles as closely as possible to
the real world scenario and thus is of high qual-
ity. We can easily run the simulations on large
sets of documents to create large amounts of sim-
ulated feedback, as there are no interventions of a
human to provide any kind of extra information or
relevance information on the document set.
6 Evaluation
In this section, we present the evaluation proce-
dure of our approach. We first collected query
602
Table 1: System Configurations
System Patience Clicks
System1 Random Random
System2 Random Proposed method
System3 Proposed method Proposed method
log data of 60 users using a browser plug-in for
two months. Our query log data consists user-id,
queries and the time at which they are entered, list
of search results ? rank, title, snippet and url of
the result ?4 and the results clicked by the user.
We used 70% of this query log data to build pro-
files of the searchers and the rest of the data is
used for evaluation purpose. Using the rest of the
query log data, we initiated the prognostic search
process giving the queries sequentially in the or-
der given by the user. We compared the simu-
lated clicks with the clicks already generated by
the user. We found that the data generated by us
is 77% accurate and its recall5 value is 68%. We
measured the accuracy of our system as follows.
Accuracy =
No. of simulated clicks clicked by the user
Total no. of results clicked by the user (4)
We also built two more systems which we con-
sidered as the baseline systems. The first system
gives a random value for the patience value of
the user ? random value is used to determine the
number of documents to be browsed during the
prognostic search process ? and random value is
given for the user?s Perceived relevance threshold
parameter. The second system generates the pa-
tience value of the user according to the process
described by us in section 3.2.3 and gives a ran-
dom value for the Perceived relevance threshold
value of the user. Systems built by us can be sum-
marized as shown in table 1:
Figure 5 shows a comparision of the accuracies
of the three systems. Here, we can see that the
4A typical search engine query log does not contain the
snippets of the results and the whole list of search results. It
only contains the link clicked by the user and the rank of that
result.
5Recall is the fraction of results clicked for this query and
simulated successfully
Figure 5: Results comparision
baseline 1 which uses random values for patience
and generating clicks is only 10% accurate in gen-
erating clickthrough data. However, with the ad-
dition of our generating clicks approach to the
baseline 1, the performance increased by 200%.
And the system 3 which uses our proposed models
for both patience and generating clicks generates
77% accurate data which is a 670% improvement
over the baseline 1.
We also performed manual evaluation of our
system. Since manual evaluation requires a lot
of effort, we performed it using 25 judges. We
randomly selected 25 users from our query log
data and used their data to build profiles. Then
we showed the clicks generated by our system to
these users. Based on their judgements, we found
our system to be 79.5% accurate6. Figure 6 shows
the accuracy levels of our system according to dif-
ferent judges. We also studied the reason behind
the increase in accuracy of our system during hu-
man evaluation. We re-examined the clicks gener-
ated by the users and found that the users selected
the results which they have not selected during
their regular search. And the reasons behind these
extra clicks are: they have missed examining these
results or they have already reached their desired
document. Thus it certifies that our system is able
to personalize the results and the perceived rele-
vance technique can be used to re-rank the results
to personalize them.
As the cascade model is the best performing
model in (Craswell et al, 2008), we evaluated our
system on that model for comparision. We found
our system to be 96% accurate. We used the data
collected in our clickthrough logs for evaluating
6We took the average of the accuracies of our system for
each of these judges/users.
603
Figure 6: Accuracy based on human judge evalu-
ation
our system using this model.
7 Conclusion and Future work
In this paper, we proposed Simulated Feedback
based on insights from clickthrough data and us-
ing prognostic search methods to generate feed-
back. There is a lot of scope for interesting fu-
ture directions to the current work. It would be
an interesting experiment to see the use of the
simulated feedback in evaluation of personalized
search algorithms. Consider a personalized search
algorithm, and use it to learn a user model from
existing explicit/implicit feedback data. Learn a
user model using the same algorithm from simu-
lated feedback and compare the results. We plan
to pursue the same in future.
As an extension to the current work, we aim
to improve the web search process especially the
query formulation step with insights from a user
study. We are working towards incorporating
much richer and complex models for query for-
mulation like HMMs etc. Ability of the system to
automatically create query reformulations of the
original when no clicks are found is another in-
teresting future work. We also plan to dig more
information about the user by analysing the query
log data. For example, the difference in the time
between the clicks and the distance between the
clicks can be used to analyze the browsing be-
haviour of the user. These observations can inturn
be used in generation of simulated feedback thus
reducing its gap with real world implicit feedback.
References
Mark Claypool, Phong Lee, Makoto Wased and David Brown. 2001. Implicit
interest indicators. In Intelligent User Interfaces.
Granka L., Joachims J., and Gay G. 2004. Eyetracking analysis of user be-
havior in www search. Conference on Research and Development in In-
formation Retrieval, SIGIR.
Harman D. 1988. Towards interactive query expansion. The 11th Annual
ACM SIGIR Conference on Research and Development in Information
Retrieval, 321-331.
Thorsten Joachims. 2002. Optimizing search engines using clickthrough
data. Proceedings of the eighth ACM SIGKDD international conference
on Knowledge discovery and data mining, 133-142.
Kelly D., and Belkin N.J. 2001. Reading time, scrolling and interaction:
Exploring implicit sources of user preferences for relevance feedback dur-
ing interactive information retrieval. In Proceedings of the 24th Annual
International Conference on Research and Development in Information
Retrieval, SIGIR, 408-409.
Mostafa J., Mukhopadhyay S., and Palakal M. 2003. Simulation studies of
different dimensions of users? interests and their impact on user modelling
and information filtering. Information Retrieval, 199-223.
Filip Radlinski and Thorsten Joachims. 2005. Evaluating the robustness of
learning from implicit feedback. In ICML Workshop on Learning In Web
Search.
Rocchio J.J. 1999. The SMART Retrieval System Experiments in Automatic
Document Processing. Relevance Feedback in Information Retrieval.
Sugiyama K., Hatano K., and Yoshikawa M. 2004. Adaptive web search
based on user profile constructed without any effort from users. In Pro-
ceedings of WWW, 675-684.
Ryen W. White, Ian Ruthven, Joemon M. Jose and C.J van Rijsbergen. 2005.
Evaluating implicit feedback models using searcher simulations. ACM
Transactions on Information Systems,ACM TOIS, 325-361.
Xuehua Shen, Bin Tane and Bin Tan. 2005. Implicit user modeling for per-
sonalized search. ACM Transactions on Information Systems.
Feng Qiu and Junghoo Cho. 2006. Automatic Identification of User interest
for personalized search. In proceedings of WWW.
Steve Fox, Kuldeep Karnawat, Mark Mydland, Susan Dumais and Thomas
White. 2005. Evaluating implicit measures to improve web search. ACM
Transactions on Information Systems, 147-168.
Eugene Agichtein, Eric Brill, Susan Dumais and Robert Ragno. 2006. Learn-
ing user interaction models for predicting web search result preferences.
In proceedings of 29th conference on research and development in infor-
mation retrieval, SIGIR, 3-10.
Thorsten Joachims, Laura Granka and Bing Pan. 2005. Accurately inter-
preting clickthrough data as implicit feedback. In proceedings of 28th
conference on research and development in information retrieval, SIGIR.
Thomas K. Landauer, Danielle S. Mc Namara and Simon Dennis. 2007.
Handbook of Latent Semantic Analysis. Lawrence Erlbaum Associates.
Craswell N., Zoeter O., Taylor M. and Ramsey B. 2008. An experimental
comparision of click position-bias models. In First ACM International
Conference on Web Search and Data Mining WSDM.
Olivier Chapelle and Ya Zhang. 2009. A Dynamic Bayesian Network Click
Model for Web Search Ranking. In proceedings of International World
Wide Web Conference(WWW).
Fan Guo, Chao Liu and Yi-Min Wang. 2009. Efficient Multipl-Click Models
in Web Search. In Second ACM International Conference on Web Search
and Data Mining WSDM.
Harman D. 1992. Relevance feedback revisited. In proceedings of 15th An-
nual International ACM SIGIR Conference on Research and Development
in Information Retrieval, 1-10.
Salton G., and Buckley C. 1990. Improving retrieval performance by rele-
vance feedback. Journal of the American Society for Information Science.
Daniel E. Rose, and Danny Levinson. 2004. Understanding user goals in
Web Search. In proceedings of International World Wide Web Confer-
ence(WWW).
604
 
 
 
Hindi and Telugu to English CLIR using Query Expansion 
 
Prasad Pingali and Vasudeva Varma 
 
Abstract 
 
This paper presents the experiments of Language Technologies Research 
Centre (LTRC) as part of their participation in CLEF2 2007 Indian language 
to English ad-hoc cross language document retrieval task. In this paper we 
discuss our Hindi and Telugu to English CLIR system and the experiments 
using CLEF 2007 dataset. We used a variant of TFIDF algorithm in 
combination with a bilingual lexicon for query translation. We also explored 
the role of a document summary in fielded queries and two different boolean 
formulations of query translations. We find that a hybrid boolean 
formulation using a combination of boolean AND and boolean OR operators 
improves ranking of documents. We also find that simple disjunctive 
combination of translated query keywords results in maximum recall. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 525?529, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
sielers : Feature Analysis and Polarity Classification of Expressions from
Twitter and SMS Data
Harshit Jain, Aditya Mogadala and Vasudeva Varma
Search and Information Extraction Lab, IIIT-H
Hyderabad
India
harshit.jain@research.iiit.ac.in, aditya.m@research.iiit.ac.in,
vv@iiit.ac.in
Abstract
In this paper, we describe our system for the
SemEval-2013 Task 2, Sentiment Analysis in
Twitter. We formed features that take into ac-
count the context of the expression and take a
supervised approach towards subjectivity and
polarity classification. Experiments were per-
formed on the features to find out whether
they were more suited for subjectivity or po-
larity Classification. We tested our model for
sentiment polarity classification on Twitter as
well as SMS chat expressions, analyzed their
F-measure scores and drew some interesting
conclusions from them.
1 Introduction
In recent years there has been a huge growth in pop-
ularity of vaious social media microblogging plat-
forms like Twitter. Users freely share their personal
opinions on various events and entities on these plat-
forms. However, while character constraints make
sure the opinions are short and to the point, they also
contribute to the noisy nature of Twitter data.
The contextual polarity of the phrase in which a
particular instance of a word appears may be quite
different from the word?s prior polarity. Positive
words are used in phrases expressing negative sen-
timents, or vice versa. Also, quite often words that
are positive or negative out of context are neutral in
context, meaning they are not even being used to ex-
press a sentiment. This is evident from the example
of underlined phrase in the following tweet:
Lana Del Rey at Hammersmith Apollo in
May...Very badly want tickets
In a technique with large lexicon of words marked
with their prior polarity, badly would have a negative
score making the whole sentence with negative sen-
timent. Even if we perform phrase-level analysis for
the phrase ?Very badly?, Very only acts as an intensi-
fier for badly and the whole sentence is still marked
negative. It?s only when we look further from the
underlined phrase that we realize that ?Very badly?
in the context of wanting something shows positive
sentiment.
Early work on sentiment analysis is based on
document-level analysis of reviews (Pang, B., and
Lee, L., 2004). This approach isn?t feasible for mi-
croblogging data due to the extremely small size of
individual documents. The results on the effective-
ness of part-of-speech features are mixed. While
most regard POS features helpful in subjectivity
classification (Barbosa, L. and Feng, J., 2010), some
report very insignificant improvement on using them
(Kouloumpis, E., Wilson, T. and Moore, J., 2011).
However, most phrase-level approaches began with
a large lexicon of words marked with their prior po-
larity (Kim, S. M., and Hovy, E., 2004; Hu, M., and
Liu, B, 2004). Wilson, Wiebe and Hoffman (2005)
sought to include contextual polarity in the foray by
using various dependency relation based features for
subjectivity and polarity classification. Our goal is
to perform contextual sentiment polarity classifica-
tion in the domain of noisy expressions from tweets
and SMS messages.
2 Data
We use the annotated Twitter expressions provided
by SemEval-2013 Task 2 (Wilson et al, 2013) or-
525
ganizers for training our model. Each instance of
the data contains an expression and its parent tweet.
There are a total of 24939 tweet expressions in the
training dataset and they are annotated into four
classes:
? Objective: Expressions carrying no opinion by
themselves or even in the context of their parent
tweet.
? Positive: Expressions carrying positive senti-
ment in the context of the parent tweet.
? Negative: Expressions carrying negative senti-
ment in the context of the parent tweet.
? Neutral: Expressions carrying prior subjectiv-
ity but are rendered objective in the context of
their parent tweet.
Two separate lexicons for emoticons and interjec-
tions having non-zero prior polarities were created.
47 Subjective emoticons were extracted from train-
ing data as well as from various popular chat ser-
vices. 212 Subjective interjections were extracted
from training data as well from Wiktionary1.
We test our trained model on two separate test
datasets provided by SemEval-2013 Task 2 organiz-
ers, 1) Twitter expressions and 2) SMS expressions.
2.1 Preprocessing
Data preprocessing consists of three steps: 1) To-
kenization, 2) Part-of-Speech (POS) tagging, and
3) Normalization. For the first two steps we use
Twitter NLP and Part-of-Speech Tagging system
(Gimpel, K., et al, 2011). It is a Tokenizer and
POS Tagger made for Twitter dataset and thus
contains separate POS tags for hash-tags(#), at-
mention(@), URLs and E-Mail addresses(U) and
emoticons(E). The POS Tagger identifies common
abbreviations and tags them accordingly. We use
Twitter NLP and Part-of-Speech Tagging system for
the SMS expressions too due to similar noisy na-
ture of SMS data. For the normalization process,
all upper case letters are converted to lower case,
and instances of repeated characters are replaced
by a repetition of two characters. This is done
1http://en.wiktionary.org/wiki/Category:
English_interjections
so that existing legal words having characters re-
peating two times aren?t harmed. #hash-tags are
stripped of the # character and then treated as a nor-
mal word/phrase, at-mention(@) denote the name of
a person/organization and thus they are treated as
proper noun and since URLs don?t carry any senti-
ment, they are ignored in the expression. We expect
the normalization process to aid in forming better
features and in turn improving the performance of
the system as a whole.
3 Features
We use three types of features for our classification
experiments,
? Phrase Prior Polarity Features
? POS Tag Pattern Features
? Noisy data specific Features
Both Phrase Prior Polarity and POS Tag features are
computed for the expression to be analyzed as well
as, if available, two words 2 before and after the ex-
pression.
3.1 Phrase Prior Polarity Feature
Every expression in the dataset is represented by
its aggregate positive and negative polarity score.
Senti-Wordnet (Baccianella, S., Esuli, A., and Se-
bastiani, F., 2010), Emoticon Lexicon and an Inter-
jection Lexicon are used to calculate these prior po-
larities. Bigrams and trigrams are identified by their
presence in Senti-Wordnet. For each identified un-
igram, bigram or trigram, we compute the mean of
all its subjective wordnet sense scores under the POS
tag assigned to it. If a unigram word isn?t present in
Senti-Wordnet, its stemmed3 form is searched keep-
ing the original POS Tag. We perform negation de-
tection by enabling a flag whenever a word occur-
ring in negation list appears. The negation list con-
sists of words like no, not, never, etc, as well all
words ending with -n?t. Negation words act as po-
larity reversers, for e.g., consider the following ex-
pression:?not so sure?. In a simple bag of words ap-
proach, ?not so sure? wouldn?t be classified as neg-
ative due to the presence of sure. To overcome this,
2The figure of two words was reached empirically upon try-
ing various lengths.
3The stemmer used is Snowball Stemmer for English.
526
prior polarities of all words are reversed on the oc-
currence of a negation word. Some negation words
such as no, not, never, also carry their own negative
score (-1), in case no subjective word is found in the
expression, their individual negative score is added
to the aggregate prior polarity of the expression. Ad-
jectives and adverbs are treated as polarity shifters.
They either shift the prior polarities of nouns and
verbs, or in case of objective nouns and verbs, con-
tribute their own prior polarities to the expression,
e.g., ?exceedingly slow?, ?little truth?, ?amazing
car?, etc.
On encountering any emoticon or interjection in
the expression that is present in our lexicon, its cor-
responding score is added to the aggregate prior po-
larity of the expression.
Finally, both positive and negative prior polarities
of the expression are normalized by the number of
words in the expression after tokenization.
3.2 POS Tag Pattern Feature
Both Tweets and SMS messages are extremely short.
Twitter is a social microblogging platform having
just 140 character space for a tweet while SMS mes-
sages have little word length due to typing con-
straints on a mobile device. All the above factors
contribute to the noisiness of data. Hence, it isn?t
enough to find prior polarities of n-grams occurring
in the expression. We thus formed a heuristic tech-
nique of using POS tag patterns as features. POS tag
patterns carry information regarding POS tags com-
bined with the location of their occurrence in the ex-
pression as a feature. For e.g., the POS tag pattern
for the expression ?not so sure? in the tweet
@thehuwdavies you think the Boro will
beat Swansea? I?m not so sure, Decem-
ber/January is when we implode
will be RRA, where R = Adverb and A = Adjective.
3.3 Noisy data specific Features
Interjections and emoticons are useful indicators of
subjectivity in a sentence. Even if many interjections
or emoticons don?t carry a defininte sentiment polar-
ity, they do indicate that some sort of opinion from
the user is available in the tweet or sms. Some ex-
amples of interjections and emoticons with no fixed
prior polarity are, ?wow?, ?oh my god?, ?:-o?, etc.
4 Experiments and Results
Our goal for these experiments is two-fold. First,
we want to evaluate the effectiveness of our features
when using them for subjectivity classification as
compared to sentiment polarity classification. Sec-
ond, we want to evaluate and compare the perfor-
mance of our learnt model when tested upon Twitter
and SMS expression data. We use Naive Bayes clas-
sifier in Weka (Hall, M., et al, 2009) as the learning
algorithm.
Feature Analysis between Subjectivity and Polar-
ity Classification For our first set of experiments,
we re-label all positive, negative and neutral expres-
sions as subjective for subjectivity classification in
the training dataset. For polarity classification we
remove all objective expressions from the training
dataset and perform 3-way classification between
positive, negative and neutral expressions. In both
cases we perform 10-fold cross validation on the
training dataset. For subjectivity classification we
have 24939 tweet expressions with 15565 objective
and 9374 subjective expressions. Subjective expres-
sions contain 5787 positive, 3131 negative and 456
neutral expressions. Table 1 shows the accuracy of
subjectivity and sentiment polarity classification re-
sults and improvement due to each feature.
It is fairly evident from Table 1 that phrase prior
polarity features are equally important for both sub-
jectivity and sentiment polarity classification. The
same however, doesn?t completely hold true for the
other two feature types. While POS Tag pattern
features provide an improvement of 1.89% in sub-
jectivity classification accuracy, they only provide a
0.64% increase in accuracy in polarity classification.
Many inferences can be drawn from this result and
a deeper analysis is required on POS tag patterns to
prove that this wasn?t a mere aberration. Emoticon
and interjection feature too give lower improvement
in accuracies during sentiment polarity classifica-
tion (0.44%) as compared to subjectivity classifica-
tion (0.83%). This, however, is expected since most
common emoticons and interjections with prior po-
larities are already covered in the total score of the
expression. Thus, the noisy data based binary fea-
tures have significant contribution only when the
emoticons and interjections aren?t present in the lex-
icon. This implies that these binary features only
527
Features Subjectivity Polarity
f1 86.58 72.93
f1 + f2 88.47 73.57
f1 + f2 + f3 89.3 74.01
f1 + f2 + f3 - context 84.38 72.25
f1 : Phrase Prior Polarity Features
f2 : POS Tag Pattern Features
f3 : Noisy Data Specific Features
context : Phrase Prior Polarity and POS Tag
pattern features defined for 2 words
before and after the expression
Table 1: Accuracies for all three features used for
Subjectivity and Sentiment Polarity Classification.
hint towards the expression being subjective. The
context features, i.e., phrase prior polarity and POS
tag pattern features defined for 2 words before and
after the expression also carry more significance dur-
ing subjectivity classification than in sentiment po-
larity classification.
Polarity Classification comparison for Twitter
and SMS expression data For the second set of
experiments comparing the performance of polarity
classification in Twitter expressions and SMS ex-
pressions, we use the polarity classification model
learnt in the above experiment. Tables 2(a) and 2(b)
shows the precision, recall and F-measure scores for
both Twitter and SMS expressions.
The polarity classification accuracies for Twitter
and SMS expressions are 74.76% and 70.82%, re-
spectively. Closer inspection of test data shows that
SMS expressions exhibit more aggressive usage of
abbreviations and slangs and are in general noisier
than Twitter expressions. This is probably due to the
fact that typing on a cellphone is more cumbersome
than on a keyboard. The quantitative distribution of
positive, negative and neutral classes in both datasets
affects the F-measure scores of individual classes.
This is evident from the difference in positive and
negative F-measures of Twitter and SMS expres-
sions data. In both datasets, neutral class F-measure
is extremely low. This is partially expected due to
the low quantity of neutral class expressions in Twit-
ter (160/4435) and SMS (159/2334) data. Still, it
Class Precision Recall F-measure
positive 0.8120 0.8120 0.8120
negative 0.6477 0.7073 0.6762
neutral 0.3333 0.0375 0.0674
(a) Twitter expression data
Class Precision Recall F-measure
positive 0.6823 0.8263 0.7475
negative 0.7520 0.6947 0.7222
neutral 0.0588 0.0063 0.0114
(b) SMS expression data
Table 2: Precision, Recall and F-measure scores for
positive, negative and neutral classes computed on
Twitter and SMS expressions data.
seems that more fine-grained analysis of neutral ex-
pressions is required for better polarity classification
accuracy.
Our method ranks 16th (F-measure: 0.7441) out
of 28 participating systems for Twitter data and 12th
(F-measure: 0.7348) out of 26 participating systems
for SMS data. The best performing system have
0.8893(NRC-Canada) and 0.8837(GUMLTLT) av-
eraged(positive, negative) F-measure score for Twit-
ter and SMS data, respectively.
5 Conclusions
Our experiments on features show that phrase prior
polarity features give good results for both subjec-
tivity and polarity classification. POS tag pattern
features, emoticon and interjection features, on the
other hand, are better suited for subjectivity classi-
fication. A deeper analysis is required and various
relational and dependency features should be iden-
tified and used to improve the performance of po-
larity classification. SMS expressions are noisier in
general than Twitter expressions and thus the polar-
ity classifier gives less accurate results for it. How-
ever, both of these datasets face problems common
to the polarity classifier. More research is needed
with a balanced dataset to understand various under-
lying relational causes for an expression to become
neutral and to further confirm the conclusions of this
paper.
528
References
Baccianella, Stefano, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexical
resource for sentiment analysis and opinion mining.
Proceedings of LREC. Malta.
Barbosa, Luciano, and Junlan Feng. 2010. Robust senti-
ment detection on twitter from biased and noisy data.
Proceedings of Coling. Beijing.
Gimpel, Kevin, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael
Heilman, Dani Yogatama, Jeffrey Flanigan, and Noah
A. Smith. 2011. Part-of-Speech Tagging for Twitter:
Annotation, Features, and Experiments. Proceedings
of ACL 2011.
Hall, Mark, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: an update.
ACM SIGKDD Explorations Newsletter, 11(1), 10?18.
Hu, Minqing, and Bing Liu. 2004. Mining and summa-
rizing customer reviews. KDD-2004.
Kim, Soo-Min, and Eduard Hovy. 2004. Determining
the sentiment of opinions. Coling-2004.
Kouloumpis, Efthymios, Theresa Wilson, and Johanna
Moore. 2011. Twitter Sentiment Analysis: The Good
the Bad and the OMG!. Proceedings of ICWSM.
Barcelona.
Pak, Alexander, and Patrick Paroubek. 2010. Twitter as
a corpus for sentiment analysis and opinion mining.
Proceedings of LREC. Malta.
Pang, Bo, and Lillian Lee. 2004. A sentimental edu-
cation: Sentiment analysis using subjectivity summa-
rization based on minimum cuts. Proceedings of the
ACL.
Theresa Wilson and Zornitsa Kozareva and Preslav
Nakov and Sara Rosenthal and Veselin Stoyanov and
Alan Ritter. SemEval-2013 Task 2: Sentiment Analysis
in Twitter. Proceedings of the International Workshop
on Semantic Evaluation. SemEval ?13. June 2013.
Atlanta, Georgia.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. Proceedings of Human
Language Technology Conference and Conference on
Empirical Methods in Natural Language Processing
(HLT/EMNLP). Vancouver.
529
Language-Independent Context Aware Query Translation using Wikipedia
Rohit Bharadwaj G
Search and Information Extraction Lab
LTRC
IIIT Hyderabad, India
bharadwaj@research.iiit.ac.in
Vasudeva Varma
Search and Information Extraction Lab
LTRC
IIIT Hyderabad, India
vv@iiit.ac.in
Abstract
Cross lingual information access (CLIA) sys-
tems are required to access the large amounts
of multilingual content generated on the world
wide web in the form of blogs, news articles
and documents. In this paper, we discuss our
approach to query formation for CLIA sys-
tems where language resources are replaced
by Wikipedia. We claim that Wikipedia,
with its rich multilingual content and struc-
ture, forms an ideal platform to build a CLIA
system. Our approach is particularly useful
for under-resourced languages, as all the lan-
guages don?t have the resources(tools) with
sufficient accuracies. We propose a context
aware language-independent query formation
method which, with the help of bilingual dic-
tionaries, forms queries in the target language.
Results are encouraging with a precision of
69.75% and thus endorse our claim on using
Wikipedia for building CLIA systems.
1 INTRODUCTION
Cross lingual information access (CLIA) systems
enable users to access the rich multilingual content
that is created on the web daily. Such systems are
vital to bridge the gap between information avail-
able and languages known to the user. Considerable
amount of research has been done on building such
systems but most of them rely heavily on the lan-
guage resources and tools developed. With a con-
stant increase in the number of languages around the
world with their content on the web, CLIA systems
are in need. Language independent approach is par-
ticularly useful for languages that fall into the cat-
egory of under-resourced (African, few Asian lan-
guages), that doesn?t have sufficient resources. In
our approach towards language-independent CLIA
system, we have developed context aware query
translation using Wikipedia. Due to voluntary con-
tribution of millions of users, Wikipedia gathers very
significant amount of updated knowledge and pro-
vides a structured way to access it.
Figure 1: Number of Wikipedia pages(Y-axis) with and
without Inter language link (ILL) to English in each lan-
guage (X-axis)
The statistics in the Figure 1 show that it has
rich multilingual content and is growing indepen-
dent of the presence of English counter part. With
its structurally rich content, it provides an ideal plat-
form to perform cross lingual research. We harness
Wikipedia and its structure to replace the language
specific resources required for CLIA.
Our work is different from existing approaches in
145
Proceedings of the 4th Workshop on Building and Using Comparable Corpora, pages 145?150,
49th Annual Meeting of the Association for Computational Linguistics,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
terms of
? No language resource has been used at any
stage of query translation.
? Wikipedia structure has been fully utilized for
achieving CLIA between English and Hindi,
unlike the existing approaches, especially for
query formation.
We have constructed a bilingual dictionary us-
ing cross lingual links present across the articles of
same topic in different languages. As each word in
the dictionary can have several translations based on
various attributes like context, sense etc, we need
a mechanism to identify the target word accurately
based on the context of the query. To identify
the context of a query, ?Content Words?, that are
built for each Wikipedia article, are used. ?Content
Words? of the article are similar to the tags of the ar-
ticle, that reflects the context of the article in a more
detailed way.
In this paper, we detail our approach in forming
this ?Content Words? and using them to form the
query. Since our approach is language-independent
and context-aware, we used a metric proposed
by (Bharadwaj and Varma, 2011) to evaluate along
with a dicationary-based metric. The system is built
between languages English and Hindi. Hindi is se-
lected as target language because of the availabil-
ity of resources for evaluation. As our approach
is language-independent, it can be used to trans-
late queries between any pair of languages present
in Wikipedia. The remainder of paper is organized
as follows. Section 2 shows the related work. Pro-
posed method is discussed in Section 3. Results and
Discussion are in Section 4. We finally conclude in
Section 5.
2 RELATED WORK
We discuss the related work of the two stages are in-
volved in our system of language-independent con-
text aware query translation,
? Resource building/ collection (Dictionaries in
our case)
? Query formation
Dictionary building can be broadly classified into
two approaches, manual and automatic. At initial
stages, various projects like (Breen, 2004) try to
build dictionaries manually, taking lot of time and
effort. Though manual approaches perform well,
they lag behind when recent vocabulary is consid-
ered. To reduce the effort involved, automatic ex-
traction of dictionaries has been envisioned. The
approach followed by (Kay and Roscheisen, 1999)
and (Brown et al, 1990) were towards statistical ma-
chine translation, that can also be applied to dic-
tionary building. The major requirement for us-
ing statistical methods is the availability of bilin-
gual parallel corpora, that again is limited for under-
resourced languages. Factors like sentence struc-
ture, grammatical differences, availability of lan-
guage resources and the amount of parallel corpus
available further hamper the recall and coverage of
the dictionaries extracted.
After parallel corpora, attempts have been made
to construct bilingual dictionaries using various
types of corpora like comparable corpus (Sadat
et al, 2003) and noisy parallel corpus (Fung and
McKeown, 1997). Though there exist various ap-
proaches, most of them make use of the language
resources. Wikipedia has also been used to mine
dictionaries. (Tyers and Pienaar, 2008), (Erdmann et
al., 2008), (Erdmann et al, 2009) have built bilin-
gual dictionaries using Wikipedia and language re-
sources. We have mined our dictionaries similarly
considering the cross lingual links present. Our ap-
proach to dictionary building is detailed in section 3.
Wikipedia has been used for CLIA at various
stages including query formation. Most recently,
Wikipedia structure has been exploited in (Gaillard
et al, 2010) for query translation and disambigua-
tion. In (Scho?nhofen et al, 2008), Wikipedia has
been exploited at all the stages of building a CLIA
system. We tread the same path of (Scho?nhofen
et al, 2008) in harnessing Wikipedia for dictionary
building and query formation. Similar to them we
extract concept words for each Wikipedia article and
use them to disambiguate and form the query.
For evaluation purposes, we adapted evaluation
measures based on Wikipedia and existing dictio-
naries (Bharadwaj and Varma, 2011). The authors
have proposed a classification based technique, us-
ing Wikipedia article and the inter-language links
146
present between them to classify the sentences as
parallel or non-parallel based on the context of the
sentences rather than at the syntactic level. We adopt
a similar classification based technique and build
feature vectors for classification using Support Vec-
tor Machines (SVM 1) for evaluation.
3 PROPOSED METHOD
The architecture of the system is given in the Fig-
ure 2.
Figure 2: Architecture of the system
The following subsections describe each module
in detail.
3.1 Dictionary Building
Bilingual dictionaries (English-Hindi) are built from
Wikipedia by mining parallel/ near-parallel text
from each structural information like title, infobox,
category and abstract (initial paragraph) of the En-
glish(En) and Hindi(Hi) articles that are connected
with Inter language link (ILL, arrows between En
Wikipedia articles and Hi Wikipedia articles in Fig-
ure 2). The motivation for considering the other
structural information of the Wikipedia article is to
increase vocabulary of the dictionary both in terms
of the number of words and categories of words. Ti-
tles, Infobox and Categories of the article consider
only named entities that are used in the language.
1http://www.cs.cornell.edu/People/tj/
svm_light/
To increase the coverage of the dictionary and also
to include other categories of words (like negations,
quantifiers etc), abstract of the article is considered.
Also the Inter language links between the articles
are assumed to be bi-directional even if they are uni-
directional. An approach similar to (Tyers and Pien-
aar, 2008) is followed to construct dictionaries. The
dictionary is constructed iteratively by using the pre-
viously constructed dictionaries from each structure.
The structural aspects of the article used are
? Title: Titles of the articles linked.
? Infobox: Infobox of the articles that are linked.
? Category: Categories of the articles linked.
? Abstract: The inital paragraph of the articles
linked are considered as the article abstracts
and are used for dictionary building.
A dictionary consists of word and its several pos-
sible translations, scored according to their align-
ment scores. Each structural information is used to
enhance the dictionary built previously. Dictionary
built from titles are used as starting point. As each
English word is mapped to several Hindi words, fil-
tering of words or re-ranking of the words at query
formation is vital. The scoring function used for the
words while building the dictionary is
score(wiE , wjH) =
W iE
?
W jH
W iE
(1)
Where wiE is the ith word in English word list; wjH
is the jth word in Hindi word list; W iE
?
W jH is the
count of co-occurrence of wiE and wjH in the parallel
corpus and; W iE is the count of occurrences of the
word wiE in the corpus.
3.2 Building Content words
The context of each English Wikipedia article Ai is
extracted from the following structural information
of the article.
? Title : Title of the article
? Redirect title : Redirect title of the article, if
present.
147
? Category : Categories of the article that are pre-
defined.
? Subsections : Titles of the different sub-
sections of the article.
? In-links : Meta data present in the links to this
article from other articles in same language.
? Out-links : Meta data of the links that link
the current article to other articles in same lan-
guage.
As these structural attributes are spread across the
article, they help to identify the context (orienta-
tion) of the article in depth when compared with
the Categories of the article. Each structural as-
pect described above have unique content that will
help to identify the context of the article. ?Content
Words? are formed from each of these structural as-
pects. Word count of the words present in each of
the above mentioned attributes are calculated and
are filtered by a threshold to form the context words
of the article. The threshold for filtering has been
calculated by manual tagging with the help of lan-
guage annotators. ?Content Words? for the Hindi
articles are also formed similarly. The formation of
?Content Words? is similar to tagging but is not a
stricly tagging mechanism as we have no constraint
on the number of tags. Category alone can help to
get the context but considering in-links, out-links,
subsections will increase the depth of context words
and will reduce the information lost by tagging the
words.
3.3 Query formation
Query formation of our system depends on the con-
text words built. For an English query (qE) that con-
tains the words wiE (i: 0 to n),
? Build WH of size m, that contains the words re-
turned by the dictionary for each of the words.
? For all words in (qE), extract all the articles aki
(k: 0 to n) with wiE as one of its context word.
? Form the corresponding Hindi set of articles Ah
using the cross lingual link, if present in the En-
glish article set constructed in the above step.
? For each Hindi word wjH (j: 0 to m), add it to
Hindi query (qH) if at least one of the articles
ai (with wjH as its context word) is present in
Ah.
This approach helps to identify the context of the
query as each query is represented by a set of articles
instead of query words, that forms the concepts that
the query can be interpreted to limited to Wikipedia
domain. Queries are translated based on the archi-
tecture described in Figure 2.
4 Results and Discussion
4.1 Evaluation, Dataset and Results
A classification based approach and a dictionary
based approach are employed to calculate the accu-
racy of the queries translated. 400 sentences with
their corresponding translations (English-Hindi)
have been used as test set to evaluate the perfor-
mance of the query formation. The sentence pairs
are provided by FIRE2. These sentences contain all
types of words (Named entities, Verbs etc) and will
be referred to as samples. The English language sen-
tences are used as queries and are translated to Hindi
using the approach described. Before forming the
query, stop words are removed from the English sen-
tence. The query lengths after removing stop words
vary from 2 words to 8 words. The dictionary used
for evaluation is an existing one, Shabdanjali3. In
the following sections, we describe our two evalu-
ation strategies and the performance of our system
using them.
4.1.1 Dictionary based evaluation
Shabdanjali dictionary has been used to evaluate
the translated queries. The evaluation metric is word
overlap, though it is relaxed further. The formula
2http://www.isical.ac.in/ clia/
3Shabdanjali is an open source bilingual dictionary that
is most used between English and Hindi. It is available
at http://ltrc.iiit.ac.in/onlineServices/
Dictionaries/Dict_Frame.html
148
used for calculating the precision is
precision = No.ofCorrectSamplesTotalNumberofSamples (2)
A sample is said to be correct if its overLapScore
is greater than threshold instead of complete over-
lap. The overLapScore of each sample is mea-
sured using Formula 3. Threshold is the average
overLapScore of the positive training set used for
training the classifier (Training dataset is discussed
in Section 4.1.2).
overLapScore = No.ofWordOverlapTotalNumberofWords (3)
The number of word overlaps are measured both
manually and automatically to avoid inconsistent re-
sults due to varios syntactic representation of the
same word in Wikipedia.
The precision for the test dataset using this ap-
proach is 42.8%.
4.1.2 Classification based evaluation
As described in Section 2, we have used a clas-
sification based technique for identifying whether
the translated queries contain the same informa-
tion or not. We have collected 1600 pairs of sen-
tences where 800 sentences are parallel to each
other (positive samples, exact translations) while
the other half have word overlaps, but not paral-
lel, (not exact translations but have similar content)
form the negative samples. Various statistics are
extracted from Wikipedia for each sentence pair to
construct feature vector as described in (Bharad-
waj and Varma, 2011). Each English and Hindi
sentences are queried as bag-of-words query to cor-
responding Wikipedia articles and statistics are ex-
tracted based on the articles retrieved. The classifier
used is SVM and is trained on the feature vectors
generated for 1600 samples. The precision in this
approach is the accuracy of the classifier. The for-
mula used for calculating the accuracy is
accuracy = No.ofSamplesCorrectlyClassifiedTotalNumberofSamples
(4)
The correctness of the sample is the prediction of the
classifier. The precision for the test set is 69.75%.
4.2 Discussion
The precision achieved by classification based eval-
uation is higher than that of existing dictionary
(Shabdanjali) primarily due to
? Dictionary (Shabdanjali) doesn?t contain words
of the query. (Coverage is less).
? Word forms present in the dictionary are differ-
ent to that of words present in translated query.
(Ex: spelling, tense etc).
To negate the effect of above factors, classification
based evaluation ( 4.1.2) has been considered. Clas-
sification based evaluation shows that the results are
better when the entire sentence and its context is
considered. As there are no existing systems that
translate queries based on the context and language
independent, our results are encouraging to work in
this direction. Since no language resources were
used, our approach is scalable and can be applied to
any pair of languages present in Wikipedia. The rel-
atively low coverage of the dictionaries built using
Wikipedia structure also affects the process of query
translation. In future, the coverage of dictionaries
can also be increased by considering other structural
properties of Wikipedia.
5 Conclusion
In this paper, we have described our approach
towards building a language-independent context
aware query translation, replacing the language re-
sources with the rich multilingual content provider,
Wikipedia. Its structural aspects have been exploited
to build the dictionary and its articles are used to
form queries and also to evaluate them. Further ex-
ploitation of Wikipedia and its structure to increase
the coverage of the dictionaries built will increase
the overall precision. Though queries are translated
in a language-independent way, using language re-
sources of English, as it is a richly resourced lan-
guage, for query formation is also envisioned.
References
Rohit G. Bharadwaj and Vasudeva Varma. 2011. Lan-
guage independent identification of parallel sentences
149
using wikipedia. In Proceedings of the 20th inter-
national conference companion on World wide web,
WWW ?11, pages 11?12, New York, NY, USA. ACM.
J. W. Breen. 2004. JMdict:A Japanese-Multilingual
Dictionary. In COLING Multilingual Linguistic Re-
sources Workshop, pages 71?78.
P.F. Brown, J. Cocke, S.A.D. Pietra, V.J.D. Pietra, F. Je-
linek, J.D. Lafferty, R.L. Mercer, and P.S. Roossin.
1990. A statistical approach to machine translation.
Computational linguistics, 16(2):85.
M. Erdmann, K. Nakayama, T. Hara, and S. Nishio.
2008. An approach for extracting bilingual terminol-
ogy from wikipedia. In Database Systems for Ad-
vanced Applications, pages 380?392. Springer.
M. Erdmann, K. Nakayama, T. Hara, and S. Nishio.
2009. Improving the extraction of bilingual termi-
nology from Wikipedia. ACM Transactions on Multi-
media Computing, Communications, and Applications
(TOMCCAP), 5(4):1?17.
P. Fung and K. McKeown. 1997. A technical word-and
term-translation aid using noisy parallel corpora across
language groups. Machine Translation, 12(1):53?87.
B. Gaillard, M. Boualem, and O. Collin. 2010. Query
Translation using Wikipedia-based resources for anal-
ysis and disambiguation.
M. Kay and M. Roscheisen. 1999. Text-translation
Alignment. In Computational Linguistics, volume 19,
pages 604?632.
F. Sadat, M. Yoshikawa, and S. Uemura. 2003. Bilin-
gual terminology acquisition from comparable corpora
and phrasal translation to cross-language information
retrieval. In Proceedings of the 41st Annual Meeting
on Association for Computational Linguistics-Volume
2, pages 141?144. Association for Computational Lin-
guistics.
P. Scho?nhofen, A. Benczu?r, I. B??ro?, and K. Csaloga?ny.
2008. Cross-language retrieval with wikipedia. Ad-
vances in Multilingual and Multimodal Information
Retrieval, pages 72?79.
F.M. Tyers and J.A. Pienaar. 2008. Extracting bilingual
word pairs from Wikipedia. Collaboration: interop-
erability between people in the creation of language
resources for less-resourced languages, page 19.
150
Proceedings of the 3rd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, pages 11?18,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
Mining Sentiments from Tweets
Akshat Bakliwal, Piyush Arora, Senthil Madhappan
Nikhil Kapre, Mukesh Singh and Vasudeva Varma
Search and Information Extraction Lab,
International Institute of Information Technology, Hyderabad.
{akshat.bakliwal, piyush.arora}@research.iiit.ac.in,
{senthil.m, nikhil.kapre, mukeshkumar.singh}@students.iiit.ac.in,
vv@iiit.ac.in
Abstract
Twitter is a micro blogging website, where
users can post messages in very short text
called Tweets. Tweets contain user opin-
ion and sentiment towards an object or per-
son. This sentiment information is very use-
ful in various aspects for business and gov-
ernments. In this paper, we present a method
which performs the task of tweet sentiment
identification using a corpus of pre-annotated
tweets. We present a sentiment scoring func-
tion which uses prior information to classify
(binary classification ) and weight various sen-
timent bearing words/phrases in tweets. Us-
ing this scoring function we achieve classifi-
cation accuracy of 87% on Stanford Dataset
and 88% on Mejaj dataset. Using supervised
machine learning approach, we achieve classi-
fication accuracy of 88% on Stanford dataset.
1 Introduction
With enormous increase in web technologies, num-
ber of people expressing their views and opinions
via web are increasing. This information is very
useful for businesses, governments and individuals.
With over 340+ million Tweets (short text messages)
per day, Twitter is becoming a major source of infor-
mation.
Twitter is a micro-blogging site, which is popular
because of its short text messages popularly known
as ?Tweets?. Tweets have a limit of 140 characters.
Twitter has a user base of 140+ million active users1
1As on March 21, 2012. Source:
http://en.wikipedia.org/wiki/Twitter
and thus is a useful source of information. Users
often discuss on current affairs and share their per-
sonals views on various subjects via tweets.
Out of all the popular social media?s like Face-
book, Google+, Myspace and Twitter, we choose
Twitter because 1) tweets are small in length, thus
less ambigious; 2) unbiased; 3) are easily accessible
via API; 4) from various socio-cultural domains.
In this paper, we introduce an approach which can
be used to find the opinion in an aggregated col-
lection of tweets. In this approach, we used two
different datasets which are build using emoticons
and list of suggestive words respectively as noisy la-
bels. We give a new method of scoring ?Popularity
Score?, which allows determination of the popular-
ity score at the level of individual words of a tweet
text. We also emphasis on various types and levels
of pre-processing required for better performance.
Roadmap for rest of the paper: Related work is
discussed in Section 2. In Section 3, we describe
our approach to address the problem of Twitter
sentiment classification along with pre-processing
steps.Datasets used in this research are discussed in
Section 4. Experiments and Results are presented in
Section 5. In Section 6, we present the feature vector
approach to twitter sentiment classification. Section
7 presents as discussion on the methods and we con-
clude the paper with future work in Section 8.
2 Related Work
Research in Sentiment Analysis of user generated
content can be categorized into Reviews (Turney,
2002; Pang et al, 2002; Hu and Liu, 2004), Blogs
(Draya et al, 2009; Chesley, 2006; He et al, 2008),
11
News (Godbole et al, 2007), etc. All these cat-
egories deal with large text. On the other hand,
Tweets are shorter length text and are difficult to
analyse because of its unique language and struc-
ture.
(Turney, 2002) worked on product reviews. Tur-
ney used adjectives and adverbs for performing
opinion classification on reviews. He used PMI-IR
algorithm to estimate the semantic orientation of the
sentiment phrase. He achieved an average accuracy
of 74% on 410 reviews of different domains col-
lected from Epinion. (Hu and Liu, 2004) performed
feature based sentiment analysis. Using Noun-Noun
phrases they identified the features of the products
and determined the sentiment orientation towards
each feature. (Pang et al, 2002) tested various ma-
chine learning algorithms on Movie Reviews. He
achieved 81% accuracy in unigram presence feature
set on Naive Bayes classifier.
(Draya et al, 2009) tried to identify domain spe-
cific adjectives to perform blog sentiment analysis.
They considered the fact that opinions are mainly
expressed by adjectives and pre-defined lexicons fail
to identify domain information. (Chesley, 2006) per-
formed topic and genre independent blog classifica-
tion, making novel use of linguistic features. Each
post from the blog is classified as positive, negative
and objective.
To the best of our knowledge, there is very less
amount of work done in twitter sentiment analy-
sis. (Go et al, 2009) performed sentiment analy-
sis on twitter. They identified the tweet polarity us-
ing emoticons as noisy labels and collected a train-
ing dataset of 1.6 million tweets. They reported an
accuracy of 81.34% for their Naive Bayes classi-
fier. (Davidov et al, 2010) used 50 hashtags and 15
emoticons as noisy labels to create a dataset for twit-
ter sentiment classification. They evaluate the effect
of different types of features for sentiment extrac-
tion. (Diakopoulos and Shamma, 2010) worked on
political tweets to identify the general sentiments of
the people on first U.S. presidential debate in 2008.
(Bora, 2012) also created their dataset based on
noisy labels. They created a list of 40 words (pos-
itive and negative) which were used to identify the
polarity of tweet. They used a combination of
a minimum word frequency threshold and Cate-
gorical Proportional Difference as a feature selec-
tion method and achieved the highest accuracy of
83.33% on a hand labeled test dataset.
(Agarwal et al, 2011) performed three class (pos-
itive, negative and neutral) classification of tweets.
They collected their dataset using Twitter stream
API and asked human judges to annotate the data
into three classes. They had 1709 tweets of each
class making a total of 5127 in all. In their research,
they introduced POS-specific prior polarity features
along with twitter specific features. They achieved
max accuracy of 75.39% for unigram + senti fea-
tures.
Our work uses (Go et al, 2009) and (Bora, 2012)
datasets for this research. We use Naive Bayes
method to decide the polarity of tokens in the tweets.
Along with that we provide an useful insight on how
preprocessing should be done on tweet. Our method
of Senti Feature Identification and Popularity Score
perform well on both the datasets. In feature vec-
tor approach, we show the contribution of individual
NLP and Twitter specific features.
3 Approach
Our approach can be divided into various steps.
Each of these steps are independent of the other but
important at the same time.
3.1 Baseline
In the baseline approach, we first clean the tweets.
We remove all the special characters, targets (@),
hashtags (#), URLs, emoticons, etc and learn the
positive & negative frequencies of unigrams in train-
ing. Every unigram token is given two probability
scores: Positive Probability (Pp) and Negative Prob-
ability (Np) (Refer Equation 1). We follow the same
cleaning process for the test tweets. After clean-
ing the test tweets, we form all the possible uni-
grams and check for their frequencies in the training
model. We sum up the positive and negative proba-
bility scores of all the constituent unigrams, and use
their difference (positive - negative) to find the over-
all score of the tweet. If tweet score is > 0 then it is
12
positive otherwise negative.
Pf = Frequency in Positive Training Set
Nf = Frequency in Negative Training Set
Pp = Positive Probability of the token.
= Pf/(Pf + Nf )
Np = Negative Probability of the token.
= Nf/(Pf + Nf )
(1)
3.2 Emoticons and Punctuations Handling
We make slight changes in the pre-processing mod-
ule for handling emoticons and punctuations. We
use the emoticons list provided by (Agarwal et al,
2011) in their research. This list2 is built from
wikipedia list of emoticons3 and is hand tagged into
five classes (extremely positive, positive, neutral,
negative and extremely negative). In this experi-
ment, we replace all the emoticons which are tagged
positive or extremely positive with ?zzhappyzz? and
rest all other emoticons with ?zzsadzz?. We append
and prepend ?zz? to happy and sad in order to pre-
vent them from mixing into tweet text. At the end,
?zzhappyzz? is scored +1 and ?zzsadzz? is scored -1.
Exclamation marks (!) and question marks (?)
also carry some sentiment. In general, ?!? is used
when we have to emphasis on a positive word and
??? is used to highlight the state of confusion or
disagreement. We replace all the occurrences of ?!?
with ?zzexclaimzz? and of ??? with ?zzquestzz?. We
add 0.1 to the total tweet score for each ?!? and sub-
tract 0.1 from the total tweet score for each ???. 0.1
is chosen by trial and error method.
3.3 Stemming
We use Porter Stemmer4 to stem the tweet words.
We modify porter stemmer and restrict it to step 1
only. Step 1 gets rid of plurals and -ed or -ing.
3.4 Stop Word Removal
Stop words play a negative role in the task of senti-
ment classification. Stop words occur in both pos-
itive and negative training set, thus adding more
ambiguity in the model formation. And also, stop
2http://goo.gl/oCSnQ
3http://en.wikipedia.org/wiki/List of emoticons
4http://tartarus.org/m?artin/PorterStemmer/
words don?t carry any sentiment information and
thus are of no use to us. We create a list of stop
words like he, she, at, on, a, the, etc. and ignore
them while scoring. We also discard words which
are of length ? 2 for scoring the tweet.
3.5 Spell Correction
Tweets are written in random form, without any fo-
cus given to correct structure and spelling. Spell
correction is an important part in sentiment analy-
sis of user- generated content. Users type certain
characters arbitrary number of times to put more em-
phasis on that. We use the spell correction algo-
rithm from (Bora, 2012). In their algorithm, they
replace a word with any character repeating more
than twice with two words, one in which the re-
peated character is placed once and second in which
the repeated character is placed twice. For example
the word ?swwweeeetttt? is replaced with 8 words
?swet?, ?swwet?, ?sweet?, ?swett?, ?swweet?, and so
on.
Another common type of spelling mistakes oc-
cur because of skipping some of characters from the
spelling. like ?there? is generally written as ?thr?.
Such types of spelling mistakes are not currently
handled by our system. We propose to use phonetic
level spell correction method in future.
3.6 Senti Features
At this step, we try to reduce the effect of non-
sentiment bearing tokens on our classification sys-
tem. In the baseline method, we considered all the
unigram tokens equally and scored them using the
Naive Bayes formula (Refer Equation 1). Here, we
try to boost the scores of sentiment bearing words.
In this step, we look for each token in a pre-defined
list of positive and negative words. We use the list of
of most commonly used positive and negative words
provided by Twitrratr5. When we come across a to-
ken in this list, instead of scoring it using the Naive
Bayes formula (Refer Equation 1), we score the to-
ken +/- 1 depending on the list in which it exist. All
the tokens which are missing from this list went un-
der step 3.3, 3.4, 3.5 and were checked for their oc-
currence after each step.
5http://twitrratr.com/
13
3.7 Noun Identification
After doing all the corrections (3.3 - 3.6) on a word,
we look at the reduced word if it is being converted
to a Noun or not. We identify the word as a Noun
word by looking at its part of speech tag in English
WordNet(Miller, 1995). If the majority sense (most
commonly used sense) of that word is Noun, we
discard the word while scoring. Noun words don?t
carry sentiment and thus are of no use in our experi-
ments.
3.8 Popularity Score
This scoring method boosts the scores of the most
commonly used words, which are domain specific.
For example, happy is used predominantly for ex-
pressing the positive sentiment. In this method, we
multiple its popularity factor (pF) to the score of
each unigram token which has been scored in the
previous steps. We use the occurrence frequency of
a token in positive and negative dataset to decide on
the weight of popularity score. Equation 2 shows
how the popularity factor is calculated for each to-
ken. We selected a threshold 0.01 min support as the
cut-off criteria and reduced it by half at every level.
Support of a word is defined as the proportion of
tweets in the dataset which contain this token. The
value 0.01 is chosen such that we cover a large num-
ber of tokens without missing important tokens, at
the same time pruning less frequent tokens.
Pf = Frequency in Positive Training Set
Nf = Frequency in Negative Training Set
if(Pf ?Nf ) > 1000)
pF = 0.9;
elseif((Pf ?Nf ) > 500)
pF = 0.8;
elseif((Pf ?Nf ) > 250)
pF = 0.7;
elseif((Pf ?Nf ) > 100)
pF = 0.5;
elseif((Pf ?Nf < 50))
pF = 0.1;
(2)
Figure 1 shows the flow of our approach.
Figure 1: Flow Chart of our Algorithm
4 Datasets
In this section, we explain the two datasets used in
this research. Both of these datasets are built using
noisy labels.
4.1 Stanford Dataset
This dataset(Go et al, 2009) was built automat-
ically using emoticons as noisy labels. All the
tweets which contain ?:)? were marked positive and
tweets containing ?:(? were marked negative. Tweets
that did not have any of these labels or had both
were discarded. The training dataset has ?1.6 mil-
lion tweets, equal number of positive and negative
tweets. The training dataset was annotated into two
classes (positive and negative) while the testing data
was hand annotated into three classes (positive, neg-
ative and neutral). For our experimentation, we use
only positive and negative class tweets from the test-
ing dataset for our experimentation. Table 1 gives
the details of dataset.
Training Tweets
Positive 800,000
Negative 800,000
Total 1,600,000
Testing Tweets
Positive 180
Negative 180
Objective 138
Total 498
Table 1: Stanford Twitter Dataset
14
4.2 Mejaj
Mejaj dataset(Bora, 2012) was built using noisy la-
bels. They collected a set of 40 words and manually
categorized them into positive and negative. They
label a tweet as positive if it contains any of the pos-
itive sentiment words and as negative if it contains
any of the negative sentiment words. Tweets which
do not contain any of these noisy labels and tweets
which have both positive and negative words were
discarded. Table 2 gives the list of words which were
used as noisy labels. This dataset contains only two
class data. Table 3 gives the details of the dataset.
Positive Labels Negative Labels
amazed, amused,
attracted, cheerful,
delighted, elated,
excited, festive, funny,
hilarious, joyful,
lively, loving,
overjoyed, passion,
pleasant, pleased,
pleasure, thrilled,
wonderful
annoyed, ashamed,
awful, defeated,
depressed,
disappointed,
discouraged,
displeased,
embarrassed, furious,
gloomy, greedy,
guilty, hurt, lonely,
mad, miserable,
shocked, unhappy,
upset
Table 2: Noisy Labels for annotating Mejaj Dataset
Training Tweets
Positive 668,975
Negative 795,661
Total 1,464,638
Testing Tweets
Positive 198
Negative 204
Total 402
Table 3: Mejaj Dataset
5 Experiment
In this section, we explain the experiments carried
out using the above proposed approach.
5.1 Stanford Dataset
On this dataset(Go et al, 2009), we perform a series
of experiments. In the first series of experiments,
we train on the given training data and test on the
testing data. In the second series of experiments,
we perform 5 fold cross validation using the training
data. Table 4 shows the results of each of these ex-
periments on steps which are explained in Approach
(Section 3).
In table 4, we give results for each step emoticons
and punctuations handling, spell correction, stem-
ming and stop word removal mentioned in Approach
Section (Section 3). The Baseline + All Combined
results refers to combination of these steps (emoti-
cons, punctuations, spell correction, Stemming and
stop word removal) performed together. Series 2 re-
sults are average of accuracy of each fold.
5.2 Mejaj Dataset
Similar series of experiments were performed on
this dataset(Bora, 2012) too. In the first series of
experiments, training and testing was done on the
respective given datasets. In the second series of ex-
periments, we perform 5 fold cross validation on the
training data. Table 5 shows the results of each of
these experiments.
In table 5, we give results for each step emoticons
and punctuations handling, spell correction, stem-
ming and stop word removal mentioned in Approach
Section (Section 3). The Baseline + All Combined
results refers to combination of these steps (emoti-
cons, punctuations, spell correction, Stemming and
stop word removal) performed together. Series 2 re-
sults are average of accuracy of each fold.
5.3 Cross Dataset
To validate the robustness of our approach, we ex-
perimented with cross dataset training and testing.
We trained our system on one dataset and tested on
the other dataset. Table 6 reports the results of cross
dataset evaluations.
6 Feature Vector Approach
In this feature vector approach, we form features us-
ing Unigrams, Bigrams, Hashtags (#), Targets (@),
Emoticons, Special Symbol (?!?) and used a semi-
supervised SVM classifier. Our feature vector com-
prised of 11 features. We divide the features into
two groups, NLP features and Twitter specific fea-
tures. NLP features include frequency of positive
15
Method Series 1 (%) Series 2 (%)
Baseline 78.8 80.1
Baseline + Emoticons + Punctuations 81.3 82.1
Baseline + Spell Correction 81.3 81.6
Baseline + Stemming 81.9 81.7
Baseline + Stop Word Removal 81.7 82.3
Baseline + All Combined (AC) 83.5 85.4
AC + Senti Features (wSF) 85.5 86.2
wSF + Noun Identification (wNI) 85.8 87.1
wNI + Popularity Score 87.2 88.4
Table 4: Results on Stanford Dataset
Method Series 1 (%) Series 2 (%)
Baseline 77.1 78.6
Baseline + Emoticons + Punctuations 80.3 80.4
Baseline + Spell Correction 80.1 80.0
Baseline + Stemming 79.1 79.7
Baseline + Stop Word Removal 80.2 81.7
Baseline + All Combined (AC) 82.9 84.1
AC + Senti Features (wSF) 86.8 87.3
wSF + Noun Identification (wNI) 87.6 88.2
wNI + Popularity Score 88.1 88.1
Table 5: Results on Mejaj Dataset
Method Training Dataset Testing Dataset Accuracy
wNI + Popularity Score Stanford Mejaj 86.4%
wNI + Popularity Score Mejaj Stanford 84.7%
Table 6: Results on Cross Dataset evaluation
NLP Unigram (f1) # of positive and negative unigramBigram (f2) # of positive and negative Bigram
Twitter Specific
Hashtags (f3) # of positive and negative hashtags
Emoticons (f4) # of positive and negative emoticons
URLs (f5) Binary Feature - presence of URLs
Targets (f6) Binary Feature - presence of Targets
Special Symbols (f7) Binary Feature - presence of ?!?
Table 7: Features and Description
16
Feature Set Accuracy (Stanford)
f1 + f2 85.34%
f3 + f4 + f7 53.77%
f3 + f4 + f5 + f6 + f7 60.12%
f1 + f2 + f3 + f4 + f7 85.89%
f1 + f2 + f3 + f4 +
f5 + f6 + f7 87.64%
Table 8: Results of Feature Vector Classifier on Stanford
Dataset
unigrams matched, negative unigrams matched, pos-
itive bigrams matched, negative bigrams matched,
etc and Twitter specific features included Emoti-
cons, Targets, HashTags, URLs, etc. Table 7 shows
the features we have considered.
HashTags polarity is decided based on the con-
stituent words of the hashtags. Using the list of pos-
itive and negative words from Twitrratr6, we try to
find if hashtags contains any of these words. If so,
we assign the polarity of that to the hashtag. For
example, ?#imsohappy? contains a positive word
?happy?, thus this hashtag is considered as posi-
tive hashtag. We use the emoticons list provided
by (Agarwal et al, 2011) in their research. This
list7 is built from wikipedia list of emoticons8 and
is hand tagged into five classes (extremely positive,
positive, neutral, negative and extremely negative).
We reduce this five class list to two class by merging
extremely positive and positive class to single posi-
tive class and rest other classes (extremely negative,
negative and neutral) to single negative class. Ta-
ble 8 reports the accuracy of our machine learning
classifier on Stanford dataset.
7 Discussion
In this section, we present a few examples evaluated
using our system. The following example denotes
the effect of incorporating the contribution of emoti-
cons on tweet classification. Example ?Ahhh I can?t
move it but hey w/e its on hell I?m elated right now
:-D?. This tweet contains two opinion words, ?hell?
and ?elated?. Using the unigram scoring method,
this tweet is classified neutral but it is actually posi-
6http://twitrratr.com/
7http://goo.gl/oCSnQ
8http://en.wikipedia.org/wiki/List of emoticons
tive. If we incorporate the effect of emoticon ?:-D?,
then this tweet is tagged positive. ?:-D? is a strong
positive emoticon.
Consider this example, ?Bill Clinton Fail -
Obama Win??. In this example, there are two senti-
ment bearing words, ?Fail? and ?Win?. Ideally this
tweet should be neutral but this is tagged as a posi-
tive tweet in the dataset as well as using our system.
In this tweet, if we calculate the popularity factor
(pF) for ?Win? and ?Fail?, they come out to be 0.9
and 0.8 respectively. Because of the popularity fac-
tor weight, the positive score domniates the negative
score and thus the tweet is tagged as positive. It is
important to identify the context flow in the text and
also how each of these words modify or depend on
the other words of the tweet.
For calculating the system performance, we as-
sume that the dataset which is used here is correct.
Most of the times this assumption is true but there
are a few cases where it fails. For example, this
tweet ?My wrist still hurts. I have to get it looked
at. I HATE the dr/dentist/scary places. :( Time to
watch Eagle eye. If you want to join, txt!? is tagged
as positive, but actually this should have been tagged
negative. Such erroneous tweets also effect the sys-
tem performance.
There are few limitations with the current pro-
posed approach which are also open research prob-
lems.
1. Spell Correction: In the above proposed ap-
proach, we gave a solution to spell correction
which works only when extra characters are en-
tered by the user. It fails when users skip some
characters like ?there? is spelled as ?thr?. We
propose the use of phonetic level spell correc-
tion to handle this problem.
2. Hashtag Segmentation: For handling hashtags,
we looked for the existence of the positive or
negative words9 in the hashtag. But there can
be some cases where it may not work correctly.
For example, ?#thisisnotgood?, in this hashtag
if we consider the presence of positive and neg-
ative words, then this hashtag is tagged posi-
tive (?good?). We fail to capture the presence
and effect of ?not? which is making this hash-
9word list taken from http://twitrratr.com/
17
tag as negative. We propose to devise and use
some logic to segment the hashtags to get cor-
rect constituent words.
3. Context Dependency: As discussed in one of
the examples above, even tweet text which is
limited to 140 characters can have context de-
pendency. One possible method to address this
problem is to identify the objects in the tweet
and then find the opinion towards those objects.
8 Conclusion and Future Work
Twitter sentiment analysis is a very important and
challenging task. Twitter being a microblog suffers
from various linguistic and grammatical errors. In
this research, we proposed a method which incorpo-
rates the popularity effect of words on tweet senti-
ment classification and also emphasis on how to pre-
process the Twitter data for maximum information
extraction out of the small content. On the Stanford
dataset, we achieved 87% accuracy using the scor-
ing method and 88% using SVM classifier. On Me-
jaj dataset, we showed an improvement of 4.77% as
compared to their (Bora, 2012) accuracy of 83.33%.
In future, This work can be extended through in-
corporation of better spell correction mechanisms
(may be at phonetic level) and word sense disam-
biguation. Also we can identify the target and enti-
ties in the tweet and the orientation of the user to-
wards them.
Acknowledgement
We would like to thank Vibhor Goel, Sourav Dutta
and Sonil Yadav for helping us with running SVM
classifier on such a large data.
References
Agarwal, A., Xie, B., Vovsha, I., Rambow, O. and Pas-
sonneau, R. (2011). Sentiment analysis of Twitter
data. In Proceedings of the Workshop on Languages
in Social Media LSM ?11.
Bora, N. N. (2012). Summarizing Public Opinions in
Tweets. In Journal Proceedings of CICLing 2012,
New Delhi, India.
Chesley, P. (2006). Using verbs and adjectives to auto-
matically classify blog sentiment. In In Proceedings
of AAAI-CAAW-06, the Spring Symposia on Compu-
tational Approaches.
Davidov, D., Tsur, O. and Rappoport, A. (2010). En-
hanced sentiment learning using Twitter hashtags and
smileys. In Proceedings of the 23rd International Con-
ference on Computational Linguistics: Posters COL-
ING ?10.
Diakopoulos, N. and Shamma, D. (2010). Characterizing
debate performance via aggregated twitter sentiment.
In Proceedings of the 28th international conference on
Human factors in computing systems ACM.
Draya, G., Planti, M., Harb, A., Poncelet, P., Roche,
M. and Trousset, F. (2009). Opinion Mining from
Blogs. In International Journal of Computer Informa-
tion Systems and Industrial Management Applications
(IJCISIM).
Go, A., Bhayani, R. and Huang, L. (2009). Twitter Sen-
timent Classification using Distant Supervision. In
CS224N Project Report, Stanford University.
Godbole, N., Srinivasaiah, M. and Skiena, S. (2007).
Large-Scale Sentiment Analysis for News and Blogs.
In Proceedings of the International Conference on We-
blogs and Social Media (ICWSM).
He, B., Macdonald, C., He, J. and Ounis, I. (2008). An
effective statistical approach to blog post opinion re-
trieval. In Proceedings of the 17th ACM conference on
Information and knowledge management CIKM ?08.
Hu, M. and Liu, B. (2004). Mining Opinion Features in
Customer Reviews. In AAAI.
Miller, G. A. (1995). WordNet: A Lexical Database for
English. Communications of the ACM 38, 39?41.
Pang, B., Lee, L. and Vaithyanathan, S. (2002). Thumbs
up? Sentiment Classification using Machine Learning
Techniques.
Turney, P. D. (2002). Thumbs Up or Thumbs Down? Se-
mantic Orientation Applied to Unsupervised Classifi-
cation of Reviews. In ACL.
18
Proceedings of the First Workshop on Multilingual Modeling, pages 11?17,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Language-Independent Named Entity Identification using Wikipedia
Mahathi Bhagavatula
Search and
Information Extraction Lab
IIIT Hyderabad
mahathi.b@research.iiit.ac.in
Santosh GSK
Search and
Information Extraction Lab
IIIT Hyderabad
santosh.gsk@research.iiit.ac.in
Vasudeva Varma
Search and
Information Extraction Lab
IIIT Hyderabad
vv@iiit.ac.in
Abstract
Recognition of Named Entities (NEs) is a dif-
ficult process in Indian languages like Hindi,
Telugu, etc., where sufficient gazetteers and
annotated corpora are not available compared
to English language. This paper details a novel
clustering and co-occurrence based approach
to map English NEs with their equivalent rep-
resentations from different languages recog-
nized in a language-independent way. We
have substituted the required language specific
resources by the richly structured multilin-
gual content of Wikipedia. The approach in-
cludes clustering of highly similar Wikipedia
articles. Then the NEs in an English article
are mapped with other language terms in in-
terlinked articles based on co-occurrence fre-
quencies. The cluster information and the
term co-occurrences are considered in ex-
tracting the NEs from non-English languages.
Hence, the English Wikipedia is used to boot-
strap the NEs for other languages. Through
this approach, we have availed the structured,
semi-structured and multilingual content of
the Wikipedia to a massive extent. Experi-
mental results suggest that the proposed ap-
proach yields promising results in rates of pre-
cision and recall.
1 Introduction
Named entity recognition (NER) is an important
subtask of information extraction that seeks to
locate and classify atomic elements in text into
predefined categories such as the names of persons,
organizations, locations, etc.
The state-of-art NER systems for English pro-
duce near-human performance. However, for
non-English languages the state-of-art NER systems
perform below par. And for languages that have a
lack of resources (e.g., Indian Languages) a NER
system with a near-human performance is a distant
future.
NER systems so far developed involved linguistic
grammar-based techniques as well as statistical
models. The grammar-based techniques require
linguistic expertise and requires strenuous efforts
to build a NER system for every new language.
Such techniques can be safely avoided when there
is a requirement to build a generic NER system
for several languages (e.g., Indian Languages).
Statistical NER systems typically require a large
amount of manually annotated training data. With
the serious lack of such manually annotated training
data, the task of high-performance NER system
projects as a major challenge for Indian languages.
This paper focuses on building a generic-purpose
NE identification system for Indian languages.
Given the constraints for resource-poor languages,
we restrain from developing a regular NE Recogni-
tion system. However, the goal here is to identify
as many NEs available in Indian languages without
using any language-dependent tools or resources.
Wikipedia is a free, web-based, collaborative,
multilingual encyclopedia. There are 283 language
editions available as of now. Wikipedia has both
structured (e.g., Infoboxes, Categories, Hyperlinks,
11
InterLanguage links, etc.) and semi-structured
(content and organization of the page) information.
Hence, the richly linked structure of Wikipedia
present across several languages (e.g., English,
Hindi, Marathi) has been used to build and enhance
many NLP applications including NE identification
systems. However, the existing approaches that
exploit Wikipedia for recognizing NEs concentrates
only on the structured parts which results in less
recall. Our approach concentrates on exploiting
structured and semi-structured parts of Wikipedia
and hence yielding better results.
The approach used is simple, efficient, easily
reproducible and can be extended to any language
as it doesn?t use any of the language specific
resources.
2 Related Work
Wikipedia has been the subject of a considerable
amount of research in recent years including
Gabrilovich and Markovitch (2005), Milne et
al. (2006), Zesch et al (2007), Timothy Weale
(2006) and Richman and Schone (2008). The most
relevant work to this paper are Kazama and Tori-
sawa (2007), Toral and Munoz (2006), Cucerzan
(2007), Richman and Schone (2008). More details
follow, however it is worth noting that all known
prior research is fundamentally monolingual, often
developing algorithms that can be adapted to other
languages pending availability of the appropriate
semantic resources.
Toral and Munoz (2006) used Wikipedia to
create lists of NE?s. They used the first sentence
of Wikipedia articles as likely definitions of the
article titles, and used them in attempting to classify
the titles as people, locations, organizations, or
none. Unlike the method presented in our paper,
their algorithm relied on WordNet (or an equivalent
resource in another language). The authors noted
that their results would need to pass a manual
supervision step before being useful for the NER
task, and thus did not evaluate their results in the
context of a full NER system.
Similarly, Kazama and Torisawa (2007) used
Wikipedia, particularly the first sentence of each ar-
ticle, to create lists of entities. Rather than building
entity dictionaries, associating words and phrases
to the classical NE tags (PERSON, LOCATION,
etc.), they used a noun phrase following the verb
forms ?to be? to derive a label. For example, they
used the sentence ?Franz Fischler ... is an Austrian
politician? to associate the label ?politician? to the
surface form ?Franz Fischler?. They proceeded to
show that the dictionaries generated by their method
are useful when integrated into an NER system.
It is to be noted that their technique relies upon a
part-of-speech tagger.
Cucerzan (2007), by contrast to the above,
used Wikipedia primarily for Named Entity Dis-
ambiguation, following the path of Bunescu and
Pasca (2006). As in our paper, and unlike the above
mentioned works, Cucerzan (2007) made use of
the explicit Category information found within
Wikipedia. In particular, Category and related list
derived data were key pieces of information used
to differentiate between various meanings of an
ambiguous surface form. Cucerzan (2007) did not
make use of the Category information in identifying
the class of a given entity. It is to be noted that the
NER component was not the focus of their research,
and was specific to the English language.
Richman and Schone (2008) emphasized on
the use of links between articles of different lan-
guages, specifically between English (the largest
and best linked Wikipedia) and other languages.
The approach uses English Wikipedia structure
namely categories and hyperlinks to get NEs and
then use language specific tools to derive multilin-
gual NEs.
The following are the majors differences be-
tween any of the above approaches to the approach
followed in this paper.
? No language resource has been used at any
stage of NE identification, unlike the above ap-
proaches that used at least one of the language
dependent tools like dictionary, POS tagger,etc.
? Our approach utilized several aspects of
Wikipedia (e.g., InterLanguage links, Cate-
12
gories, Sub-titles, Article Text), which has been
by far the best exploitation of various structural
aspects of Wikipedia.
? Language-independent mapping of mul-
tilingual similar content (i.e., the paral-
lel/comparable topics or sentences of different
languages) can be used as a reference to any
future work. Further details can be found in
the Section 4.2.
3 Wikipedia Structure
From Wikipedia, we exploited the following three
major units:
Category links: These are the links from an
article to ?Category? pages, represented in the form
of [[Category:Luzerne County, Pennsylvania]],
[[Category:Rivers of Pennsylvania]], etc.
InterLanguage links: Links from an article
to a presumably equivalent article in another lan-
guage. For example, in the English language article
?History of India?, one finds a set of links including
[[hi: ]]. In almost all cases, the articles
linked in this manner represent articles on the same
subject.
Subtitles of the document: These are consid-
ered to be semi-structured parts of a Wikipedia
article. Every page in Wikipedia consists of a
title and subtitles. Considering the data below the
subtitles, they can be referred as subparts of the
article. For example, the article regarding Jimmy
Wales has subtitles ?Early life and education?,
?Career?, etc.
4 Architecture
The system architecture involves 3 main steps and
are detailed as follows:
4.1 Related Document Clustering:
Hierarchical clustering outputs a hierarchy, a struc-
ture that is more informative than the unstructured
set of clusters returned by flat clustering. This paper
deals with large amounts of semi-structured data
and requires structured clusters as output rather
than unstructured clusters. Moreover, specifying the
number of clusters beforehand is difficult. Hence,
we prefer Hierarchical clustering over Flat clus-
tering in rest of the paper. Bottom-up algorithms
can reach a cluster configuration with a better
homogeneity than Top-Down clustering. Hence,
we prefer bottom-up clustering over top-down
clustering.
Within bottom-up clustering there are several
similarity measures that can be employed namely
single-linkage, complete-linkage, group-average
and centroid-measure. This single-link merge
criterion is local. Priority is given solely to the area
where the two clusters come closest to each other.
Other, more distant parts of the cluster and the
clusters? overall structure are not taken into account.
In complete-link clustering or complete-linkage
clustering, the similarity of two clusters is the
similarity of their most dissimilar members. In
centroid clustering, the similarity of two clusters
is defined as the similarity of their centroids.
Group-average agglomerative clustering or GAAC
evaluates cluster quality based on all similarities
between documents, thus avoiding the pitfalls of
the single-link and complete-link criteria. Hence,
in this paper, we made use of the Group-average
agglomerative clustering.
We have considered the English Wikipedia ar-
ticles which contain InterLanguage links to Hindi
articles. The English articles are clustered based on
the overlap of terms, i.e., the number of common
terms present between articles. The clustering
algorithm is detailed as follows:
Initially, consider English Wikipedia data, each
article in the dataset is considered as a single
document cluster. Now, the distance between two
clusters is calculated using
SIM-GA(?i, ?j) = 1(Ni+Nj)(Ni+Nj?1)
?
dm??i??j
?
dn??i??j ,dm 6=dn
~dm ? ~dn
where ~d is the length-normalized vector of document
d, ? denotes the dot product, and Ni and Nj are the
number of documents in ?i and ?j , respectively. Us-
ing group average agglomerative clustering, the pro-
13
cess is repeated till we reach a certain threshold (set
to 0.2) and thus the hierarchical clusters of English
data are formed. In order to cluster documents of
other languages, we availed the InterLanguage links
and structure of English clusters. The InterLanguage
links are used in replicating the cluster structure of
English Wikipedia articles across other language ar-
ticles. Therefore, we avoided the repetition of the
clustering step for non-English articles. These dif-
ferent language clusters, being interconnected, are
further utilized in our approach.
4.2 Mapping related content within interlinked
documents:
As the clustering technique used is hierarchical,
the intermediate clustering steps are gathered and
are called as subclusters. For example, if two
clusters (say Diseases, Hospitals) are merged to
form a cluster (say Medicine). Then the Diseases,
Hospitals are called subclusters for the Medicine
cluster.
We measured the average of cosine similarities
between the subtitle lists of the articles in a given
cluster. If the average similarity exceeds a threshold
(set to 0.72), it would mean the articles in the cluster
(e.g., Diseases) all share similar subtitles. Other-
wise, we go for a subcluster, until the threshold
criteria is met. E.g., any two articles of the cluster
Diseases share the common subtitles like Symptoms
of Disease, Causes, Precautions, etc. This is
illustrated in figure 1. As per our observation, the
articles of different languages pertaining to same
cluster will have same subtitles but depicted in
different languages. The Hindi articles of cluster
?Diseases? share the same subtitles with those in
English. This is illustrated in figure 2.
In order to map subtitles across languages, in
each cluster, consider the non-English article with
maximum number of subtitles and its corresponding
English article. A lookup in a bilingual dictionary
developed by Rohit et al (2010) would help in
mapping certain subtitles. The rest of the subtitles
are mapped based on their order of occurrences.
The subtitles are likely to occur at the same order
in interlinked articles with high number of sub-
titles. The dictionary is expanded by adding the
Figure 1: Subtitles of Cancer and Multiple Sclerosis
mapped subtitles obtained from such interlinked
articles. This process is repeated with the remaining
interlinked articles. Rohit et al had developed
the bilingual dictionary availing Wikipedia titles
and abstract information. Hence, their approach
is language-independent and doesn?t hinder our
algorithm from being applied to other languages.
Consider each subtitle of an article in a cluster
and collect its subtitle data from that article and
from its corresponding interlinked article in Hindi.
For example, consider the subtitle ?Causes?, collect
the subtitle data from an English article (say Cancer)
and map it with the subtitle data from the Hindi
equivalent page on Cancer. We now have a mapping
titled ?Causes - Cancer? for the Cancer articles
across languages. Repeat this for all articles and
group the mappings of common subtitles. Then, a
major group ?Causes? is formed. This group will
now have a set of mappings like ?Causes - Cancer?,
?Causes - Multiple Sclerosis?, etc. Thus the multi-
lingual grouping and mapping is done. This step
maps similar content of different languages. This
is one of the important contributions of the paper
which has the potential to be applied elsewhere.
4.3 Term co occurrences model:
Consider a map (e.g., ?Causes - Cancer?) which
contains both English and Hindi data. Given the
fact that the usage of English tools doesn?t hurt the
extensibility of the approach to other languages, the
English data is annotated with Stanford NER and
the NEs are retrieved. Hindi data is preprocessed
by removing the stop words. The stop words list is
generated by considering words that occur above a
certain frequency in the overall dataset.
14
Figure 2: Subtitles of Cancer article across languages
For a given map and preprocessed data, every
English NE is paired with every non-tagged Hindi
word. Attach a default weight (=1) for each pair.
Hence, a pair may look like (tagged English word,
non tagged Hindi word, 1). This step is repeated
with all other mappings present in a group (Ex:
?Causes - Cancer?, ?Causes - Multiple Sclerosis?
in the group ?Causes?). On repeated occurrence of
the same pair, weight of that pair increases (by 1).
Finally, for a English NE term, the Hindi term with
which it has highest frequency is identified. Then
the NE tag of English term is assigned to Hindi
term. Hence, Hindi word is labeled. This step is
repeated with the remaining English NEs and Hindi
terms.
For example, consider two small mappings,
each with two English NEs and one sentence
in Hindi. Consider the first map, with ?Alexan-
der/PERSON?, ?India/LOCATION? as English NEs
and
as Hindi sentence. Then each NE of English is
attached with each Hindi word (except the stop
words) like Alexander - , Alexander -
, Alexander - , India - , etc., in all
combinations. Consider the second map with
?Alexander/PERSON?, ?Philip/PERSON? as En-
glish NEs and as Hindi
sentence. The pairs would be Alexander - ,
Alexander - etc. Hence, the maximum co oc-
curred pair would be Alexander - (Alexander
in Hindi). Then the NE tag of Alexander/PERSON
is attached to /PERSON. Similarly, for the
remaining English NEs and Hindi terms, the max-
imum co-occurred pair is identified and the Hindi
term is tagged.
5 Evaluation and Experimental setup:
As our approach requires InterLanguage links, we
are only interested in a subset of English and Hindi
Wikipedia articles which are interconnected. There
are 22,300 articles in English and Hindi Wikipedia
that have InterLanguage links. The output of Hierar-
chical GAAC clustering on this subset was observed
to be 345 clusters. We have manually tagged Hindi
articles of 50 random clusters (as cluster size can
dictate accuracies) with three NE tags (i.e., Person,
Organization, Location), resulting in 2,328 Hindi
articles with around 11,000 NE tags. All further
experiments were performed on this tagged dataset.
Precision, Recall and F-measure are the evaluation
metrics used to estimate the performance of our
system.
In order to compare our system performance
with a baseline, we have availed the Hindi NER
system developed by Gali et al (2008) at LTRC
(Language Technologies Research Center) 1 that
recognizes and annotates Hindi NEs in a given
text using Conditional Random Fields (CRF) as
the sequential labeling mechanism. Their system
is reproduced on our dataset with a 5-fold cross
validation using spell variations, pattern of suffixes
and POS tagging as the features.
6 Experiments and Results:
The experiments conducted are broadly classified as
follows:
Experiment 1: Using the structure of Wikipedia
namely Category terms, we can cluster the articles
which are having similar category terms. Another
approach for clustering is to consider the Wikipedia
page as an unstructured page and then cluster the ar-
ticles based on the similarity of words present in it.
We have performed Hierarchical GAAC based clus-
tering for these experiments.
Experiment 2: Different clustering metrics will
yield different accuracies for a given data. Here, we
will measure which similarity metric is appropriate
1http://ltrc.iiit.ac.in
15
for the dataset under study following a Category in-
formation based clustering of articles.
6.1 Experiment 1: Whether to use structure of
the Wikipedia page:
No Category: Clustering without using the Cate-
gory information: As the first experiment, the arti-
cles are clustered based on the article text and not
using the category terms.
With Category: Clustering using the Category in-
formation: In this experiment, the category terms
are used for clustering the documents. The F-
measure suggests that category terms better capture
the semantics of an article when compared to the
text of the article. Adding to the fact that category
terms suggest a compact representation of an article
whereas the text include noisy terms. The compact
representation of articles has proved to be crucial by
our next set of experiments.
Precision Recall F-measure
NER LTRC 64.9 50.6 56.81
No Category 69.8 62.7 66.05
With Category 73.5 64.3 68.59
Table 1: Experiment to determine the impact of structure
based clustering
6.2 Experiment 2: Similarity metrics for
Clustering
SLAC: Single-linkage Agglomerative Clustering:
Single-linkage algorithm would make use of mini-
mum distance between the clusters as similarity met-
ric. One of the drawback for this measure is that if
we have even a single document related to two clus-
ters, the clusters are merged. In Wikipedia, we will
not have un-related documents, all the documents
will be having a certain overlap of terms with each
other. Hence, the number of clusters formed are rel-
atively less compared to other two similarity mea-
sures. Thus the measures of Precision, Recall and
F-measure are quite less.
CLAC: Complete-linkage Agglomerative Cluster-
ing: Complete-linkage algorithm would make use
of maximum distance between the clusters as simi-
larity metric. This results in a preference for com-
pact clusters with small diameters over long. Hence,
the accuracies are improved. The drawback is that it
causes sensitivity to outliers.
GAAC: Group Average Agglomerative Clustering:
Group Average is the average between single-
linkage metric and complete-linkage metric. Hence,
covers the advantages of the both, overcoming the
drawbacks of both metrics to some extent. Thus, the
accuracies have improved considerably over previ-
ous experiments.
Precision Recall F-measure
NER LTRC 64.9 50.6 56.81
SLAC 67.6 60.3 63.74
CLAC 70.3 61.1 65.38
GAAC 73.5 64.3 68.59
Table 2: Experiment to evaluate similarity metrics
7 Discussions:
From the above results, we have made the follow-
ing observations. (I) Experiment 1: The Category
information of Wikipedia was able to capture the se-
mantics and represent the articles in a compact way
resulting in higher accuracies over the article text
information. (II) Experiment 2: As each cluster is
processed independently while identifying NEs, the
compactness and uniformity of the clusters matter
in our approach. This is studied by considering dif-
ferent similarity metrics while forming clusters. Fi-
nally, from the experiments we conclude that forma-
tion of hard clusters matter more for better results of
the approach.
8 Conclusions
This paper proposes a method to identify the NEs
in Indian languages for which the availability of re-
sources is a major concern. The approach suggested
is simple, efficient, easily reproducible and can be
extended to any other language as it is developed un-
der a language-independent framework. Wikipedia
pages across languages are merged together at subti-
tle level and then the non-English NEs are identified
based on term-term co-occurrence frequencies. The
experimental results conclude that the use of Cate-
gory information has resulted in compact represen-
tations and the compactness of the clusters plays a
predominant role in determining the accuracies of
the system.
16
References
Daniel M. Bikel and Richard Schwartz and Ralph M.
Weischedel 1999. An Algorithm that Learns What?s
in a Name, volume 34. Journal of Machine Learning
Research.
Silviu Cucerzan 2007. Large-scale named entity dis-
ambiguation based on Wikipedia data. In Proc. 2007
Joint Conference on EMNLP and CNLL, pages 708?
716.
Evgeniy Gabrilovich and Shaul Markovitch 2007. Com-
puting semantic relatedness using Wikipedia-based ex-
plicit semantic analysis. In Proceedings of the 20th In-
ternational Joint Conference on Artificial Intelligence,
pages 1606?1611.
Evgeniy Gabrilovich and Shaul Markovitch 2006. Over-
coming the brittleness bottleneck using wikipedia: en-
hancing text categorization with encyclopedic knowl-
edge. proceedings of the 21st national conference on
Artificial intelligence - Volume 2, pages 1301?1306.
Evgeniy Gabrilovich and Shaul Markovitch 2005. Fea-
ture generation for text categorization using world
knowledge. In IJCAI05, pages 1048?1053.
Jun?ichi Kazama and Kentaro Torisawa 2007. Exploiting
Wikipedia as External Knowledge for Named Entity
Recognition. Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 698?707.
David Milne and Olena Medelyan and Ian H. Wit-
ten 2006. Mining Domain-Specific Thesauri from
Wikipedia: A Case Study. Proceedings of the 2006
IEEE/WIC/ACM International Conference on Web In-
telligence, pages 442?448.
Antonio Toral and Rafael Munoz 2006. A proposal to
automatically build and maintain gazetteers for named
entity recognition by using Wikipedia. In EACL 2006.
Timothy Weale 2006. Utilizing Wikipedia Categories for
Document Classification. Evaluation, pages 4.
Torsten Zesch and Iryna Gurevych and Max Mu?hlha?user
2007. Analyzing and Accessing Wikipedia as a Lex-
ical Semantic Resource. Biannual Conference of the
Society for Computational Linguistics and Language
Technology.
Alexander E. Richman and Patrick Schone 2008. Mining
Wiki Resources for Multilingual Named Entity Recog-
nition. ACL08.
Razvan Bunescu and Marius Pasca 2006. Using En-
cyclopedic Knowledge for Named Entity Disambigua-
tion. EACL?06.
Karthik Gali and Harshit Surana and Ashwini Vaidya and
Praneeth Shishtla and Dipti M Sharma. 2008 Aggre-
gating Machine Learning and Rule Based Heuristics
for Named Entity Recognition. IJCNLP?08.
Rohit Bharadwaj G, Niket Tandon and Vasudeva Varma.
2010 An Iterative approach to extract dictionar-
ies from Wikipedia for under-resourced languages.
ICON?10.
17
