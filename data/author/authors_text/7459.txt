Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 205?208,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Using Part-of-Speech Reranking to Improve Chinese Word Segmentation
Mengqiu Wang Yanxin Shi
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{mengqiu,yanxins}@cs.cmu.edu
Abstract
Chinese word segmentation and Part-of-
Speech (POS) tagging have been com-
monly considered as two separated tasks.
In this paper, we present a system that
performs Chinese word segmentation and
POS tagging simultaneously. We train a
segmenter and a tagger model separately
based on linear-chain Conditional Ran-
dom Fields (CRF), using lexical, morpho-
logical and semantic features. We propose
an approximated joint decoding method
by reranking the N-best segmenter out-
put, based POS tagging information. Ex-
perimental results on SIGHAN Bakeoff
dataset and Penn Chinese Treebank show
that our reranking method significantly
improve both segmentation and POS tag-
ging accuracies.
1 Introduction
Word segmentation and Part-of-speeching (POS)
tagging are the most fundamental tasks in Chinese
natural language processing (NLP). Traditionally,
these two tasks were treated as separate and in-
dependent processing steps chained together in a
pipeline. In such pipeline systems, errors intro-
duced at the early stage cannot be easily recov-
ered in later steps, causing a cascade of errors
and eventually harm overall performance. Intu-
itively, a correct segmentation of the input sen-
tence is more likely to give rise to a correct POS
tagging sequence than an incorrect segmentation.
Hinging on this idea, one way to avoid error prop-
agation in chaining subtasks such as segmentation
and POS tagging is to exploit the learning trans-
fer (Sutton and McCallum, 2005) among sub-
tasks, typically through joint inference. Sutton et
al. (2004) presented dynamic conditional random
fields (DCRF), a generalization of the traditional
linear-chain CRF that allow representation of in-
teraction among labels. They used loopy belief
propagation for inference approximation. Their
empirical results on the joint task of POS tagging
and NP-chunking suggested that DCRF gave supe-
rior performance over cascaded linear-chain CRF.
Ng and Low (2004) and Luo (2003) also trained
single joint models over the Chinese segmentation
and POS tagging subtasks. In their work, they
brought the two subtasks together by treating it as
a single tagging problem, for which they trained a
maximum entropy classifier to assign a combined
word boundary and POS tag to each character.
A major challenge, however, exists in doing
joint inference for complex and large-scale NLP
application. Sutton and McCallum (Sutton and
McCallum, 2005) suggested that in many cases ex-
act inference can be too expensive and thus formi-
dable. They presented an alternative approach in
which a linear-chain CRF is trained separately for
each subtask at training time, but at decoding time
they combined the learned weights from the CRF
cascade into a single grid-shaped factorial CRF
to perform joint decoding and make predictions
for all subtasks. Similar to (Sutton and McCal-
lum, 2005), in our system we also train a cas-
cade of linear-chain CRF for the subtasks. But
at decoding time, we experiment with an alterna-
tive approximation method to joint decoding, by
taking the n-best hypotheses from the segmenta-
tion model and use the POS tagging model for
reranking. We evaluated our system on the open
tracks of SIGHAN Bakeoff 2006 dataset. Fur-
thermore, to evaluate our reranking method?s im-
pact on the POS tagging task, we also performed
10-fold cross-validation tests on the 250k Penn
205
Chinese Treebank (CTB) (Xue et al, 2002). Re-
sults from both evaluations suggest that our simple
reranking method is very effective. We achieved
a consistent performance gain on both segmenta-
tion and POS tagging tasks over linearly-cascaded
CRF. Our official F-scores on the 2006 Bakeoff
open tracks are 0.935 (UPUC), 0.964 (CityU),
0.952 (MSRA) and 0.949 (CKIP).
2 Algorithm
Given an observed Chinese character sequence
X = {C1, C2, ..., Cn}, let S and T denote a seg-
mentation sequence and a POS tagging sequence
over X. Our goal is to find a segmentation se-
quence S? and a POS tagging sequence T? that max-
imize the posterior probability :
P (S,T|X = {C1, C2, ..., Cn}) (1)
Applying chain rule, we can further derive from
Equation 1 the following:
< S?, T? >
= arg max
S,T
P (T|S,X = {C1, C2, ..., Cn})
?P (S|X = {C1, C2, ..., Cn}) (2)
Since we have factorized the joint probability
in Equation 1 into two terms, we can now model
these two components using conditional random
fields (Lafferty et al, 2001). Linear-chain CRF
models define conditional probability, P (Z|X), by
linear-chain Markov random fields. In our case, X
is the sequence of characters or words, and Z is
the segmentation labels for characters (START or
NON-START, used to indicate word boundaries)
or the POS tagging for words (NN, VV, JJ, etc.).
The conditional probability is defined as:
P (Z|X) = 1N(X) exp (
T
?
t=1
K
?
k=1
?kfk(Z,X, t))
(3)
where N(X) is a normalization term to guaran-
tee that the summation of the probability of all
label sequences is one. fk(Z,X, t) is the kth
localfeaturefunction at sequence position t. It
maps a pair of X and Z and an index t to {0,1}.
(?1, ..., ?K) is a weight vector to be learned from
training set. A large positive value of ?i means
that the ith feature function?s value is frequent to
be 1, whereas a negative value of ?i means the ith
feature function?s value is unlikely to be 1.
At decoding time, we are interested in finding
the segmentation sequence S? and POS tagging se-
quence T? that maximizes the probability defined
in Equation 2. Instead of exhaustively searching
the whole space of all possible segmentations, we
restrict our searching to S = {S1,S2, ...,SN},
where S is the restricted search space consisting
of N-best decoded segmentation sequences. This
N-best list of segmentation sequences, S, can be
obtained using modified Viterbi algorithm and A*
search (Schwartz and Chow, 1990).
3 Features
3.1 Features for Segmentation
We adopted the basic segmentation features used
in (Ng and Low, 2004). These features are summa-
rized in Table 1 ((1.1)-(1.7)). In these templates,
C0 refers to the current character, and C?n, Cn re-
fer to the characters n positions to the left and right
of the current character, respectively. Pu(C0) in-
dicates whether C0 is a punctuation. T (Cn) clas-
sifies the character Cn into four classes: num-
bers, dates (year, month, date), English letters and
all other characters. LBegin(C0), LEnd(C0) and
LMid(C0) represent the maximum length of words
found in a lexicon1 that contain the current char-
acter as either the first, last or middle character, re-
spectively. Single(C0) indicates whether the cur-
rent character can be found as a single word in the
lexicon.
Besides the adopted basic features mentioned
above, we also experimented with additional se-
mantic features (Table 1 (1.8)). For (1.8), Sem0
refers to the semantic class of current character,
and Sem?1, Sem1 represent the semantic class
of characters one position to the left and right of
the current character, respectively. We obtained
a character?s semantic class from HowNet (Dong
and Dong, 2006). Since many characters have
multiple semantic classes defined by HowNet, it
is a non-trivial task to choose among the differ-
ent semantic classes. We performed contextual
disambiguation of characters? semantic classes by
calculating semantic class similarities. For ex-
ample, let us assume the current character is
(look,read) in a word context of ?(read
1We compiled our lexicon from three external re-
sources. HowNet: www.keenage.com; On-Line Chinese
Tools: www.mandarintools.com; Online Dictionary from
Peking University: http://ccl.pku.edu.cn/doubtfire/Course/
Chinese%20Information%20Processing/Source Code/
Chapter 8/Lexicon full 2000.zip
206
newspaper). The character (look) has two se-
mantic classes in HowNet, i.e. ?(read) and 