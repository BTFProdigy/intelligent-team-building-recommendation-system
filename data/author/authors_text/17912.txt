The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 281?288,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
NAIST at the HOO 2012 Shared Task
Keisuke Sakaguchi, Yuta Hayashibe, Shuhei Kondo, Lis Kanashiro
Tomoya Mizumoto, Mamoru Komachi, Yuji Matsumoto
Graduate School of Information Science
Nara Institute of Science and Technology
8916-5, Takayama, Ikoma, Nara 630-0192, Japan
{ keisuke-sa, yuta-h, shuhei-k, lis-k, tomoya-m, komachi, matsu }@is.naist.jp
Abstract
This paper describes the Nara Institute of Sci-
ence and Technology (NAIST) error correc-
tion system in the Helping Our Own (HOO)
2012 Shared Task. Our system targets prepo-
sition and determiner errors with spelling cor-
rection as a pre-processing step. The re-
sult shows that spelling correction improves
the Detection, Correction, and Recognition F-
scores for preposition errors. With regard to
preposition error correction, F-scores were not
improved when using the training set with cor-
rection of all but preposition errors. As for
determiner error correction, there was an im-
provement when the constituent parser was
trained with a concatenation of treebank and
modified treebank where all the articles ap-
pearing as the first word of an NP were re-
moved. Our system ranked third in preposi-
tion and fourth in determiner error corrections.
1 Introduction
Researchers in natural language processing have fo-
cused recently on automatic grammatical error de-
tection and correction for English as a Second Lan-
guage (ESL) learners? writing. There have been a lot
of papers on these challenging tasks, and remark-
ably, an independent session for grammatical error
correction took place in the ACL-2011.
The Helping Our Own (HOO) shared task (Dale
and Kilgarriff, 2010) is proposed for improving the
quality of ESL learners? writing, and a pilot run with
six teams was held in 2011.
The HOO 2012 shared task focuses on the cor-
rection of preposition and determiner errors. There
has been a lot of work on correcting preposition and
determiner errors, where discriminative models such
as Maximum Entropy and Averaged Perceptron (De
Felice and Pulman, 2008; Rozovskaya and Roth,
2011) and/or probablistic language models (Gamon,
2010) are generally used.
In addition, it is pointed out that spelling and
punctuation errors often disturb grammatical error
correction. In fact, some teams reported in the
HOO 2011 that they corrected spelling and punc-
tuation errors before correcting grammatical errors
(Dahlmeier et al, 2011).
Our strategy for HOO 2012 follows the above
procedure. In other words, we correct spelling er-
rors at the beginning, and then train classifiers for
correcting preposition and determiner errors. The
result shows our system achieved 24.42% (third-
ranked) in F-score for preposition error correc-
tion, 29.81% (fourth-ranked) for determiners, and
27.12% (fourth-ranked) for their combined.
In this report, we describe our system architec-
ture and the experimental results. Sections 2 to 4
describe the system for correcting spelling, prepo-
sition, and determiner errors. Section 5 shows the
experimental design and results.
2 System Architecture for Spelling
Correction
Spelling errors in second language learners? writing
often disturb part-of-speech (POS) tagging and de-
pendency parsing, becoming an obstacle for gram-
matical error detection and correction tasks. For ex-
ample, POS tagging for learners? writing fails be-
281
e.g. I think it is *verey/very *convent/convenient for the group.
without spelling error correction: ... (?it?, ?PRP?), (?is?, ?VBZ?), (?verey?, ?PRP?), (?convent?, ?NN?), ...
with spelling error correction : ... (?It?, ?PRP?), (?is?, ?VBZ?), (?very?, ?RB?), (?convenient?, ?JJ?), ...
Figure 1: POS tagging for learners? writing without and with spelling error correction.
cause of misspelled words (Figure 1).1
To reduce errors derived from misspelled words,
we conduct spelling error correction as a pre-
processing task. The procedure of spelling error cor-
rection we use is as follows. First of all, we look for
misspelled words and suggest candidates by GNU
Aspell2, an open-source spelling checker. The can-
didates are ranked by the probability of 5-gram lan-
guage model built from Google N-gram (Web 1T
5-gram Version 1)3 (Brants and Franz, 2006) with
IRST LM Toolkit (Federico and Cettolo, 2007).4 Fi-
nally, according to the rank, we changed the mis-
spelled word into the 1-best candidate word.
In a preliminary experiment, where we use the
original CLC FCE dataset,5 our spelling error cor-
rection obtains 52.4% of precision, 72.2% of recall,
and 60.7% of F-score.
We apply the spelling error correction to the train-
ing and test sets provided, and use both spelling-
error and spelling-error-free sets for comparison.
3 System Architecture for Preposition
Error Correction
There are so many prepositions in English. Because
it is difficult to perform multi-class classification,
we focus on twelve prepositions: of, in, for, to, by,
with, at, on, from, as, about, since, which account
for roughly 91% of preposition usage (Chodorow et
al., 2010).
The errors are classified into three categories ac-
cording to their ways of correction. First, replace-
ment error indicates that learners use a wrong
preposition. For instance, with in Example (1) is a
1The example is extracted from the CLC FCE dataset and
part-of-speech tagged by Natural Language Toolkit (NLTK).
http://www.nltk.org/
2GNU Aspell 0.60.6.1 http://aspell.net/
3http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?
catalogId=LDC2006T13
4irstlm5.70 http://sourceforge.net/projects/irstlm/
5In the CLC FCE dataset, misspelled words are corrected
and tagged with a label ?S?.
replacement error.
I went there withby bus. (1)
Second, insertion error points out they incor-
rectly inserted a preposition, such as ?about? in Ex-
ample (2).6
We discussed aboutNONE the topic. (2)
Third, deletion error means they fail to write
obligatory prepositions. For example, ?NONE? in
Example (3) is an deletion error.
This is the place to relax NONEin. (3)
Replacement and insertion error correction can be
regarded as a multi-class classification task at each
preposition occurrence. However, deletion errors
differ from the other two types of errors in that they
may occur at any place in a sentence. Therefore, we
build two models, a combined model for replace-
ment and insertion errors and a model for deletion
errors, taking the difference into account.
For the model of replacement and insertion errors,
we simultaneously perform error detection and cor-
rection with a single model.
For the model of deletion errors, we only check
whether direct objects of verbs need prepositions,
because it is time consuming to check all the gaps
between words. Still, it covers most deletion errors.7
We merge the outputs of the two models to get the
final output.
We used two types of training sets extracted from
the original CLC-FCE dataset. One is the ?gold?
set, where training sentences are corrected except
for preposition errors. In the gold set, spelling er-
rors are also corrected to the gold data in the corpus.
The other is the ?original? set, which includes the
6?NONE? means there are no words.
72,407 out of 5,324 preposition errors in CLC-FCE are be-
tween verbs and nouns.
282
Type Name Description (NP and PRED refer a noun phrase and a predicate.)
Lexical Token n-gram Token n-grams in a 2 word window around the preposition
POS n-gram POS n-grams in a 2 word window around the preposition
HEAD PREC VP The head verb in the preceding verb phrase
HEAD PREC NP The head noun in the preceding noun phrase
HEAD FOLLOW NP The head noun in the following noun phrase
Parsing HEAD Head of the preposition
HEAD POS POS of the head
COMP Complement of the preposition
COMPLEMENT POS POS of the complement
HEAD RELATION Prep-Head relation name
COMPLEMENT RELATION Prep-Comp relation name
Phrase Structure PARENT TAG TAG of the preposition?s parent
GRANDPARENT TAG TAG of the preposition?s grandparent
PARENT LEFT Left context of the preposition parent
PARENT RIGHT Right context of the preposition?s parent
Web N-gram COUNT For the frequency fprep,i of i (3 to 5) window size phrase including
the preposition prep, the value of log100(fi + 1)
PROPORTION The proportion pprep,i (i is 3 to 5).
pprep,i = fprep,i?
k?T fk,i
, given the set of target prepositions T .
Semantic WORDNET CATEGORY WordNet lexicographer classes which are about 40 broad semantic
categories for all words used as surface features. As De Felice and
Pulman (2008) did not perform word sense disambiguation, neither
did we.
Table 1: Baseline features for English preposition error correction.
original CLC-FCE plain sentences.
We performed sentence splitting using the im-
plementation of Kiss and Strunk (2006) in NLTK
2.0.1rc2. We conducted dependency parsing by
Stanford parser 1.6.9.8
We used the features described in (Tetreault et al,
2010) as shown in Table 1 with Maximum Entropy
(ME) modeling (Berger et al, 1996) as a multi-class
classifier. We used the implementation of Maximum
Entropy Modeling Toolkit9 with its default parame-
ters. For web n-gram calculation, we used Google
N-gram with a search system for giga-scale n-gram
corpus, called SSGNC 0.4.6.10
4 System Architecture for Determiner
Error Correction
We focused on article error correction in the deter-
miner error correction subtask, because the errors
related to articles significantly outnumber the errors
unrelated to them. Though more than twenty types
of determiners are involved in determiner error cor-
rections of the HOO training set, over 90% of errors
8http://nlp.stanford.edu/software/lex-parser.shtml
9https://github.com/lzhang10/maxent
10http://code.google.com/p/ssgnc/
are related to three articles a, an and the. We defined
article error correction as a multi-class classification
problem with three classes, a, the and null article,
and assumed that target articles are placed at the left
boundary of a noun phrase (NP). The indefinite ar-
ticle an was normalized to a in training and testing,
and restored to an later in an example-based post-
processing step. If the system output was a and the
word immediately after a appeared more frequently
with an than with a in the training corpus, a was re-
stored to an. If the word appeared equally frequently
with a and an or didn?t appear in the training corpus,
a was restored to an if the word?s first character was
one of a, e, i, o, u.
Each input sentence was parsed using the Berke-
ley Parser11 with two models, ?normal? and
?mixed?. The ?normal? model was trained on a tree-
bank of normal English sentences. In preliminary
experiments, the ?normal? model sometimes mis-
judged the span of NPs in ESL writers? sentences
due to missing articles. So we trained the ?mixed?
model on a concatenation of the normal treebank
and a modified treebank in which all the articles ap-
pearing as the first word of an NP were removed. By
11version 1.1, http://code.google.com/p/berkeleyparser/
283
Name Description
HeadNounWord The word form of the head noun
HeadNounTag The POS tag of the head noun
ObjOfPrep Indicates that the head noun is an object of a preposition
PrepWord The word form of the preposition
PrepHeadWord The word form of the preposition?s syntactic parent
PrepHeadTag The POS tag of the preposition?s syntactic parent
ContextWindowTag
The POS tag of the words in a 3 word window
around the candidate position for the article
ContextWindowWord
The word form of the word immediately following
the candidate position for the article
ModByDetWord The word form of the determiner that modifies the head noun
ModByAdjWord The word form of the adjective that modifies the head noun
ModByAdjTag The POS tag of the adjective that modifies the head noun
ModByPrep Indicates that the head noun is modified by a preposition
ModByPrepWord The word form of the preposition that modifies the head noun
ModByPossesive Indicates that the head noun is modified by a possesive
ModByCardinal Indicates that the head noun is modified by a cardinal number
ModByRelative Indicates that the head noun is modified by a relative clause
Table 2: Feature templates for English determiner correction.
augmenting the training data for the parser model
with sentences lacking articles, the span of NPs that
lack an article might have better chance of being cor-
rectly recognized. In addition, dependency informa-
tion was extracted from the parse using the Stanford
parser 1.6.9.
For each NP in the parse, we extracted a feature
vector representation. We used the feature templates
shown in Table 2, which are inspired by (De Felice,
2008) and adapted to the CFG representation.
For the parser models, we trained the ?normal?
model on the WSJ part of Penn Treebank sections
02-21 with the NP annotation by Vadas and Curran
(2007). The ?mixed? model was trained on the con-
catenation of the WSJ part and its modified version.
For the classification model, we used the written part
of the British National Corpus (BNC) in addition to
the CLC FCE Dataset, because the amount of in-
domain data was limited. In examples taken from
the CLC FCE Dataset, the true labels after the cor-
rection were used. In examples taken from the BNC,
the article of each NP was used as the label. We
trained a linear classifier using opal12 with the PA-I
algorithm. We also used the feature augmentation
12http://www.tkl.iis.u-tokyo.ac.jp/?ynaga/opal/
Subsystem Parameters
Run Spelling Preposition Determiner
0 no change gold mixed
1 no change gold normal
2 no change original mixed
3 no change original normal
4 corrected gold mixed
5 corrected gold normal
6 corrected original mixed
7 corrected original normal
Table 3: Distinct configurations of the system.
approach of (Daume? III, 2007) for domain adapta-
tion.
5 Experiment and Result
Previously undisclosed data extracted from the
CLC-FCE dataset was provided as a test set by the
HOO organizers. The test set includes 100 essays
and each contains 180.1 word tokens on average.
We defined eight distinct configurations based
on our subsystem parameters (Table 3). The offi-
cial task evaluation uses three metrics (Detection,
Recognition, and Correction), and three measures
Precision, Recall, and F-score were computed13 for
13For details about the evaluation metrics, see http://
284
Detection Correction Recognition
Run R P F R P F R P F
0 29.58 34.09 31.67 19.86 22.90 21.27 26.71 30.78 28.60
1 28.69 36.41 32.09 19.42 24.64 21.72 25.82 32.77 28.88
2? 28.91 37.21 32.54 20.97 26.98 23.60 26.26 33.80 29.56
3 28.03 40.18 33.02 20.52 29.43 24.18 25.38 36.39 29.90
4 30.24 33.66 31.86 20.75 23.09 21.86 27.37 30.46 28.83
5 29.13 35.57 32.03 19.64 23.98 21.60 26.26 32.07 28.88
6 29.35 36.23 32.43 21.41 26.43 23.65 26.26 32.42 29.02
7 28.25 38.67 32.65 20.30 27.29 23.46 25.16 34.44 29.08
Table 4: Result for preposition and determiner errors combined before revisions.
?We re-evaluated the Run2 because we submitted the Run2 with the same condition as Run0.
Detection Correction Recognition
Spelling Preposition R P F R P F R P F
no change gold 25.00 34.70 29.06 14.40 20.00 16.74 20.76 28.82 24.13
no change original 23.30 42.63 30.13 16.52 30.23 21.36 19.91 36.43 25.75
corrected gold 26.69 34.80 30.21 15.25 19.88 17.26 22.45 29.28 25.41
corrected original 24.57 41.13 30.76 16.52 27.65 20.68 20.33 34.04 25.46
Table 5: Result for preposition errors before revisions.
each metric.
Table 4 to Table 9 show the overall results of our
systems. In terms of the effect of pre-processing,
spelling correction improved the F-score of Detec-
tion, Correction, and Recognition for preposition er-
rors after revision, whereas there were fluctuations
in other conditions. This may be because there were
a few spelling errors corrected in the test set.14 An-
other reason why no stable improvement was found
in determiner error correction is because spelling
correction often produces nouns that affect the de-
terminer error detection and correction more sensi-
tively than prepositions. For example, a misspelled
word *freewho / free who was corrected as freezer.
This type of error may have increased false posi-
tives. The example *National Filharmony / the Na-
tional Philharmony was corrected as National Flem-
ing, where the proper noun Fleming does not need a
determiner and this type of error increased false neg-
atives.
As for preposition error correction, the classifier
performed better when it was trained with the ?origi-
nal? set rather than the error-corrected (all but prepo-
sition errors) ?gold? set. The reason for this is that
the gold set is trained with the test set that contains
correcttext.org/hoo2012/eval.html
14There was one spelling correction per document in average.
several types of errors which the original CLC-FCE
dataset alo contains. Therefore, the ?original? clas-
sifier is more optimised and suitable for the test set
than the ?gold? one.
For determiner error correction, the ?mixed?
model improved precision and F-score in the addi-
tional experiments.
5.1 Error Analysis of Preposition Correction
We briefly analyze some errors in our proposed
model according to the three categories of errors.
First, most replacement errors require deep under-
standing of context. For instance, for in Example (4)
must be changed to to. However, modifications of is
also often used, so it is hard to decide either to or of
is suitable based on the values of N-gram frequen-
cies.
Its great news to hear you have been given
extra money and that you will spend it in
modifications forto the cinema.
(4)
Second, most insertion errors need a grammatical
judgement rather than a semantic one. For instance,
?in? in Example (5) must be changed to ?NONE.?
Their love had always been kept inNONE se-
cret
(5)
In order to correct this error, we need to recog-
285
Detection Correction Recognition
Spelling Determiner R P F R P F R P F
no change mixed 34.10 33.18 33.63 25.80 25.11 25.45 33.17 32.28 32.72
no change normal 32.25 37.43 34.65 24.88 28.87 26.73 31.33 36.36 33.66
corrected mixed 33.64 32.30 32.95 26.72 25.66 26.18 32.71 31.41 32.05
corrected normal 31.33 35.78 33.41 24.42 27.89 26.04 30.41 34.73 32.43
Table 6: Result for determiner errors before revisions.
Detection Correction Recognition
Run R P F R P F R P F
0 31.28 37.65 34.18 22.62 27.22 24.71 28.54 34.35 31.17
1 30.44 40.33 34.69 22.19 29.41 25.30 27.69 36.69 31.56
2? 31.07 41.76 35.63 23.04 30.96 26.42 28.11 30.96 32.24
3 30.23 45.25 36.24 22.62 33.86 27.12 27.27 40.82 32.69
4 31.92 37.10 34.31 23.46 27.27 25.22 29.17 33.90 31.36
5 30.86 39.35 34.59 22.41 28.57 25.11 28.11 35.84 31.51
6 31.71 40.87 35.71 23.89 30.79 26.90 28.75 37.05 32.38
7 30.65 43.80 36.06 22.83 32.62 26.86 27.69 39.57 32.58
Table 7: Result for preposition and determiner errors combined after revisions.
?We re-evaluated the Run2 because we submitted the Run2 with the same condition as Run0.
nize ?keep? takes an object and a complement; in
Example (5) ?love? is the object and ?secret? is
the complement of ?keep? while the former is left-
extraposed. A rule-based approach may be better
suited for these cases than a machine learning ap-
proach.
Third, most deletion errors involve discrimination
between transitive and intransitive. For instance,
?NONE? in Example (6) must be changed to ?for?,
because ?wait? is intransitive.
I?ll wait NONEfor your next letter. (6)
To deal with these errors, we may use rich knowl-
edge about verbs such as VerbNet (Kipper et al,
2000) and FrameNet (Baker et al, 1998) in order
to judge whether a verb is transitive or intransitive.
5.2 Error Analysis of Determiner Correction
We conducted additional experiments for determiner
errors and report the results here because the sub-
mitted system contained a bug. In the submit-
ted system, while the test data were parsed by the
?mixed? model, the training data and the test data
were parsed by the default grammar provided with
Berkeley Parser. Moreover, though there were about
5.5 million sentences in the BNC corpus, only about
2.7 million of them had been extracted. Though
these errors seem to have improved the performance,
it is difficult to specify which errors had positive ef-
fects.
Table 10 shows the result of additional experi-
ments. Unlike the submitted system, the ?mixed?
model contributed toward a higher precision and F-
score. Though the two parser models parsed the
sentences differently, the difference in the syntactic
analysis of test sentences did not always led to dif-
ferent output by the downstream classifiers. On the
contrary, the classifiers often returned different out-
puts even for an identically parsed sentence. In fact,
the major source of the performance gap between the
two models was the number of the wrong outputs
rather than the number of correct ones. While the
?mixed? model without spelling correction returned
146 outputs, of which 83 were spurious, the ?nor-
mal? model without spelling correction produced
209 outputs, of which 143 were spurious. This may
suggest the difference of the two models can be at-
tributed to the difference in the syntactic analysis of
the training data.
One of the most frequent types of errors com-
mon to the two models were those caused by mis-
spelled words. For example, when your letter was
misspelled to be *yours letter, it was regarded as an
286
Detection Correction Recognition
Spelling Preposition R P F R P F R P F
no change gold 26.63 38.23 31.40 17.62 25.29 20.77 23.36 33.52 27.53
no change original 26.22 49.61 34.31 18.44 34.88 24.12 22.54 42.63 29.49
corrected gold 28.27 38.12 32.47 18.44 24.86 21.17 25.00 33.70 28.70
corrected original 27.86 48.22 35.32 19.26 33.33 24.41 24.18 41.84 30.64
Table 8: Result for preposition errors after revisions.
Detection Correction Recognition
Spelling Determiner R P F R P F R P F
no change mixed 35.37 36.32 35.84 27.94 28.69 28.31 34.06 34.97 34.51
no change normal 33.62 41.17 37.01 27.07 33.15 29.80 32.31 39.57 35.57
corrected mixed 34.93 35.39 35.16 28.82 29.20 29.01 33.62 34.07 33.84
corrected normal 32.75 39.47 35.79 26.63 32.10 29.11 31.44 37.89 34.36
Table 9: Result for determiner errors after revisions.
Detection Correction Recognition
Spelling Determiner R P F R P F R P F
no change mixed 27.39 43.15 33.51 23.04 36.30 28.19 27.39 43.15 33.51
no change normal 28.69 31.57 30.06 22.61 24.88 23.69 28.69 31.57 30.06
corrected mixed 27.39 41.44 31.98 22.61 34.21 27.22 26.96 40.79 32.46
corrected normal 30.43 33.33 31.82 24.34 26.67 25.45 30.00 32.86 31.36
Table 10: Result of additional experiments for determiner errors after revisions.
NP without a determiner resulting in a false posi-
tive such as *a yours letter. Among the other types
of errors, several seemed to be caused by the infor-
mation from the context window. For instance, the
system output for It was last month and ... was it
was *the last month and .... It is likely that the word
last triggered the misinsertion here. Such kind of
errors might be avoided by conjunctive features of
context information and the head word. Last but not
least, compound errors were also frequent and prob-
ably the most difficult to solve. For example, it is
quite difficult to correct *for a month to per month
if we are dealing with determiner errors and prepo-
sition errors separately. A more sophisticated ap-
proach such as joint modeling seems necessary to
correct this kind of errors.
6 Conclusion
This report described the architecture of our prepo-
sition and determiner error correction system. The
experimental result showed that spelling correction
advances the performance of Detection, Correction
and Recognition for preposition errors. In terms of
preposition error correction, F-scores were not im-
proved when the error-corrected dataset was used.
As to determiner error correction, there was an im-
provement when the constituent parser was trained
on a concatenation of treebank and modified tree-
bank where all the articles appearing as the first
word of an NP were removed.
Acknowledgements
This work was partly supported by the National In-
stitute of Information and Communications Tech-
nology Japan.
287
References
Collin F Baker, Charles J Fillmore, and John B Lowe.
1998. The Berkeley FrameNet Project. In Proceed-
ings of the 36th Annual Meeting of the Association for
Computational Linguistics, pages 86?90, Montreal,
Quebec, Canada.
Adam L. Berger, Vincent J. Della Pietra, and Stephen
A. Della Pietra. 1996. A Maximum Entropy Ap-
proach to Natural Language Processing. Computa-
tional Linguistics, 22(1):39?71.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram
Corpus Version 1.1. Linguistic Data Consortium.
Martin Chodorow, Michael Gamon, and Joel Tetreault.
2010. The Utility of Article and Preposition Error Cor-
rection Systems for English Language Learners: Feed-
back and Assessment. Language Testing, 27(3):419?
436.
Daniel Dahlmeier, Hwee Tou Ng, and Thanh Phu Tran.
2011. NUS at the HOO 2011 Pilot Shared Task. In
Proceedings of the 13th European Workshop on Nat-
ural Language Generation, pages 257?259, Nancy,
France.
Robert Dale and Adam Kilgarriff. 2010. Helping Our
Own: Text Massaging for Computational Linguistics
as a New Shared Task. In Proceedings of the 6th In-
ternational Natural Language Generation Conference,
pages 261?266, Trim, Co. Meath, Ireland.
Hal Daume? III. 2007. Frustratingly Easy Domain Adap-
tation. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
256?263, Prague, Czech Republic.
Rachele De Felice and Stephen G. Pulman. 2008. A
Classifier-Based Approach to Preposition and Deter-
miner Error Correction in L2 English. In Proceed-
ings of the 22nd International Conference on Compu-
tational Linguistics, pages 169?176, Manchester, UK.
Rachele De Felice. 2008. Automatic Error Detection in
Non-native English. Ph.D. thesis, University of Ox-
ford.
Marcello Federico and Mauro Cettolo. 2007. Efficient
Handling of N-gram Language Models for Statistical
Machine Translation. In Proceedings of the Second
Workshop on Statistical Machine Translation, pages
88?95, Prague, Czech Republic.
Michael Gamon. 2010. Using Mostly Native Data to
Correct Errors in Learners? Writing. In Human Lan-
guage Technologies: The 2010 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 163?171, Los An-
geles, California.
Karin Kipper, Hoa Trang Dang, and Martha Palmer.
2000. Class-based Construction of a Verb Lexicon. In
Proceedings of the 7th National Conference on Artifi-
cial Intelligence, pages 691?696, Austin, Texas, USA.
Tibor Kiss and Jan Strunk. 2006. Unsupervised Multi-
lingual Sentence Boundary Detection. Computational
Linguistics, 32(4):485?525.
Alla Rozovskaya and Dan Roth. 2011. Algorithm Selec-
tion and Model Adaptation for ESL Correction Tasks.
In Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies, pages 924?933, Portland, Ore-
gon, USA.
Joel Tetreault, Jennifer Foster, and Martin Chodorow.
2010. Using Parse Features for Preposition Selec-
tion and Error Detection. In Proceedings of the 47th
Annual Meeting of the Association for Computational
Linguistics Short Papers, pages 353?358, Uppsala,
Sweden.
David Vadas and James Curran. 2007. Adding Noun
Phrase Structure to the Penn Treebank. In Proceedings
of the 45th Annual Meeting of the Association of Com-
putational Linguistics, pages 240?247, Prague, Czech
Republic.
288
Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 134?139,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
NAIST at the NLI 2013 Shared Task
Tomoya Mizumoto, Yuta Hayashibe
Keisuke Sakaguchi, Mamoru Komachi, Yuji Matsumoto
Graduate School of Information Science
Nara Institute of Science and Technology
8916-5, Takayama, Ikoma, Nara 630-0192, Japan
{ tomoya-m, yuta-h, keisuke-sa, komachi, matsu }@is.naist.jp
Abstract
This paper describes the Nara Institute of
Science and Technology (NAIST) native lan-
guage identification (NLI) system in the NLI
2013 Shared Task. We apply feature selec-
tion using a measure based on frequency for
the closed track and try Capping and Sampling
data methods for the open tracks. Our system
ranked ninth in the closed track, third in open
track 1 and fourth in open track 2.
1 Introduction
There have been many studies using English as a
second language (ESL) learner corpora. For exam-
ple, automatic grammatical error detection and cor-
rection is one of the most active research areas in this
field. More recently, attention has been paid to na-
tive language identification (NLI) (Brooke and Hirst,
2012; Bykh and Meurers, 2012; Brooke and Hirst,
2011; Wong and Dras, 2011; Wong et al, 2011).
Native language identification is the task of identi-
fying the ESL learner?s L1 given a learner?s essay.
The NLI Shared Task 2013 (Tetreault et al, 2013)
is the first shared task on NLI using the com-
mon dataset ?TOEFL-11? (Blanchard et al, 2013;
Tetreault et al, 2012). TOEFL-11 consists of essays
written by learners of 11 native languages (Arabic,
Chinese, French, German, Hindi, Italian, Japanese,
Koran, Spanish, Telugu, Turkish), and it contains
1,100 essays for each native language. In addition,
the essay topics are balanced, and the number of top-
ics is 8.
In the closed track, we tackle feature selection
for increasing accuracy. We use a feature selection
method based on the frequency of each feature (e.g.,
document frequency, TF-IDF).
In the open tracks, to address the problem of im-
balanced data, we tried two approaches: Capping
and Sampling data in order to balance the size of
training data.
In this paper, we describe our system and exper-
imental results. Section 2 describes the features we
used in the system for NLI. Section 3 and Section 4
describe the systems for closed track and open track
in NLI Shared Task 2013. Section 5 describes the re-
sults for NLI Shared Task 2013. Section 6 describes
the experimental result for 10-fold cross validation
on the data set used by Tetreault et al (2012).
2 Features used in all tracks
In this section, we describe the features in our sys-
tems. We formulate NLI as a multiclass classifica-
tion task. Following previous work, we use LIB-
LINEAR 2 for the classification tool and tune the C
parameter using grid-search.
We select the features based on previous work
(Brooke and Hirst, 2012; Tetreault et al, 2012). All
features used are binary. We treated the features as
shown in Table 1. The example of features in Table
1 shows the case whose input is ?I think not a really
difficult question?.
We use a special symbol for the beginning and
end of sentence (or word) for bigrams and trigrams.
For surface forms, we lowercased all words. POS,
POS-function and dependency features are extracted
1http://www.lextek.com/manuals/onix/stopwords1.html
2http://www.csie.ntu.edu.tw/?cjlin/liblinear/
134
Name Description Example
Word N-gram (N=1,2) Surface form of the word. N=1 i, think, not
N=2 BOS i, i think
POS N-gram (N=2,3) POS tags of the word. N=2 BOS PRP, PRP VBP
N=3 BOS PRP VBP, PRP VBP RB
Character N-gram (N=2,3) N=2 ? t, t h, hi, in, nk, k$
N=3 ? t h, t h i
POS-function N-gram (N=2,3) We use surface form for words in stop
word list 1, otherwise we use POS form.
N=2 RB difficult, difficult NN
N=3 RB difficult NN
Dependency the surface and relation name (i, nsubj)
the surface and the dependend token?s
surface
(think, i)
the surface, relation name and the de-
pendend token?s surface
(nsubj, i, think)
Tree substitution grammer Fragments of TSG (PRP UNK-INITC-
KNOWNLC) (VB think)
(NP RB DT ADJP NN)
(JJ UNK-LC)
Table 1: All features for native language identification.
using the Stanford Parser 2.0.2 3.
We use tree substitution grammars as fea-
tures. TSGs are generalized context-free grammars
(CFGs) that allow nonterminals to re-write to tree
fragments. The fragments reflect both syntactic and
surface structures of a given sentence more effi-
ciently than using several CFG rules. In practice,
efficient Bayesian approaches have been proposed
in prior work (Post and Gildea, 2009). In terms
of the application of TSG to NLI task, (Swanson
and Charniak, 2012) have shown a promising re-
sult. Post (2011) also uses TSG to judge grammat-
icality of a sentence written by language learners.
With these previous findings in mind, we also ex-
tract TSG rules. We use the training settings and
public software from Post (2011)4, obtaining 21,020
unique TSG fragments from the training dataset of
the TOEFL-11 corpus.
3 Closed Track
In this section, we describe our system for the closed
track. We use the tools and features described in
Section 2.
In our system, feature selection is performed us-
ing a measure based on frequency. Although Tsur
3http://nlp.stanford.edu/software/lex-parser.shtml
4https://github.com/mjpost/post2011judging
and Rappoport (2007) used TF-IDF, they use it to
decrease the influence of topic bias rather than for
increasing accuracy. Brooke and Hirst (2012) used
document frequency for feature selection, however
it does not affect accuracy.
We use the native language frequency (hereafter
we refer to this as NLF). NLF is the number of na-
tive languages a feature appears in. Thus, NLF takes
values from 1 to 11. Figure 1 shows an example of
NLF. The word bigram feature ?in Japan? appears
only in essays of which the learners? native language
is Japanese, therefore the NLF is 1.
The assumption behind using this feature is that a
feature which appears in all native languages affects
NLI less, while a feature which appears in few na-
tive language affects NLI more. The features whose
NLFs are 11 include e.g. ?there are?, ?PRP VBP?
and ?a JJ NN?. Table 2 shows some examples of the
features appearing in only 1 native language in the
TOEFL-11 corpus. The features include place-name
or company name such as ?tokyo?, ?korea?, ?sam-
sung?, which are certainly specific for some native
language.
135
Native Language
Chinese Japanese Korean
carry more this : NN samsung
i hus become of tokyo of korea
JJ whole and when i worked debatable whether
striking conclusion usuful NN VBG whether
traffic tools oppotunity for in thesedays
Table 2: Example of feature appearing in 1 native language for Chinese, Japanese and Korean
Figure 1: Example of native language frequency
Native Language # of articles
Japanese 258,320
Mandarin 48,364
Korean 31,188
Spanish 5,106
Italian 2,589
Arabic 1,549
French 1,168
German 832
Turkish 504
Hindi 223
Telugu 19
Table 3: Distribution of native languages in Lang-8
corpus
4 Open tracks
4.1 Lang-8 corpus
For the open tracks, we used Lang-8 as a source to
create a learner corpus tagged with the native lan-
guages of learners. Lang-8 is a language learning
social networking service. 5 Users write articles
in their non-native languages and native speakers
correct them. We used all English articles written
through the end of 2012. We removed all sentences
which contain non-ASCII characters. 6
Almost all users register their native language on
the site. We regard users? registered native language
5http://lang-8.com/
6Some users also add translation in their native languages
for correctors? reference.
as the gold label for each article. We split the learner
corpus extracted from Lang-8 into sub-corpora by
the native languages. The numbers of articles in all
corpora are summarized in Table 3. Unfortunately,
some sub-corpora are too small to train the model.
For example, the Telugu corpus has only 19 articles.
In order to balance the size of the training data,
we tried two approaches: Capping and Sampling.
We confirmed in preliminary experiments that the
model with these approaches work better than the
model with the original sized data.
Capping
In this approach, we limit the size of a sub-corpus
for training to N articles. For a sub-corpus which
contains over N articles, we randomly extract ar-
ticles up to N . We set N = 5000 and adapt this
approach for Run 1 and Run 3 in the open tracks.
Sampling
In this approach, we equalize the size of all sub-
corpora. For corpora which contain less than N ar-
ticles, we randomly copy articles until their size be-
comesN . We setN = 5000 and adapt this approach
for Run 2 and Run 4 in the open tracks.
4.2 Models
We compared two approaches with baseline features
and all features.
The models in Run 1 and Run 3 were trained with
the data created by the Capping approach, and the
models in Run 2 and Run 47 were trained by the
Sampling approach.
We used only word N-grams (N = 1, 2) as base-
line features. As extra features we used the follow-
ing features.
7We did not have time to train the model for Run 4 in the
open 1 track.
136
? POS N-grams (N = 2, 3)
? dependency
? character N-grams (N = 2, 3)
In open track 2, we also add the TOEFL-11
dataset to the training data for all runs.
5 Result for NLI shared Task 2013
Table 4 shows the results of our systems for NLI
Shared Task. Chance accuracy is 0.09. All results
outperform random guessing.
5.1 Closed track
In the closed track, we submitted 5 runs. Run 1
is the system using only word 1,2-grams features.
Run 2 is the system using all features with NLF fea-
ture selection (1 < NLF < 11). Run 3 is the system
using word 1,2-grams and POS 2,3-grams features.
Run 4 is the system using word 1,2-grams, POS 2,3-
grams, character 2,3-grams and dependency features
without parameter tuning. Run 5 is the system us-
ing word 1,2-grams without parameter tuning. The
method using the feature selection method we pro-
posed achieved the best performance of our systems.
5.2 Open tracks
Comparison of the two data balancing
approaches
In open track 1, the method of ?Sampling? out-
performs that of ?Capping? (Run 2 > Run 1). This
means even duplicated training data can improve the
performance.
On the other hand, in open track 2, ?Capping?
works better than ?Sampling? (Run 1 > Run 2 and
Run 3>Run 4). In the first place, the models trained
with both Lang-8 data and TOEFL data do not per-
form better than ones trained with only TOEFL data.
This means the less Lang-8 data we use, the better
performance we obtain.
Comparison on two feature sets
In open track 1, adding extra features seems to
have a bad influence because the result of Run 3
is worse than that of Run 1. This may be because
Lang-8 data is out of domain of the test corpus
(TOEFL).
Closed Open 1 Open 2
Run Accuracy Accuracy Accuracy
1 0.811 0.337 0.699
2 ?0.817 0.356 0.661
3 0.808 0.285 0.703
4 0.771 - 0.665
5 0.783 - -
Table 4: Result for systems which submitted in NLI
2013 ?We re-evaluated the Run2 because we submitted the
Run1 with the same output as Run2.
In open track 2, adding extra features makes the
performance better (Run 3 > Run 1, Run 4 > Run
2). In-domain TOEFL data seem to be effective for
training with extra features. In order to improve the
result with extra features in open track 2, domain
adaptation may be effective.
6 Experiment and Result for 10 fold
Cross-Validation
We conducted an experiment using 10-fold cross
validation on the data set used by Tetreault et al
(2012). Table 5 shows the results for different fea-
ture set. The table consists of 3 blocks; the first
block is results of the system using 1 feature, the
second block is the result of the system using word
1,2-grams feature and another feature, and the third
block is the result of the system using word 1,2-
grams and more features.
In the first block results, the system using the
word 1,2-grams feature achieved 0.8075. It is the
highest accuracy in the first block, and third highest
accuracy in the results of Table 5. From the second
block of results, adding an extra feature does not im-
prove accuracy, however in the third block the sys-
tems in (14) and (15) outperform the system using
only word 1,2-grams.
Table 6 shows the results of using feature selec-
tion by NLF. The table consists of 3 blocks; the
first block is the results of the system using features
whose NLF is smaller than N (N = 11, 10, 9, 8), the
second block is the results of the system using fea-
tures whose NLF is greater than N (N = 1, 2, 3, 4),
and the third block is the results of the system using
features whose NLF is smaller than 11 and greater
than N (N = 1, 2, 3, 4).
The best accuracy is achieved by excluding fea-
137
Feature Accuracy
(1) Word 1,2-gram 0.8075
(2) POS 2,3-gram 0.5555
(3) POS,Function 2,3-gram 0.7080
(4) Chracter 2,3-gram 0.6678
(5) Dependency 0.7236
(6) Tree substitution grammar 0.6455
(7) 1 + 2 0.7825
(8) 1 + 3 0.7913
(9) 1 + 4 0.7953
(10) 1 + 5 0.8020
(11) 1 + 6 0.7999
(12) 1 + 2 + 3 0.7849
(13) 1 + 2 + 3 + 4 0.8000
(14) 1 + 2 + 3 + 4 + 5 0.8097
(15) ALL 0.8088
Table 5: 10-fold cross validation results for each
feature
tures whose NLF is 1 or 11. While the results of the
first block and the second block are intuitive, the re-
sults of the third block are not (looking at the second
block of Table 6, excluding features whose NLF is
greater than N (1, 2, 3, 4) reduces accuracy). One
possible explanation is that features whose NLF is
1 includes features that rarely appear in the training
corpus.
7 Conclusion
In this paper, we described our systems for the NLI
Shared Task 2013. We tried feature selection using
native language frequency for the closed track and
Capping and the Sampling data to balance the size of
training data for the open tracks. The feature selec-
tion we proposed improves the performance for NLI.
The system using our feature selection achieved
0.817 on the test data of NLI Shared Task and 0.821
using 10-fold cross validation. While the Sampling
system outperformed Capping system for open track
1, the Capping system outperformed Sampling sys-
tem in open track 2 (because it reduced the amount
of out of domain data).
References
Daniel Blanchard, Joel Tetreault, Derrick Higgins, Aoife
Cahill, and Martin Chodorow. 2013. Toefl11: A cor-
Accuracy
NLF < 11 0.8176
NLF < 10 0.8157
NLF < 9 0.8123
NLF < 8 0.8098
1 < NLF 0.8062
2 < NLF 0.8062
3 < NLF 0.8057
4 < NLF 0.8053
1 < NLF < 11 0.8209
2 < NLF < 11 0.8206
3 < NLF < 11 0.8201
4 < NLF < 11 0.8195
Table 6: 10-fold cross validation results using
feature selection by NLF. (feature selection is not
applied to word N-grams features.)
pus of non-native english. Technical report, Educa-
tional Testing Service.
Julian Brooke and Graeme Hirst. 2011. Native language
detection with ?cheap? learner corpora. In Proceedings
of LCR 2011.
Julian Brooke and Graeme Hirst. 2012. Robust, lexical-
ized native language identification. In Proceedings of
COLING 2012, pages 391?408.
Serhiy Bykh and Detmar Meurers. 2012. Native lan-
guage identification using recurring n-grams ? inves-
tigating abstraction and domain dependence. In Pro-
ceedings of COLING 2012, pages 425?440.
Matt Post and Daniel Gildea. 2009. Bayesian Learning
of a Tree Substitution Grammar. In Proceedings of the
ACL-IJCNLP 2009, pages 45?48.
Matt Post. 2011. Judging Grammaticality with Tree Sub-
stitution Grammar Derivations. In Proceedings of ACL
2011, pages 217?222.
Ben Swanson and Eugene Charniak. 2012. Native Lan-
guage Detection with Tree Substitution Grammars. In
Proceedings of ACL 2012, pages 193?197.
Joel Tetreault, Daniel Blanchard, Aoife Cahill, and Mar-
tin Chodorow. 2012. Native tongues, lost and
found: Resources and empirical evaluations in native
language identification. In Proceedings of COLING
2012, pages 2585?2602.
Joel Tetreault, Daniel Blanchard, and Aoife Cahill. 2013.
A report on the first native language identification
shared task. In Proceedings of the Eighth Workshop
on Building Educational Applications Using NLP.
Oren Tsur and Ari Rappoport. 2007. Using classifier
features for studying the effect of native language on
the choice of written second language words. In Pro-
ceedings of CACLA, pages 9?16.
138
Sze-Meng Jojo Wong and Mark Dras. 2011. Exploiting
parse structures for native language identification. In
Proceedings of EMNLP 2011, pages 1600?1610.
Sze-Meng Jojo Wong, Mark Dras, and Mark Johnson.
2011. Topic modeling for native language identifi-
cation. In Proceedings of the Australasian Language
Technology Association Workshop 2011, pages 115?
124.
139
Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 26?33,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
NAIST at 2013 CoNLL Grammatical Error Correction Shared Task
Ippei Yoshimoto, Tomoya Kose, Kensuke Mitsuzawa, Keisuke Sakaguchi,
Tomoya Mizumoto, Yuta Hayashibe, Mamoru Komachi, Yuji Matsumoto
Nara Institute of Science and Technology
8916-5 Takayama, Ikoma, Nara 630-0192, Japan
{ippei-y,tomoya-kos,kensuke-mi,keisuke-sa,tomoya-m,yuta-h,komachi,matsu}@is.naist.jp
Abstract
This paper describes the Nara Institute
of Science and Technology (NAIST) er-
ror correction system in the CoNLL 2013
Shared Task. We constructed three sys-
tems: a system based on the Treelet Lan-
guage Model for verb form and subject-
verb agreement errors; a classifier trained
on both learner and native corpora for
noun number errors; a statistical machine
translation (SMT)-based model for prepo-
sition and determiner errors. As for
subject-verb agreement errors, we show
that the Treelet Language Model-based
approach can correct errors in which the
target verb is distant from its subject. Our
system ranked fourth on the official run.
1 Introduction
Grammatical error correction is the task of auto-
matically detecting and correcting grammatical er-
rors in text, especially text written by second lan-
guage learners. Its purpose is to assist learners in
writing and helps them learn languages.
Last year, HOO 2012 (Dale et al, 2012) was
held as a shared task on grammatical error cor-
rection, focusing on prepositions and determiners.
The CoNLL-2013 shared task (Dahlmeier et al,
2013) includes these areas and also noun number,
verb form, and subject-verb agreement errors.
We divide the above 5 error types into three
groups: (1) subject-verb agreement (SVA) and
verb form (Vform) errors, (2) noun number (Nn)
errors, and (3) preposition (Prep) and determiner
(ArtOrDet) errors. For the subject-verb agreement
and verb form errors, we used a syntactic language
model, the Treelet Language Model, because syn-
tactic information is important for verb error cor-
rection. For the noun number errors, we used a
binary classifier trained on both learner and native
corpora. For the preposition and determiner errors,
we adopt a statistical machine translation (SMT)-
based approach, aiming at correcting errors in con-
ventional expressions. After each subsystem cor-
rects the errors of the corresponding error types,
we merge the outputs of all the subsystems.
The result shows our system achieved 21.85
in F-score on the formal run before revision and
28.14 after revision.
The rest of this paper is organized as follows.
Section 2 presents an overview of related work.
Section 3 describes the system architecture of each
of the three subsystems. Section 4 shows experi-
mental settings and results. Section 5 presents dis-
cussion. Section 6 concludes this paper.
2 Related Work
Lee and Seneff (2008) tried correcting English
verb errors including SVA and Vform. They pro-
posed correction candidates with template match-
ing on parse trees and filtered candidates by uti-
lizing n-gram counts. Our system suggests candi-
dates based on the Part-Of-Speech (POS) tag of a
target word and filters them by using a syntactic
language model.
For the noun number errors, we improved the
system proposed by Izumi et al (2003). In
Izumi et al (2003), a noun number error detec-
tion method is a part of an automatic error de-
tection system for transcribed spoken English by
Japanese learners. They used a maximum entropy
method whose features are unigrams, bigrams and
trigrams of surface words, of POS tags and of
the root forms. They trained a classifier on only
a learner corpus. The main difference between
theirs and ours is a domain of the training corpus
and features we used. We trained a classifier on
the mixed corpus of the leaner corpus and the na-
tive corpus. We employ a treepath feature in our
system.
Our SMT system for correcting preposition and
26
determiner errors is based on Mizumoto et al
(2012). They constructed a translation model from
the data of the language-exchange social network
service Lang-81 and evaluated its performance for
18 error types, including preposition and deter-
miner errors in the Konan-JIEM Learner Corpus.
On preposition error correction, they showed that
their SMT system outperformed a system using
a maximum entropy model. The main difference
with this work is that our new corpus collection
here is about three times larger.
3 System Architecture
3.1 Subject-Verb Agreement and Verb Form
For SVA and Vform errors, we used the Treelet
Language Model (Pauls and Klein, 2012) to cap-
ture syntactic information and lexical information
simultaneously. We will first show examples of
SVA and Vform errors and then describe our model
used to correct them. Finally, we explain the pro-
cedure for error correction.
3.1.1 Errors
According to Lee and Seneff (2008), both SVA and
Vform errors are classified as syntactic errors. Ex-
amples are as follows:
Subject-Verb Agreement (SVA) The verb is not
correctly inflected in number and person with
respect to its subject.
They *has been to Nara many times.
In this example, a verb ?has? is wrongly in-
flected. It should be ?have? because its subject is
the pronoun ?they?.
Verb Form (Vform) This type of error mainly
consists of two subtypes,2 one of which includes
auxiliary agreement errors.
They have *be to Nara many times.
Since the ?have? in this sentence is an auxil-
iary verb, the ?be? is incorrectly inflected and it
should be ?been?.
The other subtype includes complementation
1http://lang-8.com
2In the NUCLE (Dahlmeier et al, 2013) corpus, most of
semantic errors related to verbs are included in other error
types such as verb tense errors, not Vform errors.
errors like the following:
They want *go to Nara this summer.
Verbs can be a complement of another verb
and preposition. The ?go? in the above sentence
is incorrect. It should be in the infinitive form, ?to
go?.
3.1.2 Treelet Language Model
We used the Treelet Language Model (Pauls and
Klein, 2012) for SVA and Vform error correction.
Our model assigns probability to a production
rule of the form r = P ? C1 ? ? ?Cd in a con-
stituent tree T , conditioned on a context h consist-
ing of previously generated treelets,3 where P is
the parent symbol of a rule r and Cd1 = C1 ? ? ?Cd
are its children.
p(r) = p(Cd1 |h)
The probability of a constituent tree T is given by
the following equation:
p(T ) =
?
r?T
p(r)
The context h differs depending on whether Cd1 is
a terminal symbol or a sequence of non-terminal
symbols.
Terminal When Cd1 is a terminal symbol w,
p(Cd1 |h) = p(w|P,R, r?, w?1, w?2)
where P is the POS tag of w, R is the right sibling
of P , r? is the production rule which yields P and
its siblings, and w?2 and w?1 are the two words
preceding w.
Non-Terminal When Cd1 is a sequence of non-
terminal symbols,
p(Cd1 |h) = p(Cd1 |P, P ?, r?)
where P is the parent symbol of Cd1 , P ? is the par-
ent symbol of P .
In order to capture a richer context, we apply the
annotation and transformation rules below to parse
trees in order. We use almost the same annota-
tion and transformation rules as those proposed by
3The term treelet is used to refer to an arbitrary connected
subgraph of a tree (Quirk et al, 2005)
27
Original Candidates
am/VBP, are/VBP or is/VBZ {am/VBP, are/VBP, is/VBZ}
was/VBD or were/VBD {was/VBD, were/VBD}
being/VBG {be/VB, being/VBG}
been/VBN {be/VB, been/VBN}
be/VB {be/VB, being/VBG, been/VBN}
Table 1: Examples of candidates in the case of ?be?
ROOT
S
VP
VP
ADVP
NNS
times
JJ
many
PP
NNP
Nara
TO
to
VBN
been
VBP
have
NP
PRP
They
ROOT
S@ROOT-have
VP@S-have
ADVP-NNTS
NNTS
times
JJ
many
PP-to
NNP
Nara
TO-to
to
VBN-been
been
VBP-have
have
PRP-they
They
Figure 1: The tree on the left is before annotations and transformations which convert it to the tree on
the right.
Pauls and Klein (2012). For instance, the common
CFG tree on the left side of Figure 1 is transformed
to the one on the right side.
Temporal NPs Pauls and Klein (2012) marked ev-
ery noun which is the head of an NP-TMP at least
once in the Penn Treebank. For example, NN ?
time is replaced with NNT ? time and NNS ?
times is replaced with NNTS ? times. This rule
seems to be useful for correcting verb tense er-
rors.4
Head Annotations We annotated every non-terminal
and preterminal with its head word.5 If the head
word is not a closed class word,6 we annotated
non-terminal symbols with the head POS tag in-
stead of the head word.
NP Flattening Pauls and Klein (2012) deleted NPs
dominated by other NPs, unless the child NPs are
in coordination or apposition. These NPs typically
4Verb tense (Vt) errors are not covered in this shared task.
5We identified the head with almost the same rules used
in Collins (1999).
6We took the following to be the closed class words: all
inflections of the verbs do, be, and have; and any word tagged
with IN, WDT, PDT, WP, WP$, TO, WRB, RP, DT, SYM,
EX, POS, PRP, AUX, MD or CC.
occur when nouns are modified by PPs. Our model
therefore assigns probability to nouns conditioned
on the head of modifying PPs with prepositions
such as ?in?, ?at? and so on by applying simul-
taneously the NP Flattening and the Head Annota-
tions. However, our model cannot assign probabil-
ity to prepositions conditioned on verbs or nouns
on which the prepositions depend. For this reason
we did not use our model to correct prepositional
errors.
Number Annotations Pauls and Klein (2012) di-
vided numbers into five classes: CD-YR for num-
bers that consist of four digits, which are usually
years; CD-NM for entirely numeric numbers; CD-
DC for numbers that have a decimal; CD-MX for
numbers that mix letters and digits; and CD-AL
for numbers that are entirely alphabetic.
SBAR Flattening They removed any S nodes which
are children of an SBAR.
VP Flattening They removed any VPs immedi-
ately dominated by a VP, unless it is con-
joined with another VP. The chains of verbs
are represented as separated VPs for each verb,
such as (VP (MD will) (VP (VB be) (VP (VBG
28
playing) . . .))). This transformation turns the
above VPs into (VP (MD will) (VB be) (VBG
playing) . . .). This has an effect on the cor-
rection of auxiliary agreement errors because
our model can assign probability to main verbs
strongly conditioned on their auxiliary verbs.
Gapped Sentence Annotation They annotated all S
and SBAR nodes that have a VP before any NP.
Parent Annotation They annotated all VPs and chil-
dren of the ROOT node with their parent symbol.
Unary Deletion All unary rules are deleted except
the root and the preterminal rules. We kept only
the bottom-most symbol of the unary rule chain.
This brings many symbols into the context of a
production rule.
3.1.3 Procedure
Our system for SVA and Vform errors tries to cor-
rect the words in a sentence from left to right. Cor-
rection proceeds in the following steps.
1. If the POS tag of the word is ?VB?, ?VBD?,
?VBG?, ?VBN?, ?VBP? or ?VBZ?, our sys-
tem generates sentences which have the word
replaced with candidates. For example, if the
original word is an inflection of ?be?, the sys-
tem generates candidates as shown in Table
1.
2. The system parses those sentences and ob-
tains the k-best parses for each sentence.
3. The system keeps only the one sentence to
which our language model assigned the high-
est probability in the parses.
4. The system repeats Steps 1 to 3 with the sen-
tence kept in Step 3 until the rightmost word
of that sentence.
Note that the system uses the Berkeley Parser7 in
Step 2.
3.2 Noun Number
3.2.1 Errors
A noun number error is the mistake of using the
singular form for a plural noun, and vice versa, as
in the following:
7http://code.google.com/p/
berkeleyparser/
I saw many *student yesterday.
In this example, the inflection of ?student?
is mistaken. It should be ?students? because it is
modified by ?many?.
To correct such errors, we use a binary classi-
fication approach because the inflection of a noun
is either ?singular? or ?plural?. If the binary clas-
sifier detects an error with a sufficiently high con-
fidence, the system changes the noun form. We
adopt the adaptive regularization of weight vectors
algorithm (AROW) (Crammer et al, 2009). AROW
is a variant of a confidence weighted linear classi-
fication algorithm which is suitable for the classi-
fication of large scale data.
3.2.2 Binary classifier approach
The binary classifier indicates ?singular? or ?plu-
ral? for all nouns except proper and uncountable
nouns. First, if a noun is found in the training cor-
pus, we extract an instance with features created
by the feature template in Table 2.8 Second, we
train a classifier with extracted instances and la-
bels from the training corpus.
We use unigram, bigram, and trigram features
around the target word and the path features be-
tween the target word and all the other nodes in
the NPs that dominate the target word as the right-
most constituent. The path feature is commonly
used in semantic role labeling tasks (Pradhan et
al., 2004). For the path features, we do not use
the right subtree of the NP as the path features be-
cause we assume that right subtrees do not affect
the number of the target word. We limit the maxi-
mum depth of the subtree containing the NP to be
3 because nodes over this limit may be noisy. To
encode the relationship between the target word
and another node in the NP, we append a symbol
which reflects the direction of tree traversal to the
label: ?p? for going up (parent) and ?c? for going
down (child). For example, we show extracted fea-
tures in Table 2 for the phrase ?some interesting
and recent topics about politics and economics?.
In the training corpus, since the proportions of
singular and plural nouns are unequal, we set dif-
ferent thresholds for classifying singular and plu-
ral forms. These thresholds limit the probabilities
which the binary classifier uses for error detection.
We have used a development set to determine the
8Target word refers to a noun whose POS tag is ?NN? or
?NNS? in the Penn Treebank tagset.
29
Feature name Word, Pos used as features Example
surface unigram word?1, word?2 and, recent, about, politics
surface bigram word?2 word?1 and recent, about politics
surface trigram word?3 word?2 word?1 interesting and recent, about politics and
POS unigram POS?1, POS?2 CC, JJ, IN, NN
POS bigram POS?1 POS?2 CC JJ, IN NN
POS trigram POS?3 POS?2 POS?1 JJ CC JJ, IN NN CONJ
lemma unigram lemma?2, lemma?1 and, recent, about, politics
lemma bigram lemma?2 lemma?1 and recent, about politics
lemma trigram lemma?3 lemma?2 lemma?1 interesting and recent, about politics and
lemma target lemma of target word topic
path feature path between the target word p NP, pc JJ, pc recent, pp NP, ppc CC, ppc and,
and the other nodes in NP ppc NP, ppcc DT, ppcc some, ppcc JJ, ppcc interesting
Table 2: Features used for the detection of noun number errors and example features for the phrase ?some
interesting and recent topics about politics and economics?.
best thresholds for singular and plural forms, re-
spectively.
For proper and uncountable nouns, we do not
change number because of the nature of those
nouns. In order to determine whether to change
number or not, we create a list which consists of
words frequently used as singular forms in the na-
tive corpus.
3.3 Prepositions and Determiners
For preposition and determiner errors, we con-
struct a system using a phrase-based statistical
machine translation (Koehn et al, 2003) frame-
work. The SMT-based approach functions well
in corrections of conventional usage of determin-
ers and prepositions such as ?the young? and ?take
care of ?. The characteristic of the SMT-based ap-
proach is its ability to capture tendencies in learn-
ers? errors. This approach translates erroneous
phrases that learners often make to correct phrases.
Hence, it can handle errors in conventional expres-
sions without over-generalization.
The phrase-based SMT framework which we
used is based on the log-linear model (Och and
Ney, 2002), where the decision rule is expressed
as follow:
argmax
e
P (e|f) = argmax
e
M
?
m=1
?mhm(e, f)
Here, f is an input sentence, e are hypotheses,
hm(e, f) feature functions and ?m their weights.
The hypothesis that maximizes the weighted sum
of the feature functions is chosen as an output sen-
tence.
The feature functions encode components of
the phrase-based SMT, including the translation
model and the language model. The translation
model suggests translation hypotheses and the lan-
guage model filters out ill-formed hypotheses.
For an error correction system based on SMT,
the translation model is constructed from pairs of
original sentences and corrected sentences, and the
language model is built on a native corpus (Brock-
ett et al, 2006).
Brockett et al (2006) trained the translation
model on a corpus where the errors are restricted
to mass noun errors. In our case, we trained our
model on a corpus with no restriction on error
types. Consequently, the system corrects all types
of errors. To focus on preposition and determiner
errors, we retain proposed edits that include 48
prepositions and 25 determiners listed in Table 3.
4 Experiments
4.1 Experimental setting
4.1.1 Subject-Verb Agreement and Verb
Form
We describe here the training data and tools used
to train our model. Our model was trained with the
Berkeley LM9 version 1.1.3. We constructed the
training data by concatenating the WSJ sections of
the Penn Treebank and the AFP sections of the En-
glish Gigaword Corpus version 5.10 Our training
data consists of about 27 million sentences. Al-
though human-annotated parses for the WSJ are
available, there is no gold standard for the AFP,
so we parsed the AFP automatically by using the
Berkeley Parser released on October 9, 2012.
9http://code.google.com/p/berkeleylm/
10LDC2011T07
30
Preposition about, across, after, against, along, among, around, as, at, before, behind, below,
beside, besides, between, beyond, but, by, despite, down, during, for, from, in,
inside, into, near, of, off, on, onto, opposite, outside, over, past, round, without,
than, through, to, toward, towards, under, until, up, upon, with, within
Determiner the, a, an, all, these, those, many, much, another, no, some, any, my,
our, their, her, his, its, no, each, every, certain, its, this, that
Table 3: Preposition and determiner lists
4.1.2 Noun Number
We trained a binary classifier on a merged corpus
of the English Gigaword and the NUCLE data.
From the English Gigaword corpus, we used the
New York Times (NYT) as a training corpus. In
order to create the training corpus, we corrected
all but noun number errors in the NUCLE data us-
ing gold annotations.
The AROW++ 11 0.1.2 was used for the binary
classification. For changing noun forms, we used
the pattern.en toolkit.12
The maximum depth of subtrees containing an
NP is set to 3 when we extracted the path features.
We built and used a list of nouns that appear in
singular forms frequently in a native corpus. We
counted the frequency of nouns in entire English
Gigaword. If a noun appears in more than 99%13
of occurrences in singular form, we included it in
the list. The resulting list contains 836 nouns.
4.1.3 Prepositions and Determiners
We used Moses 2010-08-13 with default parame-
ters for our decoder14 and GIZA++ 1.0.515 as the
alignment tool. The grow-diag-final heuristics was
applied for phrase extraction. As a language mod-
eling tool we used IRSTLM version 5.8016 with
Witten-Bell smoothing.
The translation model was trained on the NU-
CLE corpus and our Lang-8 corpus.17 From the
Lang-8 corpus, we filtered out noisy sentences.
Out of 1,230,257 pairs of sentences, 1,217,124
pairs of sentences were used for training. As for
the NUCLE corpus we used 55,151 pairs of sen-
tences from the official data provided as training
11https://code.google.com/p/arowpp/
12http://www.clips.ua.ac.be/pages/
pattern-en
13We tested many thresholds, and set 99% as threshold.
14http://sourceforge.net/projects/
mosesdecoder/
15http://code.google.com/p/giza-pp/
16http://sourceforge.net/projects/
irstlm/
17consisting of entries through 2012.
data. We used a 3-gram language model built on
the entire English Gigaword corpus.
4.2 Result
Table 4 shows the overall results of our submit-
ted systems and the results of an additional ex-
periment. In the additional experiment, we tried
the SMT-based approach described in Section 3.3
for errors in SVA, Vform and Nn. While the sys-
tem based on the Treelet Language Model out-
performed the SMT-based system on the SVA er-
rors and the Vform errors, the binary classifier ap-
proach did not perform as well as the SMT-based
system on the Nn errors.
5 Discussion
5.1 Subject-Verb Agreement and Verb Form
We provide here examples of our system?s output,
beginning with a successful example.
source: This is an age which most people *is re-
tired and *has no sources of incomes.
hypothesis: This is an age which most people are
retired and have no sources of incomes.
The source sentence of this pair includes two SVA
errors. The first is that ?be? should agree with its
subject ?people? and must be ?are?. Our system is
able to correct errors where the misinflected pred-
icate is adjacent to its subject. The second error
is also an agreement error, in this case between
?have? and its subject ?people?. Our model can
assign probability to yields related to predicates
conditioned strongly on their subjects even if the
distance between the predicate and its subject is
long. The same can be said of Vform errors.
One mistake made by our system is miscorrec-
tion due to the negative effect of other errors.
source/hypothesis: The rising life *expectancies
*are like a two side sword to the modern world.
31
submitted system additional experiments
ALL Verb Nn Prep ArtOrDet Verb Nn
Precision 0.2707 0.1378 0.4452 0.2649 0.3118 0.2154 0.3687
original Recall 0.1832 0.2520 0.1641 0.1286 0.2029 0.0569 0.2020
F-score 0.2185 0.1782 0.2399 0.1732 0.2458 0.0900 0.2610
Precision 0.3392 0.1814 0.5578 0.3245 0.4027 0.3846 0.4747
revised Recall 0.2405 0.2867 0.1708 0.1494 0.2497 0.0880 0.2137
F-score 0.2814 0.2222 0.2616 0.2046 0.3082 0.1433 0.2947
Table 4: Results of the submitted system for each type of error and results of additional experiments
with the SMT-based system. The score is evaluated on the m2scorer (Dahlmeier and Ng, 2012). ALL
is the official result of formal run, and each of the others shows the result of the corresponding error
type. Since our system did not distinguish SVA and Vform, we report the combined result for them in the
column Verb.
gold: The rising life expectancy is like a two side
sword to the modern world.
Since the subject of ?are? is ?expectancies?, the
sentence looks correct at first. However, this ex-
ample includes not only an SVA error but also an
Nn error, and therefore the predicate ?are? should
be corrected along with correcting its subject ?ex-
pectancies?.
An example of a Vform error is shown below.
source/hypothesis: Besides, we can try to reduce
the bad effect *cause by the new technology.
gold: Besides, we can try to reduce the bad effect
caused by the new technology.
The word ?cause? is tagged as ?NN? in this sen-
tence. This error is ignored because our system
makes corrections on the basis of the original POS
tag. For a similar example, our system does not
make modifications between the to-infinitive and
the other forms.
5.2 Noun Number
We provide here examples of our system?s output,
beginning with a successful example.
source: many of cell *phone are equipped with
GPS
hypothesis/gold: many of cell phones are
equipped with GPS
In the example, the noun ?phone? should be in the
plural form ?phones?. This is because the phrase
?many of? modifies the noun. In this case, the un-
igrams ?many? and ?are?, and the bigram ?many
of? are features with strong weights for the plural
class as expected.
However, n-gram features sometimes work to
the contrary of our expectations.
source/hypothesis: RFID is not only used to
track products for logistical and storage *purpose,
it is also used to track people
gold: RFID is not only used to track products for
logistical and storage purposes, it is also used to
track people
The ?purpose? is in the PP which is modified by
?products?. Thus, ?purpose? should not be af-
fected by the following words. However, the verb
?is?, which is immediately after ?purpose?, has a
strong influence to keep the word in singular form.
Therefore, it may be better not to use a verb that
the word is not immediately dependent on as a fea-
ture.
5.3 Prepositions and Determiners
While the SMT-based system can capture the
statistics of learners? errors, it cannot correct
phrases that are not found in the training corpus.
(1) source: *with economic situation
gold: in economic situation
(2) source: *with such situation
gold: in such situation
Our system was not able to correct the source
phrase in (1), in spite of the fact that the similar
phrase pair (2) was in the training data. To correct
such errors, we should construct a system that al-
lows a gap in source and target phrases as in Galley
and Manning (2010).
32
6 Conclusion
This paper described the architecture of our cor-
rection system for errors in verb forms, subject
verb agreement, noun number, prepositions and
determiners. For verb form and subject verb
agreement errors, we used the Treelet Language
Model. By taking advantage of rich syntactic in-
formation, it corrects subject-verb agreement er-
rors which need to be inflected according to a dis-
tant subject. For noun number errors, we used a
binary classifier using both learner and native cor-
pora. For preposition and determiner errors, we
built an SMT-based system trained on a larger cor-
pus than those used in prior works. We show that
our subsystems are effective to each error type. On
the other hand, our system cannot handle the er-
rors strongly related to other errors well. In future
work we will explore joint correction of multiple
error types, especially noun number and subject-
verb agreement errors, which are closely related
to each other.
Acknowledgements
We would like to thank Yangyang Xi for giving us
permission to use text from Lang-8 and the anony-
mous reviewers for helpful comments.
References
Chris Brockett, William B. Dolan, and Michael Ga-
mon. 2006. Correcting ESL errors using phrasal
smt techniques. In Proceedings of COLING/ACL
2006, pages 249?256.
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania.
Koby Crammer, Alex Kulesza, and Mark Dredze.
2009. Adaptive regularization of weight vectors. In
Proceedings of NIPS 2009, pages 414?422.
Daniel Dahlmeier and Hwee Tou Ng. 2012. Better
evaluation for grammatical error correction. In Pro-
ceedings of NAACL 2012, pages 568?572.
Daniel Dahlmeier, Hwee Tou Ng, and Siew Mei Wu.
2013. Building a large annotated corpus of learner
english: The NUS corpus of learner English. In Pro-
ceedings of BEA 2013, pages 313?330.
Robert Dale, Ilya Anisimoff, and George Narroway.
2012. HOO 2012: a report on the preposition and
determiner error correction shared task. In Proceed-
ings of BEA 2012, pages 54?62.
Michel Galley and Christopher D. Manning. 2010.
Accurate non-hierarchical phrase-based translation.
In Processing of HLT/NAACL 2010, pages 966?974.
Emi Izumi, Kiyotaka Uchimoto, Toyomi Saiga, Thep-
chai Supnithi, and Hitoshi Isahara. 2003. Auto-
matic error detection in the Japanese learners? En-
glish spoken data. In Proceedings of ACL 2003,
pages 145?148.
Philipp Koehn, Franz Josef Och, and Daniel C Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of HLT/NAACL 2003, pages 48?54.
John Lee and Stephanie Seneff. 2008. Correcting mis-
use of verb forms. In Proceedings of ACL 2008,
pages 174?182.
Tomoya Mizumoto, Yuta Hayashibe, Mamoru Ko-
machi, Masaaki Nagata, and Yuji Matsumoto. 2012.
The effect of learner corpus size in grammatical er-
ror correction of ESL writings. In Proceedings of
COLING 2012, pages 863?872.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for sta-
tistical machine translation. In Proceedings of ACL
2002, pages 295?302.
Adam Pauls and Dan Klein. 2012. Large-scale syntac-
tic language modeling with treelets. In Proceedings
of ACL 2012, pages 959?968.
Sameer S Pradhan, Wayne H Ward, Kadri Hacioglu,
James H Martin, and Dan Jurafsky. 2004. Shallow
semantic parsing using support vector machines. In
Proceedings of HLT/NAACL 2004, pages 233?240.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005.
Dependency treelet translation: Syntactically in-
formed phrasal SMT. In Proceedings of ACL 2005,
pages 271?279.
33
