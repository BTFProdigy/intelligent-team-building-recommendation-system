Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1?11, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Syntactic Transfer Using a Bilingual Lexicon
Greg Durrett, Adam Pauls, and Dan Klein
Computer Science Division
University of California, Berkeley
{gdurrett,adpauls,klein}@cs.berkeley.edu
Abstract
We consider the problem of using a bilingual
dictionary to transfer lexico-syntactic infor-
mation from a resource-rich source language
to a resource-poor target language. In con-
trast to past work that used bitexts to trans-
fer analyses of specific sentences at the token
level, we instead use features to transfer the
behavior of words at a type level. In a dis-
criminative dependency parsing framework,
our approach produces gains across a range
of target languages, using two different low-
resource training methodologies (one weakly
supervised and one indirectly supervised) and
two different dictionary sources (one manu-
ally constructed and one automatically con-
structed).
1 Introduction
Building a high-performing parser for a language
with no existing treebank is still an open problem.
Methods that use no supervision at all (Klein and
Manning, 2004) or small amounts of manual su-
pervision (Haghighi and Klein, 2006; Cohen and
Smith, 2009; Naseem et al2010; Berg-Kirkpatrick
and Klein, 2010) have been extensively studied, but
still do not perform well enough to be deployed
in practice. Projection of dependency links across
aligned bitexts (Hwa et al2005; Ganchev et al
2009; Smith and Eisner, 2009) gives better perfor-
mance, but crucially depends on the existence of
large, in-domain bitexts. A more generally appli-
cable class of methods exploits the notion of univer-
sal part of speech tags (Petrov et al2011; Das and
...   the    senators    demand    strict   new    ethics    rules   ...
      DT      NNS          VBP          JJ       JJ       NNS     NNS   
Gewerkschaften     verlangen       Verzicht         auf       die     Reform
          NN                  VVFIN             NN          APPR    ART       NN
       Unions               demand     abandonment     on       the      reform
Figure 1: Sentences in English and German both contain-
ing words that mean ?demand.? The fact that the English
demand takes nouns on its left and right indicates that the
German verlangen should do the same, correctly suggest-
ing attachments to Verzicht and Gewerkschaften.
Petrov, 2011) to train parsers that can run on any lan-
guage with no adaptation (McDonald et al2011)
or unsupervised adaptation (Cohen et al2011).
While these universal parsers currently constitute
the highest-performing methods for languages with-
out treebanks, they are inherently limited by operat-
ing at the coarse POS level, as lexical features are
vital to supervised parsing models.
In this work, we consider augmenting delexical-
ized parsers by transferring syntactic information
through a bilingual lexicon at the word type level.
These parsers are delexicalized in the sense that, al-
though they receive target language words as input,
their feature sets do not include indicators on those
words. This setting is appropriate when there is too
little target language data to learn lexical features di-
rectly. Our main approach is to add features which
are lexical in the sense that they compute a function
of specific target language words, but are still un-
1
lexical in the sense that all lexical knowledge comes
from the bilingual lexicon and training data in the
source language.
Consider the example English and German sen-
tences shown in Figure 1, and suppose that we wish
to parse the German side without access to a Ger-
man treebank. A delexicalized parser operating at
the part of speech level does not have sufficient in-
formation to make the correct decision about, for ex-
ample, the choice of subcategorization frame for the
verb verlangen. However, demand, a possible En-
glish translation of verlangen, takes a noun on its
left and a noun on its right, an observation that in this
case gives us the information we need. We can fire
features in our German parser on the attachments
of Gewerkschaften and Verzicht to verlangen indi-
cating that similar-looking attachments are attested
in English for an English translation of verlangen.
This allows us to exploit fine-grained lexical cues to
make German parsing decisions even when we have
little or no supervised German data; moreover, this
syntactic transfer is possible even in spite of the fact
that demand and verlangen are not observed in par-
allel context.
Using type-level transfer through a dictionary in
this way allows us to decouple the lexico-syntactic
projection from the data conditions under which we
are learning the parser. After computing feature val-
ues using source language resources and a bilingual
lexicon, our model can be trained very simply us-
ing any appropriate training method for a supervised
parser. Furthermore, because the transfer mecha-
nism is just a set of features over word types, we are
free to derive our bilingual lexicon either from bitext
or from a manually-constructed dictionary, making
our method strictly more general than those of Mc-
Donald et al2011) or Ta?ckstro?m et al2012), who
rely centrally on bitext. This flexibility is potentially
useful for resource-poor languages, where a human-
curated bilingual lexicon may be broader in cover-
age or more robust to noise than a small, domain-
limited bitext. Of course, it is an empirical question
whether transferring type level information about
word behavior is effective; we show that, indeed,
this method compares favorably with other transfer
mechanisms used in past work.
The actual syntactic information that we transfer
consists of purely monolingual lexical attachment
statistics computed on an annotated source language
resource.1 While the idea of using large-scale sum-
mary statistics as parser features has been consid-
ered previously (Koo et al2008; Bansal and Klein,
2011; Zhou et al2011), doing so in a projection set-
ting is novel and forces us to design features suitable
for projection through a bilingual lexicon. Our fea-
tures must also be flexible enough to provide benefit
even in the presence of cross-lingual syntactic dif-
ferences and noise introduced by the bilingual dic-
tionary.
Under two different training conditions and with
two different varieties of bilingual lexicons, we
show that our method of lexico-syntactic projection
does indeed improve the performance of parsers that
would otherwise be agnostic to lexical information.
In all settings, we see statistically significant gains
for a range of languages, with our method providing
up to 3% absolute improvement in unlabeled attach-
ment score (UAS) and 11% relative error reduction.
2 Model
The projected lexical features that we propose in this
work are based on lexicalized versions of features
found in MSTParser (McDonald et al2005), an
edge-factored discriminative parser. We take MST-
Parser to be our underlying parsing model and use it
as a testbed on which to evaluate the effectiveness of
our method for various data conditions.2 By instanti-
ating the basic MSTParser features over coarse parts
of speech, we construct a state-of-the-art delexical-
ized parser in the style of McDonald et al2011),
where feature weights can be directly transferred
from a source language or languages to a desired
target language. When we add projected lexical fea-
tures on top of this baseline parser, we do so in a
way that does not sacrifice this generality: while
our new features take on values that are language-
specific, they interact with the model at a language-
independent level. We therefore have the best of
1Throughout this work, we will use English as the source
language, but it is possible to use any language for which the
appropriate bilingual lexicons and treebanks exist. One might
expect to find the best performance from using a source lan-
guage closely related to the target.
2We train MSTParser using the included implementation of
MIRA (Crammer and Singer, 2001) and use projective decoding
for all experiments described in this paper.
2
DELEX
Feature Value
VERB?NOUN 1
VERB?NOUN, L 1
??? ???
PROJ
Query Feature (signature) Value
verlangen?NOUN [VERB]?CHILD 0.723
verlangen?NOUN, L [VERB]?CHILD, DIR 0.711
VERB?Gewerkschaften PARENT? [NOUN] 0.822
??? ??? ???
Gewerkschaften     verlangen       Verzicht         auf       die     Reform
        NOUN               VERB           NOUN        ADP    DET     NOUN
        Unions              demand     abandonment     on       the      reform
DELEX
Feature Value
VERB?NOUN 1
VERB?NOUN, R 1
??? ???
PROJ
Query Feature (signature) Value
verlangen?NOUN [VERB]?CHILD 0.723
verlangen?NOUN, R [VERB]?CHILD, DIR 0.521
VERB?Verzicht PARENT?[NOUN] 0.623
??? ??? ???
Figure 2: Computation of features on a dependency arc. DELEX features are indicators over characteristics of depen-
dency links that do not involve the words in the sentence. PROJ features are real-valued analogues of DELEX features
that do contain words. We form a query from each stipulated set of characteristics, compute the values of these queries
heuristically, and then fire a feature based on each query?s signature. Signatures indicate which attachment properties
were considered, which part of the query was lexicalized (shown by brackets here), and the POS of the query word.
This procedure yields a small number of real-valued features that still capture rich lexico-syntactic information.
two worlds in that our features can be learned on
any treebank or treebanks that are available to us,
but still exploit highly specific lexical information
to achieve performance gains over using coarse POS
features alone.
2.1 DELEX Features
Our DELEX feature set consists of all of the unlexi-
calized features in MSTParser, only lightly modified
to improve performance for our setting. McDonald
et al2005) present three basic types of such fea-
tures, ATTACH, INBETWEEN, and SURROUNDING,
which we apply at the coarse POS level. The AT-
TACH features for a given dependency link consist of
indicators of the tags of the head and modifier, sep-
arately as well as together. The INBETWEEN and
SURROUNDING features are indicators on the tags
of the head and modifier in addition to each inter-
vening tag in turn (INBETWEEN) or various com-
binations of tags adjacent to the head or modifier
(SURROUNDING).3
MSTParser by default also includes a copy of
each of these indicator features conjoined with
the direction and distance of the attachment it de-
notes. These extra features are important to getting
3As in Koo et al2008), our feature set contains more
backed-off versions of the SURROUNDING features than are de-
scribed in McDonald et al2005).
good performance out of the baseline model. We
slightly modify the conjunction scheme and expand
it with additional backed-off conjunctions, since
these changes lead to features that empirically trans-
fer better than the MSTParser defaults. Specifically,
we use conjunctions with attachment direction (left
or right), coarsened distance,4 and attachment direc-
tion and coarsened distance combined.
We emphasize again that these baseline features
are entirely standard, and all the DELEX feature set
does is recreate an MSTParser-based analogue of the
direct transfer parser described by McDonald et al
(2011).
2.2 PROJ Features
We will now describe how to compute our projected
lexical features, the PROJ feature set, which con-
stitutes the main contribution of this work. Recall
that we wish our method to be as general as possible
and work under many different training conditions;
in particular, we wish to be able to train our model
on only existing treebanks in other languages when
no target language trees are available (discussed in
Section 3.3), or on only a very small target language
treebank (Section 3.4). It would greatly increase
the power of our model if we were able to include
target-language-lexicalized versions of the ATTACH
4Our five distance buckets are {1, 2, 3?5, 6?10, 11+}.
3
features, but these are not learnable without a large
target language treebank. We instead must augment
our baseline model with a relatively small number of
features that are nonetheless rich enough to transfer
the necessary lexical information.
Our overall approach is sketched in Figure 2,
where we show the features that fire on two pro-
posed edges in a German dependency parse. Fea-
tures on an edge in MSTParser incorporate a sub-
set of observable properties about that edge?s head,
modifier, and context in the sentence. For sets of
properties that do not include a lexical item, such
as VERB?NOUN, we fire an indicator feature from
the DELEX feature set. For those that do include a
lexical item, such as verlangen?NOUN, we form a
query, which resembles a lexicalized indicator fea-
ture. Rather than firing the query as an indicator
feature directly, which would result in a model pa-
rameter for each target word, we fire a broad feature
called an signature whose value reflects the specifics
of the query (computation of these values is dis-
cussed in Section 2.2.2). For example, we abstract
verlangen?NOUN to [VERB]?CHILD, with square
brackets indicating the element that was lexicalized.
Section 2.2.1 discusses this coarsening in more de-
tail. The signatures are agnostic to individual words
and even the language being parsed, so they can be
learned on small amounts of data or data from other
languages.
Our signatures allow us to instantiate features at
different levels of granularity corresponding to the
levels of granularity in the DELEX feature set. When
a small amount of target language data is present,
the variety of signatures available to us means that
we can learn language-specific transfer characteris-
tics: for example, nouns tend to follow prepositions
in both French and English, but the ordering of ad-
jectives with respect to nouns is different. We also
have the capability to train on languages other than
our target language, and while this is expected to be
less effective, it can still teach us to exploit some
syntactic properties, such as similar verb attachment
configurations if we train on a group of SVO lan-
guages distinct from a target SVO language. There-
fore, our feature set manages to provide the training
procedure with choices about how much syntactic
information to transfer at the same time as it prevents
overfitting and provides language independence.
2.2.1 Query and Signature Types
A query is a subset of the following pieces of in-
formation about an edge: parent word, parent POS,
child word, child POS, attachment direction, and
binned attachment distance. It must contain exactly
one word.5 We experimented with properties from
INBETWEEN and SURROUNDING features as well,
but found that these only helped under some circum-
stances and could lead to overfitting.6
A signature contains the following three pieces of
information:
1. The non-empty subset of attachment properties
included in the query
2. Whether we have lexicalized on the parent or
child of the attachment, indicated by brackets
3. The part of speech of the included word
Because either the parent or child POS is included
in the signature, there are three meaningful proper-
ties to potentially condition on, of which we must se-
lect a nonempty subset. Some multiplication shows
that we have 7? 2? 13 = 182 total PROJ features.
As an example, the queries
verlangen? NOUN
verlangen? ADP
sprechen? NOUN
all share the signature [VERB]?CHILD, but
verlangen? NOUN,RIGHT
Verzicht? ADP
VERB ? Verzicht
have [VERB]?CHILD,DIR, [ADP]?CHILD, and
PARENT?[NOUN] as their signatures, respectively.
The level of granularity for signatures is a param-
eter that simply must be engineered. We found some
benefit in actually instantiating two signatures for
every query, one as described above and one that
5Bilexical features are possible in our framework, but we do
not use them here, so for clarity we assume that each query has
one associated word.
6One hypothesis is that features looking at the sentence con-
text are more highly specialized to a given language, since they
examine the parent, the child, and one or more other parts of
speech or words.
4
?demand, DIR
PARENT?demand
demand
Word POS Dir Dist
that
ADP R
3
said
VERB L
7
<root>
ROOT L
6
senators
NOUN L
1
rules
NOUN R
4
We
NOUN L
1
that
ADP R
1
They
NOUN L
1
concessions
NOUN R
1
from
ADP R
2
P
a
r
e
n
t
s
C
h
i
l
d
r
e
n
DIR
Value
L
0.66
R
0.33
PARENT
Value
ADP
0.33
VERB
0.33
ROOT
0.33
  He   reports that   the   senators demand strict new ethics rules [...]
PRON     VERB     ADP     DET       NOUN          VERB        ADJ     ADJ      NOUN  NOUN
   ?      We   demand that these hostilities cease    ,        ?      said [...]
PUNC   PRON       VERB      ADP     DET        NOUN        VERB   PUNC  PUNC   VERB
 They  demand concessions  from   the  Israeli authorities    <root>
  PRON        VERB              NOUN            ADP      DET      ADJ           NOUN               ROOT
???
Figure 3: Computation of query values. For each occurrence of a given source word, we tabulate the attachments it
takes part in (parents and children) and record their properties. We then compute relative frequency counts for each
possible query type to get source language scores, which will later be projected through the dictionary to obtain target
language feature values. Only two query types are shown here, but values are computed for many others as well.
does not condition on the part of speech of the word
in the signature. One can also imagine using more
refined signatures, but we found that this led to over-
fitting in the small training scenarios under consid-
eration.
2.2.2 Query Value Estimation
Each query is given a value according to a gener-
ative heuristic that involves the source training data
and the probabilistic bilingual lexicon.7 For a par-
ticular signature, a query can be written as a tu-
ple (x1, x2, . . . , wt) where wt is the target language
query word and the xi are the values of the included
language-independent attachment properties. The
value this feature takes is given by a simple gener-
ative model: we imagine generating the attachment
properties xi given wt by first generating a source
7Lexicons such as those produced by automatic aligners in-
clude probabilities natively, but obviously human-created lexi-
cons do not. For these dictionaries, we simply assume that each
word translates with uniform probability into each of its pos-
sible translations. Tweaking this method did not substantially
change performance.
word ws from wt based on the bilingual lexicon,
then jointly generating the xi conditioned on ws.
Treating the choice of source translation as a latent
variable to be marginalized out, we have
value = p(x1, x2, . . . |wt)
=
?
ws
p(ws|wt)p(x1, x2, . . . |ws)
The first term of the sum comes directly from our
probabilistic lexicon, and the second we can esti-
mate using the maximum likelihood estimator over
our source language training data:
p(x1, x2, . . . |ws) =
c(x1, x2, . . . , ws)
c(ws)
(1)
where c(?) denotes the count of an event in the
source language data.
The final feature value is actually the logarithm
of this computed value, with a small constant added
before the logarithm is taken to avoid zeroes.
5
3 Experiments
3.1 Data Conditions
Before we describe the details of our experiments,
we sketch the data conditions under which we eval-
uate our method. As described in Section 1, there is
a continuum of lightly supervised parsing methods
from those that make no assumptions (beyond what
is directly encoded in the model), to those that use
a small set of syntactic universals, to those that use
treebanks from resource-rich languages, and finally
to those that use both existing treebanks and bitexts.
Our focus is on parsing when one does not have
access to a full-scale target language treebank, but
one does have access to realistic auxiliary resources.
The first variable we consider is whether we have
access to a small number of target language trees or
only pre-existing treebanks in a number of other lan-
guages; while not our actual target language, these
other treebanks can still serve as a kind of proxy for
learning which features generally transfer useful in-
formation (McDonald et al2011). We notate these
conditions with the following shorthand:
BANKS: Large treebanks in other target languages
SEED: Small treebank in the right target language
Previous work on essentially unsupervised meth-
ods has investigated using a small number of target
language trees (Smith and Eisner, 2009), but the be-
havior of supervised models under these conditions
has not been extensively studied. We will see in
Section 3.4 that with only 100 labeled trees, even
our baseline model can achieve performance equal
to or better than that of the model of McDonald et
al. (2011). A single linguist could plausibly anno-
tate such a number of trees in a short amount of time
for a language of interest, so we believe that this is
an important setting in which to show improvement,
even for a method primarily intended to augment un-
supervised parsing.
In addition, we consider two different sources for
our bilingual lexicon:
AUTOMATIC: Extracted from bitext
MANUAL: Constructed from human annotations
Both bitexts and human-curated bilingual dictionar-
ies are more widely available than complete tree-
banks. Bitexts can provide rich information about
lexical correspondences in terms of how words are
used in practice, but for resource-poor languages,
parallel text may only be available in small quan-
tities, or be domain-limited. We show results of our
method on bilingual dictionaries derived from both
sources, in order to show that it is applicable under a
variety of data conditions and can successfully take
advantage of such resources as are available.
3.2 Datasets
We evaluate our method on a range of languages
taken from the CoNLL shared tasks on multilingual
dependency parsing (Buchholz and Marsi, 2006;
Nivre et al2007). We make use of dependency
treebanks for Danish, German, Greek, Spanish, Ital-
ian, Dutch, Portuguese, and Swedish, all from the
2006 shared task.
For our English resource, we use 500,000 En-
glish newswire sentences from English Gigaword
version 3 (Graff et al2007), parsed with the Berke-
ley Parser (Petrov et al2006) and converted to a
dependency treebank using the head rules of Collins
(1999).8 Our English test set (used in Section 3.4)
consists of the first 300 sentences of section 23 of the
Penn treebank (Marcus et al1993), preprocessed
in the same way. Our model does not use gold fine-
grained POS tags, but we do use coarse POS tags
deterministically generated from the provided gold
fine-grained tags in the style of Berg-Kirkpatrick
and Klein (2010) using the mappings of Petrov et
al. (2011).9 Following McDonald et al2011), we
strip punctuation from all treebanks for the results of
Section 3.3. All results are given in terms of unla-
beled attachment score (UAS), ignoring punctuation
even when it is present.
We use the Europarl parallel corpus (Koehn,
2005) as the bitext from which to extract the AUTO-
MATIC bilingual lexicons. For each target language,
we produce one-to-one alignments on the English-
target bitext by running the Berkeley Aligner (Liang
et al2006) with five iterations of IBM Model 1 and
8Results do not degrade much if one simply uses Sections 2-
21 of the Penn treebank instead. Coverage of rare words in the
treebank is less important when a given word must also appear
in the bilingual lexicon as the translation of an observed German
word in order to be useful.
9Note that even in the absence of gold annotation, such tags
could be produced from bitext using the method of (Das and
Petrov, 2011) or could be read off from a bilingual lexicon.
6
This work Past work
MANUAL AUTOMATIC MPH11* TMU12**
DELEX DELEX+PROJ ? DELEX+PROJ ? Multi-dir Multi-proj ? No clusters X-lingual ?
DA 41.3 43.0 1.67 ? 43.6 2.30 ? 48.9* 0.6* 36.7** 2.0**
DE 58.5 58.7 0.20 59.5 0.94 ? 56.7* -0.1* 48.9** 1.8**
EL 57.9 59.9 1.99 ? 60.5 2.55 ? 60.1* 5.0* 59.5** 3.5**
ES 64.2 65.4 1.20 ? 65.7 1.52 ? 64.2* 0.3* 60.2** 2.7**
IT 65.9 66.5 0.58 67.4 1.54 ? 64.1* 0.9* 64.6** 4.2**
NL 57.0 57.5 0.52 58.8 1.88 ? 55.8* 9.9* 52.8** 1.5**
PT 75.4 77.2 1.83 ? 78.7 3.29 ? 74.0* 1.6* 66.8** 4.2**
SV 64.5 66.1 1.61 ? 66.9 2.34 ? 65.3* 2.7* 55.4** 1.5**
AVG 60.6 61.8 1.20 62.6 2.05 61.1* 2.7* 55.6** 2.7**
Table 1: Evaluation of features derived from AUTOMATIC and MANUAL bilingual lexicons when trained on a con-
catenation of non-target-language treebanks (the BANKS setting). Values reported are UAS for sentences of all lengths
in the standard CoNLL test sets, with punctuation removed from training and test sets. Daggers indicate statistical
significance computed using bootstrap resampling; a single dagger indicates p < 0.1 and a double dagger indicates
p < 0.05. We also include the baseline results of McDonald et al2011) and Ta?ckstro?m et al2012) and improve-
ments from their best methods of using bitext and lexical information. These results are not directly comparable to
ours, as indicated by * and **. However, we still see that the performance of our type-level transfer method approaches
that of bitext-based methods, which require complex bilingual training for each new language.
five iterations of the HMM aligner with agreement
training. Our lexicon is then read off based on rel-
ative frequency counts of aligned instances of each
word in the bitext.
We also use our method on bilingual dictionar-
ies constructed in a more conventional way. For
this purpose, we scrape our MANUAL bilingual lex-
icons from English Wiktionary (Wikimedia Founda-
tion, 2012). We mine entries for English words that
explicitly have foreign translations listed as well as
words in each target language that have English def-
initions. We discard all translation entries where
the English side is longer than one word, except
for constructions of the form ?to VERB?, where we
manually remove the ?to? and allow the word to be
defined as the English infinitive. Finally, because
our method requires a dictionary with probability
weights, we assume that each target language word
translates with uniform probability into any of the
candidates that we scrape.
3.3 BANKS
We first evaluate our model under the BANKS data
condition. Following the procedure from McDonald
et al2011), for each language, we train both our
DELEX and DELEX+PROJ features on a concate-
nation of 2000 sentences from each other CoNLL
training set, plus 2000 sentences from the Penn
Treebank. Again, despite the values of our PROJ
queries being sensitive to which language we are
currently parsing, the signatures are language in-
dependent, so discriminative training still makes
sense over such a combined treebank. Training our
PROJ features on the non-English treebanks in this
concatenation can be understood as trying to learn
which lexico-syntactic properties transfer ?univer-
sally,? or at least transfer broadly within the families
of languages we are considering.
Table 1 shows the performance of the DELEX fea-
ture set and the DELEX+PROJ feature set using both
AUTOMATIC and MANUAL bilingual lexicons. Both
methods provide positive gains across the board that
are statistically significant in the vast majority of
cases, though MANUAL is slightly less effective;
we postpone until Section 4.1 the discussion of the
shortcomings of the MANUAL lexicon.
We include for reference the baseline results of
McDonald et al2011) and Ta?ckstro?m et al2012)
(multi-direct transfer and no clusters) and the im-
provements from their best methods using lexi-
cal information (multi-projected transfer and cross-
lingual clusters). We emphasize that these results
are not directly comparable to our own, as we
have different training data (and even different train-
ing languages) and use a different underlying pars-
ing model (MSTParser instead of a transition-based
7
AUTOMATIC
100 train trees 200 train trees 400 train trees
DELEX DELEX+PROJ ? DELEX DELEX+PROJ ? DELEX DELEX+PROJ ?
DA 67.2 69.5 2.32 ? 69.5 72.3 2.77 ? 71.4 74.6 3.16 ?
DE 72.9 73.9 0.97 75.4 76.5 1.09 ? 77.3 78.5 1.25 ?
EL 70.8 72.9 2.07 ? 72.6 74.9 2.30 ? 74.3 76.7 2.41 ?
ES 72.5 73.0 0.46 74.1 75.4 1.29 ? 75.3 77.2 1.81 ?
IT 73.3 75.4 2.13 ? 74.7 77.3 2.54 ? 76.0 78.7 2.74 ?
NL 63.0 65.8 2.82 ? 64.7 67.6 2.86 ? 66.1 69.2 3.06 ?
PT 78.1 79.5 1.45 ? 79.5 81.1 1.66 ? 80.7 82.4 1.63 ?
SV 76.4 78.1 1.69 ? 78.1 80.2 2.02 ? 79.6 81.7 2.07 ?
AVG 71.8 73.5 1.74 73.6 75.7 2.07 75.1 77.4 2.27
EN 74.4 81.5 7.06 ? 76.6 83.0 6.35 ? 78.3 84.1 5.80 ?
MANUAL
DA 67.2 68.1 0.88 69.5 70.9 1.44 ? 71.4 73.3 1.92 ?
DE 72.9 73.4 0.44 75.4 76.2 0.77 77.3 78.4 1.12 ?
EL 70.8 71.9 1.06 ? 72.6 74.1 1.48 ? 74.3 75.8 1.56 ?
ES 72.5 71.9 -0.64 74.1 74.3 0.23 75.3 76.4 1.04 ?
IT 73.3 74.3 1.01 ? 74.7 76.4 1.66 ? 76.0 78.0 2.01 ?
NL 63.0 65.4 2.43 ? 64.7 67.5 2.76 ? 66.1 69.0 2.91 ?
PT 78.1 78.2 0.13 79.5 80.1 0.62 80.7 81.5 0.82 ?
SV 76.4 76.6 0.25 78.1 79.1 1.01 ? 79.6 81.0 1.40 ?
AVG 71.8 72.5 0.70 73.6 74.8 1.25 75.1 76.7 1.60
EN 74.4 81.5 7.06 ? 76.6 83.0 6.35 ? 78.3 84.1 5.80 ?
Table 2: Evaluation of features derived from AUTOMATIC and MANUAL bilingual lexicons when trained on various
small numbers of target language trees (the SEED setting). Values reported are UAS for sentences of all lengths on
our enlarged CoNLL test sets (see text); each value is based on 50 sampled training sets of the given size. Daggers
indicate statistical significance as described in the text. Statistical significance is not reported for averages.
parser (Nivre, 2008)). However, our baseline is com-
petitive with theirs,10 demonstrating that we have
constructed a state-of-the-art delexicalized parser.
Furthermore, our method appears to approach the
performance of previous bitext-based methods, and
because of its flexibility and the freedom from com-
plex cross-lingual training for each new language, it
can be applied in the MANUAL case as well, a capa-
bility which neither of the other methods has.
3.4 SEED
We now turn our attention to the SEED scenario,
where a small number of target language trees are
available for each language we consider. While it
is imaginable to continue to exploit the other tree-
banks in the presence of target language trees, we
found that training our DELEX features on the seed
treebank alone gave higher performance than any
10The baseline of Ta?ckstro?m et al2012) is lower because it
is trained only on English rather than on many languages.
attempt to also use the concatenation of treebanks
from the previous section. This is not too surpris-
ing because, with this number of sentences, there is
already good monolingual coverage of coarse POS
features, and attempting to train features on other
languages can be expected to introduce noise into
otherwise accurate monolingual feature weights.
We train our DELEX+PROJ model with both AU-
TOMATIC and MANUAL lexicons on target language
training sets of size 100, 200, and 400, and give re-
sults for each language in Table 2. The performance
of parsers trained on small numbers of trees can
be highly variable, so we create multiple treebanks
of each size by repeatedly sampling from each lan-
guage?s train treebank, and report averaged results.
Furthermore, this evaluation is not on the standard
CoNLL test sets, but is instead on those test sets with
a few hundred unused training sentences added, the
reason being that some of the CoNLL test sets are
very small (fewer than 200 sentences) and appeared
8
to give highly variable results. To compute statistical
significance, we draw a large number of bootstrap
samples for each training set used, then aggregate all
of their sufficient statistics in order to compute the fi-
nal p-value. We see that our DELEX+PROJ method
gives statistically significant gains at the 95% level
over DELEX for nearly all language and training set
size pairs, giving on average a 9% relative error re-
duction in the 400-tree case.
Because our features are relatively few in number
and capture heuristic information, one question we
might ask is how well they can perform in a non-
projection context. In the last line of the table, we
report gains that are achieved when PROJ features
computed from parsed Gigaword are used directly
on English, with no intermediate dictionary. These
are not comparable to the other values in the table
because we are using our projection strategy mono-
lingually, which removes the barriers of imperfect
lexical correspondence (from using the lexicon) and
imperfect syntactic correspondence (from project-
ing). As one might expect, the gains on English are
far higher than the gains on other languages. This
indicates that performance is chiefly limited by the
need to do cross-lingual feature adaptation, not in-
herently low feature capacity. We delay further dis-
cussion to Section 4.2.
One surprising thing to note is that the gains given
by our PROJ features are in some cases larger here
than in the BANKS setting. This result is slightly
counterintuitive, as our baseline parsers are much
better in this case and so we would expect dimin-
ished returns from our method. We conclude that ac-
curately learning which signatures transfer between
languages is important, and it is easier to learn good
feature weights when some target language data is
available. Further evidence supporting this hypothe-
sis is the fact that the gains are larger and more sig-
nificant on larger training set sizes.
4 Discussion
4.1 AUTOMATIC versus MANUAL
Overall, we see that gains from using our MANUAL
lexicons are slightly lower than those from our AU-
TOMATIC lexicons. One might expect higher per-
formance because scraped bilingual lexicons are not
prone to some of the same noise that exists in auto-
AUTOMATIC MANUAL
Voc OCC Voc OCC
DA 324K 0.91 22K 0.64
DE 320K 0.89 58K 0.55
EL 196K 0.94 23K 0.43
ES 165K 0.89 206K 0.74
IT 158K 0.91 78K 0.65
NL 251K 0.87 50K 0.72
PT 165K 0.85 46K 0.53
SV 307K 0.93 28K 0.60
Table 3: Lexicon statistics for all languages for both
sources of bilingual lexicons. ?Voc? indicates vocabulary
size and ?OCC? indicates open-class coverage, the frac-
tion of open-class tokens in the test treebanks with entries
in our bilingual lexicon.
matic aligners, but this is empirically not the case.
Rather, as we see in Table 3, the low recall of our
MANUAL lexicons on open-class words appears to
be a possible culprit. The coverage gap between
these and the AUTOMATIC lexicons is partially due
to the inconsistent structure of Wiktionary: inflected
German and Greek words often do not have their
own pages, so we miss even common morphologi-
cal variants of verb forms in those languages. The
inflected forms that we do scrape are also mapped
to the English base form rather than the correspond-
ing inflected form in English, which introduces fur-
ther noise. Coverage is substantially higher if we
translate using stems only, but this did not empir-
ically lead to performance improvements, possibly
due to conflating different parts of speech with the
same base form.
One might hypothesize that our uniform weight-
ing scheme in the MANUAL lexicon is another
source of problems, and that bitext-derived weights
are necessary to get high performance. This is not
the case here. Truncating the AUTOMATIC dictio-
nary to at most 20 translations per word and setting
the weights uniformly causes a slight performance
drop, but is still better than our MANUAL lexicon.
This further demonstrates that these problems are
more a limitation of our dictionary than our method.
English Wiktionary is not designed to be a bilingual
dictionary, and while it conveniently provided an
easy way for us to produce lexicons for a wide array
9
Frauen    wollen    weiter     f?r       die     Quote  k?mpfen
   NN     VMFIN    ADV    APPR   ART      NN    VVINF
Women     want     further     for       the     quota     fight
Women    want    to   continue   to    fight   for   the   quota
   NNP      VBP   TO      VB      TO    VB    IN   DT    NN
Figure 4: Example of a German tree and a parallel En-
glish sentence with high levels of syntactic divergence.
The English verb want takes fundamentally different chil-
dren than wollen does, so properties of the sort we present
in Section 2.2 will not transfer effectively.
of languages, it is not the resource that one would
choose if designing a parser for a specific target lan-
guage. Bitext is not necessary for our approach to
work, and results on the AUTOMATIC lexicon sug-
gest that our type-level transfer method can in fact
do much better given a higher quality resource.
4.2 Limitations
While our method does provide consistent gains
across a range of languages, the injection of lexical
information is clearly not sufficient to bridge the gap
between unsupervised and supervised parsers. We
argued in Section 3.4 that the cross-lingual transfer
step of our method imposes a fundamental limitation
on how useful any such approach can be, which we
now investigate further.
In particular, any syntactic divergence, especially
inconsistent divergences like head switching, will
limit the utility of transferred structure. Consider
the German example in Figure 4, with a parallel En-
glish sentence provided. The English tree suggests
that want should attach to an infinitival to, which has
no correlate in German. Even disregarding this, its
grandchild is the verb continue, which is realized in
the German sentence as the adverb weiter. While
it is still broadly true that want and wollen both
have verbal elements located to their right, it is less
clear how to design features that can still take advan-
tage of this while working around the differences we
have described. Therefore, a gap between the per-
formance of our features on English and the perfor-
mance of our projected features, as is observed in
Table 2, is to be expected in the absence of a more
complete model of syntactic divergence.
5 Conclusion
In this work, we showed that lexical attachment pref-
erences can be projected to a target language at the
type level using only a bilingual lexicon, improving
over a delexicalized baseline parser. This method
is broadly applicable in the presence or absence
of target language training trees and with bilingual
lexicons derived from either manually-annotated re-
sources or bitexts. The greatest improvements arise
when the bilingual lexicon has high coverage and a
number of target language trees are available in or-
der to learn exactly what lexico-syntactic properties
transfer from the source language.
In addition, we showed that a well-tuned discrim-
inative model with the correct features can achieve
good performance even on very small training sets.
While unsupervised and existing projection meth-
ods do feature great versatility and may yet pro-
duce state-of-the-art parsers on resource-poor lan-
guages, spending time constructing small supervised
resources appears to be the fastest method to achieve
high performance in these settings.
Acknowledgments
This work was partially supported by an NSF Grad-
uate Research Fellowship to the first author, by a
Google Fellowship to the second author, and by the
NSF under grant 0643742. Thanks to the anony-
mous reviewers for their insightful comments.
References
Mohit Bansal and Dan Klein. 2011. Web-scale Features
for Full-scale Parsing. In Proceedings of ACL, pages
693?702, Portland, Oregon, USA.
Taylor Berg-Kirkpatrick and Dan Klein. 2010. Phylo-
genetic Grammar Induction. In Proceedings of ACL,
pages 1288?1297, Uppsala, Sweden.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
Shared Task on Multilingual Dependency Parsing. In
Proceedings of CoNLL, pages 149?164.
Shay B. Cohen and Noah A. Smith. 2009. Shared Logis-
tic Normal Distributions for Soft Parameter Tying in
10
Unsupervised Grammar Induction. In Proceedings of
NAACL, pages 74?82, Boulder, Colorado.
Shay B. Cohen, Dipanjan Das, and Noah A. Smith. 2011.
Unsupervised Structure Prediction with Non-Parallel
Multilingual Guidance. In Proceedings of EMNLP,
pages 50?61, Edinburgh, UK.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
Koby Crammer and Yoram Singer. 2001. Ultraconserva-
tive Online Algorithms for Multiclass Problems. Jour-
nal of Machine Learning Research, 3:2003.
Dipanjan Das and Slav Petrov. 2011. Unsupervised Part-
of-Speech Tagging with Bilingual Graph-Based Pro-
jections. In Proceedings of ACL, pages 600?609, Port-
land, Oregon, USA.
Kuzman Ganchev, Jennifer Gillenwater, and Ben Taskar.
2009. Dependency Grammar Induction via Bitext Pro-
jection Constraints. In Proceedings of ACL, pages
369?377, Suntec, Singapore.
David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda.
2007. English Gigaword Third Edition. Linguistic
Data Consortium, Catalog Number LDC2007T07.
Aria Haghighi and Dan Klein. 2006. Prototype-driven
Grammar Induction. In Proceedings of CoLING-ACL,
pages 881?888, Sydney, Australia.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
Parsers via Syntactic Projection Across Parallel Texts.
Natural Language Engineering, 11:311?325, Septem-
ber.
Dan Klein and Christopher D. Manning. 2004. Corpus-
Based Induction of Syntactic Structure: Models of De-
pendency and Constituency. In Proceedings of ACL,
pages 479?486.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In MT Summit X,
pages 79?86, Phuket, Thailand. AAMT.
Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple Semi-Supervised Dependency Parsing. In Pro-
ceedings of ACL.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by Agreement. In Proceedings of NAACL, New
York, New York.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a Large Annotated Cor-
pus of English: the Penn Treebank. Computational
Linguistics, 19:313?330, June.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online Large-Margin Training of Dependency
Parsers. In Proceedings of ACL, pages 91?98, Ann
Arbor, Michigan.
Ryan McDonald, Slav Petrov, and Keith Hall. 2011.
Multi-Source Transfer of Delexicalized Dependency
Parsers. In Proceedings of EMNLP, pages 62?72, Ed-
inburgh, Scotland, UK.
Tahira Naseem, Harr Chen, Regina Barzilay, and Mark
Johnson. 2010. Using Universal Linguistic Knowl-
edge to Guide Grammar Induction. In Proceed-
ings of EMNLP, pages 1234?1244, Cambridge, Mas-
sachusetts.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mcdon-
ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007. The CoNLL 2007 Shared Task on Dependency
Parsing. In Proceedings of EMNLP-CoNLL, pages
915?932, Prague, Czech Republic.
Joakim Nivre. 2008. Algorithms for Deterministic Incre-
mental Dependency Parsing. Computational Linguis-
tics, 34:513?553, December.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning Accurate, Compact, and In-
terpretable Tree Annotation. In Proceedings of ACL,
pages 433?440, Sydney, Australia.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2011.
A Universal Part-of-Speech Tagset. In ArXiv, April.
David A. Smith and Jason Eisner. 2009. Parser Adapta-
tion and Projection with Quasi-Synchronous Grammar
Features. In Proceedings of EMNLP, pages 822?831,
Suntec, Singapore.
Oscar Ta?ckstro?m, Ryan McDonald, and Jakob Uszkoreit.
2012. Cross-lingual Word Clusters for Direct Trans-
fer of Linguistic Structure. In Proceedings of NAACL,
Montreal, Canada.
Wikimedia Foundation. 2012. Wiktionary. Online at
http://www.wiktionary.org/.
Guangyou Zhou, Jun Zhao, Kang Liu, and Li Cai. 2011.
Exploiting Web-Derived Selectional Preference to Im-
prove Statistical Dependency Parsing. In Proceedings
of ACL, pages 1556?1565, Portland, Oregon, USA.
11
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1971?1982,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Easy Victories and Uphill Battles in Coreference Resolution
Greg Durrett and Dan Klein
Computer Science Division
University of California, Berkeley
{gdurrett,klein}@cs.berkeley.edu
Abstract
Classical coreference systems encode various
syntactic, discourse, and semantic phenomena
explicitly, using heterogenous features com-
puted from hand-crafted heuristics. In con-
trast, we present a state-of-the-art coreference
system that captures such phenomena implic-
itly, with a small number of homogeneous
feature templates examining shallow proper-
ties of mentions. Surprisingly, our features
are actually more effective than the corre-
sponding hand-engineered ones at modeling
these key linguistic phenomena, allowing us
to win ?easy victories? without crafted heuris-
tics. These features are successful on syntax
and discourse; however, they do not model
semantic compatibility well, nor do we see
gains from experiments with shallow seman-
tic features from the literature, suggesting that
this approach to semantics is an ?uphill bat-
tle.? Nonetheless, our final system1 outper-
forms the Stanford system (Lee et al (2011),
the winner of the CoNLL 2011 shared task)
by 3.5% absolute on the CoNLL metric and
outperforms the IMS system (Bjo?rkelund and
Farkas (2012), the best publicly available En-
glish coreference system) by 1.9% absolute.
1 Introduction
Coreference resolution is a multi-faceted task: hu-
mans resolve references by exploiting contextual
and grammatical clues, as well as semantic infor-
mation and world knowledge, so capturing each of
1The Berkeley Coreference Resolution System is available
at http://nlp.cs.berkeley.edu.
these will be necessary for an automatic system to
fully solve the problem. Acknowledging this com-
plexity, coreference systems, either learning-based
(Bengtson and Roth, 2008; Stoyanov et al, 2010;
Haghighi and Klein, 2010; Rahman and Ng, 2011b)
or rule-based (Haghighi and Klein, 2009; Lee et
al., 2011), draw on diverse information sources and
complex heuristics to resolve pronouns, model dis-
course, determine anaphoricity, and identify seman-
tically compatible mentions. However, this leads to
systems with many heterogenous parts that can be
difficult to interpret or modify.
We build a learning-based, mention-synchronous
coreference system that aims to use the simplest pos-
sible set of features to tackle the various aspects
of coreference resolution. Though they arise from
a small number of simple templates, our features
are numerous, which works to our advantage: we
can both implicitly model important linguistic ef-
fects and capture other patterns in the data that are
not easily teased out by hand. As a result, our data-
driven, homogeneous feature set is able to achieve
high performance despite only using surface-level
document characteristics and shallow syntactic in-
formation. We win ?easy victories? without design-
ing features and heuristics explicitly targeting par-
ticular phenomena.
Though our approach is successful at modeling
syntax, we find semantics to be a much more chal-
lenging aspect of coreference. Our base system
uses only two recall-oriented features on nominal
and proper mentions: head match and exact string
match. Building on these features, we critically eval-
uate several classes of semantic features which intu-
1971
itively should prove useful but have had mixed re-
sults in the literature, and we observe that they are
ineffective for our system. However, these features
are beneficial when gold mentions are provided to
our system, leading us to conclude that the large
number of system mentions extracted by most coref-
erence systems (Lee et al, 2011; Fernandes et al,
2012) means that weak indicators cannot overcome
the bias against making coreference links. Capturing
semantic information in this shallow way is an ?up-
hill battle? due to this structural property of corefer-
ence resolution.
Nevertheless, using a simple architecture and fea-
ture set, our final system outperforms the two best
publicly available English coreference systems, the
Stanford system (Lee et al, 2011) and the IMS sys-
tem (Bjo?rkelund and Farkas, 2012), by wide mar-
gins: 3.5% absolute and 1.9% absolute, respectively,
on the CoNLL metric.
2 Experimental Setup
Throughout this work, we use the datasets from the
CoNLL 2011 shared task2 (Pradhan et al, 2011),
which is derived from the OntoNotes corpus (Hovy
et al, 2006). When applicable, we use the standard
automatic parses and NER tags for each document.
All experiments use system mentions except where
otherwise indicated. For each experiment, we report
MUC (Vilain et al, 1995), B3 (Bagga and Baldwin,
1998), and CEAFe (Luo, 2005), as well as their av-
erage, the CoNLL metric. All metrics are computed
using version 5 of the official CoNLL scorer.3
3 A Mention-Synchronous Framework
We first present the basic architecture of our corefer-
ence system, independent of a feature set. Unlike bi-
nary classification-based coreference systems where
independent binary decisions are made about each
pair (Soon et al, 2001; Bengtson and Roth, 2008;
Versley et al, 2008; Stoyanov et al, 2010), we use a
log-linear model to select at most one antecedent for
2This dataset is identical to the English portion of the
CoNLL 2012 data, except for the absence of a small pivot text.
3Note that this version of the scorer implements a modified
version ofB3, described in Cai and Strube (2010), that was used
for the CoNLL shared tasks. The implementation of CEAFe
is also not exactly as described in Luo et al (2004), but for
completeness we include this metric as well.
each mention or determine that it begins a new clus-
ter (Denis and Baldridge, 2008). In this mention-
ranking or mention-synchronous framework, fea-
tures examine single mentions to evaluate whether
or not they are anaphoric and pairs of mentions to
evaluate whether or not they corefer. While other
work has used this framework as a starting point
for entity-level systems (Luo et al, 2004; Rahman
and Ng, 2009; Haghighi and Klein, 2010; Durrett et
al., 2013), we will show that a mention-synchronous
approach is sufficient to get state-of-the-art perfor-
mance on its own.
3.1 Mention Detection
Our system first identifies a set of predicted men-
tions from text annotated with parses and named en-
tity tags. We extract three distinct types of mentions:
proper mentions from all named entity chunks ex-
cept for those labeled as QUANTITY, CARDINAL, or
PERCENT, pronominal mentions from single words
tagged with PRP or PRP$, and nominal mentions
from all other maximal NP projections. These basic
rules are similar to those of Lee et al (2011), except
that their system uses an additional set of filtering
rules designed to discard instances of pleonastic it,
partitives, certain quantified noun phrases, and other
spurious mentions. In contrast to this highly engi-
neered approach and to systems which use a trained
classifier to compute anaphoricity separately (Rah-
man and Ng, 2009; Bjo?rkelund and Farkas, 2012),
we aim for the highest possible recall of gold men-
tions with a low-complexity method, leaving us with
a large number of spurious system mentions that we
will have to reject later.
3.2 Coreference Model
Figure 1 shows the mention-ranking architecture
that serves as the backbone of our coreference sys-
tem. Assume we have extracted n mentions from
a document x, where x denotes the surface proper-
ties of a document and any precomputed informa-
tion. The ith mention in a document has an asso-
ciated random variable ai taking values in the set
{1, . . . , i?1, NEW}; this variable specifies mention
i?s selected antecedent or indicates that it begins a
new coreference chain. A setting of the ai, denoted
by a = (a1, ..., an), implies a unique set of corefer-
ence chains C that serve as our system output.
1972
[Voters]
1
 agree when [they]
1
 are given a [chance]
2
 to decide if [they]
1
 ...
NEW
False New
Correct
1?
NEW
2?
3?
1?
Correct
a
1
NEW NEW
1?
2?
False Anaphor
False Anaphor
Correct
Wrong Link
False New
Correct
Correct
a
4
a
3
a
2
Figure 1: The basic structure of our coreference model. The ith mention in a document has i possible antecedence
choices: link to one of the i? 1 preceding mentions or begin a new cluster. We place a distribution over these choices
with a log-linear model. Structurally different kinds of errors are weighted differently to optimize for final coreference
loss functions; error types are shown corresponding to the decisions for each mention.
We use a log linear model of the conditional dis-
tribution P (a|x) as follows:
P (a|x) ? exp
(
n?
i=1
w>f(i, ai, x)
)
where f(i, ai, x) is a feature function that examines
the coreference decision ai for mention i with doc-
ument context x. When ai = NEW, the features
fired indicate the suitability of the given mention to
be anaphoric or not; when ai = j for some j, the
features express aspects of the pairwise linkage, and
can examine any relevant attributes of the anaphor
i or the antecedent j, since information about each
mention is contained in x.
Inference in this model is efficient: because
logP (a|x) decomposes linearly over mentions, we
can compute ai = argmaxai P (ai|x) separately
for each mention and return the set of coreference
chains implied by these decisions.
3.3 Learning
During learning, we optimize for conditional log-
likelihood augmented with a parameterized loss
function (Durrett et al, 2013). The main compli-
cating factor in this process is that the supervision
in coreference consists of a gold clustering C? de-
fined over gold mentions. This is problematic for
two reasons: first, because the clustering is defined
over gold mentions rather than our system mentions,
and second, because a clustering does not specify a
full antecedent structure of the sort our model pro-
duces. We can address the first of these problems
by imputing singleton clusters for mentions that do
not appear in the gold standard; our system will then
simply learn to put spurious mentions in their own
clusters. Singletons are always removed before eval-
uation because the OntoNotes corpus does not anno-
tate them, so in this way we can neatly dispose of
spurious mentions. To address the lack of explicit
antecedents in C?, we simply sum over all possible
antecedent structures licensed by the gold clusters.
Formally, we will maximize the conditional log-
likelihood of the set A(C?) of antecedent vectors
a for a document that are consistent with the gold
annotation.4 Consistency for an antecedent choice
ai under gold clusters C? is defined as follows:
1. If ai = j, ai is consistent iff mentions i and j
are present in C? and are in the same cluster.
2. If ai = NEW, ai is consistent off mention i is
not present in C?, or it is present in C? and has
no gold antecedents, or it is present in C? and
none of its gold antecedents are among the set
of system predicted mentions.
Given t training examples of the form (xk, C?k),
we write the following likelihood function:
`(w) =
t?
k=1
log
?
?
?
a?A(C?k)
P ?(a|xk)
?
?+ ??w?1
where P ?(a|xk) ? P (a|xk) exp(l(a,C?k)) with
l(a,C?) being a real-valued loss function. The loss
4Because of this marginalization over latent antecedent
choices, our objective is non-convex.
1973
here plays an analogous role to the loss in struc-
tured max-margin objectives; incorporating it into a
conditional likelihood objective is a technique called
softmax-margin (Gimpel and Smith, 2010).
Our loss function l(a,C?) is a weighted linear
combination of three error types, examples of which
are shown in Figure 1. A false anaphor (FA) error
occurs when ai is chosen to be anaphoric when it
should start a new cluster. A false new (FN) error oc-
curs in the opposite case, when ai wrongly indicates
a new cluster when it should be anaphoric. Finally,
a wrong link (WL) error occurs when the antecedent
chosen for ai is the wrong antecedent (but ai is in-
deed anaphoric). Our final parameterized loss func-
tion is a weighted sum of the counts of these three
error types:
l(a,C?) = ?FAFA(a,C?) + ?FNFN(a,C?) + ?WLWL(a,C?)
where FA(a,C?) gives the number of false anaphor
errors in prediction a with gold chains C? (FN and
WL are analogous). By setting ?FA low and ?FN
high relative to ?WL, we can counterbalance the
high number of singleton mentions and bias the sys-
tem towards making more coreference linkages. We
set (?FA, ?FN, ?WL) = (0.1, 3.0, 1.0) and ? =
0.001 and optimize the objective using AdaGrad
(Duchi et al, 2011).
4 Easy Victories from Surface Features
Our primary goal with this work is to show that a
high-performance coreference system is attainable
with a small number of feature templates that use
only surface-level information sources. These fea-
tures will be general-purpose and capture linguistic
effects to the point where standard heuristic-driven
features are no longer needed in our system.
4.1 SURFACE Features and Conjunctions
Our SURFACE feature set only considers the follow-
ing properties of mentions and mention pairs:
? Mention type (nominal, proper, or pronominal)
? The complete string of a mention
? The semantic head of a mention
? The first word and last word of each mention
Feature name Count
Features on the current mention
[ANAPHORIC] + [HEAD WORD] 41371
[ANAPHORIC] + [FIRST WORD] 18991
[ANAPHORIC] + [LAST WORD] 19184
[ANAPHORIC] + [PRECEDING WORD] 54605
[ANAPHORIC] + [FOLLOWING WORD] 57239
[ANAPHORIC] + [LENGTH] 4304
Features on the antecedent
[ANTECEDENT HEAD WORD] 57383
[ANTECEDENT FIRST WORD] 24239
[ANTECEDENT LAST WORD] 23819
[ANTECEDENT PRECEDING WORD] 53421
[ANTECEDENT FOLLOWING WORD] 55718
[ANTECEDENT LENGTH] 4620
Features on the pair
[EXACT STRING MATCH (T/F)] 47
[HEAD MATCH (T/F)] 46
[SENTENCE DISTANCE, CAPPED AT 10] 2037
[MENTION DISTANCE, CAPPED AT 10] 1680
Table 1: Our SURFACE feature set, which exploits a
small number of surface-level mention properties. Fea-
ture counts for each template are computed over the train-
ing set, and include features generated by our conjunction
scheme (not explicitly shown in the table; see Figure 2),
which yields large numbers of features at varying levels
of expressivity.
? The word immediately preceding and the word
immediately following a mention
? Mention length, in words
? Two distance measures between mentions
(number of sentences and number of mentions)
Table 1 shows the SURFACE feature set. Features
that look only at the current mention fire on all de-
cisions (ai = j or ai = NEW), whereas features
that look at the antecedent in any way (the latter
two groups of features) only fire on pairwise link-
ages (ai 6= NEW).
Two conjunctions of each feature are also in-
cluded: first with the ?type? of the mention be-
ing resolved (either NOMINAL, PROPER, or, if it is
pronominal, the citation form of the pronoun), and
then additionally with the antecedent type (only if
the feature is over a pairwise link). This conjunc-
tion process is shown in Figure 2. Note that features
that just examine the antecedent will end up with
1974
[Voters]
1
 generally agree when [they]
1 
...
NEW
1?
a
2
NEW ? LEN = 1
NEW ? LEN = 1 ? [they]
ANT. HEAD = Voters
ANT. HEAD = Voters ? [they]
ANT. HEAD = Voters ? [they] ? NOM
MENT DIST = 1
MENT DIST = 1 ? [they]
MENT DIST = 1 ? [they] ? NOM
Figure 2: Demonstration of the conjunction scheme we
use. Each feature on anaphoricity is conjoined with the
type (NOMINAL, PROPER, or the citation form if it is a
pronoun) of the mention being resolved. Each feature on
a mention pair is additionally conjoined with the types of
the current and antecedent mentions.
conjunctions that examine properties of the current
mention as well, as shown with the ANT. HEAD fea-
ture in the figure.
Finally, we found it beneficial for our lexical indi-
cator features to only fire on words occurring at least
20 times in the training set; for rare words, we use
the part of speech of the word instead.
The performance of our system is shown in Ta-
ble 2. We contrast our performance with that of
the Stanford system (Lee et al (2011), the winner
of the CoNLL 2011 shared task) and the IMS sys-
tem (Bjo?rkelund and Farkas (2012), the best publicly
available English coreference system). Despite its
simplicity, our SURFACE system is sufficient to out-
perform these sophisticated systems: the Stanford
system uses a cascade of ten rule-based sieves each
of which has customized heuristics, and the IMS
system uses a similarly long pipeline consisting of
a learned referentiality classifier followed by multi-
ple resolvers, which are run in sequence and rely on
the outputs of the previous resolvers as features.
4.2 Data-Driven versus Heuristic-Driven
Features
Why are the SURFACE features sufficient to give
high coreference performance, when they do not
make apparent reference to important linguistic phe-
nomena? The main reason is that they actually do
capture the same phenomena as standard corefer-
MUC B3 CEAFe Avg.
STANFORD 60.46 65.48 47.07 57.67
IMS 62.15 65.57 46.66 58.13
SURFACE 64.39 66.78 49.00 60.06
Table 2: Results for our SURFACE system, the STAN-
FORD system, and the IMS system on the CoNLL 2011
development set. Complete results are shown in Ta-
ble 7. Despite using limited information sources, our sys-
tem is able to substantially outperform the other two, the
two best publicly-available English coreference systems.
Bolded values are significant with p < 0.05 according to
a bootstrap resampling test.
ence features, just implicitly. For example, rather
than having rules targeting person, number, gender,
or animacy of mentions, we use conjunctions with
pronoun identity, which contains this information.
Rather than explicitly writing a feature targeting def-
initeness, our indicators on the first word of a men-
tion will capture this and other effects. And finally,
rather than targeting centering theory (Grosz et al,
1995) with rule-based features identifying syntac-
tic positions (Stoyanov et al, 2010; Haghighi and
Klein, 2010), our features on word context can iden-
tify configurational clues like whether a mention
is preceded or followed by a verb, and therefore
whether it is likely in subject or object position.5
Not only are data-driven features able to capture
the same phenomena as heuristic-driven features,
but they do so at a finer level of granularity, and can
therefore model more patterns in the data. To con-
trast these two types of features, we experiment with
three ablated versions of our system, where we re-
place data-driven features with their heuristic-driven
counterparts:
1. Instead of using an indicator on the first word
of a mention (1STWORD), we instead fire
a feature based on that mention?s manually-
computed definiteness (DEF).
2. Instead of conjoining features on pronominal-
pronominal linkages with the citation form of
5Heuristic-driven approaches were historically more appro-
priate, since past coreference corpora such as MUC and ACE
were smaller and therefore more prone to overfitting feature-
rich models. However, the OntoNotes dataset contains thou-
sands of documents, so having support for features is less of a
concern.
1975
MUC B3 CEAFe Avg.
SURFACE 64.39 66.78 49.00 60.06
?1STWORD 63.32 66.22 47.89 59.14
+DEF?1STWORD 63.79 66.46 48.35 59.53
?PRONCONJ 59.97 63.46 47.94 57.12
+AGR?PRONCONJ 63.54 66.10 48.72 59.45
?CONTEXT 60.88 64.66 47.60 57.71
+POSN?CONTEXT 62.45 65.44 48.08 58.65
+DEF+AGR+POSN 64.55 66.93 48.94 60.14
Table 3: CoNLL metric scores on the development set,
for the three different ablations and replacement features
described in Section 4.2. Feature types are described in
the text; + indicates inclusion of that feature class, ? in-
dicates exclusion. Each individual shallow indicator ap-
pears to do as well at capturing its target phenomenon as
the hand-engineered features, while capturing other infor-
mation as well. Moreover, the hand-engineered features
give no benefit over the SURFACE system.
each pronoun (PRONCONJ), we only conjoin
with a PRONOUN indicator and add features
targeting the person, number, gender, and an-
imacy of the two pronouns (AGR).
3. Instead of using our context features on the
preceding and following word (CONTEXT), we
use manual determinations of when mentions
are in subject, direct object, indirect objection,
or oblique position (POSN).
All rules for computing person, number, gender, an-
imacy, definiteness, and syntactic position are taken
from the system of Lee et al (2011).
Table 3 shows each of the target ablations, as well
as the SURFACE system with the DEF, AGR, and
POSN features added. While the heuristic-driven
feature always help over the corresponding ablated
system, they cannot do the work of the fine-grained
data-driven features. Most tellingly, though, none of
the heuristic-driven features give statistically signifi-
cant improvements on top of the data-driven features
we have already included, indicating that we are at
the point of diminishing returns on modeling those
specific phenomena. While this does not preclude
further engineering to take better advantage of other
syntactic constraints, our simple features represent
an ?easy victory? on this subtask.
5 Uphill Battles on Semantics
In Section 4, we gave a simple set of features that
yielded a high-performance coreference system; this
high performance is possible because features tar-
geting only superficial properties in a fine-grained
way can actually model complex linguistic con-
straints. However, while our existing features cap-
ture syntactic and discourse-level phenomena sur-
prisingly well, they are not effective at capturing se-
mantic phenomena like type compatibility. We will
show that due to structural aspects of the coreference
resolution problem, even a combination of several
shallow semantic features from the literature fails to
adequately model semantics.
5.1 Analysis of the SURFACE System
What can the SURFACE system resolve correctly,
and what errors does it still make? To answer this
question, we will split mentions into several cate-
gories based on their observable properties and the
gold standard coreference information, and exam-
ine our system?s accuracy on each mention subclass
in order to more thoroughly characterize its perfor-
mance.6 These categories represent important dis-
tinctions in terms of the difficulty of mention reso-
lution for our system.
We first split mentions into three categories by
their status in the gold standard: singleton (unanno-
tated in the OntoNotes corpus), starting a new entity
with at least two mentions, or anaphoric. It is impor-
tant to note that while singletons and mentions start-
ing new entities are outwardly similar in that they
have no antecedents, and the prediction should be
the same in either case (NEW), we treat them as dis-
tinct because the factors that impact the coreference
decision differ in the two cases. Mentions that start
new clusters are semantically similar to anaphoric
mentions, but may be marked by heaviness or by a
tendency to be named entities, whereas singletons
may be generic or temporal NPs which might be
thought of as coreferent in a loose sense, but are not
6This method of analysis is similar to that undertaken in
Stoyanov et al (2009) and Rahman and Ng (2011b), though
we split our mentions along different axes, and can simply eval-
uate on accuracy because our decisions do not directly imply
multiple links, as they do in binary classification-based systems
(Stoyanov et al, 2009) or in entity-mention models (Rahman
and Ng, 2011b).
1976
Nominal/Proper
Pronominal
1st w/head 2nd+ w/head
Singleton 99.7% 18.1K 85.5% 7.3K 66.5% 1.7K
Starts Entity 98.7% 2.1K 78.9% 0.7K 48.5% 0.3K
Anaphoric 7.9% 0.9K 75.5% 3.9K 72.0% 4.4K
Table 4: Analysis of our SURFACE system on the de-
velopment set. We characterize each predicted mention
by its status in the gold standard (singleton, starting a
new entity, or anaphoric), its type (pronominal or nom-
inal/proper), and by whether its head has appeared as the
head of a previous mention. Each cell shows our sys-
tem?s accuracy on that mention class as well as the size
of the class. The biggest weakness of our system appears
to be its inability to resolve anaphoric mentions with new
heads (bottom-left cell).
included in the OntoNotes dataset due to choices in
the annotation standard.
Second, we divide mentions by their type,
pronominal versus nominal/proper; we then further
subdivide nominals and propers based on whether or
not the head word of the mention has appeared as the
head of a previous mention in the document.
Table 4 shows the results of our analysis. In
each cell, we show the fraction of mentions that
we correctly resolve (i.e., for which we make an
antecedence decision consistent with the gold stan-
dard), as well as the total number of mentions falling
into that cell. First, we observe that there are a sur-
prisingly large number of singleton mentions with
misleading head matches to previous mentions (of-
ten recurring temporal nouns phrases, like July).
The features in our system targeting anaphoricity are
useful for exactly this reason: the more bad head
matches we can rule out based on other criteria, the
more strongly we can rely on head match to make
correct linkages.
Our system is most noticeably poor at resolving
anaphoric mentions whose heads have not appeared
before. The fact that exact match and head match
are our only recall-oriented features on nominals
and propers is starkly apparent here: when we can-
not rely on head match, as is true for this mention
class, we only resolve 7.9% of anaphoric mentions
correctly.7 Many of the mentions in this category
7There are an additional 346 anaphoric nominal/proper men-
tions in the 2nd+ category whose heads only appeared previ-
ously as part of a different cluster; we only resolve 1.7% of
can only be correctly resolved by exploiting world
knowledge, so we will need to include features that
capture this knowledge in some fashion.
5.2 Incorporating Shallow Semantics
As we were able to incorporate syntax with shal-
low features, so too might we hope to incorporate
semantics. However, the semantic information con-
tained even in a coreference corpus of thousands
of documents is insufficient to generalize to unseen
data,8 so system designers have turned to exter-
nal resources such as semantic classes derived from
WordNet (Soon et al, 2001), WordNet hypernymy
or synonymy (Stoyanov et al, 2010), semantic simi-
larity computed from online resources (Ponzetto and
Strube, 2006), named entity type features, gender
and number match using the dataset of Bergsma and
Lin (2006), and features from unsupervised clus-
ters (Hendrickx and Daelemans, 2007; Durrett et al,
2013). In this section, we consider the following
subset of these information sources:
? WordNet hypernymy and synonymy
? Number and gender data for nominals and
propers from Bergsma and Lin (2006)
? Named entity types
? Latent clusters computed from English Giga-
word (Graff et al, 2007), where a latent cluster
label generates each nominal head (excluding
pronouns) and a conjunction of its verbal gov-
ernor and semantic role, if any (Durrett et al,
2013). We use twenty clusters, which include
clusters like president and leader (things which
announce).
Together, we call these the SEM features. We
show results from this expansion of the feature set in
Table 5. When using system mentions, the improve-
ments are not statistically significant on every met-
ric, and are quite marginal given that these features
add information that is intuitively central to corefer-
ence and otherwise unavailable to the system. We
explore the reasons behind this in the next section.
these extremely tricky cases correctly.
8We experimented with bilexical features on head pairs, but
they did not give statistically significant improvements over the
SURFACE features.
1977
MUC B3 CEAFe Avg.
SURFACE 64.39 66.78 49.00 60.06
SURFACE+SEM 64.70 67.27 49.28 60.42
SURFACE (G) 82.80 74.10 68.33 75.08
SURFACE+SEM (G) 84.49 75.65 69.89 76.68
Table 5: CoNLL metric scores on the development set
for our SEM features when added on top of our SURFACE
features. We experiment on both system mentions and
gold mentions. Surprisingly, despite the fact that absolute
performance numbers are much higher on gold mentions
and there is less room for improvement, the semantic fea-
tures help much more than they do on system mentions.
5.3 Analysis of Semantic Features
The main reason that weak semantic cues are not
more effective is the small fraction of positive coref-
erence links present in the training data. From Ta-
ble 4, the number of annotated coreferent spans in
the OntoNotes data is about a factor of five smaller
than the number of system mentions.9 This both
means that most NPs are not coreferent, and for
those that are, choosing the correct links is much
more difficult because of the large number of pos-
sible antecedents. Even head match, which is gen-
erally considered a high-precision indicator (Lee et
al., 2011), would introduce many spurious corefer-
ence arcs if applied too liberally (see Table 4).
In light of this fact, a system needs very strong
evidence to overcome the default hypothesis that a
mention is not coreferent, and a weak indicator will
have such a high ?false positive? rate that it cannot
be relied on (given high weight, this feature would
do more harm than good, by introducing many false
linkages).
To confirm this intuition, we show in the bot-
tom part of Table 5 results when we apply these se-
mantic features on top of our SURFACE system on
gold mentions, where there are no singletons. In the
gold mention setting, we see that the semantic fea-
tures give a consistent improvement on every metric.
Moreover, if we look at a breakdown of errors, the
main improvement the semantic features give us is
on resolution of anaphoric nominals with no head
9This observation is more general than just our system: the
majority of coreference systems, including the winners of the
CoNLL shared tasks (Lee et al, 2011; Fernandes et al, 2012),
opt for high mention recall and resolve a relatively large number
of system mentions.
match: accuracy on the 1601 mentions that fall into
this category improves from 28.0% to 37.9%. On
predicted mentions, by contrast, this category only
improves from 7.9% to 12.2%, a much smaller ab-
solute improvement and one that comes at the ex-
pense of performance on most other resolution class.
The one class that does not get worse, singleton pro-
nouns, actually improves by a similar 4% margin,
indicating that roughly half of the gains we observe
are not even necessarily a result of our features do-
ing what they were designed to do.
Our weak cues do yield some small gains, so there
is hope that better weak indicators of semantic com-
patibility could prove more useful. However, while
extremely high-precision approaches with carefully
engineered features have been shown to be suc-
cessful (Rahman and Ng, 2011a; Bansal and Klein,
2012; Recasens et al, 2013a), we conclude that cap-
turing semantics in a data-driven, shallow manner
remains an uphill battle.
6 FINAL System and Results
While semantic features ended up giving only
marginal benefit, we have demonstrated that nev-
ertheless our SURFACE system is a state-of-the-art
English coreference system. However, there remain
a few natural features that we omitted in order to
keep the system as simple as possible, since they
were orthogonal to the discussion of data-driven
versus heuristic-driven features and do not target
world knowledge. Before giving final results, we
will present a small set of additional features that
consider four additional mention properties beyond
those in Section 4.1:
? Whether two mentions are nested
? Ancestry of each mention head: the depen-
dency parent and grandparent POS tags and arc
directions (shown in Figure 3)
? The speaker of each mention
? Number and gender of each mention as deter-
mined by Bergsma and Lin (2006)
The specific additional features we use are shown
in Table 6. Note that unlike in Section 5, we use
the number and gender information only on the an-
tecedent. Due to our conjunction scheme, both this
1978
ROOT
... sent    it    to  the [president] ... [President Obama] signed ...
VBD PRP TO 
DET NN NNP NNP VBD 
president 
R
TO 
VBD 
R
Obama 
L
VBD 
ROOT 
Figure 3: Demonstration of the ancestry extraction pro-
cess. These features capture more sophisticated configu-
rational information than our context word features do: in
this example, president is in a characteristic indirect ob-
ject position based on its dependency parents, and Obama
is the subject of the main verb of the sentence.
semantic information and the speaker information
can apply in a fine-grained way to different pro-
nouns, and can therefore improve pronoun resolu-
tion substantially; however, these features generally
only improve pronoun resolution.
Full results for our SURFACE and FINAL feature
sets are shown in Table 7. Again, we compare to Lee
et al (2011) and Bjo?rkelund and Farkas (2012).10
Despite our system?s emphasis on one-pass resolu-
tion with as simple a feature set as possible, we are
able to outperform even these sophisticated systems
by a wide margin.
7 Related Work
Many of the individual features we employ in the FI-
NAL feature set have appeared in other coreference
systems (Bjo?rkelund and Nugues, 2011; Rahman
and Ng, 2011b; Fernandes et al, 2012). However,
other authors have often emphasized bilexical fea-
tures on head pairs, whereas our features are heavily
monolexical. For feature conjunctions, other authors
have exploited three classes (Lee et al, 2011) or au-
tomatically learned conjunction schemes (Fernandes
et al, 2012; Lassalle and Denis, 2013), but to our
knowledge we are the first to do fine-grained mod-
eling of every pronoun. Inclusion of a hierarchy of
10Discrepancies between scores here and those printed in
Pradhan et al (2012) arise from two sources: improvements
to the system of Lee et al (2011) since the first CoNLL shared
task, and a fix to the scoring of B3 in the official scorer since
results of the two CoNLL shared tasks were released. Unfor-
tunately, because of this bug in the scoring program, direct
comparison to the printed results of the other highest-scoring
English systems, Fernandes et al (2012) and Martschat et al
(2012), is impossible.
Feature name Count
Features of the SURFACE system 418704
Features on the current mention
[ANAPHORIC] + [CURRENT ANCESTRY] 46047
Features on the antecedent
[ANTECEDENT ANCESTRY] 53874
[ANTECEDENT GENDER] 338
[ANTECEDENT NUMBER] 290
Features on the pair
[HEAD CONTAINED (T/F)] 136
[EXACT STRING CONTAINED (T/F)] 133
[NESTED (T/F)] 355
[DOC TYPE] + [SAME SPEAKER (T/F)] 437
[CURRENT ANCESTRY] + [ANT. ANCESTRY] 2555359
Table 6: FINAL feature set; note that this includes the
SURFACE feature set. As with the features of the SUR-
FACE system, two conjoined variants of each feature
are included: first with the type of the current mention
(NOMINAL, PROPER, or the citation form of the pro-
noun), then with the types of both mentions in the pair.
These conjunctions allow antecedent features on gender
and number to impact pronoun resolution, and they al-
low speaker match to capture effects like I and you being
coreferent when the speakers differ.
features with regularization also means that we or-
ganically get distinctions among different mention
types without having to choose a level of granularity
a priori, unlike the distinct classifiers employed by
Denis and Baldridge (2008).
In terms of architecture, many coreference sys-
tems operate in a pipelined fashion, making par-
tial decisions about coreference or pruning arcs
before full resolution. Some systems use sepa-
rate rule-based and learning-based passes (Chen
and Ng, 2012; Fernandes et al, 2012), a series
of learning-based passes (Bjo?rkelund and Farkas,
2012), or referentiality classifiers that prune the set
of mentions before resolution (Rahman and Ng,
2009; Bjo?rkelund and Farkas, 2012; Recasens et
al., 2013b). By contrast, our system resolves all
mentions in one pass and does not need pruning:
the SURFACE system can train in less than two
hours without any subsampling of coreference arcs,
and rule-based pruning of coreference arcs actually
causes our system to perform less well, since our
features can learn valuable information from these
negative examples.
1979
MUC B3 CEAFe Avg.
Prec. Rec. F1 Prec. Rec. F1 Prec. Rec. F1 F1
CoNLL 2011 Development Set
STANFORD 61.62 59.34 60.46 74.05 58.70 65.48 45.98 48.22 47.07 57.67
IMS 66.67 58.20 62.15 77.60 56.77 65.57 42.92 51.11 46.66 58.13
SURFACE* 68.42 60.80 64.39 76.57 59.21 66.78 45.30 53.36 49.00 60.06
FINAL* 68.97 63.47 66.10 76.58 62.06 68.56 47.32 53.19 50.09 61.58
CoNLL 2011 Test Set
STANFORD 60.91 62.13 61.51 70.61 57.31 63.27 45.79 44.56 45.17 56.65
IMS 68.15 61.60 64.71 75.97 56.39 64.73 42.30 48.88 45.35 58.26
FINAL* 66.81 66.04 66.43 71.07 61.89 66.16 47.37 48.22 47.79 60.13
Table 7: CoNLL metric scores for our systems on the CoNLL development and blind test sets, compared to the results
of Lee et al (2011) (STANFORD) and Bjo?rkelund and Farkas (2012) (IMS). Starred systems are contributions of this
work. Bolded F1 values represent statistically significant improvements over other systems with p < 0.05 using a
bootstrap resampling test. Metric values reflect version 5 of the CoNLL scorer.
8 Conclusion
We have presented a coreference system that uses a
simple, homogeneous set of features in a discrim-
inative learning framework to achieve high perfor-
mance. Large numbers of lexicalized, data-driven
features implicitly model linguistic phenomena such
as definiteness and centering, obviating the need for
heuristic-driven rules explicitly targeting these same
phenomena. Additional semantic features give only
slight benefit beyond head match because they do
not provide strong enough signals of coreference to
improve performance in the system mention setting;
modeling semantic similarity still requires complex
outside information and deep heuristics.
Our system, the Berkeley Coreference
Resolution System, is publicly available at
http://nlp.cs.berkeley.edu.
Acknowledgments
This work was partially supported by BBN un-
der DARPA contract HR0011-12-C-0014 and by
an NSF fellowship for the first author. Thanks
to Sameer Pradhan for helpful discussions regard-
ing the CoNLL scoring program, and thanks to the
anonymous reviewers for their insightful comments.
References
Amit Bagga and Breck Baldwin. 1998. Algorithms for
Scoring Coreference Chains. In Proceedings of the
Conference on Language Resources and Evaluation
Workshop on Linguistics Coreference.
Mohit Bansal and Dan Klein. 2012. Coreference Seman-
tics from Web Features. In Proceedings of the Associ-
ation for Computational Linguistics.
Eric Bengtson and Dan Roth. 2008. Understanding the
Value of Features for Coreference Resolution. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing.
Shane Bergsma and Dekang Lin. 2006. Bootstrapping
Path-Based Pronoun Resolution. In Proceedings of the
Conference on Computational Linguistics and the As-
sociation for Computational Linguistics.
Anders Bjo?rkelund and Richa?rd Farkas. 2012. Data-
driven Multilingual Coreference Resolution using Re-
solver Stacking. In Proceedings of the Joint Confer-
ence on Empirical Methods in Natural Language Pro-
ceedings and Conference on Computational Natural
Language Learning - Shared Task.
Anders Bjo?rkelund and Pierre Nugues. 2011. Exploring
Lexicalized Features for Coreference Resolution. In
Proceedings of the Conference on Computational Nat-
ural Language Learning: Shared Task.
Jie Cai and Michael Strube. 2010. Evaluation Metrics for
End-to-End Coreference Resolution Systems. In Pro-
ceedings of the Special Interest Group on Discourse
and Dialogue.
Chen Chen and Vincent Ng. 2012. Combining the Best
of Two Worlds: A Hybrid Approach to Multilingual
Coreference Resolution. In Proceedings of the Joint
Conference on Empirical Methods in Natural Lan-
guage Proceedings and Conference on Computational
Natural Language Learning - Shared Task.
Pascal Denis and Jason Baldridge. 2008. Specialized
Models and Ranking for Coreference Resolution. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing.
1980
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive Subgradient Methods for Online Learning
and Stochastic Optimization. Journal of Machine
Learning Research, 12:2121?2159, July.
Greg Durrett, David Hall, and Dan Klein. 2013. Decen-
tralized Entity-Level Modeling for Coreference Reso-
lution. In Proceedings of the Association for Compu-
tational Linguistics.
Eraldo Rezende Fernandes, C??cero Nogueira dos Santos,
and Ruy Luiz Milidiu?. 2012. Latent Structure Per-
ceptron with Feature Induction for Unrestricted Coref-
erence Resolution. In Proceedings of the Joint Con-
ference on Empirical Methods in Natural Language
Proceedings and Conference on Computational Nat-
ural Language Learning - Shared Task.
Kevin Gimpel and Noah A. Smith. 2010. Softmax-
Margin CRFs: Training Log-Linear Models with Cost
Functions. In Proceedings of the North American
Chapter for the Association for Computational Lin-
guistics.
David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda.
2007. English Gigaword Third Edition. Linguistic
Data Consortium, Catalog Number LDC2007T07.
Barbara J. Grosz, Scott Weinstein, and Aravind K. Joshi.
1995. Centering: A Framework for Modeling the Lo-
cal Coherence of Discourse. Computational Linguis-
tics, 21(2):203?225, June.
Aria Haghighi and Dan Klein. 2009. Simple Coreference
Resolution with Rich Syntactic and Semantic Features.
In Proceedings of Empirical Methods in Natural Lan-
guage Processing.
Aria Haghighi and Dan Klein. 2010. Coreference Res-
olution in a Modular, Entity-Centered Model. In Pro-
ceedings of the North American Chapter of the Asso-
ciation for Computational Linguistics.
Iris Hendrickx and Walter Daelemans, 2007. Adding Se-
mantic Information: Unsupervised Clusters for Coref-
erence Resolution.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. OntoNotes:
The 90% Solution. In Proceedings of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics: Short Papers.
Emmanuel Lassalle and Pascal Denis. 2013. Improving
Pairwise Coreference Models Through Feature Space
Hierarchy Learning. In Proceedings of the Association
for Computational Linguistics.
Heeyoung Lee, Yves Peirsman, Angel Chang, Nathanael
Chambers, Mihai Surdeanu, and Dan Jurafsky. 2011.
Stanford?s Multi-Pass Sieve Coreference Resolution
System at the CoNLL-2011 Shared Task. In Proceed-
ings of the Conference on Computational Natural Lan-
guage Learning: Shared Task.
Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, Nanda
Kambhatla, and Salim Roukos. 2004. A
Mention-Synchronous Coreference Resolution Algo-
rithm Based on the Bell Tree. In Proceedings of the
Association for Computational Linguistics.
Xiaoqiang Luo. 2005. On Coreference Resolution Per-
formance Metrics. In Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing.
Sebastian Martschat, Jie Cai, Samuel Broscheit, E?va
Mu?jdricza-Maydt, and Michael Strube. 2012. A
Multigraph Model for Coreference Resolution. In
Proceedings of the Joint Conference on Empirical
Methods in Natural Language Proceedings and Con-
ference on Computational Natural Language Learning
- Shared Task.
Simone Paolo Ponzetto and Michael Strube. 2006.
Exploiting Semantic Role Labeling, WordNet and
Wikipedia for Coreference Resolution. In Proceed-
ings of the North American Chapter of the Association
of Computational Linguistics.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen Xue.
2011. CoNLL-2011 Shared Task: Modeling Unre-
stricted Coreference in OntoNotes. In Proceedings of
the Conference on Computational Natural Language
Learning: Shared Task.
Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Olga Uryupina, and Yuchen Zhang. 2012. CoNLL-
2012 Shared Task: Modeling Multilingual Unre-
stricted Coreference in OntoNotes. In Joint Confer-
ence on EMNLP and CoNLL - Shared Task.
Altaf Rahman and Vincent Ng. 2009. Supervised Mod-
els for Coreference Resolution. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing.
Altaf Rahman and Vincent Ng. 2011a. Coreference
Resolution with World Knowledge. In Proceedings
of the Association for Computational Linguistics: Hu-
man Language Technologies.
Altaf Rahman and Vincent Ng. 2011b. Narrowing
the Modeling Gap: A Cluster-Ranking Approach to
Coreference Resolution. Journal of Artificial Intelli-
gence Research, 40(1):469?521, January.
Marta Recasens, Matthew Can, and Daniel Jurafsky.
2013a. Same Referent, Different Words: Unsuper-
vised Mining of Opaque Coreferent Mentions. In Pro-
ceedings of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies.
Marta Recasens, Marie-Catherine de Marneffe, and
Christopher Potts. 2013b. The Life and Death of Dis-
course Entities: Identifying Singleton Mentions. In
1981
Proceedings of the North American Chapter of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies.
Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong
Lim. 2001. A Machine Learning Approach to Coref-
erence Resolution of Noun Phrases. Computational
Linguistics, 27(4):521?544, December.
Veselin Stoyanov, Nathan Gilbert, Claire Cardie, and
Ellen Riloff. 2009. Conundrums in Noun Phrase
Coreference Resolution: Making Sense of the State-
of-the-Art. In Proceedings of the Association for Com-
putational Linguistics.
Veselin Stoyanov, Claire Cardie, Nathan Gilbert, Ellen
Riloff, David Buttler, and David Hysom. 2010. Coref-
erence Resolution with Reconcile. In Proceedings of
the Association for Computational Linguistics: Short
Papers.
Yannick Versley, Simone Paolo Ponzetto, Massimo Poe-
sio, Vladimir Eidelman, Alan Jern, Jason Smith, Xi-
aofeng Yang, and Alessandro Moschitti. 2008. BART:
A Modular Toolkit for Coreference Resolution. In
Proceedings of the Association for Computational Lin-
guistics: Demo Session.
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A Model-
Theoretic Coreference Scoring Scheme. In Proceed-
ings of the Conference on Message Understanding.
1982
Proceedings of NAACL-HLT 2013, pages 1185?1195,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Supervised Learning of Complete Morphological Paradigms
Greg Durrett?
Computer Science Division
University of California, Berkeley
gdurrett@cs.berkeley.edu
John DeNero
Google, Inc.
denero@google.com
Abstract
We describe a supervised approach to predict-
ing the set of all inflected forms of a lexical
item. Our system automatically acquires the
orthographic transformation rules of morpho-
logical paradigms from labeled examples, and
then learns the contexts in which those trans-
formations apply using a discriminative se-
quence model. Because our approach is com-
pletely data-driven and the model is trained
on examples extracted from Wiktionary, our
method can extend to new languages without
change. Our end-to-end system is able to pre-
dict complete paradigms with 86.1% accuracy
and individual inflected forms with 94.9% ac-
curacy, averaged across three languages and
two parts of speech.
1 Introduction
For natural languages with rich morphology, knowl-
edge of how to inflect base forms is critical for both
text generation and analysis. Hand-engineered, rule-
based methods for predicting inflections can offer
extremely high accuracy, but they are laborious to
construct and do not exist with full lexical cover-
age in all languages. By contrast, a large number
of example inflections are freely available in a semi-
structured format on the Web. The English Wik-
tionary1 is a crowd-sourced lexical resource that in-
cludes complete inflection tables for many lexical
items in many languages. We present a supervised
?Research conducted during an internship at Google.
1http://en.wiktionary.org
system that, given only data from Wiktionary, au-
tomatically discovers and learns to apply the ortho-
graphic transformations governing a language?s in-
flectional morphology.2
Our data-driven approach is designed to extend to
any language for which we have a substantial num-
ber of example inflection tables. The design of our
model is guided by three structural assumptions:
1. The inflections of many lexical items are
governed by a few repeated morphological
paradigms.
2. A morphological paradigm can be decom-
posed into independent orthographic transfor-
mation rules (including prefix, suffix, and stem
changes), which are triggered by orthographic
context.
3. A base form is transformed in consistent, cor-
related ways to produce its inflected variants.
Learning proceeds in two stages that both utilize
the same training set of labeled inflection tables.
First, an inventory of interpretable transformation
rules is generated by aligning each base form to all
of its inflected forms. Second, a semi-Markov con-
ditional random field (CRF) (Sarawagi and Cohen,
2004) is trained to apply these rules correctly to un-
seen base forms. As we demonstrate experimentally,
the CRF is most effective when jointly predicting all
inflected forms of a lexical item together, forcing the
system to adopt a single consistent analysis of each
base form.
2See http://eecs.berkeley.edu/~gdurrett for
our datasets and code.
1185
Previous work has also described supervised and
semi-supervised approaches to predicting inflec-
tional morphology (Yarowsky and Wicentowski,
2000; Wicentowski, 2004; Dreyer and Eisner, 2011).
Our approach differs primarily in its use of auto-
matically extracted morphological rules and our dis-
criminative prediction method which jointly mod-
els entire inflection tables. These modeling choices
are directly inspired by the data setting: Wiktionary
contains complete inflection tables for many lexical
items in each of a large number of languages, so it
is natural to make full use of this information with a
joint model of all inflected forms.
We evaluate our predictions on held-out Wik-
tionary inflection tables for three languages and two
parts of speech. Our language-independent method
predicts inflections for unseen base forms with ac-
curacies ranging from 88.9% (German nouns) to
99.7% (Spanish verbs). For comparability with pre-
vious work, we also evaluate our approach on Ger-
man verb forms in the CELEX lexical database
(Baayen et al, 1995). Our approach outperforms
the semi-supervised hierarchical Bayesian model of
Dreyer and Eisner (2011), while employing scal-
able exact inference and interpretable transforma-
tion rules.
2 Background: Inflectional Morphology
Among the valid words W and parts of speech P
in a language, the base forms B ? W ? P are the
canonical forms of the language?s lexical items. A
base form relates to an inflected form via an inflec-
tional relation (b, w, a), where b ? B is a base form,
w ? W is the inflected form, and a is a vector of
morphological attributes. An inflection table T (b) is
the set of all such relations for a base form b.
Two partial inflection tables are shown in Table 1,
for the base forms (infinitives) of the German verbs
machen and schleichen, containing such inflec-
tional relations as (machen, mache, [1P,PRES,SING])
and (machen, gemacht, [PAST PART.]). Only a
small sample of the valid attribute combinations are
shown; a full inflection table for a German verb in
our Wiktionary dataset contains 27 relations.
The goal of this paper is to learn how to map b
to T (b). We generate candidate inflection tables by
applying compact, interpretable orthographic trans-
INFINITIVE machen schleichen
1P,PRES,SING mache schleiche
2P,PRES,SING machst schleichst
3P,PRES,SING macht schleicht
PAST PART. gemacht geschlichen
... ... ...
Table 1: Two partial inflection tables for the German
verbs machen (to make) and schleichen (to crawl).
formation rules that have been extracted from ex-
ample tables. As an example of our rule applica-
tion process, to inflect machen appropriately in the
forms listed in Table 1, one could apply the follow-
ing rules:
1. Replace a suffix -en with -e for first person, -st
for second person, -t for third person, and -t for
the past participle.
2. Add a prefix ge- for the past participle.
To inflect schleichen, one could apply a larger set of
three rules:
1. Replace a suffix -en with -e for first person, -st
for second person, -t for third person, and -en
for the past participle.
2. Add a prefix ge- for the past participle.
3. Delete the first e for the past participle.
The inflection tables of other German verbs can be
generated using precisely the same rules above, and
different inflection patterns may share rules, such as
the repeated rule 2. This example illustrates one of
our chief assumptions, that the inflections of many
base forms can be modeled with a small number of
such rules, applied in various combinations.
3 Learning Transformation Rules
From a training set of inflection tables
{T (b1), ..., T (bn)}, our system learns a set of
orthographic transformation rules. A rule is a func-
tion R : s, a? s? that takes as input a substring s of
a base form and an attribute vector a and outputs a
replacement substring s?. The suffix transformation
from Section 2 for machen can be described using a
1186
Algorithm 1 Learning rules from examples.
Input: n training instances T (b1), . . . , T (bn)
Rule setR ? {}
for i? 1 to n do
Changed source spans C ? {}
for all a ? A do
Ca ? PROJECTSPANS(ALIGN(bi, Ta(bi)))
C ? UNIONSPANS(C,Ca)
end for
for all c ? C do
R ? R? {EXTRACTRULE(c)}
end for
end for
return R
rule with four entries:
R(en, [1P,PRES,SING]) = e
R(en, [2P,PRES,SING]) = st
R(en, [3P,PRES,SING]) = t
R(en, [PAST PART.]) = t
Our method for learning rules from examples is
described in Algorithm 1 and depicted in Figure 1.
We extract rules from each observed inflection table
T (bi) independently, and the final set of rules is sim-
ply the union of the sets of rules learned from each
example. The procedure for a single inflection table
has three steps:
Alignment: Align each inflected form to the base
form with an iterated edit-distance algorithm.
Span Merging: Extract the set of spans of the
base form that changed to produce the inflected
form, and take their union across all attribute vec-
tors to identify maximal changed spans.
Rule Extraction: Extract a rule for each maxi-
mal changed span.
Alignment. For each setting of attributes a, we
find the lowest-cost transformation of the base form
b into the corresponding inflected form Ta(b) using
single-character insertions, deletions, and substitu-
tions. This minimum edit distance calculation is
computed via the following recurrence, where i is
an index into the base form b and j is an index into
s c h l e i c h e  n
s c h l e i c h e
s c h l     i c h
g e s c h l     i c h e n
s c h l  e  i c h e n
s c h l e i c h e n
s c h l e i c h e
s c h l  i c h
g e s c h l  i c h e n
...
...
Alignment
Span Merging
s c h l  e  i c h  e n
||||||||| D
||| ||||
D D D
|||| |||||
I I D
Rule Extraction
...
s c h l
i c h
e n
es c h l
s c h l
s c h l
e
e e ni c h
g e
i c h
i c h
Figure 1: Demonstration of the rule extraction algorithm
with the base form schleichen and three inflected forms:
schleiche (first person singular present), schlich (first per-
son singular past), and geschlichen (past participle). We
ideally want to extract appropriate transformation rules
like those described in Section 2. In the alignment step,
we minimize the edit distance between each inflected
form and the base form to identify changed spans. In
the span merging step, we project changes onto the base
form and take the union of adjacent or overlapping spans.
In the rule extraction step, we project these spans back
onto the inflected forms to identify transformation rules.
an inflected form Ta(b):
L(i, j) = min{L(i, j ? 1) + I,
L(i? 1, j) +D,
L(i? 1, j ? 1) + S(i, j)}
I , D, and S are insertion, deletion, and substi-
tion costs, respectively. Tracing the computation of
L(len(b), len(Ta(b))) yields an optimal sequence of
edit operations. The alignments output by this pro-
cedure are depicted in the first panel of Figure 1.
The most typical cost scheme sets I = 1, D = 1,
and S(i, j) = (1 ? I[match(i, j)]), i.e. 0 if the ith
character of b is the same as the jth character of
Ta(b), and 1 otherwise. However, this cost scheme
did not yield intuitive alignments for some of our
training instances. For example, in the case of the
verb denken aligning to its past participle gedacht,
1187
the initial d and g will be aligned and the follow-
ing e?s will be aligned, preventing the algorithm
from recognizing the addition of the prefix ge-. To
solve this problem, we use a dynamic edit distance
cost scheme in which I , D, and unmatched substi-
tutions all have a cost of 0. Matched substitutions
have a negative cost ?ci, where i is the index in the
base form and ci is the number of other inflected
forms for which i is aligned to a matching char-
acter. The inflected forms are iteratively realigned
with the base form until the ci converge (Eisner,
2002; Oncina and Sebban, 2006). This cost scheme
encourages a single consistent analysis of the base
form as it aligns to all of its inflected forms.
Span Merging. From each aligned pair of words,
the PROJECTSPANS procedure identifies sequences
of character edit operations with contiguous spans of
the base form. We construct a set of changed spans
Ca of b as follows: include the span (i, j) if and
only if no characters between i and j were aligned
to matching characters in Ta(b) and no smaller span
captures the same set of changes. Projected spans
for the inflected forms of schleichen are shown in
the ?Span Merging? panel of Figure 1.
The UNIONSPANS procedure combines two sets
of spans by iteratively merging any two spans that
are overlapping or adjacent. Repeating this proce-
dure to accumulate spans for each setting of a yields
the set C of maximal changed spans for a base form.
Any span inC is bordered either by word boundaries
or by characters that are match-aligned in every in-
flected form, meaning that we have isolated a region
characterized by a particular orthographic transfor-
mation.
Rule Extraction. The final step of Algorithm 1
extracts one rule for each maximal changed span of
the base form. The Rule Extraction panel of Figure 1
depicts how maximal changed spans in the base
form correspond to transformation rules. Because
UNIONSPANS guarantees that match-aligned char-
acters border each maximal changed span, there is
no ambiguity about the segmentation of transforma-
tions. The EXTRACTRULE procedure produces one
rule R(s, a) corresponding to each changed span.
Table 2 contains examples of the transformation
rules we extract from German verbs. The extracted
Attributes Suffix Stem Pre.
INFINITIVE en en en n e
1P,PRES,SING e e e e e
1P,PAST,SING te te te
2P,PRES,SING st t st st e
2P,PAST,SING test test st test
3P,PRES,SING t t t t e
3P,PAST,SING te te te
PAST PART. t t en t ge
... ... ... ... ... ... ...
Label Rsuf,1 Rsuf,2 Rsuf,3 Rsuf,4 Rst,1 Rpre,1
Table 2: Each column is an example of a morphological
transformation rule extracted by our approach. The first
four are suffix changes; these apply to, in order, regular
verbs such as machen, verbs ending in -zen or -sen such as
setzen, verbs such as schleichen and beheben, and verbs
ending in -ern or -eln such as sprenkeln. The stem change
occurs in strong verbs of the first class such as schleichen,
greifen, and streiten. Finally, we learn that ge- can be
added as a prefix to indicate the past participle.
rules are interpretable descriptions of common in-
flection patterns.
4 Applying Transformation Rules
For a novel base form b, the inventory of learned
transformation rules R = {R(s, a)} can typically
generate many candidate inflection tables T (b) for
us to choose between. A rule can potentially apply
to a base form in a number of places; we define an
anchored rule A = (R, i, j, b) to be the application
of R to a span (i, j) in b. A is only a valid anchoring
if the substring of b between i and j matches the
input of rule R.
Given a set A of non-overlapping anchored rules
for b, each entry of T (b) can be deterministically
produced by rewriting each anchored rule?s span
(i, j) using the ruleR. Therefore, the task of predict-
ing T (b) is equivalent to selecting a coherent subset
A of anchored rules from the set of all possible an-
chored rules for this base form. By coherent, we
mean that the selected rules are anchored to non-
overlapping, non-adjacent3 spans of b. Figure 2a
shows two coherent anchored rule subsets for schle-
ichen (the top one being correct). Underlining indi-
3During rule extraction, any adjacent changed spans are
merged into a single rule. Disallowing adjacent spans here
therefore prevents us from synthesizing new rules.
1188
cates length-one spans S = (i, i + 1, b) that are not
part of any anchored rule in A. We denote the set of
such spans by S(A); this set is uniquely defined for
the given base form by the selected anchored rules.
We use a log-linear model to place a conditional
distribution over valid anchored rule subsetsA given
the base form b:
pw(A|b) ? expwT
?
?
?
A?A
?(A) +
?
S?S(A)
?(S)
?
?
where w is a weight vector, ?(A) computes a fea-
ture vector for anchored rule A, and ?(S) computes
a feature vector for preserved spans S. We train
this model to maximize the regularized conditional
log-likelihood of the training data, which consists of
base forms bi and gold subsets of anchored rulesA?i
derived using Algorithm 1 on the gold inflection ta-
bles.
L(w) =
n?
i=1
log p(A?i |bi) +
?
2
?w?2.
We find w? = argmaxw L(w) using L-BFGS (Liu
and Nocedal, 1989), which requires computing ?L?w .This gradient takes the standard form of the differ-
ence between gold feature counts and expected fea-
ture counts under the model:
?L
?w =
n?
i=1
?
?
?
?
?
A?A?i
?(A) +
?
S?S(A?i )
?(S)
?
? ?
?
?
?
A?A(R,b)
Epw?(A) +
?
S?S(b)
Epw?(S)
?
?
?
?? ?w
where, by a slight abuse of notation, S(b) is the set
of all length-one spans of b.
In general, the normalizer of pw and the expec-
tation over pw cannot be computed directly, since
there may be exponentially many coherent subsets
of anchored rules. However, we note that A and
its corresponding S(A) form a segmentation of the
base form b, with features decomposing over indi-
vidual segments. Our model can therefore be viewed
a semi-Markov model over b (Sarawagi and Co-
hen, 2004); more precisely, a zeroth-order semi-
Markov model, since we do not include features on
state transitions. At training time, we can use the
s c h l e i c h e n
s c h l e i c h e n
a)
b)
s c h l e i c h e n
Rpre,1
Rst,1
Rst,1
Rpre,1
Rst,1:l[e]
Rst,1:[e]i
Rsuf,3
S:c[h]
S:[h]e
 
 
 
 
Figure 2: a) Two possible anchored rule sets for schle-
ichen. The indicated rules are prefix, stem, and suffix
rules as found in Table 2. The top anchoring is correct,
while the bottom misplaces the stem change and does not
include a suffix change. Underlined letters indicate pre-
served spans S. b) Bigram context features computed by
?(Rst,1), where the stem change is applied to the high-
lighted e, and similar features computed by ?(S) for the
underlined h, which is unchanged by the applied rules.
forward-backward algorithm for semi-Markov mod-
els to compute the gradient of pw, and at test time,
the Viterbi algorithm can exactly find the best rule
subset under the model: A? = argmaxA pw(A|b).
Features. The feature function ? captures contex-
tual information in the base form surrounding the
site of the anchored rule application. It is well under-
stood that different morphological rules may require
examining different amounts of context to apply cor-
rectly (Kohonen, 1986; Torkkola, 1993; Shalonova
and Gole?nia, 2010); to this end, we will use local
character n-gram features, which have been success-
fully applied to related problems (Jiampojamarn et
al., 2008; Dinu et al, 2012).
A sketch of our feature computation scheme is
shown in Figure 2b. Our basic feature template is
an indicator on a character n-gram with some off-
set from the rule application site, conjoined with the
identity of the rule R being applied. Our features
look at variable amounts of context: we include fea-
tures on unigrams through 4-grams, starting up to
five letters behind the anchored rule span and end-
ing up to five letters past the anchored rule span.
These features can model most hand-coded morpho-
logical rules, but are in many cases more numerous
than necessary. However, we find that regularization
is effective at balancing high model capacity with
generalization, and reducing the size of the feature
set empirically harms overall accuracy.
We also employ factored features that only look at
predictions over particular inflected forms; these are
1189
coarser features that are shared between two rules
when they predict the same orthographic change for
a particular setting of attributes. These features are
indicators onRa (the restriction ofR to attributes a),
the context n-gram, and its offset from the span.
The feature function ? is almost identical to ?,
but instead of indicating a rule appearing in some
context, it instead indicates that a particular length-
one span is being preserved in its n-gram context.
Examples of ? features are shown in Figure 2b.
Pruning. Thus far, the only requirement on an an-
choring A is that the source side of its rule R must
match the span it is anchored to in the base form
b. We further filter the set of possible A as follows:
if every occurrence of R in the training set is pre-
ceded by the same character (including a start-of-
word character) or followed by the same character
(including an end-of-word character), any anchoring
A must be preceded or followed accordingly. This
stipulation is most useful in restricting prefixing or
suffixing insertions, which have an empty source
side, to apply only at the beginnings or ends of base
forms (rather than at arbitrary points throughout). In
doing so, we prune out many erroneous anchored
rules and speed up inference substantially without
prohibiting correct rule applications.
5 Wiktionary Morphology Data
Our primary source of supervised inflection table
data is English Wiktionary. The collective editors
of English Wiktionary have created complete, con-
sistent inflection tables for many lexical items in
many languages. Previous work has successfully
parsed other information from Wiktionary, such as
parts of speech, glosses, and etymology (Zesch et
al., 2008; Li et al, 2012); however, to our knowl-
edge, inflection tables have not previously been ex-
tracted in a format easily amenable to natural lan-
guage processing applications. These inflection ta-
bles are challenging to extract because the layout of
tables varies substantially by language (beyond the
expected changes due to differing sets of relevant
morphological attributes), and some tables contain
annotations in addition to word forms.
In order to extract this data, we built a Wiktionary
scraper which generates fully structured output by
interpreting the templates that generate the rendered
Lang/POS Base forms Infl. forms per base
DE-NOUNS 2764 8
DE-VERBS 2027 27
ES-VERBS 4055 57
FI-NOUNS 40589 28
FI-VERBS 7249 53
Table 3: Number of full morphology tables extracted
from Wiktionary for each language and part of speech
pair that we considered, as well as the number of inflected
forms associated with each base form.
inflection tables. Table 3 gives statistics for the num-
ber of base forms and inflected forms extracted from
Wikitionary. When multiple forms were listed in an
inflection table for the same base form and attribute
vector, we selected the first in linear order; applying
the same principle, we also kept only the first inflec-
tion table when more than one was listed for a given
base form. Furthermore, base forms and inflected
forms separated by spaces, hyphens, or colons were
discarded. As a result, we discarded German verb-
preposition compounds such as ablehnen4 and Span-
ish reflexives such as lavarse.
6 Experiments
We evaluate our model under two experimental con-
ditions. First, we use the German verb lexicon in
the CELEX lexical database (Baayen et al, 1995)
with the same train/test splits as Dreyer and Eisner
(2011). Second, we train on our Wiktionary data de-
scribed in Section 5 and evaluate on held-out forms
from this same dataset.
In each case, we evaluate two variants of our
model in order to examine the importance of jointly
modeling the production of the entire inflection ta-
ble. Our JOINT model is exactly as defined in Sec-
tion 4. For our FACTORED model, the dictionary of
rules is extracted separately for each setting of the
attributes a; i.e., we run the entire procedure in Sec-
tion 3 with only one inflected form at a time and
forego the UNIONSPANS step. A separate predic-
tion model is trained for each a and so features are
not shared across multiple predictions as they are in
the JOINT case. Note that this FACTORED approach
4This class of verbs was also ignored by Dreyer and Eisner
(2011).
1190
No. of training examples
50 100 200
NAI?VE 87.61 87.70 87.70
FACTORED 89.61 91.40 92.64
JOINT 90.47 92.31 93.18
DE11 89.9 91.5
DE11+CORPUS 90.9 92.2
ORACLE 95.47 96.09 96.77
Table 4: Accuracies on reconstructing individual in-
flected forms in CELEX, averaged over the 5415 inflec-
tion tables in each of 10 test sets. Three training set
sizes are reported. DE11 indicates a reported result from
Dreyer and Eisner (2011), with blank results unreported
in that work. Our FACTORED model is able to do approx-
imately as well as the DE11 baseline method, and our
JOINT model performs better yet, performing compara-
bly to DE11+CORPUS, which uses additional monolin-
gual text. All models substantially outperform the NAI?VE
suffixing baseline. The relatively low ORACLE accuracy
indicates that some errors arise from failing to apply rules
that are not attested in these small training sets.
can produce inflection tables that the JOINT model
cannot, due to its ability to ?mix and match? ortho-
graphic changes in the same inflection table.
We also evaluate a NAI?VE method for applying
the joint rules which selects the most common suffix
rule available after pruning.5 Finally, we report the
ORACLE accuracy attainable with the morphologi-
cal rule dictionary of the JOINT model.
For our conditional likelihood objective, we use
? = 0.0002; this parameter and the feature set were
tuned on a small development set and held fixed for
all experiments.
6.1 CELEX Experiments
Dreyer and Eisner (2011) construct ten train/test
splits of the 5615 German verb forms in the CELEX
lexical database, keeping 200 forms for training in
each case, which they further subsample. These ran-
dom splits serve to control for instability due to the
small training set sizes. Each infinitive verb form
has 22 corresponding inflected forms capturing vari-
ation such as person, number, mood, and tense.
5For example, for German verbs ending in -en, this applies
the most regular -en suffix change, that exhibited by machen
and many other verbs.
Table 4 shows our results compared to those of
Dreyer and Eisner (2011). The FACTORED model
performs on par with the DE11 baseline model, but
the stronger performance of the JOINT model in-
dicates that making joint predictions is important.
With 100 training examples, our model is able to
equal the performance of DE11+CORPUS, which
additionally uses ten million tokens of monolingual
German text.
We emphasize that this is not the data condition
for which our model was designed. It is unfavor-
able for two reasons: first, feature-rich models can
be learned more stably on larger training sets, and
second, the train/test splits are chosen randomly, and
therefore the test sets may contain completely irreg-
ular verbs using morphological rules that we have
never observed. As can be seen from the ORA-
CLE results in Table 4, a substantial fraction of the
missed test examples cannot be produced using our
extracted rules simply because we have not seen the
relevant examples; in many cases, even a human
could not generalize correctly from the given ex-
amples without exploiting external knowledge of the
German language.
6.2 Held-Out Wiktionary Data
Our algorithm was designed with the fundamental
assumption that the training set should be a com-
prehensive description of the morphology of a given
language, which is not true for the CELEX data. In
order to evaluate on a broader set of languages under
these training conditions, we turn to our Wiktionary
data. For each language and part of speech, we train
on all but 400 inflection tables, holding back 200 ex-
amples as a development set and 200 examples as a
blind test set.6 The forms selected for the develop-
ment and test data were purposely chosen not to be
among the 200 most frequently occurring forms in
the language, since these common cases can be eas-
ily memorized from Wiktionary.
Results are shown in Table 5. As with the CELEX
results, we see that the joint prediction improves ac-
curacy over the factored model, obtaining a 9% er-
ror reduction on individual forms and a 35% error
reduction on exact match. The more pronounced
6For Finnish nouns, because there were so many inflection
tables, we trained only on the first 6000 examples. Using more
examples did not significantly change performance.
1191
Exact table match Individual form accuracy
Lang/POS NAI?VE FACT. JOINT ORACLE NAI?VE FACT. JOINT ORACLE
DE-VERBS 42.0 74.5 85.0 99.5 89.13 94.76 96.19 99.98
DE-NOUNS 12.0 74.0 79.5 98.5 49.06 88.31 88.94 99.25
ES-VERBS 81.5 93.5 95.0 99.5 97.20 99.61 99.67 99.99
FI-VERBS 33.5 82.0 87.5 99.5 75.32 97.23 96.43 99.86
FI-NOUNS 31.0 69.0 83.5 100.0 61.23 92.14 93.41 100.00
AVG 40.0 78.6 86.1 99.4 74.39 94.41 94.93 99.81
Table 5: Accuracies on reconstructing complete inflection tables and individual inflected forms for held-out base forms
in our Wiktionary dataset. Results are shown for our fully JOINT model, a FACTORED model that predicts individual
inflected forms independently, a NAI?VE baseline that picks the most common applicable suffix rule, and an ORACLE
that selects the best inflection table within our model?s capacity. For each language and part of speech, regardless of
training set size, evaluation is based on a blind test set of 200 held-out forms.
improvement on exact match is unsurprising, since
we expect that the joint predictions should get in-
flection tables correct in an ?all-or-nothing? fashion,
whereas factored predictions are more likely to re-
flect divergent feature weights of the different com-
ponent models. The NAI?VE baseline performs rather
poorly overall, indicating our algorithm is being so-
phisticated about applying more than just the most
common changes. Finally, we note that the ORA-
CLE performance is much higher in this case than
on the CELEX data, confirming our intuition that
with the appropriate level of supervision our model
at least has the capacity to make correct predictions
in almost every case.
6.3 Error Analysis
We conducted an error analysis on the output of
our JOINT model on German nouns. From 2364
paradigms, we learn 53 different orthographic trans-
formation rules, of which our 200-example develop-
ment set exhibits 14.7
On our development set, 196 inflection tables are
within the capacity of our model. Of those 196, 159
are exactly correct. In Table 6, we show the top
six rules by frequency in the development set, along
7Nineteen of our 53 extracted rules only occur on one ex-
ample; this suggests a few reasons that fewer rules are applied
than are extracted. First, very common base forms with irreg-
ular morphology may give rise to completely irregular rules.
Second, our edit distance alignment procedure can sometimes
merge two adjacent rules if the orthographic context is such that
there are multiple minimum-cost analyses. Finally, errors and
inconsistencies in Wiktionary can yield nonsense rules that are
never applied elsewhere.
NOM,SING a
NOM,PL n e a? en
ACC,SING a
ACC,PL n e a? en
DAT,SING a
DAT,PL n en a? n en
GEN,SING es a s
GEN,PL n e a? en
Example Klasse Krieg Haus Nutzer Frau
Gold 49 48 26 26 20
Prec 95.7 72.9 88.0 82.8 87.0
Rec 91.8 89.6 84.6 92.3 100.0
F1 93.8 80.4 86.3 87.3 93.0
Table 6: Breakdown of errors by morphological rule be-
ing applied by the JOINT model on the DE-NOUNS devel-
opment set. We show the rule itself, treating the nomina-
tive singular as the base form, an example of a German
word using that rule, and then the model?s accuracy at
predicting applications of that rule. Errors are spread out
over many rules, but it generally appears that common
rules are to blame for the errors that are made, due in
large part to gender confusion in this case.
with the precision, recall, and F-measure that our
model attains for each rule.8 These rules are mostly
interpretable: for example, the first two columns
correspond to common suffix rules for feminine and
masculine nouns, respectively. Our model?s per-
formance is consistently high for each of the rules
shown, including a stem change (a changing to a?
in plural forms), providing further evidence that our
model is useful for modeling rarer morphological
8Gold rules are obtained by running our rule extraction pro-
cedure over the examples in question.
1192
paradigms as well as more common ones.
As a concrete example of an error our model does
make, Lo?we (lion) is incorrectly predicted to have
the first suffix, instead of the correct suffix (not
shown) which adds an -n for accusative, genitive,
and dative singular as well. However, making this
prediction correctly is essentially beyond the capac-
ity of a model based purely on orthography. Words
ending in -e are commonly feminine, and none of
our other training examples end in -we, so guess-
ing that Lo?we follows a common feminine inflec-
tion pattern is reasonable (though Lo?we is, in fact,
masculine). Disambiguating this case requires ei-
ther features on observed genders, a more complex
model of the German language, or observing the
word in a large corpus. Generally, when the model
fails, as in this case, it is because of a fundamental
linguistic information source that it does not have
access to.
7 Related Work
Much of the past work on morphology has focused
on concatenative morphology using unsupervised
methods (Goldsmith, 2001; Creutz and Lagus, 2007;
Monson, 2008; Poon et al, 2009; Goldwater et al,
2009) or weak forms of supervision (Snyder and
Barzilay, 2008). These methods can handle aspects
of derivational morphology that we cannot, such as
compounding, but we can handle a much larger sub-
set of inflectional morphology, including more com-
plex prefix and suffix rules, stem changes, and ir-
regular forms. Some unsupervised work has specifi-
cally targeted these sorts of phenomena by, for ex-
ample, learning spelling rules for mildly noncon-
catenative cases (Dasgupta and Ng, 2007; Narad-
owsky and Goldwater, 2009) or mining lemma-base
form pairs from a corpus (Schone and Jurafsky,
2001), but it is extremely difficult to make unsu-
pervised methods perform as well as supervised ap-
proaches like ours.
Past supervised work on nonconcatenative inflec-
tional morphology has typically targeted individual
pairs of base forms and inflected forms for the pur-
poses of inflection (Clark, 2001) or lemmatization
(Yarowsky and Wicentowski, 2000; Wicentowski,
2004; Linde?n, 2008; Toutanova and Cherry, 2009).
Some of these methods may use analysis (Linde?n,
2008) or decoding (Toutanova and Cherry, 2009)
steps similar to those of our model, but none attempt
to jointly predict a complete inflection table based
on automatically extracted rules.
Some previous work has addressed the joint anal-
ysis (Zajac, 2001; Monson, 2008) or prediction
(Linde?n and Tuovila, 2009; Dinu et al, 2012) of
whole inflection tables, as we do, but rarely are
both aspects addressed simultaneously and most ap-
proaches are tuned to one particular language or
use language-specific, curated resources. In over-
all setup, our work most closely resembles that of
Dreyer and Eisner (2011), but they focus on incor-
porating large amounts of raw text data rather than
using large training sets effectively.
Broadly similar techniques are also employed in
systems to filter candidate rules and aid in human an-
notation of paradigms (Zajac, 2001; Forsberg et al,
2006; De?trez and Ranta, 2012) for resources such as
Grammatical Framework (Ranta, 2011).
8 Conclusion
In this work, we presented a method for inflecting
base forms in morphologically rich languages: we
first extract orthographic transformation rules from
observed inflection tables, then learn to apply these
rules to new base forms based on orthographic fea-
tures. Training examples for our supervised method
can be collected from Wiktionary for a large number
of languages and parts of speech. The changes we
extract are interpretable and can be associated with
particular classes of words. Moreover, our model
can successfully apply these changes to unseen base
forms with high accuracy, allowing us to rapidly
generate lexicons for new languages of interest.
Our Wiktionary datasets and an open-
source version of our code are available at
http://eecs.berkeley.edu/~gdurrett
Acknowledgments
We are grateful to Klaus Macherey and David Talbot
for assistance with the examples and helpful discus-
sions throughout the course of this work. We would
also like to thank the three anonymous reviewers for
their useful comments.
1193
References
R. H. Baayen, R. Piepenbrock, and L. Gulikers. 1995.
The CELEX Lexical Database (Release 2). Linguistic
Data Consortium, University of Pennsylvania.
Alexander Clark. 2001. Partially Supervised Learning
of Morphology with Stochastic Transducers. In Pro-
ceedings of Natural Language Processing Pacific Rim
Symposium, pages 341?348, Tokyo, Japan.
Mathias Creutz and Krista Lagus. 2007. Unsupervised
Models for Morpheme Segmentation and Morphology
Learning. ACM Transactions on Speech and Lan-
guage Processing, 4(1):3:1?3:34, Feb.
Sajib Dasgupta and Vincent Ng. 2007. High Per-
formance, Language-Independent Morphological Seg-
mentation. In Proceedings of the North American
Chapter of the Association for Computational Linguis-
tics.
Gre?goire De?trez and Aarne Ranta. 2012. Smart
Paradigms and the Predictability and Complexity of
Inflectional Morphology. In Proceedings of the Eu-
ropean Chapter of the Association for Computational
Linguistics.
Liviu P. Dinu, Vlad Niculae, and Octavia-Maria S?ulea.
2012. Learning How to Conjugate the Romanian
Verb: Rules for Regular and Partially Irregular Verbs.
In Proceedings of the European Chapter of the Asso-
ciation for Computational Linguistics.
Markus Dreyer and Jason Eisner. 2011. Discovering
Morphological Paradigms from Plain Text Using a
Dirichlet Process Mixture Model. In Proceedings of
Empirical Methods in Natural Language Processing,
pages 616?627, Edinburgh, Scotland, UK.
Jason Eisner. 2002. Parameter Estimation for Probabilis-
tic Finite-State Transducers. In Proceedings of the As-
sociation for Computational Linguistics.
Markus Forsberg, Harald Hammarstro?m, and Aarne
Ranta. 2006. Morphological Lexicon Extraction from
Raw Text Data. In Proceedings of Advances in Natu-
ral Language Processing.
John Goldsmith. 2001. Unsupervised Learning of the
Morphology of a Natural Language. Computational
Linguistics, 27(2):153?198, June.
Sharon Goldwater, Thomas L. Griffiths, and Mark John-
son. 2009. A Bayesian Framework for Word Segmen-
tation: Exploring the Effects of Context. Cognition,
112(1):21?54.
Sittichai Jiampojamarn, Colin Cherry, and Grzegorz
Kondrak. 2008. Joint Processing and Discrimina-
tive Training for Letter-to-Phoneme Conversion. In
Proceedings of the Association for Computational Lin-
guistics.
Teuvo Kohonen. 1986. Dynamically Expanding Con-
text, With Application to the Correction of Symbol
Strings in the Recognition of Continuous Speech. In
Proceedings of the International Conference on Pat-
tern Recognition.
Shen Li, Joa?o V. Grac?a, and Ben Taskar. 2012. Wiki-ly
Supervised Part-of-speech Tagging. In Proceedings of
Empirical Methods in Natural Language Processing.
Krister Linde?n and Jussi Tuovila. 2009. Corpus-based
Paradigm Selection for Morphological Entries. In
Proceedings of the Nordic Conference of Computa-
tional Linguistics.
Krister Linde?n. 2008. A Probabilistic Model for Guess-
ing Base Forms of New Words by Analogy. In Pro-
ceedings of Computational Linguistics and Intelligent
Text Processing.
Dong C. Liu and Jorge Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical Programming, 45(3):503?528, Decem-
ber.
Christian Monson. 2008. ParaMor: From Paradigm
Structure to Natural Language Morphology Induction.
Ph.D. thesis, Carnegie Mellon University.
Jason Naradowsky and Sharon Goldwater. 2009. Im-
proving Morphology Induction by Learning Spelling
Rules. In Proceedings of the International Joint Con-
ferences on Artificial Intelligence.
Jose Oncina and Marc Sebban. 2006. Learning Stochas-
tic Edit Distance: Application in Handwritten Char-
acter Recognition. Pattern Recognition, 39(9):1575?
1587, September.
Hoifung Poon, Colin Cherry, and Kristina Toutanova.
2009. Unsupervised Morphological Segmentation
with Log-Linear Models. In Proceedings of the North
American Chapter of the Association for Computa-
tional Linguistics.
Aarne Ranta. 2011. Grammatical Framework: Pro-
gramming with Multilingual Grammars. CSLI Pub-
lications, Stanford.
Sunita Sarawagi and William W. Cohen. 2004. Semi-
Markov Conditional Random Fields for Information
Extraction. In Advances in Neural Information Pro-
cessing Systems 17.
Patrick Schone and Daniel Jurafsky. 2001. Knowledge-
Free Induction of Inflectional Morphologies. In Pro-
ceedings of the North American Chapter of the Asso-
ciation for Computational Linguistics.
Ksenia Shalonova and Bruno Gole?nia. 2010. Weakly Su-
pervised Morphology Learning for Agglutinating Lan-
guages Using Small Training Sets. In Proceedings of
the Conference on Computational Linguistics.
Benjamin Snyder and Regina Barzilay. 2008. Unsuper-
vised Multilingual Learning for Morphological Seg-
mentation. In Proceedings of the Association for Com-
putational Linguistics.
1194
Kari Torkkola. 1993. An Efficient Way to Learn English
Grapheme-to-Phoneme Rules Automatically. In Pro-
ceedings of the International Conference on Acoustics,
Speech, and Signal Processing: Speech Processing -
Volume II.
Kristina Toutanova and Colin Cherry. 2009. A Global
Model for Joint Lemmatization and Part-of-Speech
Prediction. In Proceedings of the Association for
Computational Linguistics.
Richard Wicentowski. 2004. Multilingual Noise-Robust
Supervised Morphological Analysis Using the Word-
Frame Model. In Proceedings of the ACL Special In-
terest Group in Computational Phonology.
David Yarowsky and Richard Wicentowski. 2000. Min-
imally Supervised Morphological Analysis by Multi-
modal Alignment. In Proceedings of the Association
for Computational Linguistics.
Re?mi Zajac. 2001. Morpholog: Constrained and Super-
vised Learning of Morphology. In Proceedings of the
Conference on Natural Language Learning.
Torsten Zesch, Christof Mu?ller, and Iryna Gurevych.
2008. Extracting Lexical Semantic Knowledge from
Wikipedia and Wiktionary. In Proceedings of Lan-
guage Resources and Evaluation.
1195
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 24?29,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
An Empirical Investigation of Discounting
in Cross-Domain Language Models
Greg Durrett and Dan Klein
Computer Science Division
University of California, Berkeley
{gdurrett,klein}@cs.berkeley.edu
Abstract
We investigate the empirical behavior of n-
gram discounts within and across domains.
When a language model is trained and evalu-
ated on two corpora from exactly the same do-
main, discounts are roughly constant, match-
ing the assumptions of modified Kneser-Ney
LMs. However, when training and test corpora
diverge, the empirical discount grows essen-
tially as a linear function of the n-gram count.
We adapt a Kneser-Ney language model to
incorporate such growing discounts, result-
ing in perplexity improvements over modified
Kneser-Ney and Jelinek-Mercer baselines.
1 Introduction
Discounting, or subtracting from the count of each
n-gram, is one of the core aspects of Kneser-Ney
language modeling (Kneser and Ney, 1995). For all
but the smallest n-gram counts, Kneser-Ney uses a
single discount, one that does not grow with the n-
gram count, because such constant-discounting was
seen in early experiments on held-out data (Church
and Gale, 1991). However, due to increasing com-
putational power and corpus sizes, language model-
ing today presents a different set of challenges than
it did 20 years ago. In particular, modeling cross-
domain effects has become increasingly more im-
portant (Klakow, 2000; Moore and Lewis, 2010),
and deployed systems must frequently process data
that is out-of-domain from the standpoint of the lan-
guage model.
In this work, we perform experiments on held-
out data to evaluate how discounting behaves in the
cross-domain setting. We find that, when training
and testing on corpora that are as similar as possi-
ble, empirical discounts indeed do not grow with n-
gram count, which validates the parametric assump-
tion of Kneser-Ney smoothing. However, when the
train and evaluation corpora differ, even slightly, dis-
counts generally exhibit linear growth in the count of
the n-gram, with the amount of growth being closely
correlated with the corpus divergence. Finally, we
build a language model exploiting a parametric form
of the growing discount and show perplexity gains of
up to 5.4% over modified Kneser-Ney.
2 Discount Analysis
Underlying discounting is the idea that n-grams will
occur fewer times in test data than they do in training
data. We investigate this quantitatively by conduct-
ing experiments similar in spirit to those of Church
and Gale (1991). Suppose that we have collected
counts on two corpora of the same size, which we
will call our train and test corpora. For an n-gram
w = (w1, ..., wn), let ktrain(w) denote the number of
occurrences of w in the training corpus, and ktest(w)
denote the number of occurrences of w in the test
corpus. We define the empirical discount of w to be
d(w) = ktrain(w) ? ktest(w); this will be negative
when the n-gram occurs more in the test data than
in the training data. Let Wi = {w : ktrain(w) = i}
be the set of n-grams with count i in the training
corpus. We define the average empirical discount
function as
d?(i) =
1
|Wi|
?
w?Wi
d(w)
24
Kneser-Ney implicitly makes two assumptions:
first, that discounts do not depend on n-gram count,
i.e. that d?(i) is constant in i. Modified Kneser-Ney
relaxes this assumption slightly by having indepen-
dent parameters for 1-count, 2-count, and many-
count n-grams, but still assumes that d?(i) is constant
for i greater than two. Second, by using the same
discount for all n-grams with a given count, Kneser-
Ney assumes that the distribution of d(w) for w in a
particular Wi is well-approximated by its mean. In
this section, we analyze whether or not the behavior
of the average empirical discount function supports
these two assumptions. We perform experiments on
various subsets of the documents in the English Gi-
gaword corpus, chiefly drawn from New York Times
(NYT) and Agence France Presse (AFP).1
2.1 Are Discounts Constant?
Similar corpora To begin, we consider the NYT
documents from Gigaword for the year 1995. In
order to create two corpora that are maximally
domain-similar, we randomly assign half of these
documents to train and half of them to test, yielding
train and test corpora of approximately 50M words
each, which we denote by NYT95 and NYT95?. Fig-
ure 1 shows the average empirical discounts d?(i)
for trigrams on this pair of corpora. In this setting,
we recover the results of Church and Gale (1991)
in that discounts are approximately constant for n-
gram counts of two or greater.
Divergent corpora In addition to these two cor-
pora, which were produced from a single contigu-
ous batch of documents, we consider testing on cor-
pus pairs with varying degrees of domain difference.
We construct additional corpora NYT96, NYT06,
AFP95, AFP96, and AFP06, by taking 50M words
from documents in the indicated years of NYT
and AFP data. We then collect training counts on
NYT95 and alternately take each of our five new cor-
pora as the test data. Figure 1 also shows the average
empirical discount curves for these train/test pairs.
Even within NYT newswire data, we see growing
discounts when the train and test corpora are drawn
1Gigaword is drawn from six newswire sources and contains
both miscellaneous text and complete, contiguous documents,
sorted chronologically. Our experiments deal exclusively with
the document text, which constitutes the majority of Gigaword
and is of higher quality than the miscellaneous text.
 0 1 2 3 4 5 6
 0
 5
 1
0
 1
5
 2
0
Average empirical discount
Tr
ig
ra
m
 c
ou
nt
 in
 tr
ai
n
A
FP
06
A
FP
96
A
FP
95
N
Y
T0
6
N
Y
T9
6
N
Y
T9
5?
Figure 1: Average empirical trigram discounts d?(i) for
six configurations, training on NYT95 and testing on the
indicated corpora. For each n-gram count k, we compute
the average number of occurrences in test for all n-grams
occurring k times in training data, then report k minus
this quantity as the discount. Bigrams and bigram types
exhibit similar discount relationships.
from different years, and between the NYT and AFP
newswire, discounts grow even more quickly. We
observed these trends continuing steadily up into n-
gram counts in the hundreds, beyond which point it
becomes difficult to robustly estimate discounts due
to fewer n-gram types in this count range.
This result is surprising in light of the constant
discounts observed for the NYT95/NYT95? pair.
Goodman (2001) proposes that discounts arise from
document-level ?burstiness? in a corpus, because
language often repeats itself locally within a doc-
ument, and Moore and Quirk (2009) suggest that
discounting also corrects for quantization error due
to estimating a continuous distribution using a dis-
crete maximum likelihood estimator (MLE). Both
of these factors are at play in the NYT95/NYT95?
experiment, and yet only a small, constant discount
is observed. Our growing discounts must therefore
be caused by other, larger-scale phenomena, such as
shifts in the subjects of news articles over time or in
the style of the writing between newswire sources.
The increasing rate of discount growth as the source
changes and temporal divergence increases lends
credence to this hypothesis.
2.2 Nonuniformity of Discounts
Figure 1 considers discounting in terms of averaged
discounts for each count, which tests one assump-
tion of modified Kneser-Ney, that discounts are a
25
 0
 0
.1
 0
.2
 0
.3
 0
.4
0
5
10
15
20
Fraction of train-count-10 trigrams
Tr
ig
ra
m
 c
ou
nt
 in
 te
stN
Y
T9
5/
N
Y
T9
5?
N
Y
T9
5/
A
FP
95
Figure 2: Empirical probability mass functions of occur-
rences in the test data for trigrams that appeared 10 times
in training data. Discounting by a single value is plau-
sible in the case of similar train and test corpora, where
the mean of the distribution (8.50) is close to the median
(8.0), but not in the case of divergent corpora, where the
mean (6.04) and median (1.0) are very different.
constant function of n-gram counts. In Figure 2, we
investigate the second assumption, namely that the
distribution over discounts for a given n-gram count
is well-approximated by its mean. For similar cor-
pora, this seems to be true, with a histogram of test
counts for trigrams of count 10 that is nearly sym-
metric. For divergent corpora, the data exhibit high
skew: almost 40% of the trigrams simply never ap-
pear in the test data, and the distribution has very
high standard deviation (17.0) due to a heavy tail
(not shown). Using a discount that depends only on
the n-gram count is less appropriate in this case.
In combination with the growing discounts of sec-
tion 2.1, these results point to the fact that modified
Kneser-Ney does not faithfully model the discount-
ing in even a mildly cross-domain setting.
2.3 Correlation of Divergence and Discounts
Intuitively, corpora that are more temporally distant
within a particular newswire source should perhaps
be slightly more distinct, and still a higher degree of
divergence should exist between corpora from dif-
ferent newswire sources. From Figure 1, we see that
this notion agrees with the relative sizes of the ob-
served discounts. We now ask whether growth in
discounts is correlated with train/test dissimilarity in
a more quantitative way. For a given pair of cor-
pora, we canonicalize the degree of discounting by
selecting the point d?(30), the average empirical dis-
 0 5 1
0
 1
5
-5
00
-4
00
-3
00
Discount for count-30 trigrams
Lo
g 
lik
el
ih
oo
d 
di
ffe
re
nc
e 
(in
 m
ill
io
ns
)
Figure 3: Log likelihood difference versus average empir-
ical discount of trigrams with training count 30 (d?(30))
for the train/test pairs. More negative values of the log
likelihood indicate more dissimilar corpora, as the trained
model is doing less well relative to the jackknife model.
count for n-grams occurring 30 times in training.2
To measure divergence between the corpus pair, we
compute the difference between the log likelihood
of the test corpus under the train corpus language
model (using basic Kneser-Ney) and the likelihood
of the test corpus under a jackknife language model
from the test itself, which holds out and scores each
test n-gram in turn. This dissimilarity metric resem-
bles the cross-entropy difference used by Moore and
Lewis (2010) to subsample for domain adaptation.
We compute this canonicalization for each of
twenty pairs of corpora, with each corpus contain-
ing 240M trigram tokens between train and test. The
corpus pairs were chosen to span varying numbers
of newswire sources and lengths of time in order to
capture a wide range of corpus divergences. Our re-
sults are plotted in Figure 3. The log likelihood dif-
ference and d?(30) are negatively correlated with a
correlation coefficient value of r = ?0.88, which
strongly supports our hypothesis that higher diver-
gence yields higher discounting. One explanation
for the remaining variance is that the trigram dis-
count curve depends on the difference between the
number of bigram types in the train and test corpora,
which can be as large as 10%: observing more bi-
gram contexts in training fragments the token counts
2One could also imagine instead canonicalizing the curves
by using either the exponent or slope parameters from a fitted
power law as in section 3. However, there was sufficient non-
linearity in the average empirical discount curves that neither of
these parameters was an accurate proxy for d?(i).
26
and leads to smaller observed discounts.
2.4 Related Work
The results of section 2.1 point to a remarkably per-
vasive phenomenon of growing empirical discounts,
except in the case of extremely similar corpora.
Growing discounts of this sort were previously sug-
gested by the model of Teh (2006). However, we
claim that the discounting phenomenon in our data is
fundamentally different from his model?s prediction.
In the held-out experiments of section 2.1, growing
discounts only emerge when one evaluates against a
dissimilar held-out corpus, whereas his model would
predict discount growth even in NYT95/NYT95?,
where we do not observe it.
Adaptation across corpora has also been ad-
dressed before. Bellegarda (2004) describes a range
of techniques, from interpolation at either the count
level or the model level (Bacchiani and Roark, 2003;
Bacchiani et al, 2006) to using explicit models of
syntax or semantics. Hsu and Glass (2008) employ
a log-linear model for multiplicatively discounting
n-grams in Kneser-Ney; when they include the log-
count of an n-gram as the only feature, they achieve
75% of their overall word error rate reduction, sug-
gesting that predicting discounts based on n-gram
count can substantially improve the model. Their
work also improves on the second assumption of
Kneser-Ney, that of the inadequacy of the average
empirical discount as a discount constant, by em-
ploying various other features in order to provide
other criteria on which to discount n-grams.
Taking a different approach, both Klakow (2000)
and Moore and Lewis (2010) use subsampling to
select the domain-relevant portion of a large, gen-
eral corpus given a small in-domain corpus. This
can be interpreted as a form of hard discounting,
and implicitly models both growing discounts, since
frequent n-grams will appear in more of the re-
jected sentences, and nonuniform discounting over
n-grams of each count, since the sentences are cho-
sen according to a likelihood criterion. Although
we do not consider this second point in constructing
our language model, an advantage of our approach
over subsampling is that we use our entire training
corpus, and in so doing compromise between min-
imizing errors from data sparsity and accommodat-
ing domain shifts to the extent possible.
3 A Growing Discount Language Model
We now implement and evaluate a language model
that incorporates growing discounts.
3.1 Methods
Instead of using a fixed discount for most n-gram
counts, as prescribed by modified Kneser-Ney, we
discount by an increasing parametric function of the
n-gram count. We use a tune set to compute an av-
erage empirical discount curve d?(i), and fit a func-
tion of the form f(x) = a + bxc to this curve using
weighted least-L1-loss regression, with the weight
for each point proportional to i|Wi|, the total to-
ken counts of n-grams occurring that many times
in training. To improve the fit of the model, we
use dedicated parameters for count-1 and count-2 n-
grams as in modified Kneser-Ney, yielding a model
with five parameters per n-gram order. We call this
model GDLM. We also instantiate this model with
c fixed to one, so that the model is strictly linear
(GDLM-LIN).
As baselines for comparison, we use basic inter-
polated Kneser-Ney (KNLM), with one discount pa-
rameter per n-gram order, and modified interpolated
Kneser-Ney (MKNLM), with three parameters per
n-gram order, as described in (Chen and Goodman,
1998). We also compare against Jelinek-Mercer
smoothing (JMLM), which interpolates the undis-
counted MLEs from every order. According to Chen
and Goodman (1998), it is common to use different
interpolation weights depending on the history count
of an n-gram, since MLEs based on many samples
are presumed to be more accurate than those with
few samples. We used five history count buckets so
that JMLM would have the same number of param-
eters as GDLM.
All five models are trigram models with type
counts at the lower orders and independent discount
or interpolation parameters for each order. Param-
eters for GDLM, MKNLM, and KNLM are initial-
ized based on estimates from d?(i): the regression
thereof for GDLM, and raw discounts for MKNLM
and KNLM. The parameters of JMLM are initialized
to constants independent of the data. These initial-
izations are all heuristic and not guaranteed to be
optimal, so we then iterate through the parameters
of each model several times and perform line search
27
Train NYT00+01 Train AFP02+05+06
Voc. 157K 50K 157K 50K
GDLM(*) 151 131 258 209
GDLM-LIN(*) 151 132 259 210
JMLM 165 143 274 221
MKNLM 152 132 273 221
KNLM 159 138 300 241
Table 1: Perplexities of the growing discounts language
model (GDLM) and its purely linear variant (GDLM-
LIN), which are contributions of this work, versus
the modified Kneser-Ney (MKNLM), basic Kneser-Ney
(KNLM), and Jelinek-Mercer (JMLM) baselines. We
report results for in-domain (NYT00+01) and out-of-
domain (AFP02+05+06) training corpora, for two meth-
ods of closing the vocabulary.
in each to optimize tune-set perplexity.
For evaluation, we train, tune, and test on three
disjoint corpora. We consider two different train-
ing sets: one of 110M words of NYT from 2000
and 2001 (NYT00+01), and one of 110M words of
AFP from 2002, 2005, and 2006 (AFP02+05+06).
In both cases, we compute d?(i) and tune parameters
on 110M words of NYT from 2002 and 2003, and
do our final perplexity evaluation on 4M words of
NYT from 2004. This gives us both in-domain and
out-of-domain results for our new language model.
Our tune set is chosen to be large so that we can
initialize parameters based on the average empirical
discount curve; in practice, one could compute em-
pirical discounts based on a smaller tune set with the
counts scaled up proportionately, or simply initialize
to constant values.
We use two different methods to handle out-of-
vocabulary (OOV) words: one scheme replaces any
unigram token occurring fewer than five times in
training with an UNK token, yielding a vocabulary
of approximately 157K words, and the other scheme
only keeps the top 50K words in the vocabulary.
The count truncation method has OOV rates of 0.9%
and 1.9% in the NYT/NYT and NYT/AFP settings,
respectively, and the constant-size vocabulary has
OOV rates of 2% and 3.6%.
3.2 Results
Perplexity results are given in Table 1. As expected,
for in-domain data, GDLM performs comparably to
MKNLM, since the discounts do not grow and so
there is little to be gained by choosing a param-
eterization that permits this. Out-of-domain, our
model outperforms MKNLM and JMLM by approx-
imately 5% for both vocabulary sizes. The out-
of-domain perplexity values are competitive with
those of Rosenfeld (1996), who trained on New York
Times data and tested on AP News data under simi-
lar conditions, and even more aggressive closing of
the vocabulary. Moore and Lewis (2010) achieve
lower perplexities, but they use in-domain training
data that we do not include in our setting.
We briefly highlight some interesting features of
these results. In the small vocabulary cross-domain
setting, for GDLM-LIN, we find
dtri(i) = 1.31 + 0.27i, dbi(i) = 1.34 + 0.05i
as the trigram and bigram discount functions that
minimize tune set perplexity. For GDLM,
dtri(i) = 1.19 + 0.32i
0.45, dbi(i) = 0.86 + 0.56i
0.86
In both cases, a growing discount is indeed learned
from the tuning procedure, demonstrating the im-
portance of this in our model. Modeling nonlin-
ear discount growth in GDLM yields only a small
marginal improvement over the linear discounting
model GDLM-LIN, so we prefer GDLM-LIN for its
simplicity.
A somewhat surprising result is the strong per-
formance of JMLM relative to MKNLM on the di-
vergent corpus pair. We conjecture that this is be-
cause the bucketed parameterization of JMLM gives
it the freedom to change interpolation weights with
n-gram count, whereas MKNLM has essentially a
fixed discount. This suggests that modified Kneser-
Ney as it is usually parameterized may be a particu-
larly poor choice in cross-domain settings.
Overall, these results show that the growing dis-
count phenomenon detailed in section 2, beyond
simply being present in out-of-domain held-out data,
provides the basis for a new discounting scheme that
allows us to improve perplexity relative to modified
Kneser-Ney and Jelinek-Mercer baselines.
Acknowledgments
The authors gratefully acknowledge partial support
from the GALE program via BBN under DARPA
contract HR0011-06-C-0022, and from an NSF fel-
lowship for the first author. Thanks to the anony-
mous reviewers for their insightful comments.
28
References
Michiel Bacchiani and Brian Roark. 2003. Unsupervised
Langauge Model Adaptation. In Proceedings of Inter-
national Conference on Acoustics, Speech, and Signal
Processing.
Michiel Bacchiani, Michael Riley, Brian Roark, and
Richard Sproat. 2006. MAP adaptation of stochastic
grammars. Computer Speech & Language, 20(1):41 ?
68.
Jerome R. Bellegarda. 2004. Statistical language model
adaptation: review and perspectives. Speech Commu-
nication, 42:93?108.
Stanley Chen and Joshua Goodman. 1998. An Empirical
Study of Smoothing Techniques for Language Model-
ing. Technical report, Harvard University, August.
Kenneth Church and William Gale. 1991. A Compari-
son of the Enhanced Good-Turing and Deleted Estima-
tion Methods for Estimating Probabilities of English
Bigrams. Computer Speech & Language, 5(1):19?54.
Joshua Goodman. 2001. A Bit of Progress in Language
Modeling. Computer Speech & Language, 15(4):403?
434.
Bo-June (Paul) Hsu and James Glass. 2008. N-
gram Weighting: Reducing Training Data Mismatch in
Cross-Domain Language Model Estimation. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, pages 829?838.
Dietrich Klakow. 2000. Selecting articles from the lan-
guage model training corpus. In Proceedings of the
IEEE International Conference on Acoustics, Speech,
and Signal Processing, volume 3, pages 1695?1698.
Reinhard Kneser and Hermann Ney. 1995. Improved
Backing-off for M-Gram Language Modeling. In Pro-
ceedings of International Conference on Acoustics,
Speech, and Signal Processing.
Robert C. Moore and William Lewis. 2010. Intelligent
selection of language model training data. In Proceed-
ings of the ACL 2010 Conference Short Papers, pages
220?224, July.
Robert C. Moore and Chris Quirk. 2009. Improved
Smoothing for N-gram Language Models Based on
Ordinary Counts. In Proceedings of the ACL-IJCNLP
2009 Conference Short Papers, pages 349?352.
Ronald Rosenfeld. 1996. A Maximum Entropy Ap-
proach to Adaptive Statistical Language Modeling.
Computer, Speech & Language, 10:187?228.
Yee Whye Teh. 2006. A Hierarchical Bayesian Lan-
guage Model Based On Pitman-Yor Processes. In Pro-
ceedings of ACL, pages 985?992, Sydney, Australia,
July. Association for Computational Linguistics.
29
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 114?124,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Decentralized Entity-Level Modeling for Coreference Resolution
Greg Durrett, David Hall, and Dan Klein
Computer Science Division
University of California, Berkeley
{gdurrett,dlwh,klein}@cs.berkeley.edu
Abstract
Efficiently incorporating entity-level in-
formation is a challenge for coreference
resolution systems due to the difficulty of
exact inference over partitions. We de-
scribe an end-to-end discriminative prob-
abilistic model for coreference that, along
with standard pairwise features, enforces
structural agreement constraints between
specified properties of coreferent men-
tions. This model can be represented as
a factor graph for each document that ad-
mits efficient inference via belief propaga-
tion. We show that our method can use
entity-level information to outperform a
basic pairwise system.
1 Introduction
The inclusion of entity-level features has been a
driving force behind the development of many
coreference resolution systems (Luo et al, 2004;
Rahman and Ng, 2009; Haghighi and Klein, 2010;
Lee et al, 2011). There is no polynomial-time dy-
namic program for inference in a model with ar-
bitrary entity-level features, so systems that use
such features typically rely on making decisions
in a pipelined manner and sticking with them, op-
erating greedily in a left-to-right fashion (Rahman
and Ng, 2009) or in a multi-pass, sieve-like man-
ner (Raghunathan et al, 2010). However, such
systems may be locked into bad coreference deci-
sions and are difficult to directly optimize for stan-
dard evaluation metrics.
In this work, we present a new structured model
of entity-level information designed to allow effi-
cient inference. We use a log-linear model that can
be expressed as a factor graph. Pairwise features
appear in the model as unary factors, adjacent
to nodes representing a choice of antecedent (or
none) for each mention. Additional nodes model
entity-level properties on a per-mention basis, and
structural agreement factors softly drive properties
of coreferent mentions to agree with one another.
This is a key feature of our model: mentions man-
age their partial membership in various corefer-
ence chains, so that information about entity-level
properties is decentralized and propagated across
individual mentions, and we never need to explic-
itly instantiate entities.
Exact inference in this factor graph is in-
tractable, but efficient approximate inference can
be carried out with belief propagation. Our model
is the first discriminatively-trained model that both
makes joint decisions over an entire document and
models specific entity-level properties, rather than
simply enforcing transitivity of pairwise decisions
(Finkel and Manning, 2008; Song et al, 2012).
We evaluate our system on the dataset from
the CoNLL 2011 shared task using three differ-
ent types of properties: synthetic oracle proper-
ties, entity phi features (number, gender, animacy,
and NER type), and properties derived from un-
supervised clusters targeting semantic type infor-
mation. In all cases, our transitive model of en-
tity properties equals or outperforms our pairwise
system and our reimplementation of a previous
entity-level system (Rahman and Ng, 2009). Our
final system is competitive with the winner of the
CoNLL 2011 shared task (Lee et al, 2011).
2 Example
We begin with an example motivating our use of
entity-level features. Consider the following ex-
cerpt concerning two famous auction houses:
When looking for [art items], [people] go
to [Sotheby?s and Christie?s] because [they]A
believe [they]B can get the best price for
[them].
The first three mentions are all distinct entities,
theyA and theyB refer to people, and them refers to
art items. The three pronouns are tricky to resolve
114
automatically because they could at first glance re-
solve to any of the preceding mentions. We focus
in particular on the resolution of theyA and them.
In order to correctly resolve theyA to people rather
than Sotheby?s and Christie?s, we must take ad-
vantage of the fact that theyA appears as the sub-
ject of the verb believe, which is much more likely
to be attributed to people than to auction houses.
Binding principles prevent them from attaching
to theyB. But how do we prevent it from choos-
ing as its antecedent the next closest agreeing pro-
noun, theyA? One way is to exploit the correct
coreference decision we have already made, theyA
referring to people, since people are not as likely
to have a price as art items are. This observa-
tion argues for enforcing agreement of entity-level
semantic properties during inference, specifically
properties relating to permitted semantic roles.
Because even these six mentions have hundreds
of potential partitions into coreference chains, we
cannot search over partitions exhaustively, and
therefore we must design our model to be able to
use this information while still admitting an effi-
cient inference scheme.
3 Models
We will first present our BASIC model (Sec-
tion 3.1) and describe the features it incorporates
(Section 3.2), then explain how to extend it to use
transitive features (Sections 3.3 and 3.4).
Throughout this section, let x be a variable con-
taining the words in a document along with any
relevant precomputed annotation (such as parse in-
formation, semantic roles, etc.), and let n denote
the number of mentions in a given document.
3.1 BASIC Model
Our BASIC model is depicted in Figure 1 in stan-
dard factor graph notation. Each mention i has
an associated random variable ai taking values in
the set {1, . . . , i?1, <new>}; this variable spec-
ifies mention i?s selected antecedent or indicates
that it begins a new coreference chain. Let a =
(a1, ..., an) be the vector of the ai. Note that a set
of coreference chains C (the final desired output)
can be uniquely determined from a, but a is not
uniquely determined by C.
We use a log linear model of the conditional dis-
tribution P (a|x) as follows:
P (a|x) ? exp
( n?
i=1
wT fA(i, ai, x)
)
When looking for [art items], [people] go to [Sotheby's 
and Christie's] because [they]
A
 believe [they]
B
 can get 
the best price for [them].
art items 0.15
people 0.4
Sotheby?s and 
Christie?s
0.4
<new> 0.05
a
2
a
3
a
4
a
1
A
1
A
2
A
3
A
4
art items 0.05
<new> 0.95
antecedent 
choices
antecedent 
factors
}
}
Figure 1: Our BASIC coreference model. A de-
cision ai is made independently for each men-
tion about what its antecedent mention should
be or whether it should start a new coreference
chain. Each unary factor Ai has a log-linear form
with features examining mention i, its selected an-
tecedent ai, and the document context x.
where fA(i, ai, x) is a feature function that exam-
ines the coreference decision ai for mention i with
document context x; note that this feature function
can include pairwise features based on mention i
and the chosen antecedent ai, since information
about each mention is contained in x.
Because the model factors completely over the
individual ai, these feature functions fA can be ex-
pressed as unary factors Ai (see Figure 1), with
Ai(j) ? exp
(
wT fA(i, j, x)
). Given a setting of
w, we can determine a? = argmaxa P (a|x) and
then deterministically compute C(a), the final set
of coreference chains.
While the features of this model factor over
coreference links, this approach differs from clas-
sical pairwise systems such as Bengtson and Roth
(2008) or Stoyanov et al (2010). Because poten-
tial antecedents compete with each other and with
the non-anaphoric hypothesis, the choice of ai ac-
tually represents a joint decision about i?1 pair-
wise links, as opposed to systems that use a pair-
wise binary classifier and a separate agglomera-
tion step, which consider one link at a time during
learning. This approach is similar to the mention-
ranking model of Rahman and Ng (2009).
3.2 Pairwise Features
We now present the set of features fA used by our
unary factors Ai. Each feature examines the an-
115
tecedent choice ai of the current mention as well
as the observed information x in the document.
For each of the features we present, two conjoined
versions are included: one with an indicator of the
type of the current mention being resolved, and
one with an indicator of the types of the current
and antecedent mentions. Mention types are either
NOMINAL, PROPER, or, if the mention is pronom-
inal, a canonicalized version of the pronoun ab-
stracting away case.1
Several features, especially those based on the
precise constructs (apposition, etc.) and those in-
corporating phi feature information, are computed
using the machinery in Lee et al (2011). Other
features were inspired by Song et al (2012) and
Rahman and Ng (2009).
Anaphoricity features: Indicator of anaphoric-
ity, indicator on definiteness.
Configurational features: Indicator on distance
in mentions (capped at 10), indicator on dis-
tance in sentences (capped at 10), does the an-
tecedent c-command the current mention, are the
two mentions in a subject/object construction, are
the mentions nested, are the mentions in determin-
istic appositive/role appositive/predicate nomina-
tive/relative pronoun constructions.
Match features: Is one mention an acronym of
the other, head match, head contained (each way),
string match, string contained (each way), relaxed
head match features from Lee et al (2011).
Agreement features: Gender, number, ani-
macy, and NER type of the current mention and
the antecedent (separately and conjoined).
Discourse features: Speaker match conjoined
with an indicator of whether the document is an
article or conversation.
Because we use conjunctions of these base fea-
tures together with the antecedent and mention
type, our system can capture many relationships
that previous systems hand-coded, especially re-
garding pronouns. For example, our system has
access to features such as ?it is non-anaphoric?,
?it has as its antecedent a geopolitical entity?, or
?I has as its antecedent I with the same speaker.?
1While this canonicalization could theoretically impair
our ability to resolve, for example, reflexive pronouns, con-
joining features with raw pronoun strings does not improve
performance.
We experimented with synonymy and hyper-
nymy features from WordNet (Miller, 1995), but
these did not empirically improve performance.
3.3 TRANSITIVE Model
The BASIC model can capture many relationships
between pairs of mentions, but cannot necessarily
capture entity-level properties like those discussed
in Section 2. We could of course model entities
directly (Luo et al, 2004; Rahman and Ng, 2009),
saying that each mention refers to some prior en-
tity rather than to some prior mention. However,
inference in this model would require reasoning
about all possible partitions of mentions, which is
computationally infeasible without resorting to se-
vere approximations like a left-to-right inference
method (Rahman and Ng, 2009).
Instead, we would like to try to preserve the
tractability of the BASIC model while still being
able to exploit entity-level information. To do so,
we will allow each mention to maintain its own
distributions over values for a number of proper-
ties; these properties could include gender, named-
entity type, or semantic class. Then, we will re-
quire each anaphoric mention to agree with its an-
tecedent on the value of each of these properties.
Our TRANSITIVE model which implements this
scheme is shown in Figure 2. Each mention i
has been augmented with a single property node
pi ? {1, ..., k}. The unary Pi factors encode prior
knowledge about the setting of each pi; these fac-
tors may be hard (I will not refer to a plural entity),
soft (such as a distribution over named entity types
output by an NER tagger), or practically uniform
(e.g. the last name Smith does not specify a partic-
ular gender).
To enforce agreement of a particular property,
we require a mention to have the same property
value as its antecedent. That is, for mentions i and
j, if ai = j, we want to ensure that pi and pj
agree. We can achieve this with the following set
of structural equality factors:
Ei?j(ai, pi, pj) = 1? I[ai = j ? pi 6= pj ]
In words, this factor is zero if both ai = j and
pi disagrees with pj . These equality factors es-
sentially provide a mechanism by which these pri-
ors Pi can influence the coreference decisions: if,
for example, the factors Pi and Pj disagree very
strongly, choosing ai 6= j will be preferred in or-
der to avoid forcing one of pi or pj to take an un-
desirable value. Moreover, note that although ai
116
E4-3
a
2
a
4
p
4
p
3
p
2
E
4-2
A
2
A
3
A
4
P
2
P
3
P
4
antecedent 
choices
antecedent 
factors
property 
factors
properties
equality 
factors
a
3
}
}
}
}
}
people
Sotheby's
and Christie's
they
Figure 2: The factor graph for our TRANSI-
TIVE coreference model. Each node ai now has
a property pi, which is informed by its own unary
factor Pi. In our example, a4 strongly indicates
that mentions 2 and 4 are coreferent; the factor
E4?2 then enforces equality between p2 and p4,
while the factor E4?3 has no effect.
only indicates a single antecedent, the transitive
nature of the E factors forces pi to agree with the
p nodes of all other mentions likely to be in the
same entity.
3.4 Property Projection
So far, our model as specified ensures agreement
of our entity-level properties, but strictly enforc-
ing agreement may not always be correct. Suppose
that we are using named entity type as an entity-
level property. Organizations and geo-political en-
tities are two frequently confused and ambiguous
tags, and in the gold-standard coreference chains
it may be the case that a single chain contains in-
stances of both. We might wish to learn that or-
ganizations and geo-political entities are ?compat-
ible? in the sense that we should forgive entities
for containing both, but without losing the ability
to reject a chain containing both organizations and
people, for example.
To address these effects, we expand our model
as indicated in Figure 3. As before, we have a
set of properties pi and agreement factors Eij . On
top of that, we introduce the notion of raw prop-
erty values ri ? {1, ..., k} together with priors in
the form of the Ri factors. The ri and pi could in
principle have different domains, but for this work
we take them to have the same domain. The Pi
factors now have a new structure: they now rep-
resent a featurized projection of the ri onto the
pi, which can now be thought of as ?coreference-
p
4p
3
p
2
r
4
r
3
r
2
P
2
P
3
P
4
R
2
R
3
R
4
raw property 
factors
raw properties
projection 
factors
projected 
properties
}
}
}
}
a
2
a
4
A
2
A
3
A
4
a
3
E
3-1
E
4-1
Figure 3: The complete factor graph for our
TRANSITIVE coreference model. Compared to
Figure 2, the Ri contain the raw cluster posteriors,
and the Pi factors now project raw cluster values ri
into a set of ?coreference-adapted? clusters pi that
are used as before. This projection allows men-
tions with different but compatible raw property
values to coexist in the same coreference chain.
adapted? properties. The Pi factors are defined by
Pi(pi, ri) ? exp(wT fP (pi, ri)), where fP is a fea-
ture vector over the projection of ri onto pi. While
there are many possible choices of fP , we choose
it to be an indicator of the values of pi and ri, so
that we learn a fully-parameterized projection ma-
trix.2 The Ri are constant factors, and may come
from an upstream model or some other source de-
pending on the property being modeled.
Our description thus far has assumed that we
are modeling only one type of property. In fact,
we can use multiple properties for each mention
by duplicating the r and p nodes and the R, P ,
and E factors across each desired property. We
index each of these by l ? {1, . . . ,m} for each of
m properties.
The final log-linear model is given by the fol-
lowing formula:
P (a|x) ?
?
p,r
?
?
?
??
i,j,l
El,i?j(ai, pli, plj)
?
?
?
??
i,l
Rli(rli)
?
?
exp
(
wT
?
i
(
fA(i, ai, x) +
?
l
fP (pli, rli)
))]
where i and j range over mentions, l ranges over
2Initialized to zero (or small values), this matrix actually
causes the transitive machinery to have no effect, since all
posteriors over the pi are flat and completely uninformative.
Therefore, we regularize the weights of the indicators of pi =
ri towards 1 and all other features towards 0 to give each raw
cluster a preference for a distinct projected cluster.
117
each of m properties, and the outer sum indicates
marginalization over all p and r variables.
4 Learning
Now that we have defined our model, we must
decide how to train its weights w. The first
issue to address is one of the supervision pro-
vided. Our model traffics in sets of labels a
which are more specified than gold coreference
chains C, which give cluster membership for each
mention but not antecedence. Let A(C) be the
set of labelings a that are consistent with a set
of coreference chains C. For example, if C =
{{1, 2, 3}, {4}}, then (<new>, 1, 2, <new>) ?
A(C) and (<new>, 1, 1, <new>) ? A(C) but
(<new>, 1, <new>, 3) /? A(C), since this im-
plies the chains C = {{1, 2}, {3, 4}}
The most natural objective is a variant of
standard conditional log-likelihood that treats the
choice of a for the specified C as a latent variable
to be marginalized out:
`(w) =
t?
i=1
log
?
? ?
a?A(Ci)
P (a|xi)
?
? (1)
where (xi, Ci) is the ith labeled training example.
This optimizes for the 0-1 loss; however, we are
much more interested in optimizing with respect
to a coreference-specific loss function.
To this end, we will use softmax-margin (Gim-
pel and Smith, 2010), which augments the proba-
bility of each example with a term proportional to
its loss, pushing the model to assign less mass to
highly incorrect examples. We modify Equation 1
to use a new probability distribution P ? instead
of P , where P ?(a|xi) ? P (a|xi) exp (l(a,C))
and l(a,C) is a loss function. In order to
perform inference efficiently, l(a,C) must de-
compose linearly across mentions: l(a,C) =?n
i=1 l(ai, C). Commonly-used coreference met-
rics such as MUC (Vilain et al, 1995) and B3
(Bagga and Baldwin, 1998) do not have this prop-
erty, so we instead make use of a parameterized
loss function that does and fit the parameters to
give good performance. Specifically, we take
l(a,C) =
n?
i=1
[c1I(K1(ai, C)) + c2I(K2(ai, C))
+ c3I(K3(ai, C))]
where c1, c2, and c3 are real-valued weights, K1
denotes the event that ai is falsely anaphoric when
it should be non-anaphoric, K2 denotes the event
that ai is falsely non-anaphoric when it should be
anaphoric, and K3 denotes the event that ai is cor-
rectly determined to be anaphoric but . These can
be computed based on only ai and C. By setting
c1 low and c2 high relative to c3, we can force
the system to be less conservative about making
anaphoricity decisions and achieve a better bal-
ance with the final coreference metrics.
Finally, we incorporate L1 regularization, giv-
ing us our final objective:
`(w) =
t?
i=1
log
?
? ?
a?A(Ci)
P ?(a|xi)
?
?+ ??w?1
We optimize this objective using AdaGrad
(Duchi et al, 2011); we found this to be faster and
give higher performance than L-BFGS using L2
regularization (Liu and Nocedal, 1989). Note that
because of the marginalization over A(Ci), even
the objective for the BASIC model is not convex.
5 Inference
Inference in the BASIC model is straightforward.
Given a set of weights w, we can predict
a? = argmax
a
P (a|x)
We then report the corresponding chains C(a)
as the system output.3 For learning, the gradi-
ent takes the standard form of the gradient of a
log-linear model, a difference of expected feature
counts under the gold annotation and under no
annotation. This requires computing marginals
P ?(ai|x) for each mention i, but because the
model already factors this way, this step is easy.
The TRANSITIVE model is more complex. Ex-
act inference is intractable due to theE factors that
couple all of the ai by way of the pi nodes. How-
ever, we can compute approximate marginals for
the ai, pi, and ri using belief propagation. BP has
been effectively used on other NLP tasks (Smith
and Eisner, 2008; Burkett and Klein, 2012), and is
effective in cases such as this where the model is
largely driven by non-loopy factors (here, the Ai).
From marginals over each node, we can com-
pute the necessary gradient and decode as before:
a? = argmax
a
P? (a|x)
3One could use ILP-based decoding in the style of Finkel
and Manning (2008) and Song et al (2012) to attempt to ex-
plicitly find the optimal C with choice of a marginalized out,
but we did not explore this option.
118
This corresponds to minimum-risk decoding with
respect to the Hamming loss over antecedence pre-
dictions.
Pruning. The TRANSITIVE model requires in-
stantiating a factor for each potential setting of
each ai. This factor graph grows quadratically in
the size of the document, and even approximate in-
ference becomes slow when a document contains
over 200 mentions. Therefore, we use our BA-
SIC model to prune antecedent choices for each
ai in order to reduce the size of the factor graph
that we must instantiate. Specifically, we prune
links between pairs of mentions that are of men-
tion distance more than 100, as well as values for
ai that fall below a particular odds ratio threshold
with respect to the best setting of that ai in the
BASIC model; that is, those for which
log
( PBASIC (ai|x)
maxj PBASIC (ai = j|x)
)
is below a cutoff ?.
6 Related Work
Our BASIC model is a mention-ranking approach
resembling models used by Denis and Baldridge
(2008) and Rahman and Ng (2009), though it is
trained using a novel parameterized loss function.
It is also similar to the MLN-JOINT(BF) model
of Song et al (2012), but we enforce the single-
parent constraint at a deeper structural level, al-
lowing us to treat non-anaphoricity symmetrically
with coreference as in Denis and Baldridge (2007)
and Stoyanov and Eisner (2012). The model of
Fernandes et al (2012) also uses the single-parent
constraint structurally, but with learning via la-
tent perceptron and ILP-based one-best decod-
ing rather than logistic regression and BP-based
marginal computation.
Our TRANSITIVE model is novel; while Mc-
Callum and Wellner (2004) proposed the idea of
using attributes for mentions, they do not actu-
ally implement a model that does so. Other sys-
tems include entity-level information via hand-
written rules (Raghunathan et al, 2010), induced
rules (Yang et al, 2008), or features with learned
weights (Luo et al, 2004; Rahman and Ng, 2011),
but all of these systems freeze past coreference de-
cisions in order to compute their entities.
Most similar to our entity-level approach is
the system of Haghighi and Klein (2010), which
also uses approximate global inference; however,
theirs is an unsupervised, generative system and
they attempt to directly model multinomials over
words in each mention. Their system could be ex-
tended to handle property information like we do,
but our system has many other advantages, such as
freedom from a pre-specified list of entity types,
the ability to use multiple input clusterings, and
discriminative projection of clusters.
7 Experiments
We use the datasets, experimental setup, and scor-
ing program from the CoNLL 2011 shared task
(Pradhan et al, 2011), based on the OntoNotes
corpus (Hovy et al, 2006). We use the standard
automatic parses and NER tags for each docu-
ment. Our mentions are those output by the sys-
tem of Lee et al (2011); we also use their postpro-
cessing to remove appositives, predicate nomina-
tives, and singletons before evaluation. For each
experiment, we report MUC (Vilain et al, 1995),
B3 (Bagga and Baldwin, 1998), and CEAFe (Luo,
2005), as well as their average.
Parameter settings. We take the regularization
constant ? = 0.001 and the parameters of our
surrogate loss (c1, c2, c3) = (0.15, 2.5, 1) for all
models.4 All models are trained for 20 iterations.
We take the pruning threshold ? = ?2.
7.1 Systems
Besides our BASIC and TRANSITIVE systems, we
evaluate a strictly pairwise system that incorpo-
rates property information by way of indicator fea-
tures on the current mention?s most likely property
value and the proposed antecedent?s most likely
property value. We call this system PAIRPROP-
ERTY; it is simply the BASIC system with an ex-
panded feature set.
Furthermore, we compare against a LEFT-
TORIGHT entity-level system like that of Rahman
and Ng (2009).5 Decoding now operates in a se-
quential fashion, with BASIC features computed
as before and entity features computed for each
mention based on the coreference decisions made
thus far. Following Rahman and Ng (2009), fea-
tures for each property indicate whether the cur-
4Additional tuning of these hyper parameters did not sig-
nificantly improve any of the models under any of the exper-
imental conditions.
5Unfortunately, their publicly-available system is closed-
source and performs poorly on the CoNLL shared task
dataset, so direct comparison is difficult.
119
rent mention agrees with no mentions in the an-
tecedent cluster, at least one mention, over half of
the mentions, or all of the mentions; antecedent
clusters of size 1 or 2 fire special-cased features.
These additional features beyond those in Rah-
man and Ng (2009) were helpful, but more in-
volved conjunction schemes and fine-grained fea-
tures were not. During training, entity features of
both the gold and the prediction are computed us-
ing the Viterbi clustering of preceding mentions
under the current model parameters.6
All systems are run in a two-pass manner:
first, the BASIC model is run, then antecedent
choices are pruned, then our second-round model
is trained from scratch on the pruned data.7
7.2 Noisy Oracle Features
We first evaluate our model?s ability to exploit syn-
thetic entity-level properties. For this experiment,
mention properties are derived from corrupted or-
acle information about the true underlying corefer-
ence cluster. Each coreference cluster is assumed
to have one underlying value for each of m coref-
erence properties, each taking values over a do-
main D. Mentions then sample distributions over
D from a Dirichlet distribution peaked around the
true underlying value.8 These posteriors are taken
as the Ri for the TRANSITIVE model.
We choose this setup to reflect two important
properties of entity-level information: first, that it
may come from a variety of disparate sources, and
second, that it may be based on the determinations
of upstream models which produce posteriors nat-
urally. A strength of our model is that it can accept
such posteriors as input, naturally making use of
this information in a model-based way.
Table 1 shows development results averaged
across ten train-test splits with m = 3 proper-
ties, each taking one of |D| = 5 values. We em-
phasize that these parameter settings give fairly
weak oracle information: a document may have
hundreds of clusters, so even in the absence of
noise these oracle properties do not have high dis-
6Using gold entities for training as in Rahman and Ng
(2009) resulted in a lower-performing system.
7We even do this for the BASIC model, since we found
that performance of the pruned and retrained model was gen-
erally higher.
8Specifically, the distribution used is a Dirichlet with
? = 3.5 for the true underlying cluster and ? = 1 for other
values, chosen so that 25% of samples from the distribution
did not have the correct mode. Though these parameters af-
fect the quality of the oracle information, varying them did
not change the relative performance of the different models.
NOISY ORACLE
MUC B3 CEAFe Avg.
BASIC 61.96 70.66 47.30 59.97
PAIRPROPERTY 66.31 72.68 49.08 62.69
LEFTTORIGHT 66.49 73.14 49.46 63.03
TRANSITIVE 67.37 74.05 49.68 63.70
Table 1: CoNLL metric scores for our four dif-
ferent systems incorporating noisy oracle data.
This information helps substantially in all cases.
Both entity-level models outperform the PAIR-
PROPERTY model, but we observe that the TRAN-
SITIVE model is more effective than the LEFT-
TORIGHT model at using this information.
criminating power. Still, we see that all mod-
els are able to benefit from incorporating this in-
formation; however, our TRANSITIVE model out-
performs both the PAIRPROPERTY model and the
LEFTTORIGHT model. There are a few reasons
for this: first, our model is able to directly use soft
posteriors, so it is able to exploit the fact that more
peaked samples from the Dirichlet are more likely
to be correct. Moreover, our model can propagate
information backwards in a document as well as
forwards, so the effects of noise can be more eas-
ily mitigated. By contrast, in the LEFTTORIGHT
model, if the first or second mention in a cluster
has the wrong property value, features indicating
high levels of property agreement will not fire on
the next few mentions in those clusters.
7.3 Phi Features
As we have seen, our TRANSITIVE model can ex-
ploit high-quality entity-level features. How does
it perform using real features that have been pro-
posed for entity-level coreference?
Here, we use hard phi feature determinations
extracted from the system of Lee et al (2011).
Named-entity type and animacy are both com-
puted based on the output of a named-entity tag-
ger, while number and gender use the dataset of
Bergsma and Lin (2006). Once this informa-
tion is determined, the PAIRPROPERTY and LEFT-
TORIGHT systems can compute features over it di-
rectly. In the TRANSITIVE model, each of the Ri
factors places 34 of its mass on the determined la-bel and distributes the remainder uniformly among
the possible options.
Table 2 shows results when adding entity-level
phi features on top of our BASIC pairwise system
(which already contains pairwise features) and on
top of an ablated BASIC system without pairwise
120
PHI FEATURES
MUC B3 CEAFe Avg.
BASIC 61.96 70.66 47.30 59.97
LEFTTORIGHT 61.34 70.41 47.64 59.80
TRANSITIVE 62.66 70.92 46.88 60.16
PHI FEATURES (ABLATED BASIC)
BASIC-PHI 59.45 69.21 46.02 58.23
PAIRPROPERTY 61.88 70.66 47.14 59.90
LEFTTORIGHT 61.42 70.53 47.49 59.81
TRANSITIVE 62.23 70.78 46.74 59.92
Table 2: CoNLL metric scores for our systems in-
corporating phi features. Our standard BASIC sys-
tem already includes phi features, so no results are
reported for PAIRPROPERTY. Here, our TRAN-
SITIVE system does not give substantial improve-
ment on the averaged metric. Over a baseline
which does not include phi features, all systems
are able to incorporate them comparably.
phi features. Our entity-level systems successfully
captures phi features when they are not present in
the baseline, but there is only slight benefit over
pairwise incorporation, a result which has been
noted previously (Luo et al, 2004).
7.4 Clustering Features
Finally, we consider mention properties derived
from unsupervised clusterings; these properties
are designed to target semantic properties of nom-
inals that should behave more like the oracle fea-
tures than the phi features do.
We consider clusterings that take as input pairs
(n, r) of a noun head n and a string r which con-
tains the semantic role of n (or some approxima-
tion thereof) conjoined with its governor. Two dif-
ferent algorithms are used to cluster these pairs: a
NAIVEBAYES model, where c generates n and r,
and a CONDITIONAL model, where c is generated
conditioned on r and then n is generated from c.
Parameters for each can be learned with the ex-
pectation maximization (EM) algorithm (Demp-
ster et al, 1977), with symmetry broken by a small
amount of random noise at initialization.
Similar models have been used to learn sub-
categorization information (Rooth et al, 1999)
or properties of verb argument slots (Yao et al,
2011). We choose this kind of clustering for its rel-
ative simplicity and because it allows pronouns to
have more informed properties (from their verbal
context) than would be possible using a model that
makes type-level decisions about nominals only.
Though these specific cluster features are novel
to coreference, previous work has used similar
CLUSTERS
MUC B3 CEAFe Avg.
BASIC 61.96 70.66 47.30 59.97
PAIRPROPERTY 62.88 70.71 47.45 60.35
LEFTTORIGHT 61.98 70.19 45.77 59.31
TRANSITIVE 63.34 70.89 46.88 60.37
Table 3: CoNLL metric scores for our systems
incorporating clustering features. These features
are equally effectively incorporated by our PAIR-
PROPERTY system and our TRANSITIVE system.
government
officials
court
authorities
ARG0:said
ARG0:say
ARG0:found
ARG0:announced
prices
shares
index
rates
ARG1:rose
ARG1:fell
ARG1:cut
ARG1:closed
way
law
agreement
plan
ARG1:signed
ARG1:announced
ARG1:set
ARG1:approved
attack
problems
attacks
charges
ARG1:cause
ARG2:following
ARG1:reported
ARG1:filed
... ...
... ...
... ...
... ...
...
Figure 4: Examples of clusters produced by the
NAIVEBAYES model on SRL-tagged data with
pronouns discarded.
types of fine-grained semantic class information
(Hendrickx and Daelemans, 2007; Ng, 2007; Rah-
man and Ng, 2010). Other approaches incorpo-
rate information from other sources (Ponzetto and
Strube, 2006) or compute heuristic scores for real-
valued features based on a large corpus or the web
(Dagan and Itai, 1990; Yang et al, 2005; Bansal
and Klein, 2012).
We use four different clusterings in our
experiments, each with twenty clusters:
dependency-parse-derived NAIVEBAYES clusters,
semantic-role-derived CONDITIONAL clusters,
SRL-derived NAIVEBAYES clusters generating
a NOVERB token when r cannot be determined,
and SRL-derived NAIVEBAYES clusters with all
pronoun tuples discarded. Examples of the latter
clusters are shown in Figure 4. Each clustering
is learned for 30 iterations of EM over English
Gigaword (Graff et al, 2007), parsed with the
Berkeley Parser (Petrov et al, 2006) and with
SRL determined by Senna (Collobert et al, 2011).
Table 3 shows results of modeling these cluster
properties. As in the case of oracle features, the
PAIRPROPERTY and LEFTTORIGHT systems use
the modes of the cluster posteriors, and the TRAN-
SITIVE system uses the posteriors directly as the
Ri. We see comparable performance from incor-
porating features in both an entity-level framework
and a pairwise framework, though the TRANSI-
121
MUC B3 CEAFe Avg.
Prec. Rec. F1 Prec. Rec. F1 Prec. Rec. F1 F1
BASIC 69.99 55.59 61.96 80.96 62.69 70.66 41.37 55.21 47.30 59.97
STANFORD 61.49 59.59 60.49 74.60 68.25 71.28 47.57 49.45 48.49 60.10
NOISY ORACLE
PAIRPROPERTY 76.49 58.53 66.31 84.98 63.48 72.68 41.84 59.36 49.08 62.69
LEFTTORIGHT 76.92 58.55 66.49 85.68 63.81 73.14 42.07 60.01 49.46 63.03
TRANSITIVE 76.48 60.20 *67.37 84.84 65.69 *74.05 42.89 59.01 *49.68 63.70
PHI FEATURES
LEFTTORIGHT 69.77 54.73 61.34 81.40 62.04 70.41 41.49 55.92 47.64 59.80
TRANSITIVE 70.27 56.54 *62.66 79.81 63.82 *70.92 41.17 54.44 46.88 60.16
PHI FEATURES (ABLATED BASIC)
BASIC-PHI 67.04 53.41 59.45 78.93 61.63 69.21 40.40 53.46 46.02 58.23
PAIRPROPERTY 70.24 55.31 61.88 81.10 62.60 70.66 41.04 55.38 47.14 59.90
LEFTTORIGHT 69.94 54.75 61.42 81.38 62.23 70.53 41.29 55.87 47.49 59.81
TRANSITIVE 70.06 55.98 *62.23 79.92 63.52 70.78 40.90 54.52 46.74 59.92
CLUSTERS
PAIRPROPERTY 71.77 55.95 62.88 81.76 62.30 70.71 40.98 56.35 47.45 60.35
LEFTTORIGHT 69.75 54.82 61.39 81.48 62.29 70.60 41.62 55.89 47.71 59.90
TRANSITIVE 71.54 56.83 *63.34 80.55 63.31 *70.89 40.77 55.14 46.88 60.37
Table 4: CoNLL metric scores averaged across ten different splits of the training set for each experiment.
We include precision, recall, and F1 for each metric for completeness. Starred F1 values on the individual
metrics for the TRANSITIVE system are significantly better than all other results in the same block at the
p = 0.01 level according to a bootstrap resampling test.
MUC B3 CEAFe Avg.
Prec. Rec. F1 Prec. Rec. F1 Prec. Rec. F1 F1
BASIC 68.84 56.08 61.81 77.60 61.40 68.56 38.25 50.57 43.55 57.97
PAIRPROPERTY 70.90 56.26 62.73 78.95 60.79 68.69 37.69 51.92 43.67 58.37
LEFTTORIGHT 68.84 55.56 61.49 78.64 61.03 68.72 38.97 51.74 44.46 58.22
TRANSITIVE 70.62 58.06 *63.73 76.93 62.24 68.81 38.00 50.40 43.33 58.62
STANFORD 60.91 62.13 61.51 70.61 67.75 69.15 45.79 44.55 45.16 58.61
Table 5: CoNLL metric scores for our best systems (including clustering features) on the CoNLL blind
test set, reported in the same manner as Table 4.
TIVE system appears to be more effective than the
LEFTTORIGHT system.
7.5 Final Results
Table 4 shows expanded results on our develop-
ment sets for the different types of entity-level
information we considered. We also show in in
Table 5 the results of our system on the CoNLL
test set, and see that it performs comparably to
the Stanford coreference system (Lee et al, 2011).
Here, our TRANSITIVE system provides modest
improvements over all our other systems.
Based on Table 4, our TRANSITIVE system ap-
pears to do better on MUC andB3 than on CEAFe.
However, we found no simple way to change the
relative performance characteristics of our various
systems; notably, modifying the parameters of the
loss function mentioned in Section 4 or changing
it entirely did not trade off these three metrics but
merely increased or decreased them in lockstep.
Therefore, the TRANSITIVE system actually sub-
stantially improves over our baselines and is not
merely trading off metrics in a way that could be
easily reproduced through other means.
8 Conclusion
In this work, we presented a novel coreference ar-
chitecture that can both take advantage of standard
pairwise features as well as use transitivity to en-
force coherence of decentralized entity-level prop-
erties within coreference clusters. Our transitive
system is more effective at using properties than
a pairwise system and a previous entity-level sys-
tem, and it achieves performance comparable to
that of the Stanford coreference resolution system,
the winner of the CoNLL 2011 shared task.
Acknowledgments
This work was partially supported by BBN under
DARPA contract HR0011-12-C-0014, by an NSF
fellowship for the first author, and by a Google fel-
lowship for the second. Thanks to the anonymous
reviewers for their insightful comments.
122
References
Amit Bagga and Breck Baldwin. 1998. Algorithms for
Scoring Coreference Chains. In Proceedings of the
Conference on Language Resources and Evaluation
Workshop on Linguistics Coreference.
Mohit Bansal and Dan Klein. 2012. Coreference Se-
mantics from Web Features. In Proceedings of the
Association for Computational Linguistics.
Eric Bengtson and Dan Roth. 2008. Understanding
the Value of Features for Coreference Resolution. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing.
Shane Bergsma and Dekang Lin. 2006. Bootstrap-
ping Path-Based Pronoun Resolution. In Proceed-
ings of the Conference on Computational Linguistics
and the Association for Computational Linguistics.
David Burkett and Dan Klein. 2012. Fast Inference in
Phrase Extraction Models with Belief Propagation.
In Proceedings of the North American Chapter of
the Association for Computational Linguistics.
Ronan Collobert, Jason Weston, Le?on Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural Language Processing (Almost) from
Scratch. Journal of Machine Learning Research,
12:2493?2537, November.
Ido Dagan and Alon Itai. 1990. Automatic Process-
ing of Large Corpora for the Resolution of Anaphora
References. In Proceedings of the Conference on
Computational Linguistics - Volume 3.
Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-
bin. 1977. Maximum Likelihood from Incomplete
Data via the EM Algorithm. Journal of the Royal
Statistical Society, Series B, 39(1):1?38.
Pascal Denis and Jason Baldridge. 2007. Joint Deter-
mination of Anaphoricity and Coreference Resolu-
tion using Integer Programming. In Proceedings of
the North American Chapter of the Association for
Computational Linguistics.
Pascal Denis and Jason Baldridge. 2008. Specialized
Models and Ranking for Coreference Resolution. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive Subgradient Methods for Online Learning
and Stochastic Optimization. Journal of Machine
Learning Research, 12:2121?2159, July.
Eraldo Rezende Fernandes, C??cero Nogueira dos San-
tos, and Ruy Luiz Milidiu?. 2012. Latent Structure
Perceptron with Feature Induction for Unrestricted
Coreference Resolution. In Proceedings of the Joint
Conference on Empirical Methods in Natural Lan-
guage Proceedings and Conference on Computa-
tional Natural Language Learning - Shared Task.
Jenny Rose Finkel and Christopher D. Manning. 2008.
Enforcing Transitivity in Coreference Resolution.
In Proceedings of the Association for Computational
Linguistics: Short Papers.
Kevin Gimpel and Noah A. Smith. 2010. Softmax-
Margin CRFs: Training Log-Linear Models with
Cost Functions. In Proceedings of the North Amer-
ican Chapter for the Association for Computational
Linguistics.
David Graff, Junbo Kong, Ke Chen, and Kazuaki
Maeda. 2007. English Gigaword Third Edi-
tion. Linguistic Data Consortium, Catalog Number
LDC2007T07.
Aria Haghighi and Dan Klein. 2010. Coreference Res-
olution in a Modular, Entity-Centered Model. In
Proceedings of the North American Chapter of the
Association for Computational Linguistics.
Iris Hendrickx and Walter Daelemans, 2007. Adding
Semantic Information: Unsupervised Clusters for
Coreference Resolution.
Eduard Hovy, Mitchell Marcus, Martha Palmer,
Lance Ramshaw, and Ralph Weischedel. 2006.
OntoNotes: the 90% solution. In Proceedings of
the North American Chapter of the Association for
Computational Linguistics: Short Papers.
Heeyoung Lee, Yves Peirsman, Angel Chang,
Nathanael Chambers, Mihai Surdeanu, and Dan Ju-
rafsky. 2011. Stanford?s Multi-Pass Sieve Corefer-
ence Resolution System at the CoNLL-2011 Shared
Task. In Proceedings of the Conference on Compu-
tational Natural Language Learning: Shared Task.
Dong C. Liu and Jorge Nocedal. 1989. On the Limited
Memory BFGS Method for Large Scale Optimiza-
tion. Mathematical Programming, 45(3):503?528,
December.
Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, Nanda
Kambhatla, and Salim Roukos. 2004. A
Mention-Synchronous Coreference Resolution Al-
gorithm Based on the Bell Tree. In Proceedings of
the Association for Computational Linguistics.
Xiaoqiang Luo. 2005. On Coreference Resolution
Performance Metrics. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing.
Andrew McCallum and Ben Wellner. 2004. Condi-
tional Models of Identity Uncertainty with Applica-
tion to Noun Coreference. In Proceedings of Ad-
vances in Neural Information Processing Systems.
George A. Miller. 1995. WordNet: A Lexical
Database for English. Communications of the ACM,
38:39?41.
Vincent Ng. 2007. Semantic class induction and coref-
erence resolution. In Proceedings of the Association
for Computational Linguistics.
123
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning Accurate, Compact, and In-
terpretable Tree Annotation. In Proceedings of the
Conference on Computational Linguistics and the
Association for Computational Linguistics.
Simone Paolo Ponzetto and Michael Strube. 2006.
Exploiting Semantic Role Labeling, WordNet and
Wikipedia for Coreference Resolution. In Proceed-
ings of the North American Chapter of the Associa-
tion of Computational Linguistics.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen
Xue. 2011. CoNLL-2011 Shared Task: Modeling
Unrestricted Coreference in OntoNotes. In Proceed-
ings of the Conference on Computational Natural
Language Learning: Shared Task.
Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-
garajan, Nathanael Chambers, Mihai Surdeanu, Dan
Jurafsky, and Christopher Manning. 2010. A Multi-
Pass Sieve for Coreference Resolution. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing.
Altaf Rahman and Vincent Ng. 2009. Supervised
Models for Coreference Resolution. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing.
Altaf Rahman and Vincent Ng. 2010. Inducing Fine-
Grained Semantic Classes via Hierarchical and Col-
lective Classification. In Proceedings of the Interna-
tional Conference on Computational Linguistics.
Altaf Rahman and Vincent Ng. 2011. Narrowing
the Modeling Gap: A Cluster-Ranking Approach to
Coreference Resolution. Journal of Artificial Intel-
ligence Research, 40(1):469?521, January.
Mats Rooth, Stefan Riezler, Detlef Prescher, Glenn
Carroll, and Franz Beil. 1999. Inducing a Semanti-
cally Annotated Lexicon via EM-Based Clustering.
In Proceedings of the Association for Computational
Linguistics.
David A. Smith and Jason Eisner. 2008. Dependency
Parsing by Belief Propagation. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing.
Yang Song, Jing Jiang, Wayne Xin Zhao, Sujian Li, and
Houfeng Wang. 2012. Joint Learning for Corefer-
ence Resolution with Markov Logic. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing.
Veselin Stoyanov and Jason Eisner. 2012. Easy-first
Coreference Resolution. In Proceedings of the In-
ternational Conference on Computational Linguis-
tics.
Veselin Stoyanov, Claire Cardie, Nathan Gilbert, Ellen
Riloff, David Buttler, and David Hysom. 2010.
Coreference Resolution with Reconcile. In Pro-
ceedings of the Association for Computational Lin-
guistics: Short Papers.
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A Model-
Theoretic Coreference Scoring Scheme. In Pro-
ceedings of the Conference on Message Understand-
ing.
Xiaofeng Yang, Jian Su, and Chew Lim Tan. 2005. Im-
proving Pronoun Resolution Using Statistics-Based
Semantic Compatibility Information. In Proceed-
ings of the Association for Computational Linguis-
tics.
Xiaofeng Yang, Jian Su, Jun Lang, Chew L. Tan, Ting
Liu, and Sheng Li. 2008. An Entity-Mention Model
for Coreference Resolution with Inductive Logic
Programming. In Proceedings of the Association for
Computational Linguistics.
Limin Yao, Aria Haghighi, Sebastian Riedel, and An-
drew McCallum. 2011. Structured Relation Discov-
ery Using Generative Models. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing.
124
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 207?217,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Unsupervised Transcription of Historical Documents
Taylor Berg-Kirkpatrick Greg Durrett Dan Klein
Computer Science Division
University of California at Berkeley
{tberg,gdurrett,klein}@cs.berkeley.edu
Abstract
We present a generative probabilistic
model, inspired by historical printing pro-
cesses, for transcribing images of docu-
ments from the printing press era. By
jointly modeling the text of the docu-
ment and the noisy (but regular) process
of rendering glyphs, our unsupervised sys-
tem is able to decipher font structure and
more accurately transcribe images into
text. Overall, our system substantially out-
performs state-of-the-art solutions for this
task, achieving a 31% relative reduction
in word error rate over the leading com-
mercial system for historical transcription,
and a 47% relative reduction over Tesser-
act, Google?s open source OCR system.
1 Introduction
Standard techniques for transcribing modern doc-
uments do not work well on historical ones. For
example, even state-of-the-art OCR systems pro-
duce word error rates of over 50% on the docu-
ments shown in Figure 1. Unsurprisingly, such er-
ror rates are too high for many research projects
(Arlitsch and Herbert, 2004; Shoemaker, 2005;
Holley, 2010). We present a new, generative
model specialized to transcribing printing-press
era documents. Our model is inspired by the un-
derlying printing processes and is designed to cap-
ture the primary sources of variation and noise.
One key challenge is that the fonts used in his-
torical documents are not standard (Shoemaker,
2005). For example, consider Figure 1a. The fonts
are not irregular like handwriting ? each occur-
rence of a given character type, e.g. a, will use the
same underlying glyph. However, the exact glyphs
are unknown. Some differences between fonts are
minor, reflecting small variations in font design.
Others are more severe, like the presence of the
archaic long s character before 1804. To address
the general problem of unknown fonts, our model
(a)
(b)
(c)
Figure 1: Portions of historical documents with (a) unknown
font, (b) uneven baseline, and (c) over-inking.
learns the font in an unsupervised fashion. Font
shape and character segmentation are tightly cou-
pled, and so they are modeled jointly.
A second challenge with historical data is that
the early typesetting process was noisy. Hand-
carved blocks were somewhat uneven and often
failed to sit evenly on the mechanical baseline.
Figure 1b shows an example of the text?s baseline
moving up and down, with varying gaps between
characters. To deal with these phenomena, our
model incorporates random variables that specifi-
cally describe variations in vertical offset and hor-
izontal spacing.
A third challenge is that the actual inking was
also noisy. For example, in Figure 1c some charac-
ters are thick from over-inking while others are ob-
scured by ink bleeds. To be robust to such render-
ing irregularities, our model captures both inking
levels and pixel-level noise. Because the model
is generative, we can also treat areas that are ob-
scured by larger ink blotches as unobserved, and
let the model predict the obscured text based on
visual and linguistic context.
Our system, which we call Ocular, operates by
fitting the model to each document in an unsuper-
vised fashion. The system outperforms state-of-
the-art baselines, giving a 47% relative error re-
duction over Google?s open source Tesseract sys-
tem, and giving a 31% relative error reduction over
ABBYY?s commercial FineReader system, which
has been used in large-scale historical transcrip-
tion projects (Holley, 2010).
207
Over-inked
It appeared that the Prisoner was veryE :
X :
Wandering baseline Historical font
Figure 2: An example image from a historical document (X)
and its transcription (E).
2 Related Work
Relatively little prior work has built models specif-
ically for transcribing historical documents. Some
of the challenges involved have been addressed
(Ho and Nagy, 2000; Huang et al, 2006; Kae and
Learned-Miller, 2009), but not in a way targeted
to documents from the printing press era. For ex-
ample, some approaches have learned fonts in an
unsupervised fashion but require pre-segmentation
of the image into character or word regions (Ho
and Nagy, 2000; Huang et al, 2006), which is not
feasible for noisy historical documents. Kae and
Learned-Miller (2009) jointly learn the font and
image segmentation but do not outperform mod-
ern baselines.
Work that has directly addressed historical doc-
uments has done so using a pipelined approach,
and without fully integrating a strong language
model (Vamvakas et al, 2008; Kluzner et al,
2009; Kae et al, 2010; Kluzner et al, 2011).
The most comparable work is that of Kopec and
Lomelin (1996) and Kopec et al (2001). They
integrated typesetting models with language mod-
els, but did not model noise. In the NLP com-
munity, generative models have been developed
specifically for correcting outputs of OCR systems
(Kolak et al, 2003), but these do not deal directly
with images.
A closely related area of work is automatic de-
cipherment (Ravi and Knight, 2008; Snyder et al,
2010; Ravi and Knight, 2011; Berg-Kirkpatrick
and Klein, 2011). The fundamental problem is
similar to our own: we are presented with a se-
quence of symbols, and we need to learn a corre-
spondence between symbols and letters. Our ap-
proach is also similar in that we use a strong lan-
guage model (in conjunction with the constraint
that the correspondence be regular) to learn the
correct mapping. However, the symbols are not
noisy in decipherment problems and in our prob-
lem we face a grid of pixels for which the segmen-
tation into symbols is unknown. In contrast, deci-
pherment typically deals only with discrete sym-
bols.
3 Model
Most historical documents have unknown fonts,
noisy typesetting layouts, and inconsistent ink lev-
els, usually simultaneously. For example, the por-
tion of the document shown in Figure 2 has all
three of these problems. Our model must handle
them jointly.
We take a generative modeling approach in-
spired by the overall structure of the historical
printing process. Our model generates images of
documents line by line; we present the generative
process for the image of a single line. Our pri-
mary random variables are E (the text) andX (the
pixels in an image of the line). Additionally, we
have a random variable T that specifies the layout
of the bounding boxes of the glyphs in the image,
and a random variable R that specifies aspects of
the inking and rendering process. The joint distri-
bution is:
P (E, T,R,X) =
P (E) [Language model]
? P (T |E) [Typesetting model]
? P (R) [Inking model]
? P (X|E, T,R) [Noise model]
We let capital letters denote vectors of concate-
nated random variables, and we denote the indi-
vidual random variables with lower-case letters.
For example, E represents the entire sequence of
text, while ei represents ith character in the se-
quence.
3.1 Language Model P (E)
Our language model, P (E), is a Kneser-Ney
smoothed character n-gram model (Kneser and
Ney, 1995). We generate printed lines of text
(rather than sentences) independently, without
generating an explicit stop character. This means
that, formally, the model must separately generate
the character length of each line. We choose not to
bias the model towards longer or shorter character
sequences and let the line length m be drawn uni-
formly at random from the positive integers less
than some large constant M.1 When i < 1, let ei
denote a line-initial null character. We can now
write:
P (E) = P (m) ?
m?
i=1
P (ei|ei?1, . . . , ei?n)
1In particular, we do not use the kind of ?word bonus?
common to statistical machine translation models.
208
ei 1 ei+1ei
li gi ri
XRPADiX
LPAD
i X
GLYPH
i
P ( ? | th)P ( ? | th)
a b c . . . z
Offset: ?VERT
LM params
cb
b
1 30
15 1 5
a
Glyph weights:  c
Bounding box probs:
Left padwidth: ?LPADc
Right padwidth: ?RPADc
Glyph width: ?GLYPHc
Font params
a a a
a a a
P ( ? | pe)
Inking: ?INK
Inking params
Figure 3: Character tokens ei are generated by the language model. For each token index i, a glyph bounding box width gi,
left padding width li, and a right padding width ri, are generated. Finally, the pixels in each glyph bounding box XGLYPHi aregenerated conditioned on the corresponding character, while the pixels in left and right padding bounding boxes, XLPADi and
XRPADi , are generated from a background distribution.
3.2 Typesetting Model P (T |E)
Generally speaking, the process of typesetting
produces a line of text by first tiling bounding
boxes of various widths and then filling in the
boxes with glyphs. Our generative model, which
is depicted in Figure 3, reflects this process. As
a first step, our model generates the dimensions
of character bounding boxes; for each character
token index i we generate three bounding box
widths: a glyph box width gi, a left padding box
width li, and a right padding box width ri, as
shown in Figure 3. We let the pixel height of all
lines be fixed to h. Let Ti = (li, gi, ri) so that Ti
specifies the dimensions of the character box for
token index i; T is then the concatenation of all
Ti, denoting the full layout.
Because the width of a glyph depends on its
shape, and because of effects resulting from kern-
ing and the use of ligatures, the components of
each Ti are drawn conditioned on the character
token ei. This means that, as part of our param-
eterization of the font, for each character type c
we have vectors of multinomial parameters ?LPADc ,
?GLYPHc , and ?RPADc governing the distribution of the
dimensions of character boxes of type c. These
parameters are depicted on the right-hand side of
Figure 3. We can now express the typesetting lay-
out portion of the model as:
P (T |E) =
m?
i=1
P (Ti|ei)
=
m?
i=1
[
P (li; ?LPADei ) ? P (gi; ?
GLYPH
ei ) ? P (ri; ?
RPAD
ei )
]
Each character type c in our font has another set
of parameters, a matrix ?c. These are weights that
specify the shape of the character type?s glyph,
and are depicted in Figure 3 as part of the font pa-
rameters. ?c will come into play when we begin
generating pixels in Section 3.3.
3.2.1 Inking Model P (R)
Before we start filling the character boxes with
pixels, we need to specify some properties of
the inking and rendering process, including the
amount of ink used and vertical variation along
the text baseline. Our model does this by gener-
ating, for each character token index i, a discrete
value di that specifies the overall inking level in
the character?s bounding box, and a discrete value
vi that specifies the glyph?s vertical offset. These
variations in the inking and typesetting process are
mostly independent of character type. Thus, in
209
our model, their distributions are not character-
specific. There is one global set of multinomial
parameters governing inking level (?INK), and an-
other governing offset (?VERT); both are depicted
on the left-hand side of Figure 3. LetRi = (di, vi)
and let R be the concatenation of all Ri so that we
can express the inking model as:
P (R) =
m?
i=1
P (Ri)
=
m?
i=1
[
P (di; ?INK) ? P (vi; ?VERT)
]
The di and vi variables are suppressed in Figure 3
to reduce clutter but are expressed in Figure 4,
which depicts the process of rendering a glyph
box.
3.3 Noise Model P (X|E, T,R)
Now that we have generated a typesetting layout
T and an inking context R, we have to actually
generate each of the pixels in each of the charac-
ter boxes, left padding boxes, and right padding
boxes; the matrices that these groups of pixels
comprise are denoted XGLYPHi , XLPADi , and XRPADi ,
respectively, and are depicted at the bottom of Fig-
ure 3.
We assume that pixels are binary valued and
sample their values independently from Bernoulli
distributions.2 The probability of black (the
Bernoulli parameter) depends on the type of pixel
generated. All the pixels in a padding box have
the same probability of black that depends only on
the inking level of the box, di. Since we have al-
ready generated this value and the widths li and ri
of each padding box, we have enough information
to generate left and right padding pixel matrices
XLPADi and XRPADi .
The Bernoulli parameter of a pixel inside a
glyph bounding box depends on the pixel?s loca-
tion inside the box (as well as on di and vi, but
for simplicity of exposition, we temporarily sup-
press this dependence) and on the model param-
eters governing glyph shape (for each character
type c, the parameter matrix ?c specifies the shape
of the character?s glyph.) The process by which
glyph pixels are generated is depicted in Figure 4.
The dependence of glyph pixels on location
complicates generation of the glyph pixel matrix
XGLYPHi since the corresponding parameter matrix
2We could generate real-valued pixels with a different
choice of noise distribution.
}
}
}
}
}
aa a
a a a
a a
a
}
Interpolate, apply logistic
Sample pixels
Choosewidth
Chooseoffset
Glyph weights
gi
di
vi
 ei
?PIXEL(j, k, gi, di, vi; ei)
?XGLYPHi
?
jk ? Bernoulli
Bernoulli parameters
Pixel values
Chooseinking
Figure 4: We generate the pixels for the character token ei
by first sampling a glyph width gi, an inking level di, and
a vertical offset vi. Then we interpolate the glyph weights
?ei and apply the logistic function to produce a matrix ofBernoulli parameters of width gi, inking di, and offset vi.
?PIXEL(j, k, gi, di, vi;?ei) is the Bernoulli parameter at row jand column k. Finally, we sample from each Bernoulli distri-
bution to generate a matrix of pixel values, XGLYPHi .
?ei has some type-level width w which may dif-
fer from the current token-level width gi. Intro-
ducing distinct parameters for each possible width
would yield a model that can learn completely dif-
ferent glyph shapes for slightly different widths of
the same character. We, instead, need a parame-
terization that ties the shapes for different widths
together, and at the same time allows mobility in
the parameter space during learning.
Our solution is to horizontally interpolate the
weights of the shape parameter matrix ?ei down
to a smaller set of columns matching the token-
level choice of glyph width gi. Thus, the type-
level matrix ?ei specifies the canonical shape of
the glyph for character ei when it takes its max-
imum width w. After interpolating, we apply
the logistic function to produce the individual
Bernoulli parameters. If we let [XGLYPHi ]jk denote
the value of the pixel at the jth row and kth col-
umn of the glyph pixel matrix XGLYPHi for token i,
and let ?PIXEL(j, k, gi;?ei) denote the token-level
210
?PIXEL :
Interpolate, apply logistic
 c :
Glyph weights
Bernoulli params
?
Figure 5: In order to produce Bernoulli parameter matrices
?PIXEL of variable width, we interpolate over columns of ?c
with vectors ?, and apply the logistic function to each result.
Bernoulli parameter for this pixel, we can write:
[XGLYPHi ]jk ? Bernoulli
(
?PIXEL(j, k, gi;?ei)
)
The interpolation process for a single row is de-
picted in Figure 5. We define a constant interpola-
tion vector ?(gi, k) that is specific to the glyph box
width gi and glyph box column k. Each ?(gi, k)
is shaped according to a Gaussian centered at the
relative column position in ?ei . The glyph pixel
Bernoulli parameters are defined as follows:
?PIXEL(j, k,gi;?ei) =
logistic
( w?
k?=1
[
?(gi, k)k? ? [?ei ]jk?
])
The fact that the parameterization is log-linear will
ensure that, during the unsupervised learning pro-
cess, updating the shape parameters ?c is simple
and feasible.
By varying the magnitude of ? we can change
the level of smoothing in the logistic model and
cause it to permit areas that are over-inked. This is
the effect that di controls. By offsetting the rows
of ?c that we interpolate weights from, we change
the vertical offset of the glyph, which is controlled
by vi. The full pixel generation process is dia-
grammed in Figure 4, where the dependence of
?PIXEL on di and vi is also represented.
4 Learning
We use the EM algorithm (Dempster et al, 1977)
to find the maximum-likelihood font parameters:
?c, ?LPADc , ?GLYPHc , and ?RPADc . The image X is the
only observed random variable in our model. The
identities of the characters E the typesetting lay-
out T and the inking R will all be unobserved. We
do not learn ?INK and ?VERT, which are set to the
uniform distribution.
4.1 Expectation Maximization
During the E-step we compute expected counts
for E and T , but maximize over R, for which
we compute hard counts. Our model is an in-
stance of a hidden semi-Markov model (HSMM),
and therefore the computation of marginals is
tractable with the semi-Markov forward-backward
algorithm (Levinson, 1986).
During the M-step, we update the parame-
ters ?LPADc , ?RPADc using the standard closed-form
multinomial updates and use a specialized closed-
form update for ?GLYPHc that enforces unimodal-
ity of the glyph width distribution.3 The glyph
weights, ?c, do not have a closed-form update.
The noise model that ?c parameterizes is a lo-
cal log-linear model, so we follow the approach
of Berg-Kirkpatrick et al (2010) and use L-BFGS
(Liu and Nocedal, 1989) to optimize the expected
likelihood with respect to ?c.
4.2 Coarse-to-Fine Learning and Inference
The number of states in the dynamic programming
lattice grows exponentially with the order of the
language model (Jelinek, 1998; Koehn, 2004). As
a result, inference can become slow when the lan-
guage model order n is large. To remedy this, we
take a coarse-to-fine approach to both learning and
inference. On each iteration of EM, we perform
two passes: a coarse pass using a low-order lan-
guage model, and a fine pass using a high-order
language model (Petrov et al, 2008; Zhang and
Gildea, 2008). We use the marginals4 from the
coarse pass to prune states from the dynamic pro-
gram of the fine pass.
In the early iterations of EM, our font parame-
ters are still inaccurate, and to prune heavily based
on such parameters would rule out correct anal-
yses. Therefore, we gradually increase the ag-
gressiveness of pruning over the course of EM. To
ensure that each iteration takes approximately the
same amount of computation, we also gradually
increase the order of the fine pass, only reaching
the full order n on the last iteration. To produce a
decoding of the image into text, on the final iter-
ation we run a Viterbi pass using the pruned fine
model.
3We compute the weighted mean and weighted variance
of the glyph width expected counts. We set ?GLYPHc to be pro-
portional to a discretized Gaussian with the computed mean
and variance. This update is approximate in the sense that it
does not necessarily find the unimodal multinomial that max-
imizes expected log-likelihood, but it works well in practice.
4In practice, we use max-marginals for pruning to ensure
that there is still a valid path in the pruned lattice.
211
Old Bailey, 1725:
Old Bailey, 1875:
Trove, 1883:
Trove, 1823:
(a)
(b)
(c)
(d)
Figure 6: Portions of several documents from our test set rep-
resenting a range of difficulties are displayed. On document
(a), which exhibits noisy typesetting, our system achieves a
word error rate (WER) of 25.2. Document (b) is cleaner in
comparison, and on it we achieve a WER of 15.4. On doc-
ument (c), which is also relatively clean, we achieve a WER
of 12.5. On document (d), which is severely degraded, we
achieve a WER of 70.0.
5 Data
We perform experiments on two historical datasets
consisting of images of documents printed be-
tween 1700 and 1900 in England and Australia.
Examples from both datasets are displayed in Fig-
ure 6.
5.1 Old Bailey
The first dataset comes from a large set of im-
ages of the proceedings of the Old Bailey, a crimi-
nal court in London, England (Shoemaker, 2005).
The Old Bailey curatorial effort, after deciding
that current OCR systems do not adequately han-
dle 18th century fonts, manually transcribed the
documents into text. We will use these manual
transcriptions to evaluate the output of our system.
From the Old Bailey proceedings, we extracted a
set of 20 images, each consisting of 30 lines of
text to use as our first test set. We picked 20 doc-
uments, printed in consecutive decades. The first
document is from 1715 and the last is from 1905.
We choose the first document in each of the corre-
sponding years, choose a random page in the doc-
ument, and extracted an image of the first 30 con-
secutive lines of text consisting of full sentences.5
The ten documents in the Old Bailey dataset that
were printed before 1810 use the long s glyph,
while the remaining ten do not.
5.2 Trove
Our second dataset is taken from a collection of
digitized Australian newspapers that were printed
between the years of 1803 and 1954. This col-
lection is called Trove, and is maintained by the
the National Library of Australia (Holley, 2010).
We extracted ten images from this collection in the
same way that we extracted images from Old Bai-
ley, but starting from the year 1803. We manually
produced our own gold annotations for these ten
images. Only the first document of Trove uses the
long s glyph.
5.3 Pre-processing
Many of the images in historical collections are
bitonal (binary) as a result of how they were cap-
tured on microfilm for storage in the 1980s (Arl-
itsch and Herbert, 2004). This is part of the reason
our model is designed to work directly with bi-
narized images. For consistency, we binarized the
images in our test sets that were not already binary
by thresholding pixel values.
Our model requires that the image be pre-
segmented into lines of text. We automatically
segment lines by training an HSMM over rows of
pixels. After the lines are segmented, each line
is resampled so that its vertical resolution is 30
pixels. The line extraction process also identifies
pixels that are not located in central text regions,
and are part of large connected components of ink,
spanning multiple lines. The values of such pixels
are treated as unobserved in the model since, more
often than not, they are part of ink blotches.
5This ruled out portions of the document with extreme
structural abnormalities, like title pages and lists. These
might be interesting to model, but are not within the scope
of this paper.
212
6 Experiments
We evaluate our system by comparing our text
recognition accuracy to that of two state-of-the-art
systems.
6.1 Baselines
Our first baseline is Google?s open source OCR
system, Tesseract (Smith, 2007). Tesseract takes
a pipelined approach to recognition. Before rec-
ognizing the text, the document is broken into
lines, and each line is segmented into words.
Then, Tesseract uses a classifier, aided by a word-
unigram language model, to recognize whole
words.
Our second baseline, ABBYY FineReader 11
Professional Edition,6 is a state-of-the-art com-
mercial OCR system. It is the OCR system that
the National Library of Australia used to recognize
the historical documents in Trove (Holley, 2010).
6.2 Evaluation
We evaluate the output of our system and the base-
line systems using two metrics: character error
rate (CER) and word error rate (WER). Both these
metrics are based on edit distance. CER is the edit
distance between the predicted and gold transcrip-
tions of the document, divided by the number of
characters in the gold transcription. WER is the
word-level edit distance (words, instead of char-
acters, are treated as tokens) between predicted
and gold transcriptions, divided by the number of
words in the gold transcription. When computing
WER, text is tokenized into words by splitting on
whitespace.
6.3 Language Model
We ran experiments using two different language
models. The first language model was trained
on the initial one million sentences of the New
York Times (NYT) portion of the Gigaword cor-
pus (Graff et al, 2007), which contains about 36
million words. This language model is out of do-
main for our experimental documents. To inves-
tigate the effects of using an in domain language
model, we created a corpus composed of the man-
ual annotations of all the documents in the Old
Bailey proceedings, excluding those used in our
test set. This corpus consists of approximately 32
million words. In all experiments we used a char-
acter n-gram order of six for the final Viterbi de-
6http://www.abbyy.com
System CER WER
Old Bailey
Google Tesseract 29.6 54.8
ABBYY FineReader 15.1 40.0
Ocular w/ NYT (this work) 12.6 28.1
Ocular w/ OB (this work) 9.7 24.1
Trove
Google Tesseract 37.5 59.3
ABBYY FineReader 22.9 49.2
Ocular w/ NYT (this work) 14.9 33.0
Table 1: We evaluate the predicted transcriptions in terms of
both character error rate (CER) and word error rate (WER),
and report macro-averages across documents. We compare
with two baseline systems: Google?s open source OCR sys-
tem, Tessearact, and a state-of-the-art commercial system,
ABBYY FineReader. We refer to our system as Ocular w/
NYT and Ocular w/ OB, depending on whether NYT or Old
Bailey is used to train the language model.
coding pass and an order of three for all coarse
passes.
6.4 Initialization and Tuning
We used as a development set ten additional docu-
ments from the Old Bailey proceedings and five
additional documents from Trove that were not
part of our test set. On this data, we tuned the
model?s hyperparameters7 and the parameters of
the pruning schedule for our coarse-to-fine ap-
proach.
In experiments we initialized ?RPADc and ?LPADc to
be uniform, and initialized ?GLYPHc and ?c based
on the standard modern fonts included with the
Ubuntu Linux 12.04 distribution.8 For documents
that use the long s glyph, we introduce a special
character type for the non-word-final s, and ini-
tialize its parameters from a mixture of the modern
f and | glyphs.9
7 Results and Analysis
The results of our experiments are summarized in
Table 1. We refer to our system as Ocular w/
NYT or Ocular w/ OB, depending on whether the
language model was trained using NYT or Old
Bailey, respectively. We compute macro-averages
7One of the hyperparameters we tune is the exponent of
the language model. This balances the contributions of the
language model and the typesetting model to the posterior
(Och and Ney, 2004).
8http://www.ubuntu.com/
9Following Berg-Kirkpatrick et al (2010), we use a reg-
ularization term in the optimization of the log-linear model
parameters ?c during the M-step. Instead of regularizing to-
wards zero, we regularize towards the initializer. This slightly
improves performance on our development set and can be
thought of as placing a prior on the glyph shape parameters.
213
(c) Trove, 1883:
(b) Old Bailey, 1885:
(a) Old Bailey, 1775: the prisoner at the bar. Jacob Lazarus and his
taken ill and taken away ? I remember
how the murderers came to learn the nation in
Predicted text:
Predicted typesetting:
Image:
Predicted text:
Predicted typesetting:
Image:
Predicted text:
Predicted typesetting:
Image:
Figure 7: For each of these portions of test documents, the first line shows the transcription predicted by our model and the
second line shows a representation of the learned typesetting layout. The grayscale glyphs show the Bernoulli pixel distributions
learned by our model, while the padding regions are depicted in blue. The third line shows the input image.
across documents from all years. Our system, us-
ing the NYT language model, achieves an average
WER of 28.1 on Old Bailey and an average WER
of 33.0 on Trove. This represents a substantial er-
ror reduction compared to both baseline systems.
If we average over the documents in both Old
Bailey and Trove, we find that Tesseract achieved
an average WER of 56.3, ABBYY FineReader
achieved an average WER of 43.1, and our system,
using the NYT language model, achieved an aver-
age WER of 29.7. This means that while Tesseract
incorrectly predicts more than half of the words in
these documents, our system gets more than three-
quarters of them right. Overall, we achieve a rela-
tive reduction in WER of 47% compared to Tesser-
act and 31% compared to ABBYY FineReader.
The baseline systems do not have special pro-
visions for the long s glyph. In order to make
sure the comparison is fair, we separately com-
puted average WER on only the documents from
after 1810 (which do no use the long s glyph). We
found that using this evaluation our system actu-
ally acheives a larger relative reduction in WER:
50% compared to Tesseract and 35% compared to
ABBYY FineReader.
Finally, if we train the language model using
the Old Bailey corpus instead of the NYT corpus,
we see an average improvement of 4 WER on the
Old Bailey test set. This means that the domain of
the language model is important, but, the results
are not affected drastically even when using a lan-
guage model based on modern corpora (NYT).
7.1 Learned Typesetting Layout
Figure 7 shows a representation of the typesetting
layout learned by our model for portions of several
Initializer
1700
1740
1780 1820
1860
1900
Figure 8: The central glyph is a representation of the initial
model parameters for the glyph shape for g, and surrounding
this are the learned parameters for documents from various
years.
test documents. For each portion of a test doc-
ument, the first line shows the transcription pre-
dicted by our model, and the second line shows
padding and glyph regions predicted by the model,
where the grayscale glyphs represent the learned
Bernoulli parameters for each pixel. The third line
shows the input image.
Figure 7a demonstrates a case where our model
has effectively explained both the uneven baseline
and over-inked glyphs by using the vertical offsets
vi and inking variables di. In Figure 7b the model
has used glyph widths gi and vertical offsets to ex-
plain the thinning of glyphs and falling baseline
that occurred near the binding of the book. In sep-
arate experiments on the Old Bailey test set, using
the NYT language model, we found that remov-
ing the vertical offset variables from the model in-
creased WER by 22, and removing the inking vari-
ables increased WER by 16. This indicates that it
is very important to model both these aspects of
printing press rendering.
214
Figure 9: This Old Bailey document from 1719 has severe ink bleeding from the facing page. We annotated these blotches (in
red) and treated the corresponding pixels as unobserved in the model. The layout shown is predicted by the model.
Figure 7c shows the output of our system on
a difficult document. Here, missing characters
and ink blotches confuse the model, which picks
something that is reasonable according to the lan-
guage model, but incorrect.
7.2 Learned Fonts
It is interesting to look at the fonts learned by our
system, and track how historical fonts changed
over time. Figure 8 shows several grayscale im-
ages representing the Bernoulli pixel probabilities
for the most likely width of the glyph for g under
various conditions. At the center is the representa-
tion of the initial parameter values, and surround-
ing this are the learned parameters for documents
from various years. The learned shapes are visibly
different from the initializer, which is essentially
an average of modern fonts, and also vary across
decades.
We can ask to what extent learning the font
structure actually improved our performance. If
we turn off learning and just use the initial pa-
rameters to decode, WER increases by 8 on the
Old Bailey test set when using the NYT language
model.
7.3 Unobserved Ink Blotches
As noted earlier, one strength of our generative
model is that we can make the values of certain
pixels unobserved in the model, and let inference
fill them in. We conducted an additional experi-
ment on a document from the Old Bailey proceed-
ings that was printed in 1719. This document, a
fragment of which is shown in Figure 9, has se-
vere ink bleeding from the facing page. We manu-
ally annotated the ink blotches (shown in red), and
made them unobserved in the model. The result-
ing typesetting layout learned by the model is also
shown in Figure 9. The model correctly predicted
most of the obscured words. Running the model
with the manually specified unobserved pixels re-
duced the WER on this document from 58 to 19
when using the NYT language model.
7.4 Remaining Errors
We performed error analysis on our development
set by randomly choosing 100 word errors from
the WER alignment and manually annotating them
with relevant features. Specifically, for each word
error we recorded whether or not the error con-
tained punctuation (either in the predicted word or
the gold word), whether the text in the correspond-
ing portion of the original image was italicized,
and whether the corresponding portion of the im-
age exhibited over-inking, missing ink, or signif-
icant ink blotches. These last three feature types
are subjective in nature but may still be informa-
tive. We found that 56% of errors were accompa-
nied by over-inking, 50% of errors were accom-
panied by ink blotches, 42% of errors contained
punctuation, 21% of errors showed missing ink,
and 12% of errors contained text that was itali-
cized in the original image.
Our own subjective assessment indicates that
many of these error features are in fact causal.
More often than not, italicized text is incorrectly
transcribed. In cases of extreme ink blotching,
or large areas of missing ink, the system usually
makes an error.
8 Conclusion
We have demonstrated a model, based on the his-
torical typesetting process, that effectively learns
font structure in an unsupervised fashion to im-
prove transcription of historical documents into
text. The parameters of the learned fonts are inter-
pretable, as are the predicted typesetting layouts.
Our system achieves state-of-the-art results, sig-
nificantly outperforming two state-of-the-art base-
line systems.
215
References
Kenning Arlitsch and John Herbert. 2004. Microfilm,
paper, and OCR: Issues in newspaper digitization.
the Utah digital newspapers program. Microform &
Imaging Review.
Taylor Berg-Kirkpatrick and Dan Klein. 2011. Simple
effective decipherment via combinatorial optimiza-
tion. In Proceedings of the 2011 Conference on Em-
pirical Methods in Natural Language Processing.
Taylor Berg-Kirkpatrick, Alexandre Bouchard-Co?te?,
John DeNero, and Dan Klein. 2010. Painless un-
supervised learning with features. In Proceedings
of the 2010 Annual Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies:.
Arthur Dempster, Nan Laird, and Donald Rubin. 1977.
Maximum likelihood from incomplete data via the
EM algorithm. Journal of the Royal Statistical So-
ciety.
David Graff, Junbo Kong, Ke Chen, and Kazuaki
Maeda. 2007. English Gigaword third edi-
tion. Linguistic Data Consortium, Catalog Number
LDC2007T07.
Tin Kam Ho and George Nagy. 2000. OCR with no
shape training. In Proceedings of the 15th Interna-
tional Conference on Pattern Recognition.
Rose Holley. 2010. Trove: Innovation in access to
information in Australia. Ariadne.
Gary Huang, Erik G Learned-Miller, and Andrew Mc-
Callum. 2006. Cryptogram decoding for optical
character recognition. University of Massachusetts-
Amherst Technical Report.
Fred Jelinek. 1998. Statistical methods for speech
recognition. MIT press.
Andrew Kae and Erik Learned-Miller. 2009. Learn-
ing on the fly: font-free approaches to difficult OCR
problems. In Proceedings of the 2009 International
Conference on Document Analysis and Recognition.
Andrew Kae, Gary Huang, Carl Doersch, and Erik
Learned-Miller. 2010. Improving state-of-the-
art OCR through high-precision document-specific
modeling. In Proceedings of the 2010 IEEE Confer-
ence on Computer Vision and Pattern Recognition.
Vladimir Kluzner, Asaf Tzadok, Yuval Shimony, Eu-
gene Walach, and Apostolos Antonacopoulos. 2009.
Word-based adaptive OCR for historical books. In
Proceedings of the 2009 International Conference
on on Document Analysis and Recognition.
Vladimir Kluzner, Asaf Tzadok, Dan Chevion, and Eu-
gene Walach. 2011. Hybrid approach to adaptive
OCR for historical books. In Proceedings of the
2011 International Conference on Document Anal-
ysis and Recognition.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of the International Conference on Acous-
tics, Speech, and Signal Processing.
Philipp Koehn. 2004. Pharaoh: a beam search de-
coder for phrase-based statistical machine transla-
tion models. Machine translation: From real users
to research.
Okan Kolak, William Byrne, and Philip Resnik. 2003.
A generative probabilistic OCR model for NLP ap-
plications. In Proceedings of the 2003 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies.
Gary Kopec and Mauricio Lomelin. 1996. Document-
specific character template estimation. In Proceed-
ings of the International Society for Optics and Pho-
tonics.
Gary Kopec, Maya Said, and Kris Popat. 2001. N-
gram language models for document image decod-
ing. In Proceedings of Society of Photographic In-
strumentation Engineers.
Stephen Levinson. 1986. Continuously variable du-
ration hidden Markov models for automatic speech
recognition. Computer Speech & Language.
Dong C Liu and Jorge Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical programming.
Franz Och and Hermann Ney. 2004. The alignment
template approach to statistical machine translation.
Computational Linguistics.
Slav Petrov, Aria Haghighi, and Dan Klein. 2008.
Coarse-to-fine syntactic machine translation using
language projections. In Proceedings of the 2008
Conference on Empirical Methods in Natural Lan-
guage Processing.
Sujith Ravi and Kevin Knight. 2008. Attacking de-
cipherment problems optimally with low-order n-
gram models. In Proceedings of the 2008 Confer-
ence on Empirical Methods in Natural Language
Processing.
Sujith Ravi and Kevin Knight. 2011. Bayesian infer-
ence for Zodiac and other homophonic ciphers. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies.
Robert Shoemaker. 2005. Digital London: Creating a
searchable web of interlinked sources on eighteenth
century London. Electronic Library and Informa-
tion Systems.
Ray Smith. 2007. An overview of the tesseract ocr
engine. In Proceedings of the Ninth International
Conference on Document Analysis and Recognition.
216
Benjamin Snyder, Regina Barzilay, and Kevin Knight.
2010. A statistical model for lost language decipher-
ment. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics.
Georgios Vamvakas, Basilios Gatos, Nikolaos Stam-
atopoulos, and Stavros Perantonis. 2008. A com-
plete optical character recognition methodology for
historical documents. In The Eighth IAPR Interna-
tional Workshop on Document Analysis Systems.
Hao Zhang and Daniel Gildea. 2008. Efficient multi-
pass decoding for synchronous context free gram-
mars. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Process-
ing.
217
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 228?237,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Less Grammar, More Features
David Hall Greg Durrett Dan Klein
Computer Science Division
University of California, Berkeley
{dlwh,gdurrett,klein}@cs.berkeley.edu
Abstract
We present a parser that relies primar-
ily on extracting information directly from
surface spans rather than on propagat-
ing information through enriched gram-
mar structure. For example, instead of cre-
ating separate grammar symbols to mark
the definiteness of an NP, our parser might
instead capture the same information from
the first word of the NP. Moving context
out of the grammar and onto surface fea-
tures can greatly simplify the structural
component of the parser: because so many
deep syntactic cues have surface reflexes,
our system can still parse accurately with
context-free backbones as minimal as X-
bar grammars. Keeping the structural
backbone simple and moving features to
the surface also allows easy adaptation
to new languages and even to new tasks.
On the SPMRL 2013 multilingual con-
stituency parsing shared task (Seddah et
al., 2013), our system outperforms the top
single parser system of Bj?orkelund et al
(2013) on a range of languages. In addi-
tion, despite being designed for syntactic
analysis, our system also achieves state-
of-the-art numbers on the structural senti-
ment task of Socher et al (2013). Finally,
we show that, in both syntactic parsing and
sentiment analysis, many broad linguistic
trends can be captured via surface features.
1 Introduction
Na??ve context-free grammars, such as those em-
bodied by standard treebank annotations, do not
parse well because their symbols have too little
context to constrain their syntactic behavior. For
example, to PPs usually attach to verbs and of
PPs usually attach to nouns, but a context-free PP
symbol can equally well attach to either. Much
of the last few decades of parsing research has
therefore focused on propagating contextual in-
formation from the leaves of the tree to inter-
nal nodes. For example, head lexicalization (Eis-
ner, 1996; Collins, 1997; Charniak, 1997), struc-
tural annotation (Johnson, 1998; Klein and Man-
ning, 2003), and state-splitting (Matsuzaki et al,
2005; Petrov et al, 2006) are all designed to take
coarse symbols like PP and decorate them with
additional context. The underlying reason that
such propagation is even needed is that PCFG
parsers score trees based on local configurations
only, and any information that is not threaded
through the tree becomes inaccessible to the scor-
ing function. There have been non-local ap-
proaches as well, such as tree-substitution parsers
(Bod, 1993; Sima?an, 2000), neural net parsers
(Henderson, 2003), and rerankers (Collins and
Koo, 2005; Charniak and Johnson, 2005; Huang,
2008). These non-local approaches can actually
go even further in enriching the grammar?s struc-
tural complexity by coupling larger domains in
various ways, though their non-locality generally
complicates inference.
In this work, we instead try to minimize the
structural complexity of the grammar by moving
as much context as possible onto local surface fea-
tures. We examine the position that grammars
should not propagate any information that is avail-
able from surface strings, since a discriminative
parser can access that information directly. We
therefore begin with a minimal grammar and it-
eratively augment it with rich input features that
do not enrich the context-free backbone. Previ-
ous work has also used surface features in their
parsers, but the focus has been on machine learn-
ing methods (Taskar et al, 2004), latent annota-
tions (Petrov and Klein, 2008a; Petrov and Klein,
2008b), or implementation (Finkel et al, 2008).
By contrast, we investigate the extent to which
228
we need a grammar at all. As a thought experi-
ment, consider a parser with no grammar, which
functions by independently classifying each span
(i, j) of a sentence as an NP, VP, and so on, or
null if that span is a non-constituent. For exam-
ple, spans that begin with the might tend to be
NPs, while spans that end with of might tend to
be non-constituents. An independent classification
approach is actually very viable for part-of-speech
tagging (Toutanova et al, 2003), but is problem-
atic for parsing ? if nothing else, parsing comes
with a structural requirement that the output be a
well-formed, nested tree. Our parser uses a min-
imal PCFG backbone grammar to ensure a ba-
sic level of structural well-formedness, but relies
mostly on features of surface spans to drive accu-
racy. Formally, our model is a CRF where the fea-
tures factor over anchored rules of a small back-
bone grammar, as shown in Figure 1.
Some aspects of the parsing problem, such as
the tree constraint, are clearly best captured by a
PCFG. Others, such as heaviness effects, are nat-
urally captured using surface information. The
open question is whether surface features are ade-
quate for key effects like subcategorization, which
have deep definitions but regular surface reflexes
(e.g. the preposition selected by a verb will often
linearly follow it). Empirically, the answer seems
to be yes, and our system produces strong results,
e.g. up to 90.5 F1 on English parsing. Our parser
is also able to generalize well across languages
with little tuning: it achieves state-of-the-art re-
sults on multilingual parsing, scoring higher than
the best single-parser system from the SPMRL
2013 Shared Task on a range of languages, as well
as on the competition?s average F1 metric.
One advantage of a system that relies on surface
features and a simple grammar is that it is portable
not only across languages but also across tasks
to an extent. For example, Socher et al (2013)
demonstrates that sentiment analysis, which is
usually approached as a flat classification task,
can be viewed as tree-structured. In their work,
they propagate real-valued vectors up a tree using
neural tensor nets and see gains from their recur-
sive approach. Our parser can be easily adapted
to this task by replacing the X-bar grammar over
treebank symbols with a grammar over the sen-
timent values to encode the output variables and
then adding n-gram indicators to our feature set
to capture the bulk of the lexical effects. When
applied to this task, our system generally matches
their accuracy overall and is able to outperform it
on the overall sentence-level subtask.
2 Parsing Model
In order to exploit non-independent surface fea-
tures of the input, we use a discriminative formula-
tion. Our model is a conditional random field (Laf-
ferty et al, 2001) over trees, in the same vein as
Finkel et al (2008) and Petrov and Klein (2008a).
Formally, we define the probability of a tree T
conditioned on a sentence w as
p(T |w) ? exp
(
?
?
?
r?T
f(r,w)
)
(1)
where the feature domains r range over the (an-
chored) rules used in the tree. An anchored rule
r is the conjunction of an unanchored grammar
rule rule(r) and the start, stop, and split indexes
where that rule is anchored, which we refer to as
span(r). It is important to note that the richness of
the backbone grammar is reflected in the structure
of the trees T , while the features that condition di-
rectly on the input enter the equation through the
anchoring span(r). To optimize model parame-
ters, we use the Adagrad algorithm of Duchi et al
(2010) with L2 regularization.
We start with a simple X-bar grammar whose
only symbols are NP, NP-bar, VP, and so on. Our
base model has no surface features: formally, on
each anchored rule r we have only an indicator of
the (unanchored) rule identity, rule(r). Because
the X-bar grammar is so minimal, this grammar
does not parse very accurately, scoring just 73 F1
on the standard English Penn Treebank task.
In past work that has used tree-structured CRFs
in this way, increased accuracy partially came
from decorating trees T with additional annota-
tions, giving a tree T
?
over a more complex symbol
set. These annotations introduce additional con-
text into the model, usually capturing linguistic in-
tuition about the factors that influence grammati-
cality. For instance, we might annotate every con-
stituent X in the tree with its parent Y , giving a
tree with symbolsX[?Y ]. Finkel et al (2008) used
parent annotation, head tag annotation, and hori-
zontal sibling annotation together in a single large
grammar. In Petrov and Klein (2008a) and Petrov
and Klein (2008b), these annotations were latent;
they were inferred automatically during training.
229
Hall and Klein (2012) employed both kinds of an-
notations, along with lexicalized head word anno-
tation. All of these past CRF parsers do also ex-
ploit span features, as did the structured margin
parser of Taskar et al (2004); the current work pri-
marily differs in shifting the work from the gram-
mar to the surface features.
The problem with rich annotations is that they
increase the state space of the grammar substan-
tially. For example, adding parent annotation can
square the number of symbols, and each subse-
quent annotation causes a multiplicative increase
in the size of the state space. Hall and Klein
(2012) attempted to reduce this state space by fac-
toring these annotations into individual compo-
nents. Their approach changed the multiplicative
penalty of annotation into an additive penalty, but
even so their individual grammar projections are
much larger than the base X-bar grammar.
In this work, we want to see how much of the
expressive capability of annotations can be cap-
tured using surface evidence, with little or no an-
notation of the underlying grammar. To that end,
we avoid annotating our trees at all, opting instead
to see how far simple surface features will go in
achieving a high-performance parser. We will re-
turn to the question of annotation in Section 5.
3 Surface Feature Framework
To improve the performance of our X-bar gram-
mar, we will add a number of surface feature tem-
plates derived only from the words in the sentence.
We say that an indicator is a surface property if
it can be extracted without reference to the parse
tree. These features can be implemented with-
out reference to structured linguistic notions like
headedness; however, we will argue that they still
capture a wide range of linguistic phenomena in a
data-driven way.
Throughout this and the following section, we
will draw on motivating examples from the En-
glish Penn Treebank, though similar examples
could be equally argued for other languages. For
performance on other languages, see Section 6.
Recall that our CRF factors over anchored rules
r, where each r has identity rule(r) and anchor-
ing span(r). The X-bar grammar has only indi-
cators of rule(r), ignoring the anchoring. Let a
surface property of r be an indicator function of
span(r) and the sentence itself. For example, the
first word in a constituent is a surface property, as
averted    financial    disaster
VP
NP
VBD
JJ NN
PARENT = VP
FIRSTWORD = averted
LENGTH = 3
RULE = VP ? VBD NP
PARENT = VP
Span properties
Rule backoffs
Features
...
5 6
7
8
...
LASTWORD = disaster
?
FIRSTWORD = averted
LASTWORD = disaster PARENT = VP
?
?
FIRSTWORD = averted
RULE = VP ? VBD NP
Figure 1: Features computed over the application
of the rule VP ? VBD NP over the anchored
span averted financial disaster with the shown in-
dices. Span properties are generated as described
throughout Section 4; they are then conjoined with
the rule and just the parent nonterminal to give the
features fired over the anchored production.
is the word directly preceding the constituent. As
illustrated in Figure 1, the actual features of the
model are obtained by conjoining surface proper-
ties with various abstractions of the rule identity.
For rule abstractions, we use two templates: the
parent of the rule and the identity of the rule. The
surface features are somewhat more involved, and
so we introduce them incrementally.
One immediate computational and statistical is-
sue arises from the sheer number of possible sur-
face features. There are a great number of spans
in a typical treebank; extracting features for ev-
ery possible combination of span and rule is pro-
hibitive. One simple solution is to only extract
features for rule/span pairs that are actually ob-
served in gold annotated examples during train-
ing. Because these ?positive? features correspond
to observed constituents, they are far less numer-
ous than the set of all possible features extracted
from all spans. As far as we can tell, all past CRF
parsers have used ?positive? features only.
However, negative features?features that are
not observed in any tree?are still powerful indica-
tors of (un)grammaticality: if we have never seen
a PRN that starts with ?has,? or a span that be-
gins with a quotation mark and ends with a close
bracket, then we would like the model to be able to
place negative weights on these features. Thus, we
use a simple feature hashing scheme where posi-
tive features are indexed individually, while nega-
230
Features Section F1
RULE 4 73.0
+ SPAN FIRST WORD + SPAN LAST WORD + LENGTH 4.1 85.0
+ WORD BEFORE SPAN + WORD AFTER SPAN 4.2 89.0
+ WORD BEFORE SPLIT + WORD AFTER SPLIT 4.3 89.7
+ SPAN SHAPE 4.4 89.9
Table 1: Results for the Penn Treebank development set, reported in F1 on sentences of length ? 40
on Section 22, for a number of incrementally growing feature sets. We show that each feature type
presented in Section 4 adds benefit over the previous, and in combination they produce a reasonably
good yet simple parser.
tive features are bucketed together. During train-
ing there are no collisions between positive fea-
tures, which generally receive positive weight, and
negative features, which generally receive nega-
tive weight; only negative features can collide.
Early experiments indicated that using a number
of negative buckets equal to the number of posi-
tive features was effective.
4 Features
Our goal is to use surface features to replicate
the functionality of other annotations, without in-
creasing the state space of our grammar, meaning
that the rules rule(r) remain simple, as does the
state space used during inference.
Before we present our main features, we briefly
discuss the issue of feature sparsity. While lexical
features are a powerful driver of our parser, firing
features on rare words would allow it to overfit the
training data quite heavily. To that end, for the
purposes of computing our features, a word is rep-
resented by its longest suffix that occurs 100 or
more times in the training data (which will be the
entire word, for common words).
1
Table 1 shows the results of incrementally
building up our feature set on the Penn Treebank
development set. RULE specifies that we use only
indicators on rule identity for binary production
and nonterminal unaries. For this experiment and
all others, we include a basic set of lexicon fea-
tures, i.e. features on preterminal part-of-speech
tags. A given preterminal unary at position i in
the sentence includes features on the words (suf-
fixes) at position i ? 1, i, and i + 1. Because the
lexicon is especially sensitive to morphological ef-
fects, we also fire features on all prefixes and suf-
1
Experiments with the Brown clusters (Brown et al,
1992) provided by Turian et al (2010) in lieu of suffixes were
not promising. Moreover, lowering this threshold did not im-
prove performance.
fixes of the current word up to length 5, regardless
of frequency.
Subsequent lines in Table 1 indicate additional
surface feature templates computed over the span,
which are then conjoined with the rule identity as
shown in Figure 1 to give additional features. In
the rest of the section, we describe the features of
this type that we use. Note that many of these fea-
tures have been used before (Taskar et al, 2004;
Finkel et al, 2008; Petrov and Klein, 2008b); our
goal here is not to amass as many feature tem-
plates as possible, but rather to examine the ex-
tent to which a simple set of features can replace a
complicated state space.
4.1 Basic Span Features
We start with some of the most obvious proper-
ties available to us, namely, the identity of the first
and last words of a span. Because heads of con-
stituents are often at the beginning or the end of
a span, these feature templates can (noisily) cap-
ture monolexical properties of heads without hav-
ing to incur the inferential cost of lexicalized an-
notations. For example, in English, the syntactic
head of a verb phrase is typically at the beginning
of the span, while the head of a simple noun phrase
is the last word. Other languages, like Korean or
Japanese, are more consistently head final.
Structural contexts like those captured by par-
ent annotation (Johnson, 1998) are more subtle.
Parent annotation can capture, for instance, the
difference in distribution in NPs that have S as a
parent (that is, subjects) and NPs under VPs (ob-
jects). We try to capture some of this same intu-
ition by introducing a feature on the length of a
span. For instance, VPs embedded in NPs tend
to be short, usually as embedded gerund phrases.
Because constituents in the treebank can be quite
long, we bin our length features into 8 buckets, of
231
no  read  messages  in  his  inbox
VP
VBP NNS
VP ? no VBP NNS
Figure 2: An example showing the utility of span
context. The ambiguity about whether read is an
adjective or a verb is resolved when we construct
a VP and notice that the word proceeding it is un-
likely.
has  an  impact  on  the  market
PPNP
NP
NP ? (NP ... impact) PP)
Figure 3: An example showing split point features
disambiguating a PP attachment. Because impact
is likely to take a PP, the monolexical indicator
feature that conjoins impact with the appropriate
rule will help us parse this example correctly.
lengths 1, 2, 3, 4, 5, 10, 20, and ?21 words.
Adding these simple features (first word, last
word, and lengths) as span features of the X-
bar grammar already gives us a substantial im-
provement over our baseline system, improving
the parser?s performance from 73.0 F1 to 85.0 F1
(see Table 1).
4.2 Span Context Features
Of course, there is no reason why we should con-
fine ourselves to just the words within the span:
words outside the span also provide a rich source
of context. As an example, consider disambiguat-
ing the POS tag of the word read in Figure 2. A
VP is most frequently preceded by a subject NP,
whose rightmost word is often its head. Therefore,
we fire features that (separately) look at the words
immediately preceding and immediately follow-
ing the span.
4.3 Split Point Features
Another important source of features are the words
at and around the split point of a binary rule ap-
plication. Figure 3 shows an example of one in-
(  CEO  of  Enron  )
PRN
(XxX)
     said  ,  ?  Too  bad  ,  ?
VP
x,?Xx,?
Figure 4: Computation of span shape features on
two examples. Parentheticals, quotes, and other
punctuation-heavy, short constituents benefit from
being explicitly modeled by a descriptor like this.
stance of this feature template. impact is a noun
that is more likely to take a PP than other nouns,
and so we expect this feature to have high weight
and encourage the attachment; this feature proves
generally useful in resolving such cases of right-
attachments to noun phrases, since the last word
of the noun phrase is often the head. As another
example, coordination can be represented by an
indicator of the conjunction, which comes imme-
diately after the split point. Finally, control struc-
tures with infinitival complements can be captured
with a rule S? NP VP with the word ?to? at the
split point.
4.4 Span Shape Features
We add one final feature characterizing the span,
which we call span shape. Figure 4 shows how this
feature is computed. For each word in the span,
2
we indicate whether that word begins with a cap-
ital letter, lowercase letter, digit, or punctuation
mark. If it begins with punctuation, we indicate
the punctuation mark explicitly. Figure 4 shows
that this is especially useful in characterizing con-
structions such as parentheticals and quoted ex-
pressions. Because this feature indicates capital-
ization, it can also capture properties of NP in-
ternal structure relevant to named entities, and its
sensitivity to capitalization and punctuation makes
it useful for recognizing appositive constructions.
5 Annotations
We have built up a strong set of features by this
point, but have not yet answered the question of
whether or not grammar annotation is useful on
top of them. In this section, we examine two of the
most commonly used types of additional annota-
tion, structural annotation, and lexical annotation.
2
For longer spans, we only use words sufficiently close to
the span?s beginning and end.
232
Annotation Dev, len ? 40
v = 0, h = 0 90.1
v = 1, h = 0 90.5
v = 0, h = 1 90.2
v = 1, h = 1 90.9
Lexicalized 90.3
Table 2: Results for the Penn Treebank develop-
ment set, sentences of length ? 40, for different
annotation schemes implemented on top of the X-
bar grammar.
Recall from Section 3 that every span feature is
conjoined with indicators over rules and rule par-
ents to produce features over anchored rule pro-
ductions; when we consider adding an annotation
layer to the grammar, what that does is refine the
rule indicators that are conjoined with every span
feature. While this is a powerful way of refining
features, we show that common successful anno-
tation schemes provide at best modest benefit on
top of the base parser.
5.1 Structural Annotation
The most basic, well-understood kind of annota-
tion on top of an X-bar grammar is structural an-
notation, which annotates each nonterminal with
properties of its environment (Johnson, 1998;
Klein and Manning, 2003). This includes vertical
annotation (parent, grandparent, etc.) as well as
horizontal annotation (only partially Markovizing
rules as opposed to using an X-bar grammar).
Table 2 shows the performance of our feature
set in grammars with several different levels of
structural annotation.
3
Klein and Manning (2003)
find large gains (6% absolute improvement, 20%
relative improvement) going from v = 0, h = 0 to
v = 1, h = 1; however, we do not find the same
level of benefit. To the extent that our parser needs
to make use of extra information in order to ap-
ply a rule correctly, simply inspecting the input to
determine this information appears to be almost
as effective as relying on information threaded
through the parser.
In Section 6 and Section 7, we use v = 1 and
h = 0; we find that v = 1 provides a small, reli-
able improvement across a range of languages and
tasks, whereas other annotations are less clearly
beneficial.
3
We use v = 0 to indicate no annotation, diverging from
the notation in Klein and Manning (2003).
Test ? 40 Test all
Berkeley 90.6 90.1
This work 89.9 89.2
Table 3: Final Parseval results for the v = 1, h = 0
parser on Section 23 of the Penn Treebank.
5.2 Lexical Annotation
Another commonly-used kind of structural an-
notation is lexicalization (Eisner, 1996; Collins,
1997; Charniak, 1997). By annotating grammar
nonterminals with their headwords, the idea is to
better model phenomena that depend heavily on
the semantics of the words involved, such as coor-
dination and PP attachment.
Table 2 shows results from lexicalizing the X-
bar grammar; it provides meager improvements.
One probable reason for this is that our parser al-
ready includes monolexical features that inspect
the first and last words of each span, which cap-
tures the syntactic or the semantic head in many
cases or can otherwise provide information about
what the constituent?s type may be and how it is
likely to combine. Lexicalization allows us to cap-
ture bilexical relationships along dependency arcs,
but it has been previously shown that these add
only marginal benefit to Collins?s model anyway
(Gildea, 2001).
5.3 English Evaluation
Finally, Table 3 shows our final evaluation on Sec-
tion 23 of the Penn Treebank. We use the v =
1, h = 0 grammar. While we do not do as well as
the Berkeley parser, we will see in Section 6 that
our parser does a substantially better job of gener-
alizing to other languages.
6 Other Languages
Historically, many annotation schemes for parsers
have required language-specific engineering: for
example, lexicalized parsers require a set of head
rules and manually-annotated grammars require
detailed analysis of the treebank itself (Klein and
Manning, 2003). A key strength of a parser that
does not rely heavily on an annotated grammar is
that it may be more portable to other languages.
We show that this is indeed the case: on nine lan-
guages, our system is competitive with or better
than the Berkeley parser, which is the best single
233
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish Avg
Dev, all lengths
Berkeley 78.24 69.17 79.74 81.74 87.83 83.90 70.97 84.11 74.50 78.91
Berkeley-Rep 78.70 84.33 79.68 82.74 89.55 89.08 82.84 87.12 75.52 83.28
Our work 78.89 83.74 79.40 83.28 88.06 87.44 81.85 91.10 75.95 83.30
Test, all lengths
Berkeley 79.19 70.50 80.38 78.30 86.96 81.62 71.42 79.23 79.18 78.53
Berkeley-Tags 78.66 74.74 79.76 78.28 85.42 85.22 78.56 86.75 80.64 80.89
Our work 78.75 83.39 79.70 78.43 87.18 88.25 80.18 90.66 82.00 83.17
Table 4: Results for the nine treebanks in the SPMRL 2013 Shared Task; all values are F-scores for
sentences of all lengths using the version of evalb distributed with the shared task. Berkeley-Rep is
the best single parser from (Bj?orkelund et al, 2013); we only compare to this parser on the development
set because neither the system nor test set values are publicly available. Berkeley-Tags is a version of
the Berkeley parser run by the task organizers where tags are provided to the model, and is the best
single parser submitted to the official task. In both cases, we match or outperform the baseline parsers in
aggregate and on the majority of individual languages.
parser
4
for the majority of cases we consider.
We evaluate on the constituency treebanks from
the Statistical Parsing of Morphologically Rich
Languages Shared Task (Seddah et al, 2013).
We compare to the Berkeley parser (Petrov and
Klein, 2007) as well as two variants. First,
we use the ?Replaced? system of Bj?orkelund et
al. (2013) (Berkeley-Rep), which is their best
single parser.
5
The ?Replaced? system modi-
fies the Berkeley parser by replacing rare words
with morphological descriptors of those words
computed using language-specific modules, which
have been hand-crafted for individual languages
or are trained with additional annotation layers
in the treebanks that we do not exploit. Unfor-
tunately, Bj?orkelund et al (2013) only report re-
sults on the development set for the Berkeley-Rep
model; however, the task organizers also use a ver-
sion of the Berkeley parser provided with parts
of speech from high-quality POS taggers for each
language (Berkeley-Tags). These part-of-speech
taggers often incorporate substantial knowledge
of each language?s morphology. Both Berkeley-
Rep and Berkeley-Tags make up for some short-
comings of the Berkeley parser?s unknown word
model, which is tuned to English.
In Table 4, we see that our performance is over-
all substantially higher than that of the Berkeley
parser. On the development set, we outperform the
Berkeley parser and match the performance of the
Berkeley-Rep parser. On the test set, we outper-
4
I.e. it does not use a reranking step or post-hoc combina-
tion of parser results.
5
Their best parser, and the best overall parser from the
shared task, is a reranked product of ?Replaced? Berkeley
parsers.
form both the Berkeley parser and the Berkeley-
Tags parser on seven of nine languages, losing
only on Arabic and French.
These results suggest that the Berkeley parser
may be heavily fit to English, particularly in its
lexicon. However, even when language-specific
unknown word handling is added to the parser, our
model still outperforms the Berkeley parser over-
all, showing that our model generalizes even bet-
ter across languages than a parser for which this
is touted as a strength (Petrov and Klein, 2007).
Our span features appear to work well on both
head-initial and head-final languages (see Basque
and Korean in the table), and the fact that our
parser performs well on such morphologically-
rich languages as Hungarian indicates that our suf-
fix model is sufficient to capture most of the mor-
phological effects relevant to parsing. Of course,
a language that was heavily prefixing would likely
require this feature to be modified. Likewise, our
parser does not perform as well on Arabic and He-
brew. These closely related languages use tem-
platic morphology, for which suffixing is not ap-
propriate; however, using additional surface fea-
tures based on the output of a morphological ana-
lyzer did not lead to increased performance.
Finally, our high performance on languages
such as Polish and Swedish, whose training tree-
banks consist of 6578 and 5000 sentences, respec-
tively, show that our feature-rich model performs
robustly even on treebanks much smaller than the
Penn Treebank.
6
6
The especially strong performance on Polish relative to
other systems is partially a result of our model being able to
produce unary chains of length two, which occur frequently
in the Polish treebank (Bj?orkelund et al, 2013).
234
While ? Gangs ? is never lethargic    , it is hindered by its plot .
4
1
2
2 ? (4 While...) 1
Figure 5: An example of a sentence from the Stan-
ford Sentiment Treebank which shows the utility
of our span features for this task. The presence
of ?While? under this kind of rule tells us that the
sentiment of the constituent to the right dominates
the sentiment to the left.
7 Sentiment Analysis
Finally, because the system is, at its core, a classi-
fier of spans, it can be used equally well for tasks
that do not normally use parsing algorithms. One
example is sentiment analysis. While approaches
to sentiment analysis often simply classify the sen-
tence monolithically, treating it as a bag of n-
grams (Pang et al, 2002; Pang and Lee, 2005;
Wang and Manning, 2012), the recent dataset of
Socher et al (2013) imposes a layer of structure
on the problem that we can exploit. They annotate
every constituent in a number of training trees with
an integer sentiment value from 1 (very negative)
to 5 (very positive), opening the door for models
such as ours to learn how syntax can structurally
affect sentiment.
7
Figure 5 shows an example that requires some
analysis of sentence structure to correctly under-
stand. The first constituent conveys positive senti-
ment with never lethargic and the second conveys
negative sentiment with hindered, but to determine
the overall sentiment of the sentence, we need to
exploit the fact that while signals a discounting of
the information that follows it. The grammar rule
2 ? 4 1 already encodes the notion of the senti-
ment of the right child being dominant, so when
this is conjoined with our span feature on the first
word (While), we end up with a feature that cap-
tures this effect. Our features can also lexicalize
on other discourse connectives such as but or how-
ever, which often occur at the split point between
two spans.
7
Note that the tree structure is assumed to be given; the
problem is one of labeling a fixed parse backbone.
7.1 Adapting to Sentiment
Our parser is almost entirely unchanged from the
parser that we used for syntactic analysis. Though
the treebank grammar is substantially different,
with the nonterminals consisting of five integers
with very different semantics from syntactic non-
terminals, we still find that parent annotation is ef-
fective and otherwise additional annotation layers
are not useful.
One structural difference between sentiment
analysis and syntactic parsing lies in where the rel-
evant information is present in a span. Syntax is
often driven by heads of constituents, which tend
to be located at the beginning or the end, whereas
sentiment is more likely to depend on modifiers
such as adjectives, which are typically present
in the middle of spans. Therefore, we augment
our existing model with standard sentiment anal-
ysis features that look at unigrams and bigrams
in the span (Wang and Manning, 2012). More-
over, the Stanford Sentiment Treebank is unique
in that each constituent was annotated in isolation,
meaning that context never affects sentiment and
that every word always has the same tag. We ex-
ploit this by adding an additional feature template
similar to our span shape feature from Section 4.4
which uses the (deterministic) tag for each word
as its descriptor.
7.2 Results
We evaluated our model on the fine-grained sen-
timent analysis task presented in Socher et al
(2013) and compare to their released system. The
task is to predict the root sentiment label of each
parse tree; however, because the data is annotated
with sentiment at each span of each parse tree, we
can also evaluate how well our model does at these
intermediate computations. Following their exper-
imental conditions, we filter the test set so that it
only contains trees with non-neutral sentiment la-
bels at the root.
Table 5 shows that our model outperforms the
model of Socher et al (2013)?both the published
numbers and latest released version?on the task
of root classification, even though the system was
not explicitly designed for this task. Their model
has high capacity to model complex interactions
of words through a combinatory tensor, but it ap-
pears that our simpler, feature-driven model is just
as effective at capturing the key effects of compo-
sitionality for sentiment analysis.
235
Root All Spans
Non-neutral Dev (872 trees)
Stanford CoreNLP current 50.7 80.8
This work 53.1 80.5
Non-neutral Test (1821 trees)
Stanford CoreNLP current 49.1 80.2
Stanford EMNLP 2013 45.7 80.7
This work 49.6 80.4
Table 5: Fine-grained sentiment analysis results
on the Stanford Sentiment Treebank of Socher et
al. (2013). We compare against the printed num-
bers in Socher et al (2013) as well as the per-
formance of the corresponding release, namely
the sentiment component in the latest version of
the Stanford CoreNLP at the time of this writ-
ing. Our model handily outperforms the results
from Socher et al (2013) at root classification and
edges out the performance of the latest version of
the Stanford system. On all spans of the tree, our
model has comparable accuracy to the others.
8 Conclusion
To date, the most successful constituency parsers
have largely been generative, and operate by refin-
ing the grammar either manually or automatically
so that relevant information is available locally to
each parsing decision. Our main contribution is
to show that there is an alternative to such anno-
tation schemes: namely, conditioning on the input
and firing features based on anchored spans. We
build up a small set of feature templates as part of a
discriminative constituency parser and outperform
the Berkeley parser on a wide range of languages.
Moreover, we show that our parser is adaptable to
other tree-structured tasks such as sentiment anal-
ysis; we outperform the recent system of Socher et
al. (2013) and obtain state of the art performance
on their dataset.
Our system is available as open-source at
https://www.github.com/dlwh/epic.
Acknowledgments
This work was partially supported by BBN un-
der DARPA contract HR0011-12-C-0014, by a
Google PhD fellowship to the first author, and
an NSF fellowship to the second. We further
gratefully acknowledge a hardware donation by
NVIDIA Corporation.
References
Anders Bj?orkelund, Ozlem Cetinoglu, Rich?ard Farkas,
Thomas Mueller, and Wolfgang Seeker. 2013.
(Re)ranking Meets Morphosyntax: State-of-the-art
Results from the SPMRL 2013 Shared Task. In Pro-
ceedings of the Fourth Workshop on Statistical Pars-
ing of Morphologically-Rich Languages.
Rens Bod. 1993. Using an Annotated Corpus As a
Stochastic Grammar. In Proceedings of the Sixth
Conference on European Chapter of the Association
for Computational Linguistics.
Peter F Brown, Peter V Desouza, Robert L Mercer,
Vincent J Della Pietra, and Jenifer C Lai. 1992.
Class-based n-gram models of natural language.
Computational linguistics, 18(4):467?479.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine N-best Parsing and MaxEnt Discriminative
Reranking. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics.
Eugene Charniak. 1997. Statistical Techniques for
Natural Language Parsing. AI Magazine, 18:33?44.
Michael Collins and Terry Koo. 2005. Discriminative
Reranking for Natural Language Parsing. Computa-
tional Linguistics, 31(1):25?70, March.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In ACL, pages 16?23.
John Duchi, Elad Hazan, and Yoram Singer. 2010.
Adaptive Subgradient Methods for Online Learning
and Stochastic Optimization. COLT.
Jason Eisner. 1996. Three New Probabilistic Mod-
els for Dependency Parsing: An Exploration. In
Proceedings of the 16th International Conference on
Computational Linguistics (COLING-96).
Jenny Rose Finkel, Alex Kleeman, and Christopher D.
Manning. 2008. Efficient, feature-based, condi-
tional random field parsing. In ACL 2008, pages
959?967.
Daniel Gildea. 2001. Corpus variation and parser per-
formance. In Proceedings of Empirical Methods in
Natural Language Processing.
David Hall and Dan Klein. 2012. Training factored
PCFGs with expectation propagation. In EMNLP.
James Henderson. 2003. Inducing History Represen-
tations for Broad Coverage Statistical Parsing. In
Proceedings of the North American Chapter of the
Association for Computational Linguistics on Hu-
man Language Technology - Volume 1.
Liang Huang. 2008. Forest reranking: Discrimina-
tive parsing with non-local features. In Proceedings
of ACL-08: HLT, pages 586?594, Columbus, Ohio,
June. Association for Computational Linguistics.
236
Mark Johnson. 1998. PCFG Models of Linguistic
Tree Representations. Computational Linguistics,
24(4):613?632, December.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In ACL, pages 423?430.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional Random Fields:
Probabilistic Models for Segmenting and Labeling
Sequence Data. In Proceedings of the Eighteenth
International Conference on Machine Learning.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
ACL, pages 75?82, Morristown, NJ, USA.
Bo Pang and Lillian Lee. 2005. Seeing Stars: Ex-
ploiting Class Relationships for Sentiment Catego-
rization with Respect to Rating Scales. In Proceed-
ings of the 43rd Annual Meeting on Association for
Computational Linguistics.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs Up?: Sentiment Classification Us-
ing Machine Learning Techniques. In Proceedings
of the ACL-02 Conference on Empirical Methods in
Natural Language Processing - Volume 10.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In NAACL-HLT.
Slav Petrov and Dan Klein. 2008a. Discriminative
log-linear grammars with latent variables. In NIPS,
pages 1153?1160.
Slav Petrov and Dan Klein. 2008b. Sparse multi-scale
grammars for discriminative latent variable parsing.
In Proceedings of the 2008 Conference on Empiri-
cal Methods in Natural Language Processing, pages
867?876, Honolulu, Hawaii, October. Association
for Computational Linguistics.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 433?440,
Sydney, Australia, July.
Djam?e Seddah, Reut Tsarfaty, Sandra K?ubler, Marie
Candito, Jinho D. Choi, Rich?ard Farkas, Jen-
nifer Foster, Iakes Goenaga, Koldo Gojenola Gal-
letebeitia, Yoav Goldberg, Spence Green, Nizar
Habash, Marco Kuhlmann, Wolfgang Maier, Joakim
Nivre, Adam Przepi?orkowski, Ryan Roth, Wolf-
gang Seeker, Yannick Versley, Veronika Vincze,
Marcin Woli?nski, and Alina Wr?oblewska. 2013.
Overview of the SPMRL 2013 Shared Task: A
Cross-Framework Evaluation of Parsing Morpho-
logically Rich Languages. In Proceedings of
the Fourth Workshop on Statistical Parsing of
Morphologically-Rich Languages.
Khalil Sima?an. 2000. Tree-gram Parsing Lexical De-
pendencies and Structural Relations. In Proceedings
of the 38th Annual Meeting on Association for Com-
putational Linguistics.
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Ng, and
Christopher Potts. 2013. Recursive Deep Mod-
els for Semantic Compositionality Over a Sentiment
Treebank. In Proceedings of Empirical Methods in
Natural Language Processing.
Ben Taskar, Dan Klein, Michael Collins, Daphne
Koller, and Christopher Manning. 2004. Max-
Margin Parsing. In In Proceedings of Empirical
Methods in Natural Language Processing.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich Part-
of-speech Tagging with a Cyclic Dependency Net-
work. In Proceedings of the North American Chap-
ter of the Association for Computational Linguistics
on Human Language Technology - Volume 1.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 384?394. Association for
Computational Linguistics.
Sida Wang and Christopher Manning. 2012. Baselines
and Bigrams: Simple, Good Sentiment and Topic
Classification. In Proceedings of the 50th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 2: Short Papers).
237
