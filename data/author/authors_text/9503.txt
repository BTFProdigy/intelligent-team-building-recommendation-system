Proceedings of the ACL-08: HLT Student Research Workshop (Companion Volume), pages 7?12,
Columbus, June 2008. c?2008 Association for Computational Linguistics
An Integrated Architecture for Generating Parenthetical Constructions
Eva Banik
Department of Computing
The Open University
Walton Hall, Milton Keynes,
e.banik@open.ac.uk
Abstract
The aim of this research is to provide a prin-
cipled account of the generation of embed-
ded constructions (called parentheticals) and
to implement the results in a natural language
generation system. Parenthetical construc-
tions are frequently used in texts written in a
good writing style and have an important role
in text understanding. We propose a frame-
work to model the rhetorical properties of par-
entheticals based on a corpus study and de-
velop a unified natural language generation ar-
chitecture which integrates syntax, semantics,
rhetorical and document structure into a com-
plex representation, which can be easily ex-
tended to handle parentheticals.
1 Introduction
Parentheticals are constructions that typically occur
embedded in the middle of a clause. They are not
part of the main predicate-argument structure of the
sentence and are marked by special punctuation (e.g.
parentheses, dashes, commas) in written texts, or by
special intonation in speech.
Syntactically, parentheticals can be realized by
many different constructions, e.g.: appositive rel-
ative clauses (1a), non-restrictive relative clauses
(1b), participial clauses (1c) or subordinate clauses
(1d).
(1) a The new goal of the Voting Rights Act [?
more minorities in political office ?] is
laudable. (wsj1137)
b GE, [which vehemently denies the
government?s allegations,] denounced Mr.
Greenfield?s suit. (wsj0617)
c But most businesses in the Bay area,
[including Silicon Valley,] weren?t greatly
affected. (wsj1930)
d So far, [instead of teaming up,] GE
Capital staffers and Kidder investment
bankers have bickered. (wsj0604)
A common characteristics of parentheticals is that
they express information that is not central to the
meaning of the overall message conveyed by a text
or spoken utterance and since they are specifically
marked by punctuation or intonation, they allow the
reader to distinguish between more and less impor-
tant parts of the message. By structuring information
this way, parentheticals make it easier for readers to
decode the message conveyed by a text. Consider
for example the following message that has been ex-
pressed by two different texts: one without paren-
theticals (2a) and one that contains two parentheti-
cals (2b).
(2) a Eprex is used by dialysis patients who are
anaemic. Prepulsid is a gastro-intestinal
drug. Eprex and Prepulsid did well
overseas.
b Eprex, [used by dialysis patients who are
anaemic,] and Prepulsid, [a
gastro-intestinal drug,] did well overseas.
(wsj1156)
Parentheticals have been much studied in lin-
guistics ( see (Dehe and Kavalova, 2007), (Burton-
Roberts, 2005) for a recent overview) but so far they
7
have received less attention in computational lin-
guistics. Only a few studies have attempted a com-
putational analysis of parentheticals, the most recent
ones being (Bonami and Godard, 2007) who give an
underspecified semantics account of evaluative ad-
verbs in French and (Siddharthan, 2002) who devel-
ops a statistical tool for summarisation that separates
parentheticals from the sentence they are embedded
in. Both of these studies are limited in their scope as
they focus on a very specific type of parentheticals.
From the perspective of natural language gener-
ation (NLG), as far as we know, nobody has at-
tempted to give a principled account of parentheti-
cals, even though these constructions contribute to
the easy readability of generated texts, and therefore
could significantly enhance the performance of NLG
systems (Scott and Souza, 1990).
Most existing natural language generation sys-
tems use rhetorical structure to construct a text plan
and map arguments of rhetorical relations onto in-
dividual sentences or clauses. As a result, the ar-
guments of the same rhetorical relation will always
occur immediately next to each other, although the
surface realization of individual arguments may vary
and a clause may appear syntactically embedded
within the preceding clause. This linear succession
of rhetorical relations and their arguments makes
the generated text appear monotonous and staccato.
As commonly mentioned by style manuals,1 using
different kinds of clause-combining strategies (e.g.
semicolons, dash-interpolations, appositives) shows
a clearer writing style.
The goal of this research is to give a principled
account of parenthetical constructions and incorpo-
rate its findings into a natural language generation
system.
2 System Architecture
We propose an integrated generation architecture for
this purpose which uses a Tree Adjoining Grammar
(Joshi, 1987) to represent linguistic information at
all levels, including syntax, rhetorical structure and
document structure.
Our approach is to make the elementary trees in
the grammar as complex as possible, so that con-
straints on which trees can be combined with each
1See for example, Rule 14 of (Strunk and White, 1979)
other will be localized in the trees themselves. By
incorporating information about rhetorical structure
and document structure into the trees, we are ex-
tending the domain of locality of elementary trees
as much as possible and this allows the generator
to keep the global operations for combining trees as
simple as possible. This approach has been referred
to as the ?Complicate Locally, Simplify Globally?
principle (Joshi, 2004).
The input to the generator is a set of rhetorical
relations and semantic formulas. For each formula
the system selects a set of trees from the grammar,
resulting in a number of possible tree sets associated
with the input.
The next step is to filter out sets of trees that will
not lead to a possible realization. In the current im-
plementation this is achieved by a version of polarity
filtering where we associate not only the syntactic
categories of root, substitution and foot nodes with a
positive or negative value (Gardent and Kow, 2006)
but also add the semantic variable associated with
these nodes. The values summed up by polarity fil-
tering are [node, semantic variable] pairs, which rep-
resent restrictions on possible syntactic realizations
of semantic (or rhetorical) arguments.
Parentheticals often pose a problem for polarity
filtering because in many cases there is a shared el-
ement between the parenthetical and its host, which
normally occurs twice in non-parenthetical realiza-
tions of the same input, but only once when there
is a parenthetical. (e.g., in (2a) the NP ?Eprex? oc-
curs twice, but only once in (2b)). In order to allow
for this variation, when summing up the values for
substitution and root nodes we consider multiple oc-
currences of NP substitution nodes associated with
the same semantic variable as if they were a single
instance. This results in one or more NP substitu-
tion nodes left empty at the end of the derivation,
which are then filled with a pronoun by a referring
expression module at the final stage of the genera-
tion process.
3 Corpus Study
The generator is informed by a corpus study of em-
bedded discourse units on two discourse annotated
corpora: the RST Discourse Treebank (Carlson et
al., 2001) and the Penn Discourse Treebank (PDTB-
8
El
ab
-ad
d
Ex
am
ple
El
ab
-ge
n-s
pe
c
Re
sta
tem
en
t
El
ab
-se
t-m
em
At
tri
bu
tio
n
Co
nd
itio
n
An
tith
esi
s
Co
nc
ess
ion
Ci
rcu
ms
tan
ce
Pu
rpo
se
NP
-m
od
ifie
rs
relative clause 143 2 2 147
participial clause 96 4 1 1 11 4 117
NP 34 8 22 64
NP-coord 6 6
cue + NP 5 1 2 3 2 13
Adj + cue 2 2
number 2 2
including + NP 13 5 18
VP
-o
rS
-m
od
ifie
rs
to-infinitive 4 30 34
NP + V 106 106
cue + S 5 20 14 9 29 77
PP 11 9 1 21
S 7 1 1 9
according to NP 7 7
V + NP 6 6
as + S 4 4
Adv + number 1 1 2
cue + Adj 2 2
cue + participial 2 2
cue + V 1 1
310 19 11 22 14 125 20 18 12 54 35 640
Table 1: Syntactic types of parentheticals in the RST corpus
Relation Connective in parenthetical Connective in host distribution in corpus
TEMPORAL 101 (48.8%) 2 3434 (18.6%)
CONTINGENCY 53 (25.6%) 0 3286 (17.8%)
COMPARISON 38 (18.3%) 5 5490 (29.7%)
EXPANSION 15 (7.2%) 5 6239 (33.8%)
TOTAL: 207 12 18484
Table 2: Relations between parentheticals and their hosts in the PDTB
Group, 2008).2 The aim of the study was to es-
tablish what rhetorical relations can hold between
parentheticals and their hosts and whether individ-
ual rhetorical relations tend to correlate with specific
syntactic types.
Table 1 illustrates the findings of the study on the
RST corpus, showing the correlation between syn-
tactic types of parentheticals and rhetorical relations
between parentheticals and their hosts in the corpus.
The majority of parentheticals in this study were
syntactically related to their hosts and they can be
divided into two main groups. The most frequently
occurring type is ELABORATION/EXPANSION-type
2The details of this study are reported in (Banik and Lee,
2008)
NP-modifiers which are realized by relative clauses,
NPs or nominal postmodifiers with non-finite
clauses and express some type of ELABORATION,
EXAMPLE or RESTATEMENT relation. 73.4% of par-
entheticals belong to this group in the RST corpus.
The other type of parentheticals are NON-ELA-
BORATION/EXPANSION-type VP- or S-modifiers,
which are realized by subordinate clauses, to-
infinitives and PPs and express CIRCUMSTANCE,
PURPOSE, CONDITION, ANTITHESIS,or CONCES-
SION relations. 26.6% of parentheticals in the cor-
pus belong to this group.
Because of the decision taken in the PDTB to only
annotate clausal arguments of discourse connec-
tives, parentheticals found in this corpus are almost
9
all subordinate clauses, which is clearly an artifact
of the annotation guidelines. This corpus only anno-
tates parentheticals that contain a discourse connec-
tive and we have found that in almost all cases the
connective occurs within the parenthetical. We have
found only 12 discourse adverbs that occurred in the
host sentence.
The present corpus study is missing several types
of parentheticals because of the nature of the annota-
tion guidelines of the corpora used. For example, in
the RST corpus some phrasal elements that contain a
discourse connective (3a) and adjectives or reduced
relative clauses that contain an adjective without a
verbal element are not annotated (3b):
(3) a But the technology, [while reliable,] is far
slower than the widely used hard drives.
(wsj1971)
b Each $5000 bond carries one warrant,
[exercisable from Nov. 28, 1989, through
Oct. 26, 1994] to buy shares at an
expected premium of 2 1/2 % to the
closing share price when terms are fixed
Oct. 26. (wsj1161)
These constructions are clear examples of par-
entheticals and we would expect them to behave
similarly to subordinating conjunctions and relative
clauses respectively. As a test case we decided to
allow adjectives to function as parentheticals in the
grammar of the generator and if the results are eval-
uated as satisfactory, plan to extend this analysis to
other constructions not covered by our corpus study.
4 Generating Parentheticals ? An
Example
We associate auxiliary trees with parenthetical oc-
currences of the most frequently embedded rhetori-
cal relations based on the above corpus study.
The basic assumption behind assigning syntactic
trees to parenthetical rhetorical relations is that the
semantic type of the arguments of the relation should
be mirrored by their syntax. Thus if one of the ar-
guments of a rhetorical relation is an object then it
must be represented by an NP in the syntax; if it
is a proposition then it must be assigned an S- or
VP-auxiliary tree. The satellite of the rhetorical re-
lation is always substituted into the auxiliary tree,
i.
p: CONCESSION(n, s)
TS


HH
H
S?
arg:n
TC
 HH
though S?
arg:s
ii.
p: CONCESSION(n, s)
TS
 HH
S?
arg:n
TC
 HH
but S?
arg:s
iii.
p: CONCESSION(n,s)
VP
 HH
TE
 HH
though S
 HH
S?
arg: s
Punct
,
VP?
arg: n
iv.
p: CONCESSION(n,s)
TS
 HH
TC
 HH
though S
 HH
S?
arg:s
Punct
,
S?
arg:n
Figure 1: Elementary trees for CONCESSION
and the nucleus is associated with the footnode (this
later gets unified with the semantic label of the tree
that the auxiliary tree adjoins to).
Figure 1 illustrates four elementary trees for the
CONCESSION relation. The trees in boxes i. and
ii. correspond to regular uses of CONCESSION while
the trees in iii. and iv. correspond to its parenthet-
ical occurrences. Using these trees along with the
elementary trees in Figure 3, and given the input be-
low, the system generates the following five possible
realizations:
Input: [[l3, concession, l1, l2], [l1,legal,x], [l2, fatal,
x], [x,substance]]
Output:
1. the substance, though it is fatal, is legal
2. the substance is legal though it is fatal
3. though it is fatal, the substance is legal
4. though the substance is fatal, it is legal
5. the substance is legal but it is fatal
Figure 2 gives the elementary trees assigned to
10
i.
p: ELABORATION(n,s)
S


HH
H
S?
arg: n
and S?
arg: n
ii.
p: ELABORATION(n,s)
S
S?
arg: n
Figure 2: Elementary trees for ELABORATION
the most frequently occurring parenthetical rhetori-
cal relation, ELABORATION-ADDITIONAL. The tree
in box i. is associated with non-parenthetical uses
of the relation, and box ii. shows the tree used for
parenthetical ELABORATION. Since in parenthetical
uses of ELABORATION the two arguments of the re-
lation combine with each other and not with a third
tree, as in the case of parenthetical CONCESSION,
the role of the lexically empty parenthetical tree in
box ii. is to restrict the type of tree selected for the
nucleus of ELABORATION. Since the satellite has
to end up as the parenthetical, the nucleus has to be
restricted to the main clause, which is achieved by
associating its semantic variable with an S substitu-
tion node in the tree.
To give an example, Figure 3. illustrates elemen-
tary trees for the input below:
Input: [[l3, elaboration, l1, l2], [l1,illegal,x], [l2,
fatal, x], [x,substance]]
Output:
1. the fatal substance is illegal
2. the substance, which is fatal, is illegal
3. the substance is illegal and it is fatal
The parenthetical ELABORATION tree is used for
constructing outputs 1. and 2., which restricts the
nucleus to select the initial tree in box iii. on Figure
3. As a result, the satellite of the relation has to se-
lect on of the auxiliary trees in box i. or ii. in order
to be able to combine with the nucleus. The case
where both satellite and nucleus are assigned initial
trees is handled by the non-parenthetical tree in box
i. on Figure 2.
i.
s: fatal/legal(x)
NP


HH
H
NP?
arg:x
TE


HH
H
WH
which
S
 HH
NP
?
VP
 HH
V
is
AP
fatal/
legal
iv.
x
NP
the substance
ii.
p: fatal/legal(x)
NP
 HH
A
fatal
legal
NP?
arg: x
iii.
p: fatal/legal(x)
S
 HH
NP?
arg:x
VP
 HH
V
is
A
fatal
(il)legal
Figure 3: Elementary TAG trees for semantic formulas
5 Directions for further research
A possible way to control the generator is to enrich
the input representation by adding restrictions on the
types of trees that are allowed to be selected, simi-
larly to (Gardent and Kow, 2007) (e.g., if a rhetori-
cal relation is restricted to selecting initial trees for
its satellite then it won?t be generated as a parenthet-
ical). Another way to select a single output is to es-
tablish ranking constraints (these could depend, e.g.,
on the genre of the text to be generated) and choose
the top ranked candidate for output.
At the moment the elementary trees in the gram-
mar contain document structure nodes (Power et al,
2003) which are not used by the generator. We
plan to extend the analysis of parentheticals to big-
11
ger structures like footnotes or a paragraph separated
in a box from the rest of the text and the document
structure nodes in the elementary trees will be used
to generate these.
Given the small size of the grammar, currently po-
larity filtering is enough to filter out just the gram-
matical realizations from the set of possible treesets.
As the grammar size increases we expect that we
will need additional constraints to reduce the num-
ber of possible tree sets selected for a given input.
Also, once the generator will be capable of han-
dling longer inputs, we will need to avoid generat-
ing too many parentheticals. Both the number of
possible tree sets and the number of parentheticals
in the outputs could be reduced by allowing the gen-
erator to select parenthetical realizations for only a
predefined percentage of each rhetorical relation in
the input. This number can be first obtained from our
corpus study, and fine-tuned based on evaluations of
the generated output.
The current implementation uses a very simplis-
tic referring expression module which inserts a pro-
noun in every NP position left open at the end of
the derivation, unless it is in a sentence initial po-
sition. Parentheticals often involve the use of refer-
ring expressions and can sound more natural when
the embedded constituent involves a reference to an
element in the main clause, therefore a more sophis-
ticated algorithm for referring expression generation
will be used in the future.
Although our corpus study gives important infor-
mation about which rhetorical relation to realize as a
parenthetical, how often, and using which syntactic
construction, there seem to be additional restrictions
on the use of certain parentheticals. Consider for
example the two realizations (4 a and b) of the CON-
CESSION relation below where the parenthetical in
(4b) sounds very unnatural:
concession:
n: a few people may experience side-effects
s: most people benefit from taking Elixir
(4) a Though most people benefit from taking
Elixir, a few people may experience
side-effects.
b ?? A few people, though most people
benefit from taking Elixir, may
experience side-effects.
References
E. Banik and A. Lee. 2008. A study of parentheticals in
discourse corpora ? implications for NLG systems. In
Proceedings of LREC 2008, Marrakesh.
O. Bonami and D. Godard. 2007. Parentheticals in
underspecified semantics: The case of evaluative ad-
verbs. Research on Language and Computation,
5(4):391?413.
N. Burton-Roberts. 2005. Parentheticals. In E. K.
Brown, editor, Encyclopaedia of Language and Lin-
guistics. Elsevier Science, 2nd edition edition.
L. Carlson, D. Marcu, and M. E. Okurowski. 2001.
Building a discourse-tagged corpus in the framework
of rhetorical structure theory. In Proceedings of the
Second SIGdial Workshop on Discourse and Dialogue,
pages 1?10, Morristown, NJ, USA. Association for
Computational Linguistics.
N. Dehe and Y. Kavalova, editors, 2007. Parentheticals,
chapter Parentheticals: An introduction, pages 1?22.
Linguistik aktuell Linguistics today 106. Amsterdam
Philadelphia: John Benjamins.
C. Gardent and E. Kow. 2006. Three reasons to adopt
tag-based surface realisation. In The Eighth Interna-
tional Workshop on Tree Adjoining Grammar and Re-
lated Formalisms (TAG+8), Sydney/Australia.
C. Gardent and E. Kow. 2007. A symbolic approach
to near-deterministic surface realisation using tree ad-
joining grammar. In In 45th Annual Meeting of the
ACL.
A. K. Joshi. 1987. The relevance of tree adjoining gram-
mar to generation. In G. Kempen, editor, Natural Lan-
guage Generation, pages 233?252. Martinus Nijhoff
Press, Dordrect, The Netherlands.
A. K. Joshi. 2004. Starting with complex primitives pays
off: complicate locally, simplify globally. Cognitive
Science: A Multidisciplinary Journal, 28(5):637?668.
PDTB-Group. 2008. The Penn Discourse Treebank 2.0
Annotation Manual. Technical Report IRCS-08-01,
Institute for Research in Cognitive Science, University
of Pennsylvania.
R. Power, D. Scott, and N. Bouayad-Agha. 2003.
Document structure. Computational Linguistics,
29(4):211?260.
D. Scott and C. S. Souza. 1990. Getting the message
across in RST-based text generation. In C. Mellish
R. Dale M. Zock, editor, Current Research in Natural
Language Generation, pages 31?56. Academic Press.
A. Siddharthan. 2002. Resolving attachment and clause
boundary ambiguities for simplifying relative clause
constructs. In Student Research Workshop, ACL.
W. Jr. Strunk and E. B. White. 1979. The Elements of
Style. Macmillan, third edition.
12
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 305?308,
Suntec, Singapore, 4 August 2009.
c?2009 ACL and AFNLP
Extending a Surface Realizer to Generate Coherent Discourse
Eva Banik
The Open University
Milton Keynes, UK
e.banik@open.ac.uk
Abstract
We present a discourse-level Tree Adjoin-
ing Grammar which tightly integrates syn-
tax and discourse levels, including a repre-
sentation for discourse entities. We show
that this technique makes it possible to
extend an optimisation algorithm used in
natural language generation (polarity fil-
tering) to the discourse level. We imple-
mented the grammar in a surface realizer
and show that this technique can be used
to reduce the search space by filtering out
referentially incoherent solutions.
1 Introduction
A fundamental problem that microplanners and
surface realizers face in natural language gener-
ation is how to restrict the search space of possi-
ble solutions. A traditional solution to this compu-
tational complexity problem is to divide the gen-
eration process into tractable sub-problems, each
represented as a module in a pipeline, where every
decision made by a module restricts the number of
options available to others further down the line.
Though such pipeline architectures are computa-
tionally efficient, they severely restrict the flexibil-
ity of the system and the quality of the generated
output. Most systems with pipeline architectures
generate relatively simple, domain-specific out-
put. Systems that produce more complex linguis-
tic constructions typically achieve this by adding
more modules to the pipeline (e.g. a revision mod-
ule (Robin, 1994) or aggregation (Shaw, 2002)).
Since complex linguistic constructions often re-
quire interaction between modules, adding them to
the repertoire of pipelined NLG systems becomes
an engineering and programming task.
Integrated NLG systems have a simpler archi-
tecture because they do not need to model in-
teractions between modules. However, they still
face the problem of computational complexity
that was originally solved by the pipeline model.
Strategies that have been introduced to reduce
the search space in integrated systems include
greedy/incremental search algorithms (Stone et
al., 2003), constructing a dependency graph for a
flat semantic input and converting it into a deriva-
tion tree (Koller and Striegnitz, 2002), using plan-
ning algorithms (Appelt, 1985; Koller and Stone,
2007), polarity filtering (Kow, 2007) and using
underspecified g-derivation trees (G-TAG, Danlos
(2000)). Despite all these efforts, most systems
still don?t attempt to go above the sentence level
or generate very complex sentences. In this pa-
per we present a new technique for designing an
integrated grammar for natural language genera-
tion. Using this technique it is possible to use lin-
guistic constraints on referential coherence to au-
tomatically reduce the search space ? which in
turn makes it possible to generate longer and more
coherent texts.
First we extend the grammar of a surface real-
izer to produce complex, multi-sentential output.
Then we add a representation for discourse refer-
ents to the grammar, inspired by Centering The-
ory?s notion of a backward looking center and pre-
ferred center. Having done this, we show that by
integrating discourse-level representations into a
syntactic grammar we can extend an optimization
technique ? polarity filtering (Kow, 2007; Gar-
dent and Kow, 2006) ? from syntactic realization
to the discourse level.
2 The Problem of Referential Coherence
Referential coherence is the phenomenon which is
responsible for the contrast in (1), in the sense that
the example in (1b) is perceived to be more coher-
ent than (1a).
(1) a Elixir is approved by the FDA. Viral
skin disorders are relieved by
305
Aliprosan. Elixir is a white cream.
Aliprosan is an ingredient of Elixir.
b Elixir is a white cream. Elixir is
approved by the FDA. Elixir contains
Aliprosan. Aliprosan relieves viral skin
disorders.
Centering Theory (Grosz et al, 1995) is a fre-
quently used framework for modeling referential
coherence in discourse. It is based on the no-
tion that for each utterance in a discourse there
is a set of entities which are the centers of atten-
tion and which serve to link that utterance to other
utterances in the same discourse segment. Enti-
ties mentioned by an utterance (the set of forward
looking centers) form a partially ordered list called
the Cf list where roughly, subjects are ranked high-
est, followed by objects, indirect objects and other
arguments or adjuncts. The backward looking
center of Un is said to be the most highly ranked
element on the Cf list of Un-1 mentioned in the
previous utterance.
Centering Theory has been adapted to NLG by
Kibble (1999; 2001), and implemented in Kib-
ble and Power (2004). Rather than using the no-
tion of centering transitions as defined by Grosz et
al. (1995), in these papers centering theory is re-
defined as constraints on salience and cohesion.
These constraints state that there is a preference
for consecutive utterances to keep the same center
and that there is a preference for the center of Un
to be realized as the highest ranked entity on the
Cf list of Un. Kibble and Power (2004) show how
these constraints can be used to drive text plan-
ning, sentence planning and pronominalization in
an integrated fashion. Our approach is similar to
Kibble and Power (2004) in that we don?t use the
concept of centering transitions. However, our
method is more efficient in that Kibble and Power
(2004) use centering transitions to rank the set of
generated solutions (some of which are incoher-
ent), whereas we encode centering constraints in
elementary trees to reduce the search space of pos-
sible solutions before we start computing them.
3 GenI and Polarity Filtering
The grammar described in the next section was
implemented in the GenI surface realizer (Kow,
2007), which uses a lexicalized feature-based Tree
Adjoining Grammar to generate all possible para-
phrases for a given flat semantic input. GenI im-
plements an optimization technique called polar-
h1:white-cream(e)
D
c



H
H
H
S


H
H
NP?
[idx:e]
VP


H
H
V
is
NP
cream
[idx:e]
Punct
.
h2:contain(e,a)
D
c
[c:e]



H
H
H
D
c
?
[c:e]
D
c



H
H
H
S


H
H
NP?
[idx:e]
VP


H
H
V
contains
NP?
[idx:a]
Punct
.
Figure 1: Elementary syntax/discourse trees
ity filtering to constrain the effects of lexical am-
biguity. The basic idea of polarity filtering is to
associate elementary trees with a set of polarities.
When these polarities don?t ?cancel each other
out?, it means that it is not possible to combine
the set of trees selected for a given input. This is
a quick way to check whether the number of ar-
gument slots is the same as the number of poten-
tial arguments. For example, if the lexical selec-
tion consists of two trees for a given input, one of
which provides an NP (-NP) and one of which ex-
pects two NPs (-2NP) then the sum of polarities
will be -NP and therefore the generator will not
attempt to combine the trees.
Values for polarities are defined as follows: ev-
ery initial tree is assigned a -cat polarity for each
substitution node of category cat and a +cat po-
larity if its root node is of category cat. Auxiliary
trees are assigned a -cat polarity for each substi-
tution node only.
Polarity filtering is a very powerful optimiza-
tion technique, because it allows the generator to
reduce the search space early on in the process,
before it attempts to combine any trees.
4 An Integrated Syntax-Discourse
Grammar
In order to generate mutisentential text, we first
define a discourse-level Tree Adjoining Gram-
mar. The trees in the grammar tightly integrate
syntax and discourse representations in the sense
that sentence-level elementary trees include one
or more discourse-level nodes. The elementary
trees in Fig. 1 illustrate what we mean by this:
every lexical item that would normally project a
sentence in a syntactic grammar (i.e., an S-rooted
306
+e +a -v +e -e +a -e
D
c
[c:e]


H
H
S


H
H
NP?
[arg:e]
VP


H
H
V
approved by
NP?
[arg:f]
.
D
c
[c:a]




H
H
H
H
D
c
?
[c:v]
S


H
H
NP?
[arg:v]
VP


H
H
V
relieved by
NP?
[arg:a]
.
D
c
[c:e]




H
H
H
H
D
c
?
[c:e]
S


H
H
NP?
[arg:e]
VP


H
H
V
is
NP
a cream
.
D
c
[c:a]




H
H
H
H
D
c
?
[c:e]
S




H
H
H
H
NP?
[arg:a]
VP


H
H
V
is ingredient
of
NP?
[arg:e]
.
h3:approve(f,e)
h6:relieve(a,v)
h0:cream(e)
h4:contain(e,a)
+2a -v
Elixir is approved by the FDA. Viral skin disorders are relieved by Aliprosan. Elixir is a white cream.
Aliprosan is an ingredient of Elixir.
Figure 2: Discourse-level polarities for (1a) sum up to +2a -v
+e -e +a -a +e +a -e
D
c
[c:e]




H
H
H
H
D
c
?
[c:e]
S


H
H
NP?
[arg:e]
VP


H
H
V
approved by
NP?
[arg:f]
.
D
c
[c:a]




H
H
H
H
D
c
?
[c:a]
S


H
H
NP?
[arg:a]
VP


H
H
V
relieves
NP?
[arg:v]
.
D
c
[c:e]


H
H
S


H
H
NP?
[arg:e]
VP


H
H
V
is
NP
a cream
.
D
c
[c:a]




H
H
H
H
D
c
?
[c:e]
S




H
H
H
H
NP?
[arg:e]
VP


H
H
V
contains
NP?
[arg:a]
.
h3:approve(f,e)
h6:relieve(a,v)
h0:cream(e)
h4:contain(e,a)
+a
Elixir is a white cream. Elixir is approved by the FDA. Elixir contains Aliprosan. Aliprosan relieves
viral skin disorders.
Figure 3: Discourse-level polarities for (1b) sum up to +a
tree) here projects a discourse clause (i.e., a Dc
rooted tree). Every predicate that projects a dis-
course clause is assigned two kinds of elementary
trees: a discourse initial tree (Fig. 1a) and a dis-
course continuing tree (Fig. 1b), which takes the
preceding discourse clause as an argument.
We model referential coherence by associating
a discourse entity with every root- and substitution
node of category D
c
. A discourse entity on a root
node is ?exported? by the elementary tree to be
the center of attention in the next sentence. This
roughly corresponds to Centering Theory?s notion
of a forward looking center. A discourse entity on
a substitution node is the entity expected by the
sentence to have been the center of attention in
the previous utterance, roughly corresponding to
the notion of backward looking center in Center-
ing Theory.
For example, the tree on the left in Fig. 1. ex-
ports the discourse entity representing its subject
(?e?) as its ?forward looking center?. The tree on
the right in Fig. 1. is looking for a discourse en-
tity called ?e? as its ?backward looking center? and
exports the same discourse entity as its ?forward
looking center?. The combination of these two
trees therefore yields a coherent discourse, which
is expected to be continued with an utterance cen-
tered on ?e?.
5 Polarity Filtering on Discourse Entities
By treating discourse entities on D
c
nodes as an
additional polarity key we can apply the polarity
filtering technique on the discourse level. This
means we can filter out lexical selections that
wouldn?t lead to a coherent discourse the same
way as those lexical selections are filtered out
which won?t lead to a syntactically well formed
sentence. To give an example, given the semantic
representation in Figure 4 potential realizations by
a generator which is not aware of discourse coher-
ence would include both of the examples in (1).
As an experiment, we generated the above ex-
ample using the same input but two different
grammars. In the first case we used a grammar
which consists of discourse-level trees but no an-
notations for discourse entities. The realizer pro-
307
h0:white cream(e)
h1:elixir(e)
h2:fda(f)
h3:approve(f e)
h4:contain(e a)
h5:aliprosan(a)
h6:relieve(a v)
h7:viral skin disorders(v)
Figure 4: Input for the sentences in (1)
duced 192 solutions, including many incoherent
ones such as (1a). In the second case, we used
a grammar with the same trees, but annotated with
discourse referents. In this case the realizer pro-
duced only 16 solutions, all of which maintained
referential coherence. In the first case, the gram-
mar provided 128 ways to associate trees with the
input (tree sets), and the 192 solutions included
all possible sentence orders. Since for most trees
in the grammar there are more than one ways to
annotate them with discourse referents, in the sec-
ond case the grammar contained more trees (dif-
fering only in their discourse referent asignments).
In this case there were 1536 tree sets selected for
the same input. Of these, 1320 were discarded by
polarity filtering on discourse entities. Of the re-
maining 216 tree sets 200 were ruled out by fea-
ture unification when the trees were combined.
Figures 2 and 3 illustrate two sets of trees that
were selected by the realizer, corresponding to the
examples in (1). Discourse-level polarity filtering
in this example (for the input in (4)) discards all
tree sets whose polarities don?t sum up to one of
the discourse entities, i.e., +e, +a, +f or +v. The
polarity of the tree set in Fig.2 is +2a -v so the
tree set is discarded. For the tree set in Fig.3 the
polarities sum up to +e and the realizer attempts
to combine the trees, which in this case leads to a
referentially coherent solution (1b).
The search space of the realizer can be further
restricted by only allowing tree sets whose polari-
ties sum up to a specific discourse entity. In this
case the realizer will produce paragraphs where
the center of attention in the last sentence is the
discourse entity used for polarity filtering.
6 Conclusions
We have described a discourse-level extension of
Tree Adjoining Grammar which tightly integrates
syntax with discourse and includes a representa-
tion of discourse entities. We have shown that in-
cluding discourse entities in the grammar of a sur-
face realizer improves the coherence of the gener-
ated text and that these variables can also be used
in a very efficient optimization technique, polarity
filtering, to filter out referentially incoherent solu-
tions.
References
D.E. Appelt. 1985. Planning English sentences. Cam-
bridge University Press, Cambridge.
L. Danlos. 2000. G-TAG: A lexicalized formalism for
text generation inspired by Tree Adjoining Gram-
mar. In A. Abeille and O. Rambow, editors, Tree
Adjoining Grammars: Formalisms, linguistic analy-
sis and processing, pages 343?370. CSLI, Stanford,
CA.
C. Gardent and E. Kow. 2006. Three reasons to adopt
TAG-based surface realisation. In Proceedings of
TAG+8), Sydney/Australia.
B.J. Grosz, A.K. Joshi, and S Weinstein. 1995. Cen-
tering: a framework for modelling the local co-
herence of discourse. Computational Linguistics,
21(2):203?225.
R. Kibble and R. Power. 2004. Optimizing referential
coherence in text generation. Computational Lin-
guistics, 30(4):401?416.
R. Kibble. 1999. Cb or not Cb? centering theory ap-
plied to NLG. In ACL workshop on Discourse and
Reference Structure, pages 72?81.
R. Kibble. 2001. A reformulation of rule 2 of centering
theory. Comput. Linguist., 27(4):579?587.
A. Koller and M. Stone. 2007. Sentence generation as
planning. In Proceedings of ACL.
A. Koller and K. Striegnitz. 2002. Generation as de-
pendency parsing. In Proceedings of ACL.
E. Kow. 2007. Surface realisation: ambiguity and
determinism. Ph.D. thesis, Universite de Henri
Poincare - Nancy 1.
J. Robin. 1994. Revision-based generation of Natu-
ral Language Summaries providing historical Back-
ground. Ph.D. thesis, Columbia University.
J. Shaw. 2002. Clause Aggregation: An approach
to generating concise text. Ph.D. thesis, Columbia
University.
M. Stone, C. Doran, B. Webber, T. Bleam, and
M. Palmer. 2003. Microplanning with communica-
tive intentions: The SPUD system. Computational
Intelligence, 19(4):311?381.
308
Proceedings of the 2009 Workshop on Grammar Engineering Across Frameworks, ACL-IJCNLP 2009, pages 46?53,
Suntec, Singapore, 6 August 2009. c?2009 ACL and AFNLP
Parenthetical Constructions - an Argument against Modularity
Eva Banik
The Open University
Milton Keynes, UK
e.banik@open.ac.uk
Abstract
This paper presents an argument against
modularizing linguistic information in nat-
ural language generation systems. We ar-
gue that complex linguistic constructions
require grammatical information to be lo-
cated in the same module, in order to
avoid over-complicating the system archi-
tecture. We demonstrate this point by
showing how parenthetical constructions
? which have only been generated in pre-
vious systems using an aggregation or re-
vision module ? can be generated by a
surface realizer when using an integrated
grammar.
1 Introduction
The ultimate aim of research on natural language
generation is to develop large-scale, domain inde-
pendent NLG systems, which are able to generate
high quality, fluent and well-formatted texts. Ide-
ally the produced texts will be as long as needed
to convey the information given in the input and
should be presented in a style that is appropriate
for the purposes of the user. Current NLG systems
typically produce paragraph-length text tailored to
a specific domain and the grammars in these sys-
tems contain only a limited number of grammati-
cal constructions, typically collected during a cor-
pus study of example documents. Often the gram-
mar is implemented using schemas or ?canned?
expressions, and individual grammatical levels are
distributed in independent modules.
Organizing the grammar this way severely lim-
its the flexibility of NLG systems. It has long been
recognized in the literature that text fluency can be
improved by modeling interactions between gram-
mar modules. The most commonly mentioned
interactions are those among discourse/rhetorical
relations and syntax (Scott and Souza, 1990;
Hovy, 1993; Callaway, 2003), rhetorical rela-
tions, syntax and referring expressions (Kibble
and Power, 2004); and layout and referring ex-
pressions (N. Bouayad-Agha, 2001). It is clear
that in order to generate high quality, coherent
discourse, a generator needs access to a gram-
mar which is able to model the interdependent,
context-sensitive behaviour of these separate lin-
guistic phenomena.
In this paper we draw a parallel between gram-
mar design and the design of natural language gen-
eration systems. We argue that in order to gener-
ate complex linguistic constructions, current NLG
systems tend to have overly complicated architec-
tures. To illustrate this point we show how a sur-
face realizer can take on tasks from other com-
ponents when linguistic information from differ-
ent grammar modules (and hence, system mod-
ules) is integrated. This simplifies system archi-
tecture by reducing the need for interaction be-
tween modules and enables the generator to pro-
duce more complex and coherent text. We illus-
trate this point by first showing constraints that
parenthetical constructions impose on pronomi-
nalization. Then we present a grammar which in-
tegrates a representation for referring expressions
into a syntax/discourse grammar. Finally we show
that using this grammar, we can generate complex,
coherent paragraphs which contain parenthetical
constructions using only a surface realizer.
2 The problem of generating
parenthetical constructions
Parentheticals are constructions that provide less
important or background information in texts and
they are a prime example of interactions between
referring expressions, syntax, layout and discourse
structure. Parentheticals help readers distinguish
between more and less important propositions and
therefore significantly increase the fluency and
readability of the generated text. Despite this ma-
46
jor effect on the quality of the generated text, cur-
rent natural language generation systems still do
not have a principled way of producing paren-
theticals. In this paper we focus on parenthetical
constructions which take the form of a subordi-
nate clause introduced by a discourse connective.
Some examples of this type of parentheticals in the
Wall Street Journal are illustrated (1):
(1) a The irony is that the attack commercial,
after getting a boost in last year?s
presidential campaign, has come of age
in an off-off election year with only a
few contests scattered across the
country.
b the 1989 fall total of 80, while well
below 1988 activity, shows a steady
ratcheting up in citizen referenda and
initiatives
c pollination, while easy in corn because
the carrier is wind, is more complex
and involves insects as carriers in crops
such as cotton
The examples in (2) illustrate the difficulties in
generating parenthetical constructions by show-
ing some possible but incoherent realizations of
the same message. In particular, they illustrate
the importance of appropriate punctuation marks
(2a), syntactic requirements of discourse connec-
tives (2b), the limit on embedding (2c), and the
importance of ordering syntactic arguments (the-
matic structure/information structure) (2d).
(2) a # The FDA though it bans Elixir since it
contains Gestodene approves Elixir
Plus.
b # The FDA ? but it bans Elixir since it
contains Gestodene ? approves Elixir
Plus.
c # The FDA though since Elixir contains
Gestodene , it bans Elixir approves
Elixir Plus.
d # The FDA, since Gestodene is an
ingredient of Elixir, bans Elixir. But it
approves Elixir Plus.
Correct realizations of the same message would
include:
(3) a The FDA ? though it bans Elixir since
it contains Gestodene ? approves
Elixir Plus .
b The FDA bans Elixir because it
contains Gestodene. However, Elixir
Plus is approved by the FDA
c The FDA approves Elixir Plus although
Elixir ? since it contains Gestodene ?
is banned by the FDA.
Generation systems that produce output similar
to the examples in (3) have three kinds of strate-
gies: either a text planning module chooses a dis-
course connective and decides the position and
ordering of clauses (Hovy, 1993) or aggregation
is considered to be one of the tasks of the sen-
tence planning module (Shaw, 2002); or a revi-
sion module performs aggregation opportunisti-
cally (Robin, 1994; Callaway and Lester, 1997).
However, none of these systems handle paren-
thetical constructions in a principled way. Sys-
tems where aggregation is part of the text planning
module only produce complex sentences made
up of clauses joined by discourse connectives ?
sentence-medial subordinate clauses are not gen-
erated at all. In revision-based systems, the output
often needs to be corrected after aggregation. For
example, Robin?s system includes various trans-
formations to correct redundancies, ambiguities or
invalid lexical collocations introduced by the revi-
sion module. In Shaw?s system, the referring ex-
pression generation module is run twice, once be-
fore and once after aggregation. In general, the
ordering of aggregation rules and the interactions
between them pose further problems where ag-
gregation is separated into an independent mod-
ule.We propose a different approach to modeling
interactions between linguistic information in sep-
arate grammar modules. We argue that constraints
that are at the interface of modules (syntactic con-
straints on referring expressions, discourse-level
constraints on syntax, constraints imposed by lay-
out on discourse, etc.) should be stored in an in-
tegrated grammar, and only straightforward deci-
sions ? which do not require information from a
separate grammatical level ? should be separated
out into individual modules.
As an example, we show a grammar which is
capable of generating parenthetical constructions
in a principled way. The grammar includes
- a representation for discourse connectives
and discourse-level constraints they impose
on syntax;
- referring expressions and syntactic con-
straints on them;
47
- elements of layout (punctuation marks for
main clauses and parentheticals).
We show that by incorporating the above kinds
of linguistic information into the grammar of a
surface realizer we can improve the flexibility of
the system (i.e., generate more paraphrases for the
same input) and improve the quality of the gener-
ated text without adding more modules to the sys-
tem.
2.1 Syntactic constraints on
pronominalization
To design a grammar for parenthetical construc-
tions, we have carried out a corpus study on em-
bedded rhetorical relations in the RST treebank
(Banik and Lee, 2008). The corpus study has
shown that the most numerous class of embed-
ded subordinate clauses that occur in sentence-
medial position contain a subject pronoun (as in
4a). This embedded subject pronoun in all cases
referred back to the subject of the matrix clause,
which always immediately preceded the subordi-
nate clause. The pronoun can be either explicit (as
in 4a) or implicit (as in the examples in 1). Of the
119 sentence-medial subordinate clauses that we
looked at in the study, 35 were of this type (what
we call pseudo-relatives).1 This suggests that in
sentence-medial subordinate clauses (or sentence-
final ones immediately following the main clause
object) the type of a referring expression is solely
determined by syntax, much like a WH-pronoun
in relative clauses.
(4) a Elixir, since it contains Gestodene, is
banned by the FDA.
b # Elixir, since Elixir contains
Gestodene, is banned by the FDA.
c # It, since Elixir contains Gestodene, is
banned by the FDA.
d # The FDA, since it contains
Gestodene, banned Elixir.
The constraints on the form of referring expres-
sions selected for the matrix clause and subordi-
nate clause subjects in these cases can be stated as
follows:
1Of the rest, 30 were ?free? subordinate clauses (subor-
dinate clauses that are equally felicitous in sentence-initial
or sentence final positions, typically they do not contain any
pronouns). The rest of the cases were either time adverbials
(20) or scopal elements (22).
- the subject of the subordinate clause has to be
realized as a pronoun. (c.f. 4b)
- the subject of the main clause cannot be a
pronoun (c.f. 4c)
- the subject pronoun in the subordinate clause
will be resolved as referring to an entity men-
tioned in the matrix clause; this entity has to
precede the subordinate clause (c.f. 4d )
In addition to modeling the above constraints, in
order to generate parentheticals a generation sys-
tem also has to
- insert the appropriate discourse connective
for the subordinate clause (c.f.2b) and
- insert appropriate punctuation marks on ei-
ther side of the subordinate clause to avoid
potential garden path effects.
3 An integrated discourse-syntax
grammar
In order to generate coherent discourse, a gener-
ation system needs access to a grammar that is
capable of representing multisentential text. In
modular systems this is typically achieved by two
modules: a text planning module which constructs
a text plan and a surface realizer that converts
the text plan into sentences. However, text plan-
ning and linguistic realization are not two inde-
pendent processes and many linguistic decisions
are in fact made by the text planner. The inter-
actions between text planning and linguistic re-
alization in modular systems have been handled
in several ways, including backtracking (Appelt,
1985), interleaving the two components (McDon-
ald, 1983) and restrictive planning (Hovy, 1988).
These approaches however make the system in-
flexible because all possible interactions between
modules have to be anticipated by the system de-
signer.
Another, more recent approach to tackle this
problem is to use lexicalization not only for sen-
tences but also for texts. The theoretical back-
ground for lexicalization on the discourse level
has been laid down for Tree Adjoining Grammar
(Joshi and Schabes, 1997) by several researchers,
including Webber (2004), and Danlos (2000). In
particular, Danlos (2000) shows that extending
lexicalization to the discourse level makes it possi-
ble to completely integrate text planning and sur-
face realization.
48
We have designed a Tree Adjoining Grammar
for parenthetical constructions following this lat-
ter approach. Elementary trees in the grammar
are associated with a flat semantic representation.
The trees integrate syntax and discourse represen-
tations in the sense that each sentence-level ele-
mentary tree includes one or more discourse-level
nodes. The elementary trees in Fig. 1 illustrate
what we mean by this: every lexical item that
would normally project a sentence in a syntactic
grammar (i.e., an S-rooted tree) here projects a dis-
course clause (i.e., a Dc rooted tree). Every pred-
icate that projects a discourse clause is assigned
two kinds of elementary trees: a discourse ini-
tial tree (e.g., Fig. 1a) and a discourse continu-
ing tree (e.g., Fig. 1b), which takes the preceding
discourse clause as an argument.
h1:white-cream(e)
D
c



H
H
H
S


H
H
NP?
[idx:e]
VP


H
H
V
is
NP
cream
[idx:e]
Punct
.
(a) discourse initial
h2:contain(e,a)
D
c



H
H
H
D
c
? D
c



H
H
H
S


H
H
NP?
[idx:e]
VP


H
H
V
contains
NP?
[idx:a]
Punct
.
(b) discourse continuing
Figure 1: Elementary syntax/discourse trees
The combination of these two trees corresponds
to the empty connective (? in Danlos (2000)).
Other types of discourse connectives are imple-
mented in the grammar the usual way (see e.g.
Danlos (2000)).
4 Referring expressions
One of the challenges of generating paraphrases
from a semantic representation is that in some ver-
sions there will be a mismatch between the num-
ber of noun phrases needed to make the output
syntactically well-formed and the number of se-
mantic arguments in the input which can poten-
tially become a noun phrase.
This happens whenever a discourse entity is the
argument of more than one semantic predicate.
For example, (5) shows possible realizations of the
following input where (5a) contains three syntac-
tic slots for ?Elixir?, (5b,c) contain two slots, and
(5d) only one:
h0:white-cream(e)
h1:contains(e,g)
h2:elixir(e)
h3:gestodene(g)
h4:ban(f,e)
h5:fda(f)
(5) a Elixir is a white cream. Elixir contains
gestodene. Elixir is banned by the FDA.
b This white cream, Elixir, contains
gestodene. It is banned by the FDA.
c Elixir is a white cream, which contains
gestodene. It is banned by the FDA.
d Elixir, a white cream banned by the
FDA, contains gestodene.
The task of a generation system is to decide
what predicate-argument structure to choose and
to decide how the individual noun phrases should
be represented. In most systems creating the syn-
tactic ?slots? is the task of a text planning or sen-
tence planning module, and filling them in with the
right noun phrases is the task of a referring expres-
sion generation module, i.e., the referring expres-
sion module decides whether an NP slot should be
realized as a name, a pronoun or a description.
This division of labour makes it difficult to rep-
resent syntactic constraints on pronominalization
exhibited by the examples in the previous section,
where pronouns are either prohibited or obligatory
in specific syntactic contexts.
To model these constraints we include a repre-
sentation for underspecified referring expressions
in the grammar by replacing NP substitution nodes
with a referring expression leaf node as illustrated
in Fig.2. This allows syntactic constraints to be
?posted? on referring expressions in the appropri-
ate contexts while completely specifying the form
of the underspecified slots still remains the task of
a referring expression module. In other words, we
factor out pronominalization decisions dictated by
syntax from pronominalization decisions dictated
by discourse level constraints.
Treating pronouns in subordinate clauses dif-
ferently from pronouns in main clauses has inde-
pendent justification from psycholinguistics and
theoretical linguistics. For example, Miltsakaki
(2003) has carried out psycholinguistic experi-
ments on complex sentences containing relative
clauses. The experiments show that pronouns in
embedded clauses tend to refer back to an entity
49
h2:contain(e,a)
D
c



H
H
H
D
c
? D
c



H
H
H
S


H
H
RX
[idx:e]
VP


H
H
V
banned by
RX
[idx:a]
Punct
.
Figure 2: Elementary trees with referring expressions
in the matrix clause, whereas referring expressions
in main clauses tend to find their antecedent in the
previous main clause. This suggests that pronom-
inalization should be treated differently in subor-
dinate clauses than in main clauses. Research in
theoretical linguistics underlines this claim, where
Kehler (2002) has shown that apparent discrepan-
cies between different accounts of pronominaliza-
tion can be reconciled if each method is applied in
a different discourse context.
To sum up, in this integrated approach part of
the job of the referring expression generation mod-
ule is taken over by the grammar, namely
- pronominalization of discourse entities in
subordinate clauses and
- decisions about when not to realize under-
specified referring expressions as pronouns.
5 Representing parenthetical
constructions
Integrating referring expressions into the grammar
this way makes it possible to state syntactic con-
straints on pronominalization.
5.1 Pronoun prohibited
(6) a Elixir, an illegal drug, is banned by the
FDA.
b # It, an illegal drug, is banned by the
FDA.
The constraint that parenthetical constructions
such as appositives, relative clauses or parenthet-
ical subordinate clauses cannot follow a pronoun
is illustrated by the contrast in (6). Using the el-
ementary trees described in the previous section
this constraint can now be stated by adding a fea-
ture ([pron:no]) to the foot node of auxiliary
trees, as illustrated in Fig. 3. When the aux-
iliary tree is adjoined onto an NP, the feature is
percolated to the underspecified referring expres-
sion node, which will block the referring expres-
sion module from realizing this noun phrase as a
pronoun.
5.2 Pronoun obligatory
(7) a Elixir, since it contains Gestodene, is
banned by the FDA.
b # Elixir, since Elixir contains
Gestodene, is banned by the FDA.
Another case where syntax imposes constraints
on pronominalization is contexts where pronouns
are not allowed, as illustrated by the example in
(7). The discourse connective ?since? is assigned
an NP auxiliary tree in this context, which takes
the embedded clause as an argument. The features
on the auxiliary tree state that the subject of this
embedded clause should be expressed by a pro-
noun and that it should refer to the same discourse
entity as the head noun that the auxiliary tree ad-
joins to. When the discourse connective is com-
bined with the embedded clause, these features are
percolated to the referring expression in subject
position, requiring it to be realized by a pronoun.
Figure 4 illustrates the elementary trees and the
derived tree for the embedded clause in (7).
6 Comparison
As an experiment, we have implemented a gram-
mar fragment in the GenI surface realizer (Kow,
2007) and regenerated an example from the
ICONOCLAST generator (Power et al, 2003).
The example we used is represented by the fol-
lowing input semantics:
h1: elixir(e)
h2: fda(f)
h3: elixir plus(p)
h4: gestodene(g)
h5: contain(e g)
h6: ban(f e)
h7: approve(f p)
h8: concession(h6 h7)
h9: cause(h5 h6)
h10: contain(p o)
h11: oestradiol(o)
h12: cause(h10 h7)
50
h6:ban(f e)
D
c
e



H
H
H
S



H
H
H
NP
RX
[idx:e]
VP



H
H
H
V
banned by
NP
RX
[idx:f]
Punct
.
h2:drug(e)
NP



H
H
H
NP?
[pron:no]
TextPhrase


H
H
, NP


H
H
DET N
drug
,
(a) Elementary trees for (6)
D
c
e




H
H
H
H
S






H
H
H
H
H
H
NP




H
H
H
H
NP
RX
[
idx:e
pron:no
]
TextPhrase


H
H
, NP


H
H
DET N
drug
,
VP


H
H
V
banned by
RX
[idx:f]
Punct
.
(b) Derived tree for (6)
Figure 3: Pronouns not allowed before an appositive
h2:contain(e a)
S[s pron:?X]





H
H
H
H
H
NP
RX
[
pron:?X
idx:e
]
VP



H
H
H
V
contain
NP
RX
[idx:f]
h2:cause(h7 h0)
NP




H
H
H
H
NP?
?
?
?
pred:h7
pron:no
idx :e
?
?
?
TextClause



H
H
H
Conn
since
S?
?
?
?
idx :h0
subj :e
s pron:yes
?
?
?
(a) Elementary trees for (7)
NP





H
H
H
H
H
NP?
?
?
?
pred:h7
pron:no
idx :e
?
?
?
TextClause




H
H
H
H
Conn
since
S[s pron:yes]




H
H
H
H
NP
RX
[
pron:yes
idx :e
]
VP


H
H
V
contain
NP
RX
[idx:f]
(b) Derived tree for (7)
Figure 4: Obligatory pronouns in parenthetical subordinate clauses
ICONOCLAST is a constraint-based system
which integrates text planning, document plan-
ning and pronominalization to generate all possi-
ble paraphrases for a given input. It uses a version
of Centering Theory (Grosz et al, 1995) adapted
to natural language generation to decide when to
pronominalize noun phrases in the generated text.
ICONOCLAST has an overgenerate and test ap-
proach, where all possible paraphrases are gener-
ated and the solutions are ranked according to a set
of soft constraints. The system generated 172 so-
lutions for the above input, of which (8) illustrates
the top three:
(8) a Since Elixir contains gestodene it is
banned by the FDA. However, the FDA
approves Elixir Plus since Elixir Plus
contains oestradiol.
b Elixir contains gestodene so it is
banned by the FDA. However, the FDA
approves ElixirPlus since ElixirPlus
contains oestradiol.
c Elixir is banned by the FDA since it
contains gestodene. However,
ElixirPlus is approved by the FDA
since it contains oestradiol.
We have regenerated the same text, using only
a surface realizer and the grammar described in
the previous sections, without a referring expres-
sion generation module. A post-processing script
transforms RX nodes into a pronoun when they
have the relevant feature ([pron:yes]) and into
a name when the [pron] feature is missing or its
value is no. The surface realizer produced 208 so-
lutions for the same input, of which 96 contained
parentheticals. Some of the output is illustrated in
(9). Since sentence final parenthetical construc-
tions are impossible to distinguish from sentence-
final subordinate clauses in many cases, there is an
overlap between the solutions generated by ICON-
OCLAST and the 96 solutions generated by our
51
(9) a The FDA bans Elixir since Elixir contains gestodene. However, Elixir Plus (since it contains
oestradiol) is approved by the FDA.
b Since Elixir Plus contains oestradiol, although the FDA bans Elixir (since it contains
gestodene), Elixir Plus is approved by the FDA.
c Elixir contains gestodene. Consequently, Elixir is banned by the FDA. However, Elixir Plus
(since it contains oestradiol) is approved by the FDA.
d Elixir Plus contains oestradiol. Consequently, although the FDA bans Elixir (since it contains
gestodene), the FDA approves Elixir Plus.
e Elixir Plus (since it contains oestradiol) is approved by the FDA (although it bans Elixir since
it contains gestodene).
f The FDA bans Elixir (since it contains gestodene). However, Elixir Plus is approved by the
FDA since Elixir Plus contains oestradiol.
grammar which contain parentheticals. Also, de-
spite the fact that the two systems use the same
discourse connectives and a very similar grammar,
there are slight differences in the constructions
produced. For example, ICONOCLAST allows
subordinating conjunctions to ?dominate? coordi-
nating conjunctions, producing solutions like the
one in (10), although these solutions are assigned
at least 4 defects in all cases. These constructions
are not allowed in our grammar.
(10) Although Elixir contains gestodene so it is
banned by the FDA ElixirPlus contains
oestradiol so it is approved by the FDA.
Though comparing the generated solutions is
not a straightforward task because of these subtle
differences and the sheer number of the solutions
produced, the two systems do generate a number
of very similar outputs, including the ones shown
in (8). However, a significant difference is that our
system generates coherent texts which include par-
enthetical constructions, and which are not gener-
ated by ICONOCLAST at all.
7 Related work
Our grammar design was inspired by three
discourse-level extensions of Lexicalized Tree Ad-
joining Grammar. A common idea behind all these
approaches is to build an integrated text under-
standing or generation system in which the same
mechanisms are used for the sentence and dis-
course levels.
DLTAG (Webber, 2004) is an extension of
LTAG in which discourse syntax is projected by
different types of discourse connectives. In this
approach discourse-level syntax is considered to
be a separate layer on top of sentence-level syntax
and there are two kinds of discourse connectives:
anaphoric and structural (Webber et al, 2003).
This analysis is not suitable for natural language
generation systems which need to have an explicit
representation for the arguments of discourse con-
nectives.
G-TAG (Danlos, 2000) is another discourse-
level extension of TAG where underspecified ?g-
derivation trees? are created for a conceptual input
and grouped into lexical databases. A g-derivation
tree specifies a set of surface variants, one of
which is produced by linearization of the g-derived
tree. The other surface variants are created by a
post-processing module. While this methodology
efficiently reduces the search space of solutions by
grouping them together, it assumes that all variants
of the same sentence can be generated in the same
discourse context.
Most recently, Danlos (2008) introduces D-
STAG, a discourse level synchronous TAG cou-
pled with Segmented Discourse Representation
Theory (Asher, 1993). In this framework the sen-
tential grammar (S-TAG) and the discourse gram-
mar (D-STAG) are not integrated, therefore dis-
courses where arguments of discourse relations
come from discontinuous text spans (as in rela-
tive clauses or other types of parentheticals) are
not handled by the theory.
8 Conclusions
We have presented an argument against modular-
izing linguistic information in natural language
generation systems. We have argued that com-
plex linguistic constructions which require inter-
actions between several system components are
best represented in natural language generation
52
systems using an integrated grammar. As an ex-
ample, we have presented the problem of gener-
ating parenthetical constructions. Current natural
language generation systems either do not gener-
ate these constructions at all, or if they do, they
do not have a principled approach to the prob-
lem and generate parentheticals by adding more
modules to a pipeline. We have shown that par-
entheticals can be generated in a principled way
using a surface realizer, when it is equipped with
an integrated grammar which incorporates infor-
mation about syntax, discourse and referring ex-
pressions. The solutions produced by our surface
realizer demonstrate that this approach enhances
the fluency of the generated text and the flexibility
of generation systems, without adding extra com-
ponents or changing the system?s architecture.
References
D.E. Appelt. 1985. Planning English sentences. Cam-
bridge University Press, Cambridge.
N. Asher. 1993. Reference to Abstract Objects in En-
glish. Kluwer, Dordrecht.
E. Banik and A. Lee. 2008. A study of parenthet-
icals in discourse corpora ? implications for NLG
systems. In Proceedings of LREC 2008, Marrakesh.
C. B. Callaway and J. C. Lester. 1997. Dynami-
cally improving explanations: A revision-based ap-
proach to explanation generation. In Fifteenth Inter-
national Joint Conference on Artificial Intelligence,
pages 952?58, Nagoya, Japan.
C. B. Callaway. 2003. Integrating discourse markers
into a pipelined natural language generation archi-
tecture. In ACL ?03: Proceedings of the 41st Annual
Meeting on Association for Computational Linguis-
tics, pages 264?271.
L. Danlos. 2000. G-TAG: A lexicalized formalism for
text generation inspired by Tree Adjoining Gram-
mar. In A. Abeille and O. Rambow, editors, Tree
Adjoining Grammars: Formalisms, linguistic analy-
sis and processing, pages 343?370. CSLI, Stanford.
L. Danlos. 2008. D-STAG: Parsing discourse with
synchronous TAG and SDRT background. In Pro-
ceedings of the Third International Workshop on
Constraints in Discourse (CID?2008) Postdam.
B.J. Grosz, A.K. Joshi, and S Weinstein. 1995. Cen-
tering: a framework for modelling the local co-
herence of discourse. Computational Linguistics,
21(2):203?225.
E. H. Hovy. 1988. Two types of planning in language
generation. In Proceedings of the 26th annual meet-
ing on Association for Computational Linguistics,
pages 179?186, Morristown, NJ, USA. Association
for Computational Linguistics.
Eduard H. Hovy. 1993. Automated discourse gener-
ation using discourse structure relations. Artificial
Intelligence, 63(1-2):341?385.
A. K. Joshi and Y. Schabes. 1997. Tree-Adjoining
Grammars. In Rosenberg and Salomaa, editors,
Handbook of Formal Languages and Automata, vol-
ume 3, pages 69?124. Springer-Verlag, Heidelberg.
A. Kehler. 2002. Coherence, Reference and the Theory
of Grammar. CSLI.
R. Kibble and R. Power. 2004. Optimizing referential
coherence in text generation. Computational Lin-
guistics, 30(4):401?416.
E. Kow. 2007. Surface realisation: ambiguity and
determinism. Ph.D. thesis, Universite de Henri
Poincare.
D. D. McDonald. 1983. Natural language genera-
tion as a computational problem. In M. Brady and
Robert Berwick, editors, Computational Models of
Discourse, pages 209?265. MIT Press.
E. Miltsakaki. 2003. The Syntax-Discourse Interface:
Effects of the Main-Subordinate Distinction on At-
tention Structure. Ph.D. thesis, Department of Lin-
guistics, University of Pennsylvania.
R. Power N. Bouayad-Agha,D. Scott. 2001. The influ-
ence of layout on the interpretation of referring ex-
pressions. In L. Degand Y. Bestgen W. Spooren L.
van Waes, editor, Multidisciplinary Approaches to
Discourse, pages 133?141.
R. Power, D. Scott, and N. Bouayad-Agha. 2003.
Document structure. Computational Linguistics,
29(4):211?260.
J. Robin. 1994. Revision-based generation of Natu-
ral Language Summaries providing historical Back-
ground. Ph.D. thesis, Columbia University.
D. Scott and C. S. Souza. 1990. Getting the message
across in RST-based text generation. In C. Mellish
R. Dale M. Zock, editor, Current Research in Nat-
ural Language Generation, pages 31?56. Academic
Press.
J. Shaw. 2002. Clause Aggregation: An approach
to generating concise text. Ph.D. thesis, Columbia
University.
B. Webber, M. Stone, A. Joshi, and A. Knott. 2003.
Anaphora and discourse structure. Computational
Linguistics, 29(4):545?587.
B.Webber. 2004. D-LTAG: extending lexicalized TAG
to discourse. Cognitive Science, 28(5):751?779.
53
INLG 2012 Proceedings of the 7th International Natural Language Generation Conference, pages 125?127,
Utica, May 2012. c?2012 Association for Computational Linguistics
Natural Language Generation for a Smart Biology Textbook
Eva Banik1, Eric Kow1, Vinay Chaudhri2, Nikhil Dinesh2, and Umangi Oza3
1{ebanik,kowey}@comp-ling.co.uk, Computational Linguistics Ltd, London, UK
2 {chaudhri,dinesh}@ai.sri.com, SRI International, Menlo Park, CA
3umangi.oza@evalueserve.com, Evaluserve, New Delhi, India
1 Application Context
In this demo paper we describe the natural lan-
guage generation component of an electronic
textbook application, called Inquire1. Inquire
interacts with a knowledge base which encodes
information from a biology textbook. The
application includes a question-understanding
module which allows students to ask questions
about the contents of the book, and a question-
answering module which retrieves the corre-
sponding answer from the knowledge base. The
task of the natural language generation mod-
ule is to present specific parts of the answer in
English. Our current generation pipeline han-
dles inputs that describe the biological func-
tions of entities, the steps of biological processes,
and the spatial relations between parts of enti-
ties. Our ultimate goal is to generate paragraph-
length texts from arbitrary paths in the knowl-
edge base. We describe here the natural lan-
guage generation pipeline and demonstrate the
inputs and generated texts. In the demo pre-
sentation we will show the textbook application
and the knowledge base authoring environment,
and provide an opportunity to interact with the
system.
2 The Knowledge Base
The knowledge base contains information from
a college-level biology textbook2, encoded by bi-
1The work described in this paper and presented in
the demo is funded by Vulcan Inc.
2 Reece et al 2010. Campbell biology. Pearson
Publishing.
ologists as part of project HALO at SRI3. The
core of the knowledge base is the CLIB ontol-
ogy4, which is extended with biology-specific in-
formation. The knowledge base encodes entity-
to-event relations (similar to thematic roles in
linguistics), event-to-event relations (discourse
relations), various property values and relations
between properties, spatial relations, cardinality
constraints, and roles that participants play in
events. The input to the generation pipeline is a
set of triples extracted from the biology knowl-
edge base. Currently our content selection in-
cludes either an event and the entities that par-
ticipate in the event, or a set of entities and
spatial relations between them.
3 Generation Grammar and Lexicon
Our generation grammar consists of a set of Tree
Adjoining Grammar (TAG) elementary trees.
Each tree is associated with either a single rela-
tion, or a set of relations in the knowledge base.
As an example, Fig 1 illustrates the mapping
between elementary trees and event participant
relations in the KB for the above input. We
currently associate up to three different elemen-
tary trees with each event and the connected
set of participant relations: an active senten-
tial tree, a passive sentential tree and a complex
noun phrase.
The knowledge base provides concept-to-word
3 Gunning Et al, 2010. Project halo update
progress toward digital aristotle. AI Magazine Fall:33-
58. See also http://www.projecthalo.com/
4http://www.cs.utexas.edu/users/mfkb/RKF/clib.html
125
Figure 1: The grammar of the surface realizer
mappings (a list of synonyms) for every concept,
and the words are used in the generation lexi-
con to anchor elementary TAG trees. Our gen-
eration grammar consists of a set of TAG tree
templates, which are defined as combinations of
tree fragments and are compiled using the XMG
metgrammar toolkit5.
These underspecified elementary trees are fur-
ther specified in the generation lexicon, which
maps concepts onto elementary tree templates,
and associates a word (an anchor) with the
tree, along with other idiosynchratic information
(e.g., preposition choice). We create a genera-
tion lexicon dynamically at run-time, by map-
ping tree templates onto concepts based on the
number and types of participants, and the lexi-
cal information associated with the event (e.g.,
the preposition requirements of the verb).
Concept names for entities are included in
the elementary trees as features on the corre-
sponding NP nodes. These features form part
of the input to the referring expression genera-
tion module, which looks up the concept name
5https://sourcesup.renater.fr/xmg/
in the concept-to-word mapping to obtain a list
of possible noun phrases.
4 Realization
Our natural language generation pipeline is cen-
tered around the GenI surface realizer6,7. The
set of triples yielded by content selection are first
aggregated and converted to GenI?s input for-
mat, a set of flat semantic literals. We then feed
this input to GenI to produce an underspecified
surface form in which referring expressions are
still underspecified:
NP is detach from NP resulting in NP at NP
NP detach from NP resulting in NP at NP
Detachment of NP from NP resulting in NP at NP
A post-processing module carries out refer-
ring expression generation and morphological re-
alization to produce the fully specified output.
6 Kow, Eric. 2007. Surface realisation: ambiguity
and determinism. Doctoral Dissertation, Universite de
Henri Poincare - Nancy 1.
7 Banik, Eva 2010. A minimalist approach to gen-
erating coherent texts. Phd thesis, Department of Com-
puting, The Open University
126
Question Answering & Reasoning Algorithms
Event Instance 
Content Selection
Set of triples
Input aggregation and conversion +Stylistic control
Knowledge Base
Realization with GenI
Morphology &referring expression generation
Semantic literals +input parameters
Ranking
Underspecified realizations
Linguistic Resources
Generation Lexicon
Grammar: Description of TAG tree templates
Concept-to-Wordmappings
Mapping of KB relationsto TAG tree templates
Morphological lexicon
Verb frames (preposition choice)
NLG Pipeline
Figure 2: Linguistic resources and the generation pipeline
Our referring expression realization algorithm
performs further semantic aggregation where
necessary to produce cardinals (?two chromo-
somes?), and decides on a suitable determiner
based on previous mentions of instance names
and subclasses in the discourse context (def-
inite/indefinite determiner, ?another? or ?the
same?). For the input shown in Fig 1, our sys-
tem will produce the following three realizations:
1. A sister chromatid detaches from another sister chro-
matid resulting in two chromosomes at a kinetochore.
2. A sister chromatid is detached from another sister
chromatid resulting in two chromosomes at a kinetochore.
3. Detachment of a sister chromatid from another sister
chromatid resulting in two chromosomes at a kinetochore
We rank the generated outputs based on their
linguistic properties using optimality theoretic
constraints (e.g., active sentences are ranked
above passive sentences), where each constraint
corresponds to a (set of) tree fragments that
contributed to building the tree that appears in
the output. Our system also allows for extra in-
put parameters to be sent to GenI to restrict the
set of generated outputs to fit a specific context
(e.g., syntactic type or focused discourse entity).
Our full natural language generation pipeline is
illustrated in Fig 2.
5 Future Work
We are currently working on extending the sys-
tem to handle more relations and other data
types in the knowledge base. This involves ex-
tending the grammar to new sentence types and
other linguistic constructions, and extending the
content selection module to return more triples
from the knowledge base. Our ultimate goal is
to be able to generate arbitrary ? but in some
sense well-formed ? paths from the knowledge
base as coherent paragraphs of text.
127
INLG 2012 Proceedings of the 7th International Natural Language Generation Conference, pages 141?145,
Utica, May 2012. c?2012 Association for Computational Linguistics
KBGen ? Text Generation from Knowledge Bases as a New Shared Task
Eva Banik1, Claire Gardent2, Donia Scott3, Nikhil Dinesh4, and Fennie Liang5
1ebanik@comp-ling.co.uk, Computational Linguistics Ltd, London, UK
2claire.gardent@loria.fr, CNRS, LORIA, Nancy, France
3D.R.Scott@sussex.ac.uk, School of Informatics, University of Sussex, Brighton, UK
4dinesh@ai.sri.com, SRI International, Menlo Park, CA
5fennie.liang@cs.man.ac.uk, School of Computer Science, University of Manchester, UK
1 Introduction
In this paper we propose a new shared task, KB-
Gen, where the aim is to produce coherent descrip-
tions of concepts and relationships in a frame-based
knowledge base (KB). We propose to use the AURA
knowledge base for the shared task which contains
information about biological entities and processes.
We describe how the AURA KB provides an appli-
cation context for NLG and illustrate how this ap-
plication context generalizes to other biology KBs.
We argue that the easy availability of input data and
a research community ? both domain experts and
knowledge representation experts ? which actively
uses these knowledge bases, along with regular eval-
uation experiments, creates an ideal scenario for a
shared task.
2 Application Context and Motivation
One of the research challenges in the knowledge rep-
resentation community is to model complex knowl-
edge in order to be able to answer complex ques-
tions from a knowledge base (see e.g. the Deep
Knowledge Representation Challenge Workshop at
KCAP 20111). There are several applications of
such knowledge bases, perhaps most recently and
most prominently in the bioinformatics and educa-
tional informatics domain, where there are available
knowledge bases and reasoners that help scientists
answer questions, explain connections between con-
cepts, visualize complex processes, and help stu-
dents learn about biology. These uses of a knowl-
edge base are however difficult to implement with-
1http://sites.google.com/site/dkrckcap2011/home
out presenting the resulting answers and explana-
tions to the user in a clear, concise and coherent way,
which often requires using natural language.
2.1 The AURA Knowledge Base
The AURA biology knowledge base developed by
SRI International (Gunning et al, 2010) encodes in-
formation from a biology textbook (Reece et al,
2010)2. The purpose of this knowledge base is
to help students understand biological concepts by
allowing them to ask questions about the material
while reading the textbook. The KB is built on top
of a generic library of concepts (CLIB, Barker et al,
2001), which are specialized and/or combined to en-
code biology-specific information, and it is orga-
nized into a set of concept maps, where each con-
cept map corresponds to a biological entity or pro-
cess. The KB is being encoded by biologists and
currently encodes over 5,000 concept maps.
The AURA KB and its question answering sys-
tem is integrated with an electronic textbook appli-
cation3. The applicaton allows the students to ask
complex questions about relationships between con-
cepts, which are answered by finding a possible path
between the two concepts. The results are presented
to the students as graphs, for example the answer
produced by the system in response to the question
?what is the relationship between glycolysis and glu-
cose?? is illustrated in Fig 1.
These graphs are simplified representations of
2The development of the AURA knowledge base and related
tools and applications was funded by Vulcan Inc.
3A demo of the application will be presented in the demo
session at INLG 2012
141
Figure 1: Relationship between glycolysis and glucose
a path in the knowledge base that connects two
concepts, because presenting the full concept map
where the path was found would make it difficult for
the students to clearly see the relationship. However,
this simplification often obscures the connection by
not showing relevant information.
Given the inclusion of a few more relations from
the concept map of glycolysis (Fig 2), the answer to
the question could be generated as a complex sen-
tence or a paragraph of text, for example: ?Phos-
phorylation of glucose is the first step of the energy
investment phase of glycolysis? or ?In the first step
of the energy investment phase of glycolysis, called
phosphorylation, hexokinase catalyses the synthesis
of glucose-6-phosphate from glucose and a phos-
phate ion.?
2.2 BioCyc
Another situation in which graph-based representa-
tions are presented to the user is metabolic pathway
and genome databases, such as the BioCyc knowl-
edge base. BioCyc describes the genome, metabolic
pathways, and other important aspects of organisms
such as molecular components and their interactions
and currently contains information from 1,763 path-
Figure 2: Concept map of glycolysis
way/genome databases4.
When users query parts of the BioCyc knowledge
base, the system automatically produces a graph
to visualize complex biological processes. For ex-
ample, Fig 3 illustrates an automatically generated
graph from the knowledge base which shows the
process of glycolysis in an E. coli cell. Hovering the
mouse over the ? and 	 signs on the graph brings
up popups with additional information about gene
expressions , detailed chemical reactions in the pro-
cess, enzymes activated by certain chemicals, etc..
Figure 3: The process of glycolysis in E.coli
3 Input Data for Generation
Although there is a clear benefit from visualizing
complex processes in a graph form, one also has to
4http://www.biocyc.org
142
be well-versed in the notation and details of biolog-
ical processes in order to make sense of these rep-
resentations. Students of biology and non-experts
would certainly benefit from a more detailed ex-
planation of the process, presented as a few para-
phraphs of text along with graphs to emphasize the
most salient features of processes.
The paths and relations returned by reasoning al-
gorithms also present a good opportunity to pro-
vide inputs for natural language generation. These
chunks of data typically contain the right amount of
data because they consist of the information needed
to answer a question or describe a concept. Ad-
ditionally, many knowledge bases (including both
BioCyc and AURA) are encoded in a frame-based
representation, which has the advantage that frames
naturally correspond to linguistic units.
Frame-based systems (Minsky, 1981) are based
around the notion of frames or classes which repre-
sent collections of concepts. Each frame has an as-
sociated set of slots or attributes which can be filled
either by specific values or by other frames. Intu-
itively, frames correspond to situations, and each ter-
minal in the frame corresponds to answers to ques-
tions that could be asked about the situation, in-
cluding the participants in the situation, causes and
consequences, preceding and following situations,
purpose, etc. Frame-based representations may ei-
ther contain frames of generic concepts or instance
frames which represent information about particular
instances. Frames also have a kind-of slot, which
allows the assertion of a frame taxonomy, and the
inheritance of slots.
In the knowledge representation community,
frame-based representations are popular because
they make the encoding process more intuitive.
From a natural language generation perspective,
each frame (or a set of slots) corresponds to a lin-
guistic unit (sentence, noun phrase, clause, verb
phrase, etc), depending on the type of the frame and
the slots it contains. This organization of concepts
and relations in the knowledge base makes it easier
to select chunks of data from which coherent texts
can be generated.
Slots in these frame-based representations also
naturally correspond to the kind of flat semantic
representations and dependency structures that have
served as input to surface realization (Koller and
Striegnitz, 2002; Carroll and Oepen, 2005; White,
2006; Gardent and Kow, 2007; Nakatsu and White,
2010).
4 The shared task
We propose two tracks for the KBGen shared task: a
?complex surface realization? track, where the task
is to generate complex sentences from shorter in-
puts, and a ?discourse generation? track, where the
task is to generate longer texts made up from several
paragraphs. In the following, we describe the data
set from which the input to generation will be se-
lected; the methology we plan to use to extract text
size input for the generation challenge; and the two
tracks making up the KBGen challenge.
4.1 The AURA knowledge base as Input
Dataset
We propose to use the AURA knowledge base as
input data for the shared task for several reasons.
AURA contains a number of relations and therefore
provides varied input for generation5. The AURA
knowledge base contains linguistic resources that
can be used for generation (a morphological lexi-
con and a list of synonyms for each concept) and
the electronic textbook provides an application con-
text to evaluate the generated texts. There are regular
evaluation efforts to assess the educational benefits
of using the textbook application, and the next round
of these experiments will involve over 400 students
and biology teachers who will use the application
over an extended period of time. The evaluation of
the outputs generated for the shared task could form
part of these experiments.
4.2 Selecting Text Size Content for the Shared
Task
We propose to select data from the knowledge base
manually or semi-automatically, by selecting a set
of concepts to be described and including relevant
relations associated with the concepts. We would
first select a set of concept maps that are encoded in
most detail and have been reviewed by the encoders
for quality assurance. The input data for each con-
cept will then be a manually selected set of frames
5If there is interest, the systems developed to generate from
AURA could also be applied to the BioCyc data, which has a
more restricted set of relations.
143
from the concept map. The selected relations will be
reviewed one more time for quality and consistency
to filter out any errors in the data.
If there is interest in the community, we can
also envision a content selection challenge which
could provide input to the generation task. Although
frames in the knowledge base correspond well to
chunks of data for generation of descriptions, con-
tent selection for other communicative goals is far
from a trivial problem. One such challenge could
be for example comparing two concepts, or explain-
ing the relation between a process and its sub-type
(another process that is taxonomically related, but
different in certain parts).
4.3 Complex Surface Realization Track
For the complex surface realization track, a small
number of frames would be selected from the knowl-
edge base along with a small number of other rel-
evant relations (e.g., important parts or properties
of certain event participants, or certain relations be-
tween them, depending on the context). The output
texts to be generated would be complex sentences
describing the central entity/event in the data, or the
relationship between two concepts, such as the gly-
colysis example in section 2.1. This task would
involve aggregation and generating intrasentential
pronouns governed by syntax where necessary, but
it would not require the generation of any discourse
anaphora or referring expressions.
This track will differ from the deep generation
track of the Surface Realization Shared Task both in
form and in content. The form of the KBGen input
is a concept map extracted from an ontology rather
than a deep semantics extracted by conversion from
dependency parse trees. Similarly, its content is that
of a biology knowledge base rather than that of the
Penn Treebank textual corpus.
4.4 Discourse Generation Track
Inputs for the discourse generation task would in-
clude most frames from the concept map of an entity
or process. The output would be longer paragraphs
or 2-3 paragraphs of text, typically a description of
the subevents, results, etc, of a biological process,
or the description of the structure and function of an
entity. This task would involve text structuring and
the generation of pronouns.
4.5 Lexical Resources and potential
multilingual tracks
The knowledge base provides a mapping from con-
cepts to lexical items and a list of synonyms. It
also provides information about how specific slots
in event frames are mapped onto prepositions.
If there is interest in the community, the lex-
ical resources corresponding to the selected con-
tent could be translated to different languages semi-
automatically: the translation could be attempted
first automatically, with the help of available biol-
ogy/medical lexicons, and then the output would be
hand-corrected. Candidate languages for a multilin-
gual challenge would be French and Spanish. To
run the multilingual tracks we would need to create
multilingual development and test data and would
need to have access to French/Spanish speaking bi-
ologists.
5 Evaluation
Evaluation of the generated texts could be done both
with automatic evaluation metrics and using human
judgements. Automatic evaluation metrics could in-
clude BLUE (Papineni et al, 2002) or measuring
Levenshtein distance (Levenshtein, 1966) from hu-
man written texts. To obtain human judgements, bi-
ologists will be asked to compose texts conveying
the same content as the input for the generated texts.
The human-written texts will be presented to sub-
jects along with the generated outputs to obtain flu-
ency judgements, but the subjects will not be told
which kind of text they are judging. The evaluation
campaign could be coordinated with the evaluation
of the knowledge base and the electronic textbook
application, and/or publicized on social networking
sites or mechanical turk.
6 Next Steps
We invite feedback on this proposal with the aim
of refining our plan and discussing a suitable input
representation for the shared task in the next few
months. If there is sufficient interest in the shared
task, we would make the input data available in the
agreed format in late 2012, with the first evaluation
taking place in 2013. We would like to hear any
comments/suggestions/critisisms about the plan and
we are actively looking for people who would be in-
144
terested in getting involved in planning and running
the challenge.
References
Barker, K., B. Porter, and P. Clark. 2001. A library of
generic concepts for composing knowledgebases.
In Proceedings of the 1st Int Conf on Knowledge
Capture (K-Cap?01), 14?21.
Carroll, J., and S. Oepen. 2005. High efficiency real-
ization for a wide-coverage unification grammar.
2nd IJCNLP .
Gardent, C., and E. Kow. 2007. A symbolic ap-
proach to near-deterministic surface realisation
using tree adjoining grammar. In In 45th Annual
Meeting of the ACL.
Gunning, D., V. K. Chaudhri, P. Clark, K. Barker,
Shaw-Yi Chaw, M. Greaves, B. Grosof, A. Leung,
D. McDonald, S. Mishra, J. Pacheco, B. Porter,
A. Spaulding, D. Tecuci, and J. Tien. 2010.
Project halo update - progress toward digital aris-
totle. AI Magazine Fall:33?58.
Koller, Alexander, and Kristina Striegnitz. 2002.
Generation as dependency parsing. In Proceed-
ings of ACL.
Levenshtein, Vladimir I. 1966. Binary codes capable
of correcting deletions, insertions, and reversals.
Soviet Physics Doklady 10:707?710.
Minsky, Marvin. 1981. Mind design, chapter A
Framework for Representing Knowledge, 95?
128. MIT Press.
Nakatsu, Crystal, and Michael White. 2010. Gen-
erating with discourse combinatory categorial
grammar. submitted to Linguistic Issues in Lan-
guage Technology .
Papineni, Kishore, Salim Roukos, Todd Ward, and
Wei jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. 311?318.
Reece, Jane B., Lisa A. Urry, Michael L. Cain,
Steven A. Wasserman, Peter V. Minorsky, and
Robert B. Jackson. 2010. Campbell biology. Pear-
son Publishing.
White, Michael. 2006. Ccg chart realization from
disjunctive inputs. In Proceedings of the Fourth
International Natural Language Generation Con-
ference, 12?19. Sydney, Australia: Association
for Computational Linguistics.
145
Proceedings of the 14th European Workshop on Natural Language Generation, pages 20?29,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
User-Controlled, Robust Natural Language Generation from an Evolving
Knowledge Base
Eva Banik
Computational
Linguistics Ltd
London, UK
ebanik@comp-ling.com
Eric Kow
Computational
Linguistics Ltd
London, UK
kowey@comp-ling.com
Vinay Chaudhri?
SRI International
Menlo Park, CA
chaudhri@ai.sri.com
Abstract
In this paper we describe a natural lan-
guage generation system which produces
complex sentences from a biology knowl-
edge base. The NLG system allows do-
main experts to discover errors in the
knowledge base and generates certain
parts of answers in response to users?
questions in an e-textbook application.
The system allows domain experts to cus-
tomise its lexical resources and to set pa-
rameters which influence syntactic con-
structions in generated sentences. The
system is capable of dealing with certain
types of incomplete inputs arising from a
knowledge base which is constantly edited
and includes a referring expression gen-
eration module which keeps track of dis-
course history. Our referring expression
module is available for download as the
open source Antfarm tool1.
1 Introduction
In this paper we describe a natural language gen-
eration system we have developed to interface
with a biology knowledge base. The knowledge
base (KB) encodes sentences from a biology text-
book, and the ultimate goal of our project is to
develop an intelligent textbook application which
can eventually answer students? questions about
biology2 (Spaulding et al, 2011).
?The work reported in this paper was supported by fund-
ing from Vulcan, Inc. We would also like to thank the mem-
bers of the Inquire Biology development team: Roger Cor-
man, Nikhil Dinesh, Debbie Frazier, Stijn Heymans, Sue Hi-
nojoza, David Margolies, Adam Overholtzer, Aaron Spauld-
ing, Ethan Stone, William Webb, Michael Wessel and Neil
Yorke-Smith.
1https://github.com/kowey/antfarm
2http://www.aaaivideos.org/2012/
inquire_intelligent_textbook/
The natural language generation module is part
of a larger system, which includes a question un-
derstanding module, question answering and rea-
soning algorithms, as well as an answer presenta-
tion module which produces pages with informa-
tion from the KB. We measure the progress and
consistency of encoding by asking ?what is an X??
type questions of the application and evaluate the
quality of answers. In response to these questions,
the system generates ?glossary pages? of concepts,
which display all information about concept X in
the KB that are deemed relevant. The NLG mod-
ule is used for two purposes in our system: to
check the completeness and consistency of the KB
(instead of looking at complex graphs of the en-
coded knowledge, it is easier to detect errors in
natural language sentences), and to present parts
of answers in response to questions.
One goal of our project was to develop a tool
which empowers biology teachers to encode do-
main knowledge with little training in formal
knowledge representation. In the same spirit, we
aimed to develop an NLG system which allowed
domain experts to easily and intuitively customize
the generated sentences as much as possible, with-
out any training on the grammar or internal work-
ings of the system. This was necessary because
many domain-specific concepts in the KB are best
expressed by biology terminology and linguistic
constructions specific to the domain. We devel-
oped a utility which allows encoders to not only
associate lexical items with concepts in the KB but
also customise certain lexical parameters which
influence the structure of sentences generated to
describe events.
Another requirement was robustness: since the
knowledge base is constantly edited, the NLG sys-
tem had to be able to deal with missing lexical in-
formation, incomplete inputs, changing encoding
guidelines, and bugs in the KB as much as possi-
ble. The system also had to be flexible in the sense
20
Figure 1: Architecture of the AURA NLG system
that it had to be able to generate different versions
of the same output to suit specific contexts or types
of concepts in its input. Our system therefore gen-
erates all possible realizations for a given input,
and allows the answer presentation module to send
parameters to determine which output is returned
in a specific context.
After describing the architecture of the NLG
module in detail we explain how the system is able
to deal with unseen combination of event-to-entity
relations when describing events. We illustrate the
utility we developed to allow domain experts to
customize the system?s output by adding parame-
ters to lexical entries associated with concepts.
2 Related Work
Work on natural language generation from ontolo-
gies and knowledge bases tends to fall into two
groups. On the one hand, there are tools for ontol-
ogy verbalization which tend to handle a limited
number of relations, and where the goal of the sys-
tem is to help the work of knowledge engineers.
These systems produce template based outputs,
and the texts closely follow the structure of the
ontology (Wilcock, 2003; Galanis and Androut-
sopoulos, 2007). Some of these systems attempt
to minimize reliance on domain-specific linguistic
resources and attempt to detect words in the labels
of the ontology to use as lexical items (Mellish and
Sun, 2005). On the other hand there are NLG sys-
tems which take their input from complex knowl-
edge bases (Reiter et al, 2003; Paris, 1988) and
produce fluent texts geared towards users other
than knowledge engineers. These systems pro-
duce outputs tailored to the user or the context
and they are difficult for non-NLG-experts to cus-
tomize or port to a different domain. Our system
falls halfway between these two groups: like on-
tology verbalizers, we wanted to produce output
for all inputs, using ontology labels if necessary in
the absence of lexical entries. However, like so-
phisticated NLG systems, we also wanted to gen-
erate good quality output for inputs for which the
system had lexical resources, and we also wanted
to be able to tailor the generated output to the con-
text in which it is displayed. Our input was also
more expressive than the input of ontology verbal-
izers, because of the presence of cardinality con-
straints and co-references in our KB. Our work is
perhaps most closely related to the MIAKT sys-
tem which also allows domain experts to edit lex-
ical knowledge and schemas (Bontcheva, 2004;
Bontcheva and Wilks, 2004). Like MIAKT, we
also aimed to develop an NLG system which can
be easily maintained as the KB changes.
3 Architecture of the AURA NLG system
Our NLG system generates complex sentences
from the AURA knowledge base (Gunning et al,
2010), which contains information from a college-
level biology textbook. AURA is a frame-based
KB which encodes events, the entities that partici-
pate in events, properties, and roles that the entities
play in an event (e.g., catalyst, reactant, messen-
ger, parent). The KB specifies relations between
these types, including event-to-entity, event-to-
event, event-to-property, entity-to-property. The
AURA KB is built on top of the CLIB ontology of
general concepts (Barker et al, 2001), which was
extended with biology-specific information. The
KB consists of a set of concept maps, which de-
scribe all the statements that are true about a con-
cept in our KB. The input to our NLG system is
a set of relations extracted from the KB either in
response to users? questions or when generating
glossary pages that describe specific concepts in
21
detail. The generation pipeline consists of four
main stages: content selection, input conversion,
realisation and referring expression generation, as
illustrated in Fig1.
3.1 Content Selection
Question answering and reasoning algorithms that
return answers or other content in AURA are not
engineered to satisfy the purposes of natural lan-
guage generation. The output of these algorithms
can be best thought of as pointers to concepts in
the KB, which need to be described to provide an
answer to the user. In order for the answer to be
complete in a given context, the output of reason-
ing algorithms have to be extended with additional
relations, depending on the specific question that
was asked, and the context in which the answer
was found in the KB. The relations selected from
the KB also vary depending on the type of con-
cept that is being described (event, entity, role,
property). For example, a user might ask ?What
is a catalyst??. To answer this question, AURA
will retrieve entities from the KB (?role players?)
which play the role of catalyst in various events.
For example, it will find ?adenylyl cyclase?, which
is defined in the KB as a universal catalyst, i.e.,
this information is encoded on the concept map of
Adenylyl cyclase and is regarded as a ?universal
truth?. In this case, our content selection algorithm
will return a single plays triple, and the NLG sys-
tem will produce ?Adenylyl cyclase is a catalyst?.?
Another entity that will be returned in response to
the question is ?ribosomal RNA?. However, ribo-
somal RNA is a catalyst only in specific situations,
and therefore we need to give more detail on the
contexts in which it can play the role of a catalyst.
This includes the event in which ribosomal RNA
is a catalyst, and perhaps the larger process dur-
ing which this event occurs. Accordingly, content
selection here will return a number of relations (in-
cluding agent, object, subevent), and our NLG
system will produce:
?In translation elongation, ribosomal RNA is a
catalyst in the formation of a peptide bond by the
ribosomal RNA and a ribosome.?
Similarly, for ?triose phosphate dehydrogenase?
we will produce
?In energy payoff phase of glycolysis, NAD plus is
converted by a triose phosphate dehydrogenase to
a hydrogen ion, an NADH and a PGAP. Here, the
triose phosphate dehydrogenase is a catalyst.?
For ?cellulose synthase? the situation is slightly
different, because the event in which this entity
plays the role of catalyst is not part of a larger pro-
cess but the function of the entity. So we need
slightly different information to produce the cor-
rect sentence: ?The function of cellulose synthase
is conversion of a chemical in a cell to cellulose.
Here, a cellulose synthase is a catalyst.?
The task of the AURA content selection module is
to determine what information to include for each
entity or event that was returned as the answer to
the question. We do this by retrieving sets of rela-
tions from the KB that match contextual patterns.
We also filter out relations which contain overly
generic classes (e.g., Tangible-Entity), and any du-
plication arising from the presence of inverse rela-
tions or inferences in the KB. The output of con-
tent selection is a structured bundle (Fig. 2), which
contains
(1) the relations that form the input to NLG
(2) information about concepts in the input: what
class(es) they belong to, cardinality constraints
(3) parameters influencing the style of output texts.
3.2 Input Conversion
The realisation phase in our system is carried out
by the GenI surface realizer (Kow, 2007), using
a Tree-Adjoining Grammar (Joshi and Schabes,
1997). The task of the input conversion module
is to interpret the structured bundles returned by
content selection, and to convert the information
to GenI?s input format. We parse the structured
bundles, perform semantic aggregation, interpret
parameters in bundles which influence the style of
the generated text, and convert triples to semantic
literals as required by GenI.
4 Handling Unseen Combinations of
Relations
As Fig 3 shows, a combination of event-to-entity
relations are associated with elementary trees in
the grammar to produce a full sentence. The do-
main of the relations associated with the same
tree is the event which specifies the main pred-
icate of the sentence and the range of the rela-
tions are entities that fill in the individual argu-
ment and modifier positions. Depending on the
event, different relations can be used to fill in the
subject and object positions, and verbs might de-
termine the prepositions needed to realize some of
the arguments. Ideally the mapping between sets
22
(TRIPLES-DATA
:TRIPLES
((|_Cell56531| |has-part| |_Ribosome56523|)
(|_Ribosome56523| |has-part| |_Active-Site56548|)
(|Enzyme-Synthesis17634| |base| |_Cell56531|)
(|Enzyme-Synthesis17634| |raw-material| |_Free-Energy56632|)
(|Enzyme-Synthesis17634| |raw-material| |_Monomer56578|)
(|Enzyme-Synthesis17634| |raw-material| |_Activation-Energy56580|)
(|Enzyme-Synthesis17634| |raw-material| |_Monomer56581|)
(|Enzyme-Synthesis17634| |raw-material| |_Amino-Acid56516|)
(|Enzyme-Synthesis17634| |result| |_Free-Energy56575|)
(|Enzyme-Synthesis17634| |result| |Protein-Enzyme17635|))
:CONSTRAINTS
((|Enzyme-Synthesis17634| |raw-material| (|at-least| 3 |Amino-Acid|)))
:INSTANCE-TYPES
((|_Ribosome56523| |instance-of| |Ribosome|)
(|_Active-Site56548| |instance-of| |Active-Site|)
(|_Cell56531| |instance-of| |Cell|)
(|_Free-Energy56632| |instance-of| |Free-Energy|)
(|_Monomer56578| |instance-of| |Monomer|)
(|_Activation-Energy56580| |instance-of| |Activation-Energy|)
(|_Monomer56581| |instance-of| |Monomer|)
(|_Amino-Acid56516| |instance-of| |Amino-Acid|)
(|_Free-Energy56575| |instance-of| |Free-Energy|)
(|Enzyme-Synthesis17634| |instance-of| |Enzyme-Synthesis|)
(|Protein-Enzyme17635| |instance-of| |Protein-Enzyme|)
(|Free-Energy| |subclasses| |Energy|)
(|Activation-Energy| |subclasses| |Energy|)
(|Free-Energy| |subclasses| |Energy|))
:CONTEXT NIL
:OUTPUT-PARAMETERS NIL)
A protein enzyme is synthesized in an active site of a ribosome of a cell using at least 3 amino acids and 2 monomers.
This process transforms activation energy and free-energy to another free-energy.
Enzyme synthesis ? a protein enzyme is synthesized in an active site of a ribosome of a cell using at least 3 amino acids
and 2 monomers. This process transforms activation energy and free-energy to another free-energy.
Synthesis of a protein enzyme in an active site of a ribosome of a cell using at least 3 amino acids and 2 monomers. This
process transforms activation energy and free-energy to another free-energy.
Figure 2: An example input bundle and the three outputs generated by our system for this input
S
NP ? S
RX[1]
agent
VP
V RX[2]
object
PP PP PP PP
P RX[3]
destination
P RX[4]
instrument
P RX[5]
origin
P RX[6]
path
S
S? S Punct
Punct N VP
this
process
V RX[7]
require
raw-material
RX
RX? P RX[8]
of
has-part
Figure 3: Tree selection
of event-to-entity relations and sentences would be
given based on encoding guidelines used to cre-
ate the knowledge base. However, the goal of
our project is to continuously expand the knowl-
edge base with more information, encoding new
types of events, and enriching existing events with
more detail as we go along (e.g., by specifying en-
ergy consumption and regulation mechanisms for
processes), therefore our encoding guidelines are
continuously revised. In order to produce output,
our realizer requires a generation lexicon, which
maps sets of relations onto elementary trees in the
grammar. Determining this mapping would re-
quire knowing the number of entities that can be
23
associated with each event type, and the relations
that can be used to express them. However, be-
cause our knowledge base is continuously chang-
ing, neither the number of entities linked to spe-
cific events, nor the types of relations used are
stable and therefore it was impossible to build
such a generation lexicon from the KB. Instead,
we adopted an approach where we detect ?event
frames? in the input of the system, and automat-
ically create entries for them in the generation
lexicon, guessing sentence structure and ordering
based on the event participants. An event frame
is a set of event-to-entity relations which have the
same event in the domain of the relations, and par-
ticipating entities in the range. We currently dis-
tinguish between two types of event frames, de-
pending on the type of the entities in the range of
relations: participant frames (ranges are of type
Tangible-Entity) and energy frames (ranges are
type Energy). An example of a participant frame
and an energy frame extracted from the input il-
lustrated in section 4.2 is illustrated below:
Participant frame:
(Uptake07 path Plasma-membrane78)
(Uptake07 origin Extracellular-Side52)
(Uptake07 destination Cytoplasm39)
(Uptake07 agent Cell-Surface-Receptor79)
(Uptake07 instrument Coated-Vesicle49)
(Uptake07 object Cholesterol08)
Energy frame:
(Uptake07 raw-material Chemical-Energy70)
(Uptake07 raw-material Free-Energy89)
Our input conversion module detects event
frames and automatically creates an entry in
GenI?s generation lexicon for each frame, an-
chored on a noun or verb associated with the event
in our concept-to-word mapping lexicon. The en-
tries link the sets of relations in the frame to a tree
with the same number of arguments, attempting
to place entities that play agent and object par-
ticipants into subject/object positions in the tree if
they exist. Our algorithm also attempts to deter-
mine the best syntactic construction for the spe-
cific combination of participant relations, and de-
cides between selecting an active sentential tree,
a passive sentential tree, a complex noun phrase,
or a combination of these. This process also in-
volves deciding based on the event participants
whether the tree will be anchored on a transitive
verb, an intransitive verb, or a verb with a prepo-
sitional object, and assigning default prepositions
to event participants (unless we have more detail
specified in the lexicon, as described in the next
section). The elementary trees in the grammar
are named after the number of referring expres-
sions and prepositional phrases in the tree, and we
use this naming convention to automatically gen-
erate tree names (or tree family names) for lexi-
cal entries, thereby linking trees in the grammar to
GenI?s generation lexicon. The two S-rooted trees
in Fig 3 were selected based on automatically gen-
erated lexical entries for the two frames above.
4.1 Realisation
The GenI surface realizer selects elementary TAG
trees for (sets of) relations in its input and com-
bines them using the standard operations of sub-
stitution and adjunction to produce a single de-
rived tree. We have developed a feature-based lex-
icalized Tree Adjoining Grammar to generate sen-
tences from relations in the KB. Our grammar has
two important properties, following the approach
in (Banik, 2010):
(1) our grammar includes discourse-level elemen-
tary trees for relations that are generated in sepa-
rate sentences, and
(2) instead of the standard treatment of entities as
nouns or NPs substituted into elementary trees,
our grammar treats entities as underspecified re-
ferring expressions, leaving the generation of noun
phrases to the next stage. The underspecified re-
ferring expressions replace elementary trees in the
grammar, which the generator would otherwise
have to combine with substitution. This under-
specification saves us computational complexity
in surface realisation, and at the same time allows
us to make decisions on word choice at a later
stage when we have more information on the syn-
tax of the sentence and discourse history.
The output of the realizer is an underspecified
text in the form of a sequence of lemma - feature
structure pairs. Lemmas here can be underspeci-
fied ? instead of an actual word, they can be an in-
dex or a sequence of indices pointing to concepts
in the KB. The syntax and sentence boundaries
are fully specified, and the output can be one or
more sentences long. The feature structures asso-
ciated with lemmas include all information neces-
sary for referring expression generation and mor-
phological realisation, which is performed in the
next phase. To give an example, the set of rela-
tions below would produce an output with 8 un-
derspecified referring expressions (shown as RX),
distributed over two sentences:
(Uptake07 path Plasma-membrane78)
(Uptake07 origin Extracellular-Side52)
24
(Uptake07 destination Cytoplasm39)
(Uptake07 agent Cell-Surface-Receptor79)
(Uptake07 instrument Coated-Vesicle49)
(Uptake07 object Cholesterol08)
(Uptake07 raw-material Chemical-Energy70)
(Uptake07 raw-material Free-Energy89)
NP(Uptake07) ? RX[1] absorb RX[2] to RX[3] of RX[8]
with RX[4] from RX[5] through RX[6]. This process re-
quires RX[7].
The elementary trees selected by the realizer for
this output, and the correspondences between re-
lations and referring expressions are illustrated in
Fig.3.
4.2 Referring Expression Generation
The final stage in the NLG pipeline is performing
morphological realisation and spelling out the re-
ferring expressions left underspecified by the real-
isation module. The input to referring expression
generation is a list of lemma - feature structure
pairs, where lemmas are words on leaf nodes in
the derived tree produced by syntactic realisation.
In our system, some of the lemmas can be unspec-
ified, i.e., there is no word associated with the leaf
node, only a feature structure. For these cases, we
perform lexicon lookup and referring expression
generation based on the feature structure, as well
as morphological realisation. To give an example,
the input illustrated in the previous section will be
generated as
?Uptake of cholesterol by human cell? a cell
surface receptor absorbs cholesterol to the cyto-
plasm of a human cell with a coated vesicle from
an extracellular side through a plasma membrane.
This process requires chemical energy and free-
energy.?
Many concept labels in our ontology are very
complex, often giving a description of the concept
or the corresponding biology terminology, and
therefore these labels can only be used for NLG
under specific circumstances. To overcome this
problem, we have created a lexicon that maps con-
cept names to words, and the grammar has control
over which form is used in a particular construc-
tion. Accordingly, we distinguish between two
types of underspecified nodes:
? NP nodes where the lexical item for the
node is derived by normalizing the concept
class associated with the node (Uptake-Of-
Cholesterol-By-Human-Cell ? ?uptake of
cholesterol by human cell?)
? RX (referring expression) nodes where lex-
ical items are obtained by looking up class
names in the concept-to-word mapping lexi-
con (Uptake-Of-Cholesterol-By-Human-Cell
? ?absorb?)
The feature structures on RX nodes in the out-
put of GenI describe properties of entities in the in-
put, which were associated with that specific node
during realisation. The feature structures specify
three kinds of information:
? the identifier (or a list of identifiers) for the
specific instances of entities the RX node
refers to
? the KB class for each entity
? any cardinality constraints that were asso-
ciated with each entity for the relation ex-
pressed by the tree in which the RX node ap-
pears
We define cardinality constraints as a triple (Do-
main, Slot, Constraint) where the Constraint itself
is another triple of the form (ConstraintExpres-
sion, Number, ConstraintClass). ConstraintEx-
pression is one of at least, at most, or exactly
and ConstraintClass is a KB class over which the
constraint holds. There is usually (but not neces-
sarily) one or more relations associated with ev-
ery cardinality constraint. We say a triple (Do-
main Slot Range) is associated with a cardinality
constraint (Domain, Slot, (ConstraintExpression,
Number, ConstraintClass)) if
? the Domain and Slot of the associated triple
is equal to the Domain and Slot of the cardi-
nality constraint and
? one of the following holds:
? either (Range instance-of Constraint-
Class) holds for the range of the triple
? or Range is taxonomically related to
ConstraintClass (via a chain of subclass
relations)
We define a referring expression language
(Fig. 4) which describes groups of instance names
(variables) that belong to the same KB class, and
the associated cardinality constraints. Groups
themselves can be embedded within a larger group
(an umbrella), resulting in a complex expression
which gives examples of a concept (e.g., ?three
atoms (a carbon and two oxygens)?). Expressions
25
<refex> = <umbrella> SPACE <refex> | <umbrella>
<umbrella> = <group> ( <refex> )| <group>
<group> = <class> <instances> <constraints>
<instances> = :: <instance> <instances> | <instance>
<constraints> = : <constraint> <constraints> | <constraint>
<constraint> = <op> : <num> | unk : <dash-delimited-string>
<op> = ge | le | eq
Figure 4: Syntax of the referring expression language
in this language are constructed from triples dur-
ing the input conversion stage, when we perform
semantic aggregation. The groups are then passed
through elementary trees by the realisation module
(GenI) to appear in the output as complex feature
structures on leaf nodes of the derived tree. The re-
ferring expression generation module parses these
complex feature values, and constructs (possibly
complex) noun phrases as appropriate.
To illustrate some examples, the following fea-
ture value shows a simple referring expression
group which encodes two entities (Monomer14
and Monomer7) and two cardinality constraints (at
least 2 and at most 5). This expression will be gen-
erated as ?between 2 and 5 monomers?:
Monomer::Monomer14::Monomer7:ge:2:le:5
We also allow more complex cardinality con-
straints which give the general type of an entity
and specify examples of the general type, as in ?at
least 3 organic molecules (2 ATPs and an ethyl al-
cohol)?:
Organic-Molecule:ge:3
(ATP:: ATP80938:eq:2
Ethyl-Alcohol:: Ethyl-Alcohol80922)
The referring expression generation module
makes three main decisions based on the refer-
ring expression, additional feature structures on
the node, and discourse history: it chooses lem-
mas, constructs discriminators, and decides be-
tween singular/plural form. The algorithm for dis-
criminator choice in the referring expression gen-
eration module is illustrated in Fig 5. Our refer-
ring expression generation module, including dis-
course history tracking and determiner choice, is
made available in the Antfarm3 open source tool.
5 Giving Domain Experts Control over
Sentence Structure
By automatically associating event frames with el-
ementary trees we are able to generate a sentence
for all combinations of event-to-entity relations
3https://github.com/kowey/antfarm
Figure 6: Parameters in the concept-to-word map-
ping lexicon
without having to maintain the grammar and gen-
eration lexicon of the realizer as the knowledge
base evolves. However sentences generated this
way are not always well-formed. Events in the
KB can be realized with a wide range of verbs and
nouns, which require different prepositions or syn-
tactic constructions, and different types of events
may require different participants to be their gram-
matical subject or object. To give an example, for
events that have an agent, in the majority of the
cases we get a grammatical sentence if we place
the agent in subject position. If the frame lacks
an agent but has an object, we can usually gener-
ate a grammatical passive sentence, with the ob-
ject participant as the subject. However, it is often
the case that events do not have an agent, and we
get a grammatical (active) sentence by placing an-
other relation in the subject position e.g., base for
the event Store or instrument for Block. Which
26
for each group in the referring expression do
if all members of the group are first mentions and there are no distractors in the history: then
if the group has cardinality constraints: then
upper bound M ? at most M
lower bound N ? at least N (multiple group members in this case are also interpreted as lower bound)
both bounds? between N and M or exactly N
else
one group member? generate an indefinite determiner (a/an)
more than one member? generate a cardinal
end if
end if
if the group is a first mention but there are distractors in the discourse history then
if the group has only one member then
if the group exactly matches one previous mention? another
if the group exactly matches N > 1 previous mentions? the Nth
if there is a 2-member group in the history, and one of the members was mentioned by itself? the other
if the discourse history has more than one distractor? a(n) Nth
end if
if there are multiple group members then
if the group is a subset of a previously mentioned group which has no distractors? N of the
end if
end if
if the group is not a first mention then
if the group has upper and/or lower bounds? the same
if the group has one member only? the
if the group has multiple members? the N
end if
end for
Figure 5: Algorithm for discriminator choice in our referring expression module
event participant can appear in subject and ob-
ject positions depends not only on the type of the
event, but also on the encoding guidelines which
are continuously evolving.
In order to improve the quality of the gener-
ated output, and to give domain experts control
over customizing the system without having to un-
derstand details of the grammar, we extended the
concept-to-word mapping lexicon with parameters
which control preposition choice, and allow cus-
tomization of the position of participating entities.
We developed a graphical user interface which al-
lows encoders (biology domain experts) to add
and edit these lexical parameters as they encode
concepts in the KB.
To give an example, in the absence of a lexical
item and any parameters for the event Glycogen-
Storage, our system would produce the following
default output, attempting to use the concept label
as the main verb of the sentence in an automati-
cally produced generation lexicon entry:
?Glycogen storage ? glycogen is glycogened
storage in a vertebrate in a liver cell and a muscle
cell.?
In order to improve the quality of the output, one
of our biology teachers has customized the param-
eters in the lexicon to yield:
?Glycogen storage ? glycogen is stored by a
Figure 7: Concept map for the event ?Reduction?
vertebrate within a liver cell and a muscle cell.?
This was achieved through a graphical user inter-
face which is part of the tool used for knowledge
encoding, and is illustrated in Fig 6. Our sys-
tem allows encoders to re-generate sentences af-
ter editing the parameters to see the effect of the
changes on the output. The top half of the win-
dow in in Fig 6 allows encoders to associate words
or phrases with concepts, where they can add as
many synonyms as they see fit. One of the syn-
onyms has to be marked as the primary form, to
be used for generation by default.4 For events,
4The concept-to-word mapping lexicon is shared between
the question interpretation and the NLG module, and the ad-
ditional synonyms are currently only used for mapping ques-
27
(a) ?Plastocyanin reduces P700+? (b) ?P700+ receives an electron from plastocyanin.?
Figure 8: Concept-to-word mapping parameters for the two synonyms of Reduction
the primary form is a verb and its nominalization,
and for entities it is a noun. The bottom half of
the window shows the parameter settings for each
synonym associated with the concept. Here the
encoders can specify relations which link the sub-
ject and object of a verb to the event (grammatical
subject/object), and assign prepositions to other
event-to-entity relations for the verb, when it is
used to realize the specified event. There is also
an option to tell the NLG system to ignore some
of the event participants when using a specific verb
for the event. This functionality is used for verbs
that already imply one of the participants. For ex-
ample, the word polymerization already implies
that the result of the event is a polymer. In these
cases there is no need for the NLG system to gen-
erate the implied participant (here, result). An-
other example is the verb reduce, which implies
that the object of the event is an electron. The ed-
itor allows the users to enter different parameter
values for the synonyms of the same event. For
example, the graph in Fig 7 could be described in
at least three different ways:
1. P700+ is reduced by plastocyanin
2. Plastocyanin reduces P700+
3. P700+ receives an electron from plastocyanin.
Here sentences 1 and 2 make no mention of the
electron involved in the process, but sentence 3 ex-
plicitly includes it. In order for the system to cor-
rectly generate sentences 1 and 2, the concept-to-
word mapping parameters for ?reduce? (as a syn-
onym for Reduction) have to include an implied
participant. Otherwise the system will assume that
all participants should be mentioned in the sen-
tence, and it will generate ?P700+ is reduced by
a plastocyanin of an electron?. Fig 8. illustrates
the different concept-to-word mapping parameters
needed for the two synonyms for Reduction in or-
der to generate the above sentences correctly.
tions onto concepts in the KB.
6 Conclusions
We have presented an NLG system which gen-
erates complex sentences from a biology KB.
Our system includes a content selection module,
which tailors the selected relations to the context
in which the output is displayed, and allows the
presentation module to send parameters to influ-
ence properties of generated outputs. We have de-
veloped a referring expression generation module
which generates complex noun phrases from ag-
gregated cardinality constraints and entities in the
input, and keeps track of discourse history to dis-
tinguish mentions of different groups of concepts.
Our system allows biology teachers to detect in-
consistencies and incompleteness in the KB, such
as missing cardinality constraints, errors where
two instances of the concept were added unnec-
essarily (unification errors on entities), and miss-
ing or incorrect relations. To make the system
robust, we have developed an algorithm to pro-
duce sentences and complex noun phrases for un-
seen combinations of event-to-entity relations in
the KB by automatically generating entries in the
lexicon of the GenI surface realizer. Our algorithm
makes default decisions on sentence structure and
ordering based on relations sent to the NLG sys-
tem, expressing the event?s participants. To allow
domain experts to easily improve the default out-
puts generated by our algorithm, we have defined
a framework for adding lexical parameters to con-
cepts, which allow non-NLG-experts to customize
the structure of generated sentences for events in
the KB as they are encoded. Although our system
currently only produces one or two possibly com-
plex sentences, it was designed to ultimately gen-
erate paragraph-length texts. This can be achieved
simply by adding more discourse-level elementary
trees to the grammar of the realizer, since our sys-
tem is already able to handle referring expressions
across sentence boundaries.
28
References
E. Banik. 2010. A Minimalist Architecture for Gener-
ating Coherent Text. Ph.D. thesis, The Open Univer-
sity, UK.
K. Barker, B. Porter, and P. Clark. 2001. A library of
generic concepts for composing knowledgebases. In
Proceedings K-CAP 2001, pages 14?21.
K. Bontcheva and Y. Wilks. 2004. Automatic report
generation from ontologies: the MIAKT approach.
In 9th Int. Conf. on Applications of Natural Lan-
guage to Information Systems, page 324335, Manch-
ester, UK.
K. Bontcheva. 2004. Open-source tools for creation,
maintenance, and storage of lexical resources for
language generation from ontologies. In 4th Conf.
on Language Resources and Evaluation, Lisbon,
Portugal.
D. Galanis and I. Androutsopoulos. 2007. Generat-
ing multilingual descriptions from linguistically an-
notated owl ontologies: the NaturalOWL system. In
INLG07, Schloss Dagstuhl, Germany, page 143146.
D. Gunning, V. K. Chaudhri, P. Clark, K. Barker, Shaw-
Yi Chaw, M. Greaves, B. Grosof, A. Leung, D. Mc-
Donald, S. Mishra, J. Pacheco, B. Porter, A. Spauld-
ing, D. Tecuci, and J. Tien. 2010. Project halo up-
date - progress toward digital aristotle. AI Magazine,
Fall:33?58.
A. K. Joshi and Y. Schabes. 1997. Tree-Adjoining
Grammars. In Grzegorz Rosenberg and Arto Sa-
lomaa, editors, Handbook of Formal Languages
and Automata, volume 3, pages 69?124. Springer-
Verlag, Heidelberg.
E. Kow. 2007. Surface realisation: ambiguity and
determinism. Ph.D. thesis, Universite de Henri
Poincare, Nancy.
C. Mellish and X. Sun. 2005. The semantic web as
a linguistic resource: Opportunities for natural lan-
guage generation. In Knowledge-Based Systems.
C.L. Paris. 1988. Tailoring object descriptions to the
users level of expertise. Computational Linguistics,
14(3):6478. Special Issue on User Modelling.
E. Reiter, R. Robertson, and L. M. Osman. 2003.
Lessons from a failure: generating tailored smok-
ing cessation letters. Artificial Intelligence, 144(1-
2):41?58.
A. Spaulding, A. Overholtzer, J. Pacheco, J. Tien, V. K.
Chaudhri, D. Gunning, and P. Clark. 2011. Inquire
for ipad: Bringing question-answering ai into the
classroom. In International Conference on AI in Ed-
ucation (AIED).
G. Wilcock. 2003. Talking owls: Towards an ontology
verbalizer. In Human Lan- guage Technology for
the Semantic Web and Web Services, ISWC03, page
109112, Sanibel Island, Florida.
29
Proceedings of the 14th European Workshop on Natural Language Generation, pages 94?97,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
The KBGen Challenge
Eva Banik
Computational
Linguistics Ltd
London, UK
ebanik@comp-ling.com
Claire Gardent
CNRS, LORIA
Nancy, France
claire.gardent@loria.fr
Eric Kow?
Computational
Linguistics Ltd
London, UK
kowey@comp-ling.com
1 Introduction
The KBGen 2013 natural language generation
challenge1 was intended to survey and compare
the performance of various systems which perform
tasks in the content realization stage of generation
(Banik et al, 2012). Given a set of relations which
form a coherent unit, the task is to generate com-
plex sentences which are grammatical and fluent
in English. The relations for this year?s challenge
were selected from the AURA knowledge base
(KB) (Gunning et al, 2010). In this paper we give
an overview of the KB, describe our methodology
for selecting sets of relations from the KB to pro-
vide input-output pairs for the challenge, and give
details of the development and test data set that
was provided to participating teams. Three teams
have submitted system outputs for this year?s chal-
lenge. In this paper we show BLEU and NIST
scores for outputs generated by the teams. The full
results of our evaluation, including human judge-
ments, as well as the development and test data set
are available at http://www.kbgen.org.
2 The AURA Knowledge Base
The AURA knowledge base (Gunning et al,
2010) encodes information from a biology text-
book (Reece et al, 2010). It was developed to
support a question answering system, to help stu-
dents understand biological concepts by allowing
them to ask questions about the material while
reading the textbook. AURA is a frame-based
KB which encodes events, the entities that partic-
ipate in events, properties, and roles that the en-
tities play in an event. The relations in the KB
include relations between these types, including
event-to-entity, event-to-event, event-to-property,
entity-to-property. The KB is built on top of the
?The work reported in this paper was supported by fund-
ing from Vulcan, Inc.
1http://www.kbgen.org
CLIB generic library of concepts (Barker et al,
2001). As part of the encoding process, concepts
in CLIB are specialized and/or combined to en-
code biology-specific information. AURA is or-
ganized into a set of concept maps, where each
concept map corresponds to a biological entity or
process. The KB was encoded by biology teach-
ers and contains around 5,000 concept maps. It is
available for download for academic purposes in
various formats including OWL2.
3 The Content Selection Process for
KBGen 2012
The input provided to the participants consisted
of a set of content units extracted from the KB,
and a sentence corresponding to each content unit.
The content units were semi-automatically se-
lected from AURA such that:
? the set of relations in each content unit
formed a connected graph
? each content unit can be verbalised by a
single, possibly complex sentence which is
grammatical and meaningful
? the set of content units contain as many dif-
ferent relations and concepts of different se-
mantic types (events, entities, properties, etc)
as possible.
To produce these inputs we first asked biology
teachers to provide coherent content units using
the AURA graphical interface. The basic assump-
tion behind this approach was that, since every
content unit can be expressed by a coherent sen-
tence, each set of relations will exhibit a ?coher-
ence pattern?. We then created a search space of
candidate content units by extracting patterns from
the KB which were similar to the patterns given
by the biologists. Finally, we manually selected
coherent content units.
2http://www.ai.sri.com/halo/
halobook2010/exported-kb/biokb.html
94
Figure 1: ? A hydrophobic compound attaches to a
carrier protein at a region called the binding site.?
3.1 Manual Selection of Content Units
In the first step of our process, biology teachers
manually selected parts of concept maps which
represented educationally useful information for
biology students by searching for specific con-
cepts in AURA?s graph-based user interface. For
each content unit they wrote a sentence verbalis-
ing the selected relations (Fig. 1). The biology
teachers who identified these coherent, sentence-
sized chunks of information were familiar with the
encoding practices in AURA, the underlying biol-
ogy textbook, and had experience with the Inquire
e-book application (Spaulding et al, 2011) which
displays educationally useful content from the KB.
3.2 From Graphs to Queries
In the second step, the graphical representations
produced by the biologists were manually trans-
lated to specific knowledge base queries which
were run in AURA to retrieve the instances sat-
isfying the queries. Queries consist of two parts:
a set of triples whose domain and range are vari-
ables, and a set of instance-of triples stating type
constraints on the variables. The graph shown in
Figure 1 was translated to the following query:
Type constraints:
(?CP instance-of Carrier-Protein)
(?A instance-of Attach)
(?BS instance-of Binding-Site)
(?HP instance-of Hydrophilic-Compound)
Relation constraints:
(?A object ?HP)
(?A base ?CP)
(?A site ?BS)
(?CP has-region ?BS)
3.3 From Queries to Generalized Query
Patterns
After checking that it returns an answer, each
query was generalized to a query pattern in or-
der to find other queries which involved different
concepts and relations, but still exhibited the same
general coherence pattern. To derive generalized
query patterns, specific queries were modified in
two ways: 1) by removing type constraints on con-
cepts, and 2) by replacing specific relations with
generalized relation types.
Removing type constraints
Manually specified queries were extended by re-
moving type constraints on variables. In the above
example, types were generalised to Event or En-
tity:
(?CP instance-of Entity)
(?A instance-of Event)
(?BS instance-of Entity)
(?HP instance-of Entity)
Other generalized types we used from the ontol-
ogy were Property-Values and Roles.
Generalizing relations
Each query was generalized by defining equiva-
lence classes over semantically similar relations
and replacing the specific relation in the query
with its equivalence class. The basic assumption
behind this was that if a set of relations is coherent,
we should be able to replace a relation with an-
other, semantically similar relation in the set, and
still have a coherent content unit. For example,
whether two entities are connected by has-part
or has-region is unlikely to make a difference
to the coherence of a content unit.
Following this approach we identified groups of
semantically similar relations within each relation
type (Event-to-Event, Event-to-Entity, etc). The
equivalence classes over relations were straight-
forwardly derived from distinctions made in CLIB
(Barker et al, 2001), the upper ontology and li-
brary of general concepts that AURA is built on,
although there was some manual fine-tuning re-
quired to exclude relations which were not re-
liably encoded in the KB. For example, we di-
vided Entity-to-Entity relations into three cate-
gories, based on whether they had a spatial or
meronymic sense, or expressed a specific relation
between two chemicals:
en2en-spatial: abuts is-above is-along is-at is-
inside is-opposite is-outside is-over location
95
is-across is-on is-parallel-to is-perpendicular-
to is-under is-between is-facing is-below is-
beside is-near
en2en-part: possesses has-part has-region
encloses has-basic-structural-unit has-
structural-part has-functional-part
en2en-chemical: has-solute has-solvent has-
atom has-ion has-oxidized-form has-
reduced-form has-isomer
Here the distinction between spatial relations
and meronymic relations was given by CLIB. Re-
lations in the third group were specific to our do-
main and added during the process of encoding.
Event-to-entity relations were divided into
?aux-participant? relations, which express the spa-
tial orientation of an event, and ?core-participant?
relations which describe ways in which entities
participate in the event. Here we used the cat-
egories of spatial relations and ?participant? re-
lations from CLIB. Our terminology reflects the
fact that entities connected to an event by a
core-participant relation are typically expressed as
obligatory arguments of the verb in a sentence,
whereas aux-participants would be expressed as
optional modifiers:
core-participants: agent object donor base in-
strument raw-material recipient result
aux-participants: away-from destination origin
path site toward
With these definitions, the specific query illus-
trated above in section 3.2 was translated to the
following query pattern:
(?A core-participant ?X)
(?A core-participant ?CP)
(?A aux-participant ?BS)
(?CP en2en-part ?BS)
3.4 From Query Results to Content Units
Query patterns were expanded by producing all
valid instantiations of the pattern in order to cre-
ate a search space of candidate content units, and
we ran each expanded query in AURA. The last
step was filtering the results returned by satisfi-
able queries to obtain content units which can be
verbalised in a single sentence. We used the fol-
lowing selection criteria to do this:
? A meaningful and grammatical sentence
could be formed by verbalising all concepts,
relations and properties present in the query
result.
(KBGEN-INPUT :ID "ex03c.99-1"
:TRIPLES (
(|Secretion21994| |object| |Mucus21965|)
(|Secretion21994| |base| |Earthworm21974|)
(|Secretion21994| |site| |Alimentary-Canal21978|)
(|Earthworm21974| |has-region|
|Alimentary-Canal21978|))
:INSTANCE-TYPES (
(|Mucus21965| |instance-of| |Mucus|)
(|Secretion21994| |instance-of| |Secretion|)
(|Earthworm21974| |instance-of| |Earthworm|)
(|Alimentary-Canal21978| |instance-of|
|Alimentary-Canal|))
:ROOT-TYPES (
(|Secretion21994| |instance-of| |Event|)
(|Mucus21965| |instance-of| |Entity|)
(|Earthworm21974| |instance-of| |Entity|)
(|Alimentary-Canal21978| |instance-of| |Entity|)
))
Figure 2: Input for the sentence ?Mucus is se-
creted in the alimentary canal of earthworms.?
? The set of content units should be as varied
as possible. In particular, we did not keep
a content unit if another very similar content
unit was present in the selected units. For in-
stance, if two content units contain identical
relations (modulo concept labels), only one
of these two units would be kept.
Given the pattern shown in Fig. 1 for instance,
we obtained 27 coherent content units. Each con-
tent unit was verbalized as a sentence to provide
development data for the content realization chal-
lenge. The following sentences illustrate the vari-
ation in the resulting content units:
- Polymers are digested in the lysosomes of eu-
karyotic cells.
- Mucus is secreted in the alimentary canal of
earthworms.
- Lysosomal enzymes digest proteins and poly-
mers at the lysosome of a eukaryotic cell.
- A chemical is attached to the active site of a
protein enzyme with an ionic bond.
- An enzyme substrate complex is formed
when a chemical attaches to the active site of
a protein enzyme with a hydrogen bond.
- Starch is stored in the lateral root of carrots.
4 Development Data Set
The development data set provided to participants
contained 207 input-output pairs. These inputs
96
were based on 19 different coherence patterns.
Fig. 2 shows an input-output pair based on the
pattern illustrated above. We also provided two
lexicons: a lexicon for events which gave a map-
ping from events to verbs, their inflected forms and
nominalizations and a lexicon for entities, which
provided a noun and its plural form. The rele-
vant entries in these lexicons for the input in Fig. 2
were:
Secretion,secretes,secrete,secreted,secretion
Mucus, mucus, mucus
Earthworm,earthworm,earthworms
Alimentary-Canal,alimentary canal,alimentary canals
5 Test Set
Our test data set contained 72 inputs in the same
format (and corresponding lexical resources as
above), which were divided into three categories:
(1) seen patterns, seen relations: inputs that have
exactly the same relations as some of the inputs in
the development data set, but different concepts
(2) seen patterns, unseen relations: these in-
puts are derived from patterns in the development
data set. They have similar structure, but contain
slightly different combinations of relations.
(3) unseen patterns: inputs extracted from a pre-
viously unused pattern, containing combinations
of relations not seen in the development data set.
6 Evaluation
Participants submitted two sets of outputs:
(1) outputs generated by their system as is (mod-
ulo including the lexicon provided in the test data
set) (2) outputs generated 6 days later, during
which time teams had a chance to make improve-
ments.
Each team was allowed to submit a set of 5 ranked
outputs for each input. We have evaluated all
of the submitted outputs using BLEU and NIST
scores and we are currently in the process of col-
lecting human judgements for the final system out-
puts that were ranked first. Table 1 shows the
overall results of automatic evaluation on both the
initial and final data sets for our three teams3, as
well as the coverage of the individual systems over
the 72 test inputs. More detail including the full
results of our evaluation can be found at http:
//www.kbgen.org, along with a link to download
3IMS: Stuttgart University Institute for Computational
Language Processing, LOR: LORIA, University of Nancy,
UDEL: University of Delaware, Computer and Information
Science Department
NIST BLEU coverage
HUMAN-1 10.0098 1.0000 100%
UDEL-final-1 5.9749 0.3577 97%
UDEL-initial-1 5.6030 0.3165 100%
LOR-final-1 4.8569 0.3053 84%
LOR-final-3 4.7238 0.2993 100%
LOR-final-2 4.6711 0.2945 100%
LOR-final-5 4.5720 0.2812 100%
LOR-final-4 4.4889 0.2781 100%
IMS-final-2 3.9649 0.1107 100%
IMS-final-4 3.8813 0.1140 100%
IMS-final-1 3.8670 0.1111 100%
IMS-final-3 3.7765 0.1023 100%
IMS-initial-2 3.6726 0.1117 100%
IMS-initial-3 3.6608 0.1181 100%
IMS-initial-1 3.6384 0.1173 100%
IMS-initial-4 3.5817 0.1075 100%
LOR-initial-1 0.1206 0.0822 30%
LOR-initial-3 0.1091 0.0751 100%
LOR-initial-4 0.0971 0.0732 100%
LOR-initial-2 0.0948 0.0757 100%
LOR-initial-5 0.0881 0.0714 100%
Table 1: BLEU and NIST scores of initial and final
system outputs. The digit behind the team names
refer to the output rank
the development and test data set used in the chal-
lenge, and more information about AURA and re-
lated resources.
References
E. Banik, C. Gardent, D. Scott, N. Dinesh, and
F. Liang. 2012. Kbgen text generation from knowl-
edge bases as a new shared task. In INLG 2012,
Starved Rock State Park, Illinois,USA.
K. Barker, B. Porter, and P. Clark. 2001. A library of
generic concepts for composing knowledgebases. In
Proceedings K-CAP 2001, pages 14?21.
D. Gunning, V. K. Chaudhri, P. Clark, K. Barker, Shaw-
Yi Chaw, M. Greaves, B. Grosof, A. Leung, D. Mc-
Donald, S. Mishra, J. Pacheco, B. Porter, A. Spauld-
ing, D. Tecuci, and J. Tien. 2010. Project halo up-
date - progress toward digital aristotle. AIMagazine,
Fall:33?58.
Jane B. Reece, Lisa A. Urry, Michael L. Cain,
Steven A. Wasserman, Peter V. Minorsky, and
Robert B. Jackson. 2010. Campbell Biology. Pear-
son Publishing.
A. Spaulding, A. Overholtzer, J. Pacheco, J. Tien, V. K.
Chaudhri, D. Gunning, and P. Clark. 2011. Inquire
for iPad: Bringing question-answering AI into the
classroom. In International Conference on AI in Ed-
ucation (AIED).
97
