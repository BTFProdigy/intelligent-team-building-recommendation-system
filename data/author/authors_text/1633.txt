Feature Vector Quality and Distributional Similarity 
Maayan Geffet 
School of Computer Science and Engineering, 
Hebrew University  
Givat Ram Campus, 
Jerusalem, Israel, 91904 
mary@cs.huji.ac.il 
Ido Dagan 
Department of Computer Science,  
Bar-Ilan University 
Ramat-Gan, Israel, 52900 
 
dagan@cs.biu.ac.il 
 
Abstract 
We suggest a new goal and evaluation criterion for 
word similarity measures. The new criterion - 
meaning-entailing substitutability - fits the needs 
of semantic-oriented NLP applications and can be 
evaluated directly (independent of an application) 
at a good level of human agreement. Motivated by 
this semantic criterion we analyze the empirical 
quality of distributional word feature vectors and 
its impact on word similarity results, proposing an 
objective measure for evaluating feature vector 
quality. Finally, a novel feature weighting and se-
lection function is presented, which yields superior 
feature vectors and better word similarity perform-
ance.  
1 Introduction 
    Distributional Similarity has been an active re-
search area for more than a decade (Hindle, 1990), 
(Ruge, 1992), (Grefenstette, 1994), (Lee, 1997), 
(Lin, 1998), (Dagan et al, 1999), (Weeds and 
Weir, 2003). Inspired by Harris distributional hy-
pothesis (Harris, 1968), similarity measures com-
pare a pair of weighted feature vectors that 
characterize two words. Features typically corre-
spond to other words that co-occur with the charac-
terized word in the same context. It is then 
assumed that different words that occur within 
similar contexts are semantically similar. 
As it turns out, distributional similarity captures 
a somewhat loose notion of semantic similarity 
(see Table 1). By construction, if two words are 
distributionally similar then the occurrence of one 
word in some contexts indicates that the other 
word is also likely to occur in such contexts. But it 
does not ensure that the meaning of the first word 
is preserved when replacing it with the other one in 
the given context. For example, words of similar 
semantic types, such as company ? government, 
tend to come up as distributionally similar, even 
though they are not substitutable in a meaning pre-
serving sense. 
On the other hand, many semantic-oriented appli-
cations, such as Question Answering, Paraphrasing 
and Information Extraction, do need to recognize 
which words may substitute each other in a mean-
ing preserving manner. For example, a question 
about company may be answered by a sentence 
about firm, but not about government. Such appli-
cations usually utilize reliable taxonomies or on-
tologies like WordNet, but cannot rely on the 
?loose? type of output of distributional similarity 
measures. 
In recent work Dagan and Glickman (2004) ob-
serve that applications usually do not require a 
strict meaning preserving criterion between text 
expressions, but rather need to recognize that the 
meaning of one expression entails the other. En-
tailment modeling is thus proposed in their work as 
a generic (application-independent) framework for 
practical semantic inference. We suggest adopting 
such (directional) entailment criterion at the lexical 
level for judging whether one word can be substi-
tuted by another one.  For example, certain ques-
tions about companies might be answered by 
sentences about automakers, since the meaning of 
automaker entails the meaning of company (though 
not vice versa). In this paper we adapt this new 
criterion, termed meaning entailing substitutability, 
as a direct evaluation criterion for the "correctness" 
of the output of word similarity measures (as op-
posed to indirect evaluations through WSD or dis-
tance in WordNet).  
Our eventual research goal is improving word 
similarity measures to predict better the more deli-
cate meaning entailment relationship between 
words. As a first step it was necessary to analyze 
the typical behavior of current similarity measures 
and categorize their errors (Section 3). Our main 
observation is that the quality of similarity scores 
 nation 
 region 
 state 
 *world 
 island 
 province 
 1 
 2 
 3 
 4 
 5 
 6 
 *city 
 territory 
 area 
 *town 
 republic 
 african_country 
 
 7 
 8 
 9 
 10 
 11 
 12 
  
 *north 
 *economy 
 *neighbor 
 *member 
 *party 
 *government 
 
 13 
 14 
 15 
 16 
 17 
 18 
 *company 
 *industry 
 kingdom 
 european_country 
 place 
 colony 
 19 
 20 
 25 
 31 
 36 
 41 
 
Table 1: The 20 top most similar words of country (and their ranks) in the similarity list by Lin98, 
followed by the next 4 words in the similarity list that are judged as correct. Incorrect similarities, 
under the substitutability criterion, are marked with ?*?.  
is often hurt by improper feature weights, which 
yield rather noisy feature vectors. We quantify this 
problem by a new measure for feature vector qual-
ity, which is independent of any particular vector 
similarity measure. 
To improve feature vector quality a novel fea-
ture weighting function is introduced, called rela-
tive feature focus (RFF) (Section 4). While having 
a simple (though non-standard) definition, this 
function yields improved performance relative to 
the two suggested evaluation criteria ? for vector 
quality and for word similarity. The underlying 
idea is that a good characteristic feature for a word 
w should characterize also multiple words that are 
highly similar to w. In other words, such feature 
should have a substantial "focus" within the close 
semantic vicinity of w.   
Applying RFF weighting achieved about 10% 
improvement in predicting meaning entailing sub-
stitutability (Section 5). Further analysis shows 
that RFF also leads to "cleaner" characteristic fea-
ture vectors, which may be useful for additional 
feature-based tasks like clustering. 
2 Background and Definitions 
   In the distributional similarity scheme each word 
w is represented by a feature vector, where an entry 
in the vector corresponds to a feature f. Each fea-
ture represents another word (or term) with which 
w co-occurs, and possibly specifies also the syntac-
tic relation between the two words. The value of 
each entry is determined by some weight function 
weight(w,f), which quantifies the degree of statisti-
cal association between the feature and the corre-
sponding word. 
Typical feature weighting functions include the 
logarithm of the frequency of word-feature co-
occurrence (Ruge, 1992), and the conditional prob-
ability of the feature given the word (within prob-
abilistic-based measures) (Pereira et al, 1993), 
(Lee, 1997), (Dagan et al, 1999). Probably the 
most widely used association weight function is 
(point-wise) Mutual Information (MI) (Church et 
al., 1990), (Hindle, 1990), (Lin, 1998), (Dagan, 
2000), defined by: 
)()(
),(log),( 2 fPwP
fwPfwMI =  
A known weakness of MI is its tendency to assign 
high weights for rare features. Yet, similarity 
measures that utilize MI showed good perform-
ance. In particular, a common practice is to filter 
out features by minimal frequency and weight 
thresholds. A word's vector is then constructed 
from the remaining features, which we call here 
active features.  
Once feature vectors have been constructed, the 
similarity between two words is defined by some 
vector similarity metric. Different metrics have 
been used in the above cited papers, such as 
Weighted Jaccard (Dagan, 2000), cosine (Ruge, 
1992), various information theoretic measures 
(Lee, 1997), and others. We picked the widely 
cited and competitive (e.g. (Weeds and Weir, 
2003)) measure of Lin (1998) as a representative 
case, and utilized it for our analysis and as a start-
ing point for improvement. 
2.1 Lin's (?98) Similarity Measure  
    Lin's similarity measure between two words, w 
and v, is defined as follows: 
,),(),(
),(),(
),(
)()(
)()(
? ?
?
??
??
+
+
=
fvweightfwweight
fvweightfwweight
vwsim
vFfwFf
vFwFf  
where F(w) and F(v) are the active features of the 
two words and the weight function is defined as 
MI. A feature is defined as a pair <term, syntac-
Country- 
State 
Ranks Country-
Economy 
Ranks 
Broadcast 
Goods 
Civil_servant 
Bloc 
Nonaligned 
Neighboring 
Statistic 
Border 
Northwest 
 24 
 140 
 64 
 30 
 55 
 15 
 165 
 10 
 41 
 50 
 16 
 54 
 77 
 60 
 165 
 43 
 247 
 174 
Devastate 
Developed 
Dependent 
Industrialized 
Shattered 
Club 
Black 
Million 
Electricity 
 81 
 36 
 101 
 49 
 16 
 155 
 122 
 31 
 130 
 8 
 78 
 26 
 85 
 141 
 38 
 109 
 245 
 154 
 
Table 3: The top-10 common features for the  
word pairs country-state and country-economy, 
along with their corresponding ranks in the 
sorted feature lists of the two words.  
 Feature  Weight 
 Commercial-bank, gen ?  
 Destination, pcomp ?  
 Airspace, pcomp ?  
 Landlocked, mod ?  
 Trade_balance, gen ?  
 Sovereignty, pcomp ?  
 Ambition , nn ?  
 Bourse, gen ?  
 Politician, gen ?  
 Border, pcomp ?  
 8.08  
 7.9 7  
 7.83 
 7.79 
 7.78 
 7.78 
 7.77 
 7.72 
 7.54 
 7.53        
 
Table 2: The top-10 ranking features for 
country.  
tic_relation>. For example, given the word ?com-
pany? the feature <earnings_report, gen?> (geni-
tive) corresponds to the phrase ?company?s 
earnings report?, and <profit, pcomp?> (preposi-
tional complement) corresponds to ?the profit of 
the company?. The syntactic relations are gener-
ated by the Minipar dependency parser (Lin, 
1993). The arrows indicate the direction of the syn-
tactic dependency: a downward arrow indicates 
that the feature is the parent of the target word, and 
the upward arrow stands for the opposite. 
In our implementation we filtered out features 
with overall frequency lower than 10 in the corpus 
and with MI weights lower than 4. (In the tuning 
experiments the filtered version showed 10% im-
provement in precision over no feature filtering.) 
From now on we refer to this implementation as 
Lin98.  
3 Empirical Analysis of Lin98 and 
Vector Quality Measure 
    To gain better understanding of distributional 
similarity we first analyzed the empirical behavior 
of Lin98, as a representative case for state of the 
art (see Section 5.1 for corpus details). 
As mentioned in the Introduction, distributional 
similarity may not correspond very tightly to 
meaning entailing substitutability. Under this 
judgment criterion two main types of errors occur: 
(1) word pairs that are of similar semantic types, 
but are not substitutable, like firm and government; 
and (2) word pairs that are of different semantic 
types, like firm and contract, which might (or 
might not) be related only at a topical level. Table 
1 shows the top most similar words for the target 
word country according to Lin98 .  The two error 
types are easily recognized, e.g. world and city for 
the first type, and economy for the second. 
A deeper look at the word feature vectors re-
veals typical reasons for such errors. In many 
cases, high ranking features in a word vector, when 
sorting the features by their weight, do not seem 
very characteristic for the word meaning. This is 
demonstrated in Table 2, which shows the top-10 
features in the vector of country. As can be seen, 
some of the top features are either too specific 
(landlocked, airspace), and so are less reliable, or 
too general (destination, ambition), and hence not 
indicative and may co-occur with many different 
types of words. On the other hand, more character-
istic features, like population and governor, occur 
further down the list, at positions 461 and 832. 
Overall, features that characterize well the word 
meaning are scattered across the ranked list, while 
many non-indicative features receive high weights. 
This may yield high similarity scores for less simi-
lar word pairs, while missing other correct similari-
ties. 
An objective indication of the problematic fea-
ture ranking is revealed by examining the common 
features that contribute mostly to the similarity 
score of a pair of similar words. We look at the 
common features of the two words and sort them 
by the sum of their weights in the two word vectors 
(which is the enumerator of Lin's sim formula in 
Section 2.1). Table 3 shows the top-10 common 
features for a pair of substitutable words (country - 
state) and non-substitutable words (country - econ-
omy). In both cases the common features are scat-
tered across each feature vector, making it difficult 
to distinguish between similar and non-similar 
word pairs.  
We suggest that the desired behavior of feature 
ranking is that the common features of truly similar 
words will be concentrated at the top ranks of their 
vectors. The common features for non-similar 
words are expected to be scattered all across each 
of the vectors. More formally, given a pair of simi-
lar words (judged as substitutable) w and v we de-
fine the top joint feature rank criterion for 
evaluating feature vector quality:  
   
],),(),([
2
11
),,(
))()((? +
=?
??? fvrankfwrank
n
nvwranktop
vFwFntopf
 
where rank(w,f) is the feature?s position in the 
sorted vector of the word w, and n is the number of 
top joint features to consider (top-n), when sorted 
by the sum of their weights in the two word vec-
tors. We thus expect that a good weighting func-
tion would yield (on average) a low top-rank score 
for truly similar words.  
4 Relative Feature Focus (RFF) 
    Motivated by the observations above we propose 
a new feature weight function, called relative fea-
ture focus (RFF). The basic idea is to promote fea-
tures which characterize many words that are 
highly similar to w. These features are considered 
as having a strong "focus" around w's meaning. 
Features which do not characterize sufficiently 
many words that are sufficiently similar to w are 
demoted. Even if such features happen to have a 
strong direct association with w they are not con-
sidered reliable, as they do not have sufficient sta-
tistical mass in w's semantic vicinity. 
4.1 RFF Definition 
 RFF is defined as follows. First, a standard 
word similarity measure sim is computed to obtain 
initial approximation of the similarity space (Lin98 
was used in this work). Then, we define the word 
set of a feature f, denoted by WS(f), as the set of 
words for which f is an active feature. The seman-
tic neighborhood of w, denoted by N(w), is defined 
as the set of all words v which are considered suf-
ficiently similar to w, satisfying  sim(w,v)>s where 
s is a threshold  (0.04 in our experiments). RFF is 
then defined by: 
? ??= ),(),( )()( vwsimfwRFF wNfWSv . 
That is, we identify all words v that are in the se-
mantic neighborhood of w and are also character-
ized by f and sum their similarities to w. 
Notice that RFF is a sum of word similarity val-
ues rather than being a direct function of word-
feature association values (which is the more com-
mon approach). It thus does not depend on the ex-
act co-occurrence level between w and f. Instead, it 
depends on a more global assessment of the asso-
ciation between f and the semantic vicinity of w. 
Unlike the entropy measure, used in (Grefenstette, 
1994), our "focused" global view is relative to 
each individual word w and is not a global inde-
pendent function of the feature. 
We notice that summing the above similarity 
values captures simultaneously a desired balance 
between feature specificity and generality, address-
ing the observations in Section 3. Some features 
might characterize just a single word that is very 
similar to w. But then the sum of similarities will 
include a single element, yielding a relatively low 
weight.1 General features may characterize more 
words within N(f), but then on average the similar-
ity with w over multiple words is likely to become 
lower, contributing smaller values to the sum. A 
reliable feature has to characterize multiple words 
(not too specific) that are highly similar to w (not 
too general).  
4.2 Re-computing Similarities  
Once RFF weights have been computed they are 
sufficiently accurate to allow for aggressive feature 
reduction. In our experiments it sufficed to use 
only the top 100 features for each word in order to 
obtain optimal results, since the most informative 
features now have the highest weights. Similarity 
between words is then recomputed over the re-
duced vectors using Lin's sim function (in Section 
2.1), with RFF replacing MI as the new weight 
function. 
 
 
 
 
 
                                                          
1
 This is why the sum of similarities is used rather than 
an average value, which might become too high by 
chance when computed over just a single element (or 
very few elements). 
#Words Judge 1 (%) Judge 2 (%) Total (%) 
Top 10 63.4 / 54.1 62.6 / 53.4 63.0 / 53.7 
Top 20 57.0 / 48.3 56.4 / 45.8 56.8 / 47.0 
Top 30 55.3 / 45.1 53.3 / 43.4 54.2 / 44.2 
Top 40 53.5 / 44.6 51.6 / 42.0 52.6 / 43.3 
 
Table 4: Precision values for Top-N similar 
words by the RFF / Lin98 methods. 
5 Evaluation 
5.1 Experimental Setting 
    The performance of the RFF-based similarity 
measure was evaluated for a sample of nouns and 
compared with that of Lin98. The experiment was 
conducted using an 18 million tokens subset of the 
Reuters RCV1 corpus,2 parsed by Lin?s Minipar 
dependency parser (Lin, 1993). We considered first 
an evaluation based on WordNet data as a gold 
standard, as in (Lin, 1998; Weeds and Weir, 2003). 
However, we found that many word pairs from the 
Reuters Corpus that are clearly substitutable are 
not linked appropriately in WordNet. 
We therefore conducted a manual evaluation 
based on the judgments of two human subjects. 
The judgment criterion follows common evalua-
tions of paraphrase acquisition (Lin and Pantel, 
2001), (Barzilay and McKeown, 2001), and corre-
sponds to the meaning-entailing substitutability 
criterion discussed in Section 1. Two words are 
judged as substitutable (correct similarity) if there 
are some contexts in which one of the words can 
be substituted by the other, such that the meaning 
of the original word can be inferred from the new 
one. 
Typically substitutability corresponds to certain 
ontological relations. Synonyms are substitutable 
in both directions. For example, worker and em-
ployee entail each other's meanings, as in the con-
text ?high salaried worker/employee?. Hyponyms 
typically entail their hypernyms. For example, dog 
entails animal, as in ?I have a dog? which entails 
?I have an animal? (but not vice versa). In some 
cases part-whole and member-set relations satisfy 
the meaning-entailing substitutability criterion. For 
example, a discussion of division entails in many 
contexts the meaning of company. Similarly, the 
plural form of employee(s) often entails the mean-
ing of staff. On the other hand, non-synonymous 
words that share a common hypernym (co-
hyponyms) like company and government, or 
country and city, are not substitutable since they 
always refer to different meanings (such as differ-
ent entities). 
Our test set included a sample of 30 randomly 
selected nouns whose corpus frequency is above 
                                                          
2
 Known as Reuters Corpus, Volume 1, English Lan-
guage, 1996-08-20 to 1997-08-19. 
500. For each noun we computed the top 40 most 
similar words by both similarity measures, yielding 
a total set of about 1600 (different) suggested word 
similarity pairs. Two independent assessors were 
assigned, each judging half of the test set (800 
pairs). The output pairs from both methods were 
mixed so the assessor could not relate a pair with 
the method that suggested it. 
5.2 Similarity Results 
    The evaluation results are displayed in Table 4. 
As can be seen RFF outperformed Lin98 by 9-10 
percentage points of precision at all Top-N levels, 
by both judges. Overall, RFF extracted 111 (21%) 
more correct similarity pairs than Lin98.  The 
overall relative recall3 of RFF is quite high (89%), 
exceeding Lin98 by 16% (73%). These figures in-
dicate that our method covers most of the correct 
similarities found by Lin98, while identifying 
many additional correct pairs. 
We note that the obtained precision values for 
both judges are very close at all table rows. To fur-
ther assess human agreement level for this task the 
first author of this paper judged two samples of 
100 word pairs each, which were selected ran-
domly from the two test sets of the original judges. 
The proportions of matching decisions between the 
author's judgments and the original ones were 
91.3% (with Judge 1) and 88.9% (with Judge 2). 
The corresponding Kappa values are 0.83 (?very 
good agreement?) and 0.75 (?good agreement?).  
As for feature reduction, vector sizes were re-
duced on average to about one third of their origi-
nal size in the Lin98 method (recall that standard 
feature reduction, tuned for the corpus, was already 
applied to the Lin98 vectors). 
 
 
                                                          
3
 Relative recall shows the percentage of correct word 
similarities found by each method relative to the joint 
set of similarities that were extracted by both methods. 
 Feature Weight 
Industry, gen ?  
Airport, gen  ?  
Neighboring, mod ?  
Law, gen ?  
Economy, gen ?  
Population, gen ?  
City, gen ?  
Impoverished, mod ?  
Governor, pcomp ?  
Parliament, gen ?  
 1.21 
 1.16 
 1.06  
 1.04 
 1.02 
 1.02 
 0.93 
 0.92 
 0.92 
 0.91 
 
     Table 5: Top-10 features of country by  
                    RFF. 
5.3 Empirical Observations for RFF  
    We now demonstrate the typical behavior of 
RFF relative to the observations and motivations 
of Section 3 (through the same example).  
Table 5 shows the top-10 features of country. 
We observe (subjectively) that the list now con-
tains quite indicative and reliable features, where 
too specific (anecdotal) and too general features 
were demoted (compare with Table 2).  
More objectively, Table 6 shows that most of 
the top-10 common features for country-state are 
now ranked highly for both words. On the other 
hand, there are only two common features for the 
incorrect pair country-economy, both with quite 
low ranks (compare with Table 3). Overall, given 
the set of all the correct (judged as substitutable) 
word similarities produced by both methods, the 
average top joint feature rank of the top-10 com-
mon features by RFF is 21, satisfying the desired 
behavior which was suggested in Section 3. The 
same figure is much larger for the Lin98 vectors, 
which have an average top joint feature rank of 
105.  
Consequently, Table 7 shows a substantial im-
provement in the similarity list for country, where 
most incorrect words, like economy and company, 
disappeared. Instead, additional correct similari-
ties, like kingdom and land, were promoted (com-
pare with Table 1). Some semantically related but 
non-substitutable words, like ?world? and ?city?, 
still remain in the list, but somewhat demoted. In 
this case all errors correspond to quite close se-
mantic relatedness, being geographic concepts. 
The remaining errors are mostly of the first type 
discussed in Section 3 ? pairs of words that are 
ontologically or thematically related but are not 
substitutable. Typical examples are co-hyponyms 
(country - city) or agent-patient and agent-action 
pairs (industry ? product, worker ? job). Usually, 
such word pairs also have highly ranked common 
features since they naturally appear with similar 
characteristic features. It may therefore be difficult 
to filter out such non-substitutable similarities 
solely by the standard distributional similarity 
scheme, suggesting that additional mechanisms are 
required. 
6 Conclusions and Future Work 
    This paper proposed the following contributions:  
 
1. Considering meaning entailing substitutability 
as a target goal and evaluation criterion for word 
similarity. This criterion is useful for many seman-
tic-oriented NLP applications, and can be evalu-
ated directly by human subjects.  
2. A thorough empirical error analysis of state of 
the art performance was conducted. The main ob-
servation was deficient quality of the feature vec-
tors which reduces the quality of similarity 
measures.    
3. Inspired by the qualitative observations we iden-
tified a new qualitative condition for feature vector 
evaluation ? top joint feature rank. Thus, feature 
vector quality can be measured independently of 
the final similarity output.  
4. Finally, we presented a novel feature weighting 
function, relative feature focus. This measure was 
designed based on error analysis insights and im-
Country-
State 
Ranks Country- 
Economy 
Ranks 
Neighboring 
Industry 
Impoverished 
Governor 
Population 
City 
Economy 
Parliament 
Citizen 
Law 
 3 
 1 
 8 
 10 
 6 
 17 
 5 
 10 
 14 
 4 
 1 
 11 
 8 
 9 
 16 
 18 
 15 
 22 
 25 
 33 
Developed  
Liberalization 
50 
100 
 100 
 79 
 
Table 6: RFF weighting: Top-10 common 
features for country-state and country-
economy along with their corresponding 
ranks in the two (sorted) feature vectors. 
proves performance over all the above criteria. 
    We intend to further investigate the contribution 
of our measure to word sense disambiguation and 
to evaluate its performance for clustering methods. 
Error analysis suggests that it might be difficult 
to improve similarity output further within the 
common distributional similarity schemes. We 
need to seek additional criteria and data types, such 
as identifying evidence for non-similarity, or ana-
lyzing more carefully disjoint features. 
Further research is suggested to extend the 
learning framework towards richer notions of on-
tology generation. We would like to distinguish 
between different ontological relationships that 
correspond to the substitutability criterion, such as 
identifying the entailment direction, which was 
ignored till now. Towards these goals we plan to 
investigate combining unsupervised distributional 
similarity with supervised methods for learning 
ontological relationships, and with paraphrase ac-
quisition methods. 
References  
Barzilay, Regina and Kathleen McKeown. 2001. 
Extracting Paraphrases from a Parallel Corpus. 
In Proc. of ACL/EACL, 2001.  
Church, Kenneth W. and Hanks Patrick. 1990. 
Word association norms, mutual information, 
and Lexicography. Computational Linguistics, 
16(1), pp. 22?29. 
Dagan, Ido. 2000. Contextual Word Similarity, in 
Rob Dale, Hermann Moisl and Harold Somers 
(Eds.), Handbook of Natural Language Process-
ing, Marcel Dekker Inc, 2000, Chapter 19, pp. 
459-476.  
Dagan, Ido and Oren Glickman. 2004. Probabilis-
tic Textual Entailment: Generic Applied Model-
ing of Language Variability. PASCAL 
Workshop on Text Understanding and Mining. 
Dagan, Ido, Lillian Lee and Fernando Pereira. 
1999. Similarity-based models of cooccurrence 
probabilities. Machine Learning, 1999, Vol. 
 34(1-3), special issue on Natural Language 
Learning, pp. 43-69.  
Grefenstette, Gregory. 1994. Exploration in Auto-
matic Thesaurus Discovery. Kluwer Academic 
Publishers. 
Harris, Zelig S. 1968. Mathematical structures of 
language. Wiley, 1968. 
Hindle, D. 1990. Noun classification from predi-
cate-argument structures. In Proc. of ACL, pp. 
268?275.  
Lee, Lillian. 1997. Similarity-Based Approaches to 
Natural Language Processing. Ph.D. thesis, Har-
vard University, Cambridge, MA.   
Lin, Dekang. 1993. Principle-Based Parsing with-
out Overgeneration. In Proc. of ACL-93, pages 
112-120, Columbus, Ohio, 1993. 
Lin, Dekang. 1998. Automatic Retrieval and Clus-
tering of Similar Words.  In Proc. of COLING?
ACL98, Montreal, Canada, August, 1998. 
Lin, Dekang and Patrick Pantel. 2001. Discovery 
of Inference Rules for Question Answering. 
Natural Language Engineering 7(4), pp. 343-
360, 2001. 
Pereira, Fernando, Tishby Naftali, and Lee Lillian. 
1993. Distributional clustering of English 
words. In Proc. of ACL-93, pp. 183?190.  
Ruge,  Gerda.  1992.  Experiments on linguisti-
cally-based term associations. Information 
Processing & Management, 28(3), pp. 317?332. 
Weeds, Julie and David Weir. 2003. A General 
Framework for Distributional Similarity. In 
Proc. of EMNLP-03. Spain. 
 
 nation 
 state 
 island 
 region 
 area 
 1 
 2 
 3 
 4 
 5 
 territory  
 *neighbor 
 colony 
 *port  
 republic 
 
 6 
 7 
 8 
 9 
 10 
  
 african_country 
 province  
 *city 
 *town 
 kingdom 
 
 11 
 12  
 13 
 14 
 15 
 *district 
 european_country 
 zone  
 land 
 place 
 16 
 17 
 18 
 19 
 20 
 
Table 7: Top-20 most similar words for country and their ranks in the similarity list by the 
RFF-based measure. Incorrect similarities (non-substitutable) are marked with ?*?.  
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 849?856
Manchester, August 2008
Learning Entailment Rules for Unary Templates
Idan Szpektor
Department of Computer Science
Bar-Ilan University
Ramat Gan, Israel
szpekti@macs.biu.ac.il
Ido Dagan
Department of Computer Science
Bar-Ilan University
Ramat Gan, Israel
dagan@macs.biu.ac.il
Abstract
Most work on unsupervised entailment
rule acquisition focused on rules between
templates with two variables, ignoring
unary rules - entailment rules between
templates with a single variable. In this pa-
per we investigate two approaches for un-
supervised learning of such rules and com-
pare the proposed methods with a binary
rule learning method. The results show
that the learned unary rule-sets outperform
the binary rule-set. In addition, a novel
directional similarity measure for learning
entailment, termed Balanced-Inclusion, is
the best performing measure.
1 Introduction
In many NLP applications, such as Question An-
swering (QA) and Information Extraction (IE), it
is crucial to recognize whether a specific target
meaning is inferred from a text. For example, a
QA system has to deduce that ?SCO sued IBM? is
inferred from ?SCO won a lawsuit against IBM?
to answer ?Whom did SCO sue??. This type of
reasoning has been identified as a core semantic
inference paradigm by the generic Textual Entail-
ment framework (Giampiccolo et al, 2007).
An important type of knowledge needed for
such inference is entailment rules. An entailment
rule specifies a directional inference relation be-
tween two templates, text patterns with variables,
such as ?X win lawsuit against Y ? X sue Y ?.
Applying this rule by matching ?X win lawsuit
against Y ? in the above text allows a QA system to
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
infer ?X sue Y ? and identify ?IBM?, Y ?s instantia-
tion, as the answer for the above question. Entail-
ment rules capture linguistic and world-knowledge
inferences and are used as an important building
block within different applications, e.g. (Romano
et al, 2006).
One reason for the limited performance of
generic semantic inference systems is the lack of
broad-scale knowledge-bases of entailment rules
(in analog to lexical resources such as WordNet).
Supervised learning of broad coverage rule-sets is
an arduous task. This sparked intensive research
on unsupervised acquisition of entailment rules
(and similarly paraphrases) e.g. (Lin and Pantel,
2001; Szpektor et al, 2004; Sekine, 2005).
Most unsupervised entailment rule acquisition
methods learn binary rules, rules between tem-
plates with two variables, ignoring unary rules,
rules between unary templates (templates with
only one variable). However, a predicate quite of-
ten appears in the text with just a single variable
(e.g. intransitive verbs or passives), where infer-
ence requires unary rules, e.g. ?X take a nap?X
sleep? (further motivations in Section 3.1).
In this paper we focus on unsupervised learn-
ing of unary entailment rules. Two learning ap-
proaches are proposed. In our main approach,
rules are learned by measuring how similar the
variable instantiations of two templates in a corpus
are. In addition to adapting state-of-the-art similar-
ity measures for unary rule learning, we propose a
new measure, termed Balanced-Inclusion, which
balances the notion of directionality in entailment
with the common notion of symmetric semantic
similarity. In a second approach, unary rules are
derived from binary rules learned by state-of-the-
art binary rule learning methods.
We tested the various unsupervised unary rule
849
learning methods, as well as a binary rule learn-
ing method, on a test set derived from a standard
IE benchmark. This provides the first comparison
between the performance of unary and binary rule-
sets. Several results rise from our evaluation: (a)
while most work on unsupervised learning ignored
unary rules, all tested unary methods outperformed
the binary method; (b) it is better to learn unary
rules directly than to derive them from a binary
rule-base; (c) our proposed Balanced-Inclusion
measure outperformed all other tested methods in
terms of F1 measure. Moreover, only Balanced-
Inclusion improved F1 score over a baseline infer-
ence that does not use entailment rules at all .
2 Background
This section reviews relevant distributional simi-
larity measures, both symmetric and directional,
which were applied for either lexical similarity or
unsupervised entailment rule learning.
Distributional similarity measures follow the
Distributional Hypothesis, which states that words
that occur in the same contexts tend to have similar
meanings (Harris, 1954). Various measures were
proposed in the literature for assessing such simi-
larity between two words, u and v. Given a word q,
its set of features F
q
and feature weights w
q
(f) for
f ? F
q
, a common symmetric similarity measure
is Lin similarity (Lin, 1998a):
Lin(u, v) =
?
f?F
u
?F
v
[w
u
(f) + w
v
(f)]
?
f?F
u
w
u
(f) +
?
f?F
v
w
v
(f)
where the weight of each feature is the pointwise
mutual information (pmi) between the word and
the feature: w
q
(f) = log[
Pr(f |q)
Pr(f)
].
Weeds and Weir (2003) proposed to measure the
symmetric similarity between two words by av-
eraging two directional (asymmetric) scores: the
coverage of each word?s features by the other. The
coverage of u by v is measured by:
Cover(u, v) =
?
f?F
u
?F
v
w
u
(f)
?
f?F
u
w
u
(f)
The average can be arithmetic or harmonic:
WeedsA(u, v) =
1
2
[Cover(u, v) + Cover(v, u)]
WeedsH(u, v) =
2 ? Cover(u, v) ? Cover(v, u)
Cover(u, v) + Cover(v, u)
Weeds et al also used pmi for feature weights.
Binary rule learning algorithms adopted such
lexical similarity approaches for learning rules be-
tween templates, where the features of each tem-
plate are its variable instantiations in a corpus,
such as {X=?SCO?, Y =?IBM?} for the example
in Section 1. Some works focused on learning
rules from comparable corpora, containing com-
parable documents such as different news articles
from the same date on the same topic (Barzilay
and Lee, 2003; Ibrahim et al, 2003). Such corpora
are highly informative for identifying variations of
the same meaning, since, typically, when variable
instantiations are shared across comparable docu-
ments the same predicates are described. However,
it is hard to collect broad-scale comparable cor-
pora, as the majority of texts are non-comparable.
A complementary approach is learning from the
abundant regular, non-comparable, corpora. Yet,
in such corpora it is harder to recognize varia-
tions of the same predicate. The DIRT algorithm
(Lin and Pantel, 2001) learns non-directional bi-
nary rules for templates that are paths in a depen-
dency parse-tree between two noun variables X
and Y . The similarity between two templates t and
t
?
is the geometric average:
DIRT (t, t
?
) =
?
Lin
x
(t, t
?
) ? Lin
y
(t, t
?
)
where Lin
x
is the Lin similarity between X?s in-
stantiations of t and X?s instantiations of t
?
in
a corpus (equivalently for Lin
y
). Some works
take the combination of the two variable instantia-
tions in each template occurrence as a single com-
plex feature, e.g. {X-Y =?SCO-IBM?}, and com-
pare between these complex features of t and t
?
(Ravichandran and Hovy, 2002; Szpektor et al,
2004; Sekine, 2005).
Directional Measures Most rule learning meth-
ods apply a symmetric similarity measure between
two templates, viewing them as paraphrasing each
other. However, entailment is in general a direc-
tional relation. For example, ?X acquire Y ?
X own Y ? and ?countersuit against X ? lawsuit
against X?.
(Weeds and Weir, 2003) propose a directional
measure for learning hyponymy between two
words, ?l? r?, by giving more weight to the cov-
erage of the features of l by r (with ? >
1
2
):
WeedsD(l, r)=?Cover(l, r)+(1??)Cover(r, l)
When ?=1, this measure degenerates into
Cover(l, r), termed Precision(l, r). With
850
Precision(l, r) we obtain a ?soft? version of
the inclusion hypothesis presented in (Geffet and
Dagan, 2005), which expects l to entail r if the
?important? features of l appear also in r.
Similarly, the LEDIR algorithm (Bhagat et al,
2007) identifies the entailment direction between
two binary templates, l and r, which participate
in a relation learned by (the symmetric) DIRT, by
measuring the proportion of instantiations of l that
are covered by the instantiations of r.
As far as we know, only (Shinyama et al, 2002)
and (Pekar, 2006) learn rules between unary tem-
plates. However, (Shinyama et al, 2002) relies
on comparable corpora for identifying paraphrases
and simply takes any two templates from compa-
rable sentences that share a named entity instan-
tiation to be paraphrases. Such approach is not
feasible for non-comparable corpora where statis-
tical measurement is required. (Pekar, 2006) learns
rules only between templates related by local dis-
course (information from different documents is
ignored). In addition, their template structure is
limited to only verbs and their direct syntactic ar-
guments, which may yield incorrect rules, e.g. for
light verbs (see Section 5.2). To overcome this lim-
itation, we use a more expressive template struc-
ture.
3 Learning Unary Entailment Rules
3.1 Motivations
Most unsupervised rule learning algorithms fo-
cused on learning binary entailment rules. How-
ever, using binary rules for inference is not enough.
First, a predicate that can have multiple arguments
may still occur with only one of its arguments.
For example, in ?The acquisition of TCA was suc-
cessful?, ?TCA? is the only argument of ?acqui-
sition?. Second, some predicate expressions are
unary by nature. For example, modifiers, such as
?the elected X?, or intransitive verbs. In addition,
it appears more tractable to learn all variations for
each argument of a predicate separately than to
learn them for combinations of argument pairs.
For these reasons, it seems that unary rule learn-
ing should be addressed in addition to binary rule
learning. We are further motivated by the fact that
some (mostly supervised) works in IE found learn-
ing unary templates useful for recognizing relevant
named entities (Riloff, 1996; Sudo et al, 2003;
Shinyama and Sekine, 2006), though they did not
attempt to learn generic knowledge bases of entail-
ment rules.
This paper investigates acquisition of unary en-
tailment rules from regular non-comparable cor-
pora. We first describe the structure of unary
templates and then explore two conceivable ap-
proaches for learning unary rules. The first ap-
proach directly assesses the relation between two
given templates based on the similarity of their in-
stantiations in the corpus. The second approach,
which was also mentioned in (Iftene and Balahur-
Dobrescu, 2007), derives unary rules from learned
binary rules.
3.2 Unary Template Structure
To learn unary rules we first need to define their
structure. In this paper we work at the syntac-
tic representation level. Texts are represented by
dependency parse trees (using the Minipar parser
(Lin, 1998b)) and templates by parse sub-trees.
Given a dependency parse tree, any sub-tree can
be a candidate template, setting some of its nodes
as variables (Sudo et al, 2003). However, the num-
ber of possible templates is exponential in the size
of the sentence. In the binary rule learning litera-
ture, the main solution for exhaustively learning all
rules between any pair of templates in a given cor-
pus is to restrict the structure of templates. Typi-
cally, a template is restricted to be a path in a parse
tree between two variable nodes (Lin and Pantel,
2001; Ibrahim et al, 2003).
Following this approach, we chose the structure
of unary templates to be paths as well, where one
end of the path is the template?s variable. How-
ever, paths with one variable have more expressive
power than paths between two variables, since the
combination of two unary paths may generate a
binary template that is not a path. For example,
the combination of ?X call indictable? and ?call Y
indictable? is the template ?X call Y indictable?,
which is not a path between X and Y .
For every noun node v in a parsed sentence, we
generate templates with v as a variable as follows:
1. Traverse the path from v towards the root of
the parse tree. Whenever a candidate pred-
icate is encountered (any noun, adjective or
verb) the path from that node to v is taken as
a template. We stop when the first verb or
clause boundary (e.g. a relative clause) is en-
countered, which typically represent the syn-
tactic boundary of a specific predicate.
851
2. To enable templates with control verbs and
light verbs, e.g. ?X help preventing?, ?X
make noise?, whenever a verb is encoun-
tered we generate templates that are paths be-
tween v and the verb?s modifiers, either ob-
jects, prepositional complements or infinite
or gerund verb forms (paths ending at stop
words, e.g. pronouns, are not generated).
3. To capture noun modifiers that act as predi-
cates, e.g. ?the losingX?, we extract template
paths between v and each of its modifiers,
nouns or adjectives, that are derived from a
verb. We use the Catvar database to identify
verb derivations (Habash and Dorr, 2003).
As an example for the procedure, the templates
extracted from the sentence ?The losing party
played it safe? with ?party? as the variable are:
?losing X?, ?X play? and ?X play safe?.
3.3 Direct Learning of Unary Rules
We applied the lexical similarity measures pre-
sented in Section 2 for unary rule learning. Each
argument instantiation of template t in the corpus
is taken as a feature f , and the pmi between t and
f is used for the feature?s weight. We first adapted
DIRT for unary templates (unary-DIRT, apply-
ing Lin-similarity to the single feature vector), as
well as its output filtering by LEDIR. The various
Weeds measures were also applied
1
: symmetric
arithmetic average, symmetric harmonic average,
weighted arithmetic average and Precision.
After initial analysis, we found that given a right
hand side template r, symmetric measures such
as Lin (in DIRT) generally tend to prefer (score
higher) relations ?l, r? in which l and r are related
but do not necessarily participate in an entailment
or equivalence relation, e.g. the wrong rule ?kill X
? injure X?.
On the other hand, directional measures such as
Weeds Precision tend to prefer directional rules in
which the entailing template is infrequent. If an in-
frequent template has common instantiations with
another template, the coverage of its features is
typically high, whether or not an entailment rela-
tion exists between the two templates. This behav-
ior generates high-score incorrect rules.
Based on this analysis, we propose a new
measure that balances the two behaviors, termed
1
We applied the best performing parameter values pre-
sented in (Bhagat et al, 2007) and (Weeds and Weir, 2003).
Balanced-Inclusion (BInc). BInc identifies entail-
ing templates based on a directional measure but
penalizes infrequent templates using a symmetric
measure:
BInc(l, r) =
?
Lin(l, r) ? Precision(l, r)
3.4 Deriving Unary Rules From Binary Rules
An alternative way to learn unary rules is to first
learn binary entailment rules and then derive unary
rules from them. We derive unary rules from a
given binary rule-base in two steps. First, for each
binary rule, we generate all possible unary rules
that are part of that rule (each unary template is
extracted following the same procedure described
in Section 3.2). For example, from ?X find solu-
tion to Y ? X solve Y ? we generate the unary
rules ?X find? X solve?, ?X find solution? X
solve?, ?solution to Y ? solve Y ? and ?find solu-
tion to Y ? solve Y ?. The score of each generated
rule is set to be the score of the original binary rule.
The same unary rule can be derived from dif-
ferent binary rules. For example, ?hire Y ? em-
ploy Y ? is derived both from ?X hire Y ? X em-
ploy Y ? and ?hire Y for Z ? employ Y for Z?,
having a different score from each original binary
rule. The second step of the algorithm aggregates
the different scores yielded for each derived rule
to produce the final rule score. Three aggregation
functions were tested: sum (Derived-Sum), aver-
age (Derived-Avg) and maximum (Derived-Max).
4 Experimental Setup
We want to evaluate learned unary and binary rule
bases by their utility for NLP applications through
assessing the validity of inferences that are per-
formed in practice using the rule base.
To perform such experiments, we need a test-
set of seed templates, which correspond to a set of
target predicates, and a corpus annotated with all
argument mentions of each predicate. The evalu-
ation assesses the correctness of all argument ex-
tractions, which are obtained by matching in the
corpus either the seed templates or templates that
entail them according to the rule-base (the latter
corresponds to rule-application).
Following (Szpektor et al, 2008), we found the
ACE 2005 event training set
2
useful for this pur-
pose. This standard IE dataset includes 33 types of
event predicates such as Injure, Sue and Divorce.
2
http://projects.ldc.upenn.edu/ace/
852
All event mentions are annotated in the corpus, in-
cluding the instantiated arguments of the predicate.
ACE guidelines specify for each event its possible
arguments, each associated with a semantic role.
For instance, some of the Injure event arguments
are Agent, Victim and Time.
To utilize the ACE dataset for evaluating entail-
ment rule applications, we manually represented
each ACE event predicate by unary seed templates.
For example, the seed templates for Injure are ?A
injure?, ?injure V ? and ?injure in T ?. We mapped
each event role annotation to the corresponding
seed template variable, e.g. ?Agent? to A and
?Victim? to V in the above example. Templates
are matched using a syntactic matcher that han-
dles simple morpho-syntactic phenomena, as in
(Szpektor and Dagan, 2007). A rule application
is considered correct if the matched argument is
annotated by the corresponding ACE role.
For testing binary rule-bases, we automatically
generated binary seed templates from any two
unary seeds that share the same predicate. For ex-
ample, for Injure the binary seeds ?A injure V ?, ?A
injure in T ? and ?injure V in T ? were automatically
generated from the above unary seeds.
We performed two adaptations to the ACE
dataset to fit it better to our evaluation needs. First,
our evaluation aims at assessing the correctness of
inferring a specific target semantic meaning, which
is denoted by a specific predicate, using rules.
Thus, four events that correspond ambiguously to
multiple distinct predicates were ignored. For in-
stance, the Transfer-Money event refers to both do-
nating and lending money, and thus annotations of
this event cannot be mapped to a specific seed tem-
plate. We also omitted 3 events with less than 10
mentions, and were left with 26 events (6380 argu-
ment mentions).
Additionally, we regard all entailing mentions
under the textual entailment definition as correct.
However, event mentions are annotated as correct
in ACE only if they explicitly describe the target
event. For instance, a Divorce mention does en-
tail a preceding marriage event but it does not ex-
plicitly describe it, and thus it is not annotated as
a Marry event. To better utilize the ACE dataset,
we considered for a target event the annotations of
other events that entail it as being correct as well.
We note that each argument was considered sep-
arately. For example, we marked a mention of a
divorced person as entailing the marriage of that
person, but did not consider the place and time of
the divorce act to be those of the marriage .
5 Results and Analysis
We implemented the unary rule learning algo-
rithms described in Section 3 and the binary DIRT
algorithm (Lin and Pantel, 2001). We executed
each method over the Reuters RCV1 corpus
3
,
learning for each template r in the corpus the top
100 rules in which r is entailed by another tem-
plate l, ?l? r?. All rules were learned in canonical
form (Szpektor and Dagan, 2007). The rule-base
learned by binary DIRT was taken as the input for
deriving unary rules from binary rules.
The performance of each acquired rule-base was
measured for each ACE event. We measured the
percentage of correct argument mentions extracted
out of all correct argument mentions annotated for
the event (recall) and out of all argument mentions
extracted for the event (precision). We also mea-
sured F1, their harmonic average, and report macro
average Recall, Precision and F1 over the 26 event
types.
No threshold setting mechanism is suggested in
the literature for the scores of the different algo-
rithms, especially since rules for different right
hand side templates have different score ranges.
Thus, we follow common evaluation practice (Lin
and Pantel, 2001; Geffet and Dagan, 2005) and test
each learned rule-set by taking the top K rules for
each seed template, whereK ranges from 0 to 100.
WhenK=0, no rules are used and mentions are ex-
tracted only by direct matching of seed templates.
Our rule application setting provides a rather
simplistic IE system (for example, no named entity
recognition or approximate template matching). It
is thus useful for comparing different rule-bases,
though the absolute extraction figures do not re-
flect the full potential of the rules. In Secion 5.2
we analyze the full-system?s errors to isolate the
rules? contribution to overall system performance.
5.1 Results
In this section we focus on the best performing
variations of each algorithm type: binary DIRT,
unary DIRT, unary Weeds Harmonic, BInc and
Derived-Avg. We omitted the results of methods
that were clearly inferior to others: (a) WeedsA,
WeedsD and Weeds-Precision did not increase
3
http://about.reuters.com/researchandstandards/corpus/
853
Recall over not using rules because rules with in-
frequent templates scored highest and arithmetic
averaging could not balance well these high scores;
(b) out of the methods for deriving unary rules
from binary rule-bases, Derived-Avg performed
best; (c) filtering with (the directional) LEDIR did
not improve the performance of unary DIRT.
Figure 1 presents Recall, Precision and F1 of the
methods for different cutoff points. First, we ob-
serve that even when matching only the seed tem-
plates (K=0), unary seeds outperform the binary
seeds in terms of both Precision and Recall. This
surprising behavior is consistent through all rule
cutoff points: all unary learning algorithms per-
form better than binary DIRT in all parameters.
The inferior behavior of binary DIRT is analyzed
in Section 5.2.
The graphs show that symmetric unary ap-
proaches substantially increase recall, but dramati-
cally decrease precision already at the top 10 rules.
As a result, F1 only decreases for these methods.
Lin similarity (DIRT) and Weeds-Harmonic show
similar behaviors. They consistently outperform
Derived-Avg. One reason for this is that incorrect
unary rules may be derived even from correct bi-
nary rules. For example, from ?X gain seat on
Y ? elect X to Y ? the incorrect unary rule ?X
gain? electX? is also generated. This problem is
less frequent when unary rules are directly scored
based on their corpus statistics.
The directional measure of BInc yields a more
accurate rule-base, as can be seen by the much
slower precision reduction rate compared to the
other algorithms. As a result, it is the only algo-
rithm that improves over the F1 baseline of K=0,
with the best cutoff point at K=20. BInc?s re-
call increases moderately compared to other unary
learning approaches, but it is still substantially bet-
ter than not using rules (a relative recall increase of
50% already at K=10). We found that many of the
correct mentions missed by BInc but identified by
other methods are due to occasional extractions of
incorrect frequent rules, such as partial templates
(see Section 5.2). This is reflected in the very low
precision of the other methods. On the other hand,
some correct rules were only learned by BInc, e.g.
?countersuit againstX?X sue? and ?X take wife
? X marry?.
When only one argument is annotated for a spe-
cific event mention (28% of ACE predicate men-
tions, which account for 15% of all annotated ar-
Figure 1: Average Precision, Recall and F1 at dif-
ferent top K rule cutoff points.
guments), binary rules either miss that mention, or
extract both the correct argument and another in-
correct one. To neutralize this bias, we also tested
the various methods only on event mentions an-
notated with two or more arguments and obtained
similar results to those presented for all mentions.
This further emphasizes the general advantage of
using unary rules over binary rules.
854
5.2 Analysis
Binary-DIRT We analyzed incorrect rules both
for binary-DIRT and BInc by randomly sampling,
for each algorithm, 200 rules that extracted incor-
rect mentions. We manually classified each rule ?l
? r? as either: (a) Correct - the rule is valid in
some contexts of the event but extracted some in-
correct mentions; (b) Partial Template - l is only a
part of a correct template that entails r. For exam-
ple, learning ?X decide? X meet? instead of ?X
decide to meet ? X meet?; (e) Incorrect - other
incorrect rules, e.g. ?charge X ? convict X?.
Table 1 summarizes the analysis and demon-
strates two problems of binary-DIRT. First, rela-
tive to BInc, it tends to learn incorrect rules for
high frequency templates, and therefore extracted
many more incorrect mentions for the same num-
ber of incorrect rules. Second, a large percentage
of incorrect mentions extracted are due to partial
templates at the rule left-hand-side. Such rules are
leaned because many binary templates have a more
complex structure than paths between arguments.
As explained in Section 3.2 the unary template
structure we use is more expressive, enabling to
learn the correct rules. For example, BInc learned
?take Y into custody ? arrest Y ? while binary-
DIRT learned ?X take Y ? X arrest Y ?.
System Level Analysis We manually analyzed
the reasons for false positives (incorrect extrac-
tions) and false negatives (missed extractions) of
BInc, at its best performing cutoff point (K=20),
by sampling 200 extractions of each type.
From the false positives analysis (Table 2) we
see that 39% of the errors are due to incorrect rules.
The main reasons for learning such rules are those
discussed in Section 3.3: (a) related templates that
are not entailing; (b) infrequent templates. All
learning methods suffer from these issues. As was
shown by our results, BInc provides a first step to-
wards reducing these problems. Yet, these issues
require further research.
Apart from incorrectly learned rules, incorrect
template matching (e.g. due to parse errors) and
context mismatch contribute together 46% of the
errors. Context mismatches occur when the entail-
ing template is matched in inappropriate contexts.
For example, ?slam X ? attack X? should not be
applied when X is a ball, only when it is a person.
The rule-set net effect on system precision is better
estimated by removing these errors and fixing the
annotation errors, which yields 72% precision.
Binary DIRT Balanced Inclusion
Correct 16 (70) 38 (91)
Partial Template 27 (2665) 6 (81)
Incorrect 157 (2584) 156 (787)
Total 200 (5319) 200 (959)
Table 1: Rule type distribution of a sample of 200
rules that extracted incorrect mentions. The corre-
sponding numbers of incorrect mentions extracted
by the sampled rules is shown in parentheses.
Reason % mentions
Incorrect Rule learned 39.0
Context mismatch 27.0
Match error 19.0
Annotation problem 15.0
Table 2: Distribution of reasons for false positives
(incorrect argument extractions) by BInc at K=20.
Reason % mentions
Rule not learned 61.5
Match error 25.0
Discourse analysis needed 12.0
Argument is predicative 1.5
Table 3: Distribution of reasons for false negatives
(missed argument mentions) by BInc at K=20.
Table 3 presents the analysis of false negatives.
First, we note that 12% of the arguments cannot
be extracted by rules alone, due to necessary dis-
course analysis. Thus, a recall upper bound for en-
tailment rules is 88%. Many missed extractions are
due to rules that were not learned (61.5%). How-
ever, 25% of the mentions were missed because of
incorrect syntactic matching of correctly learned
rules. By assuming correct matches in these cases
we isolate the recall of the rule-set (along with the
seeds), which yields 39% recall.
6 Conclusions
We presented two approaches for unsupervised ac-
quisition of unary entailment rules from regular
(non-comparable) corpora. In the first approach,
rules are directly learned based on distributional
similarity measures. The second approach de-
rives unary rules from a given rule-base of binary
rules. Under the first approach we proposed a
novel directional measure for scoring entailment
rules, termed Balanced-Inclusion.
We tested the different approaches utilizing a
standard IE test-set and compared them to binary
rule learning. Our results suggest the advantage of
learning unary rules: (a) unary rule-bases perform
855
better than binary rules; (b) it is better to directly
learn unary rules than to derive them from binary
rule-bases. In addition, the Balanced-Inclusion
measure outperformed all other tested methods.
In future work, we plan to explore additional
unary template structures and similarity scores,
and to improve rule application utilizing context
matching methods such as (Szpektor et al, 2008).
Acknowledgements
This work was partially supported by ISF grant
1095/05, the IST Programme of the European
Community under the PASCAL Network of Ex-
cellence IST-2002-506778 and the NEGEV project
(www.negev-initiative.org).
References
Barzilay, Regina and Lillian Lee. 2003. Learn-
ing to paraphrase: An unsupervised approach us-
ing multiple-sequence alignment. In Proceedings of
HLT-NAACL.
Bhagat, Rahul, Patrick Pantel, and Eduard Hovy. 2007.
Ledir: An unsupervised algorithm for learning di-
rectionality of inference rules. In Proceedings of
EMNLP.
Geffet, Maayan and Ido Dagan. 2005. The distribu-
tional inclusion hypotheses and lexical entailment.
In Proceedings of ACL.
Giampiccolo, Danilo, Bernardo Magnini, Ido Dagan,
and Bill Dolan. 2007. The third pascal recogniz-
ing textual entailment challenge. In Proceedings of
WTEP.
Habash, Nizar and Bonnie Dorr. 2003. A categorial
variation database for english. In Proceedings of
NACL.
Harris, Z. 1954. Distributional structure. Word,
10(23):146?162.
Ibrahim, Ali, Boris Katz, and Jimmy Lin. 2003. Ex-
tracting structural paraphrases from aligned mono-
lingual corpora. In Proceedings of IWP.
Iftene, Adrian and Alexandra Balahur-Dobrescu. 2007.
Hypothesis transformation and semantic variability
rules used in recognizing textual entailment. In Pro-
ceedings of WTEP.
Lin, Dekang and Patrick Pantel. 2001. Discovery of
inference rules for question answering. In Natu-
ral Language Engineering, volume 7(4), pages 343?
360.
Lin, Dekang. 1998a. Automatic retrieval and cluster-
ing of similar words. In Proceedings of COLING-
ACL.
Lin, Dekang. 1998b. Dependency-based evaluation of
minipar. In Proceedings of the Workshop on Evalu-
ation of Parsing Systems at LREC.
Pekar, Viktor. 2006. Acquisition of verb entailment
from text. In Proceedings of NAACL.
Ravichandran, Deepak and Eduard Hovy. 2002. Learn-
ing surface text patterns for a question answering
system. In Proceedings of ACL.
Riloff, Ellen. 1996. Automatically generating extrac-
tion patterns from untagged text. In AAAI/IAAI, Vol.
2, pages 1044?1049.
Romano, Lorenza, Milen Kouylekov, Idan Szpektor,
Ido Dagan, and Alberto Lavelli. 2006. Investigat-
ing a generic paraphrase-based approach for relation
extraction. In Proceedings of EACL.
Sekine, Satoshi. 2005. Automatic paraphrase discov-
ery based on context and keywords between ne pairs.
In Proceedings of IWP.
Shinyama, Yusuke and Satoshi Sekine. 2006. Preemp-
tive information extraction using unrestricted rela-
tion discovery. In Proceedings of HLT-NAACL.
Shinyama, Yusuke, Satoshi Sekine, Sudo Kiyoshi, and
Ralph Grishman. 2002. Automatic paraphrase ac-
quisition from news articles. In Proceedings of HLT.
Sudo, Kiyoshi, Satoshi Sekine, and Ralph Grishman.
2003. An improved extraction pattern representation
model for automatic ie pattern acquisition. In Pro-
ceedings of ACL.
Szpektor, Idan and Ido Dagan. 2007. Learning canon-
ical forms of entailment rules. In Proceedings of
RANLP.
Szpektor, Idan, Hristo Tanev, Ido Dagan, and Bonaven-
tura Coppola. 2004. Scaling web-based acquisition
of entailment relations. In Proceedings of EMNLP.
Szpektor, Idan, Ido Dagan, Roy Bar Haim, and Jacob
Goldberger. 2008. Contextual preferences. In Pro-
ceedings of ACL.
Weeds, Julie and David Weir. 2003. A general frame-
work for distributional similarity. In Proceedings of
EMNLP.
856
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1056?1065,
Singapore, 6-7 August 2009.
c?2009 ACL and AFNLP
A Compact Forest for Scalable Inference
over Entailment and Paraphrase Rules
Roy Bar-Haim
?
, Jonathan Berant
?
, Ido Dagan
?
?
Computer Science Department, Bar-Ilan University, Ramat Gan 52900, Israel
{barhair,dagan}@cs.biu.ac.il
?
The Blavatnik School of Computer Science, Tel-Aviv University, Tel-Aviv 69978, Israel
jonatha6@post.tau.ac.il
Abstract
A large body of recent research has been
investigating the acquisition and applica-
tion of applied inference knowledge. Such
knowledge may be typically captured as
entailment rules, applied over syntactic
representations. Efficient inference with
such knowledge then becomes a funda-
mental problem. Starting out from a for-
malism for entailment-rule application we
present a novel packed data-structure and
a corresponding algorithm for its scalable
implementation. We proved the validity of
the new algorithm and established its effi-
ciency analytically and empirically.
1 Introduction
Applied semantic inference is concerned with de-
riving target meanings from texts. In the textual
entailment framework, this is reduced to infer-
ring a textual statement (the hypothesis h) from
a source text (t). Traditional formal semantics
approaches perform such inferences over logi-
cal forms derived from the text. By contrast,
most practical NLP applications operate over shal-
lower representations such as parse trees, possibly
supplemented with limited semantic information
about named entities, semantic roles etc.
Most commonly, inference over such represen-
tations is made by applying some kind of transfor-
mations or substitutions to the tree or graph rep-
resenting the text. Such transformations may be
generally viewed as entailment (inference) rules,
which capture semantic knowledge about para-
phrases, lexical relations such as synonyms and
hyponyms, syntactic variations etc. Such knowl-
edge is either composed manually, e.g. WordNet
(Fellbaum, 1998), or learned automatically.
A large body of work has been dedicated to
learning paraphrases and entailment rules, e.g.
(Lin and Pantel, 2001; Shinyama et al, 2002;
Szpektor et al, 2004; Bhagat and Ravichandran,
2008), identifying appropriate contexts for their
application (Pantel et al, 2007) and utilizing them
for inference (de Salvo Braz et al, 2005; Bar-
Haim et al, 2007). Although current avail-
able rule bases are still quite noisy and incom-
plete, the progress made in recent years suggests
that they may become increasingly valuable for
text understanding applications. Overall, applied
knowledge-based inference is a prominent line of
research gaining much interest, with recent exam-
ples including the series of workshops on Knowl-
edge and Reasoning for Answering Questions
(KRAQ)
1
and the planned evaluation of knowledge
resources in the forthcoming 5
th
Recognizing Tex-
tual Entailment challenge (RTE-5)
2
.
While many applied systems utilize semantic
knowledge via such inference rules, their use is
typically limited, application-specific, and quite
heuristic. Formalizing these practices seems im-
portant for applied semantic inference research,
analogously to the role of well-formalized mod-
els in parsing and machine translation. Bar-Haim
et al (2007) made a step in this direction by in-
troducing a generic formalism for semantic infer-
ence over parse trees. Their formalism uses entail-
ment rules as a unifying representation for various
types of inference knowledge, allowing unified in-
ference as well. In this formalism, rule application
has a clear, intuitive interpretation as generating a
new sentence parse (a consequent), semantically
entailed by the source sentence. The inferred con-
sequent may be subject to further rule applications
1
http://www.irit.fr/recherches/ILPL/kraq09.html
2
http://www.nist.gov/tac/2009/RTE/
1056
and so on. In their implementation, each conse-
quent was generated explicitly as a separate tree.
Following this line of work, our long-term re-
search goal is to investigate effective utilization
of entailment rules for inference. While the for-
malism of Bar-Haim et al provides a princi-
pled framework for modeling such inferences,
its implementation using explicit generation of
consequents raises severe efficiency issues, since
the number of consequents may grow exponen-
tially in the number of rule applications. Con-
sider, for example, the sentence ?Children are
fond of candies.?, and the following entailment
rules: ?children?kids?, ?candies?sweets?, and ?X
is fond of Y?X likes Y?. The number of derivable
sentences (including the source sentence) would
be 2
3
as each rule can either be applied or not, in-
dependently. Indeed, we found that this exponen-
tial explosion leads to poor scalability in practice.
Intuitively, we would like that each rule applica-
tion would add just the entailed part (e.g. kids) to a
packed sentence representation. Yet, we still want
the resulting structure to represent a set of entailed
sentences, rather than some mixture of sentence
fragments whose semantics is unclear.
As discussed in section 5, previous work pro-
posed only partial solutions to this problem. In this
paper we present a novel data structure, termed
compact forest, and a corresponding inference al-
gorithm, which efficiently generate and represent
all consequents while preserving the identity of
each individual one (section 3). Our work is
inspired by previous work on packed represen-
tations in other fields, such as parsing, genera-
tion and machine translation (section 5). As
we follow a well-defined formalism, we could
prove that all inference operations of Bar-Haim
et al are equivalently applied over the compact
forest. We compare inference cost over compact
forests to explicit consequent generation both the-
oretically (section 3.4), illustrating an exponential-
to-linear complexity ratio, and empirically (sec-
tion 4), showing improvement by orders of magni-
tude. These results suggest that our data-structure
and algorithm are both valid and scalable, open-
ing up the possibility to investigate large-scale en-
tailment rule application within a well-formalized
framework.
2 Inference Framework
This section briefly presents a (simplified) descrip-
tion of the tree transformations inference formal-
ism of Bar-Haim et al (2007). Given a source text,
syntactically parsed, and a set of entailment rules
representing tree transformations, the formalism
defines the set of consequents derivable from the
text using the rules. Each consequent is obtained
through a sequence of rule applications, each gen-
erates an intermediate parse tree, similar to a proof
process in logic.
More specifically, sentences are represented as
dependency trees, where nodes are annotated with
lemma and part-of-speech, and edges are anno-
tated with dependency relation. A rule ?L ? R?
is primarily composed of two templates, termed
left-hand-side (L), and right-hand-side (R). Tem-
plates are dependency subtrees which may con-
tain POS-tagged variables, matching any lemma.
Figure 1(a) shows passive-to-active transforma-
tion rule, and (b) illustrates its application.
A rule application generates a derived tree d
from a source tree s through the following steps:
L matching: First, a match of L in the source
tree s is sought. In our example, the variable V is
matched in the verb see, N1 is matched in Mary
and N2 is matched in John.
R instantiation: Next, a copy of R is generated
and its variables are instantiated according to their
matching node in L. In addition, a rule may spec-
ify alignments, defined as a partial function from
L nodes to R nodes. An alignment indicates that
for each modifier m of the source node that is not
part of the rule structure, the subtree rooted at m
should also be copied as a modifier of the target
node. In addition to defining alignments explic-
itly, each variable in L is implicitly aligned to its
counterpart in R. In our example, the alignment
between the V nodes implies that yesterday (mod-
ifying see) should be copied to the generated sen-
tence, and similarly beautiful (modifying Mary) is
copied for N1.
Derived tree generation: Let r be the instanti-
ated R, along with its descendants copied from L
through alignment, and l be the subtree matched
by L. The formalism has two methods for gen-
erating the derived tree d: substitution and intro-
duction, as specified by the rule type. Substitution
rules specify modification of a subtree of s, leav-
ing the rest of s unchanged. Thus, d is formed by
copying s while replacing l (and the descendants
1057
LV VERB
obj
ssf
f
f
f
f
f
f
f
f
f
be

by
++
X
X
X
X
X
X
X
X
X
X
R
V VERB
subj
ssf
f
f
f
f
f
f
f
f
f
obj
++
X
X
X
X
X
X
X
X
X
X
N1 NOUN be VERB by PREP
pcomp?n

N2 NOUN N1 NOUN
N2 NOUN
(a) Passive to active transformation (substitution rule)
ROOT
i

see VERB
obj
qqc
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
be
ssf
f
f
f
f
f
f
f
f
f
by

mod
++
X
X
X
X
X
X
X
X
X
X
Mary NOUN
mod

be VERB by PREP
pcomp?n

yesterday NOUN
beautiful ADJ John NOUN
ROOT
i

see VERB
subj
rre
e
e
e
e
e
e
e
e
e
e
obj

mod
,,
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
John NOUN Mary NOUN
mod

yesterday NOUN
beautiful ADJ
Source: Beautiful Mary was seen by John yesterday. Derived: John saw beautiful Mary yesterday.
(b) Application of passive to active transformation
Figure 1: An inference rule application. POS and relation labels are based on Minipar (Lin, 1998)
of l?s nodes) with r. This is the case for the pas-
sive rule, as well as for lexical rules such as ?buy
? purchase?. By contrast, introduction rules are
used to make inferences from a subtree of s, while
the other parts of s are ignored and do not affect d.
A typical example is inferring a proposition em-
bedded as a relative clause in s. In this case, the
derived tree d is simply taken to be r.
In addition to inference rules, the formalism in-
cludes annotation rules which add features to ex-
isting parse tree nodes. These rules have been used
for identifying contexts that affect the polarity of
predicates.
As shown by Bar-Haim et al, this concise, well
defined formalism allows unified representation of
diverse types of knowledge which are commonly
used for applied semantic inference.
3 Efficient Inference over Compact Parse
Forests
As shown in the introduction, explicit genera-
tion of consequents (henceforth explicit inference)
leads to an exponential explosion of the number
of generated trees. In this section we present our
efficient implementation for this formalism. Our
implementation is based on a novel data structure,
termed compact forest (Section 3.1), which com-
pactly represents a large set of trees. Each rule
application generates explicitly only the nodes of
the rule right-hand-side while the rest of the con-
sequent tree is shared with the source, which also
reduces the number of redundant rule applications.
As we shall see, this novel representation is based
primarily on disjunction edges, an extension of
dependency edges that specify a set of alterna-
tive edges of multiple trees. Section 3.2 presents
an efficient algorithm for inference over compact
forests, followed by a discussion of its correctness
and complexity (sections 3.3 and 3.4).
3.1 The Compact Forest Data Structure
A compact forestF represents a set of dependency
trees. Figure 2 shows an example of a compact
forest, containing both the source and derived sen-
tences of Figure 1. We first define a more general
data structure for directed graphs, and then narrow
the definition to the case of trees.
A Compact Directed Graph (cDG) is a pair
G = (V, E) where V is a set of nodes and E is a
set of disjunction edges (d-edges). Let D be a
set of dependency relations. A d-edge d is a triple
(S
d
, rel
d
, T
d
), where S
d
and T
d
are disjoint sets
of source nodes and target nodes; rel
d
: S
d
? D
is a function specifying the dependency relation
corresponding to each source node. Graphically,
d-edges are shown as point nodes, with incoming
edges from source nodes and outgoing edges to
target nodes. For instance, let d be the bottom-
most d-edge in Figure 3. Then S
d
= {of, like},
T
d
= {candy, sweet}, rel(of ) = pcomp-n, and
rel(like) = obj.
A d-edge represents, for each s
i
? S
d
, a set of
1058
ROOT
i
John
see
by objbe mod
by
pcomp-n
beautiful
Mary
mod
be yesterday
see
subj
objmod
Figure 2: A compact forest containing both the
source and derived sentences of Figure 1. Parts
of speech are omitted.
alternative directed edges {(s
i
, t
j
) : t
j
? T
d
}, all
of which are labeled with the same relation given
by rel
d
(s
i
). Each of these edges, termed embed-
ded edge (e-edge), would correspond to a differ-
ent graph represented in G. In the previous exam-
ple, the e-edges are like
obj
???candy, like
obj
???sweet,
of
pcomp?n
???????candy and of
pcomp?n
???????sweet (notice
that the definition implies that all source nodes in
S
d
have the same set of alternative target nodes
T
d
). d is called an outgoing d-edge of a node v if
v ? S
d
and an incoming d-edge of v if v ? T
d
.
A Compact Directed Acyclic Graph (cDAG) is a
cDG that contains no cycles of e-edges.
A DAG G rooted in a node v ? V of a cDAG
G is embedded in G if it can be derived as follows:
we initialize G with v alone; then, we expand v
by choosing exactly one target node t ? T
d
from
each outgoing d-edge d of v, and adding t and the
corresponding e-edge (v, t) to G. This expansion
process is repeated recursively for each new node
added to G.
Each such set of choices results in a different
DAG with v as its only root. In Figure 2, we may
choose to connect the root either to the left see,
resulting in the source passive sentence, or to the
right see, resulting in the derived active sentence.
A Compact Forest F is a cDAG with a single
root r (i.e. r has no incoming d-edges) where all
the embedded DAGs rooted in r are trees. This set
of trees, termed embedded trees, comprise the set
of trees represented by F .
Figure 3 shows another example for a compact
ROOT
i
child
be
pred
fond
subjmod
of
pcomp-n
candy
like
subj
obj
kid
sweet
Figure 3: A compact forest representing the 2
3
sentences derivable from the sentence ?children
are fond of candies? using the following three
rules: ?children?kids?, ?candies?sweets?, and ?X
is fond of Y?X likes Y?.
forest efficiently representing the 2
3
sentences re-
sulting from three independently-applied rules.
3.2 The Inference Process
Next, we describe the algorithm implementing the
inference process of Section 2 over the compact
forest (henceforth, compact inference), illustrating
it through Figures 1 and 2.
Forest initialization F is initialized with the
set of dependency trees representing the text sen-
tences, with their roots connected under the forest
root as the target nodes of a single d-edge. Depen-
dency edges are transformed trivially to d-edges
with a single source and target. Annotation rules
are applied at this stage to the initial F . The black
part of Figure 2 corresponds to the initial forest.
Rule application comprises the following steps:
L matching: L is matched in F if there exists
an embedded tree t in F such that L is matched
in t, as in Section 2. We denote by l the subtree
of t in which L was matched. As in section 2, the
match in our example is (V,N1, N2)=(see, Mary,
John). Notice that this definition does not allow l
to be scattered over multiple embedded trees.
As the target nodes of a d-edge specify alterna-
tives for the same position in the tree, their parts-
of-speech are expected to be of substitutable types.
1059
In this paper we further assume that all target
nodes of the same d-edge have the same part-of-
speech
3
. Consequently, variables that are leaves in
L and may match a certain target node of a d-edge
d are mapped to the whole set of target nodes T
d
rather than to a single node. This yields a compact
representation of multiple matches, and prevents
redundant rule applications. For instance, given
a compact representation of ?{Children/kids} are
fond of {candies/sweeets}? (cf. Figure 3), the rule
?X is fond of Y?X likes Y? will be matched and
applied only once, rather than four times (for each
combination of matching X and Y ).
Derived tree generation: A template r consist-
ing of R while excluding variables that are leaves
of both L and R (termed dual leaf-variables)
4
is
generated and inserted into F . In case of a substi-
tution rule (as in our example), r is set as an alter-
native to l by adding r?s root to T
d
, where d is the
incoming d-edge of l?s root. In case of an intro-
duction rule, it is set as an alternative to the other
trees in the forest by adding r?s root to the target
node set of the forest root?s outgoing d-edge. In
our example, r is the gray node (still labeled with
the variable V ) , and it becomes an additional tar-
get node of the d-edge entering the original (left)
see.
Variable instantiation: Each variable in r (i.e.
a non-dual leaf) is instantiated according to its
match in L (as in Section 2), e.g. V is instantiated
with see. As specified above, if the variable is a
leaf inL is not a dual leaf then it is matched in a set
of nodes, and hence each of them should be instan-
tiated in r. This is decomposed into a sequence
of simpler operations: first, r is instantiated with a
representative from the set, and then we apply (ad-
hoc) lexical substitution rules for creating a new
node for each other node in the set
5
.
Alignment sharing: Modifiers of aligned nodes
are shared (rather than copied) as follows. Given
a node n
L
in l aligned to a node n
R
in r, and an
outgoing d-edge d of n
L
which is not part of l, we
share d between n
L
and n
R
by adding n
R
to S
d
3
This is the case in our current implementation, which is
based on the coarse tag-set of Minipar (Lin, 1998).
4
With the following exceptions: variables that are the
only node in R (and hence are both the root and a leaf), and
variables with additional alignments (other than the implicit
alignment between their occurrences in L and R) are not con-
sidered dual-leaves.
5
Notice that these nodes, in addition to the usual align-
ment with their source nodes in l, share the same daughters
in r.
and setting rel
d
(n
R
) = rel
d
(n
L
). This is illus-
trated by the sharing of yesterday in Figure 2. We
also copy annotation features from n
L
to n
R
.
We note at this point that the instantiation of
variables that are not dual leaves (e.g. V in our
example) cannot be shared because they typically
have different modifiers at the two sides of the
rule. Yet, their modifiers which are not part of
the rule are shared through the alignment opera-
tion (recall that common variables are always con-
sidered aligned). Dual leaf variables, on the other
hand, might be shared, as described next, since the
rule doesn?t specify any modifiers for them.
Dual leaf variable sharing: This final step is
performed analogously to alignment sharing. Sup-
pose that a dual leaf variable X is matched in a
node v in l whose incoming d-edge is d. Then
we simply add the parent p of X in r to S
d
and
set rel
d
(p) to the relation between p and X (in
R). Since v itself is shared, its modifiers become
shared as well, implicitly implementing the align-
ment operation. The subtrees beautiful Mary and
John are shared this way for variablesN1 andN2.
Applying the rule in our example added only
a single node and linked it to four d-edges, com-
pared to duplicating the whole tree in explicit in-
ference.
3.3 Correctness
In this section we present two theorems, which
prove that the inference algorithm is a valid imple-
mentation of the inference formalism of Section 2.
Due to space limitations, the proofs themselves
are omitted, and instead we outline their general
scheme.
We first argue that performing any sequence of
rule applications over the set of initial trees results
in a compact forest:
Theorem 1: The compact inference process
generates a compact forest.
Proof scheme: We prove by induction on the
number of rule applications. Initialization gen-
erates a single-rooted cDAG, whose embedded
DAGs are all trees, as required. We then prove that
if applying a rule on a compact forest creates a cy-
cle or an embedded DAG that is not a tree, then
such a cycle or a non-tree DAG already existed
prior to rule application, in contradiction with the
inductive assumption. A crucial observation for
this proof is that for any path from a node u to a
node v that passes through r, where u and v are
1060
outside r, there is also an analogous path from u
to v that passes through l instead, QED.
Next, we argue that the inference process over a
compact forest is complete and sound, i.e., it gen-
erates the set of consequents derivable from a text
according to the inference formalism.
Theorem 2: Given a rule base R and a set of
initial trees T , a tree t is embedded in a compact
forest derivable from T by the compact inference
process? t is a consequent of T according to the
inference formalism.
Proof scheme: We first show completeness by
induction on the number of explicit rule applica-
tions. Let t
n+1
be a tree derived from a tree t
n
using the rule r
n
according to the inference for-
malism. The inductive assumption asserts that t
n
is embedded in some derivable compact forest F .
It is easy to verify that applying r
n
to F will yield
a compact forest F
?
in which t
n+1
is embedded.
Next, we show soundness by induction on the
number of rule applications over the compact for-
est. Let t
n+1
be a tree represented in some derived
compact forest F
n+1
. F
n+1
was derived from the
compact forest F
n
, using the rule r
n
. It can be
shown that F
n
represents a tree t
n
, such that ap-
plying r
n
on t
n
will yield t
n+1
according to the
formalism. The inductive assumption asserts that
t
n
is a consequent in the inference formalism and
therefore t
n+1
is a consequent as well, QED.
These two theorems guarantee that the compact
inference process is valid - i.e., it yields a compact
forest that represents the set of consequents deriv-
able from a given text by a given rule set.
3.4 Complexity
In this section we explain why compact inference
exponentially reduces the time and space com-
plexity in typical scenarios.
We consider a set of rule matches in a tree T
independent if their matched left-hand-sides (ex-
cluding dual-leaf variables) do not overlap in T ,
and their application over T can be chained in any
order. For example, the three rule matches pre-
sented in Figure 3 are independent.
Let us consider explicit inference first. Assume
we start with a single tree T with k independent
rules matched. Applying k rules will yield 2
k
trees, since any subset of the rules might be ap-
plied to T . Therefore, the time and space com-
plexity of applying k independent rule matches is
?(2
k
). Applying more rules on the newly derived
Compact Explicit Ratio
Time (msec) 61 24,184 396
Rule applications 12 123 10
Node count 69 5,901 86
Edge endpoints 141 11,552 82
Table 1: Compact vs. explicit inference, us-
ing generic rules. Results are averaged per text-
hypothesis pair.
consequents behaves in a similar manner.
Next, we examine compact inference. Apply-
ing a rule using compact inference adds the right-
hand-side of the rule and shares with it existing
d-edges. Since that the size of the right-hand-side
and the number of outgoing d-edges per node are
practically bounded by low constants, applying k
rules on a tree T yields a linear increase in the size
of the forest. Thus, the resulting size isO(|T |+k),
as we can see from Figure 3.
The time complexity of rule application is com-
posed of matching the rule in the forest and apply-
ing the matched rule. Applying a matched rule is
linear in its size. Matching a rule of size r in a
forest F takes O(|F|
r
) time even when perform-
ing an exhaustive search for matches in the forest.
Since r tends to be quite small and can be bounded
by a low constant, this already gives polynomial
time complexity. In practice, indexing the forest
nodes, as well as the typical low connectivity of
the forest, result in a very fast matching procedure,
as illustrated in the empirical evaluation, described
next.
4 Empirical Evaluation
This section reports empirical evaluation of the ef-
ficiency of compact inference, tested in the recog-
nizing textual entailment setting using the RTE-3
and RTE-4 datasets (Giampiccolo et al, 2007; Gi-
ampiccolo et al, 2009). These datasets consist of
(text, hypothesis) pairs, which need to be classi-
fied as entailing/non entailing. Our first experi-
ment shows, using a small rule set, that compact
inference outperforms explicit inference by orders
of magnitude (Section 4.1). The second experi-
ment shows that compact inference scales well to
a full-blown RTE setting with several large-scale
rule bases, where up to hundreds of rules are ap-
plied for a text (Section 4.2).
1061
4.1 Compact vs. Explicit Inference
To compare explicit and compact inference we
randomly sampled 100 pairs from the RTE-3 de-
velopment set, and parsed the text in each pair
using Minipar (Lin, 1998). We used a set of
manually-composed entailment rules for inference
over generic linguistic phenomena such as pas-
sive, conjunction, relative clause, apposition, pos-
sessives, and determiners, which contains a few
dozens of rules. To make a fair comparison, we
aimed to make the explicit inference implementa-
tion reasonably efficient, e.g. by preventing gen-
eration of the same tree by different permutations
of the same rule applications. Both configurations
perform rule application iteratively, until no new
matches are found. In each iteration we first find
all rule matches and then apply all matching rules.
We compare run time, number of rule applications,
and the overall generated size of nodes and edges,
where edge size is represented by the sum of its
endpoints.
The results are summarized in Table 1. As ex-
pected, the results show that compact inference is
by orders of magnitude more efficient than explicit
inference. To avoid memory overflow, inference
was terminated after reaching 100,000 nodes. 3
out of the 100 pairs reached that limit with explicit
inference, while the maximal node count for com-
pact inference was only 268. The number of rule
applications is reduced thanks to the sharing of
common subtrees in the compact forest, by which
a single rule application operates simultaneously
over a large number of embedded trees. The re-
sults suggest that scaling to larger rule bases and
longer inference chains would be feasible for com-
pact inference, but prohibitive for explicit infer-
ence.
4.2 Application to an RTE System
Experimental setting The goal of the second
experiment was to assess that compact inference
scales well for broad entailment rule bases. In
this experiment we used the Bar-Ilan RTE system
(Bar-Haim et al, 2009). The system operates in
two primary stages: Inference, in which entail-
ment rules are applied to the initial compact forest
F , aiming to bring it closer to the hypothesis H,
and Classification, in which a set of features is ex-
tracted from the resulting F and from H and fed
into an SVM classifier, which determines entail-
ment.
The classification setting and its features are
quite typical for the RTE literature. They include
lexical and structural measures for the coverage of
H by F , where high coverage is assumed to cor-
relate with entailment, as well as features aiming
to detect inconsistencies between F and H such
as incompatible arguments for the same predicate
or incompatible verb polarity (see below). For a
complete feature description, see (Bar-Haim et al,
2009).
Rule Bases In addition to the generic rules de-
scribed in Section 4.1, the following large-scale
sources for entailment rules were used: Wikipeda:
We used the lexical rulebase of Shnarch et al
(2009), who extracted rules such as ?Janis Joplin
? singer? from Wikipedia based on both its meta-
data (e.g. links and redirects) and text defini-
tions, using patterns such as ?X is a Y?. Word-
Net: We extracted from WordNet (Fellbaum,
1998) lexical rules based on synonyms, hyper-
nyms and derivation relations. DIRT: The DIRT
algorithm (Lin and Pantel, 2001) learns from a
corpus entailment rules between binary predicates,
e.g. ?X is fond of Y?X likes Y?. We used the
version described in (Szpektor and Dagan, 2007),
which learns canonical rule forms. Argument-
MappedWordNet (AmWN):A resource for entail-
ment rules between verbal and nominal predicates
(Szpektor and Dagan, 2009), including their argu-
ment mapping, based on WordNet and NomLex-
plus (Meyers et al, 2004), verified statistically
through intersection with the unary-DIRT algo-
rithm (Szpektor and Dagan, 2008). In total, these
rule bases represent millions of rules. Polarity An-
notation Rules: We compiled a small set of anno-
tation rules for marking the polarity of predicates
as negative or unknown due to verbal negation,
modal verbs, conditionals etc. (Bar-Haim et al,
2009).
Search In this work we focus on efficient rep-
resentation of the search space, leaving for future
work the complementary problem of devising ef-
fective search heuristics over our representation.
In the current experiment we implemented a sim-
ple search strategy, in the spirit of (de Salvo Braz
et al, 2005): first, we applied three exhaustive iter-
ations of generic rules. Since these rules have low
fan-out (few possible right-hand-sides for a given
left-hand-side) it is affordable to apply and chain
them more freely. We then perform a single itera-
tion of all other lexical and lexical-syntactic rules,
1062
applying them only if their L part was matched in
F and their R part was matched inH.
The system was trained over the RTE-3 devel-
opment set, and tested on both RTE-3 test set and
RTE-4 (which includes only a test set).
Results Table 2 provides statistics on rule appli-
cation using all rule bases, over the RTE-3 devel-
opment set and the RTE-4 dataset
6
. Overall, the
primary result is that the compact forest indeed ac-
commodates well extensive rule application from
large-scale rule bases. The resulting forest size is
kept small, even in the maximal cases which were
causing memory overflow for explicit inference.
The accuracies obtained in this experiment and
the overall contribution of rule-based inference are
shown in Table 3. The results on RTE-3 are quite
competitive: compared to our 66.4%, only 3 teams
out of the 26 who participated in RTE-3 scored
higher than 67%, and three more systems scored
between 66% and 67%. The results for RTE4 rank
9-10 out of 26, with only 6 teams scoring higher by
more than 1%. Overall, these results validate that
the setting of our experiment represents a state-of-
the-art system.
Inference over the rule bases utilized in our
experiment improved the accuracy on both test
sets. The contribution was more prominent for
the RTE-4 dataset. These results illustrate a typ-
ical contribution of current knowledge sources for
current RTE systems. This contribution is likely
to increase with current and near future research,
on topics such as extending and improving knowl-
edge resources, applying them only in seman-
tically suitable contexts, improved classification
features and broader search strategies. As for our
current experiment, we may conclude that the goal
of assessing the compact forest scalability in a
state-of-the-art setting was achieved
7
.
Finally, Tables 4 and 5 illustrate the usage and
contribution of individual rule bases. Table 4
shows the distribution of rule applications over the
various rule bases. Table 5 presents ablation study
showing the marginal accuracy gain for each rule
base. These results show that each of the rule
bases is applicable for a large portion of the pairs,
and contributes to the overall accuracy.
6
Running time is omitted since most of it was dedicated
to rule fetching, which was rather slow for our available im-
plementation of some resources. The elapsed time was a few
seconds per (t, h) pair.
7
We note that common RTE research issues, such as im-
proving accuracy, fall out of the scope of the current paper.
RTE3-Dev RTE4
Avg. Max. Avg. Max.
Rule applications 14 275 15 110
Node count 71 606 80 357
Edge endpoints 155 1,741 173 1,062
Table 2: Application of compact inference to the
RTE-3 Dev. and RTE-4 datasets, using all rule
types.
Accuracy
Test set No inference Inference ?
RTE3 64.6% 66.4% 1.8%
RTE4 57.5% 60.6% *3.1%
Table 3: Inference contribution to RTE perfor-
mance. The system was trained on the RTE-3 de-
velopment set. * indicates statistically significant
difference (at level p < 0.02, using McNemar?s
test).
Rule base RTE3-Dev RTE4
Rules App Rules App
WordNet 0.6 1.2 0.6 1.1
AmWN 0.3 0.4 0.3 0.4
Wikipedia 0.6 1.7 0.6 1.3
DIRT 0.5 0.7 0.5 1.0
Generic 4.7 10.4 5.4 11.5
Polarity 0.2 0.2 0.2 0.2
Table 4: Average number of rule applications per
(t, h) pair, for each rule base. App counts each rule
application, while Rules ignores multiple matches
of the same rule in the same iteration.
Rule base ?Accuracy (RTE4)
WordNet 0.8%
AmWN 0.7%
Wikipedia 1.0%
DIRT 0.9%
Generic 0.4%
Polarity 0.9%
Table 5: Contribution of various rule bases. Re-
sults show accuracy loss on RTE-4, obtained for
removing each rule base (ablation tests).
5 Related Work
This section discusses related work on applying
knowledge-based transformations within RTE sys-
tems, as well as on using packed representations in
other NLP tasks.
RTE Systems Previous RTE systems usually re-
stricted both the type of allowed transformations
and the search space. Systems based on lexical
(word-based or phrase-based) matching of h in t
typically applied only lexical rules (without vari-
1063
ables), where both sides of the rule are matched
directly in t and h (Haghighi et al, 2005; Mac-
Cartney et al, 2008). The inference formalism
we use is more expressive, allowing also syntac-
tic and lexical-syntactic transformations as well as
rule chaining.
Hickl (2008) derived from a given (t, h) pair
a small set of discourse commitments, which are
quite similar to the kind of consequents we derive
by our syntactic and lexical-syntactic rules. The
commitments were generated by several different
tools and techniques, compared to our generic uni-
fied inference process, and commitment genera-
tion efficiency was not discussed.
Braz et al (2005) presented a semantic infer-
ence framework which ?augments? the text repre-
sentation with only the right-hand-side of an ap-
plied rule, and in this respect is similar to ours.
However, in their work, both rule application and
the semantics of the resulting ?augmented? struc-
ture were not fully specified. In particular, the dis-
tinction between individual consequents was lost
in the augmented graph. By contrast, our com-
pact inference is fully formalized and is provably
equivalent to an expressive, well-defined formal-
ism operating over individual trees, and each in-
ferred consequent can be recovered from the com-
pact forest.
Packed representations Packed representations
in various NLP tasks share common principles,
which also underly our compact forest: factor-
ing out common substructures and representing
choice as local disjunctions. Applying this gen-
eral scheme to individual problems typically re-
quires specific representations and algorithms, de-
pending on the type of alternatives that should be
represented and the specified operations for creat-
ing them. In our work, alternatives are created by
rule application, where a newly derived subtree is
set as an alternative to existing subtrees. Alterna-
tives are specified locally using d-edges.
Packed chart representations for parse forests
were introduced in classical parsing algorithms
such as CYK and Earley (Jurafsky and Martin,
2008), and have been extended in later work
for various purposes (Maxwell III and Kaplan,
1991; Kay, 1996). Alternatives in the parse chart
stem from syntactic ambiguities, and are speci-
fied locally as the possible decompositions of each
phrase into its sub-phrases.
Packed representations have been utilized also
in transfer-based machine translation. Emele and
Dorna (1998) translated packed source language
representation to packed target language represen-
tation while avoiding unnecessary unpacking dur-
ing transfer. Unlike our rule application, in their
work transfer rules preserve ambiguity stemming
from source language, rather than generating new
alternatives. Mi et al(2008) applied statistical ma-
chine translation to a source language parse forest,
rather than to the 1-best parse. Their transfer rules
are tree-to-string, contrary to our tree-to-tree rules,
and chaining is not attempted (rules are applied in
a single top-down pass over the source forest), and
thus their representation and algorithms are quite
different from ours.
6 Conclusion
This work addresses the efficiency of entailment
and paraphrase rule application. We presented a
novel compact data structure and a rule application
algorithm for it, which are provably a valid imple-
mentation of a generic inference formalism. We il-
lustrated inference efficiency both analytically and
empirically. Beyond entailment inference, we sug-
gest that the compact forest would also be use-
ful in generation tasks (e.g. paraphrasing). Our
efficient representation of the consequent search
space opens the way to future investigation of the
benefit of larger-scale rule chaining, and to the de-
velopment of efficient search strategies required to
support such inferences.
Acknowledgments
This work was partially supported by the
PASCAL-2 Network of Excellence of the Eu-
ropean Community FP7-ICT-2007-1-216886, the
FBK-irst/Bar-Ilan University collaboration and
the Israel Science Foundation grant 1112/08. The
second author is grateful to the Azrieli Foundation
for the award of an Azrieli Fellowship. The au-
thors would like to thank Yonatan Aumann, Marco
Pennacchiotti and Marc Dymetman for their valu-
able feedback on this work.
References
Roy Bar-Haim, Ido Dagan, Iddo Greental, and Eyal
Shnarch. 2007. Semantic inference at the lexical-
syntactic level. In Proceedings of AAAI.
Roy Bar-Haim, Jonathan Berant, Ido Dagan, Iddo
Greental, Shachar Mirkin, Eyal Shnarch, and Idan
1064
Szpektor. 2009. Efficient semantic deduction and
approximate matching over compact parse forests.
In Proceedings of the First Text Analysis Conference
(TAC 2008).
Rahul Bhagat and Deepak Ravichandran. 2008. Large
scale acquisition of paraphrases for learning surface
patterns. In Proceedings of ACL-08: HLT.
Rodrigo de Salvo Braz, Roxana Girju, Vasin Pun-
yakanok, Dan Roth, and Mark Sammons. 2005. An
inference model for semantic entailment in natural
language. In Proceedings of AAAI.
Martin C. Emele and Michael Dorna. 1998. Ambi-
guity preserving machine translation using packed
representations. In Proceedings of Coling-ACL.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. Language, Speech and
Communication. MIT Press.
Danilo Giampiccolo, Bernardo Magnini, Ido Dagan,
and Bill Dolan. 2007. The Third PASCAL Recog-
nizing Textual Entailment Challenge. In Proceed-
ings of the ACL-PASCAL Workshop on Textual En-
tailment and Paraphrasing.
Danilo Giampiccolo, Hoa Trang Dang, Bernardo
Magnini, Ido Dagan, and Bill Dolan. 2009. The
Fourth PASCAL Recognizing Textual Entailment
Challenge. In Proceedings of the First Text Analy-
sis Conference (TAC 2008).
Aria D. Haghighi, Andrew Y. Ng, and Christopher D.
Manning. 2005. Robust textual inference via graph
matching. In Proceedings of EMNLP.
Andrew Hickl. 2008. Using discourse commitments
to recognize textual entailment. In Proceedings of
COLING.
Daniel Jurafsky and James H. Martin. 2008. Speech
and Language Processing: An Introduction to Nat-
ural Language Processing, Computational Linguis-
tics and Speech Recognition. Prentice Hall, second
edition.
Martin Kay. 1996. Chart generation. In Proceedings
of ACL.
Dekang Lin and Patrick Pantel. 2001. Discovery of in-
ference rules for question answering. Natural Lan-
guage Engineering, 7(4):343?360.
Dekang Lin. 1998. Dependency-based evaluation of
minipar. In Proceedings of the Workshop on Evalu-
ation of Parsing Systems at LREC.
Bill MacCartney, Michel Galley, and Christopher D.
Manning. 2008. A phrase-based alignment model
for natural language inference. In Proceedings of
EMNLP.
John T. Maxwell III and Ronald M. Kaplan. 1991.
A method for disjunctive constraint satisfaction. In
Masaru Tomita, editor, Current Issues in Parsing
Technology. Kluwer Academic Publishers.
A. Meyers, R. Reeves, Catherine Macleod, Rachel
Szekeley, Veronkia Zielinska, and Brian Young.
2004. The cross-breeding of dictionaries. In Pro-
ceedings of LREC.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings of ACL-08: HLT.
Patrick Pantel, Rahul Bhagat, Bonaventura Coppola,
Timothy Chklovski, and Eduard Hovy. 2007. ISP:
Learning inferential selectional preferences. In Pro-
ceedings of NAACL-HLT.
Yusuke Shinyama, Satoshi Sekine, Kiyoshi Sudo, and
Ralph Grishman. 2002. Automatic paraphrase ac-
quisition from news articles. In Proceedings of Hu-
man Language Technology Conference (HLT 2002),
San Diego, USA.
Eyal Shnarch, Libby Barak, and Ido Dagan. 2009. Ex-
tracting lexical reference rules from Wikipedia. In
Proceedings of ACL-IJCNLP.
Idan Szpektor and Ido Dagan. 2007. Learning canon-
ical forms of entailment rules. In Proceedings of
RANLP.
Idan Szpektor and Ido Dagan. 2008. Learning entail-
ment rules for unary templates. In Proceedings of
COLING.
Idan Szpektor and Ido Dagan. 2009. Augmenting
WordNet-based inference with argument mapping.
In Proceedings of ACL-IJCNLP Workshop on Ap-
plied Textual Inference (TextInfer).
Idan Szpektor, Hristo Tanev, Ido Dagan, and Bonaven-
ture Coppola. 2004. Scaling web based acquisition
of entailment patterns. In Proceedings of EMNLP.
1065
Investigating a Generic Paraphrase-based Approach
for Relation Extraction
Lorenza Romano
ITC-irst
via Sommarive, 18
38050 Povo (TN), Italy
romano@itc.it
Milen Kouylekov
ITC-irst
via Sommarive, 18
38050 Povo (TN), Italy
kouylekov@itc.it
Idan Szpektor
Department of Computer Science
Bar Ilan University
Ramat Gan, 52900, Israel
szpekti@cs.biu.ac.il
Ido Dagan
Department of Computer Science
Bar Ilan University
Ramat Gan, 52900, Israel
dagan@cs.biu.ac.il
Alberto Lavelli
ITC-irst
via Sommarive, 18
38050 Povo (TN), Italy
lavelli@itc.it
Abstract
Unsupervised paraphrase acquisition has
been an active research field in recent
years, but its effective coverage and per-
formance have rarely been evaluated. We
propose a generic paraphrase-based ap-
proach for Relation Extraction (RE), aim-
ing at a dual goal: obtaining an applicative
evaluation scheme for paraphrase acquisi-
tion and obtaining a generic and largely
unsupervised configuration for RE.We an-
alyze the potential of our approach and
evaluate an implemented prototype of it
using an RE dataset. Our findings reveal a
high potential for unsupervised paraphrase
acquisition. We also identify the need for
novel robust models for matching para-
phrases in texts, which should address syn-
tactic complexity and variability.
1 Introduction
A crucial challenge for semantic NLP applica-
tions is recognizing the many different ways for
expressing the same information. This seman-
tic variability phenomenon was addressed within
specific applications, such as question answering,
information extraction and information retrieval.
Recently, the problem was investigated within
generic application-independent paradigms, such
as paraphrasing and textual entailment.
Eventually, it would be most appealing to apply
generic models for semantic variability to concrete
applications. This paper investigates the applica-
bility of a generic ?paraphrase-based? approach to
the Relation Extraction (RE) task, using an avail-
able RE dataset of protein interactions. RE is
highly suitable for such investigation since its goal
is to exactly identify all the different variations in
which a target semantic relation can be expressed.
Taking this route sets up a dual goal: (a) from
the generic paraphrasing perspective - an objective
evaluation of paraphrase acquisition performance
on a concrete application dataset, as well as identi-
fying the additional mechanisms needed to match
paraphrases in texts; (b) from the RE perspective -
investigating the feasibility and performance of a
generic paraphrase-based approach for RE.
Our configuration assumes a set of entailing
templates (non-symmetric ?paraphrases?) for the
target relation. For example, for the target rela-
tion ?X interact with Y? we would assume a set of
entailing templates as in Tables 3 and 7. In addi-
tion, we require a syntactic matching module that
identifies template instances in text.
First, we manually analyzed the protein-
interaction dataset and identified all cases in which
protein interaction is expressed by an entailing
template. This set a very high idealized upper
bound for the recall of the paraphrase-based ap-
proach for this dataset. Yet, obtaining high cover-
age in practice would require effective paraphrase
acquisition and lexical-syntactic template match-
ing. Next, we implemented a prototype that uti-
lizes a state-of-the-art method for learning en-
tailment relations from the web (Szpektor et al,
2004), the Minipar dependency parser (Lin, 1998)
and a syntactic matching module. As expected,
the performance of the implemented system was
much lower than the ideal upper bound, yet ob-
taining quite reasonable practical results given its
unsupervised nature.
The contributions of our investigation follow
409
the dual goal set above. To the best of our knowl-
edge, this is the first comprehensive evaluation
that measures directly the performance of unsuper-
vised paraphrase acquisition relative to a standard
application dataset. It is also the first evaluation of
a generic paraphrase-based approach for the stan-
dard RE setting. Our findings are encouraging for
both goals, particularly relative to their early ma-
turity level, and reveal constructive evidence for
the remaining room for improvement.
2 Background
2.1 Unsupervised Information Extraction
Information Extraction (IE) and its subfield Rela-
tion Extraction (RE) are traditionally performed
in a supervised manner, identifying the different
ways to express a specific information or relation.
Given that annotated data is expensive to produce,
unsupervised or weakly supervised methods have
been proposed for IE and RE.
Yangarber et al (2000) and Stevenson and
Greenwood (2005) define methods for automatic
acquisition of predicate-argument structures that
are similar to a set of seed relations, which rep-
resent a specific scenario. Yangarber et al (2000)
approach was evaluated in two ways: (1) manually
mapping the discovered patterns into an IE system
and running a full MUC-style evaluation; (2) using
the learned patterns to perform document filtering
at the scenario level. Stevenson and Greenwood
(2005) evaluated their method through document
and sentence filtering at the scenario level.
Sudo et al (2003) extract dependency subtrees
within relevant documents as IE patterns. The goal
of the algorithm is event extraction, though perfor-
mance is measured by counting argument entities
rather than counting events directly.
Hasegawa et al (2004) performs unsupervised
hierarchical clustering over a simple set of fea-
tures. The algorithm does not extract entity pairs
for a given relation from a set of documents but
rather classifies all relations in a large corpus. This
approach is more similar to text mining tasks than
to classic IE problems.
To conclude, several unsupervised approaches
learn relevant IE templates for a complete sce-
nario, but without identifying their relevance to
each specific relation within the scenario. Accord-
ingly, the evaluations of these works either did not
address the direct applicability for RE or evaluated
it only after further manual postprocessing.
2.2 Paraphrases and Entailment Rules
A generic model for language variability is us-
ing paraphrases, text expressions that roughly con-
vey the same meaning. Various methods for auto-
matic paraphrase acquisition have been suggested
recently, ranging from finding equivalent lexical
elements to learning rather complex paraphrases
at the sentence level1.
More relevant for RE are ?atomic? paraphrases
between templates, text fragments containing vari-
ables, e.g. ?X buy Y ? X purchase Y?. Under a
syntactic representation, a template is a parsed text
fragment, e.g. ?X subj? interact mod? with pcomp?n? Y?
(based on the syntactic dependency relations of
the Minipar parser). The parses include part-of-
speech tags, which we omit for clarity.
Dagan and Glickman (2004) suggested that a
somewhat more general notion than paraphrasing
is that of entailment relations. These are direc-
tional relations between two templates, where the
meaning of one can be entailed from the meaning
of the other, e.g. ?X bind to Y? X interact with Y?.
For RE, when searching for a target relation, it is
sufficient to identify an entailing template since it
implies that the target relation holds as well. Un-
der this notion, paraphrases are bidirectional en-
tailment relations.
Several methods extract atomic paraphrases by
exhaustively processing local corpora (Lin and
Pantel, 2001; Shinyama et al, 2002). Learn-
ing from a local corpus is bounded by the cor-
pus scope, which is usually domain specific (both
works above processed news domain corpora). To
cover a broader range of domains several works
utilized the Web, while requiring several manu-
ally provided examples for each input relation,
e.g. (Ravichandran and Hovy, 2002). Taking a
step further, the TEASE algorithm (Szpektor et al,
2004) provides a completely unsupervised method
for acquiring entailment relations from the Web
for a given input relation (see Section 5.1).
Most of these works did not evaluate their re-
sults in terms of application coverage. Lin and
Pantel (2001) compared their results to human-
generated paraphrases. Shinyama et al (2002)
measured the coverage of their learning algorithm
relative to the paraphrases present in a given cor-
pus. Szpektor et al (2004) measured ?yield?, the
number of correct rules learned for an input re-
1See the 3rd IWP workshop for a sample of recent works
on paraphrasing (http://nlp.nagaokaut.ac.jp/IWP2005/).
410
lation. Ravichandran and Hovy (2002) evaluated
the performance of a QA system that is based
solely on paraphrases, an approach resembling
ours. However, they measured performance using
Mean Reciprocal Rank, which does not reveal the
actual coverage of the learned paraphrases.
3 Assumed Configuration for RE
Phenomenon Example
Passive form ?Y is activated by X?
Apposition ?X activates its companion, Y?
Conjunction ?X activates prot3 and Y?
Set ?X activates two proteins, Y and Z?
Relative clause ?X, which activates Y?
Coordination ?X binds and activates Y?
Transparent head ?X activates a fragment of Y?
Co-reference ?X is a kinase, though it activates Y?
Table 1: Syntactic variability phenomena, demon-
strated for the normalized template ?X activate Y?.
The general configuration assumed in this pa-
per for RE is based on two main elements: a list
of lexical-syntactic templates which entail the re-
lation of interest and a syntactic matcher which
identifies the template occurrences in sentences.
The set of entailing templates may be collected ei-
ther manually or automatically. We propose this
configuration both as an algorithm for RE and as
an evaluation scheme for paraphrase acquisition.
The role of the syntactic matcher is to iden-
tify the different syntactic variations in which tem-
plates occur in sentences. Table 1 presents a list
of generic syntactic phenomena that are known in
the literature to relate to linguistic variability. A
phenomenon which deserves a few words of ex-
planation is the ?transparent head noun? (Grish-
man et al, 1986; Fillmore et al, 2002). A trans-
parent noun N1 typically occurs in constructs of
the form ?N1 preposition N2? for which the syn-
tactic relation involving N1, which is the head of
the NP, applies to N2, the modifier. In the example
in Table 1, ?fragment? is the transparent head noun
while the relation ?activate? applies to Y as object.
4 Manual Data Analysis
4.1 Protein Interaction Dataset
Bunescu et al (2005) proposed a set of tasks re-
garding protein name and protein interaction ex-
traction, for which they manually tagged about
200 Medline abstracts previously known to con-
tain human protein interactions (a binary symmet-
ric relation). Here we consider their RE task of
extracting interacting protein pairs, given that the
correct protein names have already been identi-
fied. All protein names are annotated in the given
gold standard dataset, which includes 1147 anno-
tated interacting protein pairs. Protein names are
rather complex, and according to the annotation
adopted by Bunescu et al (2005) can be substrings
of other protein names (e.g., <prot> <prot>
GITR </prot> ligand </prot>). In such
cases, we considered only the longest names and
protein pairs involving them. We also ignored all
reflexive pairs, in which one protein is marked
as interacting with itself. Altogether, 1052 inter-
actions remained. All protein names were trans-
formed into symbols of the type ProtN , where N
is a number, which facilitates parsing.
For development purposes, we randomly split
the abstracts into a 60% development set (575 in-
teractions) and a 40% test set (477 interactions).
4.2 Dataset analysis
In order to analyze the potential of our approach,
two of the authors manually annotated the 575 in-
teracting protein pairs in the development set. For
each pair the annotators annotated whether it can
be identified using only template-based matching,
assuming an ideal implementation of the configu-
ration of Section 3. If it can, the normalized form
of the template connecting the two proteins was
annotated as well. The normalized template form
is based on the active form of the verb, stripped
of the syntactic phenomena listed in Table 1. Ad-
ditionally, the relevant syntactic phenomena from
Table 1 were annotated for each template instance.
Table 2 provides several example annotations.
A Kappa value of 0.85 (nearly perfect agree-
ment) was measured for the agreement between
the two annotators, regarding whether a protein
pair can be identified using the template-based
method. Additionally, the annotators agreed on
96% of the normalized templates that should be
used for the matching. Finally, the annotators
agreed on at least 96% of the cases for each syn-
tactic phenomenon except transparent heads, for
which they agreed on 91% of the cases. This high
level of agreement indicates both that template-
based matching is a well defined task and that nor-
malized template form and its syntactic variations
are well defined notions.
Several interesting statistics arise from the an-
411
Sentence Annotation
We have crystallized a complex between human FGF1 and
a two-domain extracellular fragment of human FGFR2.
? template: ?complex between X and Y?
? transparent head: ?fragment of X?
CD30 and its counter-receptor CD30 ligand (CD30L) are
members of the TNF-receptor / TNFalpha superfamily and
function to regulate lymphocyte survival and differentiation.
? template: ?X?s counter-receptor Y?
? apposition
? co-reference
iCdi1, a human G1 and S phase protein phosphatase that
associates with Cdk2.
? template: ?X associate with Y?
? relative clause
Table 2: Examples of annotations of interacting protein pairs. The annotation describes the normalized
template and the different syntactic phenomena identified.
Template f Template f Template f
X interact with Y 28 interaction of X with Y 12 X Y interaction 5
X bind to Y 22 X associate with Y 11 X interaction with Y 4
X Y complex 17 X activate Y 6 association of X with Y 4
interaction between X and Y 16 binding of X to Y 5 X?s association with Y 3
X bind Y 14 X form complex with Y 5 X be agonist for Y 3
Table 3: The 15 most frequent templates and their instance count (f ) in the development set.
notation. First, 93% of the interacting protein pairs
(537/575) can be potentially identified using the
template-based approach, if the relevant templates
are provided. This is a very promising finding,
suggesting that the template-based approach may
provide most of the requested information. We
term these 537 pairs as template-based pairs. The
remaining pairs are usually expressed by complex
inference or at a discourse level.
Phenomenon % Phenomenon %
transparent head 34 relative clause 8
apposition 24 co-reference 7
conjunction 24 coordination 7
set 13 passive form 2
Table 4: Occurrence percentage of each syntactic
phenomenon within template-based pairs (537).
Second, for 66% of the template-based pairs
at least one syntactic phenomenon was annotated.
Table 4 contains the occurrence percentage of each
phenomenon in the development set. These results
show the need for a powerful syntactic matcher on
top of high performance template acquisition, in
order to correctly match a template in a sentence.
Third, 175 different normalized templates were
identified. For each template we counted its tem-
plate instances, the number of times the tem-
plate occurred, counting only occurrences that ex-
press an interaction of a protein pair. In total,
we counted 341 template instances for all 175
templates. Interestingly, 50% of the template in-
stances (184/341) are instances of the 21 most fre-
quent templates. This shows that, though protein
interaction can be expressed in many ways, writ-
ers tend to choose from among just a few common
expressions. Table 3 presents the most frequent
templates. Table 5 presents the minimal number
of templates required to obtain the range of differ-
ent recall levels.
Furthermore, we grouped template variants
that are based on morphological derivations (e.g.
?X interact with Y? and ?X Y interaction?)
and found that 4 groups, ?X interact with Y?,
?X bind to Y?, ?X associate with Y? and ?X com-
plex with Y?, together with their morphological
derivations, cover 45% of the template instances.
This shows the need to handle generic lexical-
syntactic phenomena, and particularly morpholog-
ical based variations, separately from the acquisi-
tion of normalized lexical syntactic templates.
To conclude, this analysis indicates that the
template-based approach provides very high cov-
erage for this RE dataset, and a small number of
normalized templates already provides significant
recall. However, it is important to (a) develop
a model for morphological-based template vari-
ations (e.g. as encoded in Nomlex (Macleod et
al., )), and (b) apply accurate parsing and develop
syntactic matching models to recognize the rather
412
complex variations of template instantiations in
text. Finally, we note that our particular figures
are specific to this dataset and the biological ab-
stracts domain. However, the annotation and anal-
ysis methodologies are general and are suggested
as highly effective tools for further research.
R(%) # templates R(%) # templates
10 2 60 39
20 4 70 73
30 6 80 107
40 11 90 141
50 21 100 175
Table 5: The number of most frequent templates
necessary to reach different recall levels within the
341 template instances.
5 Implemented Prototype
This section describes our initial implementation
of the approach in Section 3.
5.1 TEASE
The TEASE algorithm (Szpektor et al, 2004) is
an unsupervised method for acquiring entailment
relations from the Web for a given input template.
In this paper we use TEASE for entailment rela-
tion acquisition since it processes an input tem-
plate in a completely unsupervised manner and
due to its broad domain coverage obtained from
the Web. The reported percentage of correct out-
put templates for TEASE is 44%.
The TEASE algorithm consists of 3 steps,
demonstrated in Table 6. TEASE first retrieves
from the Web sentences containing the input tem-
plate. From these sentences it extracts variable in-
stantiations, termed anchor-sets, which are identi-
fied as being characteristic for the input template
based on statistical criteria (first column in Ta-
ble 6). Characteristic anchor-sets are assumed to
uniquely identify a specific event or fact. Thus,
any template that appears with such an anchor-set
is assumed to have an entailment relationship with
the input template. Next, TEASE retrieves from
the Web a corpus S of sentences that contain the
characteristic anchor-sets (second column), hop-
ing to find occurrences of these anchor-sets within
templates other than the original input template.
Finally, TEASE parses S and extracts templates
that are assumed to entail or be entailed by the
input template. Such templates are identified as
maximal most general sub-graphs that contain the
anchor sets? positions (third column in Table 6).
Each learned template is ranked by number of oc-
currences in S.
5.2 Transformation-based Graph Matcher
In order to identify instances of entailing templates
in sentences we developed a syntactic matcher that
is based on transformations rules. The matcher
processes a sentence in 3 steps: 1) parsing the sen-
tence with the Minipar parser, obtaining a depen-
dency graph2; 2) matching each template against
the sentence dependency graph; 3) extracting can-
didate term pairs that match the template variables.
A template is considered directly matched in a
sentence if it appears as a sub-graph in the sen-
tence dependency graph, with its variables instan-
tiated. To further address the syntactic phenomena
listed in Table 1 we created a set of hand-crafted
parser-dependent transformation rules, which ac-
count for the different ways in which syntactic
relationships may be realized in a sentence. A
transformation rule maps the left hand side of the
rule, which strictly matches a sub-graph of the
given template, to the right hand side of the rule,
which strictly matches a sub-graph of the sentence
graph. If a rule matches, the template sub-graph is
mapped accordingly into the sentence graph.
For example, to match the syntactic tem-
plate ?X(N) subj? activate(V) obj? Y(N)? (POS
tags are in parentheses) in the sentence ?Prot1
detected and activated Prot2? (see Figure 1) we
should handle the coordination phenomenon.
The matcher uses the transformation rule
?Var1(V) ? and(U)mod? Word(V) conj? Var1(V)?
to overcome the syntactic differences. In this
example Var1 matches the verb ?activate?, Word
matches the verb ?detect? and the syntactic rela-
tions for Word are mapped to the ones for Var1.
Thus, we can infer that the subject and object
relations of ?detect? are also related to ?activate?.
6 Experiments
6.1 Experimental Settings
To acquire a set of entailing templates we first ex-
ecuted TEASE on the input template ?X subj? in-
teract mod? with pcomp?n? Y?, which corresponds to
the ?default? expression of the protein interaction
2We chose a dependency parser as it captures directly the
relations between words; we use Minipar due to its speed.
413
Extracted Anchor-set Sentence containing Anchor-set Learned Template
X=?chemokines?,
Y=?specific receptors?
Chemokines bind to specific receptors on the target
cells
X subj? bind mod?
to
pcomp?n
? Y
X=?Smad3?, Y=?Smad4? Smad3 / Smad4 complexes translocate to the nucleus X Y nn? complex
Table 6: TEASE output at different steps of the algorithm for ?X subj? interact mod? with pcomp?n? Y?.
1. X bind to Y 7. X Y complex 13. X interaction with Y
2. X activate Y 8. X recognize Y 14. X trap Y
3. X stimulate Y 9. X block Y 15. X recruit Y
4. X couple to Y 10. X binding to Y 16. X associate with Y
5. interaction between X and Y 11. X Y interaction 17. X be linked to Y
6. X become trapped in Y 12. X attach to Y 18. X target Y
Table 7: The top 18 correct templates learned by TEASE for ?X interact with Y?.
detect(V )
subjwwppp
pp
pp
pp
pp
conj

mod
''NN
NN
NN
NN
NN
N
obj // Prot2(N)
Prot1(N) activate(V ) and(U)
Figure 1: The dependency parse graph of the sen-
tence ?Prot1 detected and activated Prot2?.
relation. TEASE learned 118 templates for this
relation. Table 7 lists the top 18 learned templates
that we considered as correct (out of the top 30
templates in TEASE output). We then extracted
interacting protein pair candidates by applying the
syntactic matcher to the 119 templates (the 118
learned plus the input template). Candidate pairs
that do not consist of two proteins, as tagged in the
input dataset, were filtered out (see Section 4.1;
recall that our experiments were applied to the
dataset of protein interactions, which isolates the
RE task from the protein name recognition task).
In a subsequent experiment we iteratively ex-
ecuted TEASE on the 5 top-ranked learned tem-
plates to acquire additional relevant templates. In
total, we obtained 1233 templates that were likely
to imply the original input relation. The syntactic
matcher was then reapplied to extract candidate in-
teracting protein pairs using all 1233 templates.
We used the development set to tune a small
set of 10 generic hand-crafted transformation rules
that handle different syntactic variations. To han-
dle transparent head nouns, which is the only phe-
nomenon that demonstrates domain dependence,
we extracted a set of the 5 most frequent trans-
parent head patterns in the development set, e.g.
?fragment of X?.
In order to compare (roughly) our performance
with supervised methods applied to this dataset, as
summarized in (Bunescu et al, 2005), we adopted
their recall and precision measurement. Their
scheme counts over distinct protein pairs per ab-
stract, which yields 283 interacting pairs in our test
set and 418 in the development set.
6.2 Manual Analysis of TEASE Recall
experiment pairs instances
input 39% 37%
input + iterative 49% 48%
input + iterative + morph 63% 62%
Table 8: The potential recall of TEASE in terms of
distinct pairs (out of 418) and coverage of template
instances (out of 341) in the development set.
Before evaluating the system as a whole we
wanted to manually assess in isolation the cover-
age of TEASE output relative to all template in-
stances that were manually annotated in the devel-
opment set. We considered a template as covered
if there is a TEASE output template that is equal
to the manually annotated template or differs from
it only by the syntactic phenomena described in
Section 3 or due to some parsing errors. Count-
ing these matches, we calculated the number of
template instances and distinct interacting protein
pairs that are covered by TEASE output.
Table 8 presents the results of our analysis. The
414
1st line shows the coverage of the 119 templates
learned by TEASE for the input template ?X inter-
act with Y?. It is interesting to note that, though we
aim to learn relevant templates for the specific do-
main, TEASE learned relevant templates also by
finding anchor-sets of different domains that use
the same jargon, such as particle physics.
We next analyzed the contribution of the itera-
tive learning for the additional 5 templates to recall
(2nd line in Table 8). With the additional learned
templates, recall increased by about 25%, showing
the importance of using the iterative steps.
Finally, when allowing matching between a
TEASE template and a manually annotated tem-
plate, even if one is based on a morphologi-
cal derivation of the other (3rd line in Table 8),
TEASE recall increased further by about 30%.
We conclude that the potential recall of the cur-
rent version of TEASE on the protein interaction
dataset is about 60%. This indicates that signif-
icant coverage can be obtained using completely
unsupervised learning from the web, as performed
by TEASE. However, the upper bound for our cur-
rent implemented system is only about 50% be-
cause our syntactic matching does not handle mor-
phological derivations.
6.3 System Results
experiment recall precision F1
input 0.18 0.62 0.28
input + iterative 0.29 0.42 0.34
Table 9: System results on the test set.
Table 9 presents our system results for the test
set, corresponding to the first two experiments in
Table 8. The recall achieved by our current imple-
mentation is notably worse than the upper bound
of the manual analysis because of two general set-
backs of the current syntactic matcher: 1) parsing
errors; 2) limited transformation rule coverage.
First, the texts from the biology domain pre-
sented quite a challenge for the Minipar parser.
For example, in the sentences containing the
phrase ?X bind specifically to Y? the parser consis-
tently attaches the PP ?to? to ?specifically? instead
of to ?bind?. Thus, the template ?X bind to Y? can-
not be directly matched.
Second, we manually created a small number of
transformation rules that handle various syntactic
phenomena, since we aimed at generic domain in-
dependent rules. The most difficult phenomenon
to model with transformation rules is transparent
heads. For example, in ?the dimerization of Prot1
interacts with Prot2?, the transparent head ?dimer-
ization of X? is domain dependent. Transforma-
tion rules that handle such examples are difficult
to acquire, unless a domain specific learning ap-
proach (either supervised or unsupervised) is used.
Finally, we did not handle co-reference resolution
in the current implementation.
Bunescu et al (2005) and Bunescu and Mooney
(2005) approached the protein interaction RE task
using both handcrafted rules and several super-
vised Machine Learning techniques, which uti-
lize about 180 manually annotated abstracts for
training. Our results are not directly comparable
with theirs because they adopted 10-fold cross-
validation, while we had to divide the dataset into
a development and a test set, but a rough compari-
son is possible. For the same 30% recall, the rule-
based method achieved precision of 62% and the
best supervised learning algorithm achieved preci-
sion of 73%. Comparing to these supervised and
domain-specific rule-based approaches our system
is noticeably weaker, yet provides useful results
given that we supply very little domain specific in-
formation and acquire the paraphrasing templates
in a fully unsupervised manner. Still, the match-
ing models need considerable additional research
in order to achieve the potential performance sug-
gested by TEASE.
7 Conclusions and Future Work
We have presented a paraphrase-based approach
for relation extraction (RE), and an implemented
system, that rely solely on unsupervised para-
phrase acquisition and generic syntactic template
matching. Two targets were investigated: (a) a
mostly unsupervised, domain independent, con-
figuration for RE, and (b) an evaluation scheme
for paraphrase acquisition, providing a first evalu-
ation of its realistic coverage. Our approach differs
from previous unsupervised IE methods in that we
identify instances of a specific relation while prior
methods identified template relevance only at the
general scenario level.
We manually analyzed the potential of our ap-
proach on a dataset annotated with protein in-
teractions. The analysis shows that 93% of the
interacting protein pairs can be potentially iden-
tified with the template-based approach. Addi-
415
tionally, we manually assessed the coverage of
the TEASE acquisition algorithm and found that
63% of the distinct pairs can be potentially rec-
ognized with the learned templates, assuming an
ideal matcher, indicating a significant potential re-
call for completely unsupervised paraphrase ac-
quisition. Finally, we evaluated our current system
performance and found it weaker than supervised
RE methods, being far from fulfilling the poten-
tial indicated in our manual analyses due to insuf-
ficient syntactic matching. But, even our current
performance may be considered useful given the
very small amount of domain-specific information
used by the system.
Most importantly, we believe that our analysis
and evaluation methodologies for an RE dataset
provide an excellent benchmark for unsupervised
learning of paraphrases and entailment rules. In
the long run, we plan to develop and improve our
acquisition and matching algorithms, in order to
realize the observed potential of the paraphrase-
based approach. Notably, our findings point to the
need to learn generic morphological and syntactic
variations in template matching, an area which has
rarely been addressed till now.
Acknowledgements
This work was developed under the collaboration
ITC-irst/University of Haifa. Lorenza Romano
has been supported by the ONTOTEXT project,
funded by the Autonomous Province of Trento un-
der the FUP-2004 research program.
References
Razvan Bunescu and Raymond J. Mooney. 2005. Sub-
sequence kernels for relation extraction. In Proceed-
ings of the 19th Conference on Neural Information
Processing Systems, Vancouver, British Columbia.
Razvan Bunescu, Ruifang Ge, Rohit J. Kate, Ed-
ward M. Marcotte, Raymond J. Mooney, Arun K.
Ramani, and Yuk Wah Wong. 2005. Comparative
experiments on learning information extractors for
proteins and their interactions. Artificial Intelligence
in Medicine, 33(2):139?155. Special Issue on Sum-
marization and Information Extraction from Medi-
cal Documents.
Ido Dagan and Oren Glickman. 2004. Probabilis-
tic textual entailment: Generic applied modeling of
language variability. In Proceedings of the PAS-
CAL Workshop on Learning Methods for Text Un-
derstanding and Mining, Grenoble, France.
Charles J. Fillmore, Collin F. Baker, and Hiroaki Sato.
2002. Seeing arguments through transparent struc-
tures. In Proceedings of the 3rd International
Conference on Language Resources and Evaluation
(LREC 2002), pages 787?791, Las Palmas, Spain.
Ralph Grishman, Lynette Hirschman, and Ngo Thanh
Nhan. 1986. Discovery procedures for sublanguage
selectional patterns: Initial experiments. Computa-
tional Linguistics, 12(3).
Takaaki Hasegawa, Satoshi Sekine, and Ralph Grish-
man. 2004. Discoverying relations among named
entities from large corpora. In Proceedings of the
42nd Annual Meeting of the Association for Compu-
tational Linguistics (ACL 2004), Barcelona, Spain.
Dekang Lin and Patrick Pantel. 2001. Discovery of in-
ference rules for question answering. Natural Lan-
guage Engineering, 7(4):343?360.
Dekang Lin. 1998. Dependency-based evaluation on
MINIPAR. In Proceedings of LREC-98 Workshop
on Evaluation of Parsing Systems, Granada, Spain.
Catherine Macleod, Ralph Grishman, Adam Meyers,
Leslie Barrett, and Ruth Reeves. Nomlex: A lexi-
con of nominalizations. In Proceedings of the 8th
International Congress of the European Association
for Lexicography, Liege, Belgium.
Deepak Ravichandran and Eduard Hovy. 2002. Learn-
ing surface text patterns for a Question Answering
system. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguistics
(ACL 2002), Philadelphia, PA.
Yusuke Shinyama, Satoshi Sekine, Kiyoshi Sudo, and
Ralph Grishman. 2002. Automatic paraphrase ac-
quisition from news articles. In Proceedings of
the Human Language Technology Conference (HLT
2002), San Diego, CA.
Mark Stevenson and Mark A. Greenwood. 2005. A
semantic approach to IE pattern induction. In Pro-
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL 2005), Ann
Arbor, Michigan.
K. Sudo, S. Sekine, and R. Grishman. 2003. An im-
proved extraction pattern representation model for
automatic IE pattern acquisition. In Proceedings of
the 41st Annual Meeting of the Association for Com-
putational Linguistics (ACL 2003), Sapporo, Japan.
Idan Szpektor, Hristo Tanev, Ido Dagan, and Bonaven-
tura Coppola. 2004. Scaling web-based acquisi-
tion of entailment relations. In Proceedings of the
2004 Conference on Empirical Methods in Natural
Language Processing (EMNLP 2004), Barcelona,
Spain.
Roman Yangarber, Ralph Grishman, Pasi Tapanainen,
and Silja Huttunen. 2000. Automatic acquisition
of domain knowledge for information extraction. In
Proceedings of the 18th International Conference on
Computational Linguistics, Saarbruecken, Germany.
416
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 558?566,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Evaluating the Inferential Utility of Lexical-Semantic Resources
Shachar Mirkin, Ido Dagan, Eyal Shnarch
Computer Science Department, Bar-Ilan University
Ramat-Gan 52900, Israel
{mirkins,dagan,shey}@cs.biu.ac.il
Abstract
Lexical-semantic resources are used ex-
tensively for applied semantic inference,
yet a clear quantitative picture of their
current utility and limitations is largely
missing. We propose system- and
application-independent evaluation and
analysis methodologies for resources? per-
formance, and systematically apply them
to seven prominent resources. Our find-
ings identify the currently limited recall of
available resources, and indicate the po-
tential to improve performance by exam-
ining non-standard relation types and by
distilling the output of distributional meth-
ods. Further, our results stress the need
to include auxiliary information regarding
the lexical and logical contexts in which
a lexical inference is valid, as well as its
prior validity likelihood.
1 Introduction
Lexical information plays a major role in seman-
tic inference, as the meaning of one term is of-
ten inferred form another. Lexical-semantic re-
sources, which provide the needed knowledge for
lexical inference, are commonly utilized by ap-
plied inference systems (Giampiccolo et al, 2007)
and applications such as Information Retrieval and
Question Answering (Shah and Croft, 2004; Pasca
and Harabagiu, 2001). Beyond WordNet (Fell-
baum, 1998), a wide range of resources has been
developed and utilized, including extensions to
WordNet (Moldovan and Rus, 2001; Snow et al,
2006) and resources based on automatic distri-
butional similarity methods (Lin, 1998; Pantel
and Lin, 2002). Recently, Wikipedia is emerg-
ing as a source for extracting semantic relation-
ships (Suchanek et al, 2007; Kazama and Tori-
sawa, 2007).
As of today, only a partial comparative picture
is available regarding the actual utility and limi-
tations of available resources for lexical-semantic
inference. Works that do provide quantitative
information regarding resources utility have fo-
cused on few particular resources (Kouylekov and
Magnini, 2006; Roth and Sammons, 2007) and
evaluated their impact on a specific system. Most
often, works which utilized lexical resources do
not provide information about their isolated con-
tribution; rather, they only report overall per-
formance for systems in which lexical resources
serve as components.
Our paper provides a step towards clarify-
ing this picture. We propose a system- and
application-independent evaluation methodology
that isolates resources? performance, and sys-
tematically apply it to seven prominent lexical-
semantic resources. The evaluation and analysis
methodology is specified within the Textual En-
tailment framework, which has become popular in
recent years for modeling practical semantic infer-
ence in a generic manner (Dagan and Glickman,
2004). To that end, we assume certain definitions
that extend the textual entailment paradigm to the
lexical level.
The findings of our work provide useful insights
and suggested directions for two research com-
munities: developers of applied inference systems
and researchers addressing lexical acquisition and
resource construction. Beyond the quantitative
mapping of resources? performance, our analysis
points at issues concerning their effective utiliza-
tion and major characteristics. Even more impor-
tantly, the results highlight current gaps in exist-
ing resources and point at directions towards fill-
ing them. We show that the coverage of most
resources is quite limited, where a substantial
part of recall is attributable to semantic relations
that are typically not available to inference sys-
tems. Notably, distributional acquisition methods
558
are shown to provide many useful relationships
which are missing from other resources, but these
are embedded amongst many irrelevant ones. Ad-
ditionally, the results highlight the need to rep-
resent and inference over various aspects of con-
textual information, which affect the applicability
of lexical inferences. We suggest that these gaps
should be addressed by future research.
2 Sub-sentential Textual Entailment
Textual entailment captures the relation between a
text t and a textual statement (termed hypothesis)
h, such that a person reading t would infer that h
is most likely correct (Dagan et al, 2005).
The entailment relation has been defined insofar
in terms of truth values, assuming that h is a com-
plete sentence (proposition). However, there are
major aspects of inference that apply to the sub-
sentential level. First, in certain applications, the
target hypotheses are often sub-sentential. For ex-
ample, search queries in IR, which play the hy-
pothesis role from an entailment perspective, typ-
ically consist of a single term, like drug legaliza-
tion. Such sub-sentential hypotheses are not re-
garded naturally in terms of truth values and there-
fore do not fit well within the scope of the textual
entailment definition. Second, many entailment
models apply a compositional process, through
which they try to infer each sub-part of the hy-
pothesis from some parts of the text (Giampiccolo
et al, 2007).
Although inferences over sub-sentential ele-
ments are being applied in practice, so far there
are no standard definitions for entailment at sub-
sentential levels. To that end, and as a prerequisite
of our evaluation methodology and our analysis,
we first establish two relevant definitions for sub-
sentential entailment relations: (a) entailment of a
sub-sentential hypothesis by a text, and (b) entail-
ment of one lexical element by another.
2.1 Entailment of Sub-sentential Hypotheses
We first seek a definition that would capture the
entailment relationship between a text and a sub-
sentential hypothesis. A similar goal was ad-
dressed in (Glickman et al, 2006), who defined
the notion of lexical reference to model the fact
that in order to entail a hypothesis, the text has
to entail each non-compositional lexical element
within it. We suggest that a slight adaptation of
their definition is suitable to capture the notion of
entailment for any sub-sentential hypotheses, in-
cluding compositional ones:
Definition 1 A sub-sentential hypothesis h is en-
tailed by a text t if there is an explicit or implied
reference in t to a possible meaning of h.
For example, the sentence ?crude steel output
is likely to fall in 2000? entails the sub-sentential
hypotheses production, steel production and steel
output decrease.
Glickman et al, achieving good inter-annotator
agreement, empirically found that almost all non-
compositional terms in an entailed sentential hy-
pothesis are indeed referenced in the entailing text.
This finding suggests that the above definition is
consistent with the original definition of textual
entailment for sentential hypotheses and can thus
model compositional entailment inferences.
We use this definition in our annotation method-
ology described in Section 3.
2.2 Entailment between Lexical Elements
In the majority of cases, the reference to an
?atomic? (non-compositional) lexical element e in
h stems from a particular lexical element e? in t,
as in the example above where the word output
implies the meaning of production.
To identify this relationship, an entailment sys-
tem needs a knowledge resource that would spec-
ify that the meaning of e? implies the meaning of
e, at least in some contexts. We thus suggest the
following definition to capture this relationship be-
tween e? and e:
Definition 2 A lexical element e? entails another
lexical element e, denoted e??e, if there exist
some natural (non-anecdotal) texts containing e?
which entail e, such that the reference to the mean-
ing of e can be implied solely from the meaning of
e? in the text.
(Entailment of e by a text follows Definition 1).
We refer to this relation in this paper as lexical
entailment1, and call e? ? e a lexical entailment
rule. e? is referred to as the rule?s left hand side
(LHS) and e as its right hand side (RHS).
Currently there are no knowledge resources de-
signed specifically for lexical entailment model-
ing. Hence, the types of relationships they cap-
ture do not fully coincide with entailment infer-
ence needs. Thus, the definition suggests a spec-
ification for the rules that should be provided by
1Section 6 discusses other definitions of lexical entailment
559
a lexical entailment resource, following an oper-
ative rationale: a rule e? ? e should be included
in an entailment knowledge resource if it would be
needed, as part of a compositional process, to infer
the meaning of e from some natural texts. Based
on this definition, we perform an analysis of the re-
lationships included in lexical-semantic resources,
as described in Section 5.
A rule need not apply in all contexts, as long
as it is appropriate for some texts. Two contex-
tual aspects affect rule applicability. First is the
?lexical context? specifying the meanings of the
text?s words. A rules is applicable in a certain con-
text only when the intended sense of its LHS term
matches the sense of that term in the text. For ex-
ample, the application of the rule lay ? produce is
valid only in contexts where the producer is poul-
try and the products are eggs. This is a well known
issue observed, for instance, by Voorhees (1994).
A second contextual factor requiring validation
is the ?logical context?. The logical context de-
termines the monotonicity of the LHS and is in-
duced by logical operators such as negation and
(explicit or implicit) quantifiers. For example, the
rule mammal ? whale may not be valid in most
cases, but is applicable in universally quantified
texts like ?mammals are warm-blooded?. This is-
sue has been rarely addressed in applied inference
systems (de Marneffe et al, 2006). The above
mentioned rules both comply with Definition 2
and should therefore be included in a lexical en-
tailment resource.
3 Evaluating Entailment Resources
Our evaluation goal is to assess the utility of
lexical-semantic resources as sources for entail-
ment rules. An inference system applies a rule by
inferring the rule?s RHS from texts that match its
LHS. Thus, the utility of a resource depends on the
performance of its rule applications rather than on
the proportion of correct rules it contains. A rule,
whether correct or incorrect, has insignificant ef-
fect on the resource?s utility if it rarely matches
texts in real application settings. Additionally,
correct rules might produce incorrect applications
when applied in inappropriate contexts. There-
fore, we use an instance-based evaluation method-
ology, which simulates rule applications by col-
lecting texts that contain rules? LHS and manually
assessing the correctness of their applications.
Systems typically handle lexical context either
implicitly or explicitly. Implicit context valida-
tion occurs when the different terms of a compos-
ite hypothesis disambiguate each other. For exam-
ple, the rule waterside ? bank is unlikely to be
applied when trying to infer the hypothesis bank
loans, since texts that match waterside are unlikely
to contain also the meaning of loan. Explicit meth-
ods, such as word-sense disambiguation or sense
matching, validate each rule application according
to the broader context in the text. Few systems
also address logical context validation by handling
quantifiers and negation. As we aim for a system-
independent comparison of resources, and explicit
approaches are not standardized yet within infer-
ence systems, our evaluation uses only implicit
context validation.
3.1 Evaluation Methodology
Figure 1: Evaluation methodology flow chart
The input for our evaluation methodology is a
lexical-semantic resource R, which contains lex-
ical entailment rules. We evaluate R?s utility by
testing how useful it is for inferring a sample of
test hypotheses H from a corpus. Each hypothesis
in H contains more than one lexical element in or-
der to provide implicit context validation for rule
applications, e.g. h: water pollution.
We next describe the steps of our evaluation
methodology, as illustrated in Figure 1. We refer
to the examples in the figure when needed:
1) Fetch rules: For each h ? H and each
lexical element e ? h (e.g. water), we fetch all
rules e? ? e in R that might be applied to entail e
(e.g. lake ? water).
2) Generate intermediate hypotheses h?:
For each rule r: e? ? e, we generate an intermedi-
ate hypothesis h? by replacing e in h with e? (e.g.
560
h?1: lake pollution). From a text t entailing h
?, h
can be further entailed by the single application of
r. We thus simulate the process by which an en-
tailment system would infer h from t using r.
3) Retrieve matching texts: For each h? we
retrieve from a corpus all texts that contain the
lemmatized words of h? (not necessarily as a sin-
gle phrase). These texts may entail h?. We dis-
card texts that also match h since entailing h from
them might not require the application of any rule
from the evaluated resource. In our example, the
retrieved texts contain lake and pollution but do
not contain water.
4) Annotation: A sample of the retrieved texts
is presented to human annotators. The annotators
are asked to answer the following two questions
for each text, simulating the typical inference pro-
cess of an entailment system:
a) Does t entail h?? If t does not entail h?
then the text would not provide a useful example
for the application of r. For instance, t1 (in Fig-
ure 1) does not entail h?1 and thus we cannot de-
duce h from it by applying the rule r. Such texts
are discarded from further evaluation.
b) Does t entail h? If t is annotated as en-
tailing h?, an entailment system would then infer
h from h? by applying r. If h is not entailed from
t even though h? is, the rule application is consid-
ered invalid. For instance, t2 does not entail h even
though it entails h?2. Indeed, the application of r2:
*soil ? water 2, from which h?2 was constructed,
yields incorrect inference. If the answer is ?yes?,
as in the case of t3, the application of r for t is
considered valid.
The above process yields a sample of annotated
rule applications for each test hypothesis, from
which we can measure resources performance, as
described in Section 5.
4 Experimental Setting
4.1 Dataset and Annotation
Current available state-of-the-art lexical-semantic
resources mainly deal with nouns. Therefore, we
used nominal hypotheses for our experiment3.
We chose TREC 1-8 (excluding 4) as our test
corpus and randomly sampled 25 ad-hoc queries
of two-word compounds as our hypotheses. We
did not use longer hypotheses to ensure that
2The asterisk marks an incorrect rule.
3We suggest that the definitions and methodologies can be
applied for other parts of speech as well.
enough texts containing the intermediate hypothe-
ses are found in the corpus. For annotation sim-
plicity, we retrieved single sentences as our texts.
For each rule applied for an hypothesis h, we
sampled 10 sentences from the sentences retrieved
for that rule. As a baseline, we also sampled 10
sentences for each original hypothesis h in which
both words of h are found. In total, 1550 unique
sentences were sampled and annotated by two an-
notators.
To assess the validity of our evaluation method-
ology, the annotators first judged a sample of 220
sentences. The Kappa scores for inter-annotator
agreement were 0.74 and 0.64 for judging h? and
h, respectively. These figures correspond to sub-
stantial agreement (Landis and Koch, 1997) and
are comparable with related semantic annotations
(Szpektor et al, 2007; Bhagat et al, 2007).
4.2 Lexical-Semantic Resources
We evaluated the following resources:
WordNet (WNd): There is no clear agreement
regarding which set of WordNet relations is use-
ful for entailment inference. We therefore took a
conservative approach using only synonymy and
hyponymy rules, which typically comply with the
lexical entailment relation and are commonly used
by textual entailment systems, e.g. (Herrera et al,
2005; Bos and Markert, 2006). Given a term e,
we created a rule e? ? e for each e? amongst the
synonyms or direct hyponyms for all senses of e
in WordNet 3.0.
Snow (Snow30k): Snow et al (2006) pre-
sented a probabilistic model for taxonomy induc-
tion which considers as features paths in parse
trees between related taxonomy nodes. They show
that the best performing taxonomy was the one
adding 30,000 hyponyms to WordNet. We created
an entailment rule for each new hyponym added to
WordNet by their algorithm4.
LCC?s extended WordNet (XWN?): In
(Moldovan and Rus, 2001) WordNet glosses were
transformed into logical form axioms. From this
representation we created a rule e? ? e for each e?
in the gloss which was tagged as referring to the
same entity as e.
CBC: A knowledgebase of labeled clusters gen-
erated by the statistical clustering and labeling al-
gorithms in (Pantel and Lin, 2002; Pantel and
4Available at http://ai.stanford.edu/? rion/swn
561
Ravichandran, 2004)5. Given a cluster label e, an
entailment rule e? ? e is created for each member
e? of the cluster.
Lin Dependency Similarity (Lin-dep): A
distributional word similarity resource based on
syntactic-dependency features (Lin, 1998). Given
a term e and its list of similar terms, we construct
for each e? in the list the rule e? ? e. This resource
was previously used in textual entailment engines,
e.g. (Roth and Sammons, 2007).
Lin Proximity Similarity (Lin-prox): A
knowledgebase of terms with their cooccurrence-
based distributionally similar terms. Rules are cre-
ated from this resource as from the previous one6.
Wikipedia first sentence (WikiFS): Kazama
and Torisawa (2007) used Wikipedia as an exter-
nal knowledge to improve Named Entity Recog-
nition. Using the first step of their algorithm, we
extracted from the first sentence of each page a
noun that appears in a is-a pattern referring to the
title. For each such pair we constructed a rule title
? noun (e.g. Michelle Pfeiffer ? actress).
The above resources represent various meth-
ods for detecting semantic relatedness between
words: Manually and semi-automatically con-
structed (WNd and XWN?, respectively), automat-
ically constructed based on a lexical-syntactic pat-
tern (WikiFS), distributional methods (Lin-dep and
Lin-prox) and combinations of pattern-based and
distributional methods (CBC and Snow30k).
5 Results and Analysis
The results and analysis described in this section
reveal new aspects concerning the utility of re-
sources for lexical entailment, and experimentally
quantify several intuitively-accepted notions re-
garding these resources and the lexical entailment
relation. Overall, our findings highlight where ef-
forts in developing future resources and inference
systems should be invested.
5.1 Resources Performance
Each resource was evaluated using two measures -
Precision and Recall-share, macro averaged over
all hypotheses. The results achieved for each re-
source are summarized in Table 1.
5Kindly provided to us by Patrick Pantel.
6Lin?s resources were downloaded from:
http://www.cs.ualberta.ca/? lindek/demos.htm
Resource Precision (%) Recall-share (%)
Snow30k 56 8
WNd 55 24
XWN? 51 9
WikiFS 45 7
CBC 33 9
Lin-dep 28 45
Lin-prox 24 36
Table 1: Lexical resources performance
5.1.1 Precision
The Precision of a resource R is the percentage of
valid rule applications for the resource. It is esti-
mated by the percentage of texts entailing h from
those that entail h?: countR(entailing h=yes)countR(entailing h?=yes) .
Not surprisingly, resources such as WNd, XWN?
or WikiFS achieved relatively high precision
scores, due to their accurate construction meth-
ods. In contrast, Lin?s distributional resources are
not designed to include lexical entailment relation-
ships. They provide pairs of contextually simi-
lar words, of which many have non-entailing rela-
tionships, such as co-hyponyms7 (e.g. *doctor ?
journalist) or topically-related words, such as *ra-
diotherapy ? outpatient. Hence their relatively
low precision.
One visible outcome is the large gap between
the perceived high accuracy of resources con-
structed by accurate methods, most notably WNd,
and their performance in practice. This finding
emphasizes the need for instance-based evalua-
tions, which capture the ?real? contribution of a
resource. To better understand the reasons for
this gap we further assessed the three factors
that contribute to incorrect applications: incorrect
rules, lexical context and logical context (see Sec-
tion 2.2). This analysis is presented in Table 2.
From Table 2 we see that the gap for accurate
resources is mainly caused by applications of cor-
rect rules in inappropriate contexts. More inter-
estingly, the information in the table allows us to
asses the lexical ?context-sensitivity? of resources.
When considering only the COR-LEX rules to re-
calculate resources precision, we find that Lin-dep
achieves precision of 71% ( 15%15%+6% ), while WN
d
yields only 56% ( 55%55%+44% ). This result indicates
that correct Lin-dep rules are less sensitive to lexi-
cal context, meaning that their prior likelihoods to
7a.k.a. sister terms or coordinate terms
562
(%)
Invalid Rule Applications Valid Rule Applications
INCOR COR-LOG COR-LEX Total INCOR COR-LOG COR-LEX Total (P)
WNd 1 0 44 45 0 0 55 55
WikiFS 13 0 42 55 3 0 42 45
XWN? 19 0 30 49 0 0 51 51
Snow30k 23 0 21 44 0 0 56 56
CBC 51 12 4 67 14 0 19 33
Lin-prox 59 4 13 76 8 3 13 24
Lin-dep 61 5 6 72 9 4 15 28
Table 2: The distribution of invalid and valid rule applications by rule types: incorrect rules (INCOR), correct rules requiring
?logical context? validation (COR-LOG), and correct rules requiring ?lexical context? matching (COR-LEX). The numbers of each
resource?s valid applications add up to the resource?s precision.
be correct are higher. This is explained by the fact
that Lin-dep?s rules are calculated across multiple
contexts and therefore capture the more frequent
usages of words. WordNet, on the other hand, in-
cludes many anecdotal rules whose application is
rare, and thus is very sensitive to context. Simi-
larly, WikiFS turns out to be very context-sensitive.
This resource contains many rules for polysemous
proper nouns that are scarce in their proper noun
sense, e.g. Captive ? computer game. Snow30k,
when applied with the same calculation, reaches
73%, which explains how it achieved a compara-
ble result to WNd, even though it contains many
incorrect rules in comparison to WNd.
5.1.2 Recall
Absolute recall cannot be measured since the total
number of texts in the corpus that entail each hy-
pothesis is unknown. Instead, we measure recall-
share, the contribution of each resource to recall
relative to matching only the words of the origi-
nal hypothesis without any rules. We denote by
yield(h) the number of texts that match h directly
and are annotated as entailing h. This figure is es-
timated by the number of sampled texts annotated
as entailing h multiplied by the sampling propor-
tion. In the same fashion, for each resource R,
we estimate the number of texts entailing h ob-
tained through entailment rules of the resource R,
denoted yieldR(h). Recall-share of R for h is the
proportion of the yield obtained by the resource?s
rules relative to the overall yield with and without
the rules from R: yieldR(h)yield(h)+yieldR(h) .
From Table 1 we see that along with their rela-
tively low precision, Lin?s resources? recall greatly
surpasses that of any other resource, including
WordNet8. The rest of the resources are even infe-
8A preliminary experiment we conducted showed that re-
rior to WNd in that respect, indicating their limited
utility for inference systems.
As expected, synonyms and hyponyms in Word-
Net contributed a noticeable portion to recall in all
resources. Additional correct rules correspond to
hyponyms and synonyms missing from WordNet,
many of them proper names and some slang ex-
pressions. These rules were mainly provided by
WikiFS and Snow30k, significantly supplementing
WordNet, whose HasInstance relation is quite par-
tial. However, there are other interesting types of
entailment relations contributing to recall. These
are discussed in Sections 5.2 and 5.3. Examples
for various rule types are found in Table 3.
5.1.3 Valid Applications of Incorrect Rules
We observed that many entailing sentences were
retrieved by inherently incorrect rules in the distri-
butional resources. Analysis of these rules reveals
they were matched in entailing texts when the LHS
has noticeable statistical correlation with another
term in the text that does entail the RHS. For ex-
ample, for the hypothesis wildlife extinction, the
rule *species ? extinction yielded valid applica-
tions in contexts about threatened or endangered
species. Has the resource included a rule between
the entailing term in the text and the RHS, the
entailing text would have been matched without
needing the incorrect rule.
These correlations accounted for nearly a third
of Lin resources? recall. Nonetheless, in princi-
ple, we suggest that such rules, which do not con-
form with Definition 2, should not be included in a
lexical entailment resource, since they also cause
invalid rule applications, while the entailing texts
they retrieve will hopefully be matched by addi-
call does not dramatically improve when using the entire hy-
ponymy subtree from WordNet.
563
Type Correct Rules
HYPO Shevardnadze ? official Snow30k
ANT efficacy ? ineffectiveness Lin-dep
HOLO government ? official Lin-prox
HYPER arms ? gun Lin-prox
? childbirth ? motherhood Lin-dep
? mortgage ? bank Lin-prox
? Captive ? computer WikiFS
? negligence ? failure CBC
? beatification ? pope XWN?
Type Incorrect Rules
CO-HYP alcohol ? cigarette CBC
? radiotherapy ? outpatient Lin-dep
? teen-ager ? gun Snow30k
? basic ? paper WikiFS
? species ? extinction Lin-prox
Table 3: Examples of lexical resources rules by types.
HYPO: hyponymy, HYPER: hypernymy (class entailment of
its members), HOLO: holonymy, ANT: antonymy, CO-HYP: co-
hyponymy. The non-categorized relations do not correspond
to any WordNet relation.
tional correct rules in a more comprehensive re-
source.
5.2 Non-standard Entailment Relations
An important finding of our analysis is that some
less standard entailment relationships have a con-
siderable impact on recall (see Table 3). These
rules, which comply with Definition 2 but do
not conform to any WordNet relation type, were
mainly contributed by Lin?s distributional re-
sources and to a smaller degree are also included
in XWN?. In Lin-dep, for example, they accounted
for approximately a third of the recall.
Among the finer grained relations we identi-
fied in this set are topical entailment (e.g. IBM
as the company entailing the topic computers),
consequential relationships (pregnancy?mother-
hood) and an entailment of inherent arguments by
a predicate, or of essential participants by a sce-
nario description, e.g. beatification ? pope. A
comprehensive typology of these relationships re-
quires further investigation, as well as the identi-
fication and development of additional resources
from which they can be extracted.
As opposed to hyponymy and synonymy rules,
these rules are typically non-substitutable, i.e. the
RHS of the rule is unlikely to have the exact same
role in the text as the LHS. Many inference sys-
tems perform rule-based transformations, substi-
tuting the LHS by the RHS. This finding suggests
that different methods may be required to utilize
such rules for inference.
5.3 Logical Context
WordNet relations other than synonyms and hy-
ponyms, including antonyms, holonyms and hy-
pernyms (see Table 3), contributed a noticeable
share of valid rule applications for some resources.
Following common practice, these relations are
missing by construction from the other resources.
As shown in Table 2 (COR-LOG columns), such
relations accounted for a seventh of Lin-dep?s
valid rule applications, as much as was the con-
tribution of hyponyms and synonyms to this re-
source?s recall. Yet, using these rules resulted with
more erroneous applications than correct ones. As
discussed in Section 2.2, the rules induced by
these relations do conform with our lexical entail-
ment definition. However, a valid application of
these rules requires certain logical conditions to
occur, which is not the common case. We thus
suggest that such rules are included in lexical en-
tailment resources, as long as they are marked
properly by their types, allowing inference sys-
tems to utilize them only when appropriate mech-
anisms for handling logical context are in place.
5.4 Rules Priors
In Section 5.1.1 we observed that some resources
are highly sensitive to context. Hence, when con-
sidering the validity of a rule?s application, two
factors should be regarded: the actual context in
which the rule is to be applied, as well as the rule?s
prior likelihood to be valid in an arbitrary con-
text. Somewhat indicative, yet mostly indirect, in-
formation about rules? priors is contained in some
resources. This includes sense ranks in WordNet,
SemCor statistics (Miller et al, 1993), and similar-
ity scores and rankings in Lin?s resources. Infer-
ence systems often incorporated this information,
typically as top-k or threshold-based filters (Pan-
tel and Lin, 2003; Roth and Sammons, 2007). By
empirically assessing the effect of several such fil-
ters in our setting, we found that this type of data
is indeed informative in the sense that precision
increases as the threshold rises. Yet, no specific
filters were found to improve results in terms of
F1 score (where recall is measured relatively to
the yield of the unfiltered resource) due to a sig-
nificant drop in relative recall. For example, Lin-
564
prox loses more than 40% of its recall when only
the top-50 rules for each hypothesis are exploited,
and using only the first sense of WNd costs the re-
source over 60% of its recall. We thus suggest a
better strategy might be to combine the prior in-
formation with context matching scores in order
to obtain overall likelihood scores for rule appli-
cations, as in (Szpektor et al, 2008). Furthermore,
resources should include explicit information re-
garding the prior likelihoods of of their rules.
5.5 Operative Conclusions
Our findings highlight the currently limited re-
call of available resources for lexical inference.
The higher recall of Lin?s resources indicates
that many more entailment relationships can be
acquired, particularly when considering distribu-
tional evidence. Yet, available distributional ac-
quisition methods are not geared for lexical entail-
ment. This suggests the need to develop acqui-
sition methods for dedicated and more extensive
knowledge resources that would subsume the cor-
rect rules found by current distributional methods.
Furthermore, substantially better recall may be ob-
tained by acquiring non-standard lexical entail-
ment relationships, as discussed in Section 5.2, for
which a comprehensive typology is still needed.
At the same time, transformation-based inference
systems would need to handle these kinds of rules,
which are usually non-substitutable. Our results
also quantify and stress earlier findings regarding
the severe degradation in precision when rules are
applied in inappropriate contexts. This highlights
the need for resources to provide explicit informa-
tion about the suitable lexical and logical contexts
in which an entailment rule is applicable. In par-
allel, methods should be developed to utilize such
contextual information within inference systems.
Additional auxiliary information needed in lexical
resources is the prior likelihood for a given rule to
be correct in an arbitrary context.
6 Related Work
Several prior works defined lexical entailment.
WordNet?s lexical entailment is a relationship be-
tween verbs only, defined for propositions (Fell-
baum, 1998). Geffet and Dagan (2004) defined
substitutable lexical entailment as a relation be-
tween substitutable terms. We find this definition
too restrictive as non-substitutable rules may also
be useful for entailment inference. Examples are
breastfeeding ? baby and hospital ? medical.
Hence, Definition 2 is more broadly applicable for
defining the desired contents of lexical entailment
resources. We empirically observed that the rules
satisfying their definition are a proper subset of
the rules covered by our definition. Dagan and
Glickman (2004) referred to entailment at the sub-
sentential level by assigning truth values to sub-
propositional text fragments through their existen-
tial meaning. We find this criterion too permissive.
For instance, the existence of country implies the
existence of its flag. Yet, the meaning of flag is
typically not implied by country.
Previous works assessing rule application via
human annotation include (Pantel et al, 2007;
Szpektor et al, 2007), which evaluate acquisition
methods for lexical-syntactic rules. They posed an
additional question to the annotators asking them
to filter out invalid contexts. In our methodology
implicit context matching for the full hypothesis
was applied instead. Other related instance-based
evaluations (Giuliano and Gliozzo, 2007; Connor
and Roth, 2007) performed lexical substitutions,
but did not handle the non-substitutable cases.
7 Conclusions
This paper provides several methodological and
empirical contributions. We presented a novel
evaluation methodology for the utility of lexical-
semantic resources for semantic inference. To that
end we proposed definitions for entailment at sub-
sentential levels, addressing a gap in the textual
entailment framework. Our evaluation and analy-
sis provide a first quantitative comparative assess-
ment of the isolated utility of a range of prominent
potential resources for entailment rules. We have
shown various factors affecting rule applicability
and resources performance, while providing oper-
ative suggestions to address them in future infer-
ence systems and resources.
Acknowledgments
The authors would like to thank Naomi Frankel
and Iddo Greental for their excellent annotation
work, as well as Roy Bar-Haim and Idan Szpektor
for helpful discussion and advice. This work was
partially supported by the Negev Consortium of
the Israeli Ministry of Industry, Trade and Labor,
the PASCAL-2 Network of Excellence of the Eu-
ropean Community FP7-ICT-2007-1-216886 and
the Israel Science Foundation grant 1095/05.
565
References
Rahul Bhagat, Patrick Pantel, and Eduard Hovy. 2007.
LEDIR: An unsupervised algorithm for learning di-
rectionality of inference rules. In Proceedings of
EMNLP-CoNLL.
J. Bos and K. Markert. 2006. When logical infer-
ence helps determining textual entailment (and when
it doesn?t). In Proceedings of the Second PASCAL
RTE Challenge.
Michael Connor and Dan Roth. 2007. Context sensi-
tive paraphrasing with a global unsupervised classi-
fier. In Proceedings of ECML.
Ido Dagan and Oren Glickman. 2004. Probabilistic
textual entailment: Generic applied modeling of lan-
guage variability. In PASCAL Workshop on Learn-
ing Methods for Text Understanding and Mining.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005. The pascal recognising textual entailment
challenge. In Joaquin Quinonero Candela, Ido Da-
gan, Bernardo Magnini, and Florence d?Alche? Buc,
editors, MLCW, Lecture Notes in Computer Science.
Marie-Catherine de Marneffe, Bill MacCartney, Trond
Grenager, Daniel Cer, Anna Rafferty, and Christo-
pher D. Manning. 2006. Learning to distinguish
valid textual entailments. In Proceedings of the Sec-
ond PASCAL RTE Challenge.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database (Language, Speech, and
Communication). The MIT Press.
Maayan Geffet and Ido Dagan. 2004. Feature vector
quality and distributional similarity. In Proceedings
of COLING.
Danilo Giampiccolo, Bernardo Magnini, Ido Dagan,
and Bill Dolan. 2007. The third pascal recogniz-
ing textual entailment challenge. In Proceedings of
ACL-WTEP Workshop.
Claudio Giuliano and Alfio Gliozzo. 2007. Instance
based lexical entailment for ontology population. In
Proceedings of EMNLP-CoNLL.
Oren Glickman, Eyal Shnarch, and Ido Dagan. 2006.
Lexical reference: a semantic matching subtask. In
Proceedings of EMNLP.
Jesu?s Herrera, Anselmo Pen?as, and Felisa Verdejo.
2005. Textual entailment recognition based on de-
pendency analysis and wordnet. In Proceedings of
the First PASCAL RTE Challenge.
Jun?ichi Kazama and Kentaro Torisawa. 2007. Ex-
ploiting Wikipedia as external knowledge for named
entity recognition. In Proceedings of EMNLP-
CoNLL.
Milen Kouylekov and Bernardo Magnini. 2006. Build-
ing a large-scale repository of textual entailment
rules. In Proceedings of LREC.
J. R. Landis and G. G. Koch. 1997. The measurements
of observer agreement for categorical data. In Bio-
metrics, pages 33:159?174.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of COLING-ACL.
George A. Miller, Claudia Leacock, Randee Tengi, and
Ross T. Bunker. 1993. A semantic concordance. In
Proceedings of HLT.
Dan Moldovan and Vasile Rus. 2001. Logic form
transformation of wordnet and its applicability to
question answering. In Proceedings of ACL.
Patrick Pantel and Dekang Lin. 2002. Discovering
word senses from text. In Proceedings of ACM
SIGKDD.
Patrick Pantel and Dekang Lin. 2003. Automatically
discovering word senses. In Proceedings of NAACL.
Patrick Pantel and Deepak Ravichandran. 2004. Auto-
matically labeling semantic classes. In Proceedings
of HLT-NAACL.
Patrick Pantel, Rahul Bhagat, Bonaventura Coppola,
Timothy Chklovski, and Eduard Hovy. 2007. ISP:
Learning inferential selectional preferences. In Pro-
ceedings of HLT.
Marius Pasca and Sanda M. Harabagiu. 2001. The in-
formative role of wordnet in open-domain question
answering. In Proceedings of NAACL Workshop on
WordNet and Other Lexical Resources.
Dan Roth and Mark Sammons. 2007. Semantic and
logical inference model for textual entailment. In
Proceedings of ACL-WTEP Workshop.
Chirag Shah and Bruce W. Croft. 2004. Evaluating
high accuracy retrieval techniques. In Proceedings
of SIGIR.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2006.
Semantic taxonomy induction from heterogenous
evidence. In Proceedings of COLING-ACL.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: A core of semantic knowl-
edge - unifying wordnet and wikipedia. In Proceed-
ings of WWW.
Idan Szpektor, Eyal Shnarch, and Ido Dagan. 2007.
Instance-based evaluation of entailment rule acqui-
sition. In Proceedings of ACL.
Idan Szpektor, Ido Dagan, Roy Bar-Haim, and Jacob
Goldberger. 2008. Contextual preferences. In Pro-
ceedings of ACL.
Ellen M. Voorhees. 1994. Query expansion using
lexical-semantic relations. In Proceedings of SIGIR.
566
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 129?136, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Investigating Unsupervised Learning
for Text Categorization Bootstrapping
Alfio Gliozzo and Carlo Strapparava
ITC-irst
Istituto per la Ricerca Scientifica e Tecnologica
I-38050 Trento, Italy
{gliozzo,strappa}@itc.it
Ido Dagan
Computer Science Department
Bar Ilan University
Ramat Gan, Israel
dagan@cs.biu.ac.il
Abstract
We propose a generalized bootstrapping
algorithm in which categories are de-
scribed by relevant seed features. Our
method introduces two unsupervised steps
that improve the initial categorization step
of the bootstrapping scheme: (i) using La-
tent Semantic space to obtain a general-
ized similarity measure between instances
and features, and (ii) the Gaussian Mixture
algorithm, to obtain uniform classification
probabilities for unlabeled examples. The
algorithm was evaluated on two Text Cate-
gorization tasks and obtained state-of-the-
art performance using only the category
names as initial seeds.
1 Introduction
Supervised classification is the task of assigning cat-
egory labels, taken from a predefined set of cate-
gories (classes), to instances in a data set. Within the
classical supervised learning paradigm, the task is
approached by providing a learning algorithm with
a training data set of manually labeled examples. In
practice it is not always easy to apply this schema
to NLP tasks. For example supervised systems for
Text Categorization (TC) require a large amount of
hand labeled texts, while in many applicative cases
it is quite difficult to collect the required amounts of
hand labeled data. Unlabeled text collections, on the
other hand, are in general easily available.
An alternative approach is to provide the neces-
sary supervision by means of sets of ?seeds? of in-
tuitively relevant features. Adopting terminology
from computability theory, we refer to the stan-
dard example-based supervision mode as Exten-
sional Learning (EL), as classes are being specified
by means of examples of their elements (their ex-
tension). Feature-based supervision is referred to as
Intensional Learning (IL), as features may often be
perceived as describing the intension of a category,
such as providing the name or prominent key terms
for a category in text categorization.
The IL approach reflects on classical rule-based
classification methods, where the user is expected
to specify exact classification rules that operate in
the feature space. Within the machine learning
paradigm, IL has been incorporated as a technique
for bootstrapping an extensional learning algorithm,
as in (Yarowsky, 1995; Collins and Singer, 1999;
Liu et al, 2004). This way the user does not
need to specify exact classification rules (and fea-
ture weights), but rather perform a somewhat sim-
pler task of specifying few typical seed features for
the category. Given the list of seed features, the
bootstrapping scheme consists of (i) preliminary un-
supervised categorization of the unlabeled data set
based on the seed features, and (ii) training an (ex-
tensional) supervised classifier using the automatic
classification labels of step (i) as the training data
(the second step is possibly reiterated, such as by
an Expectation-Maximization schema). The core
part of IL bootstrapping is step (i), i.e. the initial
unsupervised classification of the unlabeled dataset.
This step was often approached by relatively sim-
ple methods, which are doomed to obtain mediocre
quality. Even so, it is hoped that the second step of
supervised training would be robust enough to the
noise in the initial training set.
129
The goal of this paper is to investigate additional
principled unsupervised mechanisms within the ini-
tial classification step, applied to the text catego-
rization. In particular, (a) utilizing a Latent Se-
mantic Space to obtain better similarity assessments
between seeds and examples, and (b) applying a
Gaussian Mixture (GM) algorithm, which provides a
principled unsupervised estimation of classification
probability. As shown in our experiments, incor-
porating these steps consistently improved the ac-
curacy of the initial categorization step, which in
turn yielded a better final classifier thanks to the
more accurate training set. Most importantly, we ob-
tained comparable or better performance than previ-
ous IL methods using only the category names as the
seed features; other IL methods required collecting
a larger number of seed terms, which turns out to be
a somewhat tricky task.
Interesting results were revealed when compar-
ing our IL method to a state-of-the-art extensional
classifier, trained on manually labeled documents.
The EL classifier required 70 (Reuters dataset) or
160 (Newsgroup dataset) documents per category to
achieve the same performance that IL obtained using
only the category names. These results suggest that
IL may provide an appealing cost-effective alterna-
tive when sub-optimal accuracy suffices, or when it
is too costly or impractical to obtain sufficient la-
beled training. Optimal combination of extensional
and intensional supervision is raised as a challeng-
ing topic for future research.
2 Bootstrapping for Text Categorization
The TC task is to assign category labels to docu-
ments. In the IL setting, a category Ci is described
by providing a set of relevant features, termed an
intensional description (ID), idci ? V , where V
is the vocabulary. In addition a training corpus
T = {t1, t2, . . . tn} of unlabeled texts is provided.
Evaluation is performed on a separate test corpus
of labeled documents, to which standard evaluation
metrics can be applied.
The approach of categorizing texts based on lists
of keywords has been attempted rather rarely in the
literature (McCallum and Nigam, 1999; Ko and Seo,
2000; Liu et al, 2004; Ko and Seo, 2004). Several
names have been proposed for it ? such as TC by
bootstrapping with keywords, unsupervised TC, TC
by labelling words ? where the proposed methods
fall (mostly) within the IL settings described here1.
It is possible to recognize a common structure of
these works, based on a typical bootstrap schema
(Yarowsky, 1995; Collins and Singer, 1999):
Step 1: Initial unsupervised categorization. This
step was approached by applying some similar-
ity criterion between the initial category seed
and each unlabeled document. Similarity may
be determined as a binary criterion, consider-
ing each seed keyword as a classification rule
(McCallum and Nigam, 1999), or by applying
an IR style vector similarity measure. The re-
sult of this step is an initial categorization of (a
subset of) the unlabeled documents. In (Ko and
Seo, 2004) term similarity techniques were ex-
ploited to expand the set of seed keywords, in
order to improve the quality of the initial cate-
gorization.
Step 2: Train a supervised classifier on the ini-
tially categorized set. The output of Step
1 is exploited to train an (extensional) su-
pervised classifier. Different learning algo-
rithms have been tested, including SVM, Naive
Bayes, Nearest Neighbors, and Rocchio. Some
works (McCallum and Nigam, 1999; Liu et
al., 2004) performed an additional Expectation
Maximization algorithm over the training data,
but reported rather small incremental improve-
ments that do not seem to justify the additional
effort.
(McCallum and Nigam, 1999) reported catego-
rization results close to human agreement on the
same task. (Liu et al, 2004) and (Ko and Seo,
2004) contrasted their word-based TC algorithm
with the performance of an extensional supervised
algorithm, achieving comparable results, while in
general somewhat lower. It should be noted that it
has been more difficult to define a common evalua-
tion framework for comparing IL algorithms for TC,
due to the subjective selection of seed IDs and to the
lack of common IL test sets (see Section 4).
1The major exception is the work in (Ko and Seo, 2004),
which largely follows the IL scheme but then makes use of la-
beled data to perform a chi-square based feature selection be-
fore starting the bootstrap process. This clearly falls outside the
IL setting, making their results incomparable to other IL meth-
ods.
130
3 Incorporating Unsupervised Learning
into Bootstrap Schema
In this section we show how the core Step 1 of the IL
scheme ? the initial categorization ? can be boosted
by two unsupervised techniques. These techniques
fit the IL setting and address major constraints of it.
The first is exploiting a generalized similarity metric
between category seeds (IDs) and instances, which
is defined in a Latent Semantic space. Applying
such unsupervised similarity enables to enhance the
amount of information that is exploited from each
seed feature, aiming to reduce the number of needed
seeds. The second technique applies the unsuper-
vised Gaussian Mixture algorithm, which maps sim-
ilarity scores to a principled classification probabil-
ity value. This step enables to obtain a uniform scale
of classification scores across all categories, which
is typically obtained only through calibration over
labeled examples in extensional learning.
3.1 Similarity in Latent Semantic Space
As explained above, Step 1 of the IL scheme as-
sesses a degree of ?match? between the seed terms
and a classified document. It is possible first to
follow the intuitively appealing and principled ap-
proach of (Liu et al, 2004), in which IDs (category
seeds) and instances are represented by vectors in a
usual IR-style Vector Space Model (VSM), and sim-
ilarity is measured by the cosine function:
simvsm(idci , tj) = cos (~idci , ~tj) (1)
where ~idci ? R|V | and ~tj ? R|V | are the vectorial
representations in the space R|V | respectively of the
category ID idci and the instance tj , and V is the set
of all the features (the vocabulary).
However, representing seeds and instances in a
standard feature space is severely affected in the IL
setting by feature sparseness. In general IDs are
composed by short lists of features, possibly just
a single feature. Due to data sparseness, most in-
stances do not contain any feature in common with
any category?s ID, which makes the seeds irrelevant
for most instances (documents in the text categoriza-
tion case). Furthermore, applying direct matching
only for a few seed terms is often too crude, as it ig-
nores the identity of the other terms in the document.
The above problems may be reduced by consid-
ering some form of similarity in the feature space,
as it enables to compare additional document terms
with the original seeds. As mentioned in Section
2, (Ko and Seo, 2004) expanded explicitly the orig-
inal category IDs with more terms, using a con-
crete query expansion scheme. We preferred using a
generalized similarity measure based on represent-
ing features and instances a Latent Semantic (LSI)
space (Deerwester et al, 1990). The dimensions of
the Latent Semantic space are the most explicative
principal components of the feature-by-instance ma-
trix that describes the unlabeled data set. In LSI
both coherent features (i.e. features that often co-
occur in the same instances) and coherent instances
(i.e. instances that share coherent features) are rep-
resented by similar vectors in the reduced dimen-
sionality space. As a result, a document would be
considered similar to a category ID if the seed terms
and the document terms tend to co-occur overall in
the given corpus.
The Latent Semantic Vectors for IDs and docu-
ments were calculated by an empirically effective
variation (self-reference omitted for anonymity) of
the pseudo-document methodology to fold-in docu-
ments, originally suggested in (Berry, 1992). The
similarity function simlsi is computed by the cosine
metric, following formula 1, where ~idci and ~tj are
replaced by their Latent Semantic vectors. As will
be shown in section 4.2, using such non sparse rep-
resentation allows to drastically reduce the number
of seeds while improving significantly the recall of
the initial categorization step.
3.2 The Gaussian Mixture Algorithm and the
initial classification step
Once having a similarity function between category
IDs and instances, a simple strategy is to base the
classification decision (of Step 1) directly on the
obtained similarity values (as in (Liu et al, 2004),
for example). Typically, IL works adopt in Step 1
a single-label classification approach, and classify
each instance (document) to only one category. The
chosen category is the one whose ID is most simi-
lar to the classified instance amongst all categories,
which does not require any threshold tuning over la-
beled examples. The subsequent training in Step 2
yields a standard EL classifier, which can then be
used to assign multiple categories to a document.
Using directly the output of the similarity func-
tion for classification is problematic, because the ob-
tained scales of similarity values vary substantially
across different categories. The variability in sim-
131
ilarity value ranges is caused by variations in the
number of seed terms per category and the levels of
their generality and ambiguity. As a consequence,
choosing the class with the highest absolute similar-
ity value to the instance often leads to selecting a
category whose similarity values tend to be gener-
ally higher, while another category could have been
more similar to the classified instance if normalized
similarity values were used.
As a solution we propose using an algorithm
based on unsupervised estimation of Gaussian Mix-
tures (GM), which differentiates relevant and non-
relevant category information using statistics from
unlabeled instances. We recall that mixture mod-
els have been widely used in pattern recognition and
statistics to approximate probability distributions. In
particular, a well-known nonparametric method for
density estimation is the so-called Kernel Method
(Silverman, 1986), which approximates an unknow
density with a mixture of kernel functions, such as
gaussians functions. Under mild regularity condi-
tions of the unknown density function, it can be
shown that mixtures of gaussians converge, in a sta-
tistical sense, to any distribution.
More formally, let ti ? T be an instance described
by a vector of features ~ti ? R|V | and let idci ? V
be the ID of category Ci; let sim(idci , tj) ? R be
a similarity function among instances and IDs, with
the only expectation that it monotonically increases
according to the ?closeness? of idci and tj (see Sec-
tion 3.1).
For each category Ci, GM induces a mapping
from the similarity scores between its ID and any
instance tj , sim(idci , tj), into the probability of Ci
given the text tj , P (Ci|tj). To achieve this goal GM
performs the following operations: (i) it computes
the set Si = {sim(idci , tj)|tj ? T} of the sim-
ilarity scores between the ID idci of the category
Ci and all the instances tj in the unlabeled train-
ing set T ; (ii) it induces from the empirical distri-
bution of values in Si a Gaussian Mixture distribu-
tion which is composed of two ?hypothetic? distri-
butions Ci and Ci, which are assumed to describe re-
spectively the distributions of similarity scores for
positive and negative examples; and (iii) it estimates
the conditional probability P (Ci|sim(idci , tj)) by
applying the Bayes theorem on the distributions Ci
and Ci. These steps are explained in more detail be-
low.
The core idea of the algorithm is in step (ii). Since
we do not have labeled training examples we can
only obtain the set Si which includes the similar-
ity scores for all examples together, both positive
and negative. We assume, however, that similar-
ity scores that correspond to positive examples are
drawn from one distribution, P (sim(idci , tj)|Ci),
while the similarity scores that correspond to neg-
ative examples are drawn from another distribution,
P (sim(idci , tj)|Ci). The observed distribution of
similarity values in Si is thus assumed to be a mix-
ture of the above two distributions, which are recov-
ered by the GM estimation.
Figure 1 illustrates the mapping induced by GM
from the empirical mixture distribution: dotted lines
describe the Probability Density Functions (PDFs)
estimated by GM for Ci, Ci, and their mixture from
the empirical distribution (Si) (in step (ii)). The
continuous line is the mapping induced in step (iii)
of the algorithm from similarity scores between in-
stances and IDs (x axis) to the probability of the in-
stance to belong to the category (y axis).
0
1
2
3
4
5
6
7
8
9
-0.3 -0.2 -0.1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
GM-score
Ci gaussian (relevant)
Ci gaussian (not relevant)Mixture
Similarity Score
Pro
bab
ility
 / P
DF
Figure 1: Mapping induced by GM for the category
rec.motorcycles in the 20newsgroups data set.
The probabilistic mapping estimated in step (iii)
for a category Ci given an instance tj is computed
by applying Bayes rule:
P (Ci|tj) = P (Ci|sim(idci , tj)) = (2)
=
P (sim(idci ,tj)|Ci)P (Ci)
P (sim(idci ,tj)|Ci)P (Ci)+P (sim(Ci,tj)|Ci)P (Ci)
where P (sim(idci , tj)|Ci) is the value of
the PDF of Ci at the point sim(idci , tj),
P (sim(idci , tj)|Ci) is the value of the PDF of Ci at
the same point, P (Ci) is the area of the distribution
132
Ci and P (Ci) is the area of the distribution Ci. The
mean and variance parameters of the two distribu-
tions Ci and Ci, used to evaluate equation 2, are esti-
mated by the rather simple application of the Expec-
tation Maximization (EM) algorithm for Gaussian
Mixtures, as summarized in (Gliozzo et al, 2004).
Finally, following the single-labeled categoriza-
tion setting of Step 1 in the IL scheme, the most
likely category is assigned to each instance, that is,
argmaxCiP (Ci|tj).
3.3 Summary of the Bootstrapping Algorithm
step 1.a: Latent Semantic Space. Instances and
Intensional Descriptions of categories (the seeds) are
represented by vectors in Latent Semantic space. As
an option, the algorithm can work with the classi-
cal Vector Space Model using the original feature
space. Similarity scores between IDs and instances
are computed by the Cosine measure.
step 1.b: GM. The mapping functions P (Ci|tj)
for each category, conditioned on instances tj , are
induced by the GM algorithm. To that end, an Ex-
pectation Maximization algorithm estimates the pa-
rameters of the two component distributions of the
observed mixture, which correspond to the distribu-
tions of similarity values for positive and negative
examples. As an option, the GM mapping can be
avoided.
step 1.c: Categorization. Each instance
is classified to the most probable category -
argmaxCiP (Ci|tj).
step 2: Bootstrapping an extensional classifier.
An EL classifier (SVM) is trained on the set of la-
beled instances resulting from step 1.c.
4 Evaluation
4.1 Intensional Text Categorization Datasets
Even though some typical data sets have been used
in the TC literature (Sebastiani, 2002), the datasets
used for IL learning were not standard. Often there
is not sufficient clarity regarding details such as the
exact version of the corpus used and the training/test
splitting. Furthermore, the choice of categories was
often not standard: (Ko and Seo, 2004) omitted 4
categories from the 20-Newsgroup dataset, while
(Liu et al, 2004) evaluated their method on 4 sepa-
rate subsets of the 20-Newsgroups, each containing
only 4-5 categories. Such issues make it rather diffi-
cult to compare thoroughly different techniques, yet
we have conducted several comparisons in Subsec-
tion 4.5 below. In the remainder of this Subsection
we clearly state the corpora used in our experiments
and the pre-processing steps performed on them.
20newsgroups. The 20 Newsgroups data set is
a collection of newsgroup documents, partitioned
(nearly) evenly across 20 different newsgroups. As
suggested in the dataset Web site2, we used the
?bydate? version: the corpus (18941 documents)
is sorted by date and divided in advance into a
training (60%) set and a chronologically follow-
ing test set (40%) (so there is no randomness in
train/test set selection), it does not include cross-
posts (duplicates), and (more importantly) does not
include non-textual newsgroup-identifying headers
which often help classification (Xref, Newsgroups,
Path, Followup-To, Date).
We will first report results using initial seeds
for the category ID?s, which were selected using
only the words in the category names, with some
trivial transformations (i.e. cryptography#n
for the category sci.crypt, x-windows#n
for the category comp.windows.x). We
also tried to avoid ?overlapping? seeds, i.e.
for the categories rec.sport.baseball
and rec.sport.hockey the seeds are only
{baseball#n} and {hockey#n} respec-
tively and not {sport#n, baseball#n} and
{sport#n, hockey#n}3.
Reuters-10. We used the top 10 categories
(Reuters-10) in the Reuters-21578 collection
Apte` split4. The complete Reuters collection
includes 12,902 documents for 90 categories,
with a fixed splitting between training and test
data (70/30%). Both the Apte` and Apte`-10
splits are often used in TC tasks, as surveyed
in (Sebastiani, 2002). To obtain the Reuters-10
2The collection is available at
www.ai.mit.edu/people/jrennie/20Newsgroups.
3One could propose as a guideline for seed selection
those seeds that maximize their distances in the LSI vec-
tor space model. On this perspective the LSI vectors
built from {sport#n, baseball#n} and {sport#n,
hockey#n} are closer than the vectors that represent
{baseball#n} and {hockey#n}. It may be noticed that
this is a reason for the slight initial performance decrease in the
learning curve in Figure 2 below.
4available at http://kdd.ics.uci.edu/databases/-
reuters21578/reuters21578.html).
133
Apte` split we selected the 10 most frequent cate-
gories: Earn, Acquisition, Money-fx,
Grain, Crude, Trade, Interest,
Ship, Wheat and Corn. The final data set
includes 9296 documents. The initial seeds are only
the words appearing in the category names.
Pre-processing. In both data sets we tagged the
texts for part-of-speech and represented the docu-
ments by the frequency of each pos-tagged lemma,
considering only nouns, verbs, adjectives, and ad-
verbs. We induced the Latent Semantic Space from
the training part5 and consider the first 400 dimen-
sions.
4.2 The impact of LSI similarity and GM on IL
performance
In this section we evaluate the incremental impact
of LSI similarity and the GM algorithm on IL per-
formance. When avoiding both techniques the algo-
rithm uses the simple cosine-based method over the
original feature space, which can be considered as a
baseline (similar to the method of (Liu et al, 2004)).
We report first results using only the names of the
categories as initial seeds.
Table 1 displays the F1 measure for the 20news-
groups and Reuters data sets, with and without LSI
and with and without GM. The performance figures
show the incremental benefit of both LSI and GM. In
particular, when starting with just initial seeds and
do not exploit the LSI similarity mechanism, then
the performance is heavily penalized.
As mentioned above, the bootstrapping step of the
algorithm (Step 2) exploits the initially classified in-
stances to train a supervised text categorization clas-
sifier based on Support Vector Machines. It is worth-
while noting that the increment of performance after
bootstrapping is generally higher when GM and LSI
are incorporated, thanks to the higher quality of the
initial categorization which was used for training.
4.3 Learning curves for the number of seeds
This experiment evaluates accuracy change as a
function of the number of initial seeds. The ex-
5From a machine learning point of view, we could run the
LSA on the full corpus (i.e. training and test), the LSA being a
completely unsupervised technique (i.e. it does not take into ac-
count the data annotation). However, from an applicative point
of view it is much more sensible to have the LSA built on the
training part only. If we run the LSA on the full corpus, the
performance figures increase in about 4 points.
Reuters 20 Newsgroups
LSI GM F1 F1
no no 0.38 0.25
+ bootstrap 0.42 0.28
no yes 0.41 0.30
+ bootstrap 0.46 0.34
yes no 0.46 0.50
+ bootstrap 0.47 0.53
yes yes 0.58 0.60
+ bootstrap 0.74 0.65
Table 1: Impact of LSI vector space and GM
0.2
0.25
0.3
0.35
0.4
0.45
0.5
0.55
0.6
0.65
1 5 10 15 20
F1
number of seeds (1 means only the category names)
LSI VSMClassical VSM
Figure 2: Learning curves on initial seeds for 20
newsgroups, LSI and Classical VSM (no LSI)
periment was performed for the 20 newsgroups cor-
pus using both the LSI and the Classical vector
space model. Additional seeds, beyond the cate-
gory names, were identified by two lexicographers.
For each category, the lexicographers were provided
with a list of 100 seeds produced by the LSI similar-
ity function applied to the category name (one list of
100 candidate terms for each category). From these
lists the lexicographers selected the words that were
judged as significantly related to the respective cat-
egory, picking a mean of 40 seeds per category.
As seen in Figure 2, the learning curve using
LSI vector space model dramatically outperforms
the one using classical vector space. As can be
expected, when using the original vector space (no
generalization) the curve improves quickly with a
few more terms. More surprisingly, with LSI sim-
ilarity the best performance is obtained using the
minimal initial seeds of the category names, while
adding more seeds degrades performance. This
might suggest that category names tend to be highly
134
indicative for the intensional meaning of the cate-
gory, and therefore adding more terms introduces
additional noise. Further research is needed to find
out whether other methods for selecting additional
seed terms might yield incremental improvements.
The current results, though, emphasize the bene-
fit of utilizing LSI and GM. These techniques ob-
tain state-of-the-art performance (see comparisons
in Section 4.5) using only the category names as
seeds, allowing us to skip the quite tricky phase of
collecting manually a larger number of seeds.
4.4 Extensional vs. Intensional Learning
A major point of comparison between IL and EL is
the amount of supervision effort required to obtain a
certain level of performance. To this end we trained
a supervised classifier based on Support Vector Ma-
chines, and draw its learning curves as a function
of percentage of the training set size (Figure 3). In
the case of 20newsgroups, to achieve the 65% F1
performance of IL the supervised settings requires
about 3200 documents (about 160 texts per cate-
gory), while our IL method requires only the cate-
gory name. Reuters-10 is an easier corpus, there-
fore EL achieves rather rapidly a high performance.
But even here using just the category name is equal
on average to labeling 70 documents per-category
(700 in total). These results suggest that IL may pro-
vide an appealing cost-effective alternative in prac-
tical settings when sub-optimal accuracy suffices, or
when it is too costly or impractical to obtain suffi-
cient amounts of labeled training sets.
It should also be stressed that when using the
complete labeled training corpus state-of-the-art EL
outperforms our best IL performance. This result
deviates from the flavor of previous IL literature,
which reported almost comparable performance rel-
ative to EL. As mentioned earlier, the method of (Ko
and Seo, 2004) (as we understand it) utilizes labeled
examples for feature selection, and therefore cannot
be compared with our strict IL setting. As for the
results in (Liu et al, 2004), we conjecture that their
comparable performance for IL and EL may not be
sufficiently general, for several reasons: the easier
classification task (4 subsets of 20-Newsgroups of
4-5 categories each); the use of the usually weaker
Naive-Bayes as the EL device; the use of cluster-
ing as an aid for selecting the seed terms from the
20-Newsgroup subsets, which might not scale up
well when applied to a large number of categories
of varying size.
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
F1
Percentage of training
20 NewsgroupsReuters
3200 docs
700 docs
Figure 3: Extensional learning curves on as percent-
age of the training set.
4.5 Comparisons with other algorithms
As mentioned earlier it is not easy to conduct a thor-
ough comparison with other algorithms in the litera-
ture. Most IL data sets used for training and evalua-
tion are either not available (McCallum and Nigam,
1999) or are composed by somewhat arbitrary sub-
sets of a standard data set. Another crucial aspect
is the particular choice of the seed terms selected to
compose an ID, which affects significantly the over-
all performance of the algorithm.
As a baseline system, we implemented a rule
based approach in the spirit of (McCallum and
Nigam, 1999). It is based on two steps. First, all
the documents in the unlabeled training corpus con-
taining at least one word in common with one and
only one category ID are assigned to the respective
class. Second, a supervised classifier based on SVM
is trained on the labeled examples. Finally, the su-
pervised classifier is used to perform the final cate-
gorization step on the test corpus. Table 2 reports
the F1 measure of our replication of this method, us-
ing the category name as seed, which is substantially
lower than the performance of the method we pre-
sented in this paper.
Reuters 20 Newsgroups
0.34 0.30
+ bootstrap 0.42 0.47
Table 2: Rule-based baseline performance
135
We also tried to replicate two of the non-standard
data sets used in (Liu et al, 2004)6. Table 3 displays
the performance of our approach in comparison to
the results reported in (Liu et al, 2004). Follow-
ing the evaluation metric adopted in that paper we
report here accuracy instead of F1. For each data
set (Liu et al, 2004) reported several results vary-
ing the number of seed words (from 5 to 30), as well
as varying some heuristic thresholds, so in the ta-
ble we report their best results. Notably, our method
obtained comparable accuracy by using just the cat-
egory name as ID for each class instead of multiple
seed terms. This result suggests that our method en-
ables to avoid the somewhat fuzzy process of col-
lecting manually a substantial number of additional
seed words.
Our IDs per cat. Liu et al IDs per cat.
REC 0.94 1 0.95 5
TALK 0.80 1 0.80 20
Table 3: Accuracy on 4 ?REC? and 4 ?TALK? news-
groups categories
5 Conclusions
We presented a general bootstrapping algorithm for
Intensional Learning. The algorithm can be applied
to any categorization problem in which categories
are described by initial sets of discriminative fea-
tures and an unlabeled training data set is provided.
Our algorithm utilizes a generalized similarity mea-
sure based on Latent Semantic Spaces and a Gaus-
sian Mixture algorithm as a principled method to
scale similarity scores into probabilities. Both tech-
niques address inherent limitations of the IL setting,
and leverage unsupervised information from an un-
labeled corpus.
We applied and evaluated our algorithm on some
text categorization tasks and showed the contribu-
tion of the two techniques. In particular, we obtain,
for the first time, competitive performance using
only the category names as initial seeds. This mini-
mal information per category, when exploited by the
IL algorithm, is shown to be equivalent to labeling
about 70-160 training documents per-category for
state of the art extensional learning. Future work is
6We used sequential splitting (70/30) rather than random
splitting and did not apply any feature selection. This setting
might be somewhat more difficult than the original one.
needed to investigate optimal procedures for collect-
ing seed features and to find out whether additional
seeds might still contribute to better performance.
Furthermore, it may be very interesting to explore
optimal combinations of intensional and extensional
supervision, provided by the user in the forms of
seed features and labeled examples.
Acknowledgments
This work was developed under the collaboration
ITC-irst/University of Haifa.
References
M. Berry. 1992. Large-scale sparse singular value com-
putations. International Journal of Supercomputer
Applications, 6(1):13?49.
M. Collins and Y. Singer. 1999. Unsupervised models
for named entity classification. In Proc. of EMNLP99,
College Park, MD, USA.
S. Deerwester, S. Dumais, G. Furnas, T. Landauer, and
R. Harshman. 1990. Indexing by latent semantic anal-
ysis. Journal of the American Society of Information
Science.
A. Gliozzo, C. Strapparava, and I. Dagan. 2004. Unsu-
pervised and supervised exploitation of semantic do-
mains in lexical disambiguation. Computer Speech
and Language, 18:275?299.
Y. Ko and J. Seo. 2000. Automatic text categorization by
unsupervised learning. In Proc. of COLING?2000.
Y. Ko and J. Seo. 2004. Learning with unlabeled data
for text categorization using bootstrapping abd fea-
ture projection techniques. In Proc. of the ACL-04,
Barcelona, Spain, July.
B. Liu, X. Li, W. S. Lee, and P. S. Yu. 2004. Text clas-
sification by labeling words. In Proc. of AAAI-04, San
Jose, July.
A. McCallum and K. Nigam. 1999. Text classification
by bootstrapping with keywords, em and shrinkage. In
ACL99 - Workshop for Unsupervised Learning in Nat-
ural Language Processing.
F. Sebastiani. 2002. Machine learning in automated text
categorization. ACM Computing Surveys, 34(1):1?47.
B. W. Silverman. 1986. Density Estimation for Statistics
and Data Analysis. Chapman and Hall.
D. Yarowsky. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. In Proc. of
ACL-95, pages 189?196, Cambridge, MA.
136
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 979?986, Vancouver, October 2005. c?2005 Association for Computational Linguistics
A Generalized Framework for Revealing Analogous 
Themes across Related Topics 
 
 
Zvika Marx Ido Dagan Eli Shamir 
CS and AI Laboratory Computer Science Department School of Computer Science 
MIT Bar-Ilan University The Hebrew University 
Cambridge, MA 02139, US Ramat-Gan 52900, Israel Jerusalem 91904, Israel 
zvim@csail.mit.edu dagan@cs.biu.ac.il shamir@cs.huji.ac.il 
 
 
 
 
Abstract 
This work addresses the task of identify-
ing thematic correspondences across sub-
corpora focused on different topics.  We 
introduce an unsupervised algorithmic 
framework based on distributional data 
clustering, which generalizes previous ini-
tial works on this task.  The empirical re-
sults reveal interesting commonalities of 
different religions.  We evaluate the re-
sults through measuring the overlap of our 
clusters with clusters compiled manually 
by experts.  The tested variants of our 
framework are shown to outperform al-
ternative methods applicable to the task. 
1 Introduction 
The ability to identify analogies and correspon-
dences is one of the fascinating aspects of intelli-
gence.  Research in cognitive science has 
acknowledged the significance of this ability of 
human thinking, particularly in learning across dif-
ferent situations or domains where the common 
base to learning is not straightforward.  Several 
previous computational models of analogy making 
(e.g. Falkenhainer et al, 1989) suggested symbolic 
computational mechanisms for constructing de-
tailed mappings that connect corresponding ingre-
dients across analogized systems. 
This work explores the identification of thematic 
correspondences in texts through an extension of 
the well known data clustering problem.  Previous 
works aimed at identifying ? through clusters of 
words ? concepts, sub-topics or themes that are 
prominent within a corpus of texts (e.g., Pereira et 
al., 1993; Li, 2002; Lin and Pantel, 2002).  The 
current work deals with extending this line of re-
search to identify corresponding themes across a 
corpus pre-divided to several sub-corpora, which 
are focused on different, yet related, topics. 
This research task has been defined quite re-
cently (Dagan et al, 2002), and has not been ex-
plored extensively yet.  One could think, however, 
of many potential applications for drawing corre-
spondences across textual resources: comparison 
of related firms or products, identifying equivalen-
cies in news published in different countries, and 
so on.  The experimental part of our work deals 
with revealing correspondences between different 
religions: Buddhism, Christianity, Hinduism, Islam 
and Judaism.  Given a pre-partition of the corpus to 
sub-corpora, one for each religion, our method ex-
poses common aspects for all religions, such as 
sacred writings, festivals and suffering. 
The mechanism we employ directs correspond-
ing key terms in the different sub-corpora, such as 
names of festivals of different religions, to be in-
cluded in the same cluster.  Term clustering meth-
ods in general, and in this work in particular, rely 
on word co-occurrence statistics: terms sharing 
similar words co-occurrence statistics are clustered 
together.  Different topics, however, are character-
ized by distinctive terminology and typical word 
co-locations.  Therefore, given a pre-divided cor-
pus, similar co-occurrence patterns would typically 
be extracted from the same topical sub-corpus.  
When the terminology and typical phrases em-
ployed by each topic differ greatly (even if the top-
979
ics are essentially related, e.g. different religions), 
the tendency to form topic-specific clusters intensi-
fies regardless of factors that otherwise could have 
impact this tendency, such as the co-occurrence 
window size.  Consequently, corresponding key 
terms of different topics may not be assigned by a 
standard method to the same cluster, in contrast to 
our goal.  The method described in this paper aims 
precisely at this problem: it is designed to neutral-
ize salient co-occurrence patterns within each topi-
cal sub-corpus and to promote less salient patterns 
that are shared across the sub-corpora. 
In an earlier line of research we have formulated 
the above problem and addressed it within a prob-
abilistic vector-based setting, presenting two re-
lated heuristic algorithms (Dagan et al, 2002; 
Marx et al, 2004).  Here, we devise a general prin-
cipled distributional clustering paradigm for this 
problem, termed cross-partition clustering, and 
show that the earlier algorithms are special cases of 
the new framework.   
This paper proceeds as follows: Section 2 de-
scribes in more detail the cross-partition clustering 
problem. Section 3 reviews distributional data 
clustering methods, which form the basis to our 
algorithmic framework described in Section 4.  
Section 5 presents experimental results that reveal 
interesting themes common to different religions 
and demonstrates, through an evaluation based on 
human expert data, that the different variants of 
our framework outperform alternative methods. 
2 The cross-partition clustering problem 
The cross-partition clustering problem is an exten-
sion of the standard (single-set) data clustering 
problem.  In the cross-partition setting, the dataset 
is pre-partitioned into several distinct subsets of 
elements to be clustered.  For example, in our ex-
periments each of these subsets consisted of topical 
key terms to be clustered.  Each such subset was 
extracted automatically from a sub-corpus corre-
sponding to a different religion (see Section 5). 
As in the standard clustering problem, our goal 
is to cluster the data such that each term cluster 
would capture a particular theme in the data.  
However, the generated clusters are expected to 
identify themes that cut across all the given sub-
sets.  For example, one cluster consists of names of 
festivals of different religions, such as Easter, 
Christmas, Sunday (Christianity) Ramadan, Fri-
day, Id-al-fitr (Islam) and Sukoth, Shavuot, Pass-
over (Judaism; see Figure 4 for more examples). 
3 Distributional clustering 
Our algorithmic framework elaborates on Pereira 
et al?s (1993) distributional clustering method.  
Distributional clustering probabilistically clusters 
data elements according to the distribution of a 
given set of features associated with the data.  Each 
data element x is represented as a probability dis-
tribution p
 
y x   over all features y.  In our data 
p
 
y x  is the empirical co-occurrence frequency of a 
feature word y with a key term x, normalized over 
all feature word co-occurrences with x. 
The distributional clustering algorithmic scheme 
(Figure 1) is a probabilistic (soft) version of the 
well-known K-means algorithm.  It iteratively al-
ternates between: 
(1) Calculating assignments to clusters: calculate 
an assignment probability p
 
c x  for each data ele-
ments x into each one of the clusters c.  This soft 
assignment is proportional to an information theo-
retic distance (KL divergence) between the ele-
ment's p
 
y x   representation, and the centroid of c, 
represented by a distribution p
 
y c .  The marginal 
cluster probability p
 
c  may optionally be set as a 
prior in this calculation, as in Tishby et al (1999; 
in Figure 1 we mark it with dotted underline, to 
denote it is optional).  
Set t  , and repeatedly iterate the two update-steps 
below, till convergence (at time step t  , initialize 
pt  c  x	  randomly or arbitrarily and skip step 1):
 (1) pt
 c  x 
 
),()(
)|()|(
1
1


xz
e
cp
t
cypxypKL
t
t 


     where   zt
 x    Bootstrapping Distributional Feature
Vector Quality
Maayan Zhitomirsky-Geffet?
Bar-Ilan University
Ido Dagan??
Bar-Ilan University
This article presents a novel bootstrapping approach for improving the quality of feature vector
weighting in distributional word similarity. The method was motivated by attempts to utilize
distributional similarity for identifying the concrete semantic relationship of lexical entailment.
Our analysis revealed that a major reason for the rather loose semantic similarity obtained by
distributional similarity methods is insufficient quality of the word feature vectors, caused by
deficient feature weighting. This observation led to the definition of a bootstrapping scheme
which yields improved feature weights, and hence higher quality feature vectors. The under-
lying idea of our approach is that features which are common to similar words are also most
characteristic for their meanings, and thus should be promoted. This idea is realized via a
bootstrapping step applied to an initial standard approximation of the similarity space. The
superior performance of the bootstrapping method was assessed in two different experiments,
one based on direct human gold-standard annotation and the other based on an automatically
created disambiguation dataset. These results are further supported by applying a novel quanti-
tative measurement of the quality of feature weighting functions. Improved feature weighting
also allows massive feature reduction, which indicates that the most characteristic features
for a word are indeed concentrated at the top ranks of its vector. Finally, experiments with
three prominent similarity measures and two feature weighting functions showed that the
bootstrapping scheme is robust and is independent of the original functions over which it is
applied.
1. Introduction
1.1 Motivation
Distributional word similarity has long been an active research area (Hindle 1990; Ruge
1992; Grefenstette 1994; Lee 1997; Lin 1998; Dagan, Lee, and Pereira 1999; Weeds and
? Department of Information Science, Bar-Ilan University, Ramat-Gan, Israel.
E-mail: zhitomim@mail.biu.ac.il.
?? Department of Computer Science, Bar-Ilan University, Ramat-Gan, Israel. E-mail: dagan@cs.biu.ac.il.
Submission received: 6 December 2006; revised submission received: 9 July 2008; accepted for publication:
21 November 2008.
? 2009 Association for Computational Linguistics
Computational Linguistics Volume 35, Number 3
Weir 2005). This paradigm is inspired by Harris?s distributional hypothesis (Harris
1968), which states that semantically similar words tend to appear in similar contexts.
In a computational realization, each word is characterized by a weighted feature vector,
where features typically correspond to other words that co-occur with the characterized
word in the context. Distributional similarity measures quantify the degree of similarity
between a pair of such feature vectors. It is then assumed that two words that occur
within similar contexts, as measured by similarity of their context vectors, are indeed
semantically similar.
The distributional word similarity measures were often applied for two types of
inferences. The first type is making similarity-based generalizations for smoothing
word co-occurrence probabilities, in applications such as language modeling and dis-
ambiguation. For example, assume that we need to estimate the likelihood of the verb?
object co-occurrence pair visit?country, although it did not appear in our sample corpus.
Co-occurrences of the verb visit with words that are distributionally similar to country,
such as state, city, and region, however, do appear in the corpus. Consequently, we
may infer that visit?country is also a plausible expression, using some mathematical
scheme of similarity-based generalization (Essen and Steinbiss 1992; Dagan, Marcus,
and Markovitch 1995; Karov and Edelman 1996; Ng and Lee 1996; Ng 1997; Dagan,
Lee, and Pereira 1999; Lee 1999; Weeds and Weir 2005). The rationale behind this
inference is that if two words are distributionally similar then the occurrence of one
word in some contexts indicates that the other word is also likely to occur in such
contexts.
A second type of semantic inference, which primarily motivated our own research,
is meaning-preserving lexical substitution. Many NLP applications, such as question
answering, information retrieval, information extraction, and (multi-document) sum-
marization, need to recognize that one word can be substituted by another one in a
given context while preserving, or entailing the original meaning. Naturally, recogniz-
ing such substitutable lexical entailments is a prominent component within the textual
entailment recognition paradigm, which models semantic inference as an application-
independent task (Dagan, Glickman, and Magnini 2006). Accordingly, several textual
entailment systems did utilize the output of distributional similarity measures to model
entailing lexical substitutions (Jijkoun and de Rijke 2005; Adams 2006; Ferrandez et al
2006; Nicholson, Stokes, and Baldwin 2006; Vanderwende, Menezes, and Snow 2006).
In some of these papers the distributional information typically complements man-
ual lexical resources in textual entailment systems, most notably WordNet (Fellbaum
1998).
Lexical substitution typically requires that the meaning of one word entails
the meaning of the other. For instance, in question answering, the word company
in a question can be substituted in an answer text by firm, automaker, or subsidiary,
whose meanings entail the meaning of company. However, as it turns out, traditional
distributional similarity measures do not capture well such lexical substitution
relationships, but rather capture a somewhat broader (and looser) notion of semantic
similarity. For example, quite distant co-hyponyms such as party and company
also come out as distributionally similar to country, due to a partial overlap of
their semantic properties. Clearly, the meanings of these words do not entail each
other.
Motivated by these observations, our long-term goal is to investigate whether the
distributional similarity scheme may be improved to yield tighter semantic similarities,
and eventually better approximation of lexical entailments. This article presents one
component of this research plan, which focuses on improving the underlying semantic
436
Zhitomirsky-Geffet and Dagan Bootstrapping Distributional Feature Vector Quality
quality of distributional word feature vectors. The article describes the methodology,
definitions, and analysis of our investigation and the resulting bootstrapping scheme
for feature weighting which yielded improved empirical performance.
1.2 Main Contributions and Outline
As a starting point for our investigation, an operational definition was needed for
evaluating the correctness of candidate pairs of similar words. Following the lexical
substitution motivation, in Section 3 we formulate the substitutable lexical entailment
relation (or lexical entailment, for brevity), refining earlier definitions in Geffet and
Dagan (2004, 2005). Generally speaking, this relation holds for a pair of words if a
possible meaning of one word entails a meaning of the other, and the entailing word can
substitute the entailed one in some typical contexts. Lexical entailment overlaps partly
with traditional lexical semantic relationships, while capturing more generally the
lexical substitution needs of applications. Empirically, high inter-annotator agreement
was obtained when judging the output of distributional similarity measures for lexical
entailment.
Next, we analyzed the typical behavior of existingword similaritymeasures relative
to the lexical entailment criterion. Choosing the commonly used measure of Lin (1998)
as a representative case, the analysis shows that quite noisy feature vectors are a major
cause for generating rather ?loose? semantic similarities. On the other hand, one may
expect that features which seem to be most characteristic for a word?s meaning should
receive the highest feature weights. This does not seem to be the case, however, for
common feature weighting functions, such as Point-wise Mutual Information (Church
and Patrick 1990; Hindle 1990).
Following these observations, we developed a bootstrapping formula that improves
the original feature weights (Section 4), leading to better feature vectors and better
similarity predictions. The general idea is to promote the weights of features that are
common for semantically similar words, since these features are likely to be most char-
acteristic for the word?s meaning. This idea is implemented by a bootstrapping scheme,
where the initial (and cruder) similarity measure provides an initial approximation for
semantic word similarity. The bootstrapping method yields a high concentration of
semantically characteristic features among the top-ranked features of the vector, which
also allows aggressive feature reduction.
The bootstrapping scheme was evaluated in two experimental settings, which cor-
respond to the two types of applications for distributional similarity. First, it achieved
significant improvements in predicting lexical entailment as assessed by human judg-
ments, when applied over several base similarity measures (Section 5). Additional
analysis relative to the lexical entailment dataset revealed cleaner and more charac-
teristic feature vectors for the bootstrapping method. To obtain a quantitative analysis
of this behavior, we defined a measure called average common-feature rank ratio.
This measure captures the idea that a prominent feature for a word is expected to be
prominent also for semantically similar words, while being less prominent for unrelated
words. To the best of our knowledge this is the first proposedmeasure for direct analysis
of the quality of feature weighting functions, without the need to employ them within
some vector similarity measure.
As a second evaluation, we applied the bootstrapping scheme for similarity-based
prediction of co-occurrence likelihood within a typical pseudo-word sense disambigua-
tion experiment, obtaining substantial error reductions (Section 7). Section 8 concludes
437
Computational Linguistics Volume 35, Number 3
this article, suggesting the relevance of our analysis and bootstrapping scheme for the
general use of distributional feature vectors.1
2. Background: Distributional Similarity Models
This section reviews the components of the distributional similarity approach and
specifies the measures and functions that were utilized by our work.
The Distributional Hypothesis assumes that semantically similar words appear in
similar contexts, suggesting that semantic similarity can be detected by comparing
contexts of words. This is the underlying principle of the vector-based distributional
similarity model, which comprises two phases. First, context features for each word are
constructed and assigned weights; then, the weighted feature vectors of pairs of words
are compared by a vector similarity measure. The following two subsections review
typical methods for each phase.
2.1 Features and Weighting Functions
In the typical computational setting, word contexts are represented by feature vectors.
A feature represents another word (or term) w? with which w co-occurs, and possibly
specifies also the syntactic relationship between the twowords, as in Grefenstette (1994),
Lin (1998), and Weeds and Weir (2005). Thus, a word (or term) w is represented by
a feature vector, where each entry in the vector corresponds to a feature f . Pado and
Lapata (2007) demonstrate that using syntactic dependency-based features helps to
distinguish among classes of lexical relations, which seems to be more difficult when
using ?bag of words? features that are based on co-occurrence in a text window.
A syntactic-based feature f for a word w is defined as a triple:
? fw, syn rel, f role?
where fw is a context word (or term) that co-occurs with w under the syntactic depen-
dency relation syn rel. The feature role ( f role) corresponds to the role of the feature word
fw in the syntactic dependency, being either the head (denoted h) or the modifier (de-
noted m) of the relation. For example, given the word company, the feature ?earnings, gen,
h? corresponds to the genitive relationship company?s earnings, and ?investor, pcomp of, m?
corresponds to the prepositional complement relationship the company of the investor.2
Throughout this article we use syntactic dependency relationships generated by the
Minipar dependency parser (Lin 1993). Table 1 lists common Minipar dependency
relations involving nouns. Minipar also identifies multi-word expressions, which is
1 A preliminary version of the bootstrapping method was presented in Geffet and Dagan (2004). That
paper presented initial results for the bootstrapping scheme, when applied only over Lin?s measure and
tested by the manually judged dataset of lexical entailment. The current research extends our initial
results in many respects. It refines the definition of lexical entailment; utilizes a revised test set of larger
scope and higher quality, annotated by three assessors; extends the experiments to two additional
similarity measures; provides comparative qualitative and quantitative analysis of the bootstrapped
vectors, while employing our proposed average common-feature rank ratio; and presents an additional
evaluation based on a pseudo-WSD task.
2 Following a common practice, we consider the relationship between a head noun (company in the
example) and the nominal complement of a modifying prepositional phrase (investor) as a single direct
dependency relationship. The preposition itself is encoded in the dependency relation name, with a
distinct relation for each preposition.
438
Zhitomirsky-Geffet and Dagan Bootstrapping Distributional Feature Vector Quality
Table 1
Common grammatical relations of Minipar involving nouns.
Relation Description
appo apposition
comp1 first complement
det determiner
gen genitive marker
mod the relationship between a word and its adjunct modifier
pnmod post nominal modifier
pcomp nominal complement of prepositions
post post determiner
vrel passive verb modifier of nouns
obj object of verbs
obj2 second object of ditransitive verbs
subj subject of verbs
s surface subject
advantageous for detecting distributional similarity for such terms. For example,
Curran (2004) reports that multi-word expressions make up between 14?25% of the
synonyms in a gold-standard thesaurus.
Thus, in our representation the corpus is first transformed to a set S of dependency
relationship instances of the form ?w,f ?, where each pair corresponds to a single co-
occurrence of w and f in the corpus. f is termed as a feature of w. Then, a word
w is represented by a feature vector, where each entry in the vector corresponds to
one feature f . The value of the entry is determined by a feature weighting function
weight(w, f ), which quantifies the degree of statistical association between w and f in the
set S. For example, some feature weighting functions are based on the logarithm of the
word?feature co-occurrence frequency (Ruge 1992), or on the conditional probability of
the feature given the word (Pereira, Tishby, and Lee 1993; Dagan, Lee, and Pereira 1999;
Lee 1999).
Probably the most widely used feature weighting function is (point-wise) Mutual
Information (MI) (Church and Patrick 1990; Hindle 1990; Luk 1995; Lin 1998; Gauch,
Wang, and Rachakonda 1999; Dagan 2000; Baroni and Vegnaduzzo 2004; Chklovski
and Pantel 2004; Pantel and Ravichandran 2004; Pantel, Ravichandran, and Hovy 2004;
Weeds, Weir, and McCarthy 2004), defined by:
weightMI(w, f ) = log2
P(w,f )
P(w)P( f ) (1)
We calculate the MI weights by the following statistics in the space of co-occurrence
instances S:
weightMI(w, f ) = log2
count(w, f ) ? nrels
count(w) ? count( f )
(2)
where count(w, f ) is the frequency of the co-occurrence pair ?w,f ? in S, count(w) and
count( f ) are the independent frequencies of w and f in S, and nrels is the size of S. High
MI weights are assumed to correspond to strong word?feature associations.
439
Computational Linguistics Volume 35, Number 3
Curran and Moens (2002) argue that, generally, informative features are statis-
tically correlated with their corresponding headword. Thus, they suggest that any
statistical test used for collocations is a good starting point for improving feature-
weight functions. In their experiments the t-test-based metric yielded the best empirical
performance.
However, a known weakness of MI and most of the other statistical weighting
functions used for collocation extraction, including t-test and ?2, is their tendency to
inflate the weights for rare features (Dunning 1993). In addition, a major property of
lexical collocations is their ?non-substitutability?, as termed in Manning and Schutze
(1999). That is, typically neither a headword nor a modifier in the collocation can be
substituted by their synonyms or other related terms. This implies that using modifiers
within strong collocations as features for a head word would provide a rather small
amount of common features for semantically similar words. Hence, these functions
seem less suitable for learning broader substitutability relationships, such as lexical
entailment.
Similarity measures that utilize MI weights showed good performance, however.
In particular, a common practice is to filter out features by minimal frequency and
weight thresholds. Then, a word?s vector is constructed from the remaining (not filtered)
features that are strongly associated with the word. These features are denoted here as
active features.
In the current work we use MI for data analysis, and for the evaluations of vector
quality and word similarity performance.
2.2 Vector Similarity Measures
Once feature vectors have been constructed the similarity between two words is de-
fined by some vector similarity measure. Similarity measures which have been used
in the cited papers include weighted Jaccard (Grefenstette 1994), Cosine (Ruge 1992),
and various information theoretic measures, as introduced and reviewed in Lee (1997,
1999). In the current work we experiment with the following three popular similarity
measures.
1. The basic Jaccard measure compares the number of common features with
the overall number of features for a pair of words. One of the weighted
generalizations of this scheme to non-binary values replaces intersection
with minimum weight, union with maximum weight, and set cardinality
with summation. This measure is commonly referred to as weighted Jaccard
(WJ) (Grefenstette 1994; Dagan, Marcus, and Markovitch 1995; Dagan
2000; Gasperin and Vieira 2004), defined as follows:
simWJ(w, v) =
?
f?F(w)?F(v) min(weight(w,f ),weight(v,f ))
?
f?F(w)?F(v) max(weight(w,f ),weight(v,f ))
(3)
where F(w) and F(v) are the sets of active features of the two words w
and v. The appealing property of this measure is that it considers the
association weights rather than just the number of common features.
2. The standard Cosine measure (COS), which is popularly employed for
information retrieval (Salton and McGill 1983) and also utilized for
440
Zhitomirsky-Geffet and Dagan Bootstrapping Distributional Feature Vector Quality
learning distributionally similar words (Ruge 1992; Caraballo 1999; Gauch,
Wang, and Rachakonda 1999; Pantel and Ravichandran 2004), is defined as
follows:
simCOS(w, v) =
?
f (weight(w,f )?weight(v,f ))?
?
f (weight(w,f ))
2?
?
?
f (weight(v,f ))
2 (4)
This measure computes the cosine of the angle between the two feature
vectors, which normalizes the vector lengths and thus avoids inflated
discrimination between vectors of significantly different lengths.
3. A popular state of the art measure has been developed by Lin (1998),
motivated by Information Theory principles. This measure behaves quite
similarly to the weighted Jaccard measure (Weeds, Weir, and McCarthy
2004), and is defined as follows:
simLIN(w, v) =
?
f?F(w)?F(v) (weightMI (w,f )+weightMI (v,f ))
?
f?F(w) weightMI (w,f )+
?
f?F(v) weightMI (v,f )
(5)
where F(w) and F(v) are the active features of the two words. The weight
function used originally by Lin is MI (Equation 1).
It is interesting to note that a relatively recent work by Weeds and Weir (2005) inves-
tigates a more generic similarity framework. Within their framework, the similarity of
two nouns is viewed as the ability to predict the distribution of one of them based on
that of the other. Their proposed formula combines the precision and recall of a potential
?retrieval? of similar words based on the features of the target word. The precision of
w?s prediction of v?s feature distribution indicates howmany of the features of the word
w co-occurred with the word v. The recall of w?s prediction of v?s features indicates
how many of the features of v co-occurred with w. Words with both high precision
and high recall can be obtained by computing their harmonic mean, mh (or F-score),
and a weighted arithmetic mean. However, after empirical tuning of weights for the
arithmetic mean, Weeds and Weir?s formula practically reduces to Lin?s measure, as
was anticipated by their own analysis (in Section 4 of their paper).
Consequently, we choose the Lin measure (Equation 5) (henceforth denoted as LIN)
as representative for the state of the art and utilize it for data analysis and as a starting
point for improvement. To further explore and evaluate our new weighting scheme,
independently of a single similarity measure, we conduct evaluations also with the
other two similarity measures of weighted Jaccard and Cosine.
3. Substitutable Lexical Entailment
As mentioned in the Introduction, the long term research goal which inspired our work
is modeling meaning?entailing lexical substitution. Motivated by this goal, we
proposed in earlier work (Geffet and Dagan 2004, 2005) a new type of lexical
relationship which aims to capture such lexical substitution needs. Here we adopt that
approach and formulate a refined definition for this relationship, termed substitutable
lexical entailment. In the context of the current article, utilizing a concrete target notion
of word similarity enabled us to apply direct human judgment for the ?correctness?
(relative to the defined notion) of candidate word pairs suggested by distributional
similarity. Utilizing these judgments we could analyze the behavior of alternative
441
Computational Linguistics Volume 35, Number 3
distributional vector representations and, in particular, conduct error analysis for word
pair candidates that were judged negatively.
The discussion in the Introduction suggested that multiple text understanding
applications need to identify term pairs whose meanings are both entailing and sub-
stitutable. Such pairs seem to be most appropriate for lexical substitution in a meaning
preserving scenario. Tomodel this goal we present an operational definition for a lexical
semantic relationship that integrates the two aspects of entailment and substitutability,3
which is termed substitutable lexical entailment (or lexical entailment, for brevity).
This relationship holds for a given directional pair of terms (w, v), saying that w entails
v, if the following two conditions are fulfilled:
1. Word meaning entailment: the meaning of a possible sense of w implies a
possible sense of v;
2. Substitutability: w can substitute for v in some naturally occurring sentence,
such that the meaning of the modified sentence would entail the meaning
of the original one.
To operationally assess the first condition (by annotators) we propose considering
the meaning of terms by existential statements of the form ?there exists an instance of
the meaning of the term w in some context? (notice that, unlike propositions, it is not
intuitive for annotators to assign truth values to terms). For example, the word company
would correspond to the existential statement ?there exists an instance of the concept
company in some context.? Thus, if in some context ?there is a company? (in the sense
of ?commercial organization?) then necessarily ?there is a firm? in that context (in the
corresponding sense). Therefore, we conclude that the meaning of company implies the
meaning of firm. On the other hand, ?there is an organization? does not necessarily imply
the existence of company, since organization might stand for some non-profit association,
as well. Therefore, we conclude that organization does not entail company.
To assess the second condition, the annotators need to identify some natural con-
text in which the lexical substitution would satisfy entailment between the modified
sentence and the original one. Practically, in our experiments presented in Section 5 the
human assessors could consult external lexical resources and the entireWeb to obtain all
the senses of the words and possible sentences for substitution. We note that the task of
identifying the common sense of two given words is quite easy since they mutually dis-
ambiguate each other, and once the common sense is known it naturally helps finding a
corresponding common context. We note that this condition is important, in particular,
in order to eliminate cases of anaphora and co-reference in contexts, where two words
quite different in their meaning can sometimes appear in the same contexts only due
to the text pragmatics in a particular situation. For example, in some situations worker
and demonstrator could be used interchangeably in text, but clearly it is a discourse co-
reference rather than common meaning that makes the substitution possible. Instead,
we are interested in identifying word pairs in which one word?s meaning provides
a reference to the entailed word?s meaning. This purpose is exactly captured by the
existential propositions of the first criterion above.
3 The WordNet definition of the lexical entailment relation is specified only for verbs and, therefore, is not
felicitous for general purposes: A verb X entails Y if X cannot be done unless Y is, or has been, done (e.g.,
snore and sleep).
442
Zhitomirsky-Geffet and Dagan Bootstrapping Distributional Feature Vector Quality
As reported further in Section 5.1, we observed that assessing these two conditions
for candidate word similarity pairs was quite intuitive for annotators, and yielded good
cross-annotator agreement. Overall, substitutable lexical entailment captures directly
the typical lexical substitution scenario in text understanding applications, as well as in
generic textual entailment modeling. In fact, this relation partially overlaps with several
traditional lexical semantic relations that are known as relevant for lexical substitution,
such as synonymy, hyponymy, and some cases of meronymy. For example, we say
that the meaning of company is lexically entailed by the meaning of firm (synonym)
or automaker (hyponym), while the word government entails minister (meronym) as The
government voted for the new law entails A minister in the government voted for the new law.
On the other hand, lexical entailment is not just a superset of other known relations,
but it is rather designed to select those sub-cases of other lexical relations that are needed
for applied entailment inference. For example, lexical entailment does not cover all cases
of meronyms (e.g., division does not entail company), but only some sub-cases of part-
whole relationship mentioned herein. In addition, some other relations are also covered
by lexical entailment, like ocean and water and murder and death, which do not seem to
directly correspond to meronymy or hyponymy relations.
Notice also that whereas lexical entailment is a directional relation that specifies
which word of the pair entails the other, the relation may hold in both directions
for a pair of words, as is the case for synonyms. More detailed motivations for the
substitutable lexical entailment relation and analysis of its relationship to traditional
lexical semantic relations appear in Geffet (2006) and Geffet and Dagan (2004, 2005).
4. Bootstrapping Feature Weights
To gain a better understanding of distributional similarity behavior we first analyzed
the output of the LIN measure, as a representative case for the state of the art, and
regarding lexical entailment as a reference evaluation criterion. We judge as correct,
with respect to lexical entailment, those candidate pairs of the distributional similarity
method for which entailment holds at least in one direction.
For example, the word area is entailed by country, since the existence of country
entails the existence of area, and the sentence There is no rain in subtropical countries during
the summer period entails the sentence There is no rain in subtropical areas during the summer
period. As another example, democracy is a type of country in the political sense, thus the
existence entailment holds and also the sentence Israel is a democracy in the Middle East
entails Israel is a country in the Middle East.
On the other hand, our analysis revealed that many candidate word similarity pairs
suggested by distributional similarity measures do not correspond to ?tight? semantic
relationships. In particular, many word pairs suggested by the LIN measure do not
satisfy the lexical entailment relation, as demonstrated in Table 2.
A deeper look at the corresponding word feature vectors reveals typical reasons
for these lexical entailment prediction errors. Most relevant for the scope of the cur-
rent article, in many cases highly ranked features in a word vector (when sorting the
features by their weight) do not seem very characteristic for the word meaning. This
is demonstrated in Table 3, which shows the top 10 features in the vector for country.
As can be seen, some of the top features are either too specific (landlocked, airspace),
and are thus less reliable, or too general (destination, ambition), thus not indicative and
may co-occur with many different types of words. On the other hand, intuitively more
characteristic features of country, like population and governor, occur further down the
443
Computational Linguistics Volume 35, Number 3
Table 2
The top 20 most similar words for country (and their ranks) in the similarity list of LIN, followed
by the next four words in the similarity list that were judged as entailing at least in one direction.
nation 1 *city 7 economy 13 *company 19
region 2 territory 8 *neighbor 14 *industry 20
state 3 area 9 *sector 15 kingdom 30
*world 4 *town 10 *member 16 place 35
*island 5 republic 11 *party 17 colony 41
*province 6 *north 12 government 18 democracy 82
Twelve out of 20 top similarities (60%) were judged as mutually non-entailing and are marked
with an asterisk. The similarity data was produced as described in Section 5.
Table 3
The top 10 ranked features for country produced by MI, the weighting function employed in the
LIN method.
Feature weightMI
Commercial bank, gen, h 8.08
Destination, pcomp of, m 7.97
Airspace, pcomp of, h 7.83
Landlocked, mod, m 7.79
Trade balance, gen, h 7.78
Sovereignty, pcomp of, h 7.78
Ambition, nn, h 7.77
Bourse, gen, h 7.72
Politician, gen, h 7.54
Border, pcomp of, h 7.53
sorted feature list, at positions 461 and 832. Overall, features that seem to characterize
the word meaning well are scattered across the ranked feature list, while many non-
indicative features receive high weights. This behavior often yields high similarity
scores for word pairs whose semantic similarity is rather loose while missing some
much tighter similarities.
Furthermore, we observed that characteristic features for a word w, which should
receive higher weights, are expected to be common for w and other words that are
semantically similar to it. This observation suggests a computational scheme which
would promote the weights of features that are common for semantically similar words.
Of course, there is an inherent circularity in such a scheme: to determine which features
should receive high weights we need to know which words are semantically similar,
while computing distributional semantic similarity already requires pre-determined
feature weights.
This kind of circularity can be approached by a bootstrapping scheme. We first
compute initial distributional similarity values, based on an initial feature weighting
function. Then, to learn more accurate feature weights for a word w, we promote
features that characterize other words that are initially known to be similar to w. By
the same rationale, features that do not characterize many words that are sufficiently
similar to w are demoted. Even if such features happen to have a strong direct statis-
tical association with w they would not be considered reliable, because they are not
supported by additional words that have a similar meaning to that of w.
444
Zhitomirsky-Geffet and Dagan Bootstrapping Distributional Feature Vector Quality
4.1 Bootstrapped Feature Weight Definition
The bootstrapped feature weight is defined as follows. First, some standard word
similarity measure sim is computed to obtain an initial approximation of the similarity
space. Then, we define the word set of a feature f , denoted by WS( f ), as the set of words
for which f is an active feature. Recall from Section 2.2 that an active feature is a feature
that is strongly associated with the word, that is, its (initial) weight is higher than an
empirically predefined threshold, ?weight. The semantic neighborhood of w, denoted by
N(w), is defined as the set of all words v which are considered sufficiently similar to
w, satisfying sim(w, v) > ?sim, where ?sim is a second empirically determined threshold.
The bootstrapped feature weight, denoted weightB, is then defined by:
weightB(w, f ) =
?
v?WS( f )?N(w) sim(w, v) (6)
That is, we identify all words v that are in the semantic neighborhood of w and are also
characterized by f , and then sum the values of their similarities to w.
Intuitively, summing these similarity values captures simultaneously a desired
balance between feature specificity and generality, addressing the observations in the
beginning of this section. Some features might characterize just a single word that is
very similar to w, but then the sum of similarities will include a single element, yielding
a relatively low weight. This is why the sum of similarities is used rather than an
average value, which might become too high by chance when computed over just a
single element (or very few elements). Relatively generic features, which occur with
many words and are thus less indicative, may characterize more words within N(w)
but then on average the similarity values of these words with w is likely to be lower,
contributing smaller values to the sum. To receive a high overall weight a reliable feature
has to characterize multiple words that are highly similar to w.
We note that the bootstrapped weight is a sum of word similarity values rather
than a direct function of word?feature association values, which is the more common
approach. It thus does not depend on the exact statistical co-occurrence level between
w and f . Instead, it depends on a more global assessment of the association between
f and the semantic vicinity of w. We notice that the bootstrapped weight is deter-
mined separately relative to each individual word. This differs from measures that are
global word-independent functions of the feature, such as the feature entropy used in
Grefenstette (1994) and the feature term strength relative to a predefined class as em-
ployed in Pekar, Krkoska, and Staab (2004) for supervised word classification.
4.2 Feature Reduction and Similarity Re-Computation
Once the bootstrappedweights have been computed, their accuracy is sufficient to allow
for aggressive feature reduction. As shown in the following section, in our experiments
it sufficed to use only the top 100 features for each word in order to obtain optimal
word similarity results, because the most informative features now receive the highest
weights.
Finally, similarity betweenwords is re-computed over the reduced vectors using the
sim function with weightB replacing the original feature weights. The resulting similarity
measure is further referred to as simB.
445
Computational Linguistics Volume 35, Number 3
5. Evaluation by Lexical Entailment
To test the effectiveness of the bootstrapped weighting scheme, we first evaluated
whether it contributes to better prediction of lexical entailment. This evaluation was
based on gold-standard annotations determined by human judgments of the substi-
tutable lexical entailment relation, as defined in Section 3. The new similarity scheme,
simB, based on the bootstrapped weights, was first computed using the standard LIN
method as the initial similarity measure. The resulting similarity lists of simLIN (the
original LIN method) and simBLIN (Bootstrapped LIN) schemes were evaluated for a sam-
ple of nouns (Section 5.2). Then, the evaluation was extended (Section 5.3) to apply the
bootstrapping scheme over the two additional similarity measures that were presented
in Section 2.2, simWJ (weighted Jaccard) and simCOS (Cosine). Along with these lexical
entailment evaluations we also analyzed directly the quality of the bootstrapped fea-
ture vectors, according to the average common-feature rank ratio measure, which was
defined in Section 6.
5.1 Experimental Setting
Our experiments were conducted using statistics from an 18 million token subset of the
Reuters RCV1 corpus (known as Reuters Corpus, Volume 1, English Language, 1996-
08-20 to 1997-08-19), parsed by Lin?s Minipar dependency parser (Lin 1993).
The test set of candidate word similarity pairs was constructed for a sample of
30 randomly selected nouns whose corpus frequency exceeds 500. In our primary exper-
iment we computed the top 40 most similar words for each noun by the simLIN and by
simBLIN measures, yielding 1,200 pairs for each method, and 2,400 pairs altogether. About
800 of these pairs were common for the two methods, therefore leaving approximately
1,600 distinct candidate word similarity pairs. Because the lexical entailment relation is
directional, each candidate pair was duplicated to create two directional pairs, yielding
a test set of 3,200 pairs. Thus, for each pair of words,w and v, the two ordered pairs (w, v)
and (v,w) were created to be judged separately for entailment in the specified direction
(whether the first word entails the other). Consequently, a non-directional candidate
similarity pair w, v is considered as a correct entailment if it was assessed as an entailing
pair at least in one direction.
The assessors were only provided with a list of word pairs without any contextual
information and could consult any available dictionary, WordNet, and the Web.
The judgment criterion follows the criterion presented in Section 3. In particular,
the judges were asked to apply the two operational conditions, existence and sub-
stitutability in context, to each given pair. Prior to performing the final test of the
annotation experiment, the judges were presented with an annotated set of entailing
and non-entailing pairs along with the existential statements and sample sentences for
substitution, demonstrating how the two conditions could be applied in different cases
of entailment. In addition, they had to judge a training set of several dozen pairs and
then discuss their judgment decisions with each other to gain a better understanding of
the two criteria.
The following example illustrates this process. Given a non-directional pair
{company, organization} two directional pairs are created: (company, organization) and
(organization, company). The former pair is judged as a correct entailment: the existence
of a company entails the existence of an organization, and the meaning of the sentence:
John works for a large company entails the meaning of the sentence with substitution:
John works for a large organization. Hence, company lexically entails organization, but not
446
Zhitomirsky-Geffet and Dagan Bootstrapping Distributional Feature Vector Quality
vice versa (as shown in Section 3.3), therefore the second pair is judged as not entailing.
Eventually, the non-directional pair {company, organization} is considered as a correct
entailment.
Finally, the test set of 3,200 pairs was split into three disjoint subsets that were
judged by three native English speaking assessors, each of whom possessed a Bach-
elors degree in English Linguistics. For each subset a different pair of assessors was
assigned, each person judging the entire subset. The judges were grouped into three
different pairs (i.e., JudgeI+JudgeII, JudgeII+JudgeIII, and JudgeI+JudgeIII). Each pair
was assigned initially to judge all the word similarities in each subset, and the third
assessor was employed in cases of disagreement between the first two. The majority
vote was taken as the final decision. Hence, each assessor had to fully annotate two
thirds of the data and for a third subset she only had to judge the pairs for which there
was disagreement between the other two judges. This was done in order to measure the
agreement achieved for different pairs of annotators.
The output pairs from bothmethodsweremixed so the assessors could not associate
a pair with the method that proposed it. We note that this evaluation methodology,
in which human assessors judge the correctness of candidate pairs by some semantic
substitutability criterion, is similar to common evaluation methodologies used for para-
phrase acquisition (Barzilay and McKeown 2001; Lin and Pantel 2001; Szpektor et al
2004).
Measuring human agreement level for this task, the proportions of matching de-
cisions were 93.5% between Judge I and Judge II, 90% for Judge I and Judge III, and
91.2% for Judge II and Judge III. The corresponding kappa values are 0.83, 0.80, and 0.80,
which is regarded as ?very good agreement? (Landis and Koch 1997). It is interesting
to note that after some discussion most of the disagreements were settled, and the few
remaining mismatches were due to different understandings of word meanings. These
findings seem to have a similar flavor to the human agreement findings reported for the
Recognizing Textual Entailment challenges (Bar-Haim et al 2006; Dagan, Glickman, and
Magnini 2006), in which entailment was judged for pairs of sentences. In fact, the kappa
values obtained in our evaluation are substantially higher than reported for sentence-
level textual entailment, which suggests that it is easier to make entailment judgments
at the lexical level than at the full sentence level.
The parameter values of the algorithms were tuned using a development set of
similarity pairs generated for 10 additional nouns, distinct from the 30 nouns used for
the test set. The parameters were optimized by running the algorithm systematically
with various values across the parameter scales and judging a sample subset of the
results. weightMI = 4 was found as the optimal MI threshold for active feature weights
(features included in the feature vectors), yielding a 10% precision increase of simLIN and
removing over 50% of the data relative to no feature filtering. Accordingly, this value
also serves as the ?weight threshold in the bootstrapping scheme (Section 4). As for the
?sim parameter, the best results on the development set were obtained for ?sim = 0.04,
?sim = 0.02, and ?sim = 0.01 when bootstrapping over the initial similarity measures
LIN, WJ, and COS, respectively.
5.2 Evaluation Results for simBLIN
We measured the contribution of the improved feature vectors to the resulting preci-
sion of simLIN and sim
B
LIN in predicting lexical entailment. The results are presented in
Table 4, where precision and error reduction values were computed for the top 20,
30, and 40 word similarity pairs produced by each method. It can be seen that the
447
Computational Linguistics Volume 35, Number 3
Table 4
Lexical entailment precision values for top-n similar words by the Bootstrapped LIN and the
original LIN method.
Top-n Correct Error Rate
Words Entailments (%) Reduction (%)
simLIN sim
B
LIN
Top 20 52.0 57.9 12.3
Top 30 48.2 56.2 15.4
Top 40 41.0 49.7 14.7
Bootstrapped LIN method outperformed the original LIN approach by 6?9 precision
points at all top-n levels. As expected, the precision for the shorter top 20 list is higher
for both methods, thus leaving a bit less room for improvement.
Overall, the Bootstrapped LIN method extracted 104 (21%) more correct similarity
pairs than the other measure and reduced the number of errors by almost 15%. We also
computed the relative recall, which shows the percentage of correct word similarities
found by each method relative to the joint set of similarities that were extracted by
both methods. The overall relative recall of the Bootstrapped LIN was quite high (94%),
exceeding LIN?s relative recall (of 78%) by 16 percentage points. We found that the
bootstrapped method covers over 90% of the correct similarities learned by the original
method, while also identifying many additional correct pairs.
It should be noted at this point that the current limited precision levels are deter-
mined not just by the quality of the feature vectors but significantly by the nature of the
vector comparison measure itself (i.e., the LIN formula, as well weighted Jaccard and
Cosine as reported in Section 5.3). It was observed in other work (Geffet and Dagan
2005) that these common types of vector comparison schemes exhibit certain flaws
in predicting lexical entailment. Our present work thus shows that the bootstrapping
method yields a significant improvement in feature vector quality, but future research
is needed to investigate improved vector comparison schemes.
An additional indication of the improved vector quality is the massive feature
reduction allowed by having the most characteristic features concentrated at the top
ranks of the vectors. The vectors of active features of LIN, as constructed after standard
feature filtering (Section 5.1), could be further reduced by the bootstrapped weighting
to about one third of their size. As illustrated in Figure 1, changing the vector size
significantly affects the similarity results. In simBLIN the best result was obtained with
the top 100 features per word, while using less than 100 or more than 150 features
caused a 5?10% decrease in performance. On the other hand, an attempt to cut off the
lower ranked features of the MI weighting always resulted in a noticeable decrease in
precision. These results show that for MI weighting many important features appear
further down in the ranked vectors, while for the bootstrapped weighting adding too
many features adds mostly noise, since most characteristic features are concentrated
at the top ranks. Thus, in addition to better feature weighting, the bootstrapping step
provides effective feature reduction, which improves vector quality and consequently
the similarity results.
We note that the optimal vector size we obtained conforms to previous results?
for example, by Widdows (2003), Curran (2004), and Curran and Moens (2002)?who
also used reduced vectors of up to 100 features as optimal for learning hyponymy and
synonymy, respectively. In Widdows the known SVD method for dimension reduction
448
Zhitomirsky-Geffet and Dagan Bootstrapping Distributional Feature Vector Quality
Figure 1
Percentage of correct entailments within the top 40 candidate pairs of each of the methods,
LIN and Bootstrapped LIN (denoted as LINB in the figure), when using varying numbers of
top-ranked features in the feature vector. The value of ?All? corresponds to the full size of
vectors and is typically in the range of 300?400 features.
of LSA-based vectors is applied, whereas in Curran, and Curran and Moens, only
the strongly associated verbs (direct and indirect objects of the noun) are selected as
?canonical features? that are expected to be shared by true synonyms.
Finally, we tried executing an additional bootstrapping iteration of weightB calcula-
tion over the similarity results of simBLIN. The resulting increase in precision was much
smaller, of about 2%, showing that most of the potential benefit is exploited in the
first bootstrapping iteration (which is not uncommon for natural language data). On
the other hand, computing the bootstrapping weight twice increases computation time
significantly, which led us to suggest a single bootstrapping iteration as a reasonable
cost-effectiveness tradeoff for our data.
5.3 Evaluation for simBWJ and sim
B
COS
To further validate the behavior of the bootstrapping schemewe experimentedwith two
additional similaritymeasures, weighted Jaccard (simWJ) and Cosine (simCOS) (described
in Section 2.2). For each of the additional measures the experiment repeats the main
three steps described in Section 4: Initially, the basic similarity lists are calculated for
each of the measures using MI weighting; then, the bootstrapped weighting, weightB, is
computed based on the initial similarities, yielding new word feature vectors; finally,
the similarity values are recomputed by the same vector similarity measure using the
new feature vectors.
To assess the effectiveness of weightB we computed the four alternative output
similarity lists, using the simWJ and simCOS similarity measures, each with the weightMI
449
Computational Linguistics Volume 35, Number 3
Table 5
Comparative precision values for the top 20 similarity lists of the three selected similarity
measures, with MI and Bootstrapped feature weighting for each.
Measure LIN?LINB WJ?WJB COS?COSB
Correct Similarities (%) 52.0?57.9 51.0?54.8 46.1?50.9
and weightB weighting functions. The four lists were judged for lexical entailment by
three assessors, according to the same procedure described in Section 5.1. To make the
additional manual evaluation affordable we judged the top 20 similar words in each list
for each of the 30 target nouns of Section 5.1.
Table 5 summarizes the precision values achieved by LIN, WJ, and COS with
both weightMI and weight
B. As shown in the table, bootstrapped weighting consistently
contributed between 4?6 points to the accuracy of each method in the top 20 similarity
list. We view the results as quite positive, considering that improving over top 20
similarities is a much more challenging task than improving over longer similarity lists,
while the improvement was achieved only by modifying the feature vectors without
changing the similarity measure itself (as hinted in Section 5.2). Our results are also
compatible with previous findings in the literature (Dagan, Lee, and Pereira 1999;
Weeds, Weir, and McCarthy 2004) that found LIN and WJ to be more accurate for
similarity acquisition than COS. Overall, the results demonstrate that the bootstrapped
weighting scheme consistently produces improved results.
An interesting behavior of the bootstrapping process is that the most prominent
features for a given target word converge across the different initial similarity measures,
as exemplified in Table 6. In particular, although the initial similarity lists overlap only
partly,4 the overlap of the top 30 features for our 30-word sample was ranging between
88% and 100%. This provides additional evidence that the quality of the bootstrapped
weighting is quite similar for various initial similarity measures.
6. Analyzing the Bootstrapped Feature Vector Quality
In this section we provide an in-depth analysis of the bootstrapping feature weighting
quality compared to the state-of-the-art MI weighting function.
6.1 Qualitative Observations
The problematic feature ranking noticed at the beginning of Section 4 can be revealed
more objectively by examining the common features which contribute most to the word
similarity scores. To that end, we examine the common features of the two given words
and sort them by the sum of their weights in both word vectors. Table 7 shows the top
10 common features by this sorting for a pair of truly similar (lexically entailing) words
(country?state), and for a pair of non-entailing words (country?party). For each common
feature the table shows its two corresponding ranks in the feature vectors of the two
words.
4 Overlap rate was about 40% between COS and WJ or LIN, and 70% between WJ and LIN. The overlap
was computed following the procedure of Weeds, Weir, and McCarthy (2004), disregarding the order of
the similar words in the lists. Interestingly, they obtained roughly similar figures, of 28% overlap for COS
and WJ, 32% overlap for COS and LIN, and 81% overlap between LIN and WJ.
450
Zhitomirsky-Geffet and Dagan Bootstrapping Distributional Feature Vector Quality
Table 6
Top 30 features of town by bootstrapped weighting based on LIN, WJ, and COS as initial
similarities. The three sets of words are almost identical, with relatively minor ranking
differences.
LINB WJB COSB
southern southern northern
northern northern southern
office office remote
eastern official eastern
remote coastal official
official eastern based
troop northeastern northeastern
northeastern remote office
people troop coastal
coastal people northwestern
attack based people
based populated attack
populated attack troop
northwestern home home
base northwestern south
home south western
south western city
west west populated
western resident base
neighboring neighboring resident
resident house north
plant city west
police base neighboring
held trip trip
locate camp surrounding
trip held police
city north held
site locate locate
camp surrounding house
surrounding police camp
It can be observed in Table 7 that for both word pairs the common features are
scattered across the pair of feature vectors, making it difficult to distinguish between
the truly similar and the non-similar pairs. We suggest, on the other hand, that the
desired behavior of effective feature weighting is that the common features of truly
similar words would be concentrated at the top ranks of both word vectors. In other
words, if the two words are semantically similar then we expect them to share their
most characteristic features, which are in turn expected to appear at the higher ranks
of each feature vector. The common features for non-similar words are expected to be
scattered all across each of the vectors. In fact, these expectations correspond exactly to
the rationale behind distributional similarity measures: Such measures are designed to
assign higher similarity scores for vector pairs that share highly weighted features.
Comparatively, we illustrate the behavior of the Bootstrapped LIN method relative to
the observations regarding the original LIN method, using the same running example.
Table 8 shows the top 10 features of country. We observe that the list now contains
features that are intuitively quite indicative and reliable, while many too specific or
idiomatic features, and too general ones, were demoted (compare with Table 3). Table 9
shows that most of the top 10 common features for country?state are now ranked highly
451
Computational Linguistics Volume 35, Number 3
for both words. On the other hand, there are only two common features (among the
top 100 features) for the incorrect pair country?party, bothwith quite low ranks (compare
with Table 7), while the rest of the common features for this pair did not pass the top 100
cutoff.
Consequently, Table 10 demonstrates a much more accurate similarity list for coun-
try, wheremany incorrect (non-entailing) word similarities, like party and company, were
demoted. Instead, additional correct similarities, like kingdom and land, were promoted
(compare with Table 2). In this particular case all the remaining errors correspond to
words that are related quite closely to country, denoting geographic concepts. Many of
these errors are context dependent entailments which might be substitutable in some
cases, but they violate the word meaning entailment condition (e.g., country?neighbor,
country?port). Apparently, these words tend to occur in contexts that are typical for
country in the Reuters corpus. Some errors violating the substitutability condition of
lexical entailment were identified as well, such as industry?product. These cases are
quite hard to differentiate from correct entailments, since the two words are usually
closely related to each other and also share highly ranked features, because they often
appear in similar characteristic contexts. It may therefore be difficult to filter out such
Table 7
LIN (MI) weighting: The top 10 common features for country?state and country?party, along with
their corresponding ranks in each of the two feature vectors. The features are sorted by the sum
of their feature weights with both words.
Country?State Ranks Country?Party Ranks
Broadcast, pcomp in, h 24 50 Brass, nn, h 64 22
Goods, mod, h 140 16 Concluding, pcomp of, h 73 20
Civil servant, gen, h 64 54 Representation, pcomp of, h 82 27
Bloc, gen, h 30 77 Patriarch, pcomp of, h 128 28
Nonaligned, mod, m 55 60 Friendly, mod, m 58 83
Neighboring, mod, m 15 165 Expel, pcomp from, h 59 30
Statistic, pcomp on, h 165 43 Heartland, pcomp of, h 102 23
Border, pcomp of, h 10 247 Surprising, pcomp of, h 114 38
Northwest, mod, h 41 174 Issue, pcomp between, h 103 51
Trip, pcomp to, h 105 34 Contravention, pcomp in, m 129 43
Table 8
Top 10 features of country by the Bootstrapped feature weighting.
Feature WeightB
Industry, gen, h 1.21
Airport, gen, h 1.16
Visit, pcomp to, h 1.06
Neighboring, mod, m 1.04
Law, gen, h 1.02
Economy, gen, h 1.02
Population, gen, h 0.93
Stock market, gen, h 0.92
Governor, pcomp of, h 0.92
Parliament, gen, h 0.91
452
Zhitomirsky-Geffet and Dagan Bootstrapping Distributional Feature Vector Quality
Table 9
Bootstrapped weighting: top 10 common features for country?state and country?party along with
their corresponding ranks in the two (sorted) feature vectors.
Country?State Ranks Country?Party Ranks
Neighboring, mod, m 3 1 Relation, pcomp with, h 12 26
Industry, gen, h 1 11 Minister, pcomp from, h 77 49
Impoverished, mod, m 8 8
Governor, pcomp of, h 10 9
Population, gen, h 6 16
City, gen, h 17 18
Economy, gen, h 5 15
Parliament, gen, h 10 22
Citizen, pcomp of, h 14 25
Law, gen, h 4 33
Table 10
Top 20 most similar words for country and their ranks in the similarity list by the Bootstrapped
LIN measure.
nation 1 territory 6 *province 11 zone 16
state 2 *neighbor 7 *city 12 land 17
*island 3 colony 8 *town 13 place 18
region 4 *port 9 kingdom 14 economy 19
area 5 republic 10 *district 15 *world 20
Note that four of the incorrect similarities from Table 2 were replaced with correct entailments
resulting in a 20% increase of precision (reaching 60%).
non-substitutable similarities merely by the standard distributional similarity scheme,
suggesting that additional mechanisms and data types would be required.
6.2 The Average Common-Feature Rank Ratio
It should be noted at this point that these observations regarding feature weight be-
havior are based on subjective intuition of how characteristic features are for a word
meaning, which is quite difficult to assess systematically. Therefore, we next propose a
quantitative measure for analyzing the quality of feature vector weights.
More formally, given a pair of feature vectors for words w and v we first define
their average common-feature rankwith respect to the top-n common features, denoted
acfrn, as follows:
acfrn(w, v) =
1
n
?
f?top?n(F(w)?F(v))
1
2 [rank(w, f )+ rank(v, f )]
(7)
where rank(w, f ) is the rank of feature f in the vector of the word w when features are
sorted by their weight, and F(w) is the set of features in w?s vector. top-n is the set of
top n common features to consider, where common features are sorted by the sum of
their weights in the two word vectors (the same sorting as in Table 7). In other words,
acfrn(w, v) is the average rank in the two feature vectors of their top n common features.
453
Computational Linguistics Volume 35, Number 3
Using this measure, we expect that a good feature weighting function would
typically yield lower values of acfrn for truly similar words (as low ranking values
correspond to higher positions in the vectors) than for non-similar words. Hence, given
a pre-judged test set of pairs of similar and non-similar words, we define the ratio,
acfr-ratio, between the average acfrn of the set of all the non-similar words, denoted as
Non-Sim, and the average acfrn of the set of all the known pairs of similar words, Sim, to
be an objective measure for feature weighting quality, as follows:
acfrn ? ratio =
1
|Non?Sim|
?
w,v?Non?Sim acfrn(w,v)
1
|Sim|
?
w,v?Sim acfrn(w,v)
(8)
As an illustration, the two word pairs in Table 7 yielded acfr10(country, state) = 78
and acfr10(country, party) = 64. Both values are quite high, showing no principal differ-
ence between the tighter lexically entailing similarity versus a pair of non-similar (or
rather loosely related) words. This behavior indicates the deficiency of the MI feature
weighting function in this case. On the other hand, the corresponding values for the
two pairs produced by the Bootstrapped LIN method (for the features in Table 9) are
acfr10(country, state) = 12 and acfr10(country, party) = 41. These figures clearly reflect the
desired distinction between similar and non-similar words, showing that the common
features of the similar words are indeed concentrated at much higher ranks in the
vectors than the common features of the non-similar words.
In recent work on distributional similarity (Curran 2004; Weeds and Weir 2005) a
variety of alternative weighting functions were compared. However, the quality of these
weighting functions was evaluated only through their impact on the performance of
a particular word similarity measure, as we did in Section 5. Our acfr-ratio measure
provides the first attempt to analyze the quality of weighting functions directly, relative
to a pre-judged word similarity set, without reference to a concrete similarity measure.
6.3 An Empirical Assessment of the acfr-ratio
In this subsection we report an empirical comparison of the acfr-ratio obtained for theMI
and BootstrappedLIN weighting functions. To that end, we have run the Minipar system
on the full Reuters RCV1 corpus, which contains 2.5 GB of English news stories, and
then calculated theMI-weighted feature vectors. The optimized threshold on the feature
weights, ?weight, was set to 0.2. Further, to compute the Bootstrapped LIN feature weights
a ?sim of 0.02 was applied to the LIN similarity values. In this experiment we employed
the full bootstrapped vectors (i.e., without applying feature reduction by the top 100
cutoff). This was done to avoid the effect of the feature vector size on the acfrn metric,
which tends to naturally assign higher scores to shorter vectors.
As computing the acfr-ratio requires a pre-judged sample of candidate word simi-
larity pairs, we utilized the annotated test sample of candidate pairs of word similarities
described in Section 5, which contains both entailing and non-entailing pairs.
First, we computed the average common-feature rank scores (acfrn) (with varying
values of n) forweightMI and forweight
B over all the pairs in the test sample. Interestingly,
the mean acfrn scores for weight
B range within 110?264 for n = 10. . . 100, while the
corresponding range for weightMI is by an order of magnitude higher: 780?1,254, despite
the insignificant differences in vector sizes. Therefore, we conclude that the common
features that are relevant to establishing distributional similarity in general (regardless
of entailment) are much more scattered across the vectors by MI weighting, while with
bootstrapping they tend to appear at higher positions in the vectors. These figures
454
Zhitomirsky-Geffet and Dagan Bootstrapping Distributional Feature Vector Quality
reflect a desired behavior of the bootstrapping function which concentrates most of the
prominent common features for all the distributionally similar words (whether entailing
or not) at the lower ranks of their vectors. In particular, this explains the ability of our
method to perform a massive feature reduction as demonstrated in Section 5, and to
produce more informative vectors, while demoting and eliminating much of the noise
in the original vectors.
Next, we aim to measure the discriminative power of the compared methods to
distinguish between entailing and non-entailing pairs. To this end we calculated the
acfr-ratio, which captures the difference in the average common feature ranks between
entailing vs. non-entailing pairs, for both the MI-based and bootstrapped vectors.
The obtained results are presented in Figure 2. As can be seen the acfr-ratio values
are consistently higher for Bootstrapped LIN than for MI. That is, the bootstrapping
method assigns much higher acfrn scores to entailing words than to non-entailing ones,
whereas for MI the corresponding acfrn scores for entailing and non-entailing pairs are
roughly equal. In particular, we notice that the largest gaps in acfr-ratio occur for lower
numbers of top common features, whose weights are indeed the most important and
influential in distributional similarity measures. Thus, these findings suggest a direct
indication of an improved quality of the bootstrapped feature vectors.
7. A Pseudo-Word Sense Disambiguation Evaluation
The lexical entailment evaluation reported herein corresponds to the lexical substitution
application of distributional similarity. The other type of application, as reviewed in the
Introduction, is similarity-based prediction of word co-occurrence likelihood, needed
for disambiguation applications. Comparative evaluations of distributional similarity
methods for this type of application were commonly conducted using a pseudo-word
sense disambiguation scheme, which is replicated here. In the next subsections we first
describe how distributional similarity can help improve word sense disambiguation
(WSD). Then we describe how the pseudo-word sense disambiguation task, which
Figure 2
Comparison between the acfr-ratio for MI and Bootstrapped LIN methods, when using varying
numbers of common top-ranked features in the words? feature vectors.
455
Computational Linguistics Volume 35, Number 3
corresponds to the general WSD setting, was used to evaluate the co-occurrence like-
lihood predictions obtained by alternative similarity methods.
7.1 Similarity Modeling for Word Sense Disambiguation
WSD methods need to identify the correct sense of an ambiguous word in a given
context. For example, a test instance for the verb save might be presented in the con-
text saving Private Ryan. The disambiguation method must decide whether save in this
particular context means rescue, preserve, keep, lay aside, or some other alternative.
Sense recognition is typically based on context features collected from a sense-
annotated training corpus. For example, the system might learn from the annotated
training data that the word soldier is a typical object for the rescuing sense of save, as in:
They saved the soldier. In this setting, distributional similarity is used to reduce the data
sparseness problem via similarity-based generalization. The general idea is to predict
the likelihood of unobserved word co-occurrences based on observed co-occurrences
of distributionally similar words. For example, assume that the noun private did not
occur as a direct object of save in the training data. Yet, some of the words that are
distributionally similar to private, like soldier or sergeant, might have occurred with save.
Thus, a WSD system may infer that the co-occurrence save private is more likely for the
rescuing sense of save because private is distributionally similar to soldier, which did co-
occur with this sense of save in the annotated training corpus. In general terms, theWSD
method estimates the co-occurrence likelihood for the target sense and a given context
word based on training data for words that are distributionally similar to the context
word.
This idea of similarity-based estimation of co-occurrence likelihood was applied
in Dagan, Marcus, and Markovitch (1995) to enhance WSD performance in machine
translation and recently in Gliozzo, Giuliano, and Strapparava (2005), who employed a
Latent Semantic Analysis (LSA)-based kernel function as a similarity-based represen-
tation for WSD. Other works employed the same idea for pseudo-word sense dis-
ambiguation, as explained in the next subsection.
7.2 The Pseudo-Word Sense Disambiguation Setting
Sense disambiguation typically requires annotated training data, created with consid-
erable human effort. Yarowsky (1992) suggested that when using WSD as a test bed
for comparative algorithmic evaluation it is possible to set up a pseudo-word sense
disambiguation scheme. This scheme was later adopted in several experiments, and
was popular for comparative evaluations of similarity-based co-occurrence likelihood
estimation (Dagan, Lee, and Pereira 1999; Lee 1999; Weeds andWeir 2005). We followed
closely the same experimental scheme, as described subsequently.
First, a list of pseudo-words is constructed by ?merging? pairs of words into a single
pseudo word. In our experiment each pseudo-word constitutes a pair of randomly
chosen verbs, (v, v?), where each verb represents an alternative ?sense? of the pseudo-
word. The two verbs are chosen to have almost identical probability of occurrence,
which avoids a word frequency bias on the co-occurrence likelihood predictions.
Next, we consider occurrences of pairs of the form ?n, (v, v?)? , where (v, v?) is a
pseudo-word and n is a noun representing the object of the pseudo-word. Such pairs
are constructed from all co-occurrences of either v or v? with the object n in the corpus.
For example, given the pseudo-word (rescue, keep) and the verb?object co-occurrence in
the corpus rescue?private we construct the pair ?private, (rescue, keep)?. Given such a test
456
Zhitomirsky-Geffet and Dagan Bootstrapping Distributional Feature Vector Quality
pair, the disambiguation task is to decide which of the two verbs is more likely to co-
occur with the given object noun, aiming to recover the original verb from which this
pair was constructed. In this example we would like to predict that rescue is more likely
to co-occur with private as an object than keep.
In our experiment 80% of the constructed pairs were used for training, providing
the co-occurrence statistics for the original known verb in each pair (i.e., either ?n, v?
or ?n, v??). From the remaining 20% of the pairs those occurring in the training corpus
were discarded, leaving as a test set only pairs which do not appear in the training part.
Thus, predicting the co-occurrence likelihood of the noun with each of the two verbs
cannot rely on direct frequency estimation for the co-occurrences, but rather only on
similarity-based information.
To make the similarity-based predictions we first compute the distributional sim-
ilarity scores for all pairs of nouns based on the training set statistics, where the co-
occurring verbs serve as the features in the distributional vectors of the nouns. Then,
given a test pair ?(v,v?), n? our task is to predict which of the two verbs is more likely
to co-occur with n. This verb is thus predicted as being the original verb from which
the pair was constructed. To this end, the noun n is substituted in turn with each of its
k distributionally most similar nouns, ni, and then both of the obtained ?similar? pairs
?ni, v? and ?ni, v?? are sought in the training set.
Next, wewould like to predict that the more likely co-occurrence between ?n, v? and
?n, v?? is the one for which more pairs of similar words were found in the training set.
Several approaches were used in the literature to quantify this decision procedure and
we have followed the most recent one from Weeds and Weir (2005). Each similar noun
ni is given a vote, which is equal to the difference between the frequencies of the two
co-occurrences (ni, v) and (ni, v
?), and which it casts to the verb with which it co-occurs
more frequently. The votes for each of the two verbs are summed over all k similar
nouns ni and the one with most votes wins. The winning verb is considered correct if it
is indeed the original verb from which the pair was constructed, and a tie is recorded
if the votes for both verbs are equal. Finally, the overall performance of the prediction
method is calculated by its error rate:
error = 1
T
(#of incorrect choices+
#of ties
2
) (9)
where T is the number of test instances.
In the experiment, we used the 1,000 most frequent nouns in our subset of the
Reuters corpus (of Section 5.1). The training and test data were created as described
herein, using the Minipar parser (Lin 1993) to produce verb?object co-occurrence pairs.
The k = 40 most similar nouns for each test noun were computed by each of the three
examined similarity measures LIN, WJ, and COS (as in Section 5), with and without
bootstrapping. The six similarity lists were utilized in turn for the pseudo-word sense
disambiguation task, calculating the corresponding error rate.
7.3 Results
Table 11 shows the error rate improvements after applying the bootstrapped weighting
for each of the three similarity measures. The largest error reduction, by over 15%, was
obtained for the LIN method, with quite similar results for WJ. This result is better than
the one reported by Weeds and Weir (2005), who achieved about 6% error reduction
compared to LIN.
457
Computational Linguistics Volume 35, Number 3
Table 11
The comparative error rates of the pseudo-disambiguation task for the three examined similarity
measures, with and without applying the bootstrapped weighting for each of them.
Measure LIN?LINB WJ?WJB COS?COSB
Error rate 0.157?0.133 0.150?0.132 0.155?0.145
This experiment shows that learning tighter semantic similarities, based on the im-
proved bootstrapped feature vectors, correlates also with better similarity-based infer-
ence for co-occurrence likelihood prediction. Furthermore, we have seen once again that
the bootstrapping scheme does not depend on a specific similarity measure, reducing
the error rates for all three measures.
8. Conclusions
The primary contribution of this article is the proposal of a bootstrapping method
that substantially improves the quality of distributional feature vectors, as needed for
statistical word similarity. The main idea is that features which are common for similar
words are also most characteristic for their meanings and thus should be promoted. In
fact, beyond its intuitive appeal, this idea corresponds to the underlying rationale of
the distributional similarity scheme: Semantically similar words are expected to share
exactly those context features which are most characteristic for their meaning.
The superior empirical performance of the resulting vectors was assessed in the
context of the two primary applications of distributional word similarity. The first is
lexical substitution, which was represented in our work by a human gold standard
for the substitutable lexical entailment relation. The second is co-occurrence likelihood
prediction, which was assessed by the automatically computed scores of the common
pseudo-word sense disambiguation evaluation. An additional outcome of the improved
feature weighting is massive feature reduction.
Experimenting with three prominent similarity measures showed that the boot-
strapping scheme is robust and performs well when applied over different measures.
Notably, our experiments show that the underlying assumption behind the boot-
strapping scheme is valid, that is, available similarity metrics do provide a reason-
able approximation of the semantic similarity space which can be then exploited via
bootstrapping.
The methodology of our investigation has yielded several additional contributions:
1. Utilizing a refined definition of substitutable lexical entailment both as an
end goal and as an analysis vehicle for distributional similarity. It was
shown that the refined definition can be judged directly by human subjects
with very good agreement. Overall, lexical entailment is suggested as a
useful model for lexical substitution needs in semantic-oriented
applications.
2. A thorough error analysis of state of the art distributional similarity
performance was conducted. The main observation was deficient quality
of the feature vectors, which reduces the eventual quality of similarity
measures.
458
Zhitomirsky-Geffet and Dagan Bootstrapping Distributional Feature Vector Quality
3. Inspired by the qualitative analysis, we proposed a new analytic measure
for feature vector quality, namely average common-feature rank ratio
(acfr-ratio), which is based on the common ranks of the features for pairs of
words. This measure estimates the ability of a feature weighting method to
distinguish between pairs of similar vs. non-similar words. To the best of
our knowledge this is the first proposed measure for direct analysis of the
quality of feature weighting functions, without the need to employ them
within some vector similarity measure.
The ability to identify the most characteristic features of words can have additional ben-
efits, beyond their impact on traditional word similarity measures (as evaluated in this
article). A demonstration of such potential appears in Geffet and Dagan (2005), which
presents a novel feature inclusion scheme for vector comparison. That scheme utilizes
our bootstrapping method to identify the most characteristic features of a word and
then tests whether these particular features co-occur also with a hypothesized entailed
word. The empirical success reported in that paper provides additional evidence for the
utility of the bootstrapping method.
More generally, our motivation and methodology can be extended in several di-
rections by future work on acquiring lexical entailment or other lexical-semantic rela-
tions. One direction is to explore better vector comparison methods that will utilize
the improved feature weighting, as shown in Geffet and Dagan (2005). Another direc-
tion is to integrate distributional similarity and pattern-based acquisition approaches,
which were shown to provide largely complementary information (Mirkin, Dagan, and
Geffet 2006). An additional potential is to integrate automatically acquired relationships
with the information found in WordNet, which seems to suffer from several serious
limitations (Curran 2005), and typically overlaps to a rather limited extent with the
output of automatic acquisition methods. As a parallel direction, future research should
explore in detail the impact of different lexical-semantic acquisition methods on text
understanding applications.
Finally, our proposed bootstrapping scheme seems to have a general appeal for
improving feature vector quality in additional unsupervised settings. We thus hope that
this idea will be explored further in other NLP and machine learning contexts.
References
Adams, Rod. 2006. Textual entailment
through extended lexical overlap. In
Proceedings of the Second PASCAL Challenges
Workshop on Recognising Textual Entailment,
pages 68?73, Venice.
Bar-Haim, Roy, Ido Dagan, Bill Dolan,
Lisa Ferro, Danilo Giampiccolo, Bernardo
Magnini, and Idan Szpektor. 2006. The
second PASCAL recognising textual
entailment challenge. In Proceedings of the
Second PASCAL Challenges Workshop on
Recognising Textual Entailment, pages 1?9,
Venice.
Baroni, Marco and S. Vegnaduzzo. 2004.
Identifying subjective adjectives through
web-based mutual information. In
Proceedings of KONVENS?04, pages 17?24,
Vienna.
Barzilay, Regina and Kathleen McKeown.
2001. Extracting paraphrases from a
parallel corpus. In Proceedings of ACL /
EACL?01, pages 50?57, Toulouse.
Caraballo, Sharon A. 1999. Automatic
construction of a hypernym-labeled noun
hierarchy from text. In Proceedings of
ACL?99, pages 120?126, College Park, MD.
Chklovski, Timothy and Patrick Pantel.
2004. VerbOcean: Mining the web for
fine-grained Semantic Verb Relations. In
Proceedings of EMNLP?04, pages 33?40,
Barcelona.
Church, Kenneth W. and Hanks Patrick.
1990. Word association norms, mutual
information, and lexicography.
Computational Linguistics, 16(1):22?29.
Curran, James R. 2004. From Distributional
to Semantic Similarity. Ph.D. Thesis,
459
Computational Linguistics Volume 35, Number 3
School of Informatics of the University
of Edinburgh, Scotland.
Curran, James R. 2005. Supersense tagging of
unknown nouns using semantic similarity.
In Proceedings of ACL?2005, pages 26?33,
Ann Arbor, MI.
Curran, James R. and Marc Moens. 2002.
Improvements in automatic thesaurus
extraction. In Proceedings of the Workshop
on Unsupervised Lexical Acquisition,
pages 59?67, Philadelphia, PA.
Dagan, Ido. 2000. Contextual Word
Similarity. In Rob Dale, Hermann Moisl,
and Harold Somers, editors, Handbook
of Natural Language Processing. Marcel
Dekker Inc, Chapter 19, pages 459?476,
New York, NY.
Dagan, Ido, Oren Glickman, and Bernardo
Magnini. 2006. The PASCAL recognising
textual entailment challenge. Lecture Notes
in Computer Science, 3944:177?190.
Dagan, Ido, Lillian Lee, and Fernando
Pereira. 1999. Similarity-based models of
co-occurrence probabilities. Machine
Learning, 34(1-3):43?69.
Dagan, Ido, Shaul Marcus, and Shaul
Markovitch. 1995. Contextual word
similarity and estimation from sparse
data. Computer, Speech and Language,
9:123?152.
Dunning, Ted E. 1993. Accurate methods for
the statistics of surprise and coincidence.
Computational Linguistics, 19(1):61?74.
Essen, U., and V. Steinbiss. 1992.
Co-occurrence smoothing for stochastic
language modeling. In ICASSP?92,
1:161?164, Piscataway, NJ.
Fellbaum, Christiane, editor. 1998. WordNet:
An Electronic Lexical Database. MIT Press,
Cambridge, MA.
Ferrandez, O., R. M. Terol, R. Munoz, P.
Martinez-Barco, and M. Palomar. 2006.
An approach based on logic forms and
WordNet relationships to textual
entailment performance. In Proceedings
of the Second PASCAL Challenges
Workshop on Recognising Textual Entailment,
pages 22?26, Venice.
Gasperin, Caroline and Renata Vieira. 2004.
Using word similarity lists for resolving
indirect anaphora. In Proceedings of
ACL?04 Workshop on Reference Resolution,
pages 40?46, Barcelona.
Gauch, Susan, J. Wang, and S. Mahesh
Rachakonda. 1999. A corpus analysis
approach for automatic query expansion
and its extension to multiple databases.
ACM Transactions on Information Systems
(TOIS), 17(3):250?269.
Geffet, Maayan. 2006. Refining the
Distributional Similarity Scheme for Lexical
Entailment. Ph.D. Thesis. School of
Computer Science and Engineering,
Hebrew University, Jerusalem, Israel.
Geffet, Maayan and Ido Dagan. 2004. Feature
vector quality and distributional similarity.
In Proceedings of COLING?04, Article
number: 247, Geneva.
Geffet, Maayan and Ido Dagan. 2005. The
distributional inclusion hypotheses and
lexical entailment. In Proceedings of
ACL?05, pages 107?114, Ann Arbor, MI.
Gliozzo, Alfio, Claudio Giuliano, and Carlo
Strapparava. 2005. Domain kernels for
word sense disambiguation. In Proceedings
of ACL?05, pages 403?410, Ann Arbor, MI.
Grefenstette, Gregory. 1994. Exploration in
Automatic Thesaurus Discovery. Kluwer
Academic Publishers, Norwell, MA.
Harris, Zelig S. 1968. Mathematical structures
of language. Wiley, New Jersey.
Hindle, D. 1990. Noun classification from
predicate-argument structures. In
Proceedings of ACL?90, pages 268?275,
Pittsburgh, PA.
Jijkoun, Valentin and Maarten de Rijke. 2005.
Recognizing textual entailment: Is word
similarity enough? In Joaquin Quinonero
Candela, Ido Dagan, Bernardo Magnini,
and Florence d?Alche-Buc, editors, Machine
Learning Challenges, Evaluating Predictive
Uncertainty, Visual Object Classification
and Recognizing Textual Entailment, First
PASCAL Machine Learning Challenges
Workshop, MLCW 2005, Southampton, UK,
Lecture Notes in Computer Science 3944,
pages 449?460, Springer, New York, NY.
Karov, Y. and S. Edelman. 1996.
Learning similarity-based word sense
disambiguation from sparse data.
In E. Ejerhed and I. Dagan, editors,
Fourth Workshop on Very Large Corpora.
Association for Computational Linguistics,
Somerset, NJ, pages 42?55.
Landis, J. R. and G. G. Koch. 1997. The
measurements of observer agreement for
categorical data. Biometrics, 33:159?174.
Lee, Lillian. 1997. Similarity-Based Approaches
to Natural Language Processing. Ph.D. thesis,
Harvard University, Cambridge, MA.
Lee, Lillian.1999. Measures of distributional
similarity. In Proceedings of ACL?99,
pages 25?32, College Park, MD.
Lin, Dekang. 1993. Principle-based parsing
without overgeneration. In Proceedings of
ACL?93, pages 112?120, Columbus, OH.
Lin, Dekang. 1998. Automatic retrieval and
clustering of similar words. In Proceedings
460
Zhitomirsky-Geffet and Dagan Bootstrapping Distributional Feature Vector Quality
of COLING/ACL?98, pages 768?774,
Montreal.
Lin, Dekang and Patrick Pantel. 2001.
Discovery of inference rules for question
answering. Natural Language Engineering,
7(4):343?360.
Luk, Alpha K. 1995. Statistical sense
disambiguation with relatively small
corpora using dictionary definitions.
In Proceedings of ACL?95, pages 181?188,
Cambridge, MA.
Manning, Christopher D. and Hinrich
Schutze. 1999. Foundations of Statistical
Natural Language Processing. MIT Press,
Cambridge, MA.
Mirkin, Shachar, Ido Dagan, and Maayan
Geffet. 2006. Integrating pattern-based
and distributional similarity methods
for lexical entailment acquisition. In
Proceedings of the COLING/ACL?06 Main
Conference Poster Sessions, pages 579?586,
Sydney.
Ng, H. T. 1997. Exemplar-based word
sense disambiguation: Some recent
improvements. In Proceedings of
EMNLP? 97, pages 208?213,
Providence, RI.
Ng, H. T. and H. B. Lee. 1996. Integrating
multiple knowledge sources to
disambiguate word sense: An
exemplar-based approach. In
Proceedings of ACL?1996, pages 40?47,
Santa Cruz, CA.
Nicholson, Jeremy, Nicola Stokes, and
Timothy Baldwin. 2006. Detecting
entailment using an extended
implementation of the basic elements
overlap metric. In Proceedings of the
Second PASCAL Challenges Workshop
on Recognising Textual Entailment,
pages 122?127, Venice.
Pado, Sebastian and Mirella Lapata. 2007.
Dependency-based construction of
semantic space models. Computational
Linguistics, 33(2):161?199.
Pantel, Patrick and Deepak Ravichandran.
2004. Automatically labeling semantic
classes. In Proceedings of HLT/NAACL?04,
pages 321?328, Boston, MA.
Pantel, Patrick, D. Ravichandran, and
E. Hovy. 2004. Towards terascale
knowledge acquisition. In Proceedings of
COLING?04, Article number: 771, Geneva.
Pekar, Viktor, M. Krkoska, and S. Staab.
2004. Feature weighting for
co-occurrence-based classification
of Words. In Proceedings of COLING?04,
Article number: 799, Geneva.
Pereira, Fernando, Naftali Tishby, and
Lillian Lee. 1993. Distributional
clustering of English words. In
Proceedings of ACL?93, pages 183?190,
Colombus, OH.
Ruge, Gerda. 1992. Experiments on
linguistically-based term associations.
Information Processing & Management,
28(3):317?332.
Salton, G. and M. J. McGill. 1983.
Introduction to Modern Information
Retrieval. McGraw-Hill, New York, NY.
Szpektor, Idan, H. Tanev, Ido Dagan, and
B. Coppola. 2004. Scaling web-based
acquisition of entailment relations. In
Proceedings of EMNLP?04, pages 41?48,
Barcelona.
Vanderwende, Lucy, Arul Menezes, and Rion
Snow. 2006. Microsoft research at RTE-2:
Syntactic contributions in the entailment
task: An implementation. In Proceedings
of the Second PASCAL Challenges Workshop
on Recognising Textual Entailment,
pages 27?32, Venice.
Weeds, Julie and David Weir. 2005.
Co-occurrence retrieval: A flexible
framework for lexical distributional
similarity. Computational Linguistics,
31(4):439?476.
Weeds, Julie, D. Weir, and D. McCarthy.
2004. Characterizing measures of lexical
distributional similarity. In Proceedings
of COLING?04, pages 1015?1021,
Switzerland.
Widdows, D. 2003. Unsupervised methods
for developing taxonomies by combining
syntactic and statistical information.
In Proceedings of HLT/NAACL 2003,
pages 197?204, Edmonton.
Yarowsky, D. 1992. Word-sense
disambiguation using statistical models
of Roget?s categories trained on large
corpora. In Proceedings of COLING?92,
pages 454?460, Nantes.
461

Proceedings of NAACL HLT 2009: Short Papers, pages 33?36,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Text Categorization from Category Name via Lexical Reference
Libby Barak
Department of Computer Science
University of Toronto
Toronto, Canada M5S 1A4
libbyb@cs.toronto.edu
Ido Dagan and Eyal Shnarch
Department of Computer Science
Bar-Ilan University
Ramat-Gan 52900, Israel
{dagan, shey}@cs.biu.ac.il
Abstract
Requiring only category names as user input
is a highly attractive, yet hardly explored, set-
ting for text categorization. Earlier bootstrap-
ping results relied on similarity in LSA space,
which captures rather coarse contextual sim-
ilarity. We suggest improving this scheme by
identifying concrete references to the category
name?s meaning, obtaining a special variant of
lexical expansion.
1 Introduction
Topical Text Categorization (TC), the task of clas-
sifying documents by pre-defined topics, is most
commonly addressed as a supervised learning task.
However, the supervised setting requires a substan-
tial amount of manually labeled documents, which
is often impractical in real-life settings.
Keyword-based TC methods (see Section 2) aim
at a more practical setting. Each category is rep-
resented by a list of characteristic keywords, which
should capture the category meaning. Classifica-
tion is then based on measuring similarity between
the category keywords and the classified documents,
typically followed by a bootstrapping step. The
manual effort is thus reduced to providing a key-
word list per category, which was partly automated
in some works through clustering.
The keyword-based approach still requires non-
negligible manual work in creating a representative
keyword list per category. (Gliozzo et al, 2005)
succeeded eliminating this requirement by using the
category name alone as the initial keyword, yet ob-
taining superior performance within the keyword-
based approach. This was achieved by measur-
ing similarity between category names and docu-
ments in Latent Semantic space (LSA), which im-
plicitly captures contextual similarities for the cate-
gory name through unsupervised dimensionality re-
duction. Requiring only category names as user in-
put seems very attractive, particularly when labeled
training data is too costly while modest performance
(relative to supervised methods) is still useful.
The goal of our research is to further improve the
scheme of text categorization from category name,
which was hardly explored in prior work. When an-
alyzing the behavior of the LSA representation of
(Gliozzo et al, 2005) we noticed that it captures
two types of similarities between the category name
and document terms. One type regards words which
refer specifically to the category name?s meaning,
such as pitcher for the category Baseball. How-
ever, typical context words for the category which do
not necessarily imply its specific meaning, like sta-
dium, also come up as similar to baseball in LSA
space. This limits the method?s precision, due to
false-positive classifications of contextually-related
documents that do not discuss the specific category
topic (such as other sports documents wrongly clas-
sified to Baseball). This behavior is quite typical
for query expansion methods, which expand a query
with contextually correlated terms.
We propose a novel scheme that models sepa-
rately these two types of similarity. For one, it
identifies words that are likely to refer specifically
to the category name?s meaning (Glickman et al,
2006), based on certain relations in WordNet and
33
Wikipedia. In tandem, we assess the general contex-
tual fit of the category topic using an LSA model,
to overcome lexical ambiguity and passing refer-
ences. The evaluations show that tracing lexical
references indeed increases classification precision,
which in turn improves the eventual classifier ob-
tained through bootstrapping.
2 Background: Keyword-based Text
Categorization
The majority of keyword-based TC methods fit the
general bootstrapping scheme outlined in Figure 1,
which is cast in terms of a vector-space model. The
simplest version for step 1 is manual generation of
the keyword lists (McCallum and Nigam, 1999).
(Ko and Seo, 2004; Liu et al, 2004) partly auto-
mated this step, using clustering to generate candi-
date keywords. These methods employed a standard
term-space representation in step 2.
As described in Section 1, the keyword list in
(Gliozzo et al, 2005) consisted of the category name
alone. This was accompanied by representing the
category names and documents (step 2) in LSA
space, obtained through cooccurrence-based dimen-
sionality reduction. In this space, words that tend
to cooccur together, or occur in similar contexts, are
represented by similar vectors. Thus, vector similar-
ity in LSA space (in step 3) captures implicitly the
similarity between the category name and contextu-
ally related words within the classified documents.
Step 3 yields an initial similarity-based classifi-
cation that assigns a single (most similar) category
to each document, with Sim(c, d) typically being
the cosine between the corresponding vectors. This
classification is used, in the subsequent bootstrap-
ping step, to train a standard supervised classifier
(either single- or multi-class), yielding the eventual
classifier for the category set.
3 Integrating Reference and Context
Our goal is to augment the coarse contextual simi-
larity measurement in earlier work with the identifi-
cation of concrete references to the category name?s
meaning. We were mostly inspired by (Glickman et
al., 2006), which coined the term lexical reference
to denote concrete references in text to the specific
meaning of a given term. They further showed that
Input: set of categories and unlabeled documents
Output: a classifier
1. Acquire a keyword list per category
2. Represent each category c and document d
as vectors in a common space
3. For each document d
CatSim(d) = argmaxc(Sim(c, d))
4. Train a supervised classifier on step (3) output
Figure 1: Keyword-based categorization scheme
Category name WordNet Wikipedia
Cryptography decipher digital signature
Medicine cardiology biofeedback, homeopathy
Macintosh Apple Mac, Mac
Motorcycle bike, cycle Honda XR600
Table 1: Referring terms from WordNet and Wikipedia
an entailing text (in the textual entailment setting)
typically includes a concrete reference to each term
in the entailed statement. Analogously, we assume
that a relevant document for a category typically in-
cludes concrete terms that refer specifically to the
category name?s meaning.
We thus extend the scheme in Figure 1 by cre-
ating two vectors per category (in steps 1 and 2): a
reference vector ~cref in term space, consisting of re-
ferring terms for the category name; and a context
vector ~ccon, representing the category name in LSA
space, as in (Gliozzo et al, 2005). Step 3 then com-
putes a combined similarity score for categories and
documents based on the two vectors.
3.1 References to category names
Referring terms are collected from WordNet and
Wikipedia, by utilizing relations that are likely to
correspond to lexical reference. Table 1 illustrates
that WordNet provides mostly referring terms of
general terminology while Wikipedia provides more
specific terms. While these resources were used pre-
viously for text categorization, it was mostly for en-
hancing document representation in supervised set-
tings, e.g. (Rodr??guez et al, 2000).
WordNet. Referring terms were found in Word-
Net starting from relevant senses of the category
name and transitively following relation types that
correspond to lexical reference. To that end, we
34
specified for each category name those senses which
fit the category?s meaning, such as the outer space
sense for the category Space.1
A category name sense is first expanded by its
synonyms and derivations, all of which are then ex-
panded by their hyponyms. When a term has no
hyponyms it is expanded by its meronyms instead,
since we observed that in such cases they often spec-
ify unique components that imply the holonym?s
meaning, such as Egypt for Middle East. However,
when a term is not a leaf in the hyponymy hierarchy
then its meronyms often refer to generic sub-parts,
such as door for car. Finally, the hyponyms and
meronyms are expanded by their derivations. As
a common heuristic, we considered only the most
frequent senses (top 4) of referring terms, avoiding
low-ranked (rare) senses which are likely to intro-
duce noise.
Wikipedia. We utilized a subset of a lexical ref-
erence resource extracted from Wikipedia (anony-
mous reference). For each category name we ex-
tracted referring terms of two types, capturing hy-
ponyms and synonyms. Terms of the first type are
Wikipedia page titles for which the first definition
sentence includes a syntactic ?is-a? pattern whose
complement is the category name, such as Chevrolet
for the category Autos. Terms of the second type
are extracted from Wikipedia?s redirect links, which
capture synonyms such as x11 for Windows-X.
The reference vector ~cref for a category consists
of the category name and all its referring terms,
equally weighted. The corresponding similarity
function is Simref (c, d) = cos(~cref , ~dterm)), where
~dterm is the document vector in term space.
3.2 Incorporating context similarity
Our key motivation is to utilize Simref as the ba-
sis for classification in step 3 (Figure 1). However,
this may yield false positive classifications in two
cases: (a) inappropriate sense of an ambiguous re-
ferring term, e.g., the narcotic sense of drug should
not yield classification to Medicine; (b) a passing
reference, e.g., an analogy to cars in a software doc-
ument, should not yield classification to Autos.
1We assume that it is reasonable to specify relevant senses
as part of the typically manual process of defining the set of
categories and their names. Otherwise, when expanding names
through all their senses F1-score dropped by about 2%.
In both these cases the overall context in the docu-
ment is expected to be atypical for the triggered cat-
egory. We therefore measure the contextual similar-
ity between a category c and a document d utilizing
LSA space, replicating the method in (Gliozzo et
al., 2005): ~ccon and ~dLSA are taken as the LSA vec-
tors of the category name and the document, respec-
tively, yielding Simcon(c, d) = cos(~ccon, ~dLSA)).2
The overall similarity score of step 3 is de-
fined as Sim(c, d) = Simref (c, d) ? Simcon(c, d).
This formula fulfils the requirement of finding at
least one referring term in the document; otherwise
Simref (c, d) would be zero. Simcon(c, d) is com-
puted in the reduced LSA space and is thus prac-
tically non-zero, and would downgrade Sim(c, d)
when there is low contextual similarity between the
category name and the document. Documents for
which Sim(c, d) = 0 for all categories are omitted.
4 Results and Conclusions
We tested our method on the two corpora used in
(Gliozzo et al, 2005): 20-NewsGroups, classified
by a single-class scheme (single category per doc-
ument), and Reuters-10 3, of a multi-class scheme.
As in their work, non-standard category names were
adjusted, such as Foreign exchange for Money-fx.
4.1 Initial classification
Table 2 presents the results of the initial classifica-
tion (step 3). The first 4 lines refer to classification
based on Simref alone. As a baseline, including
only the category name in the reference vector (Cat-
Name) yields particularly low recall. Expansion by
WordNet is notably more powerful than by the auto-
matically extracted Wikipedia resource; still, the lat-
ter consistently provides a small marginal improve-
ment when using both resources (Reference), indi-
cating their complementary nature.
As we hypothesized, the Reference model
achieves much better precision than the Context
model from (Gliozzo et al, 2005) alone (Simcon).
For 20-NewsGroups the recall of Reference is lim-
ited, due to partial coverage of our current expansion
2The original method includes a Gaussian Mixture re-
scaling step for Simcon, which wasn?t found helpful when
combined with Simref (as specified next).
310 most frequent categories in Reuters-21578
35
Reuters-10 20 Newsgroups
Method R P F1 R P F1
CatName 0.22 0.67 0.33 0.19 0.55 0.28
WordNet 0.67 0.78 0.72 0.29 0.56 0.38
Wikipedia 0.24 0.68 0.35 0.22 0.57 0.31
Reference 0.69 0.80 0.74 0.31 0.57 0.40
Context 0.59 0.64 0.61 0.46 0.46 0.46
Combined 0.71 0.82 0.76 0.32 0.58 0.41
Table 2: Initial categorization results (step 3)
Method Feature Reuters-10 20 NGSet R P F1 F1
Reference TF-IDF 0.91 0.50 0.65 0.51LSA 0.89 0.67 0.76 0.56
Context TF-IDF 0.84 0.48 0.61 0.48LSA 0.73 0.56 0.63 0.44
Combined TF-IDF 0.92 0.50 0.65 0.52LSA 0.89 0.71 0.79 0.56
Table 3: Final bootstrapping results (step 4)
resources, yielding a lower F1. Yet, its higher pre-
cision pays off for the bootstrapping step (Section
4.2). Finally, when the two models are Combined a
small precision improvement is observed.
4.2 Final bootstrapping results
The output of step 3 was fed as standard training
for a binary SVM classifier for each category (step
4). We used the default setting for SVM-light, apart
from the j parameter which was set to the number of
categories in each data set, as suggested by (Morik
et al, 1999). For Reuters-10, classification was
determined independently by the classifier of each
category, allowing multiple classes per document.
For 20-NewsGroups, the category which yielded the
highest classification score was chosen (one-versus-
all), fitting the single-class setting. We experimented
with two document representations for the super-
vised step: either as vectors in tf-idf weighted term
space or as vectors in LSA space.
Table 3 shows the final classification results.4
First, we observe that for the noisy bootstrapping
training data LSA document representation is usu-
ally preferred. Most importantly, our Reference and
Combined models clearly improve over the earlier
4Notice that P=R=F1 when all documents are classified to
a single class, as in step 4 for 20-NewsGroups, while in step 3
some documents are not classified, yielding distinct P/R/F1.
Context. Combining reference and context yields
some improvement for Reuters-10, but not for 20-
NewsGroups. We noticed though that the actual ac-
curacy of our method on 20-NewsGroups is notably
higher than measured relative to the gold standard,
due to its single-class scheme: in many cases, a doc-
ument should truly belong to more than one cate-
gory while that chosen by our algorithm was counted
as false positive. Future research is proposed to in-
crease the method?s recall via broader coverage lexi-
cal reference resources, and to improve its precision
through better context models than LSA, which was
found rather noisy for quite a few categories.
To conclude, the results support our main contri-
bution ? the benefit of identifying referring terms for
the category name over using noisier context mod-
els alone. Overall, our work highlights the potential
of text categorization from category names when la-
beled training sets are not available, and indicates
important directions for further research.
Acknowledgments
The authors would like to thank Carlo Strapparava
and Alfio Gliozzo for valuable discussions. This
work was partially supported by the NEGEV project
(www.negev-initiative.org).
References
O. Glickman, E. Shnarch, and I. Dagan. 2006. Lexical
reference: a semantic matching subtask. In EMNLP.
A. Gliozzo, C. Strapparava, and I. Dagan. 2005. Inves-
tigating unsupervised learning for text categorization
bootstrapping. In Proc. of HLT/EMNLP.
Y. Ko and J. Seo. 2004. Learning with unlabeled data
for text categorization using bootstrapping and feature
projection techniques. In Proc. of ACL.
B. Liu, X. Li, W. S. Lee, and P. S. Yu. 2004. Text classi-
fication by labeling words. In Proc. of AAAI.
A. McCallum and K. Nigam. 1999. Text classification
by bootstrapping with keywords, EM and shrinkage.
In ACL Workshop for Unsupervised Learning in NLP.
K. Morik, P. Brockhausen, and T. Joachims. 1999. Com-
bining statistical learning with a knowledge-based ap-
proach - a case study in intensive care monitoring. In
Proc. of the 16th Int?l Conf. on Machine Learning.
M. d. B. Rodr??guez, J. M. Go?mez-Hidalgo, and B. D??az-
Agudo, 2000. Using WordNet to complement training
information in text categorization, volume 189 of Cur-
rent Issues in Linguistic Theory, pages 353?364.
36
 	
	
Proceedings of the 43rd Annual Meeting of the ACL, pages 107?114,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
The Distributional Inclusion Hypotheses and Lexical Entailment 
 
Maayan Geffet 
School of Computer Science and Engineering 
Hebrew University, Jerusalem, Israel, 91904 
mary@cs.huji.ac.il 
Ido Dagan 
Department of Computer Science 
Bar-Ilan University, Ramat-Gan, Israel, 52900 
dagan@cs.biu.ac.il 
 
Abstract 
This paper suggests refinements for the 
Distributional Similarity Hypothesis. Our 
proposed hypotheses relate the distribu-
tional behavior of pairs of words to lexical 
entailment ? a tighter notion of semantic 
similarity that is required by many NLP 
applications. To automatically explore the 
validity of the defined hypotheses we de-
veloped an inclusion testing algorithm for 
characteristic features of two words, which 
incorporates corpus and web-based feature 
sampling to overcome data sparseness. The 
degree of hypotheses validity was then em-
pirically tested and manually analyzed with 
respect to the word sense level. In addition, 
the above testing algorithm was exploited 
to improve lexical entailment acquisition. 
1 Introduction 
Distributional Similarity between words has been 
an active research area for more than a decade. It is 
based on the general idea of Harris' Distributional 
Hypothesis, suggesting that words that occur 
within similar contexts are semantically similar 
(Harris, 1968). Concrete similarity measures com-
pare a pair of weighted context feature vectors that 
characterize two words (Church and Hanks, 1990; 
Ruge, 1992; Pereira et al, 1993; Grefenstette, 
1994; Lee, 1997; Lin, 1998; Pantel and Lin, 2002; 
Weeds and Weir, 2003). 
    As it turns out, distributional similarity captures 
a somewhat loose notion of semantic similarity 
(see Table 1). It does not ensure that the meaning 
of one word is preserved when replacing it with 
the other one in some context. 
However, many semantic information-oriented 
applications like Question Answering, Information 
Extraction and Paraphrase Acquisition require a 
tighter similarity criterion, as was also demon-
strated by papers at the recent PASCAL Challenge 
on Recognizing Textual Entailment (Dagan et al, 
2005). In particular, all these applications need to 
know when the meaning of one word can be in-
ferred (entailed) from another word, so that one 
word could substitute the other in some contexts. 
This relation corresponds to several lexical seman-
tic relations, such as synonymy, hyponymy and 
some cases of meronymy. For example, in Ques-
tion Answering, the word company in a question 
can be substituted in the text by firm (synonym), 
automaker (hyponym) or division (meronym). Un-
fortunately, existing manually constructed re-
sources of lexical semantic relations, such as 
WordNet, are not exhaustive and comprehensive 
enough for a variety of domains and thus are not 
sufficient as a sole resource for application needs1. 
    Most works that attempt to learn such concrete 
lexical semantic relations employ a co-occurrence 
pattern-based approach (Hearst, 1992; Ravi-
chandran and Hovy, 2002; Moldovan et al, 2004). 
Typically, they use a set of predefined lexico-
syntactic patterns that characterize specific seman-
tic relations. If a candidate word pair (like com-
pany-automaker) co-occurs within the same 
sentence satisfying a concrete pattern (like " 
?companies, such as automakers"), then it is ex-
pected that the corresponding semantic relation 
holds between these words (hypernym-hyponym in 
this example). 
    In recent work (Geffet and Dagan, 2004) we 
explored the correspondence between the distribu-
tional characterization of two words (which may 
hardly co-occur, as is usually the case for syno-
                                                           
1We found that less than 20% of the lexical entailment relations extracted by our 
method appeared as direct or indirect WordNet relations (synonyms, hyponyms 
or meronyms). 
107
nyms) and the kind of tight semantic relationship 
that might hold between them. We formulated a 
lexical entailment relation that corresponds to the 
above mentioned substitutability criterion, and is 
termed meaning entailing substitutability (which 
we term here for brevity as lexical entailment). 
Given a pair of words, this relation holds if there 
are some contexts in which one of the words can 
be substituted by the other, such that the meaning 
of the original word can be inferred from the new 
one. We then proposed a new feature weighting 
function (RFF) that yields more accurate distribu-
tional similarity lists, which better approximate the 
lexical entailment relation. Yet, this method still 
applies a standard measure for distributional vector 
similarity (over vectors with the improved feature 
weights), and thus produces many loose similari-
ties that do not correspond to entailment. 
    This paper explores more deeply the relationship 
between distributional characterization of words 
and lexical entailment, proposing two new hy-
potheses as a refinement of the distributional simi-
larity hypothesis. The main idea is that if one word 
entails the other then we would expect that virtu-
ally all the characteristic context features of the 
entailing word will actually occur also with the 
entailed word. 
     To test this idea we developed an automatic 
method for testing feature inclusion between a pair 
of words. This algorithm combines corpus statis-
tics with a web-based feature sampling technique. 
The web is utilized to overcome the data sparse-
ness problem, so that features which are not found 
with one of the two words can be considered as 
truly distinguishing evidence.  
    Using the above algorithm we first tested the 
empirical validity of the hypotheses. Then, we 
demonstrated how the hypotheses can be leveraged 
in practice to improve the precision of automatic 
acquisition of the entailment relation. 
 
2 Background  
2.1 Implementations of Distribu-
tional  Similarity 
This subsection reviews the relevant details of ear-
lier methods that were utilized within this paper.  
In the computational setting contexts of words 
are represented by feature vectors. Each word w is 
represented by a feature vector, where an entry in 
the vector corresponds to a feature f. Each feature 
represents another word (or term) with which w co-
occurs, and possibly specifies also the syntactic 
relation between the two words as in (Grefenstette, 
1994; Lin, 1998; Weeds and Weir, 2003). Pado 
and Lapata (2003) demonstrated that using syntac-
tic dependency-based vector space models can help 
distinguish among classes of different lexical rela-
tions, which seems to be more difficult for tradi-
tional ?bag of words? co-occurrence-based models. 
A syntactic feature is defined as a triple <term, 
syntactic_relation, relation_direction> (the direc-
tion is set to 1, if the feature is the word?s modifier 
and to 0 otherwise). For example, given the word 
?company? the feature <earnings_report, gen, 0> 
(genitive) corresponds to the phrase ?company?s 
earnings report?, and <profit, pcomp, 0> (preposi-
tional complement) corresponds to ?the profit of 
the company?. Throughout this paper we used syn-
tactic features generated by the Minipar depend-
ency parser (Lin, 1993).  
    The value of each entry in the feature vector is 
determined by some weight function weight(w,f), 
which quantifies the degree of statistical associa-
tion between the feature and the corresponding 
word. The most widely used association weight 
function is (point-wise) Mutual Information (MI) 
(Church and Hanks, 1990; Lin, 1998; Dagan, 2000; 
Weeds et al, 2004). 
<=> element, component <=> gap, spread *      town, airport <=   loan, mortgage 
=>   government, body *      warplane, bomb <=> program, plan *      tank, warplane 
*      match, winner =>   bill, program <=   conflict, war =>   town, location    
Table 1: Sample of the data set of top-40 distributionally similar word pairs produced by the RFF-
based method of (Geffet and Dagan, 2004). Entailment judgments are marked by the arrow direction, 
with '*' denoting no entailment.  
108
    Once feature vectors have been constructed, the 
similarity between two words is defined by some 
vector similarity metric. Different metrics have 
been used, such as weighted Jaccard (Grefenstette, 
1994; Dagan, 2000), cosine (Ruge, 1992), various 
information theoretic measures (Lee, 1997), and 
the widely cited and competitive (see (Weeds and 
Weir, 2003)) measure of Lin (1998) for similarity 
between two words, w and v, defined as follows: 
 
  
,
),(),(
),(),(
),(
)()(
)()(
 

??
??
+
+
=
fvweightfwweight
fvweightfwweight
vwsim
vFfwFf
vFwFf
Lin
 
 
where F(w) and F(v) are the active features of the 
two words (positive feature weight) and the weight 
function is defined as MI. As typical for vector 
similarity measures, it assigns high similarity 
scores if many of the two word?s features overlap, 
even though some prominent features might be 
disjoint. This is a major reason for getting such 
semantically loose similarities, like company - 
government and country - economy. 
Investigating the output of Lin?s (1998) similar-
ity measure with respect to the above criterion in 
(Geffet and Dagan, 2004), we discovered that the 
quality of similarity scores is often hurt by inaccu-
rate feature weights, which yield rather noisy fea-
ture vectors.  Hence, we tried to improve the 
feature weighting function to promote those fea-
tures that are most indicative of the word meaning. 
A new weighting scheme was defined for boot-
strapping feature weights, termed RFF (Relative 
Feature Focus). First, basic similarities are gener-
ated by Lin?s measure. Then, feature weights are 
recalculated, boosting the weights of features that 
characterize many of the words that are most simi-
lar to the given one2. As a result the most promi-
nent features of a word are concentrated within the 
top-100 entries of the vector. Finally, word simi-
larities are recalculated by Lin's metric over the 
vectors with the new RFF weights. 
    The lexical entailment prediction task of 
(Geffet and Dagan, 2004) measures how many of 
the top ranking similarity pairs produced by the 
                                                           
2
 In concrete terms RFF is defined by: 
 ??= ),()()(),( vwsimwNfWSvfwRFF ,  
where sim(w,v) is an initial approximation of the similarity space by Lin?s 
measure, WS(f) is a set of words co-occurring with feature f, and N(w) is the set 
of the most similar words of w by Lin?s measure. 
RFF-based metric hold the entailment relation, in 
at least one direction. To this end a data set of 
1,200 pairs was created, consisting of top-N 
(N=40) similar words of 30 randomly selected 
nouns, which were manually judged by the lexical 
entailment criterion. Quite high Kappa agreement 
values of 0.75 and 0.83 were reported, indicating 
that the entailment judgment task was reasonably 
well defined. A subset of the data set is demon-
strated in Table 1.     
The RFF weighting produced 10% precision 
improvement over Lin?s original use of MI, sug-
gesting the RFF capability to promote semantically 
meaningful features. However, over 47% of the 
word pairs in the top-40 similarities are not related 
by entailment, which calls for further improve-
ment. In this paper we use the same data set 3 and 
the RFF metric as a basis for our experiments. 
2.2 Predicting  Semantic Inclusion 
Weeds et al (2004) attempted to refine the distri-
butional similarity goal to predict whether one 
term is a generalization/specification of the other. 
They present a distributional generality concept 
and expect it to correlate with semantic generality. 
Their conjecture is that the majority of the features 
of the more specific word are included in the fea-
tures of the more general one. They define the fea-
ture recall of w with respect to v as the weighted 
proportion of features of v that also appear in the 
vector of w. Then, they suggest that a hypernym 
would have a higher feature recall for its hypo-
nyms (specifications), than vice versa.  
    However, their results in predicting the hy-
ponymy-hyperonymy direction (71% precision) are 
comparable to the na?ve baseline (70% precision) 
that simply assumes that general words are more 
frequent than specific ones. Possible sources of 
noise in their experiment could be ignoring word 
polysemy and data sparseness of word-feature co-
occurrence in the corpus. 
3 The Distributional Inclusion Hy-
potheses 
In this paper we suggest refined versions of the 
distributional similarity hypothesis which relate 
distributional behavior with lexical entailment. 
                                                           
3 Since the original data set did not include the direction of entailment, we have 
enriched it by adding the judgments of entailment direction. 
109
     Extending the rationale of Weeds et al, we 
suggest that if the meaning of a word v entails an-
other word w then it is expected that all the typical 
contexts (features) of v will occur also with w. That 
is, the characteristic contexts of v are expected to 
be included within all w's contexts (but not neces-
sarily amongst the most characteristic ones for w). 
Conversely, we might expect that if v's characteris-
tic contexts are included within all w's contexts 
then it is likely that the meaning of  v does entail 
w. Taking both directions together, lexical entail-
ment is expected to highly correlate with character-
istic feature inclusion. 
     Two additional observations are needed before 
concretely formulating these hypotheses. As ex-
plained in Section 2, word contexts should be rep-
resented by syntactic features, which are more 
restrictive and thus better reflect the restrained se-
mantic meaning of the word (it is difficult to tie 
entailment to looser context representations, such 
as co-occurrence in a text window). We also notice 
that distributional similarity principles are intended 
to hold at the sense level rather than the word 
level, since different senses have different charac-
teristic contexts (even though computational com-
mon practice is to work at the word level, due to 
the lack of robust sense annotation). 
    We can now define the two distributional inclu-
sion hypotheses, which correspond to the two di-
rections of inference relating distributional feature 
inclusion and lexical entailment. Let vi and wj be 
two word senses of the words w and v, correspond-
ingly, and let vi => wj denote the (directional) en-
tailment relation between these senses. Assume 
further that we have a measure that determines the 
set of characteristic features for the meaning of 
each word sense. Then we would hypothesize: 
Hypothesis I: 
If vi => wj then all the characteristic (syntactic-
based) features of vi are expected to appear with wj.  
Hypothesis II: 
If all the characteristic (syntactic-based) features of 
vi appear with wj then we expect that vi => wj. 
4 Word Level Testing of Feature In-
clusion  
To check the validity of the hypotheses we need to 
test feature inclusion. In this section we present an 
automated word-level feature inclusion testing 
method, termed ITA (Inclusion Testing Algorithm). 
To overcome the data sparseness problem we in-
corporated web-based feature sampling. Given a 
test pair of words, three main steps are performed, 
as detailed in the following subsections:  
Step 1: Computing the set of characteristic features 
for each word. 
Step 2: Testing feature inclusion for each pair, in 
both directions, within the given corpus data.  
Step 3: Complementary testing of feature inclusion 
for each pair in the web. 
4.1 Step 1: Corpus-based generation 
of characteristic features 
To implement the first step of the algorithm, the 
RFF weighting function is exploited and its top-
100 weighted features are taken as most character-
istic for each word. As mentioned in Section 2, 
(Geffet and Dagan, 2004) shows that RFF yields 
high concentration of good features at the top of 
the vector. 
4.2 Step 2: Corpus-based feature 
inclusion test 
We first check feature inclusion in the corpus that 
was used to generate the characteristic feature sets.  
For each word pair (w, v) we first determine which 
features of w do co-occur with v in the corpus. The 
same is done to identify features of v that co-occur 
with w in the corpus. 
4.3 Step 3: Complementary Web-
based Inclusion Test 
This step is most important to avoid inclusion 
misses due to the data sparseness of the corpus. A 
few recent works (Ravichandran and Hovy, 2002; 
Keller et al, 2002; Chklovski and Pantel, 2004) 
used the web to collect statistics on word co-
occurrences. In a similar spirit, our inclusion test is 
completed by searching the web for the missing 
(non-included) features on both sides. We call this 
web-based technique mutual web-sampling. The 
web results are further parsed to verify matching of 
the feature's syntactic relationship. 
110
     We denote the subset of w's features that are 
missing for v as M(w, v) (and equivalently M(v, 
w)). Since web sampling is time consuming we 
randomly sample a subset of k features (k=20 in 
our experiments), denoted as M(v,w,k).  
Mutual Web-sampling Procedure: 
For each pair (w, v) and their k-subsets  
M(w, v, k) and M(v, w, k) execute: 
 
1.  Syntactic Filtering of ?Bag-of-Words? Search: 
 
Search the web for sentences including v and a fea-
ture f from M(w, v, k) as ?bag of words?, i. e. sen-
tences where w and f appear in any distance and in 
either order. Then filter out the sentences that do 
not match the defined syntactic relation between f 
and v (based on parsing). Features that co-occur 
with w in the correct syntactic relation are removed 
from M(w, v, k). Do the same search and filtering 
for w and features from M(v, w, k). 
2.   Syntactic Filtering of ?Exact String? Matching: 
On the missing features on both sides (which are 
left in M(w, v, k) and M(v, w, k) after stage 1), ap-
ply ?exact string? search of the web. For this, con-
vert the tuple (v, f) to a string by adding 
prepositions and articles where needed. For exam-
ple, for (element, <project, pcomp_of, 1>) gener-
ate the corresponding string ?element of the 
project? and search the web for exact matches of 
the string. Then validate the syntactic relationship 
of f and v in the extracted sentences. Remove the 
found features from M(w, v, k) and M(v, w, k), re-
spectively. 
3.   Missing Features Validation:  
Since some of the features may be too infrequent 
or corpus-biased, check whether the remaining 
missing features do co-occur on the web with their 
original target words (with which they did occur in 
the corpus data). Otherwise, they should not be 
considered as valid misses and are also removed 
from M(w, v, k) and M(v, w, k).  
Output: Inclusion in either direction holds if the 
corresponding set of missing features is now 
empty. 
We also experimented with features consisting of 
words without syntactic relations. For example, 
exact string, or bag-of-words match. However, al-
most all the words (also non-entailing) were found 
with all the features of each other, even for seman-
tically implausible combinations (e.g. a word and a 
feature appear next to each other but belong to dif-
ferent clauses of the sentence). Therefore we con-
clude that syntactic relation validation is very 
important, especially on the web, in order to avoid 
coincidental co-occurrences.  
5 Empirical Results 
To test the validity of the distributional inclusion 
hypotheses we performed an empirical analysis on 
a selected test sample using our automated testing 
procedure. 
5.1 Data and setting 
We experimented with a randomly picked test 
sample of about 200 noun pairs of 1,200 pairs pro-
duced by RFF (for details see Geffet and Dagan, 
2004) under Lin?s similarity scheme (Lin, 1998). 
The words were judged by the lexical entailment 
criterion (as described in Section 2). The original 
percentage of correct (52%) and incorrect (48%) 
entailments was preserved. 
    To estimate the degree of validity of the distri-
butional inclusion hypotheses we decomposed 
each word pair of the sample (w, v) to two direc-
tional pairs ordered by potential entailment direc-
tion: (w, v) and (v, w). The 400 resulting ordered 
pairs are used as a test set in Sections 5.2 and 5.3.   
    Features were computed from co-occurrences in 
a subset of the Reuters corpus of about 18 million 
words. For the web feature sampling the maximal 
number of web samples for each query (word - 
feature) was set to 3,000 sentences. 
5.2 Automatic Testing the Validity 
of the Hypotheses at the  Word 
Level  
The test set of 400 ordered pairs was examined in 
terms of entailment (according to the manual 
judgment) and feature inclusion (according to the 
ITA algorithm), as shown in Table 2. 
    According to Hypothesis I we expect that a pair 
(w, v) that satisfies entailment will also preserve 
feature inclusion. On the other hand, by Hypothe-
sis II if all the features of w are included by v then 
we expect that w entails v.  
111
    We observed that Hypothesis I is better attested 
by our data than the second hypothesis. Thus 86% 
(97 out of 113) of the entailing pairs fulfilled the 
inclusion condition. Hypothesis II holds for ap-
proximately 70% (97 of 139) of the pairs for which 
feature inclusion holds. In the next section we ana-
lyze the cases of violation of both hypotheses and 
find that the first hypothesis held to an almost per-
fect extent with respect to word senses.  
    It is also interesting to note that thanks to the 
web-sampling procedure over 90% of the non-
included features in the corpus were found on the 
web, while most of the missing features (in the 
web) are indeed semantically implausible. 
5.3 Manual Sense Level Testing of 
Hypotheses Validity  
Since our data was not sense tagged, the automatic 
validation procedure could only test the hypotheses 
at the word level. In this section our goal is to ana-
lyze the findings of our empirical test at the word 
sense level as our hypotheses were defined for 
senses.  Basically, two cases of hypotheses invalid-
ity were detected: 
Case 1: Entailments with non-included features 
(violation of Hypothesis I); 
Case 2: Feature Inclusion for non-entailments 
(violation of Hypothesis II).     
    At the word level we observed 14% invalid pairs 
of the first case and 30% of the second case. How-
ever, our manual analysis shows, that over 90% of 
the first case pairs were due to a different sense of 
one of the entailing word, e.g. capital - town (capi-
tal as money) and spread - gap (spread as distribu-
tion) (Table 3). Note that ambiguity of the entailed 
word does not cause errors (like town ? area, area 
as domain) (Table 3). Thus the first hypothesis 
holds at the sense level for over 98% of the cases 
(Table 4). 
    Two remaining invalid instances of the first case 
were due to the web sampling method limitations 
and syntactic parsing filtering mistakes, especially 
for some less characteristic and infrequent features 
captured by RFF. Thus, in virtually all the exam-
ples tested in our experiment Hypothesis I was 
valid. 
   We also explored the second case of invalid 
pairs: non-entailing words that pass the feature in-
clusion test. After sense based analysis their per-
centage was reduced slightly to 27.4%. Three 
possible reasons were discovered. First, there are 
words with features typical to the general meaning 
of the domain, which tend to be included by many 
other words of this domain, like valley ? town. The 
features of valley (?eastern valley?, ?central val-
ley?, ?attack in valley?, ?industry of the valley?) 
are not discriminative enough to be distinguished 
from town, as they are all characteristic to any geo-
graphic location.  
             Inclusion 
Entailment 
       +     - 
 
              +      97       16 
               -      42           245 
Table 2: Distribution of 400 entailing/non-
entailing ordered pairs that hold/do not hold 
feature inclusion at the word level.  
           Inclusion 
Entailment 
        +     - 
 
             +        111       2 
              -        42       245 
Table 4: Distribution of the entailing/non-
entailing ordered pairs that hold/do not hold 
feature inclusion at the sense level.  
spread ? gap (mutually entail each other) 
<weapon, pcomp_of> 
The Committee was discussing the Pro-
gramme of the ?Big Eight,? aimed against 
spread of weapon of mass destruction. 
town ? area (?town? entails ?area?) 
<cooperation, pcomp_for> 
This is a promising area for cooperation and 
exchange of experiences.  
capital ? town (?capital? entails ?town?) 
<flow, nn> 
Offshore financial centers affect cross-border 
capital flow in China. 
Table 3: Examples of ambiguity of entailment-
related words, where the disjoint features be-
long to a different sense of the word. 
112
    The second group consists of words that can be 
entailing, but only in a context-dependent (ana-
phoric) manner rather than ontologically. For ex-
ample, government and neighbour, while 
neighbour is used in the meaning of ?neighbouring 
(country) government?. Finally, sometimes one or 
both of the words are abstract and general enough 
and also highly ambiguous to appear with a wide 
range of features on the web, like element (vio-
lence ? element, with all the tested features of vio-
lence included by element). 
To prevent occurrences of the second case more 
characteristic and discriminative features should be 
provided. For this purpose features extracted from 
the web, which are not domain-biased (like fea-
tures from the corpus) and multi-word features 
may be helpful. Overall, though, there might be 
inherent cases that invalidate Hypothesis II. 
6 Improving Lexical Entailment Pre-
diction by ITA (Inclusion Testing 
Algorithm) 
In this section we show that ITA can be practically 
used to improve the (non-directional) lexical en-
tailment prediction task described in Section 2. 
Given the output of the distributional similarity 
method, we employ ITA at the word level to filter 
out non-entailing pairs. Word pairs that satisfy fea-
ture inclusion of all k features (at least in one direc-
tion) are claimed as entailing.  
 The same test sample of 200 word pairs men-
tioned in Section 5.1 was used in this experiment. 
The results were compared to RFF under Lin?s 
similarity scheme (RFF-top-40 in Table 5).  
 Precision was significantly improved, filtering 
out 60% of the incorrect pairs. On the other hand, 
the relative recall (considering RFF recall as 
100%) was only reduced by 13%, consequently 
leading to a better relative F1, when considering 
the RFF-top-40 output as 100% recall (Table 5). 
 Since our method removes about 35% of the 
original top-40 RFF output, it was interesting to 
compare our results to simply cutting off the 35% 
of the lowest ranked RFF words (top-26). The 
comparison to the baseline (RFF-top-26 in Table 
5) showed that ITA filters the output much better 
than just cutting off the lowest ranking similarities.  
 We also tried a couple of variations on feature 
sampling for the web-based procedure. In one of 
our preliminary experiments we used the top-k 
RFF features instead of random selection. But we 
observed that top ranked RFF features are less dis-
criminative than the random ones due to the nature 
of the RFF weighting strategy, which promotes 
features shared by many similar words. Then, we 
attempted doubling the sampling to 40 random fea-
tures. As expected the recall was slightly de-
creased, while precision was increased by over 5%. 
In summary, the behavior of ITA sampling of 
k=20 and k=40 features is closely comparable 
(ITA-20 and ITA-40 in Table 5, respectively)4.     
7 Conclusions and Future Work 
The main contributions of this paper were: 
1.  We defined two Distributional Inclusion Hy-
potheses that associate feature inclusion with lexi-
cal entailment at the word sense level. The 
Hypotheses were proposed as a refinement for 
Harris? Distributional hypothesis and as an exten-
sion to the classic distributional similarity scheme. 
2.   To estimate the empirical validity of the de-
fined hypotheses we developed an automatic inclu-
sion testing algorithm (ITA). The core of the 
algorithm is a web-based feature inclusion testing 
procedure, which helped significantly to compen-
sate for data sparseness. 
3.    Then a thorough analysis of the data behavior 
with respect to the proposed hypotheses was con-
ducted. The first hypothesis was almost fully at-
tested by the data, particularly at the sense level, 
while the second hypothesis did not fully hold.  
4.   Motivated by the empirical analysis we pro-
posed to employ ITA for the practical task of im-
proving lexical entailment acquisition. The 
algorithm was applied as a filtering technique on 
the distributional similarity (RFF) output. We ob-
                                                           
4
 The ITA-40 sampling fits the analysis from section 5.2 and 5.3 as well. 
Method Precision Recall F1 
ITA-20 0.700 0.875 0.777 
ITA-40 0.740 0.846 0.789 
RFF-top-40 0.520 1.000 0.684 
RFF-top-26 0.561 0.701 0.624 
Table 5: Comparative results of using the 
filter, with 20 and 40 feature sampling, com-
pared to RFF top-40 and RFF top-26 simi-
larities. ITA-20 and ITA-40 denote the web-
sampling method with 20 and random 40 
features, respectively. 
113
tained 17% increase of precision and succeeded to 
improve relative F1 by 15% over the baseline.      
    Although the results were encouraging our man-
ual data analysis shows that we still have to handle 
word ambiguity. In particular, this is important in 
order to be able to learn the direction of entailment.  
     To achieve better precision we need to increase 
feature discriminativeness. To this end syntactic 
features may be extended to contain more than one 
word, and ways for automatic extraction of fea-
tures from the web (rather than from a corpus) may 
be developed. Finally, further investigation of 
combining the distributional and the co-occurrence 
pattern-based approaches over the web is desired. 
Acknowledgement 
We are grateful to Shachar Mirkin for his help in 
implementing the web-based sampling procedure 
heavily employed in our experiments. We thank 
Idan Szpektor for providing the infrastructure sys-
tem for web-based data extraction.    
References  
Chklovski, Timothy and Patrick Pantel. 2004. 
VERBOCEAN: Mining the Web for Fine-Grained Se-
mantic Verb Relations. In Proc. of EMNLP-04. Bar-
celona, Spain. 
 
Church, Kenneth W. and Hanks Patrick. 1990. Word 
association norms, mutual information, and Lexicog-
raphy. Computational Linguistics, 16(1), pp. 22?29. 
 
Dagan, Ido. 2000. Contextual Word Similarity, in Rob 
Dale, Hermann Moisl and Harold Somers (Eds.), 
Handbook of Natural Language Processing, Marcel 
Dekker Inc, 2000, Chapter 19, pp. 459-476.  
 
Dagan, Ido, Oren Glickman and Bernardo Magnini. 
2005. The PASCAL Recognizing Textual Entailment 
Challenge. In Proc. of the PASCAL Challenges 
Workshop for Recognizing Textual Entailment. 
Southampton, U.K.  
 
Geffet, Maayan and Ido Dagan, 2004. Feature Vector 
Quality and Distributional Similarity. In Proc. of Col-
ing-04. Geneva. Switzerland. 
 
Grefenstette, Gregory. 1994. Exploration in Automatic 
Thesaurus Discovery. Kluwer Academic Publishers. 
 
Harris, Zelig S. Mathematical structures of language. 
Wiley, 1968. 
 
Hearst, Marti. 1992. Automatic acquisition of hypo-
nyms from large text corpora. In Proc. of COLING-
92. Nantes, France. 
 
Keller, Frank, Maria Lapata, and Olga Ourioupina. 
2002. Using the Web to Overcome Data Sparseness. 
In Jan Hajic and Yuji Matsumoto, eds., In Proc. of 
EMNLP-02. Philadelphia, PA. 
 
Lee, Lillian. 1997. Similarity-Based Approaches to 
Natural Language Processing. Ph.D. thesis, Harvard 
University, Cambridge, MA.   
 
Lin, Dekang. 1993. Principle-Based Parsing without 
Overgeneration. In Proc. of ACL-93. Columbus, 
Ohio. 
. 
Lin, Dekang. 1998. Automatic Retrieval and Clustering 
of Similar Words.  In Proc. of COLING?ACL98, 
Montreal, Canada. 
 
Moldovan, Dan, Badulescu, A., Tatu, M., Antohe, D., 
and Girju, R. 2004. Models for the semantic classifi-
cation of noun phrases. In Proc. of HLT/NAACL-
2004 Workshop on Computational Lexical Seman-
tics. Boston. 
 
Pado, Sebastian and Mirella Lapata. 2003. Constructing 
semantic space models from parsed corpora. In Proc. 
of ACL-03, Sapporo, Japan. 
 
Pantel, Patrick and Dekang Lin. 2002. Discovering 
Word Senses from Text. In Proc. of ACM SIGKDD 
Conference on Knowledge Discovery and Data Min-
ing (KDD-02). Edmonton, Canada. 
 
Pereira, Fernando, Tishby Naftali, and Lee Lillian. 
1993. Distributional clustering of English words. In 
Proc. of ACL-93. Columbus, Ohio. 
 
Ravichandran, Deepak and Eduard Hovy. 2002. Learn-
ing Surface Text Patterns for a Question Answering 
System. In Proc. of ACL-02. Philadelphia, PA. 
 
Ruge,  Gerda.  1992.  Experiments on linguistically-
based term associations. Information Processing & 
Management, 28(3), pp. 317?332. 
 
Weeds, Julie and David Weir. 2003. A General Frame-
work for Distributional Similarity. In Proc. of 
EMNLP-03. Sapporo, Japan. 
 
Weeds, Julie, D. Weir, D. McCarthy. 2004. Characteriz-
ing Measures of Lexical Distributional Similarity. In 
Proc. of Coling-04. Geneva, Switzerland. 
114
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 449?456,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Direct Word Sense Matching for Lexical Substitution
Ido Dagan1, Oren Glickman1, Alfio Gliozzo2, Efrat Marmorshtein1, Carlo Strapparava2
1Department of Computer Science, Bar Ilan University, Ramat Gan, 52900, Israel
2ITC-Irst, via Sommarive, I-38050, Trento, Italy
Abstract
This paper investigates conceptually and
empirically the novel sense matching task,
which requires to recognize whether the
senses of two synonymous words match in
context. We suggest direct approaches to
the problem, which avoid the intermediate
step of explicit word sense disambigua-
tion, and demonstrate their appealing ad-
vantages and stimulating potential for fu-
ture research.
1 Introduction
In many language processing settings it is needed
to recognize that a given word or term may be sub-
stituted by a synonymous one. In a typical in-
formation seeking scenario, an information need
is specified by some given source words. When
looking for texts that match the specified need the
source words might be substituted with synony-
mous target words. For example, given the source
word ?weapon? a system may substitute it with the
target synonym ?arm?.
This scenario, which is generally referred here
as lexical substitution, is a common technique
for increasing recall in Natural Language Process-
ing (NLP) applications. In Information Retrieval
(IR) and Question Answering (QA) it is typically
termed query/question expansion (Moldovan and
Mihalcea, 2000; Negri, 2004). Lexical Substi-
tution is also commonly applied to identify syn-
onyms in text summarization, for paraphrasing in
text generation, or is integrated into the features of
supervised tasks such as Text Categorization and
Information Extraction. Naturally, lexical substi-
tution is a very common first step in textual en-
tailment recognition, which models semantic in-
ference between a pair of texts in a generalized ap-
plication independent setting (Dagan et al, 2005).
To perform lexical substitution NLP applica-
tions typically utilize a knowledge source of syn-
onymous word pairs. The most commonly used
resource for lexical substitution is the manually
constructed WordNet (Fellbaum, 1998). Another
option is to use statistical word similarities, such
as in the database constructed by Dekang Lin (Lin,
1998). We generically refer to such resources as
substitution lexicons.
When using a substitution lexicon it is assumed
that there are some contexts in which the given
synonymous words share the same meaning. Yet,
due to polysemy, it is needed to verify that the
senses of the two words do indeed match in a given
context. For example, there are contexts in which
the source word ?weapon? may be substituted by
the target word ?arm?; however one should recog-
nize that ?arm? has a different sense than ?weapon?
in sentences such as ?repetitive movements could
cause injuries to hands, wrists and arms.?
A commonly proposed approach to address
sense matching in lexical substitution is applying
Word Sense Disambiguation (WSD) to identify
the senses of the source and target words. Then,
substitution is applied only if the words have the
same sense (or synset, in WordNet terminology).
In settings in which the source is given as a sin-
gle term without context, sense disambiguation
is performed only for the target word; substitu-
tion is then applied only if the target word?s sense
matches at least one of the possible senses of the
source word.
One might observe that such application of WSD
addresses the task at hand in a somewhat indi-
rect manner. In fact, lexical substitution only re-
quires knowing that the source and target senses
449
do match, but it does not require that the match-
ing senses will be explicitly identified. Selecting
explicitly the right sense in context, which is then
followed by verifying the desired matching, might
be solving a harder intermediate problem than re-
quired. Instead, we can define the sense match-
ing problem directly as a binary classification task
for a pair of synonymous source and target words.
This task requires to decide whether the senses of
the two words do or do not match in a given con-
text (but it does not require to identify explicitly
the identity of the matching senses).
A highly related task was proposed in (Mc-
Carthy, 2002). McCarthy?s proposal was to ask
systems to suggest possible ?semantically similar
replacements? of a target word in context, where
alternative replacements should be grouped to-
gether. While this task is somewhat more com-
plicated as an evaluation setting than our binary
recognition task, it was motivated by similar ob-
servations and applied goals. From another per-
spective, sense matching may be viewed as a lex-
ical sub-case of the general textual entailment
recognition setting, where we need to recognize
whether the meaning of the target word ?entails?
the meaning of the source word in a given context.
This paper provides a first investigation of the
sense matching problem. To allow comparison
with the classical WSD setting we derived an
evaluation dataset for the new problem from the
Senseval-3 English lexical sample dataset (Mihal-
cea and Edmonds, 2004). We then evaluated alter-
native supervised and unsupervised methods that
perform sense matching either indirectly or di-
rectly (i.e. with or without the intermediate sense
identification step). Our findings suggest that in
the supervised setting the results of the direct and
indirect approaches are comparable. However, ad-
dressing directly the binary classification task has
practical advantages and can yield high precision
values, as desired in precision-oriented applica-
tions such as IR and QA.
More importantly, direct sense matching sets
the ground for implicit unsupervised approaches
that may utilize practically unlimited volumes
of unlabeled training data. Furthermore, such
approaches circumvent the sisyphean need for
specifying explicitly a set of stipulated senses.
We present an initial implementation of such an
approach using a one-class classifier, which is
trained on unlabeled occurrences of the source
word and applied to occurrences of the target
word. Our current results outperform the unsuper-
vised baseline and put forth a whole new direction
for future research.
2 WSD and Lexical Expansion
Despite certain initial skepticism about the useful-
ness of WSD in practical tasks (Voorhees, 1993;
Sanderson, 1994), there is some evidence that
WSD can improve performance in typical NLP
tasks such as IR and QA. For example, (Shu?tze
and Pederson, 1995) gives clear indication of the
potential for WSD to improve the precision of an IR
system. They tested the use of WSD on a standard
IR test collection (TREC-1B), improving precision
by more than 4%.
The use of WSD has produced successful exper-
iments for query expansion techniques. In partic-
ular, some attempts exploited WordNet to enrich
queries with semantically-related terms. For in-
stance, (Voorhees, 1994) manually expanded 50
queries over the TREC-1 collection using syn-
onymy and other WordNet relations. She found
that the expansion was useful with short and in-
complete queries, leaving the task of proper auto-
matic expansion as an open problem.
(Gonzalo et al, 1998) demonstrates an incre-
ment in performance over an IR test collection us-
ing the sense data contained in SemCor over a
purely term based model. In practice, they ex-
perimented searching SemCor with disambiguated
and expanded queries. Their work shows that
a WSD system, even if not performing perfectly,
combined with synonymy enrichment increases
retrieval performance.
(Moldovan and Mihalcea, 2000) introduces the
idea of using WordNet to extend Web searches
based on semantic similarity. Their results showed
that WSD-based query expansion actually im-
proves retrieval performance in a Web scenario.
Recently (Negri, 2004) proposed a sense-based
relevance feedback scheme for query enrichment
in a QA scenario (TREC-2003 and ACQUAINT),
demonstrating improvement in retrieval perfor-
mance.
While all these works clearly show the potential
usefulness of WSD in practical tasks, nonetheless
they do not necessarily justify the efforts for refin-
ing fine-grained sense repositories and for build-
ing large sense-tagged corpora. We suggest that
the sense matching task, as presented in the intro-
450
duction, may relieve major drawbacks of applying
WSD in practical scenarios.
3 Problem Setting and Dataset
To investigate the direct sense matching problem
it is necessary to obtain an appropriate dataset of
examples for this binary classification task, along
with gold standard annotation. While there is
no such standard (application independent) dataset
available it is possible to derive it automatically
from existing WSD evaluation datasets, as de-
scribed below. This methodology also allows
comparing direct approaches for sense matching
with classical indirect approaches, which apply an
intermediate step of identifying the most likely
WordNet sense.
We derived our dataset from the Senseval-3 En-
glish lexical sample dataset (Mihalcea and Ed-
monds, 2004), taking all 25 nouns, adjectives and
adverbs in this sample. Verbs were excluded since
their sense annotation in Senseval-3 is not based
on WordNet senses. The Senseval dataset includes
a set of example occurrences in context for each
word, split to training and test sets, where each ex-
ample is manually annotated with the correspond-
ing WordNet synset.
For the sense matching setting we need exam-
ples of pairs of source-target synonymous words,
where at least one of these words should occur in
a given context. Following an applicative moti-
vation, we mimic an IR setting in which a sin-
gle source word query is expanded (substituted)
by a synonymous target word. Then, it is needed
to identify contexts in which the target word ap-
pears in a sense that matches the source word. Ac-
cordingly, we considered each of the 25 words in
the Senseval sample as a target word for the sense
matching task. Next, we had to pick for each target
word a corresponding synonym to play the role of
the source word. This was done by creating a list
of all WordNet synonyms of the target word, under
all its possible senses, and picking randomly one
of the synonyms as the source word. For example,
the word ?disc? is one of the words in the Sense-
val lexical sample. For this target word the syn-
onym ?record? was picked, which matches ?disc?
in its musical sense. Overall, 59% of all possible
synsets of our target words included an additional
synonym, which could play the role of the source
word (that is, 41% of the synsets consisted of the
target word only). Similarly, 62% of the test exam-
ples of the target words were annotated by a synset
that included an additional synonym.
While creating source-target synonym pairs it
was evident that many WordNet synonyms corre-
spond to very infrequent senses or word usages,
such as the WordNet synonyms germ and source.
Such source synonyms are useless for evaluat-
ing sense matching with the target word since the
senses of the two words would rarely match in per-
ceivable contexts. In fact, considering our motiva-
tion for lexical substitution, it is usually desired to
exclude such obscure synonym pairs from substi-
tution lexicons in practical applications, since they
would mostly introduce noise to the system. To
avoid this problem the list of WordNet synonyms
for each target word was filtered by a lexicogra-
pher, who excluded manually obscure synonyms
that seemed worthless in practice. The source syn-
onym for each target word was then picked ran-
domly from the filtered list. Table 1 shows the 25
source-target pairs created for our experiments. In
future work it may be possible to apply automatic
methods for filtering infrequent sense correspon-
dences in the dataset, by adopting algorithms such
as in (McCarthy et al, 2004).
Having source-target synonym pairs, a classifi-
cation instance for the sense matching task is cre-
ated from each example occurrence of the target
word in the Senseval dataset. A classification in-
stance is thus defined by a pair of source and target
words and a given occurrence of the target word in
context. The instance should be classified as pos-
itive if the sense of the target word in the given
context matches one of the possible senses of the
source word, and as negative otherwise. Table 2
illustrates positive and negative example instances
for the source-target synonym pair ?record-disc?,
where only occurrences of ?disc? in the musical
sense are considered positive.
The gold standard annotation for the binary
sense matching task can be derived automatically
from the Senseval annotations and the correspond-
ing WordNet synsets. An example occurrence of
the target word is considered positive if the an-
notated synset for that example includes also the
source word, and Negative otherwise. Notice that
different positive examples might correspond to
different senses of the target word. This happens
when the source and target share several senses,
and hence they appear together in several synsets.
Finally, since in Senseval an example may be an-
451
source-target source-target source-target source-target source-target
statement-argument subdivision-arm atm-atmosphere hearing-audience camber-bank
level-degree deviation-difference dissimilar-different trouble-difficulty record-disc
raging-hot ikon-image crucial-important sake-interest bare-simple
opinion-judgment arrangement-organization newspaper-paper company-party substantial-solid
execution-performance design-plan protection-shelter variety-sort root-source
Table 1: Source and target pairs
sentence annotation
This is anyway a stunning disc, thanks to the playing of the Moscow Virtuosi with Spivakov. positive
He said computer networks would not be affected and copies of information should be made on
floppy discs.
negative
Before the dead soldier was placed in the ditch his personal possessions were removed, leaving
one disc on the body for identification purposes
negative
Table 2: positive and negative examples for the source-target synonym pair ?record-disc?
notated with more than one sense, it was consid-
ered positive if any of the annotated synsets for the
target word includes the source word.
Using this procedure we derived gold standard
annotations for all the examples in the Senseval-
3 training section for our 25 target words. For the
test set we took up to 40 test examples for each tar-
get word (some words had fewer test examples),
yielding 913 test examples in total, out of which
239 were positive. This test set was used to eval-
uate the sense matching methods described in the
next section.
4 Investigated Methods
As explained in the introduction, the sense match-
ing task may be addressed by two general ap-
proaches. The traditional indirect approach would
first disambiguate the target word relative to a pre-
defined set of senses, using standard WSD meth-
ods, and would then verify that the selected sense
matches the source word. On the other hand, a
direct approach would address the binary sense
matching task directly, without selecting explicitly
a concrete sense for the target word. This section
describes the alternative methods we investigated
under supervised and unsupervised settings. The
supervised methods utilize manual sense annota-
tions for the given source and target words while
unsupervised methods do not require any anno-
tated sense examples. For the indirect approach
we assume the standard WordNet sense repository
and corresponding annotations of the target words
with WordNet synsets.
4.1 Feature set and classifier
As a vehicle for investigating different classifica-
tion approaches we implemented a ?vanilla? state
of the art architecture for WSD. Following com-
mon practice in feature extraction (e.g. (Yarowsky,
1994)), and using the mxpost1 part of speech tag-
ger and WordNet?s lemmatization, the following
feature set was used: bag of word lemmas for the
context words in the preceding, current and fol-
lowing sentence; unigrams of lemmas and parts
of speech in a window of +/- three words, where
each position provides a distinct feature; and bi-
grams of lemmas in the same window. The SVM-
Light (Joachims, 1999) classifier was used in the
supervised settings with its default parameters. To
obtain a multi-class classifier we used a standard
one-vs-all approach of training a binary SVM for
each possible sense and then selecting the highest
scoring sense for a test example.
To verify that our implementation provides a
reasonable replication of state of the art WSD we
applied it to the standard Senseval-3 Lexical Sam-
ple WSD task. The obtained accuracy2 was 66.7%,
which compares reasonably with the mid-range of
systems in the Senseval-3 benchmark (Mihalcea
and Edmonds, 2004). This figure is just a few
percent lower than the (quite complicated) best
Senseval-3 system, which achieved about 73% ac-
curacy, and it is much higher than the standard
Senseval baselines. We thus regard our classifier
as a fair vehicle for comparing the alternative ap-
proaches for sense matching on equal grounds.
1ftp://ftp.cis.upenn.edu/pub/adwait/jmx/jmx.tar.gz
2The standard classification accuracy measure equals pre-
cision and recall as defined in the Senseval terminology when
the system classifies all examples, with no abstentions.
452
4.2 Supervised Methods
4.2.1 Indirect approach
The indirect approach for sense matching fol-
lows the traditional scheme of performing WSD
for lexical substitution. First, the WSD classifier
described above was trained for the target words
of our dataset, using the Senseval-3 sense anno-
tated training data for these words. Then, the clas-
sifier was applied to the test examples of the target
words, selecting the most likely sense for each ex-
ample. Finally, an example was classified as pos-
itive if the selected synset for the target word in-
cludes the source word, and as negative otherwise.
4.2.2 Direct approach
As explained above, the direct approach ad-
dresses the binary sense matching task directly,
without selecting explicitly a sense for the target
word. In the supervised setting it is easy to ob-
tain such a binary classifier using the annotation
scheme described in Section 3. Under this scheme
an example was annotated as positive (for the bi-
nary sense matching task) if the source word is
included in the Senseval gold standard synset of
the target word. We trained the classifier using the
set of Senseval-3 training examples for each tar-
get word, considering their derived binary anno-
tations. Finally, the trained classifier was applied
to the test examples of the target words, yielding
directly a binary positive-negative classification.
4.3 Unsupervised Methods
It is well known that obtaining annotated training
examples for WSD tasks is very expensive, and
is often considered infeasible in unrestricted do-
mains. Therefore, many researchers investigated
unsupervised methods, which do not require an-
notated examples. Unsupervised approaches have
usually been investigated within Senseval using
the ?All Words? dataset, which does not include
training examples. In this paper we preferred us-
ing the same test set which was used for the super-
vised setting (created from the Senseval-3 ?Lexi-
cal Sample? dataset, as described above), in order
to enable comparison between the two settings.
Naturally, in the unsupervised setting the sense la-
bels in the training set were not utilized.
4.3.1 Indirect approach
State-of-the-art unsupervised WSD systems are
quite complex and they are not easy to be repli-
cated. Thus, we implemented the unsupervised
version of the Lesk algorithm (Lesk, 1986) as a
reference system, since it is considered a standard
simple baseline for unsupervised approaches. The
Lesk algorithm is one of the first algorithms de-
veloped for semantic disambiguation of all-words
in unrestricted text. In its original unsupervised
version, the only resource required by the algo-
rithm is a machine readable dictionary with one
definition for each possible word sense. The algo-
rithm looks for words in the sense definitions that
overlap with context words in the given sentence,
and chooses the sense that yields maximal word
overlap. We implemented a version of this algo-
rithm using WordNet sense-definitions with con-
text length of ?10 words before and after the tar-
get word.
4.3.2 The direct approach: one-class learning
The unsupervised settings for the direct method
are more problematic because most of unsuper-
vised WSD algorithms (such as the Lesk algo-
rithm) rely on dictionary definitions. For this rea-
son, standard unsupervised techniques cannot be
applied in a direct approach for sense matching, in
which the only external information is a substitu-
tion lexicon.
In this subsection we present a direct unsuper-
vised method for sense matching. It is based on
the assumption that typical contexts in which both
the source and target words appear correspond to
their matching senses. Unlabeled occurrences of
the source word can then be used to provide evi-
dence for lexical substitution because they allow
us to recognize whether the sense of the target
word matches that of the source. Our strategy is
to represent in a learning model the typical con-
texts of the source word in unlabeled training data.
Then, we exploit such model to match the contexts
of the target word, providing a decision criterion
for sense matching. In other words, we expect that
under a matching sense the target word would oc-
cur in prototypical contexts of the source word.
To implement such approach we need a learning
technique that does not rely on the availability of
negative evidence, that is, a one-class learning al-
gorithm. In general, the classification performance
of one-class approaches is usually quite poor, if
compared to supervised approaches for the same
tasks. However, in many practical settings one-
class learning is the only available solution.
For our experiments we adopted the one-class
SVM learning algorithm (Scho?lkopf et al, 2001)
453
implemented in the LIBSVM package,3 and repre-
sented the unlabeled training examples by adopt-
ing the feature set described in Subsection 4.1.
Roughly speaking, a one-class SVM estimates the
smallest hypersphere enclosing most of the train-
ing data. New test instances are then classified
positively if they lie inside the sphere, while out-
liers are regarded as negatives. The ratio between
the width of the enclosed region and the number
of misclassified training examples can be varied
by setting the parameter ? ? (0, 1). Smaller val-
ues of ? will produce larger positive regions, with
the effect of increasing recall.
The appealing advantage of adopting one-class
learning for sense matching is that it allows us to
define a very elegant learning scenario, in which it
is possible to train ?off-line? a different classifier
for each (source) word in the lexicon. Such a clas-
sifier can then be used to match the sense of any
possible target word for the source which is given
in the substitution lexicon. This is in contrast to
the direct supervised method proposed in Subsec-
tion 4.2, where a different classifier for each pair
of source - target words has to be defined.
5 Evaluation
5.1 Evaluation measures and baselines
In the lexical substitution (and expansion) set-
ting, the standard WSD metrics (Mihalcea and Ed-
monds, 2004) are not suitable, because we are in-
terested in the binary decision of whether the tar-
get word matches the sense of a given source word.
In analogy to IR, we are more interested in positive
assignments, while the opposite case (i.e. when the
two words cannot be substituted) is less interest-
ing. Accordingly, we utilize the standard defini-
tions of precision, recall and F1 typically used in
IR benchmarks. In the rest of this section we will
report micro averages for these measures on the
test set described in Section 3.
Following the Senseval methodology, we evalu-
ated two different baselines for unsupervised and
supervised methods. The random baseline, used
for the unsupervised algorithms, was obtained by
choosing either the positive or the negative class
at random resulting in P = 0.262, R = 0.5,
F1 = 0.344. The Most Frequent baseline has
been used for the supervised algorithms and is ob-
tained by assigning the positive class when the
3Freely available from www.csie.ntu.edu.tw/
/?cjlin/libsvm.
percentage of positive examples in the training set
is above 50%, resulting in P = 0.65, R = 0.41,
F1 = 0.51.
5.2 Supervised Methods
Both the indirect and the direct supervised meth-
ods presented in Subsection 4.2 have been tested
and compared to the most frequent baseline.
Indirect. For the indirect methodology we
trained the supervised WSD system for each tar-
get word on the sense-tagged training sample. As
described in Subsection 4.2, we implemented a
simple SVM-based WSD system (see Section 4.2)
and applied it to the sense-matching task. Results
are reported in Table 3. The direct strategy sur-
passes the most frequent baseline F1 score, but the
achieved precision is still below it. We note that in
this multi-class setting it is less straightforward to
tradeoff recall for precision, as all senses compete
with each other.
Direct. In the direct supervised setting, sense
matching is performed by training a binary clas-
sifier, as described in Subsection 4.2.
The advantage of adopting a binary classifica-
tion strategy is that the precision/recall tradeoff
can be tuned in a meaningful way. In SVM learn-
ing, such tuning is achieved by varying the param-
eter J , that allows us to modify the cost function
of the SVM learning algorithm. If J = 1 (default),
the weight for the positive examples is equal to the
weight for the negatives. When J > 1, negative
examples are penalized (increasing recall), while,
whenever 0 < J < 1, positive examples are penal-
ized (increasing precision). Results obtained by
varying this parameter are reported in Figure 1.
Figure 1: Direct supervised results varying J
454
Supervised P R F1 Unsupervised P R F1
Most Frequent Baseline 0.65 0.41 0.51 Random Baseline 0.26 0.50 0.34
Multiclass SVM Indirect 0.59 0.63 0.61 Lesk Indirect 0.24 0.19 0.21
Binary SVM (J = 0.5) Direct 0.80 0.26 0.39 One-Class ? = 0.3 Direct 0.26 0.72 0.39
Binary SVM (J = 1) Direct 0.76 0.46 0.57 One-Class ? = 0.5 Direct 0.29 0.56 0.38
Binary SVM (J = 2) Direct 0.68 0.53 0.60 One-Class ? = 0.7 Direct 0.28 0.36 0.32
Binary SVM (J = 3) Direct 0.69 0.55 0.61 One-Class ? = 0.9 Direct 0.23 0.10 0.14
Table 3: Classification results on the sense matching task
Adopting the standard parameter settings (i.e.
J = 1, see Table 3), the F1 of the system
is slightly lower than for the indirect approach,
while it reaches the indirect figures when J in-
creases. More importantly, reducing J allows us
to boost precision towards 100%. This feature is
of great interest for lexical substitution, particu-
larly in precision oriented applications like IR and
QA, for filtering irrelevant candidate answers or
documents.
5.3 Unsupervised methods
Indirect. To evaluate the indirect unsupervised
settings we implemented the Lesk algorithm, de-
scribed in Subsection 4.3.1, and evaluated it on
the sense matching task. The obtained figures,
reported in Table 3, are clearly below the base-
line, suggesting that simple unsupervised indirect
strategies cannot be used for this task. In fact, the
error of the first step, due to low WSD accuracy
of the unsupervised technique, is propagated in
the second step, producing poor sense matching.
Unfortunately, state-of-the-art unsupervised sys-
tems are actually not much better than Lesk on all-
words task (Mihalcea and Edmonds, 2004), dis-
couraging the use of unsupervised indirect meth-
ods for the sense matching task.
Direct. Conceptually, the most appealing solu-
tion for the sense matching task is the one-class
approach proposed for the direct method (Section
4.3.2). To perform our experiments, we trained a
different one-class SVM for each source word, us-
ing a sample of its unlabeled occurrences in the
BNC corpus as training set. To avoid huge train-
ing sets and to speed up the learning process, we
fixed the maximum number of training examples
to 10000 occurrences per word, collecting on av-
erage about 6500 occurrences per word.
For each target word in the test sample, we ap-
plied the classifier of the corresponding source
word. Results for different values of ? are reported
in Figure 2 and summarized in Table 3.
Figure 2: One-class evaluation varying ?
While the results are somewhat above the base-
line, just small improvements in precision are re-
ported, and recall is higher than the baseline for
? < 0.6. Such small improvements may suggest
that we are following a relevant direction, even
though they may not be useful yet for an applied
sense-matching setting.
Further analysis of the classification results for
each word revealed that optimal F1 values are ob-
tained by adopting different values of ? for differ-
ent words. In the optimal (in retrospect) param-
eter settings for each word, performance for the
test set is noticeably boosted, achieving P = 0.40,
R = 0.85 and F1 = 0.54. Finding a principled un-
supervised way to automatically tune the ? param-
eter is thus a promising direction for future work.
Investigating further the results per word, we
found that the correlation coefficient between the
optimal ? values and the degree of polysemy of
the corresponding source words is 0.35. More in-
terestingly, we noticed a negative correlation (r
= -0.30) between the achieved F1 and the degree
of polysemy of the word, suggesting that polyse-
mous source words provide poor training models
for sense matching. This can be explained by ob-
serving that polysemous source words can be sub-
stituted with the target words only for a strict sub-
455
set of their senses. On the other hand, our one-
class algorithm was trained on all the examples
of the source word, which include irrelevant ex-
amples that yield noisy training sets. A possible
solution may be obtained using clustering-based
word sense discrimination methods (Pedersen and
Bruce, 1997; Schu?tze, 1998), in order to train dif-
ferent one-class models from different sense clus-
ters. Overall, the analysis suggests that future re-
search may obtain better binary classifiers based
just on unlabeled examples of the source word.
6 Conclusion
This paper investigated the sense matching task,
which captures directly the polysemy problem in
lexical substitution. We proposed a direct ap-
proach for the task, suggesting the advantages of
natural control of precision/recall tradeoff, avoid-
ing the need in an explicitly defined sense reposi-
tory, and, most appealing, the potential for novel
completely unsupervised learning schemes. We
speculate that there is a great potential for such
approaches, and suggest that sense matching may
become an appealing problem and possible track
in lexical semantic evaluations.
Acknowledgments
This work was partly developed under the collab-
oration ITC-irst/University of Haifa.
References
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005. The pascal recognising textual entailment
challenge. Proceedings of the PASCAL Challenges
Workshop on Recognising Textual Entailment.
C. Fellbaum. 1998. WordNet. An Electronic Lexical
Database. MIT Press.
J. Gonzalo, F. Verdejo, I. Chugur, and J. Cigarran.
1998. Indexing with wordnet synsets can improve
text retrieval. In ACL, Montreal, Canada.
T. Joachims. 1999. Making large-scale SVM learning
practical. In B. Scho?lkopf, C. Burges, and A. Smola,
editors, Advances in kernel methods: support vector
learning, chapter 11, pages 169 ? 184. MIT Press.
M. Lesk. 1986. Automatic sense disambiguation using
machine readable dictionaries: How to tell a pine
cone from an ice cream cone. In Proceedings of the
ACM-SIGDOC Conference, Toronto, Canada.
Dekang Lin. 1998. Automatic retrieval and cluster-
ing of similar words. In Proceedings of the 17th
international conference on Computational linguis-
tics, pages 768?774, Morristown, NJ, USA. Associ-
ation for Computational Linguistics.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004. Automatic identification of infre-
quent word senses. In Proceedings of COLING,
pages 1220?1226.
Diana McCarthy. 2002. Lexical substitution as a task
for wsd evaluation. In Proceedings of the ACL-
02 workshop on Word sense disambiguation, pages
109?115, Morristown, NJ, USA. Association for
Computational Linguistics.
R. Mihalcea and P. Edmonds, editors. 2004. Proceed-
ings of SENSEVAL-3: Third International Workshop
on the Evaluation of Systems for the Semantic Anal-
ysis of Text, Barcelona, Spain, July.
D. Moldovan and R. Mihalcea. 2000. Using wordnet
and lexical operators to improve internet searches.
IEEE Internet Computing, 4(1):34?43, January.
M. Negri. 2004. Sense-based blind relevance feedback
for question answering. In SIGIR-2004 Workshop
on Information Retrieval For Question Answering
(IR4QA), Sheffield, UK, July.
T. Pedersen and R. Bruce. 1997. Distinguishing word
sense in untagged text. In EMNLP, Providence, Au-
gust.
M. Sanderson. 1994. Word sense disambiguation and
information retrieval. In SIGIR, Dublin, Ireland,
June.
B. Scho?lkopf, J. Platt, J. Shawe-Taylor, A. J. Smola,
and R. C. Williamson. 2001. Estimating the support
of a high-dimensional distribution. Neural Compu-
tation, 13:1443?1471.
H. Schu?tze. 1998. Automatic word sense discrimina-
tion. Computational Linguistics, 24(1).
H. Shu?tze and J. Pederson. 1995. Information retrieval
based on word senses. In Proceedings of the 4th
Annual Symposium on Document Analysis and In-
formation Retrieval, Las Vegas.
E. Voorhees. 1993. Using WordNet to disambiguate
word sense for text retrieval. In SIGIR, Pittsburgh,
PA.
E. Voorhees. 1994. Query expansion using lexical-
semantic relations. In Proceedings of the 17th ACM
SIGIR Conference, Dublin, Ireland, June.
D. Yarowsky. 1994. Decision lists for lexical ambi-
guity resolution: Application to accent restoration
in spanish and french. In ACL, pages 88?95, Las
Cruces, New Mexico.
456
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 579?586,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Integrating Pattern-based and Distributional Similarity Methods for 
Lexical Entailment Acquisition 
 
                   Shachar Mirkin     Ido Dagan         Maayan Geffet 
School of Computer Science and Engineering 
The Hebrew University, Jerusalem, Israel, 
91904 
mirkin@cs.huji.ac.il  
 
Department of Computer Science 
Bar-Ilan University, Ramat Gan, Israel,  
52900 
{dagan,zitima}@cs.biu.ac.il 
 
Abstract 
This paper addresses the problem of acquir-
ing lexical semantic relationships, applied to 
the lexical entailment relation. Our main con-
tribution is a novel conceptual integration 
between the two distinct acquisition para-
digms for lexical relations ? the pattern-
based and the distributional similarity ap-
proaches. The integrated method exploits 
mutual complementary information of the 
two approaches to obtain candidate relations 
and informative characterizing features. 
Then, a small size training set is used to con-
struct a more accurate supervised classifier, 
showing significant increase in both recall 
and precision over the original approaches. 
1 Introduction 
Learning lexical semantic relationships is a fun-
damental task needed for most text understand-
ing applications. Several types of lexical 
semantic relations were proposed as a goal for 
automatic acquisition. These include lexical on-
tological relations such as synonymy, hyponymy 
and meronymy, aiming to automate the construc-
tion of WordNet-style relations. Another com-
mon target is learning general distributional 
similarity between words, following Harris' Dis-
tributional Hypothesis (Harris, 1968). Recently, 
an applied notion of entailment between lexical 
items was proposed as capturing major inference 
needs which cut across multiple semantic rela-
tionship types (see Section 2 for further back-
ground).  
The literature suggests two major approaches 
for learning lexical semantic relations: distribu-
tional similarity and pattern-based. The first ap-
proach recognizes that two words (or two multi-
word terms) are semantically similar based on 
distributional similarity of the different contexts 
in which the two words occur. The distributional 
method identifies a somewhat loose notion of 
semantic similarity, such as between company 
and government, which does not ensure that the 
meaning of one word can be substituted by the 
other. The second approach is based on identify-
ing joint occurrences of the two words within 
particular patterns, which typically indicate di-
rectly concrete semantic relationships. The pat-
tern-based approach tends to yield more accurate 
hyponymy and (some) meronymy relations, but 
is less suited to acquire synonyms which only 
rarely co-occur within short patterns in texts. It 
should be noted that the pattern-based approach 
is commonly applied also for information and 
knowledge extraction to acquire factual instances 
of concrete meaning relationships (e.g. born in, 
located at) rather than generic lexical semantic 
relationships in the language. 
While the two acquisition approaches are 
largely complementary, there have been just few 
attempts to combine them, usually by pipeline 
architecture. In this paper we propose a method-
ology for integrating distributional similarity 
with the pattern-based approach. In particular, 
we focus on learning the lexical entailment rela-
tionship between common nouns and noun 
phrases (to be distinguished from learning rela-
tionships for proper nouns, which usually falls 
within the knowledge acquisition paradigm).  
The underlying idea is to first identify candi-
date relationships by both the distributional ap-
proach, which is applied exhaustively to a local 
corpus, and the pattern-based approach, applied 
to the web. Next, each candidate is represented 
by a unified set of distributional and pattern-
based features. Finally, using a small training set 
we devise a supervised (SVM) model that classi-
fies new candidate relations as correct or incor-
rect. 
To implement the integrated approach we de-
veloped state of the art pattern-based acquisition 
579
methods and utilized a distributional similarity 
method that was previously shown to provide 
superior performance for lexical entailment ac-
quisition. Our empirical results show that the 
integrated method significantly outperforms each 
approach in isolation, as well as the na?ve com-
bination of their outputs. Overall, our method 
reveals complementary types of information that 
can be obtained from the two approaches. 
 
2 Background 
2.1 Distributional Similarity and 
Lexical Entailment 
The general idea behind distributional similarity 
is that words which occur within similar contexts 
are semantically similar (Harris, 1968). In a 
computational framework, words are represented 
by feature vectors, where features are context 
words weighted by a function of their statistical 
association with the target word. The degree of 
similarity between two target words is then de-
termined by a vector comparison function. 
Amongst the many proposals for distributional 
similarity measures, (Lin, 1998) is maybe the 
most widely used one, while (Weeds et al, 2004) 
provides a typical example for recent research. 
Distributional similarity measures are typically 
computed through exhaustive processing of a 
corpus, and are therefore applicable to corpora of 
bounded size. 
It was noted recently by Geffet and Dagan 
(2004, 2005) that distributional similarity cap-
tures a quite loose notion of semantic similarity, 
as exemplified by the pair country ? party (iden-
tified by Lin's similarity measure). Consequently, 
they proposed a definition for the lexical entail-
ment relation, which conforms to the general 
framework of applied textual entailment (Dagan 
et al, 2005). Generally speaking, a word w lexi-
cally entails another word v if w can substitute v 
in some contexts while implying v's original 
meaning. It was suggested that lexical entailment 
captures major application needs in modeling 
lexical variability, generalized over several types 
of known ontological relationships. For example, 
in Question Answering (QA), the word company 
in a question can be substituted in the text by 
firm (synonym), automaker (hyponym) or sub-
sidiary (meronym), all of which entail company. 
Typically, hyponyms entail their hypernyms and 
synonyms entail each other, while entailment 
holds for meronymy only in certain cases. 
In this paper we investigate automatic acquisi-
tion of the lexical entailment relation. For the 
distributional similarity component we employ 
the similarity scheme of (Geffet and Dagan, 
2004), which was shown to yield improved pre-
dictions of (non-directional) lexical entailment 
pairs. This scheme utilizes the symmetric simi-
larity measure of (Lin, 1998) to induce improved 
feature weights via bootstrapping. These weights 
identify the most characteristic features of each 
word, yielding cleaner feature vector representa-
tions and better similarity assessments. 
2.2 Pattern-based Approaches 
Hearst (1992) pioneered the use of lexical-
syntactic patterns for automatic extraction of 
lexical semantic relationships. She acquired hy-
ponymy relations based on a small predefined set 
of highly indicative patterns, such as ?X, . . . , Y 
and/or other Z?, and ?Z such as X, . . . and/or Y?, 
where X and Y are extracted as hyponyms of Z. 
Similar techniques were further applied to pre-
dict hyponymy and meronymy relationships us-
ing lexical or lexico-syntactic patterns (Berland 
and Charniak, 1999; Sundblad, 2002), and web 
page structure was exploited to extract hy-
ponymy relationships by Shinzato and Torisawa 
(2004). Chklovski and Pantel (2004) used pat-
terns to extract a set of relations between verbs, 
such as similarity, strength and antonymy. Syno-
nyms, on the other hand, are rarely found in such 
patterns. In addition to their use for learning lexi-
cal semantic relations, patterns were commonly 
used to learn instances of concrete semantic rela-
tions for Information Extraction (IE) and QA, as 
in (Riloff and Shepherd, 1997; Ravichandran and 
Hovy, 2002; Yangarber et al, 2000).  
Patterns identify rather specific and informa-
tive structures within particular co-occurrences 
of the related words. Consequently, they are rela-
tively reliable and tend to be more accurate than 
distributional evidence. On the other hand, they 
are susceptive to data sparseness in a limited size 
corpus. To obtain sufficient coverage, recent 
works such as (Chklovski and Pantel, 2004) ap-
plied pattern-based approaches to the web. These 
methods form search engine queries that match 
likely pattern instances, which may be verified 
by post-processing the retrieved texts. 
Another extension of the approach was auto-
matic enrichment of the pattern set through boot-
strapping. Initially, some instances of the sought 
580
relation are found based on a set of manually 
defined patterns.  Then, additional co-
occurrences of the related terms are retrieved, 
from which new patterns are extracted (Riloff 
and Jones, 1999; Pantel et al, 2004). Eventually, 
the list of effective patterns found for ontological 
relations has pretty much converged in the litera-
ture. Amongst these, Table 1 lists the patterns 
that were utilized in our work. 
Finally, the selection of candidate pairs for a 
target relation was usually based on some func-
tion over the statistics of matched patterns. To 
perform more systematic selection Etzioni et al 
(2004) applied a supervised Machine Learning 
algorithm (Na?ve Bayes), using pattern statistics 
as features. Their work was done within the IE 
framework, aiming to extract semantic relation 
instances for proper nouns, which occur quite 
frequently in indicative patterns. In our work we 
incorporate and extend the supervised learning 
step for the more difficult task of acquiring gen-
eral language relationships between common 
nouns. 
2.3 Combined Approaches 
It can be noticed that the pattern-based and dis-
tributional approaches have certain complemen-
tary properties. The pattern-based method tends 
to be more precise, and also indicates the direc-
tion of the relationship between the candidate 
terms. The distributional similarity approach is 
more exhaustive and suitable to detect symmetric 
synonymy relations. Few recent attempts on re-
lated (though different) tasks were made to clas-
sify (Lin et al, 2003) and label (Pantel and 
Ravichandran, 2004) distributional similarity 
output using lexical-syntactic patterns, in a pipe-
line architecture. We aim to achieve tighter inte-
gration of the two approaches, as described next. 
 
3 An Integrated Approach for Lexi-
cal Entailment Acquisition 
This section describes our integrated approach 
for acquiring lexical entailment relationships, 
applied to common nouns. The algorithm re-
ceives as input a target term and aims to acquire 
a set of terms that either entail or are entailed by 
it. We denote a pair consisting of the input target 
term and an acquired entailing/entailed term as 
entailment pair. Entailment pairs are directional, 
as in bank  company. 
Our approach applies a supervised learning 
scheme, using SVM, to classify candidate en-
tailment pairs as correct or incorrect. The SVM 
training phase is applied to a small constant 
number of training pairs, yielding a classification 
model that is then used to classify new test en-
tailment pairs. The designated training set is also 
used to tune some additional parameters of the 
method. Overall, the method consists of the fol-
lowing main components:  
1: Acquiring candidate entailment pairs for 
the input term by pattern-based and distribu-
tional similarity methods (Section 3.2); 
2: Constructing a feature set for all candidates 
based on pattern-based and distributional in-
formation (Section 3.3); 
3: Applying SVM training and classification 
to the candidate pairs (Section 3.4).  
The first two components, of acquiring candidate 
pairs and collecting features for them, utilize a 
generic module for pattern-based extraction from 
the web, which is described first in Section 3.1.    
3.1 Pattern-based Extraction Mod-
ule 
The general pattern-based extraction module re-
ceives as input a set of lexical-syntactic patterns 
(as in Table 1) and either a target term or a can-
didate pair of terms. It then searches the web for 
occurrences of the patterns with the input term(s). 
A small set of effective queries is created for 
each pattern-terms combination, aiming to re-
trieve as much relevant data with as few queries 
as possible. 
Each pattern has two variable slots to be in-
stantiated by candidate terms for the sought rela-
tion. Accordingly, the extraction module can be 
1 NP1 such as NP2 
2 Such NP1 as NP2 
3 NP1 or other NP2 
4 NP1 and other NP2 
5 NP1 ADV known as NP2 
6 NP1 especially NP2 
7 NP1 like NP2 
8 NP1 including NP2 
9 NP1-sg is (a OR an) NP2-sg 
10 NP1-sg (a OR an) NP2-sg 
11 NP1-pl are NP2-pl 
Table 1: The patterns we used for entailment ac-
quisition based on (Hearst, 1992) and (Pantel et al, 
2004). Capitalized terms indicate variables. pl and 
sg stand for plural and singular forms. 
 
581
used in two modes: (a) receiving a single target 
term as input and searching for instantiations of 
the other variable to identify candidate related 
terms (as in Section 3.2); (b) receiving a candi-
date pair of terms for the relation and searching 
pattern instances with both terms, in order to 
validate and collect information about the rela-
tionship between the terms (as in Section 3.3). 
Google proximity search1 provides a useful tool 
for these purposes, as it allows using a wildcard 
which might match either an un-instantiated term 
or optional words such as modifiers.  For exam-
ple, the query "such ** as *** (war OR wars)" is 
one of the queries created for the input pattern 
such NP1 as NP2 and the input target term war, 
allowing new terms to match the first pattern 
variable. For the candidate entailment pair war 
? struggle, the first variable is instantiated as 
well. The corresponding query would be: "such * 
(struggle OR struggles) as *** (war OR wars)?. 
This technique allows matching terms that are 
sub-parts of more complex noun phrases as well 
as multi-word terms. 
The automatically constructed queries, cover-
ing the possible combinations of multiple wild-
cards, are submitted to Google2 and a specified 
number of snippets is downloaded, while avoid-
ing duplicates. The snippets are passed through a 
word splitter and a sentence segmenter3, while 
filtering individual sentences that do not contain 
all search terms. Next, the sentences are proc-
essed with the OpenNLP4  POS tagger and NP 
chunker. Finally, pattern-specific regular expres-
sions over the chunked sentences are applied to 
verify that the instantiated pattern indeed occurs 
in the sentence, and to identify variable instantia-
tions.  
On average, this method extracted more than 
3300 relationship instances for every 1MB of 
downloaded text, almost third of them contained 
multi-word terms. 
3.2 Candidate Acquisition 
Given an input target term we first employ pat-
tern-based extraction to acquire entailment pair 
candidates and then augment the candidate set 
with pairs obtained through distributional simi-
larity. 
                                                          
1
 Previously used by (Chklovski and Pantel, 2004). 
2
 http://www.google.com/apis/ 
3 Available from the University of Illinois at Urbana-
Champaign, http://l2r.cs.uiuc.edu/~cogcomp/tools.php 
4
 www.opennlp.sourceforge.net/ 
3.2.1 Pattern-based Candidates 
At the candidate acquisition phase pattern in-
stances are searched with one input target term, 
looking for instantiations of the other pattern 
variable to become the candidate related term 
(the first querying mode described in Section 
3.1). We construct two types of queries, in which 
the target term is either the first or second vari-
able in the pattern, which corresponds to finding 
either entailing or entailed terms that instantiate 
the other variable.  
In the candidate acquisition phase we utilized 
patterns 1-8 in Table 1, which we empirically 
found as most suitable for identifying directional 
lexical entailment pairs. Patterns 9-11 are not 
used at this stage as they produce too much noise 
when searched with only one instantiated vari-
able. About 35 queries are created for each target 
term in each entailment direction for each of the 
8 patterns. For every query, the first n snippets 
are downloaded (we used n=50). The 
downloaded snippets are processed as described 
in Section 3.1, and candidate related terms are 
extracted, yielding candidate entailment pairs 
with the input target term.  
Quite often the entailment relation holds be-
tween multi-word noun-phrases rather than 
merely between their heads. For example, trade 
center lexically entails shopping complex, while 
center does not necessarily entail complex. On 
the other hand, many complex multi-word noun 
phrases are too rare to make a statistically based 
decision about their relation with other terms. 
Hence, we apply the following two criteria to 
balance these constraints:  
1. For the entailing term we extract only the 
complete noun-chunk which instantiate the 
pattern. For example: we extract housing 
project ? complex, but do not extract pro-
ject as entailing complex since the head noun 
alone is often too general to entail the other 
term. 
2. For the entailed term we extract both the 
complete noun-phrase and its head in order 
to create two separate candidate entailment 
pairs with the entailing term, which will be 
judged eventually according to their overall 
statistics. 
As it turns out, a large portion of the extracted 
pairs constitute trivial hyponymy relations, 
where one term is a modified version of the other, 
like low interest loan ? loan. These pairs were 
removed, along with numerous pairs including 
proper nouns, following the goal of learning en-
582
tailment relationships for distinct common 
nouns.  
Finally, we filter out the candidate pairs whose 
frequency in the extracted patterns is less than a 
threshold, which was set empirically to 3. Using 
a lower threshold yielded poor precision, while a 
threshold of 4 decreased recall substantially with 
just little effect on precision. 
3.2.2 Distributional Similarity 
Candidates 
As mentioned in Section 2, we employ the distri-
butional similarity measure of (Geffet and Da-
gan, 2004) (denoted here GD04 for brevity), 
which was found effective for extracting non-
directional lexical entailment pairs.  Using local 
corpus statistics, this algorithm produces for each 
target noun a scored list of up to a few hundred 
words with positive distributional similarity 
scores. 
Next we need to determine an optimal thresh-
old for the similarity score, considering words 
above it as likely entailment candidates. To tune 
such a threshold we followed the original meth-
odology used to evaluate GD04. First, the top-k 
(k=40) similarities of each training term are 
manually annotated by the lexical entailment cri-
terion (see Section 4.1). Then, the similarity 
value which yields the maximal micro-averaged 
F1 score is selected as threshold, suggesting an 
optimal recall-precision tradeoff. The selected 
threshold is then used to filter the candidate simi-
larity lists of the test words.   
Finally, we remove all entailment pairs that al-
ready appear in the candidate set of the pattern-
based approach, in either direction (recall that the 
distributional candidates are non-directional). 
Each of the remaining candidates generates two 
directional pairs which are added to the unified 
candidate set of the two approaches. 
3.3 Feature Construction 
Next, each candidate is represented by a set of 
features, suitable for supervised classification. To 
this end we developed a novel feature set based 
on both pattern-based and distributional data. 
 
To obtain pattern statistics for each pair, the 
second mode of the pattern-based extraction 
module is applied (see Section 3.1). As in this 
case, both variables in the pattern are instantiated 
by the terms of the pair, we could use all eleven 
patterns in Table 1, creating a total of about 55 
queries per pair and downloading m=20 snippets 
for each query. The downloaded snippets are 
processed as described in Section 3.1 to identify 
pattern matches and obtain relevant statistics for 
feature scores.  
Following is the list of feature types computed 
for each candidate pair. The feature set was de-
signed specifically for the task of extracting the 
complementary information of the two methods. 
Conditional Pattern Probability: This type of 
feature is created for each of the 11 individual 
patterns. The feature value is the estimated con-
ditional probability of having the pattern 
matched in a sentence given that the pair of terms 
does appear in the sentence (calculated as the 
fraction of pattern matches for the pair amongst 
all unique sentences that contain the pair). This 
feature yields normalized scores for pattern 
matches regardless of the number of snippets 
retrieved for the given pair. This normalization is 
important in order to bring to equal grounds can-
didate pairs identified through either the pattern-
based or distributional approaches, since the lat-
ter tend to occur less frequently in patterns. 
Aggregated Conditional Pattern Probability: 
This single feature is the conditional probability 
that any of the patterns match in a retrieved sen-
tence, given that the two terms appear in it. It is 
calculated like the previous feature, with counts 
aggregated over all patterns, and aims to capture 
overall appearance of the pair in patterns, regard-
less of the specific pattern. 
Conditional List-Pattern Probability: This fea-
ture was designed to eliminate the typical non-
entailing cases of co-hyponyms (words sharing 
the same hypernym), which nevertheless tend to 
co-occur in entailment patterns. We therefore 
also check for pairs' occurrences in lists, using 
appropriate list patterns, expecting that correct 
entailment pairs would not co-occur in lists. The 
probability estimate, calculated like the previous 
one, is expected to be a negative feature for the 
learning model. 
Relation Direction Ratio: The value of this fea-
ture is the ratio between the overall number of 
pattern matches for the pair and the number of 
pattern matches for the reversed pair (a pair cre-
ated with the same terms in the opposite entail-
ment direction). We found that this feature 
strongly correlates with entailment likelihood. 
Interestingly, it does not deteriorate performance 
for synonymous pairs. 
Distributional Similarity Score: The GD04 simi-
larity score of the pair was used as a feature. We 
583
also attempted adding Lin's (1998) similarity 
scores but they appeared to be redundant. 
Intersection Feature: A binary feature indicating 
candidate pairs acquired by both methods, which 
was found to indicate higher entailment likeli-
hood. 
    In summary, the above feature types utilize 
mutually complementary pattern-based and dis-
tributional information. Using cross validation 
over the training set we verified that each feature 
makes marginal contribution to performance 
when added on top of the remaining features.  
3.4 Training and Classification 
In order to systematically integrate different fea-
ture types we used the state-of-the-art supervised 
classifier SVMlight (Joachims, 1999) for entail-
ment pair classification. Using 10-fold cross-
validation over the training set we obtained the 
SVM configuration that yields an optimal micro-
averaged F1 score. Through this optimization we 
chose the RBF kernel function and obtained op-
timal values for the J, C and the RBF's Gamma 
parameters. The candidate test pairs classified as 
correct entailments constitute the output of our 
integrated method. 
 
4 Empirical Results 
4.1 Data Set and Annotation 
We utilized the experimental data set from Geffet 
and Dagan (2004). The dataset includes the simi-
larity lists calculated by GD04 for a sample of 30 
target (common) nouns, computed from an 18 
million word subset of the Reuters corpus5. We 
randomly picked a small set of 10 terms for train-
ing, leaving the remaining 20 terms for testing. 
Then, the set of entailment pair candidates for all 
nouns was created by applying the filtering 
method of Section 3.2.2 to the distributional 
similarity lists, and by extracting pattern-based 
                                                          
5
 Reuters Corpus, Volume 1, English Language, 1996-08-20 to 1997-08-19. 
candidates from the web as described in Section 
3.2.1. 
Gold standard annotations for entailment pairs 
were created by three judges. The judges were 
guided to annotate as ?Correct? the pairs con-
forming to the lexical entailment definition, 
which was reflected in two operational tests: i) 
Word meaning entailment: whether the meaning 
of the first (entailing) term implies the meaning 
of the second (entailed) term under some com-
mon sense of the two terms; and ii) Substitutabil-
ity: whether the first term can substitute the 
second term in some natural contexts, such that 
the meaning of the modified context entails the 
meaning of the original one. The obtained Kappa 
values (varying between 0.7 and 0.8) correspond 
to substantial agreement on the task. 
4.2 Results 
The numbers of candidate entailment pairs col-
lected for the test terms are shown in Table 2. 
These figures highlight the markedly comple-
mentary yield of the two acquisition approaches, 
where only about 10% of all candidates were 
identified by both methods. On average, 120 
candidate entailment pairs were acquired for 
each target term. 
The SVM classifier was trained on a quite 
small annotated sample of 700 candidate entail-
ment pairs of the 10 training terms. Table 3 pre-
sents comparative results for the classifier, for 
each of the two sets of candidates produced by 
each method alone, and for the union of these 
two sets (referred as Na?ve Combination). The 
results were computed for an annotated random 
sample of about 400 candidate entailment pairs 
of the test terms. Following common pooling 
evaluations in Information Retrieval, recall is 
calculated relatively to the total number of cor-
rect entailment pairs acquired by both methods 
together.  
METHOD P R F 
Pattern-based  0.44 0.61 0.51 
Distributional  
Similarity 0.33 0.53 0.40 
Na?ve Combina-
tion 0.36 1.00 0.53 
Integrated  0.57 0.69 0.62 
Table 3: Precision, Recall and F1 figures for the 
test words under each method. 
 
PATTERN-
BASED 
DISTRIBU-
TIONAL TOTAL 
1186 1420 2350 
Table 2: The numbers of distinct entailment pair 
candidates obtained for the test words by each of 
the methods, and when combined.  
 
584
The first two rows of the table show quite 
moderate precision and recall for the candidates 
of each separate method. The next row shows the 
great impact of method combination on recall, 
relative to the amount of correct entailment pairs 
found by each method alone, validating the com-
plementary yield of the approaches. The inte-
grated classifier, applied to the combined set of 
candidates, succeeds to increase precision sub-
stantially by 21 points (a relative increase of al-
most 60%), which is especially important for 
many precision-oriented applications like Infor-
mation Retrieval and Question Answering. The 
precision increase comes with the expense of 
some recall, yet having F1 improved by 9 points. 
The integrated method yielded on average about 
30 correct entailments per target term. Its classi-
fication accuracy (percent of correct classifica-
tions) reached 70%, which nearly doubles the 
na?ve combination's accuracy.  
It is impossible to directly compare our results 
with those of other works on lexical semantic 
relationships acquisition, since the particular task 
definition and dataset are different. As a rough 
reference point, our result figures do match those 
of related papers reviewed in Section 2, while we 
notice that our setting is relatively more difficult 
since we excluded the easier cases of proper 
nouns. (Geffet and Dagan, 2005), who exploited 
the distributional similarity approach over the 
web to address the same task as ours, obtained 
higher precision but substantially lower recall, 
considering only distributional candidates. Fur-
ther research is suggested to investigate integrat-
ing their approach with ours. 
 
 
 
4.3 Analysis and Discussion 
Analysis of the data confirmed that the two 
methods tend to discover different types of rela-
tions. As expected, the distributional similarity 
method contributed most (75%) of the synonyms 
that were correctly classified as mutually entail-
ing pairs (e.g. assault ? abuse in Table 4). On 
the other hand, about 80% of all correctly identi-
fied hyponymy relations were produced by the 
pattern-based method (e.g. abduction ? abuse). 
The integrated method provides a means to de-
termine the entailment direction for distributional 
similarity candidates which by construction are 
non-directional. Thus, amongst the (non-
synonymous) distributional similarity pairs clas-
sified as entailing, the direction of 73% was cor-
rectly identified. In addition, the integrated 
method successfully filters 65% of the non-
entailing co-hyponym candidates (hyponyms of 
the same hypernym), most of them originated in 
the distributional candidates, which is a large 
portion (23%) of all correctly discarded pairs. 
Consequently, the precision of distributional 
similarity candidates approved by the integrated 
system was nearly doubled, indicating the addi-
tional information that patterns provide about 
distributionally similar pairs. 
Yet, several error cases were detected and 
categorized. First, many non-entailing pairs are 
context-dependent, such as a gap which might 
constitute a hazard in some particular contexts, 
even though these words do not entail each other 
in their general meanings. Such cases are more 
typical for the pattern-based approach, which is 
sometimes permissive with respect to the rela-
tionship captured and may also extract candi-
dates from a relatively small number of pattern 
occurrences. Second, synonyms tend to appear 
less frequently in patterns. Consequently, some 
synonymous pairs discovered by distributional 
similarity were rejected due to insufficient pat-
tern matches. Anecdotally, some typos and spell-
ing alternatives, like privatization ? 
privatisation, are also included in this category 
as they never co-occur in patterns. 
In addition, a large portion of errors is caused 
by pattern ambiguity. For example, the pattern 
"NP1, a|an NP2", ranked among the top IS-A pat-
terns by (Pantel et al, 2004), can represent both 
apposition (entailing) and a list of co-hyponyms 
(non-entailing). Finally, some misclassifications 
can be attributed to technical web-based process-
ing errors and to corpus data sparseness.  
 
Pattern-based Distributional 
abduction ? abuse assault ? abuse 
government ?  
organization 
government ?  
administration 
drug therapy ?  
treatment budget deficit ?gap 
gap ? hazard* broker ? analyst* 
management ? issue* government ?  parliament* 
Table 4: Typical entailment pairs acquired by the 
integrated method, illustrating Section 4.3. The 
columns specify the method that produced the 
candidate pair. Asterisk indicates a non-entailing 
pair. 
 
585
5 Conclusion 
The main contribution of this paper is a novel 
integration of the pattern-based and distributional 
approaches for lexical semantic acquisition, ap-
plied to lexical entailment. Our investigation 
highlights the complementary nature of the two 
approaches and the information they provide. 
Notably, it is possible to extract pattern-based 
information that complements the weaker evi-
dence of distributional similarity. Supervised 
learning was found effective for integrating the 
different information types, yielding noticeably 
improved performance. Indeed, our analysis re-
veals that the integrated approach helps eliminat-
ing many error cases typical to each method 
alone. We suggest that this line of research may 
be investigated further to enrich and optimize the 
learning processes and to address additional lexi-
cal relationships.  
 
Acknowledgement 
We wish to thank Google for providing us with 
an extended quota for search queries, which 
made this research feasible.  
 
References  
Berland, Matthew and Charniak, Eugene. 1999. Find-
ing parts in very large corpora. In Proc. of ACL-99. 
Maryland, USA. 
Chklovski, Timothy and Patrick Pantel. 2004. VerbO-
cean: Mining the Web for Fine-Grained Semantic 
Verb Relations. In Proc. of EMNLP-04. Barcelona, 
Spain. 
Dagan, Ido, Oren Glickman and Bernardo Magnini. 
2005. The PASCAL Recognizing Textual Entail-
ment Challenge. In Proc. of the PASCAL Chal-
lenges Workshop for Recognizing Textual 
Entailment. Southampton, U.K.  
Etzioni, Oren, M. Cafarella, D. Downey, S. Kok, A.-
M. Popescu, T. Shaked, S. Soderland, D.S. Weld, 
and A. Yates. 2004. Web-scale information extrac-
tion in KnowItAll. In Proc. of WWW-04. NY, 
USA. 
Geffet, Maayan and Ido Dagan. 2004. Feature Vector 
Quality and Distributional Similarity. In Proc. of 
COLING-04. Geneva, Switzerland. 
Geffet, Maayan and Ido Dagan. 2005. The Distribu-
tional Inclusion Hypothesis and Lexical Entail-
ment. In Proc of ACL-05. Michigan, USA. 
Harris, Zelig S. 1968. Mathematical Structures of 
Language. Wiley. 
Hearst, Marti. 1992. Automatic Acquisition of Hypo-
nyms from Large Text Corpora. In Proc. of 
COLING-92. Nantes, France. 
Joachims, Thorsten. 1999. Making large-Scale SVM 
Learning Practical. Advances in Kernel Methods - 
Support Vector Learning, B. Sch?lkopf and C. 
Burges and A. Smola (ed.), MIT-Press. 
Lin, Dekang. 1998. Automatic Retrieval and Cluster-
ing of Similar Words.  In Proc. of COLING?
ACL98, Montreal, Canada. 
Lin, Dekang, Shaojun Zhao, Lijuan Qin, and Ming 
Zhou. 2003. Identifying synonyms among distribu-
tionally similar words. In Proc. of  IJCAI-03. Aca-
pulco, Mexico. 
Pantel, Patrick, Deepak Ravichandran, and Eduard 
Hovy. 2004. Towards Terascale Semantic Acquisi-
tion. In Proc. of COLING-04. Geneva, Switzer-
land. 
Pantel, Patrick and Deepak Ravichandran. 2004. 
Automatically Labeling Semantic Classes. In Proc. 
of HLT/NAACL-04. Boston, MA. 
Ravichandran, Deepak and Eduard Hovy. 2002. 
Learning Surface Text Patterns for a Question An-
swering System. In Proc. of ACL-02. Philadelphia, 
PA. 
Riloff, Ellen and Jessica Shepherd. 1997. A corpus-
based approach for building semantic lexicons. In 
Proc. of EMNLP-97. RI, USA. 
Riloff, Ellen and Rosie Jones. 1999. Learning Dic-
tionaries for Information Extraction by Multi-Level 
Bootstrapping. In Proc. of AAAI-99. Florida, USA. 
Shinzato, Kenji and Kentaro Torisawa. 2004. Acquir-
ing Hyponymy Relations from Web Documents. In 
Proc. of HLT/NAACL-04. Boston, MA. 
Sundblad, H. Automatic Acquisition of Hyponyms 
and Meronyms from Question Corpora. 2002. In 
Proc. of the ECAI-02 Workshop on Natural Lan-
guage Processing and Machine Learning for On-
tology Engineering. Lyon, France. 
Weeds, Julie, David Weir, and Diana McCarthy. 
2004. Characterizing Measures of Lexical Distribu-
tional Similarity. In Proc. of COLING-04. Geneva, 
Switzerland. 
Yangarber, Roman, Ralph Grishman, Pasi Tapanainen 
and Silja Huttunen. 2000. Automatic Acquisition 
of Domain Knowledge for Information Extraction. 
In Proc. of COLING-00. Saarbr?cken, Germany. 
586
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 456?463,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Instance-based Evaluation of Entailment Rule Acquisition
Idan Szpektor, Eyal Shnarch, Ido Dagan
Dept. of Computer Science
Bar Ilan University
Ramat Gan, Israel
{szpekti,shey,dagan}@cs.biu.ac.il
Abstract
Obtaining large volumes of inference knowl-
edge, such as entailment rules, has become
a major factor in achieving robust seman-
tic processing. While there has been sub-
stantial research on learning algorithms for
such knowledge, their evaluation method-
ology has been problematic, hindering fur-
ther research. We propose a novel evalua-
tion methodology for entailment rules which
explicitly addresses their semantic proper-
ties and yields satisfactory human agreement
levels. The methodology is used to compare
two state of the art learning algorithms, ex-
posing critical issues for future progress.
1 Introduction
In many NLP applications, such as Question An-
swering (QA) and Information Extraction (IE), it is
crucial to recognize that a particular target mean-
ing can be inferred from different text variants. For
example, a QA system needs to identify that ?As-
pirin lowers the risk of heart attacks? can be inferred
from ?Aspirin prevents heart attacks? in order to an-
swer the question ?What lowers the risk of heart at-
tacks??. This type of reasoning has been recognized
as a core semantic inference task by the generic tex-
tual entailment framework (Dagan et al, 2006).
A major obstacle for further progress in seman-
tic inference is the lack of broad-scale knowledge-
bases for semantic variability patterns (Bar-Haim et
al., 2006). One prominent type of inference knowl-
edge representation is inference rules such as para-
phrases and entailment rules. We define an entail-
ment rule to be a directional relation between two
templates, text patterns with variables, e.g. ?X pre-
vent Y ? X lower the risk of Y ?. The left-hand-
side template is assumed to entail the right-hand-
side template in certain contexts, under the same
variable instantiation. Paraphrases can be viewed
as bidirectional entailment rules. Such rules capture
basic inferences and are used as building blocks for
more complex entailment inference. For example,
given the above rule, the answer ?Aspirin? can be
identified in the example above.
The need for large-scale inference knowledge-
bases triggered extensive research on automatic ac-
quisition of paraphrase and entailment rules. Yet the
current precision of acquisition algorithms is typ-
ically still mediocre, as illustrated in Table 1 for
DIRT (Lin and Pantel, 2001) and TEASE (Szpek-
tor et al, 2004), two prominent acquisition algo-
rithms whose outputs are publicly available. The
current performance level only stresses the obvious
need for satisfactory evaluation methodologies that
would drive future research.
The prominent approach in the literature for eval-
uating rules, termed here the rule-based approach, is
to present the rules to human judges asking whether
each rule is correct or not. However, it is difficult to
explicitly define when a learned rule should be con-
sidered correct under this methodology, and this was
mainly left undefined in previous works. As the cri-
terion for evaluating a rule is not well defined, using
this approach often caused low agreement between
human judges. Indeed, the standards for evaluation
in this field are lower than other fields: many papers
456
don?t report on human agreement at all and those
that do report rather low agreement levels. Yet it
is crucial to reliably assess rule correctness in or-
der to measure and compare the performance of dif-
ferent algorithms in a replicable manner. Lacking a
good evaluation methodology has become a barrier
for further advances in the field.
In order to provide a well-defined evaluation
methodology we first explicitly specify when entail-
ment rules should be considered correct, following
the spirit of their usage in applications. We then
propose a new instance-based evaluation approach.
Under this scheme, judges are not presented only
with the rule but rather with a sample of sentences
that match its left hand side. The judges then assess
whether the rule holds under each specific example.
A rule is considered correct only if the percentage of
examples assessed as correct is sufficiently high.
We have experimented with a sample of input
verbs for both DIRT and TEASE. Our results show
significant improvement in human agreement over
the rule-based approach. It is also the first compar-
ison between such two state-of-the-art algorithms,
which showed that they are comparable in precision
but largely complementary in their coverage.
Additionally, the evaluation showed that both al-
gorithms learn mostly one-directional rules rather
than (symmetric) paraphrases. While most NLP ap-
plications need directional inference, previous ac-
quisition works typically expected that the learned
rules would be paraphrases. Under such an expec-
tation, unidirectional rules were assessed as incor-
rect, underestimating the true potential of these algo-
rithms. In addition, we observed that many learned
rules are context sensitive, stressing the need to learn
contextual constraints for rule applications.
2 Background: Entailment Rules and their
Evaluation
2.1 Entailment Rules
An entailment rule ?L ? R? is a directional rela-
tion between two templates, L and R. For exam-
ple, ?X acquire Y ? X own Y ? or ?X beat Y ?
X play against Y ?. Templates correspond to text
fragments with variables, and are typically either lin-
ear phrases or parse sub-trees.
The goal of entailment rules is to help applica-
Input Correct Incorrect
(?) X modify Y X adopt Y
X change Y (?) X amend Y X create Y
(DIRT) (?) X revise Y X stick to Y
(?) X alter Y X maintain Y
X change Y (?) X affect Y X follow Y
(TEASE) (?) X extend Y X use Y
Table 1: Examples of templates suggested by DIRT
and TEASE as having an entailment relation, in
some direction, with the input template ?X change
Y ?. The entailment direction arrows were judged
manually and added for readability.
tions infer one text variant from another. A rule can
be applied to a given text only when L can be in-
ferred from it, with appropriate variable instantia-
tion. Then, using the rule, the application deduces
that R can also be inferred from the text under the
same variable instantiation. For example, the rule
?X lose to Y ?Y beat X? can be used to infer ?Liv-
erpool beat Chelsea? from ?Chelsea lost to Liver-
pool in the semifinals?.
Entailment rules should typically be applied only
in specific contexts, which we term relevant con-
texts. For example, the rule ?X acquire Y ?
X buy Y ? can be used in the context of ?buying?
events. However, it shouldn?t be applied for ?Stu-
dents acquired a new language?. In the same man-
ner, the rule ?X acquire Y ?X learn Y ? should be
applied only when Y corresponds to some sort of
knowledge, as in the latter example.
Some existing entailment acquisition algorithms
can add contextual constraints to the learned rules
(Sekine, 2005), but most don?t. However, NLP ap-
plications usually implicitly incorporate some con-
textual constraints when applying a rule. For ex-
ample, when answering the question ?Which com-
panies did IBM buy?? a QA system would apply
the rule ?X acquire Y ?X buy Y ? correctly, since
the phrase ?IBM acquire X? is likely to be found
mostly in relevant economic contexts. We thus ex-
pect that an evaluation methodology should consider
context relevance for entailment rules. For example,
we would like both ?X acquire Y ?X buy Y ? and
?X acquire Y ?X learn Y ? to be assessed as cor-
rect (the second rule should not be deemed incorrect
457
just because it is not applicable in frequent economic
contexts).
Finally, we highlight that the common notion of
?paraphrase rules? can be viewed as a special case
of entailment rules: a paraphrase ?L? R? holds if
both templates entail each other. Following the tex-
tual entailment formulation, we observe that many
applied inference settings require only directional
entailment, and a requirement for symmetric para-
phrase is usually unnecessary. For example, in or-
der to answer the question ?Who owns Overture??
it suffices to use a directional entailment rule whose
right hand side is ?X own Y ?, such as ?X acquire
Y ?X own Y ?, which is clearly not a paraphrase.
2.2 Evaluation of Acquisition Algorithms
Many methods for automatic acquisition of rules
have been suggested in recent years, ranging from
distributional similarity to finding shared contexts
(Lin and Pantel, 2001; Ravichandran and Hovy,
2002; Shinyama et al, 2002; Barzilay and Lee,
2003; Szpektor et al, 2004; Sekine, 2005). How-
ever, there is still no common accepted framework
for their evaluation. Furthermore, all these methods
learn rules as pairs of templates {L,R} in a sym-
metric manner, without addressing rule directional-
ity. Accordingly, previous works (except (Szpektor
et al, 2004)) evaluated the learned rules under the
paraphrase criterion, which underestimates the prac-
tical utility of the learned rules (see Section 2.1).
One approach which was used for evaluating au-
tomatically acquired rules is to measure their contri-
bution to the performance of specific systems, such
as QA (Ravichandran and Hovy, 2002) or IE (Sudo
et al, 2003; Romano et al, 2006). While measuring
the impact of learned rules on applications is highly
important, it cannot serve as the primary approach
for evaluating acquisition algorithms for several rea-
sons. First, developers of acquisition algorithms of-
ten do not have access to the different applications
that will later use the learned rules as generic mod-
ules. Second, the learned rules may affect individual
systems differently, thus making observations that
are based on different systems incomparable. Third,
within a complex system it is difficult to assess the
exact quality of entailment rules independently of
effects of other system components.
Thus, as in many other NLP learning settings,
a direct evaluation is needed. Indeed, the promi-
nent approach for evaluating the quality of rule ac-
quisition algorithms is by human judgment of the
learned rules (Lin and Pantel, 2001; Shinyama et
al., 2002; Barzilay and Lee, 2003; Pang et al, 2003;
Szpektor et al, 2004; Sekine, 2005). In this evalua-
tion scheme, termed here the rule-based approach, a
sample of the learned rules is presented to the judges
who evaluate whether each rule is correct or not. The
criterion for correctness is not explicitly described in
most previous works. By the common view of con-
text relevance for rules (see Section 2.1), a rule was
considered correct if the judge could think of rea-
sonable contexts under which it holds.
We have replicated the rule-based methodology
but did not manage to reach a 0.6 Kappa agree-
ment level between pairs of judges. This approach
turns out to be problematic because the rule correct-
ness criterion is not sufficiently well defined and is
hard to apply. While some rules might obviously
be judged as correct or incorrect (see Table 1), judg-
ment is often more difficult due to context relevance.
One judge might come up with a certain context
that, to her opinion, justifies the rule, while another
judge might not imagine that context or think that
it doesn?t sufficiently support rule correctness. For
example, in our experiments one of the judges did
not identify the valid ?religious holidays? context
for the correct rule ?X observe Y ?X celebrate Y ?.
Indeed, only few earlier works reported inter-judge
agreement level, and those that did reported rather
low Kappa values, such as 0.54 (Barzilay and Lee,
2003) and 0.55 - 0.63 (Szpektor et al, 2004).
To conclude, the prominent rule-based methodol-
ogy for entailment rule evaluation is not sufficiently
well defined. It results in low inter-judge agreement
which prevents reliable and consistent assessments
of different algorithms.
3 Instance-based Evaluation Methodology
As discussed in Section 2.1, an evaluation methodol-
ogy for entailment rules should reflect the expected
validity of their application within NLP systems.
Following that line, an entailment rule ?L ? R?
should be regarded as correct if in all (or at least
most) relevant contexts in which the instantiated
template L is inferred from the given text, the instan-
458
Rule Sentence Judgment
1 X seek Y ?X disclose Y If he is arrested, he can immediately seek bail. Left not entailed
2 X clarify Y ?X prepare Y He didn?t clarify his position on the subject. Left not entailed
3 X hit Y ?X approach Y Other earthquakes have hit Lebanon since ?82. Irrelevant context
4 X lose Y ?X surrender Y Bread has recently lost its subsidy. Irrelevant context
5 X regulate Y ?X reform Y The SRA regulates the sale of sugar. No entailment
6 X resign Y ?X share Y Lopez resigned his post at VW last week. No entailment
7 X set Y ?X allow Y The committee set the following refunds. Entailment holds
8 X stress Y ?X state Y Ben Yahia also stressed the need for action. Entailment holds
Table 2: Rule evaluation examples and their judgment.
tiated template R is also inferred from the text. This
reasoning corresponds to the common definition of
entailment in semantics, which specifies that a text
L entails another text R if R is true in every circum-
stance (possible world) in which L is true (Chierchia
and McConnell-Ginet, 2000).
It follows that in order to assess if a rule is cor-
rect we should judge whether R is typically en-
tailed from those sentences that entail L (within rel-
evant contexts for the rule). We thus present a new
evaluation scheme for entailment rules, termed the
instance-based approach. At the heart of this ap-
proach, human judges are presented not only with
a rule but rather with a sample of examples of the
rule?s usage. Instead of thinking up valid contexts
for the rule the judges need to assess the rule?s va-
lidity under the given context in each example. The
essence of our proposal is a (apparently non-trivial)
protocol of a sequence of questions, which deter-
mines rule validity in a given sentence.
We shall next describe how we collect a sample of
examples for evaluation and the evaluation process.
3.1 Sampling Examples
Given a rule ?L?R?, our goal is to generate evalua-
tion examples by finding a sample of sentences from
which L is entailed. We do that by automatically re-
trieving, from a given corpus, sentences that match
L and are thus likely to entail it, as explained below.
For each example sentence, we automatically ex-
tract the arguments that instantiate L and generate
two phrases, termed left phrase and right phrase,
which are constructed by instantiating the left tem-
plate L and the right template R with the extracted
arguments. For example, the left and right phrases
generated for example 1 in Table 2 are ?he seek bail?
and ?he disclose bail?, respectively.
Finding sentences that match L can be performed
at different levels. In this paper we match lexical-
syntactic templates by finding a sub-tree of the sen-
tence parse that is identical to the template structure.
Of course, this matching method is not perfect and
will sometimes retrieve sentences that do not entail
the left phrase for various reasons, such as incorrect
sentence analysis or semantic aspects like negation,
modality and conditionals. See examples 1-2 in Ta-
ble 2 for sentences that syntactically match L but
do not entail the instantiated left phrase. Since we
should assess R?s entailment only from sentences
that entail L, such sentences should be ignored by
the evaluation process.
3.2 Judgment Questions
For each example generated for a rule, the judges are
presented with the given sentence and the left and
right phrases. They primarily answer two questions
that assess whether entailment holds in this example,
following the semantics of entailment rule applica-
tion as discussed above:
Qle: Is the left phrase entailed from the sentence?
A positive/negative answer corresponds to a
?Left entailed/not entailed? judgment.
Qre: Is the right phrase entailed from the sentence?
A positive/negative answer corresponds to an
?Entailment holds/No entailment? judgment.
The first question identifies sentences that do not en-
tail the left phrase, and thus should be ignored when
evaluating the rule?s correctness. While inappropri-
ate matches of the rule left-hand-side may happen
459
and harm an overall system precision, such errors
should be accounted for a system?s rule matching
module rather than for the rules? precision. The sec-
ond question assesses whether the rule application is
valid or not for the current example. See examples
5-8 in Table 2 for cases where entailment does or
doesn?t hold.
Thus, the judges focus only on the given sentence
in each example, so the task is actually to evaluate
whether textual entailment holds between the sen-
tence (text) and each of the left and right phrases
(hypotheses). Following past experience in textual
entailment evaluation (Dagan et al, 2006) we expect
a reasonable agreement level between judges.
As discussed in Section 2.1, we may want to ig-
nore examples whose context is irrelevant for the
rule. To optionally capture this distinction, the
judges are asked another question:
Qrc: Is the right phrase a likely phrase in English?
A positive/negative answer corresponds to a
?Relevant/Irrelevant context? evaluation.
If the right phrase is not likely in English then the
given context is probably irrelevant for the rule, be-
cause it seems inherently incorrect to infer an im-
plausible phrase. Examples 3-4 in Table 2 demon-
strate cases of irrelevant contexts, which we may
choose to ignore when assessing rule correctness.
3.3 Evaluation Process
For each example, the judges are presented with the
three questions above in the following order: (1) Qle
(2) Qrc (3) Qre. If the answer to a certain question
is negative then we do not need to present the next
questions to the judge: if the left phrase is not en-
tailed then we ignore the sentence altogether; and if
the context is irrelevant then the right phrase cannot
be entailed from the sentence and so the answer to
Qre is already known as negative.
The above entailment judgments assume that we
can actually ask whether the left or right phrases
are correct given the sentence, that is, we assume
that a truth value can be assigned to both phrases.
This is the case when the left and right templates
correspond, as expected, to semantic relations. Yet
sometimes learned templates are (erroneously) not
relational, e.g. ?X , Y , IBM? (representing a list).
We therefore let the judges initially mark rules that
include such templates as non-relational, in which
case their examples are not evaluated at all.
3.4 Rule Precision
We compute the precision of a rule by the percent-
age of examples for which entailment holds out
of all ?relevant? examples. We can calculate the
precision in two ways, as defined below, depending
on whether we ignore irrelevant contexts or not
(obtaining lower precision if we don?t). When
systems answer an information need, such as a
query or question, irrelevant contexts are sometimes
not encountered thanks to additional context which
is present in the given input (see Section 2.1). Thus,
the following two measures can be viewed as upper
and lower bounds for the expected precision of the
rule applications in actual systems:
upper bound precision: #Entailment holds#Relevant context
lower bound precision: #Entailment holds#Left entailed
where # denotes the number of examples with
the corresponding judgment.
Finally, we consider a rule to be correct only if
its precision is at least 80%, which seems sensible
for typical applied settings. This yields two alterna-
tive sets of correct rules, corresponding to the upper
bound and lower bound precision measures. Even
though judges may disagree on specific examples for
a rule, their judgments may still agree overall on the
rule?s correctness. We therefore expect the agree-
ment level on rule correctness to be higher than the
agreement on individual examples.
4 Experimental Settings
We applied the instance-based methodology to eval-
uate two state-of-the-art unsupervised acquisition al-
gorithms, DIRT (Lin and Pantel, 2001) and TEASE
(Szpektor et al, 2004), whose output is publicly
available. DIRT identifies semantically related tem-
plates in a local corpus using distributional sim-
ilarity over the templates? variable instantiations.
TEASE acquires entailment relations from the Web
for a given input template I by identifying charac-
teristic variable instantiations shared by I and other
templates.
460
For the experiment we used the published DIRT
and TEASE knowledge-bases1. For every given in-
put template I , each knowledge-base provides a list
of learned output templates {Oj}nI1 , where nI is the
number of output templates learned for I . Each out-
put template is suggested as holding an entailment
relation with the input template I , but the algorithms
do not specify the entailment direction(s). Thus,
each pair {I,Oj} induces two candidate directional
entailment rules: ?I?Oj? and ?Oj?I?.
4.1 Test Set Construction
The test set construction consists of three sampling
steps: selecting a set of input templates for the two
algorithms, selecting a sample of output rules to be
evaluated, and selecting a sample of sentences to be
judged for each rule.
First, we randomly selected 30 transitive verbs
out of the 1000 most frequent verbs in the Reuters
RCV1 corpus2. For each verb we manually
constructed a lexical-syntactic input template by
adding subject and object variables. For exam-
ple, for the verb ?seek? we constructed the template
?X subj??? seek obj??? Y ?.
Next, for each input template I we considered
the learned templates {Oj}nI1 from each knowledge-
base. Since DIRT has a long tail of templates with
a low score and very low precision, DIRT templates
whose score is below a threshold of 0.1 were filtered
out3. We then sampled 10% of the templates in each
output list, limiting the sample size to be between
5-20 templates for each list (thus balancing between
sufficient evaluation data and judgment load). For
each sampled template O we evaluated both direc-
tional rules, ?I?O? and ?O?I?. In total, we sam-
pled 380 templates, inducing 760 directional rules
out of which 754 rules were unique.
Last, we randomly extracted a sample of example
sentences for each rule ?L?R? by utilizing a search
engine over the first CD of Reuters RCV1. First, we
retrieved all sentences containing all lexical terms
within L. The retrieved sentences were parsed using
the Minipar dependency parser (Lin, 1998), keep-
ing only sentences that syntactically match L (as
1Available at http://aclweb.org/aclwiki/index.php?title=Te-
xtual Entailment Resource Pool
2http://about.reuters.com/researchandstandards/corpus/
3Following advice by Patrick Pantel, DIRT?s co-author.
explained in Section 3.1). A sample of 15 match-
ing sentences was randomly selected, or all match-
ing sentences if less than 15 were found. Finally,
an example for judgment was generated from each
sampled sentence and its left and right phrases (see
Section 3.1). We did not find sentences for 108
rules, and thus we ended up with 646 unique rules
that could be evaluated (with 8945 examples to be
judged).
4.2 Evaluating the Test-Set
Two human judges evaluated the examples. We
randomly split the examples between the judges.
100 rules (1287 examples) were cross annotated for
agreement measurement. The judges followed the
procedure in Section 3.3 and the correctness of each
rule was assessed based on both its upper and lower
bound precision values (Section 3.4).
5 Methodology Evaluation Results
We assessed the instance-based methodology by
measuring the agreement level between judges. The
judges agreed on 75% of the 1287 shared exam-
ples, corresponding to a reasonable Kappa value of
0.64. A similar kappa value of 0.65 was obtained
for the examples that were judged as either entail-
ment holds/no entailment by both judges. Yet, our
evaluation target is to assess rules, and the Kappa
values for the final correctness judgments of the
shared rules were 0.74 and 0.68 for the lower and
upper bound evaluations. These Kappa scores are
regarded as ?substantial agreement? and are substan-
tially higher than published agreement scores and
those we managed to obtain using the standard rule-
based approach. As expected, the agreement on
rules is higher than on examples, since judges may
disagree on a certain example but their judgements
would still yield the same rule assessment.
Table 3 illustrates some disagreements that were
still exhibited within the instance-based evaluation.
The primary reason for disagreements was the dif-
ficulty to decide whether a context is relevant for
a rule or not, resulting in some confusion between
?Irrelevant context? and ?No entailment?. This may
explain the lower agreement for the upper bound
precision, for which examples judged as ?Irrelevant
context? are ignored, while for the lower bound both
461
Rule Sentence Judge 1 Judge 2
X sign Y ?X set Y Iraq and Turkey sign agreement
to increase trade cooperation
Entailment holds Irrelevant context
X worsen Y ?X slow Y News of the strike worsened the
situation
Irrelevant context No entailment
X get Y ?X want Y He will get his parade on Tuesday Entailment holds No entailment
Table 3: Examples for disagreement between the two judges.
judgments are conflated and represent no entailment.
Our findings suggest that better ways for distin-
guishing relevant contexts may be sought in future
research for further refinement of the instance-based
evaluation methodology.
About 43% of all examples were judged as ?Left
not entailed?. The relatively low matching precision
(57%) made us collect more examples than needed,
since ?Left not entailed? examples are ignored. Bet-
ter matching capabilities will allow collecting and
judging fewer examples, thus improving the effi-
ciency of the evaluation process.
6 DIRT and TEASE Evaluation Results
DIRT TEASE
P Y P Y
Rules:
Upper Bound 30.5% 33.5 28.4% 40.3
Lower Bound 18.6% 20.4 17% 24.1
Templates:
Upper Bound 44% 22.6 38% 26.9
Lower Bound 27.3% 14.1 23.6% 16.8
Table 4: Average Precision (P) and Yield (Y) at the
rule and template levels.
We evaluated the quality of the entailment rules
produced by each algorithm using two scores: (1)
micro average Precision, the percentage of correct
rules out of all learned rules, and (2) average Yield,
the average number of correct rules learned for each
input template I , as extrapolated based on the sam-
ple4. Since DIRT and TEASE do not identify rule
directionality, we also measured these scores at the
4Since the rules are matched against the full corpus (as in IR
evaluations), it is difficult to evaluate their true recall.
template level, where an output template O is con-
sidered correct if at least one of the rules ?I?O? or
?O? I? is correct. The results are presented in Ta-
ble 4. The major finding is that the overall quality of
DIRT and TEASE is very similar. Under the specific
DIRT cutoff threshold chosen, DIRT exhibits some-
what higher Precision while TEASE has somewhat
higher Yield (recall that there is no particular natural
cutoff point for DIRT?s output).
Since applications typically apply rules in a spe-
cific direction, the Precision for rules reflects their
expected performance better than the Precision for
templates. Obviously, future improvement in pre-
cision is needed for rule learning algorithms. Mean-
while, manual filtering of the learned rules can prove
effective within limited domains, where our evalua-
tion approach can be utilized for reliable filtering as
well. The substantial yield obtained by these algo-
rithms suggest that they are indeed likely to be valu-
able for recall increase in semantic applications.
In addition, we found that only about 15% of the
correct templates were learned by both algorithms,
which implies that the two algorithms largely com-
plement each other in terms of coverage. One ex-
planation may be that DIRT is focused on the do-
main of the local corpus used (news articles for the
published DIRT knowledge-base), whereas TEASE
learns from the Web, extracting rules from multiple
domains. Since Precision is comparable it may be
best to use both algorithms in tandem.
We also measured whether O is a paraphrase of
I , i.e. whether both ?I ?O? and ?O? I? are cor-
rect. Only 20-25% of all correct templates were as-
sessed as paraphrases. This stresses the significance
of evaluating directional rules rather than only para-
phrases. Furthermore, it shows that in order to im-
prove precision, acquisition algorithms must iden-
tify rule directionality.
462
About 28% of all ?Left entailed? examples were
evaluated as ?Irrelevant context?, yielding the large
difference in precision between the upper and lower
precision bounds. This result shows that in order
to get closer to the upper bound precision, learning
algorithms and applications need to identify the rel-
evant contexts in which a rule should be applied.
Last, we note that the instance-based quality as-
sessment corresponds to the corpus from which the
example sentences were taken. It is therefore best to
evaluate the rules using a corpus of the same domain
from which they were learned, or the target applica-
tion domain for which the rules will be applied.
7 Conclusions
Accurate learning of inference knowledge, such as
entailment rules, has become critical for further
progress of applied semantic systems. However,
evaluation of such knowledge has been problematic,
hindering further developments. The instance-based
evaluation approach proposed in this paper obtained
acceptable agreement levels, which are substantially
higher than those obtained for the common rule-
based approach.
We also conducted the first comparison between
two state-of-the-art acquisition algorithms, DIRT
and TEASE, using the new methodology. We found
that their quality is comparable but they effectively
complement each other in terms of rule coverage.
Also, we found that most learned rules are not para-
phrases but rather one-directional entailment rules,
and that many of the rules are context sensitive.
These findings suggest interesting directions for fu-
ture research, in particular learning rule direction-
ality and relevant contexts, issues that were hardly
explored till now. Such developments can be then
evaluated by the instance-based methodology, which
was designed to capture these two important aspects
of entailment rules.
Acknowledgements
The authors would like to thank Ephi Sachs and
Iddo Greental for their evaluation. This work was
partially supported by ISF grant 1095/05, the IST
Programme of the European Community under the
PASCAL Network of Excellence IST-2002-506778,
and the ITC-irst/University of Haifa collaboration.
References
Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo
Giampiccolo, Bernardo Magnini, and Idan Szpektor.
2006. The second pascal recognising textual entail-
ment challenge. In Second PASCAL Challenge Work-
shop for Recognizing Textual Entailment.
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: An unsupervised approach using multiple-
sequence alignment. In Proceedings of NAACL-HLT.
Gennaro Chierchia and Sally McConnell-Ginet. 2000.
Meaning and Grammar (2nd ed.): an introduction to
semantics. MIT Press, Cambridge, MA.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The pascal recognising textual entailment chal-
lenge. Lecture Notes in Computer Science, 3944:177?
190.
Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules for question answering. Natural Language
Engineering, 7(4):343?360.
Dekang Lin. 1998. Dependency-based evaluation of
minipar. In Proceedings of the Workshop on Evalu-
ation of Parsing Systems at LREC.
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based alignment of multiple translations: Ex-
tracting paraphrases and generating new sentences. In
Proceedings of HLT-NAACL.
Deepak Ravichandran and Eduard Hovy. 2002. Learning
surface text patterns for a question answering system.
In Proceedings of ACL.
Lorenza Romano, Milen Kouylekov, Idan Szpektor, Ido
Dagan, and Alberto Lavelli. 2006. Investigating a
generic paraphrase-based approach for relation extrac-
tion. In Proceedings of EACL.
Satoshi Sekine. 2005. Automatic paraphrase discovery
based on context and keywords between ne pairs. In
Proceedings of IWP.
Yusuke Shinyama, Satoshi Sekine, Kiyoshi Sudo, and
Ralph Grishman. 2002. Automatic paraphrase acqui-
sition from news articles. In Proceedings of HLT.
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.
2003. An improved extraction pattern representation
model for automatic IE pattern acquisition. In Pro-
ceedings of ACL.
Idan Szpektor, Hristo Tanev, Ido Dagan, and Bonaven-
tura Coppola. 2004. Scaling web-based acquisition of
entailment relations. In Proceedings of EMNLP.
463
Proceedings of ACL-08: HLT, pages 683?691,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Contextual Preferences
Idan Szpektor, Ido Dagan, Roy Bar-Haim
Department of Computer Science
Bar-Ilan University
Ramat Gan, Israel
{szpekti,dagan,barhair}@cs.biu.ac.il
Jacob Goldberger
School of Engineering
Bar-Ilan University
Ramat Gan, Israel
goldbej@eng.biu.ac.il
Abstract
The validity of semantic inferences depends
on the contexts in which they are applied.
We propose a generic framework for handling
contextual considerations within applied in-
ference, termed Contextual Preferences. This
framework defines the various context-aware
components needed for inference and their
relationships. Contextual preferences extend
and generalize previous notions, such as se-
lectional preferences, while experiments show
that the extended framework allows improving
inference quality on real application data.
1 Introduction
Applied semantic inference is typically concerned
with inferring a target meaning from a given text.
For example, to answer ?Who wrote Idomeneo??,
Question Answering (QA) systems need to infer the
target meaning ?Mozart wrote Idomeneo? from a
given text ?Mozart composed Idomeneo?. Following
common Textual Entailment terminology (Giampic-
colo et al, 2007), we denote the target meaning by h
(for hypothesis) and the given text by t.
A typical applied inference operation is matching.
Sometimes, h can be directly matched in t (in the
example above, if the given sentence would be liter-
ally ?Mozart wrote Idomeneo?). Generally, the tar-
get meaning can be expressed in t in many differ-
ent ways. Indirect matching is then needed, using
inference knowledge that may be captured through
rules, termed here entailment rules. In our exam-
ple, ?Mozart wrote Idomeneo? can be inferred using
the rule ?X compose Y ? X write Y ?. Recently,
several algorithms were proposed for automatically
learning entailment rules and paraphrases (viewed
as bi-directional entailment rules) (Lin and Pantel,
2001; Ravichandran and Hovy, 2002; Shinyama et
al., 2002; Szpektor et al, 2004; Sekine, 2005).
A common practice is to try matching the struc-
ture of h, or of the left-hand-side of a rule r, within
t. However, context should be considered to allow
valid matching. For example, suppose that to find
acquisitions of companies we specify the target tem-
plate hypothesis (a hypothesis with variables) ?X ac-
quire Y ?. This h should not be matched in ?children
acquire language quickly?, because in this context
Y is not a company. Similarly, the rule ?X charge
Y ? X accuse Y ? should not be applied to ?This
store charged my account?, since the assumed sense
of ?charge? in the rule is different than its sense in
the text. Thus, the intended contexts for h and r
and the context within the given t should be prop-
erly matched to verify valid inference.
Context matching at inference time was of-
ten approached in an application-specific manner
(Harabagiu et al, 2003; Patwardhan and Riloff,
2007). Recently, some generic methods were pro-
posed to handle context-sensitive inference (Dagan
et al, 2006; Pantel et al, 2007; Downey et al, 2007;
Connor and Roth, 2007), but these usually treat
only a single aspect of context matching (see Sec-
tion 6). We propose a comprehensive framework for
handling various contextual considerations, termed
Contextual Preferences. It extends and generalizes
previous work, defining the needed contextual com-
ponents and their relationships. We also present and
implement concrete representation models and un-
683
supervised matching methods for these components.
While our presentation focuses on semantic infer-
ence using lexical-syntactic structures, the proposed
framework and models seem suitable for other com-
mon types of representations as well.
We applied our models to a test set derived from
the ACE 2005 event detection task, a standard In-
formation Extraction (IE) benchmark. We show the
benefits of our extended framework for textual in-
ference and present component-wise analysis of the
results. To the best of our knowledge, these are also
the first unsupervised results for event argument ex-
traction in the ACE 2005 dataset.
2 Contextual Preferences
2.1 Notation
As mentioned above, we follow the generic Tex-
tual Entailment (TE) setting, testing whether a target
meaning hypothesis h can be inferred from a given
text t. We allow h to be either a text or a template,
a text fragment with variables. For example, ?The
stock rose 8%? entails an instantiation of the tem-
plate hypothesis ?X gain Y ?. Typically, h represents
an information need requested in some application,
such as a target predicate in IE.
In this paper, we focus on parse-based lexical-
syntactic representation of texts and hypotheses, and
on the basic inference operation of matching. Fol-
lowing common practice (de Salvo Braz et al, 2005;
Romano et al, 2006; Bar-Haim et al, 2007), h is
syntactically matched in t if it can be embedded in
t?s parse tree. For template hypotheses, the matching
induces a mapping between h?s variables and their
instantiation in t.
Matching h in t can be performed either directly
or indirectly using entailment rules. An entailment
rule r: ?LHS ? RHS? is a directional entailment
relation between two templates. h is matched in t us-
ing r if LHS is matched in t and h matches RHS.
In the example above, r: ?X rise Y ?X gain Y ? al-
lows us to entail ?X gain Y ?, with ?stock? and ?8%?
instantiating h?s variables. We denote vars(z) the
set of variables of z, where z is a template or a rule.
2.2 Motivation
When matching considers only the structure of hy-
potheses, texts and rules it may result in incorrect
inference due to contextual mismatches. For exam-
ple, an IE system may identify mentions of public
demonstrations using the hypothesis h: ?X demon-
strate?. However, h should not be matched in ?Engi-
neers demonstrated the new system?, due to a mis-
match between the intended sense of ?demonstrate?
in h and its sense in t. Similarly, when looking for
physical attack mentions using the hypothesis ?X at-
tack Y ?, we should not utilize the rule r: ?X accuse
Y ?X attack Y ?, due to a mismatch between a ver-
bal attack in r and an intended physical attack in h.
Finally, r: ?X produce Y ? X lay Y ? (applicable
when X refers to poultry and Y to eggs) should not
be matched in t: ?Bugatti produce the fastest cars?,
due to a mismatch between the meanings of ?pro-
duce? in r and t. Overall, such incorrect inferences
may be avoided by considering contextual informa-
tion for t, h and r during their matching process.
2.3 The Contextual Preferences Framework
We propose the Contextual Preferences (CP) frame-
work for addressing context at inference time. In this
framework, the representation of an object z, where
z may be a text, a template or an entailment rule, is
enriched with contextual information denoted cp(z).
This information helps constraining or disambiguat-
ing the meaning of z, and is used to validate proper
matching between pairs of objects.
We consider two components within cp(z): (a)
a representation for the global (?topical?) context
in which z typically occurs, denoted cpg(z); (b)
a representation for the preferences and constraints
(?hard? preferences) on the possible terms that can
instantiate variables within z, denoted cpv(z). For
example, cpv(?X produce Y ? X lay Y ?) may
specify that X?s instantiations should be similar to
?chicken? or ?duck?.
Contextual Preferences are used when entailment
is assessed between a text t and a hypothesis h, ei-
ther directly or by utilizing an entailment-rule r. On
top of structural matching, we now require that the
Contextual Preferences of the participants in the in-
ference will also match. When h is directly matched
in t, we require that each component in cp(h) will
be matched with its counterpart in cp(t). When r is
utilized, we additionally require that cp(r) will be
matched with both cp(t) and cp(h). Figure 1 sum-
marizes the matching relationships between the CP
684
Figure 1: The directional matching relationships between
a hypothesis (h), an entailment rule (r) and a text (t) in the
Contextual Preferences framework.
components of h, t and r.
Like Textual Entailment inference, Contextual
Preferences matching is directional. When matching
h with t we require that the global context prefer-
ences specified by cpg(h) would subsume those in-
duced by cpg(t), and that the instantiations of h?s
variables in t would adhere to the preferences in
cpv(h) (since t should entail h, but not necessarily
vice versa). For example, if the preferred global con-
text of a hypothesis is sports, it would match a text
that discusses the more specific topic of basketball.
To implement the CP framework, concrete models
are needed for each component, specifying its repre-
sentation, how it is constructed, and an appropriate
matching procedure. Section 3 describes the specific
CP models that were implemented in this paper.
The CP framework provides a generic view of
contextual modeling in applied semantic inference.
Mapping from a specific application to the generic
framework follows the mappings assumed in the
Textual Entailment paradigm. For example, in QA
the hypothesis to be proved corresponds to the affir-
mative template derived from the question (e.g. h:
?X invented the PC? for ?Who invented the PC??).
Thus, cpg(h) can be constructed with respect to
the question?s focus while cpv(h) may be gener-
ated from the expected answer type (Moldovan et
al., 2000; Harabagiu et al, 2003). Construction of
hypotheses? CP for IE is demonstrated in Section 4.
3 Contextual Preferences Models
This section presents the current models that we im-
plemented for the various components of the CP
framework. For each component type we describe
its representation, how it is constructed, and a cor-
responding unsupervised match score. Finally, the
different component scores are combined to yield
an overall match score, which is used in our exper-
iments to rank inference instances by the likelihood
of their validity. Our goal in this paper is to cover the
entire scope of the CP framework by including spe-
cific models that were proposed in previous work,
where available, and elsewhere propose initial mod-
els to complete the CP scope.
3.1 Contextual Preferences for Global Context
To represent the global context of an object z we
utilize Latent Semantic Analysis (LSA) (Deerwester
et al, 1990), a well-known method for representing
the contextual-usage of words based on corpus sta-
tistics. We use LSA analysis of the BNC corpus1,
in which every term is represented by a normalized
vector of the top 100 SVD dimensions, as described
in (Gliozzo, 2005).
To construct cpg(z) we first collect a set of terms
that are representative for the preferred general con-
text of z. Then, the (single) vector which is the sum
of the LSA vectors of the representative terms be-
comes the representation of cpg(z). This LSA vec-
tor captures the ?average? typical contexts in which
the representative terms occur.
The set of representative terms for a text t con-
sists of all the nouns and verbs in it, represented
by their lemma and part of speech. For a rule r:
?LHS ? RHS?, the representative terms are the
words appearing in LHS and in RHS. For exam-
ple, the representative terms for ?X divorce Y ? X
marry Y ? are {divorce:v, marry:v}. As mentioned
earlier, construction of hypotheses and their contex-
tual preferences depends on the application at hand.
In our experiments these are defined manually, as
described in Section 4, derived from the manual de-
finitions of target meanings in the IE data.
The score of matching the cpg components of two
objects, denoted by mg(?, ?), is the Cosine similarity
of their LSA vectors. Negative values are set to 0.
3.2 Contextual Preferences for Variables
3.2.1 Representation
For comparison with prior work, we follow (Pan-
tel et al, 2007) and represent preferences for vari-
1http://www.natcorp.ox.ac.uk/
685
able instantiations using a distributional approach,
and in addition incorporate a standard specification
of named-entity types. Thus, cpv is represented by
two lists. The first list, denoted cpv:e, contains ex-
amples for valid instantiations of that variable. For
example, cpv:e(X kill Y ? Y die of X) may be
[X: {snakebite, disease}, Y : {man, patient}]. The
second list, denoted cpv:n, contains the variable?s
preferred named-entity types (if any). For exam-
ple, cpv:n(X born in Y ) may be [X: {Person}, Y :
{Location}]. We denote cpv:e(z)[j] and cpv:n(z)[j]
as the lists for a specific variable j of the object z.
For a text t, in which a template p is matched, the
preference cpv:e(t) for each template variable is sim-
ply its instantiation in t. For example, when ?X eat
Y ? is matched in t: ?Many Americans eat fish reg-
ularly?, we construct cpv:e(t) = [X: {Many Ameri-
cans}, Y : {fish}]. Similarly, cpv:n(t) for each vari-
able is the named-entity type of its instantiation in
t (if it is a named entity). We identify entity types
using the default Lingpipe2 Named-Entity Recog-
nizer (NER), which recognizes the types Location,
Person and Organization. In the above example,
cpv:n(t)[X] would be {Person}.
For a rule r: LHS ? RHS, we automatically
add to cpv:e(r) all the variable instantiations that
were found common for both LHS and RHS in a
corpus (see Section 4), as in (Pantel et al, 2007; Pen-
nacchiotti et al, 2007). To construct cpv:n(r), we
currently use a simple approach where each individ-
ual term in cpv:e(r) is analyzed by the NER system,
and its type (if any) is added to cpv:n(r).
For a template hypothesis, we currently repre-
sent cpv(h) only by its list of preferred named-entity
types, cpv:n. Similarly to cpg(h), the preferred types
for each template variable were adapted from those
defined in our IE data (see Section 4).
To allow compatible comparisons with previous
work (see Sections 5 and 6), we utilize in this
paper only cpv:e when matching between cpv(r)
and cpv(t), as only this representation was exam-
ined in prior work on context-sensitive rule applica-
tions. cpv:n is utilized for context matches involving
cpv(h). We denote the score of matching two cpv
components by mv(?, ?).
2http://www.alias-i.com/lingpipe/
3.2.2 Matching cpv:e
Our primary matching method is based on repli-
cating the best-performing method reported in (Pan-
tel et al, 2007), which utilizes the CBC distribu-
tional word clustering algorithm (Pantel, 2003). In
short, this method extends each cpv:e list with CBC
clusters that contain at least one term in the list, scor-
ing them according to their ?relevancy?. The score
of matching two cpv:e lists, denoted here SCBC(?, ?),
is the score of the highest scoring member that ap-
pears in both lists.
We applied the final binary match score presented
in (Pantel et al, 2007), denoted here binaryCBC:
mv:e(r, t) is 1 if SCBC(r, t) is above a threshold and
0 otherwise. As a more natural ranking method, we
also utilize SCBC directly, denoted rankedCBC,
having mv:e(r, t) = SCBC(r, t).
In addition, we tried a simpler method that di-
rectly compares the terms in two cpv:e lists, uti-
lizing the commonly-used term similarity metric of
(Lin, 1998a). This method, denoted LIN , uses the
same raw distributional data as CBC but computes
only pair-wise similarities, without any clustering
phase. We calculated the scores of the 1000 most
similar terms for every term in the Reuters RVC1
corpus3. Then, a directional similarity of term a
to term b, s(a, b), is set to be their similarity score
if a is in b?s 1000 most similar terms and 0 other-
wise. The final score of matching r with t is deter-
mined by a nearest-neighbor approach, as the score
of the most similar pair of terms in the correspond-
ing two lists of the same variable: mv:e(r, t) =
maxj?vars(r)[maxa?cpv:e(t)[j],b?cpv:e(r)[j][s(a, b)]].
3.2.3 Matching cpv:n
We use a simple scoring mechanism for compar-
ing between two named-entity types a and b, s(a, b):
1 for identical types and 0.8 otherwise.
A variable j has a single preferred entity type
in cpv:n(t)[j], the type of its instantiation in t.
However, it can have several preferred types for h.
When matching h with t, j?s match score is that
of its highest scoring type, and the final score is
the product of all variable scores: mv:n(h, t) =?
j?vars(h)(maxa?cpv:n(h)[j][s(a, cpv:n(t)[j])]).
Variable j may also have several types in r, the
3http://about.reuters.com/researchandstandards/corpus/
686
types of the common arguments in cpv:e(r). When
matching h with r, s(a, cpv:n(t)[j]) is replaced with
the average score for a and each type in cpv:n(r)[j].
3.3 Overall Score for a Match
A final score for a given match, denoted allCP, is
obtained by the product of all six matching scores
of the various CP components (multiplying by 1
if a component score is missing). The six scores
are the results of matching any of the two compo-
nents of h, t and r: mg(h, t), mv(h, t), mg(h, r),
mv(h, r), mg(r, t) and mv(r, t) (as specified above,
mv(r, t) is based on matching cpv:e while mv(h, r)
and mv(h, t) are based on matching cpv:n). We use
rankedCBC for calculating mv(r, t).
Unlike previous work (e.g. (Pantel et al, 2007)),
we also utilize the prior score of a rule r, which
is provided by the rule-learning algorithm (see next
section). We denote by allCP+pr the final match
score obtained by the product of the allCP score
with the prior score of the matched rule.
4 Experimental Settings
Evaluating the contribution of Contextual Prefer-
ences models requires: (a) a sample of test hypothe-
ses, and (b) a corresponding corpus that contains
sentences which entail these hypotheses, where all
hypothesis matches (either direct or via rules) are an-
notated. We found that the available event mention
annotations in the ACE 2005 training set4 provide a
useful test set that meets these generic criteria, with
the added value of a standard real-world dataset.
The ACE annotation includes 33 types of events,
for which all event mentions are annotated in the
corpus. The annotation of each mention includes the
instantiated arguments for the predicates, which rep-
resent the participants in the event, as well as general
attributes such as time and place. ACE guidelines
specify for each event type its possible arguments,
where all arguments are optional. Each argument is
associated with a semantic role and a list of possible
named-entity types. For instance, an Injure event
may have the arguments {Agent, Victim, Instrument,
Time, Place}, where Victim should be a person.
For each event type we manually created a small
set of template hypotheses that correspond to the
4http://projects.ldc.upenn.edu/ace/
given event predicate, and specified the appropri-
ate semantic roles for each variable. We consid-
ered only binary hypotheses, due to the type of
available entailment rules (see below). For In-
jure, the set of hypotheses included ?A injure V?
and ?injure V in T? where role(A)={Agent, In-
strument}, role(V)={Victim}, and role(T)={Time,
Place}. Thus, correct match of an argument corre-
sponds to correct role identification. The templates
were represented as Minipar (Lin, 1998b) depen-
dency parse-trees.
The Contextual Preferences for h were con-
structed manually: the named-entity types for
cpv:n(h) were set by adapting the entity types given
in the guidelines to the types supported by the Ling-
pipe NER (described in Section 3.2). cpg(h) was
generated from a short list of nouns and verbs that
were extracted from the verbal event definition in
the ACE guidelines. For Injure, this list included
{injure:v, injury:n, wound:v}. This assumes that
when writing down an event definition the user
would also specify such representative keywords.
Entailment-rules for a given h (rules in which
RHS is equal to h) were learned automatically by
the DIRT algorithm (Lin and Pantel, 2001), which
also produces a quality score for each rule. We im-
plemented a canonized version of DIRT (Szpektor
and Dagan, 2007) on the Reuters corpus parsed by
Minipar. Each rule?s arguments for cpv(r) were also
collected from this corpus.
We assessed the CP framework by its ability to
correctly rank, for each predicate (event), all the
candidate entailing mentions that are found for it
in the test corpus. Such ranking evaluation is suit-
able for unsupervised settings, with a perfect rank-
ing placing all correct mentions before any incor-
rect ones. The candidate mentions are found in the
parsed test corpus by matching the specified event
hypotheses, either directly or via the given set of en-
tailment rules, using a syntactic matcher similar to
the one in (Szpektor and Dagan, 2007). Finally, the
mentions are ranked by their match scores, as de-
scribed in Section 3.3. As detailed in the next sec-
tion, those candidate mentions which are also an-
notated as mentions of the same event in ACE are
considered correct.
The evaluation aims to assess the correctness of
inferring a target semantic meaning, which is de-
687
noted by a specific predicate. Therefore, we elim-
inated four ACE event types that correspond to mul-
tiple distinct predicates. For instance, the Transfer-
Money event refers to both donating and lending
money, which are not distinguished by the ACE an-
notation. We also omitted three events with less than
10 mentions and two events for which the given set
of learned rules could not match any mention. We
were left with 24 event types for evaluation, which
amount to 4085 event mentions in the dataset. Out of
these, our binary templates can correctly match only
mentions with at least two arguments, which appear
2076 times in the dataset.
Comparing with previous evaluation methodolo-
gies, in (Szpektor et al, 2007; Pantel et al, 2007)
proper context matching was evaluated by post-hoc
judgment of a sample of rule applications for a sam-
ple of rules. Such annotation needs to be repeated
each time the set of rules is changed. In addition,
since the corpus annotation is not exhaustive, re-
call could not be computed. By contrast, we use a
standard real-world dataset, in which all mentions
are annotated. This allows immediate comparison
of different rule sets and matching methods, without
requiring any additional (post-hoc) annotation.
5 Results and Analysis
We experimented with three rule setups over the
ACE dataset, in order to measure the contribution
of the CP framework. In the first setup no rules are
used, applying only direct matches of template hy-
potheses to identify event mentions. In the other two
setups we also utilized DIRT?s top 50 or 100 rules
for each hypothesis.
A match is considered correct when all matched
arguments are extracted correctly according to their
annotated event roles. This main measurement is de-
noted All. As an additional measurement, denoted
Any, we consider a match as correct if at least one
argument is extracted correctly.
Once event matches are extracted, we first mea-
sure for each event its Recall, the number of correct
mentions identified out of all annotated event men-
tions5 and Precision, the number of correct matches
out of all extracted candidate matches. These figures
5For Recall, we ignored mentions with less than two argu-
ments, as they cannot be correctly matched by binary templates.
quantify the baseline performance of the DIRT rule
set used. To assess our ranking quality, we measure
for each event the commonly used Average Preci-
sion (AP) measure (Voorhees and Harmann, 1998),
which is the area under the non-interpolated recall-
precision curve, while considering for each setup all
correct extracted matches as 100% Recall. Overall,
we report Mean Average Precision (MAP), macro
average Precision and macro average Recall over the
ACE events. Tables 1 and 2 summarize the main re-
sults of our experiments. As far as we know, these
are the first published unsupervised results for iden-
tifying event arguments in the ACE 2005 dataset.
Examining Recall, we see that it increases sub-
stantially when rules are applied: by more than
100% for the top 50 rules, and by about 150% for
the top 100, showing the benefit of entailment-rules
to covering language variability. The difference be-
tween All and Any results shows that about 65%
of the rules that correctly match one argument also
match correctly both arguments.
We use two baselines for measuring the CP rank-
ing contribution: Precision, which corresponds to
the expected MAP of random ranking, and MAP
of ranking using the prior rule score provided by
DIRT. Without rules, the baseline All Precision is
34.1%, showing that even the manually constructed
hypotheses, which correspond directly to the event
predicate, extract event mentions with limited accu-
racy when context is ignored. When rules are ap-
plied, Precision is very low. But ranking is consider-
ably improved using only the prior score (from 1.4%
to 22.7% for 50 rules), showing that the prior is an
informative indicator for valid matches.
Our main result is that the allCP and allCP+pr
methods rank matches statistically significantly bet-
ter than the baselines in all setups (according to the
Wilcoxon double-sided signed-ranks test at the level
of 0.01 (Wilcoxon, 1945)). In the All setup, ranking
is improved by 70% for direct matching (Table 1).
When entailment-rules are also utilized, prior-only
ranking is improved by about 35% and 50% when
using allCP and allCP+pr, respectively (Table 2).
Figure 2 presents the average Recall-Precision curve
of the ?50 Rules, All? setup for applying allCP or
allCP+pr, compared to prior-only ranking baseline
(other setups behave similarly). The improvement
in ranking is evident: the drop in precision is signif-
688
R P MAP (%)
(%) (%) cpv cpg allCP
All 14.0 34.1 46.5 52.2 60.2
Any 21.8 66.0 72.2 80.5 84.1
Table 1: Recall (R), Precision (P) and Mean Average Pre-
cision (MAP) when only matching template hypotheses
directly.
# R P MAP (%)
Rules (%) (%) prior allCP allCP+pr
All 50 29.6 1.4 22.7 30.6 34.1100 34.9 0.7 20.5 26.3 30.2
Any 50 46.5 3.5 41.2 43.7 48.6100 52.9 1.8 35.5 35.1 40.8
Table 2: Recall (R), Precision (P) and Mean Average Pre-
cision (MAP) when also using rules for matching.
icantly slower when CP is used. The behavior of CP
with and without the prior is largely the same up to
50% Recall, but later on our implemented CP mod-
els are noisier and should be combined with the prior
rule score.
Templates are incorrectly matched for several rea-
sons. First, there are context mismatches which are
not scored sufficiently low by our models. Another
main cause is incorrect learned rules in which LHS
and RHS are topically related, e.g. ?X convict Y ?
X arrest Y ?, or rules that are used in the wrong en-
tailment direction, e.g. ?X marry Y ?X divorce Y ?
(DIRT does not learn rule direction). As such rules
do correspond to plausible contexts of the hypothe-
sis, their matches obtain relatively high CP scores.
In addition, some incorrect matches are caused by
our syntactic matcher, which currently does not han-
dle certain phenomena such as co-reference, modal-
ity or negation, and due to Minipar parse errors.
5.1 Component Analysis
Table 3 displays the contribution of different CP
components to ranking, when adding only that com-
ponent?s match score to the baselines, and under ab-
lation tests, when using all CP component scores ex-
cept the tested component, with or without the prior.
As it turns out, matching h with t (i.e. cp(h, t),
which combines cpg(h, t) and cpv(h, t)) is most use-
ful. With our current models, using only cp(h, t)
along with the prior, while ignoring cp(r), achieves
50 Rules  -  All
0
10
20
30
40
50
60
70
80
90
100
0 10 20 30 40 50 60 70 80 90 100Relative Recall
Prec
ision
baseline CP CP + prior
Figure 2: Recall-Precision curves for ranking using: (a)
only the prior (baseline); (b) allCP; (c) allCP+pr.
the highest score in the table. The strong impact of
matching h and t?s preferences is also evident in Ta-
ble 1, where ranking based on either cpg or cpv sub-
stantially improves precision, while their combina-
tion provides the best ranking. These results indicate
that the two CP components capture complementary
information and both are needed to assess the cor-
rectness of a match.
When ignoring the prior rule score, cp(r, t) is the
major contributor over the baseline Precision. For
cpv(r, t), this is in synch with the result in (Pantel
et al, 2007), which is based on this single model
without utilizing prior rule scores. On the other
hand, cpv(r, t) does not improve the ranking when
the prior is used, suggesting that this contextual
model for the rule?s variables is not stronger than the
context-insensitive prior rule score. Furthermore,
relative to this cpv(r, t) model from (Pantel et al,
2007), our combined allCP model, with or without
the prior (first row of Table 2), obtains statistically
significantly better ranking (at the level of 0.01).
Comparing between the algorithms for match-
ing cpv:e (Section 3.2.2) we found that while
rankedCBC is statistically significantly better than
binaryCBC, rankedCBC and LIN generally
achieve the same results. When considering the
tradeoffs between the two, LIN is based on a much
simpler learning algorithm while CBC?s output is
more compact and allows faster CP matches.
689
Addition To Ablation From
P prior allCP allCP+pr
Baseline 1.4 22.7 30.6 34.1
cpg(h, t) ?10.4 ?35.4 32.4 33.7
cpv(h, t) ?11.0 29.9 27.6 32.9
cp(h, t) ?8.9 ?37.5 28.6 30.0
cpg(r, t) ?4.2 ?30.6 32.5 35.4
cpv(r, t) ?21.7 21.9 ?12.9 33.6
cp(r, t) ?26.0 ?29.6 ?17.9 36.8
cpg(h, r) ?8.1 22.4 31.9 34.3
cpv(h, r) ?10.7 22.7 ?27.9 34.4
cp(h, r) ?16.5 22.4 ?29.2 34.4
cpg(h, r, t) ?7.7 ?30.2 ?27.5 ?29.2
cpv(h, r, t) ?27.5 29.2 ?7.7 30.2
? Indicates statistically significant changes compared to the baseline,
according to the Wilcoxon test at the level of 0.01.
Table 3: MAP(%), under the ?50 rules, All? setup, when
adding component match scores to Precision (P) or prior-
only MAP baselines, and when ranking with allCP or
allCP+pr methods but ignoring that component scores.
Currently, some models do not improve the re-
sults when the prior is used. Yet, we would like to
further weaken the dependency on the prior score,
since it is biased towards frequent contexts. We
aim to properly identify also infrequent contexts (or
meanings) at inference time, which may be achieved
by better CP models. More generally, when used
on top of all other components, some of the mod-
els slightly degrade performance, as can be seen by
those figures in the ablation tests which are higher
than the corresponding baseline. However, due to
their different roles, each of the matching compo-
nents might capture some unique preferences. For
example, cp(h, r) should be useful to filter out rules
that don?t match the intended meaning of the given
h. Overall, this suggests that future research for bet-
ter models should aim to obtain a marginal improve-
ment by each component.
6 Related Work
Context sensitive inference was mainly investigated
in an application-dependent manner. For exam-
ple, (Harabagiu et al, 2003) describe techniques for
identifying the question focus and the answer type in
QA. (Patwardhan and Riloff, 2007) propose a super-
vised approach for IE, in which relevant text regions
for a target relation are identified prior to applying
extraction rules.
Recently, the need for context-aware inference
was raised (Szpektor et al, 2007). (Pantel et al,
2007) propose to learn the preferred instantiations of
rule variables, termed Inferential Selectional Prefer-
ences (ISP). Their clustering-based model is the one
we implemented for mv(r, t). A similar approach
is taken in (Pennacchiotti et al, 2007), where LSA
similarity is used to compare between the preferred
variable instantiations for a rule and their instanti-
ations in the matched text. (Downey et al, 2007)
use HMM-based similarity for the same purpose.
All these methods are analogous to matching cpv(r)
with cpv(t) in the CP framework.
(Dagan et al, 2006; Connor and Roth, 2007) pro-
posed generic approaches for identifying valid appli-
cations of lexical rules by classifying the surround-
ing global context of a word as valid or not for that
rule. These approaches are analogous to matching
cpg(r) with cpg(t) in our framework.
7 Conclusions
We presented the Contextual Preferences (CP)
framework for assessing the validity of inferences
in context. CP enriches the representation of tex-
tual objects with typical contextual information that
constrains or disambiguates their meaning, and pro-
vides matching functions that compare the prefer-
ences of objects involved in the inference. Experi-
ments with our implemented CP models, over real-
world IE data, show significant improvements rela-
tive to baselines and some previous work.
In future research we plan to investigate improved
models for representing and matching CP, and to ex-
tend the experiments to additional applied datasets.
We also plan to apply the framework to lexical infer-
ence rules, for which it seems directly applicable.
Acknowledgements
The authors would like to thank Alfio Massimiliano
Gliozzo for valuable discussions. This work was
partially supported by ISF grant 1095/05, the IST
Programme of the European Community under the
PASCAL Network of Excellence IST-2002-506778,
the NEGEV project (www.negev-initiative.org) and
the FBK-irst/Bar-Ilan University collaboration.
690
References
Roy Bar-Haim, Ido Dagan, Iddo Greental, and Eyal
Shnarch. 2007. Semantic inference at the lexical-
syntactic level. In Proceedings of AAAI.
Michael Connor and Dan Roth. 2007. Context sensitive
paraphrasing with a global unsupervised classifier. In
Proceedings of the European Conference on Machine
Learning (ECML).
Ido Dagan, Oren Glickman, Alfio Gliozzo, Efrat Mar-
morshtein, and Carlo Strapparava. 2006. Direct word
sense matching for lexical substitution. In Proceed-
ings of the 21st International Conference on Compu-
tational Linguistics and 44th Annual Meeting of ACL.
Rodrigo de Salvo Braz, Roxana Girju, Vasin Pun-
yakanok, Dan Roth, and Mark Sammons. 2005. An
inference model for semantic entailment in natural lan-
guage. In Proceedings of the National Conference on
Artificial Intelligence (AAAI).
Scott C. Deerwester, Susan T. Dumais, Thomas K. Lan-
dauer, George W. Furnas, and Richard A. Harshman.
1990. Indexing by latent semantic analysis. Jour-
nal of the American Society of Information Science,
41(6):391?407.
Doug Downey, Stefan Schoenmackers, and Oren Etzioni.
2007. Sparse information extraction: Unsupervised
language models to the rescue. In Proceedings of the
45th Annual Meeting of ACL.
Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and
Bill Dolan. 2007. The third pascal recognizing tex-
tual entailment challenge. In Proceedings of the ACL-
PASCAL Workshop on Textual Entailment and Para-
phrasing.
Alfio Massimiliano Gliozzo. 2005. Semantic Domains
in Computational Linguistics. Ph.D. thesis. Advisor-
Carlo Strapparava.
Sanda M. Harabagiu, Steven J. Maiorano, and Marius A.
Pas?ca. 2003. Open-domain textual question answer-
ing techniques. Nat. Lang. Eng., 9(3):231?267.
Dekang Lin and Patrick Pantel. 2001. Discovery of in-
ference rules for question answering. In Natural Lan-
guage Engineering, volume 7(4), pages 343?360.
Dekang Lin. 1998a. Automatic retrieval and clustering
of similar words. In Proceedings of COLING-ACL.
Dekang Lin. 1998b. Dependency-based evaluation of
minipar. In Proceedings of the Workshop on Evalua-
tion of Parsing Systems at LREC.
Dan Moldovan, Sanda Harabagiu, Marius Pasca, Rada
Mihalcea, Roxana Girju, Richard Goodrum, and
Vasile Rus. 2000. The structure and performance of
an open-domain question answering system. In Pro-
ceedings of the 38th Annual Meeting of ACL.
Patrick Pantel, Rahul Bhagat, Bonaventura Coppola,
Timothy Chklovski, and Eduard Hovy. 2007. ISP:
Learning inferential selectional preferences. In Hu-
man Language Technologies 2007: The Conference of
NAACL; Proceedings of the Main Conference.
Patrick Andre Pantel. 2003. Clustering by committee.
Ph.D. thesis. Advisor-Dekang Lin.
Siddharth Patwardhan and Ellen Riloff. 2007. Effec-
tive information extraction with semantic affinity pat-
terns and relevant regions. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL).
Marco Pennacchiotti, Roberto Basili, Diego De Cao, and
Paolo Marocco. 2007. Learning selectional prefer-
ences for entailment or paraphrasing rules. In Pro-
ceedings of RANLP.
Deepak Ravichandran and Eduard Hovy. 2002. Learning
surface text patterns for a question answering system.
In Proceedings of the 40th Annual Meeting of ACL.
Lorenza Romano, Milen Kouylekov, Idan Szpektor, Ido
Dagan, and Alberto Lavelli. 2006. Investigating a
generic paraphrase-based approach for relation extrac-
tion. In Proceedings of the 11th Conference of the
EACL.
Satoshi Sekine. 2005. Automatic paraphrase discovery
based on context and keywords between ne pairs. In
Proceedings of IWP.
Yusuke Shinyama, Satoshi Sekine, Sudo Kiyoshi, and
Ralph Grishman. 2002. Automatic paraphrase acqui-
sition from news articles. In Proceedings of Human
Language Technology Conference.
Idan Szpektor and Ido Dagan. 2007. Learning canonical
forms of entailment rules. In Proceedings of RANLP.
Idan Szpektor, Hristo Tanev, Ido Dagan, and Bonaven-
tura Coppola. 2004. Scaling web-based acquisition of
entailment relations. In Proceedings of EMNLP 2004,
pages 41?48, Barcelona, Spain.
Idan Szpektor, Eyal Shnarch, and Ido Dagan. 2007.
Instance-based evaluation of entailment rule acquisi-
tion. In Proceedings of the 45th Annual Meeting of
ACL.
Ellen M. Voorhees and Donna Harmann. 1998.
Overview of the seventh text retrieval conference
(trec?7). In The Seventh Text Retrieval Conference.
Frank Wilcoxon. 1945. Individual comparisons by rank-
ing methods. Biometrics Bulletin, 1(6):80?83.
691
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 450?458,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Extracting Lexical Reference Rules from Wikipedia
Eyal Shnarch
Computer Science Department
Bar-Ilan University
Ramat-Gan 52900, Israel
shey@cs.biu.ac.il
Libby Barak
Dept. of Computer Science
University of Toronto
Toronto, Canada M5S 1A4
libbyb@cs.toronto.edu
Ido Dagan
Computer Science Department
Bar-Ilan University
Ramat-Gan 52900, Israel
dagan@cs.biu.ac.il
Abstract
This paper describes the extraction from
Wikipedia of lexical reference rules, iden-
tifying references to term meanings trig-
gered by other terms. We present extrac-
tion methods geared to cover the broad
range of the lexical reference relation and
analyze them extensively. Most extrac-
tion methods yield high precision levels,
and our rule-base is shown to perform bet-
ter than other automatically constructed
baselines in a couple of lexical expan-
sion and matching tasks. Our rule-base
yields comparable performance to Word-
Net while providing largely complemen-
tary information.
1 Introduction
A most common need in applied semantic infer-
ence is to infer the meaning of a target term from
other terms in a text. For example, a Question An-
swering system may infer the answer to a ques-
tion regarding luxury cars from a text mentioning
Bentley, which provides a concrete reference to the
sought meaning.
Aiming to capture such lexical inferences we
followed (Glickman et al, 2006), which coined
the term lexical reference (LR) to denote refer-
ences in text to the specific meaning of a target
term. They further analyzed the dataset of the First
Recognizing Textual Entailment Challenge (Da-
gan et al, 2006), which includes examples drawn
from seven different application scenarios. It was
found that an entailing text indeed includes a con-
crete reference to practically every term in the en-
tailed (inferred) sentence.
The lexical reference relation between two
terms may be viewed as a lexical inference rule,
denoted LHS? RHS. Such rule indicates that the
left-hand-side term would generate a reference, in
some texts, to a possible meaning of the right hand
side term, as the Bentley? luxury car example.
In the above example the LHS is a hyponym of
the RHS. Indeed, the commonly used hyponymy,
synonymy and some cases of the meronymy rela-
tions are special cases of lexical reference. How-
ever, lexical reference is a broader relation. For
instance, the LR rule physician ? medicine may
be useful to infer the topic medicine in a text cate-
gorization setting, while an information extraction
system may utilize the rule Margaret Thatcher
? United Kingdom to infer a UK announcement
from the text ?Margaret Thatcher announced?.
To perform such inferences, systems need large
scale knowledge bases of LR rules. A prominent
available resource is WordNet (Fellbaum, 1998),
from which classical relations such as synonyms,
hyponyms and some cases of meronyms may be
used as LR rules. An extension to WordNet was
presented by (Snow et al, 2006). Yet, available
resources do not cover the full scope of lexical ref-
erence.
This paper presents the extraction of a large-
scale rule base from Wikipedia designed to cover
a wide scope of the lexical reference relation. As
a starting point we examine the potential of defi-
nition sentences as a source for LR rules (Ide and
Jean, 1993; Chodorow et al, 1985; Moldovan and
Rus, 2001). When writing a concept definition,
one aims to formulate a concise text that includes
the most characteristic aspects of the defined con-
cept. Therefore, a definition is a promising source
for LR relations between the defined concept and
the definition terms.
In addition, we extract LR rules from Wikipedia
redirect and hyperlink relations. As a guide-
line, we focused on developing simple extrac-
tion methods that may be applicable for other
Web knowledge resources, rather than focusing
on Wikipedia-specific attributes. Overall, our rule
base contains about 8 million candidate lexical ref-
450
erence rules. 1
Extensive analysis estimated that 66% of our
rules are correct, while different portions of the
rule base provide varying recall-precision trade-
offs. Following further error analysis we intro-
duce rule filtering which improves inference per-
formance. The rule base utility was evaluated
within two lexical expansion applications, yield-
ing better results than other automatically con-
structed baselines and comparable results to Word-
Net. A combination with WordNet achieved the
best performance, indicating the significant mar-
ginal contribution of our rule base.
2 Background
Many works on machine readable dictionaries uti-
lized definitions to identify semantic relations be-
tween words (Ide and Jean, 1993). Chodorow et
al. (1985) observed that the head of the defining
phrase is a genus term that describes the defined
concept and suggested simple heuristics to find it.
Other methods use a specialized parser or a set of
regular expressions tuned to a particular dictionary
(Wilks et al, 1996).
Some works utilized Wikipedia to build an on-
tology. Ponzetto and Strube (2007) identified
the subsumption (IS-A) relation from Wikipedia?s
category tags, while in Yago (Suchanek et al,
2007) these tags, redirect links and WordNet were
used to identify instances of 14 predefined spe-
cific semantic relations. These methods depend
on Wikipedia?s category system. The lexical refer-
ence relation we address subsumes most relations
found in these works, while our extractions are not
limited to a fixed set of predefined relations.
Several works examined Wikipedia texts, rather
than just its structured features. Kazama and Tori-
sawa (2007) explores the first sentence of an ar-
ticle and identifies the first noun phrase following
the verb be as a label for the article title. We repro-
duce this part of their work as one of our baselines.
Toral and Mun?oz (2007) uses all nouns in the first
sentence. Gabrilovich and Markovitch (2007) uti-
lized Wikipedia-based concepts as the basis for a
high-dimensional meaning representation space.
Hearst (1992) utilized a list of patterns indica-
tive for the hyponym relation in general texts.
Snow et al (2006) use syntactic path patterns as
features for supervised hyponymy and synonymy
1For download see Textual Entailment Resource Pool at
the ACL-wiki (http://aclweb.org/aclwiki)
classifiers, whose training examples are derived
automatically from WordNet. They use these clas-
sifiers to suggest extensions to the WordNet hierar-
chy, the largest one consisting of 400K new links.
Their automatically created resource is regarded in
our paper as a primary baseline for comparison.
Many works addressed the more general notion
of lexical associations, or association rules (e.g.
(Ruge, 1992; Rapp, 2002)). For example, The
Beatles, Abbey Road and Sgt. Pepper would all
be considered lexically associated. However this
is a rather loose notion, which only indicates that
terms are semantically ?related? and are likely to
co-occur with each other. On the other hand, lex-
ical reference is a special case of lexical associa-
tion, which specifies concretely that a reference to
the meaning of one term may be inferred from the
other. For example, Abbey Road provides a con-
crete reference to The Beatles, enabling to infer a
sentence like ?I listened to The Beatles? from ?I
listened to Abbey Road?, while it does not refer
specifically to Sgt. Pepper.
3 Extracting Rules from Wikipedia
Our goal is to utilize the broad knowledge of
Wikipedia to extract a knowledge base of lexical
reference rules. Each Wikipedia article provides
a definition for the concept denoted by the title
of the article. As the most concise definition we
take the first sentence of each article, following
(Kazama and Torisawa, 2007). Our preliminary
evaluations showed that taking the entire first para-
graph as the definition rarely introduces new valid
rules while harming extraction precision signifi-
cantly.
Since a concept definition usually employs
more general terms than the defined concept (Ide
and Jean, 1993), the concept title is more likely
to refer to terms in its definition rather than vice
versa. Therefore the title is taken as the LHS of
the constructed rule while the extracted definition
term is taken as its RHS. As Wikipedia?s titles are
mostly noun phrases, the terms we extract as RHSs
are the nouns and noun phrases in the definition.
The remainder of this section describes our meth-
ods for extracting rules from the definition sen-
tence and from additional Wikipedia information.
Be-Comp Following the general idea in
(Kazama and Torisawa, 2007), we identify the IS-
A pattern in the definition sentence by extract-
ing nominal complements of the verb ?be?, taking
451
No. Extraction Rule
James Eugene ?Jim? Carrey is a Canadian-American actor
and comedian
1 Be-Comp Jim Carrey? Canadian-American actor
2 Be-Comp Jim Carrey? actor
3 Be-Comp Jim Carrey? comedian
Abbey Road is an album released by The Beatles
4 All-N Abbey Road? The Beatles
5 Parenthesis Graph? mathematics
6 Parenthesis Graph? data structure
7 Redirect CPU? Central processing unit
8 Redirect Receptors IgG? Antibody
9 Redirect Hypertension? Elevated blood-pressure
10 Link pet? Domesticated Animal
11 Link Gestaltist? Gestalt psychology
Table 1: Examples of rule extraction methods
them as the RHS of a rule whose LHS is the article
title. While Kazama and Torisawa used a chun-
ker, we parsed the definition sentence using Mini-
par (Lin, 1998b). Our initial experiments showed
that parse-based extraction is more accurate than
chunk-based extraction. It also enables us extract-
ing additional rules by splitting conjoined noun
phrases and by taking both the head noun and the
complete base noun phrase as the RHS for sepa-
rate rules (examples 1?3 in Table 1).
All-N The Be-Comp extraction method yields
mostly hypernym relations, which do not exploit
the full range of lexical references within the con-
cept definition. Therefore, we further create rules
for all head nouns and base noun phrases within
the definition (example 4). An unsupervised reli-
ability score for rules extracted by this method is
investigated in Section 4.3.
Title Parenthesis A common convention in
Wikipedia to disambiguate ambiguous titles is
adding a descriptive term in parenthesis at the end
of the title, as in The Siren (Musical), The Siren
(sculpture) and Siren (amphibian). From such ti-
tles we extract rules in which the descriptive term
inside the parenthesis is the RHS and the rest of
the title is the LHS (examples 5?6).
Redirect As any dictionary and encyclopedia,
Wikipedia contains Redirect links that direct dif-
ferent search queries to the same article, which has
a canonical title. For instance, there are 86 differ-
ent queries that redirect the user to United States
(e.g. U.S.A., America, Yankee land). Redirect
links are hand coded, specifying that both terms
refer to the same concept. We therefore generate a
bidirectional entailment rule for each redirect link
(examples 7?9).
Link Wikipedia texts contain hyper links to ar-
ticles. For each link we generate a rule whose LHS
is the linking text and RHS is the title of the linked
article (examples 10?11). In this case we gener-
ate a directional rule since links do not necessarily
connect semantically equivalent entities.
We note that the last three extraction methods
should not be considered as Wikipedia specific,
since many Web-like knowledge bases contain
redirects, hyper-links and disambiguation means.
Wikipedia has additional structural features such
as category tags, structured summary tablets for
specific semantic classes, and articles containing
lists which were exploited in prior work as re-
viewed in Section 2.
As shown next, the different extraction meth-
ods yield different precision levels. This may al-
low an application to utilize only a portion of the
rule base whose precision is above a desired level,
and thus choose between several possible recall-
precision tradeoffs.
4 Extraction Methods Analysis
We applied our rule extraction methods over a
version of Wikipedia available in a database con-
structed by (Zesch et al, 2007)2. The extraction
yielded about 8 million rules altogether, with over
2.4 million distinct RHSs and 2.8 million distinct
LHSs. As expected, the extracted rules involve
mostly named entities and specific concepts, typi-
cally covered in encyclopedias.
4.1 Judging Rule Correctness
Following the spirit of the fine-grained human
evaluation in (Snow et al, 2006), we randomly
sampled 800 rules from our rule-base and pre-
sented them to an annotator who judged them for
correctness, according to the lexical reference no-
tion specified above. In cases which were too dif-
ficult to judge the annotator was allowed to ab-
stain, which happened for 20 rules. 66% of the re-
maining rules were annotated as correct. 200 rules
from the sample were judged by another annotator
for agreement measurement. The resulting Kappa
score was 0.7 (substantial agreement (Landis and
2English version from February 2007, containing 1.6 mil-
lion articles. www.ukp.tu-darmstadt.de/software/JWPL
452
Extraction Per Method Accumulated
Method P Est. #Rules P %obtained
Redirect 0.87 1,851,384 0.87 31
Be-Comp 0.78 1,618,913 0.82 60
Parenthesis 0.71 94,155 0.82 60
Link 0.7 485,528 0.80 68
All-N 0.49 1,580,574 0.66 100
Table 2: Manual analysis: precision and estimated number
of correct rules per extraction method, and precision and %
of correct rules obtained of rule-sets accumulated by method.
Koch, 1997)), either when considering all the ab-
stained rules as correct or as incorrect.
The middle columns of Table 2 present, for each
extraction method, the obtained percentage of cor-
rect rules (precision) and their estimated absolute
number. This number is estimated by multiplying
the number of annotated correct rules for the ex-
traction method by the sampling proportion. In to-
tal, we estimate that our resource contains 5.6 mil-
lion correct rules. For comparison, Snow?s pub-
lished extension to WordNet3, which covers simi-
lar types of terms but is restricted to synonyms and
hyponyms, includes 400,000 relations.
The right part of Table 2 shows the perfor-
mance figures for accumulated rule bases, created
by adding the extraction methods one at a time in
order of their precision. % obtained is the per-
centage of correct rules in each rule base out of
the total number of correct rules extracted jointly
by all methods (the union set).
We can see that excluding the All-N method
all extraction methods reach quite high precision
levels of 0.7-0.87, with accumulated precision of
0.84. By selecting only a subset of the extrac-
tion methods, according to their precision, one can
choose different recall-precision tradeoff points
that suit application preferences.
The less accurate All-N method may be used
when high recall is important, accounting for 32%
of the correct rules. An examination of the paths
in All-N reveals, beyond standard hyponymy and
synonymy, various semantic relations that satisfy
lexical reference, such as Location, Occupation
and Creation, as illustrated in Table 3. Typical re-
lations covered by Redirect and Link rules include
3http://ai.stanford.edu/?rion/swn/
4As a non-comparable reference, Snow?s fine-grained
evaluation showed a precision of 0.84 on 10K rules and 0.68
on 20K rules; however, they were interested only in the hy-
ponym relation while we evaluate our rules according to the
broader LR relation.
synonyms (NY State Trooper ? New York State
Police), morphological derivations (irritate ? ir-
ritation), different spellings or naming (Pytagoras
? Pythagoras) and acronyms (AIS? Alarm Indi-
cation Signal).
4.2 Error Analysis
We sampled 100 rules which were annotated as in-
correct and examined the causes of errors. Figure
1 shows the distribution of error types.
Wrong NP part - The most common error
(35% of the errors) is taking an inappropriate part
of a noun phrase (NP) as the rule right hand side
(RHS). As described in Section 3, we create two
rules from each extracted NP, by taking both the
head noun and the complete base NP as RHSs.
While both rules are usually correct, there are
cases in which the left hand side (LHS) refers to
the NP as a whole but not to part of it. For ex-
ample, Margaret Thatcher refers to United King-
dom but not to Kingdom. In Section 5 we suggest
a filtering method which addresses some of these
errors. Future research may exploit methods for
detecting multi-words expressions.
All-N pa
ttern er
rors
13%Tra
nsparen
t head 11%
Wrong N
P part 35%
Technic
al error
s
10%
Dates a
nd Plac
es
5% Link err
ors 5% Redirec
t errors 5%
Related
 
but not Referrin
g 16%
Figure 1: Error analysis: type of incorrect rules
Related but not Referring - Although all terms
in a definition are highly related to the defined con-
cept, not all are referred by it. For example the
origin of a person (*The Beatles? Liverpool5) or
family ties such as ?daughter of? or ?sire of?.
All-N errors - Some of the articles start with a
long sentence which may include information that
is not directly referred by the title of the article.
For instance, consider *Interstate 80 ? Califor-
nia from ?Interstate 80 runs from California to
New Jersey?. In Section 4.3 we further analyze
this type of error and point at a possible direction
for addressing it.
Transparent head - This is the phenomenon in
which the syntactic head of a noun phrase does
5The asterisk denotes an incorrect rule
453
Relation Rule Path Pattern
Location Lovek? Cambodia Lovek city in Cambodia
Occupation Thomas H. Cormen? computer science Thomas H. Cormen professor of computer science
Creation Genocidal Healer? James White Genocidal Healer novel by James White
Origin Willem van Aelst? Dutch Willem van Aelst Dutch artist
Alias Dean Moriarty? Benjamin Linus Dean Moriarty is an alias of Benjamin Linus on Lost.
Spelling Egushawa? Agushaway Egushawa, also spelled Agushaway...
Table 3: All-N rules exemplifying various types of LR relations
not bear its primary meaning, while it has a mod-
ifier which serves as the semantic head (Fillmore
et al, 2002; Grishman et al, 1986). Since parsers
identify the syntactic head, we extract an incorrect
rule in such cases. For instance, deriving *Prince
William ? member instead of Prince William ?
British Royal Family from ?Prince William is a
member of the British Royal Family?. Even though
we implemented the common solution of using a
list of typical transparent heads, this solution is
partial since there is no closed set of such phrases.
Technical errors - Technical extraction errors
were mainly due to erroneous identification of the
title in the definition sentence or mishandling non-
English texts.
Dates and Places - Dates and places where a
certain person was born at, lived in or worked at
often appear in definitions but do not comply to
the lexical reference notion (*Galileo Galilei ?
15 February 1564).
Link errors - These are usually the result of
wrong assignment of the reference direction. Such
errors mostly occur when a general term, e.g. rev-
olution, links to a more specific albeit typical con-
cept, e.g. French Revolution.
Redirect errors - These may occur in some
cases in which the extracted rule is not bidirec-
tional. E.g. *Anti-globalization ? Movement of
Movements is wrong but the opposite entailment
direction is correct, as Movement of Movements is
a popular term in Italy for Anti-globalization.
4.3 Scoring All-N Rules
We observed that the likelihood of nouns men-
tioned in a definition to be referred by the con-
cept title depends greatly on the syntactic path
connecting them (which was exploited also in
(Snow et al, 2006)). For instance, the path pro-
duced by Minipar for example 4 in Table 1 is title
subj
??album vrel??released by?subj?? bypcomp?n?? noun.
In order to estimate the likelihood that a syn-
tactic path indicates lexical reference we collected
from Wikipedia all paths connecting a title to a
noun phrase in the definition sentence. We note
that since there is no available resource which cov-
ers the full breadth of lexical reference we could
not obtain sufficiently broad supervised training
data for learning which paths correspond to cor-
rect references. This is in contrast to (Snow et al,
2005) which focused only on hyponymy and syn-
onymy relations and could therefore extract posi-
tive and negative examples from WordNet.
We therefore propose the following unsuper-
vised reference likelihood score for a syntactic
path p within a definition, based on two counts:
the number of times p connects an article title with
a noun in its definition, denoted by Ct(p), and the
total number of p?s occurrences in Wikipedia de-
finitions, C(p). The score of a path is then de-
fined as Ct(p)C(p) . The rational for this score is that
C(p)? Ct(p) corresponds to the number of times
in which the path connects two nouns within the
definition, none of which is the title. These in-
stances are likely to be non-referring, since a con-
cise definition typically does not contain terms that
can be inferred from each other. Thus our score
may be seen as an approximation for the probabil-
ity that the two nouns connected by an arbitrary
occurrence of the path would satisfy the reference
relation. For instance, the path of example 4 ob-
tained a score of 0.98.
We used this score to sort the set of rules ex-
tracted by the All-N method and split the sorted list
into 3 thirds: top, middle and bottom. As shown in
Table 4, this obtained reasonably high precision
for the top third of these rules, relative to the other
two thirds. This precision difference indicates that
our unsupervised path score provides useful infor-
mation about rule reliability.
It is worth noting that in our sample 57% of All-
N errors, 62% of Related but not Referring incor-
rect rules and all incorrect rules of type Dates and
454
Extraction Per Method Accumulated
Method P Est. #Rules P %obtained
All-Ntop 0.60 684,238 0.76 83
All-Nmiddle 0.46 380,572 0.72 90
All-Nbottom 0.41 515,764 0.66 100
Table 4: Splitting All-N extraction method into 3 sub-types.
These three rows replace the last row of Table 2
Places were extracted by the All-Nbottom method
and thus may be identified as less reliable. How-
ever, this split was not observed to improve per-
formance in the application oriented evaluations
of Section 6. Further research is thus needed to
fully exploit the potential of the syntactic path as
an indicator for rule correctness.
5 Filtering Rules
Following our error analysis, future research is
needed for addressing each specific type of error.
However, during the analysis we observed that all
types of erroneous rules tend to relate terms that
are rather unlikely to co-occur together. We there-
fore suggest, as an optional filter, to recognize
such rules by their co-occurrence statistics using
the common Dice coefficient:
2 ? C(LHS,RHS)
C(LHS) + C(RHS)
where C(x) is the number of articles in Wikipedia
in which all words of x appear.
In order to partially overcome the Wrong NP
part error, identified in Section 4.2 to be the most
common error, we adjust the Dice equation for
rules whose RHS is also part of a larger noun
phrase (NP):
2 ? (C(LHS,RHS)? C(LHS,NPRHS))
C(LHS) + C(RHS)
where NPRHS is the complete NP whose part
is the RHS. This adjustment counts only co-
occurrences in which the LHS appears with the
RHS alone and not with the larger NP. This sub-
stantially reduces the Dice score for those cases in
which the LHS co-occurs mainly with the full NP.
Given the Dice score rules whose score does not
exceed a threshold may be filtered. For example,
the incorrect rule *aerial tramway? car was fil-
tered, where the correct RHS for this LHS is the
complete NP cable car. Another filtered rule is
magic? cryptography which is correct only for a
very idiosyncratic meaning.6
We also examined another filtering score, the
cosine similarity between the vectors representing
the two rule sides in LSA (Latent Semantic Analy-
sis) space (Deerwester et al, 1990). However, as
the results with this filter resemble those for Dice
we present results only for the simpler Dice filter.
6 Application Oriented Evaluations
Our primary application oriented evaluation is
within an unsupervised lexical expansion scenario
applied to a text categorization data set (Section
6.1). Additionally, we evaluate the utility of our
rule base as a lexical resource for recognizing tex-
tual entailment (Section 6.2).
6.1 Unsupervised Text Categorization
Our categorization setting resembles typical query
expansion in information retrieval (IR), where the
category name is considered as the query. The ad-
vantage of using a text categorization test set is
that it includes exhaustive annotation for all doc-
uments. Typical IR datasets, on the other hand,
are partially annotated through a pooling proce-
dure. Thus, some of our valid lexical expansions
might retrieve non-annotated documents that were
missed by the previously pooled systems.
6.1.1 Experimental Setting
Our categorization experiment follows a typical
keywords-based text categorization scheme (Mc-
Callum and Nigam, 1999; Liu et al, 2004). Tak-
ing a lexical reference perspective, we assume that
the characteristic expansion terms for a category
should refer to the term (or terms) denoting the
category name. Accordingly, we construct the cat-
egory?s feature vector by taking first the category
name itself, and then expanding it with all left-
hand sides of lexical reference rules whose right-
hand side is the category name. For example, the
category ?Cars? is expanded by rules such as Fer-
rari F50? car. During classification cosine sim-
ilarity is measured between the feature vector of
the classified document and the expanded vectors
of all categories. The document is assigned to
the category which yields the highest similarity
score, following a single-class classification ap-
proach (Liu et al, 2004).
6Magic was the United States codename for intelligence
derived from cryptanalysis during World War II.
455
Rule Base R P F1
Baselines:
No Expansion 0.19 0.54 0.28
WikiBL 0.19 0.53 0.28
Snow400K 0.19 0.54 0.28
Lin 0.25 0.39 0.30
WordNet 0.30 0.47 0.37
Extraction Methods from Wikipedia:
Redirect + Be-Comp 0.22 0.55 0.31
All rules 0.31 0.38 0.34
All rules + Dice filter 0.31 0.49 0.38
Union:
WordNet + WikiAll rules+Dice 0.35 0.47 0.40
Table 5: Results of different rule bases for 20 newsgroups
category name expansion
It should be noted that keyword-based text
categorization systems employ various additional
steps, such as bootstrapping, which generalize to
multi-class settings and further improve perfor-
mance. Our basic implementation suffices to eval-
uate comparatively the direct impact of different
expansion resources on the initial classification.
For evaluation we used the test set of the
?bydate? version of the 20-News Groups collec-
tion,7 which contains 18,846 documents parti-
tioned (nearly) evenly over the 20 categories8.
6.1.2 Baselines Results
We compare the quality of our rule base expan-
sions to 5 baselines (Table 5). The first avoids any
expansion, classifying documents based on cosine
similarity with category names only. As expected,
it yields relatively high precision but low recall,
indicating the need for lexical expansion.
The second baseline is our implementation of
the relevant part of the Wikipedia extraction in
(Kazama and Torisawa, 2007), taking the first
noun after a be verb in the definition sentence, de-
noted as WikiBL. This baseline does not improve
performance at all over no expansion.
The next two baselines employ state-of-the-art
lexical resources. One uses Snow?s extension to
WordNet which was mentioned earlier. This re-
source did not yield a noticeable improvement, ei-
7www.ai.mit.edu/people/jrennie/20Newsgroups.
8The keywords used as category names are: athe-
ism; graphic; microsoft windows; ibm,pc,hardware;
mac,hardware; x11,x-windows; sale; car; motorcycle;
baseball; hockey; cryptography; electronics; medicine; outer
space; christian(noun & adj); gun; mideast,middle east;
politics; religion
ther over the No Expansion baseline or over Word-
Net when joined with its expansions. The sec-
ond uses Lin dependency similarity, a syntactic-
dependency based distributional word similarity
resource described in (Lin, 1998a)9. We used var-
ious thresholds on the length of the expansion list
derived from this resource. The best result, re-
ported here, provides only a minor F1 improve-
ment over No Expansion, with modest recall in-
crease and significant precision drop, as can be ex-
pected from such distributional method.
The last baseline uses WordNet for expansion.
First we expand all the senses of each category
name by their derivations and synonyms. Each ob-
tained term is then expanded by its hyponyms, or
by its meronyms if it has no hyponyms. Finally,
the results are further expanded by their deriva-
tions and synonyms.10 WordNet expansions im-
prove substantially both Recall and F1 relative to
No Expansion, while decreasing precision.
6.1.3 Wikipedia Results
We then used for expansion different subsets
of our rule base, producing alternative recall-
precision tradeoffs. Table 5 presents the most in-
teresting results. Using any subset of the rules
yields better performance than any of the other
automatically constructed baselines (Lin, Snow
and WikiBL). Utilizing the most precise extrac-
tion methods of Redirect and Be-Comp yields the
highest precision, comparable to No Expansion,
but just a small recall increase. Using the entire
rule base yields the highest recall, while filtering
rules by the Dice coefficient (with 0.1 threshold)
substantially increases precision without harming
recall. With this configuration our automatically-
constructed resource achieves comparable perfor-
mance to the manually built WordNet.
Finally, since a dictionary and an encyclopedia
are complementary in nature, we applied the union
of WordNet and the filtered Wikipedia expansions.
This configuration yields the best results: it main-
tains WordNet?s precision and adds nearly 50% to
the recall increase of WordNet over No Expansion,
indicating the substantial marginal contribution of
Wikipedia. Furthermore, with the fast growth of
Wikipedia the recall of our resource is expected to
increase while maintaining its precision.
9Downloaded from www.cs.ualberta.ca/lindek/demos.htm
10We also tried expanding by the entire hyponym hierarchy
and considering only the first sense of each synset, but the
method described above achieved the best performance.
456
Category Name Expanding Terms
Politics opposition, coalition, whip(a)
Cryptography adversary, cryptosystem, key
Mac PowerBook, Radius(b), Grab(c)
Religion heaven, creation, belief, missionary
Medicine doctor, physician, treatment, clinical
Computer Graphics radiosity(d), rendering, siggraph(e)
Table 6: Some Wikipedia rules not in WordNet, which con-
tributed to text categorization. (a) a legislator who enforce
leadership desire (b) a hardware firm specializing in Macin-
tosh equipment (c) a Macintosh screen capture software (d)
an illumination algorithm (e) a computer graphics conference
Configuration Accuracy Accuracy Drop
WordNet + Wikipedia 60.0 % -
Without WordNet 57.7 % 2.3 %
Without Wikipedia 58.9 % 1.1 %
Table 7: RTE accuracy results for ablation tests.
Table 6 illustrates few examples of useful rules
that were found in Wikipedia but not in WordNet.
We conjecture that in other application settings
the rules extracted from Wikipedia might show
even greater marginal contribution, particularly in
specialized domains not covered well by Word-
Net. Another advantage of a resource based on
Wikipedia is that it is available in many more lan-
guages than WordNet.
6.2 Recognizing Textual Entailment (RTE)
As a second application-oriented evaluation we
measured the contributions of our (filtered)
Wikipedia resource and WordNet to RTE infer-
ence (Giampiccolo et al, 2007). To that end, we
incorporated both resources within a typical basic
RTE system architecture (Bar-Haim et al, 2008).
This system determines whether a text entails an-
other sentence based on various matching crite-
ria that detect syntactic, logical and lexical cor-
respondences (or mismatches). Most relevant for
our evaluation, lexical matches are detected when
a Wikipedia rule?s LHS appears in the text and
its RHS in the hypothesis, or similarly when pairs
of WordNet synonyms, hyponyms-hypernyms and
derivations appear across the text and hypothesis.
The system?s weights were trained on the devel-
opment set of RTE-3 and tested on RTE-4 (which
included this year only a test set).
To measure the marginal contribution of the two
resources we performed ablation tests, comparing
the accuracy of the full system to that achieved
when removing either resource. Table 7 presents
the results, which are similar in nature to those ob-
tained for text categorization. Wikipedia obtained
a marginal contribution of 1.1%, about half of the
analogous contribution of WordNet?s manually-
constructed information. We note that for current
RTE technology it is very typical to gain just a
few percents in accuracy thanks to external knowl-
edge resources, while individual resources usually
contribute around 0.5?2% (Iftene and Balahur-
Dobrescu, 2007; Dinu and Wang, 2009). Some
Wikipedia rules not in WordNet which contributed
to RTE inference are Jurassic Park ? Michael
Crichton, GCC? Gulf Cooperation Council.
7 Conclusions and Future Work
We presented construction of a large-scale re-
source of lexical reference rules, as useful in ap-
plied lexical inference. Extensive rule-level analy-
sis showed that different recall-precision tradeoffs
can be obtained by utilizing different extraction
methods. It also identified major reasons for er-
rors, pointing at potential future improvements.
We further suggested a filtering method which sig-
nificantly improved performance.
Even though the resource was constructed by
quite simple extraction methods, it was proven to
be beneficial within two different application set-
ting. While being an automatically built resource,
extracted from a knowledge-base created for hu-
man consumption, it showed comparable perfor-
mance to WordNet, which was manually created
for computational purposes. Most importantly, it
also provides complementary knowledge to Word-
Net, with unique lexical reference rules.
Future research is needed to improve resource?s
precision, especially for the All-N method. As
a first step, we investigated a novel unsupervised
score for rules extracted from definition sentences.
We also intend to consider the rule base as a di-
rected graph and exploit the graph structure for
further rule extraction and validation.
Acknowledgments
The authors would like to thank Idan Szpektor
for valuable advices. This work was partially
supported by the NEGEV project (www.negev-
initiative.org), the PASCAL-2 Network of Excel-
lence of the European Community FP7-ICT-2007-
1-216886 and by the Israel Science Foundation
grant 1112/08.
457
References
Roy Bar-Haim, Jonathan Berant, Ido Dagan, Iddo
Greental, Shachar Mirkin, Eyal Shnarch, and Idan
Szpektor. 2008. Efficient semantic deduction and
approximate matching over compact parse forests.
In Proceedings of TAC.
Martin S. Chodorow, Roy J. Byrd, and George E. Hei-
dorn. 1985. Extracting semantic hierarchies from a
large on-line dictionary. In Proceedings of ACL.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The pascal recognising textual entailment
challenge. In Lecture Notes in Computer Science,
volume 3944, pages 177?190.
Scott Deerwester, Susan T. Dumais, George W. Furnas,
Thomas K. Landauer, and Richard Harshman. 1990.
Indexing by latent semantic analysis. Journal of the
American Society for Information Science, 41:391?
407.
Georgiana Dinu and Rui Wang. 2009. Inference rules
for recognizing textual entailment. In Proceedings
of the IWCS.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database (Language, Speech, and
Communication). The MIT Press.
Charles J. Fillmore, Collin F. Baker, and Hiroaki Sato.
2002. Seeing arguments through transparent struc-
tures. In Proceedings of LREC.
Evgeniy Gabrilovich and Shaul Markovitch. 2007.
Computing semantic relatedness using wikipedia-
based explicit semantic analysis. In Proceedings of
IJCAI.
Danilo Giampiccolo, Bernardo Magnini, Ido Dagan,
and Bill Dolan. 2007. The third pascal recogniz-
ing textual entailment challenge. In Proceedings of
ACL-WTEP Workshop.
Oren Glickman, Eyal Shnarch, and Ido Dagan. 2006.
Lexical reference: a semantic matching subtask. In
Proceedings of EMNLP.
Ralph Grishman, Lynette Hirschman, and Ngo Thanh
Nhan. 1986. Discovery procedures for sublanguage
selectional patterns: Initial experiments. Computa-
tional Linguistics, 12(3):205?215.
Marti Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
COLING.
Nancy Ide and Ve?ronis Jean. 1993. Extracting
knowledge bases from machine-readable dictionar-
ies: Have we wasted our time? In Proceedings of
KB & KS Workshop.
Adrian Iftene and Alexandra Balahur-Dobrescu. 2007.
Hypothesis transformation and semantic variability
rules used in recognizing textual entailment. In Pro-
ceedings of the ACL-PASCAL Workshop on Textual
Entailment and Paraphrasing.
Jun?ichi Kazama and Kentaro Torisawa. 2007. Ex-
ploiting Wikipedia as external knowledge for named
entity recognition. In Proceedings of EMNLP-
CoNLL.
J. Richard Landis and Gary G. Koch. 1997. The
measurements of observer agreement for categorical
data. In Biometrics, pages 33:159?174.
Dekang Lin. 1998a. Automatic retrieval and clustering
of similar words. In Proceedings of COLING-ACL.
Dekang Lin. 1998b. Dependency-based evaluation of
MINIPAR. In Proceedings of the Workshop on Eval-
uation of Parsing Systems at LREC.
Bing Liu, Xiaoli Li, Wee Sun Lee, and Philip S. Yu.
2004. Text classification by labeling words. In Pro-
ceedings of AAAI.
Andrew McCallum and Kamal Nigam. 1999. Text
classification by bootstrapping with keywords, EM
and shrinkage. In Proceedings of ACL Workshop for
unsupervised Learning in NLP.
Dan Moldovan and Vasile Rus. 2001. Logic form
transformation of wordnet and its applicability to
question answering. In Proceedings of ACL.
Simone P. Ponzetto and Michael Strube. 2007. De-
riving a large scale taxonomy from wikipedia. In
Proceedings of AAAI.
Reinhard Rapp. 2002. The computation of word asso-
ciations: comparing syntagmatic and paradigmatic
approaches. In Proceedings of COLING.
Gerda Ruge. 1992. Experiment on linguistically-based
term associations. Information Processing & Man-
agement, 28(3):317?332.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2005.
Learning syntactic patterns for automatic hypernym
discovery. In NIPS.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2006.
Semantic taxonomy induction from heterogenous
evidence. In Proceedings of COLING-ACL.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: A core of semantic knowl-
edge - unifying wordnet and wikipedia. In Proceed-
ings of WWW.
Antonio Toral and Rafael Mun?oz. 2007. A proposal
to automatically build and maintain gazetteers for
named entity recognition by using wikipedia. In
Proceedings of NAACL/HLT.
Yorick A. Wilks, Brian M. Slator, and Louise M.
Guthrie. 1996. Electric words: dictionaries, com-
puters, and meanings. MIT Press, Cambridge, MA,
USA.
Torsten Zesch, Iryna Gurevych, and Max Mu?hlha?user.
2007. Analyzing and accessing wikipedia as a lex-
ical semantic resource. In Data Structures for Lin-
guistic Resources and Applications, pages 197?205.
458
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 791?799,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Source-Language Entailment Modeling for Translating Unknown Terms
Shachar Mirkin?, Lucia Specia?, Nicola Cancedda?, Ido Dagan?, Marc Dymetman?, Idan Szpektor?
? Computer Science Department, Bar-Ilan University
? Xerox Research Centre Europe
{mirkins,dagan,szpekti}@cs.biu.ac.il
{lucia.specia,nicola.cancedda,marc.dymetman}@xrce.xerox.com
Abstract
This paper addresses the task of handling
unknown terms in SMT. We propose us-
ing source-language monolingual models
and resources to paraphrase the source text
prior to translation. We further present a
conceptual extension to prior work by al-
lowing translations of entailed texts rather
than paraphrases only. A method for
performing this process efficiently is pre-
sented and applied to some 2500 sentences
with unknown terms. Our experiments
show that the proposed approach substan-
tially increases the number of properly
translated texts.
1 Introduction
Machine Translation systems frequently encounter
terms they are not able to translate due to some
missing knowledge. For instance, a Statistical Ma-
chine Translation (SMT) system translating the
sentence ?Cisco filed a lawsuit against Apple for
patent violation? may lack words like filed and
lawsuit in its phrase table. The problem is espe-
cially severe for languages for which parallel cor-
pora are scarce, or in the common scenario when
the SMT system is used to translate texts of a do-
main different from the one it was trained on.
A previously suggested solution (Callison-
Burch et al, 2006) is to learn paraphrases of
source terms from multilingual (parallel) corpora,
and expand the phrase table with these para-
phrases1. Such solutions could potentially yield a
paraphrased sentence like ?Cisco sued Apple for
patent violation?, although their dependence on
bilingual resources limits their utility.
In this paper we propose an approach that con-
sists in directly replacing unknown source terms,
1As common in the literature, we use the term para-
phrases to refer to texts of equivalent meaning, of any length
from single words (synonyms) up to complete sentences.
using source-language resources and models in or-
der to achieve two goals.
The first goal is coverage increase. The avail-
ability of bilingual corpora, from which para-
phrases can be learnt, is in many cases limited.
On the other hand, monolingual resources and
methods for extracting paraphrases from monolin-
gual corpora are more readily available. These
include manually constructed resources, such as
WordNet (Fellbaum, 1998), and automatic meth-
ods for paraphrases acquisition, such as DIRT (Lin
and Pantel, 2001). However, such resources have
not been applied yet to the problem of substitut-
ing unknown terms in SMT. We suggest that by
using such monolingual resources we could pro-
vide paraphrases for a larger number of texts with
unknown terms, thus increasing the overall cover-
age of the SMT system, i.e. the number of texts it
properly translates.
Even with larger paraphrase resources, we may
encounter texts in which not all unknown terms are
successfully handled through paraphrasing, which
often results in poor translations (see Section 2.1).
To further increase coverage, we therefore pro-
pose to generate and translate texts that convey a
somewhat more general meaning than the original
source text. For example, using such approach,
the following text could be generated: ?Cisco ac-
cused Apple of patent violation?. Although less in-
formative than the original, a translation for such
texts may be useful. Such non-symmetric relation-
ships (as between filed a lawsuit and accused) are
difficult to learn from parallel corpora and there-
fore monolingual resources are more appropriate
for this purpose.
The second goal we wish to accomplish by
employing source-language resources is to rank
the alternative generated texts. This goal can be
achieved by using context-models on the source
language prior to translation. This has two advan-
tages. First, the ranking allows us to prune some
791
candidates before supplying them to the transla-
tion engine, thus improving translation efficiency.
Second, the ranking may be combined with target
language information in order to choose the best
translation, thus improving translation quality.
We position the problem of generating alterna-
tive texts for translation within the Textual Entail-
ment (TE) framework (Giampiccolo et al, 2007).
TE provides a generic way for handling language
variability, identifying when the meaning of one
text is entailed by the other (i.e. the meaning of
the entailed text can be inferred from the mean-
ing of the entailing one). When the meanings of
two texts are equivalent (paraphrase), entailment
is mutual. Typically, a more general version of
a certain text is entailed by it. Hence, through TE
we can formalize the generation of both equivalent
and more general texts for the source text. When
possible, a paraphrase is used. Otherwise, an alter-
native text whose meaning is entailed by the orig-
inal source is generated and translated.
We assess our approach by applying an SMT
system to a text domain that is different from the
one used to train the system. We use WordNet
as a source language resource for entailment rela-
tionships and several common statistical context-
models for selecting the best generated texts to be
sent to translation. We show that the use of source
language resources, and in particular the extension
to non-symmetric textual entailment relationships,
is useful for substantially increasing the amount of
texts that are properly translated. This increase is
observed relative to both using paraphrases pro-
duced by the same resource (WordNet) and us-
ing paraphrases produced from multilingual paral-
lel corpora. We demonstrate that by using simple
context-models on the source, efficiency can be
improved, while translation quality is maintained.
We believe that with the use of more sophisticated
context-models further quality improvement can
be achieved.
2 Background
2.1 Unknown Terms
A very common problem faced by machine trans-
lation systems is the need to translate terms (words
or multi-word expressions) that are not found in
the system?s lexicon or phrase table. The reasons
for such unknown terms in SMT systems include
scarcity of training material and the application
of the system to text domains that differ from the
ones used for training.
In SMT, when unknown terms are found in the
source text, the systems usually omit or copy them
literally into the target. Though copying the source
words can be of some help to the reader if the
unknown word has a cognate in the target lan-
guage, this will not happen in the most general
scenario where, for instance, languages use dif-
ferent scripts. In addition, the presence of a sin-
gle unknown term often affects the translation of
wider portions of text, inducing errors in both lex-
ical selection and ordering. This phenomenon is
demonstrated in the following sentences, where
the translation of the English sentence (1) is ac-
ceptable only when the unknown word (in bold) is
replaced with a translatable paraphrase (3):
1. ?. . . , despite bearing the heavy burden of the
unemployed 10% or more of the labor force.?
2. ?. . . , malgre? la lourde charge de compte le
10% ou plus de cho?meurs labor la force .?
3. ?. . . , malgre? la lourde charge des cho?meurs
de 10% ou plus de la force du travail.?
Several approaches have been proposed to deal
with unknown terms in SMT systems, rather than
omitting or copying the terms. For example, (Eck
et al, 2008) replace the unknown terms in the
source text by their definition in a monolingual
dictionary, which can be useful for gisting. To
translate across languages with different alpha-
bets approaches such as (Knight and Graehl, 1997;
Habash, 2008) use transliteration techniques to
tackle proper nouns and technical terms. For trans-
lation from highly inflected languages, certain ap-
proaches rely on some form of lexical approx-
imation or morphological analysis (Koehn and
Knight, 2003; Yang and Kirchhoff, 2006; Langlais
and Patry, 2007; Arora et al, 2008). Although
these strategies yield gain in coverage and transla-
tion quality, they only account for unknown terms
that should be transliterated or are variations of
known ones.
2.2 Paraphrasing in MT
A recent strategy to broadly deal with the prob-
lem of unknown terms is to paraphrase the source
text with terms whose translation is known to
the system, using paraphrases learnt from multi-
lingual corpora, typically involving at least one
?pivot? language different from the target lan-
guage of immediate interest (Callison-Burch et
792
al., 2006; Cohn and Lapata, 2007; Zhao et al,
2008; Callison-Burch, 2008; Guzma?n and Gar-
rido, 2008). The procedure to extract paraphrases
in these approaches is similar to standard phrase
extraction in SMT systems, and therefore a large
amount of additional parallel corpus is required.
Moreover, as discussed in Section 5, when un-
known texts are not from the same domain as the
SMT training corpus, it is likely that paraphrases
found through such methods will yield misleading
translations.
Bond et al (2008) use grammars to paraphrase
the whole source sentence, covering aspects like
word order and minor lexical variations (tenses
etc.), but not content words. The paraphrases are
added to the source side of the corpus and the cor-
responding target sentences are duplicated. This,
however, may yield distorted probability estimates
in the phrase table, since these were not computed
from parallel data.
The main use of monolingual paraphrases in
MT to date has been for evaluation. For exam-
ple, (Kauchak and Barzilay, 2006) paraphrase ref-
erences to make them closer to the system transla-
tion in order to obtain more reliable results when
using automatic evaluation metrics like BLEU
(Papineni et al, 2002).
2.3 Textual Entailment and Entailment Rules
Textual Entailment (TE) has recently become a
prominent paradigm for modeling semantic infer-
ence, capturing the needs of a broad range of
text understanding applications (Giampiccolo et
al., 2007). Yet, its application to SMT has been so
far limited to MT evaluation (Pado et al, 2009).
TE defines a directional relation between two
texts, where the meaning of the entailed text (hy-
pothesis, h) can be inferred from the meaning of
the entailing text, t. Under this paradigm, para-
phrases are a special case of the entailment rela-
tion, when the relation is symmetric (the texts en-
tail each other). Otherwise, we say that one text
directionally entails the other.
A common practice for proving (or generating)
h from t is to apply entailment rules to t. An
entailment rule, denoted LHS ? RHS, specifies
an entailment relation between two text fragments
(the Left- and Right- Hand Sides), possibly with
variables (e.g. build X in Y ? X is completed
in Y ). A paraphrasing rule is denoted with ?.
When a rule is applied to a text, a new text is in-
ferred, where the matched LHS is replaced with the
RHS. For example, the rule skyscraper? building
is applied to ?The world?s tallest skyscraper was
completed in Taiwan? to infer ?The world?s tallest
building was completed in Taiwan?. In this work,
we employ lexical entailment rules, i.e. rules with-
out variables. Various resources for lexical rules
are available, and the prominent one is WordNet
(Fellbaum, 1998), which has been used in virtu-
ally all TE systems (Giampiccolo et al, 2007).
Typically, a rule application is valid only under
specific contexts. For example, mouse ? rodent
should not be applied to ?Use the mouse to mark
your answers?. Context-models can be exploited
to validate the application of a rule to a text. In
such models, an explicit Word Sense Disambigua-
tion (WSD) is not necessarily required; rather, an
implicit sense-match is sought after (Dagan et al,
2006). Within the scope of our paper, rule ap-
plication is handled similarly to Lexical Substitu-
tion (McCarthy and Navigli, 2007), considering
the contextual relationship between the text and
the rule. However, in general, entailment rule ap-
plication addresses other aspects of context match-
ing as well (Szpektor et al, 2008).
3 Textual Entailment for Statistical
Machine Translation
Previous solutions for handling unknown terms in
a source text s augment the SMT system?s phrase
table based on multilingual corpora. This allows
indirectly paraphrasing s, when the SMT system
chooses to use a paraphrase included in the table
and produces a translation with the corresponding
target phrase for the unknown term.
We propose using monolingual paraphrasing
methods and resources for this task to obtain a
more extensive set of rules for paraphrasing the
source. These rules are then applied to s directly
to produce alternative versions of the source text
prior to the translation step. Moreover, further
coverage increase can be achieved by employing
directional entailment rules, when paraphrasing is
not possible, to generate more general texts for
translation.
Our approach, based on the textual entailment
framework, considers the newly generated texts as
entailed from the original one. Monolingual se-
mantic resources such as WordNet can provide en-
tailment rules required for both these symmetric
and asymmetric entailment relations.
793
Input: A text t with one or more unknown terms;
a monolingual resource of entailment rules;
k - maximal number of source alternatives to produce
Output: A translation of either (in order of preference):
a paraphrase of t OR a text entailed by t OR t itself
1. For each unknown term - fetch entailment rules:
(a) Fetch rules for paraphrasing; disregard rules
whose RHS is not in the phrase table
(b) If the set of rules is empty: fetch directional en-
tailment rules; disregard rules whose RHS is not
in the phrase table
2. Apply a context-model to compute a score for each rule
application
3. Compute total source score for each entailed text as a
combination of individual rule scores
4. Generate and translate the top-k entailed texts
5. If k > 1
(a) Apply target model to score the translation
(b) Compute final source-target score
6. Pick highest scoring translation
Figure 1: Scheme for handling unknown terms by using
monolingual resources through textual entailment
Through the process of applying entailment
rules to the source text, multiple alternatives of
entailed texts are generated. To rank the candi-
date texts we employ monolingual context-models
to provide scores for rule applications over the
source sentence. This can be used to (a) directly
select the text with the highest score, which can
then be translated, or (b) to select a subset of top
candidates to be translated, which will then be
ranked using the target language information as
well. This pruning reduces the load of the SMT
system, and allows for potential improvements in
translation quality by considering both source- and
target-language information.
The general scheme through which we achieve
these goals, which can be implemented using dif-
ferent context-models and scoring techniques, is
detailed in Figure 1. Details of our concrete im-
plementation are given in Section 4.
Preliminary analysis confirmed (as expected)
that readers prefer translations of paraphrases,
when available, over translations of directional en-
tailments. This consideration is therefore taken
into account in the proposed method.
The input is a text unit to be translated, such as a
sentence or paragraph, with one or more unknown
terms. For each unknown term we first fetch a
list of candidate rules for paraphrasing (e.g. syn-
onyms), where the unknown term is the LHS. For
example, if our unknown term is dodge, a possi-
ble candidate might be dodge ? circumvent. We
inflect the RHS to keep the original morphologi-
cal information of the unknown term and filter out
rules where the inflected RHS does not appear in
the phrase table (step 1a in Figure 1).
When no applicable rules for paraphrasing are
available (1b), we fetch directional entailment
rules (e.g. hypernymy rules such as dodge ?
avoid), and filter them in the same way as for para-
phrasing rules. To each set of rules for a given un-
known term we add the ?identity-rule?, to allow
leaving the unknown term unchanged, the correct
choice in cases of proper names, for example.
Next, we apply a context-model to compute an
applicability score of each rule to the source text
(step 2). An entailed text?s total score is the com-
bination (e.g. product, see Section 4) of the scores
of the rules used to produce it (3). A set of the
top-k entailed texts is then generated and sent for
translation (4).
If more than one alternative is produced by the
source model (and k > 1), a target model is ap-
plied on the selected set of translated texts (5a).
The combined source-target model score is a com-
bination of the scores of the source and target
models (5b). The final translation is selected to be
the one that yields the highest combined source-
target score (6). Note that setting k = 1 is equiva-
lent to using the source-language model alone.
Our algorithm validates the application of the
entailment rules at two stages ? before and af-
ter translation, through context-models applied at
each end. As the experiments will show in Sec-
tion 4, a large number of possible combinations of
entailment rules is a common scenario, and there-
fore using the source context models to reduce this
number plays an important role.
4 Experimental Setting
To assess our approach, we conducted a series of
experiments; in each experiment we applied the
scheme described in 3, changing only the mod-
els being used for scoring the generated and trans-
lated texts. The setting of these experiments is de-
scribed in what follows.
SMT data To produce sentences for our experi-
ments, we use Matrax (Simard et al, 2005), a stan-
dard phrase-based SMT system, with the excep-
tion that it allows gaps in phrases. We use approxi-
mately 1M sentence pairs from the English-French
794
Europarl corpus for training, and then translate a
test set of 5,859 English sentences from the News
corpus into French. Both resources are taken
from the shared translation task in WMT-2008
(Callison-Burch et al, 2008). Hence, we compare
our method in a setting where the training and test
data are from different domains, a common sce-
nario in the practical use of MT systems.
Of the 5,859 translated sentences, 2,494 contain
unknown terms (considering only sequences with
alphabetic symbols), summing up to 4,255 occur-
rences of unknown terms. 39% of the 2,494 sen-
tences contain more than a single unknown term.
Entailment resource We use WordNet 3.0 as
a resource for entailment rules. Paraphrases are
generated using synonyms. Directionally entailed
texts are created using hypernyms, which typically
conform with entailment. We do not rely on sense
information in WordNet. Hence, any other seman-
tic resource for entailment rules can be utilized.
Each sentence is tagged using the OpenNLP
POS tagger2. Entailment rules are applied for un-
known terms tagged as nouns, verbs, adjectives
and adverbs. The use of relations from WordNet
results in 1,071 sentences with applicable rules
(with phrase table entries) for the unknown terms
when using synonyms, and 1,643 when using both
synonyms and hypernyms, accounting for 43%
and 66% of the test sentences, respectively.
The number of alternative sentences generated
for each source text varies from 1 to 960 when
paraphrasing rules were applied, and reaches very
large numbers, up to 89,700 at the ?worst case?,
when all TE rules are employed, an average of 456
alternatives per sentence.
Scoring source texts We test our proposed
method using several context-models shown to
perform reasonably well in previous work:
? FREQ: The first model we use is a context-
independent baseline. A common useful
heuristic to pick an entailment rule is to se-
lect the candidate with the highest frequency
in the corpus (Mccarthy et al, 2004). In this
model, a rule?s score is the normalized num-
ber of occurrences of its RHS in the training
corpus, ignoring the context of the LHS.
? LSA: Latent Semantic Analysis (Deerwester
et al, 1990) is a well-known method for rep-
2http://opennlp.sourceforge.net
resenting the contextual usage of words based
on corpus statistics. We represented each
term by a normalized vector of the top 100
SVD dimensions, as described in (Gliozzo,
2005). This model measures the similarity
between the sentence words and the RHS in
the LSA space.
? NB: We implemented the unsupervised
Na??ve Bayes model described in (Glickman
et al, 2006) to estimate the probability that
the unknown term entails the RHS in the
given context. The estimation is based on
corpus co-occurrence statistics of the context
words with the RHS.
? LMS: This model generates the Language
Model probability of the RHS in the source.
We use 3-grams probabilities as produced by
the SRILM toolkit (Stolcke, 2002).
Finally, as a simple baseline, we generated a ran-
dom score for each rule application, RAND.
The score of each rule application by any of
the above models is normalized to the range (0,1].
To combine individual rule applications in a given
sentence, we use the product of their scores. The
monolingual data used for the models above is the
source side of the training parallel corpus.
Target-language scores On the target side we
used either a standard 3-gram language-model, de-
noted LMT, or the score assigned by the com-
plete SMT log-linear model, which includes the
language model as one of its components (SMT).
A pair of a source:target models comprises a
complete model for selecting the best translated
sentence, where the overall score is the product of
the scores of the two models.
We also applied several combinations of source
models, such as LSA combined with LMS, to take
advantage of their complementary strengths. Ad-
ditionally, we assessed our method with source-
only models, by setting the number of sentences to
be selected by the source model to one (k = 1).
5 Results
5.1 Manual Evaluation
To evaluate the translations produced using the
various source and target models and the different
rule-sets, we rely mostly on manual assessment,
since automatic MT evaluation metrics like BLEU
do not capture well the type of semantic variations
795
Model
Precision (%) Coverage (%)
PARAPH. TE PARAPH. TE
1 ?:SMT 75.8 73.1 32.5 48.1
2 NB:SMT 75.2 71.5 32.3 47.1
3 LSA:SMT 74.9 72.4 32.1 47.7
4 NB:? 74.7 71.1 32.1 46.8
5 LMS:LMT 73.8 70.2 31.7 46.3
6 FREQ:? 72.5 68.0 31.2 44.8
7 RAND 57.2 63.4 24.6 41.8
Table 1: Translation acceptance when using only para-
phrases and when using all entailment rules. ?:? indicates
which model is applied to the source (left side) and which to
the target language (right side).
generated in our experiments, particularly at the
sentence level.
In the manual evaluation, two native speakers
of the target language judged whether each trans-
lation preserves the meaning of its reference sen-
tence, marking it as acceptable or unacceptable.
From the sentences for which rules were applica-
ble, we randomly selected a sample of sentences
for each annotator, allowing for some overlap-
ping for agreement analysis. In total, the transla-
tions of 1,014 unique source sentences were man-
ually annotated, of which 453 were produced us-
ing only hypernyms (no paraphrases were appli-
cable). When a sentence was annotated by both
annotators, one annotation was picked randomly.
Inter-annotator agreement was measured by the
percentage of sentences the annotators agreed on,
as well as via the Kappa measure (Cohen, 1960).
For different models, the agreement rate varied
from 67% to 78% (72% overall), and the Kappa
value ranged from 0.34 to 0.55, which is compa-
rable to figures reported for other standard SMT
evaluation metrics (Callison-Burch et al, 2008).
Translation with TE For each model m, we
measured Precisionm, the percentage of accept-
able translations out of all sampled translations.
Precisionm was measured both when using only
paraphrases (PARAPH.) and when using all entail-
ment rules (TE). We also measured Coveragem,
the percentage of sentences with acceptable trans-
lations, Am, out of all sentences (2,494). As
our annotators evaluated only a sample of sen-
tences, Am is estimated as the model?s total num-
ber of sentences with applicable rules, Sm, mul-
tiplied by the model?s Precision (Sm was 1,071
for paraphrases and 1,643 for entailment rules):
Coveragem = Sm?Precisionm2,494 .
Table 1 presents the results of several source-
target combinations when using only paraphrases
and when also using directional entailment rules.
When all rules are used, a substantial improve-
ment in coverage is consistently obtained across
all models, reaching a relative increase of 50%
over paraphrases only, while just a slight decrease
in precision is observed (see Section 5.3 for some
error analysis). This confirms our hypothesis that
directional entailment rules can be very useful for
replacing unknown terms.
For the combination of source-target models,
the value of k is set depending on which rule-set
is used. Preliminary analysis showed that k = 5
is sufficient when only paraphrases are used and
k = 20 when directional entailment rules are also
considered.
We measured statistical significance between
different models for precision of the TE re-
sults according to the Wilcoxon signed ranks test
(Wilcoxon, 1945). Models 1-6 in Table 1 are sig-
nificantly better than the RAND baseline (p <
0.03), and models 1-3 are significantly better than
model 6 (p < 0.05). The difference between
?:SMT and NB:SMT or LSA:SMT is not statisti-
cally significant.
The results in Table 1 therefore suggest that
taking a source model into account preserves the
quality of translation. Furthermore, the quality is
maintained even when source models? selections
are restricted to a rather small top-k ranks, at a
lower computational cost (for the models combin-
ing source and target, like NB:SMT or LSA:SMT).
This is particularly relevant for on-demand MT
systems, where time is an issue. For such systems,
using this source-language based pruning method-
ology will yield significant performance gains as
compared to target-only models.
We also evaluated the baseline strategy where
unknown terms are omitted from the translation,
resulting in 25% precision. Leaving unknown
words untranslated also yielded very poor transla-
tion quality in an analysis performed on a similar
dataset.
Comparison to related work We compared our
algorithm with an implementation of the algo-
rithm proposed by (Callison-Burch et al, 2006)
(see Section 2.2), henceforth CB, using the Span-
ish side of Europarl as the pivot language.
Out of the tested 2,494 sentences with unknown
terms, CB found paraphrases for 706 sentences
(28.3%), while with any of our models, including
796
Model Precision (%) Coverage (%) Better (%)
NB:SMT (TE) 85.3 56.2 72.7
CB 85.3 24.2 12.7
Table 2: Comparison between our top model and the
method by Callison-Burch et al (2006), showing the per-
centage of times translations were considered acceptable, the
model?s coverage and the percentage of times each model
scored better than the other (in the 14% remaining cases, both
models produced unacceptable translations).
NB:SMT , our algorithm found applicable entail-
ment rules for 1,643 sentences (66%).
The quality of the CB translations was manually
assessed for a sample of 150 sentences. Table 2
presents the precision and coverage on this sample
for both CB and NB:SMT , as well as the number
of times each model?s translation was preferred by
the annotators. While both models achieve equally
high precision scores on this sample, the NB:SMT
model?s translations were undoubtedly preferred
by the annotators, with a considerably higher cov-
erage.
With the CB method, given that many of the
phrases added to the phrase table are noisy, the
global quality of the sentences seem to have been
affected, explaining why the judges preferred the
NB:SMT translations. One reason for the lower
coverage of CB is the fact that paraphrases were
acquired from a corpus whose domain is differ-
ent from that of the test sentences. The entail-
ment rules in our models are not limited to para-
phrases and are derived from WordNet, which has
broader applicability. Hence, utilizing monolin-
gual resources has proven beneficial for the task.
5.2 Automatic MT Evaluation
Although automatic MT evaluation metrics are
less appropriate for capturing the variations gen-
erated by our method, to ensure that there was no
degradation in the system-level scores according
to such metrics we also measured the models? per-
formance using BLEU and METEOR (Agarwal
and Lavie, 2007). The version of METEOR we
used on the target language (French) considers the
stems of the words, instead of surface forms only,
but does not make use of WordNet synonyms.
We evaluated the performance of the top mod-
els of Table 1, as well as of a baseline SMT sys-
tem that left unknown terms untranslated, on the
sample of 1,014 manually annotated sentences. As
shown in Table 3, all models resulted in improve-
ment with respect to the original sentences (base-
Model BLEU (TE) METEOR (TE)
?:SMT 15.50 0.1325
NB:SMT 15.37 0.1316
LSA:SMT 15.51 0.1318
NB:? 15.37 0.1311
CB 15.33 0.1299
Baseline SMT 15.29 0.1294
Table 3: Performance of the best models according to auto-
matic MT evaluation metrics at the corpus level. The baseline
refers to translation of the text without applying any entail-
ment rules.
line). The difference in METEOR scores is statis-
tically significant (p < 0.05) for the three top mod-
els against the baseline. The generally low scores
may be attributed to the fact that training and test
sentences are from different domains.
5.3 Discussion
The use of entailed texts produced using our ap-
proach clearly improves the quality of translations,
as compared to leaving unknown terms untrans-
lated or omitting them altogether. While it is clear
that textual entailment is useful for increasing cov-
erage in translation, further research is required to
identify the amount of information loss incurred
when non-symmetric entailment relations are be-
ing used, and thus to identify the cases where such
relations are detrimental to translation.
Consider, for example, the sentence: ?Conven-
tional military models are geared to decapitate
something that, in this case, has no head.?. In this
sentence, the unknown term was replaced by kill,
which results in missing the point originally con-
veyed in the text. Accordingly, the produced trans-
lation does not preserve the meaning of the source,
and was considered unacceptable: ?Les mode`les
militaires visent a` faire quelque chose que, dans
ce cas, n?est pas responsable.?.
In other cases, the selected hypernyms were too
generic words, such as entity or attribute, which
also fail to preserve the sentence?s meaning. On
the other hand, when the unknown term was a
very specific word, hypernyms played an impor-
tant role. For example, ?Bulgaria is the most
sought-after east European real estate target, with
its low-cost ski chalets and oceanfront homes?.
Here, chalets are replaced by houses or units (de-
pending on the model), providing a translation that
would be acceptable by most readers.
Other incorrect translations occurred when the
unknown term was part of a phrase, for exam-
ple, troughs replaced with depressions in peaks
797
and troughs, a problem that also strongly affects
paraphrasing. In another case, movement was the
hypernym chosen to replace labor in labor move-
ment, yielding an awkward text for translation.
Many of the cases which involved ambiguity
were resolved by the applied context-models, and
can be further addressed, together with the above
mentioned problems, with better source-language
context models.
We suggest that other types of entailment rules
could be useful for the task beyond the straight-
forward generalization using hypernyms, which
was demonstrated in this work. This includes
other types of lexical entailment relations, such as
holonymy (e.g. Singapore ? Southeast Asia) as
well as lexical syntactic rules (X cure Y ? treat
Y with X). Even syntactic rules, such as clause re-
moval, can be recruited for the task: ?Obama, the
44th president, declared Monday . . . ?? ?Obama
declared Monday . . . ?. When the system is un-
able to translate a term found in the embedded
clause, the translation of the less informative sen-
tence may still be acceptable by readers.
6 Conclusions and Future Work
In this paper we propose a new entailment-based
approach for addressing the problem of unknown
terms in machine translation. Applying this ap-
proach with lexical entailment rules from Word-
Net, we show that using monolingual resources
and textual entailment relationships allows sub-
stantially increasing the quality of translations
produced by an SMT system. Our experiments
also show that it is possible to perform the process
efficiently by relying on source language context-
models as a filter prior to translation. This pipeline
maintains translation quality, as assessed by both
human annotators and standard automatic mea-
sures.
For future work we suggest generating entailed
texts with a more extensive set of rules, in particu-
lar lexical-syntactic ones. Combining rules from
monolingual and bilingual resources seems ap-
pealing as well. Developing better context-models
to be applied on the source is expected to further
improve our method?s performance. Specifically,
we suggest taking into account the prior likelihood
that a rule is correct as part of the model score.
Finally, some researchers have advocated re-
cently the use of shared structures such as parse
forests (Mi and Huang, 2008) or word lattices
(Dyer et al, 2008) in order to allow a compact rep-
resentation of alternative inputs to an SMT system.
This is an approach that we intend to explore in
future work, as a way to efficiently handle the dif-
ferent source language alternatives generated by
entailment rules. However, since most current MT
systems do not accept such type of inputs, we con-
sider the results on pruning by source-side context
models as broadly relevant.
Acknowledgments
This work was supported in part by the ICT Pro-
gramme of the European Community, under the
PASCAL 2 Network of Excellence, ICT-216886
and The Israel Science Foundation (grant No.
1112/08). We wish to thank Roy Bar-Haim and
the anonymous reviewers of this paper for their
useful feedback. This publication only reflects the
authors? views.
References
Abhaya Agarwal and Alon Lavie. 2007. METEOR:
An Automatic Metric for MT Evaluation with High
Levels of Correlation with Human Judgments. In
Proceedings of WMT-08.
Karunesh Arora, Michael Paul, and Eiichiro Sumita.
2008. Translation of Unknown Words in Phrase-
Based Statistical Machine Translation for Lan-
guages of Rich Morphology. In Proceedings of
SLTU.
Francis Bond, Eric Nichols, Darren Scott Appling, and
Michael Paul. 2008. Improving Statistical Machine
Translation by Paraphrasing the Training Data. In
Proceedings of IWSLT.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved Statistical Machine Trans-
lation Using Paraphrases. In Proceedings of HLT-
NAACL.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2008.
Further Meta-Evaluation of Machine Translation. In
Proceedings of WMT.
Chris Callison-Burch. 2008. Syntactic Constraints
on Paraphrases Extracted from Parallel Corpora. In
Proceedings of EMNLP.
Jacob Cohen. 1960. A Coefficient of Agreement for
Nominal Scales. Educational and Psychological
Measurement, 20(1):37?46.
Trevor Cohn and Mirella Lapata. 2007. Machine
Translation by Triangulation: Making Effective Use
of Multi-Parallel Corpora. In Proceedings of ACL.
798
Ido Dagan, Oren Glickman, Alfio Massimiliano
Gliozzo, Efrat Marmorshtein, and Carlo Strappar-
ava. 2006. Direct Word Sense Matching for Lexical
Substitution. In Proceedings of ACL.
Scott Deerwester, S.T. Dumais, G.W. Furnas, T.K. Lan-
dauer, and R.A. Harshman. 1990. Indexing by La-
tent Semantic Analysis. Journal of the American So-
ciety for Information Science, 41.
Christopher Dyer, Smaranda Muresan, and Philip
Resnik. 2008. Generalizing Word Lattice Trans-
lation. In Proceedings of ACL-HLT.
Matthias Eck, Stephan Vogel, and Alex Waibel. 2008.
Communicating Unknown Words in Machine Trans-
lation. In Proceedings of LREC.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database (Language, Speech, and
Communication). The MIT Press.
Danilo Giampiccolo, Bernardo Magnini, Ido Dagan,
and Bill Dolan. 2007. The Third PASCAL Recog-
nising Textual Entailment Challenge. In Proceed-
ings of ACL-WTEP Workshop.
Oren Glickman, Ido Dagan, Mikaela Keller, Samy
Bengio, and Walter Daelemans. 2006. Investigat-
ing Lexical Substitution Scoring for Subtitle Gener-
ation. In Proceedings of CoNLL.
Alfio Massimiliano Gliozzo. 2005. Semantic Domains
in Computational Linguistics. Ph.D. thesis, Univer-
sity of Trento.
Francisco Guzma?n and Leonardo Garrido. 2008.
Translation Paraphrases in Phrase-Based Machine
Translation. In Proceedings of CICLing.
Nizar Habash. 2008. Four Techniques for Online
Handling of Out-of-Vocabulary Words in Arabic-
English Statistical Machine Translation. In Pro-
ceedings of ACL-HLT.
David Kauchak and Regina Barzilay. 2006. Paraphras-
ing for Automatic Evaluation. In Proceedings of
HLT-NAACL.
Kevin Knight and Jonathan Graehl. 1997. Machine
Transliteration. In Proceedings of ACL.
Philipp Koehn and Kevin Knight. 2003. Empirical
Methods for Compound Splitting. In Proceedings
of EACL.
Philippe Langlais and Alexandre Patry. 2007. Trans-
lating Unknown Words by Analogical Learning. In
Proceedings of EMNLP-CoNLL.
Dekang Lin and Patrick Pantel. 2001. DIRT ? Discov-
ery of Inference Rules from Text. In Proceedings of
SIGKDD.
Diana McCarthy and Roberto Navigli. 2007.
SemEval-2007 Task 10: English Lexical Substitu-
tion Task. In Proceedings of SemEval.
Diana Mccarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004. Finding Predominant Word Senses
in Untagged Text. In Proceedings of ACL.
Haitao Mi and Liang Huang. 2008. Forest-based
Translation Rule Extraction. In Proceedings of
EMNLP.
Sebastian Pado, Michel Galley, Daniel Jurafsky, and
Christopher D. Manning. 2009. Textual Entail-
ment Features for Machine Translation Evaluation.
In Proceedings of WMT.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic
Evaluation of Machine Translation. In Proceedings
of ACL.
M. Simard, N. Cancedda, B. Cavestro, M. Dymet-
man, E. Gaussier, C. Goutte, and K. Yamada. 2005.
Translating with Non-contiguous Phrases. In Pro-
ceedings of HLT-EMNLP.
Andreas Stolcke. 2002. SRILM ? An Extensible Lan-
guage Modeling Toolkit. In Proceedings of ICSLP.
Idan Szpektor, Ido Dagan, Roy Bar-Haim, and Jacob
Goldberger. 2008. Contextual Preferences. In Pro-
ceedings of ACL-HLT.
Frank Wilcoxon. 1945. Individual Comparisons by
Ranking Methods. Biometrics Bulletin, 1(6):80?83.
Mei Yang and Katrin Kirchhoff. 2006. Phrase-Based
Backoff Models for Machine Translation of Highly
Inflected Languages. In Proceedings of EACL.
Shiqi Zhao, Haifeng Wang, Ting Liu, and Sheng Li.
2008. Pivot Approach for Extracting Paraphrase
Patterns from Bilingual Corpora. In Proceedings of
ACL-HLT.
799
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 69?72,
Suntec, Singapore, 4 August 2009.
c?2009 ACL and AFNLP
Directional Distributional Similarity for Lexical Expansion
Lili Kotlerman, Ido Dagan, Idan Szpektor
Department of Computer Science
Bar-Ilan University
Ramat Gan, Israel
lili.dav@gmail.com
{dagan,szpekti}@cs.biu.ac.il
Maayan Zhitomirsky-Geffet
Department of Information Science
Bar-Ilan University
Ramat Gan, Israel
maayan.geffet@gmail.com
Abstract
Distributional word similarity is most
commonly perceived as a symmetric re-
lation. Yet, one of its major applications
is lexical expansion, which is generally
asymmetric. This paper investigates the
nature of directional (asymmetric) similar-
ity measures, which aim to quantify distri-
butional feature inclusion. We identify de-
sired properties of such measures, specify
a particular one based on averaged preci-
sion, and demonstrate the empirical bene-
fit of directional measures for expansion.
1 Introduction
Much work on automatic identification of seman-
tically similar terms exploits Distributional Simi-
larity, assuming that such terms appear in similar
contexts. This has been now an active research
area for a couple of decades (Hindle, 1990; Lin,
1998; Weeds and Weir, 2003).
This paper is motivated by one of the prominent
applications of distributional similarity, namely
identifying lexical expansions. Lexical expansion
looks for terms whose meaning implies that of a
given target term, such as a query. It is widely
employed to overcome lexical variability in ap-
plications like Information Retrieval (IR), Infor-
mation Extraction (IE) and Question Answering
(QA). Often, distributional similarity measures are
used to identify expanding terms (e.g. (Xu and
Croft, 1996; Mandala et al, 1999)). Here we de-
note the relation between an expanding term u and
an expanded term v as ?u ? v?.
While distributional similarity is most promi-
nently modeled by symmetric measures, lexical
expansion is in general a directional relation. In
IR, for instance, a user looking for ?baby food?
will be satisfied with documents about ?baby pap?
or ?baby juice? (?pap ? food?, ?juice ? food?);
but when looking for ?frozen juice? she will not
be satisfied by ?frozen food?. More generally, di-
rectional relations are abundant in NLP settings,
making symmetric similarity measures less suit-
able for their identification.
Despite the need for directional similarity mea-
sures, their investigation counts, to the best of
our knowledge, only few works (Weeds and Weir,
2003; Geffet and Dagan, 2005; Bhagat et al,
2007; Szpektor and Dagan, 2008; Michelbacher et
al., 2007) and is utterly lacking. From an expan-
sion perspective, the common expectation is that
the context features characterizing an expanding
word should be largely included in those of the ex-
panded word.
This paper investigates the nature of directional
similarity measures. We identify their desired
properties, design a novel measure based on these
properties, and demonstrate its empirical advan-
tage in expansion settings over state-of-the-art
measures
1
. In broader prospect, we suggest that
asymmetric measures might be more suitable than
symmetric ones for many other settings as well.
2 Background
The distributional word similarity scheme follows
two steps. First, a feature vector is constructed
for each word by collecting context words as fea-
tures. Each feature is assigned a weight indicating
its ?relevance? (or association) to the given word.
Then, word vectors are compared by some vector
similarity measure.
1
Our directional term-similarity resource will be available
at http://aclweb.org/aclwiki/index.php?
title=Textual_Entailment_Resource_Pool
69
To date, most distributional similarity research
concentrated on symmetric measures, such as the
widely cited and competitive (as shown in (Weeds
and Weir, 2003)) LIN measure (Lin, 1998):
LIN(u, v) =
?
f?FV
u
?FV
v
[w
u
(f) + w
v
(f)]
?
f?FV
u
w
u
(f) +
?
f?FV
v
w
v
(f)
where FV
x
is the feature vector of a word x and
w
x
(f) is the weight of the feature f in that word?s
vector, set to their pointwise mutual information.
Few works investigated a directional similarity
approach. Weeds and Weir (2003) and Weeds et
al. (2004) proposed a precision measure, denoted
here WeedsPrec, for identifying the hyponymy re-
lation and other generalization/specification cases.
It quantifies the weighted coverage (or inclusion)
of the candidate hyponym?s features (u) by the hy-
pernym?s (v) features:
WeedsPrec(u ? v) =
?
f?FV
u
?FV
v
w
u
(f)
?
f?FV
u
w
u
(f)
The assumption behind WeedsPrec is that if one
word is indeed a generalization of the other then
the features of the more specific word are likely to
be included in those of the more general one (but
not necessarily vice versa).
Extending this rationale to the textual entail-
ment setting, Geffet and Dagan (2005) expected
that if the meaning of a word u entails that of
v then all its prominent context features (under
a certain notion of ?prominence?) would be in-
cluded in the feature vector of v as well. Their
experiments indeed revealed a strong empirical
correlation between such complete inclusion of
prominent features and lexical entailment, based
on web data. Yet, such complete inclusion cannot
be feasibly assessed using an off-line corpus, due
to the huge amount of required data.
Recently, (Szpektor and Dagan, 2008) tried
identifying the entailment relation between
lexical-syntactic templates using WeedsPrec, but
observed that it tends to promote unreliable rela-
tions involving infrequent templates. To remedy
this, they proposed to balance the directional
WeedsPrec measure by multiplying it with the
symmetric LIN measure, denoted here balPrec:
balPrec(u?v)=
?
LIN(u, v)?WeedsPrec(u?v)
Effectively, this measure penalizes infrequent tem-
plates having short feature vectors, as those usu-
ally yield low symmetric similarity with the longer
vectors of more common templates.
3 A Statistical Inclusion Measure
Our research goal was to develop a directional
similarity measure suitable for learning asymmet-
ric relations, focusing empirically on lexical ex-
pansion. Thus, we aimed to quantify most effec-
tively the above notion of feature inclusion.
For a candidate pair ?u ? v?, we will refer to
the set of u?s features, which are those tested for
inclusion, as tested features. Amongst these fea-
tures, those found in v?s feature vector are termed
included features.
In preliminary data analysis of pairs of feature
vectors, which correspond to a known set of valid
and invalid expansions, we identified the follow-
ing desired properties for a distributional inclusion
measure. Such measure should reflect:
1. the proportion of included features amongst
the tested ones (the core inclusion idea).
2. the relevance of included features to the ex-
panding word.
3. the relevance of included features to the ex-
panded word.
4. that inclusion detection is less reliable if the
number of features of either expanding or ex-
panded word is small.
3.1 Average Precision as the Basis for an
Inclusion Measure
As our starting point we adapted the Average
Precision (AP) metric, commonly used to score
ranked lists such as query search results. This
measure combines precision, relevance ranking
and overall recall (Voorhees and Harman, 1999):
AP =
?
N
r=1
[P (r) ? rel(r)]
total number of relevant documents
where r is the rank of a retrieved document
amongst the N retrieved, rel(r) is an indicator
function for the relevance of that document, and
P (r) is precision at the given cut-off rank r.
In our case the feature vector of the expanded
word is analogous to the set of all relevant docu-
ments while tested features correspond to retrieved
documents. Included features thus correspond to
relevant retrieved documents, yielding the follow-
70
ing analogous measure in our terminology:
AP (u ? v) =
?
|FV
u
|
r=1
[P (r) ? rel(f
r
)]
|FV
v
|
rel(f) =
{
1, if f ? FV
v
0, if f /? FV
v
P (r) =
|included features in ranks 1 to r|
r
where f
r
is the feature at rank r in FV
u
.
This analogy yields a feature inclusion measure
that partly addresses the above desired properties.
Its score increases with a larger number of in-
cluded features (correlating with the 1
st
property),
while giving higher weight to highly ranked fea-
tures of the expanding word (2
nd
property).
To better meet the desired properties we in-
troduce two modifications to the above measure.
First, we use the number of tested features |FV
u
|
for normalization instead of |FV
v
|. This captures
better the notion of feature inclusion (1
st
property),
which targets the proportion of included features
relative to the tested ones.
Second, in the classical AP formula all relevant
documents are considered relevant to the same ex-
tent. However, features of the expanded word dif-
fer in their relevance within its vector (3
rd
prop-
erty). We thus reformulate rel(f) to give higher
relevance to highly ranked features in |FV
v
|:
rel
?
(f) =
{
1 ?
rank(f,FV
v
)
|FV
v
|+1
, if f ? FV
v
0 , if f /? FV
v
where rank(f, FV
v
) is the rank of f in FV
v
.
Incorporating these twomodifications yields the
APinc measure:
APinc(u?v)=
?
|FV
u
|
r=1
[P (r) ? rel
?
(f
r
)]
|FV
u
|
Finally, we adopt the balancing approach in
(Szpektor and Dagan, 2008), which, as explained
in Section 2, penalizes similarity for infrequent
words having fewer features (4
th
property) (in our
version, we truncated LIN similarity lists after top
1000 words). This yields our proposed directional
measure balAPinc:
balAPinc(u?v) =
?
LIN(u, v) ? APinc(u?v)
4 Evaluation and Results
4.1 Evaluation Setting
We tested our similarity measure by evaluating its
utility for lexical expansion, compared with base-
lines of the LIN, WeedsPrec and balPrec measures
(Section 2) and a balanced version of AP (Sec-
tion 3), denoted balAP. Feature vectors were cre-
ated by parsing the Reuters RCV1 corpus and tak-
ing the words related to each term through a de-
pendency relation as its features (coupled with the
relation name and direction, as in (Lin, 1998)). We
considered for expansion only terms that occur at
least 10 times in the corpus, and as features only
terms that occur at least twice.
As a typical lexical expansion task we used
the ACE 2005 events dataset
2
. This standard IE
dataset specifies 33 event types, such as Attack,
Divorce, and Law Suit, with all event mentions
annotated in the corpus. For our lexical expan-
sion evaluation we considered the first IE subtask
of finding sentences that mention the event.
For each event we specified a set of representa-
tive words (seeds), by selecting typical terms for
the event (4 on average) from its ACE definition.
Next, for each similarity measure, the terms found
similar to any of the event?s seeds (?u ? seed?)
were taken as expansion terms. Finally, to mea-
sure the sole contribution of expansion, we re-
moved from the corpus all sentences that contain
a seed word and then extracted all sentences that
contain expansion terms as mentioning the event.
Each of these sentences was scored by the sum of
similarity scores of its expansion terms.
To evaluate expansion quality we compared the
ranked list of sentences for each event to the gold-
standard annotation of event mentions, using the
standard Average Precision (AP) evaluation mea-
sure. We report Mean Average Precision (MAP)
for all events whose AP value is at least 0.1 for at
least one of the tested measures
3
.
4.1.1 Results
Table 1 presents the results for the different tested
measures over the ACE experiment. It shows that
the symmetric LIN measure performs significantly
worse than the directional measures, assessing that
a directional approach is more suitable for the ex-
pansion task. In addition, balanced measures con-
sistently perform better than unbalanced ones.
According to the results, balAPinc is the best-
performing measure. Its improvement over all
other measures is statistically significant accord-
ing to the two-sided Wilcoxon signed-rank test
2
http://projects.ldc.upenn.edu/ace/, training part.
3
The remaining events seemed useless for our compar-
ative evaluation, since suitable expansion lists could not be
found for them by any of the distributional methods.
71
LIN WeedsPrec balPrec AP balAP balAPinc
0.068 0.044 0.237 0.089 0.202 0.312
Table 1: MAP scores of the tested measures on the
ACE experiment.
seed LIN balAPinc
death murder, killing, inci-
dent, arrest, violence
suicide, killing, fatal-
ity, murder, mortality
marry divorce, murder, love, divorce, remarry,
dress, abduct father, kiss, care for
arrest detain, sentence,
charge, jail, convict
detain, extradite,
round up, apprehend,
imprison
birth abortion, pregnancy, wedding day,
resumption, seizure, dilation, birthdate,
passage circumcision, triplet
injure wound, kill, shoot, wound, maim, beat
detain, burn up, stab, gun down
Table 2: Top 5 expansion terms learned by LIN
and balAPinc for a sample of ACE seed words.
(Wilcoxon, 1945) at the 0.01 level. Table 2
presents a sample of the top expansion terms
learned for some ACE seeds with either LIN or
balAPinc, demonstrating the more accurate ex-
pansions generated by balAPinc. These results
support the design of our measure, based on the
desired properties that emerged from preliminary
data analysis for lexical expansion.
Finally, we note that in related experiments we
observed statistically significant advantages of the
balAPincmeasure for an unsupervised text catego-
rization task (on the 10 most frequent categories in
the Reuters-21578 collection). In this setting, cat-
egory names were taken as seeds and expanded by
distributional similarity, further measuring cosine
similarity with categorized documents similarly to
IR query expansion. These experiments fall be-
yond the scope of this paper and will be included
in a later and broader description of our work.
5 Conclusions and Future work
This paper advocates the use of directional similar-
ity measures for lexical expansion, and potentially
for other tasks, based on distributional inclusion of
feature vectors. We first identified desired proper-
ties for an inclusion measure and accordingly de-
signed a novel directional measure based on av-
eraged precision. This measure yielded the best
performance in our evaluations. More generally,
the evaluations supported the advantage of multi-
ple directional measures over the typical symmet-
ric LIN measure.
Error analysis showed that many false sentence
extractions were caused by ambiguous expanding
and expanded words. In future work we plan to
apply disambiguation techniques to address this
problem. We also plan to evaluate the performance
of directional measures in additional tasks, and
compare it with additional symmetric measures.
Acknowledgements
This work was partially supported by the NEGEV
project (www.negev-initiative.org), the PASCAL-
2 Network of Excellence of the European Com-
munity FP7-ICT-2007-1-216886 and by the Israel
Science Foundation grant 1112/08.
References
R. Bhagat, P. Pantel, and E. Hovy. 2007. LEDIR: An
unsupervised algorithm for learning directionality of
inference rules. In Proceedings of EMNLP-CoNLL.
M. Geffet and I. Dagan. 2005. The distributional in-
clusion hypotheses and lexical entailment. In Pro-
ceedings of ACL.
D. Hindle. 1990. Noun classification from predicate-
argument structures. In Proceedings of ACL.
D. Lin. 1998. Automatic retrieval and clustering of
similar words. In Proceedings of COLING-ACL.
R. Mandala, T. Tokunaga, and H. Tanaka. 1999. Com-
bining multiple evidence from different types of the-
saurus for query expansion. In Proceedings of SI-
GIR.
L. Michelbacher, S. Evert, and H. Schutze. 2007.
Asymmetric association measures. In Proceedings
of RANLP.
I. Szpektor and I. Dagan. 2008. Learning entailment
rules for unary templates. In Proceedings of COL-
ING.
E. M. Voorhees and D. K. Harman, editors. 1999. The
Seventh Text REtrieval Conference (TREC-7), vol-
ume 7. NIST.
J. Weeds and D. Weir. 2003. A general framework for
distributional similarity. In Proceedings of EMNLP.
J. Weeds, D. Weir, and D. McCarthy. 2004. Character-
ising measures of lexical distributional similarity. In
Proceedings of COLING.
F. Wilcoxon. 1945. Individual comparisons by ranking
methods. Biometrics Bulletin, 1:80?83.
J. Xu and W. B. Croft. 1996. Query expansion using
local and global document analysis. In Proceedings
of SIGIR.
72
Cross-dataset Clustering: Revealing Corresponding  
Themes Across Multiple Corpora 
 
Ido DAGAN 
Department of Computer Science  
Bar-Ilan University 
Ramat-Gan, Israel, 52900 
and LingoMotors Inc. 
dagan@lingomotors.com 
Zvika MARX 
Center for Neural Computation 
The Hebrew University 
and CS Dept., Bar-Ilan University 
Ramat-Gan, Israel, 52900 
zvim@cs.huji.ac.il 
Eli SHAMIR 
School of Computer Science 
and Engineering 
The Hebrew University 
Jerusalem, Israel, 91904 
shamir@cs.huji.ac.il 
 
Abstract  
We present a method for identifying 
corresponding themes across several corpora 
that are focused on related, but distinct, 
domains. This task is approached through 
simultaneous clustering of keyword sets 
extracted from the analyzed corpora.  Our 
algorithm extends the information-
bottleneck soft clustering method for a 
suitable setting consisting of several 
datasets.  Experimentation with topical 
corpora reveals similar aspects of three 
distinct religions.  The evaluation is by way 
of comparison to clusters constructed 
manually by an expert. 
1 Introduction 
This paper addresses the problem of detecting 
corresponding subtopics, or themes, within 
related bodies of text.  Such task is typical to 
comparative research, whether commercial or 
scientific: a conceivable application would aim 
at detecting corresponding characteristics 
regarding, e.g., companies, markets, legal 
systems or political organizations. 
Clustering has often been perceived as a mean 
for extracting meaningful components from data 
(Tishby, Pereira and Bialek, 1999).  Regarding 
textual data, clusters of words (Pereira, Tishby 
and Lee, 1993) or documents (Lee and Seung, 
1999; Dhillon and Modha, 2001) are often 
interpreted as capturing topics or themes that 
play prominent role in the analyzed texts. 
Our work extends the ?standard? clustering 
paradigm, which pertains to a single dataset.  
We address a setting in which several datasets, 
corresponding to related domains, are given.  
We focus on the comparative task of detecting 
those themes that are expressed across several 
datasets, rather than discovering internal themes 
within each individual dataset. 
More specifically, we address the task of 
clustering simultaneously multiple datasets such 
that each cluster includes elements from several 
datasets, capturing a common theme, which is 
shared across the sets.  We term this task cross-
dataset (CD) clustering. 
In this article we demonstrate CD clustering 
through detecting corresponding themes across 
three different religions.  That is: we apply our 
approach to three sets of religion-related 
keywords, extracted from three corpora, which 
include encyclopedic entries and introductory 
articles regarding Buddhism, Christianity and 
Islam.  Each one of three representative 
keyword-sets, which were extracted from the 
above corpora, presumably encapsulates topics 
and themes discussed within its source corpus.  
Our algorithm succeeds to reveal common 
themes such as scriptures, rituals and schools 
through respective keyword clusters consisting 
of terms such as Sutra, Bible and Quran; 
Full Moon, Easter and Id al Fitr; Theravada, 
Protestant and Shiite (see Table 1 below for a 
detailed depiction of our results). 
The CD clustering algorithm, presented in 
Section 2.2 below, extends the information 
bottleneck (IB) soft clustering method.  Our 
modifications to the IB formulation enable the 
clustering algorithm to capture characteristic 
patterns that run across different datasets, rather 
then being ?trapped? by unique characteristics of 
individual datasets. 
Like other topic discovery tasks that are 
approached by clustering, the goal of CD 
clustering is not defined in precise terms.  Yet, it 
is clear that its focus on detecting themes in a 
comparative manner, within multiple datasets, 
distinguishes CD clustering substantially from 
the standard single-dataset clustering paradigm.  
A related problem, of detecting analogies 
between different information systems has been 
addressed in the past within cognitive research 
(e.g. Gentner, 1983; Hofstadter et al, 1995).  
Recently, a related computational method for 
detecting corresponding themes has been 
introduced (coupled clustering, Marx et al, 
2002).  The coupled clustering setting, however, 
being focused on detecting analogies, is limited 
to two data sets.  Further, it requires similarity 
values between pairs of data elements as input: 
this setting does not seem straightforwardly 
applicable to the multiple dataset setting.  Our 
method, in distinction, uses a more direct source 
of information, namely word co-occurrence 
statistics within the analyzed corpora.  Another 
difference is that we take the ?soft? approach to 
clustering, producing probabilities of 
assignments into clusters rather than a 
deterministic 0/1 assignment values. 
2 Algorithmic Framework 
2.1 Review of the IB Clustering Algorithm 
The information bottleneck (IB) iterative 
clustering method is a recent approach to soft 
(probabilistic) clustering for a single set, denoted 
by X, consisting of elements to be clustered 
(Tishby, Pereira & Bialek, 1999).  Each element 
x?X is identified by a probabilistic feature 
vector, with an entry, p(y|x), for every feature y 
from a pre-determined set of features Y.  The 
p(y|x) values are estimated from given co-
occurrence data:  
p(y|x)  = ? ?Yy yxcount
yxcount
' )',(
),(   
(hence ?y?Y  p(y|x) = 1 for every x in X). 
The IB algorithm is derived from information 
theoretic considerations that we do not address 
here.  It computes, through an iterative EM-like 
process, probabilistic assignments p(c|x) for 
each element x into each cluster c.  Starting with 
random (or heuristically chosen) p(c|x) values at 
time t = 0, the IB algorithm iterates the 
following steps until convergence: 
IB1: Calculate for each cluster c its marginal 
probability:   
? ? ?= Xx tt xcpxpcp )|()()( 1 . 
IB2: Calculate for each feature y and cluster c 
a conditional probability p(y|c): 
? ? ?= Xx tt cxpxypcyp )|()|()|( 1 . 
(Bayes' rule is used to compute p(x|c)). 
IB3: Calculate for each element x and each 
cluster c a value p(c|x), indicating the 
?probability of assignment? of x into c: 
?= ' ,
,
)',()'(
),()()|(
c
Y
tt
Y
tt
t cxsimcp
cxsimcpxcp ?
?
 , 
with simtY,?(x,c) = exp{??DKL[ p(y|x)|| pt(y|c) ]} 
(DKL is the Kullback-Leibler divergence, see 
Cover & Thomas, 1991). 
The parameter ? controls the sensitivity of the 
clustering procedure to differences between the 
p(y|c) values.  The higher ? is, the more 
?determined? the algorithm becomes in 
assigning each element into the closest cluster.  
As ? is increased, more clusters that are 
separable from each other are obtained upon 
convergence (the target number of clusters is 
fixed).  We want to ensure, however, that 
assignments do not follow more than necessary 
minute details of the given data, as a result of 
too high ? (similarly to over generalization in 
supervised settings).  The IB algorithm is 
therefore applied repeatedly in a cooling-like 
process: it starts with a low ? value, 
corresponding to low temperature, which is 
increased every repetition of the whole iterative 
converging cycle, until the desired number of 
separate clusters is obtained. 
2.2 The Cross-dataset (CD) Clustering Method 
The (soft) CD clustering algorithm receives as 
input multiple datasets along with their feature 
vectors.  In the current application, we have 
three sets extracted from the corresponding 
corpora ? XBuddhism, XChristianity, and XIslam ? each of 
~150 keywords to be clustered.  A particular 
keyword might appear in two or more of the 
datasets, but the CD setting considers it as a 
distinct element within each dataset, thus 
keeping the sets of clustered elements disjoint.  
Like the IB clustering algorithm, the CD 
algorithm produces probabilistic assignments of 
the data elements. 
The feature set Y consists, in the current work, of 
about 7000 content words, each occurs in at least 
two of the examined corpora.  The set of 
features is used commonly for all datasets, thus 
it underlies a common representation, which 
enables the clustering process to compare 
elements of different sets. 
Naively approached, the original IB algorithm 
could be utilized unaltered to the multiple-
dataset setting, simply by applying it to the 
unified set X, consisting of the union of the 
disjoint Xi's.  The problem of this simplistic 
approach is that each dataset has its own 
characteristic features and feature combinations, 
which correspond to prominent topics discussed 
uniquely in that corpus.  A standard clustering 
method, such as the IB algorithm, would have a 
tendency to cluster together elements that 
originate in the same dataset, producing clusters 
populated mostly by elements from a single 
dataset (cf. Marx et al 2002).  The goal of CD 
clustering is to neutralize this tendency and to 
create clusters containing elements that share 
common features across different datasets. 
To accomplish this goal, we change the criterion 
by which elements are assigned into clusters.  
Recall that the assignment of an element x to a 
cluster c is determined by the similarity of their 
characterizing feature distributions, p(y|x) and 
p(y|c) (step IB3).  The problem lies in using the 
p(y|c) distribution, which is determined by 
summing p(y|x) values over all cluster elements, 
to characterize a cluster without taking into 
account dataset boundaries.  Thus, for a certain 
y, p(y|c) might be high despite of being 
characteristic only for cluster elements 
originating in a single dataset.  This results in 
the tendency discussed above to favor clusters 
consisting of elements of a single dataset. 
Therefore, we define a biased probability 
distribution, p~c(y), to be used by the CD 
clustering algorithm for characterizing a cluster 
c.  It is designed to call attention to y's that are 
typical for cluster members in all, or most, 
different datasets.  Consequently, an element x 
would be assigned to a cluster c (as in step IB3) 
in accordance to the degree of similarity 
between its own characteristic features and those 
characterizing other cluster members from all 
datasets.  The resulting clusters would thus 
contain representatives of all datasets. 
The definition of p~c(y) is based on the joint 
probability p(y,c,Xi).  First, compute the 
geometric mean of p(y,c,Xi) over all Xi, weighted 
by p(Xi): 
?(y,c) = ?i (p(y,c,Xi)) p(Xi)  
(see Appendix below for the details of how  
p(Xi) and p(y,c,Xi) are calculated). 
? is not a probability measure, but just a 
function of y and c into [0,1].  However, since a 
geometric mean reflects ?uniformity? of the 
averaged values, ? captures the degree to which 
p(y,c,Xi) values are high across all datasets. 
We found empirically that at this stage, it is 
advantageous to normalize ? across all clusters 
and then to rescale the resulting probabilities 
(over the c's, for each y) by the original p(y): 
?'(y,c) = (
 
?(y,c) / ?c' ?(y,c') ) ? p(y) . 
Finally, to obtain a probability distribution over 
y for each cluster c, normalize the obtained 
?'(y,c) over all y's: 
p~c(y) = ?'(y,c) / ?y' ?'(y',c) . 
As explained, p~c(y) characterizes c (analogously 
to p(y|c) in IB), while ensuring that the feature-
based similarity of c to any element x reflects 
feature distribution across all data sets. 
The CD clustering algorithm, starting at t = 0, 
iterates, in correspondence to the IB algorithm, 
the following steps: 
CD1: Calculate for each cluster c its marginal 
probability (same as IB1): 
? ? ?= Ui iXx tt xcpxpcp )|()()( 1 . 
CD2:  Compute p~c(y) as described above. 
CD3: Compute p(c|x) (with p~c(y) playing the 
role played by p(y|c) in IB3): 
?= ' ,
,
)',()'(
),()()|(
c
Y
tt
Y
tt
t cxSIMcp
cxSIMcpxcp ?
?
, 
with SIMtY,?(x,c) = exp {??DKL[ p(y|x)|| ctp~ (y)]}. 
3 CD Clustering for Religion Comparison 
The three corpora that are focused on the 
compared religions ? Buddhism, Christianity 
and Islam ? have been downloaded from the 
Internet.  Each corpus contains 20,000?40,000 
word tokens (5?10 Megabyte).  We have used a 
text mining tool to extract most religion 
keywords that form the three sets to which we 
applied the CD algorithm.  The software we 
have used ? TextAnalyst 2.0 ? identifies within 
the corpora key-phrases, from which we have 
excluded items that appear in fewer than three 
Table 1: Results of religion keyword CD clustering.  The authors have set the cluster titles.  For each 
cluster c and each religion, the 15 keywords x with the highest probability of assignment within the 
cluster are displayed (assignment probabilities, i.e. p(c|x) values are indicated in brackets).  Terms 
that were used by the expert (see Table 2) are underlined. 
Buddhism Christianity Islam 
?1 (Cherished Qualities) 
god (.68), amida (.58), bodhisattva (.50), 
salvation (.45), enlightenment (.43), deva 
(.43), attain (.41), sacrifice (.39), awaken 
(.25), spirit (.25), nirvana (.24),          
buddha nature (.24), humanity (.22), 
speech (.18), teach (.18) 
god (.69), good works (.65), love of god 
(.62), salvation (.60), gift (.58), intercession 
(.56), repentance (.55), righteousness (.53), 
peace (.52), love (.51), obey god (.49), 
saviour (.48), atonement (.46), holy ghost 
(.45), jesus christ (.45) 
god (.86), one god (.84), allah (.76), 
bless (.76), worship (.75), submission 
(.73), peace (.73), command (.72), 
guide (.71), divinity (.70), messanger 
(.70), believe (.62), mankind (.61), 
commandment (.58), witness (.57) 
?2  (Customs and Festivals) 
full moon (.99), stupa (.98), mantra (.96), 
pilgrim (.96), monastery (.89), temple (.86), 
statue (.73), worship (.61), monk (.54), 
mandala (.32), trained (.23), bhikkhu (.15), 
disciple (.12), meditation (.11), nun (.11) 
easter (.99), sunday (.99), christmas (.99), 
service (.98), city (.98), eucharist (.96), 
pilgrim (.95), pentecost (.93), jerusalem 
(.91), pray (.89), worship (.82), minister (.73), 
ministry (.70), read bible (.50), mass (.24) 
id al fitr (.99), friday (.99), ramadan 
(.99), eid (.99), pilgrim (.99), mosque 
(.99), mecca (.99), kaaba (.99), salat 
(.99), fasting (.99), medina (.98), city 
(.98), pray (.98), hijra (.97), charity (.96) 
?3 (Spiritual States) 
phenomena (.94), problem (.93), 
mindfulness (.92), awareness (.92), 
consciousness (.91), law (.88), emptiness 
(.88), samadhi (.87), sense (.87), 
experience (.86), wisdom (.83), moral (.83), 
karma (.82), find (.81), exist (.80) 
moral (.96), problem (.94), argue (.91), 
question (.87), argument (.74), experience 
(.73), incarnation (.72), relationship (.71), 
idolatry (.58), find (.45), law (.41), learn (.38), 
confession (.34), foundation (.32), faith (.31) 
moral (.93), spirit (.79), question (.75), 
life (.71), freedom (.67), existence 
(.56), humanity (.53), find (.52), faith 
(.52), code (.51), law (.41), universe 
(.39), being (.36), teach (.35), 
commandment (.29) 
?4  (Sorrow, Sin, Punishment and Reward) 
lamentation (.99), grief (.99), animal (.89), 
pain (.87), death (.86), kill (.84), 
reincarnation (.81), realm (.76), samsara 
(.69), rebirth (.61), dukkha (.56), anger (.53), 
soul (.43), nirvana (.43), birth (.33) 
punish (.94), hell (.93), violence (.86), fish 
(.86), sin (.83), earth (.81), soul (.78), death 
(.77), sinner (.76), sinful (.74), heaven (.73), 
satan (.72), suffer (.71), flesh (.71),  
judgment (.67) 
hell (.97), earth (.88), heaven (.87), 
death (.85), sin (.85), alcohol (.69), 
satan (.60), face (.59),                  
day of judgment (.52), deed (.48), 
angel (.25), being (.24), universe (.16), 
existence (.13), bearing (.12) 
?5 (Schools, Traditions and their Originating Places) 
korea (.99), china (.99), tibet (.99), 
theravada (.99), school (.99), asia (.99), 
founded (.99), west (.99), sri lanka (.99), 
mahayana (.99), india (.99), history (.99), 
hindu (.99), japan (.99), study (.99) 
cardinal (.99), orthodox (.99), protestant 
(.99), university (.99), vatican (.99), catholic 
(.99), bishop (.99), rome (.99), pope (.99), 
monk (.99), tradition (.99), theology (.99), 
baptist (.98), church (.98), divinity (.93) 
africa (.99), shiite (.99), sunni (.99), 
shia (.99), west (.99), christianity (.99), 
arab (.99), founded (.98), arabia (.97), 
sufi (.96), history (.96), fiqh (.95), 
scholar (.91), imam (.90), jew (.89) 
?6 (Names, Places, Characters, Narratives) 
gautama (.96), king (.95), friend (.68), 
disciple (.60), birth (.48), hear (.43), ascetic 
(.41), amida (.40), deva (.33), teach (.19), 
sacrifice (.15), statue (.14), buddha (.12), 
bodhisattva (.12), dharma (.09) 
bethlehem (.98), jordan (.97), mary (.95), 
lamb (.90), king (.90), second coming (.81), 
born (.76), israel (.74), child (.73), elijah (.72), 
baptize (.70), john the baptist (.68), priest 
(.68), adultery (.65), zion (.61) 
husband (.99), ismail (.98), father 
(.97), son (.95), mother (.94), born 
(.92), wife (.92), child (.89), ali (.88), 
musa (.71), isa (.70), ibrahim (.67), 
caliph (.43), tribe (.35), saint (.30) 
?7 (Scripture) 
tripitaka (.98), sanskrit (.94), translate (.93), 
sutra (.85), discourse (.79), pali canon 
(.74), story (.66), book (.64), word (.61), write 
(.45), buddha (.39), lama (.32), text (.23), 
dharma (.21), teacher (.17) 
hebrew (.99), translate (.99), gospels (.99), 
greek (.99), book (.98), new testament 
(.98), old testament (.96), passage (.96), 
matthew (.95), write (.94), luke (.93), 
apostle (.93),  bible (.91),  paul (.90),      
john (.90) 
translatee (.99), bible (.99), write (.98), 
book (.97), hadith (.96), sunna (.96), 
quran (.94), word (.93), story (.93), 
revelation (.88), companion (.80), 
muhammad (.80), prophet (.73), 
writing (.71), read quran (.46) 
corpus documents1. Thus, composite and rare 
terms as well as phrases that the software has 
inappropriately segmented were filtered out.  We 
have added to the automatically extracted terms 
additional items contributed by a comparative 
religion expert (about 15% of the sets were thus 
not extracted automatically, but those terms 
occur frequently enough to underlie informative 
co-occurrence vectors). 
The common set of features consists of all 
corpus words that occur in at least three different 
documents within two or three of the corpora, 
excluding a list of common function words.  Co-
occurrences were counted within a bi-directional 
five-word window, truncated by sentence ends. 
The number of clusters produced ? seven ? was 
empirically determined as the maximal number 
with relatively large proportion (p(c) > .01) for 
all clusters.  Trying eight clusters or more, we 
obtain clusters of minute size, which apparently 
do not reveal additional themes or topics.  Table 
1 presents, for each cluster c and each religion, 
the 15 keywords x with the highest p(c|x) values.  
The number 15 has no special meaning other 
than providing rich, balanced and displayable 
notion of all clusters.  The displayed 3?15 
keyword subsets are denoted ?1??7.   
We gave each cluster a title, reflecting our 
(naive) impression of its content.  As we 
interpret the clusters, they indeed reveal 
prominent aspects of religion: rituals (?2), 
schools (?5), narratives (?6) and scriptures (?7).  
More delicate issues, such as cherished qualities 
(?1), spiritual states (?3), suffering and sin (?4) 
are reflected as well, in spite of the very 
different position taken by the distinct religions 
with regard to these issues. 
3.1 Comparison to Expert Data 
We have asked an expert of comparative religion 
studies to simulate roughly the CD clustering 
task: assigning (freely-chosen) keywords into 
corresponding subsets, reflecting prominent 
resembling aspects that cut across the three 
examined religions.  The expert was not asked to 
indicate a probability of assignment, but he was 
allowed to use the same keyword in more than 
                                                     
1 An evaluation copy of TextAnalyst, by 
MicroSystems Ltd., can be downloaded from 
http://www.megaputer.com/php/eval.php3 
one cluster.  The expert clusters, with the 
exclusion of terms that we were not able to 
locate in our corpora, are displayed in Table 2.  
In addition to our tags ? e1?e8 ? the expert gave 
a title to each cluster. 
Although the keyword-clustering task is highly 
subjective, there are notable overlapped regions 
shared by the expert clusters and ours.  Two 
expert topics ? ?Books? (e1) and ?Ritual? (e3) ? 
are clearly captured (by ?7 and ?2 respectively).  
?Society and Politics? (e4) and ?Establishments? 
(e5) ? are both in some correspondence with our 
?Schools and Traditions? cluster (?5).  On the 
other hand, our output fails to capture the 
?Mysticism? expert cluster (e6).  Further, our 
output suggests the ?spiritual states? theme (?3) 
and distinguishes cherished qualities (?1) from 
sin and suffering (?4).  Such observations might 
make sense but are not covered by the expert. 
To quantify the overall level of overlap between 
our output and the expert's, we introduce 
suitable versions of recall and precision 
measures.   
We want the recall measure to reflect the 
proportion of expert terms that are captured by 
our configuration, provided that an optimal 
correspondence between our clusters to the 
expert is considered.  Hence, for each expert 
cluster, ej, we find a particular ?k with maximal 
number of overlapping terms (note that two or 
more expert clusters are allowed to be covered 
by a single ?k, to reflect cases where several 
related sub-topics are merged within our results).  
Denote this maximal number by M(ej):  
M(ej) = maxk =1?7  |{x?ej: x??k) }| . 
Consequently, the recall measure R is defined to 
be the sum of the above maximal overlap counts 
over all expert clusters, divided by all 131 expert 
terms (repetitions in distinct clusters counted): 
R = 
 
?j=1?8 M(ej) / 131. 
To estimate how precise our results are, we are 
interested in the relative part of our clusters, 
reduced to the expert terms, which has been 
assigned to the ?right? expert cluster by the 
same optimal correspondence.  Note that in this 
case we do not want to sum up several M values 
that are associated with a single ?k: a single 
cluster covering several expert clusters should 
be considered as an indication of poor precision.   
Furthermore, if we do this, we might recount 
some of ?k's terms (specifically, keywords that 
the expert has included in several clusters; this 
might result in precision > 100%).  We need 
therefore to consider at most one M value per ?k, 
namely the largest one.  Define 
M*(?k) = maxj =1?7 {M(ej): |?k ? ej| = M(ej)} 
(M*(?k) = 0 if the set on the right-hand side is 
empty, i.e. there is no ej that share M(ej) 
elements with ?k).  The global precision measure 
P is the sum of all M* values, divided by the 
number of expert terms appearing among the ?k's  
(repetitions counted), which are, in the current 
case, the 94 underlined terms in Table 1: 
P = 
 
?k =1?7 M*(?k) / 94. 
Our algorithm has achieved the following 
values: R 
 
= 
 
67/131 
 
= 
 
0.51, P 
 
=  58/94 
 
= 
 
0.62.  
This is a notable improvement relatively to the 
IB algorithm results: R 
 
= 
 
42/131 
 
= 
 
0.32 and 
P 
 
= 
 
32/82 
 
= 
 
0.39 (random assignment of the 
keywords into seven clusters yield average 
values R 
 
= 
 
0.36, P 
 
= 
 
0.33).  As we have 
expected, three of the clusters produced by the 
IB algorithms are populated, with very high 
probability, by most keywords of a single 
religion.  Within these specific religion clusters 
as well as the other sparsely populated clusters, 
the ranking inducted by the p(c|x) values is not 
very informative regarding particular sub-topics.  
Thus, the IB performs the CD clustering task 
poorly, even in comparison to random results. 
We note that, similarly to our algorithm, the IB 
algorithm produces at most 7 clusters of non-
negligible size.  This somewhat supports our 
Table 2: The expert cross-dataset clusters.  Cluster titles were assigned by the expert.  For each expert 
cluster, the best fitting automated cross-dataset cluster is indicated on the right-hand side, as well as 
the number of relevant expert words it includes.  The terms of this best-fit cluster are underlined.  
Superscripts indicate indices of the cross-dataset cluster(s), among ?1??7, to which each term 
belongs. 
Buddhism Christianity Islam 
e1: Scriptures                 ?7 ! 14 (of 19)   
sutra7, mantra2, mandala2, koan, 
pali canon7 
new  testament7, old  testament7, 
bible7, apostle7, revelation, john7, 
paul7,  luke7,  matthew7 
quran7, hadith7, sunna7, sharia, 
muhammad7 
e2: Beliefs and Ideas                 ?4 ! 10 (of 25)   
nirvana14, four  noble  truths, 
dharma6,7, dukkha4, buddha 
nature1, tantra, emptiness3, 
reincarnation4 
resurrection, heaven4, hell4, trinity, 
second  coming6,    jesus  christ1, 
love  of  god1, god1, satan4, cross, 
dove4,  fish4 
prophet7, allah1, one  god1, 
five  pillars,  heaven4,  hell4 
e3: Ritual, Prayer, Holydays                 ?2 ! 16 (of 20)  
meditation2, statue2, sacrifice1,6, 
gift,  stupa2 
sunday2, pray2, confession3, 
eucharist2,   christmas2,  baptism 
pilgrim2, charity2, ramadan2, 
fasting2, id  al  fitr2, pray2, 
friday2,   kaaba2,   mecca2 
e4: Society and Politics                 ?5 ! 9 (of 19)  
dalai  lama, monk2, bodhisattva1,6, 
lama7 
rome5, vatican5, church5, minister2, 
priest6,  cardinal5,  pope5,  bishop5 
sharia, caliph6, imam5, shia5, 
sunna7, ali6, sufi5 
e5: Establishments                 ?5 ! 6 (of 10)   
monastery2, temple2, sangha, 
school5 
church5,   cardinal5,  pope5,  bishop5 mosque2, imam5 
e6: Mysticism                 ?2 ! 2 (of 11)  
meditation2, nirvana14, samadhi3, 
tantra 
eucharist2, miracle, crucifixion, 
suffer4,   love1,   saint 
sufi5 
e7: Learning and Education                 ?5 ! 4 (of 8)   
monastery2, monk2, sutra7, 
meditation2 
monk5, university5, theology5, 
divinity5 
 
e8: Names and Places                 ?6 ! 7 (of 20)   
gautama6,  buddha6,7 jesus  christ1, john  the  baptist6, 
jordan6, jerusalem2, bethlehem6, 
mary6, rome5, john7, paul7, luke7, 
matthew7,  zion6 
muhammad7, ali6, mecca2, 
medina2 
impression that the limit on number of 
?interesting? clusters reflects intrinsic 
exhaustion of the information embodied within 
the given data.  It is yet to be carefully examined 
whether this observation provides any hint 
regarding the general issue of the ?right? number 
of clusters. 
4 Conclusion 
This paper addressed the relatively unexplored 
problem of detecting corresponding themes 
across multiple corpora.  We have developed an 
extended clustering algorithm that is based on 
the appealing and highly general Information 
Bottleneck method.  Substantial effort has been 
devoted to adopting this method for the Cross-
Dataset clustering task. 
Our approach was demonstrated empirically on 
the challenging task of finding corresponding 
themes across different religions. Subjective 
examination of the system's output, as well as its 
comparison to the output of a human expert, 
demonstrate the potential benefits of applying 
this approach in the framework of comparative 
research, and possibly in additional text mining 
applications. 
Given the early stage of this line of research, 
there is plenty of room for future work. In 
particular, further research is needed to provide 
theoretic grounding for the CD clustering 
formulations and to specify their properties.  
Empirical work is needed to explore the 
potential of the proposed paradigm for other 
textual domains as well as for related 
applications.  Particularly, we have recently 
presented a similar framework for template 
induction in information extraction (cross-
component clustering, Marx, Dagan, & Shamir, 
2002), which should be studied in relation to the 
CD algorithm presented here. 
Appendix 
The value of p(Xi), which is required for the 
calculations in Section 3.2, is given directly 
from the input co-occurrence data as follows: 
        p(Xi)  = ?
?
??
??
YyXx
YyXx
yxcount
yxcount
i
 '
 
),'(
),(
 
The values pt(c|Xi), pt(y|c,Xi) are calculated from 
values that are available at time step t?1: 
        ? ? ?= iXx tit xcpxpXcp )|()()|( 1 , 
        ? ? ?= iXx itit XcxpxypXcyp ),|()|(),|( 1  
(pt?1(x|c,Xi) is due to Bayes' rule conditioned on 
Xi: pt?1(x|c,Xi) = pt?1(c|x) ? p(x) / pt?1(c|Xi); note 
that pt?1(c|x) = pt?1(c|x,Xi) ). 
Finally we have: 
        pt(y,c,Xi) = pt(y|c,Xi) ? pt(c|Xi) ? p(Xi) . 
Acknowledgments 
We thank Eitan Reich for providing the expert 
data, as well as for illuminating discussions. 
This work has been partially supported by 
ISRAEL SCIENCE FOUNDATION founded by 
The Academy of Sciences and Humanities 
(grants 574/98-1 and 489/00). 
References 
Cover, T. M. and Thomas, J. A. (1991).  Elements of 
Information Theory.  New York: John Wiley & 
Sons, Inc. 
Dhillon I. S. and Modha D. S. (2001).  Concept 
decompositions for large sparse text data using 
clustering.  Machine Learning, 42/1, pp. 143?175. 
Gentner, D. (1983). Structure-mapping: a theoretical 
framework for analogy.  Cognitive Science, 7, pp. 
155?170. 
Hofstadter, D. R. and the Fluid Analogies Research 
Group (1995). Fluid Concepts and Creative 
Analogies. New-York: Basic Books, 518 p. 
Lee D. D. and Seung H. S. (1999).  Learning the 
parts of objects by non-negative matrix 
factorization.  Nature, 401/6755, pp. 788?791. 
Marx, Z., Dagan, I. and Shamir, E. (2002).  Cross-
component clustering for template induction. 
Workshop on Text Learning (TextML-2002), 
Sydney, Australia. 
Marx, Z., Dagan, I., Buhmann, J. M. and Shamir, E. 
(2002).  Coupled clustering: a method for detecting 
structural correspondence. Journal of Machine 
Learning Research, accepted for publication. 
Pereira, F. C. N., Tishby N. and Lee L. J.  (1993). 
Distributional clustering of English words. In: 
Proceedings of the 31st Annual Meeting of the 
Association for Computational Linguistics ACL' 
93, Columbus, OH, pp. 183?190. 
Tishby, N., Pereira, F. C. and Bialek, W.  (1999).  
The information bottleneck method.  In: The 37th 
Annual Allerton Conference on Communication, 
Control, and Computing, Urbana-Champaign, IL, 
pp. 368?379. 
Scaling Web-based Acquisition of Entailment Relations
Idan Szpektor
idan@szpektor.net
?ITC-Irst, Via Sommarive, 18 (Povo) - 38050 Trento, Italy
?DIT - University of Trento, Via Sommarive, 14 (Povo) - 38050 Trento, Italy
?Department of Computer Science, Bar Ilan University - Ramat Gan 52900, Israel
Department of Computer Science, Tel Aviv University - Tel Aviv 69978, Israel
Hristo Tanev?
tanev@itc.it
Ido Dagan?
dagan@cs.biu.ac.il
Bonaventura Coppola??
coppolab@itc.it
Abstract
Paraphrase recognition is a critical step for nat-
ural language interpretation. Accordingly, many
NLP applications would benefit from high coverage
knowledge bases of paraphrases. However, the scal-
ability of state-of-the-art paraphrase acquisition ap-
proaches is still limited. We present a fully unsuper-
vised learning algorithm for Web-based extraction
of entailment relations, an extended model of para-
phrases. We focus on increased scalability and gen-
erality with respect to prior work, eventually aiming
at a full scale knowledge base. Our current imple-
mentation of the algorithm takes as its input a verb
lexicon and for each verb searches the Web for re-
lated syntactic entailment templates. Experiments
show promising results with respect to the ultimate
goal, achieving much better scalability than prior
Web-based methods.
1 Introduction
Modeling semantic variability in language has
drawn a lot of attention in recent years. Many ap-
plications like QA, IR, IE and Machine Translation
(Moldovan and Rus, 2001; Hermjakob et al, 2003;
Jacquemin, 1999) have to recognize that the same
meaning can be expressed in the text in a huge vari-
ety of surface forms. Substantial research has been
dedicated to acquiring paraphrase patterns, which
represent various forms in which a certain meaning
can be expressed.
Following (Dagan and Glickman, 2004) we ob-
serve that a somewhat more general notion needed
for applications is that of entailment relations (e.g.
(Moldovan and Rus, 2001)). These are directional
relations between two expressions, where the mean-
ing of one can be entailed from the meaning of the
other. For example ?X acquired Y? entails ?X owns
Y?. These relations provide a broad framework for
representing and recognizing semantic variability,
as proposed in (Dagan and Glickman, 2004). For
example, if a QA system has to answer the question
?Who owns Overture?? and the corpus includes the
phrase ?Yahoo acquired Overture?, the system can
use the known entailment relation to conclude that
this phrase really indicates the desired answer. More
examples of entailment relations, acquired by our
method, can be found in Table 1 (section 4).
To perform such inferences at a broad scale, ap-
plications need to possess a large knowledge base
(KB) of entailment patterns. We estimate such a
KB should contain from between a handful to a few
dozens of relations per meaning, which may sum
to a few hundred thousands of relations for a broad
domain, given that a typical lexicon includes tens of
thousands of words.
Our research goal is to approach unsupervised ac-
quisition of such a full scale KB. We focus on de-
veloping methods that acquire entailment relations
from the Web, the largest available resource. To
this end substantial improvements are needed in or-
der to promote scalability relative to current Web-
based approaches. In particular, we address two
major goals: reducing dramatically the complexity
of required auxiliary inputs, thus enabling to apply
the methods at larger scales, and generalizing the
types of structures that can be acquired. The algo-
rithms described in this paper were applied for ac-
quiring entailment relations for verb-based expres-
sions. They successfully discovered several rela-
tions on average per each randomly selected expres-
sion.
2 Background and Motivations
This section provides a qualitative view of prior
work, emphasizing the perspective of aiming at a
full-scale paraphrase resource. As there are still
no standard benchmarks, current quantitative results
are not comparable in a consistent way.
The major idea in paraphrase acquisition is often
to find linguistic structures, here termed templates,
that share the same anchors. Anchors are lexical
elements describing the context of a sentence. Tem-
plates that are extracted from different sentences
and connect the same anchors in these sentences,
are assumed to paraphrase each other. For example,
the sentences ?Yahoo bought Overture? and ?Yahoo
acquired Overture? share the anchors {X=Yahoo,
Y =Overture}, suggesting that the templates ?X buy
Y? and ?X acquire Y? paraphrase each other. Algo-
rithms for paraphrase acquisition address two prob-
lems: (a) finding matching anchors and (b) identify-
ing template structure, as reviewed in the next two
subsections.
2.1 Finding Matching Anchors
The prominent approach for paraphrase learning
searches sentences that share common sets of mul-
tiple anchors, assuming they describe roughly the
same fact or event. To facilitate finding many
matching sentences, highly redundant comparable
corpora have been used. These include multiple
translations of the same text (Barzilay and McKe-
own, 2001) and corresponding articles from multi-
ple news sources (Shinyama et al, 2002; Pang et
al., 2003; Barzilay and Lee, 2003). While facilitat-
ing accuracy, we assume that comparable corpora
cannot be a sole resource due to their limited avail-
ability.
Avoiding a comparable corpus, (Glickman and
Dagan, 2003) developed statistical methods that
match verb paraphrases within a regular corpus.
Their limited scale results, obtaining several hun-
dred verb paraphrases from a 15 million word cor-
pus, suggest that much larger corpora are required.
Naturally, the largest available corpus is the Web.
Since exhaustive processing of the Web is not feasi-
ble, (Duclaye et al, 2002) and (Ravichandran and
Hovy, 2002) attempted bootstrapping approaches,
which resemble the mutual bootstrapping method
for Information Extraction of (Riloff and Jones,
1999). These methods start with a provided known
set of anchors for a target meaning. For example,
the known anchor set {Mozart, 1756} is given as in-
put in order to find paraphrases for the template ?X
born in Y?. Web searching is then used to find occur-
rences of the input anchor set, resulting in new tem-
plates that are supposed to specify the same relation
as the original one (?born in?). These new templates
are then exploited to get new anchor sets, which
are subsequently processed as the initial {Mozart,
1756}. Eventually, the overall procedure results in
an iterative process able to induce templates from
anchor sets and vice versa.
The limitation of this approach is the requirement
for one input anchor set per target meaning. Prepar-
ing such input for all possible meanings in broad
domains would be a huge task. As will be explained
below, our method avoids this limitation by find-
ing all anchor sets automatically in an unsupervised
manner.
Finally, (Lin and Pantel, 2001) present a notably
different approach that relies on matching sepa-
rately single anchors. They limit the allowed struc-
ture of templates only to paths in dependency parses
connecting two anchors. The algorithm constructs
for each possible template two feature vectors, rep-
resenting its co-occurrence statistics with the two
anchors. Two templates with similar vectors are
suggested as paraphrases (termed inference rule).
Matching of single anchors relies on the gen-
eral distributional similarity principle and unlike the
other methods does not require redundancy of sets
of multiple anchors. Consequently, a much larger
number of paraphrases can be found in a regular
corpus. Lin and Pantel report experiments for 9
templates, in which their system extracted 10 cor-
rect inference rules on average per input template,
from 1GB of news data. Yet, this method also suf-
fers from certain limitations: (a) it identifies only
templates with pre-specified structures; (b) accuracy
seems more limited, due to the weaker notion of
similarity; and (c) coverage is limited to the scope
of an available corpus.
To conclude, several approaches exhaustively
process different types of corpora, obtaining vary-
ing scales of output. On the other hand, the Web is
a huge promising resource, but current Web-based
methods suffer serious scalability constraints.
2.2 Identifying Template Structure
Paraphrasing approaches learn different kinds of
template structures. Interesting algorithms are pre-
sented in (Pang et al, 2003; Barzilay and Lee,
2003). They learn linear patterns within similar con-
texts represented as finite state automata. Three
classes of syntactic template learning approaches
are presented in the literature: learning of predicate
argument templates (Yangarber et al, 2000), learn-
ing of syntactic chains (Lin and Pantel, 2001) and
learning of sub-trees (Sudo et al, 2003). The last
approach is the most general with respect to the tem-
plate form. However, its processing time increases
exponentially with the size of the templates.
As a conclusion, state of the art approaches still
learn templates of limited form and size, thus re-
stricting generality of the learning process.
3 The TE/ASE Acquisition Method
Motivated by prior experience, we identify two ma-
jor goals for scaling Web-based acquisition of en-
tailment relations: (a) Covering the broadest pos-
sible range of meanings, while requiring minimal
input and (b) Keeping template structures as gen-
eral as possible. To address the first goal we re-
quire as input only a phrasal lexicon of the rel-
evant domain (including single words and multi-
word expressions). Broad coverage lexicons are
widely available or may be constructed using known
term acquisition techniques, making it a feasible
and scalable input requirement. We then aim to
acquire entailment relations that include any of the
lexicon?s entries. The second goal is addressed by a
novel algorithm for extracting the most general tem-
plates being justified by the data.
For each lexicon entry, denoted a pivot, our
extraction method performs two phases: (a) ex-
tract promising anchor sets for that pivot (ASE,
Section 3.1), and (b) from sentences contain-
ing the anchor sets, extract templates for which
an entailment relation holds with the pivot (TE,
Section 3.2). Examples for verb pivots are:
?acquire?, ?fall to?, ?prevent? . We will use the pivot
?prevent? for examples through this section.
Before presenting the acquisition method we first
define its output. A template is a dependency parse-
tree fragment, with variable slots at some tree nodes
(e.g. ?X subj? prevent obj? Y? ). An entailment rela-
tion between two templates T1 and T2 holds if
the meaning of T2 can be inferred from the mean-
ing of T1 (or vice versa) in some contexts, but
not necessarily all, under the same variable instan-
tiation. For example, ?X subj? prevent obj? Y? entails
?X
subj
? reduce
obj
? Y risk? because the sentence ?as-
pirin reduces heart attack risk? can be inferred from
?aspirin prevents a first heart attack?. Our output
consists of pairs of templates for which an entail-
ment relation holds.
3.1 Anchor Set Extraction (ASE)
The goal of this phase is to find a substantial num-
ber of promising anchor sets for each pivot. A good
anchor-set should satisfy a proper balance between
specificity and generality. On one hand, an anchor
set should correspond to a sufficiently specific set-
ting, so that entailment would hold between its dif-
ferent occurrences. On the other hand, it should be
sufficiently frequent to appear with different entail-
ing templates.
Finding good anchor sets based on just the input
pivot is a hard task. Most methods identify good re-
peated anchors ?in retrospect?, that is after process-
ing a full corpus, while previous Web-based meth-
ods require at least one good anchor set as input.
Given our minimal input, we needed refined crite-
ria that identify a priori the relatively few promising
anchor sets within a sample of pivot occurrences.
ASE ALGORITHM STEPS:
For each pivot (a lexicon entry)
1. Create a pivot template, Tp
2. Construct a parsed sample corpus S for Tp:
(a) Retrieve an initial sample from the Web
(b) Identify associated phrases for the pivot
(c) Extend S using the associated phrases
3. Extract candidate anchor sets from S:
(a) Extract slot anchors
(b) Extract context anchors
4. Filter the candidate anchor sets:
(a) by absolute frequency
(b) by conditional pivot probability
Figure 1: Outline of the ASE algorithm.
The ASE algorithm (presented in Figure 1) per-
forms 4 main steps.
STEP (1) creates a complete template, called the
pivot template and denoted Tp, for the input pivot,
denoted P . Variable slots are added for the ma-
jor types of syntactic relations that interact with P ,
based on its syntactic type. These slots enable us to
later match Tp with other templates. For verbs, we
add slots for a subject and for an object or a modifier
(e.g. ?X subj? prevent obj? Y? ).
STEP (2) constructs a sample corpus, denoted S,
for the pivot template. STEP (2.A) utilizes a Web
search engine to initialize S by retrieving sentences
containing P . The sentences are parsed by the
MINIPAR dependency parser (Lin, 1998), keeping
only sentences that contain the complete syntactic
template Tp (with all the variables instantiated).
STEP (2.B) identifies phrases that are statistically
associated with Tp in S. We test all noun-phrases
in S , discarding phrases that are too common on
the Web (absolute frequency higher than a thresh-
old MAXPHRASEF), such as ?desire?. Then we se-
lect the N phrases with highest tf ?idf score1. These
phrases have a strong collocation relationship with
the pivot P and are likely to indicate topical (rather
than anecdotal) occurrences of P . For example, the
phrases ?patient? and ?American Dental Associa-
tion?, which indicate contexts of preventing health
problems, were selected for the pivot ?prevent?. Fi-
1Here, tf ?idf = freqS(X) ? log
(
N
freqW (X)
)
where freqS(X) is the number of occurrences in S containing
X , N is the total number of Web documents, and freqW (X)
is the number of Web documents containing X .
nally, STEP (2.C) expands S by querying the Web
with the both P and each of the associated phrases,
adding the retrieved sentences to S as in step (2.a).
STEP (3) extracts candidate anchor sets for Tp.
From each sentence in S we try to generate one can-
didate set, containing noun phrases whose Web fre-
quency is lower than MAXPHRASEF. STEP (3.A)
extracts slot anchors ? phrases that instantiate the
slot variables of Tp. Each anchor is marked
with the corresponding slot. For example, the
anchors {antibioticssubj? , miscarriage obj?} were ex-
tracted from the sentence ?antibiotics in pregnancy
prevent miscarriage?.
STEP (3.B) tries to extend each candidate set with
one additional context anchor, in order to improve
its specificity. This anchor is chosen as the highest
tf ?idf scoring phrase in the sentence, if it exists. In
the previous example, ?pregnancy? is selected.
STEP (4) filters out bad candidate anchor sets by
two different criteria. STEP (4.A) maintains only
candidates with absolute Web frequency within a
threshold range [MINSETF, MAXSETF], to guaran-
tee an appropriate specificity-generality level. STEP
(4.B) guarantees sufficient (directional) association
between the candidate anchor set c and Tp, by esti-
mating
Prob(Tp|c) ?
freqW (P ? c)
freqW (c)
where freqW is Web frequency and P is the pivot.
We maintain only candidates for which this prob-
ability falls within a threshold range [SETMINP,
SETMAXP]. Higher probability often corresponds
to a strong linguistic collocation between the
candidate and Tp, without any semantic entail-
ment. Lower probability indicates coincidental co-
occurrence, without a consistent semantic relation.
The remaining candidates in S become the in-
put anchor-sets for the template extraction phase,
for example, {Aspirinsubj? , heart attackobj?} for ?pre-
vent?.
3.2 Template Extraction (TE)
The Template Extraction algorithm accepts as its in-
put a list of anchor sets extracted from ASE for each
pivot template. Then, TE generates a set of syntactic
templates which are supposed to maintain an entail-
ment relationship with the initial pivot template. TE
performs three main steps, described in the follow-
ing subsections:
1. Acquisition of a sample corpus from the Web.
2. Extraction of maximal most general templates
from that corpus.
3. Post-processing and final ranking of extracted
templates.
3.2.1 Acquisition of a sample corpus from the
Web
For each input anchor set, TE acquires from the
Web a sample corpus of sentences containing it.
For example, a sentence from the sample corpus
for {aspirin, heart attack} is: ?Aspirin stops heart
attack??. All of the sample sentences are then
parsed with MINIPAR (Lin, 1998), which gener-
ates from each sentence a syntactic directed acyclic
graph (DAG) representing the dependency structure
of the sentence. Each vertex in this graph is labeled
with a word and some morphological information;
each graph edge is labeled with the syntactic rela-
tion between the words it connects.
TE then substitutes each slot anchor (see section
3.1) in the parse graphs with its corresponding slot
variable. Therefore, ?Aspirin stops heart attack??
will be transformed into ?X stop Y?. This way all
the anchors for a certain slot are unified under the
same variable name in all sentences. The parsed
sentences related to all of the anchor sets are sub-
sequently merged into a single set of parse graphs
S = {P1, P2, . . . , Pn} (see P1 and P2 in Figure 2).
3.2.2 Extraction of maximal most general
templates
The core of TE is a General Structure Learning al-
gorithm (GSL ) that is applied to the set of parse
graphs S resulting from the previous step. GSL
extracts single-rooted syntactic DAGs, which are
named spanning templates since they must span at
least over Na slot variables, and should also ap-
pear in at least Nr sentences from S (In our exper-
iments we set Na=2 and Nr=2). GSL learns maxi-
mal most general templates: they are spanning tem-
plates which, at the same time, (a) cannot be gener-
alized by further reduction and (b) cannot be further
extended keeping the same generality level.
In order to properly define the notion of maximal
most general templates, we introduce some formal
definitions and notations.
DEFINITION: For a spanning template t we define
a sentence set, denoted with ?(t), as the set of all
parsed sentences in S containing t.
For each pair of templates t1 and t2, we use the no-
tation t1  t2 to denote that t1 is included as a sub-
graph or is equal to t2. We use the notation t1 ? t2
when such inclusion holds strictly. We define T (S)
as the set of all spanning templates in the sample S.
DEFINITION: A spanning template t ? T (S) is
maximal most general if and only if both of the fol-
lowing conditions hold:
CONDITION A: For ?t? ? T (S), t?  t, it holds that
?(t) = ?(t?).
CONDITION B: For ?t? ? T (S), t ? t?, it holds that
?(t) ? ?(t?).
Condition A ensures that the extracted templates do
not contain spanning sub-structures that are more
?general? (i.e. having a larger sentence set); con-
dition B ensures that the template cannot be further
enlarged without reducing its sentence set.
GSL performs template extraction in two main
steps: (1) build a compact graph representation of
all the parse graphs from S; (2) extract templates
from the compact representation.
A compact graph representation is an aggregate
graph which joins all the sentence graphs from S
ensuring that all identical spanning sub-structures
from different sentences are merged into a single
one. Therefore, each vertex v (respectively, edge
e) in the aggregate graph is either a copy of a cor-
responding vertex (edge) from a sentence graph Pi
or it represents the merging of several identically
labeled vertices (edges) from different sentences in
S. The set of such sentences is defined as the sen-
tence set of v (e), and is represented through the set
of index numbers of related sentences (e.g. ?(1,2)?
in the third tree of Figure 2). We will denote with
Gi the compact graph representation of the first i
sentences in S. The parse trees P1 and P2 of two
sentences and their related compact representation
G2 are shown in Figure 2.
Building the compact graph representation
The compact graph representation is built incremen-
tally. The algorithm starts with an empty aggregate
graph G0 and then merges the sentence graphs from
S one at a time into the aggregate structure.
Let?s denote the current aggregate graph with
Gi?1(Vg, Eg) and let Pi(Vp, Ep) be the parse graph
which will be merged next. Note that the sentence
set of Pi is a single element set {i}.
During each iteration a new graph is created as
the union of both input graphs: Gi = Gi?1 ? Pi.
Then, the following merging procedure is per-
formed on the elements of Gi
1. ADDING GENERALIZED VERTICES TO Gi.
For every two vertices vg ? Vg, vp ? Vp having
equal labels, a new generalized vertex vnewg is cre-
ated and added to Gi. The new vertex takes the same
label and holds a sentence set which is formed from
the sentence set of vg by adding i to it. Still with
reference to Figure 2, the generalized vertices in G2
are ?X?, ?Y? and ?stop?. The algorithm connects the
generalized vertex vnewg with all the vertices which
are connected with vg and vp.
2. MERGING EDGES. If two edges eg ? Eg and
ep ? Ep have equal labels and their corresponding
adjacent vertices have been merged, then ea and ep
are also merged into a new edge. In Figure 2 the
edges (?stop?, ?X? ) and (?stop?, ?Y? ) from P1 and
P2 are eventually merged into G2.
3. DELETING MERGED VERTICES. Every vertex
v from Vp or Vg for which at least one generalized
vertex vnewg exists is deleted from Gi.
As an optimization step, we merge only vertices
and edges that are included in equal spanning tem-
plates.
Extracting the templates
GSL extracts all maximal most general templates
from the final compact representation Gn using the
following sub-algorithm:
1. BUILDING MINIMAL SPANNING TREES. For
every Na different slot variables in Gn having a
common ancestor, a minimal spanning tree st is
built. Its sentence set is computed as the intersec-
tion of the sentence sets of its edges and vertices.
2. EXPANDING THE SPANNING TREES. Every
minimal spanning tree st is expanded to the maxi-
mal sub-graph maxst whose sentence set is equal to
?(st). All maximal single-rooted DAGs in maxst
are extracted as candidate templates. Maximality
ensures that the extracted templates cannot be ex-
panded further while keeping the same sentence set,
satisfying condition B.
3. FILTERING. Candidates which contain an-
other candidate with a larger sentence set are filtered
out. This step guarantees condition A.
In Figure 2 the maximal most general template in
G2 is ?X
subj
? stop
obj
? Y? .
3.2.3 Post-processing and ranking of extracted
templates
As a last step, names and numbers are filtered out
from the templates. Moreover, TE removes those
templates which are very long or which appear with
just one anchor set and in less than four sentences.
Finally, the templates are sorted first by the number
of anchor sets with which each template appeared,
and then by the number of sentences in which they
appeared.
4 Evaluation
We evaluated the results of the TE/ASE algorithm
on a random lexicon of verbal forms and then as-
sessed its performance on the extracted data through
human-based judgments.
P1 : stop
subj
z
z
z
||zz
z
z
obj
A
A
A
  A
AA
A
P2 : stop
subj
z
z
z
||zz
z
z
obj

by
J
J
J
J
%%J
J
J
J
G2 : stop(1, 2)
subj(1,2)
rr
rr
xxrr
rr
obj(1,2)

by(2)
OO
OO
''O
OO
O
X Y X Y absorbing X(1, 2) Y (1, 2) absorbing(2)
Figure 2: Two parse trees and their compact representation (sentence sets are shown in parentheses).
4.1 Experimental Setting
The test set for human evaluation was generated by
picking out 53 random verbs from the 1000 most
frequent ones found in a subset of the Reuters cor-
pus2. For each verb entry in the lexicon, we pro-
vided the judges with the corresponding pivot tem-
plate and the list of related candidate entailment
templates found by the system. The judges were
asked to evaluate entailment for a total of 752 tem-
plates, extracted for 53 pivot lexicon entries; Table
1 shows a sample of the evaluated templates; all of
them are clearly good and were judged as correct
ones.
Pivot Template Entailment Templates
X prevent Y X provides protection against Y
X reduces Y
X decreases the risk of Y
X be cure for Y
X a day keeps Y away
X to combat Y
X accuse Y X call Y indictable
X testifies against Y
Y defense before X
X acquire Y X snap up Y
Y shareholders approve X
buyout
Y shareholders receive shares
of X stock
X go back to Y Y allowed X to return
Table 1: Sample of templates found by TE/ASE and
included in the evaluation test set.
Concerning the ASE algorithm, threshold pa-
rameters3 were set as PHRASEMAXF=107, SET-
MINF=102, SETMAXF=105, SETMINP=0.066,
and SETMAXP=0.666. An upper limit of 30 was
imposed on the number of possible anchor sets used
for each pivot. Since this last value turned out to
be very conservative with respect to system cover-
2Known as Reuters Corpus, Volume 1, English Language,
1996-08-20 to 1997-08-19.
3All parameters were tuned on a disjoint development lexi-
con before the actual experiment.
age, we subsequently attempted to relax it to 50 (see
Discussion in Section 4.3).
Further post-processing was necessary over ex-
tracted data in order to remove syntactic variations
referring to the same candidate template (typically
passive/active variations).
Three possible judgment categories have been
considered: Correct if an entailment relationship
in at least one direction holds between the judged
template and the pivot template in some non-bizarre
context; Incorrect if there is no reasonable context
and variable instantiation in which entailment holds;
No Evaluation if the judge cannot come to a definite
conclusion.
4.2 Results
Each of the three assessors (referred to as J#1, J#2,
and J#3) issued judgments for the 752 different
templates. Correct templates resulted to be 283,
313, and 295 with respect to the three judges. No
evaluation?s were 2, 0, and 16, while the remaining
templates were judged Incorrect.
For each verb, we calculate Yield as the absolute
number of Correct templates found and Precision as
the percentage of good templates out of all extracted
templates. Obtained Precision is 44.15%, averaged
over the 53 verbs and the 3 judges. Considering Low
Majority on judges, the precision value is 42.39%.
Average Yield was 5.5 templates per verb.
These figures may be compared (informally, as
data is incomparable) with average yield of 10.1
and average precision of 50.3% for the 9 ?pivot?
templates of (Lin and Pantel, 2001). The compar-
ison suggests that it is possible to obtain from the
(very noisy) web a similar range of precision as was
obtained from a clean news corpus. It also indi-
cates that there is potential for acquiring additional
templates per pivot, which would require further re-
search on broadening efficiently the search for addi-
tional web data per pivot.
Agreement among judges is measured by the
Kappa value, which is 0.55 between J#1 and J#2,
0.57 between J#2 and J#3, and 0.63 between J#1
and J#3. Such Kappa values correspond to moder-
ate agreement for the first two pairs and substantial
agreement for the third one. In general, unanimous
agreement among all of the three judges has been
reported on 519 out of 752 templates, which corre-
sponds to 69%.
4.3 Discussion
Our algorithm obtained encouraging results, ex-
tracting a considerable amount of interesting tem-
plates and showing inherent capability of discover-
ing complex semantic relations.
Concerning overall coverage, we managed to find
correct templates for 86% of the verbs (46 out of
53). Nonetheless, presented results show a substan-
tial margin of possible improvement. In fact yield
values (5.5 Low Majority, up to 24 in best cases),
which are our first concern, are inherently depen-
dent on the breadth of Web search performed by
the ASE algorithm. Due to computational time, the
maximal number of anchor sets processed for each
verb was held back to 30, significantly reducing the
amount of retrieved data.
In order to further investigate ASE potential, we
subsequently performed some extended experiment
trials raising the number of anchor sets per pivot
to 50. This time we randomly chose a subset of
10 verbs out of the less frequent ones in the origi-
nal main experiment. Results for these verbs in the
main experiment were an average Yield of 3 and an
average Precision of 45.19%. In contrast, the ex-
tended experiments on these verbs achieved a 6.5
Yield and 59.95% Precision (average values). These
results are indeed promising, and the substantial
growth in Yield clearly indicates that the TE/ASE
algorithms can be further improved. We thus sug-
gest that the feasibility of our approach displays the
inherent scalability of the TE/ASE process, and its
potential to acquire a large entailment relation KB
using a full scale lexicon.
A further improvement direction relates to tem-
plate ranking and filtering. While in this paper
we considered anchor sets to have equal weights,
we are also carrying out experiments with weights
based on cross-correlation between anchor sets.
5 Conclusions
We have described a scalable Web-based approach
for entailment relation acquisition which requires
only a standard phrasal lexicon as input. This min-
imal level of input is much simpler than required
by earlier web-based approaches, while succeeding
to maintain good performance. This result shows
that it is possible to identify useful anchor sets in
a fully unsupervised manner. The acquired tem-
plates demonstrate a broad range of semantic rela-
tions varying from synonymy to more complicated
entailment. These templates go beyond trivial para-
phrases, demonstrating the generality and viability
of the presented approach.
From our current experiments we can expect to
learn about 5 relations per lexicon entry, at least for
the more frequent entries. Moreover, looking at the
extended test, we can extrapolate a notably larger
yield by broadening the search space. Together with
the fact that we expect to find entailment relations
for about 85% of a lexicon, it is a significant step
towards scalability, indicating that we will be able
to extract a large scale KB for a large scale lexicon.
In future work we aim to improve the yield by in-
creasing the size of the sample-corpus in a qualita-
tive way, as well as precision, using statistical meth-
ods such as supervised learning for better anchor set
identification and cross-correlation between differ-
ent pivots. We also plan to support noun phrases
as input, in addition to verb phrases. Finally, we
would like to extend the learning task to discover the
correct entailment direction between acquired tem-
plates, completing the knowledge required by prac-
tical applications.
Like (Lin and Pantel, 2001), learning the context
for which entailment relations are valid is beyond
the scope of this paper. As stated, we learn entail-
ment relations holding for some, but not necessarily
all, contexts. In future work we also plan to find the
valid contexts for entailment relations.
Acknowledgements
The authors would like to thank Oren Glickman
(Bar Ilan University) for helpful discussions and as-
sistance in the evaluation, Bernardo Magnini for his
scientific supervision at ITC-irst, Alessandro Vallin
and Danilo Giampiccolo (ITC-irst) for their help in
developing the human based evaluation, and Prof.
Yossi Matias (Tel-Aviv University) for supervising
the first author. This work was partially supported
by the MOREWEB project, financed by Provincia
Autonoma di Trento. It was also partly carried out
within the framework of the ITC-IRST (TRENTO,
ITALY) ? UNIVERSITY OF HAIFA (ISRAEL) col-
laboration project. For data visualization and analy-
sis the authors intensively used the CLARK system
(www.bultreebank.org) developed at the Bulgarian
Academy of Sciences .
References
Regina Barzilay and Lillian Lee. 2003. Learning
to paraphrase: An unsupervised approach us-
ing multiple-sequence alignment. In Proceedings
of HLT-NAACL 2003, pages 16?23, Edmonton,
Canada.
Regina Barzilay and Kathleen R. McKeown. 2001.
Extracting paraphrases from a parallel corpus. In
Proceedings of ACL 2001, pages 50?57, Toulose,
France.
Ido Dagan and Oren Glickman. 2004. Probabilis-
tic textual entailment: Generic applied modeling
of language variability. In PASCAL Workshop on
Learning Methods for Text Understanding and
Mining, Grenoble.
Florence Duclaye, Franc?ois Yvon, and Olivier
Collin. 2002. Using the Web as a linguistic re-
source for learning reformulations automatically.
In Proceedings of LREC 2002, pages 390?396,
Las Palmas, Spain.
Oren Glickman and Ido Dagan. 2003. Identifying
lexical paraphrases from a single corpus: a case
study for verbs. In Proceedings of RANLP 2003.
Ulf Hermjakob, Abdessamad Echihabi, and Daniel
Marcu. 2003. Natural language based reformula-
tion resource and Web Exploitation. In Ellen M.
Voorhees and Lori P. Buckland, editors, Proceed-
ings of the 11th Text Retrieval Conference (TREC
2002), Gaithersburg, MD. NIST.
Christian Jacquemin. 1999. Syntagmatic and
paradigmatic representations of term variation.
In Proceedings of ACL 1999, pages 341?348.
Dekang Lin and Patrick Pantel. 2001. Discovery of
inference rules for Question Answering. Natural
Language Engineering, 7(4):343?360.
Dekang Lin. 1998. Dependency-based evaluation
of MINIPAR. In Proceedings of the Workshop
on Evaluation of Parsing Systems at LREC 1998,
Granada, Spain.
Dan Moldovan and Vasile Rus. 2001. Logic form
transformation of WordNet and its applicability
to Question Answering. In Proceedings of ACL
2001, pages 394?401, Toulose, France.
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based alignment of multiple translations:
Extracting paraphrases and generating new sen-
tences. In Proceedings of HLT-NAACL 2003, Ed-
monton, Canada.
Deepak Ravichandran and Eduard Hovy. 2002.
Learning surface text patterns for a Question An-
swering system. In Proceedings of ACL 2002,
Philadelphia, PA.
Ellen Riloff and Rosie Jones. 1999. Learning dic-
tionaries for Information Extraction by multi-
level bootstrapping. In Proceedings of the Six-
teenth National Conference on Artificial Intelli-
gence (AAAI-99), pages 474?479.
Yusuke Shinyama, Satoshi Sekine, Kiyoshi Sudo,
and Ralph Grishman. 2002. Automatic para-
phrase acquisition from news articles. In Pro-
ceedings of Human Language Technology Con-
ference (HLT 2002), San Diego, USA.
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.
2003. An improved extraction pattern represen-
tation model for automatic IE pattern acquisition.
In Proceedings of ACL 2003.
Roman Yangarber, Ralph Grishman, Pasi
Tapanainen, and Silja Huttunen. 2000. Un-
supervised discovery of scenario-level patterns
for Information Extraction. In Proceedings of
COLING 2000.
Proceedings of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment, pages 43?48,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
A Probabilistic Setting and Lexical Cooccurrence Model  
for Textual Entailment  
Oren Glickman and Ido Dagan 
Department of Computer Science 
 Bar Ilan University 
{glikmao,Dagan}@cs.biu.ac.il 
 
Abstract 
This paper proposes a general probabilis-
tic setting that formalizes a probabilistic 
notion of textual entailment.  We further 
describe a particular preliminary model 
for lexical-level entailment, based on 
document cooccurrence probabilities, 
which follows the general setting. The 
model was evaluated on two application 
independent datasets, suggesting the rele-
vance of such probabilistic approaches for 
entailment modeling.  
1 Introduction 
Many Natural Language Processing (NLP) 
applications need to recognize when the meaning 
of one text can be expressed by, or inferred from, 
another text. Information Retrieval (IR), Question 
Answering (QA), Information Extraction (IE), text 
summarization and Machine Translation (MT) 
evaluation are examples of applications that need 
to assess this semantic relationship between text 
segments. The Textual Entailment Recognition 
task (Dagan et al, 2005) has recently been pro-
posed as an application independent framework for 
modeling such inferences.  
Within the textual entailment framework, a text 
t is said to entail a textual hypothesis h if the truth 
of h can be inferred from t. Textual entailment cap-
tures generically a broad range of inferences that 
are relevant for multiple applications. For example, 
a QA system has to identify texts that entail a hy-
pothesized answer. Given the question "Does John 
Speak French?", a text that includes the sentence 
"John is a fluent French speaker" entails the sug-
gested answer "John speaks French." In many 
cases, though, entailment inference is uncertain 
and has a probabilistic nature. For example, a text 
that includes the sentence "John was born in 
France." does not strictly entail the above answer. 
Yet, it is clear that it does increase substantially the 
likelihood that the hypothesized answer is true.  
The uncertain nature of textual entailment calls 
for its explicit modeling in probabilistic terms. We 
therefore propose a general generative probabilistic 
setting for textual entailment, which allows a clear 
formulation of concrete probabilistic models for 
this task. We suggest that the proposed setting may 
provide a unifying framework for modeling uncer-
tain semantic inferences from texts.   
An important sub task of textual entailment, 
which we term lexical entailment, is recognizing if 
the lexical concepts in a hypothesis h are entailed 
from a given text t, even if the relations which hold 
between these concepts may not be entailed from t. 
This is typically a necessary, but not sufficient, 
condition for textual entailment. For example, in 
order to infer from a text the hypothesis "Chrysler 
stock rose," it is a necessary that the concepts of 
Chrysler, stock and rise must be inferred from the 
text. However, for proper entailment it is further 
needed that the right relations hold between these 
concepts. In this paper we demonstrate the rele-
vance of the general probabilistic setting for mod-
eling lexical entailment, by devising a preliminary 
model that is based on document co-occurrence 
probabilities in a bag of words representation.  
Although our proposed lexical system is rela-
tively simple, as it doesn?t rely on syntactic or 
other deeper analysis, it nevertheless was among 
the top ranking systems in the first Recognising 
Textual Entailment (RTE) Challenge (Glickman et 
al., 2005a). The model was evaluated also on an 
additional dataset, where it compares favorably 
with a state-of-the-art heuristic score. These results 
suggest that the proposed probabilistic framework 
is a promising basis for devising improved models 
that incorporate richer information.  
43
2 Probabilistic Textual Entailment 
2.1 Motivation 
A common definition of entailment in formal se-
mantics (Chierchia. and McConnell-Ginet, 1990) 
specifies that a text t entails another text h (hy-
pothesis, in our terminology) if h is true in every 
circumstance (possible world) in which t is true. 
For example, in examples 1 and 3 from Table 1 
we?d assume humans to agree that the hypothesis 
is necessarily true in any circumstance for which 
the text is true. In such intuitive cases, textual en-
tailment may be perceived as being certain, or, tak-
ing a probabilistic perspective, as having a 
probability of 1. 
In many other cases, though, entailment infer-
ence is uncertain and has a probabilistic nature. In 
example 2, the text doesn?t contain enough infor-
mation to infer the hypothesis? truth. And in exam-
ple 4, the meaning of the word hometown is 
ambiguous and therefore one cannot infer for cer-
tain that the hypothesis is true. In both of these 
cases there are conceivable circumstances for 
which the text is true and the hypothesis false. Yet, 
it is clear that in both examples, the text does in-
crease substantially the likelihood of the correct-
ness of the hypothesis, which naturally extends the 
classical notion of certain entailment. Given the 
text, we expect the probability that the hypothesis 
is indeed true to be relatively high, and signifi-
cantly higher than its probability of being true 
without reading the text. Aiming to model applica-
tion needs, we suggest that the probability of the 
hypothesis being true given the text reflects an ap-
propriate confidence score for the correctness of a 
particular textual inference. In the next sub-
sections we propose a concrete probabilistic setting 
that formalizes the notion of truth probabilities in 
such cases.  
2.2 A Probabilistic Setting 
Let T denote a space of possible texts, and t?T a 
specific text. Let H denote the set of all possible 
hypotheses. A hypothesis h?H is a propositional 
statement which can be assigned a truth value. For 
now it is assumed that h is represented as a textual 
statement, but in principle it could also be ex-
pressed as a formula in some propositional lan-
guage.  
A semantic state of affairs is captured by a 
mapping from H to {0=false, 1=true}, denoted by 
w: H ? {0, 1} (called here possible world, follow-
ing common terminology). A possible world w 
represents a concrete set of truth value assignments 
for all possible propositions. Accordingly, W de-
notes the set of all possible worlds. 
2.2.1 A Generative Model 
We assume a probabilistic generative model for 
texts and possible worlds. In particular, we assume 
that texts are generated along with a concrete state 
of affairs, represented by a possible world. Thus, 
whenever the source generates a text t, it generates 
also corresponding hidden truth assignments that 
constitute a possible world w. 
The probability distribution of the source, over 
all possible texts and truth assignments T ? W, is 
assumed to reflect inferences that are based on the 
generated texts. That is, we assume that the distri-
bution of truth assignments is not bound to reflect 
the state of affairs in a particular "real" world, but 
only the inferences about propositions' truth which 
are related to the text. In particular, the probability 
for generating a true hypothesis h that is not related 
at all to the corresponding text is determined by 
some prior probability P(h). For example, h="Paris 
is the capital of France" might have a prior smaller 
than 1 and might well be false when the generated 
text is not related at all to Paris or France. In fact, 
we may as well assume that the notion of textual 
entailment is relevant only for hypotheses for 
which P(h) < 1, as otherwise (i.e. for tautologies) 
there is no need to consider texts that would sup-
port h's truth. On the other hand, we assume that 
the probability of h being true (generated within w) 
would be higher than the prior when the corre-
sponding t does contribute information that sup-
ports h's truth. 
example text hypothesis 
1 John is a French Speaker 
2 John was born in France John speaks French  
3 Harry's birthplace is Iowa 
4 Harry is returning to his Iowa hometown  Harry was born in Iowa 
Table 1: example sentence pairs  
 
44
We define two types of events over the prob-
ability space for T ? W: 
I) For a hypothesis h, we denote as Trh the random 
variable whose value is the truth value assigned to 
h in a given world. Correspondingly, Trh=1 is the 
event of h being assigned a truth value of 1 (true). 
II) For a text t, we use t itself to denote also the 
event that the generated text is t (as usual, it is 
clear from the context whether t denotes the text or 
the corresponding event).  
2.3 Probabilistic textual entailment 
definition 
We say that a text t probabilistically entails a hy-
pothesis h (denoted as t ? h) if t increases the like-
lihood of h being true, that is, if P(Trh = 1| t) > 
P(Trh  = 1) or equivalently if the pointwise mutual 
information, I(Trh=1,t), is greater then 0. Once 
knowing that t?h, P(Trh=1| t) serves as a probabil-
istic confidence value for h being true given t. 
Application settings would typically require 
that P(Trh = 1| t) obtains a high value; otherwise, 
the text would not be considered sufficiently rele-
vant to support h's truth (e.g. a supporting text in 
QA or IE should entail the extracted information 
with high confidence). Finally, we ignore here the 
case in which t contributes negative information 
about h, leaving this relevant case for further in-
vestigation. 
2.4 Model Properties 
It is interesting to notice the following properties 
and implications of our model: 
A) Textual entailment is defined as a relationship 
between texts and propositions whose representa-
tion is typically based on text as well, unlike logi-
cal entailment which is a relationship between 
propositions only. Accordingly, textual entail-
ment confidence is conditioned on the actual gen-
eration of a text, rather than its truth. For 
illustration, we would expect that the text ?His 
father was born in Italy? would logically entail 
the hypothesis ?He was born in Italy? with high 
probability ? since most people who?s father was 
born in Italy were also born there. However we 
expect that the text would actually not probabilis-
tically textually entail the hypothesis since most 
people for whom it is specifically reported that 
their father was born in Italy were not born in 
Italy.1 
B) We assign probabilities to propositions (hy-
potheses) in a similar manner to certain probabil-
istic reasoning approaches (e.g. Bacchus, 1990; 
Halpern, 1990). However, we also assume a gen-
erative model of text, similar to probabilistic lan-
guage and machine translation models, which 
supplies the needed conditional probability distri-
bution. Furthermore, since our conditioning is on 
texts rather than propositions we do not assume 
any specific logic representation language for text 
meaning, and only assume that textual hypotheses 
can be assigned truth values.     
C) Our framework does not distinguish between 
textual entailment inferences that are based on 
knowledge of language semantics (such as mur-
dering ? killing) and inferences based on domain 
or world knowledge (such as live in Paris ? live 
in France). Both are needed in applications and it 
is not clear at this stage where and how to put 
such a borderline. 
D) An important feature of the proposed frame-
work is that for a given text many hypotheses are 
likely to be true. Consequently, for a given text t 
and hypothesis h, ?hP(Trh=1|t) does not sum to 1.  
This differs from typical generative settings for 
IR and MT (Ponte and croft, 1998; Brown et al, 
1993), where all conditioned events are disjoint 
by construction.  In the proposed model, it is 
rather the case that P(Trh=1|t) + P(Trh=0|t) = 1, as 
we are interested in the probability that a single 
particular hypothesis is true (or false). 
E) An implemented model that corresponds to our 
probabilistic setting is expected to produce an 
estimate for P(Trh = 1| t). This estimate is ex-
pected to reflect all probabilistic aspects involved 
in the modeling, including inherent uncertainty of 
the entailment inference itself (as in example 2 of 
Table 1), possible uncertainty  regarding the cor-
rect disambiguation of the text (example 4), as 
well as probabilistic estimates that stem from the 
particular model structure.  
3 A Lexical Entailment Model 
We suggest that the proposed setting above pro-
vides the necessary grounding for probabilistic 
                                                          
1
 This seems to be the case, when analyzing the results of en-
tering the above text in a web search engine.     
45
modeling of textual entailment. Since modeling the 
full extent of the textual entailment problem is 
clearly a long term research goal, in this paper we 
rather focus on the above mentioned sub-task of 
lexical entailment - identifying when the lexical 
elements of a textual hypothesis h are inferred 
from a given text t.  
To model lexical entailment we first assume that 
the meanings of the individual content words in a 
hypothesis can be assigned truth values. One pos-
sible interpretation for such truth values is that 
lexical concepts are assigned existential meanings. 
For example, for a given text t, Trbook=1 if it can be 
inferred in t?s state of affairs that a book exists. 
Our model does not depend on any such particular 
interpretation, though, as we only assume that truth 
values can be assigned for lexical items but do not 
explicitly annotate or evaluate this sub-task.  
Given this setting, a hypothesis is assumed to be 
true if and only if all its lexical components are 
true as well. This captures our target perspective of 
lexical entailment, while not modeling here other 
entailment aspects. When estimating the entailment 
probability we assume that the truth probability of 
a term u in a hypothesis h is independent of the 
truth of the other terms in h, obtaining:  
P(Trh = 1| t) = ?u?hP(Tru=1|t) 
P(Trh = 1) = ?u?hP(Tru=1) (1) 
In order to estimate P(Tru=1|v1, ?, vn) for a 
given word u and text t={v1, ?, vn}, we further 
assume that the majority of the probability mass 
comes from a specific entailing word in t: 
)|1(max)|1( vutvu TTrtTr =?==? ?  (2) 
where Tv denotes the event that a generated text 
contains the word v. This corresponds to expecting 
that each word in h will be entailed from a specific 
word in t (rather than from the accumulative con-
text of t as a whole2). Alternatively, one can view 
(2) as inducing an alignment between terms in the 
h to the terms in the t, somewhat similar to align-
ment models in statistical MT (Brown et al, 1993).  
Thus we propose estimating the entailment 
probability based on lexical entailment probabili-
ties from (1) and (2) as follows: 
?? ? =?==? hu vutvh TTrtTr )|1(max)|1(  (3) 
                                                          
2
 Such a model is proposed in (Glickman et al, 2005b)  
3.1 Estimating Lexical Entailment 
Probabilities   
We perform unsupervised empirical estimation of 
the lexical entailment probabilities, P(Tru=1|Tv), 
based on word co-occurrence frequencies in a cor-
pus. Following our proposed probabilistic model 
(cf. Section  2.2.1), we assume that the domain 
corpus is a sample generated by a language source. 
Each document represents a generated text and a 
(hidden) possible world. Given that the possible 
world of the text is not observed we do not know 
the truth assignments of hypotheses for the ob-
served texts. We therefore further make the sim-
plest assumption that all hypotheses stated 
verbatim in a document are true and all others are 
false and hence P(Tru=1|Tv) = P(Tu |Tv). This simple 
co-occurrence probability, which we denote as 
lexical entailment probability ? lep(u,v), is easily 
estimated from the corpus based on maximum like-
lihood counts:  
v
vu
vu
n
n
TTrvulep ,)|1(),( ?=?=
 
(4) 
where nv is the number of documents containing 
word v and nu,v is the number of documents con-
taining both u and v.  
Given our definition of the textual entailment 
relationship (cf. Section  2.3) for a given word v we 
only consider for entailment words u for which 
P(Tru=1|Tv)> P(Tru=1) or based on our estimations, 
for which nu,v/nu > nv/N (N is total number of 
documents in the corpus).  
We denote as tep the textual entailment probability 
estimation as derived from (3) and (4) above: 
? ? ?= hu tv vulephttep ),(max),(  (5) 
3.2 Baseline model 
As a baseline model for comparison, we use a 
score developed within the context of text summa-
rization. (Monz and de Rijke, 2001) propose mod-
eling the directional entailment between two texts 
t1, t2 via the following score:  
?
?
?
??=
2
21
)(
)(
),( )(21
tw
ttw
widf
widf
ttentscore
 (6) 
where idf(w) = log(N/nw), N is total number of 
documents in corpus and nw is number of docu-
46
ments containing word w.  A practically equivalent 
measure was independently proposed in the con-
text of QA by (Saggion et al, 2004)3. This baseline 
measure captures word overlap, considering only 
words that appear in both texts and weighs them 
based on their inverse document frequency. 
4 The RTE challenge dataset 
The RTE dataset (Dagan et al, 2005) consists 
of sentence pairs annotated for entailment. Fo this 
dataset we used word cooccurrence frequencies 
obtained from a web search engine. The details of 
this experiment are described in Glickman et al, 
2005a. The resulting accuracy on the test set was 
59% and the resulting confidence weighted score 
was 0.57. Both are statistically significantly better 
than chance at the 0.01 level. The baseline model 
(6) from Section  3.2, which takes into account only 
terms appearing in both the text and hypothesis, 
achieved an accuracy of only 56%. Although our 
proposed lexical system is relatively simple, as it 
doesn?t rely on syntactic or other deeper analysis, 
it nevertheless was among the top ranking systems 
in the RTE Challenge. 
5 RCV1 dataset  
In addition to the RTE dataset we were interested 
in evaluating the model on a more representative 
set of texts and hypotheses that better corresponds 
to applicative settings. We focused on the informa-
tion seeking setting, common in applications such 
as QA and IR, in which a hypothesis is given and it 
is necessary to identify texts that entail it.  
An annotator was asked to choose 60 hypothe-
ses based on sentences from the first few docu-
ments in the Reuters Corpus Volume 1 (Rose et al, 
2002). The annotator was instructed to choose sen-
tential hypotheses such that their truth could easily 
be evaluated. We further required that the hypothe-
ses convey a reasonable information need in such a 
way that they might correspond to potential ques-
tions, semantic queries or IE relations. Table 2 
shows a few of the hypotheses.  
In order to create a set of candidate entailing 
texts for the given set of test hypotheses, we fol-
lowed the common practice of WordNet based ex-
                                                          
3
 (Saggion et al, 2004) actually proposed the above score with 
no normalizing denominator. However for a given hypothesis 
it results with the same ranking of candidate entailing texts. 
pansion (Nie and Brisebois, 1996; Yang and Chua, 
2002). Using WordNet, we expanded the hypothe-
ses? terms with morphological alternations and 
semantically related words4.  
For each hypothesis stop words were removed 
and all content words were expanded as described 
above. Boolean Search included a conjunction of 
the disjunction of the term?s expansions and was 
performed at the paragraph level over the full 
Reuters corpus, as common in IR for QA. Since we 
wanted to focus our research on semantic variabil-
ity we excluded from the result set paragraphs that 
contain all original words of the hypothesis or their 
morphological derivations. The resulting dataset 
consists of 50 hypotheses and over a million re-
trieved paragraphs (10 hypotheses had only exact 
matches). The number of paragraphs retrieved per 
hypothesis range from 1 to 400,000.5  
5.1 Evaluation 
The model?s entailment probability, tep, was com-
pared to the following two baseline models. The 
first, denoted as base, is the na?ve baseline in 
which all retrieved texts are presumed to entail the 
hypothesis with equal confidence. This baseline 
corresponds to systems which perform blind ex-
pansion with no weighting. The second baseline, 
entscore, is the entailment score (6) from  3.2.  
The top 20 best results for all methods were 
given to judges to be annotated for entailment. 
Judges were asked to annotate an example as true 
if given the text they can infer with high confi-
dence that the hypothesis is true (similar to the 
guidelines published for the RTE Challenge data-
set). Accordingly, they were instructed to annotate 
the example as false if either they believe the hy-
pothesis is false given the text or if the text is unre-
lated to the hypothesis. In total there were 1683 
text-hypothesis pairs, which were randomly di-
vided between two judges. In order to measure 
agreement, we had 200 of the pairs annotated by 
both judges, yielding a moderate agreement (a 
Kappa of 0.6). 
                                                          
4
 The following WordNet relations were used: Synonyms, see 
also, similar to, hypernyms/hyponyms, meronyms/holonyms, 
pertainyms, attribute, entailment, cause and domain 
5
 The dataset is available at:  
http://ir-srv.cs.biu.ac.il:64080/emsee05_dataset.zip 
47
5.2 Results 
 base entscore tep 
precision 0.464 0.568 0.647 
cws 0.396 0.509 0.575 
Table 2: Results 
Table 2 includes the results of macro averaging the 
precision at top-20 and the average confidence 
weighted score (cws) achieved for the 50 hypothe-
ses. Applying Wilcoxon Signed-Rank Test, our 
model performs significantly better (at the 0.01 
level) than entscore and base for both precision and 
cws. Analyzing the results showed that many of 
the mistakes were not due to wrong expansion but 
rather to a lack of a deeper analysis of the text and 
hypothesis (e.g. example 3 in Table 2). Indeed this 
is a common problem with lexical models. Incor-
porating additional linguistic levels into the prob-
abilistic entailment model, such as syntactic 
matching, co-reference resolution and word sense 
disambiguation, becomes a challenging target for 
future research. 
6 Conclusions 
This paper proposes a generative probabilistic set-
ting that formalizes the notion of probabilistic tex-
tual entailment, which is based on the conditional 
probability that a hypothesis is true given the text. 
This probabilistic setting provided the necessary 
grounding for a concrete probabilistic model of 
lexical entailment that is based on document co-
occurrence statistics in a bag of words representa-
tion.  Although the illustrated lexical system is 
relatively simple, as it doesn?t rely on syntactic or 
other deeper analysis, it nevertheless achieved en-
couraging results. The results suggest that such a 
probabilistic framework is a promising basis for 
improved implementations incorporating deeper 
types of knowledge and a common test-bed for 
more sophisticated models.   
Acknowledgments 
This work was supported in part by the IST Pro-
gramme of the European Community, under the 
PASCAL Network of Excellence, IST-2002-
506778. This publication only reflects the authors' 
views. We would also like to thank Ruthie Mandel 
and Tal Itzhak Ron for their annotation work. 
 
References 
Fahiem Bacchus. 1990. Representing and Reasoning 
with Probabilistic Knowledge, M.I.T. Press. 
Peter F. Brown, Vincent J. Della Pietra, Stephen A. 
Della Pietra, and Robert L. Mercer. 1993. The 
Mathematics of Statistical Machine Translation: Pa-
rameter Estimation. Computational Linguistics, 
19(2):263?311. 
Chierchia, Gennaro, and Sally McConnell-Ginet. 2001. 
Meaning and grammar: An introduction to seman-
tics, 2nd. edition. Cambridge, MA: MIT Press. 
Ido Dagan, Oren Glickman and Bernardo Magnini. 
2005. The PASCAL Recognising Textual Entailment 
Challenge. In Proceedings of the PASCAL Chal-
lenges Workshop for Recognizing Textual Entail-
ment. Southampton, U.K. 
Oren Glickman, Ido Dagan and Moshe Koppel. 2005a. 
Web Based Probabilistic Textual Entailment, 
PASCAL Challenges Workshop for Recognizing 
Textual Entailment. 
Oren Glickman, Ido Dagan and Moshe Koppel. 2005b. 
A Probabilistic Classification Approach for Lexical 
Textual Entailment, Twentieth National Conference 
on Artificial Intelligence (AAAI-05). 
Joseph Y. Halpern. 1990. An analysis of first-order lo-
gics of probability. Artificial Intelligence 46:311-350. 
Christof Monz, Maarten de Rijke. 2001. Light-Weight 
Entailment Checking for Computational Semantics. 
In Proc. of the third workshop on inference in com-
putational semantics (ICoS-3).  
Jian-Yun Nie and Martin Brisebois. 1996. An Inferential 
Approach to Information Retrieval and Its Implemen-
tation Using a Manual Thesaurus. Artificial Intelli-
gence Revue 10(5-6): 409-439. 
Jay M. Ponte, W. Bruce Croft, 1998. A Language Mod-
eling Approach to Information Retrieval. SIGIR con-
ference on Research and Development in Information 
Retrieval. 
Tony G. Rose, Mary Stevenson, and Miles Whitehead. 
2002. The Reuters Corpus volume 1 - from yester-
day?s news to tomorrow?s language resources. Third 
International Conference on Language Resources and 
Evaluation (LREC). 
Hui Yang and Tat-Seng Chua. 2002. The integration of 
lexical knowledge and external resources for ques-
tion answering. The eleventh Text REtrieval Confer-
ence (TREC-11). 
48
Proceedings of the Workshop on Linguistic Distances, page 7,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Semantic Similarity: What for?
Ido Dagan
Bar Ilan University
dagan@macs.biu.ac.il
Abstract
Linguistic similarity has been a promi-
nent notion and tool in computational lin-
guistics and related areas, as elaborated
nicely in the announcement of this work-
shop. Yet, what exactly counts as ?sim-
ilarity?, or when two linguistic concepts
should be regarded as similar, often re-
mains rather vague and ill posed, which is
in fact quite typical for unsupervised no-
tions. This talk will focus on similarity
at the semantic level, and will explore the
perspective that different notions of simi-
larity may be defined relative to concrete
modeling goals. In particular, I will refer
to the two major goals in semantic mod-
eling: predicting likelihood of occurrence,
which is the typical goal in disambigua-
tion and language modeling, and recogniz-
ing target meanings, which is the typical
semantic goal in text understanding appli-
cations such as question answering, infor-
mation extraction, summarization and in-
formation retrieval. We will discuss each
goal and present corresponding semantic
similarity approaches.
7
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 172?179,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Lexical Reference: a Semantic Matching Subtask
Oren Glickman and Eyal Shnarch and Ido Dagan
Computer Science Department
Bar Ilan University
Ramat Gan, Israel
{glikmao, dagan}@cs.biu.ac.il
Abstract
Semantic lexical matching is a prominent
subtask within text understanding applica-
tions. Yet, it is rarely evaluated in a di-
rect manner. This paper proposes a def-
inition for lexical reference which cap-
tures the common goals of lexical match-
ing. Based on this definition we created
and analyzed a test dataset that was uti-
lized to directly evaluate, compare and im-
prove lexical matching models. We sug-
gest that such decomposition of the global
semantic matching task is critical in order
to fully understand and improve individual
components.
1 Introduction
A fundamental task for text understanding ap-
plications is to identify semantically equivalent
pieces of text. For example, Question Answer-
ing (QA) systems need to match corresponding
parts in the question and in the answer passage,
even though such parts may be expressed in dif-
ferent terms. Summarization systems need to rec-
ognize (redundant) semantically matching parts
in multiple sentences that are phrased differently.
Other applications, such as information extraction
and retrieval, face pretty much the same seman-
tic matching task. The degree of semantic match-
ing found is typically factored into systems? scor-
ing and ranking mechanisms. The recently pro-
posed framework of textual entailment (Dagan et
al., 2006) attempts to formulate the generic seman-
tic matching problem in an application indepen-
dent manner.
The most commonly implemented semantic
matching component addresses the lexical level.
At this level the goal is to identify whether the
meaning of a lexical item of one text is expressed
also within the other text. Typically, lexical match-
ing models measure the degree of literal lexical
overlap, augmented with lexical substitution cri-
teria based on resources such as Wordnet or the
output of statistical similarity methods (see Sec-
tion 2). Many systems apply semantic matching
only at the lexical level, which is used to approx-
imate the overall degree of semantic matching be-
tween texts. Other systems incorporate lexical
matching as a component within more complex
models that examine matching at higher syntactic
and semantic levels.
While lexical matching models are so promi-
nent within semantic systems they are rarely eval-
uated in a direct manner. Typically, improve-
ments to a lexical matching model are evaluated by
their marginal contribution to overall system per-
formance. Yet, such global and indirect evaluation
does not indicate the absolute performance of the
model relative to the sheer lexical matching task
for which it was designed. Furthermore, the indi-
rect application-dependent evaluation mode does
not facilitate improving lexical matching models
in an application dependent manner, and does not
allow proper comparison of such models which
were developed (and evaluated) by different re-
searchers within different systems.
This paper proposes a generic definition for the
lexical matching task, which we term lexical ref-
erence. This definition is application indepen-
dent and enables annotating test datasets that eval-
uate directly lexical matching models. Conse-
quently, we created a dataset annotated for lexical
reference, using a sample of sentence pairs (text-
hypothesis) from the 1st Recognising Textual En-
tailment dataset. Further analysis identified sev-
172
eral sub-types of lexical reference, pointing at the
many interesting cases where lexical reference is
derived from a complete context rather than from
a particular matching lexical item.
Next, we used the lexical reference dataset to
evaluate and compare several state-of-the-art ap-
proaches for lexical matching. Having a direct
evaluation task enabled us to capture the actual
performance level of these models, to reveal their
relative strengths and weaknesses, and even to
construct a simple combination of two models that
outperforms all the original ones. Overall, we sug-
gest that it is essential to decompose global se-
mantic matching and textual entailment tasks into
proper subtasks, like lexical reference. Such de-
composition is needed in order to fully understand
the behavior of individual system components and
to guide their future improvements.
2 Background
2.1 Term Matching
Thesaurus-based term expansion is a commonly
used technique for enhancing the recall of NLP
systems and coping with lexical variability. Ex-
pansion consists of altering a given text (usu-
ally a query) by adding terms of similar meaning.
WordNet is commonly used as a source of related
words for expansion. For example, many QA sys-
tems perform expansion in the retrieval phase us-
ing query related words based on WordNet?s lexi-
cal relations such as synonymy or hyponymy (e.g
(Harabagiu et al, 2000; Hovy et al, 2001)). Lex-
ical similarity measures (e.g. (Lin, 1998)) have
also been suggested to measure semantic similar-
ity. They are based on the distributional hypothe-
sis, suggesting that words that occur within similar
contexts are semantically similar.
2.2 Textual Entailment
The Recognising Textual Entailment (RTE-1) chal-
lenge (Dagan et al, 2006) is an attempt to promote
an abstract generic task that captures major seman-
tic inference needs across applications. The task
requires to recognize, given two text fragments,
whether the meaning of one text can be inferred
(entailed) from another text. Different techniques
and heuristics were applied on the RTE-1 dataset
to specifically model textual entailment. Interest-
ingly, a number of works (e.g. (Bos and Mark-
ert, 2005; Corley and Mihalcea, 2005; Jijkoun and
de Rijke, 2005; Glickman et al, 2006)) applied or
utilized lexical based word overlap measures. Var-
ious word-to-word similarity measures where ap-
plied, including distributional similarity (such as
(Lin, 1998)), web-based co-occurrence statistics
and WordNet based similarity measures (such as
(Leacock et al, 1998)).
2.3 Paraphrase Acquisition
A substantial body of work has been dedicated to
learning patterns of semantic equivalency between
different language expressions, typically consid-
ered as paraphrases. Recently, several works ad-
dressed the task of acquiring paraphrases (semi-)
automatically from corpora. Most attempts were
based on identifying corresponding sentences in
parallel or ?comparable? corpora, where each cor-
pus is known to include texts that largely corre-
spond to texts in another corpus (e.g. (Barzilay
and McKeown, 2001)). Distributional Similarity
was also used to identify paraphrase patterns from
a single corpus rather than from a comparable
set of corpora (Lin and Pantel, 2001). Similarly,
(Glickman and Dagan, 2004) developed statistical
methods that match verb paraphrases within a reg-
ular corpus.
3 The Lexical Reference Dataset
3.1 Motivation and Definition
One of the major observations of the 1st Recog-
nizing Textual Entailment (RTE-1) challenge re-
ferred to the rich structure of entailment modeling
systems and the need to evaluate and optimize in-
dividual components within them. When building
such a compound system it is valuable to test each
component directly during its development, rather
than indirectly evaluating the component?s perfor-
mance via the behavior of the entire system. If
given tools to evaluate each component indepen-
dently researchers can target and perfect the per-
formance of the subcomponents without the need
of building and evaluating the entire end-to-end
system.
A common subtask, addressed by practically all
participating systems in RTE-1, was to recognize
whether each lexical meaning in the hypothesis is
referenced by some meaning in the corresponding
text. We suggest that this common goal can be
captured through the following definition:
Definition 1 A word w is lexically referenced by
a text t if there is an explicit or implied reference
173
from a set of words in t to a possible meaning of
w.
Lexical reference may be viewed as a natural ex-
tension of textual entailment for sub-sentential hy-
potheses such as words. In this work we fo-
cus on words meanings, however this work can
be directly generalized to word compounds and
phrases. A concrete version of detailed annotation
guidelines for lexical reference is presented in the
next section.1 Lexical Reference is, in some sense,
a more general notion than paraphrases. If the text
includes a paraphrase for w then naturally it does
refer to w?s meaning. However, a text need not
include a paraphrase for the concrete meaning of
the referenced word w, but only an implied refer-
ence. Accordingly, the referring part might be a
large segment of the text, which captures informa-
tion different than w?s meaning, but still implies a
reference to w as part of the text?s meaning.
It is typically a necessary, but not sufficient,
condition for textual entailment that the lexical
concepts in a hypothesis h are referred in a given
text t. For example, in order to infer from a text
the hypothesis ?a dog bit a man,? it is a neces-
sary that the concepts of dog, bite and man must
be referenced by the text, either directly or in an
implied manner. However, for proper entailment
it is further needed that the right relations would
hold between these concepts2. Therefore lexical
entailment should typically be a component within
a more complex entailment modeling (or semantic
matching) system.
3.2 Dataset Creation and Annotation Process
We created a lexical reference dataset derived
from the RTE-1 development set by randomly
choosing 400 out of the 567 text-hypothesis exam-
ples. We then created sentence-word examples for
all content words in the hypotheses which do not
appear in the corresponding sentence and are not
a morphological derivation of a word in it (since a
simple morphologic module could easily identify
these cases). This resulted in a total of 708 lexi-
cal reference examples. Two annotators annotated
these examples as described in the next section.
1These terms should not be confused with the use of lex-
ical entailment in WordNet, which is used to describe an en-
tailment relationship between verb lexical types, nor with the
related notion of reference in classical linguistics, generally
describing the relation between nouns or pronouns and ob-
jects that are named by them (Frege, 1892)
2or quoting the known journalism saying ? ?Dog bites
man? isn?t news, but ?Man bites dog? is.
Taking the same approach as of the RTE-1 dataset
creation (Dagan et al, 2006), we limited our ex-
periments to the resulting 580 examples that the
two annotators agreed upon3.
3.2.1 Annotation guidelines
We asked two annotators to annotate the
sentence-word examples according to the follow-
ing guidelines. Given a sentence and a target word
the annotators were asked to decide whether the
target word is referred by the sentence (true) or
not (false). Annotators were guided to mark the
pair as true in the following cases:
Word: if there is a word in the sentence which,
in the context of the sentence, implies a meaning
of the target word (e.g. a synonym or hyponym),
or which implies a reference to the target word?s
meaning (e.g. blind?see, sight). See examples 1-
2 in Table 1 where the word that implies the refer-
ence is emphasized in the text. Note that in exam-
ple 2 murder is not a synonym of died nor does it
share the same meaning of died; however it is clear
from its presence in the sentence that it refers to a
death. Also note that in example 8 although home
is a possible synonym for house, in the context of
the text it does not appear in that meaning and the
example should be annotated as false.
Phrase: if there is a multi-word independent ex-
pression in the sentence that implies the target (im-
plication in the same sense that a Word does). See
examples 3-4 in Table 1.
Context: if there is a clear reference to the mean-
ing of the target word by the overall meaning of
some part(s) of the sentence (possibly all the sen-
tence), though it is not referenced by any single
word or phrase. The reference is derived from the
complete context of the relevant sentence part. See
examples 5-7 in Table 1.
If there is no reference from the sentence to
the target word the annotators were instructed to
choose false. In example 9 in Table 1 the target
word ?HIV-positive? should be considered as one
word that cannot be broken down from its unit and
although both the general term ?HIV status? and
the more specific term ?HIV negative? are referred
to, the target word cannot be understood or derived
from the text. In example 10 although the year
1945 may refer to a specific war, there is no ?war?
either specifically or generally understood by the
text.
3dataset avaiable at http://ir-srv.cs.biu.ac.
il:64080/emnlp06_dataset.zip
174
ID TEXT TARGET VALUE
1 Oracle had fought to keep the forms from being released. document word
2 The court found two men guilty of murdering Shapour Bakhtiar. died word
3 The new information prompted them to call off the search. cancelled phrase
4 Milan, home of the famed La Scala opera house,. . . located phrase
5 Successful plaintiffs recovered punitive damages in Texas discrimination cases 53 legal context
6 Recreational marijuana smokers are no more likely to develop oral cancer than nonusers. risk context
7 A bus ticket cost nowadays 5.2 NIS whereas last year it cost 4.9. increase context
8 Pakistani officials announced that two South African men in their custody had confessed to
planning attacks at popular tourist spots in their home country.
house false
9 For women who are HIV negative or who do not know their HIV status, breastfeeding should
be promoted for six months.
HIV-positive false
10 On Feb. 1, 1945, the Polish government made Warsaw its capital, and an office for urban
reconstruction was set up.
war false
Table 1: Lexical Reference Annotation Examples
3.2.2 Annotation results
Wemeasured the agreement on the lexical refer-
ence binary task (in which Word, Phrase and Con-
text are conflated to true). The resulting kappa
statistic of 0.63 is regarded as substantial agree-
ment (Landis and Koch, 1997). The resulting
dataset is not balanced in terms of true and false
examples and a straw-baseline for accuracy is
0.61, representing a system which predicts all ex-
amples as true.
3.3 Dataset Analysis
In a similar manner to (Bar-Haim et al, 2005; Van-
derwende et al, 2005) we investigated the rela-
tionship between lexical reference and textual en-
tailment. We checked the performance of a textual
entailment system which relies solely on an ideal
lexical reference component which makes no mis-
takes and asserts that a hypothesis is entailed from
a text if and only if all content words in the hypoth-
esis are referred in the text. Based on the lexical
reference dataset annotations, such an ?ideal? sys-
tem would obtain an accuracy of 74% on the cor-
responding subset of the textual entailment task.
The corresponding precision is 68% and a recall
of 82%. This is significantly higher than the re-
sults of the best performing systems that partici-
pated in the challenge on the RTE-1 test set. This
suggests that lexical reference is a valuable sub-
task for entailment. Interestingly, a similar entail-
ment system based on a lexical reference compo-
nent which doesn?t account for the contextual lex-
ical reference (i.e. all Context annotations are re-
garded as false) would achieve an accuracy of only
63% with 41% precision and a recall of 63%. This
suggests that lexical reference in general and con-
textual entailment in particular, play an important
(though not sufficient) role in entailment recogni-
tion.
Further, we wanted to investigate the validity
of the assumption that for entailment relationship
to hold all content words in the hypothesis must
be referred by the text. We examined the exam-
ples in our dataset which were derived from text-
hypothesis pairs that were annotated as true (en-
tailing) in the RTE dataset. Out of 257 such exam-
ples only 34 were annotated as false by both anno-
tators. Table 2 lists a few such examples in which
entailment at whole holds, however, there exists a
word in the hypothesis (highlighted in the table)
which is not lexically referenced by the text. In
many cases, the target word was part of a non com-
positional compound in the hypothesis, and there-
fore should not be expected to be referenced by
the text (see examples 1-2). This finding indicates
that the basic assumption is a reasonable approxi-
mation for entailment. We could not have revealed
this fact without the dataset for the subtask of lex-
ical reference.
4 Lexical Reference Models
The lexical reference dataset facilitates qualita-
tive and quantitative comparison of various lexical
models. This section describes four state-of-the-
art models that can be applied to the lexical refer-
ence task. The performance of these models was
tested and analyzed, as described in the next sec-
tion, using the lexical reference dataset. All mod-
els assign a [0, 1] score to a given pair of text t
and target word u which can be interpreted as the
confidence that u is lexically referenced in t.
175
ID TEXT HYPOTHESIS ENTAIL-
MENT
REFER-
ENCE
1 Iran is said to give up al Qaeda members. Iran hands over al Qaeda members. true false
2 It would help the economy by putting people
back to work and more money in the hands of
consumers.
More money in the hands of consumers
means more money can be spent to get the
economy going.
true false
3 The Securities and Exchange Commission?s
new rule to beef up the independence of mutual
fund boards represents an industry defeat.
The SEC?s new rule will give boards inde-
pendence.
true false
4 Texas Data Recovery is also successful at re-
trieving lost data from notebooks and laptops,
regardless of age, make or model.
In the event of a disaster you could use Texas
Data Recovery and you will have the capabil-
ity to restore lost data.
true false
Table 2: examples demonstrating cases when lexical entailment does not correlate with entailment. Tar-
get word is shown in bold.
4.1 WordNet
Following the common practice in NLP applica-
tions (see Section 2.1) we evaluated the perfor-
mance of a straight-forward utilization of Word-
Net?s lexical information. Our wordnet model first
lemmatizes the text and target word. It then as-
signs a score of 1 if the text contains a synonym,
hyponym or derived form of the target word and a
score of 0 otherwise.
4.2 Similarity
As a second measure we used the distributional
similarity measure of (Lin, 1998). For a text t and
a word u we assign the max similarity score as fol-
lows:
similarity(t, u) = max
v?t
sim(u, v) (1)
where sim(u, v) is the similarity score for u and
v4.
4.3 Alignment model
(Glickman et al, 2006) was among the top scor-
ing systems on the RTE-1 challenge and supplies a
probabilistically motivated lexical measure based
on word co-occurrence statistics. It is defined for
a text t and a word u as follows:
align(t, u) = max
v?t
P(u|v) (2)
where P(u|v) is simply the co-occurrence proba-
bility ? the probability that a sentence containing v
also contains u. The co-occurrence statistics were
collected from the Reuters Corpus Volume 1.
4the scores were obtained from the following online re-
source: http://www.cs.ualberta.ca/?lindek/
downloads.htm
4.4 Baysean model
(Glickman et al, 2005) provide a contextual mea-
sure which takes into account the whole context
of the text rather than from a single word in the
text as do the previous models. This model is
the only model which addresses contextual refer-
ence rather than just word-to-word matching. The
model is based on a Na??ve Bayes text classification
approach in which corpus sentences serve as doc-
uments and the class is the reference of the target
word u. Sentences containing the word u are used
as positive examples while all other sentences are
considered as negative examples. It is defined for
a text t and a word u as follows:
bayes(t, u) =
P(u)
?
v?t P(v|u)
n(v,t)
P(?u)
?
v?t P(v|?u)
n(v,t)+P(u)
?
v?t P(v|u)
n(v,t)
(3)
where n(w, t) is the number of times word w ap-
pears in t, P(u) is the probability that a sentence
contains the word u andP(v|?u) is the probability
that a sentence NOT containing u contains v. In
order to reduce data size and to account for zero
probabilities we applied smoothing and informa-
tion gain based feature selection on the data prior
to running the model. The co-occurrence prob-
abilities were collected from sentences from the
Reuters corpus in a similar manner to the align-
ment model.
4.5 Combined Model
The WordNet and Bayesian models are derived
from quite different motivations. One would ex-
pect the WordNet model to be better in identify-
ing the word-to-word explicit reference examples
while the Bayesian model is expected to model the
contextualy implied references. For this reason we
tried to combine forces by evaluating a na??ve linear
176
interpolation of the two models (by simply averag-
ing the score of the two models). This model have
not been previously suggested and to the best of
our knowledge this type of combination is novel.
5 Empirical Evaluation and Analysis
5.1 Results
In order to evaluate the scores produced by the
various models as a potential component in an en-
tailment system we compared the recall-precision
graphs. In addition we compared the average pre-
cision which is a single number measure equiv-
alent to the area under an uninterpolated recall-
precision curve and is commonly used to evaluate
a systems ranking ability (Voorhees and Harman,
1999). On our dataset an average precision greater
than 0.65 is better than chance at the 0.05 level
and an average precision greater than 0.66 is sig-
nificant at the 0.01 level.
Figure 1 compares the average precision and
recall-precision results for the various models. As
can be seen, the combined wordnet+bayes model
performs best. In terms of average precision,
the similarity and wordnet models are comparable
and are slightly better than bayes. The alignment
model, however, is not significantly better than
random guessing. The recall-precision figure indi-
cates that the baysian model succeeds to rank quite
well both within the the positively scored wordnet
examples and within the negatively scored word-
net examples and thus resulting in improved av-
erage precision of the combined model. A better
understanding of the systems? performance is evi-
dent from the following analysis.
5.2 Analysis
Table 3 lists a few examples from the lexical refer-
ence dataset alng with their gold-standard anno-
tation and the Bayesian model score. Manual in-
spection of the data shows that the Bayesian model
commonly assigns a low score to correct examples
which have an entailing trigger word or phrase in
the sentence but yet the context of the sentence as a
whole is not typical for the target hypothesized en-
tailed word. For example, in example 5 the entail-
ing phrase ?set in place? and in example 6 the en-
tailing word ?founder? do appear in the text how-
ever the contexts of the sentences are not typical
news domain contexts of issued or founded. An in-
teresting future work would be to change the gen-
erative story and model to account for such cases.
The WordNet model identified a matching word
in the text for 99 out of the 580 examples. This
corresponds to a somewhat low recall of 25% and
a quite high precision of 90%. Table 4 lists typical
mistakes of the wordnet model. Examples 1-3 are
false positive examples in which there is a word
in the text (emphasized in the table) which is a
synonym or hyponym of the target word for some
sense in WordNet, however in the context of the
text it is not of such a sense. Examples 4-6 show
false negative examples, in which the annotators
identified a trigger word in the text (emphasized
in the table) but yet it or no other word in the text
is a synonym or hyponym of the target word.
5.3 Subcategory analysis
word phrase context false
word 178 16 59 32
phrase 4 12 9 4
context 15 5 56 25
false 24 5 38 226
Table 5: inter-annotator confusion matrix for the
auxiliary annotation.
As seen above, the combined model outper-
forms the others since it identifies both word-
to-word lexical reference as well as context-to-
word lexical reference. These are quite different
cases. We asked the annotators to state the sub-
category when they annotated an example as true
(as described in the annotation guidelines in Sec-
tion 3.2.1). The Word subcategory corresponds
to a word-to-word match and Phrase and Context
subcategories correspond to more than one word
to word match. As can be expected, the agreement
on such a task resulted in a lower Kappa of 0.5
which corresponds to moderate agreement (Landis
and Koch, 1997). the confusion matrix between
the two annotators is presented in Table 5. This de-
composition enables the evaluation of the strength
and weakness of different lexical reference mod-
ules, free from the context of the bigger entailment
system.
We used the subcategories dataset to test the
performances of the different models. Table 6
lists for each subcategory the recall of correctly
identified examples for each model?s 25% recall
level. The table shows that the wordnet and simi-
larity models? strength is in identifying examples
where lexical reference is triggered by a dominant
word in the sentence. The bayes model, however,
177
Figure 1: comparison of average precision (left) and recall-precision (right) results for the various models
id text token annotation score
1 QNX Software Systems Ltd., a leading provider of real-time software and ser-
vices to the embedded computing market, is pleased to announce the appoint-
ment of Mr. Sachin Lawande to the position of vice president, engineering ser-
vices.
named PHRASE 0.98
2 NIH?s FY05 budget request of $28.8 billion includes $2 billion for the National
Institute of General Medical Sciences, a 3.4-percent increase, and $1.1 billion
for the National Center for Research Resources, and a 7.2-percent decrease from
FY04 levels.
reduced WORD 0.91
3 Pakistani officials announced that two South African men in their custody had
confessed to planning attacks at popular tourist spots in their home country.
security CONTEXT 0.80
4 With $549 million in cash as of June 30, Google can easily afford to make
amends.
shares FALSE 0.03
5 In the year 538, Cyrus set in place a policy which demanded the return of the
various gods to their proper places.
issued PHRASE 7e-4
6 The black Muslim activist said that he had relieved Muhammad of his duties
?until he demonstrates that he is willing to conform to the manner of representing
Allah and the honorable Elijah Muhammad (founder of the Nation of Islam)?.
founded WORD 3e-6
Table 3: A sample from the lexical reference dataset alng with the Bayesian model?s score
id text token annotation
1 Kerry hit Bush hard on his conduct on the war in Iraq shot FALSE
2 Pakistani officials announced that two South African men in their custody had confessed to
planning attacks at popular tourist spots in their home country
forces FALSE
3 It would help the economy by putting people back to work and more money in the hands of
consumers
get FALSE
4 Eating lots of foods that are a good source of fiber may keep your blood glucose from rising
too fast after you eat
sugar WORD
5 Hippos do come into conflict with people quite often human WORD
6 Weinstock painstakingly reviewed dozens of studies for evidence of any link between sun-
screen use and either an increase or decrease in melanoma
cancer WORD
Table 4: A few erroneous examples of WordNet model
is better at identifying phrase and context exam-
ples. The combined WordNet and Bayesian mod-
els? strength can be explained by the quite dif-
ferent behaviors of the two models - the Word-
Net model seems to be better in identifying the
word-to-word explicit reference examples while
the Bayesian model is better in modeling the con-
textual implied references.
6 Conclusions
This paper proposed an explicit task definition for
lexical reference. This task captures directly the
goal of common lexical matching models, which
typically operate within more complex systems
178
method word disagreement phrase/context
wordnet 38% 9% 17%
similarity 39% 7% 17%
bayes 22% 21% 37%
Table 6: Breakdown of recall of correctly identi-
fied example types at an overall system?s recall of
25%. Disagreement refers to examples for which
the annotators did not agree on the subcategory an-
notation (word vs. phrase/context).
that address more complex tasks. This defini-
tion enabled us to create an annotated dataset for
the lexical reference task, which provided insights
into interesting sub-classes that require different
types of modeling. The dataset enabled us to
make a direct evaluation and comparison of lexical
matching models, reveal insightful differences be-
tween them, and create a simple improved model
combination. In the long run, we believe that
the availability of such datasets will facilitate im-
proved models that consider the various sub-cases
of lexical reference, as well as applying supervised
learning to optimize model combination and per-
formance.
References
[Bar-Haim et al2005] Roy Bar-Haim, Idan Szpecktor,
and Oren Glickman. 2005. Definition and analysis
of intermediate entailment levels. In Proceedings
of the ACL Workshop on Empirical Modeling of Se-
mantic Equivalence and Entailment, pages 55?60,
Ann Arbor, Michigan, June. Association for Com-
putational Linguistics.
[Barzilay and McKeown2001] Regina Barzilay and
Kathleen McKeown. 2001. Extracting paraphrases
from a parallel corpus. In ACL, pages 50?57.
[Bos and Markert2005] Johan Bos and Katja Markert.
2005. Recognising textual entailment with logical
inference techniques. In EMNLP.
[Corley and Mihalcea2005] Courtney Corley and Rada
Mihalcea. 2005. Measuring the semantic similarity
of texts. In Proceedings of the ACL Workshop on
Empirical Modeling of Semantic Equivalence and
Entailment, pages 13?18.
[Dagan et al2006] Ido Dagan, Oren Glickman, and
Bernardo Magnini, editors. 2006. The PASCAL
Recognising Textual Entailment Challenge, volume
3944. Lecture Notes in Computer Science.
[Frege1892] Gottlob Frege. 1892. On sense and
reference. Reprinted in P. Geach and M. Black,
eds., Translations from the Philosophical Writings
of Gottlob Frege. 1960.
[Glickman and Dagan2004] Oren Glickman and Ido
Dagan, 2004. Recent Advances in Natural Lan-
guage Processing III, chapter Acquiring lexical
paraphrases from a single corpus, pages 81?90.
John Benjamins.
[Glickman et al2005] Oren Glickman, Ido Dagan, and
Moshe Koppel. 2005. A probabilistic classification
approach for lexical textual entailment. In AAAI,
pages 1050?1055.
[Glickman et al2006] Oren Glickman, Ido Dagan, and
Moshe Koppel. 2006. A lexical alignment model
for probabilistic textual entailment, volume 3944.
In Lecture Notes in Computer Science, pages 287 ?
298. Springer.
[Harabagiu et al2000] Sanda M. Harabagiu, Dan I.
Moldovan, Marius Pasca, Rada Mihalcea, Mihai
Surdeanu, Razvan C. Bunescu, Roxana Girju, Vasile
Rus, and Paul Morarescu. 2000. Falcon: Boosting
knowledge for answer engines. In TREC.
[Hovy et al2001] Eduard H. Hovy, Ulf Hermjakob, and
Chin-Yew Lin. 2001. The use of external knowl-
edge of factoid QA. In Text REtrieval Conference.
[Jijkoun and de Rijke2005] Valentin Jijkoun and
Maarten de Rijke. 2005. Recognizing textual
entailment using lexical similarity. Proceedings of
the PASCAL Challenges Workshop on Recognising
Textual Entailment (and forthcoming LNAI book
chapter).
[Landis and Koch1997] J. R. Landis and G. G. Koch.
1997. The measurements of observer agreement for
categorical data. Biometrics, 33:159?174.
[Leacock et al1998] Claudia Leacock, George A.
Miller, and Martin Chodorow. 1998. Using
corpus statistics and wordnet relations for sense
identification. Comput. Linguist., 24(1):147?165.
[Lin and Pantel2001] Dekang Lin and Patrik Pantel.
2001. Discovery of inference rules for question an-
swering. Natural Language Engineering, 4(7):343?
360.
[Lin1998] Dekang Lin. 1998. Automatic retrieval and
clustering of similar words. In Proceedings of the
17th international conference on Computational lin-
guistics, pages 768?774, Morristown, NJ, USA. As-
sociation for Computational Linguistics.
[Vanderwende et al2005] Lucy Vanderwende, Deborah
Coughlin, and Bill Dolan. 2005. What syntax
can contribute in entailment task. Proceedings of
the PASCAL Challenges Workshop on Recognising
Textual Entailment.
[Voorhees and Harman1999] Ellen M. Voorhees and
Donna Harman. 1999. Overview of the seventh text
retrieval conference. In Proceedings of the Seventh
Text REtrieval Conference (TREC-7). NIST Special
Publication.
179
Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),
pages 45?52, New York City, June 2006. c?2006 Association for Computational Linguistics
Investigating Lexical Substitution Scoring for Subtitle Generation
Oren Glickman and Ido Dagan
Computer Science Department
Bar Ilan University
Ramat Gan, Israel
{glikmao,dagan}@cs.biu.ac.il
Mikaela Keller and Samy Bengio
IDIAP Research Institute
Martigny,
Switzerland
{mkeller,bengio}@idiap.ch
Walter Daelemans
CNTS
Antwerp, Belgium
walter.daelemans@ua.ac.be
Abstract
This paper investigates an isolated setting
of the lexical substitution task of replac-
ing words with their synonyms. In par-
ticular, we examine this problem in the
setting of subtitle generation and evaluate
state of the art scoring methods that pre-
dict the validity of a given substitution.
The paper evaluates two context indepen-
dent models and two contextual models.
The major findings suggest that distribu-
tional similarity provides a useful comple-
mentary estimate for the likelihood that
two Wordnet synonyms are indeed substi-
tutable, while proper modeling of contex-
tual constraints is still a challenging task
for future research.
1 Introduction
Lexical substitution - the task of replacing a word
with another one that conveys the same meaning -
is a prominent task in many Natural Language Pro-
cessing (NLP) applications. For example, in query
expansion for information retrieval a query is aug-
mented with synonyms of the original query words,
aiming to retrieve documents that contain these syn-
onyms (Voorhees, 1994). Similarly, lexical substi-
tutions are applied in question answering to identify
answer passages that express the sought answer in
different terms than the original question. In natu-
ral language generation it is common to seek lex-
ical alternatives for the same meaning in order to
reduce lexical repetitions. In general, lexical sub-
stitution aims to preserve a desired meaning while
coping with the lexical variability of expressing that
meaning. Lexical substitution can thus be viewed
within the general framework of recognizing entail-
ment between text segments (Dagan et al, 2005), as
modeling entailment relations at the lexical level.
In this paper we examine the lexical substitu-
tion problem within a specific setting of text com-
pression for subtitle generation (Daelemans et al,
2004). Subtitle generation is the task of generat-
ing target language TV subtitles for video recordings
of a source language speech. The subtitles should
be of restricted length, which is often shorter than
the full translation of the original speech, yet they
should maintain as much as possible the meaning
of the original content. In a typical (automated)
subtitling process the original speech is first trans-
lated fully into the target language and then the tar-
get translation is compressed to optimize the length
requirements. One of the techniques employed in
the text compression phase is to replace a target lan-
guage word in the original translation with a shorter
synonym of it, thus reducing the character length of
the subtitle. This is a typical lexical substitution
task, which resembles similar operations in other
text compression and generation tasks (e.g. (Knight
and Marcu, 2002)).
This paper investigates the task of assigning like-
lihood scores for the correctness of such lexical sub-
stitutions, in which words in the original translation
are replaced with shorter synonyms. In our experi-
ments we use WordNet as a source of candidate syn-
onyms for substitution. The goal is to score the like-
lihood that the substitution is admissible, i.e. yield-
ing a valid sentence that preserves the original mean-
ing. The focus of this paper is thus to utilize the
subtitling setting in order to investigate lexical sub-
45
stitution models in isolation, unlike most previous
literature in which this sub-task has been embedded
in larger systems and was not evaluated directly.
We examine four statistical scoring models, of
two types. Context independent models score the
general likelihood that the original word is ?replace-
able? with the candidate synonym, in an arbitrary
context. That is, trying to filter relatively bizarre
synonyms, often of rare senses, which are abundant
in WordNet but are unlikely to yield valid substitu-
tions. Contextual models score the ?fitness? of the
replacing word within the context of the sentence, in
order to filter out synonyms of senses of the original
word that are not the right sense in the given context.
We set up an experiment using actual subti-
tling data and human judgements and evaluate the
different scoring methods. Our findings suggest
the dominance, in this setting, of generic context-
independent scoring. In particular, considering dis-
tributional similarity amongst WordNet synonyms
seems effective for identifying candidate substitu-
tions that are indeed likely to be applicable in actual
texts. Thus, while distributional similarity alone is
known to be too noisy as a sole basis for meaning-
preserving substitutions, its combination withWord-
Net alows reducing the noise caused by the many
WordNet synonyms that are unlikely to correspond
to valid substitutions.
2 Background and Setting
2.1 Subtitling
Automatic generation of subtitles is a summariza-
tion task at the level of individual sentences or occa-
sionally of a few contiguous sentences. Limitations
on reading speed of viewers and on the size of the
screen that can be filled with text without the image
becoming too cluttered, are the constraints that dy-
namically determine the amount of compression in
characters that should be achieved in transforming
the transcript into subtitles. Subtitling is not a trivial
task, and is expensive and time-consuming when ex-
perts have to carry it out manually. As for other NLP
tasks, both statistical (machine learning) and linguis-
tic knowledge-based techniques have been consid-
ered for this problem. Examples of the former are
(Knight and Marcu, 2002; Hori et al, 2002), and of
the latter are (Grefenstette, 1998; Jing and McKe-
own, 1999). A comparison of both approaches in
the context of a Dutch subtitling system is provided
in (Daelemans et al, 2004). The required sentence
simplification is achieved either by deleting mate-
rial, or by paraphrasing parts of the sentence into
shorter expressions with the same meaning. As a
special case of the latter, lexical substitution is often
used to achieve a compression target by substituting
a word by a shorter synonym. It is on this subtask
that we focus in this paper. Table 1 provides a few
examples. E.g. by substituting ?happen? by ?occur?
(example 3), one character is saved without affecting
the sentence meaning .
2.2 Experimental Setting
The data used in our experiments was collected in
the context of the MUSA (Multilingual Subtitling of
Multimedia Content) project (Piperidis et al, 2004)1
and was kindly provided for the current study. The
data was provided by the BBC in the form of Hori-
zon documentary transcripts with the corresponding
audio and video. The data for two documentaries
was used to create a dataset consisting of sentences
from the transcripts and the corresponding substitu-
tion examples in which selected words are substi-
tuted by a shorter Wordnet synonym. More con-
cretely, a substitution example thus consists of an
original sentence s = w1 . . . wi . . . wn, a specific
source word wi in the sentence and a target (shorter)
WordNet synonym w? to substitute the source. See
Table 1 for examples. The dataset consists of 918
substitution examples originating from 231 different
sentences.
An annotation environment was developed to al-
low efficient annotation of the substitution examples
with the classes true (admissible substitution, in the
given context) or false (inadmissible substitution).
About 40% of the examples were judged as true.
Part of the data was annotated by an additional an-
notator to compute annotator agreement. The Kappa
score turned out to be 0.65, corresponding to ?Sub-
stantial Agreement? (Landis and Koch, 1997). Since
some of the methods we are comparing need tuning
we held out a random subset of 31 original sentences
(with 121 corresponding examples) for development
and kept for testing the resulting 797 substitution ex-
1http://sinfos.ilsp.gr/musa/
46
id sentence source target judgment
1 The answer may be found in the behaviour of animals. answer reply false
2 . . . and the answer to that was - Yes answer reply true
3
We then wanted to know what would happen if
we delay the movement of the subject?s left hand
happen occur true
4 subject topic false
5 subject theme false
6 people weren?t laughing they were going stone sober. stone rock false
7 if we can identify a place where the seizures are coming from then we can go in
and remove just that small area.
identify place false
8 my approach has been the first to look at the actual structure of the laugh sound. approach attack false
9 He quickly ran into an unexpected problem. problem job false
10 today American children consume 5 times more Ritalin than the rest of the world
combined
consume devour false
Table 1: Substitution examples from the dataset alng with their annotations
amples from the remaining 200 sentences.
3 Compared Scoring Models
We compare methods for scoring lexical substitu-
tions. These methods assign a score which is ex-
pected to correspond to the likelihood that the syn-
onym substitution results in a valid subtitle which
preserves the main meaning of the original sentence.
We examine four statistical scoring models, of
two types. The context independent models score
the general likelihood that the source word can be
replaced with the target synonym regardless of the
context in which the word appears. Contextual mod-
els, on the other hand, score the fitness of the target
word within the given context.
3.1 Context Independent Models
Even though synonyms are substitutable in theory,
in practice there are many rare synonyms for which
the likelihood of substitution is very low and will be
substitutable only in obscure contexts. For exam-
ple, although there are contexts in which the word
job is a synonym of the word problem2, this is not
typically the case and overall job is not a good tar-
get substitution for the source problem (see example
9 in Table 1). For this reason synonym thesauruses
such as WordNet tend to be rather noisy for practi-
cal purposes, raising the need to score such synonym
substitutions and accordingly prioritize substitutions
that are more likely to be valid in an arbitrary con-
text.
2WordNet lists job as a possible member of the synset for a
state of difficulty that needs to be resolved, as might be used in
sentences like ?it is always a job to contact him?
As representative approaches for addressing this
problem, we chose two methods that rely on statisti-
cal information of two types: supervised sense dis-
tributions from SemCor and unsupervised distribu-
tional similarity.
3.1.1 WordNet based Sense Frequencies
(semcor)
The obvious reason that a target synonym cannot
substitute a source in some context is if the source
appears in a different sense than the one in which
it is synonymous with the target. This means that a
priori, synonyms of frequent senses of a source word
are more likely to provide correct substitutions than
synonyms of the word?s infrequent senses.
To estimate such likelihood, our first measure is
based on sense frequencies from SemCor (Miller et
al., 1993), a corpus annotated with Wordnet senses.
For a given source word u and target synonym v the
score is calculated as the percentage of occurrences
of u in SemCor for which the annotated synset con-
tains v (i.e. u?s occurrences in which its sense is
synonymous with v). This corresponds to the prior
probability estimate that an occurrence of u (in an
arbitrary context) is actually a synonym of v. There-
fore it is suitable as a prior score for lexical substi-
tution.3
3.1.2 Distributional Similarity (sim)
The SemCor based method relies on a supervised
approach and requires a sense annotated corpus. Our
3Note that WordNet semantic distance measures such as
those compared in (Budanitsky and Hirst, 2001) are not appli-
cable here since they measure similarity between synsets rather
than between synonymous words within a single synset.
47
second method uses an unsupervised distributional
similarity measure to score synonym substitutions.
Such measures are based on the general idea of
Harris? Distributional Hypothesis, suggesting that
words that occur within similar contexts are seman-
tically similar (Harris, 1968).
As a representative of this approach we use Lin?s
dependency-based distributional similarity database.
Lin?s database was created using the particular dis-
tributional similarity measure in (Lin, 1998), applied
to a large corpus of news data (64 million words) 4.
Two words obtain a high similarity score if they oc-
cur often in the same contexts, as captured by syn-
tactic dependency relations. For example, two verbs
will be considered similar if they have large common
sets of modifying subjects, objects, adverbs etc.
Distributional similarity does not capture directly
meaning equivalence and entailment but rather a
looser notion of meaning similarity (Geffet and Da-
gan, 2005). It is typical that non substitutable words
such as antonyms or co-hyponyms obtain high sim-
ilarity scores. However, in our setting we apply
the similarity score only for WordNet synonyms in
which it is known a priori that they are substitutable
is some contexts. Distributional similarity may thus
capture the statistical degree to which the two words
are substitutable in practice. In fact, it has been
shown that prominence in similarity score corre-
sponds to sense frequency, which was suggested as
the basis for an unsupervised method for identifying
the most frequent sense of a word (McCarthy et al,
2004).
3.2 Contextual Models
Contextual models score lexical substitutions based
on the context of the sentence. Such models
try to estimate the likelihood that the target word
could potentially occur in the given context of the
source word and thus may replace it. More con-
cretely, for a given substitution example consist-
ing of an original sentence s = w1 . . . wi . . . wn,
and a designated source word wi, the contextual
models we consider assign a score to the substi-
tution based solely on the target synonym v and
the context of the source word in the original sen-
4available at http://www.cs.ualberta.ca/
?lindek/downloads.htm
tence, {w1, . . . , wi?1, wi+1, . . . , wn}, which is rep-
resented in a bag-of-words format.
Apparently, this setting was not investigated much
in the context of lexical substitution in the NLP lit-
erature. We chose to evaluate two recently proposed
models that address exactly the task at hand: the first
model was proposed in the context of lexical model-
ing of textual entailment, using a generative Na??ve
Bayes approach; the second model was proposed
in the context of machine learning for information
retrieval, using a discriminative neural network ap-
proach. The two models were trained on the (un-
annotated) sentences of the BNC 100 million word
corpus (Burnard, 1995) in bag-of-words format. The
corpus was broken into sentences, tokenized, lem-
matized and stop words and tokens appearing only
once were removed. While training of these models
is done in an unsupervised manner, using unlabeled
data, some parameter tuning was performed using
the small development set described in Section 2.
3.2.1 Bayesian Model (bayes)
The first contextual model we examine is the one
proposed in (Glickman et al, 2005) to model tex-
tual entailment at the lexical level. For a given tar-
get word this unsupervised model takes a binary text
categorization approach. Each vocabulary word is
considered a class, and contexts are classified as to
whether the given target word is likely to occur in
them. Taking a probabilistic Na??ve-Bayes approach
the model estimates the conditional probability of
the target word given the context based on corpus co-
occurrence statistics. We adapted and implemented
this algorithm and trained the model on the sen-
tences of the BNC corpus.
For a bag-of-words context C =
{w1, . . . , wi?1, wi+1, . . . , wn} and target word
v the Na??ve Bayes probability estimation for the
conditional probability of a word v may occur in a
given a context C is as follows:
P(v|C) =
P(C|v) P(v)
P(C|v) P(v)+P(C|?v) P(?v)
?
P(v)
?
w?C P(w|v)
P(v)
?
w?C P(w|v)+P(?v)
?
w?C P(w|?v)
(1)
where P(w|v) is the probability that a word w ap-
pears in the context of a sentence containing v and
correspondingly P(w|?v) is the probability that w
48
appears in a sentence not containing v. The prob-
ability estimates were obtained from the processed
BNC corpus as follows:
P(w|v) =
|w appears in sentences containing v|
|words in sentences containing v|
P(w|?v) =
|w occurs in sentences not containing v|
|words in sentences not containing v|
To avoid 0 probabilities these estimates were
smoothed by adding a small constant to all counts
and normalizing accordingly. The constant value
was tuned using the development set to maximize
average precision (see Section 4.1). The estimated
probability, P(v|C), was used as the confidence
score for each substitution example.
3.2.2 Neural Network Model (nntr)
As a second contextual model we evaluated the
Neural Network for Text Representation (NNTR)
proposed in (Keller and Bengio, 2005). NNTR is
a discriminative approach which aims at modeling
how likely a given word v is in the context of a piece
of text C, while learning a more compact represen-
tation of reduced dimensionality for both v and C.
NNTR is composed of 3 Multilayer Perceptrons,
noted mlpA(), mlpB() and mlpC(), connected as
follow:
NNTR(v, C) = mlpC [mlpA(v),mlpB(C)].
mlpA(v) and mlpB(C) project respectively the
vector space representation of the word and text
into a more compact space of lower dimensionality.
mlpC() takes as input the new representations of v
and C and outputs a score for the contextual rele-
vance of v to C.
As training data, couples (v,C) from the BNC cor-
pus are provided to the learning scheme. The target
training value for the output of the system is 1 if v is
indeed in C and -1 otherwise. The hope is that the
neural network will be able to generalize to words
which are not in the piece of text but are likely to be
related to it.
In essence, this model is trained by minimizing
the weighted sum of the hinge loss function over
negative and positive couples, using stochastic Gra-
dient Descent (see (Keller and Bengio, 2005) for fur-
ther details). The small held out development set of
the substitution dataset was used to tune the hyper-
parameters of the model, maximizing average preci-
sion (see Section 4.1). For simplicity mlpA() and
mlpB() were reduced to Perceptrons. The output
size of mlpA() was set to 20, mlpB() to 100 and the
number of hidden units of mlpC() was set to 500.
There are a couple of important conceptual differ-
ences of the discriminative NNTR model compared
to the generative Bayesian model described above.
First, the relevancy of v to C in NNTR is inferred
in a more compact representation space of reduced
dimensionality, which may enable a higher degree
of generalization. Second, in NNTR we are able to
control the capacity of the model in terms of num-
ber of parameters, enabling better control to achieve
an optimal generalization level with respect to the
training data (avoiding over or under fitting).
4 Empirical Results
4.1 Evaluation Measures
We compare the lexical substitution scoring methods
using two evaluation measures, offering two differ-
ent perspectives of evaluation.
4.1.1 Accuracy
The first evaluation measure is motivated by simu-
lating a decision step of a subtitling system, in which
the best scoring lexical substitution is selected for
each given sentence. Such decision may correspond
to a situation in which each single substitution may
suffice to obtain the desired compression rate, or
might be part of a more complex decision mecha-
nism of the complete subtitling system. We thus
measure the resulting accuracy of subtitles created
by applying the best scoring substitution example
for every original sentence. This provides a macro
evaluation style since we obtain a single judgment
for each group of substitution examples that corre-
spond to one original sentence.
In our dataset 25.5% of the original sentences
have no correct substitution examples and for 15.5%
of the sentences all substitution examples were an-
notated as correct. Accordingly, the (macro aver-
aged) accuracy has a lower bound of 0.155 and up-
per bound of 0.745.
49
4.1.2 Average Precision
As a second evaluation measure we compare the
average precision of each method over all the exam-
ples from all original sentences pooled together (a
micro averaging approach). This measures the po-
tential of a scoring method to ensure high precision
for the high scoring examples and to filter out low-
scoring incorrect substitutions.
Average precision is a single figure measure com-
monly used to evaluate a system?s ranking ability
(Voorhees and Harman, 1999). It is equivalent to the
area under the uninterpolated recall-precision curve,
defined as follows:
average precision =
?N
i=1 P(i)T (i)?N
i=1
T (i)
P(i) =
?i
k=1
T (k)
i
(2)
where N is the number of examples in the test
set (797 in our case), T (i) is the gold annotation
(true=1, false=0) and i ranges over the examples
ranked by decreasing score. An average precision
of 1.0 means that the system assigned a higher score
to all true examples than to any false one (perfect
ranking). A lower bound of 0.26 on our test set cor-
responds to a system that ranks all false examples
above the true ones.
4.2 Results
Figure 1 shows the accuracy and average precision
results of the various models on our test set. The ran-
dom baseline and corresponding significance levels
were achieved by averaging multiple runs of a sys-
tem that assigned random scores. As can be seen in
the figures, the models? behavior seems to be con-
sistent in both evaluation measures.
Overall, the distributional similarity based
method (sim) performs much better than the
other methods. In particular, Lin?s similarity
also performs better than semcor, the other
context-independent model. Generally, the context
independent models perform better than the contex-
tual ones. Between the two contextual models, nntr
is superior to Bayes. In fact the Bayes model is not
significantly better than random scoring.
4.3 Analysis and Discussion
When analyzing the data we identified several rea-
sons why some of the WordNet substitutions were
judged as false. In some cases the source word as
appearing in the original sentence is not in a sense
for which it is a synonym of the target word. For ex-
ample, in many situations the word answer is in the
sense of a statement that is made in reply to a ques-
tion or request. In such cases, such as in example 2
from Table 1, answer can be successfully replaced
with reply yielding a substitution which conveys the
original meaning. However, in situations such as in
example 1 the word answer is in the sense of a gen-
eral solution and cannot be replaced with reply. This
is also the case in examples 4 and 5 in which subject
does not appear in the sense of topic or theme.
Having an inappropriate sense, however, is not the
only reason for incorrect substitutions. In example 8
approach appears in a sense which is synonymous
with attack and in example 9 problem appears in a
sense which is synonymous with a quite uncommon
use of the word job. Nevertheless, these substitu-
tions were judged as unacceptable since the desired
sense of the target word after the substitution is not
very clear from the context. In many other cases,
such as in example 7, though semantically correct,
the substitution was judged as incorrect due to stylis-
tic considerations.
Finally, there are cases, such as in example 6
in which the source word is part of a collocation
and cannot be replaced with semantically equivalent
words.
When analyzing the mistakes of the distributional
similarity method it seems as if many were not nec-
essarily due to the method itself but rather to imple-
mentation issues. The online source we used con-
tains only the top most similar words for any word.
In many cases substitutions were assigned a score of
zero since they were not listed among the top scoring
similar words in the database. Furthermore, the cor-
pus that was used for training the similarity scores
was news articles in American English spelling and
does not always supply good scores to words of
British spelling in our BBC dataset (e.g. analyse,
behavioural, etc.).
The similarity based method seems to perform
better than the SemCor based method since, as noted
above, even when the source word is in the appro-
priate sense it not necessarily substitutable with the
target. For this reason we hypothesize that apply-
ing Word Sense Disambiguation (WSD) methods to
50
Figure 1: Accuracy and Average Precision Results
classify the specificWordNet sense of the source and
target words may have only a limited impact on per-
formance.
Overall, context independent models seem to per-
form relatively well since many candidate synonyms
are a priori not substitutable. This demonstrates that
such models are able to filter out many quirky Word-
Net synonyms, such as problem and job.
Fitness to the sentence context seems to be a less
frequent factor and not that trivial to model. Local
context (adjacent words) seems to play more of a
role than the broader sentence context. However,
these two types of contexts were not distinguished in
the bag-of-words representations of the two contex-
tual methods that we examined. It will be interesting
to investigate in future research using different fea-
ture types for local and global context, as commonly
done for Word Sense Disambiguation (WSD). Yet,
it would still remain a challenging task to correctly
distinguish, for example, the contexts for which an-
swer is substitutable by reply (as in example 2) from
contexts in which it is not (as in example 1).
So far we have investigated separately the perfor-
mance of context independent and contextual mod-
els. In fact, the accuracy performance of the (con-
text independent) sim method is not that far from
the upper bound, and the analysis above indicated a
rather small potential for improvement by incorpo-
rating information from a contextual method. Yet,
there is still a substantial room for improvement in
the ranking quality of this model, as measured by av-
erage precision, and it is possible that a smart com-
bination with a high-quality contextual model would
yield better performance. In particular, we would
expect that a good contextual model will identify the
cases in which for potentially good synonyms pair,
the source word appears in a sense that is not substi-
tutable with the target, such as in examples 1, 4 and
5 in Table 1. Investigating better contextual models
and their optimal combination with context indepen-
dent models remains a topic for future research.
5 Conclusion
This paper investigated an isolated setting of the lex-
ical substitution task, which has typically been em-
bedded in larger systems and not evaluated directly.
The setting allowed us to analyze different types of
state of the art models and their behavior with re-
spect to characteristic sub-cases of the problem.
The major conclusion that seems to arise from
our experiments is the effectiveness of combining a
knowledge based thesaurus such as WordNet with
distributional statistical information such as (Lin,
1998), overcoming the known deficiencies of each
method alone. Furthermore, modeling the a pri-
ori substitution likelihood captures the majority of
cases in the evaluated setting, mostly because Word-
Net provides a rather noisy set of substitution candi-
dates. On the other hand, successfully incorporating
local and global contextual information, as similar
to WSD methods, remains a challenging task for fu-
ture research. Overall, scoring lexical substitutions
51
is an important component in many applications and
we expect that our findings are likely to be broadly
applicable.
References
[Budanitsky and Hirst2001] Alexander Budanitsky and
Graeme Hirst. 2001. Semantic distance in word-
net: An experimental, application-oriented evalua-
tion of five measures. In Workshop on WordNet and
Other Lexical Resources: Second Meeting of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 29?34.
[Burnard1995] Lou Burnard. 1995. Users Reference
Guide for the British National Corpus. Oxford Uni-
versity Computing Services, Oxford.
[Daelemans et al2004] Walter Daelemans, Anja Ho?thker,
and Erik Tjong Kim Sang. 2004. Automatic sen-
tence simplification for subtitling in dutch and english.
In Proceedings of the 4th International Conference
on Language Resources and Evaluation, pages 1045?
1048.
[Dagan et al2005] Ido Dagan, Oren Glickman, and
Bernardo Magnini. 2005. The pascal recognising tex-
tual entailment challenge. Proceedings of the PAS-
CAL Challenges Workshop on Recognising Textual
Entailment.
[Geffet and Dagan2005] Maayan Geffet and Ido Dagan.
2005. The distributional inclusion hypotheses and lex-
ical entailment. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Linguis-
tics (ACL?05), pages 107?114, Ann Arbor, Michigan,
June. Association for Computational Linguistics.
[Glickman et al2005] Oren Glickman, Ido Dagan, and
Moshe Koppel. 2005. A probabilistic classifica-
tion approach for lexical textual entailment. In AAAI,
pages 1050?1055.
[Grefenstette1998] Gregory Grefenstette. 1998. Produc-
ing Intelligent Telegraphic Text Reduction to Provide
an Audio Scanning Service for the Blind. pages 111?
117, Stanford, CA, March.
[Harris1968] Zelig Harris. 1968. Mathematical Struc-
tures of Language. New York: Wiley.
[Hori et al2002] Chiori Hori, Sadaoki Furui, RobMalkin,
Hua Yu, and Alex Waibel. 2002. Automatic
speech summarization applied to english broadcast
news speech. volume 1, pages 9?12.
[Jing and McKeown1999] Hongyan Jing and Kathleen R.
McKeown. 1999. The decomposition of human-
written summary sentences. In SIGIR ?99: Proceed-
ings of the 22nd annual international ACM SIGIR con-
ference on Research and development in information
retrieval, pages 129?136, New York, NY, USA. ACM
Press.
[Keller and Bengio2005] Mikaela Keller and Samy Ben-
gio. 2005. A neural network for text representation.
In Wodzisaw Duch, Janusz Kacprzyk, and Erkki Oja,
editors, Artificial Neural Networks: Biological Inspi-
rations ICANN 2005: 15th International Conference,
Warsaw, Poland, September 11-15, 2005. Proceedings,
Part II, volume 3697 / 2005 of Lecture Notes in Com-
puter Science, page p. 667. Springer-Verlag GmbH.
[Knight and Marcu2002] Kevin Knight and Daniel
Marcu. 2002. Summarization beyond sentence
extraction: a probabilistic approach to sentence
compression. Artif. Intell., 139(1):91?107.
[Landis and Koch1997] J. R. Landis and G. G. Koch.
1997. The measurements of observer agreement for
categorical data. Biometrics, 33:159?174.
[Lin1998] Dekang Lin. 1998. Automatic retrieval and
clustering of similar words. In Proceedings of the
17th international conference on Computational lin-
guistics, pages 768?774, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics.
[McCarthy et al2004] Diana McCarthy, Rob Koeling,
JulieWeeds, and John Carroll. 2004. Finding predom-
inant senses in untagged text. In ACL, pages 280?288,
Morristown, NJ, USA. Association for Computational
Linguistics.
[Miller et al1993] George A. Miller, Claudia Leacock,
Randee Tengi, and Ross T. Bunker. 1993. A semantic
concordance. In HLT ?93: Proceedings of the work-
shop on Human Language Technology, pages 303?
308, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
[Piperidis et al2004] Stelios Piperidis, Iason Demiros,
Prokopis Prokopidis, Peter Vanroose, Anja Ho?thker,
Walter Daelemans, Elsa Sklavounou, Manos Kon-
stantinou, and Yannis Karavidas. 2004. Multimodal
multilingual resources in the subtitling process. In
Proceedings of the 4th International Language Re-
sources and Evaluation Conference (LREC 2004), Lis-
bon.
[Voorhees and Harman1999] Ellen M. Voorhees and
Donna Harman. 1999. Overview of the seventh text
retrieval conference. In Proceedings of the Seventh
Text REtrieval Conference (TREC-7). NIST Special
Publication.
[Voorhees1994] Ellen M. Voorhees. 1994. Query expan-
sion using lexical-semantic relations. In SIGIR ?94:
Proceedings of the 17th annual international ACM SI-
GIR conference on Research and development in infor-
mation retrieval, pages 61?69, New York, NY, USA.
Springer-Verlag New York, Inc.
52
Proceedings of the Workshop on Language Technology for Cultural Heritage Data (LaTeCH 2007), pages 65?72,
Prague, 28 June 2007. c?2007 Association for Computational Linguistics
Cross Lingual and Semantic Retrieval for Cultural Heritage Appreciation
Idan Szpektor, Ido Dagan
Dept. of Computer Science
Bar Ilan University
szpekti@cs.biu.ac.il
Alon Lavie
Language Technologies Inst.
Carnegie Mellon University
alavie+@cs.cmu.edu
Danny Shacham, Shuly Wintner
Dept. of Computer Science
University of Haifa
shuly@cs.haifa.ac.il
Abstract
We describe a system which enhances the
experience of museum visits by providing
users with language-technology-based in-
formation retrieval capabilities. The sys-
tem consists of a cross-lingual search en-
gine, augmented by state of the art semantic
expansion technology, specifically designed
for the domain of the museum (history and
archaeology of Israel). We discuss the tech-
nology incorporated in the system, its adap-
tation to the specific domain and its contri-
bution to cultural heritage appreciation.
1 Introduction
Museum visits are enriching experiences: they pro-
vide stimulation to the senses, and through them to
the mind. But the experience does not have to end
when the visit ends: further exploration of the ar-
tifacts and their influence on the visitor is possible
after the visit, either on location or elsewhere. One
common means of exploration is Information Re-
trieval (IR) via a Search Engine. For example, a mu-
seum could implement a search engine over a col-
lection of documents relating to the topics exhibited
in the museum.
However, such document collections are usually
much smaller than general collections, in particular
the World Wide Web. Consequently, phenomena in-
herent to natural languages may severely hamper the
performance of human language technology when
applied to small collections. One such phenomenon
is the semantic variability of natural languages, the
ability to express a specific meaning in many dif-
ferent ways. For example, the expression ?Archae-
ologists found a new tomb? can be expressed also
by ?Archaeologists discovered a tomb? or ?A sar-
cophagus was dug up by Egyptian Researchers?. On
top of monolingual variability, the same information
can also be expressed in different languages. Ignor-
ing natural language variability may result in lower
recall of relevant documents for a given query, espe-
cially in small document collections.
This paper describes a system that attempts to
cope with semantic variability through the use of
state of the art human language technology. The
system provides both semantic expansion and cross
lingual IR (and presentation of information) in the
domain of archaeology and history of Israel. It
was specifically developed for the Hecht Museum
in Haifa, Israel, which contains a small but unique
collection of artifacts in this domain. The system
provides different users with different capabilities,
bridging over language divides; it addresses seman-
tic variation in novel ways; and it thereby comple-
ments the visit to the museum with long-lasting in-
stillation of information.
The main component of the system is a domain-
specific search engine that enables users to specify
queries and retrieve information pertaining to the do-
main of the museum. The engine is enriched by lin-
guistic capabilities which embody an array of means
for addressing semantic variation. Queries are ex-
panded using two main techniques: semantic expan-
sion based on textual entailment; and cross-lingual
expansion based on translation of Hebrew queries
to English and vice versa. Retrieved documents are
presented as links with associated snippets; the sys-
tem also translates snippets from Hebrew to English.
The main contribution of this work is, of course,
the system itself, which was recently demonstrated
65
successfully at the museum and which we believe
could be useful to a variety of museum visitor types,
from children to experts. For example, the system
provides Hebrew speakers access to English doc-
uments pertaining to the domain of the museum,
and vice versa, thereby expanding the availability
of multilingual material to museum visitors. More
generally, it is an instance of adaptation of state of
the art human language technology to the domain
of cultural heritage appreciation, demonstrating how
general resources and tools are adapted to a specific
domain, thereby improving their accuracy and us-
ability. Finally, it provides a test-bed for evaluating
the contribution of language technology in general,
as well as specific components and resources, to a
large-scale natural language processing system.
2 Background and Motivation
Internet search is hampered by the complexity of
natural languages. The two main characteristics of
this complexity are ambiguity and variability: the
former refers to the fact that a given text can be
interpreted in more than one way; the latter indi-
cates that the same meaning can be linguistically ex-
pressed in several ways. The two phenomena make
simple search techniques too weak for unsophisti-
cated users, as existing search engines perform only
direct keyword matching, with very limited linguis-
tic processing of the texts they retrieve.
Specifically, IR systems that do not address the
variability in languages may suffer from lower re-
call, especially in restricted domains and small doc-
ument locations. We next describe two prominent
types of variability that we think should be ad-
dressed in IR systems.
2.1 Textual Entailment and Entailment Rules
In many NLP applications, such as Question An-
swering (QA), Information Extraction (IE) and In-
formation Retrieval (IR), it is crucial to recognize
that a specific target meaning can be inferred from
different text variants. For example, a QA system
needs to induce that ?Mendelssohn wrote inciden-
tal music? can be inferred from ?Mendelssohn com-
posed incidental music? in order to answer the ques-
tion ?Who wrote incidental music??. This type of
reasoning has been identified as a core semantic in-
ference task by the generic textual entailment frame-
work (Dagan et al, 2006; Bar-Haim et al, 2006).
The typical way to address variability in IR is to
use lexical query expansion (Lytinen et al, 2000;
Zukerman and Raskutti, 2002). However, there are
variability patterns that cannot be described using
just constant phrase to phrase entailment. Another
important type of knowledge representation is en-
tailment rules and paraphrases. An entailment rule
is a directional relation between two templates, text
patterns with variables, e.g., ?X compose Y ?
X write Y ?. The left hand side is assumed to en-
tail the right hand side in certain contexts, under
the same variable instantiation. Paraphrases can be
viewed as bidirectional entailment rules. Such rules
capture basic inferences in the language, and are
used as building blocks for more complex entail-
ment inference. For example, given the above en-
tailment rule, a QA system can identify the answer
?Mendelssohn? in the above example. This need
sparked intensive research on automatic acquisition
of paraphrase and entailment rules.
Although knowledge-bases of entailment-rules
and paraphrases learned by acquisition algorithms
were used in other NLP applications, such as QA
(Lin and Pantel, 2001; Ravichandran and Hovy,
2002) and IE (Sudo et al, 2003; Romano et al,
2006), to the best of our knowledge the output of
such algorithms was never applied to IR before.
2.2 Cross Lingual Information Retrieval
The difficulties caused by variability are amplified
when the user is not a native speaker of the language
in which the retrieved texts are written. For exam-
ple, while most Israelis can read English documents,
fewer are comfortable with the specification of Eng-
lish queries. In a museum setting, some visitors may
be able to read Hebrew documents but still be rel-
atively poor at searching for them. Other visitors
may be unable to read Hebrew texts, but still benefit
from non-textual information that are contained in
Hebrew documents (e.g., pictures, maps, audio and
video files, external links, etc.)
This problem is addressed by the paradigm of
Cross-Lingual Information Retrieval (CLIR). This
paradigm has become a very active research area
in recent years, addressing the needs of multilingual
and non-English speaking communities, such as the
66
European Union, East-Asian nations and Spanish
speaking communities in the US (Hull and Grefen-
stette, 1996; Ballesteros and Croft, 1997; Carbonell
et al, 1997). The common approach for CLIR is
to translate a query in a source language to another
target language and then issue the translated query
to retrieve target language documents. As explained
above, CLIR research has to address various generic
problems caused by the variability and ambiguity of
natural languages, as well as specific problems re-
lated to the particular languages being addressed.
3 Coping with Semantic Variability in IR
We describe a search engine that is capable of per-
forming: (a) semantic English information retrieval;
and (b) cross-lingual (Hebrew-English and English-
Hebrew) information retrieval, allowing users to
pose queries in either of the two languages and re-
trieve documents in both. This is achieved by two
sub-processes of the search engine: first, the en-
gine performs shallow semantic linguistic inference
and supports the retrieval of documents which con-
tain phrases that imply the meaning of the translated
query, even when no exact match of the translated
keywords is found. This is enabled by automatic ac-
quisition of semantic variability patterns that are fre-
quent in the language, which extend traditional lexi-
cal query expansion techniques. Second, the engine
translates the original or expanded query to the tar-
get language, based on several linguistic processes
and a machine readable bilingual dictionary. The re-
sult is a semantic expansion of a given query to a va-
riety of alternative wordings in which an answer to
this query may be expressed in the target language
of the retrieved documents.
These enhancements are facilitated via a speci-
fication of the domain. As our system is specifi-
cally designed to work in the domain of the history
and archaeology, we could focus our attention on re-
sources and tools that are dedicated to this domain.
Thus, for example, lexicons and dictionaries, whose
preparation is always costly and time consuming,
were developed with the specific domain in mind;
and textual entailment and paraphrase patterns were
extracted for the specific domain. While the result-
ing system is focused on visiting the Hecht Museum,
the methodology which we used and discuss here
can be adapted to other areas of cultural heritage, as
well as to other narrow domains, in the same way.
3.1 Setting Up a Basic Retrieval Application
We created a basic retrieval system in two steps:
first, we collected relevant documents; then, we im-
plemented a search engine over the collected docu-
ments.
In order to construct a local corpus, an archae-
ology expert searched the Web for relevant sites
and pages. We then downloaded all the documents
linked from those pages using a crawler. The expert
looked for documents in both English and Hebrew.
In total, we collected a non-comparable bilingual
corpus for Archaeology containing several thousand
documents in English and Hebrew.
We implemented our enhanced retrieval modules
on top of the basic Jakarta Lucene indexing and
search engine1. All documents were indexed using
Lucene, but instead of inflected words, we indexed
the lemma of each word (see detailed description of
our Hebrew lemmatization in Section 3.3). In order
to match the indexed terms, query terms (either He-
brew or English) were also lemmatized before the
index was searched, in a manner similar to lemma-
tizing the documents.
3.2 Query Expansion Using Entailment Rules
As described in Section 2.1, entailment rules had not
been used as a knowledge resource for expanding IR
queries, prior to our work. In this paper we use this
resource instead of the typical lexical expansion in
order to test its benefit. Most entailment rules cap-
ture relations between different predicates. We thus
focus on documents retrieved for queries that con-
tain a predicate over one or two entities, which we
term here Relational IR. We would like to retrieve
only documents that describe an occurrence of that
predicate, but possibly in words different than the
ones used in the query. In this section we describe
in detail how we learn entailment rules and how we
apply them in query expansion.
Automatically Learning Entailment Rules from
the Web Many algorithms for automatically learn-
ing paraphrases and entailment rules have been
explored in recent years (Lin and Pantel, 2001;
1http://jakarta.apache.org/lucene/docs/index.html
67
Ravichandran and Hovy, 2002; Shinyama et al,
2002; Barzilay and Lee, 2003; Sudo et al, 2003;
Szpektor et al, 2004; Satoshi, 2005). In this pa-
per we use TEASE (Szpektor et al, 2004), a state-
of-the-art unsupervised acquisition algorithm for
lexical-syntactic entailment rules.
TEASE acquires entailment relations for a given
input template from the Web. It first retrieves from
the Web sentences that match the input template.
From these sentences it extracts the variable instan-
tiations, termed anchor-sets, which are identified as
being characteristic for the input template based on
statistical criteria.
Next, TEASE retrieves from the Web sentences
that contain the extracted anchor-sets. The retrieved
sentences are parsed and the anchors found in each
sentence are replaced with their corresponding vari-
ables. Finally, from this retrieved corpus of parsed
sentences, templates that are assumed to entail or
be entailed by the input template are learned. The
learned templates are ranked by the number of oc-
currences they were learned from.
Entailment Rules for Domain Specific Query Ex-
pansion Our goal is to use the knowledge-base of
entailment rules learned by TEASE in order to per-
form query expansion. The two subtasks that arise
are: (a) acquiring an appropriate knowledge-base
of rules; and (b) expanding a query given such a
knowledge-base.
TEASE learns entailment rules for a given input
template. As our document collection is domain
specific, a list of such relevant input templates can
be prepared. In our case, we used an archaeology
expert to generate a list of verbs and verb phrases
that relate to archaeology, such as: ?excavate?, ?in-
vade?, ?build?, ?reconstruct?, ?grow? and ?be located
in?. We then executed TEASE on each of the tem-
plates representing these verbs in order to learn from
the Web rules in which the input templates partici-
pate. An example for such rules is presented in Ta-
ble 1. We learned approximately 3900 rules for 80
input templates.
Since TEASE learns lexical-syntactic rules, we
need a syntactic representation of the query. We
parse each query using the Minipar dependency
parser (Lin, 1998). We next try to match the left
hand side template of every rule in the learned
knowledge-base. Since TEASE does not identify
the direction of the relation learned between two
templates, we try both directional rules that are in-
duced from a learned relation. Whenever a match
is found, a new query is generated, in which the
constant terms of the matched left hand side tem-
plate are replaced with the constant terms of the right
hand side template. For example, given the query
?excavations of Jerusalem by archaeologists? and a
learned rule ?excavation of Y by X ? X dig in Y ?,
a new query is generated, containing the terms ?ar-
chaeologists dig in Jerusalem?. Finally, we retrieve
all the documents that contain all the terms of at least
one of the expanded queries (including the original
query). The basic search engine provides a score for
each document. We re-score each document as the
sum of scores it obtained from the different queries
that it matched. Figure 1 shows an example of our
query expansion, where the first retrieved documents
do not contain the words used to describe the predi-
cate in the query, but other ways to describe it.
All the templates learned by TEASE contain two
variables, and thus the rules that are learned can only
be applied to queries that contain predicates over
two terms. In order to broaden the coverage of the
learned rules, we automatically generate also all the
partial templates of a learned template. These are
templates that contain just one of variables in the
original template. We then generate rules between
these partial templates that correspond to the origi-
nal rules. With partial templates/rules, expansion for
the query in Figure 1 becomes possible.
3.3 Cross-lingual IR
Until very recently, linguistic resources for Hebrew
were few and far between (Wintner, 2004). The last
few years, however, have seen a proliferation of re-
sources and tools for this language. In this work we
utilize a relatively large-scale lexicon of over 22,000
entries (Itai et al, 2006); a finite-state based mor-
phological analyzer of Hebrew that is directly linked
to the lexicon (Yona and Wintner, 2007); a medium-
size bilingual dictionary of some 24,000 word pairs;
and a rudimentary Hebrew to English machine trans-
lation system (Lavie et al, 2004). All these re-
sources had to be adapted to the domain of the Hecht
museum.
Cross-lingual language technology is utilized in
68
Figure 1: Semantic expansion example. Note that the expanded queries that were generated in the first two
retrieved texts (listed under ?matched query?) do not contain the original query.
three different components of the system: Hebrew
documents are morphologically processed to pro-
vide better indexing; query terms in English are
translated to Hebrew and vice versa; and Hebrew
snippets are translated to English. We discuss each
of these components in this section.
Linguistically-aware indexing The correct level
of indexing for morphologically-rich language has
been a matter of some debate in the information re-
trieval literature. When Arabic is concerned, Dar-
wish and Oard (2002) conclude that ?Character n-
grams or lightly stemmed words were found to
typically yield near-optimal retrieval effectiveness?.
Since Hebrew is even more morphologically (and
orthographically) ambiguous than Arabic, and espe-
cially in light of the various prefix particles which
can be attached to Hebrew words, we opted for full
morphological analysis of Hebrew documents be-
fore they are indexed, followed by indexing on the
lexeme.
We use the HAMSAH morphological analyzer
(Yona and Wintner, 2007), which was recently re-
written in Java and is therefore more portable and
efficient (Wintner, 2007). We processed the entire
domain specific corpus described above and used
the resulting lexemes to index documents. This pre-
processing brought to the foreground several omis-
sions of the analyzer, mostly due to domain-specific
terms missing in the lexicon. We selected the one
thousand most frequent words with no morphologi-
cal analysis and added their lexemes to the lexicon.
While we do not have quantitative evaluation met-
rics, the coverage of the system improved in a very
evident way.
Query translation When users submit a query in
one language they are provided with the option to re-
quest a translation of the query to the other language,
thereby retrieving documents in the other language.
The motivation behind this capability is that users
who may be able to read documents in a language
may find the specification of queries in that language
too challenging; also, retrieving documents in a for-
eign language may be useful due to the non-textual
information in the retrieved documents, especially in
a museum environment.
In order to support cross-lingual query specifica-
tion we capitalized on a medium-size bilingual dic-
tionary that was already used for Hebrew to Eng-
lish machine translation. Since the coverage of the
dictionary was rather limited, and many domain-
specific items were missing, we chose the one thou-
sand most frequent lexemes which had no transla-
69
Input Template Learned Template
X excavate Y X discover Y , X find Y ,
X uncover Y , X examine Y ,
X unearth Y , X explore Y
X construct Y X build Y , X develop Y ,
X create Y , X establish Y
X contribute to Y X cause Y , X linked to Y ,
X involve in Y
date X to Y X built in Y , X began in Y ,
X go back to Y
X cover Y X bury Y ,
X provide coverage for Y
X invade Y X occupy Y , X attack Y ,
X raid Y , X move into Y
X restore Y X protect Y , X preserve Y ,
X save Y , X conserve Y
Table 1: Examples for correct templates that were
learned by TEASE for input templates.
tions and translated them manually, augmenting the
lexicon with missing Hebrew lexemes where neces-
sary and expanding the bilingual dictionary to cover
this domain.
In order to translate query terms we use the He-
brew English dictionary also as an English-Hebrew
dictionary. While this is known to be sub-optimal,
our current results support such an adaptation in lieu
of dedicated directional bilingual dictionaries.
Translating a query from one language to another
may introduce ambiguity where none exists. For
example, the query term spinh ?vessel? is unam-
biguous in Hebrew, but once translated into English
will result in retrieving documents on both senses
of the English word. Usually, this problem is over-
come since users tend to specify multi-term queries,
and the terms disambiguate each other. However,
a more systematic solution can be offered since we
have access to semantic expansion capabilities (in a
single language). That is, expanding the query in
the source language will result in more query terms
which, when translated, are more likely to disam-
biguate the context. We leave such an extension for
future work.
Snippet translation When Hebrew documents are
retrieved, we augment the (Hebrew) snippet which
the system produces by an English translation. We
use an extended, improved version of a rudimentary
Hebrew to English MT system developed by Lavie
et al (2004). Extensions include an improved mor-
phological analysis of the input, an extended bilin-
gual dictionary and a revised set of transfer rules,
as well as a more modern transfer engine and a
much larger language model for generating the tar-
get (English) sentences.
The MT system is transfer based: it performs lin-
guistic pre-processing of the source language (in our
case, morphological analysis) and post-processing
of the target (generation of English word forms), and
uses a small set of transfer rules to translate local
structures from the source to the target and create
translation hypotheses, which are stored in a lattice.
A statistical language model is used to decode the
lattice and select the best hypotheses.
The benefit of this architecture is that domain spe-
cific adaptation of the system is relatively easy, and
does not require a domain specific parallel corpus
(which we do not have). The system has access
to our domain-specific lexicon and bilingual dictio-
nary, and we even refined some transfer rules due to
peculiarities of the domain. One advantage of the
transfer-based approach is that it enables us to treat
out-of-lexicon items in a unique way. We consider
such items proper names, and transfer rules process
them as such. As an example, Figure 2 depicts the
translation of a Hebrew snippet meaning A jar from
the early bronze period with seashells from the Nile.
The word nilws ?Nile? is missing from the lexicon,
but this does not prevent the system from producing
a legible translation, using the transliterated form
where an English equivalent is unavailable.
4 Conclusions
We described a system for cross-lingual and
semantically-enhanced retrieval of information in
the cultural heritage domain, obtained by adapting
existing state-of-the-art tools and resources to the
domain. The system enhances the experience of mu-
seum visits, using language technology as a vehi-
cle for long-lasting instillation of information. Due
to the novelty of this application and the dearth of
available multilingual annotated resources in this
domain, we are unable to provide a robust, quan-
70
Figure 2: Translation example
Query Without Expansion With Expansion
Relevant Total Relevant Total
in Top 10 Retrieved in Top 10 Retrieved
discovering boats 2 2 5 86
growing vineyards 0 0 6 8
Persian invasions 5 5 8 22
excavations of the Byzantine period 10 37 10 100
restoring mosaics 0 0 3 69
Table 2: Analysis of the number of relevant documents out of the top 10 and the total number of retrieved
documents (up to 100) for a sample of queries.
titative evaluation of the approach. A preliminary
analysis of a sample of queries is presented in Ta-
ble 2. It illustrates the potential of expansion for
document collections of narrow domain. In what
follows we provide some qualitative impressions.
We observed that the system was able to learn
many expansion rules that cannot be induced from
manually constructed lexical resources, such as the-
sauri or WordNet (Fellbaum, 1998). This is espe-
cially true for rules that are specific for a narrow do-
main, e.g. ?X restore Y ? X preserve Y ?. Fur-
thermore, the system learned lexical syntactic rules
that cannot be expressed by a mere lexical substitu-
tion, but include also a syntactic transformation. For
example, ?date X to Y ? X go back to Y ?.
In addition, since rules are acquired by searching
the Web, they are not necessarily restricted to learn-
ing from the target domain, but can be learned from
similar terminology in other domains. For example,
the rule ?X discover Y ? X find Y ? was learned
from contexts such as {X=?astronomers? ;Y =?new
planets?} and {X=?zoologists? ;Y =?new species?}.
The quality of the rules that were automatically
acquired is mediocre. We found that although many
rules were useful for expansion, they had to be
manually filtered in order to retain only rules that
achieved high precision.
Finally, we note that applying semantic query ex-
pansion (using entailment rules), followed by Eng-
lish to Hebrew query translation, results in query ex-
pansion for Hebrew using techniques that were so
far applicable only to resource-rich languages, such
as English.
Acknowledgements
This research was supported by the Israel Internet
Association; by THE ISRAEL SCIENCE FOUN-
DATION (grant No. 137/06 and grant No. 1095/05);
by the Caesarea Rothschild Institute for Interdisci-
plinary Application of Computer Science at the Uni-
versity of Haifa; by the ITC-irst/University of Haifa
collaboration; and by the US National Science Foun-
dation (grants IIS-0121631, IIS-0534217, and the
Office of International Science and Engineering).
71
We wish to thank the Hebrew Knowledge Center
at the Technion for providing resources for Hebrew.
We are grateful to Oliviero Stock, Martin Golumbic,
Alon Itai, Dalia Bojan, Erik Peterson, Nurit Mel-
nik, Yaniv Eytani and Noam Ordan for their help
and support.
References
Lisa Ballesteros and W. Bruce Croft. 1997. Phrasal
translation and query expansion techniques for cross-
language information retrieval. In ACM SIGIR Con-
ference on Research and Development in Information
Retrieval, pages 84?91.
Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo
Giampiccolo, Bernardo Magnini, and Idan Szpektor.
2006. The second pascal recognising textual entail-
ment challenge. In Second PASCAL Challenge Work-
shop for Recognizing Textual Entailment.
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: An unsupervised approach using multiple-
sequence alignment. In Proceedings of HLT-NAACL.
Jaime G. Carbonell, Yiming Yang, Robert E. Frederk-
ing, Ralf D. Brown, Yibing Geng, and Danny Lee.
1997. Translingual information retrieval: A compar-
ative evaluation. In IJCAI (1), pages 708?715.
Ido Dagan, Oren Glickman, and Bernardo. Magnini.
2006. The pascal recognising textual entailment chal-
lenge. In Lecture Notes in Computer Science, Volume
3944, volume 3944, pages 177?190.
Kareem Darwish and Douglas W. Oard. 2002. Term se-
lection for searching printed Arabic. In SIGIR ?02:
Proceedings of the 25th annual international ACM SI-
GIR conference on Research and development in in-
formation retrieval, pages 261?268, New York, NY,
USA. ACM Press.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. Language, Speech and Com-
munication. MIT Press.
D. A. Hull and G. Grefenstette. 1996. Querying across
languages. a dictionary-based approach to multilingual
information retrieval. In Proceedings of the 19th ACM
SIGIR Conference, pages 49?57.
Alon Itai, Shuly Wintner, and Shlomo Yona. 2006. A
computational lexicon of contemporary Hebrew. In
Proceedings of The fifth international conference on
Language Resources and Evaluation (LREC-2006).
Alon Lavie, Shuly Wintner, Yaniv Eytani, Erik Peterson,
and Katharina Probst. 2004. Rapid prototyping of a
transfer-based Hebrew-to-English machine translation
system. In Proceedings of TMI-2004: The 10th Inter-
national Conference on Theoretical and Methodolog-
ical Issues in Machine Translation, Baltimore, MD,
October.
Dekang Lin and Patrick Pantel. 2001. Discovery of in-
ference rules for question answering. In Natural Lan-
guage Engineering, volume 7(4), pages 343?360.
Dekang Lin. 1998. Dependency-based evaluation of
minipar. In Proceedings of the Workshop on Evalu-
ation of Parsing Systems at LREC.
S. Lytinen, N. Tomuro, and T. Repede. 2000. The use of
wordnet sense tagging in faqfinder. In Proceedings of
the AAAI00 Workshop on AI and Web Search.
Deepak Ravichandran and Eduard Hovy. 2002. Learning
surface text patterns for a question answering system.
In Proceedings of ACL.
Lorenza Romano, Milen Kouylekov, Idan Szpektor, Ido
Dagan, and Alberto Lavelli. 2006. Investigating a
generic paraphrase-based approach for relation extrac-
tion. In Proceedings of EACL.
Sekine Satoshi. 2005. Automatic paraphrase discovery
based on context and keywords between ne pairs. In
Proceedings of IWP.
Yusuke Shinyama, Satoshi Sekine, Sudo Kiyoshi, and
Ralph Grishman. 2002. Automatic paraphrase acqui-
sition from news articles. In Proceedings of HLT.
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.
2003. An improved extraction pattern representation
model for automatic ie pattern acquisition. In Pro-
ceedings of ACL.
Idan Szpektor, Hristo Tanev, Ido Dagan, and Bonaven-
tura Coppola. 2004. Scaling web-based acquisition of
entailment relations. In Proceedings of EMNLP.
Shuly Wintner. 2004. Hebrew computational linguis-
tics: Past and future. Artificial Intelligence Review,
21(2):113?138.
Shuly Wintner. 2007. Finite-state technology as a pro-
gramming environment. In Alexander Gelbukh, edi-
tor, Proceedings of the Conference on Computational
Linguistics and Intelligent Text Processing (CICLing-
2007), volume 4394 of Lecture Notes in Computer Sci-
ence, pages 97?106, Berlin and Heidelberg, February.
Springer.
Shlomo Yona and Shuly Wintner. 2007. A finite-state
morphological grammar of Hebrew. Natural Lan-
guage Engineering. To appear.
Ingrid Zukerman and Bhavani Raskutti. 2002. Lexical
query paraphrasing for document retrieval. In Pro-
ceedings of ACL.
72
Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 1?9,
Prague, June 2007. c?2007 Association for Computational Linguistics
The Third PASCAL Recognizing Textual Entailment Challenge 
 
Danilo Giampiccolo 
CELCT 
Via alla Cascata 56/c 
38100 POVO TN 
giampiccolo@celct.it 
Bernardo Magnini 
FBK-ITC 
Via Sommarive 18, 
38100 Povo TN 
magnini@itc.it  
Ido Dagan 
Computer Science Department 
Bar-Ilan University 
Ramat Gan 52900, Israel 
dagan@macs.biu.ac.il 
Bill Dolan 
Microsoft Research 
Redmond, WA, 98052, USA 
billdol@microsoft.com 
Abstract 
This paper presents the Third PASCAL 
Recognising Textual Entailment Chal-
lenge (RTE-3), providing an overview of 
the dataset creating methodology and the 
submitted systems. In creating this 
year?s dataset, a number of longer texts 
were introduced to make the challenge 
more oriented to realistic scenarios. Ad-
ditionally, a pool of resources was of-
fered so that the participants could share 
common tools. A pilot task was also set 
up, aimed at differentiating unknown en-
tailments from identified contradictions 
and providing justifications for overall 
system decisions. 26 participants submit-
ted 44 runs, using different approaches 
and generally presenting new entailment 
models and achieving higher scores than 
in the previous challenges. 
1.1 The RTE challenges 
 
The goal of the RTE challenges has been to cre-
ate a benchmark task dedicated to textual en-
tailment ? recognizing that the meaning of one 
text is entailed, i.e. can be inferred, by another1. 
In the recent years, this task has raised great in-
terest since applied semantic inference concerns 
many practical Natural Language Processing 
(NLP) applications, such as Question Answering 
(QA), Information Extraction (IE), Summariza-
tion, Machine Translation and Paraphrasing, and 
certain types of queries in Information Retrieval 
(IR). More specifically, the RTE challenges 
have aimed to focus research and evaluation on 
this common underlying semantic inference task 
and separate it from other problems that differ-
ent NLP applications need to handle. For exam-
ple, in addition to textual entailment, QA sys-
tems need to handle issues such as answer re-
trieval and question type recognition.  
By separating out the general problem of tex-
tual entailment from these task-specific prob-
lems, progress on semantic inference for many 
application areas can be promoted. Hopefully, 
research on textual entailment will finally lead to 
the development of entailment ?engines?, which 
can be used as a standard module in many appli-
cations (similar to the role of part-of-speech tag-
gers and syntactic parsers in current NLP appli-
cations). 
In the following sections, a detailed descrip-
tion of RTE-3 is presented. After a quick review 
                                                 
1
 The task was first defined by Dagan and Glickman 
(2004). 
1
of the previous challenges (1.2), section 2 de-
scribes the preparation of the dataset. In section 
3 the evaluation process and the results are pre-
sented, together with an analysis of the perform-
ance of the participating systems. 
1.2  The First and Second RTE Challenges 
 
The first RTE challenge2 aimed to provide the 
NLP community with a new benchmark to test 
progress in recognizing textual entailment, and 
to compare the achievements of different groups. 
This goal proved to be of great interest, and the 
community's response encouraged the gradual 
expansion of the scope of the original task. 
The Second RTE challenge3 built on the suc-
cess of the first, with 23 groups from around the 
world (as compared to 17 for the first challenge) 
submitting the results of their systems. Repre-
sentatives of participating groups presented their 
work at the PASCAL Challenges Workshop in 
April 2006 in Venice, Italy. The event was suc-
cessful and the number of participants and their 
contributions to the discussion demonstrated that 
Textual Entailment is a quickly growing field of 
NLP research. In addition, the workshops 
spawned an impressive number of publications 
in major conferences, with more work in pro-
gress. Another encouraging sign of the growing 
interest in the RTE challenge was represented by 
the increase in the number of downloads of the 
challenge datasets, with about 150 registered 
downloads for the RTE-2 development set. 
1.3 The Third Challenge 
 
RTE-3 followed the same basic structure of the 
previous campaigns, in order to facilitate the 
participation of newcomers and to allow "veter-
ans" to assess the improvements of their systems 
in a comparable test exercise. Nevertheless, 
some innovations were introduced, on the one 
hand to make the challenge more stimulating 
and, on the other, to encourage collaboration 
between system developers. In particular, a lim-
ited number of longer texts, i.e. up to a para-
graph in length, were incorporated in order to 
move toward more comprehensive scenarios, 
                                                 
2
 http://www.pascal-network.org/Challenges/RTE/. 
3
 http://www.pascal-network.org/Challenges/RTE2./ 
which incorporate the need for discourse analy-
sis. However, the majority of examples re-
mained similar to those in the previous chal-
lenges, providing pairs with relatively short 
texts.  
Another innovation was represented by a re-
source pool4, where contributors had the possi-
bility to share the resources they used. In fact, 
one of the key conclusions at the second RTE 
Challenge Workshop was that entailment model-
ing requires vast knowledge resources that cor-
respond to different types of entailment reason-
ing. Moreover, entailment systems also utilize 
general NLP tools such as POS taggers, parsers 
and named-entity recognizers, sometimes posing 
specialized requirements to such tools. In re-
sponse to these demands, the RTE Resource 
Pool was built, which may serve as a portal and 
forum for publicizing and tracking resources, 
and reporting on their use.  
In addition, an optional pilot task, called "Ex-
tending the Evaluation of Inferences from Texts" 
was set up by the US National Institute of Stan-
dards and Technology (NIST), in order to ex-
plore two other sub-tasks closely related to tex-
tual entailment: differentiating unknown entail-
ments from identified contradictions and provid-
ing justifications for system decisions. In the 
first sub-task, the idea was to drive systems to 
make more precise informational distinctions, 
taking a three-way decision between "YES", 
"NO" and "UNKNOWN?, so that a hypothesis 
being unknown on the basis of a text would be 
distinguished from a hypothesis being shown 
false/contradicted by a text. As for the other sub-
task, the goal for providing justifications for de-
cisions was to explore how eventual users of 
tools incorporating entailment can be made to 
understand how decisions were reached by a 
system, as users are unlikely to trust a system 
that gives no explanation for its decisions. The 
pilot task exploited the existing RTE-3 Chal-
lenge infrastructure and evaluation process by 
using the same test set, while utilizing human 
assessments for the new sub-tasks. 
                                                 
4 http://aclweb.org/aclwiki/index.php?title=Textual_Entail 
ment_Resource_Pool. 
2
 Table 1: Some examples taken from the Development Set. 
 
2 The RTE-3 Dataset 
2.1 Overview 
 
The textual entailment recognition task required the 
participating systems to decide, given two text 
snippets t and h, whether t entails h. Textual en-
tailment is defined as a directional relation between 
two text fragments, called text (t, the entailing 
text), and hypothesis (h, the entailed text), so that a 
human being, with common understanding of lan-
guage and common background knowledge, can 
infer that h is most likely true on the basis of the 
content of t. 
As in the previous challenges, the RTE-3 dataset 
consisted of 1600 text-hypothesis pairs, equally 
divided into a development set and a test set. While 
the length of the hypotheses (h) was  the same as in 
the past datasets, a certain number of texts (t) were 
longer than in previous datasets, up to a paragraph. 
The longer texts were marked as L, after being se-
lected automatically when exceeding 270 bytes. In 
the test set they were about 17% of the total.  
As in RTE-2, four applications ? namely IE, IR, 
QA and SUM ? were considered as settings or con-
texts for the pairs generation (see 2.2 for a detailed 
description). 200 pairs were selected for each ap-
plication in each dataset. Although the datasets 
were supposed to be perfectly balanced, the num-
ber of negative examples were slightly higher in 
both development and test sets (51.50% and 
51.25% respectively; this was unintentional). Posi-
tive entailment examples, where t entailed h, were 
annotated YES; the negative ones, where entailment 
did not hold, NO. Each pair was annotated with its 
TASK TEXT HYPOTHESIS ENTAILMENT 
IE At the same time the Italian digital rights group, Elec-
tronic Frontiers Italy, has asked the nation's government 
to investigate Sony over its use of anti-piracy software. 
Italy's govern-
ment investigates 
Sony. 
NO 
IE Parviz Davudi was representing Iran at a meeting of the 
Shanghai Co-operation Organisation (SCO), the fledg-
ling association that binds Russia, China and four for-
mer Soviet republics of central Asia together to fight 
terrorism 
China is a mem-
ber of SCO. 
YES 
IR Between March and June, scientific observers say, up to 
300,000 seals are killed. In Canada, seal-hunting means 
jobs, but opponents say it is vicious and endangers the 
species, also threatened by global warming 
Hunting endan-
gers seal species. 
YES 
IR The Italian parliament may approve a draft law allow-
ing descendants of the exiled royal family to return 
home. The family was banished after the Second World 
War because of the King's collusion with the fascist 
regime, but moves were introduced this year to allow 
their return. 
Italian royal fam-
ily returns home. 
NO 
QA Aeschylus is often called the father of Greek tragedy; 
he wrote the earliest complete plays which survive from 
ancient Greece. He is known to have written more than 
90 plays, though only seven survive. The most famous 
of these are the trilogy known as Orestia. Also well-
known are The Persians and Prometheus Bound. 
"The Persians" 
was written by 
Aeschylus. 
YES 
SUM A Pentagon committee and the congressionally char-
tered Iraq Study Group have been preparing reports for 
Bush, and Iran has asked the presidents of Iraq and 
Syria to meet in Tehran. 
Bush will meet 
the presidents of 
Iraq and Syria in 
Tehran. 
NO 
3
related task (IE/IR/QA/SUM) and entailment 
judgment (YES/NO, obviously released only in the 
development set). Table 1 shows some examples 
taken from the development set. 
The examples in the dataset were based mostly 
on outputs (both correct and incorrect) of Web-
based systems. In order to avoid copyright prob-
lems, input data was limited to either what had al-
ready been publicly released by official competi-
tions or else was drawn from freely available 
sources such as WikiNews and Wikipedia. 
In choosing the pairs, the following judgment 
criteria and guidelines were considered: 
 
? As entailment is a directional relation, the 
hypothesis must be entailed by the given 
text, but the text need not be entailed by 
the hypothesis. 
? The hypothesis must be fully entailed by 
the text. Judgment must be NO if the hy-
pothesis includes parts that cannot be in-
ferred from the text. 
? Cases in which inference is very probable 
(but not completely certain) were judged as 
YES.  
? Common world knowledge was assumed, 
e.g. the capital of a country is situated in 
that country, the prime minister of a state is 
also a citizen of that state, and so on. 
2.2 Pair Collection 
 
As in RTE-2, human annotators generated t-h pairs 
within 4 application settings.  
 
The IE task was inspired by the Information Ex-
traction (and Relation Extraction) application, 
where texts and structured templates were replaced 
by t-h pairs. As in the 2006 campaign, the pairs 
were generated using four different approaches: 
1) Hypotheses were taken from the relations 
tested in the ACE-2004 RDR task, while 
texts were extracted from the outputs of ac-
tual IE systems, which were provided with 
relevant news articles. Correctly extracted  
instances were used to generate positive 
examples and incorrect instances to gener-
ate negative examples. 
2) The same procedure was followed using 
output of IE systems on the dataset of the 
MUC-4 TST3 task, in which the events are 
acts of terrorism. 
3) The annotated MUC-4 dataset and the 
news articles were also used to manually 
generate entailment pairs based on ACE re-
lations.  
4) Hypotheses corresponding to relations not 
found in the ACE and MUC datasets  were 
used both to be given to IE systems and to 
manually generate t-h pairs from collected 
news articles. Examples of these relations, 
taken from various semantic fields, were 
?X beat Y?, ?X invented Y?, ?X steal Y? 
etc. 
 
The common aim of all these processes was to 
simulate the need of IE systems to recognize that 
the given text indeed entails the semantic relation 
that is expected to hold between the candidate tem-
plate slot fillers.  
 
In the IR (Information Retrieval) application set-
ting, the hypotheses were propositional IR queries, 
which specify some statement, e.g. ?robots are 
used to find avalanche victims?. The hypotheses 
were adapted and simplified from standard IR 
evaluation datasets (TREC and CLEF). Texts (t) 
that did or did not entail the hypotheses were se-
lected from documents retrieved by different search 
engines (e.g. Google, Yahoo and MSN) for each 
hypothesis. In this application setting it was as-
sumed that relevant documents (from an IR per-
spective) should entail the given propositional hy-
pothesis. 
 
For the QA (Question Answering) task, annotators 
used questions taken from the datasets of official 
QA competitions, such as TREC QA and 
QA@CLEF datasets, and the corresponding an-
swers extracted from the Web by actual QA sys-
tems. Then they transformed the question-answer 
pairs into t-h pairs as follows: 
 
? An answer term of the expected answer 
type was picked from the answer passage -
either a correct or an incorrect one.  
? The question was turned into an affirma-
tive sentence plugging in the answer term. 
? t-h pairs were generate, using the affirma-
tive sentences as hypotheses (h?s) and the 
original answer passages as texts (t?s).  
4
For example, given the question ?How high is 
Mount Everest?? and a text (t) ?The above men-
tioned expedition team comprising of 10 members 
was permitted to climb 8848m. high Mt. Everest 
from Normal Route for the period of 75 days from 
15 April, 2007 under the leadership of Mr. Wolf 
Herbert of Austria?, the annotator, extracting the 
piece of information ?8848m.? from the text, 
would turn the question into an the affirmative sen-
tence ?Mount Everest is 8848m high?, generating a 
positive entailment pair. This process simulated the 
need of a QA system to verify that the retrieved 
passage text actually entailed the provided answer. 
 
In the SUM (Summarization) setting, the 
entailment pairs were generated using two proce-
dures. 
In the first one, t?s and h?s were sentences taken 
from a news document cluster, a collection of news 
articles that describe the same news item. Annota-
tors were given the output of multi-document 
summarization systems -including the document 
clusters and the summary generated for each clus-
ter. Then they picked sentence pairs with high lexi-
cal overlap, preferably where at least one of the 
sentences was taken from the summary (this sen-
tence usually played the role of t). For positive ex-
amples, the hypothesis was simplified by removing 
sentence parts, until it was fully entailed by t. 
Negative examples were simplified in a similar 
manner. In alternative, ?pyramids? produced for 
the experimental evaluation mehod in DUC 2005 
(Passonneau et al 2005) were exploited. In this 
new evaluation method, humans select sub-
sentential content units (SCUs) in several manually 
produced summaries on a subject, and collocate 
them in a ?pyramid?, which has at the top the 
SCUs with the higher frequency, i.e. those which 
are present in most summaries. Each SCU is identi-
fied by a label, a sentence in natural language 
which expresses the content. Afterwards, the anno-
tators individuate the SCUs present in summaries 
generated automatically (called peers), and link 
them to the ones present in the pyramid, in order to 
assign each peer a weight. In this way, the SCUs in 
the automatic summaries linked to the SCUs in the 
higher tiers of the pyramid are assigned a heavier 
weight than those at the bottom. For the SUM set-
ting, the RTE-3 annotators selected relevant pas-
sages from the peers and used them as T?s, mean-
while the labels of the corresponding SCUs were 
used as H?s. Small adjustments were allowed, 
whenever the texts were not grammatically accept-
able. This process simulated the need of a summa-
rization system to identify information redundancy, 
which should be avoided in the summary. 
2.3 Final dataset  
 
Each pair of the dataset was judged by three anno-
tators. As in previous challenges, pairs on which 
the annotators disagreed were filtered-out.  
On the test set, the average agreement between 
each pair of annotators who shared at least 100 ex-
amples was 87.8%, with an average Kappa level of 
0.75, regarded as substantial agreement according 
to Landis and Koch (1997).  
19.2 % of the pairs in the dataset were removed 
from the test set due to disagreement. The dis-
agreement was generally due to the fact that the h 
was more specific than the t, for example because 
it contained more information, or made an absolute 
assertion where t proposed only a personal opinion. 
In addition, 9.4 % of the remaining pairs were dis-
carded, as they seemed controversial, too difficult, 
or too similar when compared to other pairs.  
As far as the texts extracted from the web are 
concerned, spelling and punctuation errors were 
sometimes fixed by the annotators, but no major 
change was allowed, so that the language could be 
grammatically and stylistically imperfect. The hy-
potheses were finally double-checked by a native 
English speaker. 
3 The RTE-3 Challenge 
3.1 Evaluation measures 
 
The evaluation of all runs submitted in RTE-3 was 
automatic. The judgments (classifications) returned 
by the system were compared to the Gold Standard 
compiled by the human assessors. The main 
evaluation measure was accuracy, i.e. the percent-
age of matching judgments. 
For systems that provided a confidence-ranked 
list of the pairs, in addition to the YES/NO judg-
ment, an Average Precision measure was also 
computed. This measure evaluates the ability of 
systems to rank all the T-H pairs in the test set ac-
cording to their entailment confidence (in decreas-
ing order from the most certain entailment to the 
least certain). Average precision is computed as the 
5
average of the system's precision values at all 
points in the ranked list in which recall increases, 
that is at all points in the ranked list for which the 
gold standard annotation is YES, or, more for-
mally:  
 
?
=
?n
i i
iUpToPairEntailmentiE
R 1
)(#)(1
          (1) 
 
where n is the number of the pairs in the test set, R 
is the total number of positive pairs in the test set, 
E(i) is 1 if the i-th pair is positive and 0 otherwise, 
and i ranges over the pairs, ordered by their rank-
ing.  
In other words, the more the system was confi-
dent that t entails h, the higher was the ranking of 
the pair. A perfect ranking would have placed all 
the positive pairs (for which the entailment holds) 
before all the negative ones, yielding an average 
precision value of 1. 
3.2 Submitted systems 
 
Twenty-six teams participated in the third chal-
lenge, three more than in previous year. Table 2 
presents the list of the results of each submitted 
runs and the components used by the systems. 
Overall, we noticed a move toward deep ap-
proaches, with a general consolidation of ap-
proaches based on the syntactic structure of Text 
and Hypothesis. There is an evident increase of 
systems using some form of logical inferences (at 
least seven systems). However, these approaches, 
with few notably exceptions, do not seem to be 
consolidated enough, as several systems show re-
sults  not still at the state of art (e.g. Natural Logic 
introduced by Chambers et al). For many systems 
an open issue is the availability and integration of 
different and complex semantic resources-  
A more extensive and fine grained use of spe-
cific semantic phenomena is also emerging. As an 
example, Tatu and Moldovan carry on a sophisti-
cated analysis of named entities, in particular Per-
son names, distinguishing first names from last 
names. Some form of relation extraction, either 
through manually built patterns (Chambers et al) 
or through the use of an information extraction sys-
tem (Hickl and Bensley) have been introduced this 
year, even if still on a small scale (i.e. few rela-
tions).  
On the other hand, RTE-3 confirmed that both 
machine learning using lexical-syntactic features 
and transformation-based approaches on depend-
ency representations are well consolidated tech-
niques to address textual entailment. The extension 
of transformation-based approaches toward prob-
abilistic settings is an interesting direction investi-
gated by some systems (e.g. Harmeling). On the 
side of ?light? approaches to textual entailment, 
Malakasiotis and Androutpoulos provide a useful 
baseline for the task (0.61%) using only POS tag-
ging and then applying string-based measures to 
estimate the similarity between Text and Hypothe-
sis. 
As far as resources are concerned, lexical data-
bases (mostly WordNet and DIRT) are still widely 
used. Extended WordNet is also a common re-
source (for instance in Iftene and Balahur-
Dobrescu) and the Extended Wordnet Knowledge 
Base has been successfully used in (Tatu and 
Moldovan). Verb-oriented resources are also 
largely present in several systems, including Fra-
menet (e.g. Burchardt et al), Verbnet (Bobrow et 
al.) and Propbank (e.g. Adams et al). It seems that 
the use of the Web as a resource is more limited 
when compared to the previous RTE workshop. 
However, as in RTE-2, the use of large semantic 
resources is still a crucial factor affecting the per-
formance of systems (see, for instance, the use of a 
large corpus of entailment examples in Hickl and 
Bensley).  
Finally, an interesting aspect is that, stimulated 
by the percentage of longer texts included this year, 
a number of participating systems addressed anaph-
ora resolution (e.g. Delmonte, Bar-Haim et al, 
Iftene and Balahur-Dobrescu). 
3.3 Results 
 
The accuracy achieved by the participating sys-
tems ranges from 49% to 80% (considering the best 
run of each group), while most of the systems ob-
tained a score in between 59% and 66%. One sub-
mission, Hickl and Bensley achieved 80% accu-
racy, scoring 8% higher than the second system 
(Tatu and Moldovan, 72%), and obtaining the best 
absolute result achieved in the three RTE chal-
lenges. 
6
 Table 2: Submission results and components of the systems.
 . 
System Components 
First Author Accuracy 
Average 
precision L
ex
ic
al
 
R
el
at
io
n
,
 
W
o
rd
N
et
 
 
n
-
gr
am
\w
o
rd
 
sim
ila
rit
y 
Sy
n
ta
ct
ic
 
M
at
ch
-
in
g\
A
lig
n
in
g 
Se
m
an
tic
 
R
o
le
 
La
be
lin
g\
 
Fr
am
en
et
\P
ro
ba
n
k,
 
V
er
bn
et
 
Lo
gi
ca
l I
n
fe
re
n
ce
 
Co
rp
u
s/ 
W
eb
-
ba
se
d 
St
at
ist
ic
s,
 
LS
A
 
M
L 
Cl
as
sif
ic
at
io
n
 
A
n
ap
ho
ra
 
re
so
lu
tio
n
 
 
En
ta
ilm
en
t 
Co
rp
o
ra
 
?
 
D
IR
T 
Ba
ck
gr
o
u
n
d 
K
n
o
w
le
dg
e 
Adams 0.6700  X X    X X   
0.6112 0.6118 X  X   X  X X Bar-Haim 
0.5837 0.6093  X  X   X  X  
Baral 0.4963 0.5364 X    X    X 
0.6050 0.5897 X  X    X   Blake 
  0.6587 0.6096 X  X    X   
0.5112 0.5720  X   X X     Bobrow 
  0.5150 0.5807 X   X X     
0.6250  X  X X      Burchardt 
0.6262           
0.5500   X    X    Burek 
0.5500 0.5514          
0.6050 0.6341 X  X  X  X X  Chambers 
  0.6362 0.6527 X  X  X  X X  
0.5088 0.4961  X   
 
 X    X Clark  
0.4725 0.4961  X    X    X 
Delmonte 0.5875 0.5830 X  X X X   X  
0.6563  X X X       Ferrandez 
0.6375           
0.6062  X X     X   Ferr?s 
0.6150  X X     X   
0.5600 0.5813 X  X    X   Harmling 
0.5775 0.5952 X  X    X   
Hickl 0.8000 0.8815 X X   X  X X X 
0.6913  X  X      X Iftene 
0.6913  X  X      X 
0.6400  X X     X   Li 
0.6488           
Litkowski   0.6125           
Malakasiotis  0.6175 0.6808  X     X   
Marsi 0.5913    X      X 
0.5888  X X X    X   Montejo-R?ez 
0.6038  X X X    X   
0.6238  X X X    X   Rodrigo 
0.6312  X X X    X   
0.6262  X X       X Roth 
0.5975    X     X  
0.6100 0.6195 X X     X   Settembre 
  0.6262 0.6274 X X     X   
0.7225 0.6942 X    X   X X Tatu 
  0.7175 0.6797 X    X   X  
0.6650    X    X   Wang  
0.6687           
0.6675 0.6674 X  X    X   Zanzotto 
  0.6575 0.6732 X  X    X   
7
As far as the per-task results are concerned, the 
trend registered in RTE-2 was confirmed, in that 
there was a marked difference in the performances 
obtained in different task settings. 
In fact, the average accuracy achieved in the QA 
setting (0.71) was 20 points higher than that 
achieved in the IE setting (0.52); the average accu-
racy in the IR and Sum settings was 0.66 and 0.58 
respectively. In RTE-2 the best results were 
achieved in SUM, while the lower score was al-
ways recorded in IE. As already pointed out by 
Bar-Haim (2006), these differences should be fur-
ther investigated, as they could lead to a sensible 
improvement of the performance. 
As for the LONG pairs, which represented a 
new element of this year?s challenge, no substan-
tial difference was noted in the systems? perform-
ances: the average accuracy over the long pairs 
was 58.72%, compared to 61.93% over the short 
ones.  
4 Conclusions and future work 
 
At its third round, the Recognizing Textual En-
tailment task has reached a noticeable level of ma-
turity, as the very high interest in the NLP commu-
nity and the continuously increasing number of 
participants in the challenges demonstrate. The 
relevance of Textual Entailment Recognition to 
different applications, such as the AVE5 track at 
QA at CLEF6, has also been acknowledged. Fur-
thermore, the debates and the numerous publica-
tions about the Textual Entailment have contrib-
uted to the better understanding the task and its 
nature.  
To keep a good balance between the consoli-
dated main task and the need for moving forward, 
longer texts were introduced in the dataset, in order 
to make the task more challenging, and a pilot task 
was proposed. The Third RTE Challenge have also 
confirmed that the methodology for the creation of 
the datasets, developed in the first two campaigns, 
is robust. Overall, the transition of the challenge 
coordination from Bar-Ilan ?which organized the 
first two challenges- to CELCT was successful, 
though some problems were encountered, espe-
cially in the preparation of the data set. The sys-
                                                 
5
 http://nlp.uned.es/QA/ave/. 
6
 http://clef-qa.itc.it/. 
tems which took part in RTE-3 showed that the 
technology applied to Entailment Recognition has 
made significant progress, confirmed by the results, 
which were generally better than last year. In par-
ticular, visible progress in defining several new 
principled scenarios for RTE was represented, such 
as Hickl?s commitment-based approach, Bar 
Haim?s proof system, Harmeling?s probabilistic 
model, and Standford?s use of Natural Logic. 
If, on the one hand, the success that RTE has 
had so far is very encouraging, on the other, it in-
cites to overcome certain current limitations, and to 
set realistic and, at the same time, stimulating goals 
for the future. First at all, theoretical refinements 
both of the task and the models applied to it need 
to be developed. In particular, more efforts are re-
quired to improve knowledge acquisition, as little 
progress has been made on this front so far. Also 
the data set generation and the evaluation method-
ology  need to be refined and extended. A major 
problem in the current setting of the data collection 
is that the distribution of the examples is arbitrary 
to a large extent, being determined by manual se-
lection. Therefore new evaluation methodologies, 
which can reflect realistic distributions should be 
investigated, as well as the possibility of evaluating 
Textual Entailment Recognition within additional 
concrete application scenarios, following the spirit 
of the QA Answer Validation Exercise.  
 
 
Acknowledgments 
 
The following sources were used in the preparation 
of the data: 
 
? PowerAnswer question answering system, from 
Language Computer Corporation, provided by Dan 
Moldovan and Marta Tatu. 
http://www.languagecomputer.com/solutions/question answer-
ing/power answer/ 
 
? Cicero Custom and Cicero Relation information 
extraction systems, from Language Computer Cor-
poration, provided by Sanda M. Harabagiu, An-
drew Hickl, John Lehmann and  and Paul Aarseth. 
http://www.languagecomputer.com/solutions/information_ext
action/cicero/index.html 
 
? Columbia NewsBlaster multi-document summa-
rization system, from the Natural Language Proc-
8
essing group at Columbia University?s Departmen-
tof Computer Science. 
http://newsblaster.cs.columbia.edu/ 
 
? NewsInEssence multi-document summarization 
system provided by Dragomir R. Radev and Jahna 
Otterbacher from the Computational Linguistics 
and Information Retrieval research group, Univer-
sity of Michigan. 
http://www.newsinessence.com 
 
? New York University?s information extraction 
system, provided by Ralph Grishman, Department 
of Computer Science, Courant Institute of Mathe-
matical Sciences, New York University. 
 
? MUC-4 information extraction dataset, from the 
National Institute of Standards and Technology 
(NIST).  
http://www.itl.nist.gov/iaui/894.02/related projects/muc/ 
 
? ACE 2004 information extraction templates, 
from the National Institute of Standards and Tech-
nology (NIST). 
http://www.nist.gov/speech/tests/ace/ 
 
? TREC IR queries and TREC-QA question collec-
tions, from the National Institute of Standards and 
Technology (NIST). 
http://trec.nist.gov/ 
 
? CLEF IR queries and CLEF-QA question collec-
tions, from DELOS Network of Excellence  
for Digital Libraries. 
 http://www.clef-campaign.org/, http://clef-qa.itc.it/ 
 
? DUC 2005 annotated peers, from Columbia Uni-
versity, NY, provided by Ani Nenkova. 
http://www1.cs.columbia.edu/~ani/DUC2005/ 
 
We would like to thank the people and organiza-
tions that made these sources available for the 
challenge. In addition, we thank Idan Szpektor and 
Roy Bar Haim from Bar-Ilan University  for their 
assistance and advice, and Valentina Bruseghini 
from CELCT for managing the RTE-3 website. 
 
We would also like to acknowledge the people 
and organizations involved in creating and annotat-
ing the data: Pamela Forner, Errol Hayman, Cam-
eron Fordyce from CELCT and Courtenay 
Hendricks, Adam Savel and Annika Hamalainen 
from the Butler Hill Group, which was funded by 
Microsoft Research. 
 
This work was supported in part by the IST Pro-
gramme of the European Community, under the 
PASCAL Network of Excellence, IST-2002-
506778. We wish to thank the managers of the 
PASCAL challenges program, Michele Sebag and 
Florence d?Alche-Buc, for their efforts and sup-
port, which made this challenge possible. We also 
thank David Askey, who helped manage the RTE 3 
website.  
 
References 
 
Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, 
Danilo Giampiccolo, Bernardo Magnini and Idan 
Szpektor. 2006. The Second PASCAL Recognizing 
Textual Entailment Challenge. In Proceedings of the 
Second PASCAL Challenges Workshop on Recog-
nizing Textual Entailment, Venice, Italy. 
Ido Dagan, Oren Glickman, and Bernardo Magnini. 
2006. The PASCAL Recognizing Textual Entailment 
Challenge. In Qui?onero-Candela et al, editors, 
MLCW 2005, LNAI Volume 3944, pages 177-190. 
Springer-Verlag. 
J. R. Landis and G. G. Koch. 1997. The measurements 
of observer agreement for categorical data. Biomet-
rics, 33:159?174. 
Rebecca Passonneau, Ani Nenkova., Kathleen McKe-
own, and Sergey Sigleman. 2005. Applying the 
pyramid method in DUC 2005. In Proceedings of the 
Document Understanding Conference (DUC 05), 
Vancouver, B.C., Canada. 
Ellen M. Voorhees and Donna Harman. 1999. Overview 
of the seventh text retrieval conference. In Proceed-
ings of the Seventh Text Retrieval Conference 
(TREC-7). NIST Special Publication. 
 
 
9
Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 131?136,
Prague, June 2007. c?2007 Association for Computational Linguistics
Semantic Inference at the Lexical-Syntactic Level for Textual Entailment
Recognition
Roy Bar-Haim?,Ido Dagan?, Iddo Greental?, Idan Szpektor? and Moshe Friedman?
?Computer Science Department, Bar-Ilan University, Ramat-Gan 52900, Israel
?Linguistics Department, Tel Aviv University, Ramat Aviv 69978, Israel
{barhair,dagan}@cs.biu.ac.il,greenta@post.tau.ac.il,
{szpekti,friedmm}@cs.biu.ac.il
Abstract
We present a new framework for textual en-
tailment, which provides a modular integra-
tion between knowledge-based exact infer-
ence and cost-based approximate matching.
Diverse types of knowledge are uniformly
represented as entailment rules, which were
acquired both manually and automatically.
Our proof system operates directly on parse
trees, and infers new trees by applying en-
tailment rules, aiming to strictly generate the
target hypothesis from the source text. In or-
der to cope with inevitable knowledge gaps,
a cost function is used to measure the re-
maining ?distance? from the hypothesis.
1 Introduction
According to the traditional formal semantics ap-
proach, inference is conducted at the logical level.
However, practical text understanding systems usu-
ally employ shallower lexical and lexical-syntactic
representations, augmented with partial semantic
annotations. Such practices are typically partial and
quite ad-hoc, and lack a clear formalism that speci-
fies how inference knowledge should be represented
and applied. The current paper proposes a step to-
wards filling this gap, by defining a principled se-
mantic inference mechanism over parse-based rep-
resentations.
Within the textual entailment setting a system is
required to recognize whether a hypothesized state-
ment h can be inferred from an asserted text t.
Some inferences can be based on available knowl-
edge, such as information about synonyms and para-
phrases. However, some gaps usually arise and it
is often not possible to derive a complete ?proof?
based on available inference knowledge. Such sit-
uations are typically handled through approximate
matching methods.
This paper focuses on knowledge-based infer-
ence, while employing rather basic methods for ap-
proximate matching. We define a proof system
that operates over syntactic parse trees. New trees
are derived using entailment rules, which provide a
principled and uniform mechanism for incorporat-
ing a wide variety of manually and automatically-
acquired inference knowledge. Interpretation into
stipulated semantic representations, which is often
difficult to obtain, is circumvented altogether. Our
research goal is to explore how far we can get with
such an inference approach, and identify the scope
in which semantic interpretation may not be needed.
For a detailed discussion of our approach and related
work, see (Bar-Haim et al, 2007).
2 Inference Framework
The main contribution of the current work is a prin-
cipled semantic inference mechanism, that aims to
generate a target text from a source text using en-
tailment rules, analogously to logic-based proof sys-
tems. Given two parsed text fragments, termed
text (t) and hypothesis (h), the inference system (or
prover) determines whether t entails h. The prover
applies entailment rules that aim to transform t into
h through a sequence of intermediate parse trees.
For each generated tree p, a heuristic cost function is
employed to measure the likelihood of p entailing h.
131
ROOT
i 
rain VERB
expletive
ssfff
ff
ff
ff
f wha
++XX
XX
XX
XX
XX
it OTHER when PREP
i 
see VERB
obj
qqcccccc
cccc
cccc
cccc
cc
bessff
ff
ff
ff
ff
by

mod
++XX
XX
XX
XX
XX
Mary NOUN
mod 
be VERB by PREP
pcomp?n

yesterday NOUN
beautiful ADJ John NOUN
ROOT
i 
rain VERB
expletive
rreeee
eee
eee
e wha
,,YYY
YYY
YYY
YY
it OTHER when PREP
i 
see VERB
subj
rreeee
eee
eee
e
obj

mod
,,YYY
YYY
YYY
YY
John NOUN Mary NOUN
mod 
yesterday NOUN
beautiful ADJ
Source: it rained when beautiful Mary was seen by John yester-
day
Derived: it rained when John saw beautiful Mary yesterday
(a) Application of passive to active transformation
L
V VERB
obj
ssfff
ff
ff
ff
f
be 
by
++XX
XX
XX
XX
XX
R
V VERB
subj
ssfff
ff
ff
ff
f obj
++XX
XX
XX
XX
XX
N1 NOUN be VERB by PREP
pcomp?n

N2 NOUN N1 NOUN
N2 NOUN
(b) Passive to active transformation (substitution rule). The dotted arc represents alignment.
Figure 1: Application of inference rules. POS and relation labels are based on Minipar (Lin, 1998b)
If a complete proof is found (h was generated), the
prover concludes that entailment holds. Otherwise,
entailment is determined by comparing the minimal
cost found during the proof search to some threshold
?.
3 Proof System
Like logic-based systems, our proof system consists
of propositions (t, h, and intermediate premises),
and inference (entailment) rules, which derive new
propositions from previously established ones.
3.1 Propositions
Propositions are represented as dependency trees,
where nodes represent words, and hold a set of fea-
tures and their values. In our representation these
features include the word lemma and part-of-speech,
and additional features that may be added during the
proof process. Edges are annotated with dependency
relations.
3.2 Inference Rules
At each step of the proof an inference rule gener-
ates a derived tree d from a source tree s. A rule
is primarily composed of two templates, termed left-
hand-side (L), and right-hand-side (R). Templates
are dependency subtrees which may contain vari-
ables. Figure 1(b) shows an inference rule, where
V , N1 and N2 are common variables. L specifies
the subtree of s to be modified, and R specifies the
new generated subtree. Rule application consists of
the following steps:
L matching The prover first tries to match L in s.
L is matched in s if there exists a one-to-one node
mapping function f from L to s, such that: (i) For
each node u, f(u) has the same features and feature
values as u. Variables match any lemma value in
f(u). (ii) For each edge u ? v in L, there is an
edge f(u) ? f(v) in s, with the same dependency
relation. If matching fails, the rule is not applicable
to s. Otherwise, successful matching induces vari-
132
LROOT
i 
R
ROOT
i 
V1 VERB
wha 
V2 VERB
when ADJ
i 
V2 VERB
Figure 2: Temporal clausal modifier extraction (in-
troduction rule)
able binding b(X), for each variableX in L, defined
as the full subtree rooted in f(X) if X is a leaf, or
f(X) alone otherwise. We denote by l the subtree
in s to which L was mapped (as illustrated in bold
in Figure 1(a), left tree).
R instantiation An instantiation of R, which we
denote r, is generated in two steps: (i) creating a
copy of R; (ii) replacing each variable X with a
copy of its binding b(X) (as set during L matching).
In our example this results in the subtree John saw
beautiful Mary.
Alignment copying the alignment relation be-
tween pairs of nodes in L and R specifies which
modifiers in l that are not part of the rule structure
need to be copied to the generated tree r. Formally,
for any two nodes u in l and v in r whose matching
nodes in L and R are aligned, we copy the daugh-
ter subtrees of u in s, which are not already part of
l, to become daughter subtrees of v in r. The bold
nodes in the right part of Figure 1(b) correspond to
r after alignment. yesterday was copied to r due to
the alignment of its parent verb node.
Derived tree generation by rule type Our for-
malism has two methods for generating the derived
tree: substitution and introduction, as specified by
the rule type. With substitution rules, the derived
tree d is obtained by making a local modification to
the source tree s. Except for this modification s and
d are identical (a typical example is a lexical rule,
such as buy ? purchase). For this type, d is formed
by copying s while replacing l (and the descendants
of l?s nodes) with r. This is the case for the passive
rule. The right part of Figure 1(a) shows the derived
tree for the passive rule application. By contrast, in-
troduction rules are used to make inferences from a
subtree of s, while the other parts of s are ignored
and do not affect d. A typical example is inference
of a proposition embedded as a relative clause in s.
In this case the derived tree d is simply taken to be
r. Figure 2 presents such a rule that derives propo-
sitions embedded within temporal modifiers. Note
that the derived tree does not depend on the main
clause. Applying this rule to the right part of Figure
1(b) yields the proposition John saw beautiful Mary
yesterday.
3.3 Annotation Rules
Annotation rules add features to parse tree nodes,
and are used in our system to annotate negation and
modality. Annotation rules do not have an R. In-
stead, nodes of L may contain annotation features.
If L is matched in a tree then the annotations are
copied to the matched nodes. Annotation rules are
applied to t and to each inferred premise prior to
any entailment rule application and these features
may block inappropriate subsequent rule applica-
tions, such as for negated predicates.
4 Rules for Generic Linguistic Structures
Based on the above framework we have manually
created a rule base for generic linguistic phenomena.
4.1 Syntactic-Based Rules
These rules capture entailment inferences associ-
ated with common syntactic structures. They have
three major functions: (i) simplification and canon-
ization of the source tree (categories 6 and 7 in Ta-
ble 1); (ii) extracting embedded propositions (cate-
gories 1, 2, 3); (iii) inferring propositions from non-
propositional subtrees (category 4).
4.2 Polarity-Based Rules
Consider the following two examples:
John knows that Mary is here?Mary is here.
John believes that Mary is here;Mary is here.
Valid inference of propositions embedded as verb
complements depends on the verb properties, and
the polarity of the context in which the verb appears
(positive, negative, or unknown) (Nairn et al, 2006).
We extracted from the polarity lexicon of Nairn et
al. a list of verbs for which inference is allowed in
positive polarity context, and generated entailment
133
# Category Example: source Example: derived
1 Conjunctions Helena?s very experienced and has played a long
time on the tour.
? Helena has played a long time on the tour.
2 Clausal modi-
fiers
But celebrations were muted as many Iranians ob-
served a Shi?ite mourning month.
? Many Iranians observed a Shi?ite mourning
month.
3 Relative
clauses
The assailants fired six bullets at the car, which car-
ried Vladimir Skobtsov.
? The car carried Vladimir Skobtsov.
4 Appositives Frank Robinson, a one-time manager of the Indians,
has the distinction for the NL.
? Frank Robinson is a one-time manager of the
Indians.
5 Determiners The plaintiffs filed their lawsuit last year in U.S.
District Court in Miami.
? The plaintiffs filed a lawsuit last year in U.S.
District Court in Miami.
6 Passive We have been approached by the investment banker. ? The investment banker approached us.
7 Genitive
modifier
Malaysia?s crude palm oil output is estimated to
have risen by up to six percent.
? The crude palm oil output of Malasia is esti-
mated to have risen by up to six percent.
8 Polarity Yadav was forced to resign. ? Yadav resigned.
9 Negation,
modality
What we?ve never seen is actual costs come
down.
What we?ve never seen is actual costs come down.
(;What we?ve seen is actual costs come down.)
Table 1: Summary of rule base for generic linguistic structures.
rules for these verbs (category 8). The list was com-
plemented with a few reporting verbs, such as say
and announce, assuming that in the news domain the
speaker is usually considered reliable.
4.3 Negation and Modality Annotation Rules
We use annotation rules to mark negation and
modality of predicates (mainly verbs), based on their
descendent modifiers. Category 9 in Table 1 illus-
trates a negation rule, annotating the verb seen for
negation due to the presence of never.
4.4 Generic Default Rules
Generic default rules are used to define default be-
havior in situations where no case-by-case rules are
available. We used one default rule that allows re-
moval of any modifiers from nodes.
5 Lexical-based Rules
These rules have open class lexical components, and
consequently are numerous compared to the generic
rules described in section 4. Such rules are acquired
either lexicographically or automatically.
The rules described in the section 4 are applied
whenever their L template is matched in the source
premise. For high fan-out rules such as lexical-based
rules (e.g. words with many possible synonyms),
this may drastically increase the size of the search
space. Therefore, the rules described below are ap-
plied only if L is matched in the source premise p
and R is matched in h.
5.1 Lexical Rules
Lexical entailment rules, such as ?steal ? take? and
?Britain ? UK? were created based on WordNet
(Fellbaum, 1998). Given p and h, a lexical rule
lemmap ? lemmah may be applied if lemmap
and lemmah are lemmas of open-class words ap-
pearing in p and h respectively, and there is a path
from lemmah to lemmap in the WordNet ontology,
through synonym and hyponym relations.
5.2 Lexical-Syntactic Rules
In order to find lexical-syntactic paraphrases and en-
tailment rules, such as ?X strike Y ? X hit Y ? and
?X buy Y ?X own Y ? that would bridge between p
and h, we applied the DIRT algorithm (Lin and Pan-
tel, 2001) to the first CD of the Reuters RCV1 cor-
pus1. DIRT does not identify the entailment direc-
tion, hence we assumed bi-directional entailment.
We calculate off-line only the feature vector of ev-
ery template found in the corpus, where each path
between head nouns is considered a template in-
stance. Then, given a premise p, we first mark all
lexical noun alignments between p and h. Next, for
every pair of alignments we extract the path between
the two nouns in p, labeled pathp, and the corre-
sponding path between the aligned nouns in h, la-
beled pathh. We then on-the-fly test whether there
is a rule ?pathp ? pathh? by extracting the stored
feature vectors of pathp and pathh and measuring
1http://about.reuters.com/researchandstandards/corpus/
134
their similarity. If the score exceeds a given thresh-
old2, we apply the rule to p.
Another enhancement that we added to DIRT is
template canonization. At learning time, we trans-
form every template identified in the corpus into
its canonized form3 using a set of morpho-syntactic
rules, similar to the ones described in Section 4. In
addition, we apply nominalization rules such as ?ac-
quisition of Y by X ? X acquire Y ?, which trans-
form a nominal template into its related verbal form.
We automatically generate these rules (Ron, 2006),
based on Nomlex (Macleod et al, 1998).
At inference time, before retrieving feature vec-
tors, we canonize pathp into pathcp and pathh into
pathch. We then assess the rule ?path
c
p ? path
c
h?,
and if valid, we apply the rule ?pathp ? pathh? to
p. In order to ensure the validity of the implicature
?pathp ? pathcp ? path
c
h ? pathh?, we canonize
pathp using the same rule set used at learning time,
but we apply only bi-directional rules to pathh (e.g.
conjunct heads are not removed from pathh).
6 Approximate Matching
As mentioned in section 2, approximate matching
is incorporated into our system via a cost function,
which estimates the likelihood of h being entailed
from a given premise p. Our cost function C(p, h) is
a linear combination of two measures: lexical cost,
Clex(p, h) and lexical-syntactic cost ClexSyn(p, h):
C(p, h) = ?ClexSyn(p, h)+ (1??)Clex(p, h) (1)
Let m?() be a (possibly partial) 1-1 mapping of the
nodes of h to the nodes of p, where each node
is mapped to a node with the same lemma, such
that the number of matched edges is maximized.
An edge u ? v in h is matched in p if m?(u)
and m?(v) are both defined, and there is an edge
m?(u) ? m?(v) in p, with the same dependency rela-
tion. ClexSyn(p, h) is then defined as the percentage
of unmatched edges in h.
Similarly, Clex(p, h) is the percentage of un-
matched lemmas in h, considering only open-class
words, defined as:
Clex(p, h) = 1?
?
l?h Score(l)
#OpenClassWords(h)
(2)
2We set the threshold to 0.01
3The active verbal form with direct modifiers
where Score(l) is 1 if it appears in p, or if it is
a derivation of a word in p (according to Word-
Net). Otherwise, Score(l) is the maximal Lin
dependency-based similarity score between l and the
lemmas of p (Lin, 1998a) (synonyms and hyper-
nyms/hyponyms are handled by the lexical rules).
7 System Implementation
Deriving the initial propositions t and h from the in-
put text fragments consists of the following steps:
(i) Anaphora resolution, using the MARS system
(Mitkov et al, 2002). Each anaphor was replaced by
its antecedent. (ii) Sentence splitting, using mxter-
minator (Reynar and Ratnaparkhi, 1997). (iii) De-
pendency parsing, using Minipar (Lin, 1998b).
The proof search is implemented as a depth-first
search, with maximal depth (i.e. proof length) of
4. If the text contains more than one sentence, the
prover aims to prove h from each of the parsed sen-
tences, and entailment is determined based on the
minimal cost. Thus, the only cross-sentence infor-
mation that is considered is via anaphora resolution.
8 Evaluation
Full (run1) Lexical (run2)
Dataset Task Acc. Avg.P Acc. Avg.P
Test IE 0.4950 0.5021 0.5000 0.5379
Official IR 0.6600 0.6174 0.6450 0.6539
Results QA 0.7050 0.8085 0.6600 0.8075
SUM 0.5850 0.6200 0.5300 0.5927
All 0.6112 0.6118 0.5837 0.6093
Dev. All 0.6443 0.6699 0.6143 0.6559
Table 2: Empirical evaluation - results.
The results for our submitted runs are listed in Ta-
ble 2, including per-task scores. run1 is our full sys-
tem, denoted F . It was tuned on a random sample
of 100 sentences from the development set, result-
ing in ? = 0.6 and ? = 0.6242 (entailment thresh-
old). run2 is a lexical configuration, denoted L, in
which ? = 0 (lexical cost only), ? = 0.2375 and
the only inference rules used were WordNet Lexical
rules. We found that the higher accuracy achieved
by F as compared to L might have been merely due
to a lucky choice of threshold. Setting the threshold
to its optimal value with respect to the test set re-
sulted in an accuracy of 62.4% for F , and 62.9% for
135
L. This is also hinted by the very close average pre-
cision scores for both systems, which do not depend
on the threshold. The last row in the table shows
the results obtained for 7/8 of the development set
that was not used for tuning, denoted Dev, using the
same parameter settings. Again, F performs bet-
ter than L. F is still better when using an optimal
threshold (which increases accuracy up to 65.3% for
F and 63.9% for L. Overall, F does not show yet a
consistent significant improvement over L.
Initial analysis of the results (based on Dev) sug-
gests that the coverage of the current rules is still
rather low. Without approximate matching (h must
be fully proved using the entailment rules) the re-
call is only 4.3%, although the precision (92%) is
encouraging. Lexical-syntactic rules were applied
in about 3% of the attempted proofs, and in most
cases involved only morpho-syntactic canonization,
with no lexical variation. As a result, entailment was
determined mainly by the cost function. Entailment
rules managed to reduce the cost in about 30% of the
attempted proofs.
We have qualitatively analyzed a subset of false
negative cases, to determine whether failure to com-
plete the proof is due to deficient components of
the system or due to higher linguistic and knowl-
edge levels. For each pair, we assessed the reasoning
steps a successful derivation of h from t would take.
We classified each pair according to the most de-
manding type of reasoning step it would require. We
allowed rules that are presently unavailable in our
system, as long as they are similar in power to those
that are currently available. We found that while
the single dominant cause for proof failure is lack
of world knowledge, e.g. the king?s son is a mem-
ber of the royal family, the combination of miss-
ing lexical-syntactic rules and parser failures equally
contributed to proof failure.
9 Conclusion
We defined a novel framework for semantic infer-
ence at the lexical-syntactic level, which allows a
unified representation of a wide variety of inference
knowledge. In order to reach reasonable recall on
RTE data, we found that we must scale our rule ac-
quisition, mainly by improving methods for auto-
matic rule learning.
Acknowledgments
We are grateful to Cleo Condoravdi for making the
polarity lexicon developed at PARC available for
this research. We also wish to thank Ruslan Mitkov,
Richard Evans, and Viktor Pekar from University of
Wolverhampton for running the MARS system for
us. This work was partially supported by ISF grant
1095/05, the IST Programme of the European Com-
munity under the PASCAL Network of Excellence
IST-2002-506778, the Israel Internet Association
(ISOC-IL) grant 9022 and the ITC-irst/University of
Haifa collaboration.
References
Roy Bar-Haim, Ido Dagan, Iddo Greental, and Eyal
Shnarch. 2007. Semantic inference at the lexical-
syntactic level. In AAAI (to appear).
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. Language, Speech and Com-
munication. MIT Press.
Dekang Lin and Patrik Pantel. 2001. Discovery of infer-
ence rules for question answering. Natural Language
Engineering, 4(7):343?360.
Dekang Lin. 1998a. Automatic retrieval and clustering
of similar words. In Proceedings of COLING/ACL.
Dekang Lin. 1998b. Dependency-based evaluation of
minipar. In Proceedings of the Workshop on Evalua-
tion of Parsing Systems at LREC.
C. Macleod, R. Grishman, A. Meyers, L. Barrett, and
R. Reeves. 1998. Nomlex: A lexicon of nominal-
izations. In EURALEX.
Ruslan Mitkov, Richard Evans, and Constantin Orasan.
2002. A new, fully automatic version of Mitkov?s
knowledge-poor pronoun resolution method. In Pro-
ceedings of CICLing.
Rowan Nairn, Cleo Condoravdi, and Lauri Karttunen.
2006. Computing relative polarity for textual infer-
ence. In Proceedings of ICoS-5.
Jeffrey C. Reynar and Adwait Ratnaparkhi. 1997. A
maximum entropy approach to identifying sentence
boundaries. In Proceedings of ANLP.
Tal Ron. 2006. Generating entailment rules based on
online lexical resources. Master?s thesis, Computer
Science Department, Bar-Ilan University, Ramat-Gan,
Israel.
136
Proceedings of the 2009 Workshop on Applied Textual Inference, ACL-IJCNLP 2009, pages 27?35,
Suntec, Singapore, 6 August 2009.
c
?2009 ACL and AFNLP
Augmenting WordNet-based Inference with Argument Mapping
Idan Szpektor
Department of Computer Science
Bar-Ilan University
Ramat Gan, Israel
szpekti@cs.biu.ac.il
Ido Dagan
Department of Computer Science
Bar-Ilan University
Ramat Gan, Israel
dagan@cs.biu.ac.il
Abstract
WordNet is a useful resource for lexi-
cal inference in applications. Inference
over predicates, however, often requires
a change in argument positions, which
is not specified in WordNet. We pro-
pose a novel framework for augmenting
WordNet-based inferences over predicates
with corresponding argument mappings.
We further present a concrete implementa-
tion of this framework, which yields sub-
stantial improvement to WordNet-based
inference.
1 Introduction
WordNet (Miller, 1995), a manually constructed
lexical database, is probably the mostly used re-
source for lexical inference in NLP tasks, such
as Question Answering (QA), Information Extrac-
tion (IE), Information Retrieval and Textual En-
tailment (RTE) (Moldovan and Mihalcea, 2000;
Pasca and Harabagiu, 2001; Bar-Haim et al, 2006;
Giampiccolo et al, 2007).
Inference using WordNet typically involves lex-
ical substitutions for words in text based on
WordNet relations, a process known as lexical
chains (Barzilay and Elhadad, 1997; Moldovan
and Novischi, 2002). For example, the an-
swer to ?From which country was Louisiana ac-
quired?? can be inferred from ?The United
States bought up Louisiana from France? using the
chains ?France ? European country ? country?
and ?buy up? buy? acquire?.
When performing inference between predicates
there is an additional complexity on top of lex-
ical substitution: the syntactic relationship be-
tween the predicate and its arguments may change
as well. For example, ?X buy Y for Z ?
X pay Z for Y ?.
Currently, argument mappings are not specified
for WordNet?s relations. Therefore, correct Word-
Net inference chains over predicates can be per-
formed only for substitution relations (mainly syn-
onyms and hypernyms, e.g. ?buy? acquire?), for
which argument positions do not change. Other
relation types that may be used for inference can-
not be utilized when the predicate arguments need
to be traced as well. Examples include the Word-
Net ?entailment? relation (e.g. ?buy ? pay?) and
relations between morphologically derived words
(e.g. ?acquire? acquisition?).
Our goal is to obtain argument mappings for
WordNet relations that are often used for infer-
ence. In this paper we address several prominent
WordNet relations, including verb-noun deriva-
tions and the verb-verb ?entailment? and ?cause?
relations, referred henceforth as inferential rela-
tions. Under the Textual Entailment paradigm,
all these relations can be viewed as express-
ing entailment. Accordingly, we propose a
novel framework, called Argument-mapped Word-
Net (AmWN), that represents argument map-
pings for inferential relations as entailment rules.
These rules are augmented with subcategorization
frames and functional roles, which are proposed
as a generally-needed extension for predicative en-
tailment rules.
Following our new representation scheme, we
present a concrete implementation of AmWN for
a large number of WordNet?s relations. The map-
pings for these relations are populated by com-
bining information from manual and corpus-based
resources, which provides broader coverage com-
pared to prior work and more accurate mappings.
Table 1 shows typical inference chains obtained
27
Rule Chains
shopping:n ofX
obj
? buying:n ofX
obj
? buy:vX
obj
? pay:v forX
mod
vote:v onX
mod
? decide:v onX
mod
? debate:vX
obj
X
obj
?s sentence:n ? condemn:vX
obj
? convict:vX
obj
?X
obj
?s conviction:n
X
ind?obj
?s teacher:n ? teach:v toX
ind?obj
?X
subj
learn:v
Table 1: Examples for inference chains obtained using AmWN. Arguments are subscripted with func-
tional roles, e.g. subject (subj) and indirect-object (ind-obj). For brevity, predicate frames are omitted.
using our implementation.
To further improve WordNet-based inference
for NLP applications, we address the phenom-
ena of rare WordNet senses. Rules generated for
such senses might hurt inference accuracy since
they are often applied incorrectly to texts when
matched against inappropriate, but more frequent
senses of the rule words. Since word sense disam-
biguation (WSD) solutions are typically not suf-
ficiently robust yet, most applications do not cur-
rently apply WSD methods. Hence, we propose
to optionally filter out such rules using a novel
corpus-based validation algorithm.
We tested both WordNet and AmWN on a test
set derived from a standard IE benchmark. The
results show that AmWN substantially improves
WordNet-based inference in terms of both recall
and precision
1
.
2 Argument-Mapping Entailment Rules
In our framework we represent argument map-
pings for inferential relations between predicates
through an extension of entailment rules over syn-
tactic representations. As defined in earlier works,
an entailment rule specifies an inference rela-
tion between an entailing template and an en-
tailed template, where templates are parse sub-
trees with argument variables (Szpektor and Da-
gan, 2008). For example, ?X
subj
??? buy
obj
??? Y
? ?X
subj
??? pay
prep?for
??????? Y ?.
When a rule is applied to a text, a new conse-
quent is inferred by instantiating the entailed tem-
plate variables with the argument instantiations of
the entailing template in the text. In our example,
?IBM paid for Cognos? can be inferred from ?IBM
bought Cognos?. This way, the syntactic structure
of the rule templates specifies the required argu-
ment positions for correct argument mapping.
However, representing entailment rule structure
only by syntactic argument positions is insufficient
for predicative rules. Correct argument mapping
1
We plan to make our AmWN publicly available.
depends also on the specific syntactic functional
roles of the arguments (subject, object etc.) and on
the suitable subcategorization frame (frame) for
the predicate mention - a set of functional roles
that a predicate may occur with. For example,
?X?s buyout ? buy X? is incorrectly applied to
?IBM?s buyout of Cognos? if roles are ignored,
since ?IBM? plays the subject role while X needs
to be an object.
Seeking to address this issue, we were inspired
by the Nomlex database (Macleod et al, 1998)
(see Section 3.2.1) and explicitly specify argu-
ment mapping for each frame and functional role.
As in Nomlex, we avoid the use of semantic
roles and stick to the syntactic level, augment-
ing the representation of templates with: (a) a
syntactic functional role for each argument; (b)
the valid predicate frame for this template men-
tions. We note that such functional roles typically
coincide with dependency relations of the verbal
form. A rule example is ?X
subj
break
{intrans}
?
damage
{trans}
X
obj
?
2
. More examples are shown
in Table 1.
Unlike Nomlex records, our templates can be
partial: they may contain only some of the possi-
ble predicate arguments, e.g. ?buy
{trans}
X
obj
?,
where the subject, included in the frame, is omit-
ted. Partial templates are necessary for matching
predicate occurrences that include only some of
the possible arguments, as in ?Cognos was bought
yesterday?. Additionally, some resources, such as
automatic rule learning methods (Lin and Pantel,
2001; Sekine, 2005), can provide only partial ar-
gument information, and we would want to repre-
sent such knowledge as well.
In our framework we follow (Szpektor and Da-
gan, 2008) and use only rules between unary tem-
plates, containing a single argument. Such tem-
plates can describe any argument mapping by de-
2
Functional roles are denoted by subscripts of the argu-
ments and frames by subscripts of the predicate. We short-
hand trans for transitive frame {subject, object} and intrans
for intransitive {subject}. For brevity, we will not show all
template information when examples are self explanatory.
28
composing templates with several arguments into
unary ones, while preserving the specification of
the subcategorization frame.
To apply a rule, the entailing template must be
first matched in the text, which includes match-
ing the template?s syntactic dependency structure,
functional roles, and frame. Such procedure re-
quires texts to be annotated with these types of in-
formation. This can be reasonably performed with
existing tools and resources, as described for our
own text processing in Section 4.
Explicitly matching frames and functional roles
in rules avoids incorrect rule applications. For ex-
ample, ?X
obj
?s buyout? buy X
obj
? would be ap-
plied only to ?Cognos?s buyout by IBM? follow-
ing proper role annotation of the text, but not to
?IBM?s buyout of Cognos?. As another example,
?X
subj
break
{intrans}
? damage
{trans}
X
obj
?
would be applied only to the intransitive occur-
rence of ?break?, e.g. ?The vase broke?, but not
to ?John broke the vase?.
Ambiguous cases may occur during annotation.
For example, the role of ?John? in ?John?s invita-
tion was well intended? could be either subject or
object. Such recognized ambiguities should be left
unannotated, blocking incorrect rule application.
3 Argument Mapping for WordNet
Following our extension of entailment rules, we
present Argument-mapped WordNet (AmWN), a
framework for extendingWordNet?s inferential re-
lations with argument mapping at the syntactic
representation level.
3.1 Argument Mapping Representation
The AmWN structure follows that of WordNet: a
directed graph whose nodes are WordNet synsets
and edges are relations between synsets. Since
we focus on entailment between predicates, we
include only predicative synsets: all verb synsets
and noun synsets identified as predicates (see Sec-
tion 3.2). In addition, only WordNet relations that
correspond to some type of entailment are consid-
ered, as detailed in Section 3.2.
In our framework, different subcategorization
frames are treated as having different ?meanings?,
since different frames may correspond to differ-
ent entailment rules. Each WordNet synset is split
into several nodes, one for each of its frames. We
take frame descriptions for verbs from WordNet
3
.
3
We also tried using VerbNet (Kipper et al, 2000), with-
Figure 1: A description of ?buy/purchase X ?
pay for X? as a mapping edge in AmWN.
Since WordNet does not provide frames for noun
predicates, these are taken from Nomlex-plus (see
Section 3.2).
There are two types of graph edges that rep-
resent entailment rules between nodes: mapping
edges and substitution edges. Mapping edges
specify entailment rules that require argument
mapping, where the entailing and entailed tem-
plate predicates are replaced by synsets. Thus, an
edge represents all rules between entailing and en-
tailed synset members, as in Figure 1.
Substitution edges connect pairs of predicates,
of the same part-of-speech, which preserve argu-
ment positions in inference. This is analogous to
how WordNet may be currently used for inference
via the synonym and hypernym relations. Unlike
WordNet, substitution edges in AmWN may con-
nect only nodes that have the same subcategoriza-
tion frame.
AmWN is utilized by generating rule chains for
a given input unary template. First, starting nodes
that match the input predicate are selected. Then,
rules are generated by traversing either incoming
or outgoing graph edges transitively, depending
on the entailment direction requested. Specific
synset-ids, if known, may also be added to the in-
put to constrain the relevant starting nodes for the
input predicate. Table 1 shows examples of rule
chains from AmWN.
3.2 Argument Mapping Population
After defining the AmWN representation, we next
describe our implementation of AmWN. We first
populate the AmWN graph with substitution edges
for WordNet?s hypernyms and synonyms (as self
edges), e.g. ?buy ? purchase? and ?buy ? ac-
quire?. The following subsections describe how
mapping edges are created based on various man-
ual and corpus-based information resources.
3.2.1 Nominalization Relations
The relation between a verb and its nominaliza-
tions, e.g. between ?employ? and ?employment?,
out any current performance improvement.
29
:ORTH "employment"
:VERB "employ"
:VERB-SUBC ((NOM-NP
:SUBJECT ((DET-POSS)
(N-N-MOD)
(PP :PVAL ("by")))
:OBJECT ((DET-POSS)
(PP :PVAL ("of")))
Figure 2: Part of the employment Nomlex entry,
describing the possible syntactic dependency posi-
tions for each role of the transitive frame. It states,
for example, that the verbal ?object? role can be
mapped to employment either as a possessive or as
the complement of the preposition ?of?.
is described in WordNet by the derivationally re-
lated relation. To add argument mappings for
these relations we utilize Nomlex-plus (Meyers
et al, 2004), a database of around 5000 English
nominalizations. Nomlex specifies for each ver-
bal subcategorization frame of each nominaliza-
tion how its argument positions are mapped to
functional roles of related verbs.
For each Nomlex entry, we extract all possible
argument mappings between the verbal and nom-
inal forms, as well as between different argument
realizations of the noun. For example, the map-
pings ?X
obj
?s employment ? employ X
obj
? and
?X
obj
?s employment? employment of X
obj
? are
derived from the entry in Figure 2.
The major challenge in integrating Nomlex and
WordNet is to identify for each Nomlex noun
which WordNet synsets describe its predicative
meanings. For example, one synset of ?acquisi-
tion? that is derivationally related to ?acquire? is
not predicative: ?an ability that has been acquired
by training?. We mark noun synsets as predicative
if they are (transitive) hyponyms of the act high-
level synset.
Once predicative synsets are identified, we cre-
ate, for each synset, a node for each subcate-
gorization frame of its noun members, as found
in Nomlex-plus. In some nodes not all original
synset members are retained, since not all mem-
bers share all their frames. Mapping edges are
then added between nodes that have the same
frame. We add both noun-verb edges and noun
self-edges that map different realizations of the
same functional role (e.g. ?X
obj
?s employment?
employment of X
obj
?).
As rich as Nomlex-plus is, it still does not in-
clude all nominalizations. For example, the nouns
Lexical Relation Extracted Mappings
buy ? pay
buy forX ? payX
X buy ?X pay
divorce ? marry
divorce fromX ? marryX
divorce fromX ?X marry
kill ? die
killX ?X die
kill amongX ?X die
breathe ? inhale
breatheX ? inhaleX
breathe inX ? inhaleX
remind ? remember
remindX ?X remember
remind ofX ? rememberX
teach ? learn
teachX ? learnX
teach toX ?X learn
give ? have
giveX ? haveX
give toX ?X have
Table 2: Some argument mappings for WordNet
verb-verb relations discovered by unary-DIRT.
?divorce? (related to the verb ?divorce?) and ?strik-
ing? are missing. WordNet has a much richer set
of nominalizations that we would like to use. To
do so, we inherit associated frames and argument
realizations for each nominalization synset from
its closest hypernym that does appear in Nomlex.
Thus, ?divorce? inherits its information from ?sep-
aration? and ?striking? inherits from ?hit?. A by-
product of this process is the automatic extension
of Nomlex-plus with 5100 new nominalization en-
tries, based on the inherited information
4
.
3.2.2 Verb-Verb Relations
There are two inferential relations between verbs
in WordNet that do not preserve argument posi-
tions: cause and entailment. Unlike for nomi-
nalizations, there is no broad-coverage manual re-
source of argument mapping for these relations.
Hence, we turn to unsupervised approaches that
learn entailment rules from corpus statistics.
Many algorithms were proposed for learning
entailment rules between templates from corpora
(Lin and Pantel, 2001; Szpektor et al, 2004;
Sekine, 2005), but typically with mediocre accu-
racy. However, we only search for rules between
verbs for which WordNet aleady indicates the ex-
istence of an entailment relation and are thus not
affected by rules that wrongly relate non-entailing
verbs. We acquired a rule-set containing the top
300 rules for every unary template in the Reuters
RCV1 corpus
5
by implementing the unary-DIRT
algorithm (Szpektor and Dagan, 2008), which was
shown to have relatively high recall compared to
other algorithms.
4
We plan making this extension publicly available as well.
5
http://about.reuters.com/researchandstandards/corpus/
30
To extract argument mappings, we identify all
AmWN node pairs whose synsets are related in
WordNet by a cause or an entailment relation.
For each pair, we look for unary-DIRT rules be-
tween any pair of members in the entailing and
entailed synsets. For example, the synset {buy,
purchase} entails {pay}, so we look for rules map-
ping either ?buy? pay? or ?purchase? pay?. Ta-
ble 2 presents examples for discovered mappings.
While unary-DIRT rules are not annotated with
functional roles, they can be derived straightfor-
wardly from the verbal dependency relations avail-
able in the rule?s templates. The obtained rules are
then added to AmWN as mapping edges.
We only search for rules that map a functional
role in the frame of one verb to any role for the
other verb. Focusing on frame elements avoids ex-
tracting mapping rules learned for adjuncts, which
tend to be of low precision.
3.3 Rule Filtering
In preliminary analysis we found two phenomena,
sense drifting and rare senses, which may reduce
the effectiveness of AmWN-based inference even
if each graph edge by itself, taken out of context, is
correct. To address these phenomena within prac-
tical inference we propose the following optional
methods for rule filtering.
Sense Drifting WordNet verbs typically have
a more fine-grained set of synsets than their re-
lated nominalizations. There are cases where sev-
eral verb synsets are related to the same nomi-
nal synset. Since entailment between a verb and
its nominalization is bidirectional, all such verb
synsets would end up entailing each other via the
nominal node.
Alas, some of these connected verb synsets rep-
resent quite different meanings, which results in
incorrect inferences. This problem, which we call
sense drifting, is demonstrated in Figure 3. To ad-
dress it, we constrain each rule generation chain
to include at most one verb-noun edge, which still
connects the noun and verb hierarchies.
Rare Senses Some word senses in WordNet are
rare. Thus, applying rules that correspond to such
senses yields many incorrect inferences, since
they are typically matched against other frequent
senses of the word. Such a rule is ?have X ? X
is born?, corresponding to a rare sense of ?have?.
WSD is a possible solution for this problem. How-
ever, most state-of-the-art IE, QA and RTE sys-
tems do not rely on WSD methods, which are cur-
rently not sufficiently robust.
To circumvent the rare sense problem, we in-
stead filter out such rules. Each AmWN rule is
validated against our unary-DIRT rule-set, which,
being corpus-based, contains mostly rules for fre-
quent senses. A rule is directly-validated if it is
in the corpus-based rule-set, or if it is a nominal-
verb rule which describes a reliable morpholog-
ical change for a predicate. The AmWN graph-
path that generated each rule is automatically ex-
amined. A rule is considered valid if there is a
sequence of directly-validated intermediate rules
along the path whose transitive chaining generates
the rule. Invalid rules are filtered out.
To illustrate, suppose the rule ?a? d? was gen-
erated by the chain ?a ? b ? c ? d?. It is valid
if there is a rule chain along the path that yields ?a
? d?, e.g. {?a? b?,?b? c?,?c? d?} or {?a? b?,
?b? d?}, whose rules are all directly-validated.
4 Experimental Setup
We follow here the experimental setup presented
in (Szpektor and Dagan, 2008), testing the gener-
ated rules on the ACE 2005 event dataset
6
. This
standard IE benchmark includes 33 types of event
predicates such as Injure, Sue and Divorce
7
. The
ACE guidelines specify for each event its possi-
ble arguments. For example, some of the Injure
event arguments are Agent and Victim. All event
mentions, including their instantiated arguments,
are annotated in a corpus collected from various
sources (newswire articles, blogs, etc.).
To utilize the ACE dataset for evaluating rule
applications, each ACE event predicate was rep-
resented by a set of unary seed templates, one for
each event argument. Example seed templates for
Injure are ?A injure? and ?injure V ?. Each event ar-
gument is mapped to the corresponding seed tem-
plate variable, e.g. ?Agent? to A and ?Victim? to V
in the above example.
We manually annotated each seed template with
a subcategorization frame and an argument func-
tional role, e.g. ?injure
{trans}
V
obj
?. We also in-
cluded relevant WordNet synset-ids, so only rules
fitting the target meaning of the event will be ex-
tracted. In this experiment, we focused only on
the core semantic arguments. Adjuncts (time and
6
http://projects.ldc.upenn.edu/ace/
7
Only 26 frequent event types that correspond to a unique
predicate were tested, following (Szpektor and Dagan, 2008).
31
Synset Members WordNet Gloss
(verb) collar, nail, apprehend, arrest, pick up, nab, cop take into custody
m
(noun) apprehension, arrest, catch, collar, pinch, the act of apprehending (especially apprehending
taking into custody a criminal)
m
(verb) get, catch, capture succeed in catching or seizing, especially after a chase
m
(noun) capture, seizure the act of taking of a person by force
m
(verb) seize take or capture by force
? (hypernym)
(verb) kidnap, nobble, abduct, snatch take away to an undisclosed location against their will
and usually in order to extract a ransom
Figure 3: A WordNet sense-drifting traversal, generating the incorrect inference ?kidnap? arrest?.
place) were ignored since they typically don?t re-
quire argument mapping, the main target for our
assessment.
The ACE corpus was dependency-parsed with
Minipar (Lin, 1998) and annotated with functional
roles and frames for each predicate mention. The
functional roles for a verb mention were taken di-
rectly from the corresponding dependency tree re-
lations. Its frame was chosen to be the largest
WordNet frame of that verb that matched the men-
tion?s roles.
Nominalization frames and functional roles
in the text were annotated using our extended
Nomlex-plus database. For each nominal mention,
we found the largest Nomlex frame whose syntac-
tic argument positions matched those of the men-
tion?s arguments. The arguments were then anno-
tated with the specified roles of the chosen frame.
Ambiguous cases, where the same argument posi-
tion could match multiple roles, were left unanno-
tated, as discussed in Section 2.
Argument mentions for events were found in
the annotated corpus by matching either the seed
templates or the templates entailing them in some
rules. The matching procedure follows the one de-
scribed in Section 2. Templates are matched us-
ing a syntactic matcher that handles simple syn-
tactic variations such as passive-form and con-
junctions. For example, ?wound
{trans}
V
obj
? injure
{trans}
V
obj
? was matched in the text
?Hagel
obj
was wounded
trans
in Vietnam?. A rule
application is considered correct if the matched ar-
gument is annotated in the corpus with the corre-
sponding ACE role.
We note that our system performance on the
ACE task as such is limited. First, WordNet does
not provide all types of needed rules. Second, the
system of our experimental setting is rather basic,
with limited matching capabilities and without a
WSD module. However, this test-set is still very
useful for relative comparison of WordNet and our
proposed AmWN.
5 Results and Analysis
We tested four different rule-set configurations:
a) only the seed templates, without any rules; b)
rules generated based on WordNet 3.0 without ar-
gument mapping, using only synonym and hyper-
nym relations; c) WordNet rules from (b), filtered
using our corpus-based validation method for rare
senses; d) rules generated from our AmWN.
Out of the 8953 non-substitutable inferential re-
lations that we identified in WordNet, our AmWN
implementation created mapping edges for 75% of
8325 Noun-Verb relations and 70% of 628 Verb-
Verb relations. Altogether 41549 mapping edges
between synset nodes were added. A manual er-
ror analysis of these mappings is provided in Sec-
tion 5.2.
Each configuration was evaluated for each ACE
event. We measured the percentage of correct ar-
gument mentions extracted out of all correct argu-
ment mentions annotated for the event (recall) and
out of all argument mentions extracted (precision),
and F1, their harmonic average. We report macro
averages over the 26 event types.
5.1 Results
Table 3 summarizes the results for the different
configurations. As expected, matching only the
seed templates yields the highest precision but
lowest recall. Using the standard WordNet config-
uration actually decreases overall F1 performance.
Though recall increases relatively by 30%, thanks
to WordNet expansions, F1 is penalized by a sharp
32
Configuration R (%) P (%) F1
No Rules 13.5 63.0 20.7
WordNet 17.5 35.3 18.5
WordNet with rule validation 16.5 46.9 20.4
AmWN 20.8 43.9 24.2
Table 3: Recall (R), Precision (P) and F1 results
for the different tested configurations.
relative drop in precision (by 56%). The main rea-
son for this decline is the application of rules in-
volving infrequent word senses, as elaborated in
Section 3.3.
When our rule validation approach is applied
to standard WordNet expansions, a much higher
precision is achieved with only a small decline in
recall. This shows that our corpus-based filtering
method manages to avoid many of the noisy rules
for rare senses, while maintaining those that are
frequently involved in inference.
Finally, our main result shows that adding ar-
gument mapping improves performance substan-
tially. AmWN achieves a much higher recall
than WordNet. Recall increases relatively by 26%
over validated WordNet, and by 54% over the
no-rules baseline. Furthermore, precision drops
only slightly, by 6%, compared to validated Word-
Net. This shows that argument mapping increases
WordNet?s graph connectivity, while our rule-
validation method maintains almost the same pre-
cision for many more generated rules. The im-
provement in overall F1 performance is statisti-
cally significant compared to all other configura-
tions, according to the two-sided Wilcoxon signed
rank test at the level of 0.01 (Wilcoxon, 1945).
5.2 Error Analysis
We manually analyzed the reasons for false pos-
itives (incorrect extractions) and false negatives
(missed extractions) of AmWN by sampling 300
extractions of each type.
From the false positives analysis (Table 4) we
see that practically all generated rules are correct
(99.4%), that is, they would be valid in some con-
texts. Almost all errors come frommatching errors
(including parse errors) and context mismatches,
due to our limited IE implementation. The only
two incorrect rules sampled were due to an in-
correct Nomlex entry and a WordNet synset that
should have been split into two separate senses.
Considering that correct extractions resulted, per
our analysis, from correct rules, the analysis of this
Reason % mentions
Context mismatch 57.2
Match error 33.6
Errors in gold-standard annotation 8.6
Incorrect Rule learned 0.6
Table 4: Distribution of reasons for false positives
(incorrect argument extractions).
Reason % mentions
Rule not learned 67.7
Match error 18.0
Discourse analysis needed 12.0
Argument is predicative 1.3
Errors in gold-standard annotation 1.0
Table 5: Distribution of reasons for false negatives
(missed argument mentions).
sample indicates that virtually all AmWN edges
that get utilized in practice are correct.
Context mismatches, which constitute the ma-
jority of errors (57.2%), occur when the entail-
ing template of a rule is matched in inappropriate
contexts. This occurs typically when the match is
against another sense of the predicate, or when an
argument is not of the requested type (e.g. ?The
Enron sentence? vs. ?A one month sentence?). In
future work, we plan to address this problem by
utilizing context-sensitive application of rules in
the spirit of (Szpektor et al, 2008).
Table 5 presents the false negatives analysis.
Most missed extractions are due to rules that were
not learned (67.7%). These mainly involve com-
plex templates (?file a lawsuit ? sue?) and infer-
ence rules that are not synonyms/hypernyms (?ex-
ecute ? sentence?), which are not widely anno-
tated inWordNet. From further analysis, we found
that 10% of these misses are due to rules that are
generated from AmWN but filtered out by one of
our filtering methods (Section 3.3).
12% of the arguments cannot be extracted by
rules alone, due to required discourse analysis,
while 18% of the mentions were missed due to in-
correct syntactic matching. By assuming correct
matches in these cases and avoiding rule filtering,
we can estimate the upper bound recall of the rule-
set for the ACE dataset to be 40%.
In conclusion, for better performance the sys-
tem should be augmented with context modeling
and better template matching. Additionally, other
rule-bases, e.g. DIRT (Lin and Pantel, 2001),
should be added to increase rule coverage.
33
Configuration R (%) P (%) F1
AmWN 20.8 43.9 24.2
No nominalization mappings 18.1 45.5 21.8
No verb-verb mappings 19.3 43.8 22.8
No rule validation 22.0 30.4 20.9
No sense drift blocking 22.5 37.4 21.7
Table 6: The Recall (R), Precision (P) and F1 re-
sults for ablation tests.
5.3 Component Analysis
Table 6 presents ablations tests that assess the
marginal contribution of each AmWN component.
Nominal-verb and verb-verb mappings contribute
to the graph connectivity, hence the recall reduc-
tion when they are removed.
Complementary to recall components, rule fil-
tering improves precision. When removing the
corpus-based rule-validation, recall increases rel-
atively by 6% but precision drops relatively by
30%, showing the benefit of noisy-rule filtering.
Allowing sense drifting hurts precision, a rela-
tive drop of 22%. Yet, recall increases relatively
by 8%, indicating that some verb synsets, con-
nected via a shared nominal, entail each other even
though they are not connected directly. For exam-
ple, ?foundX? createX? was generated only via
the shared nominal ?founding?. In future work, we
plan to apply AmWN to a coarse-grained set of
WordNet synsets (Palmer et al, 2007) as a possi-
ble solution to sense drifting.
6 Related Work
Several works attempt to extend WordNet with ad-
ditional lexical semantic information (Moldovan
and Rus, 2001; Snow et al, 2006; Suchanek et al,
2007; Clark et al, 2008). However, the only pre-
vious work we are aware of that enriches Word-
Net with argument mappings is (Novischi and
Moldovan, 2006). This work utilizes VerbNet?s
subcategorization frames to identify possible verb
arguments. Argument mapping is provided only
between verbs, ignoring relations between verbs
and nouns. Arguments are mapped based on the-
matic role names shared between frames of dif-
ferent verbs. However, the semantic interpretation
of thematic roles is generally inconsistent across
verbs (Lowe et al, 1997; Kaisser and Webber,
2007). Instead, we discover these mappings from
corpus statistics, offering an accurate approach (as
analyzed in Section 5.2).
A frame semantics approach for argument
mapping between predicates is proposed by the
FrameNet project (Baker et al, 1998). Currently,
FrameNet is the only resource for frame-semantic
argument mappings. However, it is manually con-
structed and currently covers much less predi-
cates and relations than WordNet. Furthermore,
frame-semantic parsers are less robust than syntac-
tic parsers, presently hindering the utilization of
this approach in applications (Burchardt and Pen-
nacchiotti, 2008).
Nomlex argument mapping patterns similar to
ours were derived for IE in (Meyers et al, 1998),
but they were not integrated with any additional
information, such as WordNet.
7 Conclusions
We presented Argument-mapped WordNet
(AmWN), a novel framework for augment-
ing WordNet with argument mappings at the
syntactic representation level. With AmWN,
non-substitutable WordNet relations can also
be utilized correctly, increasing the coverage of
WordNet-based inference. The standard entail-
ment rule representation is augmented in our
work with functional roles and subcategorization
frames, shown to be a feasible extension needed
for correct rule application in general.
Our implementation of AmWN populates
WordNet with mappings based on combining
manual and corpus-based resources. It covers a
broader range of relations compared to prior work
and yields more accurate mappings. We also in-
troduced a novel corpus-based validation mecha-
nism, avoiding rules for infrequent senses. Our
experiments show that AmWN substantially im-
proves standard WordNet-based inference.
In future work we plan to add mappings be-
tween verbs and adjectives and between different
frames of a verb. We also want to incorporate
resources for additional subcategorization frames,
such as VerbNet. Finally, we plan to enhance our
text annotation based on noun-compound disam-
biguation (Lapata and Lascarides, 2003).
Acknowledgements
This work was partially supported by the NEGEV
project (www.negev-initiative.org), the PASCAL-
2 Network of Excellence of the European Commu-
nity FP7-ICT-2007-1-216886, the FBK-irst/Bar-
Ilan University collaboration and the Israel Sci-
ence Foundation grant 1112/08.
34
References
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The berkeley framenet project. In Proceed-
ings of ACL.
Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro,
Danilo Giampiccolo, Bernardo Magnini, and Idan
Szpektor. 2006. The second pascal recognising tex-
tual entailment challenge. In Second PASCAL Chal-
lenge Workshop for Recognizing Textual Entailment.
Regina Barzilay and Michael Elhadad. 1997. Using
lexical chains for text summarization. In Proceed-
ings of ACL.
Aljoscha Burchardt and Marco Pennacchiotti. 2008.
Fate: a framenet-annotated corpus for textual entail-
ment. In Proceedings of LREC.
Peter Clark, Christiane Fellbaum, Jerry R. Hobbs, Phil
Harrison, William R. Murray, and John Thompson.
2008. Augmenting WordNet for Deep Understand-
ing of Text. In Proceedings of STEP 2008.
Danilo Giampiccolo, Bernardo Magnini, Ido Dagan,
and Bill Dolan. 2007. The third pascal recogniz-
ing textual entailment challenge. In Proceedings of
the ACL-PASCAL Workshop on Textual Entailment
and Paraphrasing.
Michael Kaisser and Bonnie Webber. 2007. Question
answering based on semantic roles. In ACL 2007
Workshop on Deep Linguistic Processing.
Karin Kipper, Hoa Trang Dang, and Martha Palmer.
2000. Class-based construction of a verb lexicon.
In Proceedings of AAAI.
Mirella Lapata and Alex Lascarides. 2003. Detect-
ing novel compounds: The role of distributional ev-
idence. In Proceedings of EACL.
Dekang Lin and Patrick Pantel. 2001. Discovery of in-
ference rules for question answering. Natural Lan-
guage Engineering, 7(4):343?360.
Dekang Lin. 1998. Dependency-based evaluation of
minipar. In Proceedings of the Workshop on Eval-
uation of Parsing Systems at LREC 1998, Granada,
Spain.
John B. Lowe, Collin F. Baker, and Charles J. Fillmore.
1997. A frame-semantic approach to semantic an-
notation. In Proceedings of the SIGLEX Workshop
on Tagging Text with Lexical Semantics: Why, What,
and How?
Catherine Macleod, Ralph Grishman, Adam Meyers,
Leslie Barrett, and Ruth Reeves. 1998. NOMLEX:
A Lexicon of Nominalizations. In Proceedings of
EURALEX.
AdamsMeyers, CatherineMacleod, Roman Yangarber,
Ralph Grishman, Leslie Barrett, and Ruth Reeves.
1998. Using nomlex to produce nominalization pat-
terns for information extraction. In Proceedings of
COLING.
Adam Meyers, Ruth Reeves, Catherine Macleod,
Rachel Szekeley, Veronkia Zielinska, and Brian
Young. 2004. The Cross-Breeding of Dictionaries.
In Proceedings of LREC.
George A. Miller. 1995. Wordnet: A lexical database
for english. In Communications of the ACM.
Dan Moldovan and Rada Mihalcea. 2000. Using
wordnet and lexical operators to improve internet
searches. IEEE Internet Computing, 4(1):34?43.
Dan Moldovan and Adrian Novischi. 2002. Lexical
chains for question answering. In Proceedings of
COLING.
Dan Moldovan and Vasile Rus. 2001. Logic form
transformation of wordnet and its applicability to
question answering. In Proceedings of ACL.
Adrian Novischi and Dan Moldovan. 2006. Question
answering with lexical chains propagating verb ar-
guments. In Proceedings of ACL.
Martha Palmer, Hoa Trang Dang, and Christiane
Fellbaum. 2007. Making fine-grained and
coarse-grained sense distinctions, both manually
and automatically. Natural Language Engineering,
13(2):137?163.
Marius Pasca and Sanda Harabagiu. 2001. The in-
formative role of wordnet in open-domain question
answering. In Proceedings of Workshop on WordNet
and Other Lexical Resources: Applications, Exten-
sions and Customizations.
Satoshi Sekine. 2005. Automatic paraphrase discovery
based on context and keywords between ne pairs. In
Proceedings of IWP.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2006.
Semantic taxonomy induction from heterogenous
evidence. In Proceedings of ACL.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: A core of semantic knowl-
edge - unifying wordnet and wikipedia. In Proceed-
ings of WWW2007.
Idan Szpektor and Ido Dagan. 2008. Learning entail-
ment rules for unary templates. In Proceedings of
COLING.
Idan Szpektor, Hristo Tanev, Ido Dagan, and Bonaven-
ture Coppola. 2004. Scaling web based acquisition
of entailment patterns. In Proceedings of EMNLP
2004.
Idan Szpektor, Ido Dagan, Roy Bar-Haim, and Jacob
Goldberger. 2008. Contextual preferences. In Pro-
ceedings of ACL.
Frank Wilcoxon. 1945. Individual comparisons by
ranking methods. Biometrics Bulletin, 1:80?83.
35
Proceedings of the 8th International Conference on Computational Semantics, page 1,
Tilburg, January 2009. c?2009 International Conference on Computational Semantics
Invited Talk
It?s time for a semantic engine
Ido Dagan
Department of Computer Science
Bar Ilan University
Ramat Gan, Israel
dagan@cs.biu.ac.il
Abstract
A common computational goal is to encapsulate the modeling of a target
phenomenon within a unified and comprehensive ?engine?, which addresses
a broad range of the required processing tasks. This goal is followed in
common modeling of the morphological and syntactic levels of natural lan-
guage, where most processing tasks are encapsulated within morphological
analyzers and syntactic parsers. In this talk I suggest that computational
modeling of the semantic level should also focus on encapsulating the var-
ious processing tasks within a unified module (engine). The input/output
specification of such engine (API) can be based on the textual entailment
paradigm, which will be described in brief and suggested as an attractive
framework for applied semantic inference. The talk will illustrate an initial
proposal for the engine?s API, designed to be embedded within the promi-
nent language processing applications. Finally, I will sketch the entailment
formalism and efficient inference algorithm developed at Bar-Ilan University,
which illustrates a principled transformational (rather than interpretational)
approach towards developing a comprehensive semantic engine.
1
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 770?778,
Beijing, August 2010
Recognising Entailment within Discourse
Shachar Mirkin?, Jonathan Berant?, Ido Dagan?, Eyal Shnarch?
? Computer Science Department, Bar-Ilan University
? The Blavatnik School of Computer Science, Tel-Aviv University
Abstract
Texts are commonly interpreted based on
the entire discourse in which they are sit-
uated. Discourse processing has been
shown useful for inference-based applica-
tion; yet, most systems for textual entail-
ment ? a generic paradigm for applied in-
ference ? have only addressed discourse
considerations via off-the-shelf corefer-
ence resolvers. In this paper we explore
various discourse aspects in entailment in-
ference, suggest initial solutions for them
and investigate their impact on entailment
performance. Our experiments suggest
that discourse provides useful informa-
tion, which significantly improves entail-
ment inference, and should be better ad-
dressed by future entailment systems.
1 Introduction
This paper investigates the problem of recognising
textual entailment within discourse. Textual En-
tailment (TE) is a generic framework for applied
semantic inference (Dagan et al, 2009). Under
TE, the relationship between a text (T) and a tex-
tual assertion (hypothesis, H) is defined such that
T entails H if humans reading T would infer that
H is most likely true (Dagan et al, 2006).
TE has been successfully applied to a variety of
natural language processing applications, includ-
ing information extraction (Romano et al, 2006)
and question answering (Harabagiu and Hickl,
2006). Yet, most entailment systems have thus
far paid little attention to discourse aspects of in-
ference. In part, this is the result of the unavail-
ability of adept tools for handling the kind of dis-
course processing required for inference. In addi-
tion in the main TE benchmarks, the Recognising
Textual Entailment (RTE) challenges, discourse
played little role. This state of affairs has started
to change with the recent introduction of the RTE
Pilot ?Search? task (Bentivogli et al, 2009b), in
which assessed texts are situated within complete
documents. In this setting, texts need to be inter-
preted based on their entire discourse (Bentivogli
et al, 2009a), hence attending to discourse issues
becomes essential. Consider the following exam-
ple from the task?s dataset:
(T) The seven men on board were said to have
as little as 24 hours of air.
For the interpretation of T, e.g. the identity and
whereabouts of the seven men, one must consider
T?s discourse. The preceding sentence T?, for in-
stance, provides useful information to that aim:
(T?) The Russian navy worked desperately to
save a small military submarine.
This example demonstrates a common situation in
texts, and is also applicable to the RTE Search
task?s setting. Still, little was done by the task?s
participants to consider discourse, and sentences
were mostly processed independently.
Analyzing the Search task?s development set,
we identified several key discourse aspects that af-
fect entailment in a discourse-dependent setting.
First, we observed that the coverage of available
coreference resolution tools is considerably lim-
ited. To partly address this problem, we extend the
set of coreference relations to phrase pairs with
a certain degree of lexical overlap, as long as no
semantic incompatibility is found between them.
Second, many bridging relations (Clark, 1975) are
realized in the form of ?global information? per-
ceived as known for entire documents. As bridg-
ing falls completely out of the scope of available
resolvers, we address this phenomenon by iden-
tifying and weighting prominent document terms
and allowing their incorporation in inference even
770
when they are not explicitly mentioned in a sen-
tence. Finally, we observed a coherence-related
discourse phenomenon, namely inter-relations be-
tween entailing sentences in the discourse, such
as the tendency of entailing sentences to be ad-
jacent to one another. To that end, we apply a
two-phase classification scheme, where a second-
phase meta-classifier is applied, extracting dis-
course and document-level features based on the
classification of each sentence on its own.
Our results show that, even when simple so-
lutions are employed, the reliance on discourse-
based information is helpful and achieves a sig-
nificant improvement of results. We analyze the
contribution of each component and suggest some
future work to better attend to discourse in entail-
ment systems. To our knowledge, this is the most
extensive effort thus far to empirically explore the
effect of discourse on entailment systems.
2 Background
Discourse plays a key role in text understanding
applications such as question answering or infor-
mation extraction. Yet, such applications typically
only handle a narrow aspect of discourse, address-
ing coreference by term substitution (Dali et al,
2009; Li et al, 2009). The limited coverage and
scope of existing tools for coreference resolution
and the unavailability of tools for addressing other
discourse aspects also contribute to this situation.
For instance, VP anaphora and bridging relations
are usually not handled at all by such resolvers. A
similar situation is seen in the TE research field.
The prominent benchmark for entailment sys-
tems evaluation is the series of RTE challenges.
The main task in these challenges has tradition-
ally been to determine, given a text-hypothesis
pair (T,H), whether T entails H. Discourse played
no role in the first two RTE challenges as
T?s were constructed of short simplified texts.
In RTE-3 (Giampiccolo et al, 2007), where
some paragraph-long texts were included, inter-
sentential relations became relevant for correct in-
ference. Yet the texts in the task were manually
modified to ensure they are self-contained. Con-
sequently, little effort was invested by the chal-
lenges? participants to address discourse issues
beyond the standard substitution of coreferring
nominal phrases, using publicly available tools
such as JavaRap (Qiu et al, 2004) or OpenNLP1,
e.g. (Bar-Haim et al, 2008).
A major step in the RTE challenges towards a
more practical setting of text processing applica-
tions occurred with the introduction of the Search
task in the Fifth RTE challenge (RTE-5). In this
task entailing sentences are situated within doc-
uments and depend on other sentences for their
correct interpretation. Thus, discourse becomes
a substantial factor impacting inference. Surpris-
ingly, discourse hardly received any treatment in
this task beyond the standard use of coreference
resolution (Castillo, 2009; Litkowski, 2009), and
an attempt to address globally-known information
by removing from H words that appear in docu-
ment headlines (Clark and Harrison, 2009).
3 The RTE Search Task
The RTE-5 Search task was derived from the
TAC Summarization task2. The dataset consists
of several corpora, each comprised of news arti-
cles concerning a specific topic, such as the im-
pact of global warming on the Arctic or the Lon-
don terrorist attacks in 2005. Hypotheses were
manually generated based on Summary Content
Units (Nenkova et al, 2007), clause-long state-
ments taken from manual summaries of the cor-
pora. Texts are unmodified sentences in the arti-
cles. Given a topic and a hypothesis, entailment
systems are required to identify all sentences in
the topic?s corpus that entail the hypothesis.
Each sentence-hypothesis pair in both the de-
velopment and test sets was annotated, judging
whether the sentence entails the hypothesis. Out
of 20,104 annotations in the development set, only
810 were judged as positive. This small ratio (4%)
of positive examples, in comparison to 50% in tra-
ditional RTE tasks, better corresponds to the natu-
ral distribution of entailing texts in a corpus, thus
better simulates practical settings.
The task may seem as a variant of information
retrieval (IR), as it requires finding specific texts
in a corpus. Yet, it is fundamentally different from
IR for two reasons. First, the target output is a set
1http://opennlp.sourceforge.net
2http://www.nist.gov/tac/2009/Summarization/
771
of sentences, each evaluated independently, rather
than a set of documents. Second, the decision cri-
terion is entailment rather than relevance.
Despite the above, apparently, IR techniques
provided hard-to-beat baselines for the RTE
Search task (MacKinlay and Baldwin, 2009), out-
performing every other system that relied on in-
ference without IR-based pre-filtering. At the cur-
rent state of performance of entailment systems, it
seems that lexical coverage largely overshadows
any other approach in this task. Still, most (6 out
of 8) participants in the challenge applied their en-
tailment systems to the entire dataset without a
prior retrieval of candidate sentences. F1 scores
for such systems vary between 10% and 33%, in
comparison to over 40% of the IR-based methods.
4 The Baseline RTE System
In this work we used BIUTEE, Bar-Ilan Univer-
sity Textual Entailment Engine (Bar-Haim et al,
2008; Bar-Haim et al, 2009), a state of the art
RTE system, as a baseline and as a basis for our
discourse-based enhancements. This section de-
scribes this system?s architecture; the methods by
which it was augmented to address discourse are
presented in Section 5.
To determine entailment, BIUTEE performs the
following main steps:
Preprocessing First, all documents are parsed
and processed with standard tools for named en-
tity recognition (Finkel et al, 2005) and corefer-
ence resolution. For the latter purpose, we use
OpenNLP and enable the substitution of corefer-
ring terms. This is the only way by which BIUTEE
addresses discourse, representing the state of the
art in entailment systems.
Entailment-based transformations Given a
T-H pair (both represented as dependency
parse trees), the system applies a sequence of
knowledge-based entailment transformations over
T, generating a set of texts which are entailed by
it. The goal is to obtain consequent texts which
are more similar to H. Based on preliminary re-
sults on the development set, in our experiments
(Section 6) we use WordNet (Fellbaum, 1998) as
the system?s only knowledge resource, using its
synonymy, hyponymy and derivation relations.
Classification A supervised classifier, trained
on the development set, is applied to determine
entailment of each pair based on a set of syntactic
and lexical syntactic features assessing the degree
by which T and its consequents cover H.
5 Addressing Discourse
In the following subsections we describe the
prominent discourse phenomena that affect infer-
ence, which we have identified in an analysis of
the development set and addressed in our imple-
mentation. As mentioned, these phenomena are
poorly addressed by available reference resolvers
or fall completely out of their scope.
5.1 Augmented coreference set
A large number of coreference relations are com-
prised of terms which share lexical elements, (e.g.
?airliners?s first flight? and ?Airbus A380?s first
flight?). Although common in coreference rela-
tions, standard resolvers miss many of these cases.
For the purpose of identifying additional corefer-
ring terms, we consider two noun phrases in the
same document as coreferring if: (i) their heads
are identical and (ii) no semantic incompatibil-
ity is found between their modifiers. The types
of incompatibility we handle are: (a) mismatch-
ing numbers, (b) antonymy and (c) co-hyponymy
(coordinate terms), as specified by WordNet. For
example, two nodes of the noun distance would
be considered incompatible if one is modified by
short and the second by its antonym long. Simi-
larly, two modifier co-hyponyms of distance, such
as walking and running would also result such
an incompatibility. Adding more incompatibility
types (e.g. first vs. second flight) may further im-
prove the precision of this method.
5.2 Global information
Key terms or prominent pieces of information that
appear in the document, typically at the title or the
first few sentences, are many times perceived as
?globally? known throughout the document. For
example, the geographic location of the document
theme, mentioned at the beginning of the docu-
ment, is assumed to be known from that point on,
and will often not be mentioned explicitly in fur-
ther sentences. This is a bridging phenomenon
772
that is typically not addressed by available dis-
course processing tools. To compensate for that,
we identify key terms for each document based
on tf-idf scores and consider them as global in-
formation for that document. For example, global
terms for the topic discussing the ice melting in
the Arctic, typically contain a location such as
Arctic or Antarctica and terms referring to ice, like
permafrost or iceshelf.
We use a variant of tf-idf, where term frequency
is computed as follows: tf(ti,j) = ni,j+~?> ? ~fi,j .
Here, ni,j is the frequency of term i in document j
(ti,j), which is incremented by additional positive
weights (~?) for a set of features ( ~fi,j) of the term.
Based on our analysis, we defined the following
features, which correlated mostly with global in-
formation: (i) does the term appear in the title?
(ii) is it a proper name? (iii) is it a location? The
weights for these features are set empirically.
The document?s top-n global terms are added
to each of its sentences. As a result, a global term
that occurs in the hypothesis is matched in each
sentence of the document, regardless of whether
the term explicitly appears in the sentence.
Considering the previous sentence Another
method for addressing missing coreference and
bridging relations is based on the assumption that
adjacent sentences often refer to the same entities
and events. Thus, when extracting classification
features for a given sentence, in addition to the
features extracted from the parse tree of the sen-
tence itself, we extract the same set of features
from the current and previous sentences together.
Recall the example presented in Section 1. T is
annotated as entailing the hypothesis ?The AS-28
mini-submarine was trapped underwater?, but the
word submarine, e.g., appears only in its preced-
ing sentence T?. Thus, considering both sentences
together when classifying T increases its coverage
of the hypothesis. Indeed, a bridging reference re-
lates on board in T with submarine in T?, justify-
ing our assumption in this case.
5.3 Document-level classification
Beyond discourse references addressed above,
further information concerning discourse and doc-
ument structure is available in the Search setting
and may contribute to entailment classification.
We observed that entailing sentences tend to come
in bulks. This reflects a common coherence as-
pect, where the discussion of a specific topic is
typically continuous rather than scattered across
the entire document. This locality phenomenon
may be useful for entailment classification since
knowing that a sentence entails the hypothesis in-
creases the probability that adjacent sentences en-
tail the hypothesis as well.
To capture this phenomenon, we use a two-
phase meta-classification scheme, in which a
meta-classifier utilizes entailment classifications
of the first classification phase to extract meta-
features and determine the final classification de-
cision. This scheme also provides a convenient
way to combine scores from multiple classifiers
used in the first classification phase. We refer
to these as base-classifiers. This scheme and the
meta-features we used are detailed hereunder.
Let us write (s, h) for a sentence-hypothesis
pair. We denote the set of pairs in the development
(training) set asD and in the test set as T . We split
D into two halves, D1 and D2. We make use of n
base-classifiers, C1, . . . , Cn, among which C? is
a designated classifier with additional roles in the
process, as described below. Classifiers may dif-
fer, for example, in their classification algorithm.
An additional meta-classifier is denoted CM . The
classification scheme is shown as Algorithm 1.
Algorithm 1 Meta-classification
Training
1: Extract features for every (s, h) in D
2: Train C1, . . . , Cn on D1
3: Classify D2, using C1, . . . , Cn
4: Extract meta-features for D2 using the
classification of C1, . . . , Cn
5: Train CM on D2
Classification
6: Extract features for every (s, h) in T
7: Classify T using C1, . . . , Cn
8: Extract meta-features for T
9: Classify T using CM
At Step 1, features are extracted for every (s, h)
pair in the training set, as in the baseline system.
773
In Steps 2 and 3 we split the training set into two
halves (taking half of each topic), train n different
classifiers on the first half and then classify the
second half using each of the n classifiers. Given
the classification scores of the n base-classifiers
to the (s, h) pairs in the second half of the train-
ing set, D2, we add in Step 4 the meta-features
described in Section 5.3.1.
After adding the meta-features, we train
(Step 5) a meta-classifier on this new set of fea-
tures. Test sentences then go through the same
process: features are extracted for them and they
are classified by the already trained n classifiers
(Steps 6 and 7), meta-features are extracted in
Step 8, and a final classification decision is made
by the meta-classifier in Step 9.
A retrieval step may precede the actual en-
tailment classification, allowing the processing of
fewer and potentially ?better? candidates.
5.3.1 Meta-features
The following features are extracted in our
meta-classification scheme:
Classification scores The classification score of
each of the n base-classifiers.
Title entailment In many texts, and in news ar-
ticles in particular, the title and the first few sen-
tences often represent the entire document?s con-
tent. Thus, knowing whether these sentences en-
tail the hypothesis may be an indicator to the gen-
eral potential of the document to include entailing
sentences. Two binary features are added accord-
ing to the classification of C? indicating whether
the title entails the hypothesis and whether the first
sentence entails it.
Second-closest entailment Considering the lo-
cality phenomenon described above, we add a fea-
ture assigning higher scores to sentences in the
vicinity of an entailment environment. This fea-
ture is computed as the distance to the second-
closest entailing sentence in the document (count-
ing the sentence itself as well), according to the
classification ofC?. Formally, let i be the index of
the current sentence and J be the set of indices of
entailing sentences in the document according to
C?. For each j ? J we compute di,j = |i?j|, and
choose the second smallest di,j as di. The idea is
Ent?
# 1110987654321
NO NOYESYESYESYESNONONONONO
d
2nd cl
oses
t
8887 or 96 or 8777777
2 3111123456
d
Close
st
8887 or 96 or 8766666
1 2111112345
Figure 1: Comparison of the closest and second-closest
schemes when applied to a bulk of entailing sentences (in
white) situated within a non-entailing environment (in gray).
Unlike the closest one, the second-closest scheme assigns
larger distance values to non-entailing sentences located on
the ?edge? of the bulk (5 and 10) than to entailing ones.
that if entailing sentences indeed always come in
bulks, then di = 1 for all entailing sentences, but
di > 1 for all non-entailing ones. Figure 1 illus-
trates such a case, comparing the second-closest
distance with the distance to the closest entailing
sentence. In the closest scheme we do not count
the sentence as closest to itself since it would dis-
regard the environment of the sentence altogether,
eliminating the desired effect. We scale the dis-
tance and add the feature score: ? log(di).
Smoothed entailment This feature addressed
the locality phenomenon by smoothing the
classification score of sentence i with the scores
of adjacent sentences, weighted by their distance
from the current sentence i. Let s(i) be the
score assigned by C? to sentence i. We add the
Smoothed Entailment feature score:
SE(i) =
?
w(b|w|?s(i+w))?
w(b|w|)
where 0 < b < 1 is the decay parameter and w is
an integer bounded between?N and N , denoting
the distance from sentence i.
1st sentence entailing title Bensley and Hickl
(2008) showed that the first sentence in a news ar-
ticle typically entails the article?s title. We there-
fore assume that in each document, s1 ? s0,
where s1 and s0 are the document?s first sentence
and title respectively. Hence, under entailment
transitivity, if s0 ? h then s1 ? h. The cor-
responding binary feature states whether the sen-
tence being classified is the document?s first sen-
tence and the title entails h according to C?.
774
P (%) R (%) F1 (%)
BIU-BL 14.53 55.25 23.00
BIU-DISC 20.82 57.25 30.53
BIU-BL3 14.86 59.00 23.74
BIU-DISCno?loc 22.35 57.12 32.13All-yes baseline 4.6 100.0 8.9
Table 1: Micro-average results.
Note that the above locality-based features rely
on high accuracy of the base classifier C?. Oth-
erwise, it will provide misleading information to
the features computation. We analyze the effect
of this accuracy in Section 6.
6 Results and Analysis
Using the RTE-5 Search data, we compare
BIUTEE in its baseline configuration (cf. Sec-
tion 4), denoted BIU-BL, with its discourse-aware
enhancement (BIU-DISC) which uses all the com-
ponents described in Section 5. To alleviate the
strong IR effect described in Section 3, both sys-
tems are applied to the complete datasets (both
training and test), without candidates pre-filtering.
BIU-DISC uses three base-classifiers (n = 3):
SVMperf (Joachims, 2006), and Na??ve Bayes and
Logistic Regression from the WEKA package
(Witten and Frank, 2005). The first among these
is set as our designated classifier C?, which is
used for the computation of the document-level
features. SVMperf is also used for the meta-
classifier. For the smoothed entailment score (cf.
Section 5.3), we used b = 0.9 and N = 3. Global
information is added by enriching each sentence
with the highest-ranking term in the document, ac-
cording to tf-idf scores (cf. Section 5.2), where
document frequencies were computed based on
about half a million documents from the TIP-
STER corpus (Harman, 1992). The set of weights
~? equals {2, 1, 4} for title terms, proper names and
locations, respectively. All parameters were tuned
based on a 10-fold cross-validation on the devel-
opment set, optimizing the micro-averaged F1.
The results are presented in Table 1. As can be
seen in the table, BIU-DISC outperforms BIU-BL in
every measure, showing the impact of addressing
discourse in this setting. To rule out the option that
the improvement is simply due to the fact that we
use three classifiers for BIU-DISC and a single one
P (%) R (%) F1 (%)
By Topic
BIU-BL 16.54 55.62 25.50
BIU-DISC 22.69 57.96 32.62
All-yes baseline 4.85 100.00 9.25
By Hypothesis
BIU-BL 22.87 59.62 33.06
BIU-DISC 27.81 61.97 38.39
All-yes baseline 4.96 100.00 9.46
Table 2: Macro-average results.
for BIU-BL, we show (BIU-BL3) the results when
the baseline system is applied in the same meta-
classification configuration as BIU-DISC, with the
same three classifiers. Apparently, without the
discourse information this configuration?s contri-
bution is limited.
As mentioned in Section 5.3, the benefit from
the locality features rely directly on the perfor-
mance of the base classifiers. Hence, considering
the low precision scores obtained here, we applied
BIU-DISC to the data in the meta-classification
scheme, but with locality features removed. The
results, shown as BIU-DISCno?loc in the Table, in-
dicate that indeed performance increases without
these features. The last line of the table shows the
results obtained by a na??ve baseline where all test-
set pairs are considered entailing.
For completeness, Table 2 shows the macro-
averaged results, when averaged over the topics or
over the hypotheses. Although we tuned our sys-
tem to maximize micro-averaged F1, these figures
comply with the ones shown in Table 1.
Analysis of locality As discussed in Section 5,
determining whether a sentence entails a hypothe-
sis should take into account whether adjacent sen-
tences also entail the hypothesis. In the above ex-
periment we were unable to show the contribution
of our system?s component that attempts to cap-
ture this information; on the contrary, the results
show it had a negative impact on performance.
Still, we claim that this information can be use-
ful when used within a more accurate system. We
try to validate this conjecture by understanding
how performance of the locality features varies as
the systems becomes more accurate. We do so via
the following simulation.
When classifying a certain sentence, the classi-
775
2025303540 0.4
0.45
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
p
F1
Figure 2: F1 performance of BIU-DISC as a function of
the accuracy in classifying adjacent sentences.
fications of its adjacent sentences are given by an
oracle classifier that provides the correct answer
with probability p. The system is applied using
two locality features: the 1st sentence entailing
title feature and a close variant of the smoothed
entailment feature, which calculates the weighted
average of adjacent sentences, but disregards the
score of the currently evaluated sentence.3 Thus
we supply information about adjacent sentences
and test whether overall performance increases
with the accuracy of this information.
We performed this experiment for p in a range
of [0.5-1.0]. Figure 2 shows the results of this sim-
ulation, based on the average F1 of five runs for
each p. Since performance, from a certain point,
increases with the accuracy of the oracle classi-
fier, we can conclude that indeed precise infor-
mation about adjacent sentences improves perfor-
mance on the current sentence, and that locality is
a true phenomenon in the data. We note, however,
that performance improves only when accuracy is
very high, suggesting the currently limited prac-
tical potential of this information, at least in the
way locality was represented in this work.
Ablation tests Table 3 presents the results of the
ablation tests performed to evaluate the contribu-
tion of each component. Based on the result re-
ported in Table 1 and the above discussion, the
tests were performed relative to BIU-DISCno?loc,
the optimal configuration. As seen in the table,
the removal of each component causes a drop
in results. For global information we see a mi-
3The second-closest entailment feature was not used as it
considers the oracle?s decision for the current sentence, while
we wish to use only information about adjacent sentences.
Component removed F1 (%) ?F1 (%)
Previous sent. features 28.55 3.58
Augmented coref. 26.73 5.40
Global information 31.76 0.37
Table 3: Results of ablation tests relative to
BIU-DISCno?loc. The columns specify the compo-nent removed, the micro-averaged F1 score achieved without
it, and the marginal contribution of the component.
nor difference, which is not surprising considering
the conservative approach we took, using a sin-
gle global term for each sentence. Possibly, this
information is also included in the other compo-
nents, thus proving no marginal contribution rel-
ative to them. Under the conditions of an over-
whelming majority of negative examples, this is
a risky method to use, and should be considered
when the ratio of positive examples is higher. For
future work, we intend to use this information via
classification features (e.g. the coverage obtained
with and without global information), rather than
the crude addition of the term to the sentence.
Analysis of augmented coreferences We an-
alyzed the performance of the component for
augmenting coreference relations relative to the
OpenNLP resolver. Recall that our component
works on top of the resolver?s output and can add
or remove coreference relations. As a complete
annotation of coreference chains in the dataset is
unavailable, we performed the following evalua-
tion. Recall is computed based on the number
of identified pairs from a sample of 100 intra-
document coreference and bridging relations from
the annotated dataset described in (Mirkin et al,
2010). Precision is computed based on 50 pairs
sampled from the output of each method, equally
distributed over topics. The results, shown in Ta-
ble 4, indicate the much higher recall obtained
by our component at some cost in precision. Al-
though rather simple, the ablation test of this com-
ponent shows its usefulness. Still, both methods
achieve low absolute recall, suggesting the need
for more robust tools for this task.
P (%) R (%) F1 (%)
OpenNLP 74 16 26.3
Augmented coref. 60 28 38.2
Table 4: Performance of coreference methods.
776
05101520253035404550 0
10
20
30
40
50
60
70
80
90
100
k
F1
BIU-B
L
BIU-D
ISC
Lucen
e
Figure 3: F1 performance as a function of the number of
retrieved candidates.
Candidate retrieval setting As mentioned in
Section 3, best performance of RTE systems in the
task was obtained when applying a first step of IR-
based candidate filtering. We therefore compare
the performance of BIU-DISC with that of BIU-BL
under this setting as well.4 For candidate retrieval
we used Lucene, a state of the art search engine5,
in a range of top-k retrieved candidates. The re-
sults are shown in Figure 3. For reference, the fig-
ure also shows the performance along this range
of Lucene as-is, when no further inference is ap-
plied to the retrieved candidates.
While BIU-DISC does not outperform BIU-BL at
every point, the area under the curve is clearly
larger for BIU-DISC. The figure also indicates that
BIU-DISC is far more robust, maintaining a stable
F1 and enabling a stable tradeoff between recall
and precision along the whole range (recall ranges
between 42% and 55% for k ? [15 ? 100], with
corresponding precision range of 51% to 33%).
Finally, Table 5 shows the results of the best
systems as determined in our first experiment.
We performed a single experiment to compare
BIU-DISCno?loc and BIU-BL3 under a candidate re-
trieval setting, using k = 20, where both systems
highly perform. We compare these results to the
highest score obtained by Lucene, as well as to the
two best submissions to the RTE-5 Search task6.
BIU-DISCno?loc outperforms all other methods and
its result is significantly better than BIU-BL3 with
p < 0.01 according to McNemar?s test.
4This time, for global information, the document?s three
highest ranking terms were added to each sentence.
5http://lucene.apache.org
6The best one is an earlier version of this work (Mirkin et
al., 2009); the second is MacKinlay and Baldwin?s (2009).
P (%) R (%) F1 (%)
BIU-DISCno?loc 50.77 45.12 47.78
BIU-BL3 51.68 40.38 45.33
Lucene, top-15 35.93 52.50 42.66
RTE-5 best 40.98 51.38 45.59
RTE-5 second-best 42.94 38.00 40.32
Table 5: Performance of best configurations.
7 Conclusions
While it is generally assumed that discourse inter-
acts with semantic entailment inference, the con-
crete impacts of discourse on such inference have
been hardly explored. This paper presented a first
empirical investigation of discourse processing
aspects related to entailment. We argue that avail-
able discourse processing tools should be substan-
tially improved towards this end, both in terms of
the phenomena they address today, namely nom-
inal coreference, and with respect to the cover-
ing of additional phenomena, such as bridging
anaphora. Our experiments show that even rather
simple methods for addressing discourse can have
a substantial positive impact on the performance
of entailment inference. Concerning the local-
ity phenomenon stemming from discourse coher-
ence, we learned that it does carry potentially use-
ful information, which might become beneficial
in the future when better-performing entailment
systems become available. Until then, integrating
this information with entailment confidence may
be useful. Overall, we suggest that entailment sys-
tems should extensively incorporate discourse in-
formation, while developing sound algorithms for
addressing various discourse phenomena, includ-
ing the ones described in this paper.
Acknowledgements
The authors are thankful to Asher Stern and Ilya
Kogan for their help in implementing and evalu-
ating the augmented coreference component, and
to Roy Bar-Haim for useful advice concerning
this paper. This work was partially supported
by the Israel Science Foundation grant 1112/08
and the PASCAL-2 Network of Excellence of the
European Community FP7-ICT-2007-1-216886.
Jonathan Berant is grateful to the Azrieli Foun-
dation for the award of an Azrieli Fellowship.
777
References
Bar-Haim, Roy, Jonathan Berant, Ido Dagan, Iddo
Greental, Shachar Mirkin, Eyal Shnarch, and Idan
Szpektor. 2008. Efficient semantic deduction and
approximate matching over compact parse forests.
In Proc. of Text Analysis Conference (TAC).
Bar-Haim, Roy, Jonathan Berant, and Ido Dagan.
2009. A compact forest for scalable inference
over entailment and paraphrase rules. In Proc. of
EMNLP.
Bensley, Jeremy and Andrew Hickl. 2008. Unsuper-
vised resource creation for textual inference appli-
cations. In Proc. of LREC.
Bentivogli, Luisa, Ido Dagan, Hoa Trang Dang, Danilo
Giampiccolo, Medea Lo Leggio, and Bernardo
Magnini. 2009a. Considering discourse refer-
ences in textual entailment annotation. In Proc. of
the 5th International Conference on Generative Ap-
proaches to the Lexicon (GL2009).
Bentivogli, Luisa, Ido Dagan, Hoa Trang Dang, Danilo
Giampiccolo, and Bernardo Magnini. 2009b. The
fifth PASCAL recognizing textual entailment chal-
lenge. In Proc. of TAC.
Castillo, Julio J. 2009. Sagan in TAC2009: Using
support vector machines in recognizing textual en-
tailment and TE search pilot task. In Proc. of TAC.
Clark, Peter and Phil Harrison. 2009. An inference-
based approach to recognizing entailment. In Proc.
of TAC.
Clark, Herbert H. 1975. Bridging. In Schank, R. C.
and B. L. Nash-Webber, editors, Theoretical issues
in natural language processing, pages 169?174. As-
sociation of Computing Machinery.
Dagan, Ido, Oren Glickman, and Bernardo Magnini.
2006. The PASCAL recognising textual entailment
challenge. In Machine Learning Challenges, vol-
ume 3944 of Lecture Notes in Computer Science,
pages 177?190. Springer.
Dagan, Ido, Bill Dolan, Bernardo Magnini, and Dan
Roth. 2009. Recognizing textual entailment: Ratio-
nal, evaluation and approaches. Natural Language
Engineering, pages 15(4):1?17.
Dali, Lorand, Delia Rusu, Blaz Fortuna, Dunja
Mladenic, and Marko Grobelnik. 2009. Question
answering based on semantic graphs. In Proc. of the
Workshop on Semantic Search (SemSearch 2009).
Fellbaum, Christiane, editor. 1998. WordNet: An
Electronic Lexical Database (Language, Speech,
and Communication). The MIT Press.
Finkel, Jenny Rose, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In Proc. of ACL.
Giampiccolo, Danilo, Bernardo Magnini, Ido Dagan,
and Bill Dolan. 2007. The third PASCAL recog-
nizing textual entailment challenge. In Proc. of the
ACL-PASCAL Workshop on Textual Entailment and
Paraphrasing.
Harabagiu, Sanda and Andrew Hickl. 2006. Methods
for using textual entailment in open-domain ques-
tion answering. In Proc. of ACL.
Harman, Donna. 1992. The DARPA TIPSTER
project. SIGIR Forum, 26(2):26?28.
Joachims, Thorsten. 2006. Training linear SVMs in
linear time. In Proc. of the ACM Conference on
Knowledge Discovery and Data Mining (KDD).
Li, Fangtao, Yang Tang, Minlie Huang, and Xiaoyan
Zhu. 2009. Answering opinion questions with ran-
dom walks on graphs. In Proc. of ACL-IJCNLP.
Litkowski, Ken. 2009. Overlap analysis in textual en-
tailment recognition. In Proc. of TAC.
MacKinlay, Andrew and Timothy Baldwin. 2009. A
baseline approach to the RTE5 search pilot. In Proc.
of TAC.
Mirkin, Shachar, Roy Bar-Haim, Jonathan Berant, Ido
Dagan Eyal Shnarch, Asher Stern, and Idan Szpek-
tor. 2009. Addressing discourse and document
structure in the RTE search task. In Proc. of TAC.
Mirkin, Shachar, Ido Dagan, and Sebastian Pado?.
2010. Assessing the role of discourse references in
entailment inference. In Proc. of ACL.
Nenkova, Ani, Rebecca Passonneau, and Kathleen
Mckeown. 2007. The pyramid method: incorpo-
rating human content selection variation in summa-
rization evaluation. In ACM Transactions on Speech
and Language Processing.
Qiu, Long, Min-Yen Kan, and Tat-Seng Chua. 2004.
A public reference implementation of the RAP
anaphora resolution algorithm. In Proc. of LREC.
Romano, Lorenza, Milen Kouylekov, Idan Szpektor,
and Ido Dagan. 2006. Investigating a generic
paraphrase-based approach for relation extraction.
In Proc. of EACL.
Witten, Ian H. and Eibe Frank. 2005. Data Mining:
Practical machine learning tools and techniques,
2nd Edition. Morgan Kaufmann, San Francisco.
778
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 194?204, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Learning Verb Inference Rules from Linguistically-Motivated Evidence
Hila Weisman?, Jonathan Berant?, Idan Szpektor?, Ido Dagan?
? Computer Science Department, Bar-Ilan University
? The Blavatnik School of Computer Science, Tel Aviv University
? Yahoo! Research Israel
{weismah1,dagan}@cs.biu.ac.il
{jonatha6}@post.tau.ac.il
{idan}@yahoo-inc.com
Abstract
Learning inference relations between verbs is
at the heart of many semantic applications.
However, most prior work on learning such
rules focused on a rather narrow set of in-
formation sources: mainly distributional sim-
ilarity, and to a lesser extent manually con-
structed verb co-occurrence patterns. In this
paper, we claim that it is imperative to uti-
lize information from various textual scopes:
verb co-occurrence within a sentence, verb co-
occurrence within a document, as well as over-
all corpus statistics. To this end, we propose
a much richer novel set of linguistically mo-
tivated cues for detecting entailment between
verbs and combine them as features in a su-
pervised classification framework. We empir-
ically demonstrate that our model significantly
outperforms previous methods and that infor-
mation from each textual scope contributes to
the verb entailment learning task.
1 Introduction
Inference rules are an important building block of
many semantic applications, such as Question An-
swering (Ravichandran and Hovy, 2002) and In-
formation Extraction (Shinyama and Sekine, 2006).
For example, given the sentence ?Churros are
coated with sugar?, one can use the rule ?coat ?
cover? to answer the question ?What are Churros
covered with??. Inference rules specify a directional
inference relation between two text fragments, and
we follow the Textual Entailment modeling of infer-
ence (Dagan et al2006), which refers to such rules
as entailment rules. In this work we focus on one
of the most important rule types, namely, lexical en-
tailment rules between verbs (verb entailment), e.g.,
?whisper ? talk?, ?win ? play? and ?buy ? own?.
The significance of such rules has led to active re-
search in automatic learning of entailment rules be-
tween verbs or verb-like structures (Zanzotto et al
2006; Abe et al2008; Schoenmackers et al2010).
Most prior efforts to learn verb entailment rules
from large corpora employed distributional similar-
ity methods, assuming that verbs are semantically
similar if they occur in similar contexts (Lin, 1998;
Berant et al2012). This led to the automatic ac-
quisition of large scale knowledge bases, but with
limited precision. Fewer works, such as VerbOcean
(Chklovski and Pantel, 2004), focused on identi-
fying verb entailment through verb instantiation of
manually constructed patterns. For example, the
sentence ?he scared and even startled me? implies
that ?startle? scare?. This led to more precise rule
extraction, but with poor coverage since contrary
to nouns, in which patterns are common (Hearst,
1992), verbs do not co-occur often within rigid pat-
terns. However, verbs do tend to co-occur in the
same document, and also in different clauses of the
same sentence.
In this paper, we claim that on top of standard
pattern-based and distributional similarity methods,
corpus-based learning of verb entailment can greatly
benefit from exploiting additional linguistically-
motivated cues that are specific to verbs. For in-
stance, when verbs co-occur in different clauses of
the same sentence, the syntactic relation between the
clauses can be viewed as a proxy for the semantic re-
lation between the verbs. Moreover, we claim that to
194
improve performance it is crucial to combine infor-
mation sources from different textual scopes: verb
co-occurrence within a sentence and within a docu-
ment, distributional similarity over the entire corpus,
etc.
Our contribution in this paper is two-fold. First,
we suggest a novel set of entailment indicators that
help to detect the likelihood of verb entailment.
Our novel indicators are specific to verbs and are
linguistically-motivated. Second, we encode our
novel indicators as features within a supervised clas-
sification framework and integrate them with other
standard features adapted from prior work. This re-
sults in a supervised corpus-based learning method
that combines verb entailment information at the
sentence, document and corpus levels.
We test our model on a manually labeled data
set, and show that it outperforms the best perform-
ing previous work by 24%. In addition, we ex-
amine the effectiveness of indicators that operate at
the sentence-level, document-level and corpus-level.
This analysis reveals that using a rich and diverse
set of indicators that capture sentence-level interac-
tions between verbs substantially improves verb en-
tailment detection.
2 Background
The main approach for learning entailment rules be-
tween verbs and verb-like structures has employed
the distributional hypothesis, which assumes that
words with similar meanings appear in similar con-
texts. For example, we expect the words ?buy? and
?purchase? to occur with similar subjects and objects
in a large corpus. This observation has led to ample
work on developing both symmetric and directional
similarity measures that attempt to capture semantic
relations between lexical items by comparing their
neighborhood context (Lin, 1998; Weeds and Weir,
2003; Geffet and Dagan, 2005; Szpektor and Dagan,
2008; Kotlerman et al2010).
A far less explored direction for learning verb en-
tailment involves exploiting verb co-occurrence in
a sentence or a document. One prominent work
is Chklovsky and Pantel?s VerbOcean (2004). In
VerbOcean, the authors manually constructed 33
patterns and divided them into five pattern groups,
where each group signals one of the following five
semantic relations: similarity, strength, antonymy,
enablement and happens-before. For example, the
pattern ?Xed and later Yed? signals the happens-
before relation between the verbs ?X? and ?Y?. Start-
ing with candidate verb pairs based on a distribu-
tional similarity measure, the patterns are used to
choose a semantic relation per verb pair based on
the different patterns this pair instantiates. This
method is more precise than distributional similarity
approaches, but it is highly susceptible to sparseness
issues, since verbs do not typically co-occur within
rigid patterns. Utilizing verb co-occurrence at the
document level, Chambers and Jurafsky (2008) es-
timate whether a pair of verbs is narratively related
by counting the number of times the verbs share an
argument in the same document. In a similar man-
ner, Pekar (2008) detects entailment rules between
templates from shared arguments within discourse-
related clauses in the same document.
Recently, supervised classification has become
standard in performing various semantic tasks.
Mirkin et al2006) introduced a system for learn-
ing entailment rules between nouns (e.g., ?novel?
book?) that combines distributional similarity and
Hearst patterns as features in a supervised clas-
sifier. Pennacchiotti and Pantel (2009) augment
Mirkin et alfeatures with web-based features for
the task of entity extraction. Hagiwara et al2009)
perform synonym identification based on both dis-
tributional and contextual features. Tremper (2010)
extract ?loose? sentence-level features in order to
identify the presupposition relation (e.g., , the verb
?win? presupposes the verb ?play?). Last, Be-
rant et al2012) utilized various distributional
similarity features to identify entailment between
lexical-syntactic predicates.
In this paper, we follow the supervised approach
for semantic relation detection in order to identify
verb entailment. While we utilize and adapt useful
features from prior work, we introduce a diverse set
of novel features for the task, effectively combining
verb co-occurrence information at the sentence, doc-
ument, and corpus levels.
3 Linguistically-Motivated Indicators
As mentioned in Section 1, verbs behave quite dif-
ferently from nouns in corpora. In this section, we
195
introduce linguistically motivated indicators that are
specific to verbs and may signal the semantic re-
lation between verb pairs. Then, in Section 4 we
describe how these indicators are exactly encoded
as features within a supervised classification frame-
work.
Verb co-occurrence When (non-auxiliary) verbs
co-occur in a sentence, they are often the main verbs
of different clauses. We thus aim to use information
about the relation between clauses to learn about
the relation between the clauses? main verbs. Dis-
course markers (Hobbs, 1979; Schiffrin, 1988) are
lexical terms such as ?because? and ?however? that
indicate a semantic relation between discourse frag-
ments (i.e., propositions or speech acts). We suggest
that these markers can indicate semantic relations
between the main verbs of the connected clauses.
For example, in the sentence ?He always snores
while he sleeps?, the marker ?while? indicates a tem-
poral relation between the clauses, indicating that
?snoring? occurs while ?sleeping? (and so ?snore?
sleep?).
Often the relation between clauses is not ex-
pressed explicitly with an overt discourse marker,
but is still implied by the syntactic structure of
the sentence. For example, in dependency parsing
the relation can be captured by labeled dependency
edges expressing that one clause is an adverbial ad-
junct of the other, or that two clauses are coordi-
nated. This can indicate the existence (or lack) of
entailment between verbs. For instance, in the sen-
tence ?When I walked into the room, he was working
out?, the verb ?walk? is an adverbial adjunct of the
verb ?work out?. Such co-occurrence structure does
not indicate a deep semantic relation, such as entail-
ment, between the two verbs.
Verb classes Verb classes are sets of semantically-
related verbs sharing some linguistic properties
(Levin, 1993). One of the most general verb classes
are stative vs. event verbs (Jackendoff, 1983). Sta-
tive verb, such as ?love? and ?think?, usually describe
a state that lasts some time. On the other hand, event
verbs, such as ?run? and ?kiss?, describe an action.
We hypothesize that verb classes are relevant for de-
termining entailment, for example, that stative verbs
are not likely to entail event verbs.
Verb generality Verb-particle constructions are
multi-word expressions consisting of a head verb
and a particle, e.g., switch off (Baldwin and Villav-
icencio, 2002). We conjecture that the more gen-
eral a verb is, the more likely it is to appear with
many different particles. Detecting verb generality
can help us tackle an infamous property of distribu-
tional similarity methods, namely, the difficulty in
detecting the direction of entailment (Berant et al
2012). For example, the verb ?cover? appears with
many different particles such as ?up? and ?for?, while
the verb ?coat? does not. Thus, assuming we have
evidence for an entailment relation between the two
verbs, this indicator can help us discern the direction
of entailment and determine that ?coat? cover?.
Typed Distributional Similarity As discussed in
section 2, distributional similarity is the most com-
mon source of information for learning semantic re-
lations between verbs. Yet, we suggest that on top
of standard distributional similarity measures, which
take several verbal arguments into account (such as
subject, object, etc.) simultaneously, we should also
focus on each type of argument independently. In
particular, we apply this approach to compute simi-
larity between verbs based on the set of adverbs that
modify them. Our hypothesis is that adverbs may
contain relevant information for capturing the direc-
tion of entailment. If a verb appears with a small set
of adverbs, it is more likely to be a specific verb that
already conveys a specific action or state, making an
additional adverb redundant. For example, the verb
?whisper? conveys a specific manner of talking and
will probably not appear with the adverb ?loudly?,
while the verb ?talk? is more likely to appear with
such an adverb. Thus, measuring similarity based
solely on adverb modifiers could reveal this phe-
nomenon.
4 Supervised Entailment Detection
In the previous section, we discussed linguistic ob-
servations regarding novel indicators that may help
in detecting entailment relations between verbs. We
next describe how to incorporate these indicators as
features within a supervised framework for learning
lexical entailment rules between verbs. We follow
prior work on supervised lexical semantics (Mirkin
et al2006; Hagiwara et al2009; Tremper, 2010)
196
and address the rule learning task as a classification
task. Specifically, given an ordered verb pair (v1, v2)
as input, we learn a classifier that detects whether the
entailment relation ?v1? v2? holds for this pair.
We next detail how our novel indicators, as well
as other diverse sources of information found useful
in prior work, are encoded as features. Then, we
describe the learning model and our feature analysis
procedure.
4.1 Entailment features
Most of our features are based on information ex-
tracted from the target verb pair co-occurring within
varying textual scopes (sentence, document, cor-
pus). Hence, we group the features according to
their related scope. Naturally, when the scope is
small, i.e., at a sentence level, the semantic rela-
tion between the verbs is easier to discern but the
information may be sparse. Conversely, when co-
occurrence is loose the relation is harder to discern
but coverage is increased.
4.1.1 Sentence-level co-occurrence
We next detail features that address co-occurrence
of the target verb pair within a sentence. These in-
clude our novel linguistically-motivated indicators,
as well as features that were adapted from prior
work.
Discourse markers As discussed in Section 3,
discourse markers may signal relations between the
main verbs of adjacent clauses. The literature is
abundant with taxonomies that classify markers to
various discourse relations (Mann and Thompson,
1988; Hovy and Maier, 1993; Knott and Sanders,
1998). Inspired by Marcu and Echihabi (2002), we
employ markers that are mapped to four discourse
relations ?Contrast?, ?Cause?, ?Condition? and ?Tem-
poral?, as specified in Table 1. This definition
can be viewed as a relaxed version of VerbOcean?s
(Chklovski and Pantel, 2004) patterns, although the
underlying intuition is different (see Section 3).
For a target verb pair (v1, v2) and each discourse
relation r, we count the number of times that v1 is
the main verb in the main clause, v2 is the main verb
in the subordinate clause, and the clauses are con-
nected via a marker mapped to r. For example, given
the sentence ?You must enroll in the competition be-
fore you can participate in it?, the verb pair (?en-
roll?,?participate?) appears in the ?Temporal? rela-
tion, indicated by the marker ?before?, where ?enroll?
is in the main clause. Each count is then normalized
by the total number of times (v1, v2) appear with any
marker. The same procedure is done when v1 is in
the subordinate clause and v2 in the main clause. We
term the features by the relevant discourse relation,
e.g., ?v1-contrast-v2? refers to v1 being in the main
clause and connected to the subordinate clause via a
contrast marker.
Dependency relations between clauses As noted
in Section 3, the syntactic structure of verb co-
occurrence can indicate the existence or lack of en-
tailment. In dependency parsing this may be ex-
pressed via the label of the dependency relation con-
necting the main and subordinate clauses. In our ex-
periments we used the ukWaC corpus1 (Baroni et al
2009) which was parsed by the MALT parser (Nivre
et al2006). Hence, we identified three MALT de-
pendency relations that connect a main clause with
its subordinate clause. The first relation is the object
complement relation ?obj?. In this case the subor-
dinate clause is an object complement of the main
clause. For example, in ?it surprised me that the
lizard could talk? the verb pair (?surprise?,?talk?) is
connected by the ?obj? relation. The second rela-
tion is the adverbial adjunct relation ?adv?, in which
the subordinate clause is adverbial and describes the
time, place, manner, etc. of the main clause, e.g., ?he
gave his consent without thinking about the reper-
cussions?. The last relation is the coordination rela-
tion ?coord?, e.g., ?every night my dog Lucky sleeps
on the bed and my cat Flippers naps in the bathtub?.
Similar to discourse markers, we compute for
each verb pair (v1,v2) and each dependency label d
the proportion of times that v1 is the main verb of the
main clause, v2 is the main verb of the subordinate
clause, and the clauses are connected by dependency
relation d, out of all the times they are connected by
any dependency relation. We term the features by
the dependency label, e.g., ?v1-adv-v2? refers to v1
being in the main clause and connected to the subor-
dinate clause via an adverbial adjunct.
1http://wacky.sslmit.unibo.it/doku.php?
id=corpora
197
Discourse Rel. Discourse Markers
Contrast although , despite , but , whereas , notwithstanding , though
Cause because , therefore , thus
Condition if , unless
Temporal whenever , after , before , until , when , finally , during , afterwards , meanwhile
Table 1: Discourse relations and their mapped markers.
Pattern-based We follow Chklovski and Pan-
tel (2004) and extract occurrences of VerbOcean pat-
terns that are instantiated by the target verb pair. As
mentioned in Section 2, VerbOcean patterns were
originally grouped into five semantic classes. Based
on a preliminary study we conducted, we decided
to utilize only four strength-class patterns as posi-
tive indicators for entailment, e.g., ?he scared and
even startled me?, and three antonym-class patterns
as negative indicators for entailment, e.g., ?you can
either open or close the door?. We note that these
patterns are also commonly used by RTE systems2.
Since the corpus pattern counts were very sparse,
we defined for a target verb pair (v1, v2) two bi-
nary features: the first denotes whether the verb
pair instantiates at least one positive pattern, and
the second denotes whether the verb pair instanti-
ates at least one negative pattern. For example, given
the aforementioned sentences, the value of the pos-
itive feature for the verb pair (?startle?,?scare?) is
?1?. Patterns are directional, and so the value of
(?scare?,?startle?) is ?0?.
Polarity We compute the proportion of times that
the two verbs appear in different polarity. For exam-
ple, in ?he didn?t say why he left?, the verb ?say? ap-
pears in negative polarity and the verb ?leave? in pos-
itive polarity. Such change in polarity is usually an
indicator of non-entailment between the two verbs.
Tense ordering The temporal relation between
verbs may provide information about their seman-
tic relation. For each verb pair co-occurrence, we
extract the verbs? tenses and order them as follows:
past < present < future. We then add the fea-
tures ?tense-v1<tense-v2?, ?tense-v1=tense-v2?, and
?tense-v1>tense-v2?, corresponding to the propor-
2http://aclweb.org/aclwiki/index.php?
title=RTE_Knowledge_Resources#Ablation_
Tests
tion of times the tense of v1 is smaller, equal to,
or bigger than the tense of v2. This indicates the
prevalent temporal relation between the verbs in the
corpus and may assist in detecting the direction of
entailment. e.g., if tense-v1>tense-v2, the verb pair
is less likely to entail.
Co-reference Following Tremper (2010), in every
co-occurrence of (v1,v2) we extract for each verb
the set of arguments at either the subject or object
positions, denoted A1 and A2 (for v1 and v2, re-
spectively). We then compute the proportion of co-
occurrences in which v1 and v2 share an argument,
i.e., A1 ? A2 6= ?, out of all the co-occurrences in
which bothA1 andA2 are non-empty. The intuition,
which is similar to distributional similarity, is that
semantically related verbs tend to share arguments.
Syntactic and lexical distance Following Trem-
per (2010) again, we compute the average distance
d in dependency edges between the co-occurring
verbs. We compute three features corresponding to
three bins indicating if d < 3, 3 ? d ? 7, or
d > 7. Similar features are computed for the dis-
tance in words (bins are 0 < d < 5, 5 ? d ? 10,
d > 10). This feature provides insight into the syn-
tactic relatedness of the verbs.
Sentence-level pmi Pointwise mutual information
(pmi) between v1 and v2 is computed, where the co-
occurrence scope is a sentence. Higher pmi should
hint at semantically related verbs.
4.1.2 Document-level co-occurrence
This group of features addresses co-occurrence of
a target verb pair within the same document. These
features are less sparse, but tend to capture coarser
semantic relations between the target verbs.
Narrative score Chambers and Jurafsky (2008)
suggested a method for learning sequences of ac-
tions or events (expressed by verbs) in which a sin-
198
gle entity is involved. They proposed a pmi-like nar-
rative score (see Eq. (1) in their paper) that esti-
mates whether a pair consisting of a verb and one
of its dependency relations (v1, r1) is narratively-
related to another such pair (v2, r2). Their estima-
tion is based on quantifying the likelihood that two
verbs will share an argument that instantiates both
the dependency position (v1, r1) and (v2, r2) within
documents in which the two verbs co-occur. For ex-
ample, given the document ?Lindsay was prosecuted
for DUI. Lindsay was convicted of DUI.? the pairs
(?prosecute?,?subj?) and (?convict?,?subj?) share the
argument ?Lindsay? and are part of a narrative chain.
Such narrative relations may provide cues to the se-
mantic relatedness of the verb pair.
We compute for every target verb pair nine fea-
tures using their narrative score. In four features,
r1 = r2 and the common dependency is either a sub-
ject, an object, a preposition complement (e.g., ?we
meet at the station.), or an adverb (termed chamb-
subj, chamb-obj, and so on). In the next three fea-
tures, r1 6= r2 and r1, r2 denote either a subject,
object, or preposition complement3 (termed chamb-
subj-obj and so on). Last, we add as features the
average of the four features where r1 = r2 (termed
chamb-same), and the average of the three features
where r1 6= r2 (termed chamb-diff ).
Document-level pmi Similar to sentence-level
pmi, we compute the pmi between v1 and v2, but
this time the co-occurrence scope is a document.
4.1.3 Corpus-level statistics
The final group of features ignores sentence or
document boundaries and is based on overall corpus
statistics.
Distributional similarity Following our hypoth-
esis regarding typed distributional similarity (Sec-
tion 3), we first compute for each verb and each
argument (subject, object, preposition complement
and adverb) a separate vector that counts the num-
ber of times each word in the corpus instantiates
the argument of that verb. In addition, we also
compute a vector that is the concatenation of the
previous separate vectors, which captures the stan-
dard distributional similarity statistics. We then
3adverbs never instantiate the subject, object or preposition
complement positions.
apply three state-of-the-art distributional similarity
measures, Lin (Lin, 1998), Weeds precision (Weeds
and Weir, 2003) and BInc (Szpektor and Dagan,
2008), to compute for every verb pair a similarity
score between each of the five count vectors4. We
term each feature by the method and argument, e.g.,
weeds-prep and lin-all represent the Weeds measure
over prepositional complements and the Lin mea-
sure over all arguments.
Verb classes Following our discussion in Sec-
tion 3, we first measure for each target verb v a ?sta-
tive? feature f by computing the proportion of times
it appears in progressive tense, since stative verbs
usually do not appear in the progressive tense (e.g.,
?knowing?). Then, given a verb pair (v1,v2) and their
corresponding stative features f1 and f2, we add two
features f1 ? f2 and
f1
f2
, which capture the interaction
between the verb classes of the two verbs.
Verb generality For each verb, we add as a feature
the number of different particles it appears with in
the corpus, following the hypothesis that this is a
cue to its generality. Then, given a verb pair (v1,v2)
and their corresponding features f1 and f2, we add
the feature f1f2 . We expect that when
f1
f2
is high, v1 is
more general than v2, which is a negative entailment
indicator.
4.2 Learning model and feature analysis
The total number of features in our model as de-
scribed above is 63. We combine the features in
a supervised classification framework with a linear
SVM. Since our model contains many novel fea-
tures, it is important to investigate their utility for
detecting verb entailment. To that end, we employ
feature ranking methods as suggested by Guyon et
al. (2003). In feature ranking methods, features are
ranked by some score computed for each feature in-
dependently. In this paper we use Pearson correla-
tion between the feature values and the correspond-
ing labels as the ranking criterion.
4We employ the common practice of using the pmi between
a verb and an argument rather than the argument count as the
argument?s weight.
199
5 Evaluation and Analysis
5.1 Experimental Setting
To evaluate our proposed supervised model, we con-
structed a dataset containing labeled verb pairs. We
started by randomly sampling 50 verbs out of the
common verbs in the RCV1 corpus5, which we de-
note here as seed verbs. Next, we extracted the 20
most similar verbs to each seed verb according to
the Lin similarity measure (Lin, 1998), which was
computed on the RCV1 corpus. Then, for each seed
verb vs and one of its extracted similar verbs vis we
generated the two directed pairs (vs, vis) and (v
i
s, vs),
which represent the candidate rules ?vs ? vis? and
?vis ? vs? respectively. To reduce noise, we filtered
out verb pairs where one of the verbs is an auxiliary
or a light verb such as ?do?, ?get? and ?have?. This
step resulted in 812 verb pairs as our dataset6, which
were manually annotated by the authors as repre-
senting a valid entailment rule or not. To annotate
these pairs, we generally followed the rule-based ap-
proach for entailment rule annotation, where a rule
?v1 ? v2? is considered as correct if the annotator
could think of reasonable contexts under which the
rule holds (Dekang and Pantel, 2001; Szpektor et
al., 2004). In total 225 verb pairs were labeled as
entailing (the rule ?v1 ? v2? was judged as correct)
and 587 verb pairs were labeled as non-entailing (the
rule ?v1 ? v2? was judged as incorrect). The Inter-
Annotator Agreement (IAA) for a random sample of
100 pairs was moderate (0.47), as expected from the
rule-based approach (Szpektor et al2007).
For each verb pair, all 63 features within our
model (Section 4) were computed using the ukWaC
corpus (Baroni et al2009), which contains 2 billion
words. For classification, we utilized SVM-perf?s
(Joachims, 2005) linear SVM implementation with
default parameters, and evaluated our model by per-
forming 10-fold cross validation (CV) over the la-
beled dataset.
5http://trec.nist.gov/data/reuters/
reuters.html
6The data set is available at http://www.cs.biu.ac.
il/?nlp/downloads/verb-pair-annotation.
html
5.2 Feature selection and analysis
As discussed in Section 4.2, we followed the feature
ranking method proposed by Guyon et al2003) to
investigate the utility of our proposed features. Ta-
ble 2 depicts the 10 most positively and negatively
correlated features with entailment according to the
Pearson correlation measure
From Table 2, it is clear that distributional simi-
larity features are amongst the most positively cor-
related with entailment, which is in line with prior
work (Geffet and Dagan, 2005; Kotlerman et al
2010). Looking more closely, our suggestion for
typed distributional similarity proved to be useful,
and indeed most of the highly correlated distribu-
tional similarity features are typed measures. Stand-
ing out are the adverb-typed measures, with two fea-
tures in the top 10, including the highest, ?Weeds-
adverb?, and ?BInc-adverb?. We also note that the
highly correlated distributional similarity measures
are directional, Weeds and BInc.
The table also indicates that document-level co-
occurrence contributes positively to entailment de-
tection. This includes both the Chambers narrative
measure, with the typed feature Chambers-obj, and
document-level PMI, which captures a more loose
co-occurrence relationship between verbs. Again,
we point at the significant correlation of our novel
typed measures with verb entailment, in this case the
typed narrative measure.
Last, our feature analysis shows that many of our
novel co-occurrence features at the sentence level
contribute useful negative information. For exam-
ple, verbs connected via an adverbial adjunct (?v2-
adverb-v1?) or an object complement (?v1-obj-v2?)
are negatively correlated with entailment. In addi-
tion, the novel ?verb generality? feature as well as
the tense difference feature (?tense-v1 > tense-v2?)
are also strong negative indicators. On the other
hand, ?v2-coord-v1? is positively correlated with en-
tailment. This shows that encoding various aspects
of verb co-occurrence at the sentence level can lead
to better prediction of verb entailment. Finally, we
note that PMI at the sentence level is highly corre-
lated with entailment even more than at the docu-
ment level, since the local textual scope is more in-
dicative, though sparser.
To conclude, our feature analysis shows that fea-
200
Rank Top Positive Top Negative
1 Weeds-adverb tense-v1 > tense-v2
2 Sentence-level PMI v2-adverb-v1 co-occurrence
3 Weeds-subj v2-obj-v1 co-occurrence
4 Weeds-prep v1-obj-v2 co-occurrence
5 Weeds-all v1-adverb-v2 co-occurrence
6 Chambers-obj verb generality f1f2
7 v2-coord-v1 co-occurrence v1-contrast-v2
8 BInc-adverb tense-v1 < tense-v2
9 Document-level PMI lexical-distance 0-5
10 Chambers-same Lin-subj
Table 2: Top 10 positive and negative features according to the Pearson correlation score.
tures at all levels: sentence, document and corpus,
contain useful information for entailment detection,
both positive and negative, and should be combined
together. Moreover, many of our novel features are
among the highly correlated features, showing that
devising a rich set of verb-specific and linguistically-
motivated features provides better discriminative ev-
idence for entailment detection.
5.3 Results and Analysis
We compared our method to the following baselines
which were mostly taken from or inspired by prior
work:
Random: A simple decision rule: for any
pair (v1, v2), randomly classify as ?yes? with a
probability equal to the number of entailing verb
pairs out of all verb pairs in the labeled dataset (i.e.,
225
812 = 0.277).
VO-KB: A simple unsupervised rule: for any
pair (v1, v2), classify as ?yes? if the pair appears in
the strength relation (corresponding to entailment)
in the VerbOcean knowledge-base, which was com-
puted over Web counts.
VO-ukWaC: A simple unsupervised rule: for any
pair (v1, v2), classify as ?yes? if the value of the
positive VerbOcean feature is ?1? (Section 4.1, com-
puted over ukWaC).
TDS: Include only the 15 distributional similarity
features in our supervised model. This baseline ex-
tends Berant et al2012), who trained an entailment
Method P% R% AUC F1
All 40.2 71.0 0.65 0.51
TDS+VO 36.8 53.2 0.58 0.41
TDS 34.6 44.8 0.56 0.37
Random 27.9 28.8 0.51 0.28
VO-KB 33.1 14.8 0.53 0.2
VO-ukWaC 23.3 4.7 0.29 0.08
Table 3: Average precision, recall, AUC and F1 for our
method and the baselines.
classifier over several distributional similarity fea-
tures, and provides an evaluation of the discrimina-
tive power of distributional similarity alone, without
co-occurrence features.
TDS+VO: Include only the 15 typed distribu-
tional similarity features and the two VerbOcean
features in our supervised model. This baseline
is inspired by Mirkin et al2006), who combined
distributional similarity features and Hearst pat-
terns (Hearst, 1992) for learning entailment between
nouns.
All: Our full-blown model, including all features
described in Section 4.1.
For all tested methods, we performed 10-fold
cross validation and averaged Precision, Recall,
Area under the ROC curve (AUC) and F1 over the 10
folds. Table 3 presents the results of our full-blown
model as well as the baselines.
First, we note that, as expected, the VerbOcean
baselines VO-KB and VO-ukWaC provide low recall,
201
Method P% R% AUC F1
All 40.2 71.0 0.65 0.51
Sent+Corpus-level 39.7 70.4 0.64 0.50
Sent+Doc-level 39.0 70.0 0.63 0.50
Doc+Corpus-level 37.7 64.0 0.62 0.47
Sent-level 35.8 63.8 0.59 0.46
Doc-level 30.0 45.4 0.52 0.35
Corpus-level 35.4 58.1 0.58 0.44
Table 4: Average precision, recall, AUC and F1 for each
subset of the feature groups.
due to the sparseness of rigid pattern instantiation
for verbs both in the ukWaC corpus and on the web.
Yet, VerbOcean positive and negative patterns do
add some discriminative power over only distribu-
tional similarity measures, as seen by the improve-
ment of TDS+VO over TDS in all criteria. But, it is
the combination of all types of information sources
that yields the best performance. Our complete
model, employing the full set of features, outper-
forms all other models in terms of both precision and
recall. Its improvement in terms of F1 over the sec-
ond best model (TDS+VO), which includes all distri-
butional similarity features as well as pattern-based
features, is by 24%. This result shows the benefits
of integrating linguistically motivated co-occurrence
features with traditional pattern-based and distribu-
tional similarity information.
To further investigate the contribution of fea-
tures at various co-occurrence levels, we trained
and tested our model with all possible combina-
tions of feature groups corresponding to a certain
co-occurrence scope (sentence, document and cor-
pus). Table 4 presents the results of these tests.
The most notable result of this analysis is that
sentence-level features play an important role within
our model. Indeed, removing either the document-
level features (Sent+Corpus-level) or the corpus-
level features (Sent+Doc-level) results in only a
slight decline in performance. Yet, removing the
sentence-level features (Doc+Corpus-level), ends in
a more substantial decline of 8.5% in F1. In addi-
tion, sentence-level features alone (Sent-level) pro-
vide the best discriminative power for verb entail-
ment, compared to document and corpus levels,
which include distributional similarity features. Yet,
we note that sentence-level features alone do not
capture all the information within our model, and
they should be combined with one of the other fea-
ture groups to reach performance close to the com-
plete model. This shows again the importance of
combining co-occurrence indicators at different lev-
els.
As an additional insight from Table 4, we point
out that document-level features are not good en-
tailment indicators by themselves (Doc-level in Ta-
ble 4), and they perform worse than the distribu-
tional similarity baseline (TDS at Table 3). Still, they
do complement each of the other feature groups. In
particular, since the Sent+Doc-level model performs
almost as good as the full model, this subset may
be a good substitute to the full model, since its fea-
tures are easier to extract from large corpora, as they
may be extracted in an on-line fashion, processing
one document at a time (contrary to corpus-level fea-
tures).
As a final analysis, we randomly sampled cor-
rect entailment rules learned by our model but
missed by the typed distributional similarity classi-
fier (TDS). Our overall impression is that employ-
ing co-occurrence information helps to better cap-
ture entailment relations other than synonymy and
troponymy. For example, our model learns that ac-
quire? own, corresponding to the cause-effect en-
tailment relation, and that patent ? invent, corre-
sponding to the presupposition entailment relation.
6 Conclusions and Future Work
We presented a supervised classification model for
detecting lexical entailment between verbs. At the
heart of our model stand novel linguistically moti-
vated indicators that capture positive and negative
entailment information. These indicators encom-
pass co-occurrence relationships between verbs at
the sentence, document and corpus level, as well
as more fine-grained typed distributional similarity
measures. Our model incorporates these novel indi-
cators together with useful features from prior work,
combining co-occurrence and distributional similar-
ity information about verb pairs.
Our experiment over a manually labeled dataset
showed that our model significantly outperforms
several state-of-the-art models both in terms of Pre-
202
cision and Recall. Further feature analysis indicated
that our novel indicators contribute greatly to the
performance of the model, and that co-occurrence
at multiple levels, combined with distributional sim-
ilarity features, is necessary to achieve the model?s
best performance.
In future work we?d like to investigate which in-
dicators may contribute to learning different fine-
grained types of entailment, such as presupposition
and cause-effect, and attempt to perform a more
fine-grained classification to subtypes of entailment.
Acknowledgments
This work was partially supported by the Israel
Science Foundation grant 1112/08, the PASCAL-
2 Network of Excellence of the European Com-
munity FP7-ICT-2007-1-216886, and the Euro-
pean Community?s Seventh Framework Programme
(FP7/2007-2013) under grant agreement no. 287923
(EXCITEMENT).
References
Shuya Abe, Kentaro Inui, and Yuji Matsumoto. 2008.
Acquiring event relation knowledge by learning cooc-
currence patterns and fertilizing cooccurrence samples
with verbal nouns. In Proceedings of IJCNLP.
Timothy Baldwin and Aline Villavicencio. 2002. Ex-
tracting the unextractable: a case study on verb-
particles. In proceedings of COLING.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and
Eros Zanchetta. 2009. The wacky wide web: A
collection of very large linguistically processed web-
crawled corpora. Language Resources and Evalua-
tion, 43(3):209?226.
Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2012. Learning entailment relations by global graph
structure optimization. Computational Linguistics,
38(1):73?111.
Nathanael Chambers and Dan Jurafsky. 2008. Unsuper-
vised learning of narrative event chains. In Proceed-
ings of ACL.
Timothy Chklovski and Patrick Pantel. 2004. Verb
ocean: Mining the web for fine-grained semantic verb
relations. In Proceedings of EMNLP.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The pascal recognising textual entailment chal-
lenge. In Machine Learning Challenges, volume 3944
of Lecture Notes in Computer Science, pages 177?190.
Springer.
Lin Dekang and Patrick Pantel. 2001. Dirt - discovery
of inference rules from text. In In Proceedings of the
ACM SIGKDD Conference on Knowledge Discovery
and Data Mining.
Maayan Geffet and Ido Dagan. 2005. The distributional
inclusion hypotheses and lexical entailment. In Pro-
ceedings of ACL.
Isabelle Guyon and Andre Elisseeff. 2003. An intro-
duction to variable and feature selection. Journal of
Machine Learning Research, 3:1157?1182.
Masato Hagiwara, Yasuhiro Ogawa, and Katsuhiko
Toyama. 2009. Supervised synonym acquisition us-
ing distributional features and syntactic patterns. In
Journal of Natural Language Processing.
Marti Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proceedings of COLING.
Jerry Hobbs. 1979. Coherence and coreference. Cogni-
tive Science, 3:67?90.
Eduard Hovy and Elisabeth Maier. 1993. Organizing
Discourse Structure Relations using Metafunctions.
Pinter Publishing.
Ray Jackendoff. 1983. Semantics and Cognition. The
MIT Press.
T. Joachims. 2005. A support vector method for mul-
tivariate performance measures. In Proceedings of
ICML.
Alistair Knott and Ted Sanders. 1998. The classification
of coherence relations and their linguistic markers: An
exploration of two languages. In Journal of Pragmat-
ics.
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-Geffet. 2010. Directional distributional
similarity for lexical inference. Natural Language En-
gineering, 16(4):359?389.
Beth Levin. 1993. English Verb Classes and Alter-
nations: A Preliminary Investigation. University Of
Chicago Press.
Dekang Lin. 1998. An information-theoretic definition
of similarity. In Proceedings of ICML.
William Mann and Sandra Thompson. 1988. Rhetorical
structure theory: Toward a functional theory of text
organization. Text, 8(3):243?281.
Daniel Marcu and Abdessamad Echihabi. 2002. An
unsupervised approach to recognizing discourse rela-
tions. In Proceedings of ACL.
Shachar Mirkin, Ido Dagan, and Maayan Geffet. 2006.
Integrating pattern-based and distributional similarity
methods for lexical entailment acquisition. In Pro-
ceedings of the COLING/ACL.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006. Malt-
parser: A data-driven parser-generator for dependency
parsing. In Proceedings of LREC.
203
Viktor Pekar. 2008. Discovery of event entailment
knowledge from text corpora. Comput. Speech Lang.,
22(1):1?16.
Marco Pennacchiotti and Patrick Pantel. 2009. Entity
extraction via ensemble semantics. In Proceedings of
EMNLP.
Deepak Ravichandran and Eduard Hovy. 2002. Learning
surface text patterns for a question answering system.
In Proceedings of ACL.
Deborah Schiffrin. 1988. Discourse Markers. Cam-
bridge University Press.
Stefan Schoenmackers, Jesse Davis, Oren Etzioni, and
Daniel S. Weld. 2010. Learning first-order horn
clauses from web text. In Proceedings of EMNLP.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive information extraction using unrestricted relation
discovery. In Proceedings of NAACL-HLT.
Idan Szpektor and Ido Dagan. 2008. Learning entailment
rules for unary templates. In Proceedings of COLING.
Idan Szpektor, Hristo Tanev, and Ido Dagan. 2004. Scal-
ing web-based acquisition of entailment relations. In
In Proceedings of EMNLP.
Idan Szpektor, Eyal Shnarch, and Ido Dagan. 2007. In-
stance based evaluation of entailment rule acquisition.
In Proceedings of the 45th Annual Meeting of the As-
sociation of Computational Linguistics.
Galina Tremper. 2010. Weakly supervised learning of
presupposition relations between verbs. In Proceed-
ings of ACL student workshop.
Julie Weeds and David Weir. 2003. A general frame-
work for distributional similarity. In Proceedings of
EMNLP.
Fabio Massimo Zanzotto, Marco Pennacchiotti, and
Maria Teresa Pazienza. 2006. Discovering asym-
metric entailment relations between verbs using se-
lectional preferences. In Proceedings of the COL-
ING/ACL.
204
Learning Entailment Relations by Global
Graph Structure Optimization
Jonathan Berant?
Tel Aviv University
Ido Dagan??
Bar-Ilan University
Jacob Goldberger?
Bar-Ilan University
Identifying entailment relations between predicates is an important part of applied semantic
inference. In this article we propose a global inference algorithm that learns such entailment
rules. First, we define a graph structure over predicates that represents entailment relations as
directed edges. Then, we use a global transitivity constraint on the graph to learn the optimal set
of edges, formulating the optimization problem as an Integer Linear Program. The algorithm is
applied in a setting where, given a target concept, the algorithm learns on the fly all entailment
rules between predicates that co-occur with this concept. Results show that our global algorithm
improves performance over baseline algorithms by more than 10%.
1. Introduction
The Textual Entailment (TE) paradigm is a generic framework for applied semantic
inference. The objective of TE is to recognize whether a target textual meaning can
be inferred from another given text. For example, a question answering system has
to recognize that alcohol affects blood pressure is inferred from the text alcohol reduces
blood pressure to answer the question What affects blood pressure? In the TE framework,
entailment is defined as a directional relationship between pairs of text expressions,
denoted by T, the entailing text, and H, the entailed hypothesis. The text T is said to
entail the hypothesis H if, typically, a human reading T would infer that H is most likely
true (Dagan et al 2009).
TE systems require extensive knowledge of entailment patterns, often captured as
entailment rules?rules that specify a directional inference relation between two text
fragments (when the rule is bidirectional this is known as paraphrasing). A common
type of text fragment is a proposition, which is a simple natural language expression
that contains a predicate and arguments (such as alcohol affects blood pressure), where
the predicate denotes some semantic relation between the concepts that are expressed
? Tel-Aviv University, P.O. Box 39040, Tel-Aviv, 69978, Israel. E-mail: jonatha6@post.tau.ac.il.
?? Bar-Ilan University, Ramat-Gan, 52900, Israel. E-mail: dagan@cs.biu.ac.il.
? Bar-Ilan University, Ramat-Gan, 52900, Israel. E-mail: goldbej@eng.biu.ac.il.
Submission received: 28 September 2010; revised submission received: 5 May 2011; accepted for publication:
5 July 2011.
? 2012 Association for Computational Linguistics
Computational Linguistics Volume 38, Number 1
by the arguments. One important type of entailment rule specifies entailment between
propositional templates, that is, propositions where the arguments are possibly re-
placed by variables. A rule corresponding to the aforementioned example may be X
reduce blood pressure ? X affect blood pressure. Because facts and knowledge are mostly
expressed by propositions, such entailment rules are central to the TE task. This has
led to active research on broad-scale acquisition of entailment rules for predicates
(Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009;
Schoenmackers et al 2010).
Previous work has focused on learning each entailment rule in isolation. It is clear,
however, that there are interactions between rules. A prominent phenomenon is that
entailment is inherently a transitive relation, and thus the rules X ? Y and Y ? Z imply
the rule X ? Z.1 In this article we take advantage of these global interactions to improve
entailment rule learning.
After reviewing relevant background (Section 2), we describe a structure termed
an entailment graph that models entailment relations between propositional templates
(Section 3). Next, we motivate and discuss a specific type of entailment graph, termed a
focused entailment graph, where a target concept instantiates one of the arguments of
all propositional templates. For example, a focused entailment graph about the target
concept nausea might specify the entailment relations between propositional templates
like X induce nausea, X prevent nausea, and nausea is a symptom of X.
In the core section of the article, we present an algorithm that uses a global approach
to learn the entailment relations, which comprise the edges of focused entailment
graphs (Section 4). We define a global objective function and look for the graph that
maximizes that function given scores provided by a local entailment classifier and a
global transitivity constraint. The optimization problem is formulated as an Integer
Linear Program (ILP) and is solved with an ILP solver, which leads to an optimal
solution with respect to the global function. In Section 5 we demonstrate that this
algorithm outperforms by 12?13% methods that utilize only local information as well
as methods that employ a greedy optimization algorithm (Snow, Jurafsky, and Ng 2006)
rather than an ILP solver.
The article also includes a comprehensive investigation of the algorithm and its
components. First, we perform manual comparison between our algorithm and the
baselines and analyze the reasons for the improvement in performance (Sections 5.3.1
and 5.3.2). Then, we analyze the errors made by the algorithm against manually pre-
pared gold-standard graphs and compare them to the baselines (Section 5.4). Last, we
perform a series of experiments in which we investigate the local entailment classifier
and specifically experiment with various sets of features (Section 6). We conclude and
suggest future research directions in Section 7.
This article is based on previous work (Berant, Dagan, and Goldberger 2010), while
substantially expanding upon it. From a theoretical point of view, we reformulate the
two ILPs previously introduced by incorporating a prior. We show a theoretical relation
between the two ILPs and prove that the optimization problem tackled is NP-hard.
From an empirical point of view, we conduct many new experiments that examine
both the local entailment classifier as well as the global algorithm. Last, a rigorous
analysis of the algorithm is performed and an extensive survey of previous work is
provided.
1 Assuming that Y has the same sense in both X ? Y and Y ? Z, as we discuss later in Section 3.
74
Berant et al Learning Entailment Relations by Global Graph Structure Optimization
2. Background
In this section we survey methods proposed in past literature for learning entailment
rules between predicates. First, we discuss local methods that assess entailment given a
pair of predicates, and then global methods that perform inference over a larger set of
predicates.
2.1 Local Learning
Three types of information have primarily been utilized in the past to learn entailment
rules between predicates: lexicographic methods, distributional similarity methods, and
pattern-based methods.
Lexicographic methods use manually prepared knowledge bases that contain in-
formation about semantic relations between lexical items. WordNet (Fellbaum 1998b),
by far the most widely used resource, specifies relations such as hyponymy, synonymy,
derivation, and entailment that can be used for semantic inference (Budanitsky and
Hirst 2006). For example, if WordNet specifies that reduce is a hyponym of affect, then
one can infer that X reduces Y ? X affects Y. WordNet has also been exploited to
automatically generate a training set for a hyponym classifier (Snow, Jurafsky, and Ng
2004), and we make a similar use of WordNet in Section 4.1.
A drawback of WordNet is that it specifies semantic relations for words and terms
but not for more complex expressions. For example, WordNet does not cover a complex
predicate such as X causes a reduction in Y. Another drawback of WordNet is that it only
supplies semantic relations between lexical items, but does not provide any information
on how to map arguments of predicates. For example, WordNet specifies that there is
an entailment relation between the predicates pay and buy, but does not describe the
way in which arguments are mapped: if X pays Y for Z then X buys Z from Y. Thus,
using WordNet directly to derive entailment rules between predicates is possible only
for semantic relations such as hyponymy and synonymy, where arguments typically
preserve their syntactic positions on both sides of the rule.
Some knowledge bases try to overcome this difficulty: Nomlex (Macleod et al
1998) is a dictionary that provides the mapping of arguments between verbs and their
nominalizations and has been utilized to derive predicative entailment rules (Meyers
et al 2004; Szpektor and Dagan 2009). FrameNet (Baker, Fillmore, and Lowe 1998) is
a lexicographic resource that is arranged around ?frames?: Each frame corresponds to
an event and includes information on the predicates and arguments relevant for that
specific event supplemented with annotated examples that specify argument positions.
Consequently, FrameNet was also used to derive entailment rules between predicates
(Coyne and Rambow 2009; Ben Aharon, Szpektor, and Dagan 2010). Additional man-
ually constructed resources for predicates include PropBank (Kingsbury, Palmer, and
Marcus 2002) and VerbNet (Kipper, Dang, and Palmer 2000).
Distributional similarity methods are used to learn broad-scale resources, because
lexicographic resources tend to have limited coverage. Distributional similarity algo-
rithms employ ?the distributional hypothesis? (Harris 1954) and predict a semantic
relation between two predicates by comparing the arguments with which they occur.
Quite a few methods have been suggested (Lin and Pantel 2001; Szpektor et al 2004;
Bhagat, Pantel, and Hovy 2007; Szpektor and Dagan 2008; Yates and Etzioni 2009;
Schoenmackers et al 2010), which differ in terms of the specifics of the ways in which
predicates are represented, the features that are extracted, and the function used to com-
pute feature vector similarity. Next, we elaborate on some of the prominent methods.
75
Computational Linguistics Volume 38, Number 1
Lin and Pantel (2001) proposed an algorithm that is based on a mutual information
criterion. A predicate is represented by a binary template, which is a dependency path
between two arguments of a predicate where the arguments are replaced by variables.
Note that in a dependency tree, a path between two arguments must pass through their
common predicate. Also note that if a predicate has more than two arguments, then it
is represented by more than one binary template, where each template corresponds to
a different aspect of the predicate. For example, the proposition I bought a gift for her
contains a predicate and three arguments, and therefore is represented by the following
three binary templates: X
subj
??? buys
obj
?? Y, X
obj
?? buys
prep
??? for
pcomp?n
?????? Y and X
subj
??? buys
prep
??? for
pcomp?n
?????? Y.
For each binary template Lin and Pantel compute two sets of features Fx and Fy,
which are the words that instantiate the arguments X and Y, respectively, in a large
corpus. Given a template t and its feature set for the X variable Ftx, every fx ? F
t
x is
weighted by the pointwise mutual information between the template and the feature:
wtx( fx) = log
Pr( fx|t)
Pr( fx )
, where the probabilities are computed using maximum likelihood
over the corpus. Given two templates u and v, the Lin measure (Lin 1998a) is computed
for the variable X in the following manner:
Linx(u, v) =
?
f?Fux?Fvx
[wux ( f ) + w
v
x( f )]
?
f?Fux
wux ( f ) +
?
f?Fvx
wvx( f )
(1)
The measure is computed analogously for the variable Y and the final distributional
similarity score, termed DIRT, is the geometric average of the scores for the two
variables:
DIRT(u, v) =
?
Linx(u, v) ? Liny(u, v) (2)
If DIRT(u, v) is high, this means that the templates u and v share many ?informative?
arguments and so it is possible that u ? v. Note, however, that the DIRT similarity
measure computes a symmetric score, which is appropriate for modeling synonymy
but not entailment, an inherently directional relation.
To remedy that, Szpektor and Dagan (2008) suggested a directional distributional
similarity measure. In their work, Szpektor and Dagan chose to represent predicates
with unary templates, which are identical to binary templates, only they contain a pred-
icate and a single argument, such as: X
subj
??? buys. Szpektor and Dagan explain that unary
templates are more expressive than binary templates, and that some predicates can only
be encoded using unary templates. They propose that if for two unary templates u ? v,
then relatively many of the features of u should be covered by the features of v. This
is captured by the asymmetric Cover measure suggested by Weeds and Weir (2003) (we
omit the subscript x from Fux and F
v
x because in their setting there is only one argument):
Cover(u, v) =
?
f?Fu?Fv w
u( f )
?
f?Fu w
u( f )
(3)
The final directional score, termed BInc (Balanced Inclusion), is the geometric average
of the Lin measure and the Cover measure:
BInc(u, v) =
?
Lin(u, v) ? Cover(u, v) (4)
76
Berant et al Learning Entailment Relations by Global Graph Structure Optimization
Both Lin and Pantel as well as Szpektor and Dagan compute a similarity score for
each argument separately, effectively decoupling the arguments from one another. It is
clear, however, that although this alleviates sparsity problems, it disregards an impor-
tant piece of information, namely, the co-occurrence of arguments. For example, if one
looks at the following propositions: coffee increases blood pressure, coffee decreases fatigue,
wine decreases blood pressure, wine increases fatigue, one can notice that the predicates occur
with similar arguments and might mistakenly infer that decrease ? increase. However,
looking at pairs of arguments reveals that the predicates do not share a single pair of
arguments.
Yates and Etzioni (2009) address this issue and propose a generative model that
estimates the probability that two predicates are synonymous (synonymy is simply
bidirectional entailment) by comparing pairs of arguments. They represent predicates
and arguments as strings and compute for every predicate a feature vector that counts
that number of times it occurs with any ordered pair of words as arguments. Their main
modeling decision is to assume that two predicates are synonymous if the number of
pairs of arguments they share is maximal. An earlier work by Szpektor et al (2004)
also tried to learn entailment rules between predicates by using pairs of arguments as
features. They utilized an algorithm that learns new rules by searching for distributional
similarity information on the Web for candidate predicates.
Pattern-based methods. Although distributional similarity measures excel at iden-
tifying the existence of semantic similarity between predicates, they are often unable
to discern the exact type of semantic similarity and specifically determine whether it is
entailment. Pattern-based methods are used to automatically extract pairs of predicates
for a specific semantic relation. Pattern-based methods identify a semantic relation
between two predicates by observing that they co-occur in specific patterns in sentences.
For example, from the single proposition He scared and even startled me one might infer
that startle is semantically stronger than scare and thus startle ? scare. Chklovski and
Pantel (2004) manually constructed a few dozen patterns and learned semantic relations
between predicates by looking for these patterns on the Web. For example, the pattern
X and even Y implies that Y is stronger than X, and the pattern to X and then Y indicates
that Y follows X. The main disadvantage of pattern-based methods is that they are based
on the co-occurrence of two predicates in a single sentence in a specific pattern. These
events are quite rare and require working on a very large corpus, or preferably, the Web.
Pattern-based methods were mainly utilized so far to extract semantic relations
between nouns, and there has been some work on automatically learning patterns for
nouns (Snow, Jurafsky, and Ng 2004). Although these methods can be expanded for
predicates, we are unaware of any attempt to automatically learn patterns that describe
semantic relations between predicates (as opposed to the manually constructed patterns
suggested by Chklovski and Pantel [2004]).
2.2 Global Learning
It is natural to describe entailment relations between predicates (or language expres-
sions in general) by a graph. Nodes represent predicates, and edges represent entail-
ment between nodes. Nevertheless, using a graph for global learning of all entailment
relations within a set of predicates, rather then between pairs of predicates, has attracted
little attention. Recently, Szpektor and Dagan (2009) presented the resource Argument-
mapped WordNet, providing entailment relations for predicates in WordNet. This re-
source was built on top of WordNet and augments it with mapping of arguments for
predicates using NomLex (Macleod et al 1998) and a corpus-based resource (Szpektor
77
Computational Linguistics Volume 38, Number 1
and Dagan 2008). Their resource makes simple use of WordNet?s global graph structure:
New rules are suggested by transitively chaining graph edges, and then verified using
distributional similarity measures. Effectively, this is equivalent to using the intersection
of the set of rules derived by this transitive chaining and the set of rules in a distribu-
tional similarity knowledge base.
The most similar work to ours is Snow, Jurafsky, and Ng?s (2006) algorithm for
taxonomy induction, although it involves learning the hyponymy relation between
nouns, which is a special case of entailment, rather than learning entailment between
predicates. We provide here a brief review of a simplified form of this algorithm.
Snow, Jurafsky, and Ng define a taxonomy T to be a set of pairs of words, expressing
the hyponymy relation between them. The notation Huv ? T means that the noun u is a
hyponym of the noun v in T. They define D to be the set of observed data over all pairs of
words, and define Duv ? D to be the observed evidence we have in the data for the event
Huv ? T. Snow, Jurafsky, and Ng assume a model exists for inferring P(Huv ? T|Duv):
the posterior probability of the event Huv ? T, given the data. Their goal is to find the
taxonomy that maximizes the likelihood of the data, that is, to find
T? = argmax
T
P(D|T) (5)
Using some independence assumptions and Bayes rule, the likelihood P(D|T) is
expressed:
P(D|T) =
?
Huv?T
P(Huv ? T|Duv)P(Duv)
P(Huv ? T)
?
?
Huv/?T
P(Huv /? T|Duv)P(Duv)
P(Huv /? T)
(6)
Crucially, they demand that the taxonomy learned respects the constraint that hy-
ponymy is a transitive relation. To ensure that, they propose the following greedy
algorithm: At each step they go over all pairs of words (u, v) that are not in the taxonomy,
and try to add the single hyponymy relation Huv. Then, they calculate the set of relations
Suv that Huv will add to the taxonomy due to the transitivity constraint (all of the
relations Huw, where w is a hypernym of v in the taxonomy). Last, they choose to
add that set of relations Suv that maximizes P(D|T) out of all the possible candidates.
This iterative process stops when P(D|T) starts dropping. Their implementation of the
algorithm uses a hyponym classifier presented in an earlier work (Snow, Jurafsky, and
Ng 2004) as a model for P(Huv ? T|Duv) and a single sparsity parameter k =
P(Huv/?T)
P(Huv?T)
. In
this article we tackle a similar problem of learning a transitive relation, but we use linear
programming (Vanderbei 2008) to solve the optimization problem.
2.3 Linear Programming
A Linear Program (LP) is an optimization problem where a linear objective function is
minimized (or maximized) under linear constraints.
min
x?Rd
cx (7)
such that Ax ? b
where c ? Rd is a coefficient vector, and A ? Rn ? Rd and b ? Rn specify the constraints.
In short, we wish to find the optimal assignment for the d variables in the vector x, such
78
Berant et al Learning Entailment Relations by Global Graph Structure Optimization
that all n linear constraints specified by the matrix A and the vector b are satisfied by this
assignment. If the variables are forced to be integers, the problem is termed an Integer
Linear Program (ILP). ILP has attracted considerable attention recently in several
fields of NLP, such as semantic role labeling, summarization, and parsing (Althaus,
Karamanis, and Koller 2004; Roth and Yih 2004; Riedel and Clarke 2006; Clarke and
Lapata 2008; Finkel and Manning 2008; Martins, Smith, and Xing 2009). In this article we
formulate the entailment graph learning problem as an ILP, which leads to an optimal
solution with respect to the objective function (vs. a greedy optimization algorithm
suggested by Snow, Jurafsky, and Ng [2006]). Recently, Do and Roth (2010) used ILP
in a related task of learning taxonomic relations between nouns, utilizing constraints
between sibling nodes and ancestor?child nodes in small graphs of three nodes.
3. Entailment Graph
In this section we define a structure termed the entailment graph that describes the
entailment relations between propositional templates (Section 3.1), and a specific type
of entailment graph, termed the focused entailment graph, that concentrates on entail-
ment relations that are relevant for some pre-defined target concept (Section 3.2).
3.1 Entailment Graph: Definition and Properties
The nodes of an entailment graph are propositional templates. A propositional tem-
plate is a binary template2 where at least one of the two arguments is a variable whereas
the second may be instantiated. In addition, the sense of the predicate is specified (ac-
cording to some sense inventory, such as WordNet) and so each sense of a polysemous
predicate corresponds to a separate template (and a separate graph node). For example,
X
subj
??? treats#1
obj
?? Y and X
subj
??? treats#2
obj
?? nausea are propositional templates for the
first and second sense of the predicate treat, respectively. An edge (u, v) represents the
fact that template u entails template v. Note that the entailment relation transcends
hyponymy/troponomy. For example, the template X is diagnosed with asthma entails the
template X suffers from asthma, although one is not a hyponym of the other. An example
of an entailment graph is given in Figure 1.
Because entailment is a transitive relation, an entailment graph is transitive, that is,
if the edges (u, v) and (v, w) are in the graph, so is the edge (u, w). Note that the property
of transitivity does not hold when the senses of the predicates are not specified. For
example, X buys Y ? X acquires Y and X acquires Y ? X learns Y, but X buys Y X learns
Y. This violation occurs because the predicate acquire has two distinct senses in the two
templates, but this distinction is lost when senses are not specified.
Transitivity implies that in each strongly connected component3 of the graph all
nodes entail each other. For example, in Figure 1 the nodes X-related-to-nausea and X-
associated-with-nausea form a strongly connected component. Moreover, if we merge
every strongly connected component to a single node, the graph becomes a Directed
Acyclic Graph (DAG), and a hierarchy of predicates can be obtained.
2 We restrict our discussion to templates with two arguments, but generalization is straightforward.
3 A strongly connected component is a subset of nodes in the graph where there is a path from any
node to any other node.
79
Computational Linguistics Volume 38, Number 1
Figure 1
A focused entailment graph. For clarity, edges that can be inferred by transitivity are omitted.
The single strongly connected component is surrounded by a dashed line.
3.2 Focused Entailment Graphs
In this article we concentrate on learning a type of entailment graph, termed the focused
entailment graph. Given a target concept, such as nausea, a focused entailment graph
describes the entailment relations between propositional templates for which the target
concept is one of the arguments (see Figure 1). Learning such entailment rules in real
time for a target concept is useful in scenarios such as information retrieval and question
answering, where a user specifies a query about the target concept. The need for such
rules has been also motivated by Clark et al (2007), who investigated what types
of knowledge are needed to identify entailment in the context of the RTE challenge,
and found that often rules that are specific to a certain concept are required. Another
example for a semantic inference algorithm that is utilized in real time is provided by
Do and Roth (2010), who recently described a system that, given two terms, determines
the taxonomic relation between them on the fly. Last, we have recently suggested an
application that uses focused entailment graphs to present information about a target
concept according to a hierarchy of entailment (Berant, Dagan, and Goldberger 2010).
The benefit of learning focused entailment graphs is three-fold. First, the target
concept that instantiates the propositional template usually disambiguates the predicate
and hence the problem of predicate ambiguity is greatly reduced. Thus, we do not
employ any form of disambiguation in this article, but assume that every node in a
focused entailment graph has a single sense (we further discuss this assumption when
describing the experimental setting in Section 5.1), which allows us to utilize transitivity
constraints.
An additional (albeit rare) reason that might also cause violations of transitivity
constraints is the notion of probabilistic entailment. Whereas troponomy rules
(Fellbaum 1998a) such as X walks ? X moves can be perceived as being almost always
correct, rules such as X coughs ? X is sick might only be true with some probability.
Consequently, chaining a few probabilistic rules such as A ? B, B ? C, and C ? D
might not guarantee the correctness of A ? D. Because in focused entailment graphs
the number of nodes and diameter4 are quite small (for example, in the data set we
4 The distance between two nodes in a graph is the number of edges in a shortest path connecting them.
The diameter of a graph is the maximal distance between any two nodes in the graph.
80
Berant et al Learning Entailment Relations by Global Graph Structure Optimization
present in Section 5 the maximal number of nodes is 26, the average number of nodes
is 22.04, the maximal diameter is 5, and the average diameter is 2.44), we do not find
this to be a problem in our experiments in practice.
Last, the optimization problem that we formulate is NP-hard (as we show in Sec-
tion 4.2). Because the number of nodes in focused entailment graphs is rather small, a
standard ILP solver is able to quickly reach the optimal solution.
To conclude, the algorithm we suggest next is applied in our experiments on
focused entailment graphs. However, we believe that it is suitable for any entailment
graph whose properties are similar to those of focused entailment graphs. For brevity,
from now on the term entailment graph will stand for focused entailment graph.
4. Learning Entailment Graph Edges
In this section we present an algorithm that, given the set of propositional templates
constituting the nodes of an entailment graph, learns its edges (i.e., the entailment
relations between all pairs of nodes). The algorithm comprises two steps (described in
Sections 4.1 and 4.2): In the first step we use a large corpus and a lexicographic resource
(WordNet) to train a generic entailment classifier that given any pair of propositional
templates estimates the likelihood that one template entails the other. This generic step
is performed only once, and is independent of the specific nodes of the target entailment
graph whose edges we want to learn. In the second step we learn on the fly the edges of
a specific target graph: Given the graph nodes, we use a global optimization approach
that determines the set of edges that maximizes the probability (or score) of the entire
graph. The global graph decision is determined by the given edge probabilities (or
scores) supplied by the entailment classifier and by the graph constraints (transitivity
and others).
4.1 Training an Entailment Classifier
We describe a procedure for learning a generic entailment classifier, which can be used
to estimate the entailment likelihood for any given pair of templates. The classifier
is constructed based on a corpus and a lexicographic resource (WordNet) using the
following four steps:
(1) Extract a large set of propositional templates from the corpus.
(2) Use WordNet to automatically generate a training set of pairs of
templates?both positive and negative examples.
(3) Represent each training set example with a feature vector of various
distributional similarity scores.
(4) Train a classifier over the training set.
(1) Template extraction. We parse the corpus with the Minipar dependency parser
(Lin 1998b) and use the Minipar representation to extract all binary templates from
every parse tree, employing the procedure described by Lin and Pantel (2001), which
considers all dependency paths between every pair of nouns in the parse tree. We
also apply over the extracted paths the syntactic normalization procedure described
by Szpektor and Dagan (2007), which includes transforming passive forms into active
forms and removal of conjunctions, appositions, and abbreviations. In addition, we use
81
Computational Linguistics Volume 38, Number 1
Table 1
Positive and negative examples for entailment in the training set. The direction of entailment is
from the left template to the right template.
Positive examples Negative examples
(X
subj
??? desires
obj
?? Y, X
subj
??? wants
obj
?? Y) (X
subj
??? pushes
obj
?? Y,X
subj
??? blows
obj
?? Y)
(X
subj
??? causes vrel?? Y, X
subj
??? creates vrel?? Y) (X
subj
??? issues vrel?? Y,X
subj
??? signs vrel?? Y)
a simple heuristic to filter out templates that probably do not include a predicate: We
omit ?uni-directional? templates where the root of template has a single child, such as
therapy
prep
???in
p?comp
?????patient nn??cancer, unless one of the edges is labeled with a passive
relation, such as in the template nausea
vrel???characterized
subj
???poisoning, which contains
the Minipar passive label vrel.5 Last, the arguments are replaced by variables, resulting
in propositional templates such as X
subj
??? affect
obj
?? Y. The lexical items that remain in
the template after replacing the arguments by variables are termed predicate words.
(2) Training set generation. WordNet is used to automatically generate a training
set of positive (entailing) and negative (non-entailing) template pairs. Let T be the set
of propositional templates extracted from the corpus. For each ti ? T with two variables
and a single predicate word w, we extract from WordNet the set H of direct hypernyms
(distance of one in WordNet) and synonyms of w. For every h ? H, we generate a new
template tj from ti by replacing w with h. If tj ? T, we consider (ti, tj) to be a positive
example. Negative examples are generated analogously, only considering direct co-
hyponyms of w, which are direct hyponyms of direct hypernyms of w that are not
synonymous to w. It has been shown in past work that in most cases co-hyponym terms
do not entail one another (Mirkin, Dagan, and Gefet 2006). A few examples for positive
and negative training examples are given in Table 1.
This generation method is similar to the ?distant supervision? method proposed by
Snow, Jurafsky, and Ng (2004) for training a noun hypernym classifier. It differs in some
important aspects, however: First, Snow, Jurafsky, and Ng consider a positive example
to be any Wordnet hypernym, irrespective of the distance, whereas we look only at
direct hypernyms. This is because predicates are mainly verbs and precision drops
quickly when looking at verb hypernyms in WordNet at a longer distance. Second,
Snow, Jurafsky, and Ng generate negative examples by looking at any two nouns where
one is not the hypernym of the other. In the spirit of ?contrastive estimation? (Smith and
Eisner 2005), we prefer to generate negative examples that are ?hard,? that is, negative
examples that, although not entailing, are still semantically similar to positive examples
and thus focus the classifier?s attention on determining the boundary of the entailment
class. Last, we use a balanced number of positive and negative examples, because
classifiers tend to perform poorly on the minority class when trained on imbalanced
data (Van Hulse, Khoshgoftaar, and Napolitano 2007; Nikulin 2008).
(3) Distributional similarity representation. We aim to train a classifier that for an
input template pair (t1, t2) determines whether t1 entails t2. Our approach is to represent
a template pair by a feature vector where each coordinate is a different distributional
similarity score for the pair of templates. The different distributional similarity scores
5 This passive construction is not handled by the normalization scheme employed by Szpektor and Dagan
(2007).
82
Berant et al Learning Entailment Relations by Global Graph Structure Optimization
are obtained by utilizing various distributional similarity algorithms that differ in one
or more of their characteristics. In this way we hope to combine the various methods
proposed in the past for measuring distributional similarity. The distributional similar-
ity algorithms we employ vary in one or more of the following dimensions: the way the
predicate is represented, the way the features are represented, and the function used to
measure similarity between the feature representations of the two templates.
Predicate representation. As mentioned, we represent predicates over dependency
tree structures. However, some distributional similarity algorithms measure similarity
between binary templates directly (Lin and Pantel 2001; Szpektor et al 2004; Bhagat,
Pantel, and Hovy 2007; Yates and Etzioni 2009), whereas others decompose binary
templates into two unary templates, estimate similarity between two pairs of unary
templates, and combine the two scores into a single score (Szpektor and Dagan 2008).
Feature representation. The features of a template are some function of the terms that
instantiated the argument variables in a corpus. Two representations that are used in
our experiments are derived from an ontology that maps natural language phrases to
semantic identifiers (see Section 5). Another variant occurs when using binary tem-
plates: a template may be represented by a pair of feature vectors, one for each variable
as in the DIRT algorithm (Lin and Pantel 2001), or by a single vector, where features
represent pairs of instantiations (Szpektor et al 2004; Yates and Etzioni 2009). The
former variant reduces sparsity problems, whereas Yates and Etzioni showed the latter
is more informative and performs favorably on their data.
Similarity function. We consider two similarity functions: The symmetric Lin (Lin
and Pantel 2001) similarity measure, and the directional BInc (Szpektor and Dagan
2008) similarity measure, reviewed in Section 2. Thus, information about the direction
of entailment is provided by the BInc measure.
We compute for any pair of templates (t1, t2) 12 distributional similarity scores using
all possible combinations of the aforementioned dimensions. These scores are then used
as 12 features representing the pair (t1, t2). (A full description of the features is given in
Section 5.) This is reminiscent of Connor and Roth (2007), who used the output of unsu-
pervised classifiers as features for a supervised classifier in a verb disambiguation task.
(4) Training a classifier Two types of classifiers may be trained in our scheme over
the training set: margin classifiers (such as SVM) and probabilistic classifiers. Given a
pair of templates (u, v) and their feature vector Fuv, we denote by an indicator variable
Iuv the event that u entails v. A margin classifier estimates a score Suv for the event
Iuv = 1, which indicates the positive or negative distance of the feature vector Fuv from
the separating hyperplane. A probabilistic classifier provides the posterior probability
Puv = P(Iuv = 1|Fuv).
4.2 Global Learning of Edges
In this step we get a set of propositional templates as input, and we would like to learn
all of the entailment relations between these propositional templates. For every pair of
templates we can compute the distributional similarity features and get a score from
the trained entailment classifier. Once all the scores are calculated we try to find the
optimal graph?that is, the best set of edges over the propositional templates. Thus, in
this scenario the input is the nodes of the graph and the output are the edges.
To learn edges we consider global constraints, which allow only certain graph
topologies. Because we seek a global solution under transitivity and other constraints,
ILP is a natural choice, enabling the use of state-of-the-art ILP optimization packages.
Given a set of nodes V and a weighting function f : V ? V ? R (derived from the
83
Computational Linguistics Volume 38, Number 1
entailment classifier in our case), we want to learn the directed graph G = (V, E), where
E = {(u, v)| Iuv = 1}, by solving the following ILP over the variables Iuv:
G? = argmax
G
?
u=v
f (u, v) ? Iuv (8)
s.t. ?u,v,w?V Iuv + Ivw ? Iuw ? 1 (9)
?u,v?Ayes Iuv = 1 (10)
?u,v?Ano Iuv = 0 (11)
?u=v Iuv ? {0, 1} (12)
The objective function in Equation (8) is simply a sum over the weights of the graph
edges. The global constraint is given in Equation (9) and states that the graph must
respect transitivity. This constraint is equivalent to the one suggested by Finkel and
Manning (2008) in a coreference resolution task, except that the edges of our graph
are directed. The constraints in Equations (10) and (11) state that for a few node pairs,
defined by the sets Ayes and Ano, respectively, we have prior knowledge that one node
does or does not entail the other node. Note that if (u, v) ? Ano, then due to transitivity
there must be no path in the graph from u to v, which rules out additional edge combi-
nations. We elaborate on how the sets Ayes and Ano are computed in our experiments in
Section 5. Altogether, this Integer Linear Program contains O(|V|2) variables and O(|V|3)
constraints, and can be solved using state-of-the-art optimization packages.
A theoretical aspect of this optimization problem is that it is NP-hard. We can phrase
it as a decision problem in the following manner: Given V, f , and a threshold k, we
wish to know if there is a set of edges E that respects transitivity and
?
(u,v)?E
f (u, v) ? k.
Yannakakis (1978) has shown that the simpler problem of finding in a graph G? =
(V?, E?) a subset of edges A ? E? that respects transitivity and |A| ? k is NP-hard. Thus,
we can conclude that our optimization problem is also NP-hard by the trivial poly-
nomial reduction defining the function f that assigns the score 0 for node pairs (u, v) /? E?
and the score 1 for node pairs (u, v) ? E?. Because the decision problem is NP-hard, it is
clear that the corresponding maximization problem is also NP-hard. Thus, obtaining a
solution using ILP is quite reasonable and in our experiments also proves to be efficient
(Section 5).
Next, we describe two ways of obtaining the weighting function f , depending on
the type of entailment classifier we prefer to train.
4.2.1 Score-Based Weighting Function. In this case, we assume that we choose to train a
margin entailment classifier estimating the score Suv (a positive score if the classifier
predicts entailment, and a negative score otherwise) and define f score(u, v) = Suv ? ?.
This gives rise to the following objective function:
G?score = argmax
G
?
u=v
(Suv ? ?) ? Iuv = argmax
G
?
?
?
u=v
Suv ? Iuv
?
?? ? ? |E| (13)
The term ? ? |E| is a regularization term reflecting the fact that edges are sparse. Intu-
itively, this means that we would like to insert into the graph only edges with a score
84
Berant et al Learning Entailment Relations by Global Graph Structure Optimization
Suv > ?, or in other words to ?push? the separating hyperplane towards the positive
half space by ?. Note that the constant ? is a parameter that needs to be estimated and
we discuss ways of estimating it in Section 5.2.
4.2.2 Probabilistic Weighting Function. In this case, we assume that we choose to train
a probabilistic entailment classifier. Recall that Iuv is an indicator variable denoting
whether u entails v, that Fuv is the feature vector for the pair of templates u and v, and de-
fine F to be the set of feature vectors for all pairs of templates in the graph. The classifier
estimates the posterior probability of an edge given its features: Puv = P(Iuv = 1|Fuv),
and we would like to look for the graph G that maximizes the posterior probability
P(G|F). In Appendix A we specify some simplifying independence assumptions under
which this graph maximizes the following linear objective function:
G?prob = argmax
G
?
u=v
(log
Puv
1 ? Puv
+ log?) ? Iuv = argmax
G
?
u=v
log
Puv
1 ? Puv
? Iuv + log? ? |E|
(14)
where ? = P(Iuv=1)P(Iuv=0) is the prior odds ratio for an edge in the graph, which needs to be
estimated in some manner. Thus, the weighting function is defined by fprob(u, v) =
log Puv1?Puv + log?.
Both the score-based and the probabilistic objective functions obtained are quite
similar: Both contain a weighted sum over the edges and a regularization component
reflecting the sparsity of the graph. Next, we show that we can provide a probabilistic
interpretation for our score-based function (under certain conditions), which will allow
us to use a margin classifier and interpret its output probabilistically.
4.2.3 Probabilistic Interpretation of Score-Based Weighting Function. We would like to use
the score Suv, which is bounded in (?,??), and derive from it a probability Puv. To
that end we project Suv onto (0, 1) using the sigmoid function, and define Puv in the
following manner:
Puv =
1
1 + exp(?Suv)
(15)
Note that under this definition the log probability ratio is equal to the inverse of the
sigmoid function:
log
Puv
1 ? Puv
= log
1
1+exp(?Suv )
exp(?Suv )
1+exp(?Suv )
= log 1
exp(?Suv)
= Suv (16)
Therefore, when we derive Puv from Suv with the sigmoid function, we can rewrite
G?prob as:
G?prob = argmax
G
?
u=v
Suv ? Iuv + log? ? |E| = G?score (17)
where we see that in this scenario the two objective functions are identical and the
regularization term ? is related to the edge prior odds ratio by: ? = ? log?.
85
Computational Linguistics Volume 38, Number 1
Moreover, assume that the score Suv is computed as a linear combination over n
features (such as a linear-kernel SVM), that is, Suv =
?n
i=1 S
i
uv ? ?i, where S
i
uv denotes
feature values and ?i denotes feature weights. In this case, the projected probability
acquires the standard form of a logistic classifier:
Puv =
1
1 + exp(?
n
?
i=1
Siuv ? ?i)
(18)
Hence, we can train the weights ?i using a margin classifier and interpret the output
of the classifier probabilistically, as we do with a logistic classifier. In our experiments
in Section 5 we indeed use a linear-kernel SVM to train the weights ?i and then we
can interchangeably interpret the resulting ILP as either score-based or probabilistic
optimization.
4.2.4 Comparison to Snow, Jurafsky, and Ng (2006). Our work resembles Snow, Jurafsky,
and Ng?s work in that both try to learn graph edges given a transitivity constraint. There
are two key differences in the model and in the optimization algorithm, however. First,
they employ a greedy optimization algorithm that incrementally adds hyponyms to a
large taxonomy (WordNet), whereas we simultaneously learn all edges using a global
optimization method, which is more sound and powerful theoretically, and leads to
the optimal solution. Second, Snow, Jurafsky, and Ng?s model attempts to determine
the graph that maximizes the likelihood P(F|G) and not the posterior P(G|F). If we cast
their objective function as an ILP we get a formulation that is almost identical to ours,
only containing the inverse prior odds ratio log 1? = ? log? rather than the prior odds
ratio as the regularization term (cf. Section 2):
G?Snow = argmax
G
?
u=v
log
Puv
(1 ? Puv)
? Iuv ? log? ? |E| (19)
This difference is insignificant when ? ? 1, or when ? is tuned empirically for optimal
performance on a development set. If, however, ? is statistically estimated, this might
cause unwarranted results: Their model will favor dense graphs when the prior odds
ratio is low (? < 1 or P(Iuv = 1) < 0.5), and sparse graphs when the prior odds ratio is
high (? > 1 or P(Iuv = 1) > 0.5), which is counterintuitive. Our model does not suffer
from this shortcoming because it optimizes the posterior rather than the likelihood. In
Section 5 we show that our algorithm significantly outperforms the algorithm presented
by Snow, Jurafsky, and Ng.
5. Experimental Evaluation
This section presents an evaluation and analysis of our algorithm.
5.1 Experimental Setting
A health-care corpus of 632MB was harvested from the Web and parsed using the Mini-
par parser (Lin 1998b). The corpus contains 2,307,585 sentences and almost 50 million
86
Berant et al Learning Entailment Relations by Global Graph Structure Optimization
Table 2
The similarity score features used to represent pairs of templates. The columns specify the
corpus over which the similarity score was computed, the template representation, the
similarity measure employed, and the feature representation (as described in Section 4.1).
# Corpus Template Similarity measure Feature representation
1 health-care binary BInc pair of CUI tuples
2 health-care binary BInc pair of CUIs
3 health-care binary BInc CUI tuple
4 health-care binary BInc CUI
5 health-care binary Lin pair of CUI tuples
6 health-care binary Lin pair of CUIs
7 health-care binary Lin CUI tuple
8 health-care binary Lin CUI
9 health-care unary BInc CUI tuple
10 health-care unary BInc CUI
11 health-care unary Lin CUI tuple
12 health-care unary Lin CUI
13 RCV1 binary Lin lexical items
14 RCV1 unary Lin lexical items
15 RCV1 unary BInc lexical items
16 Lin & Pantel binary Lin lexical items
word tokens. We used the Unified Medical Language System (UMLS)6 to annotate
medical concepts in the corpus. The UMLS is a database that maps natural language
phrases to over one million concept identifiers in the health-care domain (termed
CUIs). We annotated all nouns and noun phrases that are in the UMLS with their
(possibly multiple) CUIs. We now provide the details of training an entailment classifier
as explained in Section 4.1.
We extracted all templates from the corpus where both argument instantiations are
medical concepts, that is, annotated with a CUI (?50,000 templates). This was done to
increase the likelihood that the extracted templates are related to the health-care domain
and reduce problems of ambiguity.
As explained in Section 4.1, a pair of templates constitutes an input example for
the entailment classifier, and should be represented by a set of features. The features
we used were different distributional similarity scores for the pair of templates, as
summarized in Table 2. Twelve distributional similarity measures were computed over
the health-care corpus using the aforementioned variations (Section 4.1), where two
feature representations were considered: in the UMLS each natural language phrase
may be mapped not to a single CUI, but to a tuple of CUIs. Therefore, in the first
representation, each feature vector coordinate counts the number of times a tuple of
CUIs was mapped to the term instantiating the template argument, and in the second
representation it counts the number of times each single CUI was one of the CUIs
mapped to the term instantiating the template argument. In addition, we obtained the
original template similarity lists learned by Lin and Pantel (2001), and had available
three distributional similarity measures learned by Szpektor and Dagan (2008), over the
RCV1 corpus,7 as detailed in Table 2. Thus, each pair of templates is represented by a
total of 16 distributional similarity scores.
6 http://www.nlm.nih.gov/research/umls.
7 http://trec.nist.gov/data/reuters/reuters.html.
87
Computational Linguistics Volume 38, Number 1
We automatically generated a balanced training set of 20,144 examples using Word-
Net and the procedure described in Section 4.1, and trained the entailment classifier
with SVMperf (Joachims 2005). We use the trained classifier to obtain estimates for Puv
and Suv, given that the score-based and probabilistic scoring functions are equivalent
(cf. Section 4.2.3).
To evaluate the performance of our algorithm, we manually constructed gold-
standard entailment graphs. First, 23 medical target concepts, representing typical top-
ics of interest in the medical domain, were manually selected from a (longer) list of
the most frequent concepts in the health-care corpus. The 23 target concepts are: alcohol,
asthma, biopsy, brain, cancer, CDC, chemotherapy, chest, cough, diarrhea, FDA, headache, HIV,
HPV, lungs, mouth, muscle, nausea, OSHA, salmonella, seizure, smoking, and x-ray. For each
concept, we wish to learn a focused entailment graph (cf. Figure 1). Thus, the nodes of
each graph were defined by extracting all propositional templates in which the corre-
sponding target concept instantiated an argument at least K(= 3) times in the health-
care corpus (average number of graph nodes = 22.04, std = 3.66, max = 26, min = 13).
Ten medical students were given the nodes of each graph (propositional templates)
and constructed the gold standard of graph edges using a Web interface. We gave an
oral explanation of the annotation process to each student, and the first two graphs
annotated by every student were considered part of the annotator training phase and
were discarded. The annotators were able to select every propositional template and
observe all of the instantiations of that template in our health-care corpus. For example,
selecting the template X helps with nausea might show the propositions relaxation helps
with nausea, acupuncture helps with nausea, and Nabilone helps with nausea. The concept
of entailment was explained under the framework of TE (Dagan et al 2009), that is, the
template t1 entails the template t2 if given that the instantiation of t1 with some concept
is true then the instantiation of t2 with the same concept is most likely true.
As explained in Section 3.2, we did not perform any disambiguation because a
target concept disambiguates the propositional templates in focused entailment graphs.
In practice, cases of ambiguity were very rare, except for a single scenario where in
templates such as X treats asthma, annotators were unclear whether X is a type of doctor
or a type of drug. The annotators were instructed in such cases to select the template,
read the instantiations of the template in the corpus, and choose the sense that is most
prevalent in the corpus. This instruction was applicable to all cases of ambiguity.
Each concept graph was annotated by two students. Following the current recog-
nizing TE (RTE) practice (Bentivogli et al 2009), after initial annotation the two students
met for a reconciliation phase. They worked to reach an agreement on differences and
corrected their graphs. Inter-annotator agreement was calculated using the kappa statis-
tic (Siegel and Castellan 1988) both before (? = 0.59) and after (? = 0.9) reconciliation.
Each learned graph was evaluated against the two reconciliated graphs.
Summing the number of possible edges over all 23 concept graphs we get 10,364
possible edges, of which 882 on average were included by the annotators (averaging
over the two gold-standard annotations for each graph). The concept graphs were
randomly split into a development set (11 concepts) and a test set (12 concepts).
We used the lpsolve8 package to learn the edges of the graphs. This package ef-
ficiently solves the model without imposing integer restrictions9 and then uses the
branch-and-bound method to find an optimal integer solution. We note that in the
8 http://lpsolve.sourceforge.net/5.5/.
9 While ILP is an NP-hard problem, LP is a polynomial problem and can be solved efficiently.
88
Berant et al Learning Entailment Relations by Global Graph Structure Optimization
experiments reported in this article the optimal solution without integer restrictions
was already integer. Thus, although in general our optimization problem is NP-hard,
in our experiments we were able to reach an optimal solution for the input graphs
very efficiently (we note that in some scenarios not reported in this article the optimal
solution was not integer and so an integer solution is not guaranteed a priori).
As mentioned in Section 4.2, we added a few constraints in cases where there was
strong evidence that edges are not in the graph. This is done in the following scenarios
(examples given in Table 3): (1) When two templates u and v are identical except for
a pair of words wu and wv, and wu is an antonym of wv, or a hypernym of wv at
distance ? 2 in WordNet. (2) When two nodes u and v are transitive ?opposites,? that
is, if u = X
subj
??? w
obj
?? Y and v = X
obj
?? w
subj
??? Y, for any word w. We note that there are
some transitive verbs that express a reciprocal activity, such as X marries Y, but usually
reciprocal events are not expressed using a transitive verb structure.
In addition, in some cases we have strong evidence that edges do exist in the graph.
This is done in a single scenario (see Table 3), which is specific to the output of Minipar:
when two templates differ by a single edge and the first is of the type X
obj
?? Y and
the other is of the type X
vrel??? Y, which expresses a passive verb modifier of nouns.
Altogether, these initializations took place in less than 1% of the node pairs in the
graphs. We note that we tried to use WordNet relations such as hypernym and synonym
as ?positive? hard constraints (using the constraint Iuv = 1), but this resulted in reduced
performance because the precision of WordNet was not high enough.
The graphs learned by our algorithm were evaluated by two measures. The first
measure evaluates the graph edges directly, and the second measure is motivated by
semantic inference applications that utilize the rules in the graph. The first measure is
simply the F1 of the set of learned edges compared to the set of gold-standard edges.
In the second measure we take the set of learned rules and infer new propositions by
applying the rules over all propositions extracted from the health-care corpus. We apply
the rules iteratively over all propositions until no new propositions are inferred. For
example, given the corpus proposition relaxation reduces nausea and the edges X reduces
nausea ? X helps with nausea and X helps with nausea ? X related to nausea, we eval-
uate the set {relaxation reduces nausea, relaxation helps with nausea, relaxation related to
nausea}. For each graph we measure the F1 of the set of propositions inferred by the
learned graphs when compared to the set of propositions inferred by the gold-standard
graphs. For both measures the final score of an algorithm is a macro-average F1 over
the 24 gold-standard test-set graphs (two gold-standard graphs for each of the 12 test
concepts).
Table 3
Scenarios in which we added hard constraints to the ILP.
Scenario Example Initialization
antonym (X
subj
??? decrease
obj
?? Y,X
subj
??? increase
obj
?? Y) Iuv = 0
hypernym ? 2 (X
subj
??? affect
obj
?? Y,X
subj
??? irritate
obj
?? Y) Iuv = 0
transitive opposite (X
subj
??? cause
obj
?? Y,Y
subj
??? cause
obj
?? X) Iuv = 0
syntactic variation (X
subj
??? follow
obj
?? Y,Y
subj
??? follow vrel?? X) Iuv = 1
89
Computational Linguistics Volume 38, Number 1
Learning the edges of a graph given an input concept takes about 1?2 seconds on a
standard desktop.
5.2 Evaluated Algorithms
First, we describe some baselines that do not utilize the entailment classifier or the
ILP solver. For each of the 16 distributional similarity measures (Table 2) and for each
template t, we computed a list of templates most similar to t (or entailing t for directional
measures). Then, for each measure we learned graphs by inserting an edge (u, v), when
u is in the top K templates most similar to v. The parameter K can be optimized either on
the automatically generated training set (from WordNet) or on the manually annotated
development set. We also learned graphs using WordNet: We inserted an edge (u, v)
when u and v differ by a single word wu and wv, respectively, and wu is a direct hyponym
or synonym of wv. Next, we describe algorithms that utilize the entailment classifier.
Our algorithm, named ILP-Global, utilizes global information and an ILP formula-
tion to find maximum a posteriori graphs. Therefore, we compare it to the following
three variants: (1) ILP-Local: An algorithm that uses only local information. This is
done by omitting the global transitivity constraints, and results in an algorithm that
inserts an edge (u, v) if and only if (Suv ? ?) > 0. (2) Greedy-Global: An algorithm that
looks for the maximum a posteriori graphs but only employs the greedy optimization
procedure as described by Snow, Jurafsky, and Ng (2006). (3) ILP-Global-Likelihood:
An ILP formulation where we look for the maximum likelihood graphs, as described by
Snow, Jurafsky, and Ng (cf. Section 4.2).
We evaluate these algorithms in three settings which differ in the method by which
the edge prior odds ratio, ? (or ?), is estimated: (1) ? = 1 (? = 0), which means that
no prior is used. (2) Tuning ? and using the value that maximizes performance over the
development set. (3) Estimating ? using maximum likelihood over the development set,
which results in ? ? 0.1 (? ? 2.3), corresponding to the edge density P(Iuv = 1) ? 0.09.
For all local algorithms whose output does not respect transitivity constraints, we
added all edges inferred by transitivity. This was done because we assume that the rules
learned are to be used in the context of an inference or entailment system. Because such
systems usually perform chaining of entailment rules (Raina, Ng, and Manning 2005;
Bar-Haim et al 2007; Harmeling 2009), we conduct this chaining as well. Nevertheless,
we also measured performance when edges inferred by transitivity are not added: We
once again chose the edge prior value that maximizes F1 over the development set
and obtained macro-average recall/precision/F1 of 51.5/34.9/38.3. This performance is
comparable to the macro-average recall/precision/F1 of 44.5/45.3/38.1 we report next
in Table 4.
5.3 Experimental Results and Analysis
In this section we present experimental results and analysis that show that the
ILP-Global algorithm improves performance over baselines, specifically in terms of
precision.
Tables 4?7 and Figure 2 summarize the performance of the algorithms. Table 4
shows our main result when the parameters ? and K are optimized to maximize per-
formance over the development set. Notice that the algorithm ILP-Global-Likelihood
is omitted, because when optimizing ? over the development set it conflates with
ILP-Global. The rows Local1 and Local2 present the best algorithms that use a single
distributional similarity resource. Local1 and Local2 correspond to the configurations
90
Berant et al Learning Entailment Relations by Global Graph Structure Optimization
Table 4
Results when tuning for performance over the development set.
Edges Propositions
Recall Precision F1 Recall Precision F1
ILP-Global (? = 0.45) 46.0 50.1 43.8 67.3 69.6 66.2
Greedy-Global (? = 0.3) 45.7 37.1 36.6 64.2 57.2 56.3
ILP-Local (? = 1.5) 44.5 45.3 38.1 65.2 61.0 58.6
Local1 (K = 10) 53.5 34.9 37.5 73.5 50.6 56.1
Local2 (K = 55) 52.5 31.6 37.7 69.8 50.0 57.1
Table 5
Results when the development set is not used to estimate ? and K.
Edges Propositions
Recall Precision F1 Recall Precision F1
ILP-Global 58.0 28.5 35.9 76.0 46.0 54.6
Greedy-Global 60.8 25.6 33.5 77.8 41.3 50.9
ILP-Local 69.3 19.7 26.8 82.7 33.3 42.6
Local1 (K = 100) 92.6 11.3 20.0 95.3 18.9 31.1
Local2 (K = 100) 63.1 25.5 34.0 77.7 39.9 50.9
WordNet 10.8 44.1 13.2 39.9 72.4 47.3
Table 6
Results with prior estimated on the development set, that is ? = 0.1, which is equivalent to
? = 2.3.
Edges Propositions
Recall Precision F1 Recall Precision F1
ILP-Global 16.8 67.1 24.4 43.9 86.8 56.3
ILP-Global-Likelihood 91.8 9.8 17.5 94.0 16.7 28.0
Greedy-Global 14.7 62.9 21.2 43.5 86.6 56.2
Greedy-Global-Likelihood 100.0 9.3 16.8 100.0 15.5 26.5
described in Table 2 by features no. 5 and no. 1, respectively (see also Table 8). ILP-
Global improves performance by at least 13%, and significantly outperforms all local
methods, as well as the greedy optimization algorithm both on the edges F1 measure
(p < 0.05) and on the propositions F1 measure (p < 0.01).
10
Table 5 describes the results when the development set is not used to estimate the
parameters ? and K: A uniform prior (Puv = 0.5) is assumed for algorithms that use
the entailment classifier, and the automatically generated training set is employed to
estimate K. Again ILP-Global-Likelihood is omitted in the absence of a prior. ILP-Global
outperforms all other methods in this scenario as well, although by a smaller margin
for a few of the baselines. Comparing Table 4 to Table 5 reveals that excluding the
10 We tested significance using the two-sided Wilcoxon rank test (Wilcoxon 1945).
91
Computational Linguistics Volume 38, Number 1
Table 7
Results per concept for the ILP-Global.
Concept R P F1
Smoking 58.1 81.8 67.9
Seizure 64.7 51.2 57.1
Headache 60.9 50.0 54.9
Lungs 50.0 56.5 53.1
Diarrhea 42.1 60.0 49.5
Chemotherapy 44.7 52.5 48.3
HPV 35.2 76.0 48.1
Salmonella 27.3 80.0 40.7
X-ray 75.0 23.1 35.3
Asthma 23.1 30.6 26.3
Mouth 17.7 35.5 23.7
FDA 53.3 15.1 23.5
sparse prior indeed increases recall at a price of a sharp decrease in precision. Note,
however, that local algorithms are more vulnerable to this phenomenon. This makes
sense because in local algorithms eliminating the prior adds edges that in turn add more
edges due to the constraint of transitivity and so recall dramatically rises at the expense
of precision. Global algorithms are not as prone to this effect because they refrain from
adding edges that eventually lead to the addition of many unwarranted edges.
Table 5 also shows that WordNet, a manually constructed resource, has notably
the highest precision and lowest recall. The low recall exemplifies how the entailment
relations given by the gold-standard annotators transcend much beyond simple lexical
relations that appear in WordNet: Many of the gold-standard entailment relations are
missing from WordNet or involve multi-word phrases that do not appear in WordNet
at all.
Note that although the precision of WordNet is the highest in Table 5, its absolute
value (44.1%) is far from perfect. This illustrates that hierarchies of predicates are quite
Figure 2
Recall-precision curve comparing ILP-Global with Greedy-Global and ILP-Local.
92
Berant et al Learning Entailment Relations by Global Graph Structure Optimization
Table 8
Results of all distributional similarity measures when tuning K over the development set.
We encode the description of the measures presented in Table 2 in the following manner?
h = health-care corpus; R = RCV1 corpus; b = binary templates; u = unary templates; L = Lin similarity
measure; B = BInc similarity measure; pCt = pair of CUI tuples representation; pC = pair of CUIs
representation; Ct = CUI tuple representation; C = CUI representation; Lin & Pantel = similarity
lists learned by Lin and Pantel.
Edges Propositions
Dist. sim. measure Recall Precision F1 Recall Precision F1
h-b-B-pCt 52.5 31.6 37.7 69.8 50.0 57.1
h-b-B-pC 50.5 26.5 30.7 67.1 43.5 50.1
h-b-B-Ct 10.4 44.5 15.4 39.1 78.9 51.6
h-b-B-C 7.6 42.9 11.1 37.9 79.8 50.7
h-b-L-pCt 53.4 34.9 37.5 73.5 50.6 56.1
h-b-L-pC 47.2 35.2 35.6 68.6 52.9 56.2
h-b-L-Ct 47.0 26.6 30.2 64.9 47.4 49.6
h-b-L-C 34.6 22.9 22.5 57.2 52.6 47.6
h-u-B-Ct 5.1 37.4 8.5 35.1 91.0 49.7
h-u-B-C 7.2 42.4 11.5 36.1 90.3 50.1
h-u-L-Ct 22.8 22.0 18.3 49.7 49.2 44.5
h-u-L-C 16.7 26.3 17.8 47.0 56.8 48.1
R-b-L-l 49.4 21.8 25.2 72.4 39.0 45.5
R-u-L-l 24.1 30.0 16.8 47.1 55.2 42.1
R-u-B-l 9.5 57.1 14.1 37.2 84.0 49.5
Lin & Pantel 37.1 32.2 25.1 58.9 54.6 48.6
ambiguous and thus using WordNet directly yields relatively low precision. WordNet
is vulnerable to such ambiguity because it is a generic domain-independent resource,
whereas our algorithm learns from a domain-specific corpus. For example, the words
have and cause are synonyms according to one of the senses in WordNet and so the
erroneous rule X have asthma ? X cause asthma is learned using WordNet. Another
example is the rule X follows chemotherapy ? X takes chemotherapy, which is incorrectly
inferred because follow is a hyponym of take according to one of WordNet?s senses (she
followed the feminist movement). Due to these mistakes made by WordNet, the precision
achieved by our automatically trained ILP-Global algorithm when tuning parameters
on the development set (Table 4) is higher than that of WordNet.
Table 6 shows the results when the prior ? is estimated using maximum likelihood
over the development set (by computing the edge density over all the development
set graphs), and not tuned empirically with grid search. This allows for a comparison
between our algorithm that maximizes the a posteriori probability and Snow, Jurafsky,
and Ng?s (2006) algorithm that maximizes the likelihood. The gold-standard graphs are
quite sparse (? ? 0.1); therefore, as explained in Section 4.2.4, the effect of the prior is
substantial. ILP-Global and Greedy-Global learn sparse graphs with high precision and
low recall, whereas ILP-Global-Likelihood and Greedy-Global-Likelihood learn dense
graphs with high recall but very low precision. Overall, optimizing the a posteriori
probability is substantially better than optimizing likelihood, but still leads to a large
degradation in performance. This can be explained because our algorithm is not purely
probabilistic: The learned graphs are the product of mixing a probabilistic objective
function with non-probabilistic constraints. Thus, plugging the estimated prior into this
model results in performance that is far from optimal. In future work, we will examine
93
Computational Linguistics Volume 38, Number 1
a purely probabilistic approach that will allow us to reach good performance when
estimating ? directly. Nevertheless, currently optimal results are achieved when the
prior ? is tuned empirically.
Figure 2 shows a recall?precision curve for ILP-Global, Greedy-Global, and ILP-
Local, obtained by varying the prior parameter, ?. The figure clearly demonstrates the
advantage of using global information and ILP. ILP-Global is better than Greedy-Global
and ILP-Local in almost every point of the recall?precision curve, regardless of the exact
value of the prior parameter. Last, we present for completeness in Table 7 the results of
ILP-Global for all concepts in the test set.
In Table 8 we present the results obtained for all 16 distributional similarity mea-
sures. The main conclusion we can derive from this table is that the best distributional
similarity measures are those that represent templates using pairs of argument instan-
tiations rather than each argument separately. A similar result was found by Yates and
Etzioni (2009), who described the RESOLVER paraphrase learning system and have
shown that it outperforms DIRT. In their analysis, they attribute this result to their
representation that utilizes pairs of arguments comparing to DIRT, which computes a
separate score for each argument.
In the next two sections we perform a more thorough qualitative and quantitative
comparison trying to analyze the importance of using global information in graph
learning (Section 5.3.1), as well as the contribution of using ILP rather than a greedy
optimization procedure (Section 5.3.2). We note that the analysis presented in both sec-
tions is for the results obtained when optimizing parameters over the development set.
5.3.1 Global vs. Local Information. We looked at all edges in the test-set graphs where
ILP-Global and ILP-Local disagree and checked which algorithm was correct. Table 9
presents the result. The main advantage of using ILP-Global is that it avoids inserting
wrong edges into the graph. This is because ILP-Local adds any edge (u, v) such that
Puv crosses a certain threshold, disregarding edges that will be consequently added due
to transitivity (recall that for local algorithms we add edges inferred by transitivity, cf.
Section 5.2). ILP-Global will avoid such edges of high probability if it results in inserting
many low probability edges. This results in an improvement in precision, as exhibited
by Table 4.
Figures 3 and 4 show fragments of the graphs learned by ILP-Global and ILP-
Local (prior to adding transitive edges) for the test-set concepts diarrhea and seizure,
and illustrate qualitatively how global considerations improve precision. In Figure 3,
we witness that the single erroneous edge X results in diarrhea ? X prevents diarrhea
inserted by the local algorithm because Puv is high, effectively bridges two strongly
connected components and induces a total of 12 wrong edges (all edges from the
upper component to the lower component), whereas ILP-Global refrains from inserting
this edge. Figure 4 depicts an even more complex scenario. First, ILP-Local induces
a strongly connected component of five nodes for the predicates control, treat, stop,
Table 9
Comparing disagreements between ILP-Global and ILP-Local against the gold-standard graphs.
Global=True/Local=False Global=False/Local=True
Gold standard=true 48 42
Gold standard=false 78 494
94
Berant et al Learning Entailment Relations by Global Graph Structure Optimization
Figure 3
A comparison between ILP-Global and ILP-local for two fragments of the test-set concept
diarrhea.
reduce, and prevent, whereas ILP-Global splits this strongly connected component into
two, which although not perfect, is more compatible with the gold-standard graphs.
In addition, ILP-Local inserts four erroneous edges that connect two components of
size 4 and 5, which results in adding eventually 30 wrong edges. On the other hand,
Figure 4
A comparison between ILP-Global and ILP-Local for two fragments of the test-set concept
seizure.
95
Computational Linguistics Volume 38, Number 1
ILP-Global is aware of the consequences of adding these four seemingly good edges,
and prefers to omit them from the learned graph, leading to much higher precision.
Although the main contribution of ILP-Global, in terms of F1, is in an increase in
precision, we also notice an increase in recall in Table 4. This is because the optimal
prior is ? = 0.45 in ILP-Global but ? = 1.5 in ILP-Local. Thus, any edge (u, v) such that
0.45 < Suv < 1.5 will have positive weight in ILP-Global and might be inserted into the
graph, but will have negative weight in ILP-Local and will be rejected. The reason is that
in a local setting, reducing false positives is handled only by applying a large penalty
for every wrong edge, whereas in a global setting wrong edges can be rejected because
they induce more ?bad? edges. Overall, this leads to an improved recall in ILP-Global.
This also explains why ILP-Local is severely harmed when no prior is used at all, as
shown in Table 5.
Last, we note that across the 12 test-set graphs, ILP-Global achieves better F1 over
the edges in 7 graphs with an average advantage of 11.7 points, ILP-Local achieves
better F1 over the edges in 4 graphs with an average advantage of 3.0 points, and one
performance is equal.
5.3.2 Greedy vs. Non-Greedy Optimization. We would like to understand how using an
ILP solver improves performance compared with a greedy optimization procedure.
Table 4 demonstrates that ILP-Global and Greedy-Global reach a similar level of re-
call, although ILP-Global achieves far better precision. Again, we investigated edges
for which the two algorithms disagree and checked which one was correct. Table 10
demonstrates that the higher precision is because ILP-Global avoids inserting wrong
edges into the graph.
Figure 5 illustrates some of the reasons ILP-Global performs better than Greedy-
Global. Parts A1?A3 show the progression of Greedy-Global, which is an incremental
algorithm, for a fragment of the headache graph. In part A1 the learning algorithm still
separates the nodes X prevents headache and X reduces headache from the nodes X causes
headache and X results in headache (nodes surrounded by a bold oval shape constitute
a strongly connected component). After two iterations, however, the four nodes are
joined into a single strongly connected component, which is an error in principle
but at this point seems to be the best decision to increase the posterior probability
of the graph. This greedy decision has two negative ramifications. First, the strongly
connected component can no longer be untied. Thus, in A3 we observe that in future
iterations the strongly connected component expands further and many more wrong
edges are inserted into the graph. On the other hand, in B we see that ILP-Global takes
into consideration the global interaction between the four nodes and other nodes of the
graph, and decides to split this strongly connected component in two, which improves
the precision of ILP-Global. Second, note that in A3 the nodes Associate X with headache
and Associate headache with X are erroneously isolated. This is because connecting them
to the strongly connected component that contains six nodes will add many edges with
Table 10
Comparing disagreements between ILP-Global and Greedy-Global against the gold-standard
graphs.
ILP=True/Greedy=False ILP=False/Greedy=True
Gold standard=true 66 56
Gold standard=false 44 480
96
Berant et al Learning Entailment Relations by Global Graph Structure Optimization
Figure 5
A comparison between ILP-Global and Greedy-Global. Parts A1?A3 depict the incremental
progress of Greedy Global for a fragment of the headache graph. Part B depicts the corresponding
fragment in ILP-Global. Nodes surrounded by a bold oval shape are strongly connected
components.
low probability and so this is avoided by Greedy-Global. Because in ILP-Global the
strongly connected component was split in two, it is possible to connect these two nodes
to some of the other nodes and raise the recall of ILP-Global. Thus, we see that greedy
optimization might get stuck in local maxima and consequently suffer in terms of both
precision and recall.
Last, we note that across the 12 test-set graphs, ILP-Global achieves better F1 over
the edges in 9 graphs with an average advantage of 10.0 points, Greedy-Global achieves
better F1 over the edges in 2 graphs with an average advantage of 1.5 points, and in one
case performance is equal.
5.4 Error Analysis
In this section, we compare the results of ILP-Global with the gold-standard graphs
and perform error analysis. Error analysis was performed by comparing the 12 graphs
learned by ILP-Global to the corresponding 12 gold-standard graphs (randomly sam-
pling from the two available gold-standard graphs), and manually examining all edges
for which the two disagree. We found that the number of false positives and false
negatives is almost equal: 282 edges were learned by ILP-Global but are not in the gold-
standard graphs (false positive) and 287 edges were in the gold-standard graphs but
were not learned by ILP-Global (false negatives).
97
Computational Linguistics Volume 38, Number 1
Table 11
Error analysis for false positives and false negatives.
False positives False negatives
Total count 282 Total count 287
Classifier error 84.8% Classifier error 73.5%
Co-hyponym error 18.0% Long-predicate error 36.2%
Direction error 15.1% Generality error 26.8%
String overlap error 20.9%
Table 11 presents the results of our manual error analysis. Most evident is the fact
that the majority of mistakes are misclassifications of the entailment classifier. For 73.5%
of the false negatives the classifier?s probability was Puv < 0.5 and for 84.8% of the false
positives the classifier?s probability was Puv > 0.5. This shows that our current classifier
struggles to distinguish between positive and negative examples. Figure 6 illustrates
some of this difficulty by showing the distribution of the classifier?s probability, Puv,
over all node pairs in the 12 test-set graphs. Close to 80% of the scores are in the range
0.45?0.5, most of which are simply node pairs for which all distributional similarity
features are zero. Although in the great majority of such node pairs (t1, t2) t1 indeed
does not entail t2, there are also some cases where t1 does entail t2. This implies that the
current feature representation is not rich enough, and in the next section we explore a
larger feature set.
Table 11 also shows some other reasons found for false positives. Many false posi-
tives are pairs of predicates that are semantically related, that is, 18% of false positives
are templates that are hyponyms of a common predicate (co-hyponym error), and 15.1%
of false positives are pairs where we err in the direction of entailment (direction error).
For example ILP-Global learns that place X in mouth ? remove X from mouth, which is a
Figure 6
Distribution of probabilities given by the classifier over all node pairs of the test-set graphs.
98
Berant et al Learning Entailment Relations by Global Graph Structure Optimization
co-hyponym error, and also that X affects lungs ? X damages lungs, which is a direction
error because entailment holds in the other direction. This illustrates the infamous
difficulty of distributional similarity features to discern the type of semantic relation
between two predicates.
Table 11 also shows additional reasons for false negatives. We found that in 36.2% of
false negatives one of the two templates contained a ?long? predicate, that is a predicate
composed of more than one content word, such as Ingestion of X causes injury to Y. This
might indicate that the size of the health-care corpus is too small to collect sufficient
statistics for complex predicates. In addition, 26.8% of false negatives were manually
analyzed as ?generality errors.? An example is the edge HPV strain causes X ? associate
HPV with X that is in the gold-standard graph but was missed by ILP-Global. Indeed,
this edge falls within the definition of textual entailment and is correct: For example,
if some HPV strain causes cervical cancer then cervical cancer is associated with HPV.
Because the entailed template is much more general than the entailing template, how-
ever, they are not instantiated by similar arguments in the corpus and distributional
similarity features fail to capture their semantic similarity. Last, we note that in 20.9%
of the false negatives, there was some string overlap between the entailing and entailed
templates, for example in X controls asthma symptoms ? X controls asthma. In the next
section we experiment with a feature that is based on string similarity.
Tables 8 and 9 show that there are cases where ILP-Global makes a mistake, whereas
ILP-Local or Greedy-Global are correct. An illustrating example for such a case is
shown in Figure 7. Looking at ILP-Local we see that the entailment classifier correctly
classifies the edges X triggers asthma ? X causes asthma and X causes asthma ? Associate
X with asthma, but misclassifies X triggers asthma ? Associate X with asthma. Because this
configuration violates a transitivity constraint, ILP-Global must make a global decision
whether to add the edge X triggers asthma ? Associate X with asthma or to omit one of
Figure 7
A scenario where ILP-Global makes a mistake, but ILP-Local is correct.
99
Computational Linguistics Volume 38, Number 1
the correct edges. The optimal global decision in this case causes a mistake with respect
to the gold standard. More generally, a common phenomenon of ILP-Global is that it
splits components that are connected in ILP-Local, for example, in Figures 3 and 4. ILP-
Global splits the components in a way that is optimal according to the scores of the local
entailment classifier, but these are not always accurate according to the gold standard.
Figure 5 exemplifies a scenario where ILP-Global errs, but Greedy-Global is (partly)
correct. ILP-Global mistakenly learns entailment rules from the templates Associate
X with headache and Associate headache with X to the templates X causes headache and
X results in headache, whereas Greedy-Global isolates the templates Associate X with
headache and Associate headache with X in a separate component. This happens because
of the greedy nature of Greedy-Global. Notice that in step A2 the templates X causes
headache and X results in headache are already included (erroneously) in a connected
component with the templates X prevents headache and X reduces headache. Thus, adding
the rules from Associate X with headache and Associate headache with X to X causes headache
and X results in headache would also add the rules to X reduces headache and X prevents
headache and the Greedy-Global avoids that. ILP-Global does not have that problem: It
simply chooses the optimal choice according to the entailment classifier, which splits the
connected component presented in A2. Thus, once again we see that mistakes made by
ILP-Global are often due to the inaccuracies of the scores given by the local entailment
classifier.
6. Local Classifier Extensions
The error analysis in Section 5.4 exemplified that most errors are the result of misclassi-
fications made by the local entailment classifier. In this section, we investigate the local
entailment classifier component, focusing on the set of features used for classification.
We first present an experimental setting in which we consider a wider set of features,
then we present the results of the experiment, and last we perform feature analysis and
draw conclusions.
6.1 Feature Set and Experimental Setting
In previous sections we employed a distant supervision framework: We generated
training examples automatically with WordNet, and represented each example with
distributional similarity features. Distant supervision comes with a price, however?it
prevents us from utilizing all sources of information. For example, looking at the pair of
gold-standard templates X manages asthma and X improves asthma management, one can
exploit the fact that management is a derivation of manage to improve the estimation of
entailment. The automatically generated training set was generated by looking at Word-
Net?s hypernym, synonym, and co-hyponyms relations, however, and hence no such
examples appear in the training set, rendering this type of feature useless. Moreover,
one cannot use WordNet?s hypernym, synonym, and co-hyponym relations as features
because the generated training set is highly biased?all positive training examples are
either hypernyms or synonyms and all negative examples are co-hyponyms.
In this section we would like to examine the utility of various features, while avoid-
ing the biases that occur due to distant supervision. Therefore, we use the 23 manually
annotated gold-standard graphs for both training and testing, in a cross-validation
setting. Although this reduces the size of the training set it allows us to estimate the
utility of various features in a setting where the training set and test set are sampled
from the same underlying distribution, without the aforementioned biases.
100
Berant et al Learning Entailment Relations by Global Graph Structure Optimization
We would like to extract features that express information that is diverse and
orthogonal to the one given by distributional similarity. Therefore, we turn to existing
knowledge resources that were created using both manual and automatic methods,
expressing various types of linguistic and statistical information that is relevant for
entailment prediction:
1. WordNet: contains manually annotated relations such as hypernymy,
synonymy, antonymy, derivation, and entailment.
2. VerbOcean11 (Chklovski and Pantel 2004): contains verb relations such as
stronger-than and similar that were learned with pattern-based methods.
3. CATVAR12 (Habash and Dorr 2003): contains word derivations such as
develop?development.
4. FRED13 (Ben Aharon, Szpektor, and Dagan 2010): contains entailment
rules between templates learned automatically from FrameNet.
5. NomLex14 (Macleod et al 1998): contains English nominalizations
including their argument mapping to the corresponding verbal form.
6. BAP15 (Kotlerman et al 2010): contains directional distributional
similarity scores between lexical terms (rather than propositional
templates) calculated with the BAP similarity scoring function.
Table 12 describes the 16 new features that were generated for each of the gold-
standard examples (resulting in a total of 32 features). The first 15 features were gen-
erated by the aforementioned knowledge bases. The last feature measures the edit
distance between templates: Given a pair of templates (t1, t2), we concatenate the words
in each template and derive a pair of strings (s1, s2). Then we compute the Levenshtein
string edit-distance (Cohen, Ravikumar, and Fienberg 2003) between s1 and s2 and
divide the score by |s1|+ |s2| for normalization.
Table 12 also describes for each feature the number and percentage of examples for
which the feature value is non-zero (out of the examples generated from the 23 gold-
standard graphs). A salient property of many of the new features is that they are sparse:
The four antonymy features as well as the Derivation, Entailment, Nomlex, and FRED
features occur in very few examples in our data set, which might make training with
these features difficult.
After generating the new features we employ a leave-one-graph-out strategy to
maximally exploit the manually annotated gold standard for training. For each of the
test-set graphs, we train over all development and test-set graphs except for the one
that is left out,16 after tuning the algorithm?s parameters and test. Parameter tuning is
done by cross-validation over the development set, tuning to maximize the F1 of the set
11 http://demo.patrickpantel.com/demos/verbocean/.
12 http://clipdemos.umiacs.umd.edu/catvar/.
13 http://u.cs.biu.ac.il/?nlp/downloads/FRED.html.
14 http://nlp.cs.nyu.edu/nomlex/index.html.
15 http://u.cs.biu.ac.il/?nlp/downloads/DIRECT.html.
16 As described in Section 5, we train with a balanced number of positive and negative examples. Because
the number of positive examples in the gold standard is smaller than the number of negative examples,
we use all positives and randomly sample the same number of negatives, resulting in ? 1, 500 training
examples.
101
Computational Linguistics Volume 38, Number 1
Table 12
The set of new features. The last two columns denote the number and percentage of examples
for which the value of the feature is non-zero in examples generated from the 23 gold-standard
graphs.
Name Type Description # %
Hyper. boolean Whether (t1, t2) are identical except for a pair of words
(w1, w2) such that w2 is a hypernym (distance ? 2) of w1
in WordNet.
120 1.1
Syno. boolean Whether (t1, t2) are identical except for a pair of words
(w1, w2) such that w2 is a synonym of w1 in WordNet.
94 0.9
Co-hypo. boolean Whether (t1, t2) are identical except for a pair of words
(w1, w2) such that w2 is a co-hyponym of w1 in WordNet.
302 2.8
WN Ant. boolean Whether (t1, t2) are identical except for a pair of words
(w1, w2) such that w2 is an antonym of w1 in WordNet.
6 0.06
VO Ant. boolean Whether (t1, t2) are identical except for a pair of words
(w1, w2) such that w2 is an antonym of w1 in VerbOcean.
25 0.2
WN Ant. 2 boolean Whether there exists in (t1, t2) a pair of words (w1, w2)
such that w2 is an antonym of w1 in WordNet.
22 0.2
VO Ant. 2 boolean Whether there exists in (t1, t2) a pair of words (w1, w2)
such that w2 is an antonym of w1 in VerbOcean.
73 0.7
Derivation boolean Whether there exists in (t1, t2) a pair of words (w1, w2)
such that w2 is a derivation of w1 in WordNet or CATVAR.
78 0.7
Entailment boolean Whether there exists in (t1, t2) a pair of words (w1, w2)
such that w2 is entailed by w1 in WordNet.
20 0.2
FRED boolean Whether t1 entails t2 in FRED. 9 0.08
Nomlex boolean Whether t1 entails t2 in Nomlex. 8 0.07
VO strong boolean Whether (t1, t2) are identical except for a pair of words
(w1, w2) such that w2 is stronger than w1 in VerbOcean.
104 1
VO simil. boolean Whether (t1, t2) are identical except for a pair of words
(w1, w2) such that w2 is similar to w1 in VerbOcean.
191 1.8
Positive boolean Disjunction of the features Hypernym, Synonym, Nom-
lex, and VO stronger.
289 2.7
BAP real maxw1?t1,w2?t2BAP(w1, w2). 506 4.7
Edit real Normalized edit-distance. 100
of learned edges (the development and test set are described in Section 5). Graphs are
always learned with the LP-Global algorithm.
Our main goal is to check whether the added features improve performance, and
therefore we run the experiment both with and without the new features. In addi-
tion, we would like to test whether using different classifiers affects performance.
Therefore, we run the experiments with a linear-kernel SVM, a square-kernel SVM,
a Gaussian-kernel SVM, logistic regression, and naive Bayes. We use the SVMPerf
package (Joachims 2005) to train the SVM classifiers and the Weka package (Hall et al
2009) for logistic regression and naive Bayes.
6.2 Experiment Results
Table 13 describes the macro-average recall, precision, and F1 of all classifiers both with
and without the new features on the development set and test set. Using all features is
denoted by Xall, and using the original features is denoted by Xold.
102
Berant et al Learning Entailment Relations by Global Graph Structure Optimization
Table 13
Macro-average recall, precision, and F1 on the development set and test set using the parameters
that maximize F1 of the learned edges over the development set.
Development set Test set
Algorithm Recall Precision F1 Recall Precision F1
Linearall 48.1 31.9 36.3 51.7 37.7 40.3
Linearold 40.3 33.3 34.8 47.2 42.2 41.1
Gaussianall 41.8 32.4 35.1 48.0 41.1 40.7
Gaussianold 41.1 31.2 33.9 50.3 39.7 40.5
Squareall 39.9 32.0 34.1 43.7 39.8 38.9
Squareold 38.0 31.6 32.9 50.2 41.0 41.3
Logisticall 34.4 27.6 29.1 39.8 41.7 37.8
Logisticold 39.3 31.2 33.5 45.4 40.9 39.9
Bayesall 20.8 33.2 24.5 27.4 46.0 31.7
Bayesold 20.3 34.9 24.6 26.4 45.4 30.9
Examining the results it does not appear that the new features improve perfor-
mance. Whereas on the development set the new features add 1.2?1.5 F1 points for all
SVM classifiers, on the test set using the new features decreases performance for the
linear and square classifiers. This shows that even if there is some slight increase in
performance when using SVM on the development set, it is masked by the variance
added in the process of parameter tuning. In general, including the new features does
not yield substantial differences in performance.
Secondly, the SVM classifiers perform better than the logistic and naive Bayes clas-
sifiers. Using the more complex square and Gaussian kernels does not seem justified,
however, as the differences between the various kernels are negligible. Therefore, in our
analysis we will use a linear kernel SVM classifier.
Last, we note that although we use supervised learning rather than distant super-
vision, the results we get are slightly lower than those presented in Section 5. This is
probably due to the fact that our manually annotated data set is rather small. Nev-
ertheless, this shows that the quality of the distant supervision training set generated
automatically from WordNet is reasonable.
Next, we perform analysis of the different features of the classifier to better under-
stand the reasons for the negative result obtained.
6.3 Feature Analysis
We saw that the new features slightly improved performance for SVM classifiers on
the development set, although no clear improvement was witnessed on the test set.
To further check whether the new features carry useful information we measured the
training set accuracy for each of the 12 training sets (leaving out each time one test-
set graph). Using the new features improved the average training set accuracy from
71.6 to 72.3. More importantly, it improved performance consistently in all 12 training
sets by 0.4?1.2 points. This strengthens our belief that the new features do carry a
certain amount of information, but this information is too sparse to affect the overall
103
Computational Linguistics Volume 38, Number 1
performance of the algorithm. In addition, notice that the absolute accuracy on the
training set is low?72.3. This shows that separating entailment from non-entailment
using the current set of features is challenging.
Next, we would like to perform analysis on each of the features. First, we perform
an ablation test over the features by omitting each one of them and re-training the
classifier Linearall. In Table 14, the columns ablation F1 and ? show the F1 obtained and
the difference in performance from the Linearall classifier, which scored 40.3 F1 points.
Results show that there is no ?bad? feature that deteriorates performance. For almost all
features ablation causes a decrease in performance, although this decrease is relatively
small. There are only four features for which ablation decreases performance by more
than one point: three distributional similarity features, but also the new hypernym
feature.
The next three columns in the table describe the precision and recall of the new
boolean features. The column Feature type indicates whether we expect a feature to
indicate entailment or non-entailment and the columns Prec. and Recall specify the
Table 14
Results of feature analysis. The second column denotes the proportion of manually annotated
examples for which the feature value is non-zero. A detailed explanation of the other columns is
provided in the body of the article.
Feature name % Ablation F1 ? Feature type Prec. Recall Classification F1
h-b-B-pCt 8.2 39.3 ?1 14.9
h-b-B-pC 6.9 39.5 ?0.8 33.2
h-b-B-Ct 1.6 40.3 0 15.4
h-b-B-C 1.6 40.5 0.2 11.2
h-b-L-pCt 23.6 38.3 ?2.0 37.0
h-b-L-pC 21.4 39.4 ?0.9 35.2
h-b-L-Ct 9.7 40.1 ?0.2 27.3
h-b-L-C 8.1 39.7 ?0.6 14.1
h-u-B-Ct 1.0 39.4 ?0.9 10.9
h-u-B-C 1.1 39.8 ?0.5 12.6
h-u-L-Ct 6.1 39.8 ?0.5 18.5
h-u-L-C 6.3 39.2 ?1.1 19.3
R-b-L-l 22.5 40.1 ?0.2 26.7
R-u-L-l 8.3 39.4 ?0.9 23.2
R-u-B-l 1.9 39.8 ?0.5 16.7
Lin & Pantel 8.8 38.7 ?1.6 23.0
Hyper. 1.1 38.7 ?1.6 + 37.1 4.9 9.7
Syno. 0.9 40.3 0 + 43.1 4.5 15.8
Co-hypo. 2.8 40.1 ?0.2 ? 82.0 2.5 17.9
WN ant. 0.06 39.8 ?0.5 ? 75.0 0.05 1.2
VO ant. 0.2 40.1 ?0.2 ? 96.0 0.2 2.2
WN ant. 2. 0.2 39.4 ?0.9 ? 59.1 0.1 2.7
VO ant. 2 0.7 40.2 ?0.1 ? 98.6 0.7 2.2
Derivation 0.7 39.5 ?0.8 + 47.4 4.1 10.2
Entailment 0.2 39.7 ?0.6 + 15.0 0.3 1.2
FRED 0.08 39.7 ?0.6 + 77.8 0.8 3.2
NomLex 0.07 39.8 ?0.5 + 75.0 0.7 3.3
VO strong. 1 39.4 ?0.9 + 34.6 4 6.9
VO simil. 1.8 39.4 ?0.9 + 28.8 6.1 12.5
Positive 2.7 39.8 ?0.5 + 36.7 11.8
BAP 4.7 40.1 ?0.2 13.3
Edit 100 39.9 ?0.4 15.5
104
Berant et al Learning Entailment Relations by Global Graph Structure Optimization
precision and recall of that feature. For example, the feature FRED is a positive feature
that we expect to support entailment, and indeed 77.8% of the gold-standard examples
for which it is turned on are positive examples. It is turned on only in 0.8% of the
positive examples, however. Similarly, VO ant. is a negative feature that we expect to
support non-entailment, and indeed 96% of the gold-standard examples for which it is
on are negative examples, but it is turned on in only 0.2% of the negative examples.
The precision results are quite reasonable: For most positive features the precision is
well over the proportion of positive examples in the gold standard, which is about
10% (except for the Entailment feature whose precision is only 15%). For the negative
features it seems that the precision of VerbOcean features is very high (though they are
sparse), and the precision of WordNet antonyms and co-hyponyms is lower. Looking
at the recall we can see that the coverage of the boolean features is low.
The last column in the table describes results of training the classifier with a single
feature. For each feature we train a linear kernel SVM, tune the sparsity parameter on
the development set, and measure F1 over the test set. Naturally, classifiers that are
trained on sparse features yield low performance.
This column allows us once again (cf. Table 8) to examine the original distributional
similarity features. There are three distributional similarity features that achieve F1 of
more than 30 points, and all three represent features using pairs of argument instan-
tiations rather than treat each argument separately, as we have already witnessed in
Section 5.
Note also that the feature h-b-L-pCt, which uses binary templates, the Lin similarity
measure, and features that are pairs of CUI tuples, is the best feature both in terms of the
ablation test and when it is used as a single feature for the classifier. The result obtained
by this feature is only 3.3 points lower than that obtained when using the entire feature
set. We believe this is for two reasons: First, the 16 distributional similarity features are
correlated with one another and thus using all of them does not boost performance
substantially. For example, the Pearson correlation coefficients between the features
h-b-B-pCt, h-b-B-Ct, h-b-L-pCt, h-b-L-Ct, h-u-B-Ct, and h-u-L-Ct (all utilize CUI tuples) and
h-b-B-pC, h-b-B-C, h-b-L-pC, h-b-L-C, h-u-B-C, and h-u-L-C (all use CUIs), respectively, are
over 0.9. The second reason for gaining only 3.3 points by the remaining features is that,
as discussed, the new set of features is relatively sparse.
To sum up, we suggest several hypotheses that explain our results and analysis:
 The new features are too sparse to substantially improve the performance
of the local entailment classifier in our data set. This perhaps can be
attributed to the nature of our domain-specific health-care corpus. In the
future, we would like to examine the sparsity of these features in a general
domain.
 Looking at the training set accuracy, ablations, and precision of the new
features, it seems that the behavior of most of them is reasonable. Thus,
it is possible that in a different learning scheme that does not use the
resources as features the information they provide may become beneficial.
For example, in a simple ?back-off? approach one can use rules from
precise resources to determine entailment, and apply a classifier only
when no precise resource contains a relevant rule.
 In our corpus representing distributional similarity features with
pairs of argument instantiations is better than treating each argument
independently.
105
Computational Linguistics Volume 38, Number 1
 Given the current training set accuracy and the sparsity of the new
features, it is important to develop methods that gather large-scale
information that is orthogonal to distributional similarity. In our opinion,
the most promising direction for acquiring such rich information is by
methods that look at co-occurrence of predicates or templates on the Web
(Chklovski and Pantel 2004; Pekar 2008).
7. Conclusions and Future Work
This article presented a global optimization algorithm for learning entailment rules
between predicates, represented as propositional templates. Most previous work on
learning entailment rules between predicates focused on local learning methods, which
consider each pair of predicates in isolation. To the best of our knowledge, this is the
most comprehensive attempt to date to exploit global interactions between predicates
for improving the set of learned entailment rules.
We modeled the problem as a graph learning problem, and searched for the best
graph under a global transitivity constraint. Two objective functions were defined for
the optimization procedure, one score-based and the other probabilistic, and we have
shown that under certain conditions (specified in Appendix A) the score-based function
can be interpreted probabilistically. This allowed us to use both margin as well as
probabilistic classifiers for the underlying entailment classifier. We solved the optimiza-
tion problem using Integer Linear Programming, which provides an optimal solution
(compared to the greedy algorithm suggested by Snow, Jurafsky, and Ng [2006]), and
demonstrated empirically that this method outperforms local algorithms as well as
a state-of-the-art greedy optimization algorithm on the graph learning task. We also
analyzed quantitatively and qualitatively the reasons for the improved performance of
our global algorithm and performed detailed error analysis. Last, we experimented with
various entailment classifiers that utilize different sets of features from many knowledge
bases.
The experiments and analysis performed indicate that the current performance of
the local entailment classifier needs to be improved. We believe that the most promising
direction for improving the local classifier is to use methods that look for co-occurrence
of predicates in sentences or documents on the Web, because these methods excel at
identifying specific semantic relations. It is also possible to use other sources of infor-
mation such as lexicographic resources, although this probably will require a learning
scheme that is robust to the relatively low coverage of these resources. Increasing the
size of the training corpus is also an important direction for improving the entailment
classifier.
Another important direction for future work is to apply our algorithm to graphs
that are larger by a few orders of magnitude than the focused entailment graphs dealt
with in this article. This will introduce a challenge to our current optimization algorithm
due to complexity issues, as our ILP contains O(|V|3) constraints. In addition, this will
require careful handling of predicate ambiguity, which interferes with the transitivity
of entailment and will become a pertinent issue in large graphs. Some first steps in this
direction have already been carried out (Berant, Dagan, and Goldberger 2011).
In addition, our graphs currently contain a single type of edge, namely, the entail-
ment relation. We would like to model more types of edges in the graph, representing
additional semantic relations such as co-hyponymy, and to explicitly describe the inter-
actions between the various types of edges, aiming to further improve the quality of the
learned entailment rules.
106
Berant et al Learning Entailment Relations by Global Graph Structure Optimization
Figure 8
A hierarchical summary of propositions involving nausea as an argument, such as headache is
related to nausea, acupuncture helps with nausea, and Lorazepam treats nausea.
Last, in Section 3.1 we mentioned that by merging strongly connected components
in entailment graphs, hierarchies of predicates can be generated (recall Figure 1). As
proposed by Berant, Dagan, and Goldberger (2010), we believe that these hierarchies can
be useful not only in the context of semantic inference applications, but also in the field
of faceted search and hierarchical text exploration (Stoica, Hearst, and Richardson 2007).
Figure 8 exemplifies how a set of propositions can be presented to a user according to
the hierarchy of predicates shown in Figure 1. In the field of faceted search, information
is presented using a number of hierarchies, corresponding to different facets or dimen-
sions of the data. One can easily use the hierarchy of predicates learned by our algorithm
as an additional facet in the context of a text-exploration application. In future work,
we intend to implement this application and perform user experiments to test whether
adding this hierarchy facilitates exploration of textual information.
Appendix A: Derivation of the Probabilistic Objective Function
In this section we provide a full derivation for the probabilistic objective function
given in Section 4.2.2. Given two nodes u and v from a set of nodes V, we denote by
Iuv = 1 the event that u entails v, by Fuv the feature vector representing the ordered
pair (u, v), and by F the set of feature vectors over all ordered pairs of nodes, that is,
F = ?u=vFuv. We wish to learn a set of edges E, such that the posterior probability P(G|F)
is maximized, where G = (V, E). We assume that we have a ?local? model estimating the
edge posterior probability Puv = P(Iuv = 1|Fuv). Because this model was trained over a
balanced training set, the prior for the event that u entails v under the model is uniform:
P(Iuv = 1) = P(Iuv = 0) =
1
2 . Using Bayes?s rule we get:
P(Iuv = 1|Fuv) =
P(Iuv = 1)
P(Fuv)
? P(Fuv|Iuv = 1) = a ? P(Fuv|Iuv = 1) (A.1)
P(Iuv = 0|Fuv) =
P(Iuv = 0)
P(Fuv)
? P(Fuv|Iuv = 0) = a ? P(Fuv|Iuv = 0) (A.2)
107
Computational Linguistics Volume 38, Number 1
where a = 12?P(Fuv ) is a constant with respect to any graph. Thus, we conclude that
P(Iuv|Fuv) = a ? P(Fuv|Iuv). Next, we make three independence assumptions (the first two
are following Snow, Jurafsky, and Ng [2006]):
P(F|G) =
?
u=v
P(Fuv|G) (A.3)
P(Fuv|G) = P(Fuv|Iuv) (A.4)
P(G) =
?
u=v
P(Iuv) (A.5)
Assumption A.3 states that each feature vector is independent from other feature
vectors given the graph. Assumption A.4 states that the features Fuv for the pair (u, v)
are generated by a distribution depending only on whether entailment holds for (u, v).
Last, Assumption A.5 states that edges are independent and the prior probability of a
graph is a product of the prior probabilities of the edges. Using these assumptions and
equations A.1 and A.2, we can now express the posterior P(G|F):
P(G|F) ? P(G) ? P(F|G) (A.6)
=
?
u=v
[P(Iuv) ? P(Fuv|Iuv)] (A.7)
=
?
u=v
P(Iuv) ?
P(Iuv|Fuv)
a (A.8)
?
?
u=v
P(Iuv) ? Puv (A.9)
=
?
(u,v)?E
P(Iuv = 1) ? Puv ?
?
(u,v)/?E
P(Iuv = 0) ? (1 ? Puv) (A.10)
Note that under the ?local model? the prior for an edge in the graph was uniform,
because the model was trained over a balanced training set. Generally, however, this is
not the case, and thus we introduce an edge prior into the model when formulating the
global objective function. Now, we can formulate P(G|F) as a linear function:
G? = argmax
G
?
(u,v)?E
P(Iuv = 1) ? Puv ?
?
(u,v)/?E
P(Iuv = 0) ? (1 ? Puv) (A.11)
= argmax
G
?
(u,v)?E
log(Puv ? P(Iuv = 1)) +
?
(u,v)/?E
log[(1 ? Puv) ? P(Iuv = 0)] (A.12)
= argmax
G
?
u=v
(
Iuv ? log(Puv ? P(Iuv = 1)) + (1 ? Iuv) ? log[(1 ? Puv) ? P(Iuv = 0)]
)
(A.13)
= argmax
G
?
u=v
(
log
Puv ? P(Iuv = 1)
(1 ? Puv) ? P(Iuv = 0)
? Iuv + (1 ? Puv) ? P(Iuv = 0)
)
(A.14)
108
Berant et al Learning Entailment Relations by Global Graph Structure Optimization
= argmax
G
?
u=v
log
Puv
(1 ? Puv)
? Iuv + log? ? |E| (A.15)
In the last transition we omit
?
u=v(1 ? Puv) ? P(Iuv = 0), which is a constant with
respect to the graph and denote the prior odds ratio by ? = P(Iuv=1)P(Iuv=0) . This leads to the
final formulation described in Section 4.2.2.
Acknowledgments
We would like to thank Roy Bar-Haim, David
Carmel, and the anonymous reviewers for
their useful comments. We also thank Dafna
Berant and the nine students who prepared
the gold-standard data set. This work was
developed under the collaboration of
FBK-irst/University of Haifa and was
partially supported by the Israel Science
Foundation grant 1112/08. The first author is
grateful to the Azrieli Foundation for the
award of an Azrieli Fellowship, and has
carried out this research in partial fulfilment
of the requirements for the Ph.D. degree.
References
Althaus, Ernst, Nikiforos Karamanis, and
Alexander Koller. 2004. Computing locally
coherent discourses. In Proceedings of the
ACL, pages 399?406, Barcelona.
Baker, Collin F., Charles J. Fillmore, and
John B. Lowe. 1998. The Berkeley framenet
project. In Proceedings of COLING-ACL,
pages 86?90, Montreal.
Bar-Haim, Roy, Ido Dagan, Iddo Greental,
and Eyal Shnarch. 2007. Semantic inference
at the lexical-syntactic level. In Proceedings
of AAAI, pages 871?876, Vancouver.
Ben Aharon, Roni, Idan Szpektor, and Ido
Dagan. 2010. Generating entailment rules
from framenet. In Proceedings of ACL,
pages 241?246, Uppsala.
Bentivogli, Luisa, Ido Dagan, Hoa Trang
Dang, Danilo Giampiccolo, and Bernarde
Magnini. 2009. The fifth Pascal recognizing
textual entailment challenge. In Proceedings
of TAC-09, pages 14?24, Gaithersburg, MD.
Berant, Jonathan, Ido Dagan, and Jacob
Goldberger. 2010. Global learning of
focused entailment graphs. In Proceedings
of ACL, pages 1220?1229, Uppsala.
Berant, Jonathan, Ido Dagan, and Jacob
Goldberger. 2011. Global learning of typed
entailment rules. In Proceedings of ACL,
pages 610?619, Portland, OR.
Bhagat, Rahul, Patrick Pantel, and Eduard
Hovy. 2007. LEDIR: An unsupervised
algorithm for learning directionality of
inference rules. In Proceedings of
EMNLP-CoNLL, pages 161?170, Prague.
Budanitsky, Alexander and Graeme Hirst.
2006. Evaluating Wordnet-based measures
of lexical semantic relatedness.
Computational Linguistics, 32(1):13?47.
Chklovski, Timothy and Patrick Pantel.
2004. VerbOcean: Mining the Web for
fine-grained semantic verb relations.
In Proceedings of EMNLP, pages 33?40,
Barcelona.
Clark, Peter, William Murray, John
Thompson, Phil Harrison, Jerry Hobbs,
and Christiane Fellbaum. 2007. On the role
of lexical and world knowledge in RTE3.
In Proceedings of the Workshop on Textual
Entailment and Paraphrasing, pages 54?59,
Prague.
Clarke, James and Mirella Lapata. 2008.
Global inference for sentence compression:
An integer linear programming approach.
Journal of Artificial Intelligence Research,
31:273?381.
Cohen, William, Pradeep Ravikumar,
and Stephen E. Fienberg. 2003. A
comparison of string distance metrics for
name-matching tasks. In Proceedings of
IIWeb, pages 73?78, Acapulco.
Connor, Michael and Dan Roth. 2007.
Context sensitive paraphrasing with
a single unsupervised classifier.
In Proceedings of ECML, pages 104?115,
Warsaw.
Coyne, Bob and Owen Rambow. 2009.
Lexpar: A freely available English
paraphrase lexicon automatically extracted
from Framenet. In Proceedings of the IEEE
International Conference on Semantic
Computing, pages 53?58, Berkeley, CA.
Dagan, Ido, Bill Dolan, Bernardo Magnini,
and Dan Roth. 2009. Recognizing textual
entailment: Rational, evaluation and
approaches. Natural Language Engineering,
15(4):1?17.
Do, Quang and Dan Roth. 2010. Constraints
based taxonomic relation classification. In
Proceedings of EMNLP, pages 1099?1109,
Cambridge, MA.
Fellbaum, Christiane. 1998a. A semantic
network of English: The mother of all
109
Computational Linguistics Volume 38, Number 1
wordNets. Natural Language Engineering,
32:209?220.
Fellbaum, Christiane, editor. 1998b. WordNet:
An Electronic Lexical Database (Language,
Speech, and Communication). The MIT Press,
Cambridge, MA.
Finkel, Jenny R. and Christopher D.
Manning. 2008. Enforcing transitivity in
coreference resolution. In Proceedings of
ACL-08: HLT, Short Papers, pages 45?48,
Columbus, OH.
Habash, Nizar and Bonnie Dorr. 2003.
A categorial variation database for
English. In Proceedings of the NAACL,
pages 17?23, Edmonton.
Hall, Mark, Eibe Frank, Geoffrey Holmes,
Bernhard Pfahringer, Peter Reutemann,
and Ian H. Witten. 2009. The WEKA data
mining software: An update. SIGKDD
Explorations, 11(1):10?18.
Harmeling, Stefan. 2009. Inferring textual
entailment with a probabilistically sound
calculus. Natural Language Engineering,
15(4):459?477.
Harris, Zellig. 1954. Distributional structure.
Word, 10(23):146?162.
Joachims, Thorsten. 2005. A support vector
method for multivariate performance
measures. In Proceedings of ICML,
pages 377?384, Bonn.
Kingsbury, Paul, Martha Palmer, and
Mitch Marcus. 2002. Adding semantic
annotation to the Penn TreeBank.
In Proceedings of HLT, pages 252?256,
San Diego, CA.
Kipper, Karin, Hoa T. Dang, and Martha
Palmer. 2000. Class-based construction
of a verb lexicon. In Proceedings of AAAI,
pages 691?696, Austin, TX.
Kotlerman, Lili, Ido Dagan, Idan Szpektor,
and Maayan Zhitomirsky-Geffet. 2010.
Directional distributional similarity for
lexical inference. Natural Language
Engineering, 16:359?389.
Lin, Dekang. 1998a. Automatic retrieval
and clustering of similar words.
In Proceedings of COLING-ACL,
pages 768?774, Montreal.
Lin, Dekang. 1998b. Dependency-based
evaluation of Minipar. In Proceedings of the
Workshop on Evaluation of Parsing Systems at
LREC, pages 317?329, Granada.
Lin, Dekang and Patrick Pantel. 2001.
Discovery of inference rules for question
answering. Natural Language Engineering,
7(4):343?360.
Macleod, Catherine, Ralph Grishman,
Adam Meyers, Leslie Barrett, and
Ruth Reeves. 1998. Nomlex: A lexicon of
nominalizations. In Proceedings of Euralex,
pages 187?193, Lieg`e.
Martins, Andre, Noah Smith, and Eric Xing.
2009. Concise integer linear programming
formulations for dependency parsing.
In Proceedings of ACL, pages 342?350,
Singapore.
Meyers, Adam, Ruth Reeves, Catherine
Macleod, Rachel Szekeley, Veronika
Zielinska, and Brian Young. 2004.
The cross-breeding of dictionaries. In
Proceedings of LREC, pages 1095?1098,
Lisbon.
Mirkin, Shachar, Ido Dagan, and Maayan
Gefet. 2006. Integrating pattern-based
and distributional similarity methods
for lexical entailment acquisition.
In Proceedings of COLING-ACL,
pages 579?586, Sydney.
Nikulin, Vladimir. 2008. Classification of
imbalanced data with random sets and
mean-variance filtering. International
Journal of Data Warehousing and Mining,
4(2):63?78.
Pekar, Viktor. 2008. Discovery of event
entailment knowledge from text corpora.
Computer Speech & Language, 22(1):1?16.
Raina, Rajat, Andrew Ng, and Christopher
Manning. 2005. Robust textual inference
via learning and abductive reasoning.
In Proceedings of AAAI, pages 1099?1105,
Pittsburgh, PA.
Riedel, Sebastian and James Clarke. 2006.
Incremental integer linear programming
for non-projective dependency parsing.
In Proceedings of EMNLP, pages 129?137,
Sydney.
Roth, Dan and Wen-tau Yih. 2004. A linear
programming formulation for global
inference in natural language tasks.
In Proceedings of CoNLL, pages 1?8,
Boston, MA.
Schoenmackers, Stefan, Jesse Davis,
Oren Etzioni, and Daniel S. Weld.
2010. Learning first-order horn
clauses from Web text. In Proceedings
of EMNLP, pages 1088?1098,
Cambridge, MA.
Sekine, Satoshi. 2005. Automatic paraphrase
discovery based on context and keywords
between NE pairs. In Proceedings of IWP,
pages 80?87, Jeju Island.
Siegel, Sidney and N. John Castellan. 1988.
Non-parametric Statistics for the Behavioral
Sciences. McGraw-Hill, New-York.
Smith, Noah and Jason Eisner. 2005.
Contrastive estimation: Training log-linear
models on unlabeled data. In Proceedings
of ACL, pages 354?362, Ann Arbor, MI.
110
Berant et al Learning Entailment Relations by Global Graph Structure Optimization
Snow, Rion, Daniel Jurafsky, and Andrew Y.
Ng. 2004. Learning syntactic patterns for
automatic hypernym discovery. In
Proceedings of NIPS, pages 1297?1304,
Vancouver.
Snow, Rion, Daniel Jurafsky, and Andrew Y.
Ng. 2006. Semantic taxonomy induction
from heterogenous evidence. In
Proceedings of ACL, pages 801?808, Prague.
Stoica, Emilia, Marti Hearst, and Megan
Richardson. 2007. Automating creation of
hierarchical faceted metadata structures. In
Proceedings of NAACL-HLT, pages 244?251,
Rochester, NY.
Szpektor, Idan and Ido Dagan. 2007.
Learning canonical forms of entailment
rules. In Proceedings of RANLP, pages 1?8,
Borovetz.
Szpektor, Idan and Ido Dagan. 2008.
Learning entailment rules for unary
templates. In Proceedings of COLING,
pages 849?856, Manchester.
Szpektor, Idan and Ido Dagan. 2009.
Augmenting Wordnet-based inference
with argument mapping. In Proceedings
of TextInfer, pages 27?35, Singapore.
Szpektor, Idan, Hristo Tanev, Ido Dagan,
and Bonaventura Coppola. 2004. Scaling
Web-based acquisition of entailment
relations. In Proceedings of EMNLP,
pages 41?48, Barcelona.
Van Hulse, Jason, Taghi Khoshgoftaar, and
Amri Napolitano. 2007. Experimental
perspectives on learning from imbalanced
data. In Proceedings of ICML,
pages 935?942, Corvallis, OR.
Vanderbei, Robert. 2008. Linear Programming:
Foundations and Extensions. Springer,
New-York.
Weeds, Julie and David Weir. 2003. A general
framework for distributional similarity.
In Proceedings of EMNLP, pages 81?88,
Sapporo.
Wilcoxon, Frank. 1945. Individual
comparisons by ranking methods.
Biometrics Bulletin, 1:80?83.
Yannakakis, Mihalis. 1978. Node-and
edge-deletion NP-complete problems. In
STOC ?78: Proceedings of the Tenth Annual
ACM Symposium on Theory of Computing,
pages 253?264, New York, NY.
Yates, Alexander and Oren Etzioni. 2009.
Unsupervised methods for determining
object and relation synonyms on the web.
Journal of Artificial Intelligence Research,
34:255?296.
111
Proceedings of NAACL-HLT 2013, pages 752?757,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
TruthTeller: Annotating Predicate Truth
Amnon Lotan
Department of Linguistics
Tel Aviv University
amnonlot@post.tau.ac.il
Asher Stern and Ido Dagan
Department of Computer Science
Bar Ilan University
astern7@gmail.com dagan@cs.biu.ac.il
Abstract
We propose a novel semantic anno-
tation type of assigning truth values
to predicate occurrences, and present
TruthTeller, a standalone publicly-
available tool that produces such annota-
tions. TruthTeller integrates a range
of semantic phenomena, such as nega-
tion, modality, presupposition, implicativ-
ity, and more, which were dealt only partly
in previous works. Empirical evaluations
against human annotations show satisfac-
tory results and suggest the usefulness of
this new type of tool for NLP.
1 Introduction
In a text, the action or relation denoted by ev-
ery predicate can be seen as being either pos-
itively or negatively inferred from its sentence,
or otherwise having an unknown truth status.
Only in (3) below can we infer that Gal sold
her shop, hence the positive truth value of the
predicate sell, while according to (2) and (4) Gal
did not sell it, hence the negative truth values,
and in (1) we do not know if she sold it or not
(the notations pt+, pt- and pt? denote truth
states, defined in Subsection 2.3). Identifying
these predicate truth values is an important sub-
task within many semantic processing scenarios,
including various applications such as Question
Answering (QA), Information Extraction (IE),
paraphrasing and summarization. The follow-
ing examples illustrate the phenomenon:
(1) Gal made an attempt pt+ to sell pt? her
shop.
(2) Gal did not try pt? to sell pt? her shop af-
ter hearing pt+ the offers.
(3) Maybe Gal wasn?t smart pt? to sell pt+ her
shop.
(4) Gal wasn?t smart pt? enough to sell pt? the
shop that she had bought pt+.
Previous works addressed specific aspects of
the truth detection problem: Nairn et al
(2006), and later MacCartney & Manning (2007;
2009), were the first to build paraphrasing and
inference systems that combine negation (see try
in (2)), modality (smart in (3)) and ?natural
logic?, a recursive truth value calculus (sell in
(1-3)); recently, Mausam et al (2012) built an
open IE system that identifies granulated vari-
ants of modality and conditions on predicates
(smart in (3)); and Kiparsky & Kiparsky (1970)
and Karttunen (1971; 2012) laid the ground
work for factive and implicative entailment cal-
culus (sell in (1-4)), as well as many generic
constructions of presupposition (hearing in (2)
is presupposed because it heads an adverbial
clause and bought in (4) heads a finite relative
clause), which, to our knowledge, have not yet
been implemented computationally. Notice in
the examples that presuppositions persist under
negation, in questions and if-clauses, while en-
tailments do not. In addition, there is a growing
research line of negation and modality detection.
See, for example, Morante & Daelemans (2012).
752
We present TruthTeller1, a novel algo-
rithm and system that identifies the truth value
of each predicate in a given sentence. It anno-
tates nodes in the text?s dependency parse-tree
via a combination of pattern-based annotation
rules and a recursive algorithm based on natu-
ral logic. In the course of computing truth value,
it also computes the implicativity/factivity sig-
nature of predicates, and their negation and
modality to a basic degree, both of which are
made available in the system output. It ad-
dresses and combines the aforementioned phe-
nomena (see Section 2), many of which weren?t
dealt in previous systems.
TruthTeller is an open source and pub-
licly available annotation tool, offers a relatively
simple algebra for truth value computation, and
is accompanied by a publicly available lexicon
of over 1,700 implicative and factive predicates.
Also, we provide an intuitive GUI for viewing
and modifying the algorithm?s annotation rules.
2 Annotation Types and Algorithm
This section summarizes the annotation algo-
rithm (a detailed report is available with the sys-
tem release). We perform the annotations over
dependency parse trees, generated according to
the Stanford Dependencies standard (de Marn-
effe and Manning, 2008). For all verbs, nouns
and adjectives in a sentence?s parse tree, we pro-
duce the following 4 annotation types, given in
the order they are calculated, as described in the
following subsections:
1. Predicate Implication Signature (sig) - de-
scribes the pattern by which the predi-
cate entails or presupposes its complements,
e.g., the verb refuse entails the negative of
its complements: Ed refused to pay entails
that Ed didn?t pay.
2. Negation and Uncertainty (NU) - indicates
whether the predicate is modified by an un-
certainty modifier like might, probably, etc.,
or whether it?s negated by no, never etc.
3. Clause-Truth (CT) - indicates whether the
1http://cs.biu.ac.il/~nlp/downloads/TruthTeller
entire clause headed by the predicate is en-
tailed by the complete sentence
4. Predicate Truth (PT) - indicates whether
the predicate itself is entailed by the sen-
tence, as defined below
Before presenting the detailed definitions and
descriptions below, we give a high-level descrip-
tion of TruthTeller?s algorithm, where each
step relies on the results of its predecessor: a)
every predicate in the parse tree is annotated
with a predicate implication signature, identi-
fied by lexicon lookup; b) NU annotations are
added, according to the presence of uncertainty
modifiers (maybe, might, etc.) and negation
modifies (not, never, etc.); c) predicates in cer-
tain presupposition constructions (e.g., adver-
bial clauses, WH arguments) are annotated with
positive CT values; d) the parse tree is depth-
first scanned, in order to compute both CT and
PT annotations by the recursive effects of fac-
tives and implicatives; e) in conjunction with
the previous step, relative clause constructions
are identified and annotated with CT and PT.
Except for steps a) and d), all of the pro-
cedure is implemented as an ordered sequence
of annotation rule applications. An annotation
rule is a dependency parse tree template, pos-
sibly including variables, which assigns certain
annotations to any parse tree node that matches
against it. Step a) is implemented with signa-
ture lexicon lookups, and step d) is an algorithm
implemented in code.
To illustrate this entire process, Figure 1
presents the annotation process of a sim-
ple sentence, step by step, resulting in
TruthTeller?s complete output, fully speci-
fied below. Most other examples in this paper
show only partial annotations for brevity.
2.1 Predicate Implication Signature
Our system marks the signature of each predi-
cate, as defined in Table 1. There, each signa-
ture has a left sign and a right sign. The left sign
determines the clause truth value of the pred-
icate?s complements, when the predicate is in
positive contexts (e.g., not negated), while the
right sign applies in negative contexts (clause
753
# Sig Positive context example Negative context example
1 +/- Ed managed to escape ? Ed escaped Ed didn?t manage to escape ? Ed didn?t escape
2 +/? Ed was forced to sell ? Ed sold Ed wasn?t forced to sell ? no entailments
3 ?/- Ed was allowed to go ? no entailments Ed wasn?t allowed to go ? Ed didn?t go
4 -/+ Ed forgot to pay ? Ed didn?t pay Ed didn?t forget to pay ? Ed paid
5 -/? Ed refused to fight ? Ed didn?t fight Ed didn?t refuse to fight ? no entailments
6 ?/+ Ed hesitated to ask ? no entailments Ed didn?t hesitate to ask ? Ed asked
7 +/+ Ed was glad to come ? Ed came Ed wasn?t glad to come ? Ed came
8 -/- Ed pretended to pay ? Ed didn?t pay Ed didn?t pretend to pay ? Ed didn?t pay
9 ?/? Ed wanted to fly ? no entailments Ed didn?t want to fly ? no entailments
Table 1: Implication signatures, based on MacCartney & Manning (2009) and Karttunen (2012). The first
six signatures are named implicatives, and the last three factive, counter factive and regular, respectively.
a) Annotate signatures via lexicons lookup
Gal wasn?t allowed?/? to come?/?
b) Annotate NU
Gal wasn?t allowed?/?,nu? to come?/?,nu+
c) Annotate CT to presupposition constructions
Gal wasn?t allowed?/?,nu?,ct+ to come?/?,nu+,ct+
d) Recursive CT and PT annotation
Gal wasn?t allowed?/?,nu?,ct+,pt? to
come?/?,nu+,ct?,pt?
e) Annotate CT and PT of relative clauses
(has no effect on this example)
Gal wasn?t allowed?/?,nu?,ct+,pt? to
come?/?,nu+,ct?,pt?
Figure 1: An illustration of the annotation process
truth is defined in Subsection 2.3). See exam-
ples for both context types in the table. Each
sign can be either + (positive), - (negative) or
? (unknown). The unknown sign signifies that
the predicate does not entail its complements in
any way.
Signatures are identified via lookup, using two
lexicons, one for single-word predicates and the
other for verb+noun phrasal verbs, e.g., take the
time to X. Our single-word lexicon is similar to
those used in (Nairn et al, 2006) and (Bar-Haim
et al, 2007), but is far greater, holding over
1,700 entries, while each of the previous two has,
to the best of our knowledge, less than 300 en-
tries. It was built semi automatically, out of
a kernel of 320 manually inspected predicates,
which was then expanded with WordNet syn-
onyms (Fellbaum, 1998). The second lexicon
is the implicative phrasal verb lexicon of Kart-
tunen (2012), adapted into our framework. The
+/? implicative serves as the default signature
for all unlisted predicates.
Signature is also sensitive to the type of the
complement. Consider:
(6) Ed forgot?/+ to call pt? Joe
(7) Ed forgot+/+ that he called pt+ Joe
Therefore, signatures are specified separately for
finite and non finite complements of each pred-
icate.
After the initial signature lookup, two anno-
tation rules correct the signatures of +/+ fac-
tives modified by enough and too, into +/- and
-/+, correspondingly, see Kiparsky & Kiparsky
(1970). Compare:
(8) Ed was mad+/+ to go ? Ed went
(9) Ed was too mad?/+ to go ? Ed didn?t go
In addition, we observed, like Karttunen (2012),
that most verbs that have passive voice and the
into preposition become +/? implicatives, e.g.,
(10) Workers were pushed / maddened /
managed+/? into signing ? They signed
(11) Workers weren?t pushed / maddened /
managed+/? into signing? It is unknown
whether they signed
so we captured this construction in another rule.
754
2.2 Negation and Uncertainty (NU)
NU takes the values {nu+, nu-, nu?}, stand-
ing for non-negated certain actions, negated cer-
tain actions, and uncertain actions. The first
NU rules match against a closed set of negation
modifiers around the predicate, like not, never,
neither etc. (see (2)), while later rules detect
uncertainty modifiers, like maybe, probably, etc.
Therefore, nu? takes precedence over nu-.
Many constructions of subject-negation,
object-negation and ?double negation? are
accounted for in our rules, as in:
(12) Nobody was seennu? at the site
(13) Almost nobody was seennu+ at the site
2.3 Clause Truth and Predicate Truth
Clause Truth (CT, denoted as ct(p)) corre-
sponds to polarity of Nairn et al (2006). It
represents whether the clause headed by a pred-
icate p is entailed by the sentence, contradicted
or unknown, and thus takes three values {ct+,
ct-, ct?}.
Predicate Truth (PT) (denoted as pt(p)) rep-
resents whether we can infer from the sentence
that the action described by the predicate hap-
pened (or that its relation holds). It is defined
as the binary product of NU and CT:
Definition 1. PT = NU ? CT
and takes analogous values: {pt+, pt-, pt?}.
Intuitively, the product of two identical posi-
tive/negative values yields pt+, a positive and a
negative yield pt-, and nu? or ct? always yield
pt?. To illustrate these definitions, consider:
(14) Meg may have sleptct+,pt? after
eatingct+,pt+ the meal Ed cookedct+,pt+,
while no one was therect+,pt?
After signatures and NU are annotated, CT
and PT are calculated. At first, we apply
a set of rules that annotate generic presup-
position constructions with ct+. These in-
clude adverbial clauses opening with {while, be-
fore, after, where, how come, because, since,
owing to, though, despite, yet, therefore...},
WH arguments (who, which, whom, what), and
ct(p) =
?
?????????
?????????
ct+ :
p was already annotated
by a presupposition rule
ct(gov(p)) :
p heads a relative
clause
compCT (p) :
otherwise, and p is
a complement
ct? : otherwise (default)
Figure 2: Formula of ct(p), for any predicate p.
ct(gov(p)) is the CT of p?s governing predicate.
parataxis2. See for example the effects of after
and while in (14).
Then, we apply the following recursive se-
quential procedure. The tree root always gets
ct+ (see slept in (14)). The tree is then scanned
downwards, predicate by predicate. At each one,
we compute CT by the formula in Figure 2, as
follows. First, we check if one of the aforemen-
tioned presupposition rules already matched the
node. Second, if none matched, we apply to the
node?s entire subtree another set of rules that
annotate each relative clause with the CT of its
governing noun3, ct(gov(p)) (see failed in (15)).
Third, if no previous rule matched, and p is a
complement of another predicate gov(p), then
compCT(p) is calculated, by the following logic:
when pt(gov(p)) is pt+ or pt-, the correspond-
ing left or right sign of sig(gov(p)) is copied.
Otherwise, if pt(gov(p)) = pt?, ct? is returned,
except when the signature of gov(p) is +/+ (or
-/-) factive, which always yields ct+ (or ct-).
Third, if nothing applied to p, ct? is returned
by default. Finally, PT is set, according to Def-
inition 1.
To illustrate, consider these annotations:
(15) Gal managed+/?,ct+,pt+ a
building+/?,ct+,pt+, which Ginger
failed?/+,ct+,pt+ to sell+/?,ct?,pt?
First, managed gets ct+ as the tree root. Then,
we get compCT (building) = ct+, as the com-
plement of managed+/?,pt+. Next, a relative
clause rule copies ct+ from building to failed.
2The placing of clauses or phrases one after another,
without words to indicate coordination, as in ?veni, vidi,
vici? in contrast to ?veni, vidi and vici?.
3We also annotate nouns and adjectives as predicates
in copular constructions, and in instances where nouns
have complements.
755
Finally, compCT (sell) = ct- is calculated, as
the complement of failed?/+,pt+.
3 Evaluation
To evaluate TruthTeller?s accuracy, we sam-
pled 25 sentences from each of the RTE5 and
RTE6 Test datasets (Bentivogli et al, 2009;
Bentivogli et al, 2010), widely used for textual
inference benchmarks. In these 50 sentences, we
manually annotated each predicate, 153 in to-
tal, forming a gold standard. As baseline, we
report the most frequent value for each annota-
tion. The results, in Table 2, show high accuracy
for all types, reducing the baseline CT and PT
errors by half. Furthermore, most of the remain-
ing errors were due to parser errors, according
to a manual error analysis we conducted.
The baseline for NU annotations shows that
negations are scarce in these RTE datasets,
which was also the case for ct- and pt- an-
notations. Thus, Table 2 mostly indicates
TruthTeller?s performance in distinguishing pos-
itive CT and PT annotations from unknown
ones, the latter constituting v20% of the gold
standard. To further assess ct- and pt- annota-
tions we performed two targeted measurements.
Precision for ct- and pt- was measured by man-
ually judging the correctness of such annotations
by TruthTeller, on a sample from RTE6 Test
including 50 ct- and 124 pt- annotations. This
test yielded 78% and 83% precision, respectively.
pt- is more frequent as it is typically triggered
by ct-, as well as by other constructions involv-
ing negation. Recall was estimated by employ-
ing a human annotator to go through the dataset
and look for ct- and pt- gold standard anno-
tations. The annotator identified 40 ?ct-?s and
50 ?pt-?s, out of which TruthTeller found
47.5% of the ?ct-?s and 74% of the ?pt-?s. In
summary, TruthTeller?s performance on our
target PT annotations is quite satisfactory with
89% accuracy overall, having 83% precision and
74% recall estimates specifically for pt-.
4 Conclusions and Future Work
We have presentedTruthTeller, a novel algo-
rithm and system that identifies truth values of
Annotation TruthTeller Baseline
Signature 89.5% 81% (+/?)
NU 98% 97.3% (nu+)
CT 90.8% 78.4% (ct+)
PT 89% 77% (pt+)
Table 2: The accuracy measures for
TruthTeller?s 4 annotations. The right col-
umn gives the accuracy for the corresponding
most-frequent baseline: {+/?, nu+, ct+, pt+}.
predicates, the first such system to a) address or
combine a wide variety of relevant grammatical
constructions; b) be an open source annotation
tool; c) address the truth value annotation task
as an independent tool, which makes it possible
for client systems to use its output, while pre-
vious works only embedded annotations in their
task-specific systems; and d) annotate unknown
truth values extensively and explicitly.
TruthTeller may be used for several pur-
poses, such as inferring parts of a sentence
from the whole and improving textual entail-
ment (and contradiction) detection. It includes
a novel, large and accurate, lexicon of predicate
implication signatures.
While in this paper we evaluated the correct-
ness of TruthTeller as an individual com-
ponent, in the future we propose integrating
it in a state-of-the-art RTE system and report
its impact. One challenge in this scenario is
having other system components interact with
TruthTeller?s decisions, possibly masking its
effects. In addition, we plan to incorporate
monotonicity calculations in the annotation pro-
cess, like in MacCartney and Manning (2009).
5 Acknowledgements
This work was partially supported by the Israel
Science Foundation grant 1112/08 and the Eu-
ropean Community?s Seventh Framework Pro-
gramme (FP7/2007-2013) under grant agree-
ment no. 287923 (EXCITEMENT).
We thank Roni Katzir and Fred Landman for
useful discussions.
756
References
Roy Bar-Haim, Ido Dagan, Iddo Greental, and Eyal
Shnarch. 2007. Semantic inference at the lexical-
syntactic level. In Proceedings of AAAI, pages
871?876.
Luisa Bentivogli, Bernardo Magnini, Ido Dagan,
Hoa Trang Dang, and Danilo Giampiccolo. 2009.
The fifth pascal recognizing textual entailment
challenge. In Preproceedings of the Text Analysis
Conference (TAC).
Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa T.
Dang, and Danilo Giampiccolo. 2010. The sixth
PASCAL recognizing textual entailment chal-
lenge. In The Text Analysis Conference (TAC
2010).
Marie-Catherine de Marneffe and Christopher D.
Manning. 2008. The stanford typed dependencies
representation. In COLING Workshop on Cross-
framework and Cross-domain Parser Evaluation.
Christiane Fellbaum, editor. 1998. WordNet: An
Electronic Lexical Database (Language, Speech,
and Communication). MIT Press.
Lauri Karttunen. 1971. Implicative verbs. Lan-
guage, 47:340?358.
Lauri Karttunen. 2012. Simple and phrasal implica-
tives. In *SEM 2012, pages 124?131.
P. Kiparsky and C. Kiparsky. 1970. Fact.
In Progress in Linguistics, pages 143?173. The
Hague: Mouton de Gruyter.
Bill MacCartney and Christopher D. Manning. 2007.
Natural logic for textual inference. In Proceedings
of ACL workshop on textual entailment and para-
phrasing.
Bill MacCartney and Christopher D. Manning. 2009.
An extended model of natural logic. In Proceed-
ings of the Eighth International Conference on
Computational Semantics (IWCS-8).
Mausam, Michael Schmitz, Stephen Soderland,
Robert Bart, and Oren Etzioni. 2012. Open lan-
guage learning for information extraction. In Pro-
ceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning, pages
523?534.
Roser Morante and Walter Daelemans. 2012. An-
notating modality and negation for a machine
reading evaluation. In CLEF (Online Working
Notes/Labs/Workshop).
Rowan Nairn, Cleo Condoravdi, and Lauri Kart-
tunen. 2006. Computing relative polarity for tex-
tual inference. In In Proceedings of ICoS-5 (Infer-
ence in Computational Semantics).
757
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1209?1219,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Assessing the Role of Discourse References in Entailment Inference
Shachar Mirkin, Ido Dagan
Bar-Ilan University
Ramat-Gan, Israel
{mirkins,dagan}@cs.biu.ac.il
Sebastian Pado?
University of Stuttgart
Stuttgart, Germany
pado@ims.uni-stuttgart.de
Abstract
Discourse references, notably coreference
and bridging, play an important role in
many text understanding applications, but
their impact on textual entailment is yet to
be systematically understood. On the ba-
sis of an in-depth analysis of entailment
instances, we argue that discourse refer-
ences have the potential of substantially
improving textual entailment recognition,
and identify a number of research direc-
tions towards this goal.
1 Introduction
The detection and resolution of discourse refer-
ences such as coreference and bridging anaphora
play an important role in text understanding appli-
cations, like question answering and information
extraction. There, reference resolution is used for
the purpose of combining knowledge from multi-
ple sentences. Such knowledge is also important
for Textual Entailment (TE), a generic framework
for modeling semantic inference. TE reduces the
inference requirements of many text understand-
ing applications to the problem of determining
whether the meaning of a given textual assertion,
termed hypothesis (H), can be inferred from the
meaning of certain text (T ) (Dagan et al, 2006).
Consider the following example:
(1) T: ?Not only had he developed an aversion
to the President1 and politics in general,
Oswald2 was also a failure with Marina, his
wife. [...] Their relationship was supposedly
responsible for why he2 killed Kennedy1.?
H: ?Oswald killed President Kennedy.?
The understanding that the second sentence of the
text entails the hypothesis draws on two corefer-
ence relationships, namely that he is Oswald, and
that the Kennedy in question is President Kennedy.
However, the utilization of discourse information
for such inferences has been so far limited mainly
to the substitution of nominal coreferents, while
many aspects of the interface between discourse
and semantic inference needs remain unexplored.
The recently held Fifth Recognizing Textual
Entailment (RTE-5) challenge (Bentivogli et al,
2009a) has introduced a Search task, where the
text sentences are interpreted in the context of their
full discourse, as in Example 1 above. Accord-
ingly, TE constitutes an interesting framework ?
and the Search task an adequate dataset ? to study
the interrelation between discourse and inference.
The goal of this study is to analyze the roles
of discourse references for textual entailment in-
ference, to provide relevant findings and insights
to developers of both reference resolvers and en-
tailment systems and to highlight promising direc-
tions for the better incorporation of discourse phe-
nomena into inference. Our focus is on a manual,
in-depth assessment that results in a classification
and quantification of discourse reference phenom-
ena and their utilization for inference. On this ba-
sis, we develop an account of formal devices for
incorporating discourse references into the infer-
ence computation. An additional point of inter-
est is the interrelation between entailment knowl-
edge and coreference. E.g., in Example 1 above,
knowing that Kennedy was a president can alle-
viate the need for coreference resolution. Con-
versely, coreference resolution can often be used
to overcome gaps in entailment knowledge.
Structure of the paper. In Section 2, we pro-
vide background on the use of discourse refer-
ences in natural language processing (NLP) in
general and specifically in TE. Section 3 describes
the goals of this study, followed by our analy-
sis scheme (Section 4) and the required inference
1209
mechanisms (Section 5). Section 6 presents quan-
titative findings and further observations. Conclu-
sions are discussed in Section 7.
2 Background
2.1 Discourse in NLP
Discourse information plays a role in a range
of NLP tasks. It is obviously central to dis-
course processing tasks such as text segmenta-
tion (Hearst, 1997). Reference information pro-
vided by discourse is also useful for text under-
standing tasks such as question answering (QA),
information extraction (IE) and information re-
trieval (IR) (Vicedo and Ferrndez, 2006; Zelenko
et al, 2004; Na and Ng, 2009), as well as for the
acquisition of lexical-semantic ?narrative schema?
knowledge (Chambers and Jurafsky, 2009). Dis-
course references have been the subject of atten-
tion in both the Message Understanding Confer-
ence (Grishman and Sundheim, 1996) and the Au-
tomatic Content Extraction program (Strassel et
al., 2008).
The simplest form of information that discourse
provides is coreference, i.e., information that two
linguistic expressions refer to the same entity or
event. Coreference is particularly important for
processing pronouns and other anaphoric expres-
sions, such as he in Example 1. Ability to re-
solve this reference translates directly into, e.g., a
QA system?s ability to answer questions like Who
killed Kennedy?.
A second, more complex type of information
stems from bridging references, such as in the fol-
lowing discourse (Asher and Lascarides, 1998):
(2) ?I?ve just arrived. The camel is outside.?
While coreference indicates equivalence, bridging
points to the existence of a salient semantic rela-
tion between two distinct entities or events. Here,
it is (informally) ?means of transport?, which
would make the discourse (2) relevant for a ques-
tion like How did I arrive here?. Other types of
bridging relations include set-membership, roles
in events and consequence (Clark, 1975).
Note, however, that text understanding systems
are generally limited to the resolution of entity (or
even just pronoun) coreference, e.g. (Li et al,
2009; Dali et al, 2009). An important reason is the
unavailability of tools to resolve the more complex
(and difficult) forms of discourse reference such as
event coreference and bridging.1 Another reason
is uncertainty about their practical importance.
2.2 Discourse in Textual Entailment
Textual Entailment has been introduced in Sec-
tion 1 as a common-sense notion of inference.
It has spawned interest in the computational lin-
guistics community as a common denominator of
many NLP tasks including IE, summarization and
tutoring (Romano et al, 2006; Harabagiu et al,
2007; Nielsen et al, 2009).
Architectures for Textual Entailment. Over
the course of recent RTE challenges (Giampic-
colo et al, 2007; Giampiccolo et al, 2008), the
main benchmark for TE technology, two archi-
tectures for modeling TE have emerged as dom-
inant: transformations and alignment. The goal
of transformation-based TE models is to deter-
mine the entailment relation T ? H by find-
ing a ?proof?, i.e., a sequence of consequents,
(T, T1, . . . , Tn), such that Tn=H (Bar-Haim et al,
2008; Harmeling, 2009), and that in each trans-
formation, Ti? Ti+1, the consequent Ti+1 is en-
tailed by Ti. These transformations commonly in-
clude lexical modifications and the generation of
syntactic alternatives. The second major approach
constructs an alignment between the linguistic en-
tities of the trees (or graphs) of T and H , which
can represent syntactic structure, semantic struc-
ture, or non-hierarchical phrases (Zanzotto et al,
2009; Burchardt et al, 2009; MacCartney et al,
2008). H is assumed to be entailed by T if its en-
tities are aligned ?well? to corresponding entities
in T . Alignment quality is generally determined
based on features that assess the validity of the lo-
cal replacement of the T entity by the H entity.
While transformation- and alignment-based en-
tailment models look different at first glance, they
ultimately have the same goal, namely obtaining
a maximal coverage of H by T , i.e. to identify
matches of as many elements of H within T as
possible.2 To do so, both architectures typically
make use of inference rules such as ?Y was pur-
chased by X? X paid for Y?, either by directly ap-
plying them as transformations, or by using them
1Some studies, e.g. (Markert et al, 2003; Poesio et al,
2004), address the resolution of a few specific kinds of bridg-
ing relations; yet, wide-scope systems for bridging resolution
are unavailable.
2Clearly, the details of how the final entailment decision
is made based on the attained coverage differ substantially
among models.
1210
to score alignments. Rules are generally drawn
from external knowledge resources, such as Word-
Net (Fellbaum, 1998) or DIRT (Lin and Pantel,
2001), although knowledge gaps remain a key ob-
stacle (Bos, 2005; Balahur et al, 2008; Bar-Haim
et al, 2008).
Discourse in previous RTE challenges. The
first two rounds of the RTE challenge used ?self-
contained? texts and hypotheses, where discourse
considerations played virtually no role. A first step
towards a more comprehensive notion of entail-
ment was taken with RTE-3 (Giampiccolo et al,
2007), when paragraph-length texts were first in-
cluded and constituted 17% of the texts in the test
set. Chambers et al (2007) report that in a sample
of T ? H pairs drawn from the development set,
25% involved discourse references.
Using the concepts introduced above, the im-
pact of discourse references can be generally de-
scribed as a coverage problem, independent of the
system?s architecture. In Example 1, the hypoth-
esis word Oswald cannot be safely linked to the
text pronoun he without further knowledge about
he; the same is true for ?Kennedy ? President
Kennedy? which involves a specialization that is
only warranted in the specific discourse.
A number of systems have tried to address the
question of coreference in RTE as a preprocessing
step prior to inference proper, with most systems
using off-the-shelf coreference resolvers such as
JavaRap (Qiu et al, 2004) or OpenNLP3. Gen-
erally, anaphoric expressions were textually re-
placed by their antecedents. Results were in-
conclusive, however, with several reports about
errors introduced by automatic coreference res-
olution (Agichtein et al, 2008; Adams et al,
2007). Specific evaluations of the contribution
of coreference resolution yielded both small nega-
tive (Bar-Haim et al, 2008) and insignificant pos-
itive (Chambers et al, 2007) results.
3 Motivation and Goals
The results of recent studies, as reported in Sec-
tion 2.2, seem to show that current resolution of
discourse references in RTE systems hardly af-
fects performance. However, our intuition is that
these results can be attributed to four major lim-
itations shared by these studies: (1) the datasets,
where discourse phenomena were not well repre-
3http://opennlp.sourceforge.net
sented; (2) the off-the-shelf coreference resolution
systems which may have been not robust enough;
(3) the limitation to nominal coreference; and (4)
overly simple integration of reference information
into the inference engines.
The goal of this paper is to assess the impact of
discourse references on entailment with an anno-
tation study which removes these limitations. To
counteract (1), we use the recent RTE-5 Search
dataset (details below). To avoid (2), we perform
a manual analysis, assuming discourse references
as predicted by an oracle. With regards to (3), our
annotation scheme covers coreference and bridg-
ing relations of all syntactic categories and classi-
fies them. As for (4), we suggest several opera-
tions necessary to integrate the discourse informa-
tion into an entailment engine.
In contrast to the numerous existing datasets
annotated for discourse references (Hovy et al,
2006; Strassel et al, 2008), we do not annotate ex-
haustively. Rather, we are interested specifically in
those references instances that impact inference.
Furthermore, we analyze each instance from an
entailment perspective, characterizing the relevant
factors that have an impact on inference. To our
knowledge, this is the first such in-depth study.4
The results of our study are of twofold interest.
First, they provide guidance for the developers of
reference resolvers who might prioritize the scope
of their systems to make them more valuable for
inference. Second, they point out potential direc-
tions for the developers of inference systems by
specifying what additional inference mechanisms
are needed to utilize discourse information.
The RTE-5 Search dataset. We base our anno-
tation on the Search task dataset, a new addition
to the recent Fifth RTE challenge (Bentivogli et
al., 2009a) that is motivated by the needs of NLP
applications and drawn from the TAC summariza-
tion track. In the Search task, TE systems are re-
quired to find all individual sentences in a given
corpus which entail the hypothesis ? a setting that
is sensible not only for summarization, but also for
information access tasks like QA. Sentences are
judged individually, but ?are to be interpreted in
the context of the corpus as they rely on explicit
and implicit references to entities, events, dates,
places, etc., mentioned elsewhere in the corpus?
(Bentivogli et al, 2009b).
4The guidelines and the dataset are available at
http://www.cs.biu.ac.il/? nlp/downloads/
1211
Text Hypothesis
i T
? Once the reform becomes law, Spain will join the Netherlands
and Belgium in allowing homosexual marriages. Massachusetts allows homosexual
T Such unions are also legal in six Canadian provinces and the
northeastern US state of Massachusetts.
marriages
T ? The official name of 2003 UB313 has yet to be determined.
ii T Brown said he expected to find a moon orbiting Xena because
many Kuiper Belt objects are paired with moons.
2003 UB313 is in the Kuiper Belt
iii
T ?a
All seven aboard the AS-28 submarine appeared to be in satis-
factory condition, naval spokesman said.
T ?b
British crews were working with Russian naval authorities to ma-
neuver the unmanned robotic vehicle and untangle the AS-28.
The AS-28 mini submarine was trapped
underwater
T The Russian military was racing against time early Friday to res-
cue a mini submarine trapped on the seabed.
iv T
? China seeks solutions to its coal mine safety. A mining accident in China has killed
several minersT A recent accident has cost more than a dozen miners their lives.
v
T ??
A remote-controlled device was lowered to the stricken vessel to
cut the cables in which the AS-28 vehicle is caught.
T ?
The mini submarine was resting on the seabed at a depth of about
200 meters.
The AS-28 mini submarine was trapped
underwater
T Specialists said it could have become tangled up with a metal
cable or in sunken nets from a fishing trawler.
vi T . . . dried up lakes in Siberia, because the permafrost beneaththem has begun to thaw.
The ice is melting in the Arctic
Table 1: Examples for discourse-dependent entailment in the RTE-5 dataset, where the inference of H
depends on reference information from the discourse sentences T ? / T ??. Referring terms (in T ) and target
terms (in H) are shown in boldface.
4 Analysis Scheme
For annotating the RTE-5 data, we operationalize
reference relations that are relevant for entailment
as those that improve coverage. Recall from Sec-
tion 2.2 that the concept of coverage is applicable
to both transformation and alignment models, all
of which aim at maximizing coverage of H by T .
We represent T and H as syntactic trees, as
common in the RTE literature (Zanzotto et al,
2009; Agichtein et al, 2008). Specifically, we
assume MINIPAR-style (Lin, 1993) dependency
trees where nodes represent text expressions and
edges represent the syntactic relations between
them. We use ?term? to refer to text expressions,
and ?components? to refer to nodes, edges, and
subtrees. Dependency trees are a popular choice
in RTE since they offer a fairly semantics-oriented
account of the sentence structure that can still be
constructed robustly. In an ideal case of entail-
ment, all nodes and dependency edges of H are
covered by T .
For each T ? H pair, we annotate all relevant
discourse references in terms of three items: the
target component in H , the focus term in T , and
the reference term which stands in a reference re-
lation to the focus term. By resolving this ref-
erence, the target component can usually be in-
ferred; sometimes, however, more than one ref-
erence term needs to be found. We now define
and illustrate these concepts on examples from
Table 1.5
The target component is a tree component in
H that cannot be covered by the ?local? material
from T . An example for a tree component is Ex-
ample (v), where the target component AS-28 mini
submarine in H cannot be inferred from the pro-
noun it in T . Example (vi) demonstrates an edge
as target component. In this case, the edge in H
connecting melt with the modifier in the Arctic is
not found in T . Although each of the hypothesis?
nodes can be covered separately via knowledge-
based rules (e.g. ?Siberia ? Arctic?, ?permafrost
? ice?, ?thaw ? melt?), the resulting fragments
in T are unconnected without the (intra-sentential)
coreference between them and lakes in Siberia.
For each target component, we identify its focus
term as the expression in T that does not cover the
target component itself but participates in a refer-
ence relation that can help covering it.
We follow the focus term?s reference chain to
a reference term which can, either separately or
in combination with the focus term, help covering
the target component. In Example (ii), where the
5In our annotation, we assume throughout that some
knowledge about basic admissible transformations is avail-
able, such as passive to active or derivational transformations;
for brevity, we ignore articles in the examples and treat named
entities as single nodes.
1212
target component in H is 2003 UB313, Xena is the
focus term in T and the reference term is a men-
tion of 2003 UB313 in a previous sentence, T ?. In
this case, the reference term covers the entire tar-
get component on its own.
An additional attribute that we record for each
instance is whether resolving the discourse refer-
ence is mandatory for determining entailment, or
optional. In Example (v), it is mandatory: the in-
ference cannot be completed without the knowl-
edge provided by the discourse. In contrast, in
Example (ii), inferring 2003 UB313 from Xena
is optional. It can be done either by identify-
ing their coreference relation, or by using back-
ground knowledge in the form of an entailment
rule, ?Xena ? 2003 UB313?, that is applicable
in the context of astronomy. Optional discourse
references represent instances where discourse in-
formation and TE knowledge are interchange-
able. As mentioned, knowledge gaps constitute
a major obstacle for TE systems, and we can-
not rely on the availability of any ceratin piece of
knowledge to the inference process. Thus, in our
scheme, mandatory references provide a ?lower
bound? with regards to the necessity to resolve
discourse references, even in the presence of com-
plete knowledge; optional references, on the other
hand, set an ?upper bound? for the contribution of
discourse resolution to inference, when no knowl-
edge is available. At the same time, this scheme
allows investigating how much TE knowledge can
be replaced by (perfect) discourse processing.
When choosing a reference term, we search the
reference chain of the focus term for the nearest
expression that is identical to the target component
or a subcomponent of it. If we find such an expres-
sion, covering the identical part of the target com-
ponent requires no entailment knowledge. If no
identical reference term exists, we choose the se-
mantically ?closest? term from the reference chain,
i.e. the term which requires the least knowledge to
infer the target component. For instance, we may
pick permafrost as the semantically closet term to
the target ice if the latter is not found in the focus
term?s reference chain.
Finally, for each reference relation that we an-
notate, we record four additional attributes which
we assumed to be informative in an evaluation.
First, the reference type: Is the relation a coref-
erence or a bridging reference? Second, the syn-
tactic type of the focus and reference terms. Third,
the focus/reference terms entailment status ? does
some kind of entailment relation hold between the
two terms? Fourth, the operation that should be
performed on the focus and reference terms to ob-
tain coverage of the target component (as specified
in Section 5).
5 Integrating Discourse References into
Entailment Recognition
In initial analysis we found that the standard sub-
stitution operation applied by virtually all previous
studies for integrating coreference into entailment
is insufficient. We identified three distinct cases
for the integration of discourse reference knowl-
edge in entailment, which correspond to different
relations between the target component, the fo-
cus term and the reference term. This section de-
scribes the three cases and characterizes them in
terms of tree transformations. An initial version of
these transformations is described in (Abad et al,
2010). We assume a transformation-based entail-
ment architecture (cf. Section 2.2), although we
believe that the key points of our account are also
applicable to alignment-based architecture. Trans-
formations create revised trees that cover previ-
ously uncovered target components in H . The
output of each transformation, T1, is comprised
of copies of the components used to construct it,
and is appended to the discourse forest, which in-
cludes the dependency trees of all sentences and
their generated consequents.
We assume that we have access to a dependency
tree for H , a dependency forest for T and its dis-
course context, as well as the output of a perfect
discourse processor, i.e., a complete set of both
coreference and bridging relations, including the
type of bridging relation (e.g. part-of, cause).
We use the following notation. We use x, y
for tree nodes, and Sx to denote a (sub-)tree with
root x. lab(x) is the label of the incoming edge
of x (i.e., its grammatical function). We write
C(x, y) for a coreference relation between Sx and
Sy, the corresponding trees of the focus and refer-
ence terms, respectively. We write Br(x, y) for a
bridging relation, where r is its type.
(1) Substitution: This is the most intuitive and
widely-used transformation, corresponding to the
treatment of discourse information in existing sys-
tems. It applies to coreference relations, when an
expression found elsewhere in the text (the refer-
ence term) can cover all missing information (the
1213
be lega
l als
o
unio
n
suc
h
pred
mod
subj
be lega
l
also
mar
riage
s
hom
ose
xua
lpre
d mod
subj
mod
T
T 1
mar
riage
s
hom
ose
xua
l
modT?
pre
Figure 1: The Substitution transformation, demon-
strated on the relevant subtrees of Example (i).
The dashed line denotes a discourse reference.
target component) on its own. In such cases, the
reference term can replace the entire focus term.
Apparently (cf. Section 6), substitution applies
also to some types of bridging relations, such as
set-membership, when the member is sufficient for
representing the entire set for the necessary infer-
ence. For example, in ?I met two people yesterday.
The woman told me a story.? (Clark, 1975), sub-
stituting two people with woman results in a text
which is entailed from the discourse, and which
allows inferring ?I met a woman yesterday.?
In a parse tree representation, given a corefer-
ence relation C(x, y) (or Br(x, y)), the newly gen-
erated tree, T1, consists of a copy of T , where the
entire tree Sx is replaced by a copy of Sy . In Fig-
ure 1, which shows Example (i) from Table 1, such
unions is substituted by homosexual marriages.
Head-substitution. Occasionally, substituting
only the head of the focus term is sufficient. In
such cases, only the root nodes x and y are sub-
stituted. This is the case, for example, with syn-
onymous verbs with identical subcategorization
frames (like melt and thaw). As verbs typically
constitute tree roots in dependency parses, sub-
stituting or merging (see below) their entire trees
might be inappropriate or wasteful. In such cases,
the simpler head-substitution may be applied.
(2) Merge: In contrast to substitution, where a
match for the entire target component is found
elsewhere in the text, this transformation is re-
quired when parts of the missing information are
scattered among multiple locations in the text.
We distinguish between two types of merge trans-
formations: (a) dependent-merge, and (b) head-
merge, depending on the syntactic roles of the
merged components.
(a) Dependent-Merge. This operation is ap-
plicable when the head of either the focus or ref-
erence terms (of both) matches the head node of
subm
arine
mini
ontrapp
ed mod
T
T 1
subm
arine AS-2
8nnT? a
pcom
p-n
pnm
od
mod
sea
bed
subm
arine
mini
trapp
ed
mod
pnm
od
mod
AS-2
8nn
AS-2
8  
T? b
on
pcom
p-n sea
bed
Figure 2: The dependent-merge (T ?a) and head-
merge (T ?b) transformations (Example (iii)).
the target component, but modifiers from both of
them are required to cover the target component?s
dependents. The modifiers are therefore merged
as dependents of a single head node, to create
a tree that covers the entire target component.
Dependent-merge is illustrated in Figure 2, using
Example (iii). The component we wish to cover in
H is the noun phrase AS-28 mini submarine. Un-
fortunately, the focus term in T , ?mini submarine
trapped on the seabed?, covers only the modifier
mini, but not AS-28. This modifier can however be
provided by the coreferent term in T ?a (left upper
corner). Once merged, the inference engine can,
e.g., employ the rule ?on seabed ? underwater?
to cover H completely.
Formally, assume without loss of generality that
y, the reference term?s head, matches the root node
of the target component. Given C(x, y), we define
T1 as a copy of T , where (i) the subtree Sx is re-
placed by Sy, and (ii) for all children c of x, a copy
of Sc is placed under the copy of y in T1 with its
original edge label, lab(c).
(b) Head-merge. An alternative way to recover
the missing information in Example (iii) is to find
a reference term whose head word itself (rather
than one of its modifiers) matches the target com-
ponent?s missing dependent, as with AS-28 in Fig-
ure 2 in the bottom left corner (T ?b). In terms of
parse trees, we need to add one tree as a depen-
dent of the other. Formally, given C(x, y), simi-
larly to dependent-merge, T1 is created as a copy
of T where the subtree Sx is replaced by either Sx
or Sy, depending on whichever of x and y matches
the target component?s head. Assume it is x, for
example. Then, a copy of Sy is added as a new
child to x. In our sample, head-merge operations
correspond to internal coreferences within nomi-
nal target components (such as between AS-28 and
mini submarine in this case). The appropriate la-
bel, lab(y), in these cases is nn (nominal modi-
1214
in
T
T 1
T?
pcom
p-n Chin
acost have
thanmore comp1 pcomp
-nobj
have
doze
n
acc
identsubj recen
t
mod
cos
t have
thanmore co
mp1 pcom
p-n
obj
have
doze
n
acc
identsubj recen
tmod
mod
Solut
ionseek
Chin
a
tomod pco
mp-n
safet
y coa
lmin
e
nn
nn
itsgenobj
subj
Figure 3: The insertion transformation. Dotted
edges mark the newly inserted path (Ex. (iv)).
fier). Further analysis is required to specify what
other dependencies can hold between such core-
ferring heads.
(3) Insertion: The last transformation, insertion,
is used when a relation that is realized in H is
missing from T and is only implied via a bridg-
ing relation. In Example (iv), the location that is
explicitly mentioned in H can only be covered by
T by resolving a bridging reference with China
in T ?. To connect the bridging referents, a new
tree component representing the bridging relation
is inserted into the consequent tree T1. In this ex-
ample, the component connects China and recent
accident via the in preposition. Formally, given
a bridging relation Br(x, y), we introduce a new
subtree Srz into T1, where z is a child of x and
lab(z) = labr. Srz must contain a variable node
that is instantiated with a copy of S(y).
This transformation stands out from the others
in that it introduces new material. For each bridg-
ing relation, it adds a specific subtrees Sr via an
edge labeled with labr. These two items form the
dependency representation of the bridging relation
Br and must be provided by the interface between
the discourse and the inference systems. Clearly,
their exact form depends on the set of bridging re-
lations provided by the discourse resolver as well
as the details of the dependency parses.
As shown in Figure 3, the bridging relation
located-in (r) is represented by inserting a subtree
Srz headed by in (z) into T1 and connecting it to
accident (x) as a modifier (labr). The subtree Srz
consists of a variable node which is connected to
in with a pcomp-n dependency (a nominal head of
a prepositional phrase), and which is instantiated
with the node China (y) when the transformation
is applied. Note that the structure of Srz and the
way it is inserted into T1 are predefined by the
abovementioned interface; only the node to which
it is attached and the contents of the variable node
are determined at transformation-time.
As another example, consider the following
short text from (Clark, 1975): John was murdered
yesterday. The knife lay nearby. Here, the bridg-
ing relation between the murder event and the in-
strument, the knife (x), can be addressed by in-
serting under x a subtree for the clause with which
as Srz , with a variable which is instantiated by the
parse-tree (headed by murdered, y) of the entire
first sentence John was murdered yesterday.
Transformation chaining. Since our transfor-
mations are defined to be minimal, some cases re-
quire the application of multiple transformations
to achieve coverage. Consider Example (v), Ta-
ble 1. We wish to cover AS-28 mini submarine in
H from the coreferring it in T , mini submarine in
T ? and AS-28 vehicle in T ??. A substitution of it by
either coreference does not suffice, since none of
the antecedents contains all necessary modifiers. It
is therefore necessary to substitute it first by one of
the coreferences and then merge it with the other.
6 Results
We analyzed 120 sentence-hypothesis pairs of the
RTE-5 development set (21 different hypotheses,
111 distinct sentences, 53 different documents).
Below, we summarize our findings, focusing on
the relation between our findings and the assump-
tions of previous studies as discussed in Section 3.
General statistics. We found that 44% of the
pairs contained reference relations whose resolu-
tion was mandatory for inference. In another 28%,
references could optionally support the inference
of the hypothesis. In the remaining 28%, refer-
ences did not contribute towards inference. The
total number of relevant references was 137, and
37 pairs (27%) contained multiple relevant refer-
ences. These numbers support our assumption that
discourse references play an important role in in-
ference.
Reference types. 73% of the identified refer-
ences are coreferences and 27% are bridging re-
lations. The most common bridging relation was
the location of events (e.g. Arctic in ice melting
events), generally assumed to be known through-
out the document. Other bridging relations we en-
countered include cause (e.g. between injured and
attack), event participants and set membership.
1215
(%) Pronoun NE NP VP
Focus term 9 19 49 23
Reference term - 43 43 14
Table 2: Syntactic types of discourse references
(%) Sub. Merge Insertion
Coreference 62 38 -
Bridging 30 - 70
Total 54 28 18
Table 3: Distribution of transformation types
Syntactic types. Table 2 shows that 77% of all
focus terms and 86% of the reference terms were
nominal phrases, which justifies their prominent
position in work on anaphora and coreference res-
olution. However, almost a quarter of the focus
terms were verbal phrases. We found these focus
terms to be frequently crucial for entailment since
they included the main predicate of the hypothe-
sis.6 This calls for an increased focus on the reso-
lution of event references.
Transformations. Table 3 shows the relative
frequencies of all transformations. Again, we
found that the ?default? transformation, substitu-
tion, is the most frequent one, and is helpful for
both coreference and bridging relations. Substitu-
tion is particularly useful for handling pronouns
(14% of all substitution instances), the replace-
ment of named entities by synonymous names
(32%), the replacement of other NPs (38%), and
the substitution of verbal head nodes in event
coreference (16%). Yet, in nearly half the cases,
a different transformation had to be applied. In-
sertion accounts for the majority of bridging cases.
Head-merge is necessary to integrate proper nouns
as modifiers of other head nouns. Dependent-
merge, responsible for 85% of the merge transfor-
mations, can be used to complete nominal focus
terms with missing modifiers (e.g., adjectives), as
well as for merging other dependencies between
coreferring predicates. This result indicates the
importance of incorporating other transformations
into inference systems.
Distance of reference terms. The distance be-
tween the focus and the reference terms varied
considerably, ranging from intra-sentential refer-
ence relations and up to several dozen sentences.
For more than a quarter of the focus terms, we
6The lower proportion of VPs among reference terms
stems from bridging relations between VPs and nominal de-
pendents, such as the abovementioned ?location? relation.
had to go to other documents to find reference
terms that, possibly in conjunction with the focus
term, could cover the target components. Interest-
ingly, all such cases involved coreference (about
equally divided between the merge transforma-
tions and substitutions), while bridging was al-
ways ?document-local?. This result reaffirms the
usefulness of cross-document coreference resolu-
tion for inference (Huang et al, 2009).
Discourse resolution as preprocessing? In ex-
isting RTE systems, discourse references are typ-
ically resolved as a preprocessing step. While
our annotation was manual and cannot yield di-
rect results about processing considerations, we
observed that discourse relations often hold be-
tween complex, and deeply embedded, expres-
sions, which makes their automatic resolution dif-
ficult. Of course, many RTE systems attempt to
normalize and simplify H and T , e.g., by split-
ting conjunctions or removing irrelevant clauses,
but these operations are usually considered a part
of the inference rather the preprocessing phase (cf.
e.g., Bar-Haim et al (2007)). Since the resolu-
tion of discourse references is likely to profit from
these steps, it seems desirable to ?postpone? it un-
til after simplification. In transformation-based
systems, it might be natural to add discourse-based
transformations to the set of inference operations,
while in alignment-based systems, discourse ref-
erences can be integrated into the computation of
alignment scores.
Discourse references vs. entailment knowledge.
We have stated before that even if a discourse ref-
erence is not strictly necessary for entailment, it
may be interesting because it represents an alter-
native to the use of knowledge rules to cover the
hypothesis. Sometimes, these rules are generally
applicable (e.g., ?Alaska? Arctic?). However, of-
ten they are context-specific. Consider the follow-
ing sentence as T for the hypothesis H: ?The ice
is melting in the Arctic?:
(3) T : ?The scene at the receding edge of the Exit
Glacier was part festive gathering, part nature
tour with an apocalyptic edge.?
While it is possible to cover melting using a rule
?melting? receding?, this rule is only valid under
quite specific conditions (e.g., for the subject ice).
Instead of determining the applicability of the rule,
a discourse-aware system can take the next sen-
1216
tence into account, which contains a coreferring
event to receding that can cover melting in H:
(4) T ?: ?. . . people moved closer to the rope line
near the glacier as it shied away, practically
groaning and melting before their eyes.?
Discourse relations can in fact encode arbitrar-
ily complex world knowledge, as in the following
pair:
(5) H: ?The serial killer BTK was accused of at
least 7 killings starting in the 1970?s.?
T: ?Police say BTK may have killed as many
as 10 people between 1974 and 1991.?
Here, the H modifier serial, which does not occur
in T , can be covered either by world knowledge
(a person who killed 10 people is a serial killer),
or by resolving the coreference of BTK to the term
the serial killer BTK which occurs in the discourse
around T . Our conclusion is that not only can
discourse references often replace world knowl-
edge in principle, in practice it often seems easier
to resolve discourse references than to determine
whether a rule is applicable in a given context or
to formalize complex world knowledge as infer-
ence rules. Our annotation provides further em-
pirical support to this claim: An entailment rela-
tion exists between the focus and reference terms
in 60% of the focus-reference term pairs, and in
many of the remainder, entailment holds between
the terms? heads. Thus, discourse provides rela-
tions which are many times equivalent to entail-
ment knowledge rules and can therefore be uti-
lized in their stead.
7 Conclusions
This work has presented an analysis of the relation
between discourse references and textual entail-
ment. We have identified a set of limitations com-
mon to the handling of discourse relations in vir-
tually all entailment systems. They include the use
of off-the-shelf resolvers that concentrate on nom-
inal coreference, the integration of reference in-
formation through substitution, and the RTE eval-
uation schemes, which played down the role of
discourse. Since in practical settings, discourse
plays an important role, our goal was to develop
an agenda for improving the handling of discourse
references in entailment-based inference.
Our manual analysis of the RTE-5 dataset
shows that while the majority of discourse refer-
ences that affect inference are nominal coreference
relations, another substantial part is made up by
verbal terms and bridging relations. Furthermore,
we have demonstrated that substitution alone is in-
sufficient to extract all relevant information from
the wide range of discourse references that are
frequently relevant for inference. We identified
three general cases, and suggested matching op-
erations to obtain the relevant inferences, formu-
lated as tree transformations. Furthermore, our ev-
idence suggests that for practical reasons, the res-
olution of discourse references should be tightly
integrated into entailment systems instead of treat-
ing it as a preprocessing step.
A particularly interesting result concerns the
interplay between discourse references and en-
tailment knowledge. While semantic knowledge
(e.g., from WordNet or Wikipedia) has been used
beneficially for coreference resolution (Soon et al,
2001; Ponzetto and Strube, 2006), reference res-
olution has, to our knowledge, not yet been em-
ployed to validate entailment rules? applicability.
Our analyses suggest that in the context of de-
ciding textual entailment, reference resolution and
entailment knowledge can be seen as complemen-
tary ways of achieving the same goal, namely en-
riching T with additional knowledge to allow the
inference of H . Given that both of the technolo-
gies are still imperfect, we envisage the way for-
ward as a joint strategy, where reference resolution
and entailment rules mutually fill each other?s gaps
(cf. Example 3).
In sum, our study shows that textual entailment
can profit substantially from better discourse han-
dling. The next challenge is to translate the the-
oretical gain into practical benefit. Our analy-
sis demonstrates that improvements are necessary
both on the side of discourse reference resolution
systems, which need to cover more types of refer-
ences, as well as a better integration of discourse
information in entailment systems, even for those
relations which are within the scope of available
resolvers.
Acknowledgements
This work was partially supported by the
PASCAL-2 Network of Excellence of the Eu-
ropean Community FP7-ICT-2007-1-216886 and
the Israel Science Foundation grant 1112/08.
1217
References
Azad Abad, Luisa Bentivogli, Ido Dagan, Danilo Gi-
ampiccolo, Shachar Mirkin, Emanuele Pianta, and
Asher Stern. 2010. A resource for investigating the
impact of anaphora and coreference on inference. In
Proceedings of LREC.
Rod Adams, Gabriel Nicolae, Cristina Nicolae, and
Sanda Harabagiu. 2007. Textual entailment through
extended lexical overlap and lexico-semantic match-
ing. In Proceedings of the ACL-PASCAL Workshop
on Textual Entailment and Paraphrasing.
E. Agichtein, W. Askew, and Y. Liu. 2008. Combining
lexical, syntactic, and semantic evidence for textual
entailment classification. In Proceedings of TAC.
Nicholas Asher and Alex Lascarides. 1998. Bridging.
Journal of Semantics, 15(1):83?113.
Alexandra Balahur, Elena Lloret, O?scar Ferra?ndez,
Andre?s Montoyo, Manuel Palomar, and Rafael
Mun?oz. 2008. The DLSIUAES team?s participation
in the TAC 2008 tracks. In Proceedings of TAC.
Roy Bar-Haim, Ido Dagan, Iddo Greental, and Eyal
Shnarch. 2007. Semantic inference at the lexical-
syntactic level. In Proceedings of AAAI.
Roy Bar-Haim, Jonathan Berant, Ido Dagan, Iddo
Greental, Shachar Mirkin, and Eyal Shnarch amd
Idan Szpektor. 2008. Efficient semantic deduc-
tion and approximate matching over compact parse
forests. In Proceedings of TAC.
Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo
Giampiccolo, and Bernardo Magnini. 2009a. The
fifth pascal recognizing textual entailment chal-
lenge. In Proceedings of TAC.
Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo
Giampiccolo, Medea Lo Leggio, and Bernardo
Magnini. 2009b. Considering discourse references
in textual entailment annotation. In Proceedings of
the 5th International Conference on Generative Ap-
proaches to the Lexicon (GL2009).
Johan Bos. 2005. Recognising textual entailment with
logical inference. In Proceedings of EMNLP.
Aljoscha Burchardt, Marco Pennacchiotti, Stefan
Thater, and Manfred Pinkal. 2009. Assessing
the impact of frame semantics on textual entail-
ment. Journal of Natural Language Engineering,
15(4):527?550.
Nathanael Chambers and Dan Jurafsky. 2009. Unsu-
pervised learning of narrative schemas and their par-
ticipants. In Proceedings of ACL-IJCNLP.
Nathanael Chambers, Daniel Cer, Trond Grenager,
David Hall, Chloe Kiddon, Bill MacCartney, Marie-
Catherine de Marneffe, Daniel Ramage, Eric Yeh,
and Christopher D. Manning. 2007. Learning align-
ments and leveraging natural logic. In Proceedings
of the ACL-PASCAL Workshop on Textual Entail-
ment and Paraphrasing.
Herbert H. Clark. 1975. Bridging. In R. C. Schank
and B. L. Nash-Webber, editors, Theoretical issues
in natural language processing, pages 169?174. As-
sociation of Computing Machinery.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The PASCAL recognising textual entailment
challenge. In Machine Learning Challenges, vol-
ume 3944 of Lecture Notes in Computer Science,
pages 177?190. Springer.
Lorand Dali, Delia Rusu, Blaz Fortuna, Dunja
Mladenic, and Marko Grobelnik. 2009. Ques-
tion answering based on semantic graphs. In Pro-
ceedings of the Workshop on Semantic Search (Sem-
Search 2009).
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database (Language, Speech, and
Communication). The MIT Press.
Danilo Giampiccolo, Bernardo Magnini, Ido Dagan,
and Bill Dolan. 2007. The third pascal recogniz-
ing textual entailment challenge. In Proceedings of
the ACL-PASCAL Workshop on Textual Entailment
and Paraphrasing.
Danilo Giampiccolo, Hoa Trang Dang, Bernardo
Magnini, Ido Dagan, and Bill Dolan. 2008. The
fourth pascal recognizing textual entailment chal-
lenge. In Proceedings of TAC.
Ralph Grishman and Beth Sundheim. 1996. Mes-
sage Understanding Conference-6: a brief history.
In Proceedings of the 16th conference on Computa-
tional Linguistics.
Sanda Harabagiu, Andrew Hickl, and Finley Lacatusu.
2007. Satisfying information needs with multi-
document summaries. Information Processing &
Management, 43:1619?1642.
Stefan Harmeling. 2009. Inferring textual entailment
with a probabilistically sound calculus. Journal of
Natural Language Engineering, pages 459?477.
Marti A. Hearst. 1997. Segmenting text into multi-
paragraph subtopic passages. Computational Lin-
guistics, 23(1):33?64.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
The 90% solution. In Proceedings of HLT-NAACL.
Jian Huang, Sarah M. Taylor, Jonathan L. Smith, Kon-
stantinos A. Fotiadis, and C. Lee Giles. 2009. Pro-
file based cross-document coreference using kernel-
ized fuzzy relational clustering. In Proceedings of
ACL-IJCNLP.
Fangtao Li, Yang Tang, Minlie Huang, and Xiaoyan
Zhu. 2009. Answering opinion questions with
random walks on graphs. In Proceedings of ACL-
IJCNLP.
1218
Dekang Lin and Patrick Pantel. 2001. Discovery of in-
ference rules for question answering. Natural Lan-
guage Engineering, 4(7):343?360.
Dekang Lin. 1993. Principle-based parsing without
overgeneration. In Proceedings of ACL.
Bill MacCartney, Michel Galley, and Christopher D.
Manning. 2008. A phrase-based alignment model
for natural language inference. In Proceedings of
EMNLP.
Katja Markert, Malvina Nissim, and Natalia N. Mod-
jeska. 2003. Using the web for nominal anaphora
resolution. In Proceedings of EACL Workshop on
the Computational Treatment of Anaphora.
Seung-Hoon Na and Hwee Tou Ng. 2009. A 2-poisson
model for probabilistic coreference of named enti-
ties for improved text retrieval. In Proceedings of
SIGIR.
Rodney D. Nielsen, Wayne Ward, and James H. Mar-
tin. 2009. Recognizing entailment in intelligent
tutoring systems. Natural Language Engineering,
15(4):479?501.
Massimo Poesio, Rahul Mehta, Axel Maroudas, and
Janet Hitzeman. 2004. Learning to resolve bridging
references. In Proceedings of ACL.
Simone Paolo Ponzetto and Michael Strube. 2006.
Exploiting semantic role labeling, WordNet and
Wikipedia for coreference resolution. In Proceed-
ings of HLT.
Long Qiu, Min-Yen Kan, and Tat-Seng Chua. 2004. A
public reference implementation of the rap anaphora
resolution algorithm. In Proceedings of LREC.
Lorenza Romano, Milen Kouylekov, Idan Szpektor,
Ido Dagan, and Alberto Lavelli. 2006. Investigat-
ing a generic paraphrase-based approach for relation
extraction. In Proceedings of EACL.
Wee Meng Soon, Hwee Tou Ng, and Daniel
Chung Yong Lim. 2001. A machine learning ap-
proach to coreference resolution of noun phrases.
Computational Linguistics, 27(4):521?544.
Stephanie Strassel, Mark Przybocki, Kay Peterson,
Zhiyi Song, and Kazuaki Maeda. 2008. Linguistic
resources and evaluation techniques for evaluation
of cross-document automatic content extraction. In
Proceedings of LREC.
Jose L. Vicedo and Antonio Ferrndez. 2006. Coref-
erence in Q&A. In Tomek Strzalkowski and
Sanda M. Harabagiu, editors, Advances in Open Do-
main Question Answering, pages 71?96. Springer.
Fabio Massimo Zanzotto, Marco Pennacchiotti, and
Alessandro Moschitti. 2009. A machine learning
approach to textual entailment recognition. Journal
of Natural Language Engineering, 15(4):551?582.
Dmitry Zelenko, Chinatsu Aone, and Jason Tibbetts.
2004. Coreference resolution for information ex-
traction. In Proceedings of the ACL Workshop on
Reference Resolution and its Applications.
1219
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1220?1229,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Global Learning of Focused Entailment Graphs
Jonathan Berant
Tel-Aviv University
Tel-Aviv, Israel
jonatha6@post.tau.ac.il
Ido Dagan
Bar-Ilan University
Ramat-Gan, Israel
dagan@cs.biu.ac.il
Jacob Goldberger
Bar-Ilan University
Ramat-Gan, Israel
goldbej@eng.biu.ac.il
Abstract
We propose a global algorithm for learn-
ing entailment relations between predi-
cates. We define a graph structure over
predicates that represents entailment rela-
tions as directed edges, and use a global
transitivity constraint on the graph to learn
the optimal set of edges, by formulating
the optimization problem as an Integer
Linear Program. We motivate this graph
with an application that provides a hierar-
chical summary for a set of propositions
that focus on a target concept, and show
that our global algorithm improves perfor-
mance by more than 10% over baseline al-
gorithms.
1 Introduction
The Textual Entailment (TE) paradigm (Dagan et
al., 2009) is a generic framework for applied se-
mantic inference. The objective of TE is to recog-
nize whether a target meaning can be inferred from
a given text. For example, a Question Answer-
ing system has to recognize that ?alcohol affects
blood pressure? is inferred from ?alcohol reduces
blood pressure? to answer the question ?What af-
fects blood pressure??
TE systems require extensive knowledge of en-
tailment patterns, often captured as entailment
rules: rules that specify a directional inference re-
lation between two text fragments (when the rule
is bidirectional this is known as paraphrasing). An
important type of entailment rule refers to propo-
sitional templates, i.e., propositions comprising
a predicate and arguments, possibly replaced by
variables. The rule required for the previous ex-
ample would be ?X reduce Y ? X affect Y?. Be-
cause facts and knowledge are mostly expressed
by propositions, such entailment rules are central
to the TE task. This has led to active research
on broad-scale acquisition of entailment rules for
predicates, e.g. (Lin and Pantel, 2001; Sekine,
2005; Szpektor and Dagan, 2008).
Previous work has focused on learning each en-
tailment rule in isolation. However, it is clear that
there are interactions between rules. A prominent
example is that entailment is a transitive relation,
and thus the rules ?X ? Y ? and ?Y ? Z? imply
the rule ?X ? Z?. In this paper we take advantage
of these global interactions to improve entailment
rule learning.
First, we describe a structure termed an entail-
ment graph that models entailment relations be-
tween propositional templates (Section 3). Next,
we show that we can present propositions accord-
ing to an entailment hierarchy derived from the
graph, and suggest a novel hierarchical presenta-
tion scheme for corpus propositions referring to a
target concept. As in this application each graph
focuses on a single concept, we term those focused
entailment graphs (Section 4).
In the core section of the paper, we present an
algorithm that uses a global approach to learn the
entailment relations of focused entailment graphs
(Section 5). We define a global function and look
for the graph that maximizes that function under
a transitivity constraint. The optimization prob-
lem is formulated as an Integer Linear Program
(ILP) and solved with an ILP solver. We show that
this leads to an optimal solution with respect to
the global function, and demonstrate that the algo-
rithm outperforms methods that utilize only local
information by more than 10%, as well as meth-
ods that employ a greedy optimization algorithm
rather than an ILP solver (Section 6).
2 Background
Entailment learning Two information types have
primarily been utilized to learn entailment rules
between predicates: lexicographic resources and
distributional similarity resources. Lexicographic
1220
resources are manually-prepared knowledge bases
containing information about semantic relations
between lexical items. WordNet (Fellbaum,
1998), by far the most widely used resource, spec-
ifies relations such as hyponymy, derivation, and
entailment that can be used for semantic inference
(Budanitsky and Hirst, 2006). WordNet has also
been exploited to automatically generate a training
set for a hyponym classifier (Snow et al, 2005),
and we make a similar use of WordNet in Section
5.1.
Lexicographic resources are accurate but tend
to have low coverage. Therefore, distributional
similarity is used to learn broad-scale resources.
Distributional similarity algorithms predict a se-
mantic relation between two predicates by com-
paring the arguments with which they occur. Quite
a few methods have been suggested (Lin and Pan-
tel, 2001; Bhagat et al, 2007; Yates and Etzioni,
2009), which differ in terms of the specifics of the
ways in which predicates are represented, the fea-
tures that are extracted, and the function used to
compute feature vector similarity. Details on such
methods are given in Section 5.1.
Global learning It is natural to describe en-
tailment relations between predicates by a graph.
Nodes represent predicates, and edges represent
entailment between nodes. Nevertheless, using a
graph for global learning of entailment between
predicates has attracted little attention. Recently,
Szpektor and Dagan (2009) presented the resource
Argument-mapped WordNet, providing entailment
relations for predicates in WordNet. Their re-
source was built on top of WordNet, and makes
simple use of WordNet?s global graph structure:
new rules are suggested by transitively chaining
graph edges, and verified against corpus statistics.
The most similar work to ours is Snow et al?s al-
gorithm for taxonomy induction (2006). Snow et
al.?s algorithm learns the hyponymy relation, un-
der the constraint that it is a transitive relation.
Their algorithm incrementally adds hyponyms to
an existing taxonomy (WordNet), using a greedy
search algorithm that adds at each step the set of
hyponyms that maximize the probability of the ev-
idence while respecting the transitivity constraint.
In this paper we tackle a similar problem of
learning a transitive relation, but we use linear pro-
gramming. A Linear Program (LP) is an optimiza-
tion problem, where a linear function is minimized
(or maximized) under linear constraints. If the
variables are integers, the problem is termed an In-
teger Linear Program (ILP). Linear programming
has attracted attention recently in several fields of
NLP, such as semantic role labeling, summariza-
tion and parsing (Roth and tau Yih, 2005; Clarke
and Lapata, 2008; Martins et al, 2009). In this
paper we formulate the entailment graph learning
problem as an Integer Linear Program, and find
that this leads to an optimal solution with respect
to the target function in our experiment.
3 Entailment Graph
This section presents an entailment graph struc-
ture, which resembles the graph in (Szpektor and
Dagan, 2009).
The nodes of an entailment graph are propo-
sitional templates. A propositional template is a
path in a dependency tree between two arguments
of a common predicate1 (Lin and Pantel, 2001;
Szpektor and Dagan, 2008). Note that in a de-
pendency parse, such a path passes through the
predicate. We require that a variable appears in at
least one of the argument positions, and that each
sense of a polysemous predicate corresponds to a
separate template (and a separate graph node): X
subj
??? treat#1
obj
??? Y and X
subj
??? treat#1
obj
??? nau-
sea are propositional templates for the first sense
of the predicate treat. An edge (u, v) represents
the fact that template u entails template v. Note
that the entailment relation transcends beyond hy-
ponymy. For example, the template X is diagnosed
with asthma entails the template X suffers from
asthma, although one is not a hyponoym of the
other. An example of an entailment graph is given
in Figure 1, left.
Since entailment is a transitive relation, an en-
tailment graph is transitive, i.e., if the edges (u, v)
and (v, w) are in the graph, so is the edge (u,w).
This is why we require that nodes be sense-
specified, as otherwise transitivity does not hold:
Possibly a ? b for one sense of b, b ? c for an-
other sense of b, but a9 c.
Because graph nodes represent propositions,
which generally have a clear truth value, we can
assume that transitivity is indeed maintained along
paths of any length in an entailment graph, as en-
tailment between each pair of nodes either occurs
or doesn?t occur with very high probability. We
support this further in section 4.1, where we show
1We restrict our discussion to templates with two argu-
ments, but generalization is straightforward.
1221
X-related-to-nausea X-associated-with-nauseaX-prevent-nausea X-help-with-nauseaX-reduce-nausea X-treat-nausea
related to nauseaheadacheOxicontine
help with nausea
prevent nausea
acupuncture
ginger
reduce nausearelaxationtreat nauseadrugsNabiloneLorazepam
Figure 1: Left: An entailment graph. For clarity, edges that can be inferred by transitivity are omitted. Right: A hierarchical
summary of propositions involving nausea as an argument, such as headache is related to nausea, acupuncture helps with
nausea, and Lorazepam treats nausea.
that in our experimental setting the length of paths
in the entailment graph is relatively small.
Transitivity implies that in each strong connec-
tivity component2 of the graph, all nodes are syn-
onymous. Moreover, if we merge every strong
connectivity component to a single node, the
graph becomes a Directed Acyclic Graph (DAG),
and the graph nodes can be sorted and presented
hierarchically. Next, we show an application that
leverages this property.
4 Motivating Application
In this section we propose an application that pro-
vides a hierarchical view of propositions extracted
from a corpus, based on an entailment graph.
Organizing information in large collections has
been found to be useful for effective information
access (Kaki, 2005; Stoica et al, 2007). It allows
for easier data exploration, and provides a compact
view of the underlying content. A simple form of
structural presentation is by a single hierarchy, e.g.
(Hofmann, 1999). A more complex approach is
hierarchical faceted metadata, where a number of
concept hierarchies are created, corresponding to
different facets or dimensions (Stoica et al, 2007).
Hierarchical faceted metadata categorizes con-
cepts of a domain in several dimensions, but does
not specify the relations between them. For ex-
ample, in the health-care domain we might have
facets for categories such as diseases and symp-
toms. Thus, when querying about nausea, one
might find it is related to vomitting and chicken
pox, but not that chicken pox is a cause of nausea,
2A strong connectivity component is a subset of nodes in
the graph where there is a path from any node to any other
node.
while nausea is often accompanied by vomitting.
We suggest that the prominent information
in a text lies in the propositions it contains,
which specify particular relations between the
concepts. Propositions have been mostly pre-
sented through unstructured textual summaries or
manually-constructed ontologies, which are ex-
pensive to build. We propose using the entail-
ment graph structure, which describes entailment
relations between predicates, to naturally present
propositions hierarchically. That is, the entailment
hierarchy can be used as an additional facet, which
can improve navigation and provide a compact hi-
erarchical summary of the propositions.
Figure 1 illustrates a scenario, on which we
evaluate later our learning algorithm. Assume a
user would like to retrieve information about a tar-
get concept such as nausea. We can extract the set
of propositions where nausea is an argument auto-
matically from a corprus, and learn an entailment
graph over propositional templates derived from
the extracted propositions, as illustrated in Figure
1, left. Then, we follow the steps in the process
described in Section 3: merge synonymous nodes
that are in the same strong connectivity compo-
nent, and turn the resulting DAG into a predicate
hierarchy, which we can then use to present the
propositions (Figure 1, right). Note that in all
propositional templates one argument is the tar-
get concept (nausea), and the other is a variable
whose corpus instantiations can be presented ac-
cording to another hierarchy (e.g. Nabilone and
Lorazepam are types of drugs).
Moreover, new propositions are inferred from
the graph by transitivity. For example, from the
proposition ?relaxation reduces nausea? we can in-
1222
fer the proposition ?relaxation helps with nausea?.
4.1 Focused entailment graphs
The application presented above generates entail-
ment graphs of a specific form: (1) Propositional
templates have exactly one argument instantiated
by the same entity (e.g. nausea). (2) The predicate
sense is unspecified, but due to the rather small
number of nodes and the instantiating argument,
each predicate corresponds to a unique sense.
Generalizing this notion, we define a focused
entailment graph to be an entailment graph where
the number of nodes is relatively small (and con-
sequently paths in the graph are short), and predi-
cates have a single sense (so transitivity is main-
tained without sense specification). Section 5
presents an algorithm that given the set of nodes
of a focused entailment graph learns its edges, i.e.,
the entailment relations between all pairs of nodes.
The algorithm is evaluated in Section 6 using our
proposed application. For brevity, from now on
the term entailment graph will stand for focused
entailment graph.
5 Learning Entailment Graph Edges
In this section we present an algorithm for learn-
ing the edges of an entailment graph given its set
of nodes. The first step is preprocessing: We use
a large corpus and WordNet to train an entail-
ment classifier that estimates the likelihood that
one propositional template entails another. Next,
we can learn on the fly for any input graph: given
the graph nodes, we employ a global optimiza-
tion approach that determines the set of edges that
maximizes the probability (or score) of the entire
graph, given the edge probabilities (or scores) sup-
plied by the entailment classifier and the graph
constraints (transitivity and others).
5.1 Training an entailment classifier
We describe a procedure for learning an entail-
ment classifier, given a corpus and a lexicographic
resource (WordNet). First, we extract a large set of
propositional templates from the corpus. Next, we
represent each pair of propositional templates with
a feature vector of various distributional similar-
ity scores. Last, we use WordNet to automatically
generate a training set and train a classifier.
Template extraction We parse the corpus with
a dependency parser and extract all propositional
templates from every parse tree, employing the
procedure used by Lin and Pantel (2001). How-
ever, we only consider templates containing a
predicate term and arguments3. The arguments are
replaced with variables, resulting in propositional
templates such as X
subj
??? affect
obj
??? Y.
Distributional similarity representation We
aim to train a classifier that for an input template
pair (t1, t2) determines whether t1 entails t2. A
template pair is represented by a feature vector
where each coordinate is a different distributional
similarity score. There are a myriad of distribu-
tional similarity algorithms. We briefly describe
those used in this paper, obtained through varia-
tions along the following dimensions:
Predicate representation Most algorithms mea-
sure the similarity between templates with two
variables (binary templates) such as X
subj
??? af-
fect
obj
??? Y (Lin and Pantel, 2001; Bhagat et al,
2007; Yates and Etzioni, 2009). Szpketor and Da-
gan (2008) suggested learning over templates with
one variable (unary templates) such as X
subj
??? af-
fect, and using them to estimate a score for binary
templates.
Feature representation The features of a tem-
plate are some representation of the terms that in-
stantiated the argument variables in a corpus. Two
representations are used in our experiment (see
Section 6). Another variant occurs when using bi-
nary templates: a template may be represented by
a pair of feature vectors, one for each variable (Lin
and Pantel, 2001), or by a single vector, where fea-
tures represent pairs of instantiations (Szpektor et
al., 2004; Yates and Etzioni, 2009). The former
variant reduces sparsity problems, while Yates and
Etzioni showed the latter is more informative and
performs favorably on their data.
Similarity function We consider two similarity
functions: The Lin (2001) similarity measure, and
the Balanced Inclusion (BInc) similarity measure
(Szpektor and Dagan, 2008). The former is a
symmetric measure and the latter is asymmetric.
Therefore, information about the direction of en-
tailment is provided by the BInc measure.
We then generate for any (t1, t2) features that
are the 12 distributional similarity scores using all
combinations of the dimensions. This is reminis-
cent of Connor and Roth (2007), who used the out-
put of unsupervised classifiers as features for a su-
pervised classifier in a verb disambiguation task.
3Via a simple heuristic, omitted due to space limitations
1223
Training set generation Following the spirit of
Snow et al (2005), WordNet is used to automati-
cally generate a training set of positive (entailing)
and negative (non-entailing) template pairs. Let
T be the set of propositional templates extracted
from the corpus. For each ti ? T with two vari-
ables and a single predicate word w, we extract
from WordNet the set H of direct hypernyms and
synonyms of w. For every h ? H , we generate a
new template tj from ti by replacing w with h. If
tj ? T , we consider (ti, tj) to be a positive exam-
ple. Negative examples are generated analogously,
by looking at direct co-hyponyms of w instead of
hypernyms and synonyms. This follows the no-
tion of ?contrastive estimation? (Smith and Eisner,
2005), since we generate negative examples that
are semantically similar to positive examples and
thus focus the classifier?s attention on identifying
the boundary between the classes. Last, we filter
training examples for which all features are zero,
and sample an equal number of positive and neg-
ative examples (for which we compute similarity
features), since classifiers tend to perform poorly
on the minority class when trained on imbalanced
data (Van Hulse et al, 2007; Nikulin, 2008).
5.2 Global learning of edges
Once the entailment classifier is trained we learn
the graph edges given its nodes. This is equiv-
alent to learning all entailment relations between
all propositional template pairs for that graph.
To learn edges we consider global constraints,
which allow only certain graph topologies. Since
we seek a global solution under transitivity and
other constraints, linear programming is a natural
choice, enabling the use of state of the art opti-
mization packages. We describe two formulations
of integer linear programs that learn the edges: one
maximizing a global score function, and another
maximizing a global probability function.
Let Iuv be an indicator denoting the event that
node u entails node v. Our goal is to learn the
edges E over a set of nodes V . We start by formu-
lating the constraints and then the target functions.
The first constraint is that the graph must re-
spect transitivity. Our formulation is equivalent to
the one suggested by Finkel and Manning (2008)
in a coreference resolution task:
?u,v,w?V Iuv + Ivw ? Iuw ? 1
In addition, for a few pairs of nodes we have
strong evidence that one does not entail the other
and so we add the constraint Iuv = 0. Combined
with the constraint of transitivity this implies that
there must be no path from u to v. This is done in
the following two scenarios: (1) When two nodes
u and v are identical except for a pair of words wu
and wv, and wu is an antonym of wv, or a hyper-
nym of wv at distance ? 2. (2) When two nodes
u and v are transitive opposites, that is, if u =
X
subj
??? w
obj
??? Y and v = X
obj
??? w
subj
??? Y ,
for any word w4.
Score-based target function We assume an en-
tailment classifier estimating a positive score Suv
if it believes Iuv = 1 and a negative score other-
wise (for example, an SVM classifier). We look
for a graph G that maximizes the sum of scores
over the edges:
G? = argmax
G
S(G)
= argmax
G
?
?
?
u6=v
SuvIuv
?
?? ?|E|
where ?|E| is a regularization term reflecting
the fact that edges are sparse. Note that this con-
stant needs to be optimized on a development set.
Probabilistic target function Let Fuv be the
features for the pair of nodes (u, v) and F =
?u6=vFuv. We assume an entailment classifier es-
timating the probability of an edge given its fea-
tures: Puv = P (Iuv = 1|Fuv). We look for the
graph G that maximizes the posterior probability
P (G|F ):
G? = argmax
G
P (G|F )
Following Snow et al, we make two inde-
pendence assumptions: First, we assume each
set of features Fuv is independent of other sets
of features given the graph G, i.e., P (F |G) =
?
u6=v P (Fuv|G). Second, we assume the features
for the pair (u, v) are generated by a distribution
depending only on whether entailment holds for
(u, v). Thus, P (Fuv|G) = P (Fuv|Iuv). Last,
for simplicity we assume edges are independent
and the prior probability of a graph is a product
of the prior probabilities of the edge indicators:
4We note that in some rare cases transitive verbs are in-
deed reciprocal, as in ?X marry Y?, but in the grand ma-
jority of cases reciprocal activities are not expressed using
a transitive-verb structure.
1224
P (G) =
?
u6=v P (Iuv). Note that although we
assume edges are independent, dependency is still
expressed using the transitivity constraint. We ex-
press P (G|F ) using the assumptions above and
Bayes rule:
P (G|F ) ? P (G)P (F |G)
=
?
u6=v
[P (Iuv)P (Fuv|Iuv)]
=
?
u6=v
P (Iuv)
P (Iuv|Fuv)P (Fuv)
P (Iuv)
?
?
u6=v
P (Iuv|Fuv)
=
?
(u,v)?E
Puv ?
?
(u,v)/?E
(1? Puv)
Note that the prior P (Fuv) is constant with re-
spect to the graph. Now we look for the graph that
maximizes logP (G|F ):
G? = argmax
G
?
(u,v)?E
logPuv +
?
(u,v)/?E
log(1? Puv)
= argmax
G
?
u6=v
[Iuv ? logPuv
+ (1? Iuv) ? log(1? Puv)]
= argmax
G
?
u6=v
log
Puv
1? Puv
? Iuv
(in the last transition we omit the constant
?
u6=v log(1?Puv)). Importantly, while the score-
based formulation contains a parameter ? that re-
quires optimization, this probabilistic formulation
is parameter free and does not utilize a develop-
ment set at all.
Since the variables are binary, both formula-
tions are integer linear programs with O(|V |2)
variables and O(|V |3) transitivity constraints that
can be solved using standard ILP packages.
Our work resembles Snow et al?s in that both
try to learn graph edges given a transitivity con-
straint. However, there are two key differences
in the model and in the optimization algorithm.
First, Snow et al?s model attempts to determine
the graph that maximizes the likelihood P (F |G)
and not the posterior P (G|F ). Therefore, their
model contains an edge prior P (Iuv) that has to
be estimated, whereas in our model it cancels out.
Second, they incrementally add hyponyms to a
large taxonomy (WordNet) and therefore utilize a
greedy algorithm, while we simultaneously learn
all edges of a rather small graph and employ in-
teger linear programming, which is more sound
theoretically, and as shown in Section 6, leads to
an optimal solution. Nevertheless, Snow et al?s
model can also be formulated as a linear program
with the following target function:
argmax
G
?
u6=v
log
Puv ? P (Iuv = 0)
(1? Puv) ? P (Iuv = 1)
Iuv
Note that if the prior inverse odds k =
P (Iuv=0)
P (Iuv=1)
= 1, i.e., P (Iuv = 1) = 0.5, then
this is equivalent to our probabilistic formulation.
We implemented Snow et als model and optimiza-
tion algorithm and in Section 6.3 we compare our
model and optimization algorithm to theirs.
6 Experimental Evaluation
This section presents our evaluation, which is
geared for the application proposed in Section 4.
6.1 Experimental setting
A health-care corpus of 632MB was harvested
from the web and parsed with the Minipar parser
(Lin, 1998). The corpus contains 2,307,585
sentences and almost 50 million word tokens.
We used the Unified Medical Language System
(UMLS)5 to annotate medical concepts in the cor-
pus. The UMLS is a database that maps nat-
ural language phrases to over one million con-
cept identifiers in the health-care domain (termed
CUIs). We annotated all nouns and noun phrases
that are in the UMLS with their possibly multi-
ple CUIs. We extracted all propositional templates
from the corpus, where both argument instantia-
tions are medical concepts, i.e., annotated with a
CUI (?50,000 templates). When computing dis-
tributional similarity scores, a template is repre-
sented as a feature vector of the CUIs that instan-
tiate its arguments.
To evaluate the performance of our algo-
rithm, we constructed 23 gold standard entailment
graphs. First, 23 medical concepts, representing
typical topics of interest in the medical domain,
were manually selected from a list of the most fre-
quent concepts in the corpus. For each concept,
nodes were defined by extracting all propositional
5http://www.nlm.nih.gov/research/umls
1225
Using a development set Not using a development set
Edges Propositions Edges Propositions
R P F1 R P F1 R P F1 R P F1
LP 46.0 50.1 43.8 67.3 69.6 66.2 48.7 41.9 41.2 67.9 62.0 62.3
Greedy 45.7 37.1 36.6 64.2 57.2 56.3 48.2 41.7 41.0 67.8 62.0 62.4
Local-LP 44.5 45.3 38.1 65.2 61.0 58.6 69.3 19.7 26.8 82.7 33.3 42.6
Local1 53.5 34.9 37.5 73.5 50.6 56.1 92.9 11.1 19.7 95.4 18.6 30.6
Local2 52.5 31.6 37.7 69.8 50.0 57.1 63.2 24.9 33.6 77.7 39.3 50.5
Local?1 53.5 38.0 39.8 73.5 54.6 59.1 92.6 11.3 20.0 95.3 18.9 31.1
Local?2 52.5 32.1 38.1 69.8 50.6 57.4 63.1 25.5 34.0 77.7 39.9 50.9
WordNet - - - - - - 10.8 44.1 13.2 39.9 72.4 47.3
Table 1: Results for all experiments
templates for which the target concept instanti-
ated an argument at least K(= 3) times (average
number of graph nodes=22.04, std=3.66, max=26,
min=13).
Ten medical students constructed the gold stan-
dard of graph edges. Each concept graph was
annotated by two students. Following RTE-5
practice (Bentivogli et al, 2009), after initial an-
notation the two students met for a reconcili-
ation phase. They worked to reach an agree-
ment on differences and corrected their graphs.
Inter-annotator agreement was calculated using
the Kappa statistic (Siegel and Castellan, 1988)
both before (? = 0.59) and after (? = 0.9) rec-
onciliation. 882 edges were included in the 23
graphs out of a possible 10,364, providing a suf-
ficiently large data set. The graphs were randomly
split into a development set (11 graphs) and a test
set (12 graphs)6. The entailment graph fragment
in Figure 1 is from the gold standard.
The graphs learned by our algorithm were eval-
uated by two measures, one evaluating the graph
directly, and the other motivated by our applica-
tion: (1) F1 of the learned edges compared to the
gold standard edges (2) Our application provides
a summary of propositions extracted from the cor-
pus. Note that we infer new propositions by prop-
agating inference transitively through the graph.
Thus, we compute F1 for the set of propositions
inferred from the learned graph, compared to the
set inferred based on the gold standard graph. For
example, given the proposition from the corpus
?relaxation reduces nausea? and the edge ?X re-
duce nausea? X help with nausea?, we evaluate
the set {?relaxation reduces nausea?, ?relaxation
helps with nausea?}. The final score for an algo-
rithm is a macro-average over the 12 graphs of the
6Test set concepts were: asthma, chemotherapy, diarrhea,
FDA, headache, HPV, lungs, mouth, salmonella, seizure,
smoking and X-ray.
test set.
6.2 Evaluated algorithms
Local algorithms We described 12 distributional
similarity measures computed over our corpus
(Section 5.1). For each measure we computed for
each template t a list of templates most similar to
t (or entailing t for directional measures). In ad-
dition, we obtained similarity lists learned by Lin
and Pantel (2001), and replicated 3 similarity mea-
sures learned by Szpektor and Dagan (2008), over
the RCV1 corpus7. For each distributional similar-
ity measure (altogether 16 measures), we learned a
graph by inserting any edge (u, v), when u is in the
top K templates most similar to v. We also omit-
ted edges for which there was strong evidence that
they do not exist, as specified by the constraints
in Section 5.2. Another local resource was Word-
Net where we inserted an edge (u, v) when v was
a direct hypernym or synonym of u. For all algo-
rithms, we added all edges inferred by transitivity.
Global algorithms We experimented with all
6 combinations of the following two dimensions:
(1) Target functions: score-based, probabilistic
and Snow et al?s (2) Optimization algorithms:
Snow et al?s greedy algorithm and a standard ILP
solver. A training set of 20,144 examples was au-
tomatically generated, each example represented
by 16 features using the distributional similarity
measures mentioned above. SVMperf (Joachims,
2005) was used to train an SVM classifier yield-
ing Suv, and the SMO classifier from WEKA (Hall
et al, 2009) estimated Puv. We used the lpsolve8
package to solve the linear programs. In all re-
sults, the relaxation ?u,v0 ? Iuv ? 1 was used,
which guarantees an optimal output solution. In
7http://trec.nist.gov/data/reuters/reuters.html. The simi-
larity lists were computed using: (1) Unary templates and
the Lin function (2) Unary templates and the BInc function
(3) Binary templates and the Lin function
8http://lpsolve.sourceforge.net/5.5/
1226
Global=T/Local=F Global=F/Local=T
GS= T 50 143
GS= F 140 1087
Table 2: Comparing disagreements between the best local
and global algorithms against the gold standard
all experiments the output solution was integer,
and therefore it is optimal. Constructing graph
nodes and learning its edges given an input con-
cept took 2-3 seconds on a standard desktop.
6.3 Results and analysis
Table 1 summarizes the results of the algorithms.
The left half depicts methods where the develop-
ment set was needed to tune parameters, and the
right half depicts methods that do not require a
(manually created) development set at all. Hence,
our score-based LP (tuned-LP), where the param-
eter ? is tuned, is on the left, and the probabilis-
tic LP (untuned-LP) is on the right. The row
Greedy is achieved by using the greedy algorithm
instead of lpsolve. The row Local-LP is achieved
by omitting global transitivity constraints, making
the algorithm completely local. We omit Snow et
al.?s formulation, since the optimal prior inverse
odds k was almost exactly 1, which conflates with
untuned-LP.
The rows Local1 and Local2 present the best
distributional similarity resources. Local1 is
achieved using binary templates, the Lin function,
and a single vector with feature pairs. Local2 is
identical but employs the BInc function. Local?1
and Local?2 also exploit the local constraints men-
tioned above. Results on the left were achieved
by optimizing the top-K parameter on the devel-
opment set, and on the right by optimizing on the
training set automatically generated from Word-
Net.
The global methods clearly outperform local
methods: Tuned-LP outperforms significantly all
local methods that require a development set both
on the edges F1 measure (p<.05) and on the
propositions F1 measure (p<.01)9. The untuned-
LP algorithm also significantly outperforms all lo-
cal methods that do not require a development
set on the edges F1 measure (p<.05) and on
the propositions F1 measure (p<.01). Omitting
the global transitivity constraints decreases perfor-
mance, as shown by Local-LP. Last, local meth-
9We tested significance using the two-sided Wilcoxon
rank test (Wilcoxon, 1945)
Global
X-treat-headache
X-prevent-headache
X-reduce-headache
X-report-headache
X-suffer-from-headache
X-experience-headache
Figure 2: Subgraph of tuned-LP output for ?headache?
Global
X-treat-headache
X-prevent-headache
X-reduce-headache
X-report-headache
X-suffer-from-headache
X-experience-headache
Figure 3: Subgraph of Local?1 output for?headache?
ods are sensitive to parameter tuning and in the
absence of a development set their performance
dramatically deteriorates.
To further establish the merits of global algo-
rithms, we compare (Table 2) tuned-LP, the best
global algorithm, with Local?1, the best local al-
gorithm. The table considers all edges where the
two algorithms disagree, and counts how many
are in the gold standard and how many are not.
Clearly, tuned-LP is superior at avoiding wrong
edges (false positives). This is because tuned-
LP refrains from adding edges that subsequently
induce many undesirable edges through transitiv-
ity. Figures 2 and 3 illustrate this by compar-
ing tuned-LP and Local?1 on a subgraph of the
Headache concept, before adding missing edges
to satisfy transitivity to Local?1 . Note that Local
?
1
inserts a single wrong edge X-report-headache?
X-prevent-headache, which leads to adding 8 more
wrong edges. This is the type of global considera-
tion that is addressed in an ILP formulation, but is
ignored in a local approach and often overlooked
when employing a greedy algorithm. Figure 2 also
illustrates the utility of a local entailment graph for
information presentation. Presenting information
according to this subgraph distinguishes between
propositions dealing with headache treatments and
1227
propositions dealing with headache risk groups.
Comparing our use of an ILP algorithm to
the greedy one reveals that tuned-LP significantly
outperforms its greedy counterpart on both mea-
sures (p<.01). However, untuned-LP is practically
equivalent to its greedy counterpart. This indicates
that in this experiment the greedy algorithm pro-
vides a good approximation for the optimal solu-
tion achieved by our LP formulation.
Last, when comparing WordNet to local distri-
butional similarity methods, we observe low recall
and high precision, as expected. However, global
methods achieve much higher recall than WordNet
while maintaining comparable precision.
The results clearly demonstrate that a global ap-
proach improves performance on the entailment
graph learning task, and the overall advantage of
employing an ILP solver rather than a greedy al-
gorithm.
7 Conclusion
This paper presented a global optimization algo-
rithm for learning entailment relations between
predicates represented as propositional templates.
We modeled the problem as a graph learning prob-
lem, and searched for the best graph under a global
transitivity constraint. We used Integer Linear
Programming to solve the optimization problem,
which is theoretically sound, and demonstrated
empirically that this method outperforms local al-
gorithms as well as a greedy optimization algo-
rithm on the graph learning task.
Currently, we are investigating a generalization
of our probabilistic formulation that includes a
prior on the edges, and the relation of this prior
to the regularization term introduced in our score-
based formulation. In future work, we would like
to learn general entailment graphs over a large
number of nodes. This will introduce a challenge
to our current optimization algorithm due to com-
plexity issues, and will require careful handling of
predicate ambiguity. Additionally, we will inves-
tigate novel features for the entailment classifier.
This paper used distributional similarity, but other
sources of information are likely to improve per-
formance further.
Acknowledgments
We would like to thank Roy Bar-Haim, David
Carmel and the anonymous reviewers for their
useful comments. We also thank Dafna Berant
and the nine students who prepared the gold stan-
dard data set. This work was developed under
the collaboration of FBK-irst/University of Haifa
and was partially supported by the Israel Science
Foundation grant 1112/08. The first author is
grateful to the Azrieli Foundation for the award of
an Azrieli Fellowship, and has carried out this re-
search in partial fulllment of the requirements for
the Ph.D. degree.
References
Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo
Giampiccolo, and Bernarde Magnini. 2009. The
fifth Pascal recognizing textual entailment chal-
lenge. In Proceedings of TAC-09.
Rahul Bhagat, Patrick Pantel, and Eduard Hovy. 2007.
LEDIR: An unsupervised algorithm for learning di-
rectionality of inference rules. In Proceedings of
EMNLP-CoNLL.
Alexander Budanitsky and Graeme Hirst. 2006. Eval-
uating wordnet-based measures of lexical semantic
relatedness. Computational Linguistics, 32(1):13?
47.
James Clarke and Mirella Lapata. 2008. Global in-
ference for sentence compression: An integer linear
programming approach. Journal of Artificial Intelli-
gence Research, 31:273?381.
Michael Connor and Dan Roth. 2007. Context sensi-
tive paraphrasing with a single unsupervised classi-
fier. In Proceedings of ECML.
Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan
Roth. 2009. Recognizing textual entailment: Ratio-
nal, evaluation and approaches. Natural Language
Engineering, 15(4):1?17.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database (Language, Speech, and
Communication). The MIT Press.
Jenny Rose Finkel and Christopher D. Manning. 2008.
Enforcing transitivity in coreference resolution. In
Proceedings of ACL-08: HLT, Short Papers.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: An up-
date. SIGKDD Explorations, 11(1).
Thomas Hofmann. 1999. The cluster-abstraction
model: Unsupervised learning of topic hierarchies
from text data. In Proceedings of IJCAI.
Thorsten Joachims. 2005. A support vector method for
multivariate performance measures. In Proceedings
of ICML.
1228
Mika Kaki. 2005. Findex: Search results categories
help users when document ranking fails. In Pro-
ceedings of CHI.
Dekang Lin and Patrick Pantel. 2001. Discovery of in-
ference rules for question answering. Natural Lan-
guage Engineering, 7(4):343?360.
Dekang Lin. 1998. Dependency-based evaluation of
Minipar. In Proceedings of the Workshop on Evalu-
ation of Parsing Systems at LREC.
Andre Martins, Noah Smith, and Eric Xing. 2009.
Concise integer linear programming formulations
for dependency parsing. In Proceedings of ACL.
Vladimir Nikulin. 2008. Classification of imbalanced
data with random sets and mean-variance filtering.
IJDWM, 4(2):63?78.
Dan Roth and Wen tau Yih. 2005. Integer linear pro-
gramming inference for conditional random fields.
In Proceedings of ICML, pages 737?744.
Satoshi Sekine. 2005. Automatic paraphrase discovery
based on context and keywords between ne pairs. In
Proceedings of IWP.
Sideny Siegel and N. John Castellan. 1988. Non-
parametric Statistics for the Behavioral Sciences.
McGraw-Hill, New-York.
Noah Smith and Jason Eisner. 2005. Contrastive es-
timation: Training log-linear models on unlabeled
data. In Proceedings of ACL.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2005.
Learning syntactic patterns for automatic hypernym
discovery. In Proceedings of NIPS.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2006.
Semantic taxonomy induction from heterogenous
evidence. In Proceedings of ACL.
Emilia Stoica, Marti Hearst, and Megan Richardson.
2007. Automating creation of hierarchical faceted
metadata structures. In Proceedings of NAACL-
HLT.
Idan Szpektor and Ido Dagan. 2008. Learning entail-
ment rules for unary templates. In Proceedings of
COLING.
Idan Szpektor and Ido Dagan. 2009. Augmenting
wordnet-based inference with argument mapping.
In Proceedings of TextInfer-2009.
Idan Szpektor, Hristo Tanev, Ido Dagan, and Bonaven-
tura Coppola. 2004. Scaling web-based acquisition
of entailment relations. In Proceedings of EMNLP.
Jason Van Hulse, Taghi Khoshgoftaar, and Amri
Napolitano. 2007. Experimental perspectives on
learning from imbalanced data. In Proceedings of
ICML.
Frank Wilcoxon. 1945. Individual comparisons by
ranking methods. Biometrics Bulletin, 1:80?83.
Alexander Yates and Oren Etzioni. 2009. Unsuper-
vised methods for determining object and relation
synonyms on the web. Journal of Artificial Intelli-
gence Research, 34:255?296.
1229
Proceedings of the ACL 2010 Conference Short Papers, pages 241?246,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Generating Entailment Rules from FrameNet
Roni Ben Aharon
Department of Computer Science
Bar-Ilan University
Ramat Gan, Israel
r.ben.aharon@gmail.com
Idan Szpektor
Yahoo! Research
Haifa, Israel
idan@yahoo-inc.com
Ido Dagan
Department of Computer Science
Bar-Ilan University
Ramat Gan, Israel
dagan@cs.biu.ac.il
Abstract
Many NLP tasks need accurate knowl-
edge for semantic inference. To this end,
mostly WordNet is utilized. Yet Word-
Net is limited, especially for inference be-
tween predicates. To help filling this gap,
we present an algorithm that generates
inference rules between predicates from
FrameNet. Our experiment shows that the
novel resource is effective and comple-
ments WordNet in terms of rule coverage.
1 Introduction
Many text understanding applications, such as
Question Answering (QA) and Information Ex-
traction (IE), need to infer a target textual mean-
ing from other texts. This need was proposed as a
generic semantic inference task under the Textual
Entailment (TE) paradigm (Dagan et al, 2006).
A fundamental component in semantic infer-
ence is the utilization of knowledge resources.
However, a major obstacle to improving semantic
inference performance is the lack of such knowl-
edge (Bar-Haim et al, 2006; Giampiccolo et al,
2007). We address one prominent type of infer-
ence knowledge known as entailment rules, focus-
ing specifically on rules between predicates, such
as ?cure X ? X recover?.
We aim at highly accurate rule acquisition,
for which utilizing manually constructed sources
seem appropriate. The most widely used manual
resource is WordNet (Fellbaum, 1998). Yet it is in-
complete for generating entailment rules between
predicates (Section 2.1). Hence, other manual re-
sources should also be targeted.
In this work1, we explore how FrameNet
(Baker et al, 1998) could be effectively used for
generating entailment rules between predicates.
1The detailed description of our work can be found in
(Ben Aharon, 2010).
FrameNet is a manually constructed database
based on Frame Semantics. It models the semantic
argument structure of predicates in terms of proto-
typical situations called frames.
Prior work utilized FrameNet?s argument map-
ping capabilities but took entailment relations
from other resources, namely WordNet. We
propose a novel method for generating entail-
ment rules from FrameNet by detecting the entail-
ment relations implied in FrameNet. We utilize
FrameNet?s annotated sentences and relations be-
tween frames to extract both the entailment rela-
tions and their argument mappings.
Our analysis shows that the rules generated by
our algorithm have a reasonable ?per-rule? accu-
racy of about 70%2. We tested the generated rule-
set on an entailment testbed derived from an IE
benchmark and compared it both to WordNet and
to state-of-the-art rule generation from FrameNet.
Our experiment shows that our method outper-
forms prior work. In addition, our rule-set?s per-
formance is comparable to WordNet and it is com-
plementary to WordNet when uniting the two re-
sources. Finally, additional analysis shows that
our rule-set accuracy is 90% in practical use.
2 Background
2.1 Entailment Rules and their Acquisition
To generate entailment rules, two issues should
be addressed: a) identifying the lexical entailment
relations between predicates, e.g. ?cure ? re-
cover?; b) mapping argument positions, e.g. ?cure
X ? X recover?. The main approach for gener-
ating highly accurate rule-sets is to use manually
constructed resources. To this end, most systems
mainly utilize WordNet (Fellbaum, 1998), being
the most prominent lexical resource with broad
coverage of predicates. Furthermore, some of its
2The rule-set is available at: http://www.cs.biu.
ac.il/?nlp/downloads
241
relations capture types of entailment relations, in-
cluding synonymy, hypernymy, morphologically-
derived, entailment and cause.
Yet, WordNet is limited for entailment rule gen-
eration. First, many entailment relations, no-
tably for the WordNet entailment and cause re-
lation types, are missing, e.g. ?elect ? vote?.
Furthermore, WordNet does not include argument
mapping between related predicates. Thus, only
substitutable WordNet relations (synonymy and
hypernymy), for which argument positions are
preserved, could be used to generate entailment
rules. The other non-substitutable relations, e.g.
cause (?kill ? die?) and morphologically-derived
(?meet.v? meeting.n?), cannot be used.
2.2 FrameNet
FrameNet (Baker et al, 1998) is a knowledge-
base of frames, describing prototypical situations.
Frames can be related to each other by inter-frame
relations, e.g. Inheritance, Precedence, Usage and
Perspective.
For each frame, several semantic roles are spec-
ified, called frame elements (FEs), denoting the
participants in the situation described. Each FE
may be labeled as core if it is central to the frame.
For example, some core FEs of the Commerce pay
frame are Buyer and Goods, while a non-core FE
is Place. Each FE may also be labeled with a se-
mantic type, e.g. Sentient, Event, and Time.
A frame includes a list of predicates that can
evoke the described situation, called lexical units
(LUs). LUs are mainly verbs but may also be
nouns or adjectives. For example, the frame Com-
merce pay lists the LUs pay.v and payment.n.
Finally, FrameNet contains annotated sentences
that represent typical LU occurrences in texts.
Each annotation refers to one LU in a specific
frame and the FEs of the frame that occur in the
sentence. An example sentence is ?IBuyer have to
pay the billsMoney?. Each sentence is accompa-
nied by a valence pattern, which provides, among
other info, grammatical functions of the core FEs
with respect to the LU. The valence pattern of the
above sentence is [(Buyer Subj), (Money Obj)].
2.3 Using FrameNet for Semantic Inference
To the best of our knowledge, the only work that
utilized FrameNet for entailment rule generation
is LexPar (Coyne and Rambow, 2009). LexPar
first identifies lexical entailment relations by go-
ing over all LU pairs which are either in the
same frame or whose frames are related by one of
FrameNet?s inter-frame relations. Each candidate
pair is considered entailing if the two LUs are ei-
ther synonyms or in a direct hypernymy relation in
WordNet (providing the vast majority of LexPar?s
relations), or if their related frames are connected
via the Perspective relation in FrameNet.
Then, argument mappings between each entail-
ing LU pair are extracted based on the core FEs
that are shared between the two LUs. The syntac-
tic positions of the shared FEs are taken from the
valence patterns of the LUs. A LexPar rule exam-
ple is presented in Figure 3 (top part).
Since most of LexPar?s entailment relations
are based on WordNet?s relations, LexPar?s rules
could be viewed as an intersection of WordNet and
FrameNet lexical relations, accompanied with ar-
gument mappings taken from FrameNet.
3 Rule Extraction from FrameNet
The above prior work identified lexical entailment
relations mainly from WordNet, which limits the
use of FrameNet in two ways. First, some rela-
tions that appear in FrameNet are missed because
they do not appear in WordNet. Second, unlike
FrameNet, WordNet does not include argument
mappings for its relations. Thus, prior work for
rule generation considered only substitutable rela-
tions from WordNet (synonyms and hypernyms),
not utilizing FrameNet?s capability to map argu-
ments of non-substitutable relations.
Our goal in this paper is to generate entail-
ment rules solely from the information within
FrameNet. We present a novel algorithm for gen-
erating entailment rules from FrameNet, called
FRED (FrameNet Entailment-rule Derivation),
which operates in three steps: a) extracting tem-
plates for each LU; b) detecting lexical entailment
relations between pairs of LUs; c) generating en-
tailment rules by mapping the arguments between
two LUs in each entailing pair.
3.1 Template Extraction
Many LUs in FrameNet are accompanied by an-
notated sentences (Section 2.2). From each sen-
tence of a given LU, we extract one template for
each annotated FE in the sentence. Each tem-
plate includes the LU, one argument correspond-
ing to the target FE and their syntactic relation
in the sentence parse-tree. We focus on extract-
ing unary templates, as they can describe any ar-
242
Figure 1: Template extraction for a sentence con-
taining the LU ?arrest?.
gument mapping by decomposing templates with
several arguments into unary ones (Szpektor and
Dagan, 2008). Figure 1 exemplifies this process.
As a pre-parsing step, all FE phrases in a given
sentence are replaced by their related FE names,
excluding syntactic information such as preposi-
tions or possessives (step (b) in Figure 1). Then,
the sentence is parsed using the Minipar depen-
dency parser (Lin, 1998) (step (c)). Finally, a
path in the parse-tree is extracted between each FE
node and the node of the LU (step (d)). Each ex-
tracted path is converted into a template by replac-
ing the FE node with an argument variable.
We simplify each extracted path by removing
nodes along the path that are not part of the syn-
tactic relation between the LU and the FE, such
as conjunctions and other FE nodes. For example,
?Authorities
subj
?? enter
conj
?? arrest? is simplified
into ?Authorities
subj
?? arrest?.
Some templates originated from different anno-
tated sentences share the same LU and syntactic
structure, but differ in their FEs. Usually, one of
these templates is incorrect, due to erroneous parse
(e.g. ?Suspect
obj
?? arrest? is a correct template, in
contrast to ?Charges
obj
?? arrest?). We thus keep
only the most frequently annotated template out of
the identical templates, assuming it is the correct
one.
3.2 Identifying Lexical Entailment Relations
FrameNet groups LUs in frames and describes re-
lations between frames. However, relations be-
tween LUs are not explicitly defined. We next de-
scribe how we automatically extract several types
of lexical entailment relations between LUs using
two approaches.
In the first approach, LUs in the same frame
that are morphological derivations of each other,
e.g. ?negotiation.n? and ?negotiate.v?, are marked
as paraphrases. We take morphological derivation
information from the CATVAR database (Habash
and Dorr, 2003).
The second approach is based on our observa-
tion that some LUs express the prototypical situ-
ation that their frame describes, which we denote
dominant LUs. For example, the LU ?recover? is
dominant for the Recovery frame. We mark LUs
as dominant if they are morphologically derived
from the frame?s name.
Our assumption is that since dominant LUs ex-
press the frame?s generic meaning, their meaning
is likely to be entailed by the other LUs in this
frame. Consequently, we generate such lexical
rules between any dominant LU and any other LU
in a given frame, e.g. ?heal? recover? and ?con-
valescence? recover? for the Recovery frame.
In addition, we assume that if two frames are
related by some type of entailment relation, their
dominant LUs are also related by the same rela-
tion. Accordingly, we extract entailment relations
between dominant LUs of frames that are con-
nected via the Inheritance, Cause and Perspective
relations, where Inheritance and Cause generate
directional entailment relations (e.g. ?choose ?
decide? and ?cure? recover?, respectively) while
Perspective generates bidirectional paraphrase re-
lations (e.g. ?transfer? receive?).
Finally, we generate the transitive closure of
the set of lexical relations identified by the above
methods. For example, the combination of ?sell?
buy? and ?buy? get? generates ?sell? get?.
3.3 Generating Entailment Rules
The final step in the FRED algorithm generates
lexical syntactic entailment rules from the ex-
tracted templates and lexical entailment relations.
For each identified lexical relation ?left? right?
between two LUs, the set of FEs that are shared by
both LUs is collected. Then, for each shared FE,
we take the list of templates that connect this FE
243
Lexical Relation:
cure? recovery
Templates:
Patient
obj
?? cure (cure Patient)
Affliction
of
?? cure (cure of Affliction)
Patient
gen
?? recovery (Patient?s recovery)
Patient
of
?? recovery (recovery of Patient)
Affliction
from
?? recovery (recovery from Affliction)
Intra-LU Entailment Rules:
Patient
gen
?? recovery?? Patient
of
?? recovery
Inter-LU Entailment Rules:
Patient
obj
?? cure =? Patient
gen
?? recovery
Patient
obj
?? cure =? Patient
of
?? recovery
Affliction
of
?? cure =? Affliction
from
?? recovery
Figure 2: Some entailment rules generated for the
lexical relation ?cure.v? recovery.n?.
Configuration R (%) P (%) F1
No-Rules 13.8 57.7 20.9
LexPar 14.1 42.9 17.4
WordNet 18.3 32.2 17.8
FRED 17.6 55.1 24.6
FRED ?WordNet 21.8 33.3 20.9
Table 1: Macro average Recall (R), Precision (P)
and F1 results for the tested configurations.
to each of the LUs, denoted by T feleft and T
fe
right.
Finally, for each template pair, l ? T feleft and r ?
T feright, the rule ?l ? r? is generated. In addition,
we generate paraphrase rules between the various
templates including the same FE and the same LU.
Figure 2 illustrates this process.
To improve rule quality, we filter out rules that
map FEs of adjunct-like semantic types, such as
Time and Location, since different templates of
such FEs may have different semantic meanings
(e.g. ?Time
before
?? arrive? ?Time
after
?? arrive?).
Thus, it is hard to identify those template pairs that
correctly map these FEs for entailment.
We manually evaluated a random sample of 250
rules from the resulting rule-set, out of which we
judged 69% as correct.
4 Application-based Evaluation
4.1 Experimental Setup
We would like to evaluate the overall utility of our
resource for NLP applications, assessing the cor-
rectness of the actual rule applications performed
in practice, as well as to compare its performance
to related resources. To this end, we follow the ex-
perimental setup presented in (Szpektor and Da-
gan, 2009), which utilized the ACE 2005 event
dataset3 as a testbed for entailment rule-sets. We
briefly describe this setup here.
The task is to extract argument mentions for
26 events, such as Sue and Attack, from the ACE
annotated corpus, using a given tested entailment
rule-set. Each event is represented by a set of
unary seed templates, one for each event argu-
ment. Some seed templates for Attack are ?At-
tacker
subj
??attack? and ?attack
obj
??Target?.
Argument mentions are found in the ACE cor-
pus by matching either the seed templates or tem-
plates entailing them found in the tested rule-set.
We manually added for each event its relevant
WordNet synset-ids and FrameNet frame-ids, so
only rules fitting the event target meaning will be
extracted from the tested rule-sets.
4.2 Tested Configurations
We evaluated several rule-set configurations:
No-Rules The system matches only the seed
templates directly, without any additional rules.
WordNet Rules are generated from WordNet
3.0, using only the synonymy and hypernymy rela-
tions (see Section 2.1). Transitive chaining of re-
lations is allowed (Moldovan and Novischi, 2002).
LexPar Rules are generated from the publicly
available LexPar database. We generated unary
rules from each LexPar rule based on a manually
constructed mapping from FrameNet grammatical
functions to Minipar dependency relations. Fig-
ure 3 presents an example of this procedure.
FRED Rules are generated by our algorithm.
FRED ?WordNet The union of the rule-sets of
FRED and WordNet.
4.3 Results
Each configuration was tested on each ACE event.
We measured recall, precision and F1. Table 1
reports macro averages of the three measures over
the 26 ACE events.
As expected, using No-Rules achieves the high-
est precision and the lowest recall compared to all
other configurations. When adding LexPar rules,
3http://projects.ldc.upenn.edu/ace/
244
LexPar rule:
Lexemes: arrest ?? apprehend
Valencies: [(Authorities Subj), (Suspect Obj), (Offense (for))] =? [(Authorities Subj), (Suspect Obj), (Offense (in))]
Generated unary rules:
X
subj
?? arrest =? X
subj
?? apprehend , arrest
obj
?? Y =? apprehend
obj
?? Y , arrest
for
?? Z =? apprehend
in
?? Z
Figure 3: An example for generation of unary entailment rules from a LexPar rule.
only a slight increase in recall is gained. This
shows that the subset of WordNet rules captured
by LexPar (Section 2.3) might be too small for the
ACE application setting.
When using all WordNet?s substitutable rela-
tions, a substantial relative increase in recall is
achieved (32%). Yet, precision decreases dramat-
ically (relative decrease of 44%), causing an over-
all decrease in F1. Most errors are due to correct
WordNet rules whose LHS is ambiguous. Since
we do not apply a WSD module, these rules are
also incorrectly applied to other senses of the LHS.
While this phenomenon is common to all rule-sets,
WordNet suffers from it the most since it contains
many infrequent word senses.
Our main result is that using FRED?s rule-set,
recall increases significantly, a relative increase
of 27% compared to No-Rules, while precision
hardly decreases. Hence, overall F1 is the high-
est compared to all other configurations (a rela-
tive increase of 17% compared to No-Rules). The
improvement in F1 is statistically significant com-
pared to all other configurations, according to the
two-sided Wilcoxon signed rank test at the level of
0.01 (Wilcoxon, 1945).
FRED preforms significantly better than LexPar
in both recall, precision and F1 (a relative increase
of 25%, 28% and 41% respectively). For example,
LexPar hardly utilizes FrameNet?s argument map-
ping capabilities since most of its rules are based
on a sub-set of WordNet?s substitutable relations.
FRED?s precision is substantially higher than
WordNet. This mostly results from the fact
that FrameNet mainly contains common senses
of predicates while WordNet includes many rare
word senses; which, as said above, harms preci-
sion when WSD is not applied. Error analysis
showed that only 7.5% of incorrect extractions are
due to erronous rules in FRED, while the majority
of errors are due to sense mismatch or syntactic
matching errors of the seed templates ot entailing
templates in texts.
FRED?s Recall is somewhat lower than Word-
Net, since FrameNet is a much smaller resource.
Yet, its rules are mostly complementary to those
from WordNet. This added value is demon-
strated by the 19% recall increase for the union of
FRED and WordNet rule-sets compared to Word-
Net alne. FRED provides mainly argument map-
pings for non-substitutable WordNet relations, e.g.
?attack.n on X ? attack.v X?, but also lexical re-
lations that are missing from WordNet, e.g. ?am-
bush.v? attack.v?.
Overall, our experiment shows that the rule-
base generated by FRED seems an appropri-
ate complementary resource to the widely used
WordNet-based rules in semantic inference and
expansion over predicates. This suggestion is es-
pecially appealing since our rule-set performs well
even when a WSD module is not applied.
5 Conclusions
We presented FRED, a novel algorithm for gener-
ating entailment rules solely from the information
contained in FrameNet. Our experiment showed
that FRED?s rules perform substantially better
than LexPar, the only prior rule-set derived from
FrameNet. In addition, FRED?s rule-set largely
complements the rules generated from WordNet
because it contains argument mappings between
non-substitutable predicates, which are missing
from WordNet, as well as lexical relations that are
not included in WordNet.
In future work we plan to investigate combin-
ing FrameNet and WordNet rule-sets in a transitive
manner, instead of their simple union.
Acknowledgments
This work was partially supported by the Rec-
tor?s research grant of Bar-Ilan University, the
PASCAL-2 Network of Excellence of the Eu-
ropean Community FP7-ICT-2007-1-216886 and
the Israel Science Foundation grant 1112/08.
245
References
Collin Baker, Charles Fillmore, and John Lowe. 1998.
The berkeley framenet project. In Proceedings of
COLING-ACL, Montreal, Canada.
Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro,
Danilo Giampiccolo, Bernardo Magnini, and Idan
Szpektor. 2006. The second pascal recognising tex-
tual entailment challenge. In Second PASCAL Chal-
lenge Workshop for Recognizing Textual Entailment.
Roni Ben Aharon. 2010. Generating entailment rules
from framenet. Master?s thesis, Bar-Ilan University.
Robert Coyne and Owen Rambow. 2009. Lexpar: A
freely available english paraphrase lexicon automat-
ically extracted from framenet. In Proceedings of
the Third IEEE International Conference on Seman-
tic Computing.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The pascal recognising textual entailment
challenge. In Lecture Notes in Computer Science,
volume 3944, pages 177?190.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge,
Massachusetts.
Danilo Giampiccolo, Bernardo Magnini, Ido Dagan,
and Bill Dolan. 2007. The third pascal recogniz-
ing textual entailment challenge. In Proceedings of
the ACL-PASCAL Workshop on Textual Entailment
and Paraphrasing.
Nizar Habash and Bonnie Dorr. 2003. A categorial
variation database for english. In Proceedings of
the North American Association for Computational
Linguistics (NAACL ?03), pages 96?102, Edmonton,
Canada. Association for Computational Linguistics.
Dekang Lin. 1998. Dependency-based evaluation of
minipar. In Proceedings of the Workshop on Evalu-
ation of Parsing Systems at LREC.
Dan Moldovan and Adrian Novischi. 2002. Lexical
chains for question answering. In Proceedings of
COLING.
Idan Szpektor and Ido Dagan. 2008. Learning en-
tailment rules for unary templates. In Proceedings
of the 22nd International Conference on Compu-
tational Linguistics (Coling 2008), pages 849?856,
Manchester, UK, August.
Idan Szpektor and Ido Dagan. 2009. Augmenting
wordnet-based inference with argument mapping.
In Proceedings of the 2009 Workshop on Applied
Textual Inference, pages 27?35, Suntec, Singapore,
August.
Frank Wilcoxon. 1945. Individual comparisons by
ranking methods. Biometrics Bulletin, 1(6):80?83.
246
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 610?619,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Global Learning of Typed Entailment Rules
Jonathan Berant
Tel Aviv University
Tel Aviv, Israel
jonatha6@post.tau.ac.il
Ido Dagan
Bar-Ilan University
Ramat-Gan, Israel
dagan@cs.biu.ac.il
Jacob Goldberger
Bar-Ilan University
Ramat-Gan, Israel
goldbej@eng.biu.ac.il
Abstract
Extensive knowledge bases of entailment rules
between predicates are crucial for applied se-
mantic inference. In this paper we propose an
algorithm that utilizes transitivity constraints
to learn a globally-optimal set of entailment
rules for typed predicates. We model the task
as a graph learning problem and suggest meth-
ods that scale the algorithm to larger graphs.
We apply the algorithm over a large data set
of extracted predicate instances, from which a
resource of typed entailment rules has been re-
cently released (Schoenmackers et al, 2010).
Our results show that using global transitiv-
ity information substantially improves perfor-
mance over this resource and several base-
lines, and that our scaling methods allow us
to increase the scope of global learning of
entailment-rule graphs.
1 Introduction
Generic approaches for applied semantic infer-
ence from text gained growing attention in recent
years, particularly under the Textual Entailment
(TE) framework (Dagan et al, 2009). TE is a
generic paradigm for semantic inference, where the
objective is to recognize whether a target meaning
can be inferred from a given text. A crucial com-
ponent of inference systems is extensive resources
of entailment rules, also known as inference rules,
i.e., rules that specify a directional inference rela-
tion between fragments of text. One important type
of rule is rules that specify entailment relations be-
tween predicates and their arguments. For example,
the rule ?X annex Y? X control Y? helps recognize
that the text ?Japan annexed Okinawa? answers the
question ?Which country controls Okinawa??. Thus,
acquisition of such knowledge received considerable
attention in the last decade (Lin and Pantel, 2001;
Sekine, 2005; Szpektor and Dagan, 2009; Schoen-
mackers et al, 2010).
Most past work took a ?local learning? approach,
learning each entailment rule independently of oth-
ers. It is clear though, that there are global inter-
actions between predicates. Notably, entailment is
a transitive relation and so the rules A ? B and
B ? C imply A? C.
Recently, Berant et al (2010) proposed a global
graph optimization procedure that uses Integer Lin-
ear Programming (ILP) to find the best set of entail-
ment rules under a transitivity constraint. Imposing
this constraint raised two challenges. The first of
ambiguity: transitivity does not always hold when
predicates are ambiguous, e.g., X buy Y? X acquire
Y and X acquire Y ? X learn Y, but X buy Y 9 X
learn Y since these two rules correspond to two dif-
ferent senses of acquire. The second challenge is
scalability: ILP solvers do not scale well since ILP
is an NP-complete problem. Berant et al circum-
vented these issues by learning rules where one of
the predicate?s arguments is instantiated (e.g., ?X re-
duce nausea? X affect nausea?), which is useful for
learning small graphs on-the-fly, given a target con-
cept such as nausea. While rules may be effectively
learned when needed, their scope is narrow and they
are not useful as a generic knowledge resource.
This paper aims to take global rule learning one
step further. To this end, we adopt the represen-
tation suggested by Schoenmackers et al (2010),
who learned inference rules between typed predi-
cates, i.e., predicates where the argument types (e.g.,
city or drug) are specified. Schoenmackers et al uti-
610
lized typed predicates since they were dealing with
noisy and ambiguous web text. Typing predicates
helps disambiguation and filtering of noise, while
still maintaining rules of wide-applicability. Their
method employs a local learning approach, while the
number of predicates in their data is too large to be
handled directly by an ILP solver.
In this paper we suggest applying global opti-
mization learning to open domain typed entailment
rules. To that end, we show how to construct a
structure termed typed entailment graph, where the
nodes are typed predicates and the edges represent
entailment rules. We suggest scaling techniques that
allow to optimally learn such graphs over a large
set of typed predicates by first decomposing nodes
into components and then applying incremental ILP
(Riedel and Clarke, 2006). Using these techniques,
the obtained algorithm is guaranteed to return an op-
timal solution. We ran our algorithm over the data
set of Schoenmackers et al and release a resource
of 30,000 rules1 that achieves substantially higher
recall without harming precision. To the best of our
knowledge, this is the first resource of that scale
to use global optimization for learning predicative
entailment rules. Our evaluation shows that global
transitivity improves the F1 score of rule learning by
27% over several baselines and that our scaling tech-
niques allow dealing with larger graphs, resulting in
improved coverage.
2 Background
Most work on learning entailment rules between
predicates considered each rule independently of
others, using two sources of information: lexico-
graphic resources and distributional similarity.
Lexicographic resources are manually-prepared
knowledge bases containing semantic information
on predicates. A widely-used resource is WordNet
(Fellbaum, 1998), where relations such as synonymy
and hyponymy can be used to generate rules. Other
resources include NomLex (Macleod et al, 1998;
Szpektor and Dagan, 2009) and FrameNet (Baker
and Lowe, 1998; Ben Aharon et al, 2010).
Lexicographic resources are accurate but have
1The resource can be downloaded from
http://www.cs.tau.ac.il/j?onatha6/homepage files/resources
/ACL2011Resource.zip
low coverage. Distributional similarity algorithms
use large corpora to learn broader resources by as-
suming that semantically similar predicates appear
with similar arguments. These algorithms usually
represent a predicate with one or more vectors and
use some function to compute argument similarity.
Distributional similarity algorithms differ in their
feature representation: Some use a binary repre-
sentation: each predicate is represented by one fea-
ture vector where each feature is a pair of argu-
ments (Szpektor et al, 2004; Yates and Etzioni,
2009). This representation performs well, but suf-
fers when data is sparse. The binary-DIRT repre-
sentation deals with sparsity by representing a pred-
icate with a pair of vectors, one for each argument
(Lin and Pantel, 2001). Last, a richer form of repre-
sentation, termed unary, has been suggested where
a different predicate is defined for each argument
(Szpektor and Dagan, 2008). Different algorithms
also differ in their similarity function. Some employ
symmetric functions, geared towards paraphrasing
(bi-directional entailment), while others choose di-
rectional measures more suited for entailment (Bha-
gat et al, 2007). In this paper, We employ several
such functions, such as Lin (Lin and Pantel, 2001),
and BInc (Szpektor and Dagan, 2008).
Schoenmackers et al (2010) recently used dis-
tributional similarity to learn rules between typed
predicates, where the left-hand-side of the rule may
contain more than a single predicate (horn clauses).
In their work, they used Hearst-patterns (Hearst,
1992) to extract a set of 29 million (argument, type)
pairs from a large web crawl. Then, they employed
several filtering methods to clean this set and au-
tomatically produced a mapping of 1.1 million ar-
guments into 156 types. Examples for (argument,
type) pairs are (EXODUS, book), (CHINA, coun-
try) and (ASTHMA, disease). Schoenmackers et
al. then utilized the types, the mapped arguments
and tuples from TextRunner (Banko et al, 2007)
to generate 10,672 typed predicates (such as con-
quer(country,city) and common in(disease,place)),
and learn 30,000 rules between these predicates2. In
this paper we will learn entailment rules over the
same data set, which was generously provided by
2The rules and the mapping of arguments into types can
be downloaded from http://www.cs.washington.edu/research/
sherlock-hornclauses/
611
Schoenmackers et al
As mentioned above, Berant et al (2010) used
global transitivity information to learn small entail-
ment graphs. Transitivity was also used as an in-
formation source in other fields of NLP: Taxonomy
Induction (Snow et al, 2006), Co-reference Reso-
lution (Finkel and Manning, 2008), Temporal Infor-
mation Extraction (Ling and Weld, 2010), and Un-
supervised Ontology Induction (Poon and Domin-
gos, 2010). Our proposed algorithm applies to any
sparse transitive relation, and so might be applicable
in these fields as well.
Last, we formulate our optimization problem as
an Integer Linear Program (ILP). ILP is an optimiza-
tion problem where a linear objective function over
a set of integer variables is maximized under a set of
linear constraints. Scaling ILP is challenging since
it is an NP-complete problem. ILP has been exten-
sively used in NLP lately (Clarke and Lapata, 2008;
Martins et al, 2009; Do and Roth, 2010).
3 Typed Entailment Graphs
Given a set of typed predicates, entailment rules can
only exist between predicates that share the same
(unordered) pair of types (such as place and coun-
try)3. Hence, every pair of types defines a graph
that describes the entailment relations between pred-
icates sharing those types (Figure 1). Next, we show
how to represent entailment rules between typed
predicates in a structure termed typed entailment
graph, which will be the learning goal of our algo-
rithm.
A typed entailment graph is a directed graph
where the nodes are typed predicates. A typed pred-
icate is a triple p(t1, t2) representing a predicate in
natural language. p is the lexical realization of the
predicate and the types t1, t2 are variables repre-
senting argument types. These are taken from a
set of types T , where each type t ? T is a bag
of natural language words or phrases. Examples
for typed predicates are: conquer(country,city) and
contain(product,material). An instance of a typed
predicate is a triple p(a1, a2), where a1 ? t1 and
a2 ? t2 are termed arguments. For example, be
common in(ASTHMA,AUSTRALIA) is an instance of
be common in(disease,place). For brevity, we refer
3Otherwise, the rule would contain unbound variables.
to typed entailment graphs and typed predicates as
entailment graphs and predicates respectively.
Edges in typed entailment graphs represent en-
tailment rules: an edge (u, v) means that predicate
u entails predicate v. If the type t1 is different
from the type t2, mapping of arguments is straight-
forward, as in the rule ?be find in(material,product)
? contain(product,material)?. We term this a two-
types entailment graph. When t1 and t2 are equal,
mapping of arguments is ambiguous: we distin-
guish direct-mapping edges where the first argu-
ment on the left-hand-side (LHS) is mapped to
the first argument on the right-hand-side (RHS),
as in ?beat(team,team)
d
?? defeat(team,team)?, and
reversed-mapping edges where the LHS first argu-
ment is mapped to the RHS second argument, as
in ?beat(team,team)
r
?? lose to(team,team)?. We
term this a single-type entailment graph. Note
that in single-type entailment graphs reversed-
mapping loops are possible as in ?play(team,team)
r
?? play(team,team)?: if team A plays team B, then
team B plays team A.
Since entailment is a transitive relation, typed-
entailment graphs are transitive: if the edges (u, v)
and (v, w) are in the graph so is the edge (u,w).
Note that in single-type entailment graphs one needs
to consider whether mapping of edges is direct or re-
versed: if mapping of both (u, v) and (v, w) is either
direct or reversed, mapping of (u,w) is direct, oth-
erwise it is reversed.
Typing plays an important role in rule transitiv-
ity: if predicates are ambiguous, transitivity does not
necessarily hold. However, typing predicates helps
disambiguate them and so the problem of ambiguity
is greatly reduced.
4 Learning Typed Entailment Graphs
Our learning algorithm is composed of two steps:
(1) Given a set of typed predicates and their in-
stances extracted from a corpus, we train a (local)
entailment classifier that estimates for every pair of
predicates whether one entails the other. (2) Using
the classifier scores we perform global optimization,
i.e., learn the set of edges over the nodes that maxi-
mizes the global score of the graph under transitivity
and background-knowledge constraints.
Section 4.1 describes the local classifier training
612
province of(place,country)
be part of(place,country)
annex(country,place)
invade(country,place)
be relate to(drug,drug)be derive from(drug,drug)
be process from(drug,drug)
be convert into(drug,drug)
Figure 1: Top: A fragment of a two-types entailment
graph. bottom: A fragment of a single-type entailment
graph. Mapping of solid edges is direct and of dashed
edges is reversed.
procedure. Section 4.2 gives an ILP formulation for
the optimization problem. Sections 4.3 and 4.4 pro-
pose scaling techniques that exploit graph sparsity
to optimally solve larger graphs.
4.1 Training an entailment classifier
Similar to the work of Berant et al (2010), we
use ?distant supervision?. Given a lexicographic re-
source (WordNet) and a set of predicates with their
instances, we perform the following three steps (see
Table 1):
1) Training set generation We use WordNet to
generate positive and negative examples, where each
example is a pair of predicates. Let P be the
set of input typed predicates. For every predicate
p(t1, t2) ? P such that p is a single word, we extract
from WordNet the set S of synonyms and direct hy-
pernyms of p. For every p? ? S, if p?(t1, t2) ? P
then p(t1, t2) ? p?(t1, t2) is taken as a positive ex-
ample.
Negative examples are generated in a similar
manner, with direct co-hyponyms of p (sister nodes
in WordNet) and hyponyms at distance 2 instead of
synonyms and direct hypernyms. We also generate
negative examples by randomly sampling pairs of
typed predicates that share the same types.
2) Feature representation Each example pair of
predicates (p1, p2) is represented by a feature vec-
tor, where each feature is a specific distributional
Type example
hyper. beat(team,team)? play(team,team)
syno. reach(team,game)? arrive at(team,game)
cohypo. invade(country,city) 9 bomb(country,city)
hypo. defeat(city,city) 9 eliminate(city,city)
random hold(place,event) 9 win(place,event)
Table 1: Automatically generated training set examples.
similarity score estimating whether p1 entails p2.
We compute 11 distributional similarity scores for
each pair of predicates based on the arguments ap-
pearing in the extracted arguments. The first 6
scores are computed by trying all combinations of
the similarity functions Lin and BInc with the fea-
ture representations unary, binary-DIRT and binary
(see Section 2). The other 5 scores were provided
by Schoenmackers et al (2010) and include SR
(Schoenmackers et al, 2010), LIME (McCreath and
Sharma, 1997), M-estimate (Dzeroski and Brakto,
1992), the standard G-test and a simple implementa-
tion of Cover (Weeds and Weir, 2003). Overall, the
rationale behind this representation is that combin-
ing various scores will yield a better classifier than
each single measure.
3) Training We train over an equal number of
positive and negative examples, as classifiers tend to
perform poorly on the minority class when trained
on imbalanced data (Van Hulse et al, 2007; Nikulin,
2008).
4.2 ILP formulation
Once the classifier is trained, we would like to learn
all edges (entailment rules) of each typed entailment
graph. Given a set of predicates V and an entail-
ment score function f : V ? V ? R derived from
the classifier, we want to find a graph G = (V,E)
that respects transitivity and maximizes the sum of
edge weights
?
(u,v)?E f(u, v). This problem is
NP-hard by a reduction from the NP-hard Transitive
Subgraph problem (Yannakakis, 1978). Thus, em-
ploying ILP is an appealing approach for obtaining
an optimal solution.
For two-types entailment graphs the formulation
is simple: The ILP variables are indicators Xuv de-
noting whether an edge (u, v) is in the graph, with
the following ILP:
613
G? = argmax
?
u6=v
f(u, v) ?Xuv (1)
s.t. ?u,v,w?V Xuv +Xvw ?Xuw ? 1 (2)
?u,v?Ayes Xuv = 1 (3)
?u,v?Ano Xuv = 0 (4)
?u6=v Xuv ? {0, 1} (5)
The objective in Eq. 1 is a sum over the weights
of the eventual edges. The constraint in Eq. 2 states
that edges must respect transitivity. The constraints
in Eq. 3 and 4 state that for known node pairs, de-
fined by Ayes and Ano, we have background knowl-
edge indicating whether entailment holds or not. We
elaborate on how Ayes and Ano were constructed in
Section 5. For a graph with n nodes we get n(n?1)
variables and n(n?1)(n?2) transitivity constraints.
The simplest way to expand this formulation for
single-type graphs is to duplicate each predicate
node, with one node for each order of the types, and
then the ILP is unchanged. However, this is inef-
ficient as it results in an ILP with 2n(2n ? 1) vari-
ables and 2n(2n?1)(2n?2) transitivity constraints.
Since our main goal is to scale the use of ILP, we
modify it a little. We denote a direct-mapping edge
(u, v) by the indicator Xuv and a reversed-mapping
edge (u, v) by Yuv. The functions fd and fr provide
scores for direct and reversed mappings respectively.
The objective in Eq. 1 and the constraint in Eq. 2 are
replaced by (Eq. 3, 4 and 5 still exist and are carried
over in a trivial manner):
argmax
?
u6=v
fd(u, v)Xuv +
?
u,v
fr(u, v)Yuv (6)
s.t. ?u,v,w?V Xuv +Xvw ?Xuw ? 1
?u,v,w?V Xuv + Yvw ? Yuw ? 1
?u,v,w?V Yuv +Xvw ? Yuw ? 1
?u,v,w?V Yuv + Yvw ?Xuw ? 1
The modified constraints capture the transitivity
behavior of direct-mapping and reversed-mapping
edges, as described in Section 3. This results in
2n2 ? n variables and about 4n3 transitivity con-
straints, cutting the ILP size in half.
Next, we specify how to derive the function f
from the trained classifier using a probabilistic for-
mulation4. Following Snow et al (2006) and Be-
rant et al (2010), we utilize a probabilistic entail-
ment classifier that computes the posterior Puv =
P (Xuv = 1|Fuv). We want to use Puv to derive the
posterior P (G|F ), where F = ?u6=vFuv and Fuv is
the feature vector for a node pair (u, v).
Since the classifier was trained on a balanced
training set, the prior over the two entailment
classes is uniform and so by Bayes rule Puv ?
P (Fuv|Xuv = 1). Using that and the exact same
three independence assumptions described by Snow
et al (2006) and Berant et al (2010) we can show
that (for brevity, we omit the full derivation):
G? = argmaxG logP (G|F ) = (7)
argmax
?
u6=v
(log
Puv ? P (Xuv = 1)
(1? Puv)P (Xuv = 0)
)Xuv
= argmax
?
u6=v
(log
Puv
1? Puv
)Xuv + log ? ? |E|
where ? = P (Xuv=1)P (Xuv=0) is the prior odds ratio for
an edge in the graph. Comparing Eq. 1 and 7 we
see that f(u, v) = log Puv ?P (Xuv=1)(1?Puv)P (Xuv=0) . Note that f
is composed of a likelihood component and an edge
prior expressed by P (Xuv = 1), which we assume
to be some constant. This constant is a parameter
that affects graph sparsity and controls the trade-off
between recall and precision.
Next, we show how sparsity is exploited to scale
the use of ILP solvers. We discuss two-types entail-
ment graphs, but generalization is simple.
4.3 Graph decomposition
Though ILP solvers provide an optimal solution,
they substantially restrict the size of graphs we can
work with. The number of constraints is O(n3),
and solving graphs of size > 50 is often not feasi-
ble. To overcome this, we take advantage of graph
sparsity: most predicates in language do not entail
one another. Thus, it might be possible to decom-
pose graphs into small components and solve each
4We describe two-types graphs but extending to single-type
graphs is straightforward.
614
Algorithm 1 Decomposed-ILP
Input: A set V and a function f : V ? V ? R
Output: An optimal set of directed edges E?
1: E? = {(u, v) : f(u, v) > 0 ? f(v, u) > 0}
2: V1, V2, ..., Vk ? connected components of
G? = (V,E?)
3: for i = 1 to k do
4: Ei ? ApplyILPSolve(Vi,f)
5: end for
6: E? ?
?k
i=1Ei
component separately. This is formalized in the next
proposition.
Proposition 1. If we can partition a set of nodes
V into disjoint sets U,W such that for any cross-
ing edge (u,w) between them (in either direction),
f(u,w) < 0, then the optimal set of edgesEopt does
not contain any crossing edge.
Proof Assume by contradiction that Eopt con-
tains a set of crossing edges Ecross. We can
construct Enew = Eopt \ Ecross. Clearly?
(u,v)?Enew f(u, v) >
?
(u,v)?Eopt f(u, v), as
f(u, v) < 0 for any crossing edge.
Next, we show that Enew does not violate tran-
sitivity constraints. Assume it does, then the viola-
tion is caused by omitting the edges in Ecross. Thus,
there must be a node u ? U and w ? W (w.l.o.g)
such that for some node v, (u, v) and (v, w) are in
Enew, but (u,w) is not. However, this means either
(u, v) or (v, w) is a crossing edge, which is impossi-
ble since we omitted all crossing edges. Thus, Enew
is a better solution than Eopt, contradiction.
This proposition suggests a simple algorithm (see
Algorithm 1): Add to the graph an undirected edge
for any node pair with a positive score, then find the
connected components, and apply an ILP solver over
the nodes in each component. The edges returned
by the solver provide an optimal (not approximate)
solution to the optimization problem.
The algorithm?s complexity is dominated by the
ILP solver, as finding connected components takes
O(V 2) time. Thus, efficiency depends on whether
the graph is sparse enough to be decomposed into
small components. Note that the edge prior plays an
important role: low values make the graph sparser
and easier to solve. In Section 5 we empirically test
Algorithm 2 Incremental-ILP
Input: A set V and a function f : V ? V ? R
Output: An optimal set of directed edges E?
1: ACT,VIO? ?
2: repeat
3: E? ? ApplyILPSolve(V,f,ACT)
4: VIO? violated(V,E?)
5: ACT? ACT ? VIO
6: until |VIO| = 0
how typed entailment graphs benefit from decompo-
sition given different prior values.
From a more general perspective, this algo-
rithm can be applied to any problem of learning
a sparse transitive binary relation. Such problems
include Co-reference Resolution (Finkel and Man-
ning, 2008) and Temporal Information Extraction
(Ling and Weld, 2010). Last, the algorithm can be
easily parallelized by solving each component on a
different core.
4.4 Incremental ILP
Another solution for scaling ILP is to employ in-
cremental ILP, which has been used in dependency
parsing (Riedel and Clarke, 2006). The idea is
that even if we omit the transitivity constraints, we
still expect most transitivity constraints to be satis-
fied, given a good local entailment classifier. Thus,
it makes sense to avoid specifying the constraints
ahead of time, but rather add them when they are
violated. This is formalized in Algorithm 2.
Line 1 initializes an active set of constraints and a
violated set of constraints (ACT;VIO). Line 3 applies
the ILP solver with the active constraints. Lines 4
and 5 find the violated constraints and add them to
the active constraints. The algorithm halts when no
constraints are violated. The solution is clearly op-
timal since we obtain a maximal solution for a less-
constrained problem.
A pre-condition for using incremental ILP is that
computing the violated constraints (Line 4) is effi-
cient, as it occurs in every iteration. We do that in
a straightforward manner: For every node v, and
edges (u, v) and (v, w), if (u,w) /? E? we add
(u, v, w) to the violated constraints. This is cubic
in worst-case but assuming the degree of nodes is
bounded by a constant it is linear, and performs very
615
fast in practice.
Combining Incremental-ILP and Decomposed-
ILP is easy: We decompose any large graph into
its components and apply Incremental ILP on each
component. We applied this algorithm on our evalu-
ation data set (Section 5) and found that it converges
in at most 6 iterations and that the maximal num-
ber of active constraints in large graphs drops from
? 106 to ? 103 ? 104.
5 Experimental Evaluation
In this section we empirically answer the follow-
ing questions: (1) Does transitivity improve rule
learning over typed predicates? (Section 5.1) (2)
Do Decomposed-ILP and Incremental-ILP improve
scalability? (Section 5.2)
5.1 Experiment 1
A data set of 1 million TextRunner tuples (Banko
et al, 2007), mapped to 10,672 distinct typed predi-
cates over 156 types was provided by Schoenmack-
ers et al (2010). Readers are referred to their pa-
per for details on mapping of tuples to typed predi-
cates. Since entailment only occurs between pred-
icates that share the same types, we decomposed
predicates by their types (e.g., all predicates with the
types place and disease) into 2,303 typed entailment
graphs. The largest graph contains 118 nodes and
the total number of potential rules is 263,756.
We generated a training set by applying the proce-
dure described in Section 4.1, yielding 2,644 exam-
ples. We used SVMperf (Joachims, 2005) to train a
Gaussian kernel classifier and computed Puv by pro-
jecting the classifier output score, Suv, with the sig-
moid function: Puv = 11+exp(?Suv) . We tuned two
SVM parameters using 5-fold cross validation and a
development set of two typed entailment graphs.
Next, we used our algorithm to learn rules. As
mentioned in Section 4.2, we integrate background
knowledge using the sets Ayes and Ano that contain
predicate pairs for which we know whether entail-
ment holds. Ayes was constructed with syntactic
rules: We normalized each predicate by omitting the
first word if it is a modal and turning passives to ac-
tives. If two normalized predicates are equal they are
synonymous and inserted into Ayes. Ano was con-
structed from 3 sources (1) Predicates differing by a
single pair of words that are WordNet antonyms (2)
Predicates differing by a single word of negation (3)
Predicates p(t1, t2) and p(t2, t1) where p is a transi-
tive verb (e.g., beat) in VerbNet (Kipper-Schuler et
al., 2000).
We compared our algorithm (termed ILPscale) to
the following baselines. First, to 10,000 rules re-
leased by Schoenmackers et al (2010) (Sherlock),
where the LHS contains a single predicate (Schoen-
mackers et al released 30,000 rules but 20,000 of
those have more than one predicate on the LHS,
see Section 2), as we learn rules over the same data
set. Second, to distributional similarity algorithms:
(a) SR: the score used by Schoenmackers et al as
part of the Sherlock system. (b) DIRT: (Lin and
Pantel, 2001) a widely-used rule learning algorithm.
(c) BInc: (Szpektor and Dagan, 2008) a directional
rule learning algorithm. Third, we compared to the
entailment classifier with no transitivity constraints
(clsf ) to see if combining distributional similarity
scores improves performance over single measures.
Last, we added to all baselines background knowl-
edge with Ayes and Ano (adding the subscript Xk to
their name).
To evaluate performance we manually annotated
all edges in 10 typed entailment graphs - 7 two-
types entailment graphs containing 14, 22, 30, 53,
62, 86 and 118 nodes, and 3 single-type entailment
graphs containing 7, 38 and 59 nodes. This annota-
tion yielded 3,427 edges and 35,585 non-edges, re-
sulting in an empirical edge density of 9%. We eval-
uate the algorithms by comparing the set of edges
learned by the algorithms to the gold standard edges.
Figure 2 presents the precision-recall curve of the
algorithms. The curve is formed by varying a score
threshold in the baselines and varying the edge prior
in ILPscale5. For figure clarity, we omit DIRT and
SR, since BInc outperforms them.
Table 2 shows micro-recall, precision and F1 at
the point of maximal F1, and the Area Under the
Curve (AUC) for recall in the range of 0-0.45 for all
algorithms, given background knowledge (knowl-
edge consistently improves performance by a few
points for all algorithms). The table also shows re-
sults for the rules from Sherlockk.
5we stop raising the prior when run time over the graphs
exceeds 2 hours. Often when the solver does not terminate in 2
hours, it also does not terminate after 24 hours or more.
616
00 . 2
0 . 4
0 . 6
0 . 8
1
0 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9
prec
isio
n
recall
BInc
clsf
BInc_k
clsf_k
ILP_scale
Figure 2: Precision-recall curve for the algorithms.
micro-average
R (%) P (%) F1 (%) AUC
ILPscale 43.4 42.2 42.8 0.22
clsfk 30.8 37.5 33.8 0.17
Sherlockk 20.6 43.3 27.9 N/A
BInck 31.8 34.1 32.9 0.17
SRk 38.4 23.2 28.9 0.14
DIRTk 25.7 31.0 28.1 0.13
Table 2: micro-average F1 and AUC for the algorithms.
Results show that using global transitivity
information substantially improves performance.
ILPscale is better than all other algorithms by a large
margin starting from recall .2, and improves AUC
by 29% and the maximal F1 by 27%. Moreover,
ILPscale doubles recall comparing to the rules from
the Sherlock resource, while maintaining compara-
ble precision.
5.2 Experiment 2
We want to test whether using our scaling tech-
niques, Decomposed-ILP and Incremental-ILP, al-
lows us to reach the optimal solution in graphs that
otherwise we could not solve, and consequently in-
crease the number of learned rules and the overall
recall. To check that, we run ILPscale, with and with-
out these scaling techniques (termed ILP?).
We used the same data set as in Experiment 1
and learned edges for all 2,303 entailment graphs
in the data set. If the ILP solver was unable to
hold the ILP in memory or took more than 2 hours
log ? # unlearned # rules 4 Red.
-1.75 9/0 6,242 / 7,466 20% 75%
-1 9/1 16,790 / 19,396 16% 29%
-0.6 9/3 26,330 / 29,732 13% 14%
Table 3: Impact of scaling techinques (ILP?/ILPscale).
for some graph, we did not attempt to learn its
edges. We ran ILPscale and ILP? in three den-
sity modes to examine the behavior of the algo-
rithms for different graph densities: (a) log ? =
?0.6: the configuration that achieved the best
recall/precision/F1 of 43.4/42.2/42.8. (b) log ? =
?1 with recall/precision/F1 of 31.8/55.3/40.4. (c)
log ? = ?1.75: A high precision configuration with
recall/precision/F1 of 0.15/0.75/0.23 6.
In each run we counted the number of graphs that
could not be learned and the number of rules learned
by each algorithm. In addition, we looked at the
20 largest graphs in our data (49-118 nodes) and
measured the ratio r between the size of the largest
component after applying Decomposed-ILP and the
original size of the graph. We then computed the av-
erage 1?r over the 20 graphs to examine how graph
size drops due to decomposition.
Table 3 shows the results. Column # unlearned
and # rules describe the number of unlearned graphs
and the number of learned rules. Column 4 shows
relative increase in the number of rules learned and
column Red. shows the average 1? r.
ILPscale increases the number of graphs that we
are able to learn: in our best configuration (log ? =
?0.6) only 3 graphs could not be handled com-
paring to 9 graphs when omitting our scaling tech-
niques. Since the unlearned graphs are among the
largest in the data set, this adds 3,500 additional
rules. We compared the precision of rules learned
only by ILPscale with that of the rules learned by
both, by randomly sampling 100 rules from each and
found precision to be comparable. Thus, the addi-
tional rules learned translate into a 13% increase in
relative recall without harming precision.
Also note that as density increases, the number of
rules learned grows and the effectiveness of decom-
position decreases. This shows how Decomposed-
ILP is especially useful for sparse graphs. We re-
6Experiment was run on an Intel i5 CPU with 4GB RAM.
617
lease the 29,732 rules learned by the configuration
log ? = ?0.6 as a resource.
To sum up, our scaling techniques allow us to
learn rules from graphs that standard ILP can not
handle and thus considerably increase recall without
harming precision.
6 Conclusions and Future Work
This paper proposes two contributions over two re-
cent works: In the first, Berant et al (2010) pre-
sented a global optimization procedure to learn en-
tailment rules between predicates using transitivity,
and applied this algorithm over small graphs where
all predicates have one argument instantiated by a
target concept. Consequently, the rules they learn
are of limited applicability. In the second, Schoen-
mackers et al learned rules of wider applicability by
using typed predicates, but utilized a local approach.
In this paper we developed an algorithm that uses
global optimization to learn widely-applicable en-
tailment rules between typed predicates (where both
arguments are variables). This was achieved by
appropriately defining entailment graphs for typed
predicates, formulating an ILP representation for
them, and introducing scaling techniques that in-
clude graph decomposition and incremental ILP.
Our algorithm is guaranteed to provide an optimal
solution and we have shown empirically that it sub-
stantially improves performance over Schoenmack-
ers et al?s recent resource and over several baselines.
In future work, we aim to scale the algorithm
further and learn entailment rules between untyped
predicates. This would require explicit modeling of
predicate ambiguity and using approximation tech-
niques when an optimal solution cannot be attained.
Acknowledgments
This work was performed with financial support
from the Turing Center at The University of Wash-
ington during a visit of the first author (NSF grant
IIS-0803481). We deeply thank Oren Etzioni and
Stefan Schoenmackers for providing us with the data
sets for this paper and for numerous helpful discus-
sions. We would also like to thank the anonymous
reviewers for their useful comments. This work
was developed under the collaboration of FBK-
irst/University of Haifa and was partially supported
by the Israel Science Foundation grant 1112/08. The
first author is grateful to IBM for the award of an
IBM Fellowship, and has carried out this research
in partial fulllment of the requirements for the Ph.D.
degree.
References
J. Fillmore Baker, C. F. and J. B. Lowe. 1998. The
Berkeley framenet project. In Proc. of COLING-ACL.
Michele Banko, Michael Cafarella, Stephen Soderland,
Matt Broadhead, and Oren Etzioni. 2007. Open in-
formation extraction from the web. In Proceedings of
IJCAI.
Roni Ben Aharon, Idan Szpektor, and Ido Dagan. 2010.
Generating entailment rules from framenet. In Pro-
ceedings of ACL.
Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2010. Global learning of focused entailment graphs.
In Proceedings of ACL.
Rahul Bhagat, Patrick Pantel, and Eduard Hovy. 2007.
LEDIR: An unsupervised algorithm for learning di-
rectionality of inference rules. In Proceedings of
EMNLP-CoNLL.
James Clarke and Mirella Lapata. 2008. Global infer-
ence for sentence compression: An integer linear pro-
gramming approach. Journal of Artificial Intelligence
Research, 31:273?381.
Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan Roth.
2009. Recognizing textual entailment: Rational, eval-
uation and approaches. Natural Language Engineer-
ing, 15(4):1?17.
Quang Do and Dan Roth. 2010. Constraints based
taxonomic relation classification. In Proceedings of
EMNLP.
Saso Dzeroski and Ivan Brakto. 1992. Handling noise
in inductive logic programming. In Proceedings of the
International Workshop on Inductive Logic Program-
ming.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database (Language, Speech, and Com-
munication). The MIT Press.
Jenny Rose Finkel and Christopher D. Manning. 2008.
Enforcing transitivity in coreference resolution. In
Proceedings of ACL-08: HLT, Short Papers.
Marti Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proceedings of COLING.
Thorsten Joachims. 2005. A support vector method for
multivariate performance measures. In Proceedings of
ICML.
Karin Kipper-Schuler, Hoa Trand Dang, and Martha
Palmer. 2000. Class-based construction of verb lex-
icon. In Proceedings of AAAI/IAAI.
618
Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules for question answering. Natural Language
Engineering, 7(4):343?360.
Xiao Ling and Daniel S. Weld. 2010. Temporal informa-
tion extraction. In Proceedings of AAAI.
Catherine Macleod, Ralph Grishman, Adam Meyers,
Leslie Barrett, and Ruth Reeves. 1998. NOMLEX:
A lexicon of nominalizations. In Proceedings of COL-
ING.
Andre Martins, Noah Smith, and Eric Xing. 2009. Con-
cise integer linear programming formulations for de-
pendency parsing. In Proceedings of ACL.
Eric McCreath and Arun Sharma. 1997. ILP with noise
and fixed example size: a bayesian approach. In Pro-
ceedings of the Fifteenth international joint conference
on artificial intelligence - Volume 2.
Vladimir Nikulin. 2008. Classification of imbalanced
data with random sets and mean-variance filtering.
IJDWM, 4(2):63?78.
Hoifung Poon and Pedro Domingos. 2010. Unsuper-
vised ontology induction from text. In Proceedings of
ACL.
Sebastian Riedel and James Clarke. 2006. Incremental
integer linear programming for non-projective depen-
dency parsing. In Proceedings of EMNLP.
Stefan Schoenmackers, Oren Etzioni Jesse Davis, and
Daniel S. Weld. 2010. Learning first-order horn
clauses from web text. In Proceedings of EMNLP.
Satoshi Sekine. 2005. Automatic paraphrase discovery
based on context and keywords between ne pairs. In
Proceedings of IWP.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2006.
Semantic taxonomy induction from heterogenous evi-
dence. In Proceedings of ACL.
Idan Szpektor and Ido Dagan. 2008. Learning entailment
rules for unary templates. In Proceedings of COLING.
Idan Szpektor and Ido Dagan. 2009. Augmenting
wordnet-based inference with argument mapping. In
Proceedings of TextInfer.
Idan Szpektor, Hristo Tanev, Ido Dagan, and Bonaven-
tura Coppola. 2004. Scaling web-based acquisition of
entailment relations. In Proceedings of EMNLP.
Jason Van Hulse, Taghi Khoshgoftaar, and Amri Napoli-
tano. 2007. Experimental perspectives on learning
from imbalanced data. In Proceedings of ICML.
Julie Weeds and David Weir. 2003. A general frame-
work for distributional similarity. In Proceedings of
EMNLP.
Mihalis Yannakakis. 1978. Node-and edge-deletion NP-
complete problems. In STOC ?78: Proceedings of the
tenth annual ACM symposium on Theory of comput-
ing, pages 253?264, New York, NY, USA. ACM.
Alexander Yates and Oren Etzioni. 2009. Unsupervised
methods for determining object and relation synonyms
on the web. Journal of Artificial Intelligence Research,
34:255?296.
619
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 558?563,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
A Probabilistic Modeling Framework for Lexical Entailment
Eyal Shnarch
Computer Science Department
Bar-Ilan University
Ramat-Gan, Israel
shey@cs.biu.ac.il
Jacob Goldberger
School of Engineering
Bar-Ilan University
Ramat-Gan, Israel
goldbej@eng.biu.ac.il
Ido Dagan
Computer Science Department
Bar-Ilan University
Ramat-Gan, Israel
dagan@cs.biu.ac.il
Abstract
Recognizing entailment at the lexical level is
an important and commonly-addressed com-
ponent in textual inference. Yet, this task has
been mostly approached by simplified heuris-
tic methods. This paper proposes an initial
probabilistic modeling framework for lexical
entailment, with suitable EM-based parame-
ter estimation. Our model considers promi-
nent entailment factors, including differences
in lexical-resources reliability and the impacts
of transitivity and multiple evidence. Evalu-
ations show that the proposed model outper-
forms most prior systems while pointing at re-
quired future improvements.
1 Introduction and Background
Textual Entailment was proposed as a generic
paradigm for applied semantic inference (Dagan et
al., 2006). This task requires deciding whether a tex-
tual statement (termed the hypothesis-H) can be in-
ferred (entailed) from another text (termed the text-
T ). Since it was first introduced, the six rounds
of the Recognizing Textual Entailment (RTE) chal-
lenges1, currently organized under NIST, have be-
come a standard benchmark for entailment systems.
These systems tackle their complex task at vari-
ous levels of inference, including logical represen-
tation (Tatu and Moldovan, 2007; MacCartney and
Manning, 2007), semantic analysis (Burchardt et al,
2007) and syntactic parsing (Bar-Haim et al, 2008;
Wang et al, 2009). Inference at these levels usually
1http://www.nist.gov/tac/2010/RTE/index.html
requires substantial processing and resources (e.g.
parsing) aiming at high performance.
Nevertheless, simple entailment methods, per-
forming at the lexical level, provide strong baselines
which most systems did not outperform (Mirkin
et al, 2009; Majumdar and Bhattacharyya, 2010).
Within complex systems, lexical entailment model-
ing is an important component. Finally, there are
cases in which a full system cannot be used (e.g.
lacking a parser for a targeted language) and one
must resort to the simpler lexical approach.
While lexical entailment methods are widely
used, most of them apply ad hoc heuristics which do
not rely on a principled underlying framework. Typ-
ically, such methods quantify the degree of lexical
coverage of the hypothesis terms by the text?s terms.
Coverage is determined either by a direct match of
identical terms in T and H or by utilizing lexi-
cal semantic resources, such as WordNet (Fellbaum,
1998), that capture lexical entailment relations (de-
noted here as entailment rules). Common heuristics
for quantifying the degree of coverage are setting a
threshold on the percentage coverage of H?s terms
(Majumdar and Bhattacharyya, 2010), counting ab-
solute number of uncovered terms (Clark and Har-
rison, 2010), or applying an Information Retrieval-
style vector space similarity score (MacKinlay and
Baldwin, 2009). Other works (Corley and Mihal-
cea, 2005; Zanzotto and Moschitti, 2006) have ap-
plied a heuristic formula to estimate the similarity
between text fragments based on a similarity func-
tion between their terms.
These heuristics do not capture several important
aspects of entailment, such as varying reliability of
558
entailment resources and the impact of rule chaining
and multiple evidence on entailment likelihood. An
additional observation from these and other systems
is that their performance improves only moderately
when utilizing lexical resources2.
We believe that the textual entailment field would
benefit from more principled models for various en-
tailment phenomena. Inspired by the earlier steps
in the evolution of Statistical Machine Translation
methods (such as the initial IBM models (Brown et
al., 1993)), we formulate a concrete generative prob-
abilistic modeling framework that captures the basic
aspects of lexical entailment. Parameter estimation
is addressed by an EM-based approach, which en-
ables estimating the hidden lexical-level entailment
parameters from entailment annotations which are
available only at the sentence-level.
While heuristic methods are limited in their abil-
ity to wisely integrate indications for entailment,
probabilistic methods have the advantage of be-
ing extendable and enabling the utilization of well-
founded probabilistic methods such as the EM algo-
rithm.
We compared the performance of several model
variations to previously published results on RTE
data sets, as well as to our own implementation
of typical lexical baselines. Results show that
both the probabilistic model and our percentage-
coverage baseline perform favorably relative to prior
art. These results support the viability of the proba-
bilistic framework while pointing at certain model-
ing aspects that need to be improved.
2 Probabilistic Model
Under the lexical entailment scope, our modeling
goal is obtaining a probabilistic score for the like-
lihood that all H?s terms are entailed by T. To that
end, we model prominent aspects of lexical entail-
ment, which were mostly neglected by previous lex-
ical methods: (1) distinguishing different reliabil-
ity levels of lexical resources; (2) allowing transi-
tive chains of rule applications and considering their
length when estimating their validity; and (3) con-
sidering multiple entailments when entailing a term.
2See ablation tests reports in http://aclweb.org/aclwiki/ in-
dex.php?title=RTE Knowledge Resources#Ablation Tests
cha
in
t 1
t?
Re
so
urc
e 2
t n
h 1
h i
h m
t j
Tex
t:
Hyp
oth
es
is:
.
 
.
 
.
Re
so
urc
e 1
.
 
.
 
.
.
 
.
 
.MA
TC
H
Re
so
urc
e 1
.
 
.
 
.
Re
so
urc
e 3
Figure 1: The generative process of entailing terms of a hy-
pothesis from a text. Edges represent entailment rules. There
are 3 evidences for the entailment of hi: a rule from Resource1,
another one from Resource3 both suggesting that tj entails it,
and a chain from t1 through an intermediate term t?.
2.1 Model Description
For T to entail H it is usually a necessary, but not
sufficient, that every term h ? H would be en-
tailed by at least one term t ? T (Glickman et al,
2006). Figure 1 describes the process of entailing
hypothesis terms. The trivial case is when identical
terms, possibly at the stem or lemma level, appear
in T and H (a direct match as tn and hm in Fig-
ure 1). Alternatively, we can establish entailment
based on knowledge of entailing lexical-semantic
relations, such as synonyms, hypernyms and mor-
phological derivations, available in lexical resources
(e.g the rule inference? reasoning from WordNet).
We denote by R(r) the resource which provided the
rule r.
Since entailment is a transitive relation, rules may
compose transitive chains that connect a term t ? T
to a term h ? H through intermediate terms. For
instance, from the rules infer? inference and infer-
ence ? reasoning we can deduce the rule infer ?
reasoning (were inference is the intermediate term
as t? in Figure 1).
Multiple chains may connect t to h (as for tj and
hi in Figure 1) or connect several terms in T to h
(as t1 and tj are indicating the entailment of hi in
Figure 1), thus providing multiple evidence for h?s
entailment. It is reasonable to expect that if a term t
indeed entails a term h, it is likely to find evidences
for this relation in several resources.
Taking a probabilistic perspective, we assume a
559
parameter ?R for each resource R, denoting its re-
liability, i.e. the prior probability that applying a
rule from R corresponds to a valid entailment in-
stance. Direct matches are considered as a special
?resource?, called MATCH, for which ?MATCH is ex-
pected to be close to 1.
We now present our probabilistic model. For a
text term t ? T to entail a hypothesis term h by a
chain c, denoted by t
c
?? h, the application of every
r ? c must be valid. Note that a rule r in a chain c
connects two terms (its left-hand-side and its right-
hand-side, denoted lhs ? rhs). The lhs of the first
rule in c is t ? T and the rhs of the last rule in it is
h ? H . We denote the event of a valid rule applica-
tion by lhs
r
?? rhs. Since a-priori a rule r is valid
with probability ?R(r), and assuming independence
of all r ? c, we obtain Eq. 1 to specify the prob-
ability of the event t
c
?? h. Next, let C(h) denote
the set of chains which suggest the entailment of h.
The probability that T does not entail h at all (by
any chain), specified in Eq. 2, is the probability that
all these chains are not valid. Finally, the probabil-
ity that T entails all of H , assuming independence
of H?s terms, is the probability that every h ? H is
entailed, as given in Eq. 3. Notice that there could
be a term h which is not covered by any available
rule chain. Under this formulation, we assume that
each such h is covered by a single rule coming from
a special ?resource? called UNCOVERED (expecting
?UNCOVERED to be relatively small).
p(t
c
?? h) =
?
r?c
p(lhs
r
?? rhs) =
?
r?c
?R(r)(1)
p(T 9 h) =
?
c?C(h)
[1? p(t
c
?? h)] (2)
p(T ? H) =
?
h?H
p(T ? h) (3)
As can be seen, our model indeed distinguishes
varying resource reliability, decreases entailment
probability as rule chains grow and increases it when
entailment of a term is supported by multiple chains.
The above treatment of uncovered terms in H ,
as captured in Eq. 3, assumes that their entailment
probability is independent of the rest of the hypoth-
esis. However, when the number of covered hypoth-
esis terms increases the probability that the remain-
ing terms are actually entailed by T increases too
(even though we do not have supporting knowledge
for their entailment). Thus, an alternative model is
to group all uncovered terms together and estimate
the overall probability of their joint entailment as a
function of the lexical coverage of the hypothesis.
We denote Hc as the subset of H?s terms which are
covered by some rule chain and Huc as the remain-
ing uncovered part. Eq. 3a then provides a refined
entailment model for H , in which the second term
specifies the probability that Huc is entailed given
that Hc is validly entailed and the corresponding
lengths:
p(T?H) = [
?
h?Hc
p(T?h)]?p(T?Huc | |Hc|,|H|)
(3a)
2.2 Parameter Estimation
The difficulty in estimating the ?R values is that
these are term-level parameters while the RTE-
training entailment annotation is given for the
sentence-level. Therefore, we use EM-based esti-
mation for the hidden parameters (Dempster et al,
1977). In the E step we use the current ?R values
to compute all whcr(T,H) values for each training
pair. whcr(T,H) stands for the posterior probability
that application of the rule r in the chain c for h ? H
is valid, given that either T entails H or not accord-
ing to the training annotation (see Eq. 4). Remember
that a rule r provides an entailment relation between
its left-hand-side (lhs) and its right-hand-side (rhs).
Therefore Eq. 4 uses the notation lhs
r
?? rhs to des-
ignate the application of the rule r (similar to Eq. 1).
E :
whcr(T,H) =
?
????????
????????
p(lhs
r
?? rhs|T ? H) =
p(T?H|lhs
r??rhs)p(lhs r??rhs)
p(T?H)
if T ? H
p(lhs
r
?? rhs|T 9 H) =
p(T9H|lhs
r??rhs)p(lhs r??rhs)
p(T9H)
if T 9 H
(4)
After applying Bayes? rule we get a fraction with
Eq. 3 in its denominator and ?R(r) as the second term
of the numerator. The first numerator term is defined
as in Eq. 3 except that for the corresponding rule ap-
plication we substitute ?R(r) by 1 (per the condition-
ing event). The probabilistic model defined by Eq.
1-3 is a loop-free directed acyclic graphical model
560
(aka a Bayesian network). Hence the E-step proba-
bilities can be efficiently calculated using the belief
propagation algorithm (Pearl, 1988).
The M step uses Eq. 5 to update the parameter set.
For each resourceR we average thewhcr(T,H) val-
ues for all its rule applications in the training, whose
total number is denoted nR.
M : ?R =
1
nR
?
T,H
?
h?H
?
c?C(h)
?
r?c|R(r)=R
whcr(T,H)
(5)
For Eq. 3a we need to estimate also p(T?Huc |
|Hc|,|H|). This is done directly via maximum likeli-
hood estimation over the training set, by calculating
the proportion of entailing examples within the set
of all examples of a given hypothesis length (|H|)
and a given number of covered terms (|Hc|). As
|Hc| we take the number of identical terms in T and
H (exact match) since in almost all cases terms in
H which have an exact match in T are indeed en-
tailed. We also tried initializing the EM algorithm
with these direct estimations but did not obtain per-
formance improvements.
3 Evaluations and Results
The 5th Recognizing Textual Entailment challenge
(RTE-5) introduced a new search task (Bentivogli
et al, 2009) which became the main task in RTE-
6 (Bentivogli et al, 2010). In this task participants
should find all sentences that entail a given hypothe-
sis in a given document cluster. This task?s data sets
reflect a natural distribution of entailments in a cor-
pus and demonstrate a more realistic scenario than
the previous RTE challenges.
In our system, sentences are tokenized and
stripped of stop words and terms are lemmatized and
tagged for part-of-speech. As lexical resources we
use WordNet (WN) (Fellbaum, 1998), taking as en-
tailment rules synonyms, derivations, hyponyms and
meronyms of the first senses of T and H terms, and
the CatVar (Categorial Variation) database (Habash
and Dorr, 2003). We allow rule chains of length up
to 4 in WordNet (WN4).
We compare our model to two types of baselines:
(1) RTE published results: the average of the best
runs of all systems, the best and second best per-
forming lexical systems and the best full system of
each challenge; (2) our implementation of lexical
coverage model, tuning the percentage-of-coverage
threshold for entailment on the training set. This
model uses the same configuration as our probabilis-
tic model. We also implemented an Information Re-
trieval style baseline3 (both with and without lex-
ical expansions), but given its poorer performance
we omit its results here.
Table 1 presents the results. We can see that
both our implemented models (probabilistic and
coverage) outperform all RTE lexical baselines on
both data sets, apart from (Majumdar and Bhat-
tacharyya, 2010) which incorporates additional lex-
ical resources, a named entity recognizer and a
co-reference system. On RTE-5, the probabilis-
tic model is comparable in performance to the best
full system, while the coverage model achieves con-
siderably better results. We notice that our imple-
mented models successfully utilize resources to in-
crease performance, as opposed to typical smaller
or less consistent improvements in prior works (see
Section 1).
Model
F1%
RTE-5 RTE-6
R
T
E
avg. of all systems 30.5 33.8
2nd best lexical system 40.31 44.02
best lexical system 44.43 47.64
best full system 45.63 48.05
co
ve
ra
ge
no resource 39.5 44.8
+ WN 45.8 45.1
+ CatVar 47.2 45.5
+ WN + CatVar 48.5 44.7
+ WN4 46.3 43.1
pr
ob
ab
il
is
ti
c no resource 41.8 42.1
+ WN 45.0 45.3
+ CatVar 42.0 45.9
+ WN + CatVar 42.8 45.5
+ WN4 45.8 42.6
Table 1: Evaluation results on RTE-5 and RTE-6. RTE systems
are: (1)(MacKinlay and Baldwin, 2009), (2)(Clark and Harri-
son, 2010), (3)(Mirkin et al, 2009)(2 submitted runs), (4)(Ma-
jumdar and Bhattacharyya, 2010) and (5)(Jia et al, 2010).
While the probabilistic and coverage models are
comparable on RTE-6 (with non-significant advan-
tage for the former), on RTE-5 the latter performs
3Utilizing Lucene search engine (http://lucene.apache.org)
561
better, suggesting that the probabilistic model needs
to be further improved. In particular, WN4 performs
better than the single-step WN only on RTE-5, sug-
gesting the need to improve the modeling of chain-
ing. The fluctuations over the data sets and impacts
of resources suggest the need for further investiga-
tion over additional data sets and resources. As for
the coverage model, under our configuration it poses
a bigger challenge for RTE systems than perviously
reported baselines. It is thus proposed as an easy to
implement baseline for future entailment research.
4 Conclusions and Future Work
This paper presented, for the first time, a principled
and relatively rich probabilistic model for lexical en-
tailment, amenable for estimation of hidden lexical-
level parameters from standard sentence-level an-
notations. The positive results of the probabilistic
model compared to prior art and its ability to exploit
lexical resources indicate its future potential. Yet,
further investigation is needed. For example, analyz-
ing current model?s limitations, we observed that the
multiplicative nature of eqs. 1 and 3 (reflecting inde-
pendence assumptions) is too restrictive, resembling
a logical AND. Accordingly we plan to explore re-
laxing this strict conjunctive behavior through mod-
els such as noisy-AND (Pearl, 1988). We also in-
tend to explore the contribution of our model, and
particularly its estimated parameter values, within a
complex system that integrates multiple levels of in-
ference.
Acknowledgments
This work was partially supported by the NEGEV
Consortium of the Israeli Ministry of Industry,
Trade and Labor (www.negev-initiative.org), the
PASCAL-2 Network of Excellence of the European
Community FP7-ICT-2007-1-216886, the FIRB-
Israel research project N. RBIN045PXH and by the
Israel Science Foundation grant 1112/08.
References
Roy Bar-Haim, Jonathan Berant, Ido Dagan, Iddo Green-
tal, Shachar Mirkin, Eyal Shnarch, and Idan Szpektor.
2008. Efficient semantic deduction and approximate
matching over compact parse forests. In Proceedings
of Text Analysis Conference (TAC).
Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo
Giampiccolo, and Bernardo Magnini. 2009. The fifth
PASCAL recognizing textual entailment challenge. In
Proceedings of Text Analysis Conference (TAC).
Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa Trang
Dang, and Danilo Giampiccolo. 2010. The sixth
PASCAL recognizing textual entailment challenge. In
Proceedings of Text Analysis Conference (TAC).
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: parameter esti-
mation. Computational Linguistics, 19(2):263?311,
June.
Aljoscha Burchardt, Nils Reiter, Stefan Thater, and
Anette Frank. 2007. A semantic approach to textual
entailment: System evaluation and task analysis. In
Proceedings of the ACL-PASCAL Workshop on Textual
Entailment and Paraphrasing.
Peter Clark and Phil Harrison. 2010. BLUE-Lite: a
knowledge-based lexical entailment system for RTE6.
In Proceedings of Text Analysis Conference (TAC).
Courtney Corley and Rada Mihalcea. 2005. Measur-
ing the semantic similarity of texts. In Proceedings of
the ACLWorkshop on Empirical Modeling of Semantic
Equivalence and Entailment.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The PASCAL recognising textual entailment
challenge. In Lecture Notes in Computer Science, vol-
ume 3944, pages 177?190.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the royal statistical society, se-
ries [B], 39(1):1?38.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database (Language, Speech, and Com-
munication). The MIT Press.
Oren Glickman, Eyal Shnarch, and Ido Dagan. 2006.
Lexical reference: a semantic matching subtask. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing, pages 172?179. As-
sociation for Computational Linguistics.
Nizar Habash and Bonnie Dorr. 2003. A categorial vari-
ation database for english. In Proceedings of the North
American Association for Computational Linguistics.
Houping Jia, Xiaojiang Huang, Tengfei Ma, Xiaojun
Wan, and Jianguo Xiao. 2010. PKUTM participa-
tion at TAC 2010 RTE and summarization track. In
Proceedings of Text Analysis Conference (TAC).
Bill MacCartney and Christopher D. Manning. 2007.
Natural logic for textual inference. In Proceedings
of the ACL-PASCAL Workshop on Textual Entailment
and Paraphrasing.
562
Andrew MacKinlay and Timothy Baldwin. 2009. A
baseline approach to the RTE5 search pilot. In Pro-
ceedings of Text Analysis Conference (TAC).
Debarghya Majumdar and Pushpak Bhattacharyya.
2010. Lexical based text entailment system for main
task of RTE6. In Proceedings of Text Analysis Confer-
ence (TAC).
Shachar Mirkin, Roy Bar-Haim, Jonathan Berant, Ido
Dagan, Eyal Shnarch, Asher Stern, and Idan Szpektor.
2009. Addressing discourse and document structure in
the RTE search task. In Proceedings of Text Analysis
Conference (TAC).
Judea Pearl. 1988. Probabilistic reasoning in intelli-
gent systems: networks of plausible inference. Morgan
Kaufmann.
Marta Tatu and Dan Moldovan. 2007. COGEX at RTE
3. In Proceedings of the ACL-PASCAL Workshop on
Textual Entailment and Paraphrasing.
Rui Wang, Yi Zhang, and Guenter Neumann. 2009. A
joint syntactic-semantic representation for recognizing
textual relatedness. In Proceedings of Text Analysis
Conference (TAC).
Fabio Massimo Zanzotto and Alessandro Moschitti.
2006. Automatic learning of textual entailments with
cross-pair similarities. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the Association for Com-
putational Linguistics.
563
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 117?125,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Efficient Tree-based Approximation for Entailment Graph Learning
Jonathan Berant?, Ido Dagan?, Meni Adler?, Jacob Goldberger?
? The Blavatnik School of Computer Science, Tel Aviv University
? Department of Computer Science, Bar-Ilan University
? Faculty of Engineering, Bar-Ilan University
jonatha6@post.tau.ac.il
{dagan,goldbej}@{cs,eng}.biu.ac.il
adlerm@cs.bgu.ac.il
Abstract
Learning entailment rules is fundamental in
many semantic-inference applications and has
been an active field of research in recent years.
In this paper we address the problem of learn-
ing transitive graphs that describe entailment
rules between predicates (termed entailment
graphs). We first identify that entailment
graphs exhibit a ?tree-like? property and are
very similar to a novel type of graph termed
forest-reducible graph. We utilize this prop-
erty to develop an iterative efficient approxi-
mation algorithm for learning the graph edges,
where each iteration takes linear time. We
compare our approximation algorithm to a
recently-proposed state-of-the-art exact algo-
rithm and show that it is more efficient and
scalable both theoretically and empirically,
while its output quality is close to that given
by the optimal solution of the exact algorithm.
1 Introduction
Performing textual inference is in the heart of many
semantic inference applications such as Question
Answering (QA) and Information Extraction (IE). A
prominent generic paradigm for textual inference is
Textual Entailment (TUE) (Dagan et al, 2009). In
TUE, the goal is to recognize, given two text frag-
ments termed text and hypothesis, whether the hy-
pothesis can be inferred from the text. For example,
the text ?Cyprus was invaded by the Ottoman Em-
pire in 1571? implies the hypothesis ?The Ottomans
attacked Cyprus?.
Semantic inference applications such as QA and
IE crucially rely on entailment rules (Ravichandran
and Hovy, 2002; Shinyama and Sekine, 2006) or
equivalently inference rules, that is, rules that de-
scribe a directional inference relation between two
fragments of text. An important type of entailment
rule specifies the entailment relation between natu-
ral language predicates, e.g., the entailment rule ?X
invade Y ? X attack Y? can be helpful in inferring
the aforementioned hypothesis. Consequently, sub-
stantial effort has been made to learn such rules (Lin
and Pantel, 2001; Sekine, 2005; Szpektor and Da-
gan, 2008; Schoenmackers et al, 2010).
Textual entailment is inherently a transitive rela-
tion , that is, the rules ?x ? y? and ?y ? z? imply
the rule ?x ? z?. Accordingly, Berant et al (2010)
formulated the problem of learning entailment rules
as a graph optimization problem, where nodes are
predicates and edges represent entailment rules that
respect transitivity. Since finding the optimal set of
edges respecting transitivity is NP-hard, they em-
ployed Integer Linear Programming (ILP) to find the
exact solution. Indeed, they showed that applying
global transitivity constraints improves rule learning
comparing to methods that ignore graph structure.
More recently, Berant et al (Berant et al, 2011) in-
troduced a more efficient exact algorithm, which de-
composes the graph into connected components and
then applies an ILP solver over each component.
Despite this progress, finding the exact solution
remains NP-hard ? the authors themselves report
they were unable to solve some graphs of rather
moderate size and that the coverage of their method
is limited. Thus, scaling their algorithm to data sets
with tens of thousands of predicates (e.g., the extrac-
tions of Fader et al (2011)) is unlikely.
117
In this paper we present a novel method for learn-
ing the edges of entailment graphs. Our method
computes much more efficiently an approximate so-
lution that is empirically almost as good as the exact
solution. To that end, we first (Section 3) conjecture
and empirically show that entailment graphs exhibit
a ?tree-like? property, i.e., that they can be reduced
into a structure similar to a directed forest.
Then, we present in Section 4 our iterative ap-
proximation algorithm, where in each iteration a
node is removed and re-attached back to the graph in
a locally-optimal way. Combining this scheme with
our conjecture about the graph structure enables a
linear algorithm for node re-attachment. Section 5
shows empirically that this algorithm is by orders of
magnitude faster than the state-of-the-art exact al-
gorithm, and that though an optimal solution is not
guaranteed, the area under the precision-recall curve
drops by merely a point.
To conclude, the contribution of this paper is two-
fold: First, we define a novel modeling assumption
about the tree-like structure of entailment graphs and
demonstrate its validity. Second, we exploit this as-
sumption to develop a polynomial approximation al-
gorithm for learning entailment graphs that can scale
to much larger graphs than in the past. Finally, we
note that learning entailment graphs bears strong
similarities to related tasks such as Taxonomy In-
duction (Snow et al, 2006) and Ontology induction
(Poon and Domingos, 2010), and thus our approach
may improve scalability in these fields as well.
2 Background
Until recently, work on learning entailment rules be-
tween predicates considered each rule independently
of others and did not exploit global dependencies.
Most methods utilized the distributional similarity
hypothesis that states that semantically similar pred-
icates occur with similar arguments (Lin and Pan-
tel, 2001; Szpektor et al, 2004; Yates and Etzioni,
2009; Schoenmackers et al, 2010). Some meth-
ods extracted rules from lexicographic resources
such as WordNet (Szpektor and Dagan, 2009) or
FrameNet (Bob and Rambow, 2009; Ben Aharon et
al., 2010), and others assumed that semantic rela-
tions between predicates can be deduced from their
co-occurrence in a corpus via manually-constructed
patterns (Chklovski and Pantel, 2004).
Recently, Berant et al (2010; 2011) formulated
the problem as the problem of learning global entail-
ment graphs. In entailment graphs, nodes are predi-
cates (e.g., ?X attack Y?) and edges represent entail-
ment rules between them (?X invade Y ? X attack
Y?). For every pair of predicates i, j, an entailment
score wij was learned by training a classifier over
distributional similarity features. A positive wij in-
dicated that the classifier believes i? j and a nega-
tive wij indicated that the classifier believes i 9 j.
Given the graph nodes V (corresponding to the pred-
icates) and the weighting function w : V ? V ? R,
they aim to find the edges of a graph G = (V,E)
that maximize the objective
?
(i,j)?E wij under the
constraint that the graph is transitive (i.e., for every
node triplet (i, j, k), if (i, j) ? E and (j, k) ? E,
then (i, k) ? E).
Berant et al proved that this optimization prob-
lem, which we term Max-Trans-Graph, is NP-hard,
and so described it as an Integer Linear Program
(ILP). Let xij be a binary variable indicating the ex-
istence of an edge i ? j in E. Then, X = {xij :
i 6= j} are the variables of the following ILP for
Max-Trans-Graph:
argmax
X
?
i 6=j
wij ? xij (1)
s.t. ?i,j,k?V xij + xjk ? xik ? 1
?i,j?V xij ? {0, 1}
The objective function is the sum of weights over the
edges of G and the constraint xij + xjk ? xik ? 1
on the binary variables enforces that whenever xij=
xjk=1, then also xik = 1 (transitivity).
Since ILP is NP-hard, applying an ILP solver di-
rectly does not scale well because the number of
variables isO(|V |2) and the number of constraints is
O(|V |3). Thus, even a graph with?80 nodes (predi-
cates) has more than half a million constraints. Con-
sequently, in (Berant et al, 2011), they proposed a
method that efficiently decomposes the graph into
smaller components and applies an ILP solver on
each component separately using a cutting-plane
procedure (Riedel and Clarke, 2006). Although this
method is exact and improves scalability, it does
not guarantee an efficient solution. When the graph
does not decompose into sufficiently small compo-
nents, and the weights generate many violations of
118
transitivity, solving Max-Trans-Graph becomes in-
tractable. To address this problem, we present in
this paper a method for approximating the optimal
set of edges within each component and show that
it is much more efficient and scalable both theoreti-
cally and empirically.
Do and Roth (2010) suggested a method for a re-
lated task of learning taxonomic relations between
terms. Given a pair of terms, a small graph is con-
structed and constraints are imposed on the graph
structure. Their work, however, is geared towards
scenarios where relations are determined on-the-fly
for a given pair of terms and no global knowledge
base is explicitly constructed. Thus, their method
easily produces solutions where global constraints,
such as transitivity, are violated.
Another approximation method that violates tran-
sitivity constraints is LP relaxation (Martins et al,
2009). In LP relaxation, the constraint xij ? {0, 1}
is replaced by 0 ? xij ? 1, transforming the prob-
lem from an ILP to a Linear Program (LP), which
is polynomial. An LP solver is then applied on the
problem, and variables xij that are assigned a frac-
tional value are rounded to their nearest integer and
so many violations of transitivity easily occur. The
solution when applying LP relaxation is not a transi-
tive graph, but nevertheless we show for comparison
in Section 5 that our method is much faster.
Last, we note that transitive relations have been
explored in adjacent fields such as Temporal Infor-
mation Extraction (Ling and Weld, 2010), Ontol-
ogy Induction (Poon and Domingos, 2010), and Co-
reference Resolution (Finkel and Manning, 2008).
3 Forest-reducible Graphs
The entailment relation, described by entailment
graphs, is typically from a ?semantically-specific?
predicate to a more ?general? one. Thus, intuitively,
the topology of an entailment graph is expected to be
?tree-like?. In this section we first formalize this in-
tuition and then empirically analyze its validity. This
property of entailment graphs is an interesting topo-
logical observation on its own, but also enables the
efficient approximation algorithm of Section 4.
For a directed edge i ? j in a directed acyclic
graphs (DAG), we term the node i a child of node
j, and j a parent of i. A directed forest is a DAG
Xdisease be 
epidemic in 
 Ycountry 
Xdisease 
common in 
 Ycountry 
Xdisease 
occur in 
 Ycountry 
Xdisease 
frequent in 
 Ycountry 
Xdisease 
begin in 
 Ycountry 
be epidemic in 
common in 
frequent in 
occur in 
begin in 
be epidemic in 
common in 
 frequent in 
occur in 
begin in 
(a) 
(b) 
(c) 
Figure 1: A fragment of an entailment graph (a), its SCC
graph (b) and its reduced graph (c). Nodes are predicates
with typed variables (see Section 5), which are omitted in
(b) and (c) for compactness.
where all nodes have no more than one parent.
The entailment graph in Figure 1a (subgraph from
the data set described in Section 5) is clearly not a
directed forest ? it contains a cycle of size two com-
prising the nodes ?X common in Y? and ?X frequent in
Y?, and in addition the node ?X be epidemic in Y? has
3 parents. However, we can convert it to a directed
forest by applying the following operations. Any
directed graph G can be converted into a Strongly-
Connected-Component (SCC) graph in the follow-
ing way: every strongly connected component (a set
of semantically-equivalent predicates, in our graphs)
is contracted into a single node, and an edge is added
from SCC S1 to SCC S2 if there is an edge in G from
some node in S1 to some node in S2. The SCC graph
is always a DAG (Cormen et al, 2002), and if G is
transitive then the SCC graph is also transitive. The
graph in Figure 1b is the SCC graph of the one in
119
Xcountry annex  Yplace 
Xcountry invade  Yplace Yplace be part of Xcountry  
Figure 2: A fragment of an entailment graph that is not
an FRG.
Figure 1a, but is still not a directed forest since the
node ?X be epidemic in Y? has two parents.
The transitive closure of a directed graph G is
obtained by adding an edge from node i to node j
if there is a path in G from i to j. The transitive
reduction of G is obtained by removing all edges
whose absence does not affect its transitive closure.
In DAGs, the result of transitive reduction is unique
(Aho et al, 1972). We thus define the reduced graph
Gred = (Vred, Ered) of a directed graph G as the
transitive reduction of its SCC graph. The graph in
Figure 1c is the reduced graph of the one in Fig-
ure 1a and is a directed forest. We say a graph is a
forest-reducible graph (FRG) if all nodes in its re-
duced form have no more than one parent.
We now hypothesize that entailment graphs are
FRGs. The intuition behind this assumption is
that the predicate on the left-hand-side of a uni-
directional entailment rule has a more specific mean-
ing than the one on the right-hand-side. For instance,
in Figure 1a ?X be epidemic in Y? (where ?X? is a type
of disease and ?Y? is a country) is more specific than
?X common in Y? and ?X frequent in Y?, which are
equivalent, while ?X occur in Y? is even more gen-
eral. Accordingly, the reduced graph in Figure 1c
is an FRG. We note that this is not always the case:
for example, the entailment graph in Figure 2 is not
an FRG, because ?X annex Y? entails both ?Y be part
of X? and ?X invade Y?, while the latter two do not
entail one another. However, we hypothesize that
this scenario is rather uncommon. Consequently, a
natural variant of the Max-Trans-Graph problem is
to restrict the required output graph of the optimiza-
tion problem (1) to an FRG. We term this problem
Max-Trans-Forest.
To test whether our hypothesis holds empirically
we performed the following analysis. We sampled
7 gold standard entailment graphs from the data set
described in Section 5, manually transformed them
into FRGs by deleting a minimal number of edges,
and measured recall over the set of edges in each
graph (precision is naturally 1.0, as we only delete
gold standard edges). The lowest recall value ob-
tained was 0.95, illustrating that deleting a very
small proportion of edges converts an entailment
graph into an FRG. Further support for the prac-
tical validity of this hypothesis is obtained from
our experiments in Section 5. In these experiments
we show that exactly solving Max-Trans-Graph and
Max-Trans-Forest (with an ILP solver) results in
nearly identical performance.
An ILP formulation for Max-Trans-Forest is sim-
ple ? a transitive graph is an FRG if all nodes in
its reduced graph have no more than one parent. It
can be verified that this is equivalent to the following
statement: for every triplet of nodes i, j, k, if i ? j
and i ? k, then either j ? k or k ? j (or both).
Therefore, the ILP is formulated by adding this lin-
ear constraint to ILP (1):
?i,j,k?V xij+xik+(1? xjk)+(1? xkj) ? 3 (2)
We note that despite the restriction to FRGs, Max-
Trans-Forest is an NP-hard problem by a reduction
from the X3C problem (Garey and Johnson, 1979).
We omit the reduction details for brevity.
4 Sequential Approximation Algorithms
In this section we present Tree-Node-Fix, an efficient
approximation algorithm for Max-Trans-Forest, as
well as Graph-Node-Fix, an approximation for Max-
Trans-Graph.
4.1 Tree-Node-Fix
The scheme of Tree-Node-Fix (TNF) is the follow-
ing. First, an initial FRG is constructed, using some
initialization procedure. Then, at each iteration a
single node v is re-attached (see below) to the FRG
in a way that improves the objective function. This
is repeated until the value of the objective function
cannot be improved anymore by re-attaching a node.
Re-attaching a node v is performed by removing
v from the graph and connecting it back with a better
set of edges, while maintaining the constraint that it
is an FRG. This is done by considering all possible
edges from/to the other graph nodes and choosing
120
(a) 
d 
c 
v ? c v 
c 
d1 ? d2 
v 
? ? ? 
r1 r2 
v (b) (b?) (c) 
r3 
? 
Figure 3: (a) Inserting v into a component c ? Vred. (b)
Inserting v as a child of c and a parent of a subset of c?s
children in Gred. (b?) A node d that is a descendant but
not a child of c can not choose v as a parent, as v becomes
its second parent. (c) Inserting v as a new root.
the optimal subset, while the rest of the graph re-
mains fixed. Formally, let Sv?in =
?
i 6=v wiv ? xiv
be the sum of scores over v?s incoming edges and
Sv?out =
?
k 6=v wvk ? xvk be the sum of scores over
v?s outgoing edges. Re-attachment amounts to opti-
mizing a linear objective:
argmax
Xv
(Sv-in + Sv-out) (3)
where the variables Xv ? X are indicators for all
pairs of nodes involving v. We approximate a solu-
tion for (1) by iteratively optimizing the simpler ob-
jective (3). Clearly, at each re-attachment the value
of the objective function cannot decrease, since the
optimization algorithm considers the previous graph
as one of its candidate solutions.
We now show that re-attaching a node v is lin-
ear. To analyze v?s re-attachment, we consider the
structure of the directed forest Gred just before v is
re-inserted, and examine the possibilities for v?s in-
sertion relative to that structure. We start by defin-
ing some helpful notations. Every node c ? Vred
is a connected component in G. Let vc ? c be an
arbitrary representative node in c. We denote by
Sv-in(c) the sum of weights from all nodes in c and
their descendants to v, and by Sv-out(c) the sum of
weights from v to all nodes in c and their ancestors:
Sv-in(c) =
?
i?c
wiv +
?
k /?c
wkvxkvc
Sv-out(c) =
?
i?c
wvi +
?
k /?c
wvkxvck
Note that {xvck, xkvc} are edge indicators in G
and not Gred. There are two possibilities for re-
attaching v ? either it is inserted into an existing
component c ? Vred (Figure 3a), or it forms a new
component. In the latter, there are also two cases:
either v is inserted as a child of a component c (Fig-
ure 3b), or not and then it becomes a root in Gred
(Figure 3c). We describe the details of these 3 cases:
Case 1: Inserting v into a component c ? Vred.
In this case we add in G edges from all nodes in c
and their descendants to v and from v to all nodes in
c and their ancestors. The score (3) in this case is
s1(c) , Sv-in(c) + Sv-out(c) (4)
Case 2: Inserting v as a child of some c ? Vred.
Once c is chosen as the parent of v, choosing v?s
children in Gred is substantially constrained. A node
that is not a descendant of c can not become a child
of v, since this would create a new path from that
node to c and would require by transitivity to add a
corresponding directed edge to c (but all graph edges
not connecting v are fixed). Moreover, only a direct
child of c can choose v as a parent instead of c (Fig-
ure 3b), since for any other descendant of c, v would
become a second parent, and Gred will no longer be
a directed forest (Figure 3b?). Thus, this case re-
quires adding in G edges from v to all nodes in c and
their ancestors, and also for each new child of v, de-
noted by d ? Vred, we add edges from all nodes in
d and their descendants to v. Crucially, although the
number of possible subsets of c?s children in Gred is
exponential, the fact that they are independent trees
in Gred allows us to go over them one by one, and
decide for each one whether it will be a child of v
or not, depending on whether Sv-in(d) is positive.
Therefore, the score (3) in this case is:
s2(c) , Sv-out(c)+
?
d?child(c)
max(0, Sv-in(d)) (5)
where child(c) are the children of c.
Case 3: Inserting v as a new root in Gred. Similar
to case 2, only roots of Gred can become children of
v. In this case for each chosen root r we add in G
edges from the nodes in r and their descendants to
v. Again, each root can be examined independently.
Therefore, the score (3) of re-attaching v is:
s3 ,
?
r
max(0, Sv-in(r)) (6)
where the summation is over the roots of Gred.
It can be easily verified that Sv-in(c) and
Sv-out(c) satisfy the recursive definitions:
121
Algorithm 1 Computing optimal re-attachment
Input: FRG G = (V,E), function w, node v ? V
Output: optimal re-attachment of v
1: remove v and compute Gred = (Vred, Ered).
2: for all c ? Vred in post-order compute Sv-in(c) (Eq.
7)
3: for all c ? Vred in pre-order compute Sv-out(c) (Eq.
8)
4: case 1: s1 = maxc?Vred s1(c) (Eq. 4)
5: case 2: s2 = maxc?Vred s2(c) (Eq. 5)
6: case 3: compute s3 (Eq. 6)
7: re-attach v according to max(s1, s2, s3).
Sv-in(c) =
?
i?c
wiv +
?
d?child(c)
Sv-in(d), c ? Vred (7)
Sv-out(c) =
?
i?c
wvi + Sv-out(p), c ? Vred (8)
where p is the parent of c in Gred. These recursive
definitions allow to compute in linear time Sv-in(c)
and Sv-out(c) for all c (given Gred) using dynamic
programming, before going over the cases for re-
attaching v. Sv-in(c) is computed going over Vred
leaves-to-root (post-order), and Sv-out(c) is com-
puted going over Vred root-to-leaves (pre-order).
Re-attachment is summarized in Algorithm 1.
Computing an SCC graph is linear (Cormen et al,
2002) and it is easy to verify that transitive reduction
in FRGs is also linear (Line 1). Computing Sv-in(c)
and Sv-out(c) (Lines 2-3) is also linear, as explained.
Cases 1 and 3 are trivially linear and in case 2 we go
over the children of all nodes in Vred. As the reduced
graph is a forest, this simply means going over all
nodes of Vred, and so the entire algorithm is linear.
Since re-attachment is linear, re-attaching all
nodes is quadratic. Thus if we bound the number
of iterations over all nodes, the overall complexity is
quadratic. This is dramatically more efficient and
scalable than applying an ILP solver. In Section
5 we ran TNF until convergence and the maximal
number of iterations over graph nodes was 8.
4.2 Graph-node-fix
Next, we show Graph-Node-Fix (GNF), a similar
approximation that employs the same re-attachment
strategy but does not assume the graph is an FRG.
Thus, re-attachment of a node v is done with an
ILP solver. Nevertheless, the ILP in GNF is sim-
pler than (1), since we consider only candidate edges
v  
i  k  
v  
i  k  
v
i k
v  
i  k  
Figure 4: Three types of transitivity constraint violations.
involving v. Figure 4 illustrates the three types of
possible transitivity constraint violations when re-
attaching v. The left side depicts a violation when
(i, k) /? E, expressed by the constraint in (9) below,
and the middle and right depict two violations when
the edge (i, k) ? E, expressed by the constraints
in (10). Thus, the ILP is formulated by adding the
following constraints to the objective function (3):
?i,k?V \{v} if (i, k) /? E, xiv + xvk ? 1 (9)
if (i, k) ? E, xvi ? xvk, xkv ? xiv (10)
xiv, xvk ? {0, 1} (11)
Complexity is exponential due to the ILP solver;
however, the ILP size is reduced by an order of mag-
nitude to O(|V |) variables and O(|V |2) constraints.
4.3 Adding local constraints
For some pairs of predicates i, j we sometimes have
prior knowledge whether i entails j or not. We term
such pairs local constraints, and incorporate them
into the aforementioned algorithms in the following
way. In all algorithms that apply an ILP solver, we
add a constraint xij = 1 if i entails j or xij = 0 if i
does not entail j. Similarly, in TNF we incorporate
local constraints by settingwij =? orwij = ??.
5 Experiments and Results
In this section we empirically demonstrate that TNF
is more efficient than other baselines and its output
quality is close to that given by the optimal solution.
5.1 Experimental setting
In our experiments we utilize the data set released
by Berant et al (2011). The data set contains 10 en-
tailment graphs, where graph nodes are typed pred-
icates. A typed predicate (e.g., ?Xdisease occur in
Ycountry?) includes a predicate and two typed vari-
ables that specify the semantic type of the argu-
ments. For instance, the typed variable Xdisease can
be instantiated by arguments such as ?flu? or ?dia-
betes?. The data set contains 39,012 potential edges,
122
of which 3,427 are annotated as edges (valid entail-
ment rules) and 35,585 are annotated as non-edges.
The data set alo contains, for every pair of pred-
icates i, j in every graph, a local score sij , which is
the output of a classifier trained over distributional
similarity features. A positive sij indicates that the
classifier believes i? j. The weighting function for
the graph edges w is defined as wij = sij??, where
? is a single parameter controlling graph sparseness:
as ? increases, wij decreases and becomes nega-
tive for more pairs of predicates, rendering the graph
more sparse. In addition, the data set contains a set
of local constraints (see Section 4.3).
We implemented the following algorithms for
learning graph edges, where in all of them the graph
is first decomposed into components according to
Berant et als method, as explained in Section 2.
No-trans Local scores are used without transitiv-
ity constraints ? an edge (i, j) is inserted iffwij > 0.
Exact-graph Berant et al?s exact method (2011)
for Max-Trans-Graph, which utilizes an ILP solver1.
Exact-forest Solving Max-Trans-Forest exactly
by applying an ILP solver (see Eq. 2).
LP-relax Solving Max-Trans-Graph approxi-
mately by applying LP-relaxation (see Section 2)
on each graph component. We apply the LP solver
within the same cutting-plane procedure as Exact-
graph to allow for a direct comparison. This also
keeps memory consumption manageable, as other-
wise all |V |3 constraints must be explicitly encoded
into the LP. As mentioned, our goal is to present
a method for learning transitive graphs, while LP-
relax produces solutions that violate transitivity.
However, we run it on our data set to obtain empiri-
cal results, and to compare run-times against TNF.
Graph-Node-Fix (GNF) Initialization of each
component is performed in the following way: if the
graph is very sparse, i.e. ? ? C for some constantC
(set to 1 in our experiments), then solving the graph
exactly is not an issue and we use Exact-graph. Oth-
erwise, we initialize by applying Exact-graph in a
sparse configuration, i.e., ? = C.
Tree-Node-Fix (TNF) Initialization is done as in
GNF, except that if it generates a graph that is not an
FRG, it is corrected by a simple heuristic: for every
node in the reduced graph Gred that has more than
1We use the Gurobi optimization package in all experiments.
l
l
l
l
l
l
l
?0.8 ?0.6 ?0.4 ?0.2 0.0
10
50
100
500
500
0
500
00
?lambda
sec
l Exact?graphLP?relaxGNFTNF
Figure 5: Run-time in seconds for various ?? values.
one parent, we choose from its current parents the
single one whose SCC is composed of the largest
number of nodes in G.
We evaluate algorithms by comparing the set of
gold standard edges with the set of edges learned by
each algorithm. We measure recall, precision and
F1 for various values of the sparseness parameter
?, and compute the area under the precision-recall
Curve (AUC) generated. Efficiency is evaluated by
comparing run-times.
5.2 Results
We first focus on run-times and show that TNF is
efficient and has potential to scale to large data sets.
Figure 5 compares run-times2 of Exact-graph,
GNF, TNF, and LP-relax as ?? increases and the
graph becomes denser. Note that the y-axis is in
logarithmic scale. Clearly, Exact-graph is extremely
slow and run-time increases quickly. For ? = 0.3
run-time was already 12 hours and we were unable
to obtain results for ? < 0.3, while in TNF we easily
got a solution for any ?. When ? = 0.6, where both
Exact-graph and TNF achieve best F1, TNF is 10
times faster than Exact-graph. When ? = 0.5, TNF
is 50 times faster than Exact-graph and so on. Most
importantly, run-time for GNF and TNF increases
much more slowly than for Exact-graph.
2Run on a multi-core 2.5GHz server with 32GB of RAM.
123
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
0.0
0.2
0.4
0.6
0.8
1.0
recall
prec
isio
n
l
l
ll l
l
l
l l
l
l
l l l
l
l l l
l
l
l
l Exact?graphTNFNo?trans
Figure 6: Precision (y-axis) vs. recall (x-axis) curve.
Maximal F1 on the curve is .43 for Exact-graph, .41 for
TNF, and .34 for No-trans. AUC in the recall range 0-0.5
is .32 for Exact-graph, .31 for TNF, and .26 for No-trans.
Run-time of LP-relax is also bad compared to
TNF and GNF. Run-time increases more slowly than
Exact-graph, but still very fast comparing to TNF.
When ? = 0.6, LP-relax is almost 10 times slower
than TNF, and when ? = ?0.1, LP-relax is 200
times slower than TNF. This points to the difficulty
of scaling LP-relax to large graphs.
As for the quality of learned graphs, Figure 6 pro-
vides a precision-recall curve for Exact-graph, TNF
and No-trans (GNF and LP-relax are omitted from
the figure and described below to improve readabil-
ity). We observe that both Exact-graph and TNF
substantially outperform No-trans and that TNF?s
graph quality is only slightly lower than Exact-graph
(which is extremely slow). Following Berant et al,
we report in the caption the maximal F1 on the curve
and AUC in the recall range 0-0.5 (the widest range
for which we have results for all algorithms). Note
that compared to Exact-graph, TNF reduces AUC by
a point and the maximal F1 score by 2 points only.
GNF results are almost identical to those of TNF
(maximal F1=0.41, AUC: 0.31), and in fact for all
? configurations TNF outperforms GNF by no more
than one F1 point. As for LP-relax, results are just
slightly lower than Exact-graph (maximal F1: 0.43,
AUC: 0.32), but its output is not a transitive graph,
and as shown above run-time is quite slow. Last, we
note that the results of Exact-forest are almost iden-
tical to Exact-graph (maximal F1: 0.43), illustrating
that assuming that entailment graphs are FRGs (Sec-
tion 3) is reasonable in this data set.
To conclude, TNF learns transitive entailment
graphs of good quality much faster than Exact-
graph. Our experiment utilized an available data
set of moderate size; However, we expect TNF to
scale to large data sets (that are currently unavail-
able), where other baselines would be impractical.
6 Conclusion
Learning large and accurate resources of entailment
rules is essential in many semantic inference appli-
cations. Employing transitivity has been shown to
improve rule learning, but raises issues of efficiency
and scalability.
The first contribution of this paper is a novel mod-
eling assumption that entailment graphs are very
similar to FRGs, which is analyzed and validated
empirically. The main contribution of the paper is
an efficient polynomial approximation algorithm for
learning entailment rules, which is based on this
assumption. We demonstrate empirically that our
method is by orders of magnitude faster than the
state-of-the-art exact algorithm, but still produces an
output that is almost as good as the optimal solution.
We suggest our method as an important step to-
wards scalable acquisition of precise entailment re-
sources. In future work, we aim to evaluate TNF on
large graphs that are automatically generated from
huge corpora. This of course requires substantial ef-
forts of pre-processing and test-set annotation. We
also plan to examine the benefit of TNF in learning
similar structures, e.g., taxonomies or ontologies.
Acknowledgments
This work was partially supported by the Israel
Science Foundation grant 1112/08, the PASCAL-
2 Network of Excellence of the European Com-
munity FP7-ICT-2007-1-216886, and the Euro-
pean Community?s Seventh Framework Programme
(FP7/2007-2013) under grant agreement no. 287923
(EXCITEMENT). The first author has carried out
this research in partial fulfilment of the requirements
for the Ph.D. degree.
124
References
Alfred V. Aho, Michael R. Garey, and Jeffrey D. Ullman.
1972. The transitive reduction of a directed graph.
SIAM Journal on Computing, 1(2):131?137.
Roni Ben Aharon, Idan Szpektor, and Ido Dagan. 2010.
Generating entailment rules from framenet. In Pro-
ceedings of the 48th Annual Meeting of the Association
for Computational Linguistics.
Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2010. Global learning of focused entailment graphs.
In Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics.
Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2011. Global learning of typed entailment rules. In
Proceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics.
Coyne Bob and Owen Rambow. 2009. Lexpar: A freely
available english paraphrase lexicon automatically ex-
tracted from framenet. In Proceedings of IEEE Inter-
national Conference on Semantic Computing.
Timothy Chklovski and Patrick Pantel. 2004. Verb
ocean: Mining the web for fine-grained semantic verb
relations. In Proceedings of Empirical Methods in
Natural Language Processing.
Thomas H. Cormen, Charles E. leiserson, Ronald L.
Rivest, and Clifford Stein. 2002. Introduction to Al-
gorithms. The MIT Press.
Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan Roth.
2009. Recognizing textual entailment: Rational, eval-
uation and approaches. Natural Language Engineer-
ing, 15(4):1?17.
Quang Do and Dan Roth. 2010. Constraints based tax-
onomic relation classification. In Proceedings of Em-
pirical Methods in Natural Language Processing.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In Proceedings of Empirical Methods in Nat-
ural Language Processing.
J. R. Finkel and C. D. Manning. 2008. Enforcing transi-
tivity in coreference resolution. In Proceedings of the
46th Annual Meeting of the Association for Computa-
tional Linguistics.
Michael R. Garey and David S. Johnson. 1979. Comput-
ers and Intractability: A Guide to the Theory of NP-
Completeness. W. H. Freeman.
Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules for question answering. Natural Language
Engineering, 7(4):343?360.
Xiao Ling and Dan S. Weld. 2010. Temporal informa-
tion extraction. In Proceedings of the 24th AAAI Con-
ference on Artificial Intelligence.
Andre Martins, Noah Smith, and Eric Xing. 2009. Con-
cise integer linear programming formulations for de-
pendency parsing. In Proceedings of the 47th Annual
Meeting of the Association for Computational Linguis-
tics.
Hoifung Poon and Pedro Domingos. 2010. Unsuper-
vised ontology induction from text. In Proceedings of
the 48th Annual Meeting of the Association for Com-
putational Linguistics.
Deepak Ravichandran and Eduard Hovy. 2002. Learning
surface text patterns for a question answering system.
In Proceedings of the 40th Annual Meeting of the As-
sociation for Computational Linguistics.
Sebastian Riedel and James Clarke. 2006. Incremental
integer linear programming for non-projective depen-
dency parsing. In Proceedings of Empirical Methods
in Natural Language Processing.
Stefan Schoenmackers, Jesse Davis, Oren Etzioni, and
Daniel S. Weld. 2010. Learning first-order horn
clauses from web text. In Proceedings of Empirical
Methods in Natural Language Processing.
Satoshi Sekine. 2005. Automatic paraphrase discovery
based on context and keywords between ne pairs. In
Proceedings of IWP.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemptive
information extraction using unrestricted relation dis-
covery. In Proceedings of the Human Language Tech-
nology Conference of the NAACL, Main Conference.
Rion Snow, Dan Jurafsky, and Andrew Y. Ng. 2006.
Semantic taxonomy induction from heterogenous ev-
idence. In Proceedings of the 44th Annual Meeting of
the Association for Computational Linguistics.
Idan Szpektor and Ido Dagan. 2008. Learning entail-
ment rules for unary templates. In Proceedings of the
22nd International Conference on Computational Lin-
guistics.
Idan Szpektor and Ido Dagan. 2009. Augmenting
wordnet-based inference with argument mapping. In
Proceedings of TextInfer.
Idan Szpektor, Hristo Tanev, Ido Dagan, and Bonaven-
tura Coppola. 2004. Scaling web-based acquisition
of entailment relations. In Proceedings of Empirical
Methods in Natural Language Processing.
Alexander Yates and Oren Etzioni. 2009. Unsupervised
methods for determining object and relation synonyms
on the web. Journal of Artificial Intelligence Research,
34:255?296.
125
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 283?291,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Efficient Search for Transformation-based Inference
Asher Stern?, Roni Stern?, Ido Dagan?, Ariel Felner?
? Computer Science Department, Bar-Ilan University
? Information Systems Engineering, Ben Gurion University
astern7@gmail.com
roni.stern@gmail.com
dagan@cs.biu.ac.il
felner@bgu.ac.il
Abstract
This paper addresses the search problem in
textual inference, where systems need to infer
one piece of text from another. A prominent
approach to this task is attempts to transform
one text into the other through a sequence
of inference-preserving transformations, a.k.a.
a proof, while estimating the proof?s valid-
ity. This raises a search challenge of find-
ing the best possible proof. We explore this
challenge through a comprehensive investi-
gation of prominent search algorithms and
propose two novel algorithmic components
specifically designed for textual inference: a
gradient-style evaluation function, and a local-
lookahead node expansion method. Evalua-
tions, using the open-source system, BIUTEE,
show the contribution of these ideas to search
efficiency and proof quality.
1 Introduction
In many NLP settings it is necessary to identify
that a certain semantic inference relation holds be-
tween two pieces of text. For example, in para-
phrase recognition it is necessary to identify that the
meanings of two text fragments are roughly equiva-
lent. In passage retrieval for question answering, it
is needed to detect text passages from which a sat-
isfying answer can be inferred. A generic formula-
tion for the inference relation between two texts is
given by the Recognizing Textual Entailment (RTE)
paradigm (Dagan et al, 2005), which is adapted here
for our investigation. In this setting, a system is
given two text fragments, termed ?text? (T) and ?hy-
pothesis? (H), and has to recognize whether the hy-
pothesis is entailed by (inferred from) the text.
An appealing approach to such textual inferences
is to explicitly transform T into H, using a sequence
of transformations (Bar-Haim et al, 2007; Harmel-
ing, 2009; Mehdad, 2009; Wang and Manning,
2010; Heilman and Smith, 2010; Stern and Dagan,
2011). Examples of such possible transformations
are lexical substitutions (e.g. ?letter?? ?message?)
and predicate-template substitutions (e.g. ?X [verb-
active] Y? ? ?Y [verb-passive] by X?), which are
based on available knowledge resources. Another
example is coreference substitutions, such as replac-
ing ?he? with ?the employee? if a coreference re-
solver has detected that these two expressions core-
fer. Table 1 exemplifies this approach for a particu-
lar T-H pair. The rationale behind this approach is
that each transformation step should preserve infer-
ence validity, such that each text generated along this
process is indeed inferred from the preceding one.
An inherent aspect in transformation-based infer-
ence is modeling the certainty that each inference
step is valid. This is usually achieved by a cost-
based or probabilistic model, which quantifies con-
fidence in the validity of each individual transfor-
mation and consequently of the complete chain of
inference.
Given a set of possible transformations, there may
be many transformation sequences that would trans-
form T to H. This creates a very large search space,
where systems have to find the ?best? transformation
sequence ? the one of lowest cost, or of highest prob-
ability. To the best of our knowledge, this search
challenge has not been investigated yet in a substan-
283
# Operation Generated text
0 - He received the letter from the secretary.
1 Coreference substitution The employee received the letter from the secretary.
2 X received Y from Z? Y was sent to X by Z The letter was sent to the employee by the secretary.
3 Y [verb-passive] by X? X [verb-active] Y The secretary sent the letter to the employee.
4 X send Y? X deliver Y The secretary delivered the letter to the employee.
5 letter? message The secretary delivered the message to the employee.
Table 1: A sequence of transformations that transform the text ?He received the letter from the secretary.? into the
hypothesis ?The secretary delivered the message to the employee.?. The knowledge required for such transformations
is often obtained from available knowledge resources and NLP tools.
tial manner: each of the above-cited works described
the search method they used, but none of them tried
alternative methods while evaluating search perfor-
mance. Furthermore, while experimenting with our
own open-source inference system, BIUTEE1, we
observed that search efficiency is a major issue, of-
ten yielding practically unsatisfactory run-times.
This paper investigates the search problem in
transformation-based textual inference, naturally
falling within the framework of heuristic AI (Ar-
tificial Intelligence) search. To facilitate such in-
vestigation, we formulate a generic search scheme
which incorporates many search variants as special
cases and enable a meaningful comparison between
the algorithms. Under this framework, we identify
special characteristics of the textual inference search
space, that lead to the development of two novel al-
gorithmic components: a special lookahead method
for node expansion, named local lookahead, and a
gradient-based evaluation function. Together, they
yield a new search algorithm, which achieved sub-
stantially superior search performance in our evalu-
ations.
The remainder of this paper is organized as
follows. Section 2 provides an overview of
transformation-based inference systems, AI search
algorithms, and search methods realized in prior in-
ference systems. Section 3 formulates the generic
search scheme that we have investigated, which cov-
ers a broad range of known algorithms, and presents
our own algorithmic contributions. These new algo-
rithmic contributions were implemented in our sys-
tem, BIUTEE. In Section 4 we evaluate them empir-
ically, and show that they improve search efficiency
as well as solution?s quality. Search performance is
evaluated on two recent RTE benchmarks, in terms
1www.cs.biu.ac.il/?nlp/downloads/biutee
of runtime, ability to find lower-cost transformation
chains and impact on overall inference.
2 Background
Applying sequences of transformations to recognize
textual inference was suggested by several works.
Such a sequence may be referred to as a proof, in
the sense that it is used to ?prove? the hypothesis
from the text. Although various works along this
line differ from each other in several respects, many
of them share the common challenge of finding an
optimal proof. The following paragraphs review the
major research approaches in this direction. We fo-
cus on methods that perform transformations over
parse trees, and highlight the search challenge with
which they are faced.
2.1 Transformation-based textual inference
Several researchers suggested using various types
of transformations in order to derive H from T .
Some suggested a set of predefined transforma-
tions, for example, insertion, deletion and substitu-
tion of parse-tree nodes, by which any tree can be
transformed to any other tree. These transforma-
tions were used by the open-source system EDITS
(Mehdad, 2009), and by (Wang and Manning, 2010).
Since the above mentioned transformations are lim-
ited in capturing certain interesting and prevalent
semantic phenomena, an extended set of tree edit
operations (e.g., relabel-edge, move-sibling, etc.)
was proposed by Heilman and Smith (2010). Simi-
larly, Harmeling (2009) suggested a heuristic set of
28 transformations, which include various types of
node-substitutions as well as restructuring of the en-
tire parse-tree.
In contrast to such predefined sets of transfor-
mations, knowledge oriented approaches were sug-
284
gested by Bar-Haim et al (2007) and de Salvo Braz
et al (2005). Their transformations are defined by
knowledge resources that contain a large amount of
entailment rules, or rewrite rules, which are pairs of
parse-tree fragments that entail one another. Typical
examples for knowledge resources of such rules are
DIRT (Lin and Pantel, 2001), and TEASE (Szpek-
tor et al, 2004), as well as syntactic transforma-
tions constructed manually. In addition, they used
knowledge-based lexical substitutions.
However, when only knowledge-based transfor-
mations are allowed, transforming the text into the
hypothesis is impossible in many cases. This limi-
tation is dealt by our open-source integrated frame-
work, BIUTEE (Stern and Dagan, 2011), which
incorporates knowledge-based transformations (en-
tailment rules) with a set of predefined tree-edits.
Motivated by the richer structure and search space
provided by BIUTEE, we adopted it for our empiri-
cal investigations.
The semantic validity of transformation-based in-
ference is usually modeled by defining a cost or
a probability estimation for each transformation.
Costs may be defined manually (Kouylekov and
Magnini, 2005), but are usually learned automati-
cally (Harmeling, 2009; Mehdad, 2009; Wang and
Manning, 2010; Heilman and Smith, 2010; Stern
and Dagan, 2011). A global cost (or probability esti-
mation) for a complete sequence of transformations
is typically defined as the sum of the costs of the
involved transformations.
Finding the lowest cost proof, as needed for de-
termining inference validity, is the focus of our re-
search. Textual inference systems limited to the
standard tree-edit operations (insertion, deletion,
substitution) can use an exact algorithm that finds
the optimal solution in polynomial time under cer-
tain constraints (Bille, 2005). Nevertheless, for the
extended set of transformations it is unlikely that ef-
ficient exact algorithms for finding lowest-cost se-
quences are available (Heilman and Smith, 2010).
In this harder case, the problem can be viewed
as an AI search problem. Each state in the search
space is a parse-tree, where the initial state is the text
parse-tree, the goal state is the hypothesis parse-tree,
and we search for the shortest (in terms of costs)
path of transformations from the initial state to the
goal state. Next we briefly review major concepts
from the field of AI search and summarize some rel-
evant proposed solutions.
2.2 Search Algorithms
Search algorithms find a path from an initial state to
a goal state by expanding and generating states in
a search space. The term generating a state refers
to creating a data structure that represents it, while
expanding a state means generating all its immedi-
ate derivations. In our domain, each state is a parse
tree, which is expanded by performing all applicable
transformations.
Best-first search is a common search framework.
It maintains an open list (denoted hereafter as
OPEN) containing all the generated states that have
not been expanded yet. States in OPEN are prior-
itized by an evaluation function, f(s). A best-first
search algorithm iteratively removes the best state
(according to f(s)) from OPEN, and inserts new
states being generated by expanding this best state.
The evaluation function is usually a linear combina-
tion of the shortest path found from the start state to
state s, denoted by g(s), and a heuristic function, de-
noted by h(s), which estimates the cost of reaching
a goal state from s.
Many search algorithms can be viewed as spe-
cial cases or variations of best-first search. The
well-known A* (Hart et al, 1968). algorithm is
a best-first search that uses an evaluation function
f(s) = g(s) + h(s). Weighted A* (Pohl, 1970)
uses an evaluation function f(s) = w ? g(s) + h(s),
where w is a parameter, while pure heuristic search
uses f(s) = h(s). K-BFS (Felner et al, 2003) ex-
pands k states in each iteration. Beam search (Furcy
and Koenig, 2005; Zhou and Hansen, 2005) limits
the number of states stored in OPEN, while Greedy
search limits OPEN to contain only the single best
state generated in the current iteration.
The search algorithm has crucial impact on the
quality of proof found by a textual inference system,
as well as on its efficiency. Next, we describe search
strategies used in prior works for textual inference.
2.3 Search in prior inference models
In spite of being a fundamental problem, prior so-
lutions to the search challenge in textual inference
were mostly ad-hoc. Furthermore, there was no in-
vestigation of alternative search methods, and no
285
evaluation of search efficiency and quality was re-
ported. For example, in (Harmeling, 2009) the order
by which the transformations are performed is pre-
determined, and in addition many possible deriva-
tions are discarded, to prevent exponential explo-
sion. Handling the search problem in (Heilman and
Smith, 2010) was by a variant of greedy search,
driven by a similarity measure between the current
parse-tree and the hypothesis, while ignoring the
cost already paid. In addition, several constraints on
the search space were implemented. In the earlier
version of BIUTEE (Stern and Dagan, 2011)2, a ver-
sion of beam search was incorporated, named here-
after BIUTEE-orig. This algorithm uses the evalua-
tion function f(s) = g(s) +wi ?h(s), where in each
iteration (i) the value of w is increased, to ensure
successful termination of the search. Nevertheless,
its efficiency and quality were not investigated.
In this paper we consider several prominent
search algorithms and evaluate their quality. The
evaluation concentrates on two measures: the run-
time required to find a proof, and proof quality (mea-
sured by its cost). In addition to evaluating standard
search algorithms we propose two novel compo-
nents specifically designed for proof-based textual-
inference and evaluate their contribution.
3 Search for Textual Inference
In this section we formalize our search problem and
specify a unifying search scheme by which we test
several search algorithms in a systematic manner.
Then we propose two novel algorithmic components
specifically designed for our problem. We conclude
by presenting our new search algorithm which com-
bines these two ideas.
3.1 Inference and search space formalization
Let t be a parse tree, and let o be a transforma-
tion. Applying o on t, yielding t?, is denoted by
t `o t?. If the underlying meaning of t? can in-
deed be inferred from the underlying meaning of t,
then we refer to the application of o as valid. Let
O = (o1, o2, . . . on) be a sequence of transforma-
tions, such that t0 `o1 t1 `o2 t2 . . . `on tn. We
write t0 `O tn, and say that tn can be proven from
2More details in www.cs.biu.ac.il/?nlp/
downloads/biutee/search_ranlp_2011.pdf
t0 by applying the sequence O. The proof might be
valid, if all the transformations involved are valid, or
invalid otherwise.
An inference system specifies a cost, C(o), for
each transformation o. In most systems the costs
are automatically learned. The interpretation of a
high cost is that it is unlikely that applying o will be
valid. The cost of a sequence O = (o1, o2, . . . on)
is defined as
?n
i=1C(o) (or ,in some systems,?n
i=1C(o)). Denoting by tT and tH the text parse
tree and the hypothesis parse tree, a proof system
has to find a sequenceO with minimal cost such that
tT `O tH. This forms a search problem of finding
the lowest-cost proof among all possible proofs.
The search space is defined as follows. A state
s is a parse-tree. The start state is tT and the goal
state is tH. In some systems any state s in which tH
is embedded is considered as goal as well.
Given a state s, let {o(1), o(2) . . . o(m)} be m
transformations that can be applied on it. Expand-
ing s means generating m new states, s(j), j =
1 . . .m, such that s `o(j) s
(j). The number m is
called branching factor. Our empirical observations
on BIUTEE showed that its branching factor ranges
from 2-3 for some states to about 30 for other states.
3.2 Search Scheme
Our empirical investigation compares a range
prominent search algorithms, described in Section 2.
To facilitate such investigation, we formulate them
in the following unifying scheme (Algorithm 1).
Algorithm 1 Unified Search Scheme
Parameters: f(?): state evaluation function
expand(?): state generation function
Input: kexpand: # states expanded in each iteration
kmaintain: # states in OPEN in each iteration
sinit: initial state
1: OPEN? {sinit}
2: repeat
3: BEST? kexpand best (according to f ) states in OPEN
4: GENERATED?
?
s?BEST expand(s)
5: OPEN? (OPEN \ Best) ? GENERATED
6: OPEN? kmaintain best (according to f ) states in OPEN
7: until BEST contains the goal state
Initially, the open list, OPEN contains the initial
state. Then, the best kexpand states from OPEN are
chosen, according to the evaluation function f(s)
286
Algorithm f() expand() kmaintain kexpand
A* g + h regular ? 1
Weighted A* g+w ?h regular ? 1
K-Weighted A* g+w ?h regular ? k > 1
Pure Heuristic h regular ? 1
Greedy g+w ?h regular 1 1
Beam g + h regular k > 1 k > 1
BIUTEE-orig g+wi?h regular k > 1 k > 1
LLGS ?g?h
local-
lookahead
1 1
Table 2: Search algorithm mapped to the unified search
scheme. ?Regular? means generating all the states which
can be generated by applying a single transformation. Al-
ternative greedy implementations use f = h.
(line 3), and expanded using the expansion func-
tion expand(s). In classical search algorithms,
expand(s) means generating a set of states by ap-
plying all the possible state transition operators to s.
Next, we remove from OPEN the states which were
expanded, and add the newly generated states. Fi-
nally, we keep in OPEN only the best kmaintain states,
according to the evaluation function f(s) (line 6).
This process repeats until the goal state is found in
BEST (line 7). Table 2 specifies how known search
algorithms, described in Section 2, fit into the uni-
fied search scheme.
Since runtime efficiency is crucial in our domain,
we focused on improving one of the simple but fast
algorithms, namely, greedy search. To improve the
quality of the proof found by greedy search, we in-
troduce new algorithmic components for the expan-
sion and evaluation functions, as described in the
next two subsections, while maintaining efficiency
by keeping kmaintain=kexpand= 1
3.3 Evaluation function
In most domains, the heuristic function h(s) esti-
mates the cost of the minimal-cost path from a cur-
rent state, s, to a goal state. Having such a function,
the value g(s) + h(s) estimates the expected total
cost of a search path containing s. In our domain, it
is yet unclear how to calculate such a heuristic func-
tion. Given a state s, systems typically estimate the
difference (the gap) between s and the hypothesis
tH (the goal state). In BIUTEE this is quantified by
the number of parse-tree nodes and edges of tH that
do not exist in s. However, this does not give an
estimation for the expected cost of the path (the se-
quence of transformations) from s to the goal state.
This is because the number of nodes and edges that
can be changed by a single transformation can vary
from a single node to several nodes (e.g., by a lexi-
cal syntactic entailment rule). Moreover, even if two
transformations change the same number of nodes
and edges, their costs might be significantly differ-
ent. Consequently, the measurement of the cost ac-
cumulated so far (g(s)) and the remaining gap to tH
(h(s)) are unrelated. We note that a more sophisti-
cated heuristic function was suggested by Heilman
and Smith (2010), based on tree-kernels. Neverthe-
less, this heuristic function, serving as h(s), is still
unrelated to the transformation costs (g(s)).
We therefore propose a novel gradient-style func-
tion to overcome this difficulty. Our function is
designed for a greedy search in which OPEN al-
ways contains a single state, s. Let sj be a state
generated from s, the cost of deriving sj from s
is ?g(sj) ? g(sj) ? g(s). Similarly, the reduc-
tion in the value of the heuristic function is de-
fined ?h(sj) ? h(s) ? h(sj). Now, we define
f?(sj) ?
?g(sj)
?h(sj)
. Informally, this function mea-
sures how costly it is to derive sj relative to the
obtained decrease in the remaining gap to the goal
state. For the edge case in which h(s)? h(sj) ? 0,
we define f?(sj) =?. Empirically, we show in our
experiments that the function f?(s) performs better
than the traditional functions f(s) = g(s) + h(s)
and fw(s) = g(s) + w ? h(s) in our domain.
3.4 Node expansion method
When examining the proofs produced by the above
mentioned algorithms, we observed that in many
cases a human could construct proofs that exhibit
some internal structure, but were not revealed by the
algorithms. Observe, for example, the proof in Ta-
ble 1. It can be seen that transformations 2,3 and
4 strongly depend on each other. Applying trans-
formation 3 requires first applying transformation 2,
and similarly 4 could not be applied unless 2 and 3
are first applied. Moreover, there is no gain in apply-
ing transformations 2 and 3, unless transformation 4
is applied as well. On the other hand, transformation
1 does not depend on any other transformation. It
may be performed at any point along the proof, and
287
moreover, changing all other transformations would
not affect it.
Carefully examining many examples, we general-
ized this phenomenon as follows. Often, a sequence
of transformations can be decomposed into a set of
coherent subsequences of transformations, where in
each subsequence the transformations strongly de-
pend on each other, while different subsequences are
independent. This phenomenon can be utilized in
the following way: instead of searching for a com-
plete sequence of transformations that transform tT
into tH, we can iteratively search for independent co-
herent subsequences of transformations, such that a
combination of these subsequences will transform
tT into tH. This is somewhat similar to the tech-
nique of applying macro operators, which is used in
automated planning (Botea et al, 2005) and puzzle
solving (Korf, 1985).
One technique for finding such subsequences is
to perform, for each state being expanded, a brute-
force depth-limited search, also known as looka-
head (Russell and Norvig, 2010; Bulitko and Lus-
trek, 2006; Korf, 1990; Stern et al, 2010). How-
ever, performing such lookahead might be slow if
the branching factor is large. Fortunately, in our
domain, coherent subsequences have the following
characteristic which can be leveraged: typically, a
transformation depends on a previous one only if
it is performed over some nodes which were af-
fected by the previous transformation. Accordingly,
our proposed algorithm searches for coherent subse-
quences, in which each subsequent transformation
must be applied to nodes that were affected by the
previous transformation.
Formally, let o be a transformation that has been
applied on a tree t, yielding t?. ?affected(o, t?) denotes
the subset of nodes in t? which were affected (modi-
fied or created) by the application of o.
Next, for a transformation o, applied on a parse
tree t, we define ?required(t, o) as the subset of t?s
nodes required for applying o (i.e., in the absence of
these nodes, o could not be applied).
Finally, let t be a parse-tree and ? be a subset of
its nodes. enabled ops(t, ?) is a function that re-
turns the set of the transformations that can be ap-
plied on t, which require at least one of the nodes
in ?. Formally, enabled ops(t, ?) ? {o ? O :
? ? ?required(t, o) 6= ?}, where O is the set of trans-
formations that can be applied on t. In our algo-
rithm, ? is the set of nodes that were affected by the
preceding transformation of the constructed subse-
quence.
The recursive procedure described in Algorithm 2
generates all coherent subsequences of lengths up to
d. It should be initially invoked with t - the current
state (parse tree) being expanded, ? - the set of all its
nodes, d - the maximal required length, and ? as an
empty initial sequence. We useO?o as concatenation
of an operation o to a subsequence O.
Algorithm 2 local-lookahead (t,?,d,O)
1: if d = 0 then
2: return ? (empty-set)
3: end if
4: SUBSEQUENCES? ?
5: for all o ? enabled ops(t, ?) do
6: Let t `o t?
7: Add {O?o}?local-lookahead(t?, ?affected(o, t?), d?1, O?
o) to SUBSEQUENCES
8: end for
9: return SUBSEQUENCES
The loop in lines 5 - 8 iterates over transforma-
tions that can be applied on the input tree, t, requir-
ing the same nodes that were affected by the pre-
vious transformation of the subsequence being con-
structed. Note that in the first call enabled ops(t, ?)
contain all operations that can be applied on t, with
no restriction. Applying an operation o results in a
new subsequence O ? o. This subsequence will be
part of the set of subsequences found by the proce-
dure. In addition, it will be used in the next recur-
sive call as the prefix of additional (longer) subse-
quences.
3.5 Local-lookahead gradient search
We are now ready to define our new algorithm
LOCAL-LOOKAHEAD GRADIENT SEARCH
(LLGS). In LLGS, like in greedy search,
kmaintain=kexpand= 1. expand(s) is defined to
return all states generated by subsequences found
by the local-lookahead procedure, while the evalua-
tion function is defined as f = f? (see last row of
Table 2).
4 Evaluation
In this section we first evaluate the search perfor-
mance in terms of efficiency (run time), the quality
288
of the found proofs (as measured by proof cost), and
overall inference performance achieved through var-
ious search algorithms. Finally we analyze the con-
tribution of our two novel components.
4.1 Evaluation settings
We performed our experiments on the last two
published RTE datasets: RTE-5 (2009) and RTE-
6 (2010). The RTE-5 dataset is composed of a
training and test corpora, each containing 600 text-
hypothesis pairs, where in half of them the text en-
tails the hypothesis and in the other half it does
not. In RTE-6, each of the training and test cor-
pora consists of 10 topics, where each topic con-
tains 10 documents. Each corpus contains a set of
hypotheses (211 in the training dataset, and 243 in
the test dataset), along with a set of candidate en-
tailing sentences for each hypothesis. The system
has to find for each hypothesis which candidate sen-
tences entail it. To improve speed and results, we
used the filtering mechanism suggested by (Mirkin
et al, 2009), which filters the candidate sentences
by the Lucene IR engine3. Thus, only top 20 candi-
dates per hypothesis were tested
Evaluation of each of the algorithms was
performed by running BIUTEE while replacing
BIUTEE-orig with this algorithm. We employed a
comprehensive set of knowledge resources (avail-
able in BIUTEE?s web site): WordNet (Fellbaum,
1998), Directional similarity (Kotlerman et al,
2010), DIRT (Lin and Pantel, 2001) and generic syn-
tactic rules. In addition, we used coreference substi-
tutions, detected by ArkRef4.
We evaluated several known algorithms, de-
scribed in Table 2 above, as well as BIUTEE-orig.
The latter is a strong baseline, which outperforms
known search algorithms in generating low cost
proofs. We compared all the above mentioned al-
gorithms to our novel one, LLGS.
We used the training dataset for parameter tun-
ing, which controls the trade-off between speed and
quality. For weighted A*, as well as for greedy
search, we used w = 6.0, since, for a few instances,
lower values of w resulted in prohibitive runtime.
For beam search we used k = 150, since higher val-
3http://lucene.apache.org
4www.ark.cs.cmu.edu/ARKref/ See (Haghighi and
Klein, 2009)
ues of k did not improve the proof cost on the train-
ing dataset. The value of d in LLGS was set to 3.
d = 4 yielded the same proof costs, but was about 3
times slower.
Since lower values of w could be used by
weighted A* for most instances, we also ran ex-
periments where we varied the value of w accord-
ing to the dovetailing method suggested in (Valen-
zano et al, 2010) (denoted dovetailing WA*) as fol-
lows. When weighted A* has found a solution, we
reran it with a new value of w, set to half of the
previous value. The idea is to guide the search for
lower cost solutions. This process was halted when
the total number of states generated by all weighted
A* instances exceeded a predefined constant (set to
10, 000).
4.2 Search performance
This experiment evaluates the search algorithms in
both efficiency (run-time) and proof quality. Effi-
ciency is measured by the average CPU (Intel Xeon
2.5 GHz) run-time (in seconds) for finding a com-
plete proof for a text-hypothesis instance, and by the
average number of generated states along the search.
Proof quality is measured by its cost.
The comparison of costs requires that all experi-
ments are performed on the same model which was
learned during training. Thus, in the training phase
we used the original search of BIUTEE, and then ran
the test phase with each algorithm separately. The
results, presented in Table 3, show that our novel
algorithm, LLGS, outperforms all other algorithms
in finding lower cost proofs. The second best is
BIUTEE-orig which is much slower by a factor of
3 (on RTE-5) to 8 (on RTE-6)5. While inherently
fast algorithms, particularly greedy and pure heuris-
tic, achieve faster running times, they achieve lower
proof quality, as well as lower overall inference per-
formance (see next subsection).
4.3 Overall inference performance
In this experiment we test whether, and how much,
finding better proofs, by a better search algorithm,
improves overall success rate of the RTE system.
Table 4 summarizes the results (accuracy in RTE-5
5Calculating T-test, we found that runtime improvement is
statistically significant with p < 0.01, and p < 0.052 for cost
improvement over BIUTEE-orig.
289
Algorithm Avg. time
Avg.
generated
Avg. cost
Weighted A* 0.22 / 0.09 301 / 143 1.11 / 10.52
Dovetailing
WA*
7.85 / 8.53 9797 / 9979 1.05 / 10.28
Greedy 0.20 / 0.10 468 / 158 1.10 / 10.55
Pure heuristic 0.09 / 0.10 123 / 167 1.35 / 12.51
Beam search 20.53 / 9.48 43925 / 18992 1.08 / 10.52
BIUTEE-orig 7.86 / 14.61 14749 / 22795 1.03 / 10.28
LLGS 2.76 / 1.72 1722 / 842 0.95 / 10.14
Table 3: Comparison of algorithms on RTE-5 / RTE-6
and F1 in RTE-6). We see that in RTE-5 LLGS out-
performs all other algorithms, and BIUTEE-orig is
the second best. This result is statistically significant
with p < 0.02 according to McNemar test. In RTE-
6 we see that although LLGS tends to finds lower
cost proofs, as shown in Table 3, BIUTEE obtains
slightly lower results when utilizing this algorithm.
Algorithm RTE-5 accuracy % RTE-6 F1 %
Weighted A* 59.50 48.20
Dovetailing WA* 60.83 49.01
Greedy 60.50 48.56
Pure heuristic 60.83 45.70
Beam search 61.33 48.58
BIUTEE-orig 60.67 49.25
LLGS 64.00 49.09
Table 4: Impact of algorithms on system success rate
4.4 Component evaluation
In this experiment we examine separately our two
novel components. We examined f? by running
LLGS with alternative evaluation functions. The re-
sults, displayed in Table 5, show that using f? yields
better proofs and also improves run time.
f Avg. time Avg. cost Accuracy %
f = g + h 3.28 1.06 61.50
f = g + w ? h 3.30 1.07 61.33
f = f? 2.76 0.95 64.0
Table 5: Impact of f? on RTE-5. w = 6.0. Accuracy
obtained by retraining with corresponding f .
Our local-lookahead (Subsection 3.4) was exam-
ined by running LLGS with alternative node expan-
sion methods. One alternative to local-lookahead
is standard expansion by generating all immediate
derivations. Another alternative is to use the stan-
dard lookahead, in which a brute-force depth-limited
search is performed in each iteration, termed here
?exhaustive lookahead?. The results, presented in
Table 6, show that by avoiding any type of looka-
head one can achieve fast runtime, while compro-
mising proof quality. On the other hand, both ex-
haustive and local lookahead yield better proofs and
accuracy, while local lookahead is more than 4 times
faster than exhaustive lookahead.
lookahead Avg. time Avg. cost Accuracy (%)
exhaustive 13.22 0.95 64.0
local 2.76 0.95 64.0
none 0.24 0.97 62.0
Table 6: Impact of local and global lookahead on RTE-5.
Accuracy obtained by retraining with the corresponding
lookahead method.
5 Conclusion
In this paper we investigated the efficiency and proof
quality obtained by various search algorithms. Con-
sequently, we observed special phenomena of the
search space in textual inference and proposed two
novel components yielding a new search algorithm,
targeted for our domain. We have shown empirically
that (1) this algorithm improves run time by factors
of 3-8 relative to BIUTEE-orig, and by similar fac-
tors relative to standard AI-search algorithms that
achieve similar proof quality; and (2) outperforms
all other algorithms in finding low cost proofs.
In future work we plan to investigate other search
paradigms, e.g., Monte-Carlo style approaches
(Kocsis and Szepesva?ri, 2006), which do not fall
under the AI search scheme covered in this paper.
In addition, while our novel components were moti-
vated by the search space of textual inference, we
foresee their potential utility in other application
areas for search, such as automated planning and
scheduling.
Acknowledgments
This work was partially supported by the Israel
Science Foundation grant 1112/08, the PASCAL-
2 Network of Excellence of the European Com-
munity FP7-ICT-2007-1-216886, and the Euro-
pean Community?s Seventh Framework Programme
(FP7/2007-2013) under grant agreement no. 287923
(EXCITEMENT).
290
References
Roy Bar-Haim, Ido Dagan, Iddo Greental, and Eyal
Shnarch. 2007. Semantic inference at the lexical-
syntactic level. In Proceedings of AAAI.
Philip Bille. 2005. A survey on tree edit distance and
related problems. Theoretical Computer Science.
Adi Botea, Markus Enzenberger, Martin Mu?ller, and
Jonathan Schaeffer. 2005. Macro-FF: Improving ai
planning with automatically learned macro-operators.
J. Artif. Intell. Res. (JAIR), 24:581?621.
Vadim Bulitko and Mitja Lustrek. 2006. Lookahead
pathology in real-time path-finding. In proceedings of
AAAI.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005. The pascal recognising textual entailment chal-
lenge. In Proceedings of MLCW.
Rodrigo de Salvo Braz, Roxana Girju, Vasin Pun-
yakanok, Dan Roth, and Mark Sammons. 2005. An
inference model for semantic entailment in natural lan-
guage. In Proceedings of AAAI.
Christiane Fellbaum, editor. 1998. WordNet An Elec-
tronic Lexical Database. The MIT Press, May.
Ariel Felner, Sarit Kraus, and Richard E. Korf. 2003.
KBFS: K-best-first search. Ann. Math. Artif. Intell.,
39(1-2):19?39.
David Furcy and Sven Koenig. 2005. Limited discrep-
ancy beam search. In proceedings of IJCAI.
Aria Haghighi and Dan Klein. 2009. Simple coreference
resolution with rich syntactic and semantic features. In
Proceedings of EMNLP.
Stefan Harmeling. 2009. Inferring textual entailment
with a probabilistically sound calculus. Natural Lan-
guage Engineering.
Peter E. Hart, Nils J. Nilsson, and Bertram Raphael.
1968. A formal basis for the heuristic determination
of minimum cost paths. IEEE Transactions on Sys-
tems Science and Cybernetics, SSC-4(2):100?107.
Michael Heilman and Noah A. Smith. 2010. Tree
edit models for recognizing textual entailments, para-
phrases, and answers to questions. In Proceedings of
NAACL.
Levente Kocsis and Csaba Szepesva?ri. 2006. Bandit
based monte-carlo planning. In proceedings of ECML.
Richard E. Korf. 1985. Macro-operators: A weak
method for learning. Artif. Intell., 26(1):35?77.
Richard E. Korf. 1990. Real-time heuristic search. Artif.
Intell., 42(2-3):189?211.
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-geffet. 2010. Directional distributional
similarity for lexical inference. Natural Language En-
gineering.
Milen Kouylekov and Bernardo Magnini. 2005. Rec-
ognizing textual entailment with tree edit distance al-
gorithms. In Proceedings of Pascal Challenges Work-
shop on Recognising Textual Entailment.
Dekang Lin and Patrick Pantel. 2001. DIRT - discov-
ery of inference rules from text. In Proceedings of
ACM SIGKDD Conference on Knowledge Discovery
and Data Mining.
Yashar Mehdad. 2009. Automatic cost estimation for
tree edit distance using particle swarm optimization.
In Proceedings of the ACL-IJCNLP.
Shachar Mirkin, Roy Bar-Haim, Jonathan Berant, Ido
Dagan, Eyal Shnarch, Asher Stern, and Idan Szpektor.
2009. Addressing discourse and document structure in
the rte search task. In Proceedings of TAC.
Ira Pohl. 1970. Heuristic search viewed as path finding
in a graph. Artificial Intelligence, 1(3-4):193 ? 204.
Stuart Russell and Peter Norvig. 2010. Artificial Intel-
ligence: A Modern Approach. Prentice-Hall, Engle-
wood Cliffs, NJ, 3rd edition edition.
Asher Stern and Ido Dagan. 2011. A confidence model
for syntactically-motivated entailment proofs. In Pro-
ceedings of RANLP.
Roni Stern, Tamar Kulberis, Ariel Felner, and Robert
Holte. 2010. Using lookaheads with optimal best-first
search. In proceedings of AAAI.
Idan Szpektor, Hristo Tanev, Ido Dagan, and Bonaven-
tura Coppola. 2004. Scaling web-based acquisition of
entailment relations. In Proceedings of EMNLP.
Richard Anthony Valenzano, Nathan R. Sturtevant,
Jonathan Schaeffer, Karen Buro, and Akihiro Kishi-
moto. 2010. Simultaneously searching with multiple
settings: An alternative to parameter tuning for subop-
timal single-agent search algorithms. In proceedings
of ICAPS.
Mengqiu Wang and Christopher D. Manning. 2010.
Probabilistic tree-edit models with structured latent
variables for textual entailment and question answer-
ing. In Proceedings of COLING.
Rong Zhou and Eric A. Hansen. 2005. Beam-stack
search: Integrating backtracking with beam search. In
proceedings of ICAPS.
291
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 156?160,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Crowdsourcing Inference-Rule Evaluation
Naomi Zeichner
Bar-Ilan University
Ramat-Gan, Israel
zeichner.naomi@gmail.com
Jonathan Berant
Tel-Aviv University
Tel-Aviv, Israel
jonatha6@post.tau.ac.il
Ido Dagan
Bar-Ilan University
Ramat-Gan, Israel
dagan@cs.biu.ac.il
Abstract
The importance of inference rules to semantic
applications has long been recognized and ex-
tensive work has been carried out to automat-
ically acquire inference-rule resources. How-
ever, evaluating such resources has turned out
to be a non-trivial task, slowing progress in the
field. In this paper, we suggest a framework
for evaluating inference-rule resources. Our
framework simplifies a previously proposed
?instance-based evaluation? method that in-
volved substantial annotator training, making
it suitable for crowdsourcing. We show that
our method produces a large amount of an-
notations with high inter-annotator agreement
for a low cost at a short period of time, without
requiring training expert annotators.
1 Introduction
Inference rules are an important component in se-
mantic applications, such as Question Answering
(QA) (Ravichandran and Hovy, 2002) and Informa-
tion Extraction (IE) (Shinyama and Sekine, 2006),
describing a directional inference relation between
two text patterns with variables. For example, to an-
swer the question ?Where was Reagan raised?? a
QA system can use the rule ?X brought up in Y?X
raised in Y? to extract the answer from ?Reagan was
brought up in Dixon?. Similarly, an IE system can
use the rule ?X work as Y?X hired as Y? to ex-
tract the PERSON and ROLE entities in the ?hiring?
event from ?Bob worked as an analyst for Dell?.
The significance of inference rules has led to sub-
stantial effort into developing algorithms that au-
tomatically learn inference rules (Lin and Pantel,
2001; Sekine, 2005; Schoenmackers et al, 2010),
and generate knowledge resources for inference sys-
tems. However, despite their potential, utilization of
inference rule resources is currently somewhat lim-
ited. This is largely due to the fact that these al-
gorithms often produce invalid rules. Thus, evalu-
ation is necessary both for resource developers as
well as for inference system developers who want to
asses the quality of each resource. Unfortunately, as
evaluating inference rules is hard and costly, there is
no clear evaluation standard, and this has become a
slowing factor for progress in the field.
One option for evaluating inference rule resources
is to measure their impact on an end task, as that is
what ultimately interests an inference system devel-
oper. However, this is often problematic since infer-
ence systems have many components that address
multiple phenomena, and thus it is hard to assess the
effect of a single resource. An example is the Recog-
nizing Textual Entailment (RTE) framework (Dagan
et al, 2009), in which given a text T and a textual
hypothesis H, a system determines whether H can
be inferred from T. This type of evaluation was es-
tablished in RTE challenges by ablation tests (see
RTE ablation tests in ACLWiki) and showed that re-
sources? impact can vary considerably from one sys-
tem to another. These issues have also been noted
by Sammons et al (2010) and LoBue and Yates
(2011). A complementary application-independent
evaluation method is hence necessary.
Some attempts were made to let annotators judge
rule correctness directly, that is by asking them to
judge the correctness of a given rule (Shinyama et
al., 2002; Sekine, 2005). However, Szpektor et al
(2007) observed that directly judging rules out of
context often results in low inter-annotator agree-
ment. To remedy that, Szpektor et al (2007) and
156
Bhagat et al (2007) proposed ?instance-based eval-
uation?, in which annotators are presented with an
application of a rule in a particular context and
need to judge whether it results in a valid inference.
This simulates the utility of rules in an application
and yields high inter-annotator agreement. Unfortu-
nately, their method requires lengthy guidelines and
substantial annotator training effort, which are time
consuming and costly. Thus, a simple, robust and
replicable evaluation method is needed.
Recently, crowdsourcing services such as Ama-
zon Mechanical Turk (AMT) and CrowdFlower
(CF)1 have been employed for semantic inference
annotation (Snow et al, 2008; Wang and Callison-
Burch, 2010; Mehdad et al, 2010; Negri et al,
2011). These works focused on generating and an-
notating RTE text-hypothesis pairs, but did not ad-
dress annotation and evaluation of inference rules.
In this paper, we propose a novel instance-based
evaluation framework for inference rules that takes
advantage of crowdsourcing. Our method substan-
tially simplifies annotation of rule applications and
avoids annotator training completely. The nov-
elty in our framework is two-fold: (1) We simplify
instance-based evaluation from a complex decision
scenario to two independent binary decisions. (2)
We apply methodological principles that efficiently
communicate the definition of the ?inference? rela-
tion to untrained crowdsourcing workers (Turkers).
As a case study, we applied our method to evalu-
ate algorithms for learning inference rules between
predicates. We show that we can produce many an-
notations cheaply, quickly, at good quality, while
achieving high inter-annotator agreement.
2 Evaluating Rule Applications
As mentioned, in instance-based evaluation individ-
ual rule applications are judged rather than rules in
isolation, and the quality of a rule-resource is then
evaluated by the validity of a sample of applications
of its rules. Rule application is performed by finding
an instantiation of the rule left-hand-side in a cor-
pus (termed LHS extraction) and then applying the
rule on the extraction to produce an instantiation of
the rule right-hand-side (termed RHS instantiation).
For example, the rule ?X observe Y?X celebrate Y?
1https://www.mturk.com and http://crowdflower.com
can be applied on the LHS extraction ?they observe
holidays? to produce the RHS instantiation ?they cel-
ebrate holidays?.
The target of evaluation is to judge whether each
rule application is valid or not. Following the stan-
dard RTE task definition, a rule application is con-
sidered valid if a human reading the LHS extrac-
tion is highly likely to infer that the RHS instanti-
ation is true (Dagan et al, 2009). In the aforemen-
tioned example, the annotator is expected to judge
that ?they observe holidays? entails ?they celebrate
holidays?. In addition to this straightforward case,
two more subtle situations may arise. The first is
that the LHS extraction is meaningless. We regard
a proposition as meaningful if a human can easily
understand its meaning (despite some simple gram-
matical errors). A meaningless LHS extraction usu-
ally occurs due to a faulty extraction process (e.g.,
Table 1, Example 2) and was relatively rare in our
case study (4% of output, see Section 4). Such rule
applications can either be extracted from the sam-
ple so that the rule-base is not penalized (since the
problem is in the extraction procedure), or can be
used as examples of non-entailment, if we are in-
terested in overall performance. A second situation
is a meaningless RHS instantiation, usually caused
by rule application in a wrong context. This case is
tagged as non-entailment (for example, applying the
rule ?X observe Y?X celebrate Y? in the context of
the extraction ?companies observe dress code?).
Each rule application therefore requires an answer
to the following three questions: 1) Is the LHS ex-
traction meaningful? 2) Is the RHS instantiation
meaningful? 3) If both are meaningful, does the
LHS extraction entail the RHS instantiation?
3 Crowdsourcing
Previous works using crowdsourcing noted some
principles to help get the most out of the ser-
vice(Wang et al, 2012). In keeping with these find-
ings we employ the following principles: (a) Simple
tasks. The global task is split into simple sub-tasks,
each dealing with a single aspect of the problem. (b)
Do not assume linguistic knowledge by annota-
tors. Task descriptions avoid linguistic terms such
as ?tense?, which confuse workers. (c) Gold stan-
dard validation. Using CF?s built-in methodology,
157
Phrase Meaningful Comments
1) Doctors be treat Mary Yes Annotators are instructed to ignore simple inflectional errors
2) A player deposit an No Bad extraction for the rule LHS ?X deposit Y?
3) humans bring in bed No Wrong context, result of applying ?X turn in Y?X bring in Y? on ?humans turn in bed?
Table 1: Examples of phrase ?meaningfulness? (Note that the comments are not presented to Turkers).
gold standard (GS) examples are combined with ac-
tual annotations to continuously validate annotator
reliability.
We split the annotation process into two tasks,
the first to judge phrase meaningfulness (Questions
1 and 2 above) and the second to judge entailment
(Question 3 above). In Task 1, the LHS extrac-
tions and RHS instantiations of all rule applications
are separated and presented to different Turkers in-
dependently of one another. This task is simple,
quick and cheap and allows Turkers to focus on
the single aspect of judging phrase meaningfulness.
Rule applications for which both the LHS extrac-
tion and RHS instantiation are judged as meaningful
are passed to Task 2, where Turkers need to decide
whether a given rule application is valid. If not for
Task 1, Turkers would need to distinguish in Task 2
between non-entailment due to (1) an incorrect rule
(2) a meaningless RHS instantiation (3) a meaning-
less LHS extraction. Thanks to Task 1, Turkers are
presented in Task 2 with two meaningful phrases and
need to decide only whether one entails the other.
To ensure high quality output, each example is
evaluated by three Turkers. Similarly to Mehdad et
al. (2010) we only use results for which the confi-
dence value provided by CF is greater than 70%.
We now describe the details of both tasks. Our
simplification contrasts with Szpektor et al (2007),
whose judgments for each rule application are simi-
lar to ours, but had to be performed simultaneously
by annotators, which required substantial training.
Task 1: Is the phrase meaningful?
In keeping with the second principle above, the task
description is made up of a short verbal explana-
tion followed by positive and negative examples.
The definition of ?meaningfulness? is conveyed via
examples pointing to properties of the automatic
phrase extraction process, as seen in Table 1.
Task 2: Judge if one phrase is true given another.
As mentioned, rule applications for which both sides
were judged as meaningful are evaluated for entail-
ment. The challenge is to communicate the defini-
tion of ?entailment? to Turkers. To that end the task
description begins with a short explanation followed
by ?easy? and ?hard? examples with explanations,
covering a variety of positive and negative entail-
ment ?types? (Table 2).
Defining ?entailment? is quite difficult when deal-
ing with expert annotators and still more with non-
experts, as was noted by Negri et al (2011). We
therefore employ several additional mechanisms to
get the definition of entailment across to Turkers
and increase agreement with the GS. We run an
initial small test run and use its output to improve
annotation in two ways: First, we take examples
that were ?confusing? for Turkers and add them to
the GS with explanatory feedback presented when
a Turker answers incorrectly. (E.g., the pair (?The
owner be happy to help drivers?, ?The owner assist
drivers?) was judged as entailing in the test run but
only achieved a confidence value of 0.53). Second,
we add examples that were annotated unanimously
by Turkers to the GS to increase its size, allowing
CF to better estimate Turker?s reliability (following
CF recommendations, we aim to have around 10%
GS examples in every run). In Section 4 we show
that these mechanisms improved annotation quality.
4 Case Study
As a case study, we used our evaluation methodol-
ogy to compare four methods for learning entailment
rules between predicates: DIRT (Lin and Pantel,
2001), Cover (Weeds and Weir, 2003), BInc (Szpek-
tor and Dagan, 2008) and Berant et al (2010). To
that end, we applied the methods on a set of one
billion extractions (generously provided by Fader
et al (2011)) automatically extracted from the
ClueWeb09 web crawl2, where each extraction com-
prises a predicate and two arguments. This resulted
in four learned inference rule resources.
2http://lemurproject.org/clueweb09.php/
158
Example Entailed Explanation given to Turkers
LHS: The lawyer sign the contract Yes There is a chance the lawyer has not read the contract, but
most likely that as he signed it, he must have read it.RHS: The lawyer read the contract
LHS: John be related to Jerry No The LHS can be understood from the RHS, but not the
other way around as the LHS is more general.RHS: John be a close relative of Jerry
LHS: Women be at increased risk of cancer No Although the RHS is correct, it cannot be understood from
the LHS.RHS: Women die of cancer
Table 2: Examples given in the description of Task 2.
We randomly sampled 5,000 extractions, and for
each one sampled four rules whose LHS matches the
extraction from the union of the learned resources.
We then applied the rules, which resulted in 20,000
rule applications. We annotated rule applications
using our methodology and evaluated each learn-
ing method by comparing the rules learned by each
method with the annotation generated by CF.
In Task 1, 281 rule applications were annotated as
meaningless LHS extraction, and 1,012 were anno-
tated as meaningful LHS extraction but meaningless
RHS instantiation and so automatically annotated as
non-entailment. 8,264 rule applications were passed
on to Task 2, as both sides were judged meaning-
ful (the remaining 10,443 discarded due to low CF
confidence). In Task 2, 5,555 rule applications were
judged with a high confidence and supplied as out-
put, 2,447 of them as positive entailment and 3,108
as negative. Overall, 6,567 rule applications (dataset
of this paper) were annotated for a total cost of
$1000. The annotation process took about one week.
In tests run during development we experimented
with Task 2 wording and GS examples, seeking to
make the definition of entailment as clear as pos-
sible. To do so we randomly sampled and manu-
ally annotated 200 rule applications (from the initial
20,000), and had Turkers judge them. In our initial
test, Turkers tended to answer ?yes? comparing to
our own annotation, with 0.79 agreement between
their annotation and ours, corresponding to a kappa
score of 0.54. After applying the mechanisms de-
scribed in Section 3, false-positive rate was reduced
from 18% to 6% while false-negative rate only in-
creased from 4% to 5%, corresponding to a high
agreement of 0.9 and kappa of 0.79.
In our test, 63% of the 200 rule applications were
annotated unanimously by the Turkers. Importantly,
all these examples were in perfect agreement with
our own annotation, reflecting their high reliability.
For the purpose of evaluating the resources learned
by the algorithms we used annotations with CF con-
fidence ? 0.7 for which kappa is 0.99.
Lastly, we computed the area under the recall-
precision curve (AUC) for DIRT, Cover, BInc and
Berant et al?s method, resulting in an AUC of 0.4,
0.43, 0.44, and 0.52 respectively. We used the AUC
curve, with number of recall-precision points in the
order of thousands, to avoid tuning a threshold pa-
rameter. Overall, we demonstrated that our evalua-
tion framework allowed us to compare four different
learning methods in low costs and within one week.
5 Discussion
In this paper we have suggested a crowdsourcing
framework for evaluating inference rules. We have
shown that by simplifying the previously-proposed
instance-based evaluation framework we are able to
take advantage of crowdsourcing services to replace
trained expert annotators, resulting in good quality
large scale annotations, for reasonable time and cost.
We have presented the methodological principles we
developed to get the entailment decision across to
Turkers, achieving very high agreement both with
our annotations and between the annotators them-
selves. Using the CrowdFlower forms we provide
with this paper, the proposed methodology can be
beneficial for both resource developers evaluating
their output as well as inference system developers
wanting to assess the quality of existing resources.
Acknowledgments
This work was partially supported by the Israel
Science Foundation grant 1112/08, the PASCAL-
2 Network of Excellence of the European Com-
munity FP7-ICT-2007-1-216886, and the Euro-
pean Communitys Seventh Framework Programme
(FP7/2007-2013) under grant agreement no. 287923
(EXCITEMENT).
159
References
Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2010. Global learning of focused entailment graphs.
In Proceedings of the annual meeting of the Associa-
tion for Computational Linguistics (ACL).
Rahul Bhagat, Patrick Pantel, and Eduard Hovy. 2007.
LEDIR: An unsupervised algorithm for learning di-
rectionality of inference rules. In Proceedings of the
2007 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL).
Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan Roth.
2009. Recognizing textual entailment: Rational, eval-
uation and approaches. Natural Language Engineer-
ing, 15(Special Issue 04):i?xvii.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information
extraction. In Proceedings of the Conference of
Empirical Methods in Natural Language Processing
(EMNLP ?11).
Dekang Lin and Patrick Pantel. 2001. DIRT - discov-
ery of inference rules from text. In Proceedings of the
ACM SIGKDD Conference on Knowledge Discovery
and Data Mining.
Peter LoBue and Alexander Yates. 2011. Types of
common-sense knowledge needed for recognizing tex-
tual entailment. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies (ACL-HLT).
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2010. Towards cross-lingual textual entailment. In
Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics (ACL).
Matteo Negri, Luisa Bentivogli, Yashar Mehdad, Danilo
Giampiccolo, and Alessandro Marchetti. 2011. Di-
vide and conquer: Crowdsourcing the creation of
cross-lingual textual entailment corpora. In Proceed-
ings of the Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP ?11).
Deepak Ravichandran and Eduard Hovy. 2002. Learning
surface text patterns for a question answering system.
In Proceedings of the annual meeting of the Associa-
tion for Computational Linguistics (ACL).
Mark Sammons, V. G. Vinod Vydiswaran, and Dan Roth.
2010. ?ask not what textual entailment can do for
you...?. In Proceedings of the annual meeting of the
Association for Computational Linguistics (ACL).
Stefan Schoenmackers, Oren Etzioni Jesse Davis, and
Daniel S. Weld. 2010. Learning first-order horn
clauses from web text. In Proceedings of the 2010
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP ?10).
Satoshi Sekine. 2005. Automatic paraphrase discovery
based on context and keywords between ne pairs. In
Proceedings of the Third International Workshop on
Paraphrasing (IWP2005).
Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive information extraction using unrestricted relation
discovery. In Proceedings of the main conference
on Human Language Technology Conference of the
North American Chapter of the Association of Com-
putational Linguistics (HLT-NAACL ?06).
Yusuke Shinyama, Satoshi Sekine, and Kiyoshi Sudo.
2002. Automatic paraphrase acquisition from news
articles. In Proceedings of the second international
conference on Human Language Technology Research
(HLT ?02).
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-
drew Y. Ng. 2008. Cheap and fast?but is it good?:
evaluating non-expert annotations for natural language
tasks. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing (EMNLP
?08).
Idan Szpektor and Ido Dagan. 2008. Learning entail-
ment rules for unary templates. In Proceedings of the
22nd International Conference on Computational Lin-
guistics (Coling 2008).
Idan Szpektor, Eyal Shnarch, and Ido Dagan. 2007.
Instance-based evaluation of entailment rule acquisi-
tion. In Proceedings of the annual meeting of the As-
sociation for Computational Linguistics (ACL).
Rui Wang and Chris Callison-Burch. 2010. Cheap facts
and counter-facts. In Proceedings of the NAACL HLT
2010 Workshop on Creating Speech and Language
Data with Amazon?s Mechanical Turk.
Aobo Wang, Cong Duy Vu Hoang, and Min-Yen Kan.
2012. Perspectives on crowdsourcing annotations for
natural language processing. Journal of Language Re-
sources and Evaluation).
Julie Weeds and David Weir. 2003. A general frame-
work for distributional similarity. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP 2003).
160
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 73?78,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
BIUTEE: A Modular Open-Source System for Recognizing Textual
Entailment
Asher Stern
Computer Science Department
Bar-Ilan University
Ramat-Gan 52900, Israel
astern7@gmail.com
Ido Dagan
Computer Science Department
Bar-Ilan University
Ramat-Gan 52900, Israel
dagan@cs.biu.ac.il
Abstract
This paper introduces BIUTEE1, an open-
source system for recognizing textual entail-
ment. Its main advantages are its ability to uti-
lize various types of knowledge resources, and
its extensibility by which new knowledge re-
sources and inference components can be eas-
ily integrated. These abilities make BIUTEE
an appealing RTE system for two research
communities: (1) researchers of end applica-
tions, that can benefit from generic textual in-
ference, and (2) RTE researchers, who can in-
tegrate their novel algorithms and knowledge
resources into our system, saving the time and
effort of developing a complete RTE system
from scratch. Notable assistance for these re-
searchers is provided by a visual tracing tool,
by which researchers can refine and ?debug?
their knowledge resources and inference com-
ponents.
1 Introduction
Recognizing Textual Entailment (RTE) is the task of
identifying, given two text fragments, whether one
of them can be inferred from the other (Dagan et al,
2006). This task generalizes a common problem that
arises in many tasks at the semantic level of NLP.
For example, in Information Extraction (IE), a sys-
tem may be given a template with variables (e.g., ?X
is employed by Y?) and has to find text fragments
from which this template, with variables replaced
by proper entities, can be inferred. In Summariza-
tion, a good summary should be inferred from the
1www.cs.biu.ac.il/?nlp/downloads/biutee
given text, and, in addition, should not contain du-
plicated information, i.e., sentences which can be in-
ferred from other sentences in the summary. Detect-
ing these inferences can be performed by an RTE
system.
Since first introduced, several approaches have
been proposed for this task, ranging from shallow
lexical similarity methods (e.g., (Clark and Har-
rison, 2010; MacKinlay and Baldwin, 2009)), to
complex linguistically-motivated methods, which
incorporate extensive linguistic analysis (syntactic
parsing, coreference resolution, semantic role la-
belling, etc.) and a rich inventory of linguistic and
world-knowledge resources (e.g., (Iftene, 2008; de
Salvo Braz et al, 2005; Bar-Haim et al, 2007)).
Building such complex systems requires substantial
development efforts, which might become a barrier
for new-comers to RTE research. Thus, flexible and
extensible publicly available RTE systems are ex-
pected to significantly facilitate research in this field.
More concretely, two major research communities
would benefit from a publicly available RTE system:
1. Higher-level application developers, who
would use an RTE system to solve inference
tasks in their application. RTE systems for
this type of researchers should be adaptable
for the application specific data: they should
be configurable, trainable, and extensible
with inference knowledge that captures
application-specific phenomena.
2. Researchers in the RTE community, that would
not need to build a complete RTE system for
their research. Rather, they may integrate
73
their novel research components into an ex-
isting open-source system. Such research ef-
forts might include developing knowledge re-
sources, developing inference components for
specific phenomena such as temporal infer-
ence, or extending RTE to different languages.
A flexible and extensible RTE system is ex-
pected to encourage researchers to create and
share their textual-inference components. A
good example from another research area is the
Moses system for Statistical Machine Transla-
tion (SMT) (Koehn et al, 2007), which pro-
vides the core SMT components while being
extended with new research components by a
large scientific community.
Yet, until now rather few and quite limited RTE
systems were made publicly available. Moreover,
these systems are restricted in the types of knowl-
edge resources which they can utilize, and in the
scope of their inference algorithms. For example,
EDITS2 (Kouylekov and Negri, 2010) is a distance-
based RTE system, which can exploit only lexical
knowledge resources. NutCracker3 (Bos and Mark-
ert, 2005) is a system based on logical represen-
tation and automatic theorem proving, but utilizes
only WordNet (Fellbaum, 1998) as a lexical knowl-
edge resource.
Therefore, we provide our open-source textual-
entailment system, BIUTEE. Our system provides
state-of-the-art linguistic analysis tools and exploits
various types of manually built and automatically
acquired knowledge resources, including lexical,
lexical-syntactic and syntactic rewrite rules. Fur-
thermore, the system components, including pre-
processing utilities, knowledge resources, and even
the steps of the inference algorithm, are modu-
lar, and can be replaced or extended easily with
new components. Extensibility and flexibility are
also supported by a plug-in mechanism, by which
new inference components can be integrated with-
out changing existing code.
Notable support for researchers is provided by a
visual tracing tool, Tracer, which visualizes every
step of the inference process as shown in Figures 2
2http://edits.fbk.eu/
3http://svn.ask.it.usyd.edu.au/trac/
candc/wiki/nutcracker
and 3. We will use this tool to illustrate various in-
ference components in the demonstration session.
2 System Description
2.1 Inference algorithm
In this section we provide a high level description of
the inference components. Further details of the al-
gorithmic components appear in references provided
throughout this section.
BIUTEE follows the transformation based
paradigm, which recognizes textual entailment
by converting the text into the hypothesis via a
sequence of transformations. Such a sequence is
often referred to as a proof, and is performed, in our
system, over the syntactic representation of the text
- the text?s parse tree(s). A transformation modifies
a given parse tree, resulting in a generation of a
new parse tree, which can be further modified by
subsequent transformations.
Consider, for example, the following text-
hypothesis pair:
Text: ... Obasanjo invited him to step down as president
... and accept political asylum in Nigeria.
Hypothesis: Charles G. Taylor was offered asylum in
Nigeria.
This text-hypothesis pair requires two major
transformations: (1) substituting ?him? by ?Charles
G. Taylor? via a coreference substitution to an ear-
lier mention in the text, and (2) inferring that if ?X
accept Y? then ?X was offered Y?.
BIUTEE allows many types of transformations,
by which any hypothesis can be proven from any
text. Given a T-H pair, the system finds a proof
which generates H from T, and estimates the proof
validity. The system returns a score which indicates
how likely it is that the obtained proof is valid, i.e.,
the transformations along the proof preserve entail-
ment from the meaning of T.
The main type of transformations is application of
entailment-rules (Bar-Haim et al, 2007). An entail-
ment rule is composed of two sub-trees, termed left-
hand-side and right-hand-side, and is applied on a
parse-tree fragment that matches its left-hand-side,
by substituting the left-hand-side with the right-
hand-side. This formalism is simple yet power-
ful, and captures many types of knowledge. The
simplest type of rules is lexical rules, like car ?
74
vehicle. More complicated rules capture the en-
tailment relation between predicate-argument struc-
tures, like X accept Y ? X was offered
Y. Entailment rules can also encode syntactic
phenomena like the semantic equivalence of ac-
tive and passive structures (X Verb[active]
Y ? Y is Verb[passive] by X). Various
knowledge resources, represented as entailment
rules, are freely available in BIUTEE?s web-site. The
complete formalism of entailment rules, adopted by
our system, is described in (Bar-Haim et al, 2007).
Coreference relations are utilized via coreference-
substitution transformations: one mention of an en-
tity is replaced by another mention of the same en-
tity, based on coreference relations. In the above ex-
ample the system could apply such a transformation
to substitute ?him? with ?Charles G. Taylor?.
Since applications of entailment rules and coref-
erence substitutions are yet, in most cases, insuffi-
cient in transforming T into H, our system allows
on-the-fly transformations. These transformations
include insertions of missing nodes, flipping parts-
of-speech, moving sub-trees, etc. (see (Stern and
Dagan, 2011) for a complete list of these transforma-
tions). Since these transformations are not justified
by given knowledge resources, we use linguistically-
motivated features to estimate their validity. For ex-
ample, for on-the-fly lexical insertions we consider
as features the named-entity annotation of the in-
serted word, and its probability estimation according
to a unigram language model, which yields lower
costs for more frequent words.
Given a (T,H) pair, the system applies a search
algorithm (Stern et al, 2012) to find a proof O =
(o1, o2, . . . on) that transforms T into H. For each
proof step oi the system calculates a cost c(oi). This
cost is defined as follows: the system uses a weight-
vector w, which is learned in the training phase. In
addition, each transformation oi is represented by a
feature vector f(oi) which characterizes the trans-
formation. The cost c(oi) is defined as w ? f(oi).
The proof cost is defined as the sum of the costs of
the transformations from which it is composed, i.e.:
c(O) ,
n?
i=1
c(oi) =
n?
i=1
w ? f(oi) = w ?
n?
i=1
f(oi)
(1)
If the proof cost is below a threshold b, then the sys-
tem concludes that T entails H. The complete de-
scription of the cost model, as well as the method
for learning the parameters w and b is described in
(Stern and Dagan, 2011).
2.2 System flow
The BIUTEE system flow (Figure 1) starts with pre-
processing of the text and the hypothesis. BIUTEE
provides state-of-the-art pre-processing utilities:
Easy-First parser (Goldberg and Elhadad, 2010),
Stanford named-entity-recognizer (Finkel et al,
2005) and ArkRef coreference resolver (Haghighi
and Klein, 2009), as well as utilities for sentence-
splitting and numerical-normalizations. In addition,
BIUTEE supports integration of users? own utilities
by simply implementing the appropriate interfaces.
Entailment recognition begins with a global pro-
cessing phase in which inference related computa-
tions that are not part of the proof are performed.
Annotating the negation indicators and their scope
in the text and hypothesis is an example of such cal-
culation. Next, the system constructs a proof which
is a sequence of transformations that transform the
text into the hypothesis. Finding such a proof is a
sequential process, conducted by the search algo-
rithm. In each step of the proof construction the sys-
tem examines all possible transformations that can
be applied, generates new trees by applying selected
transformations, and calculates their costs by con-
structing appropriate feature-vectors for them.
New types of transformations can be added to
BIUTEE by a plug-in mechanism, without the need
to change the code. For example, imagine that a
researcher applies BIUTEE on the medical domain.
There might be some well-known domain knowl-
edge and rules that every medical person knows.
Integrating them is directly supported by the plug-in
mechanism. A plug-in is a piece of code which im-
plements a few interfaces that detect which transfor-
mations can be applied, apply them, and construct
appropriate feature-vectors for each applied trans-
formation. In addition, a plug-in can perform com-
putations for the global processing phase.
Eventually, the search algorithm finds a (approx-
imately) lowest cost proof. This cost is normalized
as a score between 0 and 1, and returned as output.
Training the cost model parameters w and b
(see subsection 2.1) is performed by a linear learn-
75
Figure 1: System architecture
RTE
challenge
Median Best BIUTEE
RTE-6 33.72 48.01 49.09
RTE-7 39.89 48.00 42.93
Table 1: Performance (F1) of BIUTEE on RTE chal-
lenges, compared to other systems participated in these
challenges. Median and Best indicate the median score
and the highest score of all submissions, respectively.
ing algorithm, as described in (Stern and Dagan,
2011). We use a Logistic-Regression learning algo-
rithm, but, similar to other components, alternative
learning-algorithms can be integrated easily by im-
plementing an appropriate interface.
2.3 Experimental results
BIUTEE?s performance on the last two RTE chal-
lenges (Bentivogli et al, 2011; Bentivogli et al,
2010) is presented in Table 1: BIUTEE is better than
the median of all submitted results, and in RTE-6 it
outperforms all other systems.
3 Visual Tracing Tool
As a complex system, the final score provided as
output, as well as the system?s detailed logging in-
formation, do not expose all the decisions and cal-
culations performed by the system. In particular,
they do not show all the potential transformations
that could have been applied, but were rejected by
the search algorithm. However, such information is
crucial for researchers, who need to observe the us-
age and the potential impact of each component of
the system.
We address this need by providing an interactive
visual tracing tool, Tracer, which presents detailed
information on each proof step, including potential
steps that were not included in the final proof. In the
demo session, we will use the visual tracing tool to
illustrate all of BIUTEE?s components4.
3.1 Modes
Tracer provides two modes for tracing proof con-
struction: automatic mode and manual mode. In au-
tomatic mode, shown in Figure 2, the tool presents
the complete process of inference, as conducted by
the system?s search: the parse trees, the proof steps,
the cost of each step and the final score. For each
transformation the tool presents the parse tree before
and after applying the transformation, highlighting
the impact of this transformation. In manual mode,
the user can invoke specific transformations pro-
actively, including transformations rejected by the
search algorithm for the eventual proof. As shown in
Figure 3, the tool provides a list of transformations
that match the given parse-tree, from which the user
chooses and applies a single transformation at each
step. Similar to automatic mode, their impact on the
parse tree is shown visually.
3.2 Use cases
Developers of knowledge resources, as well as other
types of transformations, can be aided by Tracer as
follows. Applying an entailment rule is a process
of first matching the rule?s left-hand-side to the text
parse-tree (or to any tree along the proof), and then
substituting it by the rule?s right-hand-side. To test a
4Our demonstration requirements are a large screen and In-
ternet connection.
76
Figure 2: Entailment Rule application visualized in tracing tool. The upper pane displays the parse-tree generated by
applying the rule. The rule description is the first transformation (printed in bold) of the proof, shown in the lower
pane. It is followed by transformations 2 and 3, which are syntactic rewrite rules.
rule, the user can provide a text for which it is sup-
posed to match, examine the list of potential trans-
formations that can be performed on the text?s parse
tree, as in Figure 3, and verify that the examined
rule has been matched as expected. Next, the user
can apply the rule, visually examine its impact on
the parse-tree, as in Figure 2, and validate that it op-
erates as intended with no side-effects.
The complete inference process depends on the
parameters learned in the training phase, as well as
on the search algorithm which looks for lowest-cost
proof from T to H. Researchers investigating these
algorithmic components can be assisted by the trac-
ing tool as well. For a given (T,H) pair, the auto-
matic mode provides the complete proof found by
the system. Then, in the manual mode the researcher
can try to construct alternative proofs. If a proof
with lower cost can be constructed manually it im-
plies a limitation of the search algorithm. On the
other hand, if the user can manually construct a bet-
ter linguistically motivated proof, but it turns out that
this proof has higher cost than the one found by the
system, it implies a limitation of the learning phase
which may be caused either by a limitation of the
learning method, or due to insufficient training data.
4 Conclusions
In this paper we described BIUTEE, an open-source
textual-inference system, and suggested it as a re-
search platform in this field. We highlighted key
advantages of BIUTEE, which directly support re-
searchers? work: (a) modularity and extensibility,
(b) a plug-in mechanism, (c) utilization of entail-
ment rules, which can capture diverse types of
knowledge, and (d) a visual tracing tool, which vi-
sualizes all the details of the inference process.
Acknowledgments
This work was partially supported by the Israel
Science Foundation grant 1112/08, the PASCAL-
77
Figure 3: List of available transformations, provided by Tracer in the manual mode. The user can manually choose
and apply each of these transformations, and observe their impact on the parse-tree.
2 Network of Excellence of the European Com-
munity FP7-ICT-2007-1-216886, and the Euro-
pean Community?s Seventh Framework Programme
(FP7/2007-2013) under grant agreement no. 287923
(EXCITEMENT).
References
Roy Bar-Haim, Ido Dagan, Iddo Greental, and Eyal
Shnarch. 2007. Semantic inference at the lexical-
syntactic level. In Proceedings of AAAI.
Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa Dang, and
Danilo Giampiccolo. 2010. The sixth pascal recog-
nizing textual entailment challenge. In Proceedings of
TAC.
Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa Dang, and
Danilo Giampiccolo. 2011. The seventh pascal recog-
nizing textual entailment challenge. In Proceedings of
TAC.
Johan Bos and Katja Markert. 2005. Recognising textual
entailment with logical inference. In Proceedings of
EMNLP.
Peter Clark and Phil Harrison. 2010. Blue-lite: a
knowledge-based lexical entailment system for rte6.
In Proceedings of TAC.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The pascal recognising textual entailment chal-
lenge. In Quionero-Candela, J.; Dagan, I.; Magnini,
B.; d?Alch-Buc, F. (Eds.) Machine Learning Chal-
lenges. Lecture Notes in Computer Science.
Rodrigo de Salvo Braz, Roxana Girju, Vasin Pun-
yakanok, Dan Roth, and Mark Sammons. 2005. An
inference model for semantic entailment in natural lan-
guage. In Proceedings of AAAI.
Christiane Fellbaum, editor. 1998. WordNet An Elec-
tronic Lexical Database. The MIT Press, May.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local information
into information extraction systems by gibbs sampling.
In Proceedings of ACL.
Yoav Goldberg and Michael Elhadad. 2010. An effi-
cient algorithm for easy-first non-directional depen-
dency parsing. In Proceedings of NAACL.
Aria Haghighi and Dan Klein. 2009. Simple coreference
resolution with rich syntactic and semantic features. In
Proceedings of EMNLP.
Adrian Iftene. 2008. Uaic participation at rte4. In Pro-
ceedings of TAC.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of ACL.
Milen Kouylekov and Matteo Negri. 2010. An open-
source package for recognizing textual entailment. In
Proceedings of ACL Demo.
Andrew MacKinlay and Timothy Baldwin. 2009. A
baseline approach to the rte5 search pilot. In Proceed-
ings of TAC.
Asher Stern and Ido Dagan. 2011. A confidence model
for syntactically-motivated entailment proofs. In Pro-
ceedings of RANLP.
Asher Stern, Roni Stern, Ido Dagan, and Ariel Felner.
2012. Efficient search for transformation-based infer-
ence. In Proceedings of ACL.
78
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 79?84,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Entailment-based Text Exploration
with Application to the Health-care Domain
Meni Adler
Bar Ilan University
Ramat Gan, Israel
adlerm@cs.bgu.ac.il
Jonathan Berant
Tel Aviv University
Tel Aviv, Israel
jonatha6@post.tau.ac.il
Ido Dagan
Bar Ilan University
Ramat Gan, Israel
dagan@cs.biu.ac.il
Abstract
We present a novel text exploration model,
which extends the scope of state-of-the-art
technologies by moving from standard con-
cept-based exploration to statement-based ex-
ploration. The proposed scheme utilizes the
textual entailment relation between statements
as the basis of the exploration process. A user
of our system can explore the result space of
a query by drilling down/up from one state-
ment to another, according to entailment re-
lations specified by an entailment graph and
an optional concept taxonomy. As a promi-
nent use case, we apply our exploration sys-
tem and illustrate its benefit on the health-care
domain. To the best of our knowledge this is
the first implementation of an exploration sys-
tem at the statement level that is based on the
textual entailment relation.
1 Introduction
Finding information in a large body of text is be-
coming increasingly more difficult. Standard search
engines output a set of documents for a given query,
but do not allow any exploration of the thematic
structure in the retrieved information. Thus, the need
for tools that allow to effectively sift through a target
set of documents is becoming ever more important.
Faceted search (Stoica and Hearst, 2007; Ka?ki,
2005) supports a better understanding of a target do-
main, by allowing exploration of data according to
multiple views or facets. For example, given a set of
documents on Nobel Prize laureates we might have
different facets corresponding to the laureate?s na-
tionality, the year when the prize was awarded, the
field in which it was awarded, etc. However, this
type of exploration is still severely limited insofar
that it only allows exploration by topic rather than
content. Put differently, we can only explore accord-
ing to what a document is about rather than what
a document actually says. For instance, the facets
for the query ?asthma? in the faceted search engine
Yippy include the concepts allergy and children, but
do not specify what are the exact relations between
these concepts and the query (e.g., allergy causes
asthma, and children suffer from asthma).
Berant et al (2010) proposed an exploration
scheme that focuses on relations between concepts,
which are derived from a graph describing textual
entailment relations between propositions. In their
setting a proposition consists of a predicate with two
arguments that are possibly replaced by variables,
such as ?X control asthma?. A graph that specifies
an entailment relation ?X control asthma ? X af-
fect asthma? can help a user, who is browsing doc-
uments dealing with substances that affect asthma,
drill down and explore only substances that control
asthma. This type of exploration can be viewed as
an extension of faceted search, where the new facet
concentrates on the actual statements expressed in
the texts.
In this paper we follow Berant et al?s proposal,
and present a novel entailment-based text explo-
ration system, which we applied to the health-care
domain. A user of this system can explore the re-
sult space of her query, by drilling down/up from
one proposition to another, according to a set of en-
tailment relations described by an entailment graph.
In Figure 1, for example, the user looks for ?things?
79
Figure 1: Exploring asthma results.
that affect asthma. She invokes an ?asthma? query
and starts drilling down the entailment graph to ?X
control asthma? (left column). In order to exam-
ine the arguments of a selected proposition, the user
may drill down/up a concept taxonomy that classi-
fies terms that occur as arguments. The user in Fig-
ure 1, for instance, drills down the concept taxon-
omy (middle column), in order to focus on Hor-
mones that control asthma, such as ?prednisone?
(right column). Each drill down/up induces a subset
of the documents that correspond to the aforemen-
tioned selections. The retrieved document in Fig-
ure 1 (bottom) is highlighted by the relevant propo-
sition, which clearly states that prednisone is often
given to treat asthma (and indeed in the entailment
graph ?X treat asthma? entails ?X control asthma?).
Our system is built over a corpus of documents,
a set of propositions extracted from the documents,
an entailment graph describing entailment relations
between propositions, and, optionally, a concept hi-
erarchy. The system implementation for the health-
care domain, for instance, is based on a web-crawled
health-care corpus, the propositions automatically
extracted from the corpus, entailment graphs bor-
rowed from Berant et al (2010), and the UMLS1
taxonomy. To the best of our knowledge this is the
first implementation of an exploration system, at the
proposition level, based on the textual entailment re-
lation.
2 Background
2.1 Exploratory Search
Exploratory search addresses the need of users to
quickly identify the important pieces of information
in a target set of documents. In exploratory search,
users are presented with a result set and a set of ex-
ploratory facets, which are proposals for refinements
of the query that can lead to more focused sets of
documents. Each facet corresponds to a clustering
of the current result set, focused on a more specific
topic than the current query. The user proceeds in
the exploration of the document set by selecting spe-
cific documents (to read them) or by selecting spe-
cific facets, to refine the result set.
1http://www.nlm.nih.gov/research/umls/
80
Early exploration technologies were based on a
single hierarchical conceptual clustering of infor-
mation (Hofmann, 1999), enabling the user to drill
up and down the concept hierarchies. Hierarchi-
cal faceted meta-data (Stoica and Hearst, 2007), or
faceted search, proposed more sophisticated explo-
ration possibilities by providing multiple facets and
a hierarchy per facet or dimension of the domain.
These types of exploration techniques were found to
be useful for effective access of information (Ka?ki,
2005).
In this work, we suggest proposition-based ex-
ploration as an extension to concept-based explo-
ration. Our intuition is that text exploration can
profit greatly from representing information not only
at the level of individual concepts, but also at the
propositional level, where the relations that link con-
cepts to one another are represented effectively in a
hierarchical entailment graph.
2.2 Entailment Graph
Recognizing Textual Entailment (RTE) is the task
of deciding, given two text fragments, whether the
meaning of one text can be inferred from another
(Dagan et al, 2009). For example, ?Levalbuterol
is used to control various kinds of asthma? entails
?Levalbuterol affects asthma?. In this paper, we use
the notion of proposition to denote a specific type
of text fragments, composed of a predicate with two
arguments (e.g., Levalbuterol control asthma).
Textual entailment systems are often based on en-
tailment rules which specify a directional inference
relation between two fragments. In this work, we
focus on leveraging a common type of entailment
rules, in which the left-hand-side of the rule (LHS)
and the right-hand-side of the rule (RHS) are propo-
sitional templates - a proposition, where one or both
of the arguments are replaced by a variable, e.g., ?X
control asthma? X affect asthma?.
The entailment relation between propositional
templates of a given corpus can be represented by an
entailment graph (Berant et al, 2010) (see Figure 2,
top). The nodes of an entailment graph correspond
to propositional templates, and its edges correspond
to entailment relations (rules) between them. Entail-
ment graph representation is somewhat analogous to
the formation of ontological relations between con-
cepts of a given domain, where in our case the nodes
correspond to propositional templates rather than to
concepts.
3 Exploration Model
In this section we extend the scope of state-of-the-
art exploration technologies by moving from stan-
dard concept-based exploration to proposition-based
exploration, or equivalently, statement-based explo-
ration. In our model, it is the entailment relation
between propositional templates which determines
the granularity of the viewed information space. We
first describe the inputs to the system and then detail
our proposed exploration scheme.
3.1 System Inputs
Corpus A collection of documents, which form
the search space of the system.
Extracted Propositions A set of propositions, ex-
tracted from the corpus document. The propositions
are usually produced by an extraction method, such
as TextRunner (Banko et al, 2007) or ReVerb (Fader
et al, 2011). In order to support the exploration
process, the documents are indexed by the proposi-
tional templates and argument terms of the extracted
propositions.
Entailment graph for predicates The nodes of
the entailment graph are propositional templates,
where edges indicate entailment relations between
templates (Section 2.2). In order to avoid circular-
ity in the exploration process, the graph is trans-
formed into a DAG, by merging ?equivalent? nodes
that are in the same strong connectivity component
(as suggested by Berant et al (2010)). In addition,
for clarity and simplicity, edges that can be inferred
by transitivity are omitted from the DAG. Figure 2
illustrates the result of applying this procedure to a
fragment of the entailment graph for ?asthma? (i.e.,
for propositional templates with ?asthma? as one of
the arguments).
Taxonomy for arguments The optional concept
taxonomy maps terms to one or more pre-defined
concepts, arranged in a hierarchical structure. These
terms may appear in the corpus as arguments of
predicates. Figure 3, for instance, illustrates a sim-
ple medical taxonomy, composed of three concepts
(medical, diseases, drugs) and four terms (cancer,
asthma, aspirin, flexeril).
81
Figure 2: Fragment of the entailment graph for ?asthma?
(top), and its conversion to a DAG (bottom).
3.2 Exploration Scheme
The objective of the exploration scheme is to support
querying and offer facets for result exploration, in
a visual manner. The following components cover
the various aspects of this objective, given the above
system inputs:
Querying The user enters a search term as a query,
e.g., ?asthma?. The given term induces a subgraph of
the entailment graph that contains all propositional
templates (graph nodes) with which this term ap-
pears as an argument in the extracted propositions
(see Figure 2). This subgraph is represented as a
DAG, as explained in Section 3.1, where all nodes
that have no parent are defined as the roots of the
DAG. As a starting point, only the roots of the DAG
are displayed to the user. Figure 4 shows the five
roots for the ?asthma? query.
Exploration process The user selects one of the
entailment graph nodes (e.g., ?associate X with
asthma?). At each exploration step, the user can
drill down to a more specific template or drill up to a
Figure 3: Partial medical taxonomy. Ellipses denote con-
cepts, while rectangles denote terms.
Figure 4: The roots of the entailment graph for the
?asthma? query.
more general template, by moving along the entail-
ment hierarchy. For example, the user in Figure 5,
expands the root ?associate X with asthma?, in order
to drill down through ?X affect asthma? to ?X control
Asthma?.
Selecting a propositional template (Figure 1, left
column) displays a concept taxonomy for the argu-
ments that correspond to the variable in the selected
template (Figure 1, middle column). The user can
explore these argument concepts by drilling up and
down the concept taxonomy. For example, in Fig-
ure 1 the user, who selected ?X control Asthma?,
explores the arguments of this template by drilling
down the taxonomy to the concept ?Hormone?.
Selecting a concept opens a third column, which
lists the terms mapped to this concept that occurred
as arguments of the selected template. For example,
in Figure 1, the user is examining the list of argu-
ments for the template ?X control Asthma?, which
are mapped to the concept ?Hormone?, focusing on
the argument ?prednisone?.
82
Figure 5: Part of the entailment graph for the ?asthma?
query, after two exploration steps. This corresponds to
the left column in Figure 1.
Document retrieval At any stage, the list of docu-
ments induced by the current selected template, con-
cept and argument is presented to the user, where
in each document snippet the relevant proposition
components are highlighted. Figure 1 (bottom)
shows such a retrieved document. The highlighted
extraction in the snippet, ?prednisone treat asthma?,
entails the proposition selected during exploration,
?prednisone control asthma?.
4 System Architecture
In this section we briefly describe system compo-
nents, as illustrated in the block diagram (Figure 6).
The search service implements full-text and
faceted search, and document indexing. The data
service handles data (e.g., documents) replication
for clients. The entailment service handles the logic
of the entailment relations (for both the entailment
graph and the taxonomy).
The index server applies periodic indexing of new
texts, and the exploration server serves the explo-
ration application on querying, exploration, and data
Figure 6: Block diagram of the exploration system.
access. The exploration application is the front-end
user application for the whole exploration process
described above (Section 3.2).
5 Application to the Health-care Domain
As a prominent use case, we applied our exploration
system to the health-care domain. With the advent
of the internet and social media, patients now have
access to new sources of medical information: con-
sumer health articles, forums, and social networks
(Boulos and Wheeler, 2007). A typical non-expert
health information searcher is uncertain about her
exact questions and is unfamiliar with medical ter-
minology (Trivedi, 2009). Exploring relevant infor-
mation about a given medical issue can be essential
and time-critical.
System implementation For the search service,
we used SolR servlet, where the data service is
built over FTP. The exploration application is im-
plemented as a web application.
Input resources We collected a health-care cor-
pus from the web, which contains more than 2M
sentences and about 50M word tokens. The texts
deal with various aspects of the health care domain:
answers to questions, surveys on diseases, articles
on life-style, etc. We extracted propositions from
the health-care corpus, by applying the method de-
scribed by Berant et al (2010). The corpus was
parsed, and propositions were extracted from depen-
dency trees according to the method suggested by
Lin and Pantel (2001), where propositions are de-
pendency paths between two arguments of a predi-
83
cate. We filtered out any proposition where one of
the arguments is not a term mapped to a medical
concept in the UMLS taxonomy.
For the entailment graph we used the 23 entail-
ment graphs published by Berant et al2. For the ar-
gument taxonomy we employed UMLS ? a database
that maps natural language phrases to over one mil-
lion unique concept identifiers (CUIs) in the health-
care domain. The CUIs are also mapped in UMLS
to a concept taxonomy for the health-care domain.
The web application of our system is
available at: http://132.70.6.148:
8080/exploration
6 Conclusion and Future Work
We presented a novel exploration model, which ex-
tends the scope of state-of-the-art exploration tech-
nologies by moving from standard concept-based
exploration to proposition-based exploration. Our
model combines the textual entailment paradigm
within the exploration process, with application to
the health-care domain. According to our model, it
is the entailment relation between propositions, en-
coded by the entailment graph and the taxonomy,
which leads the user between more specific and
more general statements throughout the search re-
sult space. We believe that employing the entail-
ment relation between propositions, which focuses
on the statements expressed in the documents, can
contribute to the exploration field and improve in-
formation access.
Our current application to the health-care domain
relies on a small set of entailment graphs for 23
medical concepts. Our ongoing research focuses on
the challenging task of learning a larger entailment
graph for the health-care domain. We are also in-
vestigating methods for evaluating the exploration
process (Borlund and Ingwersen, 1997). As noted
by Qu and Furnas (2008), the success of an ex-
ploratory search system does not depend simply on
how many relevant documents will be retrieved for a
given query, but more broadly on how well the sys-
tem helps the user with the exploratory process.
2http://www.cs.tau.ac.il/?jonatha6/
homepage_files/resources/HealthcareGraphs.
rar
Acknowledgments
This work was partially supported by the Israel
Ministry of Science and Technology, the PASCAL-
2 Network of Excellence of the European Com-
munity FP7-ICT-2007-1-216886, and the Euro-
pean Communitys Seventh Framework Programme
(FP7/2007-2013) under grant agreement no. 287923
(EXCITEMENT).
References
Michele Banko, Michael J Cafarella, Stephen Soderl,
Matt Broadhead, and Oren Etzioni. 2007. Open in-
formation extraction from the web. In Proceedings of
IJCAI, pages 2670?2676.
Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2010. Global learning of focused entailment graphs.
In Proceedings of ACL, Uppsala, Sweden.
Pia Borlund and Peter Ingwersen. 1997. The develop-
ment of a method for the evaluation of interactive in-
formation retrieval systems. Journal of Documenta-
tion, 53:225?250.
Maged N. Kamel Boulos and Steve Wheeler. 2007. The
emerging web 2.0 social software: an enabling suite of
sociable technologies in health and health care educa-
tion. Health Information & Libraries, 24:2?23.
Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan Roth.
2009. Recognizing textual entailment: Rational, eval-
uation and approaches. Natural Language Engineer-
ing, 15(Special Issue 04):i?xvii.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In EMNLP, pages 1535?1545. ACL.
Thomas Hofmann. 1999. The cluster-abstraction model:
Unsupervised learning of topic hierarchies from text
data. In Proceedings of IJCAI, pages 682?687.
Mika Ka?ki. 2005. Findex: search result categories help
users when document ranking fails. In Proceedings
of SIGCHI, CHI ?05, pages 131?140, New York, NY,
USA. ACM.
Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules for question answering. Natural Language
Engineering, 7:343?360.
Yan Qu and George W. Furnas. 2008. Model-driven for-
mative evaluation of exploratory search: A study un-
der a sensemaking framework. Inf. Process. Manage.,
44:534?555.
Emilia Stoica and Marti A. Hearst. 2007. Automating
creation of hierarchical faceted metadata structures. In
Proceedings of NAACL HLT.
Mayank Trivedi. 2009. A study of search engines for
health sciences. International Journal of Library and
Information Science, 1(5):69?73.
84
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1331?1340,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
A Two Level Model for Context Sensitive Inference Rules
Oren Melamud?, Jonathan Berant?, Ido Dagan?, Jacob Goldberger?, Idan Szpektor?
? Computer Science Department, Bar-Ilan University
? Computer Science Department, Stanford University
? Faculty of Engineering, Bar-Ilan University
? Yahoo! Research Israel
{melamuo,dagan,goldbej}@{cs,cs,eng}.biu.ac.il
joberant@stanford.edu
idan@yahoo-inc.com
Abstract
Automatic acquisition of inference rules
for predicates has been commonly ad-
dressed by computing distributional simi-
larity between vectors of argument words,
operating at the word space level. A re-
cent line of work, which addresses context
sensitivity of rules, represented contexts in
a latent topic space and computed similar-
ity over topic vectors. We propose a novel
two-level model, which computes simi-
larities between word-level vectors that
are biased by topic-level context repre-
sentations. Evaluations on a naturally-
distributed dataset show that our model
significantly outperforms prior word-level
and topic-level models. We also release a
first context-sensitive inference rule set.
1 Introduction
Inference rules for predicates have been identi-
fied as an important component in semantic ap-
plications, such as Question Answering (QA)
(Ravichandran and Hovy, 2002) and Information
Extraction (IE) (Shinyama and Sekine, 2006). For
example, the inference rule ?X treat Y ? X relieve
Y? can be useful to extract pairs of drugs and the
illnesses which they relieve, or to answer a ques-
tion like ?Which drugs relieve headache??. Along
this vein, such inference rules constitute a crucial
component in generic modeling of textual infer-
ence, under the Textual Entailment paradigm (Da-
gan et al, 2006; Dinu and Wang, 2009).
Motivated by these needs, substantial research
was devoted to automatic learning of inference
rules from corpora, mostly in an unsupervised dis-
tributional setting. This research line was mainly
initiated by the highly-cited DIRT algorithm (Lin
and Pantel, 2001), which learns inference for bi-
nary predicates with two argument slots (like the
rule in the example above). DIRT represents a
predicate by two vectors, one for each of the ar-
gument slots, where the vector entries correspond
to the argument words that occurred with the pred-
icate in the corpus. Inference rules between pairs
of predicates are then identified by measuring the
similarity between their corresponding argument
vectors. This general scheme was further en-
hanced in several directions, e.g. directional sim-
ilarity (Bhagat et al, 2007; Szpektor and Dagan,
2008) and meta-classification over similarity val-
ues (Berant et al, 2011). Consequently, several
knowledge resources of inference rules were re-
leased, containing the top scoring rules for each
predicate (Schoenmackers et al, 2010; Berant et
al., 2011; Nakashole et al, 2012).
The above mentioned methods provide a sin-
gle confidence score for each rule, which is based
on the obtained degree of argument-vector sim-
ilarities. Thus, a system that applies an infer-
ence rule to a text may estimate the validity of
the rule application based on the pre-specified rule
score. However, the validity of an inference rule
may depend on the context in which it is applied,
such as the context specified by the given predi-
cate?s arguments. For example, ?AT&T acquire T-
Mobile ? AT&T purchase T-Mobile?, is a valid
application of the rule ?X acquire Y ? X pur-
chase Y?, while ?Children acquire skills ? Chil-
dren purchase skills? is not. To address this issue, a
line of works emerged which computes a context-
sensitive reliability score for each rule application,
based on the given context.
The major trend in context-sensitive inference
models utilizes latent or class-based methods for
context modeling (Pantel et al, 2007; Szpektor et
al., 2008; Ritter et al, 2010; Dinu and Lapata,
2010b). In particular, the more recent methods
(Ritter et al, 2010; Dinu and Lapata, 2010b) mod-
eled predicates in context as a probability distribu-
tion over topics learned by a Latent Dirichlet Allo-
1331
cation (LDA) model. Then, similarity is measured
between the two topic distribution vectors corre-
sponding to the two sides of the rule in the given
context, yielding a context-sensitive score for each
particular rule application.
We notice at this point that while context-
insensitive methods represent predicates by ar-
gument vectors in the original fine-grained word
space, context-sensitive methods represent them
as vectors at the level of latent topics. This raises
the question of whether such coarse-grained topic
vectors might be less informative in determining
the semantic similarity between the two predi-
cates.
To address this hypothesized caveat of prior
context-sensitive rule scoring methods, we pro-
pose a novel generic scheme that integrates word-
level and topic-level representations. Our scheme
can be applied on top of any context-insensitive
?base? similarity measure for rule learning, which
operates at the word level, such as Cosine or
Lin (Lin, 1998). Rather than computing a single
context-insensitive rule score, we compute a dis-
tinct word-level similarity score for each topic in
an LDA model. Then, when applying a rule in a
given context, these different scores are weighed
together based on the specific topic distribution
under the given context. This way, we calculate
similarity over vectors in the original word space,
while biasing them towards the given context via
a topic model.
In order to promote replicability and equal-term
comparison with our results, we based our experi-
ments on publicly available datasets, both for un-
supervised learning of the evaluated models and
for testing them over a random sample of rule ap-
plications. We apply our two-level scheme over
three state-of-the-art context-insensitive similar-
ity measures. The evaluation compares perfor-
mances both with the original context-insensitive
measures and with recent LDA-based context-
sensitive methods, showing consistent and robust
advantages of our scheme. Finally, we release
a context-sensitive rule resource comprising over
2,000 frequent verbs and one million rules.
2 Background and Model Setting
This section presents components of prior work
which are included in our model and experiments,
setting the technical preliminaries for the rest of
the paper. We first present context-insensitive rule
learning, based on distributional similarity at the
word level, and then context-sensitive scoring for
rule applications, based on topic-level similarity.
Some further discussion of related work appears
in Section 6.
2.1 Context-insensitive Rule Learning
A predicate inference rule ?LHS ? RHS?, such
as ?X acquire Y ? X purchase Y?, specifies a
directional inference relation between two predi-
cates. Each rule side consists of a lexical pred-
icate and (two) variable slots for its arguments.1
Different representations have been used to spec-
ify predicates and their argument slots, such as
word lemma sequences, regular expressions and
dependency parse fragments. A rule can be ap-
plied when its LHS matches a predicate with a
pair of arguments in a text, allowing us to infer its
RHS, with the corresponding instantiations for the
argument variables. For example, given the text
?AT&T acquires T-Mobile?, the above rule infers
?AT&T purchases T-Mobile?.
The DIRT algorithm (Lin and Pantel, 2001)
follows the distributional similarity paradigm to
learn predicate inference rules. For each predi-
cate, DIRT represents each of its argument slots
by an argument vector. We denote the two vectors
of the X and Y slots of a predicate pred by vxpred
and vypred, respectively. Each entry of a vector vcorresponds to a particular word (or term) w that
instantiated the argument slot in a learning corpus,
with a value v(w) = PMI(pred, w) (with PMI
standing for point-wise mutual information).
To learn inference rules, DIRT considers (in
principle) each pair of binary predicates that
occurred in the corpus for a candidate rule,
?LHS ? RHS?. Then, DIRT computes a reliabil-
ity score for the rule by combining the measured
similarities between the corresponding argument
vectors of the two rule sides. Concretely, denot-
ing by l and r the predicates appearing in the two
rule sides, DIRT?s reliability score is defined as
follows:
(1)scoreDIRT(LHS ? RHS)
=
?
sim(vxl , vxr ) ? sim(v
y
l , v
y
r )
where sim(v, v?) is a vector similarity measure.
Specifically, DIRT employs the Lin similarity
1We follow most of the inference-rule learning literature,
which focused on binary predicates. However, our context-
sensitive scheme can be applied to any arity.
1332
measure from (Lin, 1998), defined as follows:
(2)Lin(v, v?) =
?
w?v?v? [v(w) + v?(w)]?
w?v?v? [v(w) + v?(w)]
We note that the general DIRT scheme may be
used while employing other ?base? vector similar-
ity measures. For example, the Lin measure is
symmetric, and thus using it would yield the same
reliability score when swapping the two sides of
a rule. This issue has been addressed in a sepa-
rate line of research which introduced directional
similarity measures suitable for inference rela-
tions (Bhagat et al, 2007; Szpektor and Dagan,
2008; Kotlerman et al, 2010). In our experiments
we apply our proposed context-sensitive similarity
scheme over three different base similarity mea-
sures.
DIRT and similar context-insensitive inference
methods provide a single reliability score for a
learned inference rule, which aims to predict the
validity of the rule?s applications. However, as
exemplified in the Introduction, an inference rule
may be valid in some contexts but invalid in oth-
ers (e.g. acquiring entails purchasing for goods,
but not for skills). Since vector similarity in DIRT
is computed over the single aggregate argument
vector, the obtained reliability score tends to be
biased towards the dominant contexts of the in-
volved predicates. For example, we may expect
a higher score for ?acquire ? purchase? than for
?acquire ? learn?, since the former matches a
more frequent sense of acquire in a typical corpus.
Following this observation, it is desired to obtain
a context-sensitive reliability score for each rule
application in a given context, as described next.
2.2 Context-sensitive Rule Applications
To assess the reliability of applying an inference
rule in a given context we need some model for
context representation, that should affect the rule
reliability score. A major trend in past work is
to represent contexts in a reduced-dimensionality
latent or class-based model. A couple of earlier
works utilized a cluster-based model (Pantel et al,
2007) and an LSA-based model (Szpektor et al,
2008), in a selectional-preferences style approach.
Several more recent works utilize a Latent Dirich-
let Allocation (LDA) (Blei et al, 2003) frame-
work. We now present an underlying unified view
of the topic-level models in (Ritter et al, 2010;
Dinu and Lapata, 2010b), which we follow in our
own model and in comparative model evaluations.
We note that a similar LDA model construction
was employed also in (Se?aghdha, 2010), for esti-
mating predicate-argument likelihood.
First, an LDA model is constructed, as follows.
Similar to the construction of argument vectors
in the distributional model (described above in
subsection 2.1), all arguments instantiating each
predicate slot are extracted from a large learning
corpus. Then, for each slot of each predicate, a
pseudo-document is constructed containing the set
of all argument words that instantiated this slot in
the corpus. We denote the two documents con-
structed for the X and Y slots of a predicate pred
by dxpred and dypred, respectively. In comparison tothe distributional model, these two documents cor-
respond to the analogous argument vectors vxpred
and vypred, both containing exactly the same set ofwords.
Next, an LDA model is learned from the set
of all pseudo-documents, extracted for all predi-
cates.2 The learning process results in the con-
struction of K latent topics, where each topic t
specifies a distribution over all words, denoted by
p(w|t), and a topic distribution for each pseudo-
document d, denoted by p(t|d).
Within the LDA model we can derive the
a-posteriori topic distribution conditioned on a
particular word within a document, denoted by
p(t|d,w) ? p(w|t) ? p(t|d). In the topic-level
model, d corresponds to a predicate slot and w to
a particular argument word instantiating this slot.
Hence, p(t|d,w) is viewed as specifying the rele-
vance (or likelihood) of the topic t for the predi-
cate slot in the context of the given argument in-
stantiation. For example, for the predicate slot ?ac-
quire Y? in the context of the argument ?IBM?, we
expect high relevance for a topic about companies,
while in the context of the argument ?knowledge?
we expect high relevance for a topic about abstract
concepts. Accordingly, the distribution p(t|d,w)
over all topics provides a topic-level representa-
tion for a predicate slot in the context of a particu-
lar argument w. This representation is used by the
topic-level model to compute a context-sensitive
score for inference rule applications, as follows.
2We note that there are variants in the type of LDA model
and the way the pseudo-documents are constructed in the
referenced prior work. In order to focus on the inference
methods rather than on the underlying LDA model, we use
the LDA framework described in this paper for all compared
methods.
1333
Consider the application of an inference rule
?LHS ? RHS? in the context of a particular pair
of arguments for the X and Y slots, denoted by
wx and wy, respectively. Denoting by l and r the
predicates appearing in the two rule sides, the reli-
ability score of the topic-level model is defined as
follows (we present a geometric mean formulation
for consistency with DIRT):
(3)scoreTopic(LHS ? RHS, wx, wy)
=
?
sim(dxl , dxr , wx) ? sim(d
y
l , d
y
r , wy)
where sim(d, d?, w) is a topic-distribution similar-
ity measure conditioned on a given context word.
Specifically, Ritter et al (2010) utilized the dot
product form for their similarity measure:
(4)simDC(d, d?, w) = ?t[p(t|d,w) ? p(t|d?, w)]
(the subscript DC stands for double-conditioning,
as both distributions are conditioned on the argu-
ment word, unlike the measure below).
Dinu and Lapata (2010b) presented a slightly
different similarity measure for topic distributions
that performed better in their setting as well as in a
related later paper on context-sensitive scoring of
lexical similarity (Dinu and Lapata, 2010a). In this
measure, the topic distribution for the right hand
side of the rule is not conditioned on w:
(5)simSC(d, d?, w) = ?t[p(t|d,w) ? p(t|d?)]
(the subscript SC stands for single-conditioning,
as only the left distribution is conditioned on the
argument word). They also experimented with a
few variants for the structure of the similarity mea-
sure and assessed that best results are obtained
with the dot product form. In our experiments,
we employ these two similarity measures for topic
distributions as baselines representing topic-level
models.
Comparing the context-insensitive and context-
sensitive models, we see that both of them mea-
sure similarity between vector representations of
corresponding predicate slots. However, while
DIRT computes sim(v, v?) over vectors in the
original word-level space, topic-level models com-
pute sim(d, d?, w) by measuring similarity of vec-
tors in a reduced-dimensionality latent space. As
conjectured in the introduction, such coarse-grain
representation might lead to loss of information.
Hence, in the next section we propose a com-
bined two-level model, which represents predicate
slots in the original word-level space while biasing
the similarity measure through topic-level context
models.
3 Two-level Context-sensitive Inference
Our model follows the general DIRT scheme
while extending it to handle context-sensitive scor-
ing of rule applications, addressing the scenario
dealt by the context-sensitive topic models. In
particular, we define the context-sensitive score
scoreWT, where WT stands for the combination
of the Word/Topic levels:
(6)scoreWT(LHS ? RHS, wx, wy)
=
?
sim(vxl , vxr , wx) ? sim(v
y
l , v
y
r , wy)
Thus, our model computes similarity over word-
level (rather than topic-level) argument vectors,
while biasing it according to the specific argu-
ment words in the given rule application con-
text. The core of our contribution is thus defining
the context-sensitive word-level vector similarity
measure sim(v, v?, w), as described in the remain-
der of this section.
Following the methods in Section 2, for each
predicate pred we construct, from the learning
corpus, its argument vectors vxpred and vypred aswell as its argument pseudo-documents dxpred and
dypred. For convenience, when referring to an ar-gument vector v, we will denote the correspond-
ing pseudo-document by dv. Based on all pseudo-
documents we learn an LDA model and obtain its
associated probability distributions.
The calculation of sim(v, v?, w) is composed of
two steps. At learning time, we compute for each
candidate rule a separate, topic-biased, similarity
score per each of the topics in the LDA model.
Then, at rule application time, we compute an
overall reliability score for the rule by combining
the per-topic similarity scores, while biasing the
score combination according to the given context
of w. These two steps are described in the follow-
ing two subsections.
3.1 Topic-biased Word-vector Similarities
Given a pair of word vectors v and v?, and
any desired ?base? vector similarity measure sim
(e.g. simLin), we compute a topic-biased sim-
ilarity score for each LDA topic t, denoted by
simt(v, v?). simt(v, v?) is computed by applying
1334
the original similarity measure over topic-biased
versions of v and v?, denoted by vt and v?t:
simt(v, v?) = sim(vt, v?t)
where
vt(w) = v(w) ? p(t|dv, w)
That is, each value in the biased vector, vt(w),
is obtained by weighing the original value v(w)
by the relevance of the topic t to the argument
word w within dv. This way, rather than replac-
ing altogether the word-level values v(w) by the
topic probabilities p(t|dv, w), as done in the topic-
level models, we use the latter to only bias the for-
mer while preserving fine-grained word-level rep-
resentations. The notation Lint denotes the simt
measure when applied using Lin as the base simi-
larity measure sim.
This learning process results in K different
topic-biased similarity scores for each candidate
rule, where K is the number of LDA topics. Ta-
ble 1 illustrates topic-biased similarities for the Y
slot of two rules involving the predicate ?acquire?.
As can be seen, the topic-biased score Lint for ?ac-
quire? learn? for t2 is higher than the Lin score,
since this topic is characterized by arguments that
commonly appear with both predicates of the rule.
Consequently, the two predicates are found to be
distributionally similar when biased for this topic.
On the other hand, the topic-biased similarity for
t1 is substantially lower, since prominent words
in this topic are likely to occur with ?acquire? but
not with ?learn?, yielding low distributional simi-
larity. Opposite behavior is exhibited for the rule
?acquire? purchase?.
3.2 Context-sensitive Similarity
When applying an inference rule, we compute
for each slot its context-sensitive similarity score
simWT(v, v?, w), where v and v? are the slot?s ar-
gument vectors for the two rule sides and w is the
word instantiating the slot in the given rule appli-
cation. This score is computed as a weighted aver-
age of the rule?s K topic-biased similarity scores
simt. In this average, each topic is weighed by
its ?relevance? for the context in which the rule is
applied, which consists of the left-hand-side pred-
icate v and the argument w. This relevance is cap-
Topic t1 t2
Top 5
words
calbiochem rights
corel syndrome
networks majority
viacom knowledge
financially skill
acquire ? learn
Lint(v, v?) 0.040 0.334
Lin(v, v?) 0.165
acquire ? purchase
Lint(v, v?) 0.427 0.241
Lin(v, v?) 0.267
Table 1: Two characteristic topics for the Y slot of
?acquire?, along with their topic-biased Lin sim-
ilarities scores Lint, compared with the original
Lin similarity, for two rules. The relevance of each
topic to different arguments of ?acquire? is illus-
trated by showing the top 5 words in the argument
vector vyacquire for which the illustrated topic is the
most likely one.
tured by p(t|dv, w):
simWT(v, v?, w) =
?
t
[p(t|dv, w) ? simt(v, v?)]
(7)
This way, a rule application would obtain a high
score only if the current context fits those topics
for which the rule is indeed likely to be valid, as
captured by a high topic-biased similarity. The no-
tation LinWT denotes the simWT measure, when
using Lint as the topic-biased similarity measure.
Table 2 illustrates the calculation of context-
sensitive similarity scores in four rule applica-
tions, involving the Y slot of the predicate ?ac-
quire?. We observe that relative to the fixed
context-insensitive Lin score, the score of ?ac-
quire ? learn? is substantially promoted for
the argument ?skill? while being demoted for
?Skype?. The opposite behavior is observed for
?acquire ? purchase?, altogether demonstrating
how our model successfully biases the similarity
score according to rule validity in context.
4 Experimental Settings
To evaluate our model, we compare it both to
context-insensitive similarity measures as well as
to prior context-sensitive methods. Furthermore,
to better understand its applicability in typical
NLP tasks, we focus on an evaluation setting that
corresponds to a natural distribution of examples
from a large corpus.
1335
Topic t1 t2
Top 5
words
calbiochem rights
corel syndrome
networks majority
viacom knowledge
financially skill
?acquire Skype ? learn Skype?
p(t|dv, w) 0.974 0.000
Lint(v, v?) 0.040 0.334
LinWT(v, v?, w) 0.039
Lin(v, v?) 0.165
?acquire Skype ? purchase Skype?
p(t|dv, w) 0.974 0.000
Lint(v, v?) 0.427 0.241
LinWT(v, v?, w) 0.417
Lin(v, v?) 0.267
?acquire skill ? learn skill?
p(t|dv, w) 0.000 0.380
Lint(v, v?) 0.040 0.334
LinWT(v, v?, w) 0.251
Lin(v, v?) 0.165
?acquire skill ? purchase skill?
p(t|dv, w) 0.000 0.380
Lint(v, v?) 0.427 0.241
LinWT(v, v?, w) 0.181
Lin(v, v?) 0.267
Table 2: Context-sensitive similarity scores (in
bold) for the Y slots of four rule applications. The
components of the score calculation are shown for
the topics of Table 1. For each rule application,
the table shows a couple of the topic-biased scores
Lint of the rule (as in Table 1), along with the topic
relevance for the given context p(t|dv, w), which
weighs the topic-biased scores in the LinWT cal-
culation. The context-insensitive Lin score is
shown for comparison.
4.1 Evaluated Rule Application Methods
We evaluated the following rule application meth-
ods: the original context-insensitive word model,
following DIRT (Lin and Pantel, 2001), as de-
scribed in Equation 1, denoted by CI; our own
topic-word context-sensitive model, as described
in Equation 6, denoted by WT. In addition, we
evaluated two variants of the topic-level context-
sensitive model, denoted DC and SC. DC follows
the double conditioned contextualized similarity
measure according to Equation 4, as implemented
by (Ritter et al, 2010), while SC follows the sin-
gle conditioned one at Equation 5, as implemented
by (Dinu and Lapata, 2010b; Dinu and Lapata,
2010a).
Since our model can contextualize various dis-
tributional similarity measures, we evaluated the
performance of all the above methods on several
base similarity measures and their learned rule-
sets, namely Lin (Lin, 1998), BInc (Szpektor and
Dagan, 2008) and vector Cosine similarity. The
Lin similarity measure is described in Equation 2.
Binc (Szpektor and Dagan, 2008) is a directional
similarity measure between word vectors, which
outperformed Lin for predicate inference (Szpek-
tor and Dagan, 2008).
To build the rule-sets and models for the tested
approaches we utilized the ReVerb corpus (Fader
et al, 2011), a large scale publicly available web-
based open extractions data set, containing about
15 million unique template extractions.3 ReVerb
template extractions/instantiations are in the form
of a tuple (x, pred, y), containing pred, a verb
predicate, x, the argument instantiation of the tem-
plate?s slot X , and y, the instantiation of the tem-
plate?s slot Y .
ReVerb includes over 600,000 different tem-
plates that comprise a verb but may also include
other words, for example ?X can accommodate up
to Y?. Yet, many of these templates share a similar
meaning, e.g. ?X accommodate up to Y?, ?X can
accommodate up to Y?, ?X will accommodate up
to Y?, etc. Following Sekine (2005), we clustered
templates that share their main verb predicate in
order to scale down the number of different pred-
icates in the corpus and collect richer word co-
occurrence statistics per predicate.
Next, we applied some clean-up preprocessing
to the ReVerb extractions. This includes discard-
ing stop words, rare words and non-alphabetical
words instantiating either the X or the Y argu-
ments. In addition, we discarded all predicates
that co-occur with less than 100 unique argument
words in each slot. The remaining corpus consists
of 7 million unique extractions and 2,155 verb
predicates.
Finally, we trained an LDA model, as described
in Section 2, using Mallet (McCallum, 2002).
Then, for each original context-insensitive simi-
larity measure, we learned from ReVerb a rule-set
comprised of the top 500 rules for every identi-
fied predicate. To complete the learning, we cal-
culated the topic-biased similarity score for each
learned rule under each LDA topic, as specified
in our context-sensitive model. We release a rule
set comprising the top 500 context-sensitive rules
that we learned for each of the verb predicates in
our learning corpus, along with our trained LDA
3ReVerb is available at http://reverb.cs.
washington.edu/
1336
Method Lin BInc Cosine
Valid 266 254 272
Invalid 545 523 539
Total 811 777 811
Table 3: Sizes of rule application test set for each
learned rule-set.
model.4
4.2 Evaluation Task
To evaluate the performance of the different meth-
ods we chose the dataset constructed by Zeich-
ner et al (2012). 5 This publicly available dataset
contains about 6,500 manually annotated predi-
cate template rule applications, each one labeled
as correct or incorrect. For example, ?Jack agree
with Jill 9 Jack feel sorry for Jill? is a rule ap-
plication in this dataset, labeled as incorrect, and
?Registration open this month? Registration be-
gin this month? is another rule application, labeled
as correct. Rule applications were generated by
randomly sampling extractions from ReVerb, such
as (?Jack?,?agree with?,?Jill?) and then sampling
possible rules for each, such as ?agree with? feel
sorry for?. Hence, this dataset provides naturally
distributed rule inferences with respect to ReVerb.
Whenever we evaluated a distributional similar-
ity measure (namely Lin, BInc, or Cosine), we
discarded instances from Zeichner et al?s dataset
in which the assessed rule is not in the context-
insensitive rule-set learned for this measure or the
argument instantiation of the rule is not in the LDA
lexicon. We refer to the remaining instances as the
test set per measure, e.g. Lin?s test set. Table 3
details the size of each such test set in our experi-
ment.
Finally, the task under which we assessed the
tested models is to rank all rule applications in
each test set, aiming to rank the valid rule appli-
cations above the invalid ones.
5 Results
We evaluated the performance of each tested
method by measuring Mean Average Precision
(MAP) (Manning et al, 2008) of the rule appli-
cation ranking computed by this method. In order
4Our resource is available at: http://www.cs.biu.
ac.il/? nlp/downloads/wt-rules.html
5The dataset is available at: http://
www.cs.biu.ac.il/?nlp/downloads/
annotation-rule-application.htm
Method Lin BInc Cosine
CI 0.503 0.513 0.513
DC 0.451 (1200) 0.455 (1200) 0.455 (1200)
SC 0.443 (1200) 0.458 (1200) 0.452 (1200)
WT 0.562 (100) 0.584 (50) 0.565 (25)
Table 4: MAP values on corresponding test set ob-
tained by each method. Figures in parentheses in-
dicate optimal number of LDA topics.
to compute MAP values and corresponding statis-
tical significance, we randomly split each test set
into 30 subsets. For each method we computed
Average Precision on every subset and then took
the average over all subsets as the MAP value.
Since all tested context-sensitive approaches are
based on LDA topics, we varied for each method
the number of LDA topics K that optimizes its
performance, ranging from 25 to 1600 topics. We
used LDA hyperparameters ? = 0.01 and ? = 0.1
for K < 600 and ? = 50K for K >= 600.
Table 4 presents the optimal MAP performance
of each tested measure. Our main result is that
our model outperforms all other methods, both
context-insensitive and context-sensitive, by a rel-
ative increase of more than 10% for all three sim-
ilarity measures that we tested. This improvement
is statistically significant at p < 0.01 for BInc and
Lin, and p < 0.015 for Cosine, using paired t-
test. This shows that our model indeed success-
fully leverages contextual information beyond the
basic context-agnostic rule scores and is robust
across measures.
Surprisingly, both baseline topic-level context-
sensitive methods, namely DC and SC, underper-
formed compared to their context-insensitive base-
lines. While Dinu and Lapata (Dinu and Lap-
ata, 2010b) did show improvement over context-
insensitive DIRT, this result was obtained on the
verbs of the Lexical Substitution Task in SemEval
(McCarthy and Navigli, 2007), which was manu-
ally created with a bias for context-sensitive sub-
stitutions. However, our result suggests that topic-
level models might not be robust enough when ap-
plied to a random sample of inferences.
An interesting indication of the differences be-
tween our word-topic model, WT, and topic-only
models, DC and SC, lies in the optimal number of
LDA topics required for each method. The num-
ber of topics in the range 25-100 performed almost
equally well under the WT model for all base mea-
sures, with a moderate decline for higher numbers.
1337
The need for this rather small number of topics is
due to the nature of utilization of topics in WT.
Specifically, topics are leveraged for high-level
domain disambiguation, while fine grained word-
level distributional similarity is computed for each
rule under each such domain. This works best for
a relatively low number of topics. However, in
higher numbers, topics relate to narrower domains
and then topic biased word level similarity may
become less effective due to potential sparseness.
On the other hand, DC and SC rely on topics as
a surrogate to predicate-argument co-occurrence
features, and thus require a relatively large num-
ber of them to be effective.
Delving deeper into our test-set, Zeichner et al
provided a more detailed annotation for each in-
valid rule application. Specifically, they annotated
whether the context under which the rule is ap-
plied is valid. For example, in ?John bought my
car 9 John sold my car? the inference is invalid
due to an inherently incorrect rule, but the con-
text is valid. On the other hand in ?my boss raised
my salary 9 my boss constructed my salary? the
context {?my boss?, ?my salary?} for applying
?raise? construct? is invalid. Following, we split
the test-set for the base Lin measure into two test-
sets: (a) test-setvc, which includes all correct rule
applications and incorrect ones only under valid
contexts, and (b) test-setivc, which includes again
all correct rule applications but incorrect ones only
under invalid contexts.
Table 5 presents the performance of each com-
pared method on the two test sets. On test-
setivc, where context mismatches are abundant,
our model outperformed all other baselines (sta-
tistically significant at p < 0.01). In addition,
this time DC slightly outperformed CI. This re-
sult more explicitly shows the advantages of in-
tegrating word-level and context-sensitive topic-
level similarities for differentiating valid and in-
valid contexts for rule applications. Yet, many in-
valid rule applications occur under valid contexts
due to inherently incorrect rules, and we want to
make sure that also in this scenario our model
does not fall behind the context-insensitive mea-
sure. Indeed, on test-setvc, in which context mis-
matches are rare, our algorithm is still better than
the original measure, indicating that WT can be
safely applied to distributional similarity measures
without concerns of reduced performance in dif-
ferent context scenarios.
test-setivc test-setvc
Size
(valid:invalid)
432
(266:166)
645
(266:379)
CI 0.780 0.587
DC 0.796 0.498
SC 0.779 0.512
WT 0.854 0.621
Table 5: MAP results for the two split Lin test-
sets.
6 Discussion and Future Work
This paper addressed the problem of computing
context-sensitive reliability scores for predicate in-
ference rules. In particular, we proposed a novel
scheme that applies over any base distributional
similarity measure which operates at the word
level, and computes a single context-insensitive
score for a rule. Based on such a measure, our
scheme constructs a context-sensitive similarity
measure that computes a reliability score for pred-
icate inference rules applications in the context of
given arguments.
The contextualization of the base similarity
score was obtained using a topic-level LDA
model, which was used in a novel way. First,
it provides a topic bias for learning separate per-
topic word-level similarity scores between predi-
cates. Then, given a specific candidate rule ap-
plication, the LDA model is used to infer the
topic distribution relevant to the context speci-
fied by the given arguments. Finally, the context-
sensitive rule application score is computed as a
weighted average of the per-topic word-level sim-
ilarity scores, which are weighed according to the
inferred topic distribution.
While most works on context-insensitive pred-
icate inference rules, such as DIRT (Lin and Pan-
tel, 2001), are based on word-level similarity mea-
sures, almost all prior models addressing context-
sensitive predicate inference rules are based on
topic models (except for (Pantel et al, 2007),
which was outperformed by later models). We
therefore focused on comparing the performance
of our two-level scheme with state-of-the-art prior
topic-level and word-level models of distributional
similarity, over a random sample of inference rule
applications. Under this natural setting, the two-
level scheme consistently outperformed both types
of models when tested with three different base
similarity measures. Notably, our model shows
stable performance over a large subset of the data
1338
where context sensitivity is rare, while topic-level
models tend to underperform in such cases com-
pared to the base context-insensitive methods.
Our work is closely related to another research
line that addresses lexical similarity and substi-
tution scenarios in context. While we focus on
lexical-syntactic predicate templates and instanti-
ations of their argument slots as context, lexical
similarity methods consider various lexical units
that are not necessarily predicates, with their con-
text typically being the collection of words in a
window around them.
Various approaches have been proposed to ad-
dress lexical similarity. A number of works are
based on a compositional semantics approach,
where a prior representation of a target lexical unit
is composed with the representations of words in
its given context (Mitchell and Lapata, 2008; Erk
and Pado?, 2008; Thater et al, 2010). Other works
(Erk and Pado?, 2010; Reisinger and Mooney,
2010) use a rather large word window around tar-
get words and compute similarities between clus-
ters comprising instances of word windows. In ad-
dition, (Dinu and Lapata, 2010a) adapted the pred-
icate inference topic model from (Dinu and Lap-
ata, 2010b) to compute lexical similarity in con-
text.
A natural extension of our work would be to ex-
tend our two level model to accommodate context-
sensitive lexical similarity. For this purpose we
will need to redefine the scope of context in our
model, and adapt our method to compute context-
biased lexical similarities accordingly. Then we
will also be able to evaluate our model on the
Lexical Substitution Task (McCarthy and Navigli,
2007), which has been commonly used in recent
years as a benchmark for context-sensitive lexical
similarity models.
In a different NLP task, Eidelman et al (2012)
utilize a similar approach to ours for improving
the performance of statistical machine translation
(SMT). They learn an LDA model on the source
language side of the training corpus with the pur-
pose of identifying implicit sub-domains. Then
they utilize the distribution over topics inferred for
each document in their corpus to compute sepa-
rate per-topic translation probability tables. Fi-
nally, they train a classifier to translate a given
target word based on these tables and the inferred
topic distribution of the given document in which
the target word appears. A notable difference be-
tween our approach and theirs is that we use predi-
cate pseudo-documents consisting of argument in-
stantiations to learn our LDA model, while Eidel-
man et al use the real documents in a corpus.
We believe that combining these two approaches
may improve performance for both textual infer-
ence and SMT and plan to experiment with this
direction in future work.
Acknowledgments
This work was partially supported by the Israeli
Ministry of Science and Technology grant 3-8705,
the Israel Science Foundation grant 880/12, and
the European Community?s Seventh Framework
Programme (FP7/2007-2013) under grant agree-
ment no. 287923 (EXCITEMENT).
References
Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2011. Global learning of typed entailment rules. In
ACL.
Rahul Bhagat, Patrick Pantel, Eduard Hovy, and Ma-
rina Rey. 2007. Ledir: An unsupervised algorithm
for learning directionality of inference rules. In Pro-
ceedings of EMNLP-CoNLL.
David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent dirichlet alocation. the Journal of ma-
chine Learning research, 3:993?1022.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The pascal recognising textual entailment
challenge. In Lecture Notes in Computer Science,
volume 3944, pages 177?190.
Georgiana Dinu and Mirella Lapata. 2010a. Measur-
ing distributional similarity in context. In Proceed-
ings of EMNLP.
Georgiana Dinu and Mirella Lapata. 2010b. Topic
models for meaning similarity in context. In Pro-
ceedings of COLING: Posters.
Georgiana Dinu and Rui Wang. 2009. Inference rules
and their application to recognizing textual entail-
ment. In Proceedings EACL.
Vladimir Eidelman, Jordan Boyd-Graber, and Philip
Resnik. 2012. Topic models for dynamic transla-
tion model adaptation. In Proceedings of the ACL
conference short papers.
Katrin Erk and Sebastian Pado?. 2008. A structured
vector space model for word meaning in context. In
Proceedings of EMNLP.
Katrin Erk and Sebastian Pado?. 2010. Exemplar-based
models for word meaning in context. In Proceedings
of the ACL conference short papers.
1339
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In Proceedings of EMNLP.
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-Geffet. 2010. Directional distribu-
tional similarity for lexical inference. Natural Lan-
guage Engineering, 16(4):359?389.
Dekang Lin and Patrick Pantel. 2001. DIRT ? discov-
ery of inference rules from text. In Proceedings of
ACM SIGKDDConference on Knowledge Discovery
and Data Mining 2001.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of COLING-ACL.
Christopher D Manning, Prabhakar Raghavan, and
Hinrich Schu?tze. 2008. Introduction to information
retrieval, volume 1. Cambridge University Press
Cambridge.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Diana McCarthy and Roberto Navigli. 2007. Semeval-
2007 task 10: English lexical substitution task. In
Proceedings of SemEval.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL-08: HLT.
Ndapandula Nakashole, Gerhard Weikum, and Fabian
Suchanek. 2012. Patty: A taxonomy of relational
patterns with semantic types. EMNLP12.
Patrick Pantel, Rahul Bhagat, Bonaventura Coppola,
Timothy Chklovski, and Eduard Hovy. 2007. ISP:
Learning inferential selectional preferences. In Hu-
man Language Technologies 2007: The Conference
of the North American Chapter of the Association
for Computational Linguistics.
Deepak Ravichandran and Eduard Hovy. 2002. Learn-
ing surface text patterns for a question answering
system. In Proceedings of ACL.
Joseph Reisinger and Raymond J Mooney. 2010.
Multi-prototype vector-space models of word mean-
ing. In Human Language Technologies: The 2010
Annual Conference of the North American Chapter
of the Association for Computational Linguistics.
Alan Ritter, Oren Etzioni, et al 2010. A latent dirich-
let alocation method for selectional preferences. In
Proceedings of ACL.
Stefan Schoenmackers, Jesse Davis, Oren Etzioni, and
Daniel Weld. 2010. Learning first-order horn
clauses from web text. In Proceedings of EMNLP.
Diarmuid O Se?aghdha. 2010. Latent variable models
of selectional preference. In Proceedings of ACL.
Satoshi Sekine. 2005. Automatic paraphrase discovery
based on context and keywords between ne pairs. In
Proceedings of the Third International Workshop on
Paraphrasing (IWP2005).
Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive information extraction using unrestricted rela-
tion discovery. In Proceedings of the Human Lan-
guage Technology Conference of the NAACL, Main
Conference.
Idan Szpektor and Ido Dagan. 2008. Learning entail-
ment rules for unary templates. In Proceedings of
COLING.
Idan Szpektor, Ido Dagan, Roy Bar-Haim, and Jacob
Goldberger. 2008. Contextual preferences. In Pro-
ceedings of ACL-08: HLT.
Stefan Thater, Hagen Fu?rstenau, and Manfred Pinkal.
2010. Contextualizing semantic representations us-
ing syntactically enriched vector models. In Pro-
ceedings of ACL.
Naomi Zeichner, Jonathan Berant, and Ido Dagan.
2012. Crowdsourcing inference-rule evaluation. In
Proceedings of ACL (short papers).
1340
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 283?288,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Using Lexical Expansion to Learn Inference Rules from Sparse Data
Oren Melamud?, Ido Dagan?, Jacob Goldberger?, Idan Szpektor?
? Computer Science Department, Bar-Ilan University
? Faculty of Engineering, Bar-Ilan University
? Yahoo! Research Israel
{melamuo,dagan,goldbej}@{cs,cs,eng}.biu.ac.il
idan@yahoo-inc.com
Abstract
Automatic acquisition of inference rules
for predicates is widely addressed by com-
puting distributional similarity scores be-
tween vectors of argument words. In
this scheme, prior work typically refrained
from learning rules for low frequency
predicates associated with very sparse ar-
gument vectors due to expected low reli-
ability. To improve the learning of such
rules in an unsupervised way, we propose
to lexically expand sparse argument word
vectors with semantically similar words.
Our evaluation shows that lexical expan-
sion significantly improves performance
in comparison to state-of-the-art baselines.
1 Introduction
The benefit of utilizing template-based inference
rules between predicates was demonstrated in
NLP tasks such as Question Answering (QA)
(Ravichandran and Hovy, 2002) and Information
Extraction (IE) (Shinyama and Sekine, 2006). For
example, the inference rule ?X treat Y ? X relieve
Y?, between the templates ?X treat Y? and ?X re-
lieve Y? may be useful to identify the answer to
?Which drugs relieve stomach ache??.
The predominant unsupervised approach for
learning inference rules between templates is via
distributional similarity (Lin and Pantel, 2001;
Ravichandran and Hovy, 2002; Szpektor and Da-
gan, 2008). Specifically, each argument slot in
a template is represented by an argument vector,
containing the words (or terms) that instantiate this
slot in all of the occurrences of the template in a
learning corpus. Two templates are then deemed
semantically similar if the argument vectors of
their corresponding slots are similar.
Ideally, inference rules should be learned for
all templates that occur in the learning corpus.
However, many templates are rare and occur only
few times in the corpus. This is a typical NLP
phenomenon that can be associated with either a
small learning corpus, as in the cases of domain
specific corpora and resource-scarce languages, or
with templates with rare terms or long multi-word
expressions such as ?X be also a risk factor to Y?
or ?X finish second in Y?, which capture very spe-
cific meanings. Due to few occurrences, the slots
of rare templates are represented with very sparse
argument vectors, which in turn lead to low relia-
bility in distributional similarity scores.
A common practice in prior work for learn-
ing predicate inference rules is to simply disre-
gard templates below a minimal frequency thresh-
old (Lin and Pantel, 2001; Kotlerman et al, 2010;
Dinu and Lapata, 2010; Ritter et al, 2010). Yet,
acquiring rules for rare templates may be benefi-
cial both in terms of coverage, but also in terms
of more accurate rule application, since rare tem-
plates are less ambiguous than frequent ones.
We propose to improve the learning of rules be-
tween infrequent templates by expanding their ar-
gument vectors. This is done via a ?dual? distribu-
tional similarity approach, in which we consider
two words to be similar if they instantiate similar
sets of templates. We then use these similarities
to expand the argument vector of each slot with
words that were identified as similar to the original
arguments in the vector. Finally, similarities be-
tween templates are computed using the expanded
vectors, resulting in a ?smoothed? version of the
original similarity measure.
Evaluations on a rule application task show
that our lexical expansion approach significantly
improves the performance of the state-of-the-art
DIRT algorithm (Lin and Pantel, 2001). In addi-
tion, our approach outperforms a similarity mea-
sure based on vectors of latent topics instead of
word vectors, a common way to avoid sparseness
issues by means of dimensionality reduction.
283
2 Technical Background
The distributional similarity score for an inference
rule between two predicate templates, e.g. ?X re-
sign Y? X quit Y?, is typically computed by mea-
suring the similarity between the argument vec-
tors of the corresponding X slots and Y slots of
the two templates. To this end, first the argument
vectors should be constructed and then a similarity
measure between two vectors should be provided.
We note that we focus here on binary templates
with two slots each, but this approach can be ap-
plied to any template.
A common starting point is to compute a
co-occurrence matrix M from a learning cor-
pus. M ?s rows correspond to the template slots
and the columns correspond to the various terms
that instantiate the slots. Each entry Mi,j , e.g.
Mx quit,John, contains a count of the number of
times the term j instantiated the template slot i in
the corpus. Thus, each row Mi,? corresponds to
an argument vector for slot i. Next, some func-
tion of the counts is used to assign weights to all
Mi,j entries. In this paper we use pointwise mu-
tual information (PMI), which is common in prior
work (Lin and Pantel, 2001; Szpektor and Dagan,
2008).
Finally, rules are assessed using some similar-
ity measure between corresponding argument vec-
tors. The state-of-the-art DIRT algorithm (Lin and
Pantel, 2001) uses the highly cited Lin similarity
measures (Lin, 1998) to score rules between bi-
nary templates as follows:
(1)Lin(v, v?) =
?
w?v?v? [v(w) + v?(w)]?
w?v?v? [v(w) + v?(w)]
(2)DIRT (l ? r)
=
?
Lin(vl:x, vr:x) ? Lin(vl:y, vr:y)
where v and v? are two argument vectors, l and
r are the templates participating in the inference
rule and vl:x corresponds to the argument vector
of slot X of template l, etc. While the original
DIRT algorithm utilizes the Lin measure, one can
replace it with any other vector similarity measure.
A separate line of research for word simi-
larity introduced directional similarity measures
that have a bias for identifying generaliza-
tion/specification relations, i.e. relations be-
tween predicates with narrow (or specific) seman-
tic meanings to predicates with broader meanings
inferred by them (unlike the symmetric Lin). One
such example is the Cover measure (Weeds and
Weir, 2003):
(3)Cover(v, v?) =
?
w?v?v? [v(w)]?
w?v?v? [v(w)]
As can be seen, in the core of the Lin and Cover
measures, as well as in many other well known
distributional similarity measures such as Jaccard,
Dice and Cosine, stand the number of shared ar-
guments vs. the total number of arguments in the
two vectors. Therefore, when the argument vec-
tors are sparse, containing very few non-zero fea-
tures, these scores become unreliable and volatile,
changing greatly with every inclusion or exclusion
of a single shared argument.
3 Lexical Expansion Scheme
We wish to overcome the sparseness issues in rare
feature vectors, especially in cases where argu-
ment vectors of semantically similar predicates
comprise similar but not exactly identical argu-
ments. To this end, we propose a three step
scheme. First, we learn lexical expansion sets for
argument words, such as the set {euros, money}
for the word dollars. Then we use these sets to ex-
pand the argument word vectors of predicate tem-
plates. For example, given the template ?X can
be exchanged for Y?, with the following argument
words instantiating slot X {dollars, gold}, and
the expansion set above, we would expand the ar-
gument word vector to include all the following
words {dollars, euros, money, gold}. Finally, we
use the expanded argument word vectors to com-
pute the scores for predicate inference rules with a
given similarity measure.
When a template is instantiated with an ob-
served word, we expect it to also be instantiated
with semantically similar words such as the ones
in the expansion set of the observed word. We
?blame? the lack of such template occurrences
only on the size of the corpus and the sparseness
phenomenon in natural languages. Thus, we uti-
lize our lexical expansion scheme to synthetically
add these expected but missing occurrences, ef-
fectively smoothing or generalizing over the ex-
plicitly observed argument occurrences. Our ap-
proach is inspired by query expansion (Voorhees,
1994) in Information Retrieval (IR), as well as by
the recent lexical expansion framework proposed
in (Biemann and Riedl, 2013), and the work by
284
Miller et al (2012) on word sense disambigua-
tion. Yet, to the best of our knowledge, this is the
first work that applies lexical expansion to distri-
butional similarity feature vectors. We next de-
scribe our scheme in detail.
3.1 Learning Lexical Expansions
We start by constructing the co-occurrence matrix
M (Section 2), where each entry Mt:s,w indicates
the number of times that word w instantiates slot
s of template t in the learning corpus, denoted by
?t:s?, where s can be either X or Y.
In traditional distributional similarity, the rows
Mt:s,? serve as argument vectors of template slots.
However, to learn expansion sets we take a ?dual?
view and consider each matrix column M?:?,w (de-
noted vw) as a feature vector for the argument
word w. Under this view, templates (or more
specifically, template slots) are the features. For
instance, for the word dollars the respective fea-
ture vector may include entries such as ?X can be
exchanged for?, ?can be exchanged for Y?, ?pur-
chase Y? and ?sell Y?.
We next learn an expansion set per each word
w by computing the distributional similarity be-
tween the vectors of w and any other argument
word w?, sim(vw, vw?). Then we take the N most
similar words as w?s expansion set with degree
N , denoted by LNw = {w?1, ..., w?N}. Any simi-
larity measure could be used, but as our experi-
ments show, different measures generate sets with
different properties, and some may be fitter for ar-
gument vector expansion than others.
3.2 Expanding Argument Vectors
Given a row count vector Mt:s,? for slot s of tem-
plate t, we enrich it with expansion sets as fol-
lows. For each w in Mt:s,?, the original count in
vt:s(w) is redistributed equally between itself and
all words in w?s expansion set, i.e. all w? ? LNw ,
(possibly yielding fractional counts) where N is a
global parameter of the model. Specifically, the
new count that is assigned to each word w is its
remaining original count after it has been redis-
tributed (or zero if no original count), plus all the
counts that were distributed to it from other words.
Next, PMI weights are recomputed according to
the new counts, and the resulting expanded vector
is denoted by v+t:s. Similarity between template
slots is now computed over the expanded vectors
instead of the original ones, e.g. Lin(v+l:x, v+r:x).
4 Experimental Settings
We constructed a relatively small learning corpus
for investigating the sparseness issues of such cor-
pora. To this end, we used a random sample from
the large scale web-based ReVerb corpus1 (Fader
et al, 2011), comprising tuple extractions of pred-
icate templates with their argument instantiations.
We applied some clean-up preprocessing to these
extractions, discarding stop words, rare words and
non-alphabetical words that instantiated either the
X or the Y argument slots. In addition, we dis-
carded templates that co-occur with less than 5
unique argument words in either of their slots, as-
suming that such few arguments cannot convey re-
liable semantic information, even with expansion.
Our final corpus consists of around 350,000 ex-
tractions and 14,000 unique templates. In this cor-
pus around one third of the extractions refer to
templates that co-occur with at most 35 unique ar-
guments in both their slots.
We evaluated the quality of inference
rules using the dataset constructed by Zeich-
ner et al (2012)2, which contains about 6,500
manually annotated template rule applications,
each labeled as correct or not. For example,
?The game develop eye-hand coordination9 The
game launch eye-hand coordination? is a rule
application in this dataset of the rule ?X develop
Y ? X launch Y?, labeled as incorrect, and
?Captain Cook sail to Australia? Captain Cook
depart for Australia? is a rule application of the
rule ?X sail to Y ? X depart for Y?, labeled as
correct. Specifically, we induced two datasets
from Zeichner et al?s dataset, denoted DS-5-35
and DS-5-50, which consist of all rule applica-
tions whose templates are present in our learning
corpus and co-occurred with at least 5 and at
most 35 and 50 unique argument words in both
their slots, respectively. DS-5-35 includes 311
rule applications (104 correct and 207 incorrect)
and DS-5-50 includes 502 rule applications (190
correct and 312 incorrect).
Our evaluation task is to rank all rule applica-
tions in each test set based on the similarity scores
of the applied rules. Optimal performance would
rank all correct rule applications above the in-
correct ones. As a baseline for rule scoring we
1http://reverb.cs.washington.edu/
2http://www.cs.biu.ac.il/nlp/
downloads/annotation-rule-application.
htm
285
used the DIRT algorithm scheme, denoted DIRT-
LE-None. We then compared between the perfor-
mance of this baseline and its expanded versions,
testing two similarity measures for generating the
expansion sets of arguments: Lin and Cover. We
denote these expanded methods DIRT-LE-SIM-N,
where SIM is the similarity measure used to gen-
erate the expansion sets and N is the lexical expan-
sion degree, e.g. DIRT-LE-Lin-2.
We remind the reader that our scheme utilizes
two similarity measures. The first measure as-
sesses the similarity between the argument vectors
of the two templates in the rule. This measure
is kept constant in our experiments and is iden-
tical to DIRT?s similarity measure (Lin). 3 The
second measure assesses the similarity between
words and is used for the lexical expansion of ar-
gument vectors. Since this is the research goal
of this paper, we experimented with two different
measures for lexical expansion: a symmetric mea-
sure (Lin) and an asymmetric measure (Cover).
To this end we evaluated their effect on DIRT?s
rule ranking performance and compared them to a
vanilla version of DIRT without lexical expansion.
As another baseline, we follow Dinu and La-
pata (2010) inducing LDA topic vectors for tem-
plate slots and computing predicate template infer-
ence rule scores based on similarity between these
vectors. We use standard hyperparameters for
learning the LDA model (Griffiths and Steyvers,
2004). This method is denoted LDA-K, where K is
the number of topics in the model.
5 Results
We evaluated the performance of each tested
method by measuring Mean Average Precision
(MAP) (Manning et al, 2008) of the rule applica-
tion ranking computed by this method. In order
to compute MAP values and corresponding sta-
tistical significance, we randomly split each test
set into 30 subsets. For each method we com-
puted Average Precision on every subset and then
took the average as the MAP value. We varied
the degree of the lexical expansion in our model
and the number of topics in the topic model base-
line to analyze their effect on the performance of
these methods on our datasets. We note that in our
model a greater degree of lexical expansion cor-
3Experiments with Cosine as the template similarity mea-
sure instead of Lin for both DIRT and its expanded versions
yielded similar results. We omit those for brevity.
responds to more aggressive smoothing (or gen-
eralization) of the explicitly observed data, while
the same goes for a lower number of topics in the
topic model. The results on DS-5-35 and DS-5-50
are illustrated in Figure 1.
The most dramatic improvement over the base-
lines is evident in DS-5-35, where DIRT-LE-
Cover-2 achieves a MAP score of 0.577 in com-
parison to 0.459 achieved by its DIRT-LE-None
baseline. This is indeed the dataset where we ex-
pected expansion to affect most due the extreme
sparseness of argument vectors. Both DIRT-LE-
Cover-N and DIRT-LE-Lin-N outperform DIRT-
LE-None for all tested values of N , with statisti-
cal significance via a paired t-test at p < 0.05 for
DIRT-LE-Cover-N where 1 ? N ? 5, and p <
0.01 for DIRT-LE-Cover-2. On DS-5-50, improve-
ment over the DIRT-LE-None baseline is still sig-
nificant with both DIRT-LE-Cover-N and DIRT-
LE-Lin-N outperforming DIRT-LE-None. DIRT-
LE-Cover-N again performs best and achieves a
relative improvement of over 10% with statistical
significance at p < 0.05 for 2 ? N ? 3.
The above shows that expansion is effective for
improving rule learning between infrequent tem-
plates. Furthermore, the fact that DIRT-LE-Cover-
N outperforms DIRT-LE-Lin-N suggests that us-
ing directional expansions, which are biased to
generalizations of the observed argument words,
e.g. vehicle as an expansion for car, is more ef-
fective than using symmetrically related words,
such as bicycle or automobile. This conclusion
appears also to be valid from a semantic reason-
ing perspective, as given an observed predicate-
argument occurrence, such as ?drive car? we can
more likely infer that a presumed occurrence of
the same predicate with a generalization of the ar-
gument, such as ?drive vehicle?, is valid, i.e. ?drive
car ? drive vehicle?. On the other hand while
?drive car ? drive automobile? is likely to be
valid, ?drive car ? drive bicycle? and ?drive ve-
hicle? drive bicycle? are not.
Figure 1 also depicts the performance of LDA
as a vector smoothing approach. LDA-K out-
performs the DIRT-LE-None baseline under DS-
5-35 but with no statistical significance. Under
DS-5-50 LDA-K performs worst, slightly outper-
forming DIRT-LE-None only for K=450. Further-
more, under both datasets, LDA-K is outperformed
by DIRT-LE-Cover-N. These results indicate that
LDA is less effective than our expansion approach.
286
Figure 1: MAP scores on DS-5-35 and DS-5-50 for the original DIRT scheme, denoted DIRT-LE-None,
and for the compared smoothing methods as follows. DIRT with varied degrees of lexical expansion
is denoted as DIRT-LE-Lin-N and DIRT-LE-Cover-N. The topic model with varied number of topics is
denoted as LDA-K. Data labels indicate the expansion degree (N) or the number of LDA topics (K),
depending on the tested method.
One reason may be that in our model, every expan-
sion set may be viewed as a cluster around a spe-
cific word, an outstanding difference in compari-
son to topics, which provide a global partition over
all words. We note that performance improve-
ment of singleton document clusters over global
partitions was also shown in IR (Kurland and Lee,
2009).
In order to further illustrate our lexical expan-
sion scheme we focus on the rule application
?Captain Cook sail to Australia? Captain Cook
depart for Australia?, which is labeled as correct
in our test set and corresponds to the rule ?X sail
to Y ? X depart for Y?. There are 30 words in-
stantiating the X slot of the predicate ?sail to?
in our learning corpus including {Columbus, em-
peror, James, John, trader}. On the other hand,
there are 18 words instantiating the X slot of the
predicate ?depart for? including {Amanda, Jerry,
Michael, mother, queen}. While semantic simi-
larity between these two sets of words is evident,
they share no words in common, and therefore the
original DIRT algorithm, DIRT-LE-None, wrongly
assigns a zero score to the rule.
The following are descriptions of some of the
argument word expansions performed by DIRT-
LE-Cover-2 (using the notation LNw defined in Sec-
tion 3.1) for the X slot of ?sail to? L2John = {mr.,
dr.}, L2trader = {people, man}, and for the X slot
of ?depart for?, L2Michael = {John, mr.}, L2mother =
{people, woman}. Given these expansions the two
slots now share the following words {mr. ,people,
John} and the rule score becomes positive.
It is also interesting to compare the expansions
performed by DIRT-LE-Lin-2 to the above. For
instance in this case L2mother = {father, sarah},
which does not identify people as a shared argu-
ment for the rule.
6 Conclusions
We propose to improve the learning of infer-
ence rules between infrequent predicate templates
with sparse argument vectors by utilizing a novel
scheme that lexically expands argument vectors
with semantically similar words. Similarities be-
tween argument words are discovered using a dual
distributional representation, in which templates
are the features.
We tested the performance of our expansion
approach on rule application datasets that were
biased towards rare templates. Our evaluation
showed that rule learning with expanded vectors
outperformed the baseline learning with original
vectors. It also outperformed an LDA-based simi-
larity model that overcomes sparseness via dimen-
sionality reduction.
In future work we plan to investigate how our
scheme performs when integrated with manually
constructed resources for lexical expansion, such
as WordNet (Fellbaum, 1998).
Acknowledgments
This work was partially supported by the Israeli
Ministry of Science and Technology grant 3-8705,
the Israel Science Foundation grant 880/12, and
the European Community?s Seventh Framework
Programme (FP7/2007-2013) under grant agree-
ment no. 287923 (EXCITEMENT).
287
References
Chris Biemann and Martin Riedl. 2013. Text: Now
in 2d! a framework for lexical expansion with con-
textual similarity. Journal of Language Modeling,
1(1).
Georgiana Dinu and Mirella Lapata. 2010. Topic mod-
els for meaning similarity in context. In Proceedings
of COLING: Posters.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In Proceedings of EMNLP.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. Bradford Books.
Thomas L Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
academy of Sciences of the United States of Amer-
ica, 101(Suppl 1):5228?5235.
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-Geffet. 2010. Directional distribu-
tional similarity for lexical inference. Natural Lan-
guage Engineering, 16(4):359?389.
Oren Kurland and Lillian Lee. 2009. Clusters, lan-
guage models, and ad hoc information retrieval.
ACM Transactions on Information Systems (TOIS),
27(3):13.
Dekang Lin and Patrick Pantel. 2001. DIRT ? discov-
ery of inference rules from text. In Proceedings of
KDD.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of COLING-ACL.
Christopher D Manning, Prabhakar Raghavan, and
Hinrich Schu?tze. 2008. Introduction to information
retrieval, volume 1. Cambridge University Press
Cambridge.
Tristan Miller, Chris Biemann, Torsten Zesch, and
Iryna Gurevych. 2012. Using distributional similar-
ity for lexical expansion in knowledge-based word
sense disambiguation. Proceedings of COLING,
Mumbai, India.
Deepak Ravichandran and Eduard Hovy. 2002. Learn-
ing surface text patterns for a question answering
system. In Proceedings of ACL.
Alan Ritter, Oren Etzioni, et al 2010. A latent dirich-
let alocation method for selectional preferences. In
Proceedings of ACL.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive information extraction using unrestricted rela-
tion discovery. In Proceedings of NAACL.
Idan Szpektor and Ido Dagan. 2008. Learning entail-
ment rules for unary templates. In Proceedings of
COLING.
Ellen M Voorhees. 1994. Query expansion using
lexical-semantic relations. In Proceedings of SIGIR.
Julie Weeds and David Weir. 2003. A general frame-
work for distributional similarity. In Proceedings of
EMNLP.
Naomi Zeichner, Jonathan Berant, and Ido Dagan.
2012. Crowdsourcing inference-rule evaluation. In
Proceedings of ACL (short papers).
288
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 451?455,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Recognizing Partial Textual Entailment
Omer Levy? Torsten Zesch? Ido Dagan? Iryna Gurevych?
? Natural Language Processing Lab ? Ubiquitous Knowledge Processing Lab
Computer Science Department Computer Science Department
Bar-Ilan University Technische Universita?t Darmstadt
Abstract
Textual entailment is an asymmetric rela-
tion between two text fragments that de-
scribes whether one fragment can be in-
ferred from the other. It thus cannot cap-
ture the notion that the target fragment
is ?almost entailed? by the given text.
The recently suggested idea of partial tex-
tual entailment may remedy this problem.
We investigate partial entailment under the
faceted entailment model and the possibil-
ity of adapting existing textual entailment
methods to this setting. Indeed, our results
show that these methods are useful for rec-
ognizing partial entailment. We also pro-
vide a preliminary assessment of how par-
tial entailment may be used for recogniz-
ing (complete) textual entailment.
1 Introduction
Approaches for applied semantic inference over
texts gained growing attention in recent years,
largely triggered by the textual entailment frame-
work (Dagan et al, 2009). Textual entailment is
a generic paradigm for semantic inference, where
the objective is to recognize whether a textual hy-
pothesis (labeled H) can be inferred from another
given text (labeled T ). The definition of textual
entailment is in some sense strict, in that it requires
that H?s meaning be implied by T in its entirety.
This means that from an entailment perspective, a
text that contains the main ideas of a hypothesis,
but lacks a minor detail, is indiscernible from an
entirely unrelated text. For example, if T is ?mus-
cles move bones?, and H ?the main job of muscles
is to move bones?, then T does not entail H , and
we are left with no sense of how close (T,H) were
to entailment.
In the related problem of semantic text similar-
ity, gradual measures are already in use. The se-
mantic text similarity challenge in SemEval 2012
(Agirre et al, 2012) explicitly defined different
levels of similarity from 5 (semantic equivalence)
to 0 (no relation). For instance, 4 was defined
as ?the two sentences are mostly equivalent, but
some unimportant details differ?, and 3 meant that
?the two sentences are roughly equivalent, but
some important information differs?. Though this
modeling does indeed provide finer-grained no-
tions of similarity, it is not appropriate for seman-
tic inference for two reasons. First, the term ?im-
portant information? is vague; what makes one de-
tail more important than another? Secondly, simi-
larity is not sufficiently well-defined for sound se-
mantic inference; for example, ?snowdrops bloom
in summer? and ?snowdrops bloom in winter?
may be similar, but have contradictory meanings.
All in all, these measures of similarity do not quite
capture the gradual relation needed for semantic
inference.
An appealing approach to dealing with the
rigidity of textual entailment, while preserving the
more precise nature of the entailment definition, is
by breaking down the hypothesis into components,
and attempting to recognize whether each one is
individually entailed by T . It is called partial tex-
tual entailment, because we are only interested in
recognizing whether a single element of the hy-
pothesis is entailed. To differentiate the two tasks,
we will refer to the original textual entailment task
as complete textual entailment.
Partial textual entailment was first introduced
by Nielsen et al (2009), who presented a ma-
chine learning approach and showed significant
improvement over baseline methods. Recently, a
public benchmark has become available through
the Joint Student Response Analysis and 8th Rec-
ognizing Textual Entailment (RTE) Challenge in
SemEval 2013 (Dzikovska et al, 2013), on which
we focus in this paper.
Our goal in this paper is to investigate the idea
of partial textual entailment, and assess whether
451
existing complete textual entailment methods can
be used to recognize it. We assume the facet
model presented in SemEval 2013, and adapt ex-
isting technologies to the task of recognizing par-
tial entailment (Section 3). Our work further ex-
pands upon (Nielsen et al, 2009) by evaluating
these adapted methods on the new RTE-8 bench-
mark (Section 4). Partial entailment may also fa-
cilitate an alternative divide and conquer approach
to complete textual entailment. We provide an ini-
tial investigation of this approach (Section 5).
2 Task Definition
In order to tackle partial entailment, we need to
find a way to decompose a hypothesis. Nielsen et
al. (2009) defined a model of facets, where each
such facet is a pair of words in the hypothesis
and the direct semantic relation connecting those
two words. We assume the simplified model that
was used in RTE-8, where the relation between the
words is not explicitly stated. Instead, it remains
unstated, but its interpreted meaning should corre-
spond to the manner in which the words are related
in the hypothesis. For example, in the sentence
?the main job of muscles is to move bones?, the
pair (muscles, move) represents a facet. While it is
not explicitly stated, reading the original sentence
indicates that muscles is the agent of move.
Formally, the task of recognizing faceted entail-
ment is a binary classification task. Given a text T ,
a hypothesis H , and a facet within the hypothesis
(w1, w2), determine whether the facet is either ex-
pressed or unaddressed by the text. Nielsen et al
included additional classes such as contradicting,
but in the scope of this paper we will only tend to
the binary case, as was done in RTE-8.
Consider the following example:
T: Muscles generate movement in the body.
H: The main job of muscles is to move bones.
The facet (muscles, move) refers to the agent role
in H , and is expressed by T . However, the facet
(move, bones), which refers to a theme or direct
object relation in H , is unaddressed by T .
3 Recognizing Faceted Entailment
Our goal is to investigate whether existing entail-
ment recognition approaches can be adapted to
recognize faceted entailment. Hence, we speci-
fied relatively simple decision mechanisms over a
set of entailment detection modules. Given a text
and a facet, each module reports whether it rec-
ognizes entailment, and the decision mechanism
then determines the binary class (expressed or un-
addressed) accordingly.
3.1 Entailment Modules
Current textual entailment systems operate across
different linguistic levels, mainly on lexical infer-
ence and syntax. We examined three representa-
tive modules that reflect these levels: Exact Match,
Lexical Inference, and Syntactic Inference.
Exact Match We represent T as a bag-of-words
containing all tokens and lemmas appearing in the
text. We then check whether both facet lemmas
w1, w2 appear in the text?s bag-of-words. Exact
matching was used as a baseline in previous rec-
ognizing textual entailment challenges (Bentivogli
et al, 2011), and similar methods of lemma-
matching were used as a component in recogniz-
ing textual entailment systems (Clark and Harri-
son, 2010; Shnarch et al, 2011).
Lexical Inference This feature checks whether
both facet words, or semantically related words,
appear in T . We use WordNet (Fellbaum, 1998)
with the Resnik similarity measure (Resnik, 1995)
and count a facet term wi as matched if the sim-
ilarity score exceeds a certain threshold (0.9, em-
pirically determined on the training set). Both w1
and w2 must match for this module?s entailment
decision to be positive.
Syntactic Inference This module builds upon
the open source1 Bar-Ilan University Textual En-
tailment Engine (BIUTEE) (Stern and Dagan,
2011). BIUTEE operates on dependency trees by
applying a sequence of knowledge-based transfor-
mations that converts T into H . It determines en-
tailment according to the ?cost? of generating the
hypothesis from the text. The cost model can be
automatically tuned with a relatively small train-
ing set. BIUTEE has shown state-of-the-art per-
formance on previous recognizing textual entail-
ment challenges (Stern and Dagan, 2012).
Since BIUTEE processes dependency trees,
both T and the facet must be parsed. We therefore
extract a path in H?s dependency tree that repre-
sents the facet. This is done by first parsing H ,
and then locating the two nodes whose words com-
pose the facet. We then find their lowest common
ancestor (LCA), and extract the path P from w1 to
1cs.biu.ac.il/?nlp/downloads/biutee
452
w2 through the LCA. This path is in fact a depen-
dency tree. BIUTEE can now be given T and P
(as the hypothesis), and try to recognize whether
the former entails the latter.
3.2 Decision Mechanisms
We started our experimentation process by defin-
ing Exact Match as a baseline. Though very sim-
ple, this unsupervised baseline performed surpris-
ingly well, with 0.96 precision and 0.32 recall on
expressed facets of the training data. Given its
very high precision, we decided to use this mod-
ule as an initial filter, and employ the others for
classifying the ?harder? cases.
We present all the mechanisms that we tested:
Baseline Exact
BaseLex Exact ? Lexical
BaseSyn Exact ? Syntactic
Disjunction Exact ? Lexical ? Syntactic
Majority Exact ? (Lexical ? Syntactic)
Note that since every facet that Exact Match
classifies as expressed is also expressed by Lexi-
cal Inference, BaseLex is essentially Lexical Infer-
ence on its own, and Majority is equivalent to the
majority rule on all three modules.
4 Empirical Evaluation
4.1 Dataset: Student Response Analysis
We evaluated our methods as part of RTE-8. The
challenge focuses on the domain of scholastic
quizzes, and attempts to emulate the meticulous
marking process that teachers do on a daily basis.
Given a question, a student?s response, and a refer-
ence answer, the task of student response analysis
is to determine whether the student answered cor-
rectly. This task can be approximated as a special
case of textual entailment; by assigning the stu-
dent?s answer as T and the reference answer as H ,
we are basically asking whether one can infer the
correct (reference) answer from the student?s re-
sponse.
Recall the example from Section 2. In this case,
H is a reference answer to the question:
Q: What is the main job of muscles?
T is essentially the student answer, though it is
also possible to define T as the union of both the
question and the student answer. In this work, we
chose to exclude the question.
There were two tracks in the challenge: com-
plete textual entailment (the main task) and partial
Unseen Unseen Unseen
Answers Questions Domains
Baseline .670 .688 .731
BaseLex .756 .710 .760
BaseSyn .744 .733 .770
Disjunction .695 .655 .703
Majority .782 .765 .816
Table 1: Micro-averaged F1 on the faceted Sci-
EntsBank test set.
entailment (the pilot task). Both tasks made use of
the SciEntsBank corpus (Dzikovska et al, 2012),
which is annotated at facet-level, and provides a
convenient test-bed for evaluation of both partial
and complete entailment. This dataset was split
into train and test subsets. The test set has 16,263
facet-response pairs based on 5,106 student re-
sponses over 15 domains (learning modules). Per-
formance was measured using micro-averaged F1,
over three different scenarios:
Unseen Answers Classify new answers to ques-
tions seen in training. Contains 464 student re-
sponses.
Unseen Questions Classify new answers to
questions that were not seen in training, but other
questions from the same domain were. Contains
631 student responses.
Unseen Domains Classify new answers to un-
seen questions from unseen domains. Contains
4,011 student responses.
4.2 Results
Table 1 shows the F1-measure of each configu-
ration in each scenario. There is some variance
between the different scenarios; this may be at-
tributed to the fact that there are much fewer Un-
seen Answers and Unseen Questions instances. In
all cases, Majority significantly outperformed the
other configurations. While BaseLex and BaseSyn
improve upon the baseline, they seem to make dif-
ferent mistakes, in particular false positives. Their
conjunction is thus a more conservative indicator
of entailment, and proves helpful in terms of F1.
All improvements over the baseline were found
to be statistically significant using McNemar?s test
with p < 0.01 (excluding Disjunction). It is also
interesting to note that the systems? performance
does not degrade in ?harder? scenarios; this is a re-
sult of the mostly unsupervised nature of our mod-
ules.
453
Unfortunately, our system was the only submis-
sion in the partial entailment pilot track of RTE-
8, so we have no comparisons with other sys-
tems. However, the absolute improvement from
the exact-match baseline to the more sophisticated
Majority is in the same ballpark as that of the best
systems in previous recognizing textual entailment
challenges. For instance, in the previous recogniz-
ing textual entailment challenge (Bentivogli et al,
2011), the best system yielded an F1 score of 0.48,
while the baseline scored 0.374. We can therefore
conclude that existing approaches for recognizing
textual entailment can indeed be adapted for rec-
ognizing partial entailment.
5 Utilizing Partial Entailment for
Recognizing Complete Entailment
Encouraged by our results, we ask whether the
same algorithms that performed well on the
faceted entailment task can be used for recogniz-
ing complete textual entailment. We performed an
initial experiment that examines this concept and
sheds some light on the potential role of partial en-
tailment as a possible facilitator for complete en-
tailment.
We suggest the following 3-stage architecture:
1. Decompose the hypothesis into facets.
2. Determine whether each facet is entailed.
3. Aggregate the individual facet results and de-
cide on complete entailment accordingly.
Facet Decomposition For this initial investiga-
tion, we use the facets provided in SciEntsBank;
i.e. we assume that the step of facet decomposition
has already been carried out. When the dataset
was created for RTE-8, many facets were extracted
automatically, but only a subset was selected. The
facet selection process was done manually, as part
of the dataset?s annotation. For example, in ?the
main job of muscles is to move bones?, the facet
(job, muscles) was not selected, because it was not
critical for answering the question. We refer to the
issue of relying on manual input further below.
Recognizing Faceted Entailment This step was
carried out as explained in the previous sections.
We used the Baseline configuration and Majority,
which performed best in our experiments above.
In addition, we introduce GoldBased that uses the
gold annotation of faceted entailment, and thus
Unseen Unseen Unseen
Answers Questions Domains
Baseline .575 .582 .683
Majority .707 .673 .764
GoldBased .842 .897 .852
BestComplete .773 .745 .712
Table 2: Micro-averaged F1 on the 2-way com-
plete entailment SciEntsBank test set.
provides a certain upper bound on the perfor-
mance of determining complete entailment based
on facets.
Aggregation We chose the simplest sensible ag-
gregation rule to decide on overall entailment: a
student answer is classified as correct (i.e. it en-
tails the reference answer) if it expresses each
of the reference answer?s facets. Although this
heuristic is logical from a strict entailment per-
spective, it might yield false negatives on this par-
ticular dataset. This happens because tutors may
sometimes grade answers as valid even if they
omit some less important, or indirectly implied,
facets.
Table 2 shows the experiment?s results. The
first thing to notice is that GoldBased is not per-
fect. There are two reasons for this behavior.
First, the task of student response analysis is only
an approximation of textual entailment, albeit a
good one. This discrepancy was also observed
by the RTE-8 challenge organizers (Dzikovska et
al., 2013). The second reason is because some of
the original facets were filtered when creating the
dataset. This caused both false positives (when
important facets were filtered out) and false neg-
atives (when unimportant facets were retained).
Our Majority mechanism, which requires that
the two underlying methods for partial entailment
detection (Lexical Inference and Syntactic Infer-
ence) agree on a positive classification, bridges
about half the gap from the baseline to the gold
based method. As a rough point of comparison,
we also show the performance of BestComplete,
the winning entry in each setting of the RTE-8
main task. This measure is not directly compara-
ble to our facet-based systems, because it did not
rely on manually selected facets, and due to some
variations in the dataset size (about 20% of the stu-
dent responses were not included in the pilot task
dataset). However, these results may indicate the
454
prospects of using faceted entailment for complete
entailment recognition, suggesting it as an attrac-
tive research direction.
6 Conclusion and Future Work
In this paper, we presented an empirical attempt
to tackle the problem of partial textual entail-
ment. We demonstrated that existing methods for
recognizing (complete) textual entailment can be
successfully adapted to this setting. Our experi-
ments showed that boolean combinations of these
methods yield good results. Future research may
add additional features and more complex fea-
ture combination methods, such as weighted sums
tuned by machine learning. Furthermore, our
work focused on a specific decomposition model
? faceted entailment. Other flavors of partial en-
tailment should be investigated as well. Finally,
we examined the possibility of utilizing partial en-
tailment for recognizing complete entailment in a
semi-automatic setting, which relied on the man-
ual facet annotation in the RTE-8 dataset. Our
preliminary results suggest that this approach is
indeed feasible, and warrant further research on
facet-based entailment methods that rely on fully-
automatic facet extraction.
Acknowledgements
This work has been supported by the Volk-
swagen Foundation as part of the Lichtenberg-
Professorship Program under grant No. I/82806,
and by the European Community?s Seventh
Framework Programme (FP7/2007-2013) under
grant agreement no. 287923 (EXCITEMENT).
We would like to thank the Minerva Foundation
for facilitating this cooperation with a short term
research grant.
References
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. SemEval-2012 Task 6:
A pilot on semantic textual similarity. In Proceed-
ings of the 6th International Workshop on Semantic
Evaluation, in conjunction with the 1st Joint Con-
ference on Lexical and Computational Semantics,
pages 385?393, Montreal, Canada.
Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa Dang,
and Danilo Giampiccolo. 2011. The seventh pascal
recognizing textual entailment challenge. Proceed-
ings of TAC.
Peter Clark and Phil Harrison. 2010. Blue-lite: a
knowledge-based lexical entailment system for rte6.
Proc. of TAC.
Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan
Roth. 2009. Recognizing textual entailment: Ratio-
nale, evaluation and approaches. Natural Language
Engineering, 15(4):i?xvii.
Myroslava O Dzikovska, Rodney D Nielsen, and Chris
Brew. 2012. Towards effective tutorial feedback
for explanation questions: A dataset and baselines.
In Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 200?210. Association for Computational Lin-
guistics.
Myroslava O. Dzikovska, Rodney Nielsen, Chris Brew,
Claudia Leacock, Danilo Giampiccolo, Luisa Ben-
tivogli, Peter Clark, Ido Dagan, and Hoa Trang
Dang. 2013. Semeval-2013 task 7: The joint stu-
dent response analysis and 8th recognizing textual
entailment challenge. In *SEM 2013: The First
Joint Conference on Lexical and Computational Se-
mantics, Atlanta, Georgia, USA, 13-14 June. Asso-
ciation for Computational Linguistics.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge,
MA.
Rodney D Nielsen, Wayne Ward, and James H Mar-
tin. 2009. Recognizing entailment in intelligent
tutoring systems. Natural Language Engineering,
15(4):479?501.
Philip Resnik. 1995. Using information content to
evaluate semantic similarity in a taxonomy. In Pro-
ceedings of the 14th International Joint Conference
on Artificial Intelligence (IJCAI 1995), pages 448?
453.
Eyal Shnarch, Jacob Goldberger, and Ido Dagan. 2011.
A probabilistic modeling framework for lexical en-
tailment. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies, pages 558?
563, Portland, Oregon, USA, June. Association for
Computational Linguistics.
Asher Stern and Ido Dagan. 2011. A confidence
model for syntactically-motivated entailment proofs.
In Proceedings of the 8th International Conference
on Recent Advances in Natural Language Process-
ing (RANLP 2011), pages 455?462.
Asher Stern and Ido Dagan. 2012. Biutee: A mod-
ular open-source system for recognizing textual en-
tailment. In Proceedings of the ACL 2012 System
Demonstrations, pages 73?78, Jeju Island, Korea,
July. Association for Computational Linguistics.
455
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 97?102,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
PLIS: a Probabilistic Lexical Inference System
Eyal Shnarch1, Erel Segal-haLevi1, Jacob Goldberger2, Ido Dagan1
1Computer Science Department, Bar-Ilan University, Israel
2Faculty of Engineering, Bar-Ilan University, Israel
{shey,erelsgl,dagan}@cs.biu.ac.il
goldbej@eng.biu.ac.il
Abstract
This paper presents PLIS, an open source
Probabilistic Lexical Inference System
which combines two functionalities: (i)
a tool for integrating lexical inference
knowledge from diverse resources, and (ii)
a framework for scoring textual inferences
based on the integrated knowledge. We
provide PLIS with two probabilistic im-
plementation of this framework. PLIS is
available for download and developers of
text processing applications can use it as
an off-the-shelf component for injecting
lexical knowledge into their applications.
PLIS is easily configurable, components
can be extended or replaced with user gen-
erated ones to enable system customiza-
tion and further research. PLIS includes
an online interactive viewer, which is a
powerful tool for investigating lexical in-
ference processes.
1 Introduction and background
Semantic Inference is the process by which ma-
chines perform reasoning over natural language
texts. A semantic inference system is expected to
be able to infer the meaning of one text from the
meaning of another, identify parts of texts which
convey a target meaning, and manipulate text units
in order to deduce new meanings.
Semantic inference is needed for many Natural
Language Processing (NLP) applications. For in-
stance, a Question Answering (QA) system may
encounter the following question and candidate
answer (Example 1):
Q: which explorer discovered the New World?
A: Christopher Columbus revealed America.
As there are no overlapping words between the
two sentences, to identify that A holds an answer
for Q, background world knowledge is needed
to link Christopher Columbus with explorer and
America with New World. Linguistic knowledge
is also needed to identify that reveal and discover
refer to the same concept.
Knowledge is needed in order to bridge the gap
between text fragments, which may be dissimilar
on their surface form but share a common mean-
ing. For the purpose of semantic inference, such
knowledge can be derived from various resources
(e.g. WordNet (Fellbaum, 1998) and others, de-
tailed in Section 2.1) in a form which we denote as
inference links (often called inference/entailment
rules), each is an ordered pair of elements in which
the first implies the meaning of the second. For in-
stance, the link ship?vessel can be derived from
the hypernym relation of WordNet.
Other applications can benefit from utilizing in-
ference links to identify similarity between lan-
guage expressions. In Information Retrieval, the
user?s information need may be expressed in rele-
vant documents differently than it is expressed in
the query. Summarization systems should identify
text snippets which convey the same meaning.
Our work addresses a generic, application in-
dependent, setting of lexical inference. We there-
fore adopt the terminology of Textual Entailment
(Dagan et al, 2006), a generic paradigm for ap-
plied semantic inference which captures inference
needs of many NLP applications in a common un-
derlying task: given two textual fragments, termed
hypothesis (H) and text (T ), the task is to recog-
nize whether T implies the meaning of H , denoted
T?H. For instance, in a QA application, H rep-
resents the question, and T a candidate answer. In
this setting, T is likely to hold an answer for the
question if it entails the question.
It is challenging to properly extract the needed
inference knowledge from available resources,
and to effectively utilize it within the inference
process. The integration of resources, each has its
own format, is technically complex and the quality
97
? 
Lexical Inference 
Lexical Integrator 
? ? ? ?  
WordNet 
Wikipedia 
VerbOcean 
Text 
Hypothesis ?1 ?2 ?3 ?4 
?1 ?3 
?(? ? ?3) 
?2 
?? 
Lexical 
Resources 
?(?3 ? ?2) 
Figure 1: PLIS schema - a text-hypothesis pair is processed
by the Lexical Integrator which uses a set of lexical resources
to extract inference chains which connect the two. The Lexi-
cal Inference component provides probability estimations for
the validity of each level of the process.
of the resulting inference links is often unknown in
advance and varies considerably. For coping with
this challenge we developed PLIS, a Probabilis-
tic Lexical Inference System1. PLIS, illustrated in
Fig 1, has two main modules: the Lexical Integra-
tor (Section 2) accepts a set of lexical resources
and a text-hypothesis pair, and finds all the lex-
ical inference relations between any pair of text
term ti and hypothesis term hj , based on the avail-
able lexical relations found in the resources (and
their combination). The Lexical Inference module
(Section 3) provides validity scores for these rela-
tions. These term-level scores are used to estimate
the sentence-level likelihood that the meaning of
the hypothesis can be inferred from the text, thus
making PLIS a complete lexical inference system.
Lexical inference systems do not look into the
structure of texts but rather consider them as bag
of terms (words or multi-word expressions). These
systems are easy to implement, fast to run, practi-
cal across different genres and languages, while
maintaining a competitive level of performance.
PLIS can be used as a stand-alone efficient in-
ference system or as the lexical component of any
NLP application. PLIS is a flexible system, al-
lowing users to choose the set of knowledge re-
sources as well as the model by which inference
1The complete software package is available at http://
www.cs.biu.ac.il/nlp/downloads/PLIS.html and an online in-
teractive viewer is available for examination at http://irsrv2.
cs.biu.ac.il/nlp-net/PLIS.html.
is done. PLIS can be easily extended with new
knowledge resources and new inference models. It
comes with a set of ready-to-use plug-ins for many
common lexical resources (Section 2.1) as well
as two implementation of the scoring framework.
These implementations, described in (Shnarch et
al., 2011; Shnarch et al, 2012), provide probabil-
ity estimations for inference. PLIS has an inter-
active online viewer (Section 4) which provides a
visualization of the entire inference process, and is
very helpful for analysing lexical inference mod-
els and lexical resources usability.
2 Lexical integrator
The input for the lexical integrator is a set of lex-
ical resources and a pair of text T and hypothe-
sis H . The lexical integrator extracts lexical in-
ference links from the various lexical resources to
connect each text term ti?T with each hypothesis
term hj ?H2. A lexical inference link indicates a
semantic relation between two terms. It could be
a directional relation (Columbus?navigator) or a
bidirectional one (car?? automobile).
Since knowledge resources vary in their rep-
resentation methods, the lexical integrator wraps
each lexical resource in a common plug-in inter-
face which encapsulates resource?s inner repre-
sentation method and exposes its knowledge as a
list of inference links. The implemented plug-ins
that come with PLIS are described in Section 2.1.
Adding a new lexical resource and integrating it
with the others only demands the implementation
of the plug-in interface.
As the knowledge needed to connect a pair of
terms, ti and hj , may be scattered across few re-
sources, the lexical integrator combines inference
links into lexical inference chains to deduce new
pieces of knowledge, such as Columbus resource1???????
navigator resource2??????? explorer. Therefore, the only
assumption the lexical integrator makes, regarding
its input lexical resources, is that the inferential
lexical relations they provide are transitive.
The lexical integrator generates lexical infer-
ence chains by expanding the text and hypothesis
terms with inference links. These links lead to new
terms (e.g. navigator in the above chain example
and t? in Fig 1) which can be further expanded,
as all inference links are transitive. A transitivity
2Where i and j run from 1 to the length of the text and
hypothesis respectively.
98
limit is set by the user to determine the maximal
length for inference chains.
The lexical integrator uses a graph-based rep-
resentation for the inference chains, as illustrates
in Fig 1. A node holds the lemma, part-of-speech
and sense of a single term. The sense is the ordi-
nal number of WordNet sense. Whenever we do
not know the sense of a term we implement the
most frequent sense heuristic.3 An edge represents
an inference link and is labeled with the semantic
relation of this link (e.g. cytokine?protein is la-
beled with the WordNet relation hypernym).
2.1 Available plug-ins for lexical resources
We have implemented plug-ins for the follow-
ing resources: the English lexicon WordNet
(Fellbaum, 1998)(based on either JWI, JWNL
or extJWNL java APIs4), CatVar (Habash and
Dorr, 2003), a categorial variations database,
Wikipedia-based resource (Shnarch et al, 2009),
which applies several extraction methods to de-
rive inference links from the text and structure
of Wikipedia, VerbOcean (Chklovski and Pantel,
2004), a knowledge base of fine-grained semantic
relations between verbs, Lin?s distributional simi-
larity thesaurus (Lin, 1998), and DIRECT (Kotler-
man et al, 2010), a directional distributional simi-
larity thesaurus geared for lexical inference.
To summarize, the lexical integrator finds all
possible inference chains (of a predefined length),
resulting from any combination of inference links
extracted from lexical resources, which link any
t, h pair of a given text-hypothesis. Developers
can use this tool to save the hassle of interfac-
ing with the different lexical knowledge resources,
and spare the labor of combining their knowledge
via inference chains.
The lexical inference model, described next,
provides a mean to decide whether a given hypoth-
esis is inferred from a given text, based on weigh-
ing the lexical inference chains extracted by the
lexical integrator.
3 Lexical inference
There are many ways to implement an infer-
ence model which identifies inference relations
between texts. A simple model may consider the
3This disambiguation policy was better than considering
all senses of an ambiguous term in preliminary experiments.
However, it is a matter of changing a variable in the configu-
ration of PLIS to switch between these two policies.
4http://wordnet.princeton.edu/wordnet/related-projects/
number of hypothesis terms for which inference
chains, originated from text terms, were found. In
PLIS, the inference model is a plug-in, similar to
the lexical knowledge resources, and can be easily
replaced to change the inference logic.
We provide PLIS with two implemented base-
line lexical inference models which are mathemat-
ically based. These are two Probabilistic Lexical
Models (PLMs), HN-PLM and M-PLM which are
described in (Shnarch et al, 2011; Shnarch et al,
2012) respectively.
A PLM provides probability estimations for the
three parts of the inference process (as shown in
Fig 1): the validity probability of each inference
chain (i.e. the probability for a valid inference re-
lation between its endpoint terms) P (ti ? hj), the
probability of each hypothesis term to be inferred
by the entire text P (T ? hj) (term-level proba-
bility), and the probability of the entire hypothesis
to be inferred by the text P (T ? H) (sentence-
level probability).
HN-PLM describes a generative process by
which the hypothesis is generated from the text.
Its parameters are the reliability level of each of
the resources it utilizes (that is, the prior proba-
bility that applying an arbitrary inference link de-
rived from each resource corresponds to a valid in-
ference). For learning these parameters HN-PLM
applies a schema of the EM algorithm (Demp-
ster et al, 1977). Its performance on the recog-
nizing textual entailment task, RTE (Bentivogli et
al., 2009; Bentivogli et al, 2010), are in line with
the state of the art inference systems, including
complex systems which perform syntactic analy-
sis. This model is improved by M-PLM, which de-
duces sentence-level probability from term-level
probabilities by a Markovian process. PLIS with
this model was used for a passage retrieval for a
question answering task (Wang et al, 2007), and
outperformed state of the art inference systems.
Both PLMs model the following prominent as-
pects of the lexical inference phenomenon: (i)
considering the different reliability levels of the
input knowledge resources, (ii) reducing inference
chain probability as its length increases, and (iii)
increasing term-level probability as we have more
inference chains which suggest that the hypothesis
term is inferred by the text. Both PLMs only need
sentence-level annotations from which they derive
term-level inference probabilities.
To summarize, the lexical inference module
99
?(? ? ?) 
?(??? ??) 
?(? ? ??) 
configuration 
1 
2 
3 
4 
Figure 2: PLIS interactive viewer with Example 1 demonstrates knowledge integration of multiple inference chains and
resource combination (additional explanations, which are not part of the demo, are provided in orange).
provides the setting for interfacing with the lexi-
cal integrator. Additionally, the module provides
the framework for probabilistic inference models
which estimate term-level probabilities and inte-
grate them into a sentence-level inference deci-
sion, while implementing prominent aspects of
lexical inference. The user can choose to apply
another inference logic, not necessarily probabilis-
tic, by plugging a different lexical inference model
into the provided inference infrastructure.
4 The PLIS interactive system
PLIS comes with an online interactive viewer5 in
which the user sets the parameters of PLIS, inserts
a text-hypothesis pair and gets a visualization of
the entire inference process. This is a powerful
tool for investigating knowledge integration and
lexical inference models.
Fig 2 presents a screenshot of the processing of
Example 1. On the right side, the user configures
the system by selecting knowledge resources, ad-
justing their configuration, setting the transitivity
limit, and choosing the lexical inference model to
be applied by PLIS.
After inserting a text and a hypothesis to the
appropriate text boxes, the user clicks on the in-
fer button and PLIS generates all lexical inference
chains, of length up to the transitivity limit, that
connect text terms with hypothesis terms, as avail-
able from the combination of the selected input re-
5http://irsrv2.cs.biu.ac.il/nlp-net/PLIS.html
sources. Each inference chain is presented in a line
between the text and hypothesis.
PLIS also displays the probability estimations
for all inference levels; the probability of each
chain is presented at the end of its line. For each
hypothesis term, term-level probability, which
weighs all inference chains found for it, is given
below the dashed line. The overall sentence-level
probability integrates the probabilities of all hy-
pothesis terms and is displayed in the box at the
bottom right corner.
Next, we detail the inference process of Exam-
ple 1, as presented in Fig 2. In this QA example,
the probability of the candidate answer (set as the
text) to be relevant for the given question (the hy-
pothesis) is estimated. When utilizing only two
knowledge resources (WordNet and Wikipedia),
PLIS is able to recognize that explorer is inferred
by Christopher Columbus and that New World is
inferred by America. Each one of these pairs has
two independent inference chains, numbered 1?4,
as evidence for its inference relation.
Both inference chains 1 and 3 include a single
inference link, each derived from a different rela-
tion of the Wikipedia-based resource. The infer-
ence model assigns a higher probability for chain
1 since the BeComp relation is much more reliable
than the Link relation. This comparison illustrates
the ability of the inference model to learn how to
differ knowledge resources by their reliability.
Comparing the probability assigned by the in-
100
ference model for inference chain 2 with the prob-
abilities assigned for chains 1 and 3, reveals the
sophisticated way by which the inference model
integrates lexical knowledge. Inference chain 2
is longer than chain 1, therefore its probability is
lower. However, the inference model assigns chain
2 a higher probability than chain 3, even though
the latter is shorter, since the model is sensitive
enough to consider the difference in reliability lev-
els between the two highly reliable hypernym re-
lations (from WordNet) of chain 2 and the less re-
liable Link relation (from Wikipedia) of chain 3.
Another aspect of knowledge integration is ex-
emplified in Fig 2 by the three circled probabili-
ties. The inference model takes into consideration
the multiple pieces of evidence for the inference
of New World (inference chains 3 and 4, whose
probabilities are circled). This results in a term-
level probability estimation for New World (the
third circled probability) which is higher than the
probabilities of each chain separately.
The third term of the hypothesis, discover, re-
mains uncovered by the text as no inference chain
was found for it. Therefore, the sentence-level
inference probability is very low, 37%. In order
to identify that the hypothesis is indeed inferred
from the text, the inference model should be pro-
vided with indications for the inference of dis-
cover. To that end, the user may increase the tran-
sitivity limit in hope that longer inference chains
provide the needed information. In addition, the
user can examine other knowledge resources in
search for the missing inference link. In this ex-
ample, it is enough to add VerbOcean to the in-
put of PLIS to expose two inference chains which
connect reveal with discover by combining an in-
ference link from WordNet and another one from
VerbOcean. With this additional information, the
sentence-level probability increases to 76%. This
is a typical scenario of utilizing PLIS, either via
the interactive system or via the software, for ana-
lyzing the usability of the different knowledge re-
sources and their combination.
A feature of the interactive system, which is
useful for lexical resources analysis, is that each
term in a chain is clickable and links to another
screen which presents all the terms that are in-
ferred from it and those from which it is inferred.
Additionally, the interactive system communi-
cates with a server which runs PLIS, in a full-
duplex WebSocket connection6. This mode of op-
eration is publicly available and provides a method
for utilizing PLIS, without having to install it or
the lexical resources it uses.
Finally, since PLIS is a lexical system it can
easily be adjusted to other languages. One only
needs to replace the basic lexical text processing
tools and plug in knowledge resources in the tar-
get language. If PLIS is provided with bilingual
resources,7 it can operate also as a cross-lingual
inference system (Negri et al, 2012). For instance,
the text in Fig 3 is given in English, while the hy-
pothesis is written in Spanish (given as a list of
lemma:part-of-speech). The left side of the figure
depicts a cross-lingual inference process in which
the only lexical knowledge resource used is a man-
ually built English-Spanish dictionary. As can be
seen, two Spanish terms, jugador and casa remain
uncovered since the dictionary alone cannot con-
nect them to any of the English terms in the text.
As illustrated in the right side of Fig 3,
PLIS enables the combination of the bilingual
dictionary with monolingual resources to pro-
duce cross-lingual inference chains, such as foot-
baller hypernym???????player manual??????jugador. Such in-
ference chains have the capability to overcome
monolingual language variability (the first link
in this chain) as well as to provide cross-lingual
translation (the second link).
5 Conclusions
To utilize PLIS one should gather lexical re-
sources, obtain sentence-level annotations and
train the inference model. Annotations are avail-
able in common data sets for task such as QA,
Information Retrieval (queries are hypotheses and
snippets are texts) and Student Response Analysis
(reference answers are the hypotheses that should
be inferred by the student answers).
For developers of NLP applications, PLIS of-
fers a ready-to-use lexical knowledge integrator
which can interface with many common lexical
knowledge resources and constructs lexical in-
ference chains which combine the knowledge in
them. A developer who wants to overcome lex-
ical language variability, or to incorporate back-
ground knowledge, can utilize PLIS to inject lex-
6We used the socket.io implementation.
7A bilingual resource holds inference links which connect
terms in different languages (e.g. an English-Spanish dictio-
nary can provide the inference link explorer?explorador).
101
Figure 3: PLIS as a cross-lingual inference system. Left: the process with a single manual bilingual resource. Right: PLIS
composes cross-lingual inference chains to increase hypothesis coverage and increase sentence-level inference probability.
ical knowledge into any text understanding appli-
cation. PLIS can be used as a lightweight infer-
ence system or as the lexical component of larger,
more complex inference systems.
Additionally, PLIS provides scores for infer-
ence chains and determines the way to combine
them in order to recognize sentence-level infer-
ence. PLIS comes with two probabilistic lexical
inference models which achieved competitive per-
formance levels in the tasks of recognizing textual
entailment and passage retrieval for QA.
All aspects of PLIS are configurable. The user
can easily switch between the built-in lexical re-
sources, inference models and even languages, or
extend the system with additional lexical resources
and new inference models.
Acknowledgments
The authors thank Eden Erez for his help with
the interactive viewer and Miquel Espla` Gomis
for the bilingual dictionaries. This work was par-
tially supported by the European Community?s
7th Framework Programme (FP7/2007-2013) un-
der grant agreement no. 287923 (EXCITEMENT)
and the Israel Science Foundation grant 880/12.
References
Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo
Giampiccolo, and Bernardo Magnini. 2009. The
fifth PASCAL recognizing textual entailment chal-
lenge. In Proc. of TAC.
Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa Trang
Dang, and Danilo Giampiccolo. 2010. The sixth
PASCAL recognizing textual entailment challenge.
In Proc. of TAC.
Timothy Chklovski and Patrick Pantel. 2004. VerbO-
cean: Mining the web for fine-grained semantic verb
relations. In Proc. of EMNLP.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The PASCAL recognising textual entailment
challenge. In Lecture Notes in Computer Science,
volume 3944, pages 177?190.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the
EM algorithm. Journal of the royal statistical soci-
ety, series [B], 39(1):1?38.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge,
Massachusetts.
Nizar Habash and Bonnie Dorr. 2003. A categorial
variation database for English. In Proc. of NAACL.
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-Geffet. 2010. Directional distribu-
tional similarity for lexical inference. Natural Lan-
guage Engineering, 16(4):359?389.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proc. of COLOING-ACL.
Matteo Negri, Alessandro Marchetti, Yashar Mehdad,
Luisa Bentivogli, and Danilo Giampiccolo. 2012.
Semeval-2012 task 8: Cross-lingual textual entail-
ment for content synchronization. In Proc. of Se-
mEval.
Eyal Shnarch, Libby Barak, and Ido Dagan. 2009. Ex-
tracting lexical reference rules from Wikipedia. In
Proc. of ACL.
Eyal Shnarch, Jacob Goldberger, and Ido Dagan. 2011.
Towards a probabilistic model for lexical entailment.
In Proc. of the TextInfer Workshop.
Eyal Shnarch, Ido Dagan, and Jacob Goldberger. 2012.
A probabilistic lexical model for ranking textual in-
ferences. In Proc. of *SEM.
Mengqiu Wang, Noah A. Smith, and Teruko Mita-
mura. 2007. What is the Jeopardy model? A quasi-
synchronous grammar for QA. In Proc. of EMNLP.
102
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 739?744,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Recognizing Implied Predicate-Argument Relationships
in Textual Inference
Asher Stern
Computer Science Department
Bar-Ilan University
astern7@cs.biu.ac.il
Ido Dagan
Computer Science Department
Bar-Ilan University
dagan@cs.biu.ac.il
Abstract
We investigate recognizing implied
predicate-argument relationships which
are not explicitly expressed in syntactic
structure. While prior works addressed
such relationships as an extension to se-
mantic role labeling, our work investigates
them in the context of textual inference
scenarios. Such scenarios provide prior
information, which substantially eases
the task. We provide a large and freely
available evaluation dataset for our task
setting, and propose methods to cope with
it, while obtaining promising results in
empirical evaluations.
1 Motivation and Task
This paper addresses a typical sub-task in tex-
tual inference scenarios, of recognizing implied
predicate-argument relationships which are not
expressed explicitly through syntactic structure.
Consider the following example:
(i)
The crucial role Vioxx plays in Merck?s port-
folio was apparent last week when Merck?s
shares plunged 27 percent to 33 dollars after the
withdrawal announcement.
While a human reader understands that the
withdrawal refers to Vioxx, and hence an im-
plied predicate-argument relationship holds be-
tween them, this relationship is not expressed in
the syntactic structure, and will be missed by syn-
tactic parsers or standard semantic role labelers.
This paper targets such types of implied rela-
tionships in textual inference scenarios. Partic-
ularly, we investigate the setting of Recognizing
Textual Entailment (RTE) as a typical scenario of
textual inference. We suggest, however, that the
same challenge, as well as the solutions proposed
in our work, are applicable, with proper adap-
tations, to other textual-inference scenarios, like
Question Answering, and Information Extraction
(see Section 6).
An RTE problem instance is composed of two
text fragments, termed Text and Hypothesis, as in-
put. The task is to recognize whether a human
reading the Text would infer that the Hypothesis
is most likely true (Dagan et al, 2006). For our
problem, consider a positive Text Hypothesis pair,
where the Text is example (i) above and the Hy-
pothesis is:
(ii)
Merck withdrew Vioxx.
A common approach for recognizing textual en-
tailment is to verify that all the textual elements
of the Hypothesis are covered, or aligned, by el-
ements of the Text. These elements typically in-
clude lexical terms as well as relationships be-
tween them. In our example, the Hypothesis lexi-
cal terms (?Merck?, ?withdrew? and ?Vioxx?) are
indeed covered by the Text. Yet, the predicate-
argument relationships (e.g., ?withdrawal-Vioxx?)
are not expressed in the text explicitly. In such
a case, an RTE system has to verify that the
predicate-argument relationships which are ex-
plicitly expressed in the Hypothesis, are implied
from the Text discourse. Such cases are quite fre-
quent (?17%) in the settings of our dataset, de-
scribed in Section 3.
Consequently, we define the task of recognizing
implied predicate-argument relationships, with il-
lustrating examples in Table 1, as follows. The
input includes a Text and a Hypothesis. Two terms
in the Hypothesis, predicate and argument, are
marked, where a predicate-argument relationship
between them is explicit in the Hypothesis syntac-
tic structure. Two terms in the Text, candidate-
predicate and candidate-argument, aligned to the
Hypothesis predicate and argument, are marked
as well. However, no predicate-argument rela-
tionship between them is expressed syntactically.
The task is to recognize whether the predicate-
739
# Hypothesis Text Y/N
1 Merck [withdrew]
pred
[Vioxx]
arg
from the market.
The crucial role [Vioxx]
cand-arg
plays in Merck?s
portfolio was apparent last week when Merck?s
shares plunged 27 percent to 33 dollars after the
[withdrawal]
cand-pred
announcement.
Y
2 Barbara Cummings heard the tale
of a woman who was coming
to Crawford to [join]
pred
Cindy
Sheehans [protest]
arg
.
Sheehan?s [protest]
cand-arg
is misguided and is hurting
troop morale. . . .
Sheehan never wanted Casey to [join]
cand-pred
the mil-
itary.
N
3 Casey Sheehan was [killed]
pred
in
[Iraq]
arg
.
5 days after he arrived in [Iraq]
cand-arg
last year, Casey
Sheehan was [killed]
cand-pred
.
Y
4 Hurricane Rita [threatened]
pred
[New Orleans]
arg
.
Hurricane Rita was upgraded from a tropical storm as
it [threatened]
cand-pred
the southeastern United States,
forcing an alert in southern Florida and scuttling plans
to repopulate [New Orleans]
cand-arg
after Hurricane
Katrina turned it into a ghost city 3 weeks earlier.
Y
5 Alberto Gonzales defends
[renewal]
pred
of the [Patriot
Act]
arg
to Congress.
A senior official defended the [Patriot Act]
cand-arg
. . .
. . . President Bush has urged Congress to
[renew]
cand-pred
the law . . .
Y
6 The [train]
arg
[crash]
pred
injured
nearly 200 people.
At least 10 people were killed . . . in the [crash]
cand-pred
. . .
Alvarez is accused of . . . causing the derailment of one
[train]
cand-arg
. . .
Y
Table 1: Example task instances from our dataset. The last column specifies the Yes/No annotation,
indicating whether the sought predicate-argument relationship is implied in the Text. For illustration, a
dashed line indicates an explicit argument that is related to the candidate argument through some kind of
discourse reference. Pred, arg and cand abbreviate predicate, argument and candidate respectively.
argument relationship, as expressed in the Hypoth-
esis, holds implicitly also in the Text.
To address this task, we provide a large and
freely available annotated dataset, and propose
methods for coping with it. A related task, de-
scribed in the next section, deals with such implied
predicate-argument relationships as an extension
to Semantic Role Labeling. While the results re-
ported so far on that annotation task were rela-
tively low, we suggest that the task itself may be
more complicated than what is actually required
in textual inference scenarios. On the other hand,
the results obtained for our task, which does fit
textual inference scenarios, are promising, and en-
courage utilizing algorithms for this task in actual
inference systems.
2 Prior Work
The most notable work targeting implied
predicate-argument relationships is the 2010
SemEval task of Linking Events and Their Par-
ticipants in Discourse (Ruppenhofer et al, 2009).
This task extends Semantic Role Labeling to cases
in which a core argument of a predicate is missing
in the syntactic structure but a filler for the
corresponding semantic role appears elsewhere
and can be inferred from discourse. For example,
in the following sentence the semantic role goal is
unfilled:
(iii)
He arrived (0
Goal
) at 8pm.
Yet, we can expect to find an implied filler for
goal elsewhere in the document.
The SemEval task, termed henceforth as Im-
plied SRL, involves three major sub-tasks. First,
for each predicate, the unfilled roles, termed Null
Instantiations (NI), should be detected. Second,
each NI should be classified as Definite NI (DNI),
meaning that the role filler must exist in the dis-
course, or Indefinite NI otherwise. Third, the DNI
fillers should be found (DNI linking).
Later works that followed the SemEval chal-
lenge include (Silberer and Frank, 2012) and
(Roth and Frank, 2013), which proposed auto-
740
matic dataset generation methods and features
which capture discourse phenomena. Their high-
est result was 12% F1-score. Another work is the
probabilistic model of Laparra and Rigau (2012),
which is trained by properties captured not only
from implicit arguments but also from explicit
ones, resulting in 19% F1-score. Another notable
work is (Gerber and Chai, 2012), which was lim-
ited to ten carefully selected nominal predicates.
2.1 Annotations vs. Recognition
Comparing to the implied SRL task, our task may
better fit the needs of textual inference. First, some
relatively complex steps of the implied SRL task
are avoided in our setting, while on the other hand
it covers more relevant cases.
More concretely, in textual inference the can-
didate predicate and argument are typically iden-
tified, as they are aligned by the RTE system to
a predicate and an argument of the Hypothesis.
Thus, the only remaining challenge is to verify
that the sought relationship is implied in the text.
Therefore, the sub-tasks of identifying and classi-
fying DNIs can be avoided.
On the other hand, in some cases the candi-
date argument is not a DNI, but is still required
in textual inference. One type of such cases are
non-core arguments, which cannot be Definite NIs.
However, textual inference deals with non-core ar-
guments as well (see example 3 in Table 1).
Another case is when an implied predicate-
argument relationship holds even though the cor-
responding role is already filled by another argu-
ment, hence not an NI. Consider example 4 of Ta-
ble 1. While the object of ?threatened? is filled (in
the Text) by ?southeastern United States?, a hu-
man reader also infers the ?threatened-New Or-
leans? relationship. Such cases might follow a
meronymy relation between the filler (?southeast-
ern United States?) and the candidate argument
(?New Orleans?), or certain types of discourse (co-
)references (e.g., example 5 in Table 1), or some
other linguistic phenomena. Either way, they are
crucial for textual inference, while not being NIs.
3 Dataset
This section describes a semi-automatic method
for extracting candidate instances of implied
predicate-argument relationship from an RTE
dataset. This extraction process directly follows
our task formalization. Given a Text Hypothe-
sis pair, we locate a predicate-argument relation-
ship in the Hypothesis, where both the predicate
and the argument appear also in the Text, while
the relationship between them is not expressed in
its syntactic structure. This process is performed
automatically, based on syntactic parsing (see be-
low). Then, a human reader annotates each in-
stance as ?Yes? ? meaning that the implied rela-
tionship indeed holds in the Text, or ?No? other-
wise. Example instances, constructed by this pro-
cess, are shown in Table 1.
In this work we used lemma-level lexical
matching, as well as nominalization matching, to
align the Text predicates and arguments to the Hy-
pothesis. We note that more advanced match-
ing, e.g., by utilizing knowledge resources (like
WordNet), can be performed as well. To identify
explicit predicate-argument relationships we uti-
lized dependency parsing by the Easy-First parser
(Goldberg and Elhadad, 2010). Nominalization
matching (e.g., example 1 of Table 1) was per-
formed with Nomlex (Macleod et al, 1998).
By applying this method on the RTE-6 dataset
(Bentivogli et al, 2010), we constructed a
dataset of 4022 instances, where 2271 (56%)
are annotated as positive instances, and 1751
as negative ones. This dataset is significantly
larger than prior datasets for the implied SRL
task. To calculate inter-annotator agreement, the
first author also annotated 185 randomly-selected
instances. We have reached high agreement score
of 0.80 Kappa. The dataset is freely available at
www.cs.biu.ac.il/
?
nlp/resources/
downloads/implied-relationships.
4 Recognition Algorithm
We defined 15 features, summarized in Table 2,
which capture local and discourse phenomena.
These features do not depend on manually built
resources, and hence are portable to resource-poor
languages. Some features were proposed in prior
works, and are marked by G&C (Gerber and Chai,
2012) or S&F (Silberer and Frank, 2012). Our best
results were obtained with the Random Forests
learning algorithm (Breiman, 2001). The first two
features are described in the next subsection, while
the others are explained in the table itself.
4.1 Statistical discourse features
Statistical features in prior works mostly cap-
ture general properties of the predicate and the
741
# Category Feature Prev. work
1 co-occurring predicate (explained in subsection 4.1) New
2
statistical
discourse co-occurring argument (explained in subsection 4.1) New
3 co-reference: whether an explicit argument of p co-refers with a. New
4 last known location: If the NE of a is ?location?, and it is the last
location mentioned before p in the document.
New
5 argument prominence: The frequency of the lemma of a in a two-
sentence windows of p, relative to all entities in that window.
S&F
6
local
discourse
predicate frequency in document: The frequency of p in the docu-
ment, relative to all predicates appear in the document.
G&C
7 statistical argument frequency: The Unigram-model likelihood of a
in English documents, calculated from a large corpus.
New
8 definite NP: Whether a is a definite NP G&C
9 indefinite NP: Whether a is an indefinite NP G&C
10 quantified predicate: Whether p is quantified (i.e., by expressions
like ?every . . . ?, ?a good deal of . . . ?, etc.)
G&C
11
local
candidate
properties
NE mismatch: Whether a is a named entity but the corresponding
argument in the hypothesis is not, or vice versa.
New
12 predicate-argument frequency: The likelihood of a to be an argu-
ment of p (formally: Pr(a|p)) in a large corpus.
similar feature
in G&C
13 sentence distance: The distance between p and a in sentences. G&C, S&F
14 mention distance: The distance between p and a in entity-mentions. S&F
15
predicate-
argument
relatedness
shared head-predicate: Whether p and a are themselves arguments
of another predicate.
G&C
Table 2: Algorithmic features. p and a denote the candidate predicate and argument respectively.
argument, like selectional preferences, lexical
similarities, etc. On the contrary, our statis-
tical features follow the intuition that explicit
predicate-argument relationships in the discourse
provide plausible indication that an implied
relationship holds as well. In our experiments
we collected the statistics from Reuters corpus
RCV1 (trec.nist.gov/data/reuters/
reuters.html), which contains more than
806,000 documents.
We defined two features: Co-occurring predi-
cate and Co-occurring argument. Let p and a be
the candidate predicate and the argument in the
text. While they are not connected syntactically,
each of them often has an explicit relationships
with other terms in the text, that might support the
sought (implied) relationship between a and p.
More concretely, a is often an explicit argument
of another predicate p
?
. For example, example 6 in
Table 1 includes the explicit relationship ?derail-
ment of train?, which might indicate the implied
relationship ?crash of train?. Hence p=?crash?,
a=?train? and p
?
=?derailment?. The Co-occurring
predicate feature estimates the probability that a
document would contain a as an argument of p,
given that a appears elsewhere in that document
as an argument of p
?
, based on explicit predicate-
argument relationships in a large corpus.
Similarly, the Co-occurring argument feature
captures cases where p has another explicit argu-
ment, a
?
. This is exemplified in example 5 of
Table 1, where p=?renew?, a=?Patriot Act? and
a
?
=?law?. Accordingly, the feature quantifies the
probability that a document including the relation-
ship p-a
?
would also include the relationship p-a.
More details about these features can be found
in the first author?s Ph.D. thesis at www.cs.biu.
ac.il/
?
nlp/publications/theses/
5 Results
We tested our method in a cross-validation setting,
and obtained high result as shown in the first row
of Table 3. Since our task and dataset are novel,
there is no direct baseline with which we can com-
pare this result. As a reference point we mention
the majority class proportion, and also report a
configuration in which only features adopted from
prior works (G&C and S&F) are utilized. This
742
Configuration Accuracy % ? %
Full algorithm 81.0 ?
Union of prior work 78.0 3.0
Major category (all true) 56.5 24.5
Ablation tests
no statistical discourse 79.9 1.1
no local discourse 79.3 1.7
no local candidate properties 79.2 1.8
no predicate-argument relatedness 79.7 1.3
Table 3: Accuracy of our method, followed by
baselines and ablation tests.
Configuration (input) Recall Precision F1 %
Explicit only 44.6 44.3 44.4
Human annotations 50.9 43.4 46.8
Algorithm recognition 48.5 42.3 45.2
Table 4: RTE-6 Experiment
comparison shows that the contribution of our new
features (3%) is meaningful, which is also statis-
tically significant with p < 0.01 using Bootstrap
Resampling test (Koehn, 2004). The high results
show that this task is feasible, and its solutions
can be adopted as a component in textual infer-
ence systems. The positive contribution of each
feature category is shown in ablation tests.
An additional experiment tests the contribution
of recognizing implied predicate-argument rela-
tionships for overall RTE, specifically on the RTE-
6 dataset. For the scope of this experiment we de-
veloped a simple RTE system, which uses the F1
optimized logistic regression classifier of Jansche
(2005) with two features: lexical coverage and
predicate-argument relationships coverage. We
ran three configurations for the second feature,
where in the first only syntactically expressed re-
lationships are used, in the second all the implied
relationships, as detected by a human annotator,
are added, and in the third only the implied rela-
tionships detected by our algorithm are added.
The results, presented in Table 4, first demon-
strate the full potential of the implied relation-
ship recognition task to improve textual entail-
ment recognition (Human annotation vs. Explicit
only). One third of this potential improvement is
achieved by our algorithm
1
. Note that all these re-
sults are higher than the median result in the RTE-
6 challenge (36.14%). While the delta in the F1
score is small in absolute terms, such magnitudes
1
Following the relatively modest size of the RTE dataset,
the Algorithm vs. Explicit result is not statistically significant
(p ' 0.1). However, the Human annotation vs. Explicit
result is statistically significant with p < 0.01.
are typical in RTE for most resources and tools
(see (Bentivogli et al, 2010)).
6 Discussion and Conclusions
We formulated the task of recognizing implied
predicate-argument relationships within textual in-
ference scenarios. We compared this task to the
labeling task of SemEval 2010, where no prior in-
formation about candidate arguments in the text is
available. We point out that in textual inference
scenarios the candidate predicate and argument
are given by the Hypothesis, while the challenge
is only to verify that a predicate-argument rela-
tionship between these candidates is implied from
the given Text. Accordingly, some complex steps
necessitated in the SemEval task can be avoided,
while additional relevant cases are covered.
Moreover, we have shown that this simpler task
is more feasibly solvable, where our 15 features
achieved more than 80% accuracy.
While our dataset and algorithm were presented
in the context of RTE, the same challenge and
methods are applicable to other textual inference
tasks as well. Consider, for example, the Ques-
tion Answering (QA) task. Typically QA sys-
tems detect a candidate predicate that matches the
question?s predicate. Similarly, candidate argu-
ments, which match either the expected answer
type or other arguments in the question are de-
tected too. Consequently, our methods which ex-
ploit the availability of the candidate predicate and
argument can be adapted to this scenario as well.
Similarly, a typical approach for Event Extrac-
tion (a sub task of Information Extraction) is to
start by applying an entity extractor, which identi-
fies argument candidates. Accordingly, candidate
predicate and arguments are detected in this sce-
nario too, while the remaining challenge is to as-
sess the likelihood that a predicate-argument rela-
tionship holds between them.
Following this observation, we propose future
work of applying our methods to other tasks. An
additional direction for future work is to further
develop new methods for our task, possibly by
incorporating SRL resources and/or linguistically
oriented rules, in order to improve the results we
achieved so far.
Acknowledgments
This work was partially supported by the EC-
funded project EXCITEMENT (FP7ICT-287923).
743
References
Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa Trang
Dang, and Danilo Giampiccolo. 2010. The sixth
pascal recognizing textual entailment challenge. In
Proccidings of TAC.
Leo Breiman. 2001. Random forests. Machine Learn-
ing, 45(1).
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The pascal recognising textual entailment
challenge. Machine Learning Challenges. Evaluat-
ing Predictive Uncertainty, Visual Object Classifi-
cation, and Recognising Tectual Entailment, pages
177?190.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The stanford typed dependencies rep-
resentation. In proceedings of COLING 2008 Work-
shop on Cross-framework and Cross-domain Parser
Evaluation.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In Proceedings of ACL.
Matthew Gerber and Joyce Y. Chai. 2012. Seman-
tic role labeling of implicit arguments for nominal
predicates. Computational Linguistics.
Yoav Goldberg and Michael Elhadad. 2010. An effi-
cient algorithm for easy-first non-directional depen-
dency parsing. In Proceedings of NAACL.
Aria Haghighi and Dan Klein. 2009. Simple coref-
erence resolution with rich syntactic and semantic
features. In Proceedings of EMNLP.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: An update.
SIGKDD Explorations, 11(1).
Martin Jansche. 2005. Maximum expected f-measure
training of logistic regression models. In Proceed-
ings of EMNLP.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP.
Egoitz Laparra and German Rigau. 2012. Exploiting
explicit annotations and semantic types for implicit
argument resolution. In Proceedings of IEEE-ICSC.
Catherine Macleod, Ralph Grishman, Adam Meyers,
Leslie Barrett, and Ruth Reeves. 1998. Nomlex: A
lexicon of nominalizations. In Proceedings of EU-
RALEX.
Michael Roth and Anette Frank. 2013. Automatically
identifying implicit arguments to improve argument
linking and coherence modeling. In Proceedings of
*SEM.
Josef Ruppenhofer, Caroline Sporleder, Roser
Morante, Collin Baker, and Martha Palmer. 2009.
Semeval-2010 task 10: Linking events and their
participants in discourse. In The NAACL-HLT
2009 Workshop on Semantic Evaluations: Recent
Achievements and Future Directions (SEW-09).
Josef Ruppenhofer, Caroline Sporleder, Roser
Morante, Collin Baker, and Martha Palmer. 2010.
Semeval-2010 task 10: Linking events and their
participants in discourse. In Proceedings of the 5th
International Workshop on Semantic Evaluation.
Carina Silberer and Anette Frank. 2012. Casting im-
plicit role linking as an anaphora resolution task. In
Proceedings of *SEM.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of NAACL.
744
Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 43?48,
Baltimore, Maryland USA, June 23-24, 2014.
c?2014 Association for Computational Linguistics
The Excitement Open Platform for Textual Inferences
Bernardo Magnini
?
, Roberto Zanoli
?
, Ido Dagan
?
, Kathrin Eichler
?
, G?unter Neumann
?
,
Tae-Gil Noh
?
, Sebastian Pado
?
, Asher Stern
?
, Omer Levy
?
?
FBK (magnini|zanoli@fbk.eu)
?
Heidelberg, Stuttgart Univ. (pado|noh@cl.uni-heidelberg.de)
?
DFKI (neumann|eichler@dfki.de)
?
Bar Ilan University (dagan|sterna3|omerlevy@cs.biu.ac.il)
Abstract
This paper presents the Excitement Open
Platform (EOP), a generic architecture and
a comprehensive implementation for tex-
tual inference in multiple languages. The
platform includes state-of-art algorithms,
a large number of knowledge resources,
and facilities for experimenting and test-
ing innovative approaches. The EOP is
distributed as an open source software.
1 Introduction
In the last decade textual entailment (Dagan et al.,
2009) has been a very active topic in Computa-
tional Linguistics, providing a unifying framework
for textual inference. Several evaluation exercises
have been organized around Recognizing Textual
Entailment (RTE) challenges and many method-
ologies, algorithms and knowledge resources have
been proposed to address the task. However, re-
search in textual entailment is still fragmented and
there is no unifying algorithmic framework nor
software architecture.
In this paper, we present the Excitement Open
Platform (EOP), a generic architecture and a com-
prehensive implementation for multilingual textual
inference which we make available to the scien-
tific and technological communities. To a large
extent, the idea is to follow the successful experi-
ence of the Moses open source platform (Koehn et
al., 2007) in Machine Translation, which has made
a substantial impact on research in that field. The
EOP is the result of a two-year coordinated work
under the international project EXCITEMENT.
1
A
consortium of four academic partners has defined
the EOP architectural specifications, implemented
the functional interfaces of the EOP components,
imported existing entailment engines into the EOP
1
http://www.excitement-project.eu
and finally designed and implemented a rich envi-
ronment to support open source distribution.
The goal of the platform is to provide function-
ality for the automatic identification of entailment
relations among texts. The EOP is based on a modu-
lar architecture with a particular focus on language-
independent algorithms. It allows developers and
users to combine linguistic pipelines, entailment al-
gorithms and linguistic resources within and across
languages with as little effort as possible. For ex-
ample, different entailment decision approaches
can share the same resources and the same sub-
components in the platform. A classification-based
algorithm can use the distance component of an
edit-distance based entailment decision approach,
and two different approaches can use the same set
of knowledge resources. Moreover, the platform
has various multilingual components for languages
like English, German and Italian. The result is an
ideal software environment for experimenting and
testing innovative approaches for textual inferences.
The EOP is distributed as an open source software
2
and its use is open both to users interested in using
inference in applications and to developers willing
to extend the current functionalities.
The paper is structured as follows. Section 2
presents the platform architecture, highlighting
how the EOP component-based approach favors
interoperability. Section 3 provides a picture of
the current population of the EOP in terms of both
entailment algorithms and knowledge resources.
Section 4 introduces expected use cases of the plat-
form. Finally, Section 5 presents the main features
of the open source package.
2 Architecture
The EOP platform takes as input two text portions,
the first called the Text (abbreviated with T), the
second called the Hypothesis (abbreviated with H).
2
http://hltfbk.github.io/
Excitement-Open-Platform/
43
Linguis'c)Analysis)Pipeline)(LAP))
Entailment)Core)(EC))
Entailment)Decision))Algorithm)(EDA))
Dynamic)and)Sta'c)Components)(Algorithms)and)Knowledge))
Linguis'c)Analysis)Components)
Decision)
1)
Raw)Data)
Figure 1: EOP architecture
The output is an entailment judgement, either ?En-
tailment? if T entails H, or ?NonEntailment? if the
relation does not hold. A confidence score for the
decision is also returned in both cases.
The EOP architecture (Pad?o et al., 2014) is based
on the concept of modularization with pluggable
and replaceable components to enable extension
and customization. The overall structure is shown
in Figure 1 and consists of two main parts. The
Linguistic Analysis Pipeline (LAP) is a series of
linguistic annotation components. The Entailment
Core (EC) performs the actual entailment recog-
nition. This separation ensures that (a) the com-
ponents in the EC only rely on linguistic analysis
in well-defined ways and (b) the LAP and EC can
be run independently of each other. Configuration
files are the principal means of configuring the EOP.
In the rest of this section we first provide an intro-
duction to the LAP, then we move to the EC and
finally describe the configuration files.
2.1 Linguistic Analysis Pipeline (LAP)
The Linguistic Analysis Pipeline is a collection of
annotation components for Natural Language Pro-
cessing (NLP) based on the Apache UIMA frame-
work.
3
Annotations range from tokenization to
part of speech tagging, chunking, Named Entity
Recognition and parsing. The adoption of UIMA
enables interoperability among components (e.g.,
substitution of one parser by another one) while
ensuring language independence. Input and output
of the components are represented in an extended
version of the DKPro type system based on UIMA
3
http://uima.apache.org/
Common Analysis Structure (CAS) (Gurevych et
al., 2007; Noh and Pad?o, 2013).
2.2 Entailment Core (EC)
The Entailment Core performs the actual entail-
ment recognition based on the preprocessed text
made by the Linguistic Analysis Pipeline. It con-
sists of one or more Entailment Decision Algo-
rithms (EDAs) and zero or more subordinate com-
ponents. An EDA takes an entailment decision
(i.e., ?entailment? or ?no entailment?) while com-
ponents provide static and dynamic information for
the EDA.
Entailment Decision Algorithms are at the top
level in the EC. They compute an entailment deci-
sion for a given Text/Hypothesis (T/H) pair, and
can use components that provide standardized al-
gorithms or knowledge resources. The EOP ships
with several EDAs (cf. Section 3).
Scoring Components accept a Text/Hypothesis
pair as an input, and return a vector of scores.
Their output can be used directly to build minimal
classifier-based EDAs forming complete RTE sys-
tems. An extended version of these components are
the Distance Components that can produce normal-
ized and unnormalized distance/similarity values
in addition to the score vector.
Annotation Components can be used to add dif-
ferent annotations to the Text/Hypothesis pairs. An
example of such a type of component is one that
produces word or phrase alignments between the
Text and the Hypothesis.
Lexical Knowledge Components describe se-
mantic relationships between words. In the
EOP, this knowledge is represented as directed
rules made up of two word?POS pairs, where
the LHS (left-hand side) entails the RHS (right-
hand side), e.g., (shooting star,Noun) =?
(meteorite,Noun). Lexical Knowledge Compo-
nents provide an interface that allows for (a) listing
all RHS for a given LHS; (b) listing all LHS for
a given RHS; and (c) checking for an entailment
relation for a given LHS?RHS pair. The interface
also wraps all major lexical knowledge sources cur-
rently used in RTE research, including manually
constructed ontologies like WordNet, and encyclo-
pedic resources like Wikipedia.
Syntactic Knowledge Components capture en-
tailment relationships between syntactic and
44
lexical-syntactic expressions. We represent such
relationships by entailment rules that link (option-
ally lexicalized) dependency tree fragments that
can contain variables as nodes. For example, the
rule fall of X =? X falls, or X sells Y to Z =?
Z buys Y from X express general paraphrasing pat-
terns at the predicate-argument level that cannot be
captured by purely lexical rules. Formally, each
syntactic rule consists of two dependency tree frag-
ments plus a mapping from the variables of the
LHS tree to the variables of the RHS tree.
4
2.3 Configuration Files
The EC components can be combined into actual
inference engines through configuration files which
contain information to build a complete inference
engine. A configuration file completely describes
an experiment. For example, it specifies the re-
sources that the selected EDA has to use and the
data set to be analysed. The LAP needed for data
set preprocessing is another parameter that can be
configured too. The platform ships with a set of
predefined configuration files accompanied by sup-
porting documentation.
3 Entailment Algorithms and Resources
This section provides a description of the Entail-
ment Algorithms and Knowledge Resources that
are distributed with the EOP.
3.1 Entailment Algorithms
The current version of the EOP platform ships with
three EDAs corresponding to three different ap-
proaches to RTE: an EDA based on transformations
between T and H, an EDA based on edit distance
algorithms, and a classification based EDA using
features extracted from T and H.
Transformation-based EDA applies a sequence
of transformations on T with the goal of making
it identical to H. If each transformation preserves
(fully or partially) the meaning of the original text,
then it can be concluded that the modified text
(which is actually the Hypothesis) can be inferred
from the original one. Consider the following sim-
ple example where the text is ?The boy was located
by the police? and the Hypothesis is ?The child
was found by the police?. Two transformations for
?boy? ? ?child? and ?located? ? ?found? do the
job.
4
Variables of the LHS may also map to null, when material
of the LHS must be present but is deleted in the inference step.
In the EOP we include a transformation based
inference system that adopts the knowledge based
transformations of Bar-Haim et al. (2007), while in-
corporating a probabilistic model to estimate trans-
formation confidences. In addition, it includes a
search algorithm which finds an optimal sequence
of transformations for any given T/H pair (Stern et
al., 2012).
Edit distance EDA involves using algorithms
casting textual entailment as the problem of map-
ping the whole content of T into the content of H.
Mappings are performed as sequences of editing
operations (i.e., insertion, deletion and substitu-
tion) on text portions needed to transform T into H,
where each edit operation has a cost associated with
it. The underlying intuition is that the probability
of an entailment relation between T and H is related
to the distance between them; see Kouylekov and
Magnini (2005) for a comprehensive experimental
study.
Classification based EDA uses a Maximum En-
tropy classifier to combine the outcomes of sev-
eral scoring functions and to learn a classification
model for recognizing entailment. The scoring
functions extract a number of features at various
linguistic levels (bag-of-words, syntactic dependen-
cies, semantic dependencies, named entities). The
approach was thoroughly described in Wang and
Neumann (2007).
3.2 Knowledge Resources
As described in Section 2.2, knowledge resources
are crucial to recognize cases where T and H use
different textual expressions (words, phrases) while
preserving entailment. The EOP platform includes
a wide range of knowledge resources, including lex-
ical and syntactic resources, where some of them
are grabbed from manual resources, like dictionar-
ies, while others are learned automatically. Many
EOP resources are inherited from pre-existing RTE
systems migrated into the EOP platform, but now
use the same interfaces, which makes them acces-
sible in a uniform fashion.
There are about two dozen lexical (e.g. word-
nets) and syntactic resources for three languages
(i.e. English, Italian and German). However,
since there is still a clear predominance of En-
glish resources, the platform includes lexical and
syntactic knowledge mining tools to bootstrap re-
sources from corpora, both for other languages and
45
EDA Accuracy / F1
Transformation-based English RTE-3 67.13%
Transformation-based English RTE-6 49.55%
Edit-Distance English RTE-3 64.38%
Edit-Distance German RTE-3 59.88%
Edit-Distance Italian RTE-3 63.50%
Classification-based English RTE-3 65.25%
Classification-based German RTE-3 63.75%
Median of RTE-3 (English) submissions 61.75%
Median of RTE-6 (English) submissions 33.72%
Table 1: EDAs results
for specific domains. Particularly, the EOP plat-
form includes a language independent tool to build
Wikipedia resources (Shnarch et al., 2009), as well
as a language-independent framework for building
distributional similarity resources like DIRT (Lin
and Pantel, 2002) and Lin similarity(Lin, 1998).
3.3 EOP Evaluation
Results for the three EDAs included in the EOP
platform are reported in Table 1. Each line rep-
resents an EDA, the language and the dataset
on which the EDA was evaluated. For brevity,
we omit here the knowledge resources used for
each EDA, even though knowledge configuration
clearly affects performance. The evaluations were
performed on RTE-3 dataset (Giampiccolo et al.,
2007), where the goal is to maximize accuracy. We
(manually) translated it to German and Italian for
evaluations: in both cases the results fix a refer-
ence for the two languages. The two new datasets
for German and English are available both as part
of the EOP distribution and independently
5
. The
transformation-based EDA was also evaluated on
RTE-6 dataset (Bentivogli et al., 2010), in which
the goal is to maximize the F1 measure.
The results of the included EDAs are higher than
median values of participated systems in RTE-3,
and they are competing with state-of-the-arts in
RTE-6 results. To the best of our knowledge, the
results of the EDAs as provided in the platform are
the highest among those available as open source
systems for the community.
4 Use Cases
We see four primary use cases for the EOP. Their
requirements were reflected in our design choices.
Use Case 1: Applied Textual Entailment. This
category covers users who are not interested in the
5
http://www.excitement-project.eu/
index.php/results
details of RTE but who are interested in an NLP
task in which textual entailment can take over part
of or all of the semantic processing, such as Ques-
tion Answering or Intelligent Tutoring. Such users
require a system that is as easy to deploy as possi-
ble, which motivates our offer of the EOP platform
as a library. They also require a system that pro-
vides good quality at a reasonable efficiency as
well as guidance as to the best choice of parame-
ters. The latter point is realized through our results
archive in the official EOP Wiki on the EOP site.
Use Case 2: Textual Entailment Development.
This category covers researchers who are interested
in Recognizing Textual Entailment itself, for exam-
ple with the goal of developing novel algorithms
for detecting entailment. In contrast to the first
category, this group need to look ?under the hood?
of the EOP platform and access the source code of
the EOP. For this reason, we have spent substantial
effort to provide the code in a well-structured and
well-documented form.
A subclass of this group is formed by researchers
who want to set up a RTE infrastructure for lan-
guages in which it does not yet exist (that is, al-
most all languages). The requirements of this class
of users comprises clearly specified procedures to
replace the Linguistic Analysis Pipeline, which are
covered in our documentation, and simple methods
to acquire knowledge resources for these languages
(assuming that the EDAs themselves are largely
language-independent). These are provided by the
language-independent knowledge acquisition tools
which we offer alongside the platform (cf. Section
3.2).
Use Case 3: Lexical Semantics Evaluation. A
third category consists of researchers whose pri-
mary interest is in (lexical) semantics.
As long as their scientific results can be phrased
in terms of semantic similarities or inference rules,
the EOP platform can be used as a simple and stan-
dardized workbench for these results that indicates
the impact that the semantic knowledge under con-
sideration has on deciding textual entailment. The
main requirement for this user group is the simple
integration of new knowledge resources into the
EOP platform. This is catered for through the defi-
nition of the generic knowledge component inter-
faces (cf. Section 2.2) and detailed documentation
on how to implement these interfaces.
46
Use Case 4: Educational Use. The fourth and
final use case is as an educational tool to support
academic courses and projects on Recognizing Tex-
tual Entailment and inference more generally. This
use case calls, in common with the others, for easy
usability and flexibility. Specifically for this use
case, we have also developed a series of tutorials
aimed at acquainting new users with the EOP plat-
form through a series of increasingly complexity
exercises that cover all areas of the EOP. We are
also posting proposals for projects to extend the
EOP on the EOP Wiki.
5 EOP Distribution
The EOP infrastructure follows state-of-the-art soft-
ware engineering standards to support both users
and developers with a flexible, scalable and easy to
use software environment. In addition to communi-
cation channels, like the mailing list and the issue
tracking system, the EOP infrastructure comprises
the following set of facilities.
Version Control System: We use GitHub,
6
a
web-based hosting service for code and documen-
tation storage, development, and issue tracking.
Web Site: The GitHub Automatic Page Genera-
tor was used to build the EOP web site and Wiki,
containing a general introduction to the software
platform, the terms of its license, mailing lists to
contact the EOP members and links to the code
releases.
Documentation: Both user and developer docu-
mentation is available from Wiki pages; the pages
are written with the GitHub Wiki Editor and hosted
on the GitHub repository. The documentation in-
cludes a Quick Start guide to start using the EOP
platform right away, and a detailed step by step
tutorial.
Results Archive: As a new feature for commu-
nity building, EOP users can, and are encouraged
to, share their results: the platform configuration
files used to produce results as well as contact infor-
mation can be saved and archived into a dedicated
page on the EOP GitHub repository. That allows
other EOP users to replicate experiments under
the same condition and/or avoid doing experiments
that have already been done.
6
https://github.com/
Build Automation Tool: The EOP has been de-
veloped as a Maven
7
multi-modules project, with
all modules sharing the same Maven standard struc-
ture, making it easier to find files in the project once
one is used to Maven.
Maven Artifacts Repository: Using a Maven
repository has a twofold goal: (i) to serve as an
internal private repository of all software libraries
used within the project (libraries are binary files
and should not be stored under version control sys-
tems, which are intended to be used with text files);
(ii) to make the produced EOP Maven artifacts
available (i.e., for users who want to use the EOP
as a library in their own code). We use Artifactory
8
repository manager to store produced artifacts.
Continuous Integration: The EOP uses Jenk-
ins
9
for Continuous Integration, a software develop-
ment practice where developers of a team integrate
their work frequently (e.g., daily).
Code Quality Tool: Ensuring the quality of the
produced software is one of the most important
aspects of software engineering. The EOP uses
tools like PMD
10
that can automatically be run
during development to help the developers check
the quality of their software.
5.1 Project Repository
The EOP Java source code is hosted on the EOP
Github repository and managed using Git. The
repository consists of three main branches: the
release branch contains the code that is supposed to
be in a production-ready state, whereas the master
branch contains the code to be incorporated into the
next release. When the source code in the master
branch reaches a stable point and is ready to be
released, all of the changes are merged back into
release. Finally, the gh-pages branch contains the
web site pages.
5.2 Licensing
The software of the platform is released under the
terms of General Public License (GPL) version
3.
11
The platform contains both components and
resources designed by the EOP developers, as well
as others that are well known and freely available
7
http://maven.apache.org/
8
http://www.jfrog.com/
9
http://jenkins-ci.org/
10
http://pmd.sourceforge.net
11
http://www.gnu.org/licenses/gpl.html
47
in the NLP research community. Additional com-
ponents and resources whose license is not compat-
ible with the EOP license have to be downloaded
and installed separately by the user.
6 Conclusion
This paper has presented the main characteristics
of Excitement Open Platform platform, a rich envi-
ronment for experimenting and evaluating textual
entailment systems. On the software side, the EOP
is a complex endeavor to integrate tools and re-
sources in Computational Linguistics, including
pipelines for three languages, three pre-existing
entailment engines, and about two dozens of lex-
ical and syntactic resources. The EOP assumes a
clear and modular separation between linguistic
annotations, entailment algorithms and knowledge
resources which are used by the algorithms. A
relevant benefit of the architectural design is that
a high level of interoperability is reached, provid-
ing a stimulating environment for new research in
textual inferences.
The EOP platform has been already tested in sev-
eral pilot research projects and educational courses,
and it is currently distributed as open source soft-
ware under the GPL-3 license. To the best of our
knowledge, the entailment systems and their con-
figurations provided in the platform are the best
systems available as open source for the commu-
nity. As for the future, we are planning several
initiatives for the promotion of the platform in the
research community, as well as its active experi-
mentation in real application scenarios.
Acknowledgments
This work was partially supported by the EC-
funded project EXCITEMENT (FP7ICT-287923).
References
Roy Bar-Haim, Ido Dagan, Iddo Greental, and Eyal
Shnarch. 2007. Semantic inference at the lexical-
syntactic level. In Proceedings of AAAI, pages 871?
876, Vancouver, BC.
Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa Trang
Dang, and Danilo Giampiccolo. 2010. The Sixth
PASCAL Recognizing Textual Entailment Chal-
lenge. In Proceedings of TAC, Gaithersburg, MD.
Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan
Roth. 2009. Recognizing textual entailment: Ratio-
nal, evaluation and approaches. Journal of Natural
Language Engineering, 15(4):i?xvii.
Danilo Giampiccolo, Bernardo Magnini, Ido Dagan,
and Bill Dolan. 2007. The Third PASCAL Recog-
nising Textual Entailment Challenge. In Proceed-
ings of the ACL-PASCAL Workshop on Textual En-
tailment and Paraphrasing, Prague, Czech Repub-
lic.
Iryna Gurevych, Max M?uhlh?auser, Christof M?uller,
J?urgen Steimle, Markus Weimer, and Torsten Zesch.
2007. Darmstadt knowledge processing repository
based on UIMA. In Proceedings of the First Work-
shop on Unstructured Information Management Ar-
chitecture (UIMA@GSCL 2007), T?ubingen, Ger-
many.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In Proceedings of the ACL demo session, pages 177?
180, Prague, Czech Republic.
Milen Kouylekov and Bernardo Magnini. 2005. Rec-
ognizing textual entailment with tree edit distance al-
gorithms. In Proceedings of the First PASCAL Chal-
lenges Workshop on Recognising Textual Entailment,
pages 17?20, Southampton, UK.
Dekang Lin and Patrick Pantel. 2002. Discovery of
Inference Rules for Question Answering. Journal of
Natural Language Engineering, 7(4):343?360.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of ACL/COLING,
pages 768?774, Montr?eal, Canada.
Tae-Gil Noh and Sebastian Pad?o. 2013. Using
UIMA to structure an open platform for textual en-
tailment. In Proceedings of the 3rd Workshop on
Unstructured Information Management Architecture
(UIMA@GSCL 2013).
Sebastian Pad?o, Tae-Gil Noh, Asher Stern, Rui Wang,
and Roberto Zanoli. 2014. Design and realiza-
tion of a modular architecture for textual entailment.
Journal of Natural Language Engineering. doi:
10.1017/S1351324913000351.
Eyal Shnarch, Libby Barak, and Ido Dagan. 2009. Ex-
tracting lexical reference rules from Wikipedia. In
Proceedings of ACL-IJCNLP, pages 450?458, Sin-
gapore.
Asher Stern, Roni Stern, Ido Dagan, and Ariel Felner.
2012. Efficient search for transformation-based in-
ference. In Proceedings of ACL, pages 283?291,
Jeju Island, South Korea.
Rui Wang and G?unter Neumann. 2007. Recogniz-
ing textual entailment using a subsequence kernel
method. In Proceedings of AAAI, pages 937?945,
Vancouver, BC.
48
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 38?43,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Sentence Clustering via Projection over Term Clusters
Lili Kotlerman, Ido Dagan
Bar-Ilan University
Israel
Lili.Kotlerman@biu.ac.il
dagan@cs.biu.ac.il
Maya Gorodetsky, Ezra Daya
NICE Systems Ltd.
Israel
Maya.Gorodetsky@nice.com
Ezra.Daya@nice.com
Abstract
This paper presents a novel sentence cluster-
ing scheme based on projecting sentences over
term clusters. The scheme incorporates exter-
nal knowledge to overcome lexical variability
and small corpus size, and outperforms com-
mon sentence clustering methods on two real-
life industrial datasets.
1 Introduction
Clustering is a popular technique for unsupervised
text analysis, often used in industrial settings to ex-
plore the content of large amounts of sentences. Yet,
as may be seen from the results of our research,
widespread clustering techniques, which cluster sen-
tences directly, result in rather moderate perfor-
mance when applied to short sentences, which are
common in informal media.
In this paper we present and evaluate a novel
sentence clustering scheme based on projecting
sentences over term clusters. Section 2 briefly
overviews common sentence clustering approaches.
Our suggested clustering scheme is presented in
Section 3. Section 4 describes an implementation of
the scheme for a particular industrial task, followed
by evaluation results in Section 5. Section 6 lists
directions for future research.
2 Background
Sentence clustering aims at grouping sentences with
similar meanings into clusters. Commonly, vector
similarity measures, such as cosine, are used to de-
fine the level of similarity over bag-of-words encod-
ing of the sentences. Then, standard clustering algo-
rithms can be applied to group sentences into clus-
ters (see Steinbach et al (2000) for an overview).
The most common practice is representing the
sentences as vectors in term space and applying the
K-means clustering algorithm (Shen et al (2011);
Pasquier (2010); Wang et al (2009); Nomoto and
Matsumoto (2001); Boros et al (2001)). An alterna-
tive approach involves partitioning a sentence con-
nectivity graph by means of a graph clustering algo-
rithm (Erkan and Radev (2004); Zha (2002)).
The main challenge for any sentence clustering
approach is language variability, where the same
meaning can be phrased in various ways. The
shorter the sentences are, the less effective becomes
exact matching of their terms. Compare the fol-
lowing newspaper sentence ?The bank is phasing out
the EZ Checking package, with no monthly fee charged
for balances over $1,500, and is instead offering cus-
tomers its Basic Banking account, which carries a fee?
with two tweets regarding the same event: ?Whats
wrong.. charging $$ for checking a/c? and ?Now they
want a monthly fee!?. Though each of the tweets can
be found similar to the long sentence by exact term
matching, they do not share any single term. Yet,
knowing that the words fee and charge are semanti-
cally related would allow discovering the similarity
between the two tweets.
External resources can be utilized to provide such
kind of knowledge, by which sentence representa-
tion can be enriched. Traditionally, WordNet (Fell-
baum, 1998) has been used for this purpose (She-
hata (2009); Chen et al (2003); Hotho et al (2003);
Hatzivassiloglou et al (2001)). Yet, other resources
38
of semantically-related terms can be beneficial, such
as WordNet::Similarity (Pedersen et al, 2004), sta-
tistical resources like that of Lin (1998) or DIRECT
(Kotlerman et al, 2010), thesauri, Wikipedia (Hu et
al., 2009), ontologies (Suchanek et al, 2007) etc.
3 Sentence Clustering via Term Clusters
This section presents a generic sentence clustering
scheme, which involves two consecutive steps: (1)
generating relevant term clusters based on lexical se-
mantic relatedness and (2) projecting the sentence
set over these term clusters. Below we describe each
of the two steps.
3.1 Step 1: Obtaining Term Clusters
In order to obtain term clusters, a term connectivity
graph is constructed for the given sentence set and is
clustered as follows:
1. Create initially an undirected graph with
sentence-set terms as nodes and use lexical re-
sources to extract semantically-related terms
for each node.
2. Augment the graph nodes with the extracted
terms and connect semantically-related nodes
with edges. Then, partition the graph into term
clusters through a graph clustering algorithm.
Extracting and filtering related terms. In Sec-
tion 2 we listed a number of lexical resources pro-
viding pairs of semantically-related terms. Within
the suggested scheme, any combination of resources
may be utilized.
Often resources contain terms, which are
semantically-related only in certain contexts. E.g.,
the words visa and passport are semantically-related
when talking about tourism, but cannot be consid-
ered related in the banking domain, where visa usu-
ally occurs in its credit card sense. In order to dis-
card irrelevant terms, filtering procedures can be em-
ployed. E.g., a simple filtering applicable in most
cases of sentence clustering in a specific domain
would discard candidate related terms, which do not
occur sufficiently frequently in a target-domain cor-
pus. In the example above, this procedure would
allow avoiding the insertion of passport as related to
visa, when considering the banking domain.
Clustering the graph nodes. Once the term
graph is constructed, a graph clustering algorithm
is applied resulting in a partition of the graph nodes
(terms) into clusters. The choice of a particular al-
gorithm is a parameter of the scheme. Many clus-
tering algorithms consider the graph?s edge weights.
To address this trait, different edge weights can be
assigned, reflecting the level of confidence that the
two terms are indeed validly related and the reliabil-
ity of the resource, which suggested the correspond-
ing edge (e.g. WordNet synonyms are commonly
considered more reliable than statistical thesauri).
3.2 Step 2: Projecting Sentences to Term
Clusters
To obtain sentence clusters, the given sentence set
has to be projected in some manner over the term
clusters obtained in Step 1. Our projection pro-
cedure resembles unsupervised text categorization
(Gliozzo et al, 2005), with categories represented
by term clusters that are not predefined but rather
emerge from the analyzed data:
1. Represent term clusters and sentences as vec-
tors in term space and calculate the similarity
of each sentence with each of the term clusters.
2. Assign each sentence to the best-scoring term
cluster. (We focus on hard clustering, but the
procedure can be adapted for soft clustering).
Various metrics for feature weighting and vector
comparison may be chosen. The top terms of term-
cluster vectors can be regarded as labels for the cor-
responding sentence clusters.
Thus each sentence cluster corresponds to a sin-
gle coherent cluster of related terms. This is con-
trasted with common clustering methods, where if
sentence A shares a term with B, and B shares an-
other term with C, then A and C might appear in the
same cluster even if they have no related terms in
common. This behavior turns out harmful for short
sentences, where each incidental term is influential.
Our scheme ensures that each cluster contains only
sentences related to the underlying term cluster, re-
sulting in more coherent clusters.
4 Application: Clustering Customer
Interactions
In industry there?s a prominent need to obtain busi-
ness insights from customer interactions in a contact
center or social media. Though the number of key
39
sentences to analyze is often relatively small, such
as a couple hundred, manually analyzing just a hand-
ful of clusters is much preferable. This section de-
scribes our implementation of the scheme described
in Section 3 for the task of clustering customer in-
teractions, as well as the data used for evaluation.
Results and analysis are presented in Section 5.
4.1 Data
We apply our clustering approach over two real-life
datasets. The first one consists of 155 sentences
containing reasons of account cancelation, retrieved
from automatic transcripts of contact center interac-
tions of an Internet Service Provider (ISP). The sec-
ond one contains 194 sentences crawled from Twit-
ter, expressing reasons for customer dissatisfaction
with a certain banking company. The sentences in
both datasets were gathered automatically by a rule-
based extraction algorithm. Each dataset is accom-
panied by a small corpus of call transcripts or tweets
from the corresponding domain.1
The goal of clustering these sentences is to iden-
tify the prominent reasons of cancelation and dissat-
isfaction. To obtain the gold-standard (GS) anno-
tation, sentences were manually grouped to clusters
according to the reasons stated in them.
Table 1 presents examples of sentences from the
ISP dataset. The sentences are short, with only one
or two words expressing the actual reason stated in
them. We see that exact term matching is not suffi-
cient to group the related sentences. Moreover, tra-
ditional clustering algorithms are likely to mix re-
lated and unrelated sentences, due to matching non-
essential terms (e.g. husband or summer). We note
that such short and noisy sentences are common
in informal media, which became a most important
channel of information in industry.
4.2 Implementation of the Clustering Scheme
Our proposed sentence clustering scheme presented
in Section 3 includes a number of choices. Below
we describe the choices we made in our current im-
plementation.
Input sentences were tokenized, lemmatized and
cleaned from stopwords in order to extract content-
word terms. Candidate semantically-related terms
1The bank dataset with the output of the tested methods will
be made publicly available.
he hasn?t been using it all summer long
it?s been sitting idle for about it almost a year
I?m getting married my husband has a computer
yeah I bought a new laptop this summer so
when I said faces my husband got laid off from work
well I?m them going through financial difficulties
Table 1: Example sentences expressing 3 reasons for can-
celation: the customer (1) does not use the service, (2)
acquired a computer, (3) cannot afford the service.
were extracted for each of the terms, using Word-
Net synonyms and derivations, as well as DIRECT2,
a directional statistical resource learnt from a news
corpus. Candidate terms that did not appear in the
accompanying domain corpus were filtered out as
described in Section 3.1.
Edges in the term graph were weighted with the
number of resources supporting the corresponding
edge. To cluster the graph we used the Chinese
Whispers clustering tool3 (Biemann, 2006), whose
algorithm does not require to pre-set the desired
number of clusters and is reported to outperform
other algorithms for several NLP tasks.
To generate the projection, sentences were rep-
resented as vectors of terms weighted by their fre-
quency in each sentence. Terms of the term-cluster
vectors were weighted by the number of sentences
in which they occur. Similarity scores were calcu-
lated using the cosine measure. Clusters were la-
beled with the top terms appearing both in the un-
derlying term cluster and in the cluster?s sentences.
5 Results and Analysis
In this section we present the results of evaluating
our projection approach, compared to the common
K-means clustering method4 applied to:
(A) Standard bag-of-words representation of sen-
tences;
2Available for download at www.cs.biu.ac.il/
?nlp/downloads/DIRECT.html. For each term we
extract from the resource the top-5 related terms.
3Available at http://wortschatz.informatik.
uni-leipzig.de/?cbiemann/software/CW.html
4We use the Weka (Hall et al, 2009) implementation. Due
to space limitations and for more meaningful comparison we re-
port here one value of K, which is equal to the number of clus-
ters returned by projection (60 for the ISP and 65 for the bank
dataset). For K = 20, 40 and 70 the performance was similar.
40
(B) Bag-of-words representation, where sentence?s
words are augmented with semantically-related
terms (following the common scheme of prior
work, see Section 2). We use the same set of
related terms as is used by our method.
(C) Representation of sentences in term-cluster
space, using the term clusters generated by our
method as vector features. A feature is acti-
vated in a sentence vector if it contains a term
from the corresponding term cluster.
Table 2 shows the results in terms of Purity, Recall
(R), Precision (P) and F1 (see ?Evaluation of clus-
tering?, Manning et al (2008)). Projection signifi-
cantly5 outperforms all baselines for both datasets.
Dataset Algorithm Purity R P F1
ISP
Projection .74 .40 .68 .50
K-means A .65 .18 .22 .20
K-means B .65 .13 .24 .17
K-means C .65 .18 .26 .22
Bank
Projection .79 .26 .53 .35
K-means A .61 .14 .14 .14
K-means B .64 .13 .19 .16
K-means C .67 .17 .21 .19
Table 2: Evaluation results.
For completeness we experimented with applying
Chinese Whispers clustering to sentence connectiv-
ity graphs, but the results were inferior to K-means.
Table 3 presents sample sentences from clusters
produced by projection and K-means for illustration.
Our initial analysis showed that our approach indeed
produces more homogenous clusters than the base-
line methods, as conjectured in Section 3.2. We con-
sider it advantageous, since it?s easier for a human to
merge clusters than to reveal sub-clusters. E.g., a GS
cluster of 20 sentences referring to fees and charges
is covered by three projection clusters labeled fee,
charge and interest rate, with 9, 8 and 2 sentences
correspondingly. On the other hand, K-means C
method places 11 out of the 20 sentences in a messy
cluster of 57 sentences (see Table 3), scattering the
remaining 9 sentences over 7 other clusters.
In our current implementation fee, charge and in-
terest rate were not detected by the lexical resources
we used as semantically similar and thus were not
5p=0.001 according to McNemar test (Dietterich, 1998).
grouped in one term cluster. However, adding more
resources may introduce additional noise. Such de-
pendency on coverage and accuracy of resources is
apparently a limitation of our approach. Yet, as
our experiments indicate, using only two generic re-
sources already yielded valuable results.
a. Projection
credit card, card, mastercard, visa (38 sentences)
XXX has the worst credit cards ever
XXX MasterCard is the worst credit card I?ve ever had
ntuc do not accept XXX visa now I have to redraw $150...
XXX card declined again , $40 dinner in SF...
b. K-means C
fee, charge (57 sentences)
XXX playing games wit my interest
arguing w incompetent pol at XXX damansara perdana
XXX?s upper management are a bunch of rude pricks
XXX are ninjas at catching fraudulent charges.
Table 3: Excerpt from resulting clusterings for the bank
dataset. Bank name is substituted with XXX. Cluster la-
bels are given in italics. Two most frequent terms are
assigned as cluster labels for K-means C.
6 Conclusions and Future Work
We presented a novel sentence clustering scheme
and evaluated its implementation, showing signifi-
cantly superior performance over common sentence
clustering techniques. We plan to further explore
the suggested scheme by utilizing additional lexical
resources and clustering algorithms. We also plan
to compare our approach with co-clustering meth-
ods used in document clustering (Xu et al (2003),
Dhillon (2001), Slonim and Tishby (2000)).
Acknowledgments
This work was partially supported by the MAGNE-
TON grant no. 43834 of the Israel Ministry of Indus-
try, Trade and Labor, the Israel Ministry of Science
and Technology, the Israel Science Foundation grant
1112/08, the PASCAL-2 Network of Excellence of
the European Community FP7-ICT-2007-1-216886
and the European Community?s Seventh Framework
Programme (FP7/2007-2013) under grant agree-
ment no. 287923 (EXCITEMENT).
41
References
Chris Biemann. 2006. Chinese whispers - an efficient
graph clustering algorithm and its application to nat-
ural language processing problems. In Proceedings
of TextGraphs: the Second Workshop on Graph Based
Methods for Natural Language Processing, pages 73?
80, New York City, USA.
Endre Boros, Paul B. Kantor, and David J. Neu. 2001. A
clustering based approach to creating multi-document
summaries.
Hsin-Hsi Chen, June-Jei Kuo, and Tsei-Chun Su.
2003. Clustering and visualization in a multi-lingual
multi-document summarization system. In Proceed-
ings of the 25th European conference on IR re-
search, ECIR?03, pages 266?280, Berlin, Heidelberg.
Springer-Verlag.
Inderjit S. Dhillon. 2001. Co-clustering documents and
words using bipartite spectral graph partitioning. In
Proceedings of the seventh ACM SIGKDD interna-
tional conference on Knowledge discovery and data
mining, KDD ?01, pages 269?274, New York, NY,
USA. ACM.
Thomas G. Dietterich. 1998. Approximate statistical
tests for comparing supervised classification learning
algorithms.
Gu?nes Erkan and Dragomir R. Radev. 2004. Lexrank:
graph-based lexical centrality as salience in text sum-
marization. J. Artif. Int. Res., 22(1):457?479, Decem-
ber.
C. Fellbaum. 1998. WordNet ? An Electronic Lexical
Database. MIT Press.
Alfio Massimiliano Gliozzo, Carlo Strapparava, and Ido
Dagan. 2005. Investigating unsupervised learning for
text categorization bootstrapping. In HLT/EMNLP.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: an update.
SIGKDD Explor. Newsl., 11(1):10?18.
Vasileios Hatzivassiloglou, Judith L. Klavans, Melissa L.
Holcombe, Regina Barzilay, Min yen Kan, and Kath-
leen R. McKeown. 2001. Simfinder: A flexible clus-
tering tool for summarization. In In Proceedings of the
NAACL Workshop on Automatic Summarization, pages
41?49.
A. Hotho, S. Staab, and G. Stumme. 2003. Word-
net improves text document clustering. In Ying
Ding, Keith van Rijsbergen, Iadh Ounis, and Joe-
mon Jose, editors, Proceedings of the Semantic Web
Workshop of the 26th Annual International ACM SI-
GIR Conference on Research and Development in In-
formaion Retrieval (SIGIR 2003), August 1, 2003,
Toronto Canada. Published Online at http://de.
scientificcommons.org/608322.
Xiaohua Hu, Xiaodan Zhang, Caimei Lu, E. K. Park, and
Xiaohua Zhou. 2009. Exploiting wikipedia as exter-
nal knowledge for document clustering. In Proceed-
ings of the 15th ACM SIGKDD international confer-
ence on Knowledge discovery and data mining, KDD
?09, pages 389?396, New York, NY, USA. ACM.
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-Geffet. 2010. Directional distributional
similarity for lexical inference. JNLE, 16:359?389.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 17th interna-
tional conference on Computational linguistics - Vol-
ume 2, COLING ?98, pages 768?774, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Schu?tze. 2008. Introduction to Information Re-
trieval. Cambridge University Press, Cambridge, Juli.
Tadashi Nomoto and Yuji Matsumoto. 2001. A new ap-
proach to unsupervised text summarization. In Pro-
ceedings of the 24th annual international ACM SIGIR
conference on Research and development in informa-
tion retrieval, SIGIR ?01, pages 26?34, New York, NY,
USA. ACM.
Claude Pasquier. 2010. Task 5: Single document
keyphrase extraction using sentence clustering and la-
tent dirichlet alocation. In Proceedings of the 5th
International Workshop on Semantic Evaluation, Se-
mEval ?10, pages 154?157, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. Wordnet::similarity: measuring the
relatedness of concepts. In Demonstration Papers
at HLT-NAACL 2004, HLT-NAACL?Demonstrations
?04, pages 38?41, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Shady Shehata. 2009. A wordnet-based semantic model
for enhancing text clustering. Data Mining Work-
shops, International Conference on, 0:477?482.
Chao Shen, Tao Li, and Chris H. Q. Ding. 2011. Integrat-
ing clustering and multi-document summarization by
bi-mixture probabilistic latent semantic analysis (plsa)
with sentence bases. In AAAI.
Noam Slonim and Naftali Tishby. 2000. Document clus-
tering using word clusters via the information bottle-
neck method. In Proceedings of the 23rd annual inter-
national ACM SIGIR conference on Research and de-
velopment in information retrieval, SIGIR ?00, pages
208?215, New York, NY, USA. ACM.
M. Steinbach, G. Karypis, and V. Kumar. 2000. A
comparison of document clustering techniques. KDD
Workshop on Text Mining.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: A large ontology from
wikipedia and wordnet.
42
Dingding Wang, Shenghuo Zhu, Tao Li, and Yihong
Gong. 2009. Multi-document summarization us-
ing sentence-based topic models. In Proceedings
of the ACL-IJCNLP 2009 Conference Short Papers,
ACLShort ?09, pages 297?300, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Wei Xu, Xin Liu, and Yihong Gong. 2003. Document
clustering based on non-negative matrix factorization.
In Proceedings of the 26th annual international ACM
SIGIR conference on Research and development in in-
formaion retrieval, SIGIR ?03, pages 267?273, New
York, NY, USA. ACM.
Hongyuan Zha. 2002. Generic summarization and
keyphrase extraction using mutual reinforcement prin-
ciple and sentence clustering. In SIGIR, pages 113?
120.
43
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 59?64,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Statistical Thesaurus Construction for a Morphologically Rich Language 
 
Chaya Liebeskind, Ido Dagan and Jonathan Schler                    
Computer Science Department 
Bar-Ilan University 
Ramat-Gan, Israel 
liebchaya@gmail.com, dagan@cs.biu.ac.il, schler@gmail.com 
 
 
 
 
 
Abstract 
Corpus-based thesaurus construction for Mor-
phologically Rich Languages (MRL) is a com-
plex task, due to the morphological variability 
of MRL. In this paper we explore alternative 
term representations, complemented by cluster-
ing of morphological variants. We introduce a 
generic algorithmic scheme for thesaurus con-
struction in MRL, and demonstrate the empiri-
cal benefit of our methodology for a Hebrew 
thesaurus. 
1 Introduction 
Corpus-based thesaurus construction has been an 
active research area (Grefenstette, 1994; Curran 
and Moens, 2002; Kilgarriff, 2003; Rychly and 
Kilgarriff, 2007). Typically, two statistical ap-
proaches for identifying semantic relationships 
between words were investigated: first-order, co-
occurrence-based methods which assume that 
words that occur frequently together are topically 
related (Schutze and Pederson, 1997) and second-
order, distributional similarity methods (Hindle, 
1990; Lin, 1998; Gasperin et al 2001; Weeds and 
Weir, 2003; Kotlerman et al, 2010), which suggest 
that words occurring within similar contexts are 
semantically similar (Harris, 1968).  
While most prior work focused on English, we 
are interested in applying these methods to MRL. 
Such languages, Hebrew in our case, are character-
ized by highly productive morphology which may 
produce as many as thousands of word forms for a 
given root form.    
Thesauri usually provide related terms for each 
entry term (denoted target term). Since both target 
and related terms correspond to word lemmas, sta-
tistics collection from the corpus would be most 
directly applied at the lemma level as well, using a 
morphological analyzer and tagger (Linden and 
Piitulainen, 2004; Peirsman et al, 2008; Rapp, 
2009). However, due to the rich and challenging 
morphology of MRL, such tools often have limited 
performance. In our research, the accuracy of a 
state-of-the-art modern Hebrew tagger on a cross 
genre corpus was only about 60%. 
Considering such limited performance of mor-
phological processing, we propose a schematic 
methodology for generating a co-occurrence based 
thesaurus in MRL. In particular, we propose and 
investigate three options for term representation, 
namely surface form, lemma and multiple lemmas, 
supplemented with clustering of term variants. 
While the default lemma representation is depend-
ent on tagger performance, the two other represen-
tations avoid choosing the right lemma for each 
word occurrence. Instead, the multiple-lemma rep-
resentation assumes that the right analysis will ac-
cumulate enough statistical prominence throughout 
the corpus, while the surface representation solves 
morphological disambiguation "in retrospect", by 
clustering term variants at the end of the extraction 
process. As the methodology provides a generic 
scheme for exploring the alternative representation 
levels, each corpus and language-specific tool set 
might yield a different optimal configuration. 
2 Methodology  
Thesauri usually contain thousands of entries, 
termed here target terms. Each entry holds a list of 
related terms, covering various semantic relations. 
In this paper we assume that the list of target terms 
59
is given as input, and focus on the process of ex-
tracting a ranked list of candidate related terms 
(termed candidate terms) for each target term. The 
top ranked candidates may be further examined 
(manually) by a lexicographer, who will select the 
eventual related terms for the thesaurus entry. 
Our methodology was applied for statistical 
measures of first order similarity (word co-
occurrence). These statistics consider the number 
of times each candidate term co-occurs with the 
target term in the same document, relative to their 
total frequencies in the corpus. Common co-
occurrence metrics are Dice coefficient (Smadja et 
al, 1996), Pointwise Mutual Information (PMI) 
(Church and Hanks, 1990) and log-likelihood test 
(Dunning, 1993). 
2.1 Term Representation 
Statistical extraction is affected by term 
representation in the corpus. Usually, related terms 
in a thesaurus are lemmas, which can be identified 
by morphological disambiguation tools. However, 
we present two other approaches for term 
representation (either a target term or a candidate 
related term), which are less dependent on 
morphological processing.  
Typically, a morphological analyzer produces 
all possible analyses for a given token in the cor-
pus. Then, a Part Of Speech (POS) tagger selects 
the most probable analysis and solves morphology 
disambiguation. However, considering the poor 
performance of the POS tagger on our corpus, we 
distinguish between these two analysis levels. 
Consequently, we examined three levels of term 
representation: (i) Surface form (surface) (ii) Best 
lemma, as indentified by a POS tagger (best), and 
(iii) All possible lemmas, produced by a morpho-
logical analyzer (all). 
2.2 Algorithmic Scheme 
We used the following algorithmic scheme for the-
saurus construction. Our input is a target term in 
one of the possible term representations (surface, 
best or all). For each target term we retrieve all the 
documents in the corpus where the target term ap-
pears. Then, we define a set of candidate terms that 
consists of all the terms that appear in all these 
documents (this again for each of the three possible 
term representations). Next, a co-occurrence score 
between the target term and each of the candidates 
is calculated. Then, candidates are sorted, and the 
highest rated candidate terms are clustered into 
lemma-oriented clusters. Finally, we rank the clus-
ters according to their members' co-occurrence 
scores and the highest rated clusters become relat-
ed terms in the thesaurus. 
Figure 1 presents the algorithm?s pseudo code. 
The notion rep(term) is used to describe the possi-
ble term representations and may be either surface, 
best or all. In our experiments, when 
rep(target_term)=best, the correct lemma was 
manually assigned (assuming a lexicographer in-
volvement with each thesaurus entry in our set-
ting). While, when rep(word)=best, the most prob-
able lemma is assigned by the tagger (since there 
are numerous candidates for each target term we 
cannot resort the manual involvement for each of 
them).  The two choices for rep(term) are inde-
pendent, resulting in nine possible configurations 
of the algorithm for representing both the target 
term and the candidate terms. Thus, these 9 con-
figurations cover the space of possibilities for term 
representation. Exploring all of them in a systemat-
ic manner would reveal the best configuration in a 
particular setting.  
Figure 1: Methodology implementation algorithm 
2.3 Clustering 
The algorithm of Figure 1 suggests clustering the 
extracted candidates before considering them for 
the thesaurus. Clustering aims at grouping together 
related terms with the same lemma into clusters, 
using some measure of morphological equivalence. 
Accordingly, an equivalence measure between re-
lated terms needs to be defined, and a clustering 
Input: target term, corpus, a pair of values for 
rep(target_term) and rep(word) 
Output: clusters of related terms 
 
target_term   rep(target_term) 
docs_list   search(target_term) 
FOR doc IN docs_list 
    FOR word IN doc 
        add rep(word) to candidates 
    ENDFOR 
ENDFOR 
compute co-occurrence scores for all candidates 
sort(candidates) by score 
clusters  cluster(top(candidates)) 
rank(clusters) 
related terms  top(clusters) 
60
algorithm needs to be selected. Each obtained clus-
ter is intended to correspond to the lemma of a sin-
gle candidate term. Obviously, clustering is mostly 
needed for surface-level representation, in order to 
group all different inflections of the same lemma. 
Yet, we note that it was also found necessary for 
the lemma-level representations, because the tag-
ger often identifies slightly different lemmas for 
the same term.  
The equivalence measure is used for building a 
graph representation of the related terms. We rep-
resented each term by a vertex and added an edge 
between each pair of terms that were deemed 
equivalent. We investigated alternative equiva-
lence measures for measuring the morphological 
distance between two vertices in our graph. We 
considered the string edit distance measure and 
suggested two morphological-based equivalence 
measures. The first measure, given two vertices' 
terms, extracts all possible lemmas for each term 
and searches for an overlap of at least one lemma. 
The second measure considers the most probable 
lemma of the vertices' terms and checks whether 
these lemmas are equal. The probability of a lem-
ma was defined as the sum of probabilities for all 
morphological analyses containing the lemma, us-
ing a morpho-lexical context-independent proba-
bilities approximation (Goldberg et al, 2008). The 
clustering was done by finding the connected com-
ponents in our graph of terms using the JUNG1 
implementation (WeakComponentVertexClusterer 
algorithm with default parameters). The connected 
components are expected to correspond to different 
lemmas of terms. Hierarchical clustering methods 
(Jain et al, 1999) were examined as well (Single-
link and Complete-link clustering), but they were 
inferior.  
After applying the clustering algorithm, we re-
ranked the clusters aiming to get the best clusters 
at the top of clusters list. We investigated two scor-
ing approaches for cluster ranking; maximization 
and averaging. The maximization approach assigns 
the maximal score of the cluster members as the 
cluster score. While the averaging approach as-
signs the average of the cluster members' scores as 
the cluster score. The score obtained by either of 
the approaches may be scaled by the cluster length, 
to account for the accumulative impact of all class 
                                                           
1
 http://jung.sourceforge.net/  
members (corresponding to morphological variants 
of the candidate term).  
3 Case Study: Cross-genre Hebrew 
Thesaurus 
Our research targets the construction of a cross 
genre thesaurus for the Responsa project 2 . The 
corpus includes questions posed to rabbis along 
with their detailed rabbinic answers, consisting of 
various genres and styles. It contains 76,710 arti-
cles and about 100 million word tokens, and was 
used for previous IR and NLP research (Choueka, 
1972; Fraenkel, 1976; Choueka et al, 1987; Kernel 
et al 2008). 
Unfortunately, due to the different genres in the 
Responsa corpus, available tools for Hebrew pro-
cessing perform poorly on this corpus. In a prelim-
inary experiment, the POS tagger (Adler and 
Elhadad, 2006) accuracy on the Responsa Corpus 
was less than 60%, while the accuracy of the same 
tagger on modern Hebrew corpora is ~90% (Bar-
Haim et al, 2007).  
For this project, we utilized the MILA Hebrew 
Morphological Analyzer3 (Itai and Wintner, 2008; 
Yona and Wintner, 2008) and the (Adler and 
Elhadad 2006) POS tagger for lemma representa-
tion. The latter had two important characteristics: 
The first is flexibility- This tagger allows adapting 
the estimates of the prior (context-independent) 
probability of each morphological analysis in an 
unsupervised manner, from an unlabeled corpus of 
the target domain (Goldberg et al, 2008). The se-
cond advantage is its mechanism for analyzing un-
known tokens (Adler et al, 2008). Since about 
50% of the words in our corpora are unknown 
(with respect to MILA's lexicon), such mechanism 
is essential.  
For statistics extraction, we used Lucene4. We 
took the top 1000 documents retrieved for the tar-
get term and extracted candidate terms from them. 
Dice coefficient was used as our co-occurrence 
measure, most probable lemma was considered for 
clustering equivalence, and clusters were ranked 
based on maximization, where the maximal score 
was multiplied by cluster size. 
 
                                                           
2
 Corpus kindly provided - http://www.biu.ac.il/jh/Responsa/ 
3
 http://mila.cs.technion.ac.il/mila/eng/tools_analysis.html 
3
 http://mila.cs.technion.ac.il/mila/eng/tools_analysis.html 
4
 http://lucene.apache.org/ 
 i .il
4 lucene.apache.org/ 
 
61
4 Evaluation 
4.1 Dataset and Evaluation Measures 
The results reported in this paper were obtained 
from a sample of 108 randomly selected terms 
from a list of 5000 terms, extracted from two pub-
licly available term lists: the University of Haifa?s 
entry list5 and Hebrew Wikipedia entries6. 
In our experiments, we compared the perfor-
mance of the alternative 9 configurations by four 
commonly used IR measures: precision (P), rela-
tive recall (R), F1, and Average Precision (AP). 
The scores were macro-averaged. We assumed that 
our automatically-generated candidate terms will 
be manually filtered, thus, recall becomes more 
important than precision. Since we do not have any 
pre-defined thesaurus, we evaluated the relative-
recall. Our relative-recall considered the number of 
suitable related terms from the output of all meth-
ods as the full set of related terms. As our system 
yielded a ranked sequence of related terms clusters, 
we also considered their ranking order. Therefore, 
we adopted the recall-oriented AP for ranking 
(Voorhees and Harman, 1999). 
4.2  Annotation Scheme 
The output of the statistical extraction is a ranked 
list of clusters of candidate related terms. Since 
manual annotation is expensive and time consum-
ing, we annotated for the gold standard the top 15 
clusters constructed from the top 50 candidate 
terms, for each target term. Then, an annotator 
judged each of the clusters' terms. A cluster was 
considered as relevant if at least one of its terms 
was judged relevant7. 
4.3 Results 
Table 1 compares the performance of all nine term 
representation configurations. Due to data sparse-
ness, the lemma-based representations of the target 
term outperform its surface representation. How-
ever, the best results were obtained from candidate 
representation at the surface level, which was 
complemented by grouping term variants to lem-
mas in the clustering phase. 
                                                           
5 http://lib.haifa.ac.il/systems/ihp.html 
6 http://he.wikipedia.org 
7
 This was justified by empirical results that found only a few 
clusters with some terms judged positive and others negative  
All best surface Candidate 
  Target 
26.68 29.37 36.59 R 
Surface 18.71 21.09 24.29 P 21.99 24.55 29.20 F1 
14.13 15.83 20.87 AP 
36.97 39.88 46.70 R 
Best 
lemma 
20.94 23.08 25.03 P 
26.74 29.24 32.59 F1 
19.32 20.86 26.84 AP 
42.13 42.52 47.13 R 
All 
 lemmas 
21.23 22.47 23.72 P 
28.24 29.40 31.56 F1 
21.14 22.99 27.86 AP 
Table 1: Performances of the nine configuratrions 
 
Furthermore, we note that the target representa-
tion by all possible lemmas (all) yielded the best R 
and AP scores, which we consider as most im-
portant for the thesaurus construction setting. The 
improvement over the common default best lemma 
representation, for both target and candidate, is 
notable (7 points) and is statistically significant 
according to the two-sided Wilcoxon signed-rank 
test (Wilcoxon, 1945) at the 0.01 level for AP and 
0.05 for R.  
5 Conclusions and Future Work 
We presented a methodological scheme for ex-
ploring alternative term representations in statisti-
cal thesaurus construction for MRL, complemented 
by lemma-oriented clustering at the end of the pro-
cess. The scheme was investigated for a Hebrew 
cross-genre corpus, but can be generically applied 
in other settings to find the optimal configuration 
in each case. 
We plan to adopt our methodology to second 
order distributional similarity methods as well. In 
this case there is an additional dimension, namely 
feature representation, whose representation level 
should be explored as well. In addition, we plan to 
extend our methods to deal with Multi Word Ex-
pressions (MWE). 
Acknowledgments 
This work was partially supported by the 
PASCAL-2 Network of Excellence of the Europe-
an Community FP7-ICT-2007-1-216886. 
 
62
References  
 
Adler Meni and Michael Elhadad. 2006. An Unsuper-
vised Morpheme-Based HMM for Hebrew Morpho-
logical Disambiguation, in Proceedings of COLING-
ACL, Sydney, Australia. 
Adler Meni, Yoav Goldberg, David Gabay and Michael 
Elhadad. 2008. Unsupervised Lexicon-Based Resolu-
tion of Unknown Words for Full Morphological 
Analysis, in Proceedings of ACL. 
Bar-Haim Roy, Khalil Sima'an, and Yoad Winter. 2007. 
Part-of-speech tagging of Modern Hebrew text. Nat-
ural Language Engineering, 14(02):223.251. 
Choueka, Yaacov. 1972. Fast searching and retrieval 
techniques for large dictionaries and concordances. 
Hebrew Computational Linguistics, 6:12?32, July.  
Choueka, Y., A.S. Fraenkel, S.T. Klein and E. Segal. 
1987. Improved techniques for processing queries in 
full-text systems. Proceedings of the 10th annual in-
ternational ACM SIGIR conference on Research and 
development in information retrieval. 
Church, K. W., and Hanks, P. 1990. Word association 
norms, mutual information and lexicography. Com-
putational Linguistics 16(1): 22?29. 
Curran, James R. and Marc Moens. 2002. Improve-
ments in automatic thesaurus extraction. In Proceed-
ings of the ACL-SIGLEX Workshop on Unsupervised 
Lexical Acquisition, pages 59?67, Philadelphia, PA. 
Dunning, T.E. 1993. Accurate methods for the statistics 
of surprise and coincidence. Computational Linguis-
tics 19, 61?74 (1993). 
Fraenkel, Aviezri S. 1976. All about the Responsa re-
trieval project ? what you always wanted to know but 
were afraid to ask. Jurimetrics Journal, 16(3):149?
156, Spring. 
Gasperin, C., Gamallo, P., Agustini, A., Lopes, G., and 
de Lima, V. 2001. Using syntactic contexts for meas-
uring word similarity. In the Workshop on Semantic 
Knowledge Acquisition and Categorisation (ESSLI 
2001), Helsinki, Finland. 
Goldberg Yoav, Meni Adler and Michael Elhadad, 
2008. EM Can Find Pretty Good HMM POS-Taggers 
(When Given a Good Start), in Proceedings of ACL. 
Grefenstette, G. 1994. Explorations in Automatic The-
saurus Construction. Kluwer Academic Publishers, 
Boston, USA 
Harris, Zelig S. 1968. Mathematical Structures of Lan-
guage. John Wiley, New York. 
Hindle, D. 1990. Noun classification from predicate 
argument structures. In Proceedings of ACL. 
Itai Alon and Shuly Wintner. 2008. Language Re-
sources for Hebrew. Language Resources and Evalu-
ation 42(1):75-98, March 2008. 
Jain, A. K., M. N. Murty, P. J. Flynn. 1999. Data Clus-
tering: A Review. ACM Computing Surveys 
31(3):264-323. 
Kerner Yaakov HaCohen, Ariel Kass, Ariel Peretz. 
2008. Combined One Sense Disambiguation of Ab-
breviations. In Proceedings of ACL (Short Papers), 
pp. 61-64. 
Kilgarriff, Adam. 2003. Thesauruses for natural lan-
guage processing. In Proceedings of the Joint Con-
ference on Natural Language Processing and 
Knowledge Engineering, pages 5?13, Beijing, China. 
Kotlerman Lili, Dagan Ido, Szpektor Idan, and 
Zhitomirsky-Geffet Maayan. 2010. Directional Dis-
tributional Similarity for Lexical Inference. Natural 
Language Engineering, 16(4):359?389.  
Linden Krister, and Jussi Olavi Piitulainen. 2004.  Dis-
covering Synonyms and Other Related Words. In 
Proceedings of COLING 2004 : CompuTerm 2004: 
3rd International Workshop on Computational Ter-
minology, Pages 63-70, Geneva, Switzerland 
Lin, D. 1998. Automatic retrieval and clustering of 
similar words. In Proceedings of COLING-ACL. 
Peirsman Yves, Kris Heylen, and Dirk Speelman. 2008. 
Putting things in order. first and second order con-
texts models for the calculation of semantic similari-
ty. In Actes des 9i`emes Journ?ees internationales 
d?Analyse statistique des Donn?ees Textuelles (JADT 
2008), pages 907?916. 
Rapp, R. 2009. The Automatic Generation of Thesauri 
of Related Words for English, French, German, and 
Russian, International Journal of Speech Technolo-
gy, 11, 3-4, 147-156. 
Rychly, P. and Kilgarriff, A. 2007. An efficient algo-
rithm for building a distributional thesaurus (and oth-
er Sketch Engine developments). In Proceedings of 
ACL-07, demo session. Prague, Czech Republic. 
Schutze Hinrich and Jan 0. Pederson. 1997. A 
cooccurrence-based thesaurus and two applications 
to information retrieval. Information Processing 
and Management, 33(3):307-318. 
Smadja, F., McKeown, K.R., Hatzivassiloglou, V. 1996. 
Translating collocations for bilingual lexicons: A sta-
tistical approach. Computational Linguistics 22, 1?38 
63
Voorhees E.M. and D. Harman. 1999. Overview of the 
seventh text retrieval conference . In Proceedings of 
the Seventh Text Retrieval 73 Conference, 1999. 
NIST Special Publication. 58. 
Weeds, J., and Weir, D. 2003. A general  framework for 
distributional similarity. In Proceedings of EMNLP, 
Sapporo, Japan. 
Wilcoxon F. 1945. Individual comparisons by ranking 
methods. Biometrics Bulletin, 1:80?83. 
Yona Shlomo and Shuly Wintner. 2008. A Finite-State 
Morphological Grammar of Hebrew.  Natural Lan-
guage Engineering 14(2):173-190, April 2008. Lan-
guage Resources and Evaluation 42(1):75-98, March 
2008. 
 
64
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 237?245,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
A Probabilistic Lexical Model for Ranking Textual Inferences
Eyal Shnarch and Ido Dagan
Computer Science Department
Bar-Ilan University
Ramat-Gan 52900, Israel
{shey,dagan}@cs.biu.ac.il
Jacob Goldberger
Faculty of Engineering
Bar-Ilan University
Ramat-Gan 52900, Israel
goldbej@eng.biu.ac.il
Abstract
Identifying textual inferences, where the
meaning of one text follows from another, is
a general underlying task within many natu-
ral language applications. Commonly, it is ap-
proached either by generative syntactic-based
methods or by ?lightweight? heuristic lexical
models. We suggest a model which is confined
to simple lexical information, but is formu-
lated as a principled generative probabilistic
model. We focus our attention on the task of
ranking textual inferences and show substan-
tially improved results on a recently investi-
gated question answering data set.
1 Introduction
The task of identifying texts which share semantic
content arises as a general need in many natural lan-
guage processing applications. For instance, a para-
phrasing application has to recognize texts which
convey roughly the same content, and a summariza-
tion application needs to single out texts which con-
tain the content stated by other texts. We refer to this
general task as textual inference similar to prior use
of this term (Raina et al, 2005; Schoenmackers et
al., 2008; Haghighi et al, 2005).
In many textual inference scenarios the setting re-
quires a classification decision of whether the infer-
ence relation holds or not. But in other scenarios
ranking according to inference likelihood would be
the natural task. In this work we focus on ranking
textual inferences; given a sentence and a corpus,
the task is to rank the corpus passages by their plau-
sibility to imply as much of the sentence meaning as
possible. Most naturally, this is the case in question
answering (QA), where systems search for passages
that cover the semantic components of the question.
A recent line of research was dedicated to this task
(Wang et al, 2007; Heilman and Smith, 2010; Wang
and Manning, 2010).
A related scenario is the task of Recognizing Tex-
tual Entailment (RTE) within a corpus (Bentivogli
et al, 2010)1. In this task, inference systems should
identify, for a given hypothesis, the sentences which
entail it in a given corpus. Even though RTE was
presented as a classification task, it has an appeal-
ing potential as a ranking task as well. For instance,
one may want to find texts that validate a claim such
as cellular radiation is dangerous for children, or to
learn more about it from a newswire corpus. To that
end, one should look for additional mentions of this
claim such as extensive usage of cell phones may be
harmful for youngsters. This can be done by rank-
ing the corpus passages by their likelihood to entail
the claim, where the top ranked passages are likely
to contain additional relevant information.
Two main approaches have been used to address
textual inference (for either ranking or classifica-
tion). One is based on transformations over syntac-
tic parse trees (Echihabi and Marcu, 2003; Heilman
and Smith, 2010). Some works in this line describe
a probabilistic generative process in which the parse
tree of the question is generated from the passage
(Wang et al, 2007; Wang and Manning, 2010).
In the second approach, lexical models have been
employed for textual inference (MacKinlay and
Baldwin, 2009; Clark and Harrison, 2010). Typi-
1http://www.nist.gov/tac/2010/RTE/index.html
237
cally, lexical models consider a text fragment as a
bag of terms and split the inference decision into
two steps. The first is a term-level estimation of the
inference likelihood for each term independently,
based on direct lexical match and on lexical knowl-
edge resources. Some commonly used resources are
WordNet (Fellbaum, 1998), distributional-similarity
thesauri (Lin, 1998), and web knowledge resources
such as (Suchanek et al, 2007). The second step
is making a final sentence-level decision based on
these estimations for the component terms. Lex-
ical models have the advantage of being fast and
easy to utilize (e.g. no dependency on parsing tools)
while being highly competitive with top performing
systems, e.g. the system of Majumdar and Bhat-
tacharyya (2010).
In this work, we investigate how well such lexi-
cal models can perform in textual inference ranking
scenarios. However, while lexical models usually
apply heuristic methods, we would like to pursue a
principled learning-based generative framework, in
analogy to the approaches for syntactic-based infer-
ence. An attractive work in this spirit is presented in
(Shnarch et al, 2011a), that propose a model which
is both lexical and probabilistic. Later, Shnarch et
al. (2011b) improved this model and reported re-
sults that outperformed previous lexical models and
were on par with state-of-the-art RTE models.
Whereas their term-level model provides means
to integrate lexical knowledge in a probabilistic
manner, their sentence-level model depends to a
great extent on heuristic normalizations which were
introduced to incorporate prominent aspects of the
sentence-level decision. This deviates their model
from a pure probabilistic methodology.
Our work aims at amending this deficiency and
proposes a new probabilistic sentence-level model
based on a Markovian process. In that model, all
parameters are estimated by an EM algorithm. We
evaluate this model on the tasks of ranking passages
for QA and ranking textual entailments within a cor-
pus, and show that eliminating the need for heuris-
tic normalizations greatly improves state-of-the-art
performance. The full implementation of our model
is available for download2 and can be used as an
easy-to-install and highly competitive inference en-
2http://www.cs.biu.ac.il/?nlp/downloads/probLexModel.html
gine that operates only on lexical knowledge, or as a
lexical component integrated within a more complex
inference system.
2 Background
Wang et al (2007) provided an annotated data set,
based on the Text REtrieval Conference (TREC) QA
tracks3, specifically for the task of ranking candidate
answer passages. We adopt their experimental setup
and next review the line of syntactic-based works
which reported results on this data set.
2.1 Syntactic generative models
Wang et al (2007) propose a quasi-synchronous
grammar formulation which specifies the generation
of the question parse tree, loosely conditioned on the
parse tree of the candidate answer passage. Their
model showed improvement over previous syntac-
tic models for QA: Punyakanok et al (2004), who
computed similarity between question-answer pairs
with a generalized tree-edit distance, and Cui et al
(2005), who developed an information measure for
sentence similarity based on dependency paths of
aligned words. Wang et al (2007) reproduced these
methods and extended them to utilize WordNet.
More recently, Heilman and Smith (2010) im-
proved Wang et al (2007) results with a classifica-
tion based approach. Feature for the classifier were
extracted from a greedy algorithm which searches
for tree-edit sequences which transform the parse
tree of the candidate answer into the one of the ques-
tion. Unlike other works reviewed here, this one
does not utilize lexical knowledge resources.
Similarly, Wang and Manning (2010) present an
extended tree-edit operations set and search for edit
sequences to generate the question from the answer
candidate. Their CRF-based classifier models these
sequences as latent variables.
An important merit of these methods is that they
offer principled, often probabilistic, generative mod-
els for the task of ranking candidate answers. Their
drawback is the need for syntactic analysis which
makes them slower to run, dependent on parsing per-
formance, which is often mediocre in many text gen-
res, and inadequate for languages which lack proper
parsing tools.
3http://trec.nist.gov/data/qamain.html
238
2.2 Lexical models
Lexical models, on the other hand, are faster, eas-
ier to implement and are more practical for vari-
ous genres and languages. Such models derive from
knowledge resources lexical inference rules which
indicate that the meaning of a lexical term can be
inferred from the meaning of another term (e.g.
youngsters? children and harmful? dangerous).
They are common in the Recognizing Textual En-
tailment (RTE) systems and we present some rep-
resentative methods for that task. We adopt textual
entailment terminology and henceforth use Hypoth-
esis (denoted H) for the inferred text fragment and
Text (denoted T ) for the text from which it is being
inferred4.
Majumdar and Bhattacharyya (2010) utilized a
simple union of lexical rules derived from vari-
ous lexical resources for the term-level step. They
derived their sentence-level decision based on the
number of matched hypothesis terms. The results
of this simple model were only slightly worse than
the best results of the RTE-6 challenge which were
achieved by a syntactic-based system (Jia et al,
2010). Clark and Harrison (2010), on the other hand,
considered the number of mismatched terms in es-
tablishing their sentence-level decision. MacKinlay
and Baldwin (2009) represented text and hypothe-
sis as word vectors augmented with lexical knowl-
edge. For sentence-level similarity they used a vari-
ant of the cosine similarity score. Common to most
of these lexical models is the application of heuris-
tic methods in both the term and the sentence level
steps.
Targeted to replace heuristic methods with princi-
pled ones, Shnarch et al (2011a) present a model
which aims at combining the advantages of a proba-
bilistic generative model with the simplicity of lex-
ical methods. In some analogy to generative parse-
tree based models, they propose a generative process
for the creation of the hypothesis from the text.
At the term-level, their model combines knowl-
edge from various input resources and has the ad-
vantages of considering the effect of transitive rule
application (e.g. mobile phone? cell phone? cel-
lular) as well as the integration of multiple pieces
4In the task of passage ranking for QA, the hypothesis is the
question and the text is the candidate passage.
of evidence for the inference of a term (e.g. both
the appearance of harmful and risky in T provide
evidence for the inference of dangerous in H). We
denote this term-level Probabilistic Lexical Model
as PLMTL, and have reproduced it in our work as
presented in Section 4.1. For the sentence-level de-
cision they describe an AND gate mechanism, i.e.
deducing a positive inference decision for H as a
whole only if all its terms were inferred from T .
In an extension to that work, Shnarch et al
(2011b) modified PLMTL to improve the sentence-
level step. They pointed out some prominent aspects
for the sentence-level decision. First, they suggest
that a hypothesis as a whole can be inferred from
the text even if some of its terms are not inferred.
To model this, they introduced a noisy-AND mech-
anism (Pearl, 1988). Additionally, they emphasized
the effect of hypothesis length and the dependency
between terms on the sentence-level decision. How-
ever, they did not fully achieve their target of pre-
senting a fully coherent probabilistic model, as their
model included heuristic normalization formulae.
On the contrary, the model we present is the first
along this line to be fully specified in terms of a
generative setting and formulated in pure probabilis-
tic terms. We introduce a Markovian-style proba-
bilistic model for the sentence-level decision. This
model receives as input term-level probabilistic es-
timates, which may be provided by any term-level
model. In our implementation we embed PLMTL as
the term-level model and present a complete coher-
ent Markovian-based Probabilistic Lexical Model,
which we term M-PLM.
3 Markovian sentence-level model
The goal of a sentence-level model is to integrate
term-level inputs into an inference decision for the
hypothesis as a whole. For a hypothesis H =
h1, . . . , hn and a text T , term-level models first esti-
mate independently for each term ht its probability
to be inferred from T . Let xt be a binary random
variable representing the event that ht is indeed in-
ferred from T (i.e., xt = 1 if ht is inferred and 0
otherwise).
Given these term-level probabilities, a sentence-
level model is employed to estimate the probability
that H as a whole is inferred from T . This step is
239
term
-leve
l
sente
nce-l
evel
Text: Hypo
:
t 1
t m
?
h 1
h 2
h n
?
x 1
x 2
x n
?
y 1
y 2
y n
?
Figure 1: A probabilistic lexical model: the upper part is the
term-level input to the sentence-level Markovian process, de-
picted in the lower part. xi is a binary variable representing the
inference of hi and yj is a variable for the accumulative infer-
ence decision for the first j terms of Hypo. The final sentence-
level decision is given by yn.
the focus of our work. We assume that the term-
level probabilities are given as input. Section 4.1
describes PLMTL, as a concrete method for deriving
these probabilities.
Our sentence-level model is based on a Marko-
vian process and is described in Section 3.1. In par-
ticular, it takes into account, in probabilistic terms,
the prominent factors in lexical entailment, men-
tioned in Section 2. An efficient inference algorithm
for our model is given in Section 3.2 and EM-based
learning is specified in Section 3.3.
3.1 Markovian sentence-level decision
The motivation for proposing a Markovian process
for the sentence-level is to establish an intermedi-
ate model, lying between two extremes: assuming
full independence between hypothesis terms versus
assuming that every term is dependent on all other
terms. The former alternative is too weak, while
the latter alternative is computationally hard and
not very informative, and thus hard to capture in
a model. Our model specifies a Markovian depen-
dence structure, which limits the dependence scope
to adjacent terms, as follows.
We define a binary variable yt to be the accumu-
lated sentence-level inference decision up to ht. In
other words, yt=1 if the subset {h1, . . . , ht} of H?s
terms is inferred as a whole from T .
Note that this means that yt can be 1 even if some
terms amongst h1, . . . , ht are not inferred. As yn is
the decision for the complete hypothesis, our model
addresses this way the prominent aspect that the hy-
pothesis as a whole may be inferred even if some of
its terms are not inferred. The reason for allowing
this is that such un-inferred terms may be inferred
from the global context of T , or alternatively, are ac-
tually inferred from T but the knowledge resources
in use do not contain the proper lexical rule to make
such inference.
Figure 1 describes both steps of a full lexical in-
ference model. Its lower part depicts our Markovian
process. In the proposed model the inference deci-
sion at each position t is a combination of xt, the
variable for the event of ht being inferred, and yt?1,
the accumulated decision at the previous position.
Therefore, the transition parameters of M-PLM can
be modeled as:
qij(k)=P (yt=k|yt?1 = i, xt=j) ?k, i, j?{0, 1}
where y1=x1. For instance, q01(1) is the probability
that yt=1, given that yt?1 =0 and xt=1.
Applying the Markovian process on the entire
hypothesis we get yn, which represents the final
sentence-level decision, where a soft decision is ob-
tained by computing the probability of yn=1:
P (yn=1) =
?
x1, ..., xn
y2, ..., yn?1, yn=1
P (x1)
n?
t=2
P (xt)P (yt|yt?1, xt)
The summation is done over all possible binary
values of the term-level variables x1, ..., xn and the
accumulated sentence-level variables y2, ..., yn?1
where yn=1. Note that for clarity, in this formula xt
and yt denote the binary values at the corresponding
variable positions. A tractable form for computing
P (yn=1) is presented in Section 3.2.
Overall, the prominent factors in lexical entail-
ment, raised by prior works, are incorporated within
the core structure of this probabilistic model, with-
out the need to resort to heuristic normalizations.
Reducing the negative affect of hypothesis length on
the entailment probability is achieved by having yt,
at each position, being directly dependent only on xt
and yt?1 as opposed to being affected by all hypoth-
esis terms. The second factor, modeling the depen-
dency between hypothesis terms, is addressed by the
240
indirect dependency of yn on all preceding hypothe-
sis terms. This dependency arises from the recursive
nature of the Markovian model, as can be seen in the
next section.
Our proposed Markovian process presents a linear
dependency between terms which, to some extent,
poses an anomaly with respect to the structure of the
entailment phenomenon. Yet, as we do want to limit
the dependence structure, following the natural or-
der of the sentence words seems the most reasonable
choice, as common in many other types of sequential
models. We also tried randomizing the word order
which, on average, did not improve performance.
3.2 Inference
The accumulated sentence-level inference can be
efficiently computed using a typical forward algo-
rithm. We denote the probability of xt=j, j?{0, 1}
by ht(j) = P (xt = j). The forward step is given in
Eq. (1) and its initialization is defined in Eq. (2).
?t(k) = P (yt=k)=
?
i,j?{0,1}
?t?1(i)ht(j)qij(k) (1)
?1(k) = P (x1 =k) (2)
where k?{0, 1} and t = 2, ..., n.
?t(k) is the probability that the accumulated de-
cision at position t is k. It is calculated by sum-
ming over the probabilities of all four combinations
of ?t?1(i) and ht(j), multiplied by the correspond-
ing transition probability, qij(k).
The soft sentence-level decision can be efficiently
calculated by:
P (yn=a) = ?n(a) a?{0, 1} (3)
3.3 Learning
Typically, natural language applications work at the
sentence-level. The training data for such applica-
tions is, therefore, available as annotations at the
sentence-level. Term-level alignments between pas-
sage terms and question terms are rarely available.
Hence, we learn our term-level parameters from
available sentence-level annotations, using the gen-
erative process described above to bridge the gap be-
tween these two levels.
For learning we use the typical backwards algo-
rithm which is described by Eq. (4) and Eq. (5),
where ?t(a|i) is the probability that the full hypoth-
esis inference value is a given that yt= i.
?n(a|i) = P (yn=a|yn= i) = 1{a=i} (4)
?t(a|i) = P (yn=a|yt= i) =
=
?
j,k?{0,1}
ht+1(j)qij(k)?t+1(a|k) (5)
where t = n?1, .., 1, a ? {0, 1} and 1{condition} is
the indicator function which returns 1 if condition
holds and 0 otherwise.
To estimate qij(k), the parameters of the Marko-
vian process, we employ the EM algorithm:
E-step: For each (T,H) pair in the training
data set, annotated with a ? {0, 1} as its sentence-
level inference value, we evaluate the expected
probability of every transition given the annotation
value a:
wtijk(T,H) = P (yt?1 = i, xt=j, yt=k|yn=a)
=
?t?1(i)ht(j)qij(k)?t(a|k)
P (yn=a)
(6)
?i, j, k?{0, 1} and t = 2, ..., |H|.
M-step: Given the values of wtijk(T,H) we
can estimate each qij(1), i, j?{0, 1}, by taking the
proportion of transitions in which yt?1 = i, xt = j
and yt = 1, out of the total transitions in which
yt?1 = i and xt=j:
qij(1)?
?
(T,H)
?|H|
t=2wtij1(T,H)
?
(T,H)
?|H|
t=2
?
k?{0,1}wtijk(T,H)
(7)
qij(0) = 1?qij(1)
4 Complete model implementation
We next describe the end-to-end probabilistic lexical
inference model we used in our evaluations. We im-
plemented PLMTL as our term-level model to pro-
vide us with ht(j), the term-level probabilities. We
chose this model since it is fully lexical, has the ad-
vantages of lexical knowledge integration described
in Section 2 and achieved top results on RTE data
sets. Next, we summarize PLMTL, and in Appendix
A we show how to adjust the learning schema to fit
into our sentence-level model.
241
4.1 PLMTL
Shnarch et al (2011a) provide a term-level model
which integrates lexical rules from various knowl-
edge resources. As described below it also consid-
ers transitive chains of rule applications as well as
the impact of parallel chains which provide multiple
evidence that h?H is inferred from T .
Their model assumes a parameter ?R for each
knowledge resource R in use. ?R specifies the re-
source?s reliability, i.e. the prior probability that ap-
plying a rule from R to an arbitrary text-hypothesis
pair would yield a valid inference.
Next, transitive chains may connect a text term to
a hypothesis term via intermediate term(s). For in-
stance, starting from the text term T-Mobile, a chain
that utilizes the lexical rules T-Mobile? telecom
and telecom? cell phone enables the inference of
the term cell phone from T . They compute, for each
step in a chain, the probability that this step is valid
based on the ?R values. Denoting the resource which
provided a rule r by R(r), Eq. (8) specifies that the
validity probability of the inference step correspond-
ing to the application of the rule r within the chain c
pointing at ht (as represented by xtcr) is ?R(r).
Next, for a chain c pointing at ht (represented by
xtc) to be valid, all its rule steps should be valid for
this pair. Eq. (9) estimates this probability by the
joint probability that the applications of all rules r?
c are valid, assuming independence of rules.
Several chains may connect terms in T to ht, thus
providing multiple pieces of evidence that ht is in-
ferred from T . For instance, both youngsters and
kids in T may indicate the inference of children in
H . For a term ht to be inferred from the entire sen-
tence T it is enough that at least one of the chains
from T to ht is valid. This is the complement event
of ht not being inferred from T which happens when
all chains which suggest the inference of ht, denoted
by C(ht), are invalid. Eq. (10) specifies this proba-
bility (again assuming independence of chains).
P (xtcr = 1) = ?R(r) (8)
P (xtc = 1) =
?
r?c
P (xtcr = 1) (9)
ht(1) = P (xt = 1) = 1?P (xt = 0) (10)
= 1?
?
c?C(ht)
P (xtc = 0)
With respect to the contributions of our work, we
note that previous works resorted to applying some
heuristic amendments on these equations to achieve
valuable results. In contrast, our work is the first
to present a purely generative model. This achieve-
ment shows that it is possible to shift from ad-hoc
heuristic methods, which are common practice, to
more solid mathematically-based methods.
Finally, for ranking text passages from a corpus
for a given hypothesis (question in the QA scenario),
our Markovian sentence-level model takes as its in-
put the outcome of Eq. (10) for each ht ? H . For
PLMTL we need to estimate the model parameters,
that is the various ?R values. In our Markovian
model this is done by the scheme detailed in Ap-
pendix A. Given these term-level probabilities, our
model computes for each hypothesis its probabil-
ity to be inferred from each of the corpus passages,
namely P (yn = 1) in Eq (3). Passages are then
ranked according to this probability.
5 Evaluations and Results
To evaluate the performance of M-PLM for ranking
textual inferences we focused on the task of ranking
candidate answer passages for question answering
(QA) as presented in Section 5.1. Additionally, we
demonstrate the added value of our sentence-level
model in another ranking experiment based on RTE
data sets, described in Section 5.2.
5.1 Answer ranking for question answering
Data set We adopted the experimental setup of
Wang et al (2007) who also provided an annotated
data set for answer passage ranking in QA5.
In their data set an instance is a pair of a factoid
question and a candidate answer passage (a single
sentence in this data set). It was constructed from the
data of the QA tracks at TREC 8?13. The question-
candidate pairs were manually judged and a pair was
annotated as positive if the candidate passage indi-
cates the correct answer for the question. The train-
ing and test sets roughly contain 5700 and 1500 pairs
correspondingly.
5The data set was kindly provided to us by
Mengqiu Wang and is available for download at
http://www.cs.stanford.edu/?mengqiu/data/qg-emnlp07-
data.tgz.
242
Method PLMTL utilizes WordNet and the Catvar
(Categorial Variation) derivations database (Habash
and Dorr, 2003) as generic and publicly available
lexical knowledge resources, when question and
answer terms are restricted to the first WordNet
sense. In order to be consistent with (Shnarch et al,
2011b), the best performing model of prior work,
we restricted our model to utilize only these two re-
sources which they used. However, additional lexi-
cal resources can be provided as input to our model
(e.g. a distributional similarity-base thesaurus).
We report Mean Average Precision (MAP) and
Mean Reciprocal Rank (MRR), the standard mea-
sures for ranked lists. In the cases of tie we took
a conservative approach and ranked positive anno-
tated instances below the negative instances scored
with the same probability. Hence, the reported fig-
ures are lower-bounds for any tie-breaking method
that could have been applied.
Results We compared our model to all 5 mod-
els evaluated for this data set, described in Sec-
tion 2, and to our own implementation of (Shnarch
et al, 2011b). We term this model Heuristically-
Normalized Probabilistic Lexical Model, HN-PLM,
since it modifies PLMTL by introducing heuristic
normalization formulae. As explained earlier, both
M-PLM and HN-PLM embed PLMTL in their im-
plementation but they differ in their sentence-level
model. In our implementation of both models,
PLMTL applies chains of transitive rule applications
whose maximal length is 3.
As seen in Table 1, M-PLM outperforms all prior
models by a large margin. A comparison of M-PLM
and HN-PLM reveals the major positive effect of
choosing the Markovian process for the sentence-
level decision. By avoiding heuristically-normalized
formulae and having all our parameters being part of
the Markovian model, we managed to increases both
MAP and MRR by nearly 2.5%6.
Ablation Test As an additional examination of
the impact of the Markovian process components,
we evaluated the contribution of having 4 transition
parameters. The AND-logic applied by (Shnarch et
6The difference is not significant according to the Wilcoxon
test, however we note that given the data set size it is hard to get
a significant difference and that both Heilman and Smith (2010)
and Wang and Manning (2010) improvements over the results
of Wang et al (2007) were not statistically significant.
System MAP MRR
Punyakanok et al 41.89 49.39
Cui et al 43.50 55.69
Wang & Manning 59.51 69.51
Wang et al 60.29 68.52
Heilman & Smith 60.91 69.17
Shnarch et al HN-PLM 61.89 70.24
M-PLM 64.38 72.69
Table 1: Results (in %) for the task of answer ranking for
question answering (sorted by MAP).
al., 2011a) to their sentence-level decision roughly
corresponds to 2 of the Markovian parameters. A
binary AND outputs 1 if both its inputs are 1. This
corresponds to q11(1) which is indeed estimate to be
near 1. In any other case an AND gate outputs 0.
This corresponds to q00(1) which was estimated to
be near zero.
The two parameters q01 and q10 are novel to the
Markovian process and do not have counterparts in
(Shnarch et al, 2011a). These parameters are the
cases in which the sentence-level decision accumu-
lated so far and the term-level decision do not agree.
Introducing these 2 parameters enables our model to
provide a positive decision for the hypothesis as a
whole (or for a part of it) even if some of its terms
were not inferred. We performed an ablation test on
each of these two parameters by forcing the value of
the ablated parameter to be zero. The notable perfor-
mance drop presented in Table 2 indicates the crucial
contribution of these parameters to our model.
Ablated parameter ? MAP ? MRR
q01(1) = 0 -2.61 -4.91
q10(1) = 0 -2.12 -2.86
Table 2: Ablation test for the novel parameters of the Marko-
vian process. Results (in %) indicate performance drop when
forcing a parameter to be zero.
5.2 RTE evaluations
To assess the added value of our model on an addi-
tional ranking evaluation, we utilize the search task
data sets of the recent Recognizing Textual Entail-
ment (RTE) benchmarks (Bentivogli et al, 2009;
Bentivogli et al, 2010), which were originally con-
243
structed for the task of entailment classification. In
that task a hypothesis is given with a corpus and the
goal is to identify which sentences of the corpus en-
tail the hypothesis. This setting naturally lends itself
to a ranking scenario, in which the desired output is
a list of the corpus sentences ranked by their proba-
bility to entail the given hypothesis.
To that end, we employed the same method-
ology as described in the previous section. Ta-
ble 3 presents the improvement of our model over
HN-PLM, whose classification performance was re-
ported to be on par with best-performing systems on
these data sets7. As can be seen, the improvement
is substantial for both measures on both data sets.
These results further assess the contribution of our
Markovian sentence-level model.
RTE-5 RTE-6
MAP MRR MAP MRR
HN-PLM 58.0 82.9 54.0 71.9
M-PLM 61.6 84.8 60.0 79.2
? +3.6 +1.9 +6.0 +7.3
Table 3: Improvements of our sentence-level model over
HN-PLM. Results (in %) are shown for the last RTE and
for the search task in RTE-5.
6 Discussion
This paper investigated probabilistic lexical mod-
els for ranking textual inferences focusing on pas-
sage ranking for QA. We showed that our coher-
ent probabilistic model, whose sentence-level model
is based on a Markovian process, considerably im-
proves five prior syntactic-based models as well as
a heuristically-normalized lexical model. Therefore,
it raises the baseline for future methods.
In future work we would like to further explore
a broader range of related probabilistic models. Es-
pecially, as our Markovian process is dependent on
term order, it would be interesting to investigate
models which are not order dependent.
Initial experiments on the classification task show
that M-PLM performs well above the average sys-
tem but below HN-PLM, since it does not normalize
7RTE data sets were only used for the classification task
so far, therefore there are no state-of-the-art results to compare
with, when utilizing them for the ranking task.
the estimated probability well across hypothesis. We
therefore suggest a future work on better classifica-
tion models.
Finally, we view this work as joining a line of re-
search which develops principled probabilistic mod-
els for the task of textual inference and demonstrates
their superiority over heuristic methods.
A Appendix: Adaptation of PLMTL
learning
M-PLM embeds PLMTL as its term-level model.
PLMTL introduces ?R values as additional parame-
ters for the complete model. We show how we mod-
ify (Shnarch et al, 2011a) E-step formula to fit our
Markovian modeling, described in Section 3.1. The
M-step formula remains exactly the same.
Eq. (11) estimates the a-posteriori validity prob-
ability of a single application of the rule r in the
transitive chain c pointing at ht, given that the an-
notation of the pair is a.
wtcr(T,H) = P (xtcr = 1|yn = a) =
(11)?
i,j,k?{0,1} ?t?1(i)P (xt=j|xtcr =1)?R(r)qij(k)?t(a|k)
P (yn = a)
where t=2 . . . n and P (xt =j|xtcr =1) is the prob-
ability that the inference value of xt is j, given that
the application of r provides a valid inference step.
As appeared in (Shnarch et al, 2011b) this probabil-
ity can be evaluated as follows:
P (xt=1|xtcr =1)=1?
P (xt = 0)
P (xtc = 0)
(
1?
P (xtc = 1)
?R(r)
)
For t = 1 there is no accumulated sentence-level
decision at the previous position (i.e. no ?t?1) there-
fore Eq. (11) becomes:
w1cr(T,H) =
?
j?{0,1}P (x1 =j|x1cr =1)?R(r)?1(a|j)
P (yn = a)
Acknowledgments
This work was partially supported by the Israel
Science Foundation grant 1112/08, the PASCAL-
2 Network of Excellence of the European Com-
munity FP7-ICT-2007-1-216886, and the Euro-
pean Community?s Seventh Framework Programme
(FP7/2007-2013) under grant agreement no. 287923
(EXCITEMENT).
244
References
Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo
Giampiccolo, and Bernardo Magnini. 2009. The fifth
PASCAL recognizing textual entailment challenge. In
Proceedings of the Text Analysis Conference.
Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa Trang
Dang, and Danilo Giampiccolo. 2010. The sixth
PASCAL recognizing textual entailment challenge. In
Proceedings of the Text Analysis Conference.
Peter Clark and Phil Harrison. 2010. BLUE-Lite: a
knowledge-based lexical entailment system for RTE6.
In Proceedings of the Text Analysis Conference.
Hang Cui, Renxu Sun, Keya Li, Min yen Kan, and Tat
seng Chua. 2005. Question answering passage re-
trieval using dependency relations. In Proceedings of
SIGIR.
Abdessamad Echihabi and Daniel Marcu. 2003. A
noisy-channel approach to question answering. In
Proceedings of ACL.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database (Language, Speech, and Com-
munication). The MIT Press.
Nizar Habash and Bonnie Dorr. 2003. A categorial vari-
ation database for english. In Proceedings of the Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics.
Aria Haghighi, Andrew Ng, and Christopher Manning.
2005. Robust textual inference via graph matching. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing.
Michael Heilman and Noah A. Smith. 2010. Tree
edit models for recognizing textual entailments, para-
phrases, and answers to questions. In Proceedings of
the Conference of the North American Chapter of the
Association for Computational Linguistics.
Houping Jia, Xiaojiang Huang, Tengfei Ma, Xiaojun
Wan, and Jianguo Xiao. 2010. PKUTM participation
at the Text Analysis Conference 2010 RTE and sum-
marization track. In Proceedings of the Text Analysis
Conference.
Dekang Lin. 1998. Automatic retrieval and clustering of
similar words. In Proceedings of COLING.
Andrew MacKinlay and Timothy Baldwin. 2009. A
baseline approach to the RTE5 search pilot. In Pro-
ceedings of the Text Analysis Conference.
Debarghya Majumdar and Pushpak Bhattacharyya.
2010. Lexical based text entailment system for main
task of RTE6. In Proceedings of the Text Analysis
Conference.
Judea Pearl. 1988. Probabilistic reasoning in intelli-
gent systems: networks of plausible inference. Morgan
Kaufmann.
Vasin Punyakanok, Dan Roth, and Wen tau Yih. 2004.
Mapping dependencies trees: An application to ques-
tion answering. In Proceedings of the International
Symposium on Artificial Intelligence and Mathemat-
ics.
Rajat Raina, Andrew Y. Ng, and Christopher D. Man-
ning. 2005. Robust textual inference via learning and
abductive reasoning. In Proceedings of AAAI.
Stefan Schoenmackers, Oren Etzioni, and Daniel Weld.
2008. Scaling textual inference to the web. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing.
Eyal Shnarch, Jacob Goldberger, and Ido Dagan. 2011a.
A probabilistic modeling framework for lexical entail-
ment. In Proceedings of ACL.
Eyal Shnarch, Jacob Goldberger, and Ido Dagan. 2011b.
Towards a probabilistic model for lexical entailment.
In Proceedings of the TextInfer Workshop on Textual
Entailment.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: A Core of Semantic Knowl-
edge. In Proceedings of WWW.
Mengqiu Wang and Christopher Manning. 2010. Proba-
bilistic tree-edit models with structured latent variables
for textual entailment and question answering. In Pro-
ceedings of Coling.
Mengqiu Wang, Noah A. Smith, and Teruko Mita-
mura. 2007. What is the Jeopardy model? a quasi-
synchronous grammar for QA. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing.
245
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 263?274, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
SemEval-2013 Task 7: The Joint Student Response Analysis and 8th
Recognizing Textual Entailment Challenge
Myroslava O. Dzikovska
School of Informatics, University of Edinburgh
Edinburgh, United Kingdom
m.dzikovska@ed.ac.uk
Rodney D. Nielsen
University of North Texas
Denton, TX, USA
Rodney.Nielsen@UNT.edu
Chris Brew
Nuance Communications
USA
cbrew@acm.org
Claudia Leacock
CTB McGraw-Hill
USA
claudia leacock@mheducation.com
Danilo Giampiccolo
CELCT
Italy
giampiccolo@celct.it
Luisa Bentivogli
CELCT and FBK
Italy
bentivo@fbk.eu
Peter Clark
Vulcan Inc.
USA
peterc@vulcan.com
Ido Dagan
Bar-Ilan University
Israel
dagan@cs.biu.ac.il
Hoa Trang Dang
NIST
hoa.dang@nist.gov
Abstract
We present the results of the Joint Student
Response Analysis and 8th Recognizing Tex-
tual Entailment Challenge, aiming to bring to-
gether researchers in educational NLP tech-
nology and textual entailment. The task of
giving feedback on student answers requires
semantic inference and therefore is related to
recognizing textual entailment. Thus, we of-
fered to the community a 5-way student re-
sponse labeling task, as well as 3-way and 2-
way RTE-style tasks on educational data. In
addition, a partial entailment task was piloted.
We present and compare results from 9 partic-
ipating teams, and discuss future directions.
1 Introduction
One of the tasks in educational NLP systems is pro-
viding feedback to students in the context of exam
questions, homework or intelligent tutoring. Much
previous work has been devoted to the automated
scoring of essays (Attali and Burstein, 2006; Sher-
mis and Burstein, 2013), error detection and correc-
tion (Leacock et al, 2010), and classification of texts
by grade level (Petersen and Ostendorf, 2009; Shee-
han et al, 2010; Nelson et al, 2012). In these appli-
cations, NLP methods based on shallow features and
supervised learning are often highly effective. How-
ever, for the assessment of responses to short-answer
questions (Leacock and Chodorow, 2003; Pulman
and Sukkarieh, 2005; Nielsen et al, 2008a; Mohler
et al, 2011) and in tutorial dialog systems (Graesser
et al, 1999; Glass, 2000; Pon-Barry et al, 2004; Jor-
dan et al, 2006; VanLehn et al, 2007; Dzikovska et
al., 2010) deeper semantic processing is likely to be
appropriate.
Since the task of making and testing a full edu-
cational dialog system is daunting, Dzikovska et al
(2012) identified a key subtask and proposed it as a
new shared task for the NLP community. Student
response analysis (henceforth SRA) is the task of
labeling student answers with categories that could
263
Example 1 QUESTION You used several methods to separate and identify the substances in mock rocks. How did you
separate the salt from the water?
REF. ANS. The water was evaporated, leaving the salt.
STUD. ANS. The water dried up and left the salt.
Example 2 QUESTION Georgia found one brown mineral and one black mineral. How will she know which one is harder?
REF. ANS. The harder mineral will leave a scratch on the less hard mineral. If the black mineral is harder, the
brown mineral will have a scratch.
STUD. ANS. The harder will leave a scratch on the other.
Figure 1: Example questions and answers
help a full dialog system to generate appropriate and
effective feedback on errors. System designers typi-
cally create a repertoire of questions that the system
can ask a student, together with reference answers
(see Figure 1 for an example). For each student an-
swer, the system needs to decide on the appropriate
tutorial feedback, either confirming that the answer
was correct, or providing additional help to indicate
how the answer is flawed and help the student im-
prove. This task requires semantic inference, for ex-
ample, to detect when the student answers are ex-
plaining the same content but in different words, or
when they are contradicting the reference answers.
Recognizing Textual Entailment (RTE) is a se-
ries of highly successful challenges used to evalu-
ate tasks related to semantic inference, held annually
since 2005. Initial challenges used examples from
information retrieval, question answering, machine
translation and information extraction tasks (Dagan
et al, 2006; Giampiccolo et al, 2008). Later chal-
lenges started to explore the applicability and im-
pact of RTE technology on specific application set-
tings such as Summarization and Knowledge Base
Population (Bentivogli et al, 2009; Bentivogli et al,
2010; Bentivogli et al, 2011). The SRA Task offers
a similar opportunity.
We therefore organized a joint challenge at
SemEval-2013, aiming to bring together the educa-
tional NLP and the semantic inference communities.
The goal of the challenge is to compare approaches
for student answer assessment and to evaluate the
methods typically used in RTE on data from educa-
tional applications.
We present the corpus used in the task (Section
2) and describe the Main task, including educational
NLP and textual entailment perspectives and data set
creation (Section 3). We discuss evaluation metrics
and results in Section 4. Section 5 describes the Pi-
lot task, including data set creation and evaluation
results. Section 6 presents conclusions and future
directions.
2 Student Response Analysis Corpus
We used the Student Response Analysis corpus
(henceforth SRA corpus) (Dzikovska et al, 2012)
as the basis for our data set creation. The corpus
contains manually labeled student responses to ex-
planation and definition questions typically seen in
practice exercises, tests, or tutorial dialogue.
Specifically, given a question, a known correct
?reference answer? and a 1- or 2-sentence ?student
answer?, each student answer in the corpus is label-
led with one of the following judgments:
? ?Correct?, if the student answer is a complete
and correct paraphrase of the reference answer;
? ?Partially correct incomplete?, if it is a par-
tially correct answer containing some but not
all information from the reference answer;
? ?Contradictory?, if the student answer explicitly
contradicts the reference answer;
? ?Irrelevant? if the student answer is talking
about domain content but not providing the
necessary information;
? ?Non domain? if the student utterance does not
include domain content, e.g., ?I don?t know?,
?what the book says?, ?you are stupid?.
The SRA corpus consists of two distinct subsets:
BEETLE data, based on transcripts of students in-
teracting with BEETLE II tutorial dialogue system
(Dzikovska et al, 2010), and SCIENTSBANK data,
264
based on the corpus of student answers to assess-
ment questions collected by Nielsen et al (2008b).
The BEETLE corpus consists of 56 questions in
the basic electricity and electronics domain requir-
ing 1- or 2- sentence answers, and approximately
3000 student answers to those questions. The SCI-
ENTSBANK corpus contains approximately 10,000
answers to 197 assessment questions in 15 different
science domains (after filtering, see Section 3.3)
Student answers in the BEETLE corpus were man-
ually labeled by trained human annotators using a
scheme that straightforwardly mapped into SRA an-
notations. The annotations in the SCIENTSBANK
corpus were converted into SRA labels from a sub-
stantially more fine-grained scheme by first auto-
matically labeling them using a set of question-
specific heuristics and then manually revising them
according to the class definitions (Dzikovska et al,
2012). We further filtered and transformed the cor-
pus to produce training and test data sets as dis-
cussed in the next section.
3 Main Task
3.1 Educational NLP perspective
The 5-way SRA task focuses on associating student
answers with categorical labels that can be used in
providing tutoring feedback. Most NLP research on
short answer scoring reports agreement with a nu-
meric score (Leacock and Chodorow, 2003; Pulman
and Sukkarieh, 2005; Mohler et al, 2011), which
is a potential contrast with our task. However, the
majority of the NLP work makes use of underlying
representations in terms of concepts, so the 5-way
task is still likely to mesh well with the available
technology. Research on tutorial dialog has empha-
sized generic methods that use latent semantic anal-
ysis or other machine learning methods to determine
when text strings express similar concepts (Hu et al,
2003; Jordan et al, 2004; VanLehn et al, 2007; Mc-
Carthy et al, 2008). Most of these methods, like
the NLP methods, (with the notable exception of
(Nielsen et al, 2008a)), are however strongly depen-
dent on domain expertise for the definitions of the
concepts. In educational applications, there would
be great value in a system that could operate more
or less unchanged across a range of domains and
question-types, requiring only a question text and a
reference answer supplied by the instructional de-
signers. Thus, the 5-way classification task at Se-
mEval was set up to evaluate the feasibility of such
answer assessment, either by adapting the existing
educational NLP methods to the categorical labeling
task or by employing the RTE approaches.
3.2 RTE perspective and 2- and 3-way Tasks
According to the standard definition of Textual En-
tailment, given two text fragments called Text (T)
and Hypothesis (H), it is said that T entails H if, typ-
ically, a human reading T would infer that H is most
likely true (Dagan et al, 2006).
In a typical answer assessment scenario, we ex-
pect that a correct student answer would entail the
reference answer, while an incorrect answer would
not. However, students often skip details that are
mentioned in the question or may be inferred from
it, while reference answers often repeat or make ex-
plicit information that appears in or is implied from
the question, as in Example 2 in Figure 1. Hence, a
more precise formulation of the task in this context
considers the entailing text T as consisting of both
the original question and the student answer, while
H is the reference answer.
We carried out a feasibility study to check how
well the entailment judgments in this formulation
align with the annotated response assessment, by an-
notating a sample of the data used in the SRA task
with entailment judgments. We found that some an-
swers labeled as ?correct? implied inferred or as-
sumed pieces of information not present in the text.
These reflected the teachers? assessment of student
understanding but would not be considered entailed
from the traditional RTE perspective. However, we
observed that in most such cases, a substantial part
of the hypothesis was still implied by the text. More-
over, answers assigned labels other than ?correct?
were always judged as ?not entailed?.
Overall, we concluded that the correlation be-
tween assessment judgments of the two types was
sufficiently high to consider an RTE approach. The
challenge for the textual entailment community was
to address the answer assessment task at varying
levels of granularity, using textual entailment tech-
niques, and explore how well these techniques can
help in this real-world educational setting.
In order to make the setup more similar to pre-
265
vious RTE tasks, we introduced 3-way and 2-way
versions of the task. The data for those tasks were
obtained by automatically collapsing the 5-way la-
bels. In the 3-way task, the systems were required to
classify the student answer as either (i) correct; (ii)
contradictory; or (iii) incorrect (combining the cat-
egories partially correct but incomplete, irrelevant
and not in the domain from the 5-way classification).
In the two-way task, the systems were required to
classify the student answer as either correct or in-
correct (combining the categories contradictory and
incorrect from the 3-way classification)
3.3 Data Preparation and Training Data
In preparation of the task four of the organizers ex-
amined all questions in the SRA corpus, and decided
that to remove some of the questions to make the
dataset more uniform.
We observed two main issues. First, a num-
ber of questions relied on external material, e.g.,
charts and graphs. In some cases, the information
in the reference answer was sufficient to make a rea-
sonable assessment of student answer correctness,
but in other cases the information contained in the
questions was deemed insufficient and the questions
were removed.
Second, some questions in the SCIENTSBANK
dataset could have multiple possible correct an-
swers, e.g., a question asking for any example out
of two or more unrelated possibilities. Such ques-
tions were also removed as they do not align well
with the RTE perspective.
Finally, parts of the data were re-checked for re-
liability. In BEETLE data, a second manual annota-
tion pass was carried out on a subset of questions
to check for consistency. In SCIENTSBANK, we
manually re-checked the test data. The automatic
conversion from the original SCIENTSBANK anno-
tations into SRA labels was not perfectly accurate
(Dzikovska et al, 2012). We did not have the re-
sources to check the entire data set. However, four of
the organizers jointly hand-checked approximately
100 examples to establish consensus, and then one
organizer hand-checked all of the test data set.
3.4 Test Data
We followed the evaluation methodology of Nielsen
et al (2008a) for creating the test data. Since our
goal is to support systems that generalize across
problems and domains (see Section 3.1), we created
three distinct test sets:
1. Unseen answers (UA): a held-out set to assess
system performance on the answers to ques-
tions contained in the training set (for which
the system has seen example student answers).
It was created by setting aside a subset if ran-
domly selected learner answers to each ques-
tion included in the training data set.
2. Unseen questions (UQ): a test set to assess
system performance on responses to previously
unseen questions but which still fall within the
application domains represented in the training
data. It was created by holding back all student
answers to a subset of randomly selected ques-
tions in each dataset.
3. Unseen domains (UD): a domain-independent
test set of responses to topics not seen in the
training data, available only in the SCIENTS-
BANK dataset. It was created by setting aside
the complete set of questions and answers from
three science modules from the fifteen modules
in the SCIENTSBANK data.
The final label distribution for train and test data
is shown in Table 1.
4 Main Task Results
4.1 Participants
The participants were invited to submit up to three
runs in any combination of the tasks. Nine teams
participated in the main task, most choosing to at-
tempt all subtasks (5-way, 3-way and 2-way), with
1 team entering only the 5-way and 1 team entering
only the 2-way task.
At least 6 (CNGL, CoMeT, CU, BIU, EHUALM,
LIMSI) of the 9 systems used some form of syn-
tactic processing, in most cases going beyond parts
of speech to dependencies or constituency structure.
CNGL emphasized this as an important aspect of the
system. At least 5 (CoMeT, CU, EHUALM, ETS
UKP) of the 9 systems used a system combination
approach, with several components feeding into a
final decision made by some form of stacked clas-
sifier. The majority of the systems used some kind
266
label BEETLE SCIENTSBANK
train (%) UA UQ Test-Total (%) train (%) UA UQ UD Test-Total (%)
correct 1665 (0.42) 176 344 520 (0.41) 2008 (0.40) 233 301 1917 2451 (0.42)
pc inc 919 (0.23) 112 172 284 (0.23) 1324 (0.27) 113 175 986 1274 (0.22)
contra 1049 (0.27) 111 244 355 (0.28) 499 (0.10) 58 64 417 539 (0.09)
irrlvnt 113 (0.03) 17 19 36 (0.03) 1115 (0.22) 133 193 1222 1548 (0.27)
non dom 195 (0.05) 23 40 63 (0.05) 23 (0.005) 3 0 20 23 (0.004)
incorr-3way 1227 (0.31) 152 231 383 (0.30) 2462 (0.495) 249 368 2228 2845 (0.49)
incorr-2way 2276 (0.58) 263 475 538 (0.59) 2961 (0.596) 307 432 2645 3384 (0.58)
Table 1: Label distribution. Percentages in parentheses. UA, UQ, UD correspond to individual test sets.
of measure of text-to-text similarity, whether the in-
spiration was LSA, MT measures such as BLEU
or in-house methods. These methods were em-
phasized as especially important by Celi, ETS and
SOFTCARDINALITY. These impressions are based
on short summaries sent to us by the participants
prior to the availability of the full system descrip-
tions. Check the individual system papers for detail.
4.2 Evaluation Metrics
For each evaluation data set (test set), we computed
the per-class precision, recall and F1 score. We also
computed three main summary metrics: accuracy,
macro-average F1 and weighted average F1.
Accuracy is the overall percentage of correctly
classified examples.
Macroaverage is the average value of each met-
ric (precision, recall, F1) across classes, without
taking class size into account. It is defined as
1/Nc
?
c metric(c), where Nc is the number of
classes (2, 3, or 5 depending on the task). Note
that in the 5-way SCIENTSBANK dataset the ?non-
domain? class is severely underrepresented, with
only 23 examples out of 4335 total (see Table 1).
Therefore, we calculated macro-averaged P/R/F1
over only 4 classes (i.e. excluding the ?non-domain?
class) for SCIENTSBANK 5-way data.
Weighted Average (or simply weighted) is the
average value for each metric weighted by class size,
defined as 1/N
?
c |c| ? metric(c) where N is the
total number of test items and |c| is the number of
items labeled as c in gold-standard data.1
1This metric is called microaverage in (Dzikovska et al,
2012). However, microaverage is used to define a different
metric in tasks where more than one label can be associated
with each data item (Tsoumakas et al, 2010). therefore, we use
weighted average to match the terminology used by the Weka
toolkit. The micro-average precision, recall and F1 computed
In general, macro-averaging favors systems that
perform well across all classes regardless of class
size. Accuracy and weighted average prefer systems
that perform best on the largest number of examples,
favoring higher performance on the most frequent
classes. In practice, only a small number of the sys-
tems were ranked differently by the different met-
rics. We discuss this further in Section 4.7. Results
for all metrics are available online, and this paper
focuses on two metrics for brevity: weighted and
macro-average F1 scores.
4.3 Results
The evaluation results for all metrics and all partic-
ipant runs are provided online.2 The tables in this
paper present the F1 scores for the best system runs.
Results are shown separately for each test set (TS),
with the simple mean over the five TSs reported in
the final column.
We used two baselines: the majority (most fre-
quent) class baseline and a lexical overlap baseline
described in detail in (Dzikovska et al, 2012). The
performance of the baselines is presented jointly
with system scores in the results tables.
For each participant, we report the single run with
the best average TS performance, identified by the
subscript in the run title, with the exception of ETS.
With all other participants, there was almost always
one run that performed best for a given metric on all
the TSs. In the small number of cases where another
run performed best on a given TS, we instead report
that value and indicate its run with a subscript (these
changes never resulted in meaningful changes in the
performance rankings). ETS, on the other hand, sub-
using the multi-label metric are all equal and mathematically
equivalent to accuracy.
2http://bit.ly/11a7QpP
267
Dataset: BEETLE SCIENTSBANK
Run UA UQ UA UQ UD Mean
CELI1 0.423 0.386 0.372 0.389 0.367 0.387
CNGL2 0.547 0.469 0.266 0.297 0.294 0.375
CoMeT1 0.675 0.445 0.598 0.299 0.252 0.454
EHUALM2 0.566 0.4163 0.5253 0.446 0.437 0.471
ETS1 0.552 0.547 0.535 0.487 0.447 0.514
ETS2 0.705 0.614 0.625 0.356 0.434 0.547
LIMSIILES1 0.505 0.424 0.419 0.456 0.422 0.445
SoftCardinality1 0.558 0.450 0.537 0.492 0.471 0.502
UKP-BIU1 0.448 0.269 0.590 0.3972 0.407 0.418
Median 0.552 0.445 0.535 0.397 0.422 0.454
Baselines:
Lexical 0.483 0.463 0.435 0.402 0.396 0.436
Majority 0.229 0.248 0.260 0.239 0.249 0.245
Table 2: Five-way task weighted-average F1
mitted results for systems that were substantially dif-
ferent from one another, with performance varying
from being the top rank to nearly the lowest. Hence,
it seemed more appropriate to report two separate
runs.3 In the rest of the discussion system is used to
refer to a row in the tables as just described.
Systems with performance that was not statisti-
cally different from the best results for a given TS
are all shown in bold (significance was not cal-
culated for the TS mean). Systems with perfor-
mance statistically better than the lexical baseline
are displayed in italics. Statistical significance tests
were conducted using approximate randomization
test (Yeh, 2000) with 10,000 iterations; p ? 0.05
was considered statistically significant.
4.4 Five-way Task
The results for the five-way task are shown in Tables
2 and 3.
Comparison to baselines All of the systems per-
formed substantially better than the majority class
baseline (?correct? for both BEETLE and SCIENTS-
BANK), on average exceeding it on the TS mean by
0.21 on the weighted F1 and 0.24 on the macro-
average F1. Six systems outperformed the lexical
baseline on the mean TS results for the weighted
F1 and five for the macro-average F1. Nearly all
of the top results on a given TS (shown in bold in
the tables) were statistically better than correspond-
ing lexical baselines according to significance tests
3In a small number of cases, ETS?s third run performed
marginally better, see full results online.
Dataset: BEETLE 5way SCIENTSBANK 4way
Run UA UQ UA UQ UD Mean
CELI1 0.315 0.300 0.278 0.286 0.269 0.270
CNGL2 0.431 0.382 0.252 0.262 0.239 0.274
CoMeT1 0.569 0.300 0.551 0.201 0.151 0.312
EHUALM2 0.526 0.3703 0.4473 0.353 0.340 0.382
ETS1 0.444 0.461 0.467 0.372 0.334 0.377
ETS2 0.619 0.552 0.581 0.274 0.339 0.428
LIMSIILES1 0.327 0.280 0.335 0.361 0.337 0.308
SoftCardinality1 0.455 0.436 0.474 0.384 0.375 0.389
UKP-BIU1 0.423 0.285 0.560 0.3252 0.348 0.364
Median 0.444 0.370 0.467 0.325 0.337 0.367
Baselines:
Lexical 0.424 0.414 0.375 0.329 0.311 0.333
Majority 0.114 0.118 0.151 0.146 0.148 0.129
Table 3: Five-way task macro-average F1
(indicated by italics in the tables).
Comparing UA and UQ/UD performance The
BEETLE UA (BUA) and SCIENTSBANK UA (SUA)
test sets represent questions with example answers
in training data, while the UQ and UD test sets repre-
sent transfer performance to new questions and new
domains respectively.
The top performers on UA test sets were CoMeT1
and ETS2, with the addition of UKP-BIU1 on SUA.
However, there was not a single best performer on
UQ and UD sets. ETS2 performed statistically bet-
ter than all other systems on BEETLE UQ (BUQ),
but it performed statistically worse than the lexical
baseline on SCIENTSBANK UQ (SUQ), resulting in
no overlap in the top performing systems on the two
UQ test sets. SoftCardinality1 performed statisti-
cally better than all other systems on SUD and was
among the three or four top performers on SUQ, but
was not a top performer on the other three TSs, gen-
erally not performing statistically better than the lex-
ical baseline on the BEETLE TSs.
Group performance The two UA TSs had more
systems that performed statistically better than the
lexical baseline (generally six systems) than did the
UQ TSs where on average only two systems per-
formed statistically better than the lexical baseline.
Over twice as many systems outperformed the lexi-
cal baseline on UD as on the UQ TSs. The top per-
forming systems according to the macro-average F1
were nearly identical to the top performing systems
according to the weighted F1.
268
4.5 Three-way Task
The results for the three-way task are shown in Ta-
bles 4 and 5.
Comparison to baselines All of the systems per-
formed substantially better than the majority base-
line (?correct? for BEETLE and ?incorrect? for SCI-
ENTSBANK), on average exceeding it on the TS
mean by 0.28 on the weighted F1 and 0.31 on the
macro-average F1. Five of the eight systems out-
performed the lexical baseline on the mean TS re-
sults for the weighted F1 and five on the macro-
average F1, and all top systems outperformed the
lexical baseline with statistical significance.
Comparing UA and UQ/UD performance The top
performers on both BUA and SUA were CoMeT1
and ETS2. As for the 5-way task there was no single
best performer for UQ and UD sets, and no overlap
in top performing systems on BUQ and SUQ test
sets, with ETS2 being the top performer on BUQ,
but statistically worse than the baseline on SUQ
and SUD. On the weighted F1, SoftCardinality1
performed statistically better than all other systems
on SUD and was among the two statistically best
systems on SUQ, but was not a top performer on
BUQ or BUA/SUA TSs. On the macro-average F1,
UKP-BIU1 became one of the statistically best per-
formers on all SCIENTSBANK TSs but, along with
SoftCardinality1, never performed statistically bet-
ter than the lexical baseline on the BEETLE TSs.
Group performance With the exception of SUA,
only around two systems performed statistically bet-
ter than the lexical baseline on each TS. The top per-
forming systems were nearly the same according to
the weighted F1 and the macro-average F1.
4.6 Two-way Task
The results for the two-way task are shown in Ta-
ble 6. Because the labels are roughly balanced in
the two-way task, the results on the weighted and
macro-average F1 are very similar and the top per-
forming systems are identical. Hence this section
will focus only on the macro-average F1.
As in the previous tasks, all of the systems per-
formed substantially better than the majority base-
line (?incorrect? for all sets), on average exceeding
it on the TS mean by 0.25 on the weighted F1 and
0.30 on the macro-average F1. However, just four of
Dataset: BEETLE SCIENTSBANK
Run UA UQ UA UQ UD Mean
CELI1 0.519 0.463 0.500 0.555 0.534 0.514
CNGL2 0.592 0.471 0.383 0.367 0.360 0.435
CoMeT1 0.728 0.488 0.707 0.522 0.550 0.599
ETS1 0.619 0.542 0.603 0.631 0.600 0.599
ETS2 0.723 0.597 0.709 0.537 0.505 0.614
LIMSIILES1 0.587 0.454 0.532 0.553 0.564 0.538
SoftCardinality1 0.616 0.451 0.647 0.634 0.620 0.594
UKP-BIU1 0.472 0.313 0.670 0.573 0.5772 0.521
Median 0.604 0.467 0.625 0.554 0.557 0.566
Baselines:
Lexical 0.578 0.500 0.523 0.520 0.554 0.535
Majority 0.229 0.248 0.260 0.239 0.249 0.245
Table 4: Three-way task weighted-average F1
Dataset: BEETLE SCIENTSBANK
Run UA UQ UA UQ UD Mean
CELI1 0.494 0.441 0.373 0.412 0.415 0.427
CNGL2 0.567 0.450 0.330 0.308 0.311 0.393
CoMeT1 0.715 0.466 0.640 0.380 0.404 0.521
ETS1 0.592 0.521 0.477 0.459 0.439 0.498
ETS2 0.710 0.585 0.643 0.389 0.367 0.539
LIMSIILES1 0.563 0.431 0.404 0.409 0.429 0.447
SoftCardinality1 0.596 0.439 0.555 0.469 0.486 0.509
UKP-BIU1 0.468 0.333 0.620 0.458 0.487 0.473
Median 0.580 0.446 0.516 0.411 0.422 0.485
Baselines:
Lexical 0.552 0.477 0.405 0.390 0.416 0.448
Majority 0.191 0.197 0.201 0.194 0.197 0.196
Table 5: Three-way task macro-average F1
the nine systems in the two-way task outperformed
the lexical baseline on the mean TS results. In fact,
the average performance fell below the lexical base-
line. The differences in the macro-average F1 be-
tween the top results on a SCIENTSBANK TS and
the corresponding lexical baselines were all statis-
tically significant. Two of the top results on BUA
were not statistically better than the lexical base-
line, and all systems performed below the baseline
on BUQ.
4.7 Discussion
All of the systems consistently outperformed the
most frequent class baseline. Beating the lexical
overlap baseline proved to be more challenging, be-
ing achieved by just over half of the results with
about half of those being statistically significant im-
provements. This underscores the fact that there is
still a considerable opportunity to improve student
269
Dataset: BEETLE SCIENTSBANK
Run UA UQ UA UQ UD Mean
CELI1 0.640 0.656 0.588 0.619 0.615 0.624
CNGL2 0.800 0.666 0.5911 0.561 0.556 0.635
CoMeT1 0.833 0.695 0.768 0.579 0.670 0.709
CU1 0.778 0.689 0.603 0.638 0.673 0.676
ETS1 0.802 0.720 0.705 0.688 0.683 0.720
ETS2 0.833 0.702 0.762 0.602 0.543 0.688
LIMSIILES1 0.723 0.641 0.583 0.629 0.648 0.645
SoftCardinality1 0.774 0.635 0.715 0.737 0.705 0.713
UKP-BIU1 0.608 0.481 0.726 0.669 0.6662 0.630
Median 0.778 0.666 0.705 0.629 0.666 0.676
Baselines:
Lexical 0.788 0.725 0.617 0.630 0.650 0.682
Majority 0.375 0.367 0.362 0.371 0.367 0.368
Table 6: Two-way task macro-average F1
response assessment systems.
The set of top performing systems on the
weighted F1 for a given TS were also always in the
top on the macro-average F1, but a small number of
additional systems joined the top performing set on
the macro-average F1. Specifically, one, three, and
two results joined the top set in the five-way, three-
way, and two-way tasks, respectively. In principle,
the metrics could differ substantially, because of the
treatment of minority classes, but in practice they
rarely did. Only one pair of participants swap adja-
cent TS mean rankings on the macro-average F1 rel-
ative to the weighted F1 on the two-way task. On the
five-way task, two pairs swap rankings and another
participant moved up two positions in the ranking,
ending at the median value.
Most (28/34) rank changes were only one position
and most (21/34) were in positions at or below the
median ranking. In the five-way task, a pair of sys-
tems, UKP-BIU1 and ETS1, had a meaningful per-
formance rank swap on the macro-average F1 rela-
tive to the weighted F1 on the UD test set. Specifi-
cally, UKP-BIU1 moved up four positions from rank
6, where it was not statistically better than the lexical
baseline, to the second best performance.
Not surprisingly, performance on UA was sub-
stantially higher than on UQ and UD, since the UA
is the only set which contains questions with exam-
ple answers in training data. Performance on BUA
was usually better than performance on SUA, most
likely because BUA contains more similar questions
and answers, focusing on a single science area, Elec-
tricity and Magnetism, compared to 12 distinct sci-
ence topics in SUA). In addition, the BEETLE study
participants may have used simpler language, since
they were aware that they were talking to a computer
system instead of writing down answers for human
teachers to assess as in SCIENTSBANK.
Performance on BUQ versus SUQ was much
more varied, presumably since there was no direct
training data for either TS. For the five-way task, the
best performance on the weighted F1 measure for
BUQ is 0.09 below the best result for BUA and the
analogous decrease from SUA to SUQ is 0.13, with
an additional 0.02 drop on SUD. On the two-way
task, the best weighted F1 for BUQ drops 0.11 from
the best BUA value, but the decrease from SUA to
SUQ is just 0.03, with another 0.03 drop to SUD.
While the drop in performance is fairly similar from
BUA to BUQ on all tasks and either metric, the de-
crease from SUA to SUQ seems to potentially be
dependent on the task, ranging from 0.13 on the five-
way task to 0.08 on the three-way task and 0.03 on
the two-way task.
5 Pilot Task on Partial Entailment
The SCIENTSBANK corpus was originally devel-
oped to assess student answers at a very fine-grained
level and contains additional annotations that break
down the answers into ?facets?, or low-level con-
cepts and relationships connecting them (hence-
forth, SCIENTSBANK Extra). This annotation aims
to support educational systems in recognizing when
specific parts of a reference answer are expressed
in the student answer, even if the reference answer
is not entailed as a whole (Nielsen et al, 2008b).
The task of recognizing such partial entailment rela-
tionships may also have various uses in applications
such as summarization or question answering, but it
has not been explored in previous RTE challenges.
Therefore, we proposed a pilot task on partial en-
tailment, in which systems are required to recognize
whether the semantic relation between specific parts
of the Hypothesis is expressed by the Text, directly
or by implication, even though entailment might not
be recognized for the Hypothesis as a whole, based
on the SCIENTSBANK facet annotation.
Each reference answer in SCIENTSBANK data is
broken down into facets, where a facet is a triplet
270
consisting of two key terms (both single words and
multi-words, e.g. carbon dioxide, each other, burns
out) and a relation linking them, as shown in Figure
2. The student answers were then annotated with
regards to each reference answer facet in order to
indicate whether the facet was (i) expressed, either
explicitly or by assumption or easy inference; (ii)
contradicted; or (iii) left unaddressed. Considering
the SCIENTSBANK reference answers as Hypothe-
ses, the facets capture their atomic components, and
facet annotations may correspond to the judgments
on the sub-parts of the H which are entailed by T.
We carried out a feasibility study to explore this
idea and to verify how well the facet annotations
align with traditional entailment judgments. We
focused on the reference answer facets labeled in
the gold standard annotation as Expressed or Unad-
dressed. The working hypothesis was that Expressed
labels assigned in SCIENTSBANK annotations cor-
responded to Entailed judgments in traditional tex-
tual entailment annotations, while Unaddressed la-
bels corresponded to No-entailment judgments.
Similarly to the feasibility study reported in Sec-
tion 3.2, we concluded that the correspondence be-
tween educational labels and entailment judgments
was not perfect due to the difference in educational
and textual entailment perspectives. Nevertheless,
the two classes of assessment appeared to be suffi-
ciently well correlated so as to offer a good testbed
for partial entailment in a natural setting.
5.1 Task Definition
Given (i) a text T, made up of a Question and a Stu-
dent Answer; (ii) a hypothesis H, i.e. the Reference
Answer for that question and (iii) a facet, i.e. a pair
of key terms in H, the task consists of determining
whether T expresses, either directly or by implica-
tion, the same relationship between the facet words
as in H. In other words, for each of H?s facets the
system assign one of the following judgments: Ex-
pressed, if the Student Answer expresses the same
relationship between the meaning of the facet terms
as in H; Unaddressed, if it does not.
Consider the example shown in Figure 2. For
facet 3, the system must decide whether the same re-
lation between the two terms ?contains? and ?seeds?
in H (the reference answer) is expressed, explicitly
or implicitly, in T (the combination of question and
student response). If the student answer is ?The part
of a plant you are observing is a fruit if it has seeds.?,
the answer to the question is ?yes? and the correct
judgment is ?Expressed?. But if the student says
?My rule is has to be sweet.?, T does not express
the same semantic relationship between ?contains?
and ?seeds? exhibited in H, thus the correct judgment
is ?Unaddressed?. Note that even though this is an
exercise in textual entailment, student response as-
sessment labels were used instead of traditional en-
tailment judgments, due to the partial mismatch be-
tween the two assessment classes found in the feasi-
bility study.
5.2 Dataset
We used a subset of the SCIENTSBANK Extra cor-
pus (Nielsen et al, 2008b) with the same problem-
atic questions filtered out as the main task (see Sec-
tion 3.3). We further filtered out all the student
answer facets which were labeled other than ?Ex-
pressed? or ?Unaddressed? in the gold standard an-
notation; the facets in which the relationship be-
tween the two key terms, as classified in the manual
annotation, proved to be problematic to define and
judge, namely Topic, Agent, Root, Cause, Quanti-
fier, Neg; and inter-propositional facets, i.e. facets
that expressed relations between higher-level propo-
sitions. Finally, the facet relations were removed
from the dataset, leaving the relationship between
the two facet terms unspecified so as to allow a more
fuzzy approach to the inference problem posed by
the exercise.
We used the same training/test split as reported in
Section 3.4. The training set created from the Train-
ing SCIENTSBANK Extra corpus contains 13,145
reference answer facets, 5,939 of which were la-
beled as ?Expressed? in the student answers and
7,206 as ?Unaddressed?. The Test set was created
from the SCIENTSBANK Extra unseen data and is
divided into the same subsets as the main task (Un-
seen Answers, Unseen Questions and Unseen Do-
mains). It contains 16,263 facets total, with 5,945
instances labeled as ?Expressed?, and 10,318 labeled
as ?Unaddressed?.
5.3 Evaluation Metrics and Baselines
The metrics used in the Pilot task were the same as in
the Main task, i.e. Overall Accuracy, Macroaverage
271
QUESTION: What is your ?rule? for deciding if the part of a plant you are observing is a fruit?
REFERENCE ANSWER: If a part of the plant contains seeds, that part is the fruit.
FACET 1: Relation NMod of Term1 part Term2 plant
FACET 2: Relation Theme Term1 contains Term2 part
FACET 3: Relation Material Term1 contains Term2 seeds
FACET 4: Relation Be Term1 fruit Term2 part
Figure 2: Example of facet annotations supporting the partial entailment task
Run UA UQ UD UA UQ UD
Weighted Averaged Macro Average
Run1 0.756 0.71 0.76 0.7370 0.686 0.755
Run 2 0.782 0.765 0.816 0.753 0.73 0.804
Run 3 0.744 0.733 0.77 0.719 0.7050 0.761
Baseline 0.54 0.547 0.478 0.402 0.404 0.384
Table 7: Weighted-average and macro-average F1 scores
(UA: Unseen Answers; UQ: Unseen Questions; UD Un-
seen Domains)
.
and Weighted Average Precision, Recall and F1, and
computed as described in Section 4.2. We used only
a majority class baseline, which labeled all facets
as ?Unaddressed?. Its performance is presented in
Section 5.4 jointly with the system results.
5.4 Participants and results
Only one participant, UKP-BIU, participated in the
Partial Entailment Pilot task. The UKP-BIU system
is a hybrid of two semantic relationship approaches,
namely (i) computing semantic textual similarity
by combining multiple content similarity measures
(Ba?r et al, 2012), and (ii) recognizing textual en-
tailment with BIUTEE (Stern and Dagan, 2011).
The two approaches are combined by generating in-
dicative features from each one and then applying
standard supervised machine learning techniques to
train a classifier. The system used several lexical-
semantic resources as part of the BIUTEE entail-
ment system, together with SCIENTSBANK depen-
dency parses and ESA semantic relatedness indexes
from Wikipedia.
The team submitted the maximum allowed of 3
runs. Table 7 shows Weighted Average and Macro
Average F1 scores respectively, also for the major-
ity baseline. The system outperformed the majority
baseline on both metrics. The best performance was
observed on Run 2, with the highest results on the
Unseen Domains test set.
6 Conclusions and Future Work
The Joint Student Response Analysis and 8th Rec-
ognizing Textual Entailment challenge has proven
to be a useful, interdisciplinary task using a realis-
tic dataset from the educational domain. In almost
all cases the best systems significantly outperformed
the lexical overlap baseline, sometimes by a large
margin, showing that computational linguistics ap-
proaches can contribute to educational tasks. How-
ever, the lexical baseline was not trivial to beat, par-
ticularly in the 2-way task. These results are consis-
tent with similar findings in previous RTE exercises.
Moreover, there is still significant room for improve-
ment in the absolute scores, reflecting the interesting
challenges that both educational data and RTE tasks
present to computational linguistics.
The educational setting places new stresses on
semantic inference technology because the educa-
tional notion of ?Expressed? and the RTE notion of
?Entailed? are slightly different. This raises the ed-
ucational question of whether RTE can work in this
setting, and the RTE question of whether this set-
ting is meaningful for evaluating RTE system per-
formance. The experimental results suggests that the
answer to both questions is ?yes?, a significant find-
ing for both educators and RTE technologists going
forward.
The Pilot task, aimed at exploring notions of par-
tial entailment, so far not explored in the series of
RTE challenges, has proven to be an interesting,
though challenging exercise. The novelty of the
task, namely performing textual entailment not on a
pair of full texts, but between a text and a hypothesis
consisting of a pair of words, may have represented
a more complex task than expected for some textual
entailment engines. Despite this, the encouraging
results obtained by the team which carried out the
exercise has shown that this partial entailment task
is worthy of further investigation.
272
Acknowledgments
The research reported here was supported by the US
ONR award N000141010085 and by the Institute of
Education Sciences, U.S. Department of Education,
through Grant R305A120808 to the University of
North Texas. The opinions expressed are those of
the authors and do not represent views of the Insti-
tute or the U.S. Department of Education. The RTE-
related activities were partially supported by the
Pascal-2 Network of Excellence, ICT-216886-NOE.
We would also like to acknowledge the contribution
of Alessandro Marchetti and Giovanni Moretti from
CELCT to the organization of the challenge.
References
Yigal Attali and Jill Burstein. 2006. Automated essay
scoring with e-rater v.2. The Journal of Technology,
Learning, and Assessment, 4(3), February.
Daniel Ba?r, Chris Biemann, Iryna Gurevych, and Torsten
Zesch. 2012. Ukp: Computing semantic textual sim-
ilarity by combining multiple content similarity mea-
sures. In Proceedings of the 6th International Work-
shop on Semantic Evaluation, held in conjunction with
the 1st Joint Conference on Lexical and Computa-
tional Semantics, pages 435?440, Montreal, Canada,
June.
Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo
Giampiccolo, and Bernando Magnini. 2009. The fifth
PASCAL recognizing textual entailment challenge. In
Proceedings of Text Analysis Conference (TAC) 2009.
Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa Trang
Dang, and Danilo Giampiccolo. 2010. Thesixth PAS-
CAL recognizing textual entailment challenge. In
Notebook papers and results, Text Analysis Confer-
ence (TAC).
Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa Trang
Dang, and Danilo Giampiccolo. 2011. The seventh
PASCAL recognizing textual entailment challenge. In
Notebook papers and results, Text Analysis Confer-
ence (TAC).
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The pascal recognising textual entailment chal-
lenge. In J. Quin?onero-Candela, I. Dagan, B. Magnini,
and F. d?Alche? Buc, editors, Machine Learning Chal-
lenges, volume 3944 of Lecture Notes in Computer
Science. Springer.
Myroslava O. Dzikovska, Johanna D. Moore, Natalie
Steinhauser, Gwendolyn Campbell, Elaine Farrow,
and Charles B. Callaway. 2010. Beetle II: a system
for tutoring and computational linguistics experimen-
tation. In Proc. of ACL 2010 System Demonstrations,
pages 13?18.
Myroslava O. Dzikovska, Rodney D. Nielsen, and Chris
Brew. 2012. Towards effective tutorial feedback for
explanation questions: A dataset and baselines. In
Proc. of 2012 Conference of NAACL: Human Lan-
guage Technologies, pages 200?210.
Danilo Giampiccolo, Hoa Trang Dang, Bernardo
Magnini, Ido Dagan, Elena Cabrio, and Bill Dolan.
2008. The fourth PASCAL recognizing textual entail-
ment challenge. In Proceedings of Text Analysis Con-
ference (TAC) 2008, Gaithersburg, MD, November.
Michael Glass. 2000. Processing language input in the
CIRCSIM-Tutor intelligent tutoring system. In Pa-
pers from the 2000 AAAI Fall Symposium, Available
as AAAI technical report FS-00-01, pages 74?79.
A. C. Graesser, K. Wiemer-Hastings, P. Wiemer-
Hastings, and R. Kreuz. 1999. Autotutor: A simu-
lation of a human tutor. Cognitive Systems Research,
1:35?51.
Xiangen Hu, Zhiqiang Cai, Max Louwerse, Andrew Ol-
ney, Phanni Penumatsa, and Art Graesser. 2003. A
revised algorithm for latent semantic analysis. In Pro-
ceedings of the 18th International Joint Conference on
Artificial intelligence (IJCAI?03), pages 1489?1491,
San Francisco, CA, USA. Morgan Kaufmann Publish-
ers Inc.
Pamela W. Jordan, Maxim Makatchev, and Kurt Van-
Lehn. 2004. Combining competing language under-
standing approaches in an intelligent tutoring system.
In Proc. of Intelligent Tutoring Systems Conference,
pages 346?357.
Pamela Jordan, Maxim Makatchev, Umarani Pap-
puswamy, Kurt VanLehn, and Patricia Albacete.
2006. A natural language tutorial dialogue system for
physics. In Proc. of 19th Intl. FLAIRS conference,
pages 521?527.
Claudia Leacock and Martin Chodorow. 2003. C-rater:
Automated scoring of short-answer questions. Com-
puters and the Humanities, 37(4):389?405.
Claudia Leacock, Martin Chodorow, Michael Gamon,
and Joel R. Tetreault. 2010. Automated Grammati-
cal Error Detection for Language Learners. Synthesis
Lectures on Human Language Technologies. Morgan
& Claypool Publishers.
Philip M. McCarthy, Vasile Rus, Scott A. Crossley,
Arthur C. Graesser, and Danielle S. McNamara. 2008.
Assessing forward-, reverse-, and average-entailment
indices on natural language input from the intelligent
tutoring system, iSTART. In Proc. of 21st Intl. FLAIRS
conference, pages 165?170.
Michael Mohler, Razvan Bunescu, and Rada Mihalcea.
2011. Learning to grade short answer questions using
273
semantic similarity measures and dependency graph
alignments. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies, pages 752?762, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
Jessica Nelson, Charles Perfetti, David Liben, and
Meredith Liben. 2012. Measures of text difficulty:
Testing their predictive value for grade levels and stu-
dent performance. Technical report, Student Achieve-
ment Partners. http://www.ccsso.org/
Documents/2012/Measures%20ofText%
20Difficulty_fina%l.2012.pdf.
Rodney D. Nielsen, Wayne Ward, and James H. Martin.
2008a. Learning to assess low-level conceptual under-
standing. In Proc. of 21st Intl. FLAIRS Conference,
pages 427?432.
Rodney D. Nielsen, Wayne Ward, James H. Martin, and
Martha Palmer. 2008b. Annotating students? under-
standing of science concepts. In Proceedings of the
Sixth International Language Resources and Evalua-
tion Conference, (LREC08), Marrakech, Morocco.
Sarah Petersen and Mari Ostendorf. 2009. A machine
learning approach to reading level assessment. Com-
puter, Speech and Language, 23(1):89?106.
Heather Pon-Barry, Brady Clark, Karl Schultz, Eliza-
beth Owen Bratt, and Stanley Peters. 2004. Advan-
tages of spoken language interaction in dialogue-based
intelligent tutoring systems. In Proc. of ITS-2004 Con-
ference, pages 390?400.
Stephen G Pulman and Jana Z Sukkarieh. 2005. Au-
tomatic short answer marking. In Proceedings of the
Second Workshop on Building Educational Applica-
tions Using NLP, pages 9?16, Ann Arbor, Michigan,
June. Association for Computational Linguistics.
Kathryn M. Sheehan, Irene Kostin, Yoko Futagi, and
Michael Flor. 2010. Generating automated text com-
plexity classifications that are aligned with targeted
text complexity standards. Technical Report RR-10-
28, Educational Testing Service.
Mark D. Shermis and Jill Burstein, editors. 2013. Hand-
book on Automated Essay Evaluation: Current Appli-
cations and New Directions. Routledge.
Asher Stern and Ido Dagan. 2011. A confidence
model for syntactically-motivated entailment proofs.
In Recent Advances in Natural Language Process-
ing (RANLP 2011), pages 455?462, Hissar, Bulgaria,
September.
Grigorios Tsoumakas, Ioannis Katakis, and Ioannis Vla-
havas. 2010. Mining multi-label data. In Oded
Maimon and Lior Rokach, editors, Data Mining
and Knowledge Discovery Handbook, pages 667?685.
Springer US.
Kurt VanLehn, Pamela Jordan, and Diane Litman. 2007.
Developing pedagogically effective tutorial dialogue
tactics: Experiments and a testbed. In Proc. of SLaTE
Workshop on Speech and Language Technology in Ed-
ucation, Farmington, PA, October.
Alexander Yeh. 2000. More accurate tests for the sta-
tistical significance of result differences. In Proceed-
ings of the 18th International Conference on Compu-
tational linguistics (COLING 2000), pages 947?953,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
274
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 285?289, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
UKP-BIU: Similarity and Entailment Metrics for Student Response Analysis
Torsten Zesch? Omer Levy? Iryna Gurevych? Ido Dagan?
? Ubiquitous Knowledge Processing Lab ? Natural Language Processing Lab
Computer Science Department Computer Science Department
Technische Universita?t Darmstadt Bar-Ilan University
Abstract
Our system combines text similarity measures
with a textual entailment system. In the main
task, we focused on the influence of lexical-
ized versus unlexicalized features, and how
they affect performance on unseen questions
and domains. We also participated in the pi-
lot partial entailment task, where our system
significantly outperforms a strong baseline.
1 Introduction
The Joint Student Response Analysis and 8th Rec-
ognizing Textual Entailment Challenge (Dzikovska
et al, 2013) brings together two important dimen-
sions of Natural Language Processing: real-world
applications and semantic inference technologies.
The challenge focuses on the domain of middle-
school quizzes, and attempts to emulate the metic-
ulous marking process that teachers do on a daily
basis. Given a question, a reference answer, and a
student?s answer, the task is to determine whether
the student answered correctly. While this is not
a new task in itself, the challenge focuses on em-
ploying textual entailment technologies as the back-
bone of this educational application. As a conse-
quence, we formalize the question ?Did the student
answer correctly?? as ?Can the reference answer be
inferred from the student?s answer??. This question
can (hopefully) be answered by a textual entailment
system (Dagan et al, 2009).
The challenge contains two tasks: In the main
task, the system must analyze each answer as a
whole. There are three settings, where each one de-
fines ?correct? in a different resolution. The highest-
resolution setting defines five different classes or
?correctness values?: correct, partially correct, con-
tradictory, irrelevant, non-domain. In the pilot task,
critical elements of the answer need to be analyzed
separately. Each such element is called a facet, and
is defined as a pair of words that are critical in an-
swering the question. As there is a substantial dif-
ference between the two tasks, we designed sibling
architectures for each task, and divide the main part
of the paper accordingly.
Our goal is to provide a robust architecture for stu-
dent response analysis, that can generalize and per-
form well in multiple domains. Moreover, we are
interested in evaluating how well general-purpose
technologies will perform in this setting. We there-
fore approach the challenge by combining two such
technologies: DKPro Similarity ?an extensive suite
of text similarity measures? that has been success-
fully applied in other settings like the SemEval 2012
task on semantic textual similarity (Ba?r et al, 2012a)
or reuse detection (Ba?r et al, 2012b).
BIUTEE, the Bar-Ilan University Textual Entail-
ment Engine (Stern and Dagan, 2011), which has
shown state-of-the-art performance on recognizing
textual entailment challenges. Our systems use both
technologies to extract features, and combine them
in a supervised model. Indeed, this approach works
relatively well (with respect to other entries in the
challenge), especially in unseen domains.
2 Background
2.1 Text Similarity
Text similarity is a bidirectional, continuous func-
tion which operates on pairs of texts of any length
and returns a numeric score of how similar one text
is to the other. In previous work (Mihalcea et al,
285
2006; Gabrilovich and Markovitch, 2007; Landauer
et al, 1998), only a single text similarity measure
has typically been applied to text pairs. However,
as recent work (Ba?r et al, 2012a; Ba?r et al, 2012b)
has shown, text similarity computation can be much
improved when a variety of measures are combined.
In recent years, UKP lab at TU Darmstadt has de-
veloped DKPro Similarity1, an open source toolkit
for analyzing text similarity. It is part of the
DKPro framework for natural language processing
(Gurevych et al, 2007). DKPro Similarity excels
at the tasks of measuring semantic textual simi-
larity (STS) and detecting text reuse (DTR), hav-
ing achieved the best performance in previous chal-
lenges (Ba?r et al, 2012a; Ba?r et al, 2012b).
2.2 Textual Entailment
The textual entailment paradigm is a generic frame-
work for applied semantic inference (Dagan et al,
2009). The most prevalent task of textual entailment
is to recognize whether the meaning of a target nat-
ural language statement (H for hypothesis) can be
inferred from another piece of text (T for text). Ap-
parently, this core task underlies semantic inference
in many text applications. The task of analyzing stu-
dent responses is one such example. By assigning
the student?s answer as T and the reference answer
as H , we are basically asking whether one can in-
fer the correct (reference) answer from the student?s
response. In recent years, Bar-Ilan University has
developed BIUTEE (Stern and Dagan, 2011), an ex-
tensive textual entailment recognition engine. BI-
UTEE tries to convert T (represented as a depen-
dency tree) to H . It does so by applying a series of
knowledge-based transformations, such as synonym
substitution, active-passive conversion, and more.
BIUTEE is publicly available as open source.2
3 Main Task
In this section, we explain how we approached the
main task, in which the system needs to analyze each
answer as a whole. After describing our system?s ar-
chitecture, we explain how we selected training data
for the different scenarios in the main task. We then
1code.google.com/p/dkpro-similarity-asl
2cs.biu.ac.il/?nlp/downloads/biutee
provide the details for each submitted run, and fi-
nally, our empirical results.
3.1 System Description
We build a system based on the Apache UIMA
framework (Ferrucci and Lally, 2004) and DKPro
Lab (Eckart de Castilho and Gurevych, 2011). We
use DKPro Core3 for preprocessing. Specifically,
we used the default DKPro segmenter, TreeTagger
POS tagger and chunker, Jazzy Spell Checker, and
the Stanford parser.4 We trained a supervised model
(Naive Bayes) using Weka (Hall et al, 2009) with
feature extraction based on clearTK (Ogren et al,
2008). The following features have been used:
BOW features Bag-of-word features are based on
the assumption that certain words need to appear in
a correct answer. We used a mixture of token uni-
grams, bigrams, and trigrams, where each n-gram is
a binary feature that can either be true or false for a
document.5 Additionally, we also used the number
of tokens in the student answer as another feature in
this group.
Syntactic Features We extend BOW features
with syntactic functions by adding dependency and
phrase n-grams. Dependency n-grams are combina-
tions of two tokens and their dependency relation.
Phrase n-grams are combinations of the main verb
and the noun phrase left and right of the verb. In
both cases, we use the 10 most frequent n-grams.
Basic Similarity Features This group of features
computes the similarity between the reference an-
swer and the student answer. In case there is more
than one reference answer, we compute all pairwise
similarity scores and add the minimum, maximum,
average, and median similarity.6
Semantic Similarity Features are very similar to
the basic similarity features, except that we use se-
mantic similarity measures in order to bridge a pos-
sible vocabulary gap between the student and refer-
ence answer. We use the ESA measure (Gabrilovich
3code.google.com/p/dkpro-core-asl/
4DKPro Core v1.4.0, TreeTagger models v20130204.0,
Stanford parser PCFG model v20120709.0.
5Using the 750 most frequent n-grams gave good results on
the training set, so we also used this number for the test runs.
6As basic similarity measures, we use greedy string tiling
(Wise, 1996) with n = 3, longest common subsequence and
longest common substring (Allison and Dix, 1986), and word
n-gram containment(Lyon et al, 2001) with n = 2.
286
and Markovitch, 2007) based on concept vectors
build from WordNet, Wiktionary, and Wikipedia.
Spelling Features As spelling errors might be in-
dicative of the answer quality, we use the number of
spelling errors normalized by the text length as an
additional feature.
Entailment Features We run BIUTEE (Stern and
Dagan, 2011) on the test instance (as T ) with each
reference answer (as H), which results in an array
of numerical entailment confidence values. If there
is more than one reference answer, we compute all
pairwise confidence scores and add the minimum,
maximum, average, and median confidence.
3.2 Data Selection Regime
There are three scenarios under which our system
is expected to perform. For each one, we chose (a-
priori) a different set of examples for training.
Unseen Answers Classify new answers to famil-
iar questions. Train on instances that have the same
question as the test instance.
Unseen Questions Classify new answers to un-
seen (but related) questions. Partition the questions
according to their IDs, creating sets of related ques-
tions, and then train on all the instances that share
the same partition as the test instance.
Unseen Domains Classify new answers to unseen
questions from unseen domains. Use all available
training data from the same dataset.
3.3 Submitted Runs
The runs represent the different levels of lexicaliza-
tion of the model which we expect to have strong
influence in each scenario:
Run 1 uses all features as described above. We
expect the BOW features to be helpful for the Un-
seen Answers setting, but to be misleading for un-
seen questions or domains, as the same word indi-
cating a correct answer for one question might cor-
respond to a wrong answer for another question.
Run 2 uses only non-lexicalized features, i.e. all
features except the BOW and syntactic features, in
order to assess the impact of the lexicalization that
overfits on the topic of the questions. We expect this
run to be less sensitive to the topic changes in the
Unseen Questions and Unseen Domains settings.
Run 3 uses only the basic similarity and the en-
tailment features. It should indicate the baseline per-
Unseen Unseen Unseen
Task Run Answers Questions Domains
2-way
1 .734 .678 .671
2 .665 .644 .677
3 .662 .625 .677
3-way
1 .670 .573 .572
2 .595 .561 .577
3 .574 .540 .576
5-way
1 .590 .376 .407
2 .495 .397 .371
3 .461 .394 .376
Table 1: Main task performance for the SciEntsBank test
set. We show weighted averageF1 for the three scenarios.
Cor. Par Con. Irr. Non.
Correct 903 463 164 309 78
Partially Correct 219 261 93 333 80
Contradictory 61 126 91 103 36
Irrelevant 209 229 119 476 189
Non-Domain 0 0 0 2 18
Table 2: Confusion matrix of Run 1 in the 5-way Unseen
Domains scenario. The vertical axis is the real class, the
horizontal axis is the predicted class.
formance that can be expected without targeting the
system towards a certain topic.
3.4 Empirical Results
Table 1 shows the F1-measure (weighted average)
of the three runs. As was expected for the Unseen
Answers scenario, Run 1 using a lexicalized model
outperformed the other two runs. However, in the
other scenarios Run 1 is not significantly better, as
lexicalized features do not have the same impact if
the question or the domain changes.
Table 2 shows the confusion matrix of Run 1 in
the 5-way Unseen Domains scenario. The Correct
category was classified quite reliably, but the Irrele-
vant category was especially hard. While the refer-
ence answer provides some clues for what is correct
or incorrect, the range of things that are ?irrelevant?
for a given question is potentially very big and thus
cannot be easily learned. We also see that the system
ability to distinguish Correct and Partially Correct
answers need to be improved.
It is difficult to provide an exact assessment of our
system?s performance (with respect to other systems
in the challenge), since there are multiple tasks, sce-
287
narios, datasets, and even metrics. However, we can
safely say that our system performed above average
in most settings, and showed competitive results in
the Unseen Domains scenario.
4 Pilot Task
In the pilot task each facet needs to be analyzed sep-
arately, which requires some changes in the system
architecture.
4.1 System Description
We segmented and lemmatized the input data using
the default DKPro segmenter and the TreeTagger
lemmatizer. The partial entailment system is com-
posed of three methods: Exact, WordNet, and BI-
UTEE. These were combined in different combina-
tions to form the different runs.
Exact In this baseline method, we represent a
student answer as a bag-of-words containing all to-
kens and lemmas appearing in the text. Lemmas
are used to account for minor morphological dif-
ferences, such as tense or plurality. We then check
whether both facet words appear in the set.
WordNet checks whether both facet words, or
their semantically related words, appear in the stu-
dent?s answer. We use WordNet (Fellbaum, 1998)
with the Resnik similarity measure (Resnik, 1995)
and count a facet term as matched if the similarity
score exceeds a certain threshold (0.9, empirically
determined on the training set).
BIUTEE processes dependency trees instead of
bags of words. We therefore added a pre-processing
stage that extracts a path in the dependency parse
that represents the facet. This is done by first pars-
ing the entire reference answer, and then locating the
two nodes mentioned in the facet. We then find their
lowest common ancestor (LCA), and extract the path
from the facet?s first word to the second through the
LCA. BIUTEE can now be given the student an-
swer and the pre-processed facet, and try to recog-
nize whether the former entails the latter.
4.2 Submitted Runs
In preliminary experiments using the provided train-
ing data, we found that the very simple Exact Match
baseline performed surprisingly well, with 0.96 pre-
cision and 0.32 recall on positive class instances (ex-
pressed facets). We therefore decided to use this fea-
Unseen Unseen Unseen
Answers Questions Domains
Baseline .670 .688 .731
Run 1 .756 .710 .760
Run 2 .782 .765 .816
Run 3 .744 .733 .770
Table 3: Pilot task performance across different scenar-
ios. The scores are F1-measures (weighted average).
ture as an initial filter, and employ the others for
classifying the ?harder? cases. Training BIUTEE
only on these cases, while dismissing easy ones, im-
proved our system?s performance significantly.
Run 1: Exact OR WordNet This is essentially
just the WordNet feature on its own, because every
instance that Exact classifies as positive is also pos-
itive by WordNet.
Run 2: Exact OR (BIUTEE AND WordNet) If
the instance is non-trivial, this configuration requires
that both BIUTEE and WordNet Match agree on pos-
itive classification. Equivalent to the majority rule.
Run 3: Exact OR BIUTEE BIUTEE increases
recall of expressed facets at the expense of precision.
4.3 Empirical Results
Table 3 shows the F1-measure (weighted average) of
each run in each scenario, including Exact Match as
a quite strong baseline. In the majority of cases, Run
2 that combines entailment and WordNet-based lex-
ical matching, significantly outperformed the other
two. It is interesting to note that the systems? perfor-
mance does not degrade in ?harder? scenarios; this is
a result of the non-lexicalized nature of our methods.
Unfortunately, our system was the only submission
in this track, so we do not have any means to perform
relative comparison.
5 Conclusion
We combined semantic textual similarity with tex-
tual entailment to solve the problem of student re-
sponse analysis. Though our features were not tai-
lored for this task, they proved quite indicative, and
adapted well to unseen domains. We believe that ad-
ditional generic features and knowledge resources
are the best way to improve on our results, while
retaining the same robustness and generality as our
current architecture.
288
Acknowledgements
This work has been supported by the Volkswagen Foundation as
part of the Lichtenberg-Professorship Program under grant No.
I/82806, and by the European Community?s Seventh Frame-
work Programme (FP7/2007-2013) under grant agreement no.
287923 (EXCITEMENT). We would like to thank the Minerva
Foundation for facilitating this cooperation with a short term
research grant.
References
Lloyd Allison and Trevor I. Dix. 1986. A bit-string
longest-common-subsequence algorithm. Information
Processing Letters, 23:305?310.
Daniel Ba?r, Chris Biemann, Iryna Gurevych, and Torsten
Zesch. 2012a. UKP: Computing semantic textual sim-
ilarity by combining multiple content similarity mea-
sures. In Proceedings of the 6th International Work-
shop on Semantic Evaluation and the 1st Joint Confer-
ence on Lexical and Computational Semantics, pages
435?440, June.
Daniel Ba?r, Torsten Zesch, and Iryna Gurevych. 2012b.
Text reuse detection using a composition of text sim-
ilarity measures. In Proceedings of the 24th In-
ternational Conference on Computational Linguistics
(COLING 2012), pages 167?184, December.
Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan Roth.
2009. Recognizing textual entailment: Rationale,
evaluation and approaches. Natural Language Engi-
neering, 15(4):i?xvii.
Myroslava O. Dzikovska, Rodney Nielsen, Chris Brew,
Claudia Leacock, Danilo Giampiccolo, Luisa Ben-
tivogli, Peter Clark, Ido Dagan, and Hoa Trang Dang.
2013. Semeval-2013 task 7: The joint student re-
sponse analysis and 8th recognizing textual entailment
challenge. In *SEM 2013: The First Joint Conference
on Lexical and Computational Semantics, Atlanta,
Georgia, USA, 13-14 June. Association for Compu-
tational Linguistics.
Richard Eckart de Castilho and Iryna Gurevych. 2011.
A lightweight framework for reproducible parame-
ter sweeping in information retrieval. In Proceed-
ings of the 2011 workshop on Data infrastructurEs for
supporting information retrieval evaluation (DESIRE
?11), New York, NY, USA. ACM.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge, MA.
David Ferrucci and Adam Lally. 2004. UIMA: An ar-
chitectural approach to unstructured information pro-
cessing in the corporate research environment. Natu-
ral Language Engineering, 10(3-4):327?348.
Evgeniy Gabrilovich and Shaul Markovitch. 2007. Com-
puting semantic relatedness using Wikipedia-based
explicit semantic analysis. In Proceedings of the 20th
International Joint Conference on Artificial Intelli-
gence (IJCAI 2007), pages 1606?1611.
Iryna Gurevych, Max Mu?hlha?user, Christof Mu?ller,
Ju?rgen Steimle, Markus Weimer, and Torsten Zesch.
2007. Darmstadt Knowledge Processing Repository
based on UIMA. In Proceedings of the 1st Work-
shop on Unstructured Information Management Ar-
chitecture at Biannual Conference of the Society for
Computational Linguistics and Language Technology,
Tu?bingen, Germany, April.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA Data Mining Software: An Update.
SIGKDD Explorations, 11(1).
Thomas K. Landauer, Peter W. Foltz, and Darrell La-
ham. 1998. An introduction to latent semantic analy-
sis. Discourse Processes, 25(2&3):259?284.
Caroline Lyon, James Malcolm, and Bob Dickerson.
2001. Detecting short passages of similar text in
large document collections. In Proceedings of the
6th Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP 2001), pages 118?125,
Pittsburgh, PA USA.
Rada Mihalcea, Courtney Corley, and Carlo Strapparava.
2006. Corpus-based and knowledge-based measures
of text semantic similarity. In Proceedings of the 21st
National Conference on Artificial Intelligence, pages
775?780, Boston, MA.
Philip V. Ogren, Philipp G. Wetzler, and Steven Bethard.
2008. ClearTK: A UIMA Toolkit for Statistical Nat-
ural Language Processing. In Towards Enhanced
Interoperability for Large HLT Systems: UIMA for
NLP workshop at Language Resources and Evaluation
Conference (LREC).
Philip Resnik. 1995. Using information content to evalu-
ate semantic similarity in a taxonomy. In Proceedings
of the 14th International Joint Conference on Artificial
Intelligence (IJCAI 1995), pages 448?453.
Asher Stern and Ido Dagan. 2011. A confidence
model for syntactically-motivated entailment proofs.
In Proceedings of the 8th International Conference
on Recent Advances in Natural Language Processing
(RANLP 2011), pages 455?462.
Michael J. Wise. 1996. YAP3: Improved detection of
similarities in computer program and other texts. In
Proceedings of the 27th SIGCSE Technical Sympo-
sium on Computer Science Education (SIGCSE 1996),
pages 130?134, Philadelphia, PA.
289
Proceedings of the TextInfer 2011 Workshop on Textual Entailment, EMNLP 2011, pages 10?19,
Edinburgh, Scotland, UK, July 30, 2011. c?2011 Association for Computational Linguistics
Towards a Probabilistic Model for Lexical Entailment
Eyal Shnarch
Computer Science Department
Bar-Ilan University
Ramat-Gan, Israel
shey@cs.biu.ac.il
Jacob Goldberger
School of Engineering
Bar-Ilan University
Ramat-Gan, Israel
goldbej@eng.biu.ac.il
Ido Dagan
Computer Science Department
Bar-Ilan University
Ramat-Gan, Israel
dagan@cs.biu.ac.il
Abstract
While modeling entailment at the lexical-level
is a prominent task, addressed by most textual
entailment systems, it has been approached
mostly by heuristic methods, neglecting some
of its important aspects. We present a prob-
abilistic approach for this task which cov-
ers aspects such as differentiating various re-
sources by their reliability levels, considering
the length of the entailed sentence, the num-
ber of its covered terms and the existence of
multiple evidence for the entailment of a term.
The impact of our model components is vali-
dated by evaluations, which also show that its
performance is in line with the best published
entailment systems.
1 Introduction
Textual Entailment was proposed as a generic
paradigm for applied semantic inference (Dagan et
al., 2006). Given two textual fragments, termed hy-
pothesis (H) and text (T ), the text is said to textually
entail the hypothesis (T?H) if a person reading the
text can infer the meaning of the hypothesis. Since it
was first introduced, the six rounds of the Recogniz-
ing Textual Entailment (RTE) challenges1 have be-
come a standard benchmark for entailment systems.
Entailment systems apply various techniques to
tackle this task, including logical inference (Tatu
and Moldovan, 2007; MacCartney and Manning,
2007), semantic analysis (Burchardt et al, 2007)
and syntactic parsing (Bar-Haim et al, 2008; Wang
1http://www.nist.gov/tac/
et al, 2009). Inference at these levels usually re-
quires substantial processing and resources, aim-
ing at high performance. Nevertheless, simple lex-
ical level entailment systems pose strong baselines
which most complex entailment systems did not out-
perform (Mirkin et al, 2009a; Majumdar and Bhat-
tacharyya, 2010). Additionally, within a complex
system, lexical entailment modeling is one of the
most effective component. Finally, the simpler lex-
ical approach can be used in cases where complex
systems cannot be used, e.g. when there is no parser
for a targeted language.
For these reasons lexical entailment systems are
widely used. They derive sentence-level entailment
decision base on lexical-level entailment evidence.
Typically, this is done by quantifying the degree of
lexical coverage of the hypothesis terms by the text
terms (where a term may be multi-word). A hy-
pothesis term is covered by a text term if either they
are identical (possibly at the stem or lemma level)
or there is a lexical entailment rule suggesting the
entailment of the former by the latter. Such rules
are derived from lexical semantic resources, such
as WordNet (Fellbaum, 1998), which capture lexi-
cal entailment relations.
Common heuristics for quantifying the degree of
coverage are setting a threshold on the percentage
of coverage of H?s terms (Majumdar and Bhat-
tacharyya, 2010), counting the absolute number of
uncovered terms (Clark and Harrison, 2010), or ap-
plying an Information Retrieval-style vector space
similarity score (MacKinlay and Baldwin, 2009).
Other works (Corley and Mihalcea, 2005; Zanzotto
and Moschitti, 2006) have applied heuristic formu-
10
las to estimate the similarity between text fragments
based on a similarity function between their terms.
The above mentioned methods do not capture sev-
eral important aspects of entailment. Such aspects
include the varying reliability levels of entailment
resources and the impact of rule chaining and multi-
ple evidence on entailment likelihood. An additional
observation from these and other systems is that
their performance improves only moderately when
utilizing lexical-semantic resources2.
We believe that the textual entailment field would
benefit from more principled models for various en-
tailment phenomena. In this work we formulate a
concrete generative probabilistic modeling frame-
work that captures the basic aspects of lexical entail-
ment. A first step in this direction was proposed in
Shnarch et al (2011) (a short paper), where we pre-
sented a base model with a somewhat complicated
and difficult to estimate extension to handle cover-
age. This paper extends that work to a more mature
model with new extensions.
We first consider the ?logical? structure of lexical
entailment reasoning and then interpret it in proba-
bilistic terms. Over this base model we suggest sev-
eral extensions whose significance is then assessed
by our evaluations. Learning the parameters of a
lexical model poses a challenge since there are no
lexical-level entailment annotations. We do, how-
ever, have sentence-level annotations available for
the RTE data sets. To bridge this gap, we formu-
late an instance of the EM algorithm (Dempster et
al., 1977) to estimate hidden lexical-level entailment
parameters from sentence-level annotations.
Overall, we suggest that the main contribution of
this paper is in presenting a probabilistic model for
lexical entailment. Such a model can better integrate
entailment indicators and has the advantage of being
able to utilize well-founded probabilistic methods
such as the EM algorithm. Our model?s performance
is in line with the best entailment systems, while
opening up directions for future improvements.
2 Background
We next review several entailment systems, mostly
those that work at the lexical level and in particular
2See ablation tests reports in http://aclweb.org/aclwiki/ in-
dex.php?title=RTE Knowledge Resources#Ablation Tests
those with which we compare our results on the RTE
data sets.
The 5th Recognizing Textual Entailment chal-
lenge (RTE-5) introduced a new pilot task (Ben-
tivogli et al, 2009) which became the main task in
RTE-6 (Bentivogli et al, 2010). In this task the goal
is to find all sentences that entail each hypothesis in
a given document cluster. This task?s data sets re-
flect a natural distribution of entailments in a corpus
and demonstrate a more realistic scenario than the
earlier RTE challenges.
As reviewed in the following paragraphs there are
several characteristic in common to most entailment
systems: (1) lexical resources have a minimal im-
pact on their performance, (2) they heuristically uti-
lize lexical resources, and (3) there is no principled
method for making the final entailment decision.
The best performing system of RTE-5 was pre-
sented by Mirkin et. al (2009a). It applies super-
vised classifiers over a parse tree representations to
identify entailment. They reported that utilizing lex-
ical resources only slightly improved their perfor-
mance.
MacKinlay and Baldwin (2009) presented the
best lexical-level system at RTE-5. They use a vec-
tor space method to measure the lexical overlap be-
tween the text and the hypothesis. Since usually
texts of RTE are longer than their corresponding hy-
potheses, the standard cosine similarity score came
out lower than expected. To overcome this prob-
lem they suggested a simple ad-hoc variant of the
cosine similarity score which removed from the text
all terms which did not appear in the correspond-
ing hypothesis. While this heuristic improved per-
formance considerably, they reported a decrease in
performance when utilizing synonym and derivation
relations from WordNet.
On the RTE-6 data set, the syntactic-based sys-
tem of Jia et. al (2010) achieved the best results,
only slightly higher than the lexical-level system
of (Majumdar and Bhattacharyya, 2010). The lat-
ter utilized several resources for matching hypoth-
esis terms with text terms: WordNet, VerbOcean
(Chklovski and Pantel, 2004), utilizing two of its
relations, as well as an acronym database, num-
ber matching module, co-reference resolution and
named entity recognition tools. Their final entail-
ment decision was based on a threshold over the
11
number of matched hypothesis terms. They found
out that hypotheses of different length require dif-
ferent thresholds.
While the above systems measure the number of
hypothesis terms matched by the text, Clark and
Harrison (2010) based their entailment decision on
the number of mismatched hypothesis terms. They
utilized both WordNet and the DIRT paraphrase
database (Lin and Pantel, 2001). With WordNet,
they used one set of relations to identify the concept
of a term while another set of relations was used to
identify entailment between concepts. Their results
were inconclusive about the overall effect of DIRT
while WordNet produced a net benefit in most con-
figurations. They have noticed that setting a global
threshold for the entailment decision, decreased per-
formance for some topics of the RTE-6 data set.
Therefore, they tuned a varying threshold for each
topic based on an idiosyncracy of the data, by which
the total number of entailments per topic is approxi-
mately a constant.
Glickman et al (2005) presented a simple model
that recasted the lexical entailment task as a variant
of text classification and estimated entailment prob-
abilities solely from co-occurrence statistics. Their
model did not utilize any lexical resources.
In contrary to these systems, our model shows
improvement when utilizing high quality resources
such as WordNet and the CatVar (Categorial Varia-
tion) database (Habash and Dorr, 2003). As Majum-
dar and Bhattacharyya (2010), our model considers
the impact of hypothesis length, however it does not
require the tuning of a unique threshold for each
length. Finally, most of the above systems do not
differentiate between the various lexical resources
they use, even though it is known that resources re-
liability vary considerably (Mirkin et al, 2009b).
Our probabilistic model, on the other hand, learns
a unique reliability parameter for each resource it
utilizes. As mentioned above, this work extends the
base model in (Shnarch et al, 2011), which is de-
scribed in the next section.
3 A Probabilistic Model
We aim at obtaining a probabilistic score for the like-
lihood that the hypothesis terms are entailed by the
terms of the text. There are several prominent as-
cro
wd 
 
sur
rou
nd  
Jag
uar
Text Hyp
othe
sis
h jh j
h nh n
t 1t 1
t it i
t mt m
t'
Res
ourc
e 1
chain
yy
OR
OR
50 p
eop
le s
urr
ou
nd c
ar
OR
OR
so
cial gro
up
Res
ourc
e 1 O
R
Res
ourc
e 1
Res
ourc
e 3
Res
ourc
e 2 h 1h 1
Res
ourc
e 1
MA
TCH
MA
TCH
Res
ourc
e 3
Res
ourc
e 2
UNC
OVE
RED
Figure 1: Left: the base model of entailing a hypothesis from
a text; Right: a concrete example for it (stop-words removed).
Edges in the upper part of the diagram represent entailment
rules. Rules compose chains through AND gates (omitted for
visual clarity). Chains are gathered by OR gates to entail terms,
and the final entailment decision y is the result of their AND
gate.
pects of entailment, mostly neglected by previous
lexical methods, which our model aims to capture:
(1) the reliability variability of different lexical re-
sources; (2) the effect of the length of transitive rule
application chain on the likelihood of its validity;
and (3) addressing cases of multiple entailment evi-
dence when entailing a term.
3.1 The Base Model
Our base model follows the one presented in
(Shnarch et al, 2011), which is described here in
detail to make the current paper self contained.
3.1.1 Entailment generation process
We first specify the process by which a decision
of lexical entailment between T andH using knowl-
edge resources should be determined, as illustrated
in Figure 1 (a general description on the left and
a concrete example on the right). There are two
ways by which a term h ? H is entailed by a term
t ? T . A direct MATCH is the case in which t and
h are identical terms (possibly at the stem or lemma
level). Alternatively, lexical entailment can be es-
tablished based on knowledge of entailing lexical-
12
semantic relations, such as synonyms, hypernyms
and morphological derivations, available in lexical
resources. These relations provide lexical entail-
ment rules, e.g. Jaguar ? car. We denote the re-
source which provided the rule r by R(r).
It should be noticed at this point that such rules
specify a lexical entailment relation that might hold
for some (T,H) pairs but not necessarily for all
pairs, e.g. the rule Jaguar ? car does not hold
in the wildlife context. Thus, the application of an
available rule to infer lexical entailment in a given
(T,H) pair might be either valid or invalid. We note
here the difference between covering a term and en-
tailing it. A term is covered when the available re-
sources suggest its entailment. However, since a rule
application may be invalid for the particular (T,H)
context, a term is entailed only if there is a valid rule
application from T to it.
Entailment is a transitive relation, therefore rules
may compose transitive chains that connect t to h
via intermediate term(s) t? (e.g. crowd ? social
group ? people). For a chain to be valid for the
current (T,H) pair, all its composing rule applica-
tions should be valid for this pair. This corresponds
to a logical AND gate (omitted in Figure 1 for visual
clarity) which takes as input the validity values (1/0)
of the individual rule applications.
Next, multiple chains may connect t to h (as for
ti and hj in Figure 1) or connect several terms in
T to h (as t1 and ti are indicating the entailment of
hj in Figure 1), thus providing multiple evidence for
h?s entailment. For a term h to be entailed by T it
is enough that at least one of the chains from T to
h would be valid. This condition is realized in the
model by an OR gate. Finally, for T to lexically en-
tail H it is usually assumed that every h?H should
be entailed by T (Glickman et al, 2006). Therefore,
the final decision follows an AND gate combining
the entailment decisions for all hypothesis terms.
Thus, the 1-bit outcome of this gate y corresponds
to the sentence-level entailment status.
3.1.2 Probabilistic Setting
When assessing entailment for (T,H) pair, we do
not know for sure which rule applications are valid.
Taking a probabilistic perspective, we assume a pa-
rameter ?R for each resourceR, denoting its reliabil-
ity, i.e. the prior probability that applying a rule from
R for an arbitrary (T,H) pair corresponds to valid
entailment3. Under this perspective, direct MATCHs
are considered as rules coming from a special ?re-
source?, for which ?MATCH is expected to be close to
1. Additionally, there could be a term h which is not
covered by any of the resources at hand, whose cov-
erage is inevitably incomplete. We assume that each
such h is covered by a single rule coming from a
dummy resource called UNCOVERED, while expect-
ing ?UNCOVERED to be relatively small. Based on the
?R values we can now estimate, for each entailment
inference step in Figure 1, the probability that this
step is valid (the corresponding bit is 1).
Equations (1) - (3) correspond to the three steps in
calculating the probability for entailing a hypothesis.
p(t
c
?? h) =
?
r?c
p(L
r
?? R) =
?
r?c
?R(r) (1)
p(T?h) =1?p(T9h)=1?
?
c?C(h)
[1?p(t
c
?? h)] (2)
p(T?H) =
?
h?H
p(T?h) (3)
First, Eq. (1) specifies the probability of a partic-
ular chain c, connecting a text term t to a hypothesis
term h, to correspond to a valid entailment between
t and h. This event is denoted by t
c
??h and its prob-
ability is the joint probability that the applications
of all rules r ? c are valid. Note that every rule r
in a chain c connects two terms, its left-hand-side L
and its right-hand-side R. The left-hand-side of the
first rule in c is t? T and the right-hand-side of the
last rule in it is h ? H . Let us denote the event of
a valid rule application by L
r
??R. Since a-priori a
rule r is valid with probability ?R(r), and assuming
independence of all r?c, we obtain Eq. (1).
Next, Eq. (2) utilizes Eq. (1) to specify the prob-
ability that T entails h (at least by one chain). Let
C(h) denote the set of chains which suggest the en-
tailment of h. The requested probability is equal to
1 minus the probability of the complement event,
that is, T does not entail h by any chain. The lat-
ter probability is the product of probabilities that all
3Modeling a conditional probability for the validity of r,
which considers contextual aspects of r?s validity in the current
(T,H) context, is beyond the scope of this paper (see discus-
sion in Section 6)
13
chains c?C(h) are not valid (again assuming inde-
pendence of chains).
Finally, Eq. (3) gives the probability that T entails
all of H (T ? H), assuming independence of H?s
terms. This is the probability that every h ? H is
entailed by T , as specified by Eq. (2).
Altogether, these formulas fall out of the standard
probabilistic estimate for the output of AND and OR
gates when assuming independence amongst their
input bits.
As can be seen, the base model distinguishes
varying resource reliabilities, as captured by ?R, de-
creases entailment probability as rule chain grows,
having more elements in the product of Eq. (1), and
increases it when entailment of a term is supported
by multiple chains with more inputs to the OR gate.
Next we describe two extensions for this base model
which address additional important phenomena of
lexical entailment.
3.2 Relaxing the AND Gate
Based on term-level decisions for the entailment of
each h ? H , the model has to produce a sentence-
level decision of T ? H . In the model described so
far, for T to entailH it must entail all its terms. This
demand is realized by the AND gate at the bottom of
Figure 1. In practice, this demand is too strict, and
we would like to leave some option for entailing H
even if not every h?H is entailed. Thus, it is desired
to relax this strict demand enforced by the AND gate
in the model.
OR
AND
b1
OR
xn
bn
x1
Noisy-AND
y
Figure 2: A noisy-AND gate
The Noisy-AND model (Pearl, 1988), depicted in
Figure 2, is a soft probabilistic version of the AND
gate, which is often used to describe the interaction
between causes and their common effect. In this
variation, each one of the binary inputs b1, ..., bn of
the AND gate is first joined with a ?noise? bit xi by
an OR gate. Each ?noise? bit is 1 with probability p,
which is the parameter of the gate. The output bit y
is defined as:
y = (b1 ? x1) ? (b2 ? x2) ? ? ? ? ? (bn ? xn)
and the conditional probability for it to be 1 is:
p(y = 1|b1, ..., bn, n) =
n?
i=1
p(1?bi) = p(n?
?
i bi)
If all the binary input values are 1, the output is de-
terministically 1. Otherwise, the probability that the
output is 1 is proportional to the number of ones in
the input, where the distribution depends on the pa-
rameter p. In case p = 0 the model reduces to the
regular AND.
In our model we replace the final strict AND with
a noisy-AND, thus increasing the probability of T to
entail H , to account for the fact that sometimes H
might be entailed from T even though some h ?H
is not directly entailed.
The input size n for the noisy-AND is the length
of the hypotheses and therefore it varies from H to
H . Had we used the same model parameter p for all
lengths, the probability to output 1 would have de-
pended solely on the number of 0 bits in the input
without considering the number of ones. For exam-
ple, the probability to entail a hypothesis with 10
terms given that 8 of them are entailed by T (and 2
are not) is p2. The same probability is obtained for a
hypothesis of length 3 with a single entailed term.
We, however, expect the former to have a higher
probability since a larger portion of its terms is en-
tailed by T .
There are many ways to incorporate the length of
a hypothesis into the noisy-AND model in order to
normalize its parameter. The approach we take is
defining a separate parameter pn for each hypothesis
length n such that pn = ?
1
n
NA, where ?NA becomes
the underlying parameter value of the noisy-AND,
i.e.
p(y = 1|b1, ..., bn, n) = p
(n?
?
bi)
n = ?
n?
?
bi
n
NA
This way, if non of the hypothesis terms is entailed,
the probability for its entailment is ?NA, indepen-
dent of its length:
p(y = 1|0, 0, ..., 0, n) = pnn = ?NA
14
As can be seen from Figure 1, replacing the final
AND gate by a noisy-AND gate is equivalent to
adding an additional chain to the OR gate of each
hypothesis term. Therefore we update Eq. (2) to:
p(T ? h) =1? p(T 9 h)
=1? [(1? ?
1
n
NA) ?
?
c?C(h)
[1? p(t
c
?? h)]]
(2?)
In the length-normalized noisy-AND model the
value of the parameter p becomes higher for longer
hypotheses. This increases the probability to entail
such hypotheses, compensating for the lower proba-
bility to strictly entail all of their terms.
3.3 Considering Coverage Level
The second extension of the base model follows our
observation that the prior validity likelihood for a
rule application, increases as more of H?s terms are
covered by the available resources. In other words,
if we have a hypothesis H1 with k covered terms
and a hypothesis H2 in which only j < k terms are
covered, then an arbitrary rule application for H1 is
more likely to be valid than an arbitrary rule appli-
cation for H2.
We chose to model this phenomenon by normal-
izing the reliability ?R of each resource according
to the number of covered terms in H . The normal-
ization is done in a similar manner to the length-
normalized noisy-AND described above, obtaining
a modified version of Eq. (1):
p(t
c
?? h) =
?
r?c
?
1
#covered
R(r) (1
?)
As a results, the larger the number of covered terms
is, the larger ?R values our model uses and, in total,
the entailment probability increases.
To sum up, we have presented the base model,
providing a probabilistic estimate for the entailment
status in our generation process specified in 3.1.
Two extensions were then suggested: one that re-
laxes the strict AND gate and normalizes this re-
laxation by the length of the hypothesis; the second
extension adjusts the validity of rule applications as
a function of the number of the hypothesis covered
terms. Overall, our full model combines both exten-
sions over the base probabilistic model.
4 Parameter Estimation
The difficulty in estimating the ?R values from train-
ing data arises because these are term-level param-
eters while the RTE-training entailment annotation
is given for the sentence-level, each (T,H) pair in
the training is annotated as either entailing or not.
Therefore, we use an instance of the EM algorithm
(Dempster et al, 1977) to estimate these hidden pa-
rameters.
4.1 E-Step
In the E-step, for each application of a rule r in a
chain c for h?H in a training pair (T,H), we com-
pute whcr(T,H), the posterior probability that the
rule application was valid given the training annota-
tion:
whcr(T,H) =
?
?
?
p(L
r
??R|T?H) if T?H
p(L
r
??R|T9H) if T9H
(4)
where the two cases refer to whether the training pair
is annotated as entailing or non-entailing. For sim-
plicity, we write whcr when the (T,H) context is
clear.
The E-step can be efficiently computed using
dynamic programming as follows; For each train-
ing pair (T,H) we first compute the probability
p(T ? H) and keep all the intermediate computa-
tions (Eq. (1)- (3)). Then, the two cases of Eq. (4),
elaborated next, can be computed from these expres-
sions. For computing Eq. (4) in the case that T?H
we have:
p(L
r
?? R|T?H) = p(L
r
?? R|T ? h) =
p(T?h|L
r
?? R)p(L
r
??R)
p(T?h)
The first equality holds since when T entails H ev-
ery h ? H is entailed by it. Then we apply Bayes?
rule. We have already computed the denominator
(Eq. (2)), p(L
r
?? R) ? ?R(r) and it can be shown
4
that:
p(T?h|L
r
??R) = 1?
p(T9h)
1? p(t
c
??h)
? (1?
p(t
c
??h)
?R(r)
)
(5)
4The first and second denominators reduce elements from
the products in Eq. 2 and Eq. 1 correspondingly
15
where c is the chain which contains the rule r.
For computing Eq. (4), in the second case, that
T9H , we have:
p(L
r
??R|T9H) =
p(T9H|L r??R)p(L r??R)
p(T9H)
In analogy to Eq. (5) it can be shown that
p(T9H|L r??R) = 1?
p(T?H)
p(T?h)
?p(T?h|L
r
??R)
(6)
while the expression for p(T?h|L
r
??R) appears in
Eq. (5).
This efficient computation scheme is an instance
of the belief-propagation algorithm (Pearl, 1988) ap-
plied to the entailment process, which is a loop-free
directed graph (Bayesian network).
4.2 M-Step
In the M-step we need to maximize the EM auxiliary
function Q(?) where ? is the set of all resources re-
liability values. Applying the derivation of the aux-
iliary function to our model (first without the exten-
sions) we obtain:
Q(?) =
?
T,H
?
h?H
?
c?C(h)
?
r?c
(whcr log ?R(r) +
(1? whcr) log(1? ?R(r)))
We next denote by nR the total number of applica-
tions of rules from resource R in the training data.
We can maximize Q(?) for each R separately to ob-
tain the M-step parameter-updating formula:
?R =
1
nR
?
T,H
?
h?H
?
c?C(h)
?
r?c|R(r)=R
whcr (7)
The updated parameter value averages the posterior
probability that rules from resource R have been
validly applied, across all its utilizations in the train-
ing data.
4.3 EM for the Extended Model
In case we normalize the noisy-AND parameter by
the hypothesis length, for each length we use a dif-
ferent parameter value for the noisy-AND and we
cannot simply merge the information from all the
training pairs (T,H). To find the optimal param-
eter value for ?NA, we need to maximize the fol-
lowing expression (the derivation of the auxiliary
function to the hypothesis-length-normalized noisy-
AND ?resource?):
Q(?NA) =
?
T,H
?
h?H
(whNA log(?
1
n
NA) +
(1? whNA) log(1? ?
1
n
NA)) (8)
where n is the length of H , ?NA is the parameter
value of the noisy-AND model andwhNA is the pos-
terior probability that the noisy-AND was used to
validly entail the term h5, i.e.
whNA(T,H) =
?
??
??
p(T
NA
???h|T?H) if T?H
p(T
NA
???h|T9H) if T9H
The two cases of the above equation are similar to
Eq. (4) and can be efficiently computed in analogy
to Eq. (5) and Eq. (6).
There is no close-form expression for the param-
eter value ?NA that maximizes expression (8). Since
?NA?[0, 1] is a scalar parameter, we can find ?NA
value that maximizes Q(?NA) using an exhaustive
grid search on the interval [0, 1], in each iteration of
the M-step. Alternatively, for an iterative procedure
to maximize expression (8), see Appendix A.
In the same manner we address the normalization
of the reliability ?R of each resourcesR by the num-
ber of H?s covered terms. Expression (8) becomes:
Q(?R) =
?
T,H
?
h?H
?
c?C(h)
?
r?c|R(r)=R
(whcr log(?
cov
R ) + (1? whcr) log(1? ?
cov
R ))
were 1cov is the number of H terms which are cov-
ered. We can find the ?R that maximizes this equa-
tion in one of the methods described above.
5 Evaluation and Results
For our evaluation we use the RTE-5 pilot task and
the RTE-6 main task data sets described in Sec-
tion 2. In our system, sentences are tokenized and
stripped of stop words and terms are tagged for part-
of-speech and lemmatized. We utilized two lexical
resources, WordNet (Fellbaum, 1998) and CatVar
5In contrary to Eq. 4, here there is no specific t ? T that
entails h, therefore we write T
NA
???h
16
(Habash and Dorr, 2003). From WordNet we took as
entailment rules synonyms, derivations, hyponyms
and meronyms of the first senses of T and H terms.
CatVar is a database of clusters of uninflected words
(lexemes) and their categorial (i.e. part-of-speech)
variants (e.g. announce (verb), announcer and an-
nouncement(noun) and announced (adjective)). We
deduce an entailment relation between any two lex-
emes in the same cluster. Model?s parameters were
estimated from the development set, taken as train-
ing. Based on these parameters, the entailment prob-
ability was estimated for each pair (T,H) in the test
set, and the classification threshold was tuned by
classification over the development set.
We next present our evaluation results. First we
investigate the impact of utilizing lexical resources
and of chaining rules. In section 5.2 we evaluate the
contribution of each extension of the base model and
in Section 5.3 we compare our performance to that
of state-of-the-art entailment systems.
5.1 Resources and Rule-Chaining Impact
As mentioned in Section 2, in the RTE data sets it
is hard to show more than a moderate improvement
when utilizing lexical resources. Our analysis as-
cribes this fact to the relatively small amount of rule
applications in both data sets. For instance, in RTE-
6 there are 10 times more direct matches of identi-
cal terms than WordNet and CatVar rule applications
combined, while in RTE-5 this ratio is 6. As a result
the impact of rule applications can be easily shad-
owed by the large amount of direct matches.
Table 1 presents the performance of our (full)
model when utilizing no resources at all, WordNet,
CatVar and both, with chains of a single step. We
also considered rule chains of length up to 4 and
present here the results of 2 chaining steps with
WordNet-2 and (WordNet+CatVar)-2.
Overall, despite the low level of rule applications,
we see that incorporating lexical resources in our
model significantly6 and quite consistently improves
performance over using no resources at all. Natu-
rally, the optimal combination of resources may vary
somewhat across the data sets.
In RTE-6 WordNet-2 significantly improved per-
6All significant results in this section are according to Mc-
Nemar?s test with p < 0.01 unless stated otherwise
formance over the single-stepped WordNet. How-
ever, mostly chaining did not help, suggesting the
need for future work to improve chain modeling in
our framework.
Model
F1%
RTE-5 RTE-6
no resources 41.6 44.9
WordNet 45.8 44.6
WordNet-2 45.7 45.5
CatVar 46.9 45.6
WordNet + CatVar 48.3 45.6
(WordNet + CatVar)-2 47.1 44.0
Table 1: Evaluation of the impact of resources and chaining.
5.2 Model Components impact
We next assess the impact of each of our proposed
extensions to the base probabilistic model. To that
end, we incorporate WordNet+CatVar (our best con-
figuration above) as resources for the base model
(Section 3.1) and compare it with the noisy-AND
extension (Eq. (2?)), the covered-norm extension
which normalizes the resource reliability parame-
ter by the number of covered terms (Eq. (1?)) and
the full model which combines both extensions. Ta-
ble 2 presents the results: both noisy-AND and
covered-norm extensions significantly increase F1
over the base model (by 4.5-8.4 points). This scale
of improvement was observed with all resources and
chain-length combinations. In both data sets, the
combination of noisy-AND and covered-norm ex-
tensions in the full model significantly outperforms
each of them separately7, showing their complemen-
tary nature. We also observed that applying noisy-
AND without the hypothesis length normalization
hardly improved performance over the base model,
emphasising the importance of considering hypothe-
sis length. Overall, we can see that both base model
extensions improve performance.
Table 3 illustrates a set of maximum likelihood
parameters that yielded our best results (full model).
The parameter value indicates the learnt reliability
of the corresponding resource.
7With the following exception: in RTE-5 the full model is
better than the noisy-AND extension with significance of only
p = 0.06
17
Model
F1%
RTE-5 RTE-6
base model 36.2 38.5
noisy-AND 44.6 43.1
covered-norm 42.8 44.7
full model 48.3 45.6
Table 2: Impact of model components.
?MATCH ?WORDNET ?CATVAR ?UNCOVERED ?NA
0.80 0.70 0.65 0.17 0.05
Table 3: A parameter set of the full model which maximizes
the likelihood of the training set.
5.3 Comparison to Prior Art
Finally, in Table 4, we put these results in the con-
text of the best published results on the RTE task.
We compare our model to the average of the best
runs of all systems, the best and second best per-
forming lexical systems and the best full system of
each challenge. For both data sets our model is situ-
ated high above the average system. For the RTE-6
data set, our model?s performance is third best with
Majumdar and Bhattacharyya (2010) being the only
lexical-level system which outperforms it. However,
their system utilized additional processing that we
did not, such as named entity recognition and co-
reference resolution8. On the RTE-5 data set our
model outperforms any other published result.
Model
F1%
RTE-5 RTE-6
full model 48.3 45.6
avg. of all systems 30.5 33.8
2nd best lexical system 40.3a 44.0b
best lexical system 44.4c 47.6d
best full system 45.6c 48.0e
Table 4: Comparison to RTE-5 and RTE-6 best entailment
systems: (a)(MacKinlay and Baldwin, 2009), (b)(Clark and
Harrison, 2010), (c)(Mirkin et al, 2009a)(2 submitted runs),
(d)(Majumdar and Bhattacharyya, 2010) and (e)(Jia et al,
2010).
8We note that the submitted run which outperformed our re-
sult utilized a threshold which was a manual modification of the
threshold obtained systematically in another run. The latter run
achieved F1 of 42.4% which is below our result.
We conclude that our probabilistic model demon-
strates quality results which are also consistent,
without applying heuristic methods of the kinds re-
viewed in Section 2
6 Conclusions and Future Work
We presented, a probabilistic model for lexical en-
tailment whose innovations are in (1) considering
each lexical resource separately by associating an
individual reliability value for it, (2) considering the
existence of multiple evidence for term entailment
and its impact on entailment assessment, (3) setting
forth a probabilistic method to relax the strict de-
mand that all hypothesis terms must be entailed, and
(4) taking account of the number of covered terms in
modeling entailment reliability.
We addressed the impact of the various compo-
nents of our model and showed that its performance
is in line with the best state-of-the-art inference sys-
tems. Future work is still needed to reflect the im-
pact of transitivity. We consider replacing the AND
gate on the rules of a chain by a noisy-AND, to relax
its strict demand that all its input rules must be valid.
Additionally, we would like to integrate Contextual
Preferences (Szpektor et al, 2008) and other works
on Selectional Preference (Erk and Pado, 2010) to
verify the validity of the application of a rule in a
specific (T,H) context. We also intend to explore
the contribution of our model within a complex sys-
tem that integrates multiple levels of inference as
well as its contribution for other applications, such
as Passage Retrieval.
References
Roy Bar-Haim, Jonathan Berant, Ido Dagan, Iddo Green-
tal, Shachar Mirkin, Eyal Shnarch, and Idan Szpektor.
2008. Efficient semantic deduction and approximate
matching over compact parse forests. In Proc. of TAC.
Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo
Giampiccolo, and Bernardo Magnini. 2009. The fifth
PASCAL recognizing textual entailment challenge. In
Proc. of TAC.
Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa Trang
Dang, and Danilo Giampiccolo. 2010. The sixth
PASCAL recognizing textual entailment challenge. In
Proc. of TAC.
Aljoscha Burchardt, Nils Reiter, Stefan Thater, and
Anette Frank. 2007. A semantic approach to textual
18
entailment: System evaluation and task analysis. In
Proc. of the ACL-PASCAL Workshop on Textual En-
tailment and Paraphrasing.
Timothy Chklovski and Patrick Pantel. 2004. Verbo-
cean: Mining the web for fine-grained semantic verb
relations. In Proc. of EMNLP.
Peter Clark and Phil Harrison. 2010. BLUE-Lite: a
knowledge-based lexical entailment system for RTE6.
In Proc. of TAC.
Courtney Corley and Rada Mihalcea. 2005. Measuring
the semantic similarity of texts. In Proc. of the ACL
Workshop on Empirical Modeling of Semantic Equiv-
alence and Entailment.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The PASCAL recognising textual entailment
challenge. In Lecture Notes in Computer Science, vol-
ume 3944, pages 177?190.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the royal statistical society, se-
ries [B], 39(1):1?38.
Katrin Erk and Sebastian Pado. 2010. Exemplar-based
models for word meaning in context. In Proc. of the
ACL.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database (Language, Speech, and Com-
munication). The MIT Press.
Oren Glickman, Ido Dagan, and Moshe Koppel. 2005. A
probabilistic classification approach for lexical textual
entailment. In Proc. of AAAI.
Oren Glickman, Eyal Shnarch, and Ido Dagan. 2006.
Lexical reference: a semantic matching subtask. In
Proceedings of the EMNLP.
Nizar Habash and Bonnie Dorr. 2003. A categorial vari-
ation database for english. In Proc. of NAACL.
Houping Jia, Xiaojiang Huang, Tengfei Ma, Xiaojun
Wan, and Jianguo Xiao. 2010. PKUTM participation
at TAC 2010 RTE and summarization track. In Proc.
of TAC.
Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules for question answering. Natural Language
Engineering, 7:343?360.
Bill MacCartney and Christopher D. Manning. 2007.
Natural logic for textual inference. In Proc. of the
ACL-PASCAL Workshop on Textual Entailment and
Paraphrasing.
Andrew MacKinlay and Timothy Baldwin. 2009. A
baseline approach to the RTE5 search pilot. In Proc.
of TAC.
Debarghya Majumdar and Pushpak Bhattacharyya.
2010. Lexical based text entailment system for main
task of RTE6. In Proc. of TAC.
Shachar Mirkin, Roy Bar-Haim, Jonathan Berant, Ido
Dagan, Eyal Shnarch, Asher Stern, and Idan Szpektor.
2009a. Addressing discourse and document structure
in the RTE search task. In Proc. of TAC.
Shachar Mirkin, Ido Dagan, and Eyal Shnarch. 2009b.
Evaluating the inferential utility of lexical-semantic re-
sources. In Proc. of EACL.
Judea Pearl. 1988. Probabilistic reasoning in intelli-
gent systems: networks of plausible inference. Morgan
Kaufmann.
Eyal Shnarch, Jacob Goldberger, and Ido Dagan. 2011.
A probabilistic modeling framework for lexical entail-
ment. In Proc. of ACL, pages 558?563.
Idan Szpektor, Ido Dagan, Roy Bar-Haim, and Jacob
Goldberger. 2008. Contextual preferences. In Proc.
of ACL-08: HLT.
Marta Tatu and Dan Moldovan. 2007. COGEX at RTE
3. In Proc. of the ACL-PASCAL Workshop on Textual
Entailment and Paraphrasing.
Rui Wang, Yi Zhang, and Guenter Neumann. 2009. A
joint syntactic-semantic representation for recognizing
textual relatedness. In Proc. of TAC.
Fabio Massimo Zanzotto and Alessandro Moschitti.
2006. Automatic learning of textual entailments with
cross-pair similarities. In Proc. of ACL.
A Appendix: An Iterative Procedure to
Maximize Q(?NA)
There is no close-form expression for the parameter
value ?NA that maximizes expression (8) from Sec-
tion 4.3. Instead we can apply the following iterative
procedure. The derivative of Q(?NA) is:
dQ(?NA)
d?NA
=
?
(
l?whNA
?NA
?
(1?whNA)l??
(l?1)
NA
1? ?lNA
)
where 1l is the hypothesis length and the summation
is over all terms h in the training set. Setting this
derivative to zero yields an equation which the opti-
mal value satisfies:
?NA =
?
l?whNA
? (1?whNA)l??
(l?1)
NA
1??lNA
(9)
Eq. (9) can be utilized as a heuristic iterative proce-
dure to find the optimal value of ?NA:
?NA ?
?
l?whNA
? (1?whNA)l??
(l?1)
NA
1??lNA
19
Proceedings of the TextInfer 2011 Workshop on Textual Entailment, EMNLP 2011, pages 20?29,
Edinburgh, Scotland, UK, July 30, 2011. c?2011 Association for Computational Linguistics
Classification-based Contextual Preferences
Shachar Mirkin, Ido Dagan, Lili Kotlerman
Bar-Ilan University
Ramat Gan, Israel
{mirkins,dagan,davidol}@cs.biu.ac.il
Idan Szpektor
Yahoo! Research
Haifa, Israel
idan@yahoo-inc.com
Abstract
This paper addresses context matching in tex-
tual inference. We formulate the task under
the Contextual Preferences framework which
broadly captures contextual aspects of infer-
ence. We propose a generic classification-
based scheme under this framework which co-
herently attends to context matching in infer-
ence and may be employed in any inference-
based task. As a test bed for our scheme we use
the Name-based Text Categorization (TC) task.
We define an integration of Contextual Prefer-
ences into the TC setting and present a concrete
self-supervised model which instantiates the
generic scheme and is applied to address con-
text matching in the TC task. Experiments on
standard TC datasets show that our approach
outperforms the state of the art in context mod-
eling for Name-based TC.
1 Introduction
Textual inference is prevalent in text understanding
applications. For example, in Question Answering
(QA) the expected answer should be inferred from
retrieved passages, and in Information Extraction (IE)
the meaning of the target event is inferred from its
mention in the text.
Lexical inferences make a substantial part of the
inference process. In such cases, a target term is
inferred from text expressions based on either one of
two types of lexical matches: (i) a direct match of
the target term in the text. For instance, the IE event
injure may be detected by finding the word injure in
the text; (ii) an indirect match, through a term that
implies the meaning of the target term, e.g. inferring
injure from hurt.
In either case, due to word ambiguity, it is nec-
essary to validate that the context of the match con-
forms with the intended meaning of the target term
before carrying out an inference operation based on
this match. For example, ?You hurt my feelings? con-
stitutes an invalid context for the injure event as hurt
in this text does not refer to a physical injury. Simi-
larly, inferring the protest-related event demonstrate
based on demo is deemed invalid although demo im-
plies the meaning of the word demonstrate in other
contexts, e.g., concerning software demonstration.
Although seemingly equivalent, a closer look re-
veals that the above two examples correspond to two
distinct contextual mismatch situations. While the
match of hurt is invalid for injure in the particular
given context, an inference based on demo is invalid
for the protest demonstrate event in any context.
Thus, several types of context matching are in-
volved in textual inference. While most prior work
addressed only specific context matching scenarios,
Szpektor et al (2008) presented a broader view,
proposing a generic framework for context match-
ing in inference, termed Contextual Preferences (CP).
CP specifies the types of context matching that need
to be considered in inference, allowing a model of
choice to be applied for validating each type of match.
Szpektor et al applied CP to an IE task using differ-
ent models to validate each type of context match.
In this work we adopt CP as our context matching
framework and propose a novel classification-based
scheme which provides unified modeling for CP. We
represent typical contexts of the textual objects that
participate in inference using classifiers; at inference
time, each match is assessed by the respective classi-
fiers which determine its contextual validity.
As a test bed we applied our scheme to the task
20
of Name-based Text Categorization. This is an unsu-
pervised setting of TC where the only input given is
the category name, and in which context validation
is of high importance. We instantiate the scheme
with a novel self-supervised model and apply it to
the TC task. We suggest a method for integrating any
CP-based context matching model into TC and use it
to combine the context matching scores generated by
our model. Results on two standard TC datasets show
that our approach outperforms the state of the art con-
text model for this task and suggest applying this
scheme to additional inference-based applications.
2 Background
2.1 Context matching in inference
Word ambiguity has been traditionally addressed
through Word Sense Disambiguation (WSD) (Nav-
igli, 2009). The WSD task requires selecting the
meaning of a target term from amongst a predefined
set of senses, based on sense-inventories such as
WordNet (Fellbaum, 1998).
An alternative approach eliminates the reliance on
such inventories. Instead of explicit sense identifi-
cation, a direct sense-match between terms is pur-
sued (Dagan et al, 2006). Lexical substitution (Mc-
Carthy and Navigli, 2009) is probably the most com-
monly known task that follows this approach. Con-
text matching is a generalization of lexical substitu-
tion, which seeks a match between terms in context,
not necessarily for the purpose of substitution. For in-
stance, the word played in ?U2 played their first-ever
concert in Russia? contextually matches music, al-
though music cannot substitute played in this context.
The context matching task, therefore, is to determine
(by quantifying or giving a binary decision) the va-
lidity of a match between two terms in context.
In Section 1 we informally presented two cases of
contextual mismatches. A comprehensive view of
context matching types is provided by the Contextual
Preferences framework (Szpektor et al, 2008). CP
is phrased in terms of the Textual Entailment (TE)
paradigm (Dagan et al, 2009). In TE, a text t entails
a textual hypothesis h if the meaning of h can be
inferred from t. Formulating the IE example from
Section 1 within TE, h may be the name of the target
event, injure, and t is a text segment from which h
can be inferred. A direct match occurs when a term
in h is identical to a term in t. An inference based
on an indirect match is viewed as the application of
a lexical entailment rule, r, such as ?hurt? injure?,
where the entailing left-hand side (LHS) of the rule
(hurt) is matched in the text, while the entailed right-
hand side (RHS), injure, is matched in the hypothesis.
Hence, three inference objects take part in infer-
ence operations: t, h and r. Most prior work ad-
dressed only specific contextual matches between
these objects. For example, Harabagiu et al (2003)
matched the contexts of t and h for QA (answer and
question, respectively); Barak et al (2009) matched
t and h (document and category) in TC, while other
works, including those applying lexical substitution,
typically validated the context match between t and r
(Kauchak and Barzilay, 2006; Dagan et al, 2006;
Pantel et al, 2007; Connor and Roth, 2007).
In comparison, in the CP framework, all possible
contextual matches among t, h and r are considered:
t?h, t? r and r?h. The three context matches are
depicted in Figure 1 (left). In CP, the representation
of each inference object is enriched with contextual
information which is used to characterize its valid
contexts. Such information may be the words of the
event description in IE, corpus instances based on
which a rule was learned, or an annotation of relevant
WordNet senses in Name-based TC. For example,
a category name hockey may be assigned with the
sense number corresponding to ice hockey, but not
to field hockey, in order to designate information that
limits the valid contexts of the category to the former
among the two meanings of the name.
Before an inference operation is performed, the
context representations of each pair among the partic-
ipating objects should be matched by a context model
in order to assess the contextual validity of the opera-
tion. Along with the context representation and the
specific context matching models, the way context
model decisions are combined needs to be specified
in a concrete implementation of the CP framework.
2.2 Context matching models
Several approaches were taken in prior work to
model context matching, mostly within the scope
of learning selectional preferences of templatized
lexical-syntactic rules (e.g. ?X
subj
???? hit
obj
??? Y ??
?X
subj
???? attack
obj
??? Y ?).
21
Pantel et al (2007) and Szpektor et al (2008) rep-
resented the context of such rules as the intersection
of preferences of the rule?s LHS and RHS, namely the
observed argument instantiations or their semantic
classes. A rule is deemed applicable to a given text if
the argument instantiations in the text are similar to
the selectional preferences of the rule. To overcome
sparseness, other works represented context in latent
space. Pennacchiotti et al (2007) and Szpektor et al
(2008) measured the similarity between the Latent
Semantic Analysis (LSA) (Deerwester et al, 1990)
representations of matched contexts. Dinu and La-
pata (2010) used Latent Dirichlet Allocation (LDA)
(Blei et al, 2003) to model templates? latent senses,
determining rule applicability based on the similarity
between the two sides of the rule when instantiated
by the context, while Ritter et al (2010) used LDA
to model argument classes, considering a rule valid
for a given argument instantiation if its instantiated
templates are drawn from the same hidden topic.
A different approach is provided by classification-
based models which learn classifiers for inference
objects. A classifier is trained based on positive and
negative examples which represent valid or invalid
contexts of the object; from those, features charac-
terizing the context are extracted, e.g. words in a
window around the target term or syntactic links with
it. Given a new context, the classifier assesses its va-
lidity with respect to the learned classification model.
Classifiers in prior work were applied to determine
rule applicability in a given context (t ? r). Train-
ing a classifier for word paraphrasing, Kauchak and
Barzilay (2006) used occurrences of the rule?s RHS as
positive context examples, and randomly picked neg-
ative examples. A similar approach was applied by
Dagan et al (2006), which used a single-class SVM
to avoid selecting negative examples. In both works,
a resulting classifier represents a word with all its
senses intermixed. Clearly, this poses no problem for
monosemous words, but is biased towards the more
common senses of polysemous words. Indeed, Dagan
et al (2006) report a negative correlation between the
degree of polysemy of a word and the performance of
its classifier. Connor and Roth (2007) used per-rule
classifiers to produce a noisy training set for learning
a global classifier for verb substitution.
In this work we follow the classification-based ap-
proach which seems appealing for several reasons.
t
r
t
C h(
r)
h
C r(
t)
C h(
t)r
h
Figure 1: Left: An illustration of the CP relationships as in
(Szpektor et al, 2008), with arrows indicting the context
matching direction; Right: The application of classifiers
to the tested contexts under our scheme.
First, it allows seamlessly integrating various types
of information via classifiers? features; unlike some
of the above models, it is not inherently dependent on
the type of rules that are utilized and easily accom-
modates to both lexical and lexical-syntactic rules
through the choice of features. In addition, it does
not rely on a predefined similarity measure and pro-
vides flexibility in terms of model?s parameters. Fi-
nally, this approach captures the notion of direction-
ality which is fundamental in textual inference, and
is therefore better suited to applied inference than
previously proposed symmetric context models.
In comparison to prior classification-based models,
our approach addresses all three context matches
specified by CP, rather than only the rule-text match.
It is not limited to substitutable terms or even to
terms with the same part of speech. In addition, we
avoid learning a classifier for all senses combined,
but rather learn it for the specific intended meaning.
2.3 Name-based Text Categorization
Name-based TC (Gliozzo et al, 2009) is an unsu-
pervised setting of Text Categorization in which the
only input provided is the category name, e.g. trade,
?mergers and acquisitions? or guns. When category
names are ambiguous, e.g. space, categories are not
well defined; thus, auxiliary information is expected
to accompany the name for disambiguation, such as
a list of relevant senses or a category description.
Typically, unsupervised TC consists of two steps.
First, an unsupervised method is applied to an unla-
beled corpus, automatically labeling some of the doc-
uments to categories. Then, the labeled documents
from the first step are used to train a supervised TC
classifier which is used to label any document in the
test set (Gliozzo et al, 2009; Downey and Etzioni,
2009; Barak et al, 2009).
22
In this work we focus on the above unsupervised
step. Gliozzo et al (2009) addressed this task by rep-
resenting both documents and categories by LSA vec-
tors which implicitly capture contextual similarities
between terms. Each document was then assigned
to the most similar category based on cosine simi-
larity between the LSA vectors. Barak et al (2009)
required an occurrence of a term entailing the cat-
egory name (or the category name itself) in order
to regard the category as a candidate for the docu-
ment. To assess the contextual validity of the match,
they used LSA document-category similarity as in
(Gliozzo et al, 2009). For example, to classify a doc-
ument into the category medicine, at least one lexical
entailment rule, e.g. ?drug? medicine?, should be
matched in the document. Then, the validity of drug
for medicine in the matched document is assessed by
the LSA context model. In this work we adopt Barak
et al?s requirement for a match for the category in
the document, but address context matching in an
entirely different way.
Name-based TC provides a convenient setting for
evaluating context matching approaches for two main
reasons. First, all types of context matchings are real-
ized in this application (see Section 3); second, as the
hypothesis consists of a single term or a few terms,
the TC gold standard annotation corresponds quite
directly to the context matching task for lexical infer-
ences; in other applications where longer hypotheses
are involved, context matching performance may be
masked by other factors.
3 Contextual Matches in TC
Within Name-based TC, the Textual Entailment ter-
minology is mapped as follows: h is a term denoting
the category name (e.g. merger or acquisition); t is
a matched term in the document to be categorized
from which h may be inferred; and a match refers
to an occurrence in the document of either h (direct
match) or the LHS of an entailment rule r whose RHS
is a category name (indirect match).1
Under the CP view, a context model needs to ad-
dress the following three context matching cases
within a TC setting.
t?h: Assessing the validity of a match in the docu-
ment with respect to the category?s intended meaning.
1Note that t and h both refer here to individual terms.
For example, the occurrence of the category name
space (in the sense of outer space) in ?the server ran
out of disk space? does not indicate a space-related
text, and should be dismissed by the context model.
t? r: This case refers to a rule match in the docu-
ment. A context model should ensure that the mean-
ing of a match is compatible with that of the rule.
For example, ?alien? space? is a valid rule for the
space category. Yet, it should not be applied to ?The
US welcomes a large number of aliens every year?,
since alien in this sentence has a different meaning
than the intended meaning of the rule.
r ? h: The match between the intended meanings
of the category name and the RHS of the rule. For
instance, the rule ?room? space? is not suitable at
all for the (outer) space category.
4 A Classification-based Scheme for CP
Szpektor et al (2008) introduced a vector-space
model to implement CP, in which the text t, the rule
r and the hypothesis h share the same contextual
representation. However, in CP, r, h and t have non-
symmetric roles: the context of t should be tested as
valid for r and h and not vice versa, and the context
of r should be validated for h and not the other way
around. This stems from the need to consider direc-
tionality in context matching. For instance, a text
about football typically constitutes a valid context
for the more general sports context, but not vice
versa. Indeed, directionality may be captured in
vector-space models by using a directional similarity
measure (Kotlerman et al, 2010), but only symmetric
measures were used in context matching work so far.
Based on this distinction between the inference
objects? roles, we present a novel scheme that uses
two types of classifiers to represent context:
Ch: A classifier that identifies valid contexts for h. It
tests contexts of t (for t? h matching) or r (for
r ? h matching), assigning them scores Ch(t)
and Ch(r), respectively.
Cr: A classifier that identifies valid contexts for ap-
plying the rule r. It tests the context of t, assign-
ing it a score Cr(t).
Figure 1 (right) shows the classifiers scores which
are assigned to each of the matching types.
23
Hence, h always acts as the classifying object, t is
always the classified object, while r acts as both. Con-
text matching is quantified by the degree by which
the classified object represents a valid context for the
classifying object in a given inference scenario.
In comparison to the CP implementation in (Szpek-
tor et al, 2008), our approach uses a unified model
which captures directionality in context matching.
To instantiate the scheme, one needs to define the
way training examples are obtained and processed.
This may be done within supervised classification,
where labeled examples are provided, or ? as we do
in this work ? using self-supervised classifiers which
obtain training examples automatically. We present
such an instantiation in Section 5, where a classifier is
trained for each category and each rule. When more
complex hypotheses are involved, Ch classifiers can
be trained separately for each relevant part of the
hypothesis, using the rest for disambiguation.
A combination of the three model scores provides
a final context matching score. In Section 6 we sug-
gest a way to combine the actual classification scores
as part of the integration of CP into TC, but other
combinations are plausible. In particular, binary clas-
sifications (valid vs. invalid) may be used as filters.
That is, the context is classified as valid only if all
relevant models classify it as such.
5 A Self-supervised Context Model
We now turn to demonstrate how our classification-
based scheme may be implemented. The model be-
low is exemplified on Name-based TC, but may be
applicable to other tasks, with few changes.
5.1 Training-set generation
Our implementation is self-supervised as we want
to integrate it within the unsupervised TC setting.
That is, the classifiers automatically obtain training
examples for the classifying object (a category or a
rule) without relying on labeled documents.
We obtain examples by querying the TC training
corpus with automatically-generated search queries.
The difficulty lies in correctly constructing queries
that will retrieve documents representing either valid
or invalid contexts for the classifying object. To this
end, we retrieve examples through a gradual process
in which the most accurate (least ambiguous) query
is used first and less accurate queries follow, until the
designated number of examples is acquired.
5.1.1 Obtaining positive examples
To acquire positive training examples, we con-
struct queries which are comprised of two main
clauses. The first contains the seeds, terms which
characterize the classifying object. Primarily, these
are the category name or the LHS of the rule. The sec-
ond consists of context words which are used when
the seeds are polysemous, and are intended to assist
disambiguation. When context words are used, at
least one seed and at least one context word must
be matched to retrieve a document. For example,
given the highly ambiguous category name space,
we first construct the query using only the monose-
mous term outer space; if the number of retrieved
documents does not meet the requirement, a second
query may be constructed: (?outer space? OR space)
AND (infinite OR science OR . . . ).
To generate a rule classifier Cr, we retrieve posi-
tive examples as follows. If the LHS term is monose-
mous according to WordNet2, we first query using
this term alone (e.g. decrypt), and add its monose-
mous synonyms and hyponyms if more examples are
required (e.g. decrypt OR decode). If the LHS is
polysemous, we carry out Procedure 1. Intuitively,
this procedure tries to minimize ambiguity by using
monosemous terms as much as possible; when poly-
semous terms must be used, it tries to ensure there are
monosemous terms to disambiguate them. Note that
entailment directionality is maintained throughout
the process, as seeds are only expanded with more
specific (entailing) terms, while context words are
only expanded with more general (entailed) terms.
Procedure 1 : Retrieval of Cr positive examples
Apply sequentially until sufficient examples are obtained:
1: Set the LHS as seed and the RHS?s monosemous syn-
onyms, hypernyms and derivations as context words.
2: Add monosemous synonyms and hyponyms of the
LHS to the seeds.
3: As in 2, but use polysemous terms as well.
4: Add polysemous context words.
Positive examples for category classifiers (Ch) are
obtained through a similar procedure as for rule clas-
2Terms not in WordNet are assumed monosemous.
24
sifiers. If the category is part of a hierarchy, we also
use the name of the parent category (e.g. sport for
rec.sport.hockey) as a context word.
5.1.2 Obtaining negative examples
Negative examples are even more challenging to
acquire. In prior work negative examples were se-
lected randomly (Kauchak and Barzilay, 2006; Con-
nor and Roth, 2007). We follow this method, but
also attempt to identify negative examples that are
semantically similar to the positive ones in order to
improve the discriminative power of the classifier
(Smith and Eisner, 2005). We do that by applying
a similar procedure which uses cohyponyms of the
seeds, e.g. baseball for hockey or islam for christian-
ity. Cohyponymy is a non-entailing relation; hence,
by using it we expect to obtain semantically-related,
yet invalid contexts. If not enough negative exam-
ples are retrieved using cohyponyms, we select the
remaining required examples randomly.
As the distribution of positive and negative ex-
amples in the data is unknown, we set the ratio of
negative to positive examples as a parameter of the
model, as in (Bergsma et al, 2008).
5.1.3 Insufficient examples
When the number of training examples for a rule
or a category is below a certain minimum, the re-
sulting classifier is expected to be of poor quality.
This usually happens for positive examples in any of
the following two cases: (i) the seed is rare in the
training set; (ii) the desired sense of the seed is rarely
found in the training set, and unwanted senses were
filtered by our retrieval query. For instance, nazarene
does not occur at all in the training set, and the classi-
fier corresponding to the rule ?nazarene? christian?
cannot be generated. On the other hand, cone does
appear in the corpus but not in the astrophysical sense
the rule ?cone? space? refers to. In such cases we
refrain from generating the classifier and use instead
a default score of 0 for each classified object. The
idea is that rare terms will also occur infrequently in
the test set, while cases where the term is found in
the corpus, but in a different sense than the desired
one, will be blocked.
5.1.4 Feature extraction
We extract global and local lexical features that are
standard in WSD work. Global features include all
the terms in the document or in the sentence in which
a match was found. Local features are extracted
around matches of seeds which comprised the query
that retrieved the document. These features include
the terms in a window around the match, and the
noun, verb, adjective and adverb nearest to the match
in either direction. For randomly sampled negative
examples, where no matched query terms exist, we
randomly select terms in the document as ?matches?
for local feature extraction. If more than one match of
the same term is found in a document, we assume one-
sense-per-discourse (Gale et al, 1992) and jointly
extract features for all matches of the term.
5.2 Applying the classifiers
During inference, for each direct match in a docu-
ment, the corresponding Ch is applied. For an indi-
rect match, the respective Cr is also applied.
In addition, Ch is applied to the matched rules.
Unlike t, a rule is not represented by a single text.
Therefore, to test a rule?s match with the category,
we randomly sample from the training set documents
containing the rule?s LHS. We apply Ch to each sam-
pled example and compute the ratio of positive classi-
fications. The result is a score indicating the domain-
specific probability of the rule to be applicable to
the category, and may be interpreted as an in-domain
prior. For instance, the rule ?check ? hockey? is
assigned a score of 0.05, since the sense of check
as a hockey defense technique is rare in the corpus.
On the other hand, non ambiguous rules, e.g. ?war-
ship ? ship? are assigned a high probability (1.0),
and so are rules whose LHS is ambiguous but its dom-
inant sense in the training corpus is the same one the
rule refers to, e.g. ?margin? earnings?(0.85).
We do not assign negative classifier scores to in-
valid matches but rather set them to zero instead. The
reason is that an invalid context only indicates that
the term cannot be used for entailing the category
name, but not that the document itself is irrelevant.
6 CP for Text Categorization
CP may be employed in any inference-based task,
but the integration with each task is somewhat dif-
ferent and needs to be specified. Below we present
a methodology for integrating CP into Name-based
Text Categorization.
25
As in (Barak et al, 2009) (Barak09 below), we
represent documents and categories by term-vectors
in the following way: a document vector contains
the document terms; a category vector contains two
sets of terms: C, the terms denoting the category
name, and E , their entailing terms. For example, oil
is added to the vector of the category crude by the
rule ?oil? crude? (i.e. crude ? C and oil ? E).
Barak09 assigned equal values of 1 to all vector
entries. We suggest integrating a CP-based context
model into TC by re-weighting the terms in the vec-
tors, prior to determining the final document-category
categorization score through vector similarity. Given
a category c, with term vector C, and a document d
with term vector D, the model re-weights vector en-
tries of matching terms (i.e., terms in C ?D), based
on the validity of the context match. Valid matches
should be assigned with higher scores than invalid
ones, leading to higher overall vector similarity for
documents with valid matches for the given category.
Non-matching terms are ignored as their weights are
canceled out in the subsequent vector product.
Specifically, the model assigns a new weight
wD(u) to a matching term u in the document vec-
tor D based on the model?s assessment of: (a) t? h,
the context match between the (match in the) doc-
ument and the category; and (if an indirect match)
(b) t ? r, the context match between the document
and the rule ?u? ci?, where ci ? C. The model also
sets a new weight wC(v) to a term v in the category
vector C based on the context match for r ? h, be-
tween the rule ?v ? cj? (cj ? C) and the category.
For instance, using our context matching scheme in
TC, wD(u) is set to Ch(u) or
Ch(u)+Cr(u)
2 for direct
and indirect matches, respectively; wC(v) is left as 1
if v ? C and set to Ch(v) when v ? E .
Barak09 assigned a single global context score to
a document-category pair using the LSA representa-
tions of their vectors. In our approach, however, we
consider the actual matches from the three different
views, hence the re-weighting of the vector entries
using three model scores.
7 Experimental Setting
7.1 Datasets and knowledge resources
Following (Gliozzo et al, 2009) and (Barak et al,
2009), we evaluated our method on two standard TC
datasets: Reuters-10 and 20-Newsgroups.
The Reuters-10 (R10, for short) is a sub-corpus
of the Reuters-21578 collection3, constructed from
the ten most frequent categories in the Reuters tax-
onomy. We used the Apte split of the Reuters-21578
collection, often used in TC tasks. The top 10 cate-
gories include about 9,000 documents, split into train-
ing (70%) and test (30%) sets. The 20-Newsgroups
(20NG) corpus is a collection of newsgroup postings
gathered from twenty different categories from the
Usenet Newsgroups hierarchy4. We used the ?by-
date? version of the corpus, which contains approxi-
mately 20,000 documents partitioned (nearly) evenly
across the categories and divided in advance to train-
ing (60%) and test (40%) sets.
As in (Gliozzo et al, 2009; Barak et al, 2009), we
adjusted non-standard category names (e.g. forsale
was renamed to sale) and manually specified for each
category its relevant WordNet senses. The sense tag-
ging properly defines the categories, and is expected
to accompany such hypotheses. Other types of in-
formation may be used for this purpose, e.g. words
from category descriptions, if such exist.
We applied standard preprocessing (sentence split-
ting, tokenization, lemmatization and part of speech
tagging) to all documents in the datasets. All terms,
including those denoting category names and rules,
are represented by their lemma and part of speech.
As sources for lexical entailment rules we used
WordNet 3.0 (synonyms, hyponyms, derivations
and meronyms) and a Wikipedia-derived rule-base
(Shnarch et al, 2009). Unlike Barak09 we did not
limit the rules extracted from WordNet to the most
frequent senses and used all rule types from the
Wikipedia-based resource.
7.2 Self-supervised model tuning
Tuning of the self-supervised context model?s pa-
rameters (number of training examples, negative to
positive ratio, feature set and the way negative exam-
ples are obtained) was performed over development
sets sampled from the training sets. Based on this tun-
ing, some parameters varied between the datasets and
between classifier types (Ch vs. Cr). For example,
3http://kdd.ics.uci.edu/databases/
reuters21578/reuters21578.html
4http://people.csail.mit.edu/jrennie/
20Newsgroups/
26
selection of negative examples based on cohyponyms
was found useful for Cr classifiers in R10, while ran-
dom examples were used in the rest of the cases.
We used SVMperf (Joachims, 2006) with a linear
kernel and binary feature weighting.
For querying the corpus we used the Lucene search
engine5 in its default setting. Up to 150 positive
examples were retrieved for each classifier, with 5
examples set as the required minimum. This resulted
in generating 100% of the hypothesis classifiers for
both datasets and 95% and 70% of the rule classifiers
for R10 and 20NG, respectively.
We computed Ch(r) scores based on up to 20 sam-
pled instances. If less than 2 examples were found in
the training set, we assigned an ?unknown? context
match probability of 0.5, since a rare LHS occurrence
does not indicate anything about its meaning in the
corpus. Such cases constituted 2% (R10) and 11%
(20NG) of the utilized rules.
7.3 Baseline models
To provide a more meaningful comparison with prior
work, we focus on the first unsupervised step in the
typical Name-based TC flow, without the subsequent
supervised training. Our goal is to improve the accu-
racy of this first step, and we therefore compare our
context model?s performance to two unsupervised
methods used by Barak09.
The first baseline, denoted Barakno-cxt, is the co-
sine similarity score between the document and cate-
gory vectors where all terms are equally weighted to
a score of 1.6 This baseline shows the performance
when no context model is employed.
The second baseline, denoted Barakfull, is a repli-
cation of the state of the art context model for Name-
based TC. In this method, LSA vectors are con-
structed for a document by averaging the LSA vectors
of its individual terms, and for a category by averag-
ing the LSA vectors of the terms denoting its name.
The categorization score of a document-category pair
is set to be the product between the cosine similarity
score of the LSA vectors and the score given by the
above Barakno-cxt method. We note that LSA-based
context models performed best also in (Gliozzo et al,
2009) and (Szpektor et al, 2008).
5http://lucene.apache.org
6Other attempted weighting schemes, such as tf-idf, did not
yield better performance.
Model
Reuters-10
Accuracy P R F1
Barakno-cxt 73.2 63.6 77.0 69.7
Barakfull 76.3 68.0 79.2 73.2
Class.-based 79.3 71.8 83.6 77.2
Model
20-Newsgroups
Accuracy P R F1
Barakno-cxt 63.7 44.5 74.6 55.8
Barakfull 69.4 50.1 82.8 62.4
Class.-based 73.4 54.7 76.4 63.7
Table 1: Evaluation results.
All models were constructed based on the TC train-
ing sets, using no external corpora. The vocabulary
consists of terms that appear more than once in the
training set. The terms we consider include nouns,
verbs, adjectives and adverbs, as well as nominal
multi-word expressions.
8 Results and Analysis
Given a document, all categories for which a lexical
match was found in the document are considered,
and the document is classified to the highest scoring
category. If all categories are assigned non-positive
scores, the document is not assigned to any of them.
Based on this requirement that a document con-
tains at least one match for the category, 4862
document-category pairs were considered for clas-
sification in R10 and 9955 pairs in 20NG. We eval-
uated our context model, as well as the baselines,
based on the accuracy of these classifications, i.e.
the percentage of correct decisions among the candi-
date document-category pairs. We also measured the
models? performance in terms of micro-averaged pre-
cision (P ), relative recall (R) and F1. Like Barak09,
recall is computed relative to the potential recall of
the rule-set which provides the entailing terms.
Table 1 presents the evaluation results. As in
Barak09, the LSA-based model outperforms the first
baseline, supporting its usefulness as a context model.
In both datasets our model outperformed the base-
lines in terms of accuracy. This result is statistically
significant with p < 0.01 according to McNemar?s
test (McNemar, 1947). Recall is lower for our model
in 20NG but F1 scores are higher for both datasets.
These results indicate that the classification-based
context model provides a favorable alternative to the
27
Removed
Reuters-10 20-Newsgroups
Accuracy F1 Accuracy F1
- 79.3 77.2 73.4 63.7
Ch(t) 76.2 72.3 71.9 61.0
Cr(t) 80.5 77.6 74.3 64.5
Ch(r) 78.4 75.7 73.1 63.4
Table 2: Ablation tests results.
state of the art LSA-based method.
Table 2 presents ablation tests of our model. In
each test we measured the classification performance
when one of the three classification scores is ignored.
Clearly, Ch(t) is the most beneficial component, and
in general the category classifiers help improving
overall performance. The limited performance of Cr
may be related to higher ambiguity in rules relative to
category names, resulting in noisier training data. In
addition, the small size of the training set limits the
number of training examples for rule classifiers. This
problem affects Cr more than Ch since, by nature,
the corpus includes more occurrences of category
names. Still, Cr contributes to improved recall (this
fact is not visible in Table 2).
The coverage of the utilized rule-set determines
the maximal (absolute) recall that can be achieved
by any model. With the rule-set we used in this ex-
periment, the recall upper bound was 59.1% for R10
and 40.6% for 20NG. However, rule coverage af-
fects precision as well: In many cases documents are
assigned to incorrect categories because the correct
category is not even a candidate as no entailing term
was matched for it in the document. For instance,
a document with the sentence ?For sale or trade!!!
BMW R60US. . . ? was classified by our method to
the category forsale, while its gold-standard category
is motorcycles. Yet, none of the rules in our rule-set
triggered motorcycles as a candidate category for this
document. Ideally, a context model would rule out
all incorrect candidate categories; in practice even a
single low score for one of the competing categories
results in a false positive error in such cases (in addi-
tion to the recall loss). To reduce these problems we
intend to employ additional knowledge resources in
future work.
Our algorithm for retrieving training examples
turned out to be not sufficiently accurate, particularly
for negative examples. This is a challenging task that
requires further research. Although useful for some
classifier types, the use of cohyponyms may retrieve
potentially positive examples as negative ones, since
terms that are considered cohyponyms in WordNet
are often perceived as near synonyms in common
usage, e.g. buyout and purchase in the context of
acquisitions. Likewise, using WordNet senses to de-
termine ambiguity is also inaccurate. Rare or too
fine-grained senses, common in WordNet, cause a
term to be considered ambiguous, which in turn trig-
gers the use of less accurate retrieval methods. For
example, auction has a bridge-related WordNet sense
which is irrelevant for our dataset, but made the term
be considered ambiguous. This calls for develop-
ment of other methods for determining word ambigu-
ity, which consider the actual usage of terms in the
domain rather than relying solely on WordNet.
9 Conclusions
In this paper we presented a generic classification-
based scheme for comprehensively addressing con-
text matching in textual inference scenarios. We
presented a concrete implementation of the proposed
scheme for Name-based TC, and showed how CP
decisions can be integrated within the TC setting.
Utilizing classifiers for context matching offers
several advantages. They naturally incorporate di-
rectionality and allow integrating various types of
information, including ones not used in this work
such as syntactic features. Our results indeed support
this approach. Still, further research is required re-
garding issues raised by the use of multiple classifiers,
scalability in particular.
Hypotheses in TC are available in advance. While
also the case in other applications, it constitutes a
practical challenge when hypotheses are given ?on-
line?, like Information Retrieval queries, since classi-
fiers will have to be generated on the fly. We intend
to address this issue in future work.
Lastly, we plan to apply the generic classification-
based approach to address context matching in other
inference-based applications.
Acknowledgments
This work was partially supported by the Israel Sci-
ence Foundation grant 1112/08 and the NEGEV
project (www.negev-initiative.org).
28
References
Libby Barak, Ido Dagan, and Eyal Shnarch. 2009. Text
Categorization from Category Name via Lexical Refer-
ence. In HLT-NAACL (Short Papers).
Shane Bergsma, Dekang Lin, and Randy Goebel. 2008.
Discriminative Learning of Selectional Preference from
Unlabeled Text. In In Proceedings of EMNLP.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of Machine
Learning Research, 3:993?1022, March.
Michael Connor and Dan Roth. 2007. Context Sensitive
Paraphrasing with a Global Unsupervised Classifier. In
Proceedings of ECML.
Ido Dagan, Oren Glickman, Alfio Gliozzo, Efrat Mar-
morshtein, and Carlo Strapparava. 2006. Direct Word
Sense Matching for Lexical Substitution. In Proceed-
ings of COLING-ACL.
Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan Roth.
2009. Recognizing Textual Entailment: Rational, Eval-
uation and Approaches. Natural Language Engineer-
ing, pages 15(4):1?17.
Scott Deerwester, Scott Deerwester, Susan T. Dumais,
George W. Furnas, Thomas K. Landauer, and Richard
Harshman. 1990. Indexing by Latent Semantic Anal-
ysis. Journal of the American Society for Information
Science, 41:391?407.
Georgiana Dinu and Mirella Lapata. 2010. Topic Models
for Meaning Similarity in Context. In Proceedings of
Coling 2010: Posters.
Doug Downey and Oren Etzioni. 2009. Look Ma, No
Hands: Analyzing the Monotonic Feature Abstraction
for Text Classification. In Proceedings of NIPS.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database (Language, Speech, and Com-
munication). The MIT Press.
William A. Gale, Kenneth W. Church, and David
Yarowsky. 1992. One Sense per Discourse. In Proceed-
ings of the workshop on Speech and Natural Language.
Alfio Gliozzo, Carlo Strapparava, and Ido Dagan. 2009.
Improving Text Categorization Bootstrapping via Unsu-
pervised Learning. ACM Trans. Speech Lang. Process.,
6:1:1?1:24, October.
Sanda M. Harabagiu, Steven J. Maiorano, and Marius A.
Pas?ca. 2003. Open-domain Textual Question Answer-
ing Techniques. Natural Language Engineering, 9:231?
267, September.
Thorsten Joachims. 2006. Training Linear SVMs in
Linear Time. In Proceedings of the ACM Conference
on Knowledge Discovery and Data Mining (KDD).
David Kauchak and Regina Barzilay. 2006. Paraphras-
ing for Automatic Evaluation. In Proceedings of HLT-
NAACL.
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-Geffet. 2010. Directional Distributional
Similarity for Lexical Inference. Natural Language
Engineering, 16(4):359?389.
Diana McCarthy and Roberto Navigli. 2009. The English
Lexical Substitution Task. Language Resources and
Evaluation, 43(2):139?159.
Quinn McNemar. 1947. Note on the Sampling Error
of the Difference between Correlated Proportions or
Percentages. Psychometrika, 12(2):153?157, June.
Roberto Navigli. 2009. Word Sense Disambiguation: A
Survey. ACM Computing Surveys, 41(2):1?69.
Patrick Pantel, Rahul Bhagat, Bonaventura Coppola, Timo-
thy Chklovski, and Eduard Hovy. 2007. ISP: Learning
Inferential Selectional Preferences. In Proceedings of
NAACL-HLT.
Marco Pennacchiotti, Roberto Basili, Diego De Cao, and
Paolo Marocco. 2007. Learning Selectional Prefer-
ences for Entailment or Paraphrasing Rules. In Pro-
ceedings of RANLP.
Alan Ritter, Mausam, and Oren Etzioni. 2010. A La-
tent Dirichlet Allocation Method for Selectional Prefer-
ences. In Proceedings of ACL.
Eyal Shnarch, Libby Barak, and Ido Dagan. 2009. Ex-
tracting Lexical Reference Rules from Wikipedia. In
Proceedings of IJCNLP-ACL.
Noah A. Smith and Jason Eisner. 2005. Contrastive
Estimation: Training Log-linear Models on Unlabeled
Data. In Proceedings of ACL.
Idan Szpektor, Ido Dagan, Roy Bar-Haim, and Jacob Gold-
berger. 2008. Contextual Preferences. In Proceedings
of ACL-08: HLT.
29
Proceedings of the 7th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 29?35,
Sofia, Bulgaria, August 8 2013. c?2013 Association for Computational Linguistics
Semi-automatic Construction of Cross-period Thesaurus
Chaya Liebeskind, Ido Dagan, Jonathan Schler
Computer Science Department
Bar-Ilan University
Ramat-Gan, Israel
liebchaya@gmail.com, dagan@cs.biu.ac.il, schler@gmail.com
Abstract
Cross-period (diachronic) thesaurus con-
struction aims to enable potential users to
search for modern terms and obtain se-
mantically related terms from earlier pe-
riods in history. This is a complex task not
previously addressed computationally. In
this paper we introduce a semi-automatic
iterative Query Expansion (QE) scheme
for supporting cross-period thesaurus con-
struction. We demonstrate the empirical
benefit of our scheme for a Jewish cross-
period thesaurus and evaluate its impact on
recall and on the effectiveness of lexicog-
rapher manual effort.
1 Introduction and Background
In the last decade, there is a growing interest in ap-
plying Natural Language Processing (NLP) meth-
ods to historical texts due to the increased avail-
ability of these texts in digital form (Sporleder,
2010; Sa?nchez-Marco et al, 2011; Piotrowski,
2012). The specific linguistic properties of histor-
ical texts, such as nonstandard orthography, gram-
mar and abbreviations, pose special challenges for
NLP. One of this challenges, which has not been
addressed so far, is the problem of bridging the
lexical gap between modern and ancient language.
In this paper, we address the interesting task
of cross-period thesaurus (a.k.a. diachronic the-
saurus) construction. A thesaurus usually contains
thousands of entries, denoted here as target terms.
Each entry includes a list of related terms, cover-
ing various semantic relations. A cross-period the-
saurus aims to enable the potential user to search
for a modern term and get related terms from ear-
lier periods. Thus, in a cross-period thesaurus
the target terms are modern while their related
terms are ancient. In many cases, while the actual
modern term (or its synynom) does not appear in
earlier historical periods, different aspects of that
term were mentioned. For example, in our Jewish
historical corpora, the modern term birth control,
has no equivalent ancient term, However, different
contraceptive methods were described in our his-
torical texts that are semantically similar to birth
control. Thus, a related term is considered sim-
ilar to the target term when it refers to the same
concept.
The goal of our research is to support con-
structing a high-quality publishable thesaurus, as
a cultural resource on its own, alongside being
a useful tool for supporting searches in the do-
main. Since the precision of fully automatically-
constructed thesauri is typically low (e.g. (Mi-
halcea et al, 2006)), we present a semi-automatic
setting for supporting thesaurus construction by a
domain expert lexicographer. Our recall-oriented
setting assumes that manual effort is worthwhile
for increasing recall as long as it is being utilized
effectively.
Corpus-based thesaurus construction is an ac-
tive research area (Curran and Moens, 2002; Kil-
garriff, 2003; Rychly? and Kilgarriff, 2007; Liebe-
skind et al, 2012; Zohar et al, 2013). Typi-
cally, two statistical approaches for identifying se-
mantic relatedness between words were investi-
gated: first-order (co-occurrence-based) similarity
and second-order (distributional) similarity (Lin,
1998; Gasperin et al, 2001; Weeds and Weir,
2003; Kotlerman et al, 2010). In this research,
we focus on statistical measures of first-order sim-
ilarity (see Section 2). These methods were found
to be effective for thesaurus construction as stand-
alone methods and as complementary to second-
order methods (Peirsman et al, 2008). First-order
measures assume that words that frequently occur
together are topically related (Schu?tze and Peder-
sen, 1997). Thus, co-occurrence provides an ap-
propriate approach to identify highly related terms
for the thesaurus entries.
29
In general, there are two types of historically-
relevant corpora: ancient corpora of ancient lan-
guage, and modern corpora with references and
mentions to ancient language (termed here mixed
corpora). Since in our setting the thesaurus? target
terms are modern terms, which do not appear in
ancient corpora, co-occurrence methods would be
directly applicable only over a mixed corpus. In
a preliminary experiment, we applied the Liebe-
skind et al (2012) algorithmic scheme, which ap-
plies first-order similarity and morphological as-
pects of corpus-based thesaurus construction, on
a mixed corpus of our historical domain. We ob-
served that the target terms had low frequency in
this corpus. Since statistical co-occurrence mea-
sures have poor performance over low statistics,
the experiment?s results were not satisfactory. We
therefore looked for ways to increase the number
of documents in the statistical extraction process,
and decided that applying query expansion (QE)
techniques might be a viable solution.
We recognized two potential types of sources
of lexical expansions for the target terms. The
first is lexical resources available over the inter-
net for extracting different types of semantic rela-
tions (Shnarch et al, 2009; Bollegala et al, 2011;
Hashimoto et al, 2011). The second is lists of
related terms extracted from a mixed corpus by
a first-order co-occurrence measure. These lists
contain both ancient and modern terms. Although
only ancient terms will be included in the final
thesaurus, modern terms can be utilized for QE
to increase thesaurus coverage. Furthermore, ex-
panding the target term with ancient related terms
enables the use of ancient-only corpora for co-
occurrence extraction.
Following these observations, we present an it-
erative interactive QE scheme for bootstrapping
thesaurus construction. This approach is used to
bridge the lexical gap between modern and ancient
terminology by means of statistical co-occurrence
approaches. We demonstrate the empirical advan-
tage of our scheme over a cross-period Jewish do-
main and evaluate its impact on recall and on the
effectiveness of the lexicographer manual effort.
The remainder of this paper is organized as fol-
lows: we start with a description of the statistical
thesaurus construction method that we utilize in
our scheme. Our main contribution of the itera-
tive scheme is described in Section 3, followed by
a case-study in Section 4 and evaluation and sum-
mary in Sections 5 and 6.
2 Automatic Thesaurus Construction
Automatic thesaurus construction focuses on the
process of extracting a ranked list of candidate
related terms (termed candidate terms) for each
given target term. We assume that the top ranked
candidates will be further examined (manually) by
a lexicographer, who will select the eventual re-
lated terms for the thesaurus entry.
Statistical measures of first-order similarity
(word co-occurrence), such as Dice coefficient
(Smadja et al, 1996) and Pointwise Mutual In-
formation (PMI) (Church and Hanks, 1990), were
commonly used to extract ranked lists of candi-
date related terms. These measures consider the
number of times in which each candidate term co-
occurs with the target term, in the same document,
relative to their total frequencies in the corpus.
In our setting, we construct a thesaurus for a
morphologically rich language (Hebrew). There-
fore, we followed the Liebeskind et al (2012) al-
gorithmic scheme designed for these cases, sum-
marized below. First, our target term is repre-
sented in its lemma form. For each target term we
retrieve all the corpus documents containing this
given target term. Then, we define a set of candi-
date terms, which are represented in their surface
form, that consists of all the terms in all these doc-
uments. Next, the Dice co-occurrence score be-
tween the target term and each of the candidates
is calculated, based on their document-level statis-
tics in the corpus. After sorting the terms based on
their scores, the highest rated candidate terms are
clustered into lemma-based clusters. Finally, we
rank the clusters by summing the co-occurrence
scores of their members and the highest rated clus-
ters constitute the candidate terms for the given
target term, to be presented to a domain expert.
3 Iterative Semi-automatic Scheme for
Cross-period Thesaurus Construction
As explained in Section 1, our research focuses
on a semi-automatic setting for supporting cross-
period thesaurus construction by a lexicographer.
In this work, we assume that a list of modern tar-
get terms is given as input. Then, we automatically
extract a ranked list of candidate related terms for
each target term using statistical measures, as de-
tailed in Section 2. Notice that at this first step re-
lated terms can be extracted only from the mixed
30
corpora, in which the given (modern) target term
may occur. Next, a lexicographer manually se-
lects, from the top ranked candidates, ancient re-
lated terms for the thesaurus entry as well as terms
for QE. The QE terms may be either ancient or
modern terms from the candidate list, or terms
from a lexical resource. Our iterative QE scheme
iterates over the QE terms. In each iteration, a QE
term replaces the target term?s role in the statistics
extraction process. Candidate related terms are
extracted for the QE term and the lexicographer
judges their relevancy with respect to the original
target term. Notice that if the QE term is modern,
only the mixed corpora can be utilized. However,
if the QE term is ancient, the ancient corpora are
also utilized and may contribute additional related
terms.
The algorithmic scheme we developed for the-
saurus construction is illustrated in Figure 1. Our
input is a modern target term. First, we au-
tomatically extract candidates by statistical co-
occurrence measures, as described in Section 2.
Then, a domain-expert annotates the candidates.
The manual selection process includes two de-
cisions on each candidate (either modern or an-
cient): (i) whether the candidate is related to the
target term and should be included in its thesaurus
entry, and (ii) whether this candidate can be used
as a QE term for the original target term. The
second decision provides input to the QE process,
which triggers the subsequent iterations. Follow-
ing the first decision we filter the modern terms
and include only ancient ones in the actual the-
saurus.
The classification of a candidate term as ancient
or modern is done automatically by a simple clas-
sification rule: If a term appears in an ancient cor-
pus, then it is necessarily an ancient term; other-
wise, it is a modern term (notice that the converse
is not true, since an ancient term might appear in
modern documents).
In parallel to extracting candidate related terms
from the corpus, we extract candidate terms also
from our lexical resources, and the domain expert
judges their fitness as well. Our iterative process
is applied over the expansions list. In each itera-
tion, we take out an expansion term and automat-
ically extract related candidates for it. Then, the
annotator selects both ancient related terms for the
thesaurus and suitable terms, either modern or an-
cient, for the expansion list for further iterations.
Figure 1: Semi-automatic Algorithmic Scheme
For efficiency, only new candidates that were not
judged in pervious iterations are given for judge-
ment. The stopping criterion is when there are no
additional expansions in the expansions list.
Since the scheme is recall-oriented, the aim of
the annotation process is to maximize the the-
saurus coverage. In each iteration, the domain
expert annotates the extracted ranked list of can-
didate terms until k sequential candidates were
judged as irrelevant. This stopping criterion for
each iteration controls the efforts to increase recall
while maintaining a low, but reasonable precision.
In our setting, we extract ancient related terms
for modern terms. Therefore, in order to utilize
co-occurrence statistics extraction, our scheme re-
quires both ancient and mixed corpora, where
the first iteration utilizes only the mixed corpora.
Then, our iterative scheme enables subsequent it-
erations to utilize the ancient corpora as well.
4 Case Study: Cross-period Jewish
Thesaurus
Our research targets the construction of a cross-
period thesaurus for the Responsa project1. The
corpus includes questions on various daily issues
posed to rabbis and their detailed rabbinic an-
swers, collected over fourteen centuries, and was
used for previous IR and NLP research (Choueka
et al, 1971; Choueka et al, 1987; HaCohen-
Kerner et al, 2008; Liebeskind et al, 2012; Zohar
et al, 2013).
The Responsa corpus? documents are divided to
four periods: the 11th century until the end of the
15th century, the 16th century, the 17th through
the 19th centuries, and the 20th century until to-
1Corpus kindly provided: http://biu.ac.il/jh/Responsa/
31
day. We considered the first three periods as our
ancient corpora along with the RaMBaM (Hebrew
acronym for Rabbi Mosheh Ben Maimon) writ-
ings from the 12th century. For the mixed corpus
we used the corpus? documents from the last pe-
riod, but due to relatively low volume of modern
documents we enriched it with additional modern
collections (Tchumin collection 2, ASSIA (a Jour-
nal of Jewish Ethics and Halacha), the Medical-
Halachic Encyclopedia3, a collection of questions
and answers written by Rabbi Shaul Israeli4, and
the Talmudic Encyclopedia (a Hebrew language
encyclopedia that summarizes halachic topics of
the Talmud in alphabetical order). Hebrew Wik-
tionary was used as a lexical resource for syn-
onyms.
For statistics extraction, we applied (Liebeskind
et al, 2012) algorithmic scheme using Dice coef-
ficient as our co-occurrence measure (see Section
2). Statistics were calculated over bigrams from
corpora consisting of 81993 documents.
5 Evaluation
5.1 Evaluation Setting
We assessed our iterative algorithmic scheme by
evaluating its ability to increase the thesaurus cov-
erage, compared to a similar non-iterative co-
occurrence-based thesaurus construction method.
In our experiments, we assumed that it is worth
spending the lexicographer?s time as long as it is
productive, thus, all the manual annotations were
based on the lexicographer efforts to increase re-
call until reaching the stopping criterion.
We used Liebeskind et al (2012) algorithmic
scheme as our non-iterative baseline (Baseline).
For comparison, we ran our iterative scheme, cal-
culated the average number of judgments per tar-
get term (88) and set the baseline stopping crite-
rion to be the same number of judgements per tar-
get. Thus, we ensured that the number of judge-
ments for our iterative algorithm and for the base-
line is equal, and thus coverage increase is due to a
better use of lexicographer?s effort. For complete-
ness, we present the results of the non-iterative al-
gorithm with the stopping criterion of the iterative
algorithm, when reaching k (k=10 was empirically
2http://www.zomet.org.il/?CategoryID=170
3http://medethics.org.il/website/index.php/en/research-
2/encyclopedia-of-jewish-medical-ethics
4http://www.eretzhemdah.org/data/uploadedfiles/ebooks/14-
sfile.pdf
Method RT R Pro J
First-iteration 50 0.31 0.038 1307
Baseline 63 0.39 0.024 2640
Iterative 151 0.94 0.057 2640
Table 1: Results Comparison
selected in our case) sequential irrelevant candi-
dates (First-iteration).
To evaluate our scheme?s performance, we used
several measures: total number of ancient related
terms extracted (RT), relative recall (R) and pro-
ductivity (Pro). Since we do not have any pre-
defined thesaurus, our micro-averaged relative-
recall considered the number of ancient related
terms from the output of both methods (baseline
and iterative) as the full set of related terms. Pro-
ductivity was measured by dividing the total num-
ber of ancient related terms extracted (RT) by the
total number of the judgments performed for the
method (J).
5.2 Results
Table 1 compares the performance of our semi-
automatic iterative scheme with that of the base-
line over a test set of 30 modern target terms. Our
iterative scheme increases the average number of
extracted related terms from 2.1 to 5, i.e., increas-
ing recall by 240%. The relative recall of the first-
iteration (0.31) is included in the relative recall of
both the baseline and our iterative method. Iterat-
ing over the first iteration increases recall by 300%
(from 50 to 151 terms), while adding more judge-
ments to the non-iterative method increases recall
only by 26% (to 63 terms). The productivity of the
iterative process is higher even than the productiv-
ity of the first iteration, showing that the iterative
process optimizes the lexicographer?s manual ef-
fort.
Table 2 shows examples of thesaurus target
terms and their ancient related terms, which were
added by our iterative scheme5. Since the related
terms are ancient Halachic terms, we explain them
rather than translate them to English.
We further analyze our scheme by comparing
the use of ancient versus modern terms in the itera-
tive process. Although modern related terms were
not included in our cross-period thesaurus, in the
judgement process the lexicographer judged their
5To facilitate readability we use a transliteration of He-
brew using Roman characters; the letters used, in Hebrew
lexico-graphic order, are abgdhwzxTiklmns`pcqrs?t.
32
Figure 2: The extraction of ancient terms versus modern terms in the iterative process
Target term Related term
zkwiwt iwcrim (copyright) hsgt gbwl (trespassing)
iwrd lamnwt xbrw ([competitively] enter his friend?s profession)
`ni hmhpk bxrrh (a poor man is deciding whether to buy a cake and another
person comes and takes it)
hmtt xsd (euthanasia) rwb gwssin lmith (most dying people die)
xii s?`h (living for the moment)
hpsqt hriwn (abortion) xwtkin h`wbr (killing the fetus)
hwrg nps? (killing a person)
rwdp (pursuer, a fetus endangering its mother?s life)
tiknwn hms?pxh (birth control) s?lws? ns?im ms?ms?wt bmwk (three types of women allowed to use cotton di-
aphragm)
ds? mbpnim wzwrh mbxwc (withdrawal method)
hprt xwzh (breach of contract) biTwl mqx (cancelling a purchase)
dina dgrmi (indirect damage)
mqx t`wt (erroneous bargain)
srwb pqwdh (insubordination) mwrd bmlkwt (rebel against the sovereign [government])
imrh at pik (to disobey)
Avner and khni Nob (a biblical story: king Saul ordered to slay Ahimilech to-
gether with 85 priests. Avner, the captain of Saul?s guard, disobeyed the order.)
Table 2: Examples for the iterative scheme?s contribution
relevancy too. In Figure 2, we report the number
of modern related terms in comparison to the num-
ber of ancient related terms for each iteration. In
parallel, we illustrate the number of ancient expan-
sions in proportion to the number of modern ex-
pansions. The x-axis? values denote the iterations,
while the y-axis? values denote the number of ex-
pansions and related terms respectively. For each
iteration, the expansions chart presents the expan-
sions that were extracted while the related terms
chart presents the extracted related terms, of which
the ancient ones were included in the thesaurus.
Since the input for our scheme is a modern target
terms, the first iteration extracted more modern re-
lated terms than ancient terms and utilized more
modern expansions than ancient. However, this
proportion changed in the second iteration, prob-
ably thanks to the ancient expansions retrieved in
the first iteration.
Although there are often mixed results on
the effectiveness of QE for information retrieval
(Voorhees, 1994; Xu and Croft, 1996), our results
show that QE for thesaurus construction in an iter-
ative interactive setting is beneficial for increasing
thesaurus? coverage substantially.
6 Conclusions and Future Work
We introduced an iterative interactive scheme for
cross-period thesaurus construction, utilizing QE
techniques. Our semi-automatic algorithm signif-
icantly increased thesaurus coverage, while op-
timizing the lexicographer manual effort. The
scheme was investigated for Hebrew, but can be
generically applied for other languages.
We plan to further explore the suggested scheme
by utilizing additional lexical resources and QE
algorithms. We also plan to adopt second-order
distributional similarity methods for cross-period
thesaurus construction.
33
References
D. Bollegala, Y. Matsuo, and M. Ishizuka. 2011.
A web search engine-based approach to mea-
sure semantic similarity between words. Knowl-
edge and Data Engineering, IEEE Transactions on,
23(7):977?990.
Yaacov Choueka, M. Cohen, J. Dueck, Aviezri S.
Fraenkel, and M. Slae. 1971. Full text document
retrieval: Hebrew legal texts. In SIGIR, pages 61?
79.
Yaacov Choueka, Aviezri S. Fraenkel, Shmuel T. Klein,
and E. Segal. 1987. Improved techniques for pro-
cessing queries in full-text systems. In SIGIR, pages
306?315.
Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicog-
raphy. Comput. Linguist., 16(1):22?29, March.
James R. Curran and Marc Moens. 2002. Improve-
ments in automatic thesaurus extraction. In Pro-
ceedings of the ACL-02 workshop on Unsupervised
lexical acquisition - Volume 9, ULA ?02, pages 59?
66, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Caroline Gasperin, Pablo Gamallo, Alexandre Agus-
tini, Gabriel Lopes, Vera De Lima, et al 2001. Us-
ing syntactic contexts for measuring word similarity.
In Workshop on Knowledge Acquisition and Catego-
rization, ESSLLI, Helsinki, Finland.
Yaakov HaCohen-Kerner, Ariel Kass, and Ariel Peretz.
2008. Combined one sense disambiguation of ab-
breviations. Proceedings of ACL08: HLT, Short Pa-
pers, pages 61?64.
Chikara Hashimoto, Kentaro Torisawa, Stijn
De Saeger, Junichi Kazama, and Sadao Kuro-
hashi. 2011. Extracting paraphrases from definition
sentences on the web. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 1087?1097.
Adam Kilgarriff. 2003. Thesauruses for natu-
ral language processing. In Proceedings of the
Joint Conference on Natural Language Processing
and Knowledge Engineering, pages 5?13, Beijing,
China.
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-geffet. 2010. Directional distributional
similarity for lexical inference. Nat. Lang. Eng.,
16(4):359?389, October.
Chaya Liebeskind, Ido Dagan, and Jonathan Schler.
2012. Statistical thesaurus construction for a mor-
phologically rich language. In *SEM 2012: The
First Joint Conference on Lexical and Computa-
tional Semantics, pages 59?64, Montre?al, Canada,
7-8 June. Association for Computational Linguis-
tics.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 36th Annual
Meeting of the Association for Computational Lin-
guistics and 17th International Conference on Com-
putational Linguistics - Volume 2, ACL ?98, pages
768?774, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Rada Mihalcea, Courtney Corley, and Carlo Strappa-
rava. 2006. Corpus-based and knowledge-based
measures of text semantic similarity. In Proceedings
of the 21st national conference on Artificial intelli-
gence - Volume 1, AAAI?06, pages 775?780. AAAI
Press.
Yves Peirsman, Kris Heylen, and Dirk Speelman.
2008. Putting things in order. First and second order
context models for the calculation of semantic sim-
ilarity. In 9es Journe?es internationales d?Analyse
statistique des Donne?es Textuelles (JADT 2008).
Lyon, France.
Michael Piotrowski. 2012. Natural language process-
ing for historical texts. Synthesis Lectures on Hu-
man Language Technologies, 5(2):1?157.
Pavel Rychly? and Adam Kilgarriff. 2007. An ef-
ficient algorithm for building a distributional the-
saurus (and other sketch engine developments). In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
ACL ?07, pages 41?44, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Cristina Sa?nchez-Marco, Gemma Boleda, and Llu??s
Padro?. 2011. Extending the tool, or how to anno-
tate historical language varieties. In Proceedings of
the 5th ACL-HLT Workshop on Language Technol-
ogy for Cultural Heritage, Social Sciences, and Hu-
manities, LaTeCH ?11, pages 1?9, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Hinrich Schu?tze and Jan O. Pedersen. 1997. A
cooccurrence-based thesaurus and two applications
to information retrieval. Inf. Process. Manage.,
33(3):307?318, May.
Eyal Shnarch, Libby Barak, and Ido Dagan. 2009. Ex-
tracting lexical reference rules from wikipedia. In
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing
of the AFNLP: Volume 1 - Volume 1, ACL ?09, pages
450?458, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Frank Smadja, Kathleen R. McKeown, and Vasileios
Hatzivassiloglou. 1996. Translating collocations for
bilingual lexicons: a statistical approach. Comput.
Linguist., 22(1):1?38, March.
Caroline Sporleder. 2010. Natural language process-
ing for cultural heritage domains. Language and
Linguistics Compass, 4(9):750?768.
34
Ellen M. Voorhees. 1994. Query expansion using
lexical-semantic relations. In Proceedings of the
17th annual international ACM SIGIR conference
on Research and development in information re-
trieval, SIGIR ?94, pages 61?69, New York, NY,
USA. Springer-Verlag New York, Inc.
Julie Weeds and David Weir. 2003. A general frame-
work for distributional similarity. In Proceedings of
the 2003 conference on Empirical methods in nat-
ural language processing, EMNLP ?03, pages 81?
88, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Jinxi Xu and W. Bruce Croft. 1996. Query expan-
sion using local and global document analysis. In
Proceedings of the 19th annual international ACM
SIGIR conference on Research and development in
information retrieval, SIGIR ?96, pages 4?11, New
York, NY, USA. ACM.
Hadas Zohar, Chaya Liebeskind, Jonathan Schler, and
Ido Dagan. 2013. Automatic thesaurus construction
for cross generation corpus. Journal on Comput-
ing and Cultural Heritage (JOCCH), 6(1):4:1?4:19,
April.
35
Proceedings of the Eighteenth Conference on Computational Language Learning, pages 87?97,
Baltimore, Maryland USA, June 26-27 2014.
c?2014 Association for Computational Linguistics
Focused Entailment Graphs for Open IE Propositions
Omer Levy
?
Ido Dagan
?
Jacob Goldberger
?
? Computer Science Department ? Faculty of Engineering
Bar-Ilan University
Ramat-Gan, Israel
{omerlevy,dagan,goldbej}@{cs,cs,eng}.biu.ac.il
Abstract
Open IE methods extract structured propo-
sitions from text. However, these propo-
sitions are neither consolidated nor gen-
eralized, and querying them may lead
to insufficient or redundant information.
This work suggests an approach to or-
ganize open IE propositions using entail-
ment graphs. The entailment relation uni-
fies equivalent propositions and induces a
specific-to-general structure. We create a
large dataset of gold-standard proposition
entailment graphs, and provide a novel
algorithm for automatically constructing
them. Our analysis shows that predicate
entailment is extremely context-sensitive,
and that current lexical-semantic resources
do not capture many of the lexical infer-
ences induced by proposition entailment.
1 Introduction
Open information extraction (open IE) extracts
natural language propositions from text without
pre-defined schemas as in supervised relation ex-
traction (Etzioni et al., 2008). These proposi-
tions represent predicate-argument structures as
tuples of natural language strings. Open IE en-
ables knowledge search by aggregating billions of
propositions from the web
1
. It may also be per-
ceived as capturing an unsupervised knowledge
representation schema, complementing supervised
knowledge bases such as Freebase (Bollacker et
al., 2008), as suggested by Riedel et al (2013).
However, language variability obstructs open IE
from becoming a viable knowledge representation
framework. As it does not consolidate natural lan-
guage expressions, querying a database of open IE
propositions may lead to either insufficient or re-
dundant information. As an illustrative example,
1
See demo: openie.cs.washington.edu
querying the demo (footnote 1) for the generally
equivalent relieves headache or treats headache
returns two different lists of entities; out of the top
few results, the only answers these queries seem
to agree on are caffeine and sex. This is a major
drawback relative to supervised knowledge rep-
resentations, which map natural language expres-
sions to structured formal representations, such as
treatments in Freebase.
In this work, we investigate an approach for or-
ganizing and consolidating open IE propositions
using the novel notion of proposition entailment
graphs (see Figure 1) ? graphs in which each
node represents a proposition and each directed
edge reflects an entailment relation, in the spirit
of textual entailment (Dagan et al., 2013). En-
tailment provides an effective structure for ag-
gregating natural-language based information; it
merges semantically equivalent propositions into
cliques, and induces specification-generalization
edges between them. For example, (aspirin, elim-
inate, headache) entails, and is more specific than,
(headache, respond to, painkiller).
We thus propose the task of constructing an
entailment graph over a set of open IE proposi-
tions (Section 3), which is closely related to Be-
rant et al?s work (2012) who introduced predicate
entailment graphs. In contrast, our work explores
propositions, which are essentially predicates in-
stantiated with arguments, and thus semantically
richer. We provide a dataset of 30 such graphs,
which represent 1.5 million pairwise entailment
decisions between propositions (Section 4).
To approach this task, we extend the state-of-
the-art method for building entailment graphs (Be-
rant et al., 2012) from predicates to complete
propositions. Both Snow et al (2006) and Berant et
al used WordNet as distant supervision when train-
ing a local pairwise model of lexical entailment.
However, analyzing our data revealed that the lex-
ical inferences captured in WordNet are quite dif-
87
Figure 1: An excerpt from a proposition entailment graph focused on the topic headache. The dashed boundaries in the figure
denote cliques, meaning that all propositions within them are equivalent.
ferent from the real lexical inferences induced by
proposition entailment, making WordNet a mis-
leading form of supervision. We therefore employ
direct proposition-level supervision, and design a
probabilistic model that captures the underlying
lexical-component inferences (Section 5). We ex-
plore a variety of natural extensions to prior art as
baselines (Section 6) and show that our model out-
performs them (Section 7).
While our model increases performance on this
task, there is still much room for improvement. A
deeper analysis (Section 8) shows that common
lexical-semantic resources, on which we rely as
well, are either too noisy or provide inadequate re-
call regarding lexical entailment. In particular, we
find that predicate inference within propositions
often goes beyond inference between the predi-
cates? linguistic meanings. While pneumonia re-
quires antibiotics and pneumonia is treated by an-
tibiotics mean the same, the inherent meanings of
require and treat are different. These inferences
pertain to specific world knowledge, and warrant
future research.
Our work also contributes to textual entailment
research. First, we extend entailment graphs to
complete propositions. Secondly, we investigate
an intermediate problem of recognizing entail-
ment between language-based predicate-argument
tuples. Though this problem is simpler than
sentence-level entailment, it does capture entail-
ment of complete statements, which proves to be
quite challenging indeed.
2 Background
Our work builds upon two major research threads:
open IE, and entailment graphs.
2.1 Open Information Extraction
Research in open IE (Etzioni et al., 2008) has fo-
cused on transforming text to predicate-argument
tuples (propositions). The general approach is to
learn proposition extraction patterns, and use them
to create tuples while denoting extraction confi-
dence. Various methods differ in the type of pat-
terns they acquire. For instance, (Banko et al.,
2007) and (Fader et al., 2011) used surface pat-
terns, while (Mausam et al., 2012) and (Xu et al.,
2013) used syntactic dependencies.
Yates and Etzioni (2009) tried to mitigate the
issue of language variability (as exemplified in
the introduction) by clustering synonymous predi-
cates and arguments. While these clusters do con-
tain semantically related items, they do not neces-
sarily reflect equivalence or implication. For ex-
ample, coffee, tea, and caffeine may all appear
in one cluster, but coffee does not imply tea; on
the other hand, separating any element from this
cluster removes a valid implication. Entailment,
however, can capture the fact that both beverages
imply caffeine, but not one another. Also related,
Riedel et al (2013) try to generalize over open IE
extractions by combining knowledge from Free-
base and globally predicting which unobserved
propositions are true. In contrast, our work identi-
fies inference relations between concrete pairs of
observed propositions.
2.2 Entailment Graphs of Words and Phrases
Previous work focused on entailment graphs or
similar structures at the sub-propositional level.
In these graphs, each node represents a natu-
ral language word or phrase, and each directed
edge an entailment (or generalization) relation.
Snow et al (2006) created a taxonomy of sense-
88
disambiguated nouns and their hyponymy rela-
tions. Berant et al (2012) constructed entailment
graphs of predicate templates. Recently, Mehdad
et al (2013) built an entailment graph of noun
phrases and partial sentences for topic labeling.
The notion of proposition entailment graphs, how-
ever, is novel. This distinction is critical, be-
cause apparently, entailment in the context of spe-
cific propositions does not behave like context-
oblivious lexical entailment (see Section 8).
Berant et al?s work was implemented in Adler
et al?s (2012) text exploration demo, which instan-
tiated manually-annotated predicate entailment
graphs with arguments, and used an additional
lexical resource to determine argument entail-
ment. The combined graphs of predicate and argu-
ment entailments induced a proposition entailment
graph, which could then be explored in a faceted-
search scheme. Our work goes beyond, and at-
tempts to build entailment graphs of propositions
automatically.
2.2.1 Berant et al?s Algorithm for Predicate
Entailment Graph Construction
We present Berant et al?s algorithm in detail, as we
rely on it later on. Given a set of predicates {i}
1..n
as input (constituting the graph nodes), it returns
a set of entailment decisions (i, j), which become
the directed edges of the entailment graph. The
method works in two phases: (1) local estimation,
and (2) global optimization.
The local estimation model considers every po-
tential edge (i, j) and estimates the probability p
ij
that this edge indeed exists, i.e. that i entails j.
Each predicate pair is represented with distribu-
tional similarity features, providing some indica-
tion of whether i entails j. The estimator then uses
logistic regression (or a linear SVM) over those
features to predict the probability of entailment. It
is trained with distant supervision from WordNet,
employing synonyms, hypernyms, and (WordNet)
entailments as positive examples, and antonyms,
hyponyms, and cohyponyms as negative.
The global optimization phase then searches
for the most probable transitive entailment graph,
given the local probability estimations. It does so
with an integer linear program (ILP), where each
pair of predicates is represented by a binary vari-
able x
ij
, denoting whether there is an entailment
edge from i to j. The objective function corre-
sponds to the log likelihood of the assignment:
?
i 6=j
x
ij
(
log
(
p
ij
1?p
ij
)
+ log
(
pi
1?pi
))
. The prior
term pi is the probability of a random pair of pred-
icates to be in an entailment relation, and can be
estimated in advance. The ILP solver searches
for the optimal assignment that maximizes the ob-
jective function under transitivity constraints, ex-
pressed as linear constraints ?
i,j,k
x
ij
+ x
jk
?
x
ik
? 1.
3 Task Definition
A proposition entailment graph is a directed graph
where each node is a proposition s
i
(s for sen-
tence) and each edge (s
i
, s
j
) represents an en-
tailment relation from s
i
to s
j
. A proposi-
tion s
i
is a predicate-argument structure s
i
=
(
p
i
, a
1
i
, a
2
i
, ..., a
m
i
i
)
with one predicate p
i
and its
arguments. A proposition-level entailment (s
i
, s
j
)
holds if the verbalization of s
i
implies s
j
, accord-
ing to the definition of textual entailment (Dagan
et al., 2013); i.e. if humans reading s
i
would typi-
cally infer that s
j
is most likely true. Given a set of
propositions (graph nodes), the task of construct-
ing a proposition entailment graph is to recognize
all the entailments among the propositions, i.e.
deciding which directional edges connect which
pairs of nodes.
In this paper, we consider the narrower task
of constructing focused proposition entailment
graphs, following Berant et al?s methodology
in creating focused predicate entailment graphs.
First, all predicates are binary (have two argu-
ments) and are denoted s
i
=
(
a
1
i
, p
i
, a
2
i
)
. Sec-
ondly, we assume that the propositions were re-
trieved by querying for a particular concept; out
of the two arguments, one argument t (topic) is
common to all the propositions in a single graph.
We denote the non-topic argument as a
i
. Figure 1
presents an example of an informative entailment
graph focused on the topic headache.
Though confined, this setting still challenges
the state-of-the-art in textual entailment (see Sec-
tion 7). Moreover, these restrictions facilitate
piece-wise investigation of the entailment problem
(see Section 8).
4 Dataset
To construct our dataset of open IE extractions, we
found Google?s syntactic ngrams (Goldberg and
Orwant, 2013) as a useful source of high-quality
propositions. Based on a corpus of 3.5 million En-
glish books, it aggregates every syntactic ngram
89
? subtree of a dependency parse ? with at most
4 dependency arcs. The resource contains only
tree fragments that appeared at least 10 times in
the corpus, filtering out many low-quality syntac-
tic ngrams.
We extracted the syntactic ngrams that reflect
propositions, i.e. subject-verb-object fragments
where object modifies the verb with either dobj
or pobj. Prepositions in pobj were concatenated
to the verb (e.g. use with). In addition, both sub-
ject and object must each be a noun phrase con-
taining two tokens at most, which are either nouns
or adjectives. Each token in the extracted frag-
ments was then lemmatized using WordNet. After
lemmatization, we grouped all identical proposi-
tions and aggregated their counts. Approximately
68 million propositions were collected.
We chose 30 topics from the healthcare domain
(such as influenza, hiv, and penicillin). For each
topic, we collected the set of propositions con-
taining it, and manually filtered noisy extractions.
This yielded 30 high-quality sets of 5,714 propo-
sitions in total, where each set becomes the set of
nodes in a separate focused entailment graph. The
graphs range from 55 propositions (scurvy) to 562
(headache), with an average of over 190 proposi-
tions per graph. Summing the number of propo-
sition pairs within each graph amounts to a total
of 1.5 million potential entailment edges, which
makes it by far the largest annotated textual entail-
ment dataset to date.
We used a semi-automatic annotation process,
which dramatically narrows down the number of
manual decisions, and hence, the required anno-
tation time. In short, the annotators are given a
series of small clustering tasks before annotating
entailment between those clusters.
2
The annotation process was carried out by two
native English speakers, with the aid of encyclope-
dic knowledge for unfamiliar medical terms. The
agreement on a subset of five randomly sampled
graphs was ? = 0.77. Annotating a single graph
took about an hour and a half on average.
Positive entailment judgements constituted only
8.4% of potential edges, and were found to be
100% transitive. We observe that in nearly all of
those cases, a natural alignment between entail-
ing components occurs: predicates align with each
other, the topic is shared, and the remaining non-
2
The annotated dataset is publicly available on the first
author?s website.
topic argument aligns with its counterpart. Con-
sider the topic arthritis and the entailing proposi-
tion pair (arthritis, cause, pain)?(symptom, as-
sociate with, arthritis); cause?associate with,
while pain?symptom. Rarely, some mis-
alignments do occur; for instance (vaccine,
protects, body)?(vaccine, provides, protection).
However, it is almost always the case that proposi-
tions entail if and only if their aligned lexical com-
ponents entail as well.
5 Algorithm
In this section, we extend Berant et al?s algorithm
(2012) to construct entailment graphs of proposi-
tions. As described in Section 2.2.1, their method
first performs local estimation of predicate entail-
ment and then global optimization. We modify the
local estimation phase to estimate proposition en-
tailment instead, and then apply the same global
optimization in the second phase.
In Section 4, we observed the alignment-based
relationship between proposition and lexical en-
tailment. We leverage this observation to predict
proposition entailment with lexical entailment fea-
tures (as Berant et al), using the Component En-
tailment Conjunction (CEC) model in Section 5.1.
Following Snow et al (2006) and Berant et
al, we could train CEC using distant supervision
from WordNet. In fact, we did try this approach
(presented as baseline methods, Section 6) and
found that it performed poorly. Furthermore, our
analysis (Section 8) suggests that WordNet rela-
tions do not adequately capture the lexical infer-
ences induced by proposition-level entailment. In-
stead, we use a more realistic signal to train CEC ?
direct supervision from the annotated dataset. Sec-
tion 5.2 describes how we propagate proposition-
level entailment annotations to the latent lexical
components.
5.1 Component Entailment Conjunction
CEC assumes that proposition-level entailment
is the result of entailment within each pair of
aligned components, i.e. a pair of propositions
entail if and only if both their predicate and ar-
gument pairs entail. This assumption stems from
our observation of alignment in Section 4. Fur-
thermore, CEC leverages this interdependence to
learn separate predicate-entailment and argument-
entailment features through proposition-level su-
pervision.
90
Formally, for every ordered pair of propositions
(i, j) we denote proposition entailment as a binary
random variable x
s
ij
and predicate and argument
entailments as x
p
ij
and x
a
ij
, respectively. In our
setting, proposition entailment (x
s
ij
) is observed,
but component entailments (x
p
ij
, x
a
ij
) are hidden.
We use logistic regression, with features ?
p
ij
and
parameter w
p
, as a probabilistic model of predi-
cate entailment (and so for arguments with ?
a
ij
and
w
a
):
p
ij
= P
(
x
p
ij
= 1
?
?
?
?
p
ij
;w
p
)
= ?
(
?
p
ij
? w
p
)
a
ij
= P
(
x
a
ij
= 1
?
?
?
?
a
ij
;w
a
)
= ?
(
?
a
ij
? w
a
)
(1)
where ? is the sigmoid ? (z) =
1
1+e
?z
. We then
define proposition entailment as the conjunction of
its binary components: x
s
ij
= x
p
ij
?x
a
ij
. Therefore,
the probability of proposition entailment given the
component features is:
s
ij
= P
(
x
s
ij
= 1
?
?
?
?
p
ij
, ?
a
ij
;w
p
, w
a
)
= P
(
x
p
ij
= 1, x
a
ij
= 1
?
?
?
?
p
ij
, ?
a
ij
;w
p
, w
a
)
= P
(
x
p
ij
= 1
?
?
?
?
p
ij
;w
p
)
? P
(
x
a
ij
= 1
?
?
?
?
a
ij
;w
a
)
= p
ij
? a
ij
The proposition entailment probability is thus the
product of component entailment probabilities.
Given the proposition-level information
{
x
s
ij
}
,
the log-likelihood is:
` (w
p
, w
a
)=
?
i 6=j
logP
(
x
s
ij
?
?
?
?
p
ij
, ?
a
ij
;w
p
, w
a
)
=
?
i 6=j
(
x
s
ij
log (p
ij
a
ij
) +
(
1? x
s
ij
)
log (1? p
ij
a
ij
)
)
5.2 Learning Component Models
We wish to learn the model?s parameters (w
p
, w
a
).
Our approach uses direct proposition-level super-
vision from our annotated dataset to train the com-
ponent logistic regression models. Since compo-
nent entailment (x
p
ij
, x
a
ij
) is not observed in the
data, we apply the iterative EM algorithm (Demp-
ster et al., 1977). In the E-step we estimate their
probabilities from proposition-level labels (x
s
ij
),
and in the M-step we use those estimates as ?soft?
labels to learn the component-level model param-
eters (w
p
, w
a
).
E-Step During the E-step in iteration t + 1,
we compute the probability of component entail-
ments given the proposition entailment informa-
tion, based on the parameters at iteration t (w
p
t
,
w
a
t
). The predicate probabilities are given by:
c
p
ij
= P
(
x
p
ij
= 1
?
?
?
x
s
ij
, ?
p
ij
, ?
a
ij
;w
p
t
, w
a
t
)
(2)
and are computed with Bayes? law:
c
p
ij
=
?
?
?
1 if x
s
ij
= 1
p
t
ij
(
1?a
t
ij
)
1?p
t
ij
a
t
ij
if x
s
ij
= 0
(3)
where p
t
ij
is computed as in Equations 1, with the
parameters at iteration t (w
p
t
). Argument entail-
ment probabilities (c
a
ij
) are computed analogously.
M-Step In the M-step, we compute new values
for the parameters (w
p
t+1
, w
a
t+1
). In our case, there
is no closed-form formula for updating the param-
eters. Instead, at each iteration, we solve a sepa-
rate logistic regression for each component. While
we have each component model?s features (?
p
ij
,
assuming predicates for notation), we do not ob-
serve the component-level entailment labels (x
p
ij
);
instead, we obtain their probabilities (c
p
ij
) from the
expectation step.
To learn the parameters (w
p
t+1
, w
a
t+1
) from the
component entailment probabilities (c
p
ij
), we em-
ploy a weighted variant of logistic regression, that
can utilize ?soft? class labels (i.e. a probability
distribution over {0, 1}). To solve such a logistic
regression (e.g. for w
p
t+1
), we maximize the log-
likelihood:
`
(
w
p
t+1
)
=
?
ij
(
c
p
ij
log
(
P
(
x
p
ij
= 1
?
?
?
?
p
ij
;w
p
t+1
))
+
(
1? c
p
ij
)
log
(
P
(
x
p
ij
= 0
?
?
?
?
p
ij
;w
p
t+1
)))
For optimization, we calculate the derivative, and
use gradient ascent to update w
p
t+1
:
?w
p
t+1
=
?`
(
w
p
t+1
)
?w
p
t+1
=
?
ij
(
c
p
ij
? P
(
x
p
ij
= 1
?
?
?
?
p
ij
;w
p
t+1
))
?
p
ij
This optimization is concave, and therefore the
unique global maximum can be efficiently ob-
tained.
5.3 Features
Similar to Berant et al, we used three types of fea-
tures to describe both predicate pairs (?
p
ij
) and ar-
gument pairs (?
a
ij
): distributional similarities, lex-
ical resources, and string distances.
91
We used the entire database of 68 million ex-
tracted propositions (see Section 4) to create a
word-context matrix; context was defined as other
words that appeared in the same proposition, and
each word was represented as (string, role), role
being the location within the proposition, either
a
1
, p, or a
2
. The matrix was then normalized with
pointwise mutual information (Church and Hanks,
1990). We used various metrics to measure dif-
ferent types of similarities between each compo-
nent pair, including: cosine similarity, Lin?s sim-
ilarity (1998), inclusion (Weeds and Weir, 2003),
average precision, and balanced average precision
(Kotlerman et al., 2010). Weed?s and Kotlerman?s
metrics are directional (asymmetric) and indicate
the direction of a potential entailment relation.
These features were used for both predicates and
arguments. In addition, we used Melamud et al?s
(2013) method to learn a context-sensitive model
of predicate entailment, which estimates predicate
similarity in the context of the given arguments.
We leveraged the Unified Medical Language
System (UMLS) to check argument entailment,
using the parent and synonym relations. A single
feature indicated whether such a connection ex-
ists. We also used WordNet relations as features,
specifically: synonyms, hypernyms, entailments,
hyponyms, cohyponyms, antonyms. Each Word-
Net relation constituted a different feature for both
predicates and arguments.
Finally, we added a string equality feature and a
Levenshtein distance feature (Levenshtein, 1966)
for different spellings of the same word to both
predicate and argument feature vectors.
6 Baseline Methods
We consider four algorithms that naturally ex-
tend the state-of-the-art to propositions, while us-
ing distant supervision (from WordNet). Since
CEC uses direct supervision, we also examined
another (simpler) directly-supervised algorithm.
As a naive unsupervised baseline, we use Argu-
ment Equality, which returns ?entailing? if the ar-
gument pair is identical. Predicate Equality is de-
fined similarly for predicates.
Component-Level Distant Supervision The
following methods use distant supervision from
WordNet (as in Berant et al?s work, Section 2.2.1)
to explicitly train component-level entailment esti-
mators. Specifically, we train a logistic regression
model for each component as specified in Equa-
tions 1 in Section 5.1. We present four methods,
which differ in the way they obtain global graph-
level entailment decisions for propositions, based
on the local component entailment estimates (p
ij
,
a
ij
in Section 5.1).
The first method, Opt(Arg ? Pred), uses the
product of both component models to estimate lo-
cal proposition-level entailment: s
ij
= p
ij
? a
ij
.
The global set of proposition entailments is then
determined using Berant et al?s global optimiza-
tion, according to the proposition-level scores s
ij
.
Note that this method is identical to CEC dur-
ing inference, but differs in the way the local es-
timators are learned (with component-level super-
vision from WordNet).
An alternative is Opt(Arg) ? Opt(Pred). It
first obtains local probabilities (p
ij
, a
ij
) for each
component as in Opt(Arg ? Pred), but then em-
ploys component-level global optimization (tran-
sitivity enforcement), yielding two sets of entail-
ment decisions, x
p
ij
and x
a
ij
. Proposition entail-
ment is then determined by the conjunction x
s
ij
=
x
p
ij
? x
a
ij
, as in (Adler et al., 2012).
Finally, Opt(Arg) ignores the predicate com-
ponent. Instead, it uses only the argument en-
tailment graph (as produced by Opt(Arg) ?
Opt(Pred)) to decide on proposition entailment;
i.e. a pair of propositions entail if and only if their
arguments entail. Opt(Pred) is defined analo-
gously.
Proposition-Level Direct Supervision A sim-
pler alternative to CEC that also employs
proposition-level supervision is Joint Features,
which concatenates the component level features
into a unified feature vector: ?
s
ij
= ?
p
ij
? ?
a
ij
. We
then couple them with the gold-standard annota-
tions x
s
ij
to create a training set for a single logistic
regression. We use the trained logistic regression
to estimate the local probability of proposition en-
tailment, and then perform global optimization to
construct the entailment graph.
7 Empirical Evaluation
We evaluate the models in Sections 5 & 6 on the
30 annotated entailment graphs presented in Sec-
tion 4. During testing, each graph was evaluated
separately. The results presented in this section
are all micro-averages, though macro-averages
were also computed and found to reflect the same
trends. Models trained with distant supervision
were evaluated on all graphs. For directly super-
92
vised methods, we used 2 ? 6-fold cross valida-
tion (25 training graphs per fold). In this scenario,
each graph induced a set of labeled examples ?
its edges being positive examples, and the miss-
ing potential edges being negative ones ? and the
union of these sets was used as the training set of
that cross-validation fold.
7.1 Results
Table 1 compares the performance of CEC with
that of the baseline methods.
While Joint Features and CEC share exactly the
same features, CEC exploits the inherent conjunc-
tion between predicate and argument entailments
(as observed in Section 4 and modeled in Sec-
tion 5.1), and forces both components to decide on
entailment separately. This differs from the sim-
pler log-linear model (Joint Features) where, for
example, a very strong predicate entailment fea-
ture might override the overall proposition-level
decision, even if there was no strong indication
of argument entailment. As a result, CEC dom-
inates Joint Features in both precision and recall.
The F
1
difference between these methods is sta-
tistically significant with McNemar?s test (1947)
with p  0.01. Specifically, CEC corrected Joint
Features 7621 times, while the opposite occurred
only 4048 times.
CEC also yields relatively high precision
and recall. While it has 2% less recall than
Opt(Arg) (the highest-recall baseline), it sur-
passes Opt(Arg)?s precision by 14%. Along with
a similar comparison to Argument Equality (the
highest precision baseline), CEC notably outper-
forms all baselines.
It is also evident that both directly super-
vised methods outperform the distantly super-
vised methods. Our analysis (Section 8.1) shows
that WordNet lacks significant coverage, and may
therefore be a problematic source of supervision.
Perhaps the most surprising result is the com-
plete failure of WordNet-supervised methods that
consider predicate information. A deeper analy-
sis (Section 8.2) shows that predicate inference is
highly context-sensitive, and deviates beyond the
lexical inferences provided by WordNet.
7.2 Learning Curve
We measure the supervision needed to train the di-
rectly supervised models by their learning curves
(Figure 2). Each point is the average F
1
score
Supervision Method Prec. Rec. F1
None
Argument
81.6% 42.2% 55.6%
Equality
Predicate
9.3% 1.5% 2.6%
Equality
Component
(WordNet)
Opt(Arg
73.8% 3.8% 7.2%
? Pred)
Opt(Arg) ?
72.3% 3.2% 6.0%
Opt(Pred)
Opt(Arg) 64.6% 55.4% 59.7%
Opt(Pred) 11.0% 6.2% 8.0%
Proposition
(Annotated)
Joint
76.3% 51.7% 61.6%
Features
CEC 78.7% 53.5% 63.7%
Table 1: Performance on gold-standard (micro averaged).
Figure 2: Learning curve of directly supervised methods.
across 12 cross-validation folds; e.g. for 10 train-
ing graphs, we used 4 ? 3-fold cross validation.
Even 5 training graphs (a day?s worth of annota-
tion) are enough for CEC to perform on-par with
the best distantly supervised method, and with 15
training graphs it outperforms every baseline, in-
cluding Joint Features trained with 25 graphs.
7.3 Effects of Global Optimization
We evaluate the effects of enforcing transitivity by
considering CEC with and without the global op-
timization phase. Table 2 shows how many entail-
ment edges were added (and removed) by enforc-
ing transitivity, and measures how many of those
modifications were correct. Apparently, transi-
tivity?s greatest effect is the removal of incorrect
entailment edges. The same phenomenon was
also observed in the work on predicate entailment
graphs (Berant et al., 2012). Overall, transitivity
made 4,848 correct modifications out of 6,734 in
total. A ?
2
test reveals that the positive contribu-
tion of enforcing transitivity is indeed statistically
significant (p 0.01).
93
Gold Global Opt Global Opt
Standard Added Edge Removed Edge
Edge Exists 1150 482
No Edge 1404 3698
Table 2: The modifications made by enforcing transitivity
w.r.t. the gold standard. 55% of the edges added by enforcing
transitivity are incorrect, but it removed even more incorrect
edges, improving the overall performance.
8 Analysis of Lexical Inference
Although CEC had a statistically-significant im-
provement upon the baselines, its absolute perfor-
mance leaves much room for improvement. We
hypothesize that the lexical entailment features we
used, following state-of-the-art lexical entailment
modeling, do not capture many of the actual lexi-
cal inferences induced by proposition entailment.
We demonstrate that this is indeed the case.
8.1 Argument Entailment
To isolate the effect of different features on pre-
dicting argument entailment, we collected all
proposition pairs that shared exactly the same
predicate and topic, and thus differed in only their
?free? argument. This yielded 20,336 aligned ar-
gument pairs, whose entailment annotations are
equal to the corresponding proposition-entailment
annotation in the dataset.
Using WordNet synonyms and hypernyms to
predict entailment yielded a precision of about
88%, at 40% recall. Though relatively precise,
WordNet?s coverage is limited, and misses many
inferences. We describe three typical types of in-
ferences that were absent from WordNet.
The first type constitutes of widely-
used paraphrases such as people?persons,
woman?female, and pain?ache. These may be
seen as weaker types of synonyms, which may
have nuances, but are typically interchangeable.
Another type is metonymy, in which a concept
is not referred to by its own name, but by that of
an associated concept. This is very common in
our healthcare dataset, where a disease is often re-
ferred to by its underlying pathogen and vice-versa
(e.g. pneumonia?pneumococcus).
The third type of missing inferences is causal-
ity. Many instances of metonymy (such as the
disease-pathogen example) may be seen as causal-
ity as well. Other examples can be drug and ef-
fect (laxative?diarrhea) or condition and symp-
tom (influenza?fever).
WordNet?s lack of such common-sense infer-
ences, which are abundant in our proposition en-
tailment dataset, might make WordNet a problem-
atic source of distant supervision. The fact that
60% of the entailing examples in our dataset are
labeled by WordNet as non-entailing, means that
for each truly positive training example, there is a
higher chance that it will have a negative label.
Distributional similarity is commonly used to
capture such missing inferences and complement
WordNet-like resources. On this dataset, how-
ever, it failed to do so. One of the more in-
dicative similarity measures, inclusion (Weeds and
Weir, 2003), yielded only 27% precision at 40%
recall when tuning a threshold to optimize F
1
. In-
creasing precision caused a dramatic drop in re-
call: 50% precision limited recall to 3.2%. Other
similarity measures performed similarly or worse.
It seems that current methods of distributional
word similarity also capture relations quite differ-
ent from inference, such as cohyponyms and do-
main relatedness, and might be less suitable for
modeling lexical entailment on their own.
8.2 Context-Sensitive Predicate Entailment
The proposition-level entailment annotation in-
duces an entailment relation between the predi-
cates, which holds in the particular context of the
proposition pair. We wish to understand the na-
ture of this predicate-level entailment, and how it
compares to classic lexical inference as portrayed
in the lexical semantics literature. To that end, we
collected all the entailing proposition pairs with
equal arguments, and extracted the corresponding
predicate pairs (which, assuming alignment, are
necessarily entailing in that context). This list con-
tains 52,560 predicate pairs.
In our first analysis, we explored which Word-
Net relations correlate with predicate entailment,
by checking how well each relation covers the set
of entailed predicate pairs. Synonyms and hyper-
nyms, which are considered positive entailment
indicators, covered only about 8% each. Sur-
prisingly, the hyponym and cohyponym relations
(which are considered negative entailment indica-
tors) covered over 9% and 14%, respectively. Ta-
ble 3 shows the exact details.
It seems that WordNet relations are hardly cor-
related with the context-sensitive predicate-level
entailments in our dataset, and that the classic in-
terpretation of WordNet relations with respect to
entailment does not hold in practice, where en-
94
Interpretation WordNet Relation Coverage
Positive
Synonyms 7.85%
Direct Hypernyms 5.62%
Indirect Hypernyms 3.14%
Entailment 0.33%
Negative
Antonyms 0.31%
Direct Hyponyms 5.74%
Indirect Hyponyms 3.51%
Cohyponyms 14.30%
Table 3: The portion of positive predicate entailments cov-
ered by each WordNet relation. WordNet relations are di-
vided according to their common interpretations with respect
to lexical entailment.
tailments are judged in the context of concrete
propositions. In fact, negative indicators in Word-
Net seem to cover more predicate entailments
than positive ones. This explains the failure of
WordNet-supervised methods with predicate en-
tailment features (Section 7.1).
Since we do not expect WordNet to cover all
shades of entailment, we conducted a manual anal-
ysis as well. 100 entailing predicate pairs were
randomly sampled, and manually annotated for
lexical-level entailment, without seeing their argu-
ments. To compensate for the lack of context, we
guided the annotators to assume a general health-
care scenario, and use a more lenient interpretation
of textual entailment (biased towards positive en-
tailment decisions). Nevertheless, only 56% of the
predicate pairs were labeled as entailing, indicat-
ing that the context-sensitive predicate inferences
captured in our dataset can be quite different from
generic predicate inferences.
We suggest that this phenomenon goes one step
beyond what the current literature considers as
context-sensitive entailment, and that it is more
specific than determining an appropriate lexical
sense. To demonstrate, we present four such
predicate-entailment phenomena.
First, there are cases in which an appropriate
lexical sense could exist in principle, but it is too
specific to be practically covered by a manual re-
source. For example, cures cancer?kills cancer,
but the appropriate sense for kill (cause to cease
existing) does not exist, and in turn, neither does
the hypernymy relation from cure to kill. It is hard
to expect these kinds of obscure senses or relation-
ships to comprehensively appear in a manually-
constructed resource.
In many cases, such a specific sense does not
exist. For example, (pneumonia, require, antibi-
otic)?(pneumonia, treated by, antibiotics), but re-
quire does not have a general sense which means
treat by. The inference in this example does not
stem from the linguistic meaning of each predi-
cate, but rather from the real-world situation their
encapsulating propositions describe.
Another aspect of predicate entailment that
may change when considering propositional con-
text is the direction of inference. For instance,
cause9trigger. While it may be the case that trig-
ger entails cause, the converse is not necessarily
true since cause is far more general. However,
when considering (caffeine, cause, headache) and
(caffeine, trigger, headache), both propositions de-
scribe the same real-world situation, and thus both
propositions are mutually entailing. In this con-
text, cause does indeed entail trigger as well.
Finally, figures of speech (such as metaphors)
are abundant and diverse. Though it may not be
so common to read about a drug that ?banishes?
headaches, most readers would understand the un-
derlying meaning. These phenomena exceed the
current scope of lexical-semantic resources such
as WordNet, and require world knowledge.
9 Conclusion
This paper proposes a novel approach, based on
entailment graphs, for consolidating information
extracted from large corpora. We define the prob-
lem of building proposition entailment graphs, and
provide a large annotated dataset. We also present
the CEC model, which models the connection be-
tween proposition entailment and lexical entail-
ment. Although it outperforms the state-of-the-
art, its performance is not ideal because it relies on
inadequate lexical-semantic resources that do not
capture the common-sense and context-sensitive
inferences which are inherent in proposition en-
tailment. In future work, we intend to further in-
vestigate lexical entailment as induced by proposi-
tion entailment, and hope to develop richer meth-
ods of lexical inference that address the phenom-
ena exhibited in this setting.
Acknowledgements
This work has been supported by the Israeli Min-
istry of Science and Technology grant 3-8705, the
Israel Science Foundation grant 880/12, and the
European Communitys Seventh Framework Pro-
gramme (FP7/2007-2013) under grant agreement
no. 287923 (EXCITEMENT). We would like to
thank our reviewers for their insightful comments.
95
References
Meni Adler, Jonathan Berant, and Ido Dagan. 2012.
Entailment-based text exploration with application
to the health-care domain. In Proceedings of the
System Demonstrations of the 50th Annual Meet-
ing of the Association for Computational Linguistics
(ACL 2012), pages 79?84.
Michele Banko, Michael J. Cafarella, Stephen Soder-
land, Matthew Broadhead, and Oren Etzioni. 2007.
Open information extraction from the web. In IJ-
CAI, volume 7, pages 2670?2676.
Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2012. Learning entailment relations by global graph
structure optimization. Computational Linguistics,
38(1):73?111.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a col-
laboratively created graph database for structuring
human knowledge. In Proceedings of the 2008 ACM
SIGMOD international conference on Management
of data, pages 1247?1250. ACM.
Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicog-
raphy. Computational Linguistics, 16(1):22?29.
Ido Dagan, Dan Roth, Mark Sammons, and Fabio Mas-
simo Zanzotto. 2013. Recognizing textual entail-
ment: Models and applications. Synthesis Lectures
on Human Language Technologies, 6(4):1?220.
Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-
bin. 1977. Maximum likelihood from incomplete
data via the em algorithm. Journal of the Royal Sta-
tistical Society. Series B (Methodological), pages 1?
38.
Oren Etzioni, Michele Banko, Stephen Soderland, and
Daniel S Weld. 2008. Open information extrac-
tion from the Web. Communications of the ACM,
51(12):68?74.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1535?1545, Edinburgh, Scotland, UK.,
July. Association for Computational Linguistics.
Yoav Goldberg and Jon Orwant. 2013. A dataset of
syntactic-ngrams over time from a very large cor-
pus of english books. In Second Joint Conference
on Lexical and Computational Semantics (*SEM),
Volume 1: Proceedings of the Main Conference and
the Shared Task: Semantic Textual Similarity, pages
241?247, Atlanta, Georgia, USA, June. Association
for Computational Linguistics.
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-Geffet. 2010. Directional distribu-
tional similarity for lexical inference. Natural Lan-
guage Engineering, 16(4):359?389.
Vladimir I. Levenshtein. 1966. Binary codes capable
of correcting deletions, insertions and reversals. In
Soviet Physics Doklady, volume 10, page 707.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 36th Annual
Meeting of the Association for Computational Lin-
guistics and 17th International Conference on Com-
putational Linguistics, Volume 2, pages 768?774,
Montreal, Quebec, Canada, August. Association for
Computational Linguistics.
Mausam, Michael Schmitz, Stephen Soderland, Robert
Bart, and Oren Etzioni. 2012. Open language learn-
ing for information extraction. In Proceedings of
the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 523?534, Jeju
Island, Korea, July. Association for Computational
Linguistics.
Quinn McNemar. 1947. Note on the sampling error
of the difference between correlated proportions or
percentages. Psychometrika, 12(2):153?157.
Yashar Mehdad, Giuseppe Carenini, Raymond T. Ng,
and Shafiq Joty. 2013. Towards topic labeling
with phrase entailment and aggregation. In Pro-
ceedings of the 2013 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
179?189, Atlanta, Georgia, June. Association for
Computational Linguistics.
Oren Melamud, Jonathan Berant, Ido Dagan, Jacob
Goldberger, and Idan Szpektor. 2013. A two level
model for context sensitive inference rules. In Pro-
ceedings of the 51st Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), pages 1331?1340, Sofia, Bulgaria, August.
Association for Computational Linguistics.
Sebastian Riedel, Limin Yao, Andrew McCallum, and
Benjamin M. Marlin. 2013. Relation extraction
with matrix factorization and universal schemas. In
Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 74?84, Atlanta, Georgia, June. Association
for Computational Linguistics.
Rion Snow, Dan Jurafsky, and Andrew Y. Ng. 2006.
Semantic taxonomy induction from heterogenous
evidence. In Proceedings of the 21st International
Conference on Computational Linguistics and the
44th Annual Meeting of the Association for Com-
putational Linguistics (ACL-COLING 2006), pages
801?808.
Julie Weeds and David Weir. 2003. A general
framework for distributional similarity. In Michael
Collins and Mark Steedman, editors, Proceedings of
the 2003 Conference on Empirical Methods in Nat-
ural Language Processing, pages 81?88.
96
Ying Xu, Mi-Young Kim, Kevin Quinn, Randy Goebel,
and Denilson Barbosa. 2013. Open information
extraction with tree kernels. In Proceedings of the
2013 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 868?877, At-
lanta, Georgia, June. Association for Computational
Linguistics.
Alexander Yates and Oren Etzioni. 2009. Unsuper-
vised methods for determining object and relation
synonyms on the web. Journal of Artificial Intelli-
gence Research, 34(1):255.
97
Proceedings of the Eighteenth Conference on Computational Language Learning, pages 181?190,
Baltimore, Maryland USA, June 26-27 2014.
c
?2014 Association for Computational Linguistics
Probabilistic Modeling of Joint-context in Distributional Similarity
Oren Melamud
?
, Ido Dagan
?
, Jacob Goldberger
?
, Idan Szpektor
?
, Deniz Yuret
?
? Computer Science Department, Bar-Ilan University
? Faculty of Engineering, Bar-Ilan University
? Yahoo! Research Israel
? Koc? University
{melamuo,dagan,goldbej}@{cs,cs,eng}.biu.ac.il
idan@yahoo-inc.com, dyuret@ku.edu.tr
Abstract
Most traditional distributional similarity
models fail to capture syntagmatic patterns
that group together multiple word features
within the same joint context. In this work
we introduce a novel generic distributional
similarity scheme under which the power
of probabilistic models can be leveraged
to effectively model joint contexts. Based
on this scheme, we implement a concrete
model which utilizes probabilistic n-gram
language models. Our evaluations sug-
gest that this model is particularly well-
suited for measuring similarity for verbs,
which are known to exhibit richer syntag-
matic patterns, while maintaining compa-
rable or better performance with respect
to competitive baselines for nouns. Fol-
lowing this, we propose our scheme as a
framework for future semantic similarity
models leveraging the substantial body of
work that exists in probabilistic language
modeling.
1 Introduction
The Distributional Hypothesis is commonly
phrased as ?words which are similar in meaning
occur in similar contexts? (Rubenstein and Good-
enough, 1965). Distributional similarity models
following this hypothesis vary in two major as-
pects, namely the representation of the context and
the respective computational model. Probably the
most prominent class of distributional similarity
models represents context as a vector of word fea-
tures and computes similarity using feature vector
arithmetics (Lund and Burgess, 1996; Turney et
al., 2010). To construct the feature vectors, the
context of each target word token
1
, which is com-
monly a word window around it, is first broken
1
We use word type to denote an entry in the vocabulary,
and word token for a particular occurrence of a word type.
into a set of individual independent words. Then
the weights of the entries in the word feature vec-
tor capture the degree of association between the
target word type and each of the individual word
features, independently of one another.
Despite its popularity, it was suggested that
the word feature vector approach misses valu-
able information, which is embedded in the co-
location and inter-relations of words (e.g. word
order) within the same context (Ruiz-Casado et al.,
2005). Following this motivation, Ruiz-Casado
et al. (2005) proposed an alternative composite-
feature model, later adopted in (Agirre et al.,
2009). This model adopts a richer context repre-
sentation by considering entire word window con-
texts as features, while keeping the same compu-
tational vector-based model. Although showing
interesting potential, this approach suffers from a
very high-dimensional feature space resulting in
data sparseness problems. Therefore, it requires
exceptionally large learning corpora to consider
large windows effectively.
A parallel line of work adopted richer context
representations as well, with a different compu-
tational model. These works utilized neural net-
works to learn low dimensional continuous vector
representations for word types, which were found
useful for measuring semantic similarity (Col-
lobert and Weston, 2008; Mikolov et al., 2013).
These vectors are trained by optimizing the pre-
diction of target words given their observed con-
texts (or variants of this objective). Most of these
models consider each observed context as a joint
set of context words within a word window.
In this work we follow the motivation in the pre-
vious works above to exploit richer joint-context
representations for modeling distributional simi-
larity. Under this approach the set of features in
the context of each target word token is consid-
ered to jointly reflect on the meaning of the target
word type. To further facilitate this type of mod-
181
eling we propose a novel probabilistic computa-
tional scheme for distributional similarity, which
leverages the power of probabilistic models and
addresses the data sparseness challenge associated
with large joint-contexts. Our scheme is based on
the following probabilistic corollary to the distri-
butional hypothesis:
(1)?words are similar in meaning if
they are likely to occur in the same contexts?
To realize this corollary, our distributional sim-
ilarity scheme assigns high similarity scores to
word pairs a and b, for which a is likely in the con-
texts that are observed for b and vice versa. The
scheme is generic in the sense that various under-
lying probabilistic models can be used to provide
the estimates for the likelihood of a target word
given a context. This allows concrete semantic
similarity models based on this scheme to lever-
age the capabilities of probabilistic models, such
as established language models, which typically
address the modeling of joint-contexts.
We hypothesize that an underlying model that
could capture syntagmatic patterns in large word
contexts, yet is flexible enough to deal with data
sparseness, is desired. It is generally accepted
that the semantics of verbs in particular are cor-
related with their syntagmatic properties (Levin,
1993; Hanks, 2013). This provides grounds to ex-
pect that such model has the potential to excel for
verbs. To capture syntagmatic patterns, we choose
in this work standard n-gram language models as
the basis for a concrete model implementing our
scheme. This choice is inspired by recent work on
learning syntactic categories (Yatbaz et al., 2012),
which successfully utilized such language mod-
els to represent word window contexts of target
words. However, we note that other richer types
of language models, such as class-based (Brown
et al., 1992) or hybrid (Tan et al., 2012), can be
seamlessly integrated into our scheme.
Our evaluations suggest that our model is in-
deed particularly advantageous for measuring se-
mantic similarity for verbs, while maintaining
comparable or better performance with respect to
competitive baselines for nouns.
2 Background
In this section we provide additional details re-
garding previous works that we later use as base-
lines in our evaluations.
To implement the composite-feature approach,
Ruiz-Casado et al. (2005) used a Web search en-
gine to compare entire window contexts of target
word types. For example, a single feature that
could be retrieved this way for the target word like
is ?Children cookies and milk?. They showed
good results on detecting synonyms in the 80
multiple-choice questions TOEFL test. Agirre et
al. (2009) constructed composite-feature vectors
using an exceptionally large 1.6 Teraword learn-
ing corpus. They found that this approach out-
performs the traditional independent feature vec-
tor approach on a subset of the WordSim353 test-
set (Finkelstein et al., 2001), which is designed to
test the more restricted relation of semantic simi-
larity (to be distinguished from looser semantic re-
latedness). We are not aware of additional works
following this approach, of using entire word win-
dows as features.
Neural networks have been used to train lan-
guage models that are based on low dimensional
continuous vector representations for word types,
also called word embeddings (Bengio et al., 2003;
Mikolov et al., 2010). Although originally de-
signed to improve language models, later works
have shown that such word embeddings are useful
in various other NLP tasks, including measuring
semantic similarity with vector arithmetics (Col-
lobert and Weston, 2008; Mikolov et al., 2013).
Specifically, the recent work by Mikolov et al.
(2013) introduced the CBOW and Skip-gram mod-
els, achieving state-of-the-art results in detecting
semantic analogies. The CBOW model is trained
to predict a target word given the set of context
words in a word window around it, where this
context is considered jointly as a bag-of-words.
The Skip-gram model is trained to predict each of
the context words independently given the target
word.
3 Probabilistic Distributional Similarity
3.1 Motivation
In this section we briefly demonstrate the bene-
fits of considering joint-contexts of words. As an
illustrative example, we note that the target words
like and surround may share many individual word
features such as ?school? and ?campus? in the sen-
tences ?Mary?s son likes the school campus? and
?The forest surrounds the school campus?. This
potentially implies that individual features may
not be sufficient to accurately reflect the difference
182
between such words. Alternatively, we could use
the following composite features to model the con-
text of these words, ?Mary?s son the school
campus? and ?The forest the school campus?.
This would discriminate better between like and
surround. However, in this case sentences such as
?Mary?s son likes the school campus? and ?John?s
son loves the school campus? will not provide any
evidence to the similarity between like and love,
since ?Mary?s son the school campus? is a dif-
ferent feature than ?John?s son the school cam-
pus?.
In the remainder of this section we propose
a modeling scheme and then a concrete model,
which can predict that like and love are likely to
occur in each other?s joint-contexts, whereas like
and surround are not, and then assign similarity
scores accordingly.
3.2 The probabilistic similarity scheme
We now present a computational scheme that re-
alizes our proposed corollary (1) to the distribu-
tional hypothesis and facilitates robust probabilis-
tic modeling of joint contexts. First, we slightly
rephrase this corollary as follows: ?words a and
b are similar in meaning if word b is likely in
the contexts of a and vice versa?. We denote the
probability of an occurrence of a target word b
given a joint-context c by p(b|c). For example,
p(love|?Mary?s son the school campus?) is the
probability of the word love to be the filler of the
?place-holder? in the given joint-context ?Mary?s
son the school campus?. Similarly, we denote
p(c|a) as the probability of a joint-context c given
a word a, which fills its place-holder. We now
propose p
sim
(b|a) to reflect how likely b is in the
joint-contexts of a. We define this measure as:
(2)p
sim
(b|a) =
?
c
p(c|a) ? p(b|c)
where c goes over all possible joint-contexts in the
language.
To implement this measure we need to find
an efficient estimate for p
sim
(b|a). The most
straight forward strategy is to compute sim-
ple corpus count ratio estimates for p(b|c) and
p(c|a), denoted p
#
(b|c) =
count(b,c)
count(?,c)
and
p
#
(c|a) =
count(a,c)
count(a,?)
. However, when consid-
ering large joint-contexts for c, this approach be-
comes similar to the composite-feature approach
since it is based on co-occurrence counts of tar-
get words with large joint-contexts. Therefore, we
expect in this case to encounter the data sparse-
ness problems mentioned in Section 1, where se-
mantically similar word type pairs that share only
few or no identical joint-contexts yield very low
p
sim
(b|a) estimates.
To address the data sparseness challenge and
adopt more advanced context modeling, we aim to
use a more robust underlying probabilistic model
? for our scheme and denote the probabilities es-
timated by this model by p
?
(b|c) and p
?
(c|a). We
note that contrary to the count ratio model, given a
robust model ?, such as a language model, p
?
(b|c)
and p
?
(c|a) can be positive even if the target words
b and a were not observed with the joint-context c
in the learning corpus.
While using p
?
(b|c) and p
?
(c|a) to estimate the
value of p
sim
(b|a) addresses the sparseness chal-
lenge, it introduces a computational challenge.
This is because estimating p
sim
(b|a) would re-
quire computing the sum over all of the joint-
contexts in the learning corpus regardless of
whether they were actually observed with either
word type a or b. For that reason we choose a
middle ground approach, estimating p(b|c) with
?, while using a count ratio estimate for p(c|a),
as follows. We denote the collection of all joint-
contexts observed for the target word a in the
learning corpus by C
a
, where |C
a
|= count(a, ?).
For example, C
like
= {c
1
=?Mary?s son the
school campus?, c
2
=?John?s daughter to read
poetry?,...}. We note that this collection is a multi-
set, where the same joint-context can appear more
than once.
We now approximate p
sim
(b|a) from Equation
(2) as follows:
(3)
p?
sim
?
(b|a) =
?
c
p
#
(c|a) ? p
?
(b|c) =
1
|C
a
|
?
?
c?C
a
p
?
(b|c)
We note that this formulation still addresses
sparseness of data by using a robust model, such as
a language model, to estimate p
?
(b|c). At the same
time it requires our model to sum only over the
joint-contexts in the collection C
a
, since contexts
not observed for a yield p
#
(c|a) = 0. Even so,
since the size of these context collections grows
linearly with the corpus size, considering all ob-
served contexts may still present a scalability chal-
lenge. Nevertheless, we expect our approximation
p?
sim
?
(b|a) to converge with a reasonable sample
183
size from a?s joint-contexts. Therefore, in order
to bound computational complexity, we limit the
size of the context collections used to train our
model to a maximum of N by randomly sampling
N entries from larger collections. In all our ex-
periments we use N = 10, 000. Higher values
of N yielded negligible performances differences.
Overall we see that our model estimates p?
sim
?
(b|a)
as the average probability predicted for b in (a
large sample of) the contexts observed for a.
Finally, we define our similarity measure for tar-
get word types a and b:
(4)sim
?
(a, b) =
?
p?
sim
?
(b|a) ? p?
sim
?
(a|b)
As intended, this similarity measure promotes
word pairs in which both b is likely in the con-
texts of a and vice versa. Next, we describe a
model which implements this scheme with an n-
gram language model as a concrete choice for ?.
3.3 Probabilistic similarity using language
models
In this work we focus on the word window context
representation, which is the most common. We
define a word window of order k around a target
word as a window with up to k words to each side
of the target word, not crossing sentence bound-
aries. The word window does not include the tar-
get word itself, but rather a ?place-holder? for it.
Since word windows are sequences of words,
probabilistic language models are a natural choice
of a model ? for estimating p
?
(b|c). Language
models assign likelihood estimates to sequences
of words using approximation strategies. In
this work we choose n-gram language models,
aiming to capture syntagmatic properties of the
word contexts, which are sensitive to word or-
der. To approximate the probability of long se-
quences of words, n-gram language models com-
pute the product of the estimated probability of
each word in the sequence conditioned on at most
the n ? 1 words preceding it. Furthermore, they
use ?discounting? methods to improve the esti-
mates of conditional probabilities when learning
data is sparse. Specifically, in this work we use
the Kneser-Ney n-gram model (Kneser and Ney,
1995).
We compute p
?
(b|c) as follows:
(5)p
?
(b|c) =
p
?
(b, c)
p
?
(c)
where p
?
(b, c) is the probability of the word se-
quence comprising the word window c, in which
the word b fills the place-holder. For instance, for
c = ?I drive my to work every? and b = car,
p
?
(b, c) is the estimated language model probabil-
ity of ?I drive my car to work every?. p
?
(c) is the
marginal probability of p
?
(?, c) over all possible
words in the vocabulary.
2
4 Experimental Settings
Although sometimes used interchangeably, it is
common to distinguish between semantic simi-
larity and semantic relatedness (Budanitsky and
Hirst, 2001; Agirre et al., 2009). Semantic simi-
larity is used to describe ?likeness? relations, such
as the relations between synonyms, hypernym-
hyponyms, and co-hyponyms. Semantic relat-
edness refers to a broader range of relations in-
cluding also meronymy and various other asso-
ciative relations as in ?pencil-paper? or ?penguin-
Antarctica?. In this work we focus on semantic
similarity and evaluate all compared methods on
several semantic similarity tasks.
Following previous works (Lin, 1998; Riedl and
Biemann, 2013) we use Wordnet to construct large
scale gold standards for semantic similarity evalu-
ations. We perform the evaluations separately for
nouns and verbs to test our hypothesis that our
model is particularly well-suited for verbs. To fur-
ther evaluate our results on verbs we use the verb
similarity test-set released by (Yang and Powers,
2006), which contains pairs of verbs associated
with semantic similarity scores based on human
judgements.
4.1 Compared methods
We compare our model with a traditional fea-
ture vector model, the composite-feature model
(Agirre et al., 2009), and the recent state-of-the-art
word embedding models, CBOW and Skip-gram
(Mikolov et al., 2013), all trained on the same
learning corpus and evaluated on equal grounds.
We denote the traditional feature vector baseline
by IFV
W?k
, where IFV stands for ?Independent-
Feature Vector? and k is the order of the con-
text word window considered. Similarly, we
2
Computing p
?
(c) by summing over all possible place-
holder filler words, as we did in this work, is computation-
ally intensive. However, this can be done more efficiently
by implementing customized versions of (at least some) n-
gram language models with little computational overhead,
e.g. by counting the learning corpus occurrences of n-gram
templates, in which one of the elements matches any word.
184
denote the composite-feature vector baseline by
CFV
W?k
, where CFV stands for ?Composite-
Feature Vector?. This baseline constructs
traditional-like feature vectors, but considers en-
tire word windows around target word tokens as
single features. In both of these baselines we use
Cosine as the vector similarity measure, and posi-
tive pointwise mutual information (PPMI) for the
feature vector weights. PPMI is a well-known
variant of pointwise mutual information (Church
and Hanks, 1990), and the combination of Cosine
with PPMI was shown to perform particularly well
in (Bullinaria and Levy, 2007).
We denote Mikolov?s CBOW and Skip-gram
baseline models by CBOW
W?k
and SKIP
W?k
respectively, where k denotes again the order of
the window used to train these models. We used
Mikolov?s word2vec utility
3
with standard param-
eters (600 dimensions, negative sampling 15) to
learn the word embeddings, and Cosine as the vec-
tor similarity measure between them.
As the underlying probabilistic language model
for our method we use the Berkeley implementa-
tion
4
(Pauls and Klein, 2011) of the Kneser-Ney
n-gram model with the default discount parame-
ters. We denote our model PDS
W?k
, where PDS
stands for ?Probabilistic Distributional Similar-
ity?, and k is the order of the context word win-
dow. In order to avoid giving our model an un-
fair advantage of tuning the order of the language
model n as an additional parameter, we use a fixed
n = k + 1. This means that the conditional prob-
abilities that our n-gram model learns consider a
scope of up to half the size of the window, which
is the distance in words between the target word
and either end of the window. We note that this
is the smallest reasonable value for n, as smaller
values effectively mean that there will be context
words within the window that are more than n
words away from the target word, and therefore
will not be considered by our model.
As learning corpus we used the first CD of
the freely available Reuters RCV1 dataset (Rose
et al., 2002). This learning corpus contains ap-
proximately 100M words, which is comparable in
size to the British National Corpus (BNC) (As-
ton, 1997). We first applied part-of-speech tag-
ging and lemmatization to all words. Then we
represented each word w in the corpus as the pair
3
http://code.google.com/p/word2vec
4
http://code.google.com/p/berkeleylm/
[pos(w), lemma(w)], where pos(w) is a coarse-
grained part-of-speech category and lemma(w) is
the lemmatized form of w. Finally, we converted
every pair [pos(w), lemma(w)] that occurs less
than 100 times in the learning corpus to the pair
[pos(w), ? ], which represents all rare words of the
same part-of-speech tag. Ignoring rare words is a
common practice used in order to clean up the cor-
pus and reduce the vocabulary size (Gorman and
Curran, 2006; Collobert and Weston, 2008).
The above procedure resulted in a word vocabu-
lary of 27K words. From this vocabulary we con-
structed a target verb set with over 2.5K verbs by
selecting all verbs that exist in Wordnet (Fellbaum,
2010). We repeated this procedure to create a tar-
get noun set with over 9K nouns. We used our
learning corpus for all compared methods and had
them assign a semantic similarity score for every
pair of verbs and every pair of nouns in these tar-
get sets. These scores were later used in all of our
evaluations.
4.2 Wordnet evaluation
There is a shortage of large scale test-sets for se-
mantic similarity. Popular test-sets such as Word-
Sim353 and the TOEFL synonyms test contain
only 353 and 80 test items respectively, and there-
fore make it difficult to obtain statistically signif-
icant results. To automatically construct larger-
scale test-sets for semantic similarity, we sampled
large target word subsets from our corpus and used
Wordnet as a gold standard for their semantically
similar words, following related previous evalua-
tions (Lin, 1998; Riedl and Biemann, 2013). We
constructed two test-sets for our primary evalua-
tion, one for verb similarity and another for noun
similarity.
To perform the verb similarity evaluation, we
randomly sampled 1,000 verbs from the target
verb set, where the probability of each verb to be
sampled is set to be proportional to its frequency in
the learning corpus. Next, for each sampled verb
a we constructed a Wordnet-based gold standard
set of semantically similar words. In this set each
verb a
?
is annotated as a ?synonym? of a if at least
one of the senses of a
?
is a synonym of any of the
senses of a. In addition, each verb a
?
is annotated
as a ?semantic neighbor? of a if at least one of the
senses of a
?
is a synonym, co-hyponym, or a di-
rect hypernym/hyponym of any of the senses of a.
We note that by definition all verbs annotated as
185
synonyms of a are annotated as semantic neigh-
bors as well. Next, per each verb a and an evalu-
ated method, we generated a ranked list of all other
verbs, which was induced according to the similar-
ity scores of this method.
Finally, we evaluated the compared methods
on two tasks, ?synonym detection? and ?seman-
tic neighbor detection?. In the synonym detection
task we evaluated the methods? ability to retrieve
as much verbs annotated in our gold standard as
?synonyms?, in the top-n entries of their ranked
lists. Similarly, we evaluated all methods on the
?semantic neighbors? task. The synonym detec-
tion task is designed to evaluate the ability of the
compared methods to identify a more restrictive
interpretation of semantic similarity, while the se-
mantic neighbor detection task does the same for
a somewhat broader interpretation.
We repeated the above procedure for sam-
pling 1,000 target nouns, constructing the noun
Wordnet-based gold standards and evaluating on
the two semantic similarity tasks.
4.3 VerbSim evaluation
The publicly available VerbSim test-set contains
130 verb pairs, each annotated with an average of
6 human judgements of semantic similarity (Yang
and Powers, 2006). We extracted a 107 pairs sub-
set of this dataset for which all verbs are in our
learning corpus. We followed works such as (Yang
and Powers, 2007; Agirre et al., 2009) and com-
pared the Spearman correlations between the verb-
pair similarity scores assigned by the compared
methods and the manually annotated scores in this
dataset.
5 Results
For each method and verb a in our 1,000 tested
verbs, we used the Wordnet gold standard to com-
pute the precision at top-1, top-5 and top-10 of the
ranked list generated by this method for a. We
then computed mean precision values averaged
over all verbs for each of the compared methods,
denoted as P@1, P@5 and P@10. The detailed
report of P@10 results is omitted for brevity, as
they behave very similarly to P@5. We varied the
context window order used by all methods to test
its effect on the results. We measured the same
metrics for nouns.
The results of our Wordnet-based 1,000 verbs
evaluation are presented in the upper part of Fig-
ure 1. The results show significant improvement
of our method over all baselines, with a margin
between 2 to 3 points on the synonyms detection
task and 5 to 7 points on the semantic neighbors
detection task. Our best performing configura-
tions are PDS
W?3
and PDS
W?4
, outperform-
ing all other baselines on both tasks and in all pre-
cision categories. This difference is statistically
significant at p < 0.001 using a paired t-test in all
cases except for the P@1 in the synonyms detec-
tion task. Within the baselines, the composite fea-
ture vector (CFV) performs somewhat better than
the independent feature vector (IFV) baseline, and
both methods perform best around window order
of two, with gradual decline for larger windows.
The word embedding baselines, CBOW and SKIP,
perform comparably to the feature vector base-
lines and to one another, with best performance
achieved around window order of four.
When gradually increasing the context window
order within the range of up to 4 words, our PDS
model shows improvement. This is in contrast to
the feature vector baselines, whose performance
declines for context window orders larger than 2.
This suggests that our approach is able to take ad-
vantage of larger contexts in comparison to stan-
dard feature vector models. The decline in perfor-
mance for the independent feature vector baseline
(IFV) may be related to the fact that independent
features farther away from the target word are gen-
erally more loosely related to it. This seems con-
sistent with previous works, where narrow win-
dows of the order of two words performed well
(Bullinaria and Levy, 2007; Agirre et al., 2009;
Bruni et al., 2012) and in particular so when eval-
uating semantic similarity rather than relatedness.
On the other hand, the decline in performance for
the composite feature vector baseline (CFV) may
be attributed to the data sparseness phenomenon
associated with larger windows. The performance
of the word embedding baselines (CBOW and
SKIP) starts declining very mildly only for win-
dow orders larger than 4. This might be attributed
to the fact that these models assign lower weights
to context words the farther away they are from the
center of the window.
The results of our Wordnet-based 1,000 nouns
evaluation are presented in the lower part of Fig-
ure 1. These results are partly consistent with the
results achieved for verbs, but with a couple of
notable differences. First, though our model still
186
Figure 1: Mean precision scores as a function of window order, obtained against the Wordnet-based gold
standard, on both the verb and noun test-sets with both the synonyms and semantic neighbor detection
tasks. ?P@n? stands for precision in the top-n words of the ranked lists. Note that the Y-axis scale varies
between graphs.
outperforms or performs comparably to all other
baselines, in this case the advantage of our model
over the feature vector baselines is much more
moderate and not statistically significant. Second,
the word embedding baselines generally perform
worst (with CBOW performing a little better than
SKIP), and our model outperforms them in both
P@5 and P@10 with a margin of around 2 points
for the synonyms detection task and 3-4 points for
the neighbor detection task, with statistical signif-
icance at p < 0.001.
Next, to reconfirm the particular applicability
of our model to verb similarity as apparent from
the Wordnet evaluation, we performed the Verb-
Sim evaluation and present the results in Table 1.
We compared the Spearman correlation obtained
for the top-performing window order of each of
the evaluated methods in the Wordnet verbs eval-
uation. We present two sets of results. The ?all
scores? results follow the standard evaluation pro-
cedure, considering all similarity scores produced
by each method. In the ?top-100 scores? results,
for each method we converted to zero the scores
that it assigned to word pairs, where neither of
the words is in the top-100 most similar words
of the other. Then we performed the evaluation
with these revised scores. This procedure focuses
on evaluating the quality of the methods? top-
100 ranked word lists. The results show that our
method outperforms all baselines by a nice mar-
187
Method All scores top-100 scores
PDS W-4 0.616 0.625
CFV W-2 0.477 0.497
IFV W-2 0.467 0.546
SKIP W-4 0.469 0.512
CBOW W-5 0.528 0.469
Table 1: Spearman correlation values obtained for
the VerbSim evaluation. Each method was evalu-
ated with the optimal window order found in the
Wordnet verbs evaluation.
gin of more than 8 points with the score of 0.616
and 0.625 for the ?all scores? and ?top-100 scores?
evaluations respectively. Though not statistically
significant, due to the small test-set size, these re-
sults support the ones from the Wordnet evalu-
ation, suggesting that our model performs better
than the baselines on measuring verb similarity.
In summary, our results suggest that in lack of a
robust context modeling scheme it is hard for dis-
tributional similarity models to effectively lever-
age larger word window contexts for measuring
semantic similarity. It appears that this is some-
what less of a concern when it comes to noun sim-
ilarity, as the simple feature vector models reach
near-optimal performance with small word win-
dows of order 2, but it is an important factor for
verb similarity. In his recent book, Hanks (2013)
claims that contrary to nouns, computational mod-
els that are to capture the meanings of verbs must
consider their syntagmatic patterns in text. Our
particularly good results on verb similarity sug-
gest that our modeling approach is able to cap-
ture such information in larger context windows.
We further conjecture that the reason the word em-
bedding baselines did not do as well as our model
on verb similarity might be due to their particular
choice of joint-context formulation, which is not
sensitive to word order. However, these conjec-
tures should be further validated with additional
evaluations in future work.
6 Future Directions
In this paper we investigated the potential for im-
proving distributional similarity models by model-
ing jointly the occurrence of several features under
the same context. We evaluated several previous
works with different context modeling approaches
and suggest that the type of the underlying con-
text modeling may have significant effect on the
performance of the semantic model. Further-
more, we introduced a generic probabilistic distri-
butional similarity approach, which can leverage
the power of established probabilistic language
models to effectively model joint-contexts for the
purpose of measuring semantic similarity. Our
concrete model utilizing n-gram language models
outperforms several competitive baselines on se-
mantic similarity tasks, and appears to be partic-
ularly well-suited for verbs. In the remainder of
this section we describe some potential future di-
rections that can be pursued.
First, the performance of our generic scheme
is largely inherited from the nature of its under-
lying language model. Therefore, we see much
potential in exploring the use of other types of
language models, such as class-based (Brown et
al., 1992), syntax-based (Pauls and Klein, 2012)
or hybrid (Tan et al., 2012). Furthermore, a sim-
ilar approach to ours could be attempted in word
embedding models. For instance, our syntagmatic
joint-context modeling approach could be investi-
gated by word embedding models to generate bet-
ter embeddings for verbs.
Another direction relates to the well known ten-
dency of many words, and particularly verbs, to
assume different meanings (or senses) under dif-
ferent contexts. To address this phenomenon con-
text sensitive similarity and inference models have
been proposed (Dinu and Lapata, 2010; Melamud
et al., 2013). Similarly to many semantic similar-
ity models, our current model aggregates informa-
tion from all observed contexts of a target word
type regardless of its different senses. However,
we believe that our approach is well suited to ad-
dress context sensitive similarity with proper en-
hancements, as it considers joint-contexts that can
more accurately disambiguate the meaning of tar-
get words. As an example, it is possible to con-
sider the likelihood of word b to occur in a subset
of the contexts observed for word a, which is bi-
ased towards a particular sense of a.
Finally, we note that our model is not a classic
vector space model and therefore common vec-
tor composition approaches (Mitchell and Lap-
ata, 2008) cannot be directly applied to it. In-
stead, other methods, such as similarity of com-
positions (Turney, 2012), should be investigated to
extend our approach for measuring similarity be-
tween phrases.
188
Acknowledgments
This work was partially supported by the Israeli
Ministry of Science and Technology grant 3-8705,
the Israel Science Foundation grant 880/12, the
European Community?s Seventh Framework Pro-
gramme (FP7/2007-2013) under grant agreement
no. 287923 (EXCITEMENT) and the Scien-
tific and Technical Research Council of Turkey
(T
?
UB
?
ITAK, Grant Number 112E277).
References
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pas?ca, and Aitor Soroa. 2009.
A study on similarity and relatedness using distribu-
tional and wordnet-based approaches. In Proceed-
ings of NAACL. Association for Computational Lin-
guistics.
Guy Aston. 1997. The BNC Handbook Exploring the
British National Corpus with SARA Guy Aston and
Lou Burnard.
Yoshua Bengio, Rjean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, 3:1137?1155.
Peter F. Brown, Peter V. Desouza, Robert L. Mer-
cer, Vincent J. Della Pietra, and Jenifer C. Lai.
1992. Class-based n-gram models of natural lan-
guage. Computational linguistics, 18(4):467?479.
Elia Bruni, Gemma Boleda, Marco Baroni, and Nam-
Khanh Tran. 2012. Distributional semantics in tech-
nicolor. In Proceedings of ACL.
Alexander Budanitsky and Graeme Hirst. 2001.
Semantic distance in wordnet: An experimental,
application-oriented evaluation of five measures. In
Workshop on WordNet and Other Lexical Resources.
John A. Bullinaria and Joseph P. Levy. 2007. Ex-
tracting semantic representations from word co-
occurrence statistics: A computational study. Be-
havior Research Methods, 39(3):510?526.
Kenneth W. Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicog-
raphy. Computational Linguistics, 16(1):22?29.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of the 25th international conference on
Machine learning, pages 160?167. ACM.
Georgiana Dinu and Mirella Lapata. 2010. Measuring
distributional similarity in context. In Proceedings
of EMNLP.
Christiane Fellbaum. 2010. WordNet. Springer.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2001. Placing search in context: The
concept revisited. In Proceedings of the 10th inter-
national conference on World Wide Web. ACM.
James Gorman and James R. Curran. 2006. Scaling
distributional similarity to large corpora. In Pro-
ceedings of ACL.
Patrick Hanks. 2013. Lexical Analysis: Norms and
Exploitations. Mit Press.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In In-
ternational Conference on Acoustics, Speech, and
Signal Processing. IEEE.
Beth Levin. 1993. English verb classes and alter-
nations: A preliminary investigation. University of
Chicago press.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of COLING-ACL.
Kevin Lund and Curt Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, Instru-
ments, & Computers.
Oren Melamud, Jonathan Berant, Ido Dagan, Jacob
Goldberger, and Idan Szpektor. 2013. A two level
model for context sensitive inference rules. In Pro-
ceedings of ACL.
Tomas Mikolov, Martin Karafi?at, Lukas Burget, Jan
Cernock`y, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In IN-
TERSPEECH.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL.
Adam Pauls and Dan Klein. 2011. Faster and Smaller
N -Gram Language Models. In Proceedings of ACL.
Adam Pauls and Dan Klein. 2012. Large-scale syntac-
tic language modeling with treelets. In Proceedings
of ACL.
Martin Riedl and Chris Biemann. 2013. Scaling to
large?3 data: An efficient and effective method to
compute distributional thesauri. In Proceedings of
EMNLP.
Tony Rose, Mark Stevenson, and Miles Whitehead.
2002. The Reuters Corpus Volume 1-from Yester-
day?s News to Tomorrow?s Language Resources. In
LREC.
189
Herbert Rubenstein and John B. Goodenough. 1965.
Contextual correlates of synonymy. Communica-
tions of the ACM.
Maria Ruiz-Casado, Enrique Alfonseca, and Pablo
Castells. 2005. Using context-window overlapping
in synonym discovery and ontology extension. Pro-
ceedings of RANLP.
Ming Tan, Wenli Zhou, Lei Zheng, and Shaojun Wang.
2012. A scalable distributed syntactic, semantic,
and lexical language model. Computational Lin-
guistics, 38(3):631?671.
Peter D. Turney, Patrick Pantel, et al. 2010. From fre-
quency to meaning: Vector space models of seman-
tics. Journal of artificial intelligence research.
Peter D. Turney. 2012. Domain and function: A
dual-space model of semantic relations and compo-
sitions. Journal of Artificial Intelligence Researc,
44(1):533?585, May.
Dongqiang Yang and David M. W. Powers. 2006. Verb
similarity on the taxonomy of wordnet. In the 3rd
International WordNet Conference (GWC-06).
Dongqiang Yang and David M. W. Powers. 2007.
An empirical investigation into grammatically con-
strained contexts in predicting distributional similar-
ity. In Australasian Language Technology Workshop
2007, pages 117?124.
Mehmet Ali Yatbaz, Enis Sert, and Deniz Yuret. 2012.
Learning syntactic categories using paradigmatic
representations of word context. In Proceedings of
EMNLP.
190
Proceedings of the ACL 2014 Workshop on Semantic Parsing, pages 66?70,
Baltimore, Maryland USA, June 26 2014.
c
?2014 Association for Computational Linguistics
Intermediary Semantic Representation
through Proposition Structures
Gabriel Stanovsky
?
, Jessica Ficler
?
, Ido Dagan, Yoav Goldberg
Computer Science Department, Bar-Ilan University
?Both authors equally contributed to this paper
{gabriel.satanovsky,jessica.ficler,yoav.goldberg}@gmail.com
dagan@cs.biu.ac.il
Abstract
We propose an intermediary-level seman-
tic representation, providing a higher level
of abstraction than syntactic parse trees,
while not committing to decisions in cases
such as quantification, grounding or verb-
specific roles assignments. The proposal
is centered around the proposition struc-
ture of the text, and includes also im-
plicit propositions which can be inferred
from the syntax but are not transparent in
parse trees, such as copular relations intro-
duced by appositive constructions. Other
benefits over dependency-trees are ex-
plicit marking of logical relations between
propositions, explicit marking of multi-
word predicate such as light-verbs, and a
consistent representation for syntactically-
different but semantically-similar struc-
tures. The representation is meant to
serve as a useful input layer for semantic-
oriented applications, as well as to provide
a better starting point for further levels of
semantic analysis such as semantic-role-
labeling and semantic-parsing.
1 Introduction
Parsers for semantic formalisms (such as Neo-
davidsonian (Artzi and Zettlemoyer, 2013) and
DRT (Kamp, 1988)) take unstructured natural lan-
guage text as input, and output a complete seman-
tic representation, aiming to capture the mean-
ing conveyed by the text. We suggest that this
task may be effectively separated into a sequential
combination of two different tasks. The first of
these tasks is syntactic abstraction over phenom-
ena such as expression of tense, negation, modal-
ity, and passive versus active voice, which are all
either expressed or implied from syntactic struc-
ture. The second task is semantic interpretation
over the syntactic abstraction, deriving quantifi-
cation, grounding, etc. Current semantic parsers
(such as Boxer (Bos, 2008)) tackle these tasks si-
multaneously, mixing syntactic and semantic is-
sues in a single framework. We believe that sepa-
rating semantic parsing into two well defined tasks
will help to better target and identify challenges
in syntactic and semantic domains. Challenges
which are often hidden due to the one-step archi-
tecture of current parsers.
Many of today?s semantic parsers, and semantic
applications in general, leverage dependency pars-
ing (De Marneffe and Manning, 2008a) as an ab-
straction layer, since it directly represents syntac-
tic dependency relations between predicates and
arguments. Some systems exploit Semantic Role
Labeling (SRL) (Carreras and M?arquez, 2005),
where predicate-argument relationships are cap-
tured at a thematic (rather than syntactic) level,
though current SRL technology is less robust and
accurate for open domains than syntactic pars-
ing. While dependency structures and semantic
roles capture much of the proposition structure of
sentences, there are substantial aspects which are
not covered by these representations and therefore
need to be handled by semantic applications on
their own (or they end up being ignored).
Such aspects, as detailed in Section 3, include
propositions which are not expressed directly as
such but are rather implied by syntactic struc-
ture, like nominalizations, appositions and pre-
modifying adjectives. Further, the same proposi-
tion structure may be expressed in many differ-
ent ways by the syntactic structure, forcing sys-
tems to recognize this variability and making the
task of recognizing semantic roles harder. Other
aspects not addressed by common representations
include explicit marking of links between propo-
sitions within a sentence, which affect their asser-
tion or truth status, and the recognition of multi-
word predicates (e.g., considering ?take a deci-
66
Figure 1: Proposed representation for the sentence: ?If you
leave the park, you will find the Peak Tram terminal?
sion? as a single predicate, rather than considering
decision as an argument).
In this position paper we propose an intermedi-
ary representation level for the first syntactic ab-
straction phase described above, intended to re-
place syntactic parsing as a more abstract repre-
sentation layer. It is designed to capture the full
proposition structure which is expressed, either
explicitly or implicitly, by the syntactic structure
of sentences. Thus, we aim to both extract im-
plicit propositions as well as to abstract away syn-
tactic variations which yield the same proposition
structure. At the same time, we aim to remain at
a representation level that corresponds to syntac-
tic properties and relationships, while avoiding se-
mantic interpretations, to be targeted by systems
implementing the further step of semantic inter-
pretation, as discussed above.
In addition, we suggest our representation as a
useful input for semantic applications which need
to recognize the proposition structure of sentences
in order to identify targeted information, such as
Question Answering(QA), Information Extraction
(IE) and multidocument summarization. We ex-
pect that our representation may be more useful
in comparison with current popular use of depen-
dency parsing, in such applications.
2 Representation Scheme
Our representation is centered around proposi-
tions, where a proposition is a statement for which
a truth-value can be assigned. We propose to rep-
resent sentences as a set of inter-linked proposi-
tions. Each proposition is composed of one pred-
icate and a set of arguments. An example rep-
resentation can be seen in Figure 1. Predicates
are usually centered around verbs, and we con-
sider multi-word verbs (e.g., ?take apart?) as sin-
gle predicates. Both the predicates and arguments
are represented as sets of feature-value pairs. Each
argument is marked with a relation to its predicate,
and the same argument can appear in different
propositions. The relation-set we use is syntactic
in nature, including relations such as Subject,
Object, and Preposition with, in contrast
to semantic relations such as instrument.
Canonical Representation The same proposi-
tion can be realized syntactically in many forms.
An important goal of our proposal is abstracting
over idiosyncrasies in the syntactic structure and
presenting unified structures when possible. We
canonicalize on two levels:
? We canonicalize each predicate and argument
by representing each predicate as its main
lemma, and indicating other aspects of the
predication (e.g., tense, negation and time) as
features; Similarly, we mark arguments with
features such as definiteness and plurality.
? We canonicalize the argument structure by
abstracting away over word order and phe-
nomena such as topicalization and pas-
sive/active voice, and present a unified rep-
resentation in terms of the argument roles (so
that, for example, in the sentence ?the door
was opened? the argument ?door? will re-
ceive the object role, with the passive be-
ing indicated as a feature of the predicate).
Relations Between Propositions Some propo-
sitions must be interpreted taking into account
their relations to other propositions. These in-
clude conditionals (?if congress does nothing,
President Bush will have won? (wsj 0112));
temporal relations (?UAL?s announcement came
after the market closed yesterday?(wsj 0112));
and conjunctions (?They operate ships and
banks.?(wsj 0083)).
We model such relations as typed links between
extracted propositions. Figure 1 presents an exam-
ple of handling a conditional relation: the depen-
dence between the propositions is made explicit by
the Cond(if) relation.
3 Implicit Propositions
Crucially, our proposal aims to capture not only
explicit but also implicit propositions ? proposi-
tions that can be inferred from the syntactic struc-
67
ture but which are not explicitly marked in syn-
tactic dependency trees, as we elaborate below.
Some of these phenomena are relatively easy to
address by post-processing over syntactic parsers,
and could thus be included in a first implemen-
tation that produces our proposed representations.
Other phenomena are more subtle and would re-
quire further research, yet they seem important
while not being addressed by current techniques.
The syntactic structures giving rise to implicit
propositions include:
Copular sentences such as ?This is not a triv-
ial issue.? (wsj 0108) introduces a proposition by
linking between a non-verbal predicate and its ar-
gument. We represent this by making ?not a triv-
ial issue? a predicate, and ?this? an argument of
type Predication.
Appositions, we distinguish between co-reference
and predicative appositions. In Co-reference in-
dication appositions (?The company, Random
House, doesn?t report its earnings.? (adaption of
wsj 0111)) we produce a proposition to indicate
the co-reference between two lexical items. Other
propositions relating to the entity use the main
clause as the referent for this entity. In this ex-
ample, we will produce:
1. Random House == the company.
2. The company doesn?t report its earnings.
In Predicative appositions (?Pierre Vinken, 61
years old, will join the board as a nonexecutive di-
rector Nov. 29.? (wsj 0001)) an apposition is used
in order to convey knowledge about an entity. In
our representation this will produce:
1. Pierre Vinken is 61 years old (which is canoni-
calized to the representation of copular sentences)
2. Pierre Vinken will join the board as a nonexec-
utive director Nov. 29.
Adjectives, as in the sentence ?you emphasized
the high prevalence of mental illness? (wsj 0105).
Here an adjective is used to describe a definite sub-
ject and introduces another proposition, namely
the high prevalence of mental illness.
Nominalizations, for instance in the sentence
?Googles acquisition of Waze occurred yester-
day?, introduce the implicit proposition that
?Google acquired Waze?. Such propositions were
studied and annotated in the NOMLEX (Macleod
et al., 1998) and NOMBANK (Meyers et al., 2004)
resources. It remains an open issue how to repre-
sent or distinguish cases in which nominalization
introduce an underspecified proposition. For ex-
ample, consider ?dancing? in ?I read a book about
dancing?.
Possessives, such as ?John?s book? introduce the
proposition that John has a book. Similarly, ex-
amples such as ?John?s Failure? combine a pos-
sessive construction with nominalization and in-
troduce the proposition that John has failed.
Conjunctions - for example in ?They operate
ships and banks.? (wsj 0083), introduce several
propositions in one sentence:
1. They operate ships
2. They operate banks
We mark that they co-refer to the same lexical unit
in the original sentence. Such cases are already
represented explicitly in the ?collapsed? version
of Stanford-dependencies (De Marneffe and Man-
ning, 2008a).
1
Implicit future tense indication, for instance
in ?I?m going to vote for it? (wsj 0098) and
?The economy is about to slip into recession.?
(wsj 0036), verbs like ?going to? and ?about to?
are used as future-tense markers of the proposi-
tion following them, rather than predicates on their
own. We represent these as a single predicate
(?vote?) in which the tense is marked as a fea-
ture.
2
Other phenomena, omitted for lack of space,
include propositional modifiers (e.g., relative
clause modifiers), propositional arguments (such
as ?John asserted that he will go home?), condi-
tionals, and the canonicalization of passive and
active voice.
4 Relation to Other Representations
Our proposed representation is intended to serve
as a bridging layer between purely syntactic rep-
resentations such as dependency trees, and seman-
tic oriented applications. In particular, we explic-
itly represent many semantic relations expressed
in a sentence that are not captured by contempo-
rary proposition-directed semantic representations
(Baker et al., 1998; Kingsbury and Palmer, 2003;
Meyers et al., 2004; Carreras and M`arquez, 2005).
Compared to dependency-based representations
such as Stanford-dependency trees (De Marneffe
1
A case of conjunctions requiring special treatment is in-
troduced by reciprocals, in which the entities roles are ex-
changeable. For example: ?John and Mary bet against each
other on future rates? (adaption of wsj 0117).
2
Care needs to be taken to distinguish from cases such as
?going to Italy? in which ?going to? is not followed by a
verbal predicate.
68
and Manning, 2008b), we abstract away over
many syntactic details (e.g., the myriad of ways
of expressing tense, negation and modality, or the
difference between passive and active) which are
not necessary for semantic interpretation and mark
them instead using a unified set of features and ar-
gument types. We make explicit many relations
that can be inferred from the syntax but which
are not directly encoded in dependency relations.
We directly connect predicates with all of their ar-
guments in e.g., conjunctions and embedded con-
structions, and we do not commit to a tree struc-
ture. We also explicitly mark predicate and argu-
ment boundaries, and explicitly mark multi-word
predicates such as light-verb constructions.
Compared to proposition-based semantic rep-
resentations, we do not attempt to assign frame-
specific thematic roles, nor do we attempt to dis-
ambiguate or interpret word meanings. We restrict
ourselves to representing predicates by their (lem-
matized) surface forms, and labeling arguments
based on a ?syntactic? role inventory, similar to the
label-sets available in dependency representations.
This design choice makes our representation much
easier to assign automatically to naturally occur-
ring text (perhaps pre-annotated using a syntactic
parser) than it is to assign semantic roles. At the
same time, as described in Section 3, we capture
many relations that are currently not annotated in
resources such as FrameNet, and provide a com-
prehensive set of propositions present in the sen-
tence (either explicitly or implicitly) as well as the
relations between them ? an objective which is not
trivial even when presented with full semantic rep-
resentation.
Compared to more fine-grained semantic repre-
sentations used in semantic-parsers (i.e. lambda-
calculus (Zettlemoyer and Collins, 2005), neo-
davidsonian semantics (Artzi and Zettlemoyer,
2013), DRT (Kamp, 1988) or the DCS represen-
tation of Liang (2011)), we do not attempt to
tackle quantification, nor to ground the arguments
and predicates to a concrete domain-model or on-
tology. These important tasks are orthogonal to
our representation, and we believe that semantic-
parsers can benefit from our proposal by using it
as input in addition to or instead of the raw sen-
tence text ? quantification, binding and grounding
are hard enough without needing to deal with the
subtleties of syntax or the identification of implicit
propositions.
5 Conclusion and Future Work
We proposed an intermediate semantic repre-
sentation through proposition extraction, which
captures both explicit and implicit propositions,
while staying relatively close to the syntactic
level. We believe that this kind of representation
will serve not only as an advantageous input for
semantically-centered applications, such as ques-
tion answering, summarization and information
extraction, but also serve as a rich representation
layer that can be used as input for systems aiming
to provide a finer level of semantic analysis, such
as semantic-parsers.
We are currently at the beginning of our in-
vestigation. In the near future we plan to semi-
automatically annotate the Penn Tree Bank (Mar-
cus et al., 1993) with these structures, as well as
to provide software for deriving (some of) the im-
plicit and explicit annotations from automatically
produced parse-trees. We believe such resources
will be of immediate use to semantic-oriented ap-
plications. In the longer term, we plan to inves-
tigate dedicated algorithms for automatically pro-
ducing such representation from raw text.
The architecture we describe can easily accom-
modate additional layers of abstraction, by en-
coding these layers as features of propositions,
predicates or arguments. Such layers can include
the marking of named entities, the truth status of
propositions and author commitment.
In the current version infinitive constructions
are treated as nested propositions, similar to their
representation in syntactic parse trees. Providing
a consistent, useful and transparent representation
for infinitive constructions is a challenging direc-
tion for future research.
Other extensions of the proposed representa-
tion are also possible. One appealing direction
is going beyond the sentence level and represent-
ing discourse level relations, including implied
propositions and predicate - argument relation-
ships expressed by discourse (Stern and Dagan,
2014; Ruppenhofer et al., 2010; Gerber and Chai,
2012). Such an extension may prove useful as an
intermediary representation for parsers of seman-
tic formalisms targeted at the discourse level (such
as DRT).
6 Acknowledgments
This work was partially supported by the Eu-
ropean Community?s Seventh Framework Pro-
69
gramme (FP7/2007-2013) under grant agreement
no. 287923 (EXCITEMENT).
References
Yoav Artzi and Luke Zettlemoyer. 2013. Weakly supervised
learning of semantic parsers for mapping instructions to
actions. Transactions of the Association for Computa-
tional Linguistics, 1(1):49?62.
Collin F Baker, Charles J Fillmore, and John B Lowe. 1998.
The berkeley framenet project. In Proceedings of ACL,
pages 86?90. Association for Computational Linguistics.
Johan Bos. 2008. Wide-coverage semantic analysis with
boxer. In Proceedings of the 2008 Conference on Seman-
tics in Text Processing, pages 277?286. Association for
Computational Linguistics.
Xavier Carreras and Llu??s M`arquez. 2005. Introduction to
the conll-2005 shared task: Semantic role labeling. In
Proceedings of CONLL, pages 152?164.
Marie-Catherine De Marneffe and Christopher D Manning.
2008a. Stanford typed dependencies manual. Technical
report, Stanford University.
Marie-Catherine De Marneffe and Christopher D Manning.
2008b. The stanford typed dependencies representation.
In Coling 2008: Proceedings of the workshop on Cross-
Framework and Cross-Domain Parser Evaluation, pages
1?8.
Matthew Gerber and Joyce Y Chai. 2012. Semantic role la-
beling of implicit arguments for nominal predicates. Com-
putational Linguistics, 38(4):755?798.
Hans Kamp. 1988. Discourse representation theory. In Nat-
ural Language at the computer, pages 84?111. Springer.
Paul Kingsbury and Martha Palmer. 2003. Propbank: the
next level of treebank. In Proceedings of Treebanks and
lexical Theories, volume 3.
P. Liang, M. I. Jordan, and D. Klein. 2011. Learning
dependency-based compositional semantics. In Proceed-
ings of ACL, pages 590?599.
Catherine Macleod, Ralph Grishman, Adam Meyers, Leslie
Barrett, and Ruth Reeves. 1998. Nomlex: A lexicon
of nominalizations. In Proceedings of EURALEX, vol-
ume 98, pages 187?193.
Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice
Santorini. 1993. Building a large annotated corpus of
english: The penn treebank. Computational linguistics,
19(2):313?330.
Adam Meyers, Ruth Reeves, Catherine Macleod, Rachel
Szekely, Veronika Zielinska, Brian Young, and Ralph Gr-
ishman. 2004. The nombank project: An interim report.
In HLT-NAACL 2004 workshop: Frontiers in corpus an-
notation, pages 24?31.
Josef Ruppenhofer, Caroline Sporleder, Roser Morante,
Collin Baker, and Martha Palmer. 2010. Semeval-2010
task 10: Linking events and their participants in discourse.
In Proceedings of the 5th International Workshop on Se-
mantic Evaluation, pages 45?50. Association for Compu-
tational Linguistics.
Asher Stern and Ido Dagan. 2014. Recognizing implied
predicate-argument relationships in textual inference. In
Proceedings of ACL. Association for Computational Lin-
guistics.
Luke S. Zettlemoyer and Michael Collins. 2005. Learning
to map sentences to logical form: Structured classifica-
tion with probabilistic categorial grammars. In UAI, pages
658?666. AUAI Press.
70
Proceedings of the AHA! Workshop on Information Discovery in Text, pages 19?24,
Dublin, Ireland, August 23 2014.
Proposition Knowledge Graphs
Gabriel Stanovsky Omer Levy Ido Dagan
Computer Science Department, Bar-Ilan University
{gabriel.satanovsky, omerlevy}@gmail.com
dagan@cs.biu.ac.il
Abstract
Open Information Extraction (Open IE) is a promising approach for unrestricted Information
Discovery (ID). While Open IE is a highly scalable approach, allowing unsupervised relation
extraction from open domains, it currently has some limitations. First, it lacks the expressiveness
needed to properly represent and extract complex assertions that are abundant in text. Second, it
does not consolidate the extracted propositions, which causes simple queries above Open IE as-
sertions to return insufficient or redundant information. To address these limitations, we propose
in this position paper a novel representation for ID ? Propositional Knowledge Graphs (PKG).
PKGs extend the Open IE paradigm by representing semantic inter-proposition relations in a
traversable graph. We outline an approach for constructing PKGs from single and multiple texts,
and highlight a variety of high-level applications that may leverage PKGs as their underlying
information discovery and representation framework.
1 Introduction
Information discovery from text (ID) aims to provide a consolidated and explorable data representation of
an input document or a collection of documents addressing a common topic. Ideally, this representation
would separate the input into logically discrete units, omit redundancies in the original text, and provide
semantic relations between the basic units of the representation. This representation can then be used
by human readers as a convenient and succinct format, or by subsequent NLP tasks (such as question
answering and multidocument summarization) as a structured input representation.
A common approach to ID is to extract propositions conveyed in the text by applying either supervised
Information Extraction (IE) techniques (Cowie and Lehnert, 1996), to recover propositions covering a
predefined set of relations (Auer et al., 2007; Suchanek et al., 2008), or more recently, Open Information
Extraction (Open IE) (Etzioni et al., 2008), which discovers open-domain relations (Zhu et al., 2009;
Wu et al., 2008). In Open IE, natural language propositions are extracted from text, based on surface
or syntactic patterns, and are then represented as predicate-argument tuples, where each element is a
natural language string. While Open IE presents a promising direction for ID, thanks to its robustness
and scalability across domains, we argue that it currently lacks representation power in two major aspects:
representing complex propositions extracted from discourse, such as interdependent propositions or
implicitly conveyed propositions, and consolidating propositions extracted across multiple sources,
which leads to either insufficient or redundant information when exploring a set of Open IE extractions.
In this position paper we outline Propositional Knowledge Graphs (PKG), a representation which
addresses both of Open IE?s mentioned drawbacks. The graph?s nodes are discrete propositions extracted
from text, and edges are drawn where semantic relations between propositions exists. Such relations can
be inferred from a single discourse, or from multiple text fragments along with background knowledge ?
by applying methods such as textual entailment recognition (Dagan et al., 2013) ? which consolidates the
information within the graph. We discuss this representation as a useful input for semantic applications,
and describe work we have been doing towards implementing such a framework.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
19
Figure 1: An excerpt from a PKG, containing a few propositions extracted from news reports about Curiosity (the Mars rover)
and their relations. The dashed boundaries in the figure denote paraphrase cliques, meaning that all propositions within them
are mutually entailing. Some of these propositions are complex, and the bottom-right corner illustrates how one of them can be
represented by inter-connected sub-propositions.
2 Approach: Discover Inter-Proposition Relations
We propose a novel approach for textual information discovery and representation that enhances the
expressiveness of Open IE with structural power similar to traditional knowledge graphs. Our represen-
tation aims to extract all the information conveyed by text to a traversable graph format ? a Propositional
Knowledge Graph (PKG). The graph?s nodes are natural language propositions and its labeled edges are
semantic relations between these propositions. Figure 1 illustrates an excerpt of a PKG.
We separate the construction of such graphs into two phases, each of which addresses one of the afore-
mentioned limitations of current Open IE. The first phase (described in section 2.1) is the extraction of
complex propositions from a single discourse. This phase extends upon the definition of Open IE ex-
tractions to gain a more expressive paradigm and improve the recall of extracted propositions. In this
extension, a single assertion is represented by a set of interconnected propositions. An example can be
seen in the bottom right of Figure 1. The second phase (described in section 2.2) deals with the consolida-
tion of propositions extracted in the first phase. This is done by drawing relations such as entailment and
temporal succession between these propositions, which can be inferred utilizing background knowledge
applied on multiple text fragments.
2.1 Relations Implied by Discourse
Current Open IE representation schemes lack the expressibility to represent certain quite common propo-
sitions implied by syntax, hindering Open IE?s potential as an information discovery framework. We dis-
cuss several cases in which this limitation is evident, and describe possible solutions within our proposed
framework.
Embedded and Interrelated Propositions Common Open IE systems retrieve only propositions in
which both predicates and arguments are instantiated in succession in the surface form. For such propo-
sitions, these systems produce independent tuples (typically a (subject, verb, object) triplet) consisting of
a predicate and a list of its arguments, all expressed in natural language, in the same way they originally
appeared in the sentence. This methodology lacks the ability to represent cases in which propositions are
inherently embedded, such as conditionals and propositional arguments (e.g. ?Senator Kennedy asked
congress to pass the bill?). Mausam et al. (2012) introduced a context analysis layer, extending this
20
representation with an additional field per tuple, which intends to represent the factuality of the extrac-
tion, accounting specifically for cases of conditionals and attribution. For instance, the assertion ?If he
wins five key states, Romney will be elected President? will be represented as ((Romney; will be elected;
President) ClausalModifier if; he wins five key states).
While these methods capture some of the propositions conveyed by text, they fail to retrieve other
propositions expressed by more sophisticated syntactic constructs. Consider the sentence from Figure 1
?Curiosity will look for evidence that Mars might have had conditions for supporting life?. It exhibits a
construction which the independent tuples format seems to fall short from representing. Our proposed
representation for this sentence is depicted in the bottom right of Figure 1. We represent the complexity
of the sentence through a nested structure of interlinked propositions, each composed of a single pred-
icate and its syntactic arguments and modifiers. In addition, we model certain syntactic variabilities as
features, such as tense, negation, passive voice, etc. Thus, a single assertion is represented through the
discrete propositions it conveys, along with their inter-relations. In addition to the expressibility that this
representation offers, an immediate gain is the often recurring case in which a part of a proposition (for
example, one of the arguments) immediately implies another proposition. For instance, ?The Mars rover
Curiosity is a mobile science lab? implies that ?Curiosity is a rover?, and does so syntactically.
Implicit propositions Certain propositions which are conveyed by the text are not explicitly expressed
in the surface form. Consider, for instance, the sentence ?Facebook?s acquisition of WhatsApp occurred
yesterday?. It introduces the proposition (Facebook, acquired, WhatsApp) through nominalization. Cur-
rent Open IE formalisms are unable to extract such triplets, since the necessary predicate (namely ?ac-
quired?) does not appear in the surface form. Implicit propositions might be introduced in many other
linguistic constructs, such as: appositions (?The company, Random House, doesn?t report its earnings.?
implies that Random House is a company), adjectives (?Tall John walked home? implies that John is tall),
and possessives (?John?s book is on the table? implies that John has a book). We intend to syntactically
identify these implicit propositions, and make them explicit in our representation.
For further analysis of syntax-driven proposition representation, see our recent work (Stanovsky et al.,
2014). We believe that this extension of Open IE representation is feasibly extractable from syntactic
parse trees, and are currently working on automatic conversion from Stanford dependencies (de Marneffe
and Manning, 2008) to interconnected propositions as described.
2.2 Consolidating Information across Propositions
While Open IE is indeed much more scalable than supervised approaches, it does not consolidate natu-
ral language expressions, which leads to either insufficient or redundant information when accessing a
repository of Open IE extractions. As an illustrating example, querying the University of Washington?s
Open IE demo (openie.cs.washington.edu) for the generally equivalent relieves headache or
treats headache returns two different lists of entities; out of the top few results, the only answers these
queries seem to agree on are caffeine and sex. Desirably, an information discovery platform should re-
turn identical results (or at least very similar ones) to these queries. This is a major drawback relative
to supervised knowledge representations, such as Freebase (Bollacker et al., 2008), which map natural
language expressions to canonical formal representations (e.g. the treatments relation in Freebase).
While much relational information can be salvaged from the original text, many inter-propositional
relations stem from background knowledge and our understanding of language. Perhaps the most promi-
nent of these is the entailment relation, as demonstrated in Figure 1. We rely on the definition of textual
entailment as defined by Dagan et al. (2013): proposition T entails proposition H if humans reading T
would typically infer that H is most likely true. Entailment provides an effective structure for aggregat-
ing natural-language based information; it merges semantically equivalent propositions into cliques, and
induces specification-generalization edges between them (if T entails H , then H is more general).
Figure 1 demonstrates the usefulness of entailment in organizing the propositions within a PKG. For
example, the two statements describing Curiosity as a mobile science lab (middle right) originated from
two different texts. However, in a PKG, they are marked as paraphrases (mutually entailing), and both
entail an additional proposition from a third source: ?Curiosity is a lab?. If one were to query all the
21
propositions that entail ?Curiosity is a lab? ? e.g. in response to the query ?What is Curiosity?? ? all
three propositions would be retrieved, even though their surface forms may have ?functions as? instead
of ?is? or ?laboratory? instead of ?lab?.
We have recently taken some first steps in this direction, investigating algorithms for constructing
entailment edges over sets of related propositions (Levy et al., 2014). Even between simple propositions,
recognizing entailment is challenging. We are currently working on new methods that will leverage
structured and unstructured data to recognize entailment for Open IE propositions. There are additional
relations, besides entailment, that should desirably be represented in PKGs as well. Two such examples
are temporal relations (depicted in Figure 1) and causality. Investigating and adapting methods for
recognizing and utilizing these relations is intended for future work.
3 Applications
An appealing application of knowledge graphs is question answering (QA). In this section we demon-
strate how our representation may facilitate more sophisticated information access scenarios.
Structured Queries Queries over structured data give the user the power to receive targeted answers
for her queries. Consider for example the query ?electric cars on sale in Canada?. PKGs can give the
power of queries over structured data to the domain of unstructured information. To answer our query,
we can search the PKG for all of the propositions that entail these two propositions: (1) ?X is an electric
car?, (2) ?X is on sale in Canada?, where X is a variable. The list of X instantiations is the answer
to our structured query. Our knowledge structure enables even more sophisticated queries that involve
more than one variable. For example, ?Japanese corporations that bought Australian start-ups? retrieves
a collection of pairs (X,Y ) where X is the Japanese corporation that bought Y , an Australian start-up.
Summarization Multi-document summarization gives the user the ability to compactly assimilate in-
formation from multiple documents on the same topic. PKGs can be a natural platform leveraged by
summarization because: (1) they would contain the information from those documents as fine-grained
propositions (2) they represent the semantic relations between those propositions. These semantic re-
lations can be leveraged to create high-quality summarizations. For example, the paraphrase (mutual
entailment) relation prevents redundancy. Links of a temporal or causal nature can also dictate the order
in which each proposition is presented. A recent method of summarizing text with entailment graphs
(Gupta et al., 2014) demonstrates the appeal and feasibility of this application.
Faceted Search Faceted search allows a user to interactively navigate a PKG. Adler et al. (2012)
demonstrate this concept on a limited proposition graph. When searching for ?headache? in their demo,
the user can drill-down to find possible causes or remedies, and even focus on subcategories of those;
for example, finding the foods which relieve headaches. As opposed to the structured query application,
retrieval is not fully automated, but rather interactive. It thus allows users to explore and discover new
information they might not have considered a-priori.
4 Discussion
In this position paper we outlined a framework for information discovery that leverages and extends Open
IE, while addressing two of its current major drawbacks. The proposed framework enriches Open IE by
representing natural language in a traversable graph, composed of propositions and their semantic inter-
relations ? A Propositional Knowledge Graph (PKG). The resulting structure provides a representation
in two levels: locally, at sentence level, by representing the syntactic proposition structure embedded in a
single sentence, and globally, at inter-proposition level, where relations are drawn between propositions
from discourse, or from various sources.
At the sentence level, PKG can be compared to Abstract Meaning Representation (AMR) (Banarescu
et al., 2013), which maps a sentence onto a hierarchical structure of propositions (predicate-argument
relations) - a ?meaning representation?. AMR uses Propbank (Kingsbury and Palmer, 2003) for pred-
icates? meaning representation, where possible, and ungrounded natural language, where no respective
22
Propbank lexicon entry exists. While AMR relies on a deep semantic interpretation, our sentence level
representation is more conservative (and thus, hopefully, more feasible) and can be obtained by syntactic
interpretation.
At inter-proposition level, PKG can be compared with traditional Knowledge Graphs (such as Freebase
and Google?s Knowledge Graph). These Knowledge Graphs, in contrast with PKGs, require manual
intervention and aim to cover a rich set of relations using formal language and a pre-specified schema,
thus many relations are inevitably left out (e.g. the relation cracked, as in (Alan Turing, cracked, the
Enigma) does not exist in Freebase).
We believe that PKGs are a promising extension of Open IE?s unsupervised traits, for combining as-
pects of information representation - on a local scale, providing a rich schema for representing sentences,
and on a global scale providing an automated and consolidated method for structuring knowledge.
References
Meni Adler, Jonathan Berant, and Ido Dagan. 2012. Entailment-based text exploration with application to the
health-care domain. In Proceedings of the System Demonstrations of the 50th Annual Meeting of the Association
for Computational Linguistics (ACL 2012), pages 79?84.
S?oren Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak, and Zachary Ives. 2007. Dbpe-
dia: A nucleus for a web of open data. In The semantic web, pages 722?735. Springer.
Laura Banarescu, Claire Bonial, Shu Cai, Madalina Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin Knight, Philipp
Koehn, Martha Palmer, and Nathan Schneider. 2013. Abstract meaning representation for sembanking.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: a collaboratively
created graph database for structuring human knowledge. In Proceedings of the 2008 ACM SIGMOD interna-
tional conference on Management of data, pages 1247?1250. ACM.
Jim Cowie and Wendy Lehnert. 1996. Information extraction. Communications of the ACM, 39(1):80?91.
Ido Dagan, Dan Roth, Mark Sammons, and Fabio Massimo Zanzotto. 2013. Recognizing textual entailment:
Models and applications. Synthesis Lectures on Human Language Technologies, 6(4):1?220.
Marie-Catherine de Marneffe and Christopher D. Manning. 2008. The Stanford typed dependencies representa-
tion. In Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation,
pages 1?8, Manchester, UK, August. Coling 2008 Organizing Committee.
Oren Etzioni, Michele Banko, Stephen Soderland, and Daniel S Weld. 2008. Open information extraction from
the Web. Communications of the ACM, 51(12):68?74.
Anand Gupta, Manpreet Kathuria, Shachar Mirkin, Adarsh Singh, and Aseem Goyal. 2014. Text summarization
through entailment-based minimum vertex cover. In *SEM.
Paul Kingsbury and Martha Palmer. 2003. Propbank: the next level of treebank. In Proceedings of Treebanks and
lexical Theories, volume 3.
Omer Levy, Ido Dagan, and Jacob Goldberger. 2014. Focused entailment graphs for open ie propositions. In
Proceedings of the Eighteenth Conference on Computational Natural Language Learning, Baltimore, Maryland,
USA, June. Association for Computational Linguistics.
Mausam, Michael Schmitz, Stephen Soderland, Robert Bart, and Oren Etzioni. 2012. Open language learning
for information extraction. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural Language Learning, pages 523?534, Jeju Island, Korea, July.
Association for Computational Linguistics.
Gabriel Stanovsky, Jessica Ficler, Ido Dagan, and Yoav Goldberg. 2014. Intermediary semantic representation
through proposition structures. In Workshop on Semantic Parsing, Baltimore, Maryland, USA, June. Associa-
tion for Computational Linguistics.
Fabian M Suchanek, Gjergji Kasneci, and Gerhard Weikum. 2008. Yago: A large ontology from wikipedia and
wordnet. Web Semantics: Science, Services and Agents on the World Wide Web, 6(3):203?217.
23
Fei Wu, Raphael Hoffmann, and Daniel S Weld. 2008. Information extraction from wikipedia: Moving down the
long tail. In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data
mining, pages 731?739. ACM.
Jun Zhu, Zaiqing Nie, Xiaojiang Liu, Bo Zhang, and Ji-Rong Wen. 2009. Statsnowball: a statistical approach to
extracting entity relationships. In Proceedings of the 18th international conference on World wide web, pages
101?110. ACM.
24
