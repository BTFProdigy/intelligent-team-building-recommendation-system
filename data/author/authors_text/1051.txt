Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 65?68,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Assessing the Costs of Sampling Methods in Active Learning for Annotation
Robbie Haertel, Eric Ringger, Kevin Seppi, James Carroll, Peter McClanahan
Department of Computer Science
Brigham Young University
Provo, UT 84602, USA
robbie haertel@byu.edu, ringger@cs.byu.edu, kseppi@cs.byu.edu,
jlcarroll@gmail.com, petermcclanahan@gmail.com
Abstract
Traditional Active Learning (AL) techniques
assume that the annotation of each datum costs
the same. This is not the case when anno-
tating sequences; some sequences will take
longer than others. We show that the AL tech-
nique which performs best depends on how
cost is measured. Applying an hourly cost
model based on the results of an annotation
user study, we approximate the amount of time
necessary to annotate a given sentence. This
model allows us to evaluate the effectiveness
of AL sampling methods in terms of time
spent in annotation. We acheive a 77% re-
duction in hours from a random baseline to
achieve 96.5% tag accuracy on the Penn Tree-
bank. More significantly, we make the case
for measuring cost in assessing AL methods.
1 Introduction
Obtaining human annotations for linguistic data is
labor intensive and typically the costliest part of the
acquisition of an annotated corpus. Hence, there is
strong motivation to reduce annotation costs, but not
at the expense of quality. Active learning (AL) can
be employed to reduce the costs of corpus annotation
(Engelson and Dagan, 1996; Ringger et al, 2007;
Tomanek et al, 2007). With the assistance of AL,
the role of the human oracle is either to label a da-
tum or simply to correct the label from an automatic
labeler. For the present work, we assume that cor-
rection is less costly than annotation from scratch;
testing this assumption is the subject of future work.
In AL, the learner leverages newly provided anno-
tations to select more informative sentences which
in turn can be used by the automatic labeler to pro-
vide more accurate annotations in future iterations.
Ideally, this process yields accurate labels with less
human effort.
Annotation cost is project dependent. For in-
stance, annotators may be paid for the number of an-
notations they produce or by the hour. In the context
of parse tree annotation, Hwa (2004) estimates cost
using the number of constituents needing labeling
and Osborne & Baldridge (2004) use a measure re-
lated to the number of possible parses. With few ex-
ceptions, previous work on AL has largely ignored
the question of actual labeling time. One excep-
tion is (Ngai and Yarowsky, 2000) (discussed later)
which compares the cost of manual rule writing with
AL-based annotation for noun phrase chunking. In
contrast, we focus on the performance of AL algo-
rithms using different estimates of cost (including
time) for part of speech (POS) tagging, although the
results are applicable to AL for sequential labeling
in general. We make the case for measuring cost in
assessing AL methods by showing that the choice of
a cost function significantly affects the choice of AL
algorithm.
2 Benefit and Cost in Active Learning
Every annotation task begins with a set of un-
annotated items U . The ordered set A ? U con-
sists of all annotated data after annotation is com-
plete or after available financial resources (or time)
have been exhausted. We expand the goal of AL
to produce the annotated set A? such that the benefit
gained is maximized and cost is minimized.
In the case of POS tagging, tag accuracy is usu-
65
ally used as the measure of benefit. Several heuristic
AL methods have been investigated for determining
which data will provide the most information and
hopefully the best accuracy. Perhaps the best known
are Query by Committee (QBC) (Seung et al, 1992)
and uncertainty sampling (or Query by Uncertainty,
QBU) (Thrun and Moeller, 1992). Unfortunately,
AL algorithms such as these ignore the cost term of
the maximization problem and thus assume a uni-
form cost of annotating each item. In this case, the
ordering of annotated dataAwill depend entirely on
the algorithm?s estimate of the expected benefit.
However, for AL in POS tagging, the cost term
may not be uniform. If annotators are required to
change only those automatically generated tags that
are incorrect, and depending on how annotators are
paid, the cost of tagging one sentence can depend
greatly on what is known from sentences already an-
notated. Thus, in POS tagging both the benefit (in-
crease in accuracy) and cost of annotating a sentence
depend not only on properties of the sentence but
also on the order in which the items are annotated.
Therefore, when evaluating the performance of an
AL technique, cost should be taken into account. To
illustrate this, consider some basic AL algorithms
evaluated using several simple cost metrics. The re-
sults are presented against a random baseline which
selects sentences at random; the learning curves rep-
resent the average of five runs starting from a ran-
dom initial sentence. If annotators are paid by the
sentence, Figure 1(a) presents a learning curve in-
dicating that the AL policy that selects the longest
sentence (LS) performs rather well. Figure 1(a) also
shows that given this cost model, QBU and QBC are
essentially tied, with QBU enjoying a slight advan-
tage. This indicates that if annotators are paid by
the sentence, QBU is the best solution, and LS is a
reasonable alternative. Next, Figure 1(b) illustrates
that the results differ substantially if annotators are
paid by the word. In this case, using LS as an AL
policy is worse than random selection. Furthermore,
QBC outperforms QBU. Finally, Figure 1(c) shows
what happens if annotators are paid by the number
of word labels corrected. Notice that in this case, the
random selector marginally outperforms the other
techniques. This is because QBU, QBC, and LS tend
to select data that require many corrections. Con-
sidered together, Figures 1(a)-Figure 1(c) show the
significant impact of choosing a cost model on the
relative performance of AL algorithms. This leads
us to conclude that AL techniques should be eval-
uated and compared with respect to a specific cost
function.
While not all of these cost functions are neces-
sarily used in real-life annotation, each can be re-
garded as an important component of a cost model
of payment by the hour. Since each of these func-
tions depends on factors having a significant effect
on the perceived performance of the various AL al-
gorithms, it is important to combine them in a way
that will accurately reflect the true performance of
the selection algorithms.
In prior work, we describe such a cost model for
POS annotation on the basis of the time required for
annotation (Ringger et al, 2008). We refer to this
model as the ?hourly cost model?. This model is
computed from data obtained from a user study in-
volving a POS annotation task. In the study, tim-
ing information was gathered from many subjects
who annotated both sentences and individual words.
This study included tests in which words were pre-
labeled with a candidate labeling obtained from an
automatic tagger (with a known error rate) as would
occur in the context of AL. Linear regression on the
study data yielded a model of POS annotation cost:
h = (3.795 ? l + 5.387 ? c + 12.57)/3600 (1)
where h is the time in hours spent on the sentence, l
is the number of tokens in the sentence, and c is the
number of words in the sentence needing correction.
For this model, the Relative Standard Error (RSE) is
89.5, and the adjusted correlation (R2) is 0.181. This
model reflects the abilities of the annotators in the
study and may not be representative of annotators in
other projects. However, the purpose of this paper is
to create a framework for accounting for cost in AL
algorithms. In contrast to the model presented by
Ngai and Yarowsky (2000), which predicts mone-
tary cost given time spent, this model estimates time
spent from characteristics of a sentence.
3 Evaluation Methodology and Results
Our test data consists of English prose from the
POS-tagged Wall Street Journal text in the Penn
Treebank (PTB) version 3. We use sections 2-21 as
66
 0.86
 0.88
 0.9
 0.92
 0.94
 0.96
 0  500  1000  1500  2000
Ta
g 
Ac
cu
ra
cy
Annotated Sentences
Random
LS
QBU
QBC
(a)
 0.86
 0.88
 0.9
 0.92
 0.94
 0.96
 0  20000  40000  60000  80000  100000
Ta
g 
Ac
cu
ra
cy
Annotated Words
Random
LS
QBU
QBC
(b)
 0.86
 0.88
 0.9
 0.92
 0.94
 0.96
 0  2000  4000  6000  8000  10000
Ta
g 
Ac
cu
ra
cy
Cumulative Tags Corrected
Random
LS
QBU
QBC
(c)
Figure 1: QBU, LS, QBC, and the random baseline plotted in terms of accuracy versus various cost functions: (a)
number of sentences annotated; (b) number of words annotated; and (c) number of tags corrected.
initially unannotated data. We employ section 24 as
the development test set on which tag accuracy is
computed at the end of every iteration of AL.
For tagging, we employ an order two Maximum
EntropyMarkovModel (MEMM). For decoding, we
found that a beam of size five sped up the decoder
with almost no degradation in accuracy fromViterbi.
The features used in this work are typical for modern
MEMM POS tagging and are mostly based on work
by Toutanova and Manning (2000).
In our implementation, QBU employs a single
MEMM tagger. We approximate the entropy of the
per-sentence tag sequences by summing over per-
word entropy and have found that this approxima-
tion provides equivalent performance to the exact se-
quence entropy. We also consider another selection
algorithm introduced in (Ringger et al, 2007) that
eliminates the overhead of entropy computations al-
together by estimating per-sentence uncertainty with
1 ? P (t?), where t? is the Viterbi (best) tag sequence.
We label this scheme QBUOMM (OMM = ?One
Minus Max?).
Our implementation of QBC employs a commit-
tee of three MEMM taggers to balance computa-
tional cost and diversity, following Tomanek et al
(2007). Each committee member?s training set is a
random bootstrap sample of the available annotated
data, but is otherwise as described above for QBU.
We follow Engelson & Dagan (1996) in the imple-
mentation of vote entropy for sentence selection us-
ing these models.
When comparing the relative performance of AL
algorithms, learning curves can be challenging to in-
terpret. As curves proceed to the right, they can ap-
proach one another so closely that it may be difficult
to see the advantage of one curve over another. For
this reason, we introduce the ?cost reduction curve?.
In such a curve, the accuracy is the independent vari-
able. We then compute the percent reduction in cost
(e.g., number of words or hours) over the cost of the
random baseline for the same accuracy a:
redux(a) = (costrnd(a) ? cost(a))/costrnd(a)
Consequently, the random baseline represents the
trajectory redux(a) = 0.0. Algorithms less costly
than the baseline appear above the baseline. For a
specific accuracy value on a learning curve, the cor-
responding value of the cost on the random baseline
is estimated by interpolation between neighboring
points on the baseline. Using hourly cost, Figure 2
shows the cost reduction curves of several AL al-
gorithms, including those already considered in the
learning curves of Figure 1 (except LS). Restricting
the discussion to the random baseline, QBC, and
QBU: for low accuracies, random selection is the
cheapest according to hourly cost; QBU begins to
be cost-effective at around 91%; and QBC begins to
outperform the baseline and QBU around 80%.
4 Normalized Methods
One approach to convert existing AL algorithms into
cost-conscious algorithms is to normalize the results
of the original algorithm by the estimated cost. It
should be somewhat obvious that many selection al-
gorithms are inherently length-biased for sequence
labeling tasks. For instance, since QBU is the sum
67
-0.1
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.86  0.88  0.9  0.92  0.94  0.96
R
ed
uc
tio
n 
in
 H
ou
rly
 C
os
t
Tag Accuracy
Random
QBUOMM/N
QBC/N
QBU/N
QBUOMM
QBC
QBU
Figure 2: Cost reduction curves for QBU, QBC,
QBUOMM, their normalized variants, and the random
baseline on the basis of hourly cost
of entropy over all words, longer sentences will tend
to have higher uncertainty. The easiest solution is
to normalize by sentence length, as has been done
previously (Engelson and Dagan, 1996; Tomanek et
al., 2007). This of course assumes that annotators
are paid by the word, which may or may not be true.
Nevertheless, this approach can be justified by the
hourly cost model. Replacing the number of words
needing correction, c, with the product of l (the sen-
tence length) and the accuracy p of the model, equa-
tion 1 can be re-written as the estimate:
h? = ((3.795 + 5.387p) ? l + 12.57)/3600
Within a single iteration of AL, p is constant, so the
cost is approximately proportional to the length of
the sentence. Figure 2 shows that normalized AL al-
gorithms (suffixed with ?/N?) generally outperform
the standard algorithms based on hourly cost (in
contrast to the cost models used in Figures 1(a) -
(c)). All algorithms shown have significant cost
savings over the random baseline for accuracy lev-
els above 92%. Furthermore, all algorithms except
QBU depict trends of further increasing the advan-
tage after 95%. According to the hourly cost model,
QBUOMM/N has an advantage over all other algo-
rithms for accuracies over 91%, achieving a signifi-
cant 77% reduction in cost at 96.5% accuracy.
5 Conclusions
We have shown that annotation cost affects the as-
sessment of AL algorithms used in POS annotation
and advocate the use of a cost estimate that best es-
timates the true cost. For this reason, we employed
an hourly cost model to evaluate AL algorithms for
POS annotation. We have also introduced the cost
reduction plot in order to assess the cost savings pro-
vided by AL. Furthermore, inspired by the notion
of cost, we evaluated normalized variants of well-
known AL algorithms and showed that these vari-
ants out-perform the standard versions with respect
to the proposed hourly cost measure. In future work
we will build better cost-conscious AL algorithms.
References
S. Engelson and I. Dagan. 1996. Minimizing manual
annotation cost in supervised training from corpora. In
Proc. of ACL, pages 319?326.
R. Hwa. 2004. Sample selection for statistical parsing.
Computational Linguistics, 30:253?276.
G. Ngai and D. Yarowsky. 2000. Rule writing or an-
notation: cost-efficient resource usage for base noun
phrase chunking. In Proc. of ACL, pages 117?125.
M. Osborne and J. Baldridge. 2004. Ensemble-based
active learning for parse selection. In Proc. of HLT-
NAACL, pages 89?96.
E. Ringger, P. McClanahan, R. Haertel, G. Busby,
M. Carmen, J. Carroll, K. Seppi, and D. Lonsdale.
2007. Active learning for part-of-speech tagging: Ac-
celerating corpus annotation. In Proc. of Linguistic
Annotation Workshop, pages 101?108.
E. Ringger, M. Carmen, R. Haertel, K. Seppi, D. Lond-
sale, P. McClanahan, J. Carroll, and N. Ellison. 2008.
Assessing the costs of machine-assisted corpus anno-
tation through a user study. In Proc. of LREC.
H. S. Seung, M. Opper, and H. Sompolinsky. 1992.
Query by committee. In Proc. of CoLT, pages 287?
294.
S. Thrun and K. Moeller. 1992. Active exploration in dy-
namic environments. In NIPS, volume 4, pages 531?
538.
K. Tomanek, J. Wermter, and U. Hahn. 2007. An ap-
proach to text corpus construction which cuts annota-
tion costs and maintains reusability of annotated data.
Proc. of EMNLP-CoNLL, pages 486?495.
K. Toutanova and C. Manning. 2000. Enriching the
knowledge sources used in a maximum entropy part-
of-speech tagger. In Proc. of EMNLP, pages 63?70.
68
Proceedings of the Linguistic Annotation Workshop, pages 101?108,
Prague, June 2007. c?2007 Association for Computational Linguistics
Active Learning for Part-of-Speech Tagging: 
Accelerating Corpus Annotation 
 
Eric Ringger*, Peter McClanahan*, Robbie Haertel*, George Busby*, Marc Carmen**, 
James Carroll*, Kevin Seppi*, Deryle Lonsdale** 
*Computer Science Department; **Linguistics Department 
Brigham Young University 
Provo, Utah, USA 84602 
 
Abstract 
In the construction of a part-of-speech an-
notated corpus, we are constrained by a 
fixed budget. A fully annotated corpus is 
required, but we can afford to label only a 
subset. We train a Maximum Entropy Mar-
kov Model tagger from a labeled subset 
and automatically tag the remainder. This 
paper addresses the question of where to 
focus our manual tagging efforts in order to 
deliver an annotation of highest quality. In 
this context, we find that active learning is 
always helpful. We focus on Query by Un-
certainty (QBU) and Query by Committee 
(QBC) and report on experiments with sev-
eral baselines and new variations of QBC 
and QBU, inspired by weaknesses particu-
lar to their use in this application. Experi-
ments on English prose and poetry test 
these approaches and evaluate their robust-
ness. The results allow us to make recom-
mendations for both types of text and raise 
questions that will lead to further inquiry. 
1 Introduction 
We are operating (as many do) on a fixed budget 
and need annotated text in the context of a larger 
project. We need a fully annotated corpus but can 
afford to annotate only a subset. To address our 
budgetary constraint, we train a model from a ma-
nually annotated subset of the corpus and automat-
ically annotate the remainder. At issue is where to 
focus manual annotation efforts in order to produce 
a complete annotation of highest possible quality. 
A follow-up question is whether these techniques 
work equally well on different types of text. 
In particular, we require part-of-speech (POS) 
annotations. In this paper we employ a state-of-the-
art tagger on both prose and poetry, and we ex-
amine multiple known and novel active learning 
(or sampling) techniques in order to determine 
which work best in this context. We show that the 
results obtained by a state-of-the-art tagger trained 
on a small portion of the data selected through ac-
tive learning can approach the accuracy attained by 
human annotators and are on par with results from 
exhaustively trained automatic taggers. 
In a study based on English language data pre-
sented here, we identify several active learning 
techniques and make several recommendations that 
we hope will be portable for application to other 
text types and to other languages. In section 2 we 
briefly review the state of the art approach to POS 
tagging. In section 3, we survey the approaches to 
active learning employed in this study, including 
variations on commonly known techniques. Sec-
tion 4 introduces the experimental regime and 
presents results and their implications. Section 5 
draws conclusions and identifies opportunities for 
follow-up research. 
2 Part of Speech Tagging 
Labeling natural language data with part-of-speech 
tags can be a complicated task, requiring much 
effort and expense, even for trained annotators. 
Several efforts, notably the Alembic workbench 
(Day et al, 1997) and similar tools, have provided 
interfaces to aid annotators in the process.  
Automatic POS tagging of text using probabilis-
tic models is mostly a solved problem but requires 
supervised learning from substantial amounts of 
training data. Previous work demonstrates the sui-
tability of Hidden Markov Models for POS tagging 
(Kupiec, 1992; Brants, 2000). More recent work 
has achieved state-of-the-art results with Maxi-
101
mum entropy conditional Markov models (MaxEnt 
CMMs, or MEMMs for short) (Ratnaparkhi, 1996; 
Toutanova & Manning, 2000; Toutanova et al, 
2003). Part of the success of MEMMs can be attri-
buted to the absence of independence assumptions 
among predictive features and the resulting ease of 
feature engineering. To the best of our knowledge, 
the present work is the first to present results using 
MEMMs in an active learning framework.  
An MEMM is a probabilistic model for se-
quence labeling. It is a Conditional Markov Model 
(CMM as illustrated in Figure 1) in which a Max-
imum Entropy (MaxEnt) classifier is employed to 
estimate the probability distribution
1.. 1 1 2( | , ) ( | , , , )i i ME i i i i ip t w t p t w f t t? ? ?? over 
possible labels it  for each element in the se-
quence?in our case, for each word iw  in a sen-
tence w . The MaxEnt model is trained from la-
beled data and has access to any predefined 
attributes (represented here by the collection if ) of 
the entire word sequence and to the labels of pre-
vious words ( 1.. 1it ? ). Our implementation employs 
an order-two Markov assumption so the classifier 
has access only to the two previous tags 1 2,i it t? ? . 
We refer to the features 1 2( , , , )i i i iw f t t? ? from 
which the classifier predicts the distribution over 
tags as ?the local trigram context?. 
A Viterbi decoder is a dynamic programming 
algorithm that applies the MaxEnt classifier to 
score multiple competing tag-sequence hypotheses 
efficiently and to produce the best tag sequence, 
according to the model. We approximate Viterbi 
very closely using a fast beam search. Essentially, 
the decoding process involves sequential classifi-
cation, conditioned on the (uncertain) decisions of 
the previous local trigram context classifications. 
The chosen tag sequence t? is the tag sequence 
maximizing the following quantity: 
1 2
1..
? arg max ( | )
arg max ( | , , , )
t
t ME i i i i i
i n
t P t w
p t w f t t? ?
=
=
= ?  
The features used in this work are reasonably 
typical for modern MEMM feature-based POS 
tagging and consist of a combination of lexical, 
orthographic, contextual, and frequency-based in-
formation. In particular, for each word the follow-
ing features are defined: the textual form of the 
word itself, the POS tags of the preceding two 
words, and the textual form of the following word. 
Following Toutanova and Manning (2000) approx-
imately, more information is defined for words that 
are considered rare (which we define here as words 
that occur fewer than fifteen times). We consider 
the tagger to be near-state-of-the-art in terms of 
tagging accuracy. 
 
Figure 1. Simple Markov order 2 CMM, with focus on 
the i-th hidden label (or tag). 
3 Active Learning 
The objective of this research is to produce more 
high quality annotated data with less human anno-
tator time and effort. Active learning is an ap-
proach to machine learning in which a model is 
trained with the selective help of an oracle. The 
oracle provides labels on a sufficient number of 
?tough? cases, as identified by the model. Easy 
cases are assumed to be understood by the model 
and to require no additional annotation by the 
oracle. Many variations have been proposed in the 
broader active learning and decision theory litera-
ture under many names, including ?active sam-
pling? and ?optimal sampling.? 
In active learning for POS tagging, as in other 
applications, the oracle can be a human. For expe-
rimental purposes, a human oracle is simulated 
using pre-labeled data, where the labels are hidden 
until queried. To begin, the active learning process 
requires some small amount of training data to 
seed the model. The process proceeds by identify-
ing the data in the given corpus that should be 
tagged first for maximal impact. 
3.1 Active Learning in the Language Context 
When considering the role of active learning, we 
were initially drawn to the work in active learning 
for classification. In a simple configuration, each 
instance (document, image, etc.) to be labeled can 
be considered to be independent. However, for ac-
tive learning for the POS tagging problem we con-
sidered the nature of human input as an oracle for 
the task. As an approximation, people read sen-
tences as propositional atoms, gathering contextual 
cues from the sentence in order to assemble the 
102
meaning of the whole. Consequently, we thought it 
unreasonable to choose the word as the granularity 
for active learning. Instead, we begin with the as-
sumption that a human will usually require much 
of the sentence or at least local context from the 
sentence in order to label a single word with its 
POS label. While focusing on a single word, the 
human may as well label the entire sentence or at 
least correct the labels assigned by the tagger for 
the sentence. Consequently, the sentence is the 
granularity of annotation for this work. (Future 
work will question this assumption and investigate 
tagging a word or a subsequence of words at a 
time.) This distinguishes our work from active 
learning for classification since labels are not 
drawn from a fixed set of labels. Rather, every sen-
tence of length n can be labeled with a tag se-
quence drawn from a set of size nT , where T  is 
the size of the per-word tag set. Granted, many of 
the options have very low probability. 
To underscore our choice of annotating at the 
granularity of a sentence, we also note that a max-
imum entropy classifier for isolated word tagging 
that leverages attributes of neighboring words?
but is blind to all tags?will underperform an 
MEMM that includes the tags of neighboring 
words (usually on the left) among its features. Pre-
vious experiments demonstrate the usefulness of 
tags in context on the standard Wall Street Journal 
data from the Penn Treebank (Marcus et al, 1999). 
A MaxEnt isolated word tagger achieves 93.7% on 
words observed in the training set and 82.6% on 
words unseen in the training set. Toutanova and 
Manning (2000) achieves 96.9% (on seen) and 
86.9% (on unseen) with an MEMM. They sur-
passed their earlier work in 2003 with a ?cyclic 
dependency network tagger?, achieving 
97.2%/89.05% (seen/unseen) (Toutanova et al, 
2003). The generally agreed upon upper bound is 
around 98%, due to label inconsistencies in the 
Treebank. The main point is that effective use of 
contextual features is necessary to achieve state of 
the art performance in POS tagging. 
In active learning, we employ several sets of 
data that we refer to by the following names: 
? Initial Training: the small set of data used 
to train the original model before active 
learning starts 
? Training: data that has already been la-
beled by the oracle as of step i in the learn-
ing cycle 
? Unannotated: data not yet labeled by the 
oracle as of step i 
? Test (specifically Development Test): la-
beled data used to measure the accuracy of 
the model at each stage of the active learn-
ing process. Labels on this set are held in 
reserve for comparison with the labels 
chosen by the model. It is the accuracy on 
this set that we report in our experimental 
results in Section 4. 
Note that the Training set grows at the expense of 
the Unannotated set as active learning progresses. 
Active Learning for POS Tagging consists of the 
following steps: 
1. Train a model with Initial Training data 
2. Apply model to Unannotated data 
3. Compute potential informativeness of 
each sentence 
4. Remove top n sentences with most po-
tential informativeness from Unanno-
tated data and give to oracle 
5. Add n sentences annotated (or corrected) 
by the oracle to Training data 
6. Retrain model with Training data 
7. Return to step 2 until stopping condition 
is met. 
There are several possible stopping conditions, 
including reaching a quality bar based on accuracy 
on the Test set, the rate of oracle error corrections 
in the given cycle, or even the cumulative number 
of oracle error corrections. In practice, the exhaus-
tion of resources, such as time or money, may 
completely dominate all other desirable stopping 
conditions. 
Several methods are available for determining 
which sentences will provide the most information. 
Expected Value of Sample Information (EVSI) 
(Raiffa & Schlaiffer, 1967) would be the optimal 
approach from a decision theoretic point of view, 
but it is computationally prohibitive and is not con-
sidered here. We also do not consider the related 
notion of query-by-model-improvement or other 
methods (Anderson & Moore, 2005; Roy & 
McCallum, 2001a, 2001b). While worth exploring, 
they do not fit in the context of this current work 
and should be considered in future work. We focus 
here on the more widely used Query by Committee 
(QBC) and Query by Uncertainty (QBU), includ-
ing our new adaptations of these. 
Our implementation of maximum entropy train-
ing employs a convex optimization procedure 
known as LBFGS. Although this procedure is rela-
tively fast, training a model (or models in the case 
103
of QBC) from scratch on the training data during 
every round of the active learning loop would pro-
long our experiments unnecessarily. Instead we 
start each optimization search with a parameter set 
consisting of the model parameters from the pre-
vious iteration of active learning (we call this ?Fast 
MaxEnt?). In practice, this converges quickly and 
produces equivalent results. 
3.2 Query by Committee 
Query by Committee (QBC) was introduced by 
Seung, Opper, and Sompolinsky (1992). Freund, 
Seung, Shamir, and Tishby (1997) provided a care-
ful analysis of the approach. Engelson and Dagan 
(1996) experimented with QBC using HMMs for 
POS tagging and found that selective sampling of 
sentences can significantly reduce the number of 
samples required to achieve desirable tag accura-
cies. Unlike the present work, Engelson & Dagan 
were restricted by computational resources to se-
lection from small windows of the Unannotated set, 
not from the entire Unannotated set. Related work 
includes learning ensembles of POS taggers, as in 
the work of Brill and Wu (1998), where an ensem-
ble consisting of a unigram model, an N-gram 
model, a transformation-based model, and an 
MEMM for POS tagging achieves substantial re-
sults beyond the individual taggers. Their conclu-
sion relevant to this paper is that different taggers 
commit complementary errors, a useful fact to ex-
ploit in active learning. QBC employs a committee 
of N models, in which each model votes on the 
correct tagging of a sentence. The potential infor-
mativeness of a sentence is measured by the total 
number of tag sequence disagreements (compared 
pair-wise) among the committee members. Possi-
ble variants of QBC involve the number of com-
mittee members, how the training data is split 
among the committee members, and whether the 
training data is sampled with or without replace-
ment. 
A potential problem with QBC in this applica-
tion is that words occur with different frequencies 
in the corpus. Because of the potential for greater 
impact across the corpus, querying for the tag of a 
more frequent word may be more desirable than 
querying for the tag of a word that occurs less fre-
quently, even if there is greater disagreement on 
the tags for the less frequent word. We attempted 
to compensate for this by weighting the number of 
disagreements by the corpus frequency of the word 
in the full data set (Training and Unannotated). 
Unfortunately, this resulted in worse performance; 
solving this problem is an interesting avenue for 
future work. 
3.3 Query by Uncertainty 
The idea behind active sampling based on uncer-
tainty appears to originate with Thrun and Moeller 
(1992). QBU has received significant attention in 
general. Early experiments involving QBU were 
conducted by Lewis and Gale (1994) on text classi-
fication, where they demonstrated significant bene-
fits of the approach. Lewis and Catlett (1994) ex-
amined its application for non-probabilistic learn-
ers in conjunction with other probabilistic learners 
under the name ?uncertainty sampling.? Brigham 
Anderson (2005) explored QBU using HMMs and 
concluded that it is sometimes advantageous. We 
are not aware of any published work on the appli-
cation of QBU to POS tagging. In our implementa-
tion, QBU employs a single MEMM tagger. The 
MaxEnt model comprising the tagger can assess 
the probability distribution over tags for any word 
in its local trigram context, as illustrated in the ex-
ample in Figure 2. 
Figure 2. Distribution over tags for the word ?hurdle? in 
italics. The local trigram context is in boldface. 
In Query by Uncertainty (QBU), the informa-
tiveness of a sample is assumed to be the uncer-
tainty in the predicted distribution over tags for 
that sample, that is the entropy of 
1 2( | , , , )ME i i i i ip t w f t t? ? . To determine the poten-
tial informativeness of a word, we can measure the 
entropy in that distribution. Since we are selecting 
sentences, we must extend our measure of uncer-
tainty beyond the word. 
3.4 Adaptations of QBU 
There are several problems with the use of QBU in 
this context: 
? Some words are more important; i.e., they 
contain more information perhaps because 
they occur more frequently. 
   NN 0 .85 
   VB  0.13 
   ... 
RB    DT JJS CD  2.0E-7 
 
Perhaps     the biggest   hurdle ? 
104
? MaxEnt estimates per-word distributions 
over tags, not per-sentence distributions 
over tag sequences. 
? Entropy computations are relatively costly. 
We address the first issue in a new version of QBU 
which we call ?Weighted Query by Uncertainty? 
(WQBU). In WQBU, per-word uncertainty is 
weighted by the word's corpus frequency. 
To address the issue of estimating per-sentence 
uncertainty from distributions over tag sequences, 
we have considered several different approaches. 
The per-word (conditional) entropy is defined as 
follows: 
 
 
 
 
 
 
where iT  is the random variable for the tag it  on 
word iw , and the features of the context in which 
iw  occurs are denoted, as before, by the collection 
if  and the prior tags 1 2,i it t? ? . It is straightforward 
to calculate this entropy for each word in a sen-
tence from the Unannotated set, if we assume that 
previous tags 1 2,i it t? ?  are from the Viterbi (best) 
tag sequence (for the entire sentence) according to 
the model. 
For an entire sentence, we estimate the tag-
sequence entropy by summing over all possible tag 
sequences. However, computing this estimate ex-
actly on a 25-word sentence, where each word can 
be labeled with one of 35 tags, would require 3525 
= 3.99*1038 steps. Instead, we approximate the per-
sentence tag sequence distribution entropy by 
summing per-word entropy: 
 
 
This is the approach we refer to as QBU in the 
experimental results section. We have experi-
mented with a second approach that estimates the 
per-sentence entropy of the tag-sequence distribu-
tion by Monte Carlo decoding. Unfortunately, cur-
rent active learning results involving this MC POS 
tagging decoder are negative on small Training set 
sizes, so we do not present them here. Another al-
ternative approximation worth pursuing is compu-
ting the per-sentence entropy using the n-best POS 
tag sequences. Very recent work by Mann and 
McCallum (2007) proposes an approach in which 
exact sequence entropy can be calculated efficient-
ly. Further experimentation is required to compare 
our approximation to these alternatives. 
An alternative approach that eliminates the 
overhead of entropy computations entirely is to 
estimate per-sentence uncertainty with ?1 ( )P t? , 
where t? is the Viterbi (best) tag sequence. We call 
this scheme QBUV. In essence, it selects a sample 
consisting of the sentences having the highest 
probability that the Viterbi sequence is wrong. To 
our knowledge, this is a novel approach to active 
learning. 
4 Experimental Results 
In this section, we examine the experimental setup, 
the prose and poetry data sets, and the results from 
using the various active learning algorithms on 
these corpora. 
4.1 Setup 
The experiments focus on the annotation scenario 
posed earlier, in which budgetary constraints af-
ford only some number x of sentences to be anno-
tated. The x-axis in each graph captures the num-
ber of sentences. For most of the experiments, the 
graphs present accuracies on the (Development) 
Test set. Later in this section, we present results for 
an alternate metric, namely number of words cor-
rected by the oracle. 
In order to ascertain the usefulness of the active 
learning approaches explored here, the results are 
presented against a baseline in which sentences are 
selected randomly from the Unannotated set. We 
consider this baseline to represent the use of a 
state-of-the-art tagger trained on the same amount 
of data as the active learner. Due to randomization, 
the random baseline is actually distinct from expe-
riment to experiment without any surprising devia-
tions. Also, each result curve in each graph 
represents the average of three distinct runs. 
Worth noting is that most of the graphs include 
active learning curves that are run to completion; 
namely, the rightmost extent of all curves 
represents the exhaustion of the Unannotated data. 
At this extreme point, active learning and random 
sample selection all have the same Training set. In 
the scenarios we are targeting, this far right side is 
not of interest. Points representing smaller amounts 
of annotated data are our primary interest. 
In the experiments that follow, we address sev-
eral natural questions that arise in the course of 
applying active learning. We also compare the va-
1 2
1 2
1 2
( | , , , )
( | , , , )
log ( | , , , )
i
i i i i i
ME i i i i i
t Tagset
ME i i i i i
H T w f t t
p t w f t t
p t w f t t
? ?
? ?
?
? ?
= ?
?
?
1 2
? ( | ) ( | , , , )
i
i i i i i
w w
H T w H T w f t t? ?
?
? ??
105
riants of QBU and QBC. For QBC, committee 
members divide the training set (at each stage of 
the active learning process) evenly. All committee 
members and final models are MEMMs. Likewise, 
all variants of QBU employ MEMMs. 
4.2 Data Sets 
The experiments involve two data sets in search 
of conclusions that generalize over two very dif-
ferent kinds of English text. The first data set con-
sists of English prose from the POS-tagged one-
million-word Wall Street Journal text in the Penn 
Treebank (PTB) version 3. We use a random sam-
ple of the corpus constituting 25% of the tradition-
al training set (sections 2?21). Initial Training data 
consists of 1% of this set. We employ section 24 as 
the Development Test set. Average sentence length 
is approximately 25 words. 
Our second experimental set consists of English 
poetry from the British National Corpus (BNC) 
(Godbert & Ramsay, 1991; Hughes, 1982; Raine, 
1984). The text is also fully tagged with 91 parts of 
speech from a different tag set than the one used 
for the PTB. The BNC XML data was taken from 
the files B1C.xml, CBO.xml, and H8R.xml. This 
results in a set of 60,056 words and 8,917 sen-
tences. 
4.3 General Results 
To begin, each step in the active learning process 
adds a batch of 100 sentences from the Unanno-
tated set at a time. Figure 3 demonstrates (using 
QBU) that the size of a query batch is not signifi-
cant in these experiments.  
The primary question to address is whether ac-
tive learning helps or not. Figure 4 demonstrates 
that QBU, QBUV, and QBC all outperform the 
random baseline in terms of total, per-word accu-
racy on the Test set, given the same amount of 
Training data. Figure 5 is a close-up version of 
Figure 4, placing emphasis on points up to 1000 
annotated sentences. In these figures, QBU and 
QBUV vie for the best performing active learning 
algorithm. These results appear to give some useful 
advice captured in Table 1. The first column in the 
table contains the starting conditions. The remain-
ing columns indicate that for between 800-1600 
sentences of annotation, QBUV takes over from 
QBU as the best selection algorithm. 
The next question to address is how much initial 
training data should be used; i.e., when should we 
start using active learning? The experiment in Fig-
ure 6 demonstrates (using QBU) that one should 
use as little data as possible for Initial Training 
Data. There is always a significant advantage to 
starting early. In the experiment documented in  
 
Figure 3. Varying the size of the query batch in active 
learning yields identical results after the first query batch.  
 
Figure 4. The best representatives of each type of active 
learner beat the baseline. QBU and QBUV trade off the 
top position over QBC and the Baseline. 
Figure 5. Close-up of the low end of the graph from Figure 
4. QBUV and QBU are nearly tied for best performance. 
 75
 80
 85
 90
 95
 100  1000  10000
A
cc
u
ra
cy
 (
%
)
Number of Sentences in Training Set
Batch Query Size of 10 Sentences
Batch Query Size of 100 Sentences
Batch Query Size of 500 Sentences
 75
 80
 85
 90
 95
 100  1000  10000
A
cc
ur
ac
y 
(%
)
Number of Sentences in Training Set
QBUV 
QBU
QBC
Baseline
 76
 78
 80
 82
 84
 86
 88
 90
 92
 100  1000
A
cc
u
ra
cy
 (
%
)
Number of Sentences in Training Set
QBUV 
QBU
QBC
Baseline
106
this figure, a batch query size of one was employed 
in order to make the point as clearly as possible. 
Larger batch query sizes produce a graph with sim-
ilar trends as do experiments involving larger Un-
annotated sets and other active learners. 
 
 100 200 400 800 1600 3200 6400 
QBU 76.26 86.11 90.63 92.27 93.67 94.65 95.42 
QBUV 76.65 85.09 89.75 92.24 93.72 94.96 95.60 
QBC 76.19 85.77 89.37 91.78 93.49 94.62 95.36 
Base 76.57 82.13 86.68 90.12 92.49 94.02 95.19 
Table 1. The best models (on PTB WSJ data) with various 
amounts of annotation (columns). 
 
Figure 6. Start active learning as early as possible for a 
head start. 
4.4 QBC Results 
An important question to address for QBC is 
what number of committee members produces the 
best results? There was no significant difference in 
results from the QBC experiments when using be-
tween 3 and 7 committee members. For brevity we 
omit the graph. 
4.5 QBU Results 
For Query by Uncertainty, the experiment in Fig-
ure 7 demonstrates that QBU is superior to QBUV 
for low counts, but that QBUV slightly overtakes 
QBU beyond approximately 300 sentences. In fact, 
all QBU variants, including the weighted version, 
surpassed the baseline. WQBU has been omitted 
from the graph, as it was inferior to straight-
forward QBU. 
4.6 Results on the BNC 
Next we introduce results on poetry from the Brit-
ish National Corpus. Recall that the feature set 
employed by the MEMM tagger was optimized for 
performance on the Wall Street Journal. For the 
experiment presented in Figure 8, all data in the 
Training and Unannotated sets is from the BNC, 
but we employ the same feature set from the WSJ 
experiments. This result on the BNC data shows 
first of all that tagging poetry with this tagger 
leaves a final shortfall of approximately 8% from 
the WSJ results. Nonetheless and more importantly, 
the active learning trends observed on the WSJ still 
hold. QBC is better than the baseline, and QBU 
and QBUV trade off for first place. Furthermore, 
for low numbers of sentences, it is overwhelmingly 
to one?s advantage to employ active learning for 
annotation. 
 
 
Figure 7. QBUV is superior to QBU overall, but QBU is 
better for very low counts. Both are superior to the ran-
dom baseline and the Longest Sentence (LS) baseline. 
 
Figure 8. Active learning results on the BNC poetry data. 
Accuracy of QBUV, QBU, and QBC against the random 
baseline. QBU and QBUV are nearly indistinguishable. 
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 10  100
A
cc
u
ra
cy
 (
%
)
Number of Sentences in Training Set
1%
5%
10%
25%
 75
 80
 85
 90
 95
 100  1000  10000
A
cc
u
ra
cy
 (
%
)
Number of Sentences in Training Set
QBU
QBUV
LS
Baseline 
 40
 45
 50
 55
 60
 65
 70
 75
 80
 85
 90
 100  1000  10000
A
cc
u
ra
cy
 (
%
)
Number of Sentences in Training Set
QBU
QBUV
Baseline
QBC
107
4.7 Another Perspective 
Next, briefly consider a different metric on the ver-
tical axis. In Figure 9, the metric is the total num-
ber of words changed (corrected) by the oracle. 
This quantity reflects the cumulative number of 
differences between the tagger?s hypothesis on a 
sentence (at the point in time when the oracle is 
queried) and the oracle?s answer (over the training 
set). It corresponds roughly to the amount of time 
that would be required for a human annotator to 
correct the tags suggested by the model. This fig-
ure reveals that QBUV makes significantly more 
changes than QBU, QBC, or LS (the Longest Sen-
tence baseline). Hence, the superiority of QBU 
over QBUV, as measured by this metric, appears to 
outweigh the small wins provided by QBUV when 
measured by accuracy alone. That said, the random 
baseline makes the fewest changes of all. If this 
metric (and not some combination with accuracy) 
were our only consideration, then active learning 
would appear not to serve our needs. 
This metric is also a measure of how well a par-
ticular query algorithm selects sentences that espe-
cially require assistance from the oracle. In this 
sense, QBUV appears most effective. 
 
Figure 9. Cumulative number of corrections made by the 
oracle for several competitive active learning algorithms. 
QBU requires fewer corrections than QBUV. 
5 Conclusions 
Active learning is a viable way to accelerate the 
efficiency of a human annotator and is most effec-
tive when done as early as possible. We have pre-
sented state-of-the-art tagging results using a frac-
tion of the labeled data. QBUV is a cheap approach 
to performing active learning, only to be surpassed 
by QBU when labeling small numbers of sentences. 
We are in the midst of conducting a user study to 
assess the true costs of annotating a sentence at a 
time or a word at a time. We plan to incorporate 
these specific costs into a model of cost measured 
in time (or money) that will supplant the metrics 
reported here, namely accuracy and number of 
words corrected. As noted earlier, future work will 
also evaluate active learning at the granularity of a 
word or a subsequence of words, to be evaluated 
by the cost metric. 
References 
Anderson, B., and Moore, A. (2005). ?Active Learning for HMM: 
Objective Functions and Algorithms.? ICML, Germany. 
Brants, T., (2000). ?TnT -- a statistical part-of-speech tagger.? ANLP, 
Seattle, WA. 
Brill, E., and Wu, J. (1998). ?Classifier combination for improved 
lexical disambiguation.? Coling/ACL, Montreal, Quebec, Canada. 
Pp. 191-195.  
Day, D., et al (1997). ?Mixed-Initiative Development of Language 
Processing Systems.? ANLP, Washington, D.C. 
Engelson, S. and Dagan, I. (1996). ?Minimizing manual annotation 
cost in supervised training from corpora.? ACL, Santa Cruz, Cali-
fornia. Pp. 319-326. 
Freund, Y., Seung, H., Shamir, E., and Tishby, N. (1997). ?Selective 
sampling using the query by committee algorithm.? Machine 
Learning, 28(2-3):133-168.  
Godbert, G. and Ramsay, J. (1991). ?For now.? In the British National 
Corpus file B1C.xml. London: The Diamond Press (pp. 1-108).  
Hughes, T. (1982). ?Selected Poems.? In the British National Corpus 
file H8R.xml. London: Faber & Faber Ltd. (pp. 35-235).  
Kupiec, J. (1992). ?Robust part-of-speech tagging using a hidden 
Markov model.? Computer Speech and Language 6, pp. 225-242. 
Lewis, D., and Catlett, J. (1994). ?Heterogeneous uncertainty sam-
pling for supervised learning.? ICML. 
Lewis, D., and Gale, W. (1995). ?A sequential algorithm for training 
text classifiers: Corrigendum and additional data.? SIGIR Forum, 
29 (2), 13--19. 
Mann, G., and McCallum, A. (2007). "Efficient Computation of En-
tropy Gradient for Semi-Supervised Conditional Random Fields". 
NAACL-HLT. 
Marcus, M. et al (1999). ?Treebank-3.? Linguistic Data Consortium, 
Philadelphia, PA. 
Raiffa, H. and Schlaiffer, R. (1967). Applied Statistical Decision 
Theory. New York: Wiley Interscience.  
Raine, C. (1984). ?Rich.? In the British National Corpus file CB0.xml. 
London: Faber & Faber Ltd. (pp. 13-101).  
Ratnaparkhi, A. (1996). ?A Maximum Entropy Model for Part-Of-
Speech Tagging.? EMNLP. 
Roy, N., and McCallum, A. (2001a). ?Toward optimal active learning 
through sampling estimation of error reduction.? ICML. 
Roy, N. and McCallum, A. (2001b). ?Toward Optimal Active Learn-
ing through Monte Carlo Estimation of Error Reduction.? ICML, 
Williamstown. 
Seung, H., Opper, M., and Sompolinsky, H. (1992). ?Query by com-
mittee?.  COLT. Pp. 287-294. 
Thrun S., and Moeller, K. (1992). ?Active exploration in dynamic 
environments.? NIPS.  
Toutanova, K., Klein, D., Manning, C., and Singer, Y. (2003). ?Fea-
ture-Rich Part-of-Speech Tagging with a Cyclic Dependency Net-
work.? HLT-NAACL. Pp. 252-259. 
Toutanova, K. and Manning, C. (2000). ?Enriching the Knowledge 
Sources Used in a Maximum Entropy Part-of-Speech Tagger.? 
EMNLP, Hong Kong. Pp. 63-70. 
 0
 1000
 2000
 3000
 4000
 5000
 6000
 7000
 8000
 9000
 10000
 100  1000  10000
N
u
m
b
er
 o
f 
C
h
an
g
ed
 W
o
rd
s
Number of Sentences in Training Set
QBUV 
QBU 
QBC 
Baseline 
LS 
108
