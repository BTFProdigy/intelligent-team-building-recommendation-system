Proceedings of the Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties, pages 12?19,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Automatic Identification of Non-Compositional Multi-Word Expressions
using Latent Semantic Analysis
Graham Katz
Institute of Cognitive Science
University of Osnabru?ck
gkatz@uos.de
Eugenie Giesbrecht
Institute of Cognitive Science
University of Osnabru?ck
egiesbre@uos.de
Abstract
Making use of latent semantic analy-
sis, we explore the hypothesis that lo-
cal linguistic context can serve to iden-
tify multi-word expressions that have non-
compositional meanings. We propose that
vector-similarity between distribution vec-
tors associated with an MWE as a whole
and those associated with its constitutent
parts can serve as a good measure of the
degree to which the MWE is composi-
tional. We present experiments that show
that low (cosine) similarity does, in fact,
correlate with non-compositionality.
1 Introduction
Identifying non-compositional (or idiomatic)
multi-word expressions (MWEs) is an important
subtask for any computational system (Sag et al,
2002), and significant attention has been paid
to practical methods for solving this problem in
recent years (Lin, 1999; Baldwin et al, 2003;
Villada Moiro?n and Tiedemann, 2006). While
corpus-based techniques for identifying collo-
cational multi-word expressions by exploiting
statistical properties of the co-occurrence of the
component words have become increasingly
sophisticated (Evert and Krenn, 2001; Evert,
2004), it is well known that mere co-occurrence
does not well distinguish compositional from
non-compositional expressions (Manning and
Schu?tze, 1999, Ch. 5).
While expressions which may potentially have
idiomatic meanings can be identified using various
lexical association measures (Evert and Krenn,
2001; Evert and Kermes, 2003), other techniques
must be used to determining whether or not a par-
ticular MWE does, in fact, have an idiomatic use.
In this paper we explore the hypothesis that the
local linguistic context can provide adequate cues
for making this determination and propose one
method for doing this.
We characterize our task on analogy with word-
sense disambiguation (Schu?tze, 1998; Ide and
Ve?ronis, 1998). As noted by Schu?tze, WSD
involves two related tasks: the general task of
sense discrimination?determining what senses
a given word has?and the more specific task
of sense selection?determining for a particular
use of the word in context which sense was in-
tended. For us the discrimination task involves
determining for a given expression whether it has
a non-compositional interpretation in addition to
its compositional interpretation, and the selec-
tion task involves determining in a given context,
whether a given expression is being used compo-
sitionally or non-compostionally. The German ex-
pression ins Wasser fallen, for example, has a non-
compositional interpretation on which it means ?to
fail to happen? (as in (1)) and a compositional in-
terpretation on which it means ?to fall into water
(as in (2)).1
(1) Das Kind war beim Baden von einer Luftma-
tratze ins Wasser gefallen.
?The child had fallen into the water from an a
air matress while swimming?
(2) Die Ero?fnung des Skateparks ist ins Wasser
gefallen.
?The opening of the skatepark was cancelled?
The discrimination task, then, is to identify ins
Wasser fallen as an MWE that has an idiomatic
meaning and the selection task is to determine that
1Examples taken from a newspaper corpus of the German
Su?ddeutsche Zeitung (1994-2000)
12
in (1) it is the compositional meaning that is in-
tended, while in (2) it is the non-compositional
meaning.
Following Schu?tze (1998) and Landauer & Du-
mais (1997) our general assumption is that the
meaning of an expression can be modelled in
terms of the words that it co-occurs with: its
co-occurrence signature. To determine whether
a phrase has a non-compositional meaning we
compute whether the co-occurrence signature of
the phrase is systematically related to the co-
occurrence signatures of its parts. Our hypoth-
esis is that a systematic relationship is indica-
tive of compositional interpretation and lack of
a systematic relationship is symptomatic of non-
compositionality. In other words, we expect com-
positional MWEs to appear in contexts more sim-
ilar to those in which their component words ap-
pear than do non-compositional MWEs.
In this paper we describe two experiments that
test this hypothesis. In the first experiment we
seek to confirm that the local context of a known
idiom can reliably distinguish idiomatic uses from
non-idiomatic uses. In the second experiment we
attempt to determine whether the difference be-
tween the contexts in which an MWE appears and
the contexts in which its component words appear
can indeed serve to tell us whether the MWE has
an idiomatic use.
In our experiments we make use of lexical se-
mantic analysis (LSA) as a model of context-
similarity (Deerwester et al, 1990). Since this
technique is often used to model meaning, we will
speak in terms of ?meaning? similiarity. It should
be clear, however, that we are only using the LSA
vectors?derived from context of occurrence in a
corpus?to model meaning and meaning composi-
tion in a very rough way. Our hope is simply that
this rough model is sufficient to the task of identi-
fying non-compositional MWEs.
2 Previous work
Recent work which attempts to discriminate
between compositional and non-compositional
MWEs include Lin (1999), who used mutual-
information measures identify such phrases, Bald-
win et al (2003), who compare the distribution
of the head of the MWE with the distribution of
the entire MWE, and Vallada Moiro?n & Tiede-
mann (2006), who use a word-alignment strat-
egy to identify non-compositional MWEs making
use of parallel texts. Schone & Jurafsky (2001)
applied LSA to MWE identification, althought
they did not focus on distinguishing compositional
from non-compositional MWEs.
Lin?s goal, like ours, was to discriminate non-
compositional MWEs from compositional MWEs.
His method was to compare the mutual informa-
tion measure of the constituents parts of an MWE
with the mutual information of similar expressions
obtained by substituting one of the constituents
with a related word obtained by thesaurus lookup.
The hope was that a significant difference between
these measures, as in the case of red tape (mutual
information: 5.87) compared to yellow tape (3.75)
or orange tape (2.64), would be characteristic of
non-compositional MWEs. Although intuitively
appealing, Lin?s algorithm only achieves precision
and recall of 15.7% and 13.7%, respectively (as
compared to a gold standard generate from an id-
iom dictionary?but see below for discussion).
Schone & Jurafsky (2001) evaluated a num-
ber of co-occurrence-based metrics for identify-
ing MWEs, showing that, as suggested by Lin?s
results, there was need for improvement in this
area. Since LSA has been used in a number
of meaning-related language tasks to good ef-
fect (Landauer and Dumais, 1997; Landauer and
Psotka, 2000; Cederberg and Widdows, 2003),
they had hoped to improve their results by identify
non-compositional expressions using a method
similar to that which we are exploring here. Al-
though they do not demonstrate that this method
actually identifies non-compositional expressions,
they do show that the LSA similarity technique
only improves MWE identification minimally.
Baldwin et al, (2003) focus more narrowly
on distinguishing English noun-noun compounds
and verb-particle constructions which are com-
positional from those which are not composi-
tional. Their approach is methodologically similar
to ours, in that they compute similarity on the ba-
sis of contexts of occurrance, making use of LSA.
Their hypothesis is that high LSA-based similar-
ity between the MWE and each of its constituent
parts is indicative of compositionality. They evalu-
ate their technique by assessing the correlation be-
tween high semantic similarity of the constituents
of an MWE to the MWE as a whole with the like-
lihood that the MWE appears in WordNet as a hy-
ponym of one of the constituents. While the ex-
pected correlation was not attested, we suspect this
13
to be more an indication of the inappropriateness
of the evaluation used than of the faultiness of the
general approach.
Lin, Baldwin et al, and Schone & Jurafsky, all
use as their gold standard either idiom dictionaries
or WordNet (Fellbaum, 1998). While Schone &
Jurafsky show that WordNet is as good a standard
as any of a number of machine readable dictionar-
ies, none of these authors shows that the MWEs
that appear in WordNet (or in the MRDs) are gen-
erally non-compositional, in the relevant sense. As
noted by Sag et al (2002) many MWEs are sim-
ply ?institutionalized phrases? whose meanings
are perfectly compositional, but whose frequency
of use (or other non-linguistic factors) make them
highly salient. It is certainly clear that many
MWEs that appear in WordNet?examples being
law student, medical student, college man?are
perfectly compositional semantically.
Zhai (1997), in an early attempt to apply
statistical methods to the extraction of non-
compositional MWEs, made use of what we take
to be a more appropriate evaluation metric. In his
comparison among a number of different heuris-
tics for identifying non-compositional noun-noun
compounds, Zhai did his evaluation by applying
each heuristic to a corpus of items hand-classified
as to their compositionality. Although Zhai?s clas-
sification appears to be problematic, we take this
to be the appropirate paradigm for evaluation in
this domain, and we adopt it here.
3 Proceedure
In our work we made use of the Word Space
model of (semantic) similiarty (Schu?tze, 1998)
and extended it slightly to MWEs. In this frame-
work, ?meaning? is modeled as an n-dimensional
vector, derived via singular value decomposition
(Deerwester et al, 1990) from word co-occurrence
counts for the expression in question, a technique
frequently referred to as Latent Semantic Analysis
(LSA). This kind of dimensionality reduction has
been shown to improve performance in a number
of text-based domains (Berry et al, 1999).
For our experiments we used a local German
newspaper corpus.2 We built our LSA model
with the Infomap Software package.3, using the
1000 most frequent words not on the 102-word
2Su?ddeutsche Zeitung (SZ) corpus for 2003 with about 42
million words.
3Available from infomap.stanford.edu.
Figure 1: Two dimensional Word Space
hand-generated stop list as the content-bearing di-
mension words (the columns of the matrix). The
20,000 most frequent content words were assigned
row values by counting occurrences within a 30-
word window. SVD was used to reduce the di-
mensionality from 1000 to 100, resulting in 100
dimensional ?meaning?-vectors for each word. In
our experiments, MWEs were assigned meaning-
vectors as a whole, using the same proceedure.
For meaning similarity we adopt the standard mea-
sure of cosine of the angle between two vectors
(the normalized correlation coefficient) as a met-
ric (Schu?tze, 1998; Baeza-Yates and Ribeiro-Neto,
1999). On this metric, two expressions are taken
to be unrelated if their meaning vectors are orthog-
onal (the cosine is 0) and synonymous if their vec-
tors are parallel (the cosine is 1).
Figure 1 illustrates such a vector space in two
dimensions. Note that the meaning vector for
Lo?ffel ?spoon? is quite similar to that for es-
sen ?to eat? but distant from sterben ?to die?,
while the meaning vector for the MWE den Lo?ffel
abgeben is close to that for sterben. Indeed den
Lo?ffel abgeben, like to kick the bucket, is a non-
compositional idiom meaning ?to die?.
While den Lo?ffel abgeben is used almost ex-
clusively in its idiomatic sense (all four occur-
rences in our corpus), many MWEs are used reg-
ularly in both their idiomatic and in their literal
senses. About two thirds of the uses of the MWE
ins Wasser fallen in our corpus are idiomatic uses,
and the remaing one third are literal uses. In
our first experiment we tested the hypothesis that
these uses could reliably be distinguished using
distribution-based models of their meaning.
14
3.1 Experiment I
For this experiment we manually annotated the
67 occurrences of ins Wasser fallen in our cor-
pus as to whether the expression was used com-
positionally (literally) or non-compositionally (id-
iomatically).4 Marking this distinction we gen-
erate an LSA meaning vectors for the composi-
tional uses and an LSA meaning vector for the
non-compositional uses of ins Wasser fallen. The
vectors turned out, as expected, to be almost or-
thogonal, with a cosine of the angle between them
of 0.02. This result confirms that the linguis-
tic contexts in which the literal and the idiomatic
use of ins Wasser fallen appear are very differ-
ent, indicating?not surprisingly?that the seman-
tic difference between the literal meaning and the
idiomatic meaning is reflected in the way these
these phrases are used.
Our next task was to investigate whether this
difference could be used in particular cases to de-
termine what the intended use of an MWE in a
particular context was. To evaluate this, we did a
10-fold cross-validation study, calculating the lit-
eral and idiomatic vectors for ins Wasser fallen on
the basis of the training data and doing a simple
nearest neighbor classification of each memember
of the test set on the basis of the meaning vectors
computed from its local context (the 30 word win-
dow). Our result of an average accurace of 72%
for our LSA-based classifier far exceeds the sim-
ple maximum-likelihood baseline of 58%.
In the final part of this experiment we compared
the meaning vector that was computed by sum-
ming over all uses of ins Wasser fallen with the
literal and idiomatic vectors from above. Since id-
iomatic uses of ins Wasser fallen prevail in the cor-
pus (2/3 vs. 1/3), it is not surprisingly that the sim-
ilarity to the literal vector (0.0946) is much than
similarity to the idiomatic vector (0.3712).
To summarize Experiment I, which is a vari-
ant of a supervised phrase sense disambiguation
task, demonstrates that we can use LSA to distin-
guish between literal and the idiomatic usage of an
MWE by using local linguistic context.
4This was a straightforward task; two annotators anno-
tated independently, with very high agreement?kappa score
of over 0.95 (Carletta, 1996). Occurrences on which the an-
notators disagreed were thrown out. Of the 64 occurrences
we used, 37 were idiomatic and 27 were literal.
3.2 Experiment II
In our second experiment we sought to make
use of the fact that there are typically clear
distributional difference between compositional
and non-compositional uses of MWEs to deter-
mine whether a given MWE indeed has non-
compositional uses at all. In this experi-
ment we made use of a test set of German
Preposition-Noun-Verb ?collocation candidate?
database whose extraction is described by Krenn
(2000) and which has been made available elec-
tronically.5 From this database only word com-
binations with frequency of occurrence more than
30 in our test corpus were considered. Our task
was to classify these 81 potential MWEs accord-
ing whether or not thay have an idiomatic mean-
ing.
To accomplish this task we took the following
approach. We computed on the basis of the dis-
tribution of the components of the MWE an esti-
mate for the compositional meaning vector for the
MWE. We then compared this to the actual vec-
tor for the MWE as a whole, with the expecta-
tion MWEs which indeed have non-compositinoal
uses will be distinguished by a relatively low vec-
tor similarity between the estimated compositional
meaning vector and the actual meaning vector.
In other words small similarity values should be
diagnostic for the presense of non-compositinoal
uses of the MWE.
We calculated the estimated compositional
meaning vector by taking it to be the sum of the
meaning vector of the parts, i.e., the compositional
meaning of an expression w1w2 consisting of two
words is taken to be sum of the meaning vectors
for the constituent words.6 In order to maximize
the independent contribution of the constituent
words, the meaning vectors for these words were
always computed from contexts in which they ap-
pear alone (that is, not in the local context of the
other constituent). We call the estimated composi-
tional meaning vector the ?composed? vector.7
The comparisons we made are illustrated in Fig-
ure 2, where vectors for the MWE auf die Strecke
bleiben ?to fall by the wayside? and the words
Strecke ?route? and bleiben ?to stay? are mapped
5Available as an example data collection in UCS-Toolkit
5 from www.collocations.de.
6For all our experiments we consider only two-word com-
binations.
7Schone & Jurafsky (2001) explore a few modest varia-
tions of this estimate.
15
Figure 2: Composed versus Multi-Word
into two dimensions8. (the words Autobahn ?high-
way? and eigensta?ndig ?independent? are given for
comparison). Here we see that the linear com-
bination of the component words of the MWE is
clearly distinct from that of the MWE as a whole.
As a further illustration of the difference be-
tween the composed vector and the MWE vector,
in Table 2 we list the words whose meaning vector
is most similar to that of the MWE auf dis Strecke
bleiben along with their similarity values, and in
Table 3 we list those words whose meaning vec-
tor is most similar to the composed vector. The
semantic differences among these two classes are
readily apparent.
folgerung ?consequence? 0.769663
eigensta?ndig ?independent? 0.732372
langfristiger ?long-term? 0.731411
herbeifu?hren ?to effect? 0.717294
ausnahmefa?lle ?exceptions? 0.704939
Table 1: auf die Strecke bleiben
strecken ?to lengthen? 0.743309
fahren ?to drive? 0.741059
laufen ?to run? 0.726631
fahrt ?drives? 0.712352
schlie?en ?to close? 0.704364
Table 2: Strecke+bleiben
We recognize that the composed vector is
clearly nowhere near a perfect model of compo-
sitional meaning in the general case. This can be
illustrated by considering, for example, the MWE
fire breathing. This expression is clearly com-
positional, as it denotes the process of producing
8The preposition auf and the article die are on the stop list
combusting exhalation, exactly what the seman-
tic combination rules of the English would pre-
dict. Nevertheless the distribution of fire breath-
ing is quite unrelated to that of its constituents
fire and breathing ( the former appears frequently
with dragon and circus while the later appear fre-
quently with blaze and lungs, respectively). De-
spite these principled objections, the composed
vector provides a useful baseline for our investiga-
tion. We should note that a number of researchers
in the LSA tradition have attempted to provide
more compelling combinatory functions to cap-
ture the non-linearity of linguistic compositional
interpretation (Kintsch, 2001; Widdows and Pe-
ters, 2003).
As a check we chose, at random, a number of
simple clearly-compositional word combinations
(not from the candidate MWE list). We expected
that on the whole these would evidence a very high
similarity measure when compared with their as-
sociated composed vector, and this is indeed the
case, as shown in Table 1. We also compared
vor Gericht verantworten 0.80735103
?to appear in court?
im Bett liegen 0.76056000
?to lie in bed?
aus Gefa?ngnis entlassen 0.66532673
?dismiss from prison?
Table 3: Non-idiomatic phrases
the literal and non-literal vectors for ins Wasser
fallen from the first experiment with the composed
vector, computed out of the meaning vectors for
Wasser and for fallen.9 The difference isn?t large,
but nevertheless the composed vector is more sim-
ilar to the literal vector (cosine of 0.2937) than to
the non-literal vector (cosine of 0.1733).
Extending to the general case, our task was to
compare the composed vector to the actual vec-
tor for all the MWEs in our test set. The result-
ing cosine similarity values range from 0.01 to
0.80. Our hope was that there would be a similar-
ity threshold for distinguishing MWEs that have
non-compositional interpretations from those that
do not. Indeed of the MWEs with a similarity val-
ues of under 0.1, just over half are MWEs which
were hand-annotated to have non-literal uses.10 It
9The preposition ins is on the stop list and plays no role
in the computation.
10The similarity scores for the entire test set are given in
16
is clear then that the technique described is, prima
facie, capable of detecting idiomatic MWEs.
3.3 Evaluation and Discussion
To evaluate the method, we used the careful man-
ual annotation of the PNV database described by
Krenn (2000) as our gold standard. By adopt-
ing different threshholds for the classification de-
cision, we obtained a range of results (trading off
precision and recall). Table 4 illustrates this range.
The F-score measure is maximized in our ex-
periments by adopting a similarity threshold of
0.2. This means that MWEs which have a mean-
ing vector whose cosine is under this value when
compared with with the combined vector should
be classified as having a non-literal meaning.
To compare our method with that proposed by
Baldwin et al (2003), we applied their method
to our materials, generating LSA vectors for the
component content words in our candidate MWEs
and comparing their semantic similarity to the
MWEs LSA vector as a whole, with the expecta-
tion being that low similarity between the MWE as
a whole and its component words is indication of
the non-compositionality of the MWE. The results
are given in Table 5.
It is clear that while Baldwin et al?s expectation
is borne out in the case of the constituent noun
(the non-head), it is not in the case of the con-
stituent verb (the head). Even in the case of the
nouns, however, the results are, for the most part,
markedly inferior to the results we achieved using
the composed vectors.
There are a number of issues that complicate
the workability of the unsupervised technique de-
scribed here. We rely on there being enough
non-compositional uses of an idiomatic MWE in
the corpus that the overall meaning vector for the
MWE reflects this usage. If the literal meaning
is overwhelmingly frequent, this will reduce the
effectivity of the method significantly. A second
problem concerns the relationship between the lit-
eral and the non-literal meaning. Our technique
relies on these meaning being highly distinct. If
the meanings are similar, it is likely that local con-
text will be inadequate to distinguish a composi-
tional from a non-compositional use of the expres-
sion. In our investigation it became apparent, in
fact, that in the newspaper genre, highly idiomatic
expressions such as ins Wasser fallen were often
Appendix I.
used in their idiomatic sense (apparently for hu-
morous effect) particularly frequently in contexts
in which elements of the literal meaning were also
present.11
4 Conclusion
To summarize, in order to classify an MWE as
non-compositional, we compute an approximation
of its compositional meaning and compare this
with the meaning of the expression as it is used
on the whole. One of the obvious improvements
to the algorithm could come from better mod-
els for simulating compositional meaning. A fur-
ther issue that can be explored is whether linguis-
tic preprocessing would influence the results. We
worked only on raw text data. There is some ev-
idence (Baldwin et al, 2003) that part of speech
tagging might improve results in this kind of task.
We also only considered local word sequences.
Certainly some recognition of the syntactic struc-
ture would improve results. These are, however,
more general issues associated with MWE pro-
cessing.
Rather promising results were attained using
only local context, however. Our study shows
that the F-score measure is maximized by taking
as threshold for distinguishing non-compositional
phrases from compositional ones a cosine simi-
larity value somewhere between 0.1-0.2. An im-
portant point to be explored is that compositional-
ity appears to come in degrees. As Bannard and
Lascarides (2003) have noted, MWEs ?do not fall
cleanly into the binary classes of compositional
and non-compositional expressions, but populate
a continuum between the two extremes.? While
our experiment was designed to classify MWEs,
the technique described here, of course, provides
a means, if rather a blunt one, for quantifying the
degreee of compositonality of an expression.
References
Ricardo A. Baeza-Yates and Berthier A. Ribeiro-Neto.
1999. Modern Information Retrieval. ACM Press /
Addison-Wesley.
Timothy Baldwin, Colin Bannard, Takaaki Tanaka, and
Dominic Widdows. 2003. An empirical model
11One such example from the SZ corpus:
Der Auftakt wa?re allerdings fast ins Wasser gefallen, weil ein
geplatzter Hydrant eine fu?nfzehn Meter hohe Wasserfonta?ne
in die Luft schleuderte.
?The prelude almost didn?t occur, because a burst hydrant
shot a fifteen-meter high fountain into the sky.?
17
cos < 0.1 cos < 0.2 cos < 0.3 cos < 0.4 cos < 0.5
Precision 0.53 0.39 0.29 0.22 0.21
Recall 0.42 0.63 0.84 0.89 0.95
F-measure 0.47 0.48 0.43 0.35 0.34
Table 4: Evaluation of Various Similarity Thresholds
cos < 0.1 cos < 0.2 cos < 0.3 cos < 0.4 cos < 0.5
Verb F-measure 0.21 0.16 0.29 0.26 0.27
Noun F-measure 0.28 0.51 0.43 0.39 0.33
Table 5: Evaluation of Method of Baldwin et al (2003)
of multiword expression decomposability. In Pro-
ceedings of the ACL-2003 Workshop on Multiword
Expressions: Analysis, Acquisition and Treatment,
pages 89?96, Sapporo, Japan.
Colin Bannard, Timothy Baldwin, and Alex Las-
carides. 2003. A statistical approach to the seman-
tics of verb-particles. In Proceedings of the ACL-
2003 Workshop on Multiword Expressions: Analy-
sis, Acquisition and Treatment, pages 65?72, Sap-
poro, Japan.
Michael W. Berry, Zlatko Drmavc, and Elisabeth R.
Jessup. 1999. Matrices, vector spaces, and infor-
mation retrieval. SIAM Review, 41(2):335?362.
Jean Carletta. 1996. Assessing agreement on classi-
fication tasks: The kappa statistic. Computational
Linguistics, 22(2):249?254.
Scott Cederberg and Dominic Widdows. 2003. Using
LSA and noun coordination information to improve
the precision and recall of automatic hyponymy ex-
traction. In In Seventh Conference on Computa-
tional Natural Language Learning, pages 111?118,
Edmonton, Canada, June.
Scott C. Deerwester, Susan T. Dumais, Thomas K. Lan-
dauer, George W. Furnas, and Richard A. Harshman.
1990. Indexing by latent semantic analysis. Jour-
nal of the American Society of Information Science,
41(6):391?407.
Stefan Evert and Hannah Kermes. 2003. Experi-
ments on candidate data for collocation extraction.
In Companion Volume to the Proceedings of the 10th
Conference of The European Chapter of the Associ-
ation for Computational Linguistics, pages 83?86,
Budapest, Hungary.
Stefan Evert and Brigitte Krenn. 2001. Methods for
the qualitative evaluation of lexical association mea-
sures. In Proceedings of the 39th Annual Meeting
of the Association for Computational Linguistics,
pages 188?195, Toulouse, France.
Stefan Evert. 2004. The Statistics of Word Cooccur-
rences: Word Pairs and Collocations. Ph.D. thesis,
University of Stuttgart.
Christiane Fellbaum. 1998. WordNet, an electronic
lexical database. MIT Press, Cambridge, MA.
Nancy Ide and Jean Ve?ronis. 1998. Word sense dis-
ambiguation: The state of the art. Computational
Linguistics, 14(1).
Walter Kintsch. 2001. Predication. Cognitive Science,
25(2):173?202.
Brigitte Krenn. 2000. The Usual Suspects: Data-
Oriented Models for Identification and Representa-
tion of Lexical Collocations. Dissertations in Com-
putational Linguistics and Language Technology.
German Research Center for Artificial Intelligence
and Saarland University, Saarbru?cken, Germany.
Thomas K. Landauer and Susan T. Dumais. 1997.
A solution to plato?s problem: The latent seman-
tic analysis theory of the acquisition, induction, and
representation of knowledge. Psychological Review,
104:211?240.
Thomas K. Landauer and Joseph Psotka. 2000. Sim-
ulating text understanding for educational applica-
tions with latent semantic analysis: Introduction to
LSA. Interactive Learning Environments, 8(2):73?
86.
Dekang Lin. 1999. Automatic identification of non-
compositional phrases. In Proceedings of the 37th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 317?324, College Park,
MD.
Christopher D. Manning and Hinrich Schu?tze. 1999.
Foundations of Statistical NaturalLanguage Pro-
cessing. The MIT Press, Cambridge, MA.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann A.
Copestake, and Dan Flickinger. 2002. Multiword
expressions: A pain in the neck for NLP. In Pro-
ceedings of the 3rd International Conferences on
Intelligent Text Processing and Computational Lin-
guistics, pages 1?15.
Patrick Schone and Daniel Jurafsky. 2001. Is
knowledge-free induction of multiword unit dictio-
nary headwords a solved problem? In Proceedings
18
of Empirical Methods in Natural Language Process-
ing, Pittsburgh, PA.
Hinrich Schu?tze. 1998. Automatic word sense dis-
crimination. Computational Linguistics, 24(1):97?
124.
Begon?a Villada Moiro?n and Jo?rg Tiedemann. 2006.
Identifying idiomatic expressions using automatic
word-alignment. In Proceedings of the EACL 2006
Workshop on Multiword Expressions in a Multilin-
gual Context, Trento, Italy.
Dominic Widdows and Stanley Peters. 2003. Word
vectors and quantum logic: Experiments with nega-
tion and disjunction. In Eighth Mathematics of Lan-
guage Conference, pages 141?150, Bloomington,
Indiana.
Chengxiang Zhai. 1997. Exploiting context to iden-
tify lexical atoms ? a statistical view of linguistic
context. In Proceedings of the International and In-
terdisciplinary Conference on Modelling and Using
Context (CONTEXT-97), pages 119?129.
APPENDIX
Similarity (cosine) values for the combined and
the MWE vector. Uppercase entries are those
hand-annotated as being MWEs which have an id-
iomatic interpretation.
Word Combinations Cosines
(vor) gericht verantworten 0.80735103
(in) bett liegen 0.76056000
(aus) gefa?ngnis entlassen 0.66532673
(zu) verfu?ung stellen 0.60310321
(aus) haft entlassen 0.59105617
(um) prozent steigern 0.55889772
(ZU) KASSE BITTEN 0.526331
(auf) prozent sinken 0.51281725
(IN) TASCHE GREIFEN 0.49350031
(zu) verfu?gung stehen 0.49236563
(auf) prozent steigen 0.47422122
(um) prozent zulegen 0.47329672
(in) betrieb gehen 0.47262171
(unter) druck geraten 0.44377297
(in) deutschland leben 0.44226071
(um) prozent steigen 0.41498688
(in) rechnung stellen 0.40985534
(von) prozent erreichen 0.39407666
(auf) markt kommen 0.38740534
(unter) druck setzen 0.37822936
(in) vergessenheit geraten 0.36654168
(um) prozent sinken 0.36600216
(in) rente gehen 0.36272313
(zu) einsatz kommen 0.3562527
(zu) schule gehen 0.35595884
(in) frage stellen 0.35406327
(in) frage kommen 0.34714701
(in) luft sprengen 0.34241143
(ZU) GESICHT BEKOMMEN 0.34160325
(vor) gericht ziehen 0.33405685
(in) gang setzen 0.33231573
(in) anspruch nehmen 0.32217044
(auf) prozent erho?hen 0.31574088
(um) prozent wachsen 0.3151615
(in) empfang nehmen 0.31420746
(fu?r) sicherheit sorgen 0.30230156
(zu) ausdruck bringen 0.30001438
(IM) MITTELPUNKT STEHEN 0.29770654
(zu) ruhe kommen 0.29753093
(IM) AUGE BEHALTEN 0.2969367
(in) urlaub fahren 0.29627064
(in) kauf nehmen 0.2947628
(in) pflicht nehmen 0.29470704
(in) ho?he treiben 0.29450525
(in) kraft treten 0.29311349
(zu) kenntnis nehmen 0.28969961
(an) start gehen 0.28315812
(auf) markt bringen 0.2800427
(in) ruhe standgehen 0.27575604
(bei) prozent liegen 0.27287073
(um) prozent senken 0.26506203
(UNTER) LUPE NEHMEN 0.2607078
(zu) zug kommen 0.25663165
(zu) ende bringen 0.25210009
(in) brand geraten 0.24819525
( ?UBER) B ?UHNE GEHEN 0.24644366
(um) prozent erho?hen 0.24058016
(auf) tisch legen 0.23264335
(auf) bu?hne stehen 0.23136641
(auf) idee kommen 0.23097735
(zu) ende gehen 0.20237252
(auf) spiel setzen 0.20112171
(IM) VORDERGRUND STEHEN 0.18957473
(IN) LEERE LAUFEN 0.18390151
(zu) opfer fallen 0.17724105
(in) gefahr geraten 0.17454816
(in) angriff nehmen 0.1643926
(auer) kontrolle geraten 0.16212899
(IN) HAND NEHMEN 0.15916243
(in) szene setzen 0.15766861
(ZU) SEITE STEHEN 0.14135151
(zu) geltung kommen 0.13119923
(in) geschichte eingehen 0.12458956
(aus) ruhe bringen 0.10973377
(zu) fall bringen 0.10900036
(zu) wehr setzen 0.10652383
(in) griff bekommen 0.10359659
(auf) tisch liegen 0.10011075
(IN) LICHTER SCHEINEN 0.08507655
(zu) sprache kommen 0.08503791
(IM) STICH LASSEN 0.0735844
(unter) beweis stellen 0.06064519
(IM) WEG STEHEN 0.05174435
(AUS) FUGEN GERATEN 0.05103952
(in) erinnerung bleiben 0.04339438
(ZU) WORT KOMMEN 0.03808749
(AUF) STRA?E GEHEN 0.03492515
(AUF) STRECKE BLEIBEN 0.03463844
(auer) kraft setzen 0.0338813
(AUF) WEG BRINGEN 0.03122951
(zu) erfolg fu?hren 0.02882997
(in) sicherheit bringen 0.02862914
(in) erfu?hlung gehen 0.01515792
(in) zeitung lesen 0.00354598
19
Proceedings of the NAACL HLT 2010 Student Research Workshop, pages 23?28,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Towards a Matrix-based Distributional Model of Meaning
Eugenie Giesbrecht
FZI Forschungszentrum Informatik
at the University of Karlsruhe
Haid-und-Neu-Str. 10-14, Karlsruhe, Germany
giesbrecht@fzi.de
Abstract
Vector-based distributional models
of semantics have proven useful
and adequate in a variety of natural
language processing tasks. How-
ever, most of them lack at least
one key requirement in order to
serve as an adequate representa-
tion of natural language, namely
sensitivity to structural information
such as word order. We propose a
novel approach that offers a poten-
tial of integrating order-dependent
word contexts in a completely un-
supervised manner by assigning to
words characteristic distributional
matrices. The proposed model is
applied to the task of free associa-
tions. In the end, the first results as
well as directions for future work
are discussed.
1 Introduction
In natural language processing as well as in informa-
tion retrieval, Vector Space Model (VSM) (Salton et
al., 1975) and Word Space Model (WSM) (Schu?tze,
1993; Lund and Burgess, 1996) have become the
mainstream for text representation. VSMs embody
the distributional hypothesis of meaning, the main
assumption of which is that a word is known ?by
the company it keeps? (Firth, 1957). VSMs proved
to perform well in a number of cognitive tasks such
as synonymy identification (Landauer and Dumais,
1997), automatic thesaurus construction (Grefen-
stette, 1994) and many others. However, it has been
long recognized that these models are too weak to
represent natural language to a satisfactory extent.
With VSMs, the assumption is made that word co-
occurrence is essentially independent of word order.
All the co-occurrence information is thus fed into
one vector per word.
Suppose our ?background knowledge? corpus
consists of one sentence: Peter kicked the ball. It
follows that the distributional meanings of both PE-
TER and BALL would be in a similar way defined by
the co-occurring KICK which is insufficient, as BALL
can be only kicked by somebody but not kick itself;
in case of PETER, both ways of interpretation should
be possible. To overcome the aforementioned prob-
lems with vector-based models, we suggest a novel
distributional paradigm for representing text in that
we introduce a further dimension into a ?standard?
two-dimensional word space model. That allows us
to count correlations for three words at a time. In
short, given a vocabulary V , context width w = m
and tokens t1, t2, t3, ..., ti ? V , for token ti a matrix
of size V ? V is generated that has nonzero values
in cells where ti appears between ti?m and ti+m.
Note that this 3-dimensional representation al-
lows us to integrate word order information into the
model in a completely unsupervised manner as well
as to achieve a richer word representation as a matrix
instead of a vector.
The remainder of the paper is organized as fol-
lows. After a recap of basic mathematical no-
tions and operations used in the model in Section 2,
we introduce the proposed three-dimensional tensor-
based model of text representation in Section 3. First
evaluation experiments are reported in Section 4.
23
After a brief overview of related work in Section 5,
we provide some concluding remarks and sugges-
tions for future work in Section 6.
2 Preliminaries
In this section, we provide a brief introduction to
tensors and the basics of mathematical operations
that are employed in the suggested model.
First, given d natural numbers n1, . . . , nd, a (real)
n1? . . .?nd tensor can be defined as a function
T : {1, . . . , n1}? . . .?{1, . . . , nd} ? R, map-
ping d-tuples of natural numbers to real numbers.
Intuitively, a tensor can best be thought of as a d-
dimensional table (or array) carrying real numbers
as entries. Thereby n1, . . . , nd determine the exten-
sion of the array in the different directions. Obvi-
ously, matrices can be conceived as n1?n2-tensors
and vectors as n1-tensors.
In our setting, we will work with tensors where
d = 3 and for the sake of better understandability
we will introduce the necessary notions for this case
only.
Our work employs higher-order singular value
decomposition (HOSVD), which generalizes the
method of singular value decomposition (SVD)
from matrices to arbitrary tensors.
Given an n1?n2?n3 tensor T , its Tucker decom-
position (Tucker, 1966) for given natural numbers
m1, m2, m3 consists of an m1?m2?m3 tensor G
and three matrices A,B, and C of formats n1?m1,
n2?m2, and n3?m3, respectively, such that
T (i, j, k) =
m1?
r=1
m2?
s=1
m3?
t=1
G(r, s, t)?A(i, r)?B(j, s)?C(k, t).
The idea here is to represent the large-size ten-
sor T by the smaller ?core? tensor G. The matrices
A, B, and C can be seen as linear transformations
?compressing? input vectors from dimension ni into
dimension mi. Note that a precise representation of
T is not always possible. Rather one may attempt
to approximate T as well as possible, i.e. find the
tensor T ? for which a Tucker decomposition exists
and which has the least distance to T . Thereby, the
notion of distance is captured by ?T ? T ??, where
T ? T ? is the tensor obtained by entry-wise subtrac-
tion and ? ? ? is the Frobenius norm defined by
?M? =
?
?
?
?
n1?
r=1
n2?
s=1
n3?
t=1
(M(r, s, t))2.
In fact, the described way of approximating a ten-
sor is called dimensionality reduction and is often
used for reducing noise in multi-dimensional data.
3 Proposed Model
Our motivation is to integrate structure into the ge-
ometrical representation of text meaning while ad-
hering to the ideas of distributional semantics. For
this, we introduce a third dimension that allows us
to separate the left and right contexts of the words.
As we process text, we accumulate the left and right
word co-occurrences to represent the meaning of the
current word. Formally, given a corpus K, a list L
of tokens, and a context width w, we define its ten-
sor representation TK by letting TK(i, j, k) be the
number of occurrences of L(j) s L(i) s? L(k) in
sentences in K where s, s? are (possibly empty) se-
quences of at most w ? 1 tokens. For example, sup-
pose our corpus consists of three sentences: ?Paul
kicked the ball slowly. Peter kicked the ball slowly.
Paul kicked Peter.? We let w = 1, presuming prior
stop words removal. We obtain a 5 ? 5 ? 5 tensor.
Table 1 displays two i-slices of the resulting tensor
T showing left vs. right context dependencies.
KICK PETER PAUL KICK BALL SLOWLY
PETER 0 0 0 1 0
PAUL 1 0 0 1 0
KICK 0 0 0 0 0
BALL 0 0 0 0 0
SLOWLY 0 0 0 0 0
BALL PETER PAUL KICK BALL SLOWLY
PETER 0 0 0 0 0
PAUL 0 0 0 0 0
KICK 0 0 0 0 2
BALL 0 0 0 0 0
SLOWLY 0 0 0 0 0
Table 1: Slices of T for the terms KICK (i = 3) and BALL
(i = 4).
Similarly to traditional vector-based distributional
models, dimensionality reduction needs to be per-
formed in three dimensions either, as the resulting
tensor is very sparse (see the examples of KICK and
24
BALL). To this end, we employ Tucker decompo-
sition for 3 dimensions as introduced in Section 2.
For this, Matlab Tensor Toolbox1 (Bader and Kolda,
2006) is used.
A detailed overview of computational complexity
of Tucker decomposition algorithms in Tensor Tool-
box is provided in Turney (2007). The drawback of
those is that their complexity is cubic in the number
of factorization dimensions and unfeasible for large
datasets. However, new memory efficient tensor de-
composition algorithms have been proposed in the
meantime. Thus, Memory Efficient Tucker (MET)
is available in Matlab Tensor Toolbox since Version
2.3. Rendle and Schmidt-Thieme (2010) present a
new factorization method with linear complexity.
4 Evaluation Issues
4.1 Task
Vector-based distributional similarity methods have
proven to be a valuable tool for a number of tasks
on automatic discovery of semantic relatedness be-
tween words, like synonymy tests (Rapp, 2003) or
detection of analogical similarity (Turney, 2006).
A somewhat related task is the task of finding out
to what extent (statistical) similarity measures cor-
relate with free word associations2. Furthermore,
this task was suggested as a shared task for the eval-
uation of word space models at Lexical Semantics
Workshop at ESSLLI 2008. Free associations are
the words that come to the mind of a native speaker
when he or she is presented with a so-called stimu-
lus word. The percent of test subjects that produce
certain response to a given stimulus determines the
degree of a free association between a stimulus and
a response.
Despite the widespread usage of vector-based
models to retrieve semantically similar words, it is
still rather unclear what type of linguistic phenom-
ena they model (cf. Heylen et al (2008), Wand-
macher et al (2008)). The same is true for free as-
sociations. There are a number of relations accord-
ing to which a word may be associated with another
1Version 2.3
2One of the reasons to choose this evaluation setting was that
the dataset for free word associations task is freely available at
http://wordspace.collocations.de/doku.php/data:esslli2008:start
(in contrast to, e.g., the synonymy test set).
word. For example, Aitchison (2003) distinguishes
four types of associations: co-ordination, colloca-
tion, superordination and synonymy. This affords
an opportunity to use the task of free associations as
a ?baseline? for distributional similarity.
For this task, workshop organizers have proposed
three subtasks, one of which - discrimination - we
adapt in this paper. Test sets have been provided
by the workshop organizers. The former are based
on the Edinburgh Associative Thesaurus3 (EAT),
a freely available database of English association
norms.
Discrimination task includes a test set of over-
all 300 word pairs that were classified according to
three classes of association strengths:
? FIRST strongly associated word pairs as indi-
cated by more than 50% of test subjects as first
responses;
? HAPAX word associations that were produced
by a single test subject;
? RANDOM random combinations of words from
EAT that were never produced as a stimulus -
response pair.
4.2 Procedure
To collect the three-way co-occurrence information,
we experiment with the UKWAC corpus (A. Fer-
raresi and Bernardini, 2008), as suggested by the
workshop organizers, in order to get comparable re-
sults. As UKWAC is a huge Web-derived corpus
consisting of about 2 billion tokens, it was impos-
sible at the current stage to process the whole cor-
pus. As the subsections of UKWAC contain ran-
domly chosen documents, one can train the model
on any of the subsections.
We limited out test set to the word pairs for which
the constituent words occur more than 50 times in
the test corpus. Thereby, we ended up with a test set
consisting of 222 word pairs.
We proceed in the following way. For each pair
of words:
1. Gather N sentences, i.e. contexts, for each of
the two words4, here N = 50;
3http://www.eat.rl.ac.uk/
4This corpus ?preprocessing? step was mainly due to lim-
25
2. Build a 3-dimensional tensor from the subcor-
pus obtained in (1), given a context width w=5,
i.e. 5 words to the left and 5 words to the right
of the target word), taking sentence boundaries
into consideration;
3. Reduce 5 times the dimensionality of the tensor
obtained in (2) by means of Tucker decomposi-
tion;
4. Extract two matrices of both constituents of the
word pair and compare those by means of co-
sine similarity5.
Here, we follow the tradition of vector-based
models where cosine is usually used to measure se-
mantic relatedness. One of the future direction in
matrix-based meaning representation is to investi-
gate further matrix comparison metrics.
4.3 Results and Discussion
Tables 2 and 3 show the resulting accuracies6 for
training and test sets. th denotes cosine threshold
values that were used for grouping the results. Here,
th is taken to be the function of the size s of the data
set. Thus, given a training set of size s = 60 and
3 classes, we define an ?equally distributed? thresh-
old th1 = 60/3 = 20 (s. Table 2) and a ?linearly
growing? threshold th2 = 14 ,
1
3 , rest (s. Table 3).
It is not quite apparent, how the threshold for
differentiating between the groups should be deter-
mined under given conditions. Usually, such mea-
sures are defined on the basis of training data (e.g.
Wandmacher et al (2008)). It was not applicable
in our case as, due to the current implementation of
the model as well as insufficient computational re-
sources for the time being, we could not build one
big model for all experiment iterations.
Also, the intuition we have gained with this kind
of thresholds is that as soon as you change the un-
derlying corpus or the model parameters, you may
need to define new thresholds (cf. Tables 2 and 3).
ited processing power we had at our disposal at the moment the
experiments were conducted. With this step, we considerably
reduced the size of the corpus and guaranteed a certain number
of contexts per relevant word.
5Cosine similarity is determined as a normalized inner prod-
uct
6Accuracy is defined in the following way: Accuracy =
right/(right + wrong)
Thresholds in geometric models of meaning can not
be just fixed, just as the measure of similarity cannot
be easily quantified by humans.
It would be straightforward to compare the perfor-
mance of the proposed model with its 2-dimensional
analogue. Wandmacher et al (2008) obtain in aver-
age better results with their LSA-based model for
this task. Specifically, they observe very good re-
sults for RANDOM associations (78.2% accuracy)
but the lowest results for the FIRST, i.e. strongest,
associations (50%). In constrast, the outcome for
RANDOM in our model is the worst. However, the
bigger the threshold, the more accurate is getting
the model for the FIRST associations. For exam-
ple, with a threshold of th = 0.2 for the test set
- 4 out of 5 highest ranked pairs were highly asso-
ciated (FIRST) and the fifth pair was from the HA-
PAX group. For HAPAX word associations, no simi-
lar regularities could be observed.
The resulting accuracies may seem to be poor at
this stage. However, it is worth mentioning that
this is a highly difficult and corpus-dependent task
for automatic processing. The reported results have
been obtained based on very small corpora, contain-
ing ca. 100 sentences per iteration (cf. Wandmacher
et al (2008) use a corpus of 108 million words to
train their LSA-Model). Consequently, it is not pos-
sible to compare both results directly, as they have
been produced under very different conditions.
5 Related Work
5.1 Matrix Approaches
There have been a number of efforts to integrate syn-
tax into vector-based models with alternating suc-
cess. Some used (dependency) parsing to feed the
models (Grefenstette, 1994; Lin, 1998; Pado? and La-
pata, 2007); the others utilized only part of speech
information, e.g., Widdows (2003).
In many cases, these syntactically enhanced mod-
els improved the performance (Grefenstette, 1994;
Lin, 1998; Pado? and Lapata, 2007). Sometimes,
however, rather controversial results were observed.
Thus, Widdows (2003) reported both positive and
negative effects for the task of developing tax-
onomies. On the one side, POS information in-
creased the performance for common nouns; on the
other side, it degraded the outcome for proper nouns
26
TRAIN TEST
FIRST 12/20 (60%) (th = 0.022) 25/74 (33%) (th = 0.078))
HAPAX 7/20 (35%) (th = 0.008) 35/74 (47%) th = 0.042)
RANDOM 8/20 (40%) 23/74 (31%)
TOTAL (F/H/R) 27/60 (45%) 83/222 (37.4%)
FIRST/HORR7 44/60 (73.33%) 125/222 (56.3%)
Table 2: Accuracies for the ?equally distributed? threshold for training and test sets
TRAIN TEST
FIRST 9/15 (60%) (th = 0.0309) 20/55 (36.4%) (th = 0.09)
HAPAX 8/20 (40%) (th = 0.0101) 39/74 (52.7%) (th = 0.047)
RANDOM 10/25 (40%) 24/93 (25.8%)
TOTAL (F/H/R) 27/60 (45%) 108/222 (48.6%)
FIRST/HORR8 43/60 (71.60%) 113/222 (50.9%)
Table 3: Accuracies for a ?linearly growing? threshold for training and test sets
and verbs.
Sahlgren et al (2008) incorporate word order in-
formation into context vectors in an unsupervised
manner by means of permutation.
Recently, Erk and Pado? (2008) proposed a struc-
tured vector space model where a word is repre-
sented by several vectors reflecting the words lexical
meaning as well as its selectional preferences. The
motivation behind their work is very close to ours,
namely, that single vectors are too weak to represent
word meaning. However, we argue that a matrix-
based representation allows us to integrate contex-
tual information in a more general manner.
5.2 Tensor Approaches
Among the early attempts to apply higher-order ten-
sors instead of vectors to text data is the work of Liu
et al (2005) who show that Tensor Space Model is
consistently better than VSM for text classification.
Cai et al (2006) suggest a 3-dimensional represen-
tation for documents and evaluate the model on the
task of document clustering.
The above as well as a couple of other projects in
this area in information retrieval community leave
open the question of how to convey text into a three-
dimensional tensor. They still use vector-based rep-
resentation as the basis and then just mathematically
convert vectors into tensors, without linguistic justi-
fication of such transformations.
Further, there are few works that extend the term-
document matrix with metadata as a third dimension
(Chew et al, 2007; Sun et al, 2006).
Turney (2007) is one of the few to study the ap-
plication of tensors to word space models. However,
the emphasis in that paper is more on the evaluation
of different tensor decomposition models for such
spaces than on the formal model of text representa-
tion in three dimensions. Van de Cruys (2009) sug-
gests a three-way model of co-occurrence similar to
ours. In contrast to Van de Cruys (2009), we are
not using any explicit syntactic preprocessing. Fur-
thermore, our focus is more on the model itself as a
general model of meaning.
6 Summary and Future Work
In this paper, we propose a novel approach to text
representation inspired by the ideas of distributional
semantics. In particular, our model suggests a solu-
tion to the problem of integrating word order infor-
mation in vector spaces in an unsupervised manner.
First experiments on the task of free associations are
reported. However, we are not in the position yet to
commit ourselves to any representative statements.
A thorough evaluation of the model still needs to be
done. Next steps include, amongst others, evaluat-
ing the suggested model with a bigger data corpus as
well as using stemming and more sophisticated fill-
ing of word matrices, e.g., by introducing advanced
weighting schemes into the matrices instead of sim-
ple counts.
Furthermore, we started with evaluation on the
task which has been proposed for the evaluation of
27
word space models at the level of word meaning. We
need, however, to evaluate the model for the tasks
where word order information matters more, e.g. on
selectional preferences or paraphrasing.
Last but not least, we plan to address the issue of
modeling compositional meaning with matrix-based
distributional model of meaning.
Acknowledgments
This work is supported by German ?Federal Min-
istry of Economics? (BMWi) under the project The-
seus (number 01MQ07019). Many thanks to the
anonymous reviewers for their insightful comments.
References
M. Baroni A. Ferraresi, E. Zanchetta and S. Bernardini.
2008. Introducing and evaluating ukWaC, a very large
Web-derived corpus of English. In Proceedings of the
WAC4 Workshop at LREC?08.
Jean Aitchison. 2003. Words in the Mind: An Introduc-
tion to the Mental Lexicon. Wiley-Blackwell.
Brett W. Bader and Tamara G. Kolda. 2006. Algorithm
862: Matlab tensor classes for fast algorithm prototyp-
ing. ACM Trans. Math. Softw., 32(4):635?653.
Deng Cai, Xiaofei He, and Jiawei Han. 2006. Tensor
space model for document analysis. In SIGIR, pages
625?626. ACM.
Peter Chew, Brett Bader, Tamara Kolda, and Ahmed Ab-
delali. 2007. Cross-language information retrieval us-
ing PARAFAC2. In Proc. KDD?07, pages 143?152.
ACM.
Katrin Erk and Sebastian Pado?. 2008. A structured
vector space model for word meaning in context. In
EMNLP, pages 897?906. ACL.
J.R. Firth. 1957. A synopsis of linguistic theory 1930-
55. Studies in linguistic analysis, pages 1?32.
Gregory Grefenstette. 1994. Explorations in Automatic
Thesaurus Discovery. Springer.
Kris Heylen, Yves Peirsman, Dirk Geeraerts, and Dirk
Speelman. 2008. Modelling word similarity: an eval-
uation of automatic synonymy extraction algorithms.
In Proceedings of LREC?08, pages 3243?3249.
T. K. Landauer and S. T Dumais. 1997. Solution to
Plato?s Problem: The Latent Semantic Analysis The-
ory of Acquisition, Induction and Representation of
Knowledge. Psychological Review, 104(2):211?240.
Dekang Lin. 1998. Automatic retrieval and clustering of
similar words. In Proceedings of ACL?98, pages 768?
774. ACL.
Ning Liu, Benyu Zhang, Jun Yan, Zheng Chen, Wenyin
Liu, Fengshan Bai, and Leefeng Chien. 2005. Text
representation: from vector to tensor. In Proc.
ICDM05.
Kevin Lund and Curt Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, Instrumen-
tation, and Computers, pages 203?20.
Sebastian Pado? and Mirella Lapata. 2007. Dependency-
based construction of semantic space models. Compu-
tational Linguistics, 33(2):161?199.
Reinhard Rapp. 2003. Word sense discovery based on
sense descriptor dissimilarity. In Proceedings of the
Ninth Machine Translation Summit, pages 315?322.
Steffen Rendle and Lars Schmidt-Thieme. 2010. Pair-
wise interaction tensor factorization for personalized
tag recommendation. In WSDM ?10: Proceedings of
the third ACM international conference on Web search
and data mining, pages 81?90, New York, NY, USA.
ACM.
M. Sahlgren, A. Holst, and P. Kanerva. 2008. Permu-
tations as a means to encode order in word space. In
Proc. CogSci08, pages 1300?1305.
G. Salton, A. Wong, and C. S. Yang. 1975. A vector
space model for automatic indexing. Commun. ACM,
18(11):613?620.
Hinrich Schu?tze. 1993. Word space. In Advances in
NIPS 5, pages 895?902.
J. Sun, D. Tao, and C. Faloutsos. 2006. Beyond
streams and graphs: Dynamic tensor analysis. In Proc.
KDD?06, pages 374?383.
L.R. Tucker. 1966. Some mathematical notes on three-
mode factor analysis. Psychometrika, 31(3).
Peter D. Turney. 2006. Similarity of semantic relations.
Computational Linguistics, 32(3):379?416.
P. Turney. 2007. Empirical evaluation of four tensor de-
composition algorithms. Technical report. Technical
Report ERB-1152.
Tim Van de Cruys. 2009. A non-negative tensor factor-
ization model for selectional preference induction. In
GEMS ?09: Proceedings of the Workshop on Geomet-
rical Models of Natural Language Semantics, pages
83?90, Morristown, NJ, USA. ACL.
Tonio Wandmacher, Ekaterina Ovchinnikova, and
Theodore Alexandrov. 2008. Does Latent Semantic
Analysis reflect human associations. In Proceedings
of the Lexical Semantics workshop at ESSLLI, Ham-
burg, Germany.
Dominic Widdows. 2003. Unsupervised methods for de-
veloping taxonomies by combining syntactic and sta-
tistical information. In Proceedings of NAACL?03,
pages 197?204. ACL.
28
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 907?916,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Compositional Matrix-Space Models of Language
Sebastian Rudolph
Karlsruhe Institute of Technology
Karlsruhe, Germany
rudolph@kit.edu
Eugenie Giesbrecht
FZI Forschungszentrum Informatik
Karlsuhe, Germany
giesbrecht@fzi.de
Abstract
We propose CMSMs, a novel type of
generic compositional models for syntac-
tic and semantic aspects of natural lan-
guage, based on matrix multiplication. We
argue for the structural and cognitive plau-
sibility of this model and show that it is
able to cover and combine various com-
mon compositional NLP approaches rang-
ing from statistical word space models to
symbolic grammar formalisms.
1 Introduction
In computational linguistics and information re-
trieval, Vector Space Models (Salton et al, 1975)
and its variations ? such as Word Space Models
(Sch?tze, 1993), Hyperspace Analogue to Lan-
guage (Lund and Burgess, 1996), or Latent Se-
mantic Analysis (Deerwester et al, 1990) ? have
become a mainstream paradigm for text represen-
tation. Vector Space Models (VSMs) have been
empirically justified by results from cognitive sci-
ence (G?rdenfors, 2000). They embody the distri-
butional hypothesis of meaning (Firth, 1957), ac-
cording to which the meaning of words is defined
by contexts in which they (co-)occur. Depending
on the specific model employed, these contexts
can be either local (the co-occurring words), or
global (a sentence or a paragraph or the whole doc-
ument). Indeed, VSMs proved to perform well in a
number of tasks requiring computation of seman-
tic relatedness between words, such as synonymy
identification (Landauer and Dumais, 1997), auto-
matic thesaurus construction (Grefenstette, 1994),
semantic priming, and word sense disambiguation
(Pad? and Lapata, 2007).
Until recently, little attention has been paid
to the task of modeling more complex concep-
tual structures with such models, which consti-
tutes a crucial barrier for semantic vector models
on the way to model language (Widdows, 2008).
An emerging area of research receiving more and
more attention among the advocates of distribu-
tional models addresses the methods, algorithms,
and evaluation strategies for representing compo-
sitional aspects of language within a VSM frame-
work. This requires novel modeling paradigms,
as most VSMs have been predominantly used
for meaning representation of single words and
the key problem of common bag-of-words-based
VSMs is that word order information and thereby
the structure of the language is lost.
There are approaches under way to work out
a combined framework for meaning representa-
tion using both the advantages of symbolic and
distributional methods. Clark and Pulman (2007)
suggest a conceptual model which unites sym-
bolic and distributional representations by means
of traversing the parse tree of a sentence and ap-
plying a tensor product for combining vectors of
the meanings of words with the vectors of their
roles. The model is further elaborated by Clark et
al. (2008).
To overcome the aforementioned difficulties
with VSMs and work towards a tight integra-
tion of symbolic and distributional approaches,
we propose a Compositional Matrix-Space Model
(CMSM) which employs matrices instead of vec-
tors and makes use of matrix multiplication as the
one and only composition operation.
The paper is structured as follows: We start by
providing the necessary basic notions in linear al-
gebra in Section 2. In Section 3, we give a for-
mal account of the concept of compositionality,
introduce our model, and argue for the plausibil-
ity of CMSMs in the light of structural and cogni-
tive considerations. Section 4 shows how common
VSM approaches to compositionality can be cap-
tured by CMSMs while Section 5 illustrates the
capabilities of our model to likewise cover sym-
bolic approaches. In Section 6, we demonstrate
907
how several CMSMs can be combined into one
model. We provide an overview of related work
in Section 7 before we conclude and point out av-
enues for further research in Section 8.
2 Preliminaries
In this section, we recap some aspects of linear
algebra to the extent needed for our considerations
about CMSMs. For a more thorough treatise we
refer the reader to a linear algebra textbook (such
as Strang (1993)).
Vectors. Given a natural number n, an n-
dimensional vector v over the reals can be seen
as a list (or tuple) containing n real numbers
r1, . . . , rn ? R, written v = (r1 r2 ? ? ? rn).
Vectors will be denoted by lowercase bold font
letters and we will use the notation v(i) to refer
to the ith entry of vector v. As usual, we write
Rn to denote the set of all n-dimensional vectors
with real entries. Vectors can be added entry-
wise, i.e., (r1 ? ? ? rn) + (r?1 ? ? ? r
?
n) = (r1 +
r?1 ? ? ? rn +r
?
n). Likewise, the entry-wise prod-
uct (also known as Hadamard product) is defined
by (r1 ? ? ? rn)  (r?1 ? ? ? r
?
n) = (r1 ?r
?
1 ? ? ? rn ?r
?
n).
Matrices. Given two real numbers n, m, an n?m
matrix over the reals is an array of real numbers
with n rows and m columns. We will use capital
letters to denote matrices and, given a matrix M
we will write M(i, j) to refer to the entry in the ith
row and the jth column:
M =
?
??????????????????????????????????
M(1, 1) M(1, 2) ? ? ? M(1, j) ? ? ? M(1,m)
M(2, 1) M(2, 2)
...
...
...
M(i, 1) M(i, j)
...
...
...
M(n, 1) M(1, 2) ? ? ? ? ? ? ? ? ? M(n,m)
?
??????????????????????????????????
The set of all n ? m matrices with real num-
ber entries is denoted by Rn?m. Obviously, m-
dimensional vectors can be seen as 1 ? m matri-
ces. A matrix can be transposed by exchanging
columns and rows: given the n ? m matrix M, its
transposed version MT is a m ? n matrix defined
by MT (i, j) = M( j, i).
Linear Mappings. Beyond being merely array-
like data structures, matrices correspond to certain
type of functions, so-called linear mappings, hav-
ing vectors as in- and output. More precisely, an
n ? m matrix M applied to an m-dimensional vec-
tor v yields an n-dimensional vector v? (written:
vM = v?) according to
v?(i) =
m?
j=1
v( j) ? M(i, j)
Linear mappings can be concatenated, giving
rise to the notion of standard matrix multiplica-
tion: we write M1M2 to denote the matrix that
corresponds to the linear mapping defined by ap-
plying first M1 and then M2. Formally, the matrix
product of the n? l matrix M1 and the l?m matrix
M2 is an n ? m matrix M = M1M2 defined by
M(i, j) =
l?
k=1
M1(i, k) ? M2(k, j)
Note that the matrix product is associative (i.e.,
(M1M2)M3 = M1(M2M3) always holds, thus
parentheses can be omitted) but not commutative
(M1M2 = M2M1 does not hold in general, i.e., the
order matters).
Permutations. Given a natural number n, a per-
mutation on {1 . . . n} is a bijection (i.e., a map-
ping that is one-to-one and onto) ? : {1 . . . n} ?
{1 . . . n}. A permutation can be seen as a ?reorder-
ing scheme? on a list with n elements: the element
at position i will get the new position ?(i) in the
reordered list. Likewise, a permutation can be ap-
plied to a vector resulting in a rearrangement of
the entries. We write ?n to denote the permutation
corresponding to the n-fold application of ? and
??1 to denote the permutation that ?undoes? ?.
Given a permutation ?, the corresponding per-
mutation matrix M? is defined by
M?(i, j) =
{
1 if ?( j) = i,
0 otherwise.
Then, obviously permuting a vector according
to ? can be expressed in terms of matrix multipli-
cation as well as we obtain for any vector v ? Rn:
?(v) = vM?
Likewise, iterated application (?n) and the in-
verses ??n carry over naturally to the correspond-
ing notions in matrices.
908
3 Compositionality and Matrices
The underlying principle of compositional seman-
tics is that the meaning of a sentence (or a word
phrase) can be derived from the meaning of its
constituent tokens by applying a composition op-
eration. More formally, the underlying idea can
be described as follows: given a mapping [[ ? ]] :
? ? S from a set of tokens (words) ? into some
semantical space S (the elements of which we will
simply call ?meanings?), we find a semantic com-
position operation ./: S? ? S mapping sequences
of meanings to meanings such that the meaning of
a sequence of tokens ?1?2 . . . ?n can be obtained
by applying ./ to the sequence [[?1]][[?2]] . . . [[?n]].
This situation qualifies [[ ? ]] as a homomorphism
between (??, ?) and (S, ./) and can be displayed as
follows:
?1
[[ ? ]]

concatenation ?
''
?2
[[ ? ]]

((
? ? ? ?n
[[ ? ]]

))
?1?2 . . . ?n
[[ ? ]]

[[?1]]
composition ./
66
[[?2]] 55? ? ? [[?n]] 55[[?1?2 . . . ?n]]
A great variety of linguistic models are sub-
sumed by this general idea ranging from purely
symbolic approaches (like type systems and cate-
gorial grammars) to rather statistical models (like
vector space and word space models). At the first
glance, the underlying encodings of word seman-
tics as well as the composition operations differ
significantly. However, we argue that a great vari-
ety of them can be incorporated ? and even freely
inter-combined ? into a unified model where the
semantics of simple tokens and complex phrases
is expressed by matrices and the composition op-
eration is standard matrix multiplication.
More precisely, in Compositional Marix-Space
Models, we have S = Rn?n, i.e. the semantical
space consists of quadratic matrices, and the com-
position operator ./ coincides with matrix multi-
plication as introduced in Section 2. In the follow-
ing, we will provide diverse arguments illustrating
that CMSMs are intuitive and natural.
3.1 Algebraic Plausibility ?
Structural Operation Properties
Most linear-algebra-based operations that have
been proposed to model composition in language
models are associative and commutative. Thereby,
they realize a multiset (or bag-of-words) seman-
tics that makes them insensitive to structural dif-
ferences of phrases conveyed through word order.
While associativity seems somewhat acceptable
and could be defended by pointing to the stream-
like, sequential nature of language, commutativity
seems way less justifiable, arguably.
As mentioned before, matrix multiplication is
associative but non-commutative, whence we pro-
pose it as more adequate for modeling composi-
tional semantics of language.
3.2 Neurological Plausibility ?
Progression of Mental States
From a very abstract and simplified perspective,
CMSMs can also be justified neurologically.
Suppose the mental state of a person at one spe-
cific moment in time can be encoded by a vector v
of numerical values; one might, e.g., think of the
level of excitation of neurons. Then, an external
stimulus or signal, such as a perceived word, will
result in a change of the mental state. Thus, the
external stimulus can be seen as a function being
applied to v yielding as result the vector v? that
corresponds to the persons mental state after re-
ceiving the signal. Therefore, it seems sensible to
associate with every signal (in our case: token ?) a
respective function (a linear mapping, represented
by a matrix M = [[?]] that maps mental states to
mental states (i.e. vectors v to vectors v? = vM).
Consequently, the subsequent reception of in-
puts ?, ?? associated to matrices M and M?
will transform a mental vector v into the vector
(vM)M? which by associativity equals v(MM?).
Therefore, MM? represents the mental state tran-
sition triggered by the signal sequence ???. Nat-
urally, this consideration carries over to sequences
of arbitrary length. This way, abstracting from
specific initial mental state vectors, our semantic
space S can be seen as a function space of mental
transformations represented by matrices, whereby
matrix multiplication realizes subsequent execu-
tion of those transformations triggered by the in-
put token sequence.
909
3.3 Psychological Plausibility ?
Operations on Working Memory
A structurally very similar argument can be pro-
vided on another cognitive explanatory level.
There have been extensive studies about human
language processing justifying the hypothesis of
a working memory (Baddeley, 2003). The men-
tal state vector can be seen as representation of a
person?s working memory which gets transformed
by external input. Note that matrices can per-
form standard memory operations such as storing,
deleting, copying etc. For instance, the matrix
Mcopy(k,l) defined by
Mcopy(k,l)(i, j) =
{
1 if i = j , l or i = k, j = l,
0 otherwise.
applied to a vector v, will copy its kth entry to the
lth position. This mechanism of storage and inser-
tion can, e.g., be used to simulate simple forms of
anaphora resolution.
4 CMSMs Encode Vector Space Models
In VSMs numerous vector operations have been
used to model composition (Widdows, 2008),
some of the more advanced ones being related to
quantum mechanics. We show how these com-
mon composition operators can be modeled by
CMSMs.1 Given a vector composition operation
./: Rn?Rn ? Rn, we provide a surjective function
?./ : Rn ? Rn
??n? that translates the vector rep-
resentation into a matrix representation in a way
such that for all v1, . . . vk ? Rn holds
v1 ./ . . . ./ vk = ??1./ (?./(v1) . . . ?./(vk))
where ?./(vi)?./(v j) denotes matrix multiplication
of the matrices assigned to vi and v j.
4.1 Vector Addition
As a simple basic model for semantic composi-
tion, vector addition has been proposed. Thereby,
tokens ? get assigned (usually high-dimensional)
vectors v? and to obtain a representation of the
meaning of a phrase or a sentence w = ?1 . . . ?k,
the vector sum of the vectors associated to the con-
stituent tokens is calculated: vw =
?k
i=1 v?i .
1In our investigations we will focus on VSM composi-
tion operations which preserve the format (i.e. which yield a
vector of the same dimensionality), as our notion of composi-
tionality requires models that allow for iterated composition.
In particular, this rules out dot product and tensor product.
However the convolution product can be seen as a condensed
version of the tensor product.
This kind of composition operation is subsumed
by CMSMs; suppose in the original model, a token
? gets assigned the vector v?, then by defining
?+(v?) =
?
??????????????
1 ? ? ? 0 0
...
. . .
...
0 1 0
v? 1
?
??????????????
(mapping n-dimensional vectors to (n+1)? (n+1)
matrices), we obtain for a phrase w = ?1 . . . ?k
??1+ (?+(v?1) . . . ?+(v?k )) = v?1 + . . . + v?k = vw.
Proof. By induction on k. For k = 1, we have
vw = v? = ??1+ (?+(v?1)). For k > 1, we have
??1+ (?+(v?1) . . . ?+(v?k?1)?+(v?k ))
= ??1+ (?+(?
?1
+ (?+(v?1) . . . ?+(v?k?1)))?+(v?k ))
i.h.
= ??1+ (?+(
?k?1
i=1 v?i)?+(v?k ))
=??1+
?
?????????????
?
?????????????
1 ? ? ? 0 0
.
.
.
. . .
.
.
.
0 1 0
?k?1
i=1 v?i (1)? ? ?
?k?1
i=1 v?i (n) 1
?
?????????????
?
?????????????
1 ? ? ? 0 0
.
.
.
. . .
.
.
.
0 1 0
v?k (1)? ? ? v?k (n) 1
?
?????????????
?
?????????????
=??1+
?
?????????????
1 ? ? ? 0 0
.
.
.
. . .
.
.
.
0 1 0
?k
i=1v?i (1)? ? ?
?k
i=1v?i (n) 1
?
?????????????
=
k?
i=1
v?i
q.e.d.2
4.2 Component-wise Multiplication
On the other hand, the Hadamard product (also
called entry-wise product, denoted by ) has been
proposed as an alternative way of semantically
composing token vectors.
By using a different encoding into matrices,
CMSMs can simulate this type of composition op-
eration as well. By letting
?(v?) =
?
????????????????
v?(1) 0 ? ? ? 0
0 v?(2)
...
. . . 0
0 ? ? ? 0 v?(n)
?
????????????????
,
we obtain an n?n matrix representation for which
??1 (?(v?1) . . . ?(v?k )) = v?1  . . .  v?k = vw.
4.3 Holographic Reduced Representations
Holographic reduced representations as intro-
duced by Plate (1995) can be seen as a refinement
2The proofs for the respective correspondences for  and
~ as well as the permutation-based approach in the following
sections are structurally analog, hence, we will omit them for
space reasons.
910
of convolution products with the benefit of pre-
serving dimensionality: given two vectors v1, v2 ?
Rn, their circular convolution product v1 ~ v2 is
again an n-dimensional vector v3 defined by
v3(i + 1) =
n?1?
k=0
v1(k + 1) ? v2((i ? k mod n) + 1)
for 0 ? i ? n?1. Now let ?~(v) be the n?n matrix
M with
M(i, j) = v(( j ? i mod n) + 1).
In the 3-dimensional case, this would result in
?~(v(1) v(2) v(3)) =
?
?????????
v(1) v(2) v(3)
v(3) v(1) v(2)
v(2) v(3) v(1)
?
?????????
Then, it can be readily checked that
??1~ (?~(v?1) . . . ?~(v?k )) = v?1 ~ . . . ~ v?k = vw.
4.4 Permutation-based Approaches
Sahlgren et al (2008) use permutations on vec-
tors to account for word order. In this approach,
given a token ?m occurring in a sentence w =
?1 . . . ?k with predefined ?uncontextualized? vec-
tors v?1 . . . v?k , we compute the contextualized
vector vw,m for ?m by
vw,m = ?1?m(v?1) + . . . + ?
k?m(v?k ),
which can be equivalently transformed into
?1?m
(
v?1 + ?(. . . + ?(v?k?1 + (?(v?k ))) . . .)
)
.
Note that the approach is still token-centered, i.e.,
a vector representation of a token is endowed with
contextual representations of surrounding tokens.
Nevertheless, this setting can be transferred to a
CMSM setting by recording the position of the fo-
cused token as an additional parameter. Now, by
assigning every v? the matrix
??(v?) =
?
??????????????
0
M?
...
0
v? 1
?
??????????????
we observe that for
Mw,m := (M
?
?)
m?1??(v?1) . . . ??(v?k )
we have
Mw,m =
?
??????????????
0
Mk?m
?
...
0
vw,m 1
?
??????????????
,
whence ??1
?
(
(M?
?
)m?1??(v?1) . . . ??(v?k )
)
= vw,m.
5 CMSMs Encode Symbolic Approaches
Now we will elaborate on symbolic approaches to
language, i.e., discrete grammar formalisms, and
show how they can conveniently be embedded into
CMSMs. This might come as a surprise, as the ap-
parent likeness of CMSMs to vector-space models
may suggest incompatibility to discrete settings.
5.1 Group Theory
Group theory and grammar formalisms based on
groups and pre-groups play an important role
in computational linguistics (Dymetman, 1998;
Lambek, 1958). From the perspective of our com-
positionality framework, those approaches employ
a group (or pre-group) (G, ?) as semantical space S
where the group operation (often written as multi-
plication) is used as composition operation ./.
According Cayley?s Theorem (Cayley, 1854),
every group G is isomorphic to a permutation
group on some set S . Hence, assuming finite-
ness of G and consequently S , we can encode
group-based grammar formalisms into CMSMs in
a straightforward way by using permutation matri-
ces of size |S | ? |S |.
5.2 Regular Languages
Regular languages constitute a basic type of lan-
guages characterized by a symbolic formalism.
We will show how to select the assignment [[ ? ]]
for a CMSM such that the matrix associated to a
token sequence exhibits whether this sequence be-
longs to a given regular language, that is if it is
accepted by a given finite state automaton. As
usual (cf. e.g., Hopcroft and Ullman (1979)) we
define a nondeterministic finite automaton A =
(Q,?,?,QI,QF) with Q = {q0, . . . , qn?1} being the
set of states, ? the input alphabet, ? ? Q???Q the
transition relation, and QI and QF being the sets of
initial and final states, respectively.
911
Then we assign to every token ? ? ? the n ? n
matrix [[?]] = M with
M(i, j) =
{
1 if (qi, ?, q j) ? ?,
0 otherwise.
Hence essentially, the matrix M encodes all state
transitions which can be caused by the input ?.
Likewise, for a word w = ?1 . . . ?k ? ??, the
matrix Mw := [[?1]] . . . [[?k]] will encode all state
transitions mediated by w. Finally, if we define
vectors vI and vF by
vI(i) =
{
1 if qi ? QI,
0 otherwise,
vF(i) =
{
1 if qi ? QF,
0 otherwise,
then we find that w is accepted by A exactly if
vIMwvTF ? 1.
5.3 The General Case: Matrix Grammars
Motivated by the above findings, we now define a
general notion of matrix grammars as follows:
Definition 1 Let ? be an alphabet. A matrix
grammar M of degree n is defined as the pair
? [[ ? ]], AC? where [[ ? ]] is a mapping from ? to n?n
matrices and AC = {?v?1, v1, r1?, . . . , ?v
?
m, vm, rm?}
with v?1, v1, . . . , v
?
m, vm ? R
n and r1, . . . , rm ? R
is a finite set of acceptance conditions. The lan-
guage generated by M (denoted by L(M)) con-
tains a token sequence ?1 . . . ?k ? ?? exactly if
v?i[[?1]] . . . [[?k]]v
T
i ? ri for all i ? {1, . . . ,m}. We
will call a language L matricible if L = L(M) for
some matrix grammarM.
Then, the following proposition is a direct con-
sequence from the preceding section.
Proposition 1 Regular languages are matricible.
However, as demonstrated by the subsequent
examples, also many non-regular and even non-
context-free languages are matricible, hinting at
the expressivity of our grammar model.
Example 1 We defineM? [[ ? ]], AC? with
? = {a, b, c} [[a]] =
?
??????????????
3 0 0 0
0 1 0 0
0 0 3 0
0 0 0 1
?
??????????????
[[b]] =
?
??????????????
3 0 0 0
0 1 0 0
0 1 3 0
1 0 0 1
?
??????????????
[[c]] =
?
??????????????
3 0 0 0
0 1 0 0
0 2 3 0
2 0 0 1
?
??????????????
AC = { ?(0 0 1 1), (1 ?1 0 0), 0?,
?(0 0 1 1), (?1 1 0 0), 0?}
Then L(M) contains exactly all palindromes from
{a, b, c}?, i.e., the words d1d2 . . . dn?1dn for which
d1d2 . . . dn?1dn = dndn?1 . . . d2d1.
Example 2 We defineM = ? [[ ? ]], AC? with
? = {a, b, c} [[a]]=
?
?????????????????
1 0 0 0 0 0
0 0 0 0 0 0
0 0 0 0 0 0
0 0 0 2 0 0
0 0 0 0 1 0
0 0 0 0 0 1
?
?????????????????
[[b]]=
?
?????????????????
0 1 0 0 0 0
0 1 0 0 0 0
0 0 0 0 0 0
0 0 0 1 0 0
0 0 0 0 2 0
0 0 0 0 0 1
?
?????????????????
[[c]]=
?
?????????????????
0 0 0 0 0 0
0 0 1 0 0 0
0 0 1 0 0 0
0 0 0 1 0 0
0 0 0 0 1 0
0 0 0 0 0 2
?
?????????????????
AC = { ?(1 0 0 0 0 0), (0 0 1 0 0 0), 1?,
?(0 0 0 1 1 0), (0 0 0 1 ?1 0), 0?,
?(0 0 0 0 1 1), (0 0 0 0 1 ?1), 0?,
?(0 0 0 1 1 0), (0 0 0 ?1 0 1), 0?}
Then L(M) is the (non-context-free) language
{ambmcm | m > 0}.
The following properties of matrix grammars
and matricible language are straightforward.
Proposition 2 All languages characterized by a
set of linear equations on the letter counts are ma-
tricible.
Proof. Suppose ? = {a1, . . . an}. Given a word w,
let xi denote the number of occurrences of ai in w.
A linear equation on the letter counts has the form
k1x1 + . . . + knxn = k
(
k, k1, . . . , kn ? R
)
Now define [[ai]] = ?+(ei), where ei is the ith
unit vector, i.e. it contains a 1 at he ith position and
0 in all other positions. Then, it is easy to see that
w will be mapped to M = ?+(x1 ? ? ? xn). Due
to the fact that en+1M = (x1 ? ? ? xn 1) we can
enforce the above linear equation by defining the
acceptance conditions
AC = { ?en+1, (k1 . . . kn ? k), 0?,
??en+1, (k1 . . . kn ? k), 0?}.
q.e.d.
Proposition 3 The intersection of two matricible
languages is again a matricible language.
Proof. This is a direct consequence of the con-
siderations in Section 6 together with the observa-
tion, that the new set of acceptance conditions is
trivially obtained from the old ones with adapted
dimensionalities. q.e.d.
912
Note that the fact that the language {ambmcm |
m > 0} is matricible, as demonstrated in Ex-
ample 2 is a straightforward consequence of the
Propositions 1, 2, and 3, since the language in
question can be described as the intersection of
the regular language a+b+c+ with the language
characterized by the equations xa ? xb = 0 and
xb ? xc = 0. We proceed by giving another ac-
count of the expressivity of matrix grammars by
showing undecidability of the emptiness problem.
Proposition 4 The problem whether there is a
word which is accepted by a given matrix gram-
mar is undecidable.
Proof. The undecidable Post correspondence
problem (Post, 1946) is described as follows:
given two lists of words u1, . . . , un and v1, . . . , vn
over some alphabet ??, is there a sequence of num-
bers h1, . . . , hm (1 ? h j ? n) such that uh1 . . . uhm =
vh1 . . . vhm?
We now reduce this problem to the emptiness
problem of a matrix grammar. W.l.o.g., let ?? =
{a1, . . . , ak}. We define a bijection # from ??? to N
by
#(an1an2 . . . anl) =
l?
i=1
(ni ? 1) ? k(l?i)
Note that this is indeed a bijection and that for
w1,w2 ? ???, we have
#(w1w2) = #(w1) ? k|w2 | + #(w2).
Now, we defineM as follows:
? = {b1, . . . bn} [[bi]] =
?
?????????
k|ui | 0 0
0 k|vi | 0
#(ui) #(vi) 1
?
?????????
AC = { ?(0 0 1), (1 ? 1 0), 0?,
?(0 0 1), (?1 1 0), 0?}
Using the above fact about # and a simple induc-
tion on m, we find that
[[ah1]] . . . [[ahm]] =
?
?????????
k|uh1...uhm | 0 0
0 k|vh1...vhm | 0
#(uh1 . . .uhm) #(vh1 . . .vhm) 1
?
?????????
Evaluating the two acceptance conditions, we
find them satisfied exactly if #(uh1 . . . uhm) =
#(vh1 . . . vhm). Since # is a bijection, this is the
case if and only if uh1 . . . uhm = vh1 . . . vhm . There-
foreM accepts bh1 . . . bhm exactly if the sequence
h1, . . . , hm is a solution to the given Post Corre-
spondence Problem. Consequently, the question
whether such a solution exists is equivalent to
the question whether the language L(M) is non-
empty. q.e.d.
These results demonstrate that matrix grammars
cover a wide range of formal languages. Never-
theless some important questions remain open and
need to be clarified next:
Are all context-free languages matricible? We
conjecture that this is not the case.3 Note that this
question is directly related to the question whether
Lambek calculus can be modeled by matrix gram-
mars.
Are matricible languages closed under concatena-
tion? That is: given two arbitrary matricible lan-
guages L1, L2, is the language L = {w1w2 | w1 ?
L1,w2 ? L2} again matricible? Being a property
common to all language types from the Chomsky
hierarchy, answering this question is surprisingly
non-trivial for matrix grammars.
In case of a negative answer to one of the above
questions it might be worthwhile to introduce an
extended notion of context grammars to accom-
modate those desirable properties. For example,
allowing for some nondeterminism by associating
several matrices to one token would ensure closure
under concatenation.
How do the theoretical properties of matrix gram-
mars depend on the underlying algebraic struc-
ture? Remember that we considered matrices con-
taining real numbers as entries. In general, ma-
trices can be defined on top of any mathemati-
cal structure that is (at least) a semiring (Golan,
1992). Examples for semirings are the natural
numbers, boolean algebras, or polynomials with
natural number coefficients. Therefore, it would
be interesting to investigate the influence of the
choice of the underlying semiring on the prop-
erties of the matrix grammars ? possibly non-
standard structures turn out to be more appropri-
ate for capturing certain compositional language
properties.
6 Combination of Different Approaches
Another central advantage of the proposed matrix-
based models for word meaning is that several
matrix models can be easily combined into one.
3For instance, we have not been able to find a matrix
grammar that recognizes the language of all well-formed
parenthesis expressions.
913
Again assume a sequence w = ?1 . . . ?k of
tokens with associated matrices [[?1]], . . . , [[?k]]
according to one specific model and matrices
([?1]), . . . , ([?k]) according to another.
Then we can combine the two models into one
{[ ? ]} by assigning to ?i the matrix
{[?i]} =
?
??????????????????????????????
0 ? ? ? 0
[[?i]]
...
. . .
0 0
0 ? ? ? 0
...
. . . ([?i])
0 0
?
??????????????????????????????
By doing so, we obtain the correspondence
{[?1]} . . . {[?k]} =
?
??????????????????????????????
0 ? ? ? 0
[[?1]] . . . [[?k]]
...
. . .
0 0
0 ? ? ? 0
...
. . . ([?1]) . . . ([?k])
0 0
?
??????????????????????????????
In other words, the semantic compositions belong-
ing to two CMSMs can be executed ?in parallel.?
Mark that by providing non-zero entries for the up-
per right and lower left matrix part, information
exchange between the two models can be easily
realized.
7 Related Work
We are not the first to suggest an extension of
classical VSMs to matrices. Distributional mod-
els based on matrices or even higher-dimensional
arrays have been proposed in information retrieval
(Gao et al, 2004; Antonellis and Gallopoulos,
2006). However, to the best of our knowledge, the
approach of realizing compositionality via matrix
multiplication seems to be entirely original.
Among the early attempts to provide more com-
pelling combinatory functions to capture word or-
der information and the non-commutativity of lin-
guistic compositional operation in VSMs is the
work of Kintsch (2001) who is using a more so-
phisticated addition function to model predicate-
argument structures in VSMs.
Mitchell and Lapata (2008) formulate seman-
tic composition as a function m = f (w1,w2,R,K)
where R is a relation between w1 and w2 and K
is additional knowledge. They evaluate the model
with a number of addition and multiplication op-
erations for vector combination on a sentence sim-
ilarity task proposed by Kintsch (2001). Widdows
(2008) proposes a number of more advanced vec-
tor operations well-known from quantum mechan-
ics, such as tensor product and convolution, to
model composition in vector spaces. He shows
the ability of VSMs to reflect the relational and
phrasal meanings on a simplified analogy task.
Giesbrecht (2009) evaluates four vector compo-
sition operations (+, , tensor product, convolu-
tion) on the task of identifying multi-word units.
The evaluation results of the three studies are not
conclusive in terms of which vector operation per-
forms best; the different outcomes might be at-
tributed to the underlying word space models; e.g.,
the models of Widdows (2008) and Giesbrecht
(2009) feature dimensionality reduction while that
of Mitchell and Lapata (2008) does not. In the
light of these findings, our CMSMs provide the
benefit of just one composition operation that is
able to mimic all the others as well as combina-
tions thereof.
8 Conclusion and Future Work
We have introduced a generic model for compo-
sitionality in language where matrices are associ-
ated with tokens and the matrix representation of a
token sequence is obtained by iterated matrix mul-
tiplication. We have given algebraic, neurological,
and psychological plausibility indications in favor
of this choice. We have shown that the proposed
model is expressive enough to cover and combine
a variety of distributional and symbolic aspects of
natural language. This nourishes the hope that ma-
trix models can serve as a kind of lingua franca for
compositional models.
This having said, some crucial questions remain
before CMSMs can be applied in practice:
How to acquire CMSMs for large token sets and
specific purposes? We have shown the value
and expressivity of CMSMs by providing care-
fully hand-crafted encodings. In practical cases,
however, the number of token-to-matrix assign-
ments will be too large for this manual approach.
Therefore, methods to (semi-)automatically ac-
quire those assignments from available data are re-
quired. To this end, machine learning techniques
need to be investigated with respect to their ap-
plicability to this task. Presumably, hybrid ap-
proaches have to be considered, where parts of
914
the matrix representation are learned whereas oth-
ers are stipulated in advance guided by external
sources (such as lexical information).
In this setting, data sparsity may be overcome
through tensor methods: given a set T of tokens
together with the matrix assignment [[]] : T ?
Rn?n, this datastructure can be conceived as a 3-
dimensional array (also known as tensor) of size
n?n?|T |wherein the single token-matrices can be
found as slices. Then tensor decomposition tech-
niques can be applied in order to find a compact
representation, reduce noise, and cluster together
similar tokens (Tucker, 1966; Rendle et al, 2009).
First evaluation results employing this approach to
the task of free associations are reported by Gies-
brecht (2010).
How does linearity limit the applicability of
CMSMs? In Section 3, we justified our model by
taking the perspective of tokens being functions
which realize mental state transitions. Yet, us-
ing matrices to represent those functions restricts
them to linear mappings. Although this restric-
tion brings about benefits in terms of computabil-
ity and theoretical accessibility, the limitations in-
troduced by this assumption need to be investi-
gated. Clearly, certain linguistic effects (like a-
posteriori disambiguation) cannot be modeled via
linear mappings. Instead, we might need some
in-between application of simple nonlinear func-
tions in the spirit of quantum-collapsing of a "su-
perposed" mental state (such as the winner takes
it all, survival of the top-k vector entries, and so
forth). Thus, another avenue of further research is
to generalize from the linear approach.
Acknowledgements
This work was supported by the German Research
Foundation (DFG) under the Multipla project
(grant 38457858) as well as by the German Fed-
eral Ministry of Economics (BMWi) under the
project Theseus (number 01MQ07019).
References
[Antonellis and Gallopoulos2006] Ioannis Antonellis
and Efstratios Gallopoulos. 2006. Exploring
term-document matrices from matrix models in text
mining. CoRR, abs/cs/0602076.
[Baddeley2003] Alan D. Baddeley. 2003. Working
memory and language: An overview. Journal of
Communication Disorder, 36:198?208.
[Cayley1854] Arthur Cayley. 1854. On the theory of
groups as depending on the symbolic equation ?n =
1. Philos. Magazine, 7:40?47.
[Clark and Pulman2007] Stephen Clark and Stephen
Pulman. 2007. Combining symbolic and distribu-
tional models of meaning. In Proceedings of the
AAAI Spring Symposium on Quantum Interaction,
Stanford, CA, 2007, pages 52?55.
[Clark et al2008] Stephen Clark, Bob Coecke, and
Mehrnoosh Sadrzadeh. 2008. A compositional dis-
tributional model of meaning. In Proceedings of
the Second Symposium on Quantum Interaction (QI-
2008), pages 133?140.
[Deerwester et al1990] Scott Deerwester, Susan T. Du-
mais, George W. Furnas, Thomas K. Landauer, and
Richard Harshman. 1990. Indexing by latent se-
mantic analysis. Journal of the American Society
for Information Science, 41:391?407.
[Dymetman1998] Marc Dymetman. 1998. Group the-
ory and computational linguistics. J. of Logic, Lang.
and Inf., 7(4):461?497.
[Firth1957] John R. Firth. 1957. A synopsis of linguis-
tic theory 1930-55. Studies in linguistic analysis,
pages 1?32.
[Gao et al2004] Kai Gao, Yongcheng Wang, and Zhiqi
Wang. 2004. An efficient relevant evaluation model
in information retrieval and its application. In CIT
?04: Proceedings of the The Fourth International
Conference on Computer and Information Technol-
ogy, pages 845?850. IEEE Computer Society.
[G?rdenfors2000] Peter G?rdenfors. 2000. Concep-
tual Spaces: The Geometry of Thought. MIT Press,
Cambridge, MA, USA.
[Giesbrecht2009] Eugenie Giesbrecht. 2009. In search
of semantic compositionality in vector spaces. In
Sebastian Rudolph, Frithjof Dau, and Sergei O.
Kuznetsov, editors, ICCS, volume 5662 of Lec-
ture Notes in Computer Science, pages 173?184.
Springer.
[Giesbrecht2010] Eugenie Giesbrecht. 2010. Towards
a matrix-based distributional model of meaning. In
Proceedings of Human Language Technologies: The
2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, Student Research Workshop. ACL.
[Golan1992] Jonathan S. Golan. 1992. The theory of
semirings with applications in mathematics and the-
oretical computer science. Addison-Wesley Long-
man Ltd.
[Grefenstette1994] Gregory Grefenstette. 1994. Ex-
plorations in Automatic Thesaurus Discovery.
Springer.
915
[Hopcroft and Ullman1979] John E. Hopcroft and Jef-
frey D. Ullman. 1979. Introduction to Automata
Theory, Languages and Computation. Addison-
Wesley.
[Kintsch2001] Walter Kintsch. 2001. Predication.
Cognitive Science, 25:173?202.
[Lambek1958] Joachim Lambek. 1958. The mathe-
matics of sentence structure. The American Math-
ematical Monthly, 65(3):154?170.
[Landauer and Dumais1997] Thomas K. Landauer and
Susan T. Dumais. 1997. Solution to Plato?s prob-
lem: The latent semantic analysis theory of acqui-
sition, induction and representation of knowledge.
Psychological Review, (104).
[Lund and Burgess1996] Kevin Lund and Curt Burgess.
1996. Producing high-dimensional semantic spaces
from lexical co-occurrence. Behavior Research
Methods, Instrumentation, and Computers, 28:203?
208.
[Mitchell and Lapata2008] Jeff Mitchell and Mirella
Lapata. 2008. Vector-based models of seman-
tic composition. In Proceedings of ACL-08: HLT,
pages 236?244. ACL.
[Pad? and Lapata2007] Sebastian Pad? and Mirella La-
pata. 2007. Dependency-based construction of se-
mantic space models. Computational Linguistics,
33(2):161?199.
[Plate1995] Tony Plate. 1995. Holographic reduced
representations. IEEE Transactions on Neural Net-
works, 6(3):623?641.
[Post1946] Emil L. Post. 1946. A variant of a recur-
sively unsolvable problem. Bulletin of the American
Mathematical Society, 52:264?268.
[Rendle et al2009] Steffen Rendle, Leandro Balby
Marinho, Alexandros Nanopoulos, and Lars
Schmidt-Thieme. 2009. Learning optimal ranking
with tensor factorization for tag recommendation.
In John F. Elder IV, Fran?oise Fogelman-Souli?,
Peter A. Flach, and Mohammed Javeed Zaki,
editors, KDD, pages 727?736. ACM.
[Sahlgren et al2008] Magnus Sahlgren, Anders Holst,
and Pentti Kanerva. 2008. Permutations as a means
to encode order in word space. In Proc. CogSci?08,
pages 1300?1305.
[Salton et al1975] Gerard Salton, Anita Wong, and
Chung-Shu Yang. 1975. A vector space model for
automatic indexing. Commun. ACM, 18(11):613?
620.
[Sch?tze1993] Hinrich Sch?tze. 1993. Word space.
In Lee C. Giles, Stephen J. Hanson, and Jack D.
Cowan, editors, Advances in Neural Information
Processing Systems 5, pages 895?902. Morgan-
Kaufmann.
[Strang1993] Gilbert Strang. 1993. Introduction to
Linear Algebra. Wellesley-Cambridge Press.
[Tucker1966] Ledyard R. Tucker. 1966. Some math-
ematical notes on three-mode factor analysis. Psy-
chometrika, 31(3).
[Widdows2008] Dominic Widdows. 2008. Semantic
vector products: some initial investigations. In Pro-
ceedings of the Second AAAI Symposium on Quan-
tum Interaction.
916
Proceedings of the Workshop on Distributional Semantics and Compositionality (DiSCo?2011), pages 21?28,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
Distributional Semantics and Compositionality 2011:
Shared Task Description and Results
Chris Biemann
UKP lab, Technical University of Darmstadt
Hochschulstr. 10
64289 Darmstadt, Germany
biemann@tk.informatik.tu-darmstadt.de
Eugenie Giesbrecht
FZI Forschungszentrum Informatik
Haid-und-Neu-Str. 10-14
76131 Karlsruhe, Germany
giesbrecht@fzi.de
Abstract
This paper gives an overview of the shared
task at the ACL-HLT 2011 DiSCo (Distribu-
tional Semantics and Compositionality) work-
shop. We describe in detail the motivation
for the shared task, the acquisition of datasets,
the evaluation methodology and the results
of participating systems. The task of assign-
ing a numerical score for a phrase accord-
ing to its compositionality showed to be hard.
Many groups reported features that intuitively
should work, yet showed no correlation with
the training data. The evaluation reveals that
most systems outperform simple baselines, yet
have difficulties in reliably assigning a compo-
sitionality score that closely matches the gold
standard. Overall, approaches based on word
space models performed slightly better than
methods relying solely on statistical associa-
tion measures.
1 Introduction
Any NLP system that does semantic processing re-
lies on the assumption of semantic compositionality:
the meaning of a phrase is determined by the mean-
ings of its parts and their combination. However,
this assumption does not hold for lexicalized phrases
such as idiomatic expressions, which causes troubles
not only for semantic, but also for syntactic process-
ing (Sag et al, 2002). In particular, while distribu-
tional methods in semantics have proved to be very
efficient in tackling a wide range of tasks in natural
language processing, e.g., document retrieval, clus-
tering and classification, question answering, query
expansion, word similarity, synonym extraction, re-
lation extraction, textual advertisement matching in
search engines, etc. (see Turney and Pantel (2010)
for a detailed overview), they are still strongly lim-
ited by being inherently word-based. While dictio-
naries and other lexical resources contain multiword
entries, these are expensive to obtain and not avail-
able for all languages to a sufficient extent. Fur-
thermore, the definition of a multiword varies across
resources, and non-compositional phrases are often
merely a subclass of multiword units.
This shared task addressed researchers that are
interested in extracting non-compositional phrases
from large corpora by applying distributional mod-
els that assign a graded compositionality score to a
phrase, as well as researchers interested in express-
ing compositional meaning with such models. The
score denotes the extent to which the composition-
ality assumption holds for a given expression. The
latter can be used, for example, to decide whether
the phrase should be treated as a single unit in ap-
plications. We emphasized that the focus is on au-
tomatically acquiring semantic compositionality and
explicitly did not invite approaches that employ pre-
fabricated lists of non-compositional phrases.
It is often the case that compositionality of a
phrase depends on the context. Though we have
used a sentence context in the process of construct-
ing the gold standard, we have decided not to pro-
vide it with the dataset: we have asked for a sin-
gle compositionality score per phrase. In an appli-
cation, this could play the role of a compositional-
ity prior that could, e.g., be stored in a dictionary.
There is a long-living tradition within the research
21
community working on multiword units (MWUs) to
automatically classify MWUs into either composi-
tional or non-compositional ones. However, it has
been often noted that compositionality comes in de-
grees, and a binary classification is not valid enough
in many cases (Bannard et al, 2003; Katz and Gies-
brecht, 2006). To the best of our knowledge, this has
been the first attempt to offer a dataset and a shared
task that allows to explicitly evaluate the models of
graded compositionality.
2 Shared Task Description
For the shared task, we aimed to get composition-
ality scores for phrases frequently occurring in cor-
pora. Since distributional models need large corpora
to perform reliable statistics, and these statistics are
more reliable for frequent items, we chose to restrict
the candidate set to the most frequent phrases from
the freely available WaCky1 web corpora (Baroni et
al., 2009). Those are currently downloadable for En-
glish, French, German and Italian. They have al-
ready been automatically sentence-split, tokenized,
part-of-speech (POS) tagged and lemmatized, which
reduces the load on both organizers and participants
that decide to make use of these corpora. Further,
WaCky corpora provide a good starting point for ex-
perimenting with distributional models due to their
size, ranging between 1-2 billion tokens, and exten-
sive efforts to make these corpora as clean as possi-
ble.
2.1 Candidate Selection
There is a wide range of subsentential units that can
function as a non-compositional construction. These
units do not have to be realized continuously in the
surface realization and can consist of an arbitrary
number of lexical items. While it would be interest-
ing to examine unrestricted forms of multiwords and
compositional phrases, we decided to restrict candi-
date selection to certain grammatical constructions
to make the task more tangible. Specifically, we use
word pairs in the following relations:
? ADJ NN: Adjective modifying a noun, e.g.
?red herring? or ?blue skies?
1http://wacky.sslmit.unibo.it
? V SUBJ: Noun in subject position and verb,
e..g. ?flies fly? or ?people transfer (sth.)?
? V OBJ: Noun in object position and verb, e.g.
?lose keys?, ?play song?
While it is possible to extract the relations fairly ac-
curately from parsed English text, there is ? to our
knowledge ? no reliable, freely available method
that can tell verb-subjects from verb-objects for Ger-
man. Thus, we employed a three-step selection
procedure for producing a set of candidate phrases
per grammatical relation and language that involved
heavy manual intervention.
1. Extract candidates using (possibly over-
generating) patterns over part-of-speech
sequences and sort by frequency
2. Manually select plausible candidates for the
target grammatical relation in order of decreas-
ing frequency
3. Balance the candidate set to select enough non-
compositional phrases
For English, we used the following POS pat-
terns: ADJ NN: ?JJ* NN*?; V SUBJ: ?NN* VV*?;
V OBJ: ?VV* DT|CD NN*? and ?VV* NN*?. The
star * denotes continuation of tag labels: e.g. VV*
matches all tags starting with ?VV?, such as VV,
VVD, VVG, VVN, VVP and VVZ.
For German, we used ?ADJ* NN*? for ADJ NN.
For relations involving nouns and verbs, we ex-
tracted all noun-verb pairs in a window of 4 tokens
and manually filtered by relation on the aggregated
frequency list. Frequencies were computed on the
lemma forms.
This introduces a bias on the possible construc-
tions that realize the target relations, especially for
the verb-noun pairs. Further, the selection procedure
is biased by the intuition of the person that performs
the selection. We only admitted what we thought
were clear-cut cases (only nouns that are typically
found in subject respectively object position) to the
candidate set at this stage.
Since non-compositional phrases are much less
in numbers than compositional phrases, we tried to
somewhat balance this in the third step in the se-
lection. If the candidates would have been randomly
22
selected, an overwhelming number of compositional
phrases would have rendered the task very hard to
evaluate, since a baseline system predicting high
compositionality in all cases would have achieved
a very high score. We argue that since we are es-
pecially interested in non-compositional phrases in
this competition, it is valid to bias the dataset in this
way.
After we collected a candidate list, we randomly
selected seven sentences per candidate from the cor-
pus. Through manual filtering, we checked whether
the target word pair was in fact found in the target
relation in these sentences. Further we removed in-
complete and too long sentences, so that we ended
up with five sentences per target phrase. Some can-
didate phrases that only occurred in very fixed con-
texts (e.g. disclaimers) or did not have enough well-
formed sentences were removed in this step.
Figure 1 shows the sentences for ?V OBJ: buck
trend? as an example output of this procedure.
2.2 Annotation
The sample usages of target phrases now had to be
annotated for compositionality. We employed the
crowdsourcing service Amazon Turk2 for realizing
these annotations. The advantage of crowdsourc-
ing is its scalability through the large numbers of
workers that are ready to perform small tasks for
pay. The disadvantage is that tasks usually cannot be
very complex, since quality issues (scammers) have
to be addressed either with test items or redundancy
or both ? mechanisms that only work for types of
tasks where there is clearly a correct answer.
Previous experiences in constructing linguistic
annotations with Amazon Turk (Biemann and Ny-
gaard, 2010) made us stick to the following two-step
procedure that more or less ensured the quality of
annotation by hand-picking workers:
1. Gather high quality workers: In an open task
for a small data sample with unquestionable de-
cisions, we collected annotations from a large
number of workers. Workers were asked to
provide reasons for their decisions. Workers
that performed well, gave reasons that demon-
strated their understanding of the task and com-
pleted a significant amount of the examples
2http://www.mturk.com
were invited for a closed task. Net pay was 2
US cents for completing a HIT.
2. Get annotations for the real task: In the closed
task, only invited workers were admitted and
redundancy was reduced to four workers per
HIT. Net pay was 3 US cents for completing
a HIT.
Figure 2 shows a sample HIT (human intelligence
task) for English on Amazon turk, including in-
structions. Workers were asked to enter a judgment
from 0-10 about the literacy of the highlighted tar-
get phrase in the respective context. For the German
data, we used an equivalent task definition in Ger-
man.
All five contexts per target phrase were scored by
four workers each. A few items were identified as
problematic by the workers (e.g. missing highlight-
ing, too little context), and one worker was excluded
during the English experiment for starting to delib-
erately scam. For this worker, all judgments were re-
moved and not repeated. Thus, the standard number
of judgments per target phrase was 20, with some
targets receiving less judgments because of these
problems. The minimum number of judgments per
target phrase was 12: four HITs with three judg-
ments each.
From this, we computed a score by averaging over
all judgments per phrase and multiplying the over-
all score by 10 to get scores in the range of 0-100.
This score cannot help in discriminating moderately
compositional phrases like ?V OBJ: make decision?
from phrases that are dependent on the context like
?V OBJ: wait minute? which had two HITs for the
idiomatic use of ?wait a minute!? and three HITs
with literally minutes to spend idling.
As each HIT was annotated by a possibly differ-
ent set of workers, it is not possible to compute inter-
annotator agreement. Eyeballing the scores revealed
that some workers generally tend to give higher re-
spectively lower scores than others. Overall, work-
ers agreed more for clearly compositional or clearly
non-compositional HITs. We believe that using this
comparatively high number of judgments per target,
averaged over several contexts, should give us fairly
reliable judgments, as worker biases should cancel
out each other.
23
? I would like to buck the trend of complaint !
? One company that is bucking the trend is Flowcrete Group plc located in Sandbach , Cheshire .
? ? We are now moving into a new phase where we are hoping to buck the trend .
? With a claimed 11,000 customers and what look like aggressive growth plans , including recent acquisitions of
Infinium Software , Interbiz and earlier also Max international , the firm does seem to be bucking the trend of
difficult times .
? Every time we get a new PocketPC in to Pocket-Lint tower , it seems to offer more features for less money and
the HP iPaq 4150 is n?t about to buck the trend .
Figure 1: sentences for V OBJ: buck trend after manual filtering and selection. The target is highlighted.
How literal is this phrase?
Can you infer the meaning of a given phrase by only considering their parts literally, or does the phrase carry a ?special? meaning?
In the context below, how literal is the meaning of the phrase in bold?
Enter a number between 0 and 10.
? 0 means: this phrase is not to be understood literally at all.
? 10 means: this phrase is to be understood very literally.
? Use values in between to grade your decision. Please, however, try to take a stand as often as possible.
In case the context is unclear or nonsensical, please enter ?66? and use the comment field to explain. However, please try to make sense of it even if the sentences are incomplete.
Example 1 :
There was a red truck parked curbside. It looked like someone was living in it.
YOUR ANSWER: 10
reason: the color of the truck is red, this can be inferred from the parts ?red? and ?truck? only - without any special knowledge.
? Example 2 :
What a tour! We were on cloud nine when we got back to headquarters but we kept our mouths shut.
YOUR ANSWER: 0
reason: ?cloud nine? means to be blissfully happy. It does NOT refer to a cloud with the number nine.
Example 3 :
Yellow fever is found only in parts of South America and Africa.
YOUR ANSWER: 7
reason: ?yellow fever? refers to a disease causing high body temperature. However, the fever itself is not yellow. Overall, this phrase is fairly literal, but not totally, hence answering with a value
between 5 and 8 is appropriate.
We take rejection seriously and will not reject a HIT unless done carelessly. Entering anything else but numbers between 0 and 10 or 66 in the judgment field will automatically trigger rejection.
YOUR CONTEXT with big day
Special Offers : Please call FREEPHONE 0800 0762205 to receive your free copy of ? Groom ? the full
colour magazine dedicated to dressing up for the big day and details of Moss Bros Hire rates .
How literal is the bolded phrase in the context above between 0 and 10?
[ ]
OPTIONAL: leave a comment, tell us about what is broken, help us to improve this type of HIT:
[ ]
Figure 2: Sample Human Intelligence Task on Amazon Turk with annotation instructions
24
EN ADJ NN V SUBJ V OBJ Sum
Train 58 (43) 30 (23) 52 (41) 140 (107)
Vali. 10 (7) 9 (6) 16 (13) 35 (26)
Test 77 (52) 35 (26) 62 (40) 174 (118)
All 145 (102) 74 (55) 130 (94) 349 (251)
Table 1: English dataset: number of target phrases (with
coarse scores)
DE ADJ NN V SUBJ V OBJ Sum
Train 49 (42) 26 (23) 44 (33) 119 (98)
Vali. 11 (8) 9 (8) 9 (7) 29 (23)
Test 63 (48) 29 (28) 57 (44) 149 (120)
All 123 (98) 64 (59) 110 () 297 (241)
Table 2: German dataset: number of target phrases (with
coarse scores)
Additionally to the numerical scores, we?ve also
provided coarse-grained labels. This is motivated
by the following: for some applications, it is prob-
ably enough to decide whether a phrase is always
compositional, somewhat compositional or usually
not compositional, without the need of more fine-
grained distinctions. For this, we?ve transformed the
numerical scores in the range of 0-25 to coarse la-
bel ?low?, those between 38-62 have been labeled
as ?medium?, and the ones from 75 to 100 have re-
ceived the value ?high?. All other phrases have been
excluded from the corresponding training and test
datasets for ?coarse evaluation? (s. Section 2.4.2):
28.1% of English and 18.9% of German phrases.
2.3 Datasets
Now we describe the datasets in detail. Table 1 sum-
marizes the English data, Table 2 describes the Ger-
man data quantitatively. Per language and relation,
the data was randomly split in approximatively 40%
training, 10% validation and 50% test.
2.4 Scoring of system responses
We provided evaluation scripts along with the train-
ing and validation data. Additionally, we report cor-
relation values (Spearman?s rho and Kendall?s tau)
in Section 4.
2.4.1 Numerical Scoring
For numerical scoring, the evaluation script com-
putes the distance between the system responses
S = {starget1, starget2, ...stargetN} and the gold
standard G = {gtarget1, gtarget2, ...gtargetN} in
points, averaged over all items:
NUMSCORE(S,G) = 1N
?
i=1..N |gi ? si|.
Missing values in the system scores are filled
with the default value of 50. A perfect score is
0, indicating no difference between the system
responses and the gold standard.
2.4.2 Coarse Scoring
We use precision on coarse label predictions for
coarse scoring:
COARSE(S,G) =
1
N
?
i=1..N
{ si == gi : 1
otherwise : 0.
As with numerical scoring, missing system re-
sponses are filled with a default value, in this case
?medium?. A perfect score would be 1.00, connot-
ing complete congruence of gold standard and sys-
tem response labels.
3 Participants
Seven teams participated in the shared task. Table 3
summarizes the participants and their systems. Four
of the teams (Duluth, UoY, JUCSE, SCSS-TCD)
submitted three runs for the whole English test set.
One team participated with two systems, one of
which was for the entire English dataset and an-
other one included entries only for English V SUBJ
and V OBJ relations. A team from UNED provided
scores solely for English ADJ NN pairs. UCPH was
the only team that delivered results for both English
and German.
Systems can be split into approaches based on sta-
tistical association measures and approaches based
on word space models. On top, some systems used
a machine-learned classifier to predict numerical
scores or coarse labels.
4 Results
The results of the official evaluation for English are
shown in Tables 4 and 5.
Table 4 reports the results for numerical scor-
ing. UCPH-simple.en performed best with the score
of 16.19. The second best system UoY: Exm-Best
achieved 16.51, and the third was UoY:Pro-Best
with 16.79. It is worth noting that the top six systems
25
Systems Institution Team Approach
Duluth-1 Dept. of Computer Science, Ted Pedersen statistical
Duluth-2 University of Minnesota association measures:
Duluth-3 t-score and pmi
JUCSE-1 Jadavpur University Tanmoy Chakraborty, Santanu Pal mix of statistical
JUCSE-2 Tapabrata Mondal, Tanik Saikh, association measures
JUCSE-3 Sivaju Bandyopadhyay
SCSS-TCD:conf1 SCSS, Alfredo Maldonado-Guerra, unsupervised WSM,
SCSS-TCD:conf2 Trinity College Dublin Martin Emms cosine similarity
SCSS-TCD:conf3
submission-ws Gavagai Hillevi Ha?gglo?f, random indexing
submission-pmi Lisa Tengstrand association measures (pmi)
UCPH-simple.en University of Copenhagen Anders Johannsen, Hector Martinez, support vector regression
Christian Rish?j, Anders S?gaard with COALS-based
endocentricity features
UoY: Exm University of York, UK; Siva Reddy, Diana McCarthy, exemplar-based WSMs
UoY: Exm-Best Lexical Computing Ltd., UK Suresh Manandhar,
UoY: Pro-Best Spandana Gella prototype-based WSM
UNED-1: NN NLP and IR Group at UNED Guillermo Garrido, syntactic VSM,
UNED-2: NN Anselmo Peas dependency-parsed UKWaC,
UNED-3: NN SVM classifier
Table 3: Participants of DiSCo?2011 Shared Task
in the numerical evaluation are all based on different
variations of word space models.
The outcome of evaluation for coarse scores is
displayed in Table 5. Here, Duluth-1 performs high-
est with 0.585, followed closely by UoY:ExmBest
with 0.576 and UoY: ProBest with 0.567. Duluth-
1 is an approach purely based on association mea-
sures.
Both tables also report ZERO-response and
RANDOM-response baselines. ZERO-response
means that, if no score is reported for a phrase, it
gets a default value of 50 (fifty) points in numerical
evaluation and ?medium? in coarse evaluation. Ran-
dom baselines were created by using random labels
from a uniform distribution. Most systems beat the
RANDOM-response baseline, only about half of the
systems are better than ZERO-response.
Apart from the officially announced scoring meth-
ods, we provide Spearman?s rho and Kendall?s tau
rank correlations for numerical scoring. Rank cor-
relation scores that are not significant are noted in
parentheses. With correlations, the higher the score,
the better is the system?s ability to order the phrases
according to their compositionality scores. Here,
systems UoY: Exm-Best, UoY: Pro-Best / JUCSE-
1 and JUCSE-2 achieved the first, second and third
best results respectively.
Overall, there is no clear winner for the English
dataset. However, across different scoring mecha-
nisms, UoY: Exm-Best is the most robust of the sys-
tems. The UCPH-simple.en system has a stellar per-
formance on V OBJ but apparently uses a subopti-
mal way of assigning coarse labels. The Duluth-1
system, on the other hand, is not able to produce a
numerical ranking that is significant according to the
correlation measures, but excels in the coarse scor-
ing.
When comparing word space models and asso-
ciation measures, it seems that the former do a
slightly better job on modeling graded composition-
ality, which is especially obvious in the numerical
evaluation.
Since word space models and statistical associa-
tion measures are language-independent approaches
and most teams have not used syntactic preprocess-
ing other than POS tagging, it is a pity that only one
team has tried the German task (see Tables 6 and
7). The comparison to the baselines shows that the
UCPH system is robust across languages and per-
forms (relatively speaking) equally well in the nu-
merical scoring both for the German and the English
tasks.
26
numerical scores responses ? ? EN all EN ADJ NN EN V SUBJ EN V OBJ
number of phrases 174 77 35 62
0-response baseline 0 - - 23.42 24.67 17.03 25.47
random baseline 174 (0.02) (0.02) 32.82 34.57 29.83 32.34
UCPH-simple.en 174 0.27 0.18 16.19 14.93 21.64 14.66
UoY: Exm-Best 169 0.35 0.24 16.51 15.19 15.72 18.6
UoY: Pro-Best 169 0.33 0.23 16.79 14.62 18.89 18.31
UoY: Exm 169 0.26 0.18 17.28 15.82 18.18 18.6
SCSS-TCD: conf1 174 0.27 0.19 17.95 18.56 20.8 15.58
SCSS-TCD: conf2 174 0.28 0.19 18.35 19.62 20.2 15.73
Duluth-1 174 (-0.01) (-0.01) 21.22 19.35 26.71 20.45
JUCSE-1 174 0.33 0.23 22.67 25.32 17.71 22.16
JUCSE-2 174 0.32 0.22 22.94 25.69 17.51 22.6
SCSS-TCD: conf3 174 0.18 0.12 25.59 24.16 32.04 23.73
JUCSE-3 174 (-0.04) (-0.03) 25.75 30.03 26.91 19.77
Duluth-2 174 (-0.06) (-0.04) 27.93 37.45 17.74 21.85
Duluth-3 174 (-0.08) (-0.05) 33.04 44.04 17.6 28.09
submission-ws 173 0.24 0.16 44.27 37.24 50.06 49.72
submission-pmi 96 - - - - 52.13 50.46
UNED-1: NN 77 - - - 17.02 - -
UNED-2: NN 77 - - - 17.18 - -
UNED-3: NN 77 - - - 17.29 - -
Table 4: Numerical evaluation scores for English: average point difference and correlation measures (not significant
values in parentheses)
coarse values responses EN all EN ADJ NN EN V SUBJ EN V OBJ
number of phrases 118 52 26 40
zero-response baseline 0 0.356 0.288 0.654 0.250
random baseline 118 0.297 0.288 0.308 0.300
Duluth-1 118 0.585 0.654 0.385 0.625
UoY: Exm-Best 114 0.576 0.692 0.500 0.475
UoY: Pro-Best 114 0.567 0.731 0.346 0.500
UoY: Exm 114 0.542 0.692 0.346 0.475
SCSS-TCD: conf2 118 0.542 0.635 0.192 0.650
SCSS-TCD: conf1 118 0.534 0.64 0.192 0.625
JUCSE-3 118 0.475 0.442 0.346 0.600
JUCSE-2 118 0.458 0.481 0.462 0.425
SCSS-TCD: conf3 118 0.449 0.404 0.423 0.525
JUCSE-1 118 0.441 0.442 0.462 0.425
submission-ws 117 0.373 0.346 0.269 0.475
UCPH-simple.en 118 0.356 0.346 0.500 0.275
Duluth-2 118 0.322 0.173 0.346 0.500
Duluth-3 118 0.322 0.135 0.577 0.400
submission-pmi - - - 0.346 0.550
UNED-1-NN 52 - 0.289 - -
UNED-2-NN 52 - 0.404 - -
UNED-3-NN 52 - 0.327 - -
Table 5: Coarse evaluation scores for English
27
numerical scores responses ? ? DE all DE ADJ NN DE V SUBJ DE V OBJ
number of phrases 149 63 29 57
0-response baseline 0 - - 32.51 32.21 38.00 30.05
random baseline 149 (0.005) (0.004) 37.79 36.27 47.45 34.54
UCPH-simple.de 148 0.171 0.116 24.03 27.09 15.55 24.06
Table 6: Numerical evaluation scores for German
heightcoarse values responses DE all DE ADJ NN DE V SUBJ DE V OBJ
number of phrases 120 48 28 44
0-response baseline 0 0.158 0.208 0.071 0.159
random baseline 120 0.283 0.313 0.214 0.295
UCPH-simple.de 119 0.283 0.375 0.286 0.182
Table 7: Coarse evaluation scores for German
For more details on the systems as well as fine-
grained analysis of the results, please consult the
corresponding system description papers.
5 Conclusion
DiSCo Shared Task attracted seven groups that sub-
mitted results for 19 systems. We consider this
a success, taking into consideration that the task
is new and difficult. The opportunity to evaluate
language-independent models for languages other
than English was unfortunately not taken up by most
participants.
The teams applied a variety of approaches that
can be classified into lexical association measures
and word space models of various flavors. From
the evaluation, it is hard to decide what method is
currently more suited for the task of automatic ac-
quisition of compositionality, with a slight favor for
approaches based on word space model.
A takeaway message is that a pure corpus-based
acquisition of graded compositionality is a hard task.
While some approaches clearly outperform base-
lines, further advances are needed for automatic sys-
tems to be able to reproduce semantic composition-
ality.
Acknowledgments
We thank Emiliano Guevara for helping with the
preparation of the evaluation scripts and the ini-
tial task description. This work was partially sup-
ported by the German Federal Ministry of Eco-
nomics (BMWi) under the project Theseus (number
01MQ07019).
References
Colin Bannard, Timothy Baldwin, and Alex Lascarides.
2003. A statistical approach to the semantics of verb-
particles. In Proc. of the ACL-SIGLEX Workshop
on Multiword Expressions: Analysis, Acquisition and
Treatment, pages 65?72, Sapporo, Japan.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and
Eros Zanchetta. 2009. The WaCky wide web: a
collection of very large linguistically processed web-
crawled corpora. Language Resources and Evalua-
tion.
Chris Biemann and Valerie Nygaard. 2010. Crowdsourc-
ing WordNet. In Proc. of the 5th International Confer-
ence of the Global WordNet Association (GWC-2010),
Mumbai, India.
Graham Katz and Eugenie Giesbrecht. 2006. Auto-
matic identification of non-compositional multi-word
expressions using latent semantic analysis. In Pro-
ceedings of the ACL/COLING-06 Workshop on Multi-
word Expressions: Identifying and Exploiting Under-
lying Properties, pages 12?19, Sydney, Australia.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multiword Ex-
pressions: A Pain in the Neck for NLP. In Proc. of
the 3rd International Conference on Intelligent Text
Processing and Computational Linguistics (CICLing-
2002), pages 1?15, Mexico City, Mexico.
Peter D. Turney and Patrick Pantel. 2010. From Fre-
quency to Meaning: Vector Space Models of Se-
mantics. Journal of Artificial Intelligence Research,
37:141?188.
28
