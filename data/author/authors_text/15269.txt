Proceedings of the 5th ACL-HLT Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 54?62,
Portland, OR, USA, 24 June 2011. c?2011 Association for Computational Linguistics
Structure-Preserving Pipelines for Digital Libraries
Massimo Poesio
University of Essex, UK and
Universit? di Trento, Italy
Eduard Barbu
Egon W. Stemle
Universit? di Trento, Italy
{massimo.poesio,eduard.barbu,egon.stemle}
@unitn.it
Christian Girardi
FBK-irst, Trento, Italy
cgirardi@fbk.eu
Abstract
Most existing HLT pipelines assume the input
is pure text or, at most, HTML and either ig-
nore (logical) document structure or remove
it. We argue that identifying the structure of
documents is essential in digital library and
other types of applications, and show that it
is relatively straightforward to extend existing
pipelines to achieve ones in which the struc-
ture of a document is preserved.
1 Introduction
Many off-the-shelf Human Language Technology
(HLT) pipelines are now freely available (examples
include LingPipe,1 OpenNLP,2 GATE3 (Cunning-
ham et al, 2002), TextPro4 (Pianta et al, 2008)),
and although they support a variety of document for-
mats as input, actual processing (mostly) takes no
advantage of structural information, i.e. structural
information is not used, or stripped off during pre-
processing. Such processing can be considered safe,
e.g. in case of news wire snippets, when processing
does not need to be aware of sentence or paragraph
boundaries, or of text being part of a table or a fig-
ure caption. However, when processing large doc-
uments, section or chapter boundaries may be con-
sidered an important segmentation to use, and when
working with the type of data typically found in dig-
ital libraries or historical archives, such as whole
1
http://alias-i.com/lingpipe/
2
http://incubator.apache.org/opennlp/
3
http://http://gate.ac.uk/
4
http://textpro.fbk.eu/
books, exhibition catalogs, scientific articles, con-
tracts we should keep the structure. At least three
types of problems can be observed when trying to
use a standard HLT pipeline for documents whose
structure cannot be easily ignored:
? techniques for extracting content from plain
text do not work on, say, bibliographic refer-
ences, or lists;
? simply removing the parts of a document that
do not contain plain text may not be the right
thing to do for all applications, as sometimes
the information contained in them may also be
useful (e.g., keywords are often useful for clas-
sification, bibliographic references are useful in
a variety of applications) or even the most im-
portant parts of a text (e.g., in topic classifica-
tion information provided by titles and other
types of document structure is often the most
important part of a document);
? even for parts of a document that still can be
considered as containing basically text?e.g.,
titles?knowing that we are dealing with what
we will call here non-paragraph text can be
useful to achieve good - or improve - perfor-
mance as e.g., the syntactic conventions used
in those type of document elements may be dif-
ferent - e.g., the syntax of NPs in titles can be
pretty different from that in other sections of
text.
In this paper we summarize several years of work
on developing structure-preserving pipelines for dif-
ferent applications. We discuss the incorporation of
54
document structure parsers both in pipelines which
the information is passed in BOI format (Ramshaw
and Marcus, 1995), such as the TEXTPRO pipeline
(Pianta et al, 2008), and in pipelines based on a
standoff XML (Ide, 1998). We also present sev-
eral distinct applications that require preserving doc-
ument structure.
The structure of the paper is as follows. We first
discuss the notion of document structure and previ-
ous work in extracting it. We then introduce our ar-
chitecture for a structure-preserving pipeline. Next,
we discuss two pipelines based on this general archi-
tecture. A discussion follows.
2 The Logical Structure of a Document
Documents have at least two types of structure5.
The term geometrical, or layout, structure, refers
to the structuring of a document according to its vi-
sual appearance, its graphical representation (pages,
columns, etc). The logical structure (Luong et al,
2011) refers instead to the content?s organization to
fulfill an intended overall communicative purpose
(title, author list, chapter, section, bibliography, etc).
Both of these structures can be represented as trees;
however, these two tree structures may not be mu-
tually compatible (i.e. representable within a single
tree structure with non-overlapping structural ele-
ments): e.g. a single page may contain the end of
one section and the beginning of the next, or a para-
graph may just span part of a page or column. In this
paper we will be exclusively concerned with logical
structure.
2.1 Proposals concerning logical structure
Early on the separation of presentation and content,
i.e. of layout and logical structure, was promoted by
the early adopters of computers within the typeset-
ting community; well-known, still widely used, sys-
tems include the LATEXmeta-package for electronic
typesetting. The importance of separating document
logical structure from document content for elec-
tronic document processing and for the document
creators lead to the ISO 8613-1:1989(E) specifica-
tion where logical structure is defined as the result
of dividing and subdividing the content of a docu-
5other structure types include e.g. (hyper)links, cross-
references, citations, temporal and spatial relationships
ment into increasingly smaller parts, on the basis of
the human-perceptible meaning of the content, for
example, into chapters, sections, subsections, and
paragraphs. The influential ISO 8879:1986 Stan-
dard Generalized Markup Language (SGML) spec-
ification fostered document format definitions like
the Open Document Architecture (ODA) and inter-
change format, CCITT T.411-T.424 / ISO 8613.
Even though the latter format never gained
wide-spread support, its technological ideas influ-
enced many of today?s formats, like HTML and
CSS as well as, the Extensible Markup Language
(XML), today?s successor of SGML. Today, the ISO
26300:2006 Open Document Format for Office Ap-
plications (ODF), and the ISO 29500:2008 Office
Open XML (OOXML) format are the important
XML-based document file formats.
For the work on digital libraries the Text Encod-
ing Initiative (TEI)6,most notably, developed guide-
lines specifying encoding methods for machine-
readable texts. They have been widely used, e.g. by
libraries, museums, and publishers.
The most common logical elements in such
proposals?chapters, sections, paragraphs, foot-
notes, etc.?can all be found in HTML, LATEX, or
any other modern text processor. It should be
pointed out however that many modern types of doc-
uments found on the Web do not fit this pattern:
e.g. blog posts with comments, and the structure of
reply threads and inner-linkings to other comments
cannot be captured; or much of wikipedia?s non-
paragraph text. (For an in depth comparison and
discussion of logical formats, and formal characteri-
zations thereof we suggest (Power et al, 2003; Sum-
mers, 1998).)
2.2 Extracting logical structure
Two families of methods have been developed to ex-
tract document structure. Older systems tend to fol-
low the template-matching paradigm. In this ap-
proach the assignment of the categories to parts of
the string is done by matching a sequence of hand
crafted templates against the input string S. An
instance of this kind of systems is DeLos (Deriva-
tion of Logical Structure) (Niyogi and Srihari, 1995)
which uses control rules, strategy rules and knowl-
6
http://www.tei-c.org
55
edge rules to derive the logical document structure
from a scanned image of the document. A more elab-
orate procedure for the same task is employed by
Ishitani (Ishitani, 1999). He uses rules to classify the
text lines derived from scanned document image and
then employs a set of heuristics to assign the classi-
fied lines to logical document components. The tem-
plate based approach is also used by the ParaTools,
a set of Perl modules for parsing reference strings
(Jewell, 2000). The drawback of the template based
approaches is that they are usually not portable to
new domains and are not flexible enough to accom-
modate errors. Domain adaptation requires the de-
vising of new rules many of them from scratch. Fur-
ther the scanned documents or the text content ex-
tracted from PDF have errors which are not easily
dealt with by template based systems.
Newer systems use supervised machine learning
techniques which are much more flexible but re-
quire training data. Extracting document structure
is an instance of (hierarchical) sequence labeling,
a well known problem which naturally arises in di-
verse fields like speech recognition, digital signal
processing or bioinformatics. Two kinds of machine
learning techniques are most commonly used for this
problem: Hidden Markov Models (HMM) and Con-
ditional Random Fields (CRF). A system for pars-
ing reference strings based on HMMs was developed
in (Hetzner, 2008) for the California Digital Library.
The system implements a first order HMM where the
set of states of the model are represented by the cat-
egories in C; the alphabet is hand built and tailored
for the task and the probabilities in the probability
matrix are derived empirically. The system obtains
an average F1 measure of 93 for the Cora dataset.
A better performance for sequence labeling is ob-
tained if CRF replaces the traditional HMM. The
reason for this is that CRF systems better tolerate
errors and they have good performance even when
richer features are not available. A system which
uses CRF and a series of post-processing rules for
both document logical structure identification and
reference string parsing is ParsCit (Councill et al,
2008). ParsCit comprises three sub-modules: Sect-
Label and ParseHead for document logical structure
identification and ParsCit for reference string pars-
ing. The system is built on top of the well known
CRF++ package.
The linguistic surface level, i.e. the linear order
of words, sentences, and paragraphs, and the hi-
erarchical, tree-like, logical structure also lends it-
self to parsing-like methods for the structure analy-
sis. However, the complexity of fostering, maintain-
ing, and augmenting document structure grammars
is challenging, and the notorious uncertainty of the
input demands for the whole set of stochastic tech-
niques the field has to offer ? this comes at a high
computing price; cf. e.g.,(Lee et al, 2003; Mao et
al., 2003). It is therefore not surprising that high-
throughput internet sites like CiteSeerX7 use a flat
text classifier (Day et al, 2007).8
3 Digital Libraries and Document
Structure Preservation
Our first example of application in which document
structure preservation is essential are digital libraries
(Witten et al, 2003). In a digital library setting, HLT
techniques can be used for a variety of purposes,
ranging from indexing the documents in the library
for search to classifying them to automatically ex-
tracting metadata. It is therefore becoming more and
more common for HLT techniques to be incorporated
in document management platforms and used to sup-
port a librarian when he / she enters a new document
in the library. Clearly, it would be beneficial if such
a pipeline could identify the logical structure of the
documents being entered, and preserve it: this infor-
mation could be used by the document management
platform to, for instance, suggest the librarian the
most important keywords, find the text to be indexed
or even summarized, and produce citations lists, pos-
sibly to be compared with the digital library?s list of
citations to decide whether to add them.
We are in the process of developing a Portal
for Research in the Humanities (Portale Ricerca
Umanistica-PRU). This digital library will eventu-
ally include research articles about the Trentino re-
gion from Archeology, History, and History of Art.
So far, the pipeline to be discussed next has been
used to include in the library texts from the Italian
archeology journal Preistoria Alpina. One of our
goals was to develop a pipeline that could be used
7
http://citeseerx.ist.psu.edu/
8Still, especially multimedia documents with their possible
temporal and spatial relationships might need more sophisti-
cated methods.
56
whenever a librarian uploads an article in this digital
library, to identify title, authors, abstract, keywords,
content, and bibliographic references from the arti-
cle. The implemented portal already incorporates in-
formation extraction techniques that are used to iden-
tify in the ?content? part of the output of the pipeline
temporal expressions, locations, and entities such
as archeological sites, cultures, and artifacts. This
information is used to allow spatial, temporal, and
entity-based access to articles.
We are in the process of enriching the portal so
that title and author information are also used to au-
tomatically produce a bibliographical card for the ar-
ticle that will be entered in the PRU Library Catalog,
and bibliographical references are processed in or-
der to link the article to related articles and to the
catalog as well. The next step will be to modify the
pipeline (in particular, to modify the Named Entity
Recognition component) to include in the library ar-
ticles from other areas of research in the Humanities,
starting with History. There are also plans to make
it possible for authors themselves to insert their re-
search articles and books in the Portal, as done e.g.,
in the Semantics Archive.9.
We believe the functionalities offered by this por-
tal are or will become pretty standard in digital li-
braries, and therefore that the proposals discussed in
this paper could find an application beyond the use
in our Portal. We will also see below that a docu-
ment structure-sensitive pipeline can find other ap-
plications.
4 Turning an Existing Pipeline into One
that Extracts and Preserves Document
Structure
Most freely available HLT pipelines simply elimi-
nate markup during the initial phases of processing
in order to eliminate parts of the document struc-
ture that cannot be easily processed by their mod-
ules (e.g., bibliographic references), but this is not
appropriate for the Portal described in the previous
section, where different parts of the output of the
pipeline need to be processed in different ways. On
the other end, it was not really feasible to develop
a completely new pipeline from scratch. The ap-
proach we pursued in this work was to take an exist-
9
http://semanticsarchive.net/
ing pipeline and turn it into one which extracts and
outputs document structure. In this Section we dis-
cuss the approach we followed. In the next Section
we discuss the first pipeline we developed according
to this approach; then we discuss how the approach
was adopted for other purposes, as well.
Incorporating a document structure extractor in a
pipeline requires the solution of two basic problems:
where to insert the module, and how to pass on doc-
ument structure information. Concerning the first
issue, we decided to insert the document structure
parser after tokenization but before sentence process-
ing. In regards to the second issue, there are at
present three main formats for exchanging informa-
tion between elements of an HLT pipeline:
? inline, where each module inserts information
in a pre-defined fashion into the file received as
input;
? tabular format as done in CONLL, where to-
kens occupy the first column and each new
layer of information is annotated in a separate
new column, using the so-called IOB format
to represent bracketing (Ramshaw and Marcus,
1995);
? standoff format, where new layers of informa-
tion are stored in separate files.
The two main formats used by modern HLT pipelines
are tabular format, and inline or standoff XML for-
mat. Even though we will illustrate the problem of
preserving document structure in a pipeline of the
former type the PRU pipeline itself supports tabular
format and inline XML (TEI compliant).
The solution we adopted, illustrated in Figure 1,
involves using sentence headers to preserve docu-
ment structure information. In most pipelines using
a tabular interchange information, the output of a
module consists of a number of sentences each of
which consists of
? a header: a series of lines with a hash character
# at the beginning;
? a set of tab-delimited lines representing tokens
and token annotations;
? an empty EOF line.
57
 
# FILE: 11
# PART: id1
# SECTION: title
# FIELDS: token tokenstart sentence pos lemma entity nerType
Spondylus 0 - SPN Spondylus O B-SITE
gaederopus 10 - YF gaederopus O O
, 20 - XPW , O O
gioiello 22 - SS gioiello O O
dell ' 31 - E dell ' O O
Europa 36 - SPN europa B-GPE B-SITE
preistorica 43 - AS preistorico O O
. 55 <eos > XPS full_stop O O
# FILE: 11
# PART: id2
# SECTION: author
# FIELDS: token tokenstart sentence pos lemma entity nerType
MARIA 0 - SPN maria B-PER O
A 6 - E a I-PER O
BORRELLO 8 - SPN BORRELLO I-PER O
& 17 - XPO & O O
. 19 <eos > XPS full_stop O O
(TEI compliant inline XML snippet :)
<text >
<body >
<div type=" section" xml:lang="it">
[...]
<p id="p2" type=" author">
<s id="p2s1"><name key="PER1" type=" person">MARIA A BORRELLO </name >&.</s>
</p>
</div >
</body >
</text >
 
Figure 1: Using sentence headers to preserve document structure information. For illustration, the TEI compliant
inline XML snippet of the second sentence has been added.
58
The header in such pipelines normally specifies only
the file id (constant through the file), the number of
the sentence within the file, and the columns (see
Figure 1). This format however can also be used
to pass on document structure information provided
that the pipeline modules ignore all lines beginning
with a hash, as these lines can then be used to pro-
vide additional meta information. We introduce an
additional tag, SECTION, with the following mean-
ing: a line beginning with # SECTION: specifies the
position in the document structure of the following
sentence. Thus for instance, in Figure 1, the line
# SECTION: title
specifies that the following sentence is a title.
5 An Pipeline for Research Articles in
Archeology
The pipeline currently in use in the PRU Portal
we are developing is based on the strategy just dis-
cussed. In this Section We discuss the pipeline in
more detail.
5.1 Modules
The pipeline for processing archaeological articles
integrates three main modules: a module for recov-
ering the logical structure of the documents, a mod-
ule for Italian and English POS tagging and a gen-
eral Name Entity Recognizer and finally, a Gazetteer
Based Name Entity Recognizer. The architecture of
the system is presented in figure 2. Each module
except the first one takes as input the output of the
previous module in the sequence.
1. Text Extraction. This module extracts the text
from PDF documents. Text extraction from
PDF is a notoriously challenging task. We ex-
perimented with many software packages and
obtained the best results with pdftotext. This is
a component of XPDF, an open source viewer
for PDF documents. pdftotext allows the extrac-
tion of the text content of PDF documents in a
variety of encodings. The main drawback of the
text extractor is that it does not always preserve
the original text order.
2. Language Identification. The archeology
repository contains articles written in one of
the two languages: Italian or English. This
module uses the TextCat language guesser10 for
guessing the language of sentences. The lan-
guage identification task is complicated by the
fact that some articles contain text in both lan-
guages: for example, an article written in En-
glish may have an Italian abstract and/or an Ital-
ian conclusion.
3. Logical Structure Identification. This mod-
ule extracts the logical structure of a document.
For example, it identifies important parts like
the title, the authors, the main headers, tables
or figures. For this task we train the SectLa-
bel component of ParsCit on the articles in the
archeology repository. Details on the training
process, the tag set and the performance of the
module are provided in section 5.2.
4. Linguistic Processing. A set of modules in the
pipeline then perform linguistic processing on
specific parts of the document (the Bibliogra-
phy Section is excluded for example). First En-
glish or Italian POS is carried out as appropri-
ate, followed by English or Italian NER. NER
adaptation techniques have been developed to
identify non-standard types entities that are im-
portant in the domain, such as Archeological
Sites and Archeological Cultures. (This work
is discussed elsewhere.)
5. Reference Parsing. This module relies on
the output of ParsCit software to update the
Archeology Database Bibliography table with
the parsed references for each article. First,
each parsed reference is corrected in an auto-
matic post processing step. Then, the module
checks, using a simple heuristic, if the entry al-
ready exists in the table and updates the table,
if appropriate, with the new record.
Finally, the documents processed by the pipeline
are indexed using the Lucene search engine.
5.2 Training the Logical Document Structure
Identifier
As mentioned in Section 5, we use ParsCit to find the
logical structure of the documents in the archeology
10
http://odur.let.rug.nl/~vannoord/TextCat/
59
Figure 2: The pipeline of the system for PDF article processing in the Archeology Domain
domain. ParsCit comes with general CRF trained
models; unfortunately, they do not perform well on
archeology documents. There are some particulari-
ties of archeology repository articles that require the
retraining of the models. First, as said before, the
text extracted from PDF is not perfect. Second, the
archeology articles contain many figures with bilin-
gual captions. Third, the articles have portions of
the texts in both languages: Italian and English. To
improve the parsing performance two models are
trained: the first model should capture the logical
documents structure for those documents that have
Italian as main language but might contain portions
in English (like the abstract or summary). The sec-
ond model is trained with documents that have En-
glish as main language but might contain fragments
in Italian (like abstract or summary).
The document structure annotation was per-
formed by a student in the archeology department,
and was checked by one of the authors. In total 55
documents have been annotated (35 with Italian as
main language, 20 with English as main Language).
The tagset used for the annotation was specifically
devised for archeology articles. However, as it can
be seen below most of the devised tags can also be
found in general scientific articles. In Table 1 we
present the tag set used for annotation. The column
"Tag Count" gives the number of each tag in the an-
notated documents.
In general the meaning of the tags is self-
explanatory with the possible exception of the
tag VolumeInfo, which reports information for vol-
ume the article is part of. An annotation exam-
ple using this tag is: "<VolumeInfo> Preistoria
Alpina v. 38 (2002) Trento 2002 ISSN 0393-0157
</VolumeInfo>". The volume information can be
further processed by extracting the volume number,
the year of the issue and the International Standard
Serial Number (ISSN). To asses the performance of
the trained models we performed a five fold cross-
validation. The results are reported in the table 2
and are obtained for each tag using the F1 measure
(1):
F1 =
2?P?R
P+R
(1)
The results obtained for the Archeology articles
are in line with those obtained by the authors of
ParsCit and reported in (Luong et al, 2011). The
tag categories for which the performance of the sys-
tem is bad are the multilingual tags (e.g. ItalianAb-
stract or Italian Summary in articles where the main
language is English). We will address this issue in
the future by adapting the language identifier to label
multilingual documents. We also noticed that many
mis-tagged titles, notes or section headers are split
on multiple lines after the text extraction stage. The
system performance might be further improved if a
pre-processing step immediately after the text extrac-
tion is introduced.
60
Tag Tag Count
ItalianFigureCaption 456
ItalianBodyText 347
EnglishFigureCaption 313
SectionHeader 248
EnglishTableCaption 58
ItalianTableCaption 58
Author 71
AuthorEmail 71
AuthorAddress 65
SubsectionHeader 50
VolumeInfo 57
Bibliography 55
English Summary 31
ItalianKeywords 35
EnglishKeywords 35
Title 55
ItalianSummary 29
ItalianAbstract 10
Table 25
EnglishAbstract 13
Note 18
Table 1: The tag set used for Archeology Article Annota-
tion.
6 Additional Applications for
Structure-Sensitive Pipelines
The pipeline discussed above can be used for a va-
riety of other types of documents?archeology doc-
uments from other collections, or documents from
other domains?by simply replacing the document
structure extractor. We also found however that the
pipeline is useful for a variety of other text-analysis
tasks. We briefly discuss these in turn.
6.1 Blogs and Microblogging platforms
Content creation platforms like blogs, microblogs,
community QA sites, forums, etc., contain user gen-
erated data. This data may be emotional, opin-
ionated, personal, and sentimental, and as such,
makes it interesting for sentiment analysis, opinion
retrieval, and mood detection. In their survey on
opinion mining and sentiment analysis Pang and Lee
(2008) report that logical structure can be used to uti-
lize the relationships between different units of con-
tent, in order to achieve a more accurate labeling;
Tag F1
ItalianFigureCaption 70
ItalianBodyText 90
EnglishFigureCaption 71
SectionHeader 90
EnglishTableCaption 70
ItalianTableCaption 75
Author 72
AuthorEmail 75
AuthorAddress 73
SubsectionHeader 65
VolumeInfo 85
Bibliography 98
English Summary 40
ItalianKeywords 55
EnglishKeywords 56
Title 73
ItalianSummary 40
ItalianAbstract 50
Table 67
EnglishAbstract 50
Note 70
Table 2: The Precision and Recall for the trained models.
e.g. the relationships between discourse participants
in discussions on controversial topics when respond-
ing are more likely to be antagonistic than to be re-
inforcing, or the way of quoting?a user can refer to
another post by quoting part of it or by addressing
the other user by name or user ID?in posts on politi-
cal debates hints at the perceived opposite end of the
political spectrum of the quoted user.
We are in the process of creating an annotated cor-
pus of blogs; the pipeline discussed in this paper
was easily adapted to pre-process this type of data
as well.
6.2 HTML pages
In the IR literature it has often been observed that
certain parts of document structure contain infor-
mation that is particularly useful for document re-
trieval. For instance, Kruschwitz (2003) automati-
cally builds domain models ? simple trees of related
terms ? from documents marked up in HTML to
assist users during search tasks by performing auto-
matic query refinements, and improves users? experi-
61
ence for browsing the document collection. He uses
term counts in different markup contexts like non-
paragraph text and running text, and markups like
bold, italic, underline to identify concepts and the
corresponding shallow trees. However, this domain-
independent method is suited for all types of data
with logical structure annotation and similar data
sources can be found in many places, e.g. corporate
intranets, electronic archives, etc.
6.3 Processing Wikipedia pages
Wikipedia, as a publicly available web knowledge
base, has been leveraged for semantic information
in much work, including from our lab. Wikipedia
articles consist mostly of free text, but also con-
tain different types of structured information, e.g. in-
foboxes, categorization and geo information, links
to other articles, to other wiki projects, and to exter-
nal Web pages. Preserving this information is there-
fore useful for a variety of projects.
7 Discussion and Conclusions
The main point of this paper is to argue that the field
should switch to structure-sensitive pipelines. These
are particularly crucial in digital library applications,
but novel type of documents require them as well.
We showed that such extension can be achieved
rather painlessly even in tabular-based pipelines pro-
vided they allow for meta-lines.
References
Isaac G. Councill, C. Lee Giles, and Min-Yen Kan. 2008.
Parscit: An open-source crf reference string parsing
package. In Proceedings of the Language Resources
and Evaluation Conference (LREC 08), May.
Hamish Cunningham, Diana Maynard, Kalina Bontcheva,
and Valentin Tablan. 2002. GATE: A framework and
graphical development environment for robust NLP
tools and applications. In Proceedings of the 40th
Anniversary Meeting of the Association for Computa-
tional Linguistics.
Min-Yuh Day, Richard Tzong-Han Tsai, Cheng-Lung
Sung, Chiu-Chen Hsieh, Cheng-Wei Lee, Shih-Hung
Wu, Kun-Pin Wu, Chorng-Shyong Ong, and Wen-Lian
Hsu. 2007. Reference metadata extraction using a hi-
erarchical knowledge representation framework. Deci-
sion Support Systems, 43(1):152?167, February.
Erik Hetzner. 2008. A simple method for citation meta-
data extraction using hidden markov models. In Pro-
ceedings of the 8th ACM/IEEE-CS joint conference
on Digital libraries, JCDL ?08, pages 280?284, New
York, NY, USA. ACM.
Nancy Ide. 1998. Corpus encoding standard: SGML
guidelines for encoding linguistic corpora. In Proceed-
ings of LREC, pages 463?70, Granada.
Yasuto Ishitani. 1999. Logical structure analysis of doc-
ument images based on emergent computation. In
Proceedings of International Conference on Document
Analysis and Recognition.
Michael Jewell. 2000. Paracite: An overview.
Udo Kruschwitz. 2003. An Adaptable Search System for
Collections of Partially Structured Documents. IEEE
Intelligent Systems, 18(4):44?52, July.
Kyong-Ho Lee, Yoon-Chul Choy, and Sung-Bae Cho.
2003. Logical structure analysis and generation for
structured documents: a syntactic approach. IEEE
transactions on knowledge and data engineering,
15(5):1277?1294, September.
Minh-Thang Luong, Thuy Dung Nguyen, and Min-Yen
Kan. 2011. Logical structure recovery in scholarly
articles with rich document feature. Journal of Digital
Library Systems. Forthcoming.
Song Mao, Azriel Rosenfeld, and Tapas Kanungo. 2003.
Document Structure Analysis Algorithms: A Litera-
ture Survey.
Debashish Niyogi and Sargur N. Srihari. 1995.
Knowledge-based derivation of document logical
structure. In Proceedings of International Conference
on Document Analysis and Recognition, pages 472?
475.
Bo Pang and Lillian Lee. 2008. Opinion Mining and
Sentiment Analysis. Foundations and Trends in Infor-
mation Retrieval, 2(1-2):1?135, January.
Emanuele Pianta, Christian Girardi, and Roberto Zanoli.
2008. The TextPro tool suite. In LREC, 6th edition of
the Language Resources and Evaluation Conference,
Marrakech (Marocco).
Richard Power, Donia Scott, and Nadjet Bouayad-Agha.
2003. Document Structure. Computational Linguis-
tics, 29(2):211?260, June.
Lance A. Ramshaw and Mitchell P. Marcus. 1995. Text
chunking using tranformation-based learning. In Pro-
ceedings of Third ACL Workshop on Very Large Cor-
pora, pages 82?94.
Kristen M. Summers. 1998. Automatic discovery of log-
ical document structure. Ph.D. thesis, Cornell Univer-
sity.
Ian H. Witten, David Bainbridge, and David M. Nichols.
2003. How to build a digital library. Morgan Kauf-
mann.
62
Proceedings of EMNLP 2011, Conference on Empirical Methods in Natural Language Processing, pages 22?29,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
PaddyWaC: A Minimally-Supervised Web-Corpus of Hiberno-English
Brian Murphy
Centre for Mind/Brain Sciences,
University of Trento
38068 Rovereto (TN), Italy
brian.murphy@unitn.it
Egon Stemle
Centre for Mind/Brain Sciences,
University of Trento
38068 Rovereto (TN), Italy
egon.stemle@unitn.it
Abstract
Small, manually assembled corpora may be avail-
able for less dominant languages and dialects,
but producing web-scale resources remains a chal-
lenge. Even when considerable quantities of text
are present on the web, finding this text, and distin-
guishing it from related languages in the same region
can be difficult. For example less dominant vari-
ants of English (e.g. New Zealander, Singaporean,
Canadian, Irish, South African) may be found under
their respective national domains, but will be par-
tially mixed with Englishes of the British and US
varieties, perhaps through syndication of journalism,
or the local reuse of text by multinational compa-
nies. Less formal dialectal usage may be scattered
more widely over the internet through mechanisms
such as wiki or blog authoring. Here we automati-
cally construct a corpus of Hiberno-English (English
as spoken in Ireland) using a variety of methods: fil-
tering by national domain, filtering by orthographic
conventions, and bootstrapping from a set of Ireland-
specific terms (slang, place names, organisations).
We evaluate the national specificity of the resulting
corpora by measuring the incidence of topical terms,
and several grammatical constructions that are par-
ticular to Hiberno-English. The results show that
domain filtering is very effective for isolating text
that is topic-specific, and orthographic classification
can exclude some non-Irish texts, but that selected
seeds are necessary to extract considerable quanti-
ties of more informal, dialectal text.
1 Introduction
For less dominant language variants, corpora are usu-
ally painstakingly constructed by hand. This results in
high quality collections of text, classified and balanced
by genre, register and modality. But the process is time-
consuming and expensive, and results in relatively small
resources. For example the International Corpus of En-
glish (ICE) project (Greenbaum, 1996) has already re-
sulted in the publication of corpora covering ten dialects
of English, following a common schema, but the indi-
vidual corpora are limited to approximately one million
words.
An alternative is to use automatic methods to harvest
corpora from the Web. Identification of major languages
is a robust technology, and where the regional boundaries
of a language or dialect correspond closely to a national
top-level internet domain, very large collections (of sev-
eral billion words) can now can be produced easily, with
close to no manual intervention (Baroni et al, 2009).
These methods can also deal with some issues of text
quality found on the web, successfully extracting coher-
ent pieces of running text from web pages (i.e. discard-
ing menu text, generic headings, copyright and other le-
gal notices), reducing textual duplication, and identifying
spam, portal pages and other files that do not contain lin-
guistically interesting text.
Corpora of minor languages that lack their own do-
main, but that have clear orthographic differences from
more dominant neighbouring languages can be collected
automatically by using a small set of seed documents,
from which language-specific search terms can be ex-
tracted (Scannell, 2007). These methods, combined with
automated language identification methods, can quickly
produce large, clean collections with close to no manual
intervention.
However for language variants that do not have their
own domain (e.g. Scots, Bavarian), it is less clear
that such web corpora can be automatically constructed.
Smaller or politically less dominant countries that do
have their own domain (e.g. Belgium, New Zealand),
may also find the language of their ?national? web
strongly influenced by other language varieties, for ex-
ample through syndication of journalistic articles, or ma-
terials published by foreign companies.
In this paper we use minimally supervised methods
(Baroni and Bernardini, 2004; Baroni et al, 2009) to
quickly and cheaply build corpora of Hiberno-English
(English as spoken in Ireland), which are many times
larger than ICE-Ireland, the largest published collection
22
currently available (Kallen and Kirk, 2007). We investi-
gate several combinations of strategies (based on domain
names, and on regional variations in vocabulary and or-
thography) to distinguish text written in this minor lan-
guage variant from related dominant variants (US and UK
English). We validate the specificity of the resulting cor-
pora by measuring the incidence of Ireland-specific lan-
guage, both topically (the frequency with which Irish re-
gions and organisations are mentioned), and structurally,
by the presence of grammatical constructions that are par-
ticular to Hiberno-English. We also compare our cor-
pus to another web-corpus of Hiberno-English that is in
development (Cr?bad?n, Scannell, personal communica-
tion) that relies on domain filtering of crawled web-pages.
The results show that filtering by national domain is
very effective in identifying text that deals with Irish top-
ics, but that the grammar of the resulting text is largely
standard. Using a set of seed terms tailored to the lan-
guage variant (Irish slang, names of Ireland-based organ-
isations, loanwords from Irish Gaelic), yields text which
is much more particular to Hiberno-English usage. At the
same time, such tailored seed terms increase the danger
of finding ?non-authentic? uses of Irishisms (sometimes
termed paddywhackery or oirish), either in fictional di-
alogues, or in documents discussing distinctive patterns
in Irish English. The application of a British/American
spelling filter has less clear effects, increasing topical
incidence slightly, while reducing structural incidences
somewhat.
The paper proceeds as follows: in the next section we
introduce Hiberno-English, situating it relative to other
variants of English, and concentrating on the characteris-
tic features that will be used as metrics of ?Irishness? of
text retrieved from the Web. Next we describe the process
by which several candidate corpora of Hiberno-English
were constructed (section 3), and the methods we used
to quantify incidence of distinctive usage (section 4). In
the final two sections we compare the incidence of these
markers with those found in corpora of other variants
of English (UK, US), Scannell?s IE-domain filtered cor-
pus, and a hand-crafted corpus of Hiberno-English (ICE-
Ireland), and reflect on the wider applicability of these
methods to variants of other languages and orthographies.
2 Structures and Lexicon of
Hiberno-English
Hiberno-English differs in a range of ways from other
varieties of English. In broad terms it can be grouped
with British English, in that its lexicon, grammar and or-
thographic conventions are more similar to that of Great
Britain, than to that of North America. For example with
lexical variants such as bumper/fender, rubbish bin/trash
can, lift/elevator and zed/zee it shares the former British
usage rather than the latter American usage, though there
are exceptions (in Irish usage the North Americans term
truck is replacing the British lorry). Similarly in syntax
it tends to follow British conventions, for instance He?s
familiar with X rather than X is familiar to him, write to
me rather than write me and the acceptability of singu-
lar verbal marking with group subjects, as in the team are
pleased ? though there are counterexamples again, in that
Irish English tends to follow American dialects in dis-
pensing with the shall/will distinction. Most obviously,
Irish writing uses British spellings rather than American
spellings.
However, there are still dialectal differences between
Irish and British English. Beyond the usual regional dif-
ferences that one might find between the words used in
different parts of England, the English spoken in Ireland
is particularly influenced by the Irish language (Gaelic,
Gaeilge) (Kirk and Kallen, 2007). While English is the
first language of the overwhelming majority of residents
of Ireland (estimates of Irish mother-tongue speakers are
of the order of 50,000, or about 1% of the population),
Irish retains status as the first official language of the Re-
public of Ireland, maintained as a core subject at all levels
of school education, and through state-maintained radio
and television channels. As recently as the early 19th
century, Irish was the majority language, and so many
traces of it remain in modern Hiberno-English, in the
form of Irish loan-words (e.g. sl?n ?goodbye?, gaelscoil
?Irish (speaking) school?), Anglicizations (e.g. ?gansey?,
jumper, from Irish geansa?), and composites (e.g. ?jack-
een?, a pejorative term for Dubliners, combining the Irish
diminutive -?n with the English ?Jack?).
In this paper we take a series of characteristic terms
and structures from Hiberno-English, mostly inspired by
(Kirk and Kallen, 2007), and use them as markers of the
Irishness of the text we assemble from the web. While
there are many more interesting grammatical differences
between Hiberno-English and other variants (e.g. per-
fective use of the simple present: I know that family for
years), we restrict ourselves to those that can be automat-
ically identified in a corpus through searching of plain
text, or of shallow syntactic patterns (parts of speech).
The first marker we use is to measure the incidence of
a set of terms that are topically related to Ireland: proper
names of Ireland-based organisations, and geographical
terms. The method for assembling this list is described in
section 4.
The most simple structure that we use as a marker of
Hiberno-English is the contraction I amn?t (I?m not or I
ain?t in other varieties). The next is the ?after? perfec-
tive, which often expresses immediacy, and a negative
outcome:
(1) I?m after losing my wallet
?I just lost my wallet?
23
A further structure that is novel from the point of view
of other variants of English is a particular use of verbs
that take a complement that expresses a question (most
commonly ask, wonder, see and know), without the use
of a complementizer such as if or whether and with an
inversion of subject-verb order (typical of interrogatives):
(2) I wonder is he coming?
?I wonder if/whether he is coming?
Finally we consider the expanded usage of reflexive pro-
nouns in Hiberno-English, where they may be used for
emphasis, in any argument position, and without be-
ing anaphorically bound, as is usually required. Here
we limit ourselves to subject position reflexives, which
can be identified from word order patterns, without any
deeper semantic analysis:
(3) himself is in big trouble
?he is in big trouble?
With the exception of the amn?t contraction, all of these
phenomena are demonstrated by (Kirk and Kallen, 2007)
to be common in the ICE-Ireland corpus, though some-
what less common in Northern Irish portion of that col-
lection, and to be very rare or completely absent in
the ICE-GB corpus of the English of Britain (Nelson
et al, 2002). Significantly, these constructions are found
predominantly in the spoken language portion of the
ICE-Ireland corpus, suggesting that speakers are perhaps
aware that they are not ?standard? English, and so not
considered appropriate in the written register.
3 Constructing a Web-Corpus of
Hiberno-English
Within the WaCky initiative (Web-as-Corpus kool ynitia-
tive) (Baroni and Bernardini, 2006) a community of lin-
guists and information technology specialists developed
a set of tools to selectively crawl sections of the Web, and
then process, index and search the resulting data. Contri-
butions like BootCaT (Baroni and Bernardini, 2004), an
iterative procedure to bootstrap specialised corpora and
terms from the Web, have been successfully used in a
range of projects: first in the construction of the WaCky
corpora, a collection of very large (>1 billion words) cor-
pora of English (ukWaC), German (deWaC) and Italian
(itWaC); and subsequently by other groups, e.g. noWaC
and jpWaC (Baroni et al, 2009; Guevara, 2010; Erjavec
et al, 2008).
Here we use BootCaT to build seven prototype corpora
of Hiberno-English, and evaluate the dialect-specificity
of each by measuring the incidence of proper terms and
constructions that are associated with this language vari-
ant. Additionally, we use ukWaC as the de-facto stan-
dard British English Web corpus, and construct a medium
size web-corpus of the US domain to represent Ameri-
can usage. Each corpus is preprocessed and formatted for
the IMS Open Corpus Workbench (CWB, (Christ, 1994;
Web, 2008)), a generic query engine for large text corpora
that was developed for applications in computational lex-
icography.
BootCaT first takes a set of manually assembled seed
terms, these (possibly multi-word) terms are randomly
combined, and then are used as search queries with a
Web search engine; the HTML documents of the top re-
sults are downloaded and cleaned to extract running text
and discard all web-markup. Preprocessing and format-
ting for the CWB consists of tokenising, lemmatising,
and part-of-speech tagging the corpus, and then convert-
ing the result into CWB?s internal format; we replicated
the processing stages employed for ukWaC.
The construction of the nine corpora differs on three
dimensions:
Seeds: two seed sets were used namely, an Hiberno-
English one (IEs), and the original ukWaC list of
mid-frequency terms (UKs) from the British Na-
tional Corpus (Burnard, 1995); the Irish seeds were
used in pairs and triples to attempt to vary the degree
of regional specificity.
TLDs: two types of top-level internet domain (TLD) re-
strictions were imposed during (or after) the con-
struction of the corpora; either no restriction was im-
posed (.ALL), or a corpus was filtered by a specific
national TLD (e.g. .ie).
Spelling: two types of spelling filter were imposed;
either none, or an ?orthographic convention fac-
tor? (OCF) was calculated to detect American and
British spellings, and a corpus was filtered accord-
ingly (BrEn).
The IE seeds contained 81 seed terms, gathered using
one author?s native intuition, and words indicated as be-
ing specific to Irish English by the Oxford English Dic-
tionary, and from various Web pages about Hiberno-
English. 76 single-word and 5 two-word terms were used
falling into three main categories: Irish place names, re-
gional variant terms (mostly slang), and load words from
Irish Gaelic (many being state institutions). The full list-
ing of terms is given here:
Place names: Dublin, Galway, Waterford, Drogheda, Antrim, Derry,
Kildare, Meath, Donegal, Armagh, Wexford, Wicklow,
Louth, Kilkenny, Westmeath, Offaly, Laois, Belfast, Cavan,
Sligo, Roscommon, Monaghan, Fermanagh, Carlow, Longford,
Leitrim, Navan, Ennis, Tralee, Leinster, Connaught, Munster, Ul-
ster
Regional variants: banjaxed (wrecked), craic (fun), fecking (variant
of fucking), yoke (thing), yer man/one/wan (that man/woman),
culchie (country dweller), da (father), footpath (pavement),
24
gaff (home), gobshite (curse), gurrier (young child), jack-
een (Dubliner), jacks (toilet), janey mac (exclamation), jaysus
(variant of exclamation ?jesus?), kip (sleep; hovel), knacker
(Traveller, gypsy), knackered (wrecked), langer (penis; id-
iot), langers/langered (drunk), scallion (spring onion), skanger
(disgusting person), strand (beach, seaside), scuttered (drunk),
boreen (small road), gob (mouth; spit), eejit (variant of idiot),
lough (lake), fooster (dawdle), barmbrack (traditional Hallow?een
cake), shebeen (unlicensed bar), bogman (contry dweller), old
one (old lady), quare (variant queer), gansey (pullover)
Loan words: garda, garda? (police), taoiseach (prime minister), d?il
(parliament), Sl?inte (?cheers?), Gaeltacht (Irish speaking areas),
Seanad (senate), T?naiste (deputy prime minister), ceol ((tradi-
tional Irish) music), sl?n (?goodbye?), gr? (affection, love for),
gaelscoil (Irish speaking school)
These seed terms were combined into a set of 3000 3-
tuple (3T) and a set of 3000 2-tuple (2T) search queries,
i.e. two-word terms were enclosed in inverted commas to
form one single term for the search engine. For 3T this re-
sulted in over 80% 3-tuples with 3 single-word terms, and
slightly over 17% with 2 single-word terms, and the re-
maining percentages for 3-tuples with 1 single-word and
no single-word terms; for 2T this resulted in almost 88%
2-tuples with 2 single-word terms, almost 12% with only
1 single-word terms, and less than 1% with no single-
word terms. The UK seeds were the original ones used
during the construction of the ukWaC corpus and they
were combined into 3000 3-tuple search queries.
No TLD restriction means that the search engine was
not instructed to return search results within a specific
domain, and hence, documents originate from typical
English-language domains (.com, .ie, .uk, etc.) but also
from .de and potentially any other. A restriction meant
that the documents could only originate from one TLD.
No spelling filter means that nothing was done. The
OCF indicates the degree to which terms within a docu-
ment are predominantly spelled according to one prede-
fined word list relative to another. The number of term
intersections with each list is counted and OCF is calcu-
lated as the difference between counts over their sum. To
simplify matters, we utilised a spell-checker to return the
list of known words from a document, this corresponds to
checking a document for spelling errors and only keeping
the non-erroneous words. In our case we used an en_GB
dictionary, an en_US one, and the two together. The three
lists yield the needed numbers of words only known by
one of the two dictionaries, and, hence unknown by the
other dictionary, and the ratio in the range of [?1,+1] can
be calculated.
The search engine we used for all queries was Yahoo
(Yahoo! Inc., 1995); for all search queries English results
were requested, that is we relied on the search engine?s
built-in language identification algorithm1, and from all
1This restriction is very effective at distinguishing non-English from
English content, but returns content from any English variant.
search queries the top 10 results were used. Cleaning
of the Web pages (termed boilerplate removal) was ac-
complished by BootCaT?s implementation of the BTE
method (Finn et al, 2001); it strives to extract the main
body of a Web page, that is the largest contiguous text
area with the least amount of intervening non-text ele-
ments (HTML tags), and discards the rest.
Several corpora were constructed from the Irish seeds
using 2- or 3-tuple search terms: either without restrict-
ing the TLDs; subsequent restriction to the .ie TLD; or
subsequent filtering according to spelling. Corpora were
also constructed with the search engine instructed to di-
rectly return documents from the .us or the .ie TLD, re-
spectively, where the latter one was later also filtered ac-
cording to spelling. The ukWaC corpus is restricted to
the .uk TLD.
4 Evaluating Variety Specificity of the
Corpus
To evaluate the dialectal specificity of the text in each pu-
tative corpus of Hiberno-English, we measured the inci-
dence of several characteristic terms and structures. The
same phenomena were counted in corpora of US and UK
English (identified as that found under the .us and .uk
TLDs respectively) to establish baseline frequencies. All
corpora were HTML-cleaned, lemmatised and part-of-
speech tagged using the same methods described above,
and searches were made with identical, case-insensitive,
queries in the CQP language.
First we quantified topical specificity by searching
for a set of Irish geographical terms (towns, counties,
regions), and Ireland-based organisations (companies,
NGOs, public-private bodies), to identify text which is
?about Ireland?. There were 80 terms, evenly split be-
tween the two categories. In this list we avoided proper
names which are orthographically identical to content
words (e.g. Down, Cork, Clones, Trim, Limerick, Mal-
low, Mayo), given names (Clare, Kerry, Tyrone), place
names found in other territories (Baltimore, Skibbereen,
Newbridge, Westport, Passage West), or names that
might be found as common noun-phrases (e.g. Horse
Racing Ireland, Prize Bond Company, Electricity Supply
Board). While political terms might have been appropri-
ate markers (e.g. the political party Fianna F?il; the par-
liamentary speaker the Ceann Comhairle), the seed terms
we used contained many governmental institutions, and
so this could be considered an unfairly biased diagnostic
marker. The full list of terms is given below.
Topical terms: ActionAid, Aer, Aer, Allied, An, Arklow, Athlone,
Athy, Balbriggan, Ballina, Ballinasloe, Bantry, Bord, Bord, Bord,
Buncrana, Bundoran, Bus, Carrick-on-Suir, Carrickmacross,
Cashel, Castlebar, Christian, Clonakilty, Clonmel, Cobh, Coillte,
Comhl(?|a)mh, Connacht, C(?|o)ras, Donegal, Dublin, Dublin,
Dungarvan, Eircom, EirGrid, Enniscorthy, Fermoy, Fyffes, Glan-
25
bia, Gorta, Grafton, Greencore, Iarnr(?|o)d, IONA, Irish, Irish,
Irish, Kerry, Kilkee, Kilrush, Kinsale, Laois, Leixlip, Let-
terkenny, Listowel, Listowel, Loughrea, Macroom, Mullingar,
Naas, Nenagh, Oxfam, Paddy, Portlaoise, Radi(o|?), Ryanair,
Telif(?|i)s, Templemore, Thurles, Tipperary, Tramore, Trinity,
Tr(?|o)caire, Tuam, Tullamore, Tullow, Vhi, Waterford, Youghal
For the structural markers we used more conservative
query patterns where appropriate, to minimise false pos-
itives. For this reason the incidence figures given here
should be considered lower estimates of the frequency of
these structures, but they allow us to establish an inde-
pendent metric with a minimum of manual intervention.
As mentioned above, for the emphatic use of reflex-
ives, we searched only in the subject verb configuration,
even though these are possible in other argument posi-
tions also (e.g. I saw himself in the pub yesterday). The
query was restricted to reflexive pronouns (other than it-
self ) found at the start of a sentence, or immediately after
a conjunction, and directly before a finite verb (other than
have or be). The CQP query (4) yields examples such as
(5)-(7).
(4) [pos="CC" | pos="SENT"] [lemma=".+self" &
lemma!="itself"] [pos="VV[ZD]?"];
(5) ... more commonplace or didactic, less
imaginative? Himself added, "You are a romantic
idiot, and I love you more than...
(6) ... Instruments in Lansing, Michigan, where Val
and Don and myself taught bouzouki, mandolin,
guitar and fiddle workshops. It is a...
(7) ... game of crazy golf, except this time it was
outdoor. Conor and myself got bored straight away
so we formed our own game while Mike ...
For the ?after? perfective construction, we searched for a
pattern of a personal pronoun (i.e. not including it, this,
that), the lexeme after, and a gerund form of a common
verb (other than have, be). The query (8) allowed for
a modal auxiliary, and for intervening adverbs, as illus-
trated in (9)-(11).
(8) [pos="PP" & word!="it" %c & word!="that" %c &
word!="this" %c] [pos="RB.*"]* [lemma="be"]
[pos="RB.*"]* [word="after"] [pos="RB.*"]*
[pos="V[VH]G"]
(9) ... the holy angels on your head, young fellow. I
hear tell you?re after winning all in the sports
below; and wasn?t it a shame I didn?t ...
(10) ... MICHAEL ? Is the old lad killed surely?
PHILLY. I?m after feeling the last gasps quitting
his heart. MICHAEL ? Look at ...
(11) ... placards with the words ?Blind as a Batt? and
?Batman you are after robbing us?. They came
from as far away as Wexford and called ...
The use of embedded inversions in complements was
queried for the same four verbs identified by (Kirk and
Kallen, 2007): ask, see, wonder and know. Other verbs
were considered, by expansion from these four via Levin
verb classes (Levin, 1993), but preliminary results gave
many false positives. The query used search for one of
these four verbs, followed by a form of the verb be, and
then a personal pronoun specific to the subject position
(12). Examples of the instances extracted are given be-
low (13)-(15).
(12) [pos="VV.*" & lemma="(ask|know|see|wonder)"
%c] [lemma="be"] [word="(I|he|she|we|they)" %c];
(13) ... but that is the reality. I remember as a young
child being asked was I a Protestant or a Catholic:
that?s the worst thing ...
(14) ... unless I get 170+, there isn?t a chance. And then
I wonder am I mad even applying for medicine.
Anyway anyone else who?s...
(15) There was the all important question and she was
dying to know was he a married man or a widower
who had lost his wife or some ...
Finally, examples of the amn?t contraction (17)-(19) were
extracted with the simple case-insensitive query (16).
(16) "am" "n?t";
(17) Hi I?m relatively new to CCTV but work in IT and
so amn?t 100 % lost ! Anyway, I have already set
up a personal ...
(18) ... and plaster, with some pride.) It was he did that,
and amn?t I a great wonder to think I ?ve traced
him ten days with ...
(19) ?I will indeed Mrs. R, thanks very much, sure
amn?t I only parchin?? Ye needn?t have gone to the
trouble of ...
It should be noted that these structural usages differ in the
degree to which they are perceived as distinctive. While
speakers of Irish English may not be aware that amn?t
and the embedded inversion construction are dialectally
restricted, many do know that the after and reflexive con-
structions are particular to Ireland. Hence by searching
for these constructions our evaluation is biased towards
colloquial language and consciously dialectal usage.
26
5 Results
As can be seen in the first two rows of table 1, consider-
ably large Irish corpora were gathered with ease, and even
after applying several subsequent filtering strategies, the
smallest corpus was several times the size of the manually
assembled ICE-Ireland corpus.
Figure 1 (left panel) further shows that the strategy of
searching by random seed combinations yielded pages
in many domains, with a considerable proportion being
in the .ie domain, but by no means the majority. This
suggests that Ireland specific usage of English is not re-
stricted to the national internet domain, i.e. the .ie TLD.
The relative proportion of .ie domain pages (see right
panel of same figure) was increased by selecting only
pages which had predominantly British orthography, sug-
gesting that this has some efficacy in eliminating texts
written in American English.
Table 1 also shows the absolute incidence of each
of the five characteristic phenomena considered. All
matches returned by the CQP search queries were man-
ually evaluated, to ensure that they were authentic ex-
amples of the constructions in question (for the larger
ukWaC corpus only a random sample were examined).
Numbers of false positives that were excluded are shown
in brackets, such as the examples from ukWaC below:
(20) ... just as they were after receiving secret briefings
from Health Commission Wales officers.
(21) All I know is they?re getting cold.
The bars in sets one and two show figures for the man-
ually compiled ICE-Ireland corpus, and the Cr?bad?n
web-corpus. The ICE-Ireland numbers differ somewhat
from those reported in that paper (Kirk and Kallen, 2007),
since we used more selective search strategies (note that
the cut-off reported relative incidences reach about 21 per
mil. tokens), which would miss some examples such as
those below which have the after construction without a
personal pronoun, and have the non-reflexive use in ob-
ject position, respectively:
(22) There?s nothing new after coming in anyway so
(23) Again it?s up to yourself which type of pricing
policy you use
It should also be noted that ICE-Ireland, following the
standard scheme for the International Corpus of English
project (Greenbaum, 1996), is biased towards spoken lan-
guage, with written text only making up only 40% of the
total text.
The relative incidence (per million tokens) of Ireland-
specific topics and constructions is summarised in figure
2. The bars in sets three and four demonstrate that these
same characteristics, very common in Hiberno-English as
evidenced by the ICE-Ireland, appear to be exceedingly
rare in UK and US English. Unsurprisingly, web authors
in the US and UK domains do not write often about Irish
places and organisations. But constructions that are pu-
tatively exclusive to Hiberno-English are seldom found.
Those that are found might be explained by the effect
of language contact with Irish immigrants to those coun-
tries, and the fact that text by Irish authors may be found
in these domains, whether those people are resident in
those countries or not. For instance in the example below,
the given name Ronan suggests that the author might be
of Irish extraction:
(24) At about that point Cardinal Cormac of
Westminster walked right past us and Ronan and
myself went to say hello to him and tell him we
were up here from his diocese.
The sets headed ?.ie? show the figures for the corpora we
constructed by querying seed terms within the Irish na-
tional domain. The incidence of characteristic features
of Hiberno-English grammar are higher than those seen
in the US and UK domains, similar to that seen in the
Cr?bad?n corpus, and lower than in the ICE-Ireland cor-
pus, perhaps reflecting the fact that these constructions
are less common in written Hiberno-English. Subsequent
filtering out of pages with dominance of American En-
glish spelling (?.ie, BrEn?) does not have much effect on
the numbers.
The ?Irish Seeds (IEs)? bars show that the use of tai-
lored seed terms returns text which has a similar topical
specificity to that in the .ie domain generally, but which
shows more structural characteristics of Hiberno-English.
These results can also be improved upon, first by concen-
trating on the .ie domain portion of the tailored-seeds ex-
tracted pages (?Irish Seeds (IEs), IE Dom (.ie)?) which
boosts topical specificity. Filtering instead by orthogra-
phy (?IEs, BrEn?) seems to strike a happy medium, in-
creasing incidence in all categories.
However returning to table 1, it is apparent that there
are many false positives among the constructions found
using Irish seed terms. This was caused by the search
strategy retrieving a small number of pages on the topic of
Hiberno-English, that contained many constructed exam-
ples of the structures of interest. The same corpora con-
tained smaller numbers of examples from theatre scripts
and other fiction.
6 Discussion
The results show us that our methods can be effective in
extracting text that is both specific to Irish topics, and in-
cludes instances of constructions that are particular to the
variety of English spoken in Ireland. The incidences rel-
ative to corpus size are not as high as those seen in the
27
Table 1: Corpora sizes, incidences of Ireland terms and constructions; absolute numbers (false positives in brackets)
IC
E
-I
re
la
nd
C
ru
ba
da
n
uk
W
aC
U
K
s,
3T
,.
us
U
K
s,
3T
,.
ie
U
K
s,
3T
,.
ie
,B
rE
n
IE
s,
3T
,.
A
L
L
IE
s,
3T
,.
A
L
L
,.
ie
IE
s,
3T
,.
A
L
L
,B
rE
n
IE
s,
2T
,.
A
L
L
IE
s,
2T
,.
ie
Size (in 106 Tokens) 1.1 46.3 2119.9 74.7 17.8 15.0 25.2 2.6 17.3 18.4 6.4
Size (in 103 Docs) 0.5 43.0 2692.6 4.6 2.0 1.6 3.4 0.7 2.5 7.3 2.3
Ireland Terms 194 17330 12743 82 14199 13802 23527 7264 22071 12454 9935
"after" Construction 7 (-4) 12 (2) 48 (72) 1 (2) 11 (1) 7 (1) 26 (50) 2 (1) 11 (47) 14 (38) 9 (1)
"amn?t" Construction 0 (0) 0 (0) 32 (0) 0 (0) 0 (0) 0 (0) 5 (45) 1 (1) 2 (43) 6 (36) 0 (0)
embedded Inversions 24 (-18) 18 (5) 42 (309) 0 (15) 5 (2) 5 (0) 20 (4) 2 (1) 17 (2) 4 (1) 5 (0)
Subject Reflexives 22 (-19) 33 (0) 1797 (115) 35 (8) 15 (1) 10 (0) 39 (0) 2 (0) 30 (0) 17 (3) 8 (1)
Figure 1: Domain composition of Irish-Seed based Corpora
Top?Level Domains
Numb
er of D
ocum
ents (T
otal:3
382)
0
500
1000
1500
com ie uk org net edu au info Others Top?Level Domains
Numb
er of D
ocum
ents (T
otal:2
485)
0
200
400
600
800
1000
1200
com ie uk org net info au ca Others
Figure 2: Relative Incidences of Ireland terms and constructions, per million words (grey bars indicating the original counts before
manual inspection), in each copus
ICE-Ir
eland Cruba
dan ukWa
C
UKs, 
3T, .u
s
UKs, 
3T, .ie
UKs, 
3T, .ie
, BrEn
IEs, 3
T, .AL
L
IEs, 3
T, .AL
L, .ie
IEs, 3
T, .AL
L, BrE
n
IEs, 2
T, .AL
L
IEs, 2
T, .ie
0
1
2
3
4
5
6
7 "after" Construction"amn't" Constructionembedded InversionsSubject RefelxivesIreland Terms (right Scale)
0
5
10
15
20
25
30
35
40
28
manually constructed ICE-Ireland corpus. We can specu-
late on the reasons for this. It may be in part due to ?pollu-
tion? of our corpus with non-Irish English, via syndicated
journalism (e.g. some Irish newspapers are repackaging
of British newspapers with added Irish content), or via
multinational organisations with bases in Ireland. In our
view the main explanatory factor is that of modality and
register. The ICE-Ireland corpus is predominantly spoken
(~60%), with many texts coming from informal settings
(unscripted speeches, face to face and telephone conver-
sations). One reading of the figures which is consistent
with this viewpoint is that the .ie domain corpora contain
proportionally more high register, edited text (e.g. from
governmental and commercial organisations, for which
the use of the .ie domain may be an important part of cor-
porate identity), and that the tailored-seed corpora con-
tain more text contributed by individuals (forums, blogs,
etc), for whom domain endings are of little consequence.
Nevertheless, the use of Hiberno-English specific seed
terms did reveal higher incidences of distinctive Irish us-
ages than simple domain filtering.
But despite these lower incidences, in absolute terms
our corpora provide many more examples of Hiberno-
English than that were hitherto available. For example
the ICE-Ireland corpus contains a total of seven examples
of the ?after? construction, while with our Irish-seeds de-
rived corpus, and using a fairly restrictive query pattern,
we isolated 26 examples of this structure. Further the
size of these pilot corpora were kept intentionally lim-
ited, a small fraction of the approximately 150 million .ie
domain pages indexed by Google. Much larger corpora
could be constructed with relative ease, by using a larger
seed set, or with an interactive seed-discovery method,
where the text from the first round of web-harvesting
could be analysed to identify further terms that are com-
paratively specific to Hiberno-English (relative to corpora
of other varieties of English), in a similar fashion to the
methods discussed in (Scannell, 2007).
In terms of wider implications, the fact that seeds tai-
lored to a particular region and language variant is as ef-
fective as filtering by domain, is encouraging for dialects
and minority languages that lack a dedicated internet do-
main. This suggest that for less-dominant language vari-
ants without distinctive established orthographies (e.g.
Scots, Andalusian, Bavarian), large corpora displaying
characteristic features of that variant can be constructed
in a simple automatic manner with minimal supervision
(a small set of seeds provided by native speakers). Our
methods might also prove useful for dialects in which a
standard variant is dominant in the written language (e.g.
Arabic, Chinese). One might expect that the written Ara-
bic in the .ma (Morocco) domain would differ little from
that in the .qa domain (Qatar) despite the large differences
in vernacular speech. Similarly the grammar and vocabu-
lary of Chinese written in Mainland Chinese, Taiwanese,
Hong Kong and Singaporese domains (ignoring orthog-
raphy) might be less representative of the variation in ev-
eryday language. The use of regional slang and proper
names may help one to collect more examples of this
more natural language usage, and less of the dominant
standard variant.
References
Baroni, M. and Bernardini, S. (2004). BootCaT: Bootstrapping
corpora and terms from the web. In (ELRA), E. L. R. A.,
editor, Proceedings of LREC 2004, Lisbon: ELDA., pages
1313?1316.
Baroni, M. and Bernardini, S., editors (2006). Wacky! Working
papers on the Web as Corpus.
Baroni, M., Bernardini, S., Ferraresi, A., and Zanchetta, E.
(2009). The WaCky wide web: a collection of very large
linguistically processed web-crawled corpora. Language Re-
sources and Evaluation, 43(3):209?226.
Burnard, L. (1995). Users Reference Guide, British National
Corpus, Version 1.0. Oxford University Computing Ser-
vices/British National Corpus Consortium, Oxford.
Christ, O. (1994). A Modular and Flexible Architecture for an
Integrated Corpus Query System. In Papers in Computa-
tional Lexicography (COMPLEX ?94), pages 22?32.
Erjavec, I. S., Erjavec, T., and Kilgarriff, A. (2008). A web cor-
pus and word sketches for Japanese. Information and Media
Technologies, 3:529?551.
Finn, A., Kushmerick, N., and Smyth, B. (2001). Fact or fic-
tion: Content classification for digital libraries.
Greenbaum, S. (1996). Comparing English Worldwide.
Clarendon Press.
Guevara, E. (2010). NoWaC: a large web-based corpus for Nor-
wegian. In Proceedings of the Sixth Web as Corpus Work-
shop (WAC6), pages 1?7. The Association for Computational
Linguistics.
Kallen, J. and Kirk, J. (2007). ICE-Ireland: Local variations on
global standards. In Beal, J. C., Corrigan, K. P., and Moisl,
H. L., editors, Creating and Digitizing Language Corpora:
Synchronic Databases, volume 1, pages 121?162. Palgrave
Macmillan, London.
Kirk, J. and Kallen, J. (2007). Assessing Celticity in a Corpus
of Irish Standard English. In The Celtic languages in con-
tact: papers from the workshop within the framework of the
XIII International Congress of Celtic Studies, Bonn, 26-27
July 2007, page 270.
Levin, B. (1993). English Verb Classes and Alternations. Uni-
versity of Chicago Press, Chicago.
Nelson, G., Wallis, S., and Aarts, B. (2002). Exploring natural
language: working with the British component of the Inter-
national Corpus of English. John Benjamins.
Scannell, K. (2007). The Cr?bad?n project: Corpus building
for under-resourced languages. In Fairon, C., Naets, H., Kil-
garriff, A., and de Schryver, G.-M., editors, Building and Ex-
ploring Web Corpora: Proceedings of the 3rd Web as Corpus
Workshop, volume 4, pages 5?15.
Web (2008). The IMS Open Corpus Workbench (CWB).
Yahoo! Inc. (1995). The Yahoo! Internet search engine.
29
Felix Bildhauer & Roland Sch?fer (eds.), Proceedings of the 9th Web as Corpus Workshop (WaC-9) @ EACL 2014, pages 36?43,
Gothenburg, Sweden, April 26 2014.
c?2014 Association for Computational Linguistics
The PAIS
`
A Corpus of Italian Web Texts
Verena Lyding
?
verena.lyding@eurac.edu
Egon Stemle
?
egon.stemle@eurac.edu
Claudia Borghetti
?
claudia.borghetti@unibo.it
Marco Brunello
?
marcobrunello84@gmail.com
Sara Castagnoli
?
s.castagnoli@unibo.it
Felice Dell?Orletta
?
felice.dellorletta@ilc.cnr.it
Henrik Dittmann
?
henrik.dittmann@bordet.be
Alessandro Lenci
?
alessandro.lenci@ling.unipi.it
Vito Pirrelli
?
vito.pirrelli@ilc.cnr.it
Abstract
PAIS
`
A is a Creative Commons licensed,
large web corpus of contemporary Italian.
We describe the design, harvesting, and
processing steps involved in its creation.
1 Introduction
This paper provides an overview of the PAIS
`
A cor-
pus of Italian web texts and an introductory de-
scription of the motivation, procedures and facili-
ties for its creation and delivery.
Developed within the PAIS
`
A project, the cor-
pus is intended to meet the objective to help over-
come the technological barriers that still prevent
web users from making use of large quantities of
contemporary Italian texts for language and cul-
tural education, by creating a comprehensive and
easily accessible corpus resource of Italian.
The initial motivation of the initiative stemmed
from the awareness that any static repertoire of
digital data, however carefully designed and de-
veloped, is doomed to fast obsolescence, if con-
tents are not freely available for public usage, con-
tinuously updated and checked for quality, incre-
mentally augmented with new texts and annota-
tion metadata for intelligent indexing and brows-
ing. These requirements brought us to design a
resource that was (1) freely available and freely
re-publishable, (2) comprehensively covering con-
temporary common language and cultural content
and (3) enhanced with a rich set of automatically-
annotated linguistic information to enable ad-
vanced querying and retrieving of data. On top
?
EURAC Research Bolzano/Bozen, IT
?
University of Bologna, IT
?
University of Leeds, UK
?
Institute of Computational Linguistics ?Antonio Zam-
polli? - CNR, IT
?
Institut Jules Bordet, BE
?
University of Pisa, IT
of that, we set out to develop (4) a dedicated in-
terface with a low entry barrier for different target
groups. The end result of this original plan repre-
sents an unprecedented digital language resource
in the Italian scenario.
The main novelty of the PAIS
`
A web corpus is
that it exclusively draws on Creative Commons li-
censed data, provides advanced linguistic annota-
tions with respect to corpora of comparable size
and corpora of web data, and invests in a carefully
designed query interface, targeted at different user
groups. In particular, the integration of richly an-
notated language content with an easily accessible,
user-oriented interface makes PAIS
`
A a unique and
flexible resource for language teaching.
2 Related Work
The world wide web, with its inexhaustible
amount of natural language data, has become an
established source for efficiently building large
corpora (Kilgarriff and Grefenstette, 2003). Tools
are available that make it convenient to bootstrap
corpora from the web based on mere seed term
lists, such as the BootCaT toolkit (Baroni and
Bernardini, 2004). The huge corpora created by
the WaCky project (Baroni et al., 2009) are an ex-
ample of such an approach.
A large number of papers have recently been
published on the harvesting, cleaning and pro-
cessing of web corpora.
1
However, freely avail-
able, large, contemporary, linguistically anno-
tated, easily accessible web corpora are still miss-
ing for many languages; but cf. e.g. (G?en?ereux
et al., 2012) and the Common Crawl Foundations
(CCF) web crawl
2
.
1
cf. the Special Interest Group of the Association for
Computational Linguistics on Web as Corpus (SIGWAC)
http://sigwac.org.uk/
2
CCF produces and maintains a repository of web crawl
data that is openly accessible: http://commoncrawl.
org/
36
3 Corpus Composition
3.1 Corpus design
PAIS
`
A aimed at creating a comprehensive corpus
resource of Italian web texts which adheres to the
criteria laid out in section 1. For these criteria to
be fully met, we had to address a wide variety of
issues covering the entire life-cycle of a digital text
resource, ranging from robust algorithms for web
navigation and harvesting, to adaptive annotation
tools for advanced text indexing and querying and
user-friendly accessing and rendering online inter-
faces customisable for different target groups.
Initially, we targeted a size of 100M tokens, and
planned to automatically annotate the data with
lemma, part-of-speech, structural dependency, and
advanced linguistic information, using and adapt-
ing standard annotation tools (cf. section 4). In-
tegration into a querying environment and a dedi-
cated online interface were planned.
3.2 Licenses
A crucial point when planning to compile a cor-
pus that is free to redistribute without encounter-
ing legal copyright issues is to collect texts that are
in the public domain or at least, have been made
available in a copyleft regime. This is the case
when the author of a certain document decided to
share some rights (copy and/or distribute, adapt
etc.) on her work with the public, in a way that
end users do not need to ask permission to the cre-
ator/owner of the original work. This is possible
by employing licenses other than the traditional
?all right reserved? copyright, i.e. GNU, Creative
Commons etc., which found a wide use especially
on the web. Exploratory studies (Brunello, 2009)
have shown that Creative Commons licenses are
widely employed throughout the web (at least on
the Italian webspace), enough to consider the pos-
sibility to build a large corpus from the web ex-
clusively made of documents released under such
licenses.
In particular, Creative Commons provides five
basic ?baseline rights?: Attribution (BY), Share
Alike (SA), Non Commercial (NC), No Deriva-
tive Works (ND). The licenses themselves are
composed of at least Attribution (which can be
used even alone) plus the other elements, al-
lowing six different combinations:
3
(1) Attribu-
tion (CC BY), (2) Attribution-NonCommercial
3
For detailed descriptions of each license see http://
creativecommons.org/licenses/
(CC BY-NC), (3) Attribution-ShareAlike (CC BY-
SA), (4) Attribution-NoDerivs (CC BY-ND), (5)
Attribution-NonCommercial-ShareAlike (CC BY-
NC-SA), and (6) Attribution-NonCommercial-
NoDerivs (CC BY-NC-ND).
Some combinations are not possible because
certain elements are not compatible, e.g. Share
Alike and No Derivative Works. For our purposes
we decided to discard documents released with the
two licenses containing the No Derivative Works
option, because our corpus is in fact a derivative
work of collected documents.
3.3 The final corpus
The corpus contains approximately 388,000 docu-
ments from 1,067 different websites, for a total of
about 250M tokens. All documents contained in
the PAIS
`
A corpus date back to Sept./Oct. 2010.
The documents come from several web sources
which, at the time of corpus collection, provided
their content under Creative Commons license
(see section 3.2 for details). About 269,000 texts
are from Wikimedia Foundation projects, with
approximately 263,300 pages from Wikipedia,
2380 pages from Wikibooks, 1680 pages from
Wikinews, 740 pages from Wikiversity, 410 pages
from Wikisource, and 390 Wikivoyage pages.
The remaining 119,000 documents come
from guide.supereva.it (ca. 19,000),
italy.indymedia.org (ca. 10,000) and
several blog services from more than another
1,000 different sites (e.g. www.tvblog.it
(9,088 pages), www.motoblog.it (3,300),
www.ecowebnews.it (3,220), and
www.webmasterpoint.org (3,138).
Texts included in PAIS
`
A have an average length
of 683 words, with the longest text
4
counting
66,380 running tokens. A non exhaustive list of
average text lengths by source type is provided in
table 1 by way of illustration.
The corpus has been annotated for lemma, part-
of-speech and dependency information (see sec-
tion 4.2 for details). At the document level, the
corpus contains information on the URL of origin
and a set of descriptive statistics of the text, includ-
ing text length, rate of advanced vocabulary, read-
ability parameters, etc. (see section 4.3). Also,
each document is marked with a unique identifier.
4
The European Constitution from wikisource.org:
http://it.wikisource.org/wiki/Trattato_
che_adotta_una_Costituzione_per_l?Europa
37
Document source Avg text length
PAIS
`
A total 683 words
Wikipedia 693 words
Wikibooks 1844 words
guide.supereva.it 378 words
italy.indymedia.it 1147 words
tvblog.it 1472 words
motoblog.it 421 words
ecowebnews.it 347 words
webmasterpoint.org 332 words
Table 1: Average text length by source
The annotated corpus adheres to the stan-
dard CoNLL column-based format (Buchholz and
Marsi, 2006), is encoded in UTF-8.
4 Corpus Creation
4.1 Collecting and cleaning web data
The web pages for PAIS
`
A were selected in two
ways: part of the corpus collection was made
through CC-focused web crawling, and another
part through a targeted collection of documents
from specific websites.
4.1.1 Seed-term based harvesting
At the time of corpus collection (2010), we used
the BootCaT toolkit mainly because collecting
URLs could be based on the public Yahoo! search
API
5
, including the option to restrict search to CC-
licensed pages (including the possibility to specify
even the particular licenses). Unfortunately, Ya-
hoo! discontinued the free availability of this API,
and BootCaT?s remaining search engines do not
provide this feature.
An earlier version of the corpus was collected
using the tuple list originally employed to build
itWaC
6
. As we noticed that the use of this list, in
combination with the restriction to CC, biased the
final results (i.e. specific websites occurred very
often as top results) , we provided as input 50,000
medium frequent seed terms from a basic Italian
vocabulary list
7
, in order to get a wider distribu-
tion of search queries, and, ultimately, of texts.
As introduced in section 3.2, we restricted the
selection not just to Creative Commons-licensed
5
http://developer.yahoo.com/boss/
6
http://wacky.sslmit.unibo.it/doku.
php?id=seed_words_and_tuples
7
http://ppbm.paravia.it/dib_lemmario.
php
texts, but specifically to those licenses allowing
redistribution: namely, CC BY, CC BY-SA, CC
BY-NC-SA, and CC BY-NC.
Results were downloaded and automatically
cleaned with the KrdWrd system, an environment
for the unified processing of web content (Steger
and Stemle, 2009).
Wrongly CC-tagged pages were eliminated us-
ing a black-list that had been manually populated
following inspection of earlier corpus versions.
4.1.2 Targeted
In September 2009, the Wikimedia Foundation de-
cided to release the content of their wikis under
CC BY-SA
8
, so we decided to download the large
and varied amount of texts made available through
the Italian versions of these websites. This was
done using the Wikipedia Extractor
9
on official
dumps
10
of Wikipedia, Wikinews, Wikisource,
Wikibooks, Wikiversity and Wikivoyage.
4.2 Linguistic annotation and tools
adaptation
The corpus was automatically annotated with
lemma, part-of-speech and dependency infor-
mation, using state-of-the-art annotation tools
for Italian. Part-of-speech tagging was per-
formed with the Part-Of-Speech tagger described
in Dell?Orletta (2009) and dependency-parsed by
the DeSR parser (Attardi et al., 2009), using Mul-
tilayer Perceptron as the learning algorithm. The
systems used the ISST-TANL part-of-speech
11
and dependency tagsets
12
. In particular, the pos-
tagger achieves a performance of 96.34% and
DeSR, trained on the ISST-TANL treebank con-
sisting of articles from newspapers and period-
icals, achieves a performance of 83.38% and
87.71% in terms of LAS (labelled attachment
score) and UAS (unlabelled attachment score) re-
spectively, when tested on texts of the same type.
However, since Gildea (2001), it is widely ac-
knowledged that statistical NLP tools have a drop
of accuracy when tested against corpora differing
from the typology of texts on which they were
trained. This also holds true for PAIS
`
A: it contains
8
Previously under GNU Free Documentation License.
9
http://medialab.di.unipi.it/wiki/
Wikipedia_Extractor
10
http://dumps.wikimedia.org/
11
http://www.italianlp.it/docs/
ISST-TANL-POStagset.pdf
12
http://www.italianlp.it/docs/
ISST-TANL-DEPtagset.pdf
38
lexical and syntactic structures of non-canonical
languages such as the language of social media,
blogs, forum posts, consumer reviews, etc. As re-
ported in Petrov and McDonald (2012), there are
multiple reasons why parsing the web texts is dif-
ficult: punctuation and capitalization are often in-
consistent, there is a lexical shift due to increased
use of slang and technical jargon, some syntactic
constructions are more frequent in web text than
in newswire, etc.
In order to overcome this problem, two main ty-
pologies of methods and techniques have been de-
veloped: Self-training (McClosky et al., 2006) and
Active Learning (Thompson et al., 1999).
For the specific purpose of the NLP tools adap-
tation to the Italian web texts, we adopted two dif-
ferent strategies for the pos-tagger and the parser.
For what concerns pos-tagging, we used an active
learning approach: given a subset of automatically
pos-tagged sentences of PAIS
`
A, we selected the
ones with the lowest likelihood, where the sen-
tence likelihood was computed as the product of
the probabilities of the assignments of the pos-
tagger for all the tokens. These sentences were
manually revised and added to the training corpus
in order to build a new pos-tagger model incor-
porating some new knowledge from the target do-
main.
For what concerns parsing, we used a self-
training approach to domain adaptation described
in Dell?Orletta et al. (2013), based on ULISSE
(Dell?Orletta et al., 2011). ULISSE is an unsu-
pervised linguistically-driven algorithm to select
reliable parses from a collection of dependency
annotated texts. It assigns to each dependency
tree a score quantifying its reliability based on a
wide range of linguistic features. After collect-
ing statistics about selected features from a cor-
pus of automatically parsed sentences, for each
newly parsed sentence ULISSE computes a reli-
ability score using the previously extracted feature
statistics. From the top of the parses (ranked ac-
cording to their reliability score) different pools of
parses were selected to be used for training. The
new training contains the original training set as
well as the new selected parses which include lex-
ical and syntactic characteristics specific of the tar-
get domain (Italian web texts). The parser trained
on this new training set improves its performance
when tested on the target domain.
We used this domain adaptation approach for
the following three main reasons: a) it is unsuper-
vised (i.e. no need for manually annotated training
data); b) unlike the Active Learning approach used
for pos-tagging, it does not need manual revision
of the automatically parsed samples to be used for
training; c) it was previously tested on Italian texts
with good results (Dell?Orletta et al., 2013).
4.3 Readability analysis of corpus documents
For each corpus document, we calculated several
text statistics indicative of the linguistic complex-
ity, or ?readability? of a text.
The applied measures include, (1) text length in
tokens, that is the number of tokens per text, (2)
sentences per text, that is a sentence count, and (3)
type-token ratio indicated as a percentage value.
In addition, we calculated (4) the advanced vo-
cabulary per text, that is a word count of the text
vocabulary which is not part of the the basic Ital-
ian vocabulary (?vocabolario di base?) for written
texts, as defined by De Mauro (1991)
13
, and (5)
the Gulpease Index (?Indice Gulpease?) (Lucisano
and Piemontese, 1988), which is a measure for the
readability of text that is based on frequency rela-
tions between the number of sentences, words and
letters of a text.
All values are encoded as metadata for the cor-
pus. Via the PAIS
`
A online interface, they can
be employed for filtering documents and building
subcorpora. This facility was implemented with
the principal target group of PAIS
`
A users in mind,
as the selection of language examples according
to their readability level is particularly relevant for
language learning and teaching.
4.4 Attempts at text classification for genre,
topic, and function
Lack of information about the composition of cor-
pora collected from the web using unsupervised
methods is probably one of the major limitations
of current web corpora vis-`a-vis more traditional,
carefully constructed corpora, most notably when
applications to language teaching and learning are
envisaged. This also holds true for PAIS
`
A, es-
13
The advanced vocabulary was calculated on the ba-
sis of a word list consisting of De Mauro?s ?vocabolario
fondamentale? (http://it.wikipedia.org/wiki/
Vocabolario_fondamentale) and ?vocabolario
di alto uso? (http://it.wikipedia.org/wiki/
Vocabolario_di_alto_uso), together with high
frequent function words not contained in those two lists.
39
pecially for the harvested
14
subcorpus that was
downloaded as described in section 4.1. We there-
fore carried out some experiments with the ulti-
mate aim to enrich the corpus with metadata about
text genre, topic and function, using automated
techniques.
In order to gain some insights into the com-
position of PAIS
`
A, we first conducted some man-
ual investigations. Drawing on existing literature
on web genres (e.g. (Santini, 2005; Rehm et al.,
2008; Santini et al., 2010)) and text classification
according to text function and topic (e.g. (Sharoff,
2006)), we developed a tentative three-fold taxon-
omy to be used for text classification. Following
four cycles of sample manual annotation by three
annotators, categories were adjusted in order to
better reflect the nature of PAIS
`
A?s web documents
(cf. (Sharoff, 2010) about differences between do-
mains covered in the BNC and in the web-derived
ukWaC). Details about the taxonomy are provided
in Borghetti et al. (2011). Then, we started to
cross-check whether the devised taxonomy was
indeed appropriate to describe PAIS
`
A?s composi-
tion by comparing its categories with data result-
ing from the application of unsupervised methods
for text classification.
Interesting insights have emerged so far re-
garding the topic category. Following Sharoff
(2010), we used topic modelling based on La-
tent Dirichlet Allocation for the detection of top-
ics: 20 clusters/topics were identified on the ba-
sis of keywords (the number of clusters to re-
trieve is a user-defined parameter) and projected
onto the manually defined taxonomy. This re-
vealed that most of the 20 automatically iden-
tified topics could be reasonably matched to
one of the 8 categories included in the tax-
onomy; exceptions were represented by clus-
ters characterised by proper nouns and gen-
eral language words such bambino/uomo/famiglia
(?child?/?man?/?family?) or credere/sentire/sperare
(?to believe?/?feel?/?hope?), which may in fact be
indicative of genres such as diary or personal com-
ment (e.g. personal blog). Only one of the cate-
gories originally included in the taxonomy ? natu-
ral sciences ? was not represented in the clusters,
which may indicate that there are few texts within
PAIS
`
A belonging to this domain. One of the ma-
14
In fact, even the nature of the targeted texts is not pre-
cisely defined: for instance, Wikipedia articles can actually
encompass a variety of text types such as biographies, intro-
ductions to academic theories etc. (Santini et al., 2010, p. 15)
jor advantages of topic models is that each corpus
document can be associated ? to varying degrees ?
to several topics/clusters: if encoded as metadata,
this information makes it possible not only to fil-
ter texts according to their prevailing domain, but
also to represent the heterogeneous nature of many
web documents.
5 Corpus Access and Usage
5.1 Corpus distribution
The PAIS
`
A corpus is distributed in two ways: it is
made available for download and it can be queried
via its online interface. For both cases, no restric-
tions on its usage apply other than those defined
by the Creative Commons BY-NC-SA license. For
corpus download, both the raw text version and the
annotated corpus in CoNLL format are provided.
The PAIS
`
A corpus together with all project-
related information is accessible via the project
web site at http://www.corpusitaliano.it
5.2 Corpus interface
The creation of a dedicated open online interface
for the PAIS
`
A corpus has been a declared primary
objective of the project.
The interface is aimed at providing a power-
ful, effective and easy-to-employ tool for mak-
ing full use of the resource, without having to go
through downloading, installation or registration
procedures. It is targeted at different user groups,
particularly language learners, teachers, and lin-
guists. As users of PAIS
`
A are expected to show
varying levels of proficiency in terms of language
competence, linguistic knowledge, and concern-
ing the use of online search tools, the interface
has been designed to provide four separate search
components, implementing different query modes.
Initially, the user is directed to a basic keyword
search that adopts a ?Google-style? search box.
Single search terms, as well as multi-word combi-
nations or sequences can be searched by inserting
them in a simple text box.
The second component is an advanced graph-
ical search form. It provides elaborated search
options for querying linguistic annotation layers
and allows for defining distances between search
terms as well as repetitions or optionally occurring
terms. Furthermore, the advanced search supports
regular expressions.
The third component emulates a command-line
search via the powerful CQP query language of
40
the Open Corpus Workbench (Evert and Hardie,
2011). It allows for complex search queries in
CQP syntax that rely on linguistic annotation lay-
ers as well as on metadata information.
Finally, a filter interface is presented in a fourth
component. It serves the purpose of retriev-
ing full-text corpus documents based on keyword
searches as well as text statistics (see section 4.3).
Like the CQP interface, the filter interface is also
supporting the building of temporary subcorpora
for subsequent querying.
By default, search results are displayed as
KWIC (KeyWord In Context) lines, centred
around the search expression. Each search hit can
be expanded to its full sentence view. In addition,
the originating full text document can be accessed
and its source URL is provided.
Based on an interactive visualisation for depen-
dency graphs (Culy et al., 2011) for each search
result a graphical representations of dependency
relations together with the sentence and associated
lemma and part-of-speech information can be gen-
erated (see Figure 1).
Figure 1: Dependency diagram
Targeted at novice language learners of Italian,
a filter for automatically restricting search results
to sentences of limited complexity has been in-
tegrated into each search component. When ac-
tivated, search results are automatically filtered
based on a combination of the complexity mea-
sures introduced in section 4.3.
5.3 Technical details
The PAIS
`
A online interface has been developed in
several layers: in essence, it provides a front-end
to the corpus as indexed in Open Corpus Work-
bench (Evert and Hardie, 2011). This corpus
query engine provides the fundamental search ca-
pabilities through the CQP language. Based on
the CWB/Perl API that is part of the Open Corpus
Workbench package, a web service has been de-
veloped at EURAC which exposes a large part of
the CQP language
15
through a RESTful API.
16
The four types of searches provided by the on-
line interface are developed on top of this web ser-
vice. The user queries are translated into CQP
queries and passed to the web service. In many
cases, such as the free word order queries in the
simple and advanced search forms, more than one
CQP query is necessary to produce the desired
result. Other functionalities implemented in this
layer are the management of subcorpora and the
filtering by complexity. The results returned by
the web service are then formatted and presented
to the user.
The user interface as well as the mechanisms
for translation of queries from the web forms into
CQP have been developed server-side in PHP.
The visualizations are implemented client-side in
JavaScript and jQuery, the dependency graphs
based on the xLDD framework (Culy et al., 2011).
5.4 Extraction of lexico-syntactic information
PAIS
`
A is currently used in the CombiNet project
?Word Combinations in Italian ? Theoretical and
descriptive analysis, computational models, lexi-
cographic layout and creation of a dictionary?.
17
The project goal is to study the combinatory prop-
erties of Italian words by developing advanced
computational linguistics methods for extracting
distributional information from PAIS
`
A.
In particular, CombiNet uses a pattern-based
approach to extract a wide range of multiword
expressions, such as phrasal lexemes, colloca-
tions, and usual combinations. POS n-grams
are automatically extracted from PAIS
`
A, and then
ranked according to different types of associa-
tion measures (e.g., pointwise mutual informa-
tion, log-likelihood ratios, etc.). Extending the
LexIt methodology (Lenci et al., 2012), CombiNet
also extracts distributional profiles from the parsed
layer of PAIS
`
A, including the following types of
information:
1. syntactic slots (subject, complements, modi-
15
To safeguard the system against malicious attacks, secu-
rity measures had to be taken at several of the layers, which
unfortunately also make some of the more advanced CQP fea-
tures inaccessible to the user.
16
Web services based on REST (Representational State
Transfer) principles employ standard concepts such as a URI
and standard HTTP methods to provide an interface to func-
tionalities on a remote host.
17
3-year PRIN(2010/2011)-project, coordination by Raf-
faele Simone ? University of Rome Tre
41
fiers, etc.) and subcategorization frames;
2. lexical sets filling syntactic slots (e.g. proto-
typical subjects of a target verb);
3. semantic classes describing selectional pref-
erences of syntactic slots (e.g. the direct obj.
of mangiare/?to eat? typically selects nouns
referring to food, while its subject selects an-
imate nouns); semantic roles of predicates.
The saliency and typicality of combinatory pat-
terns are weighted by means of different statisti-
cal indexes and the resulting profiles will be used
to define a distributional semantic classification of
Italian verbs, comparable to the one elaborated in
the VerbNet project (Kipper et al., 2008).
6 Evaluation
We performed post-crawl evaluations on the data.
For licensing, we analysed 200,534 pages that
were originally collected for the PAIS
`
A corpus,
and only 1,060 were identified as containing no
CC license link (99.95% with CC mark-up). Then,
from 10,000 randomly selected non-CC-licensed
Italian pages 15 were wrongly identified as CC li-
censed containing CC mark-up (0.15% error). For
language identification we checked the harvested
corpus part with the CLD2 toolkit
18
, and > 99%
of the data was identified as Italian.
The pos-tagger has been adapted to peculiari-
ties of the PAIS
`
A web texts, by manually correct-
ing sample annotation output and re-training the
tagger accordingly. Following the active learning
approach as described in section 4.2 we built a new
pos-tagger model based on 40.000 manually re-
vised tokens. With the new model, we obtained
an improvement in accuracy of 1% on a test-set
of 5000 tokens extracted from PAIS
`
A. Final tag-
ger accuracy reached 96.03%.
7 Conclusion / Future Work
In this paper we showed how a contemporary and
free language resource of Italian with linguistic
annotations can be designed, implemented and de-
veloped from the web and made available for dif-
ferent types of language users.
Future work will focus on enriching the cor-
pus with metadata by means of automatic clas-
sification techniques, so as to make a better as-
sessment of corpus composition. A multi-faceted
18
Compact Language Detection 2, http://code.
google.com/p/cld2/
approach combining linguistic features extracted
from texts (content/function words ratio, sentence
length, word frequency, etc.) and information
extracted from document URLs (e.g., tags like
?wiki?, ?blog?) might be particularly suitable for
genre and function annotation.
Metadata annotation will enable more advanced
applications of the corpus for language teaching
and learning purposes. In this respect, existing
exemplifications of the use of the PAIS
`
A inter-
face for language learning and teaching (Lyding et
al., 2013) could be followed by further pedagogi-
cal proposals as well as empowered by dedicated
teaching guidelines for the exploitation of the cor-
pus and its web interface in the class of Italian as
a second language.
In a more general perspective, we envisage
a tighter integration between acquisition of new
texts, automated text annotation and development
of lexical and language learning resources allow-
ing even non-specialised users to carve out and
develop their own language data. This ambitious
goal points in the direction of a fully-automatised
control of the entire life-cycle of open-access Ital-
ian language resources with a view to address an
increasingly wider range of potential demands.
Acknowledgements
The three years PAIS
`
A project
19
, concluded in
January 2013, received funding from the Italian
Ministry of Education, Universities and Research
(MIUR)
20
, by the FIRB program (Fondo per gli
Investimenti della Ricerca di Base)
21
.
References
G. Attardi, F. Dell?Orletta, M. Simi, and J. Turian.
2009. Accurate dependency parsing with a stacked
multilayer perceptron. In Proc. of Evalita?09, Eval-
uation of NLP and Speech Tools for Italian, Reggio
Emilia.
M. Baroni and S. Bernardini. 2004. Bootcat: Boot-
strapping corpora and terms from the web. In Proc.
of LREC 2004, pages 1313?1316. ELDA.
M. Baroni, S. Bernardini, A. Ferraresi, and
E. Zanchetta. 2009. The wacky wide web: A
collection of very large linguistically processed
19
An effort of four Italian research units: University of
Bologna, CNR Pisa, University of Trento and European
Academy of Bolzano/Bozen.
20
http://www.istruzione.it/
21
http://hubmiur.pubblica.istruzione.
it/web/ricerca/firb
42
web-crawled corpora. Journal of LRE, 43(3):209?
226.
C. Borghetti, S. Castagnoli, and M. Brunello. 2011. I
testi del web: una proposta di classificazione sulla
base del corpus pais`a. In M. Cerruti, E. Corino,
and C. Onesti, editors, Formale e informale. La vari-
azione di registro nella comunicazione elettronica.,
pages 147?170. Carocci, Roma.
M. Brunello. 2009. The creation of free linguistic cor-
pora from the web. In I. Alegria, I. Leturia, and
S. Sharoff, editors, Proc. of the Fifth Web as Corpus
Workshop (WAC5), pages 9?16. Elhuyar Fundazioa.
S. Buchholz and E. Marsi. 2006. CoNLL-X Shared
Task on Multilingual Dependency Parsing. In Proc.
Tenth Conf. Comput. Nat. Lang. Learn., number
June in CoNLL-X ?06, pages 149?164. Association
for Computational Linguistics.
C. Culy, V. Lyding, and H. Dittmann. 2011. xldd:
Extended linguistic dependency diagrams. In Proc.
of the 15th International Conference on Information
Visualisation IV2011, pages 164?169, London, UK.
T. De Mauro. 1991. Guida all?uso delle parole. Edi-
tori Riuniti, Roma.
F. Dell?Orletta, G. Venturi, and S. Montemagni. 2011.
Ulisse: an unsupervised algorithm for detecting re-
liable dependency parses. In Proc. of CoNLL 2011,
Conferences on Natural Language Learning, Port-
land, Oregon.
F. Dell?Orletta, G. Venturi, and S. Montemagni. 2013.
Unsupervised linguistically-driven reliable depen-
dency parses detection and self-training for adapta-
tion to the biomedical domain. In Proc. of BioNLP
2013, Workshop on Biomedical NLP, Sofia.
F. Dell?Orletta. 2009. Ensemble system for part-of-
speech tagging. In Proceedings of Evalita?09, Eval-
uation of NLP and Speech Tools for Italian, Reggio
Emilia.
S. Evert and A. Hardie. 2011. Twenty-first century
corpus workbench: Updating a query architecture
for the new millennium. In Proc. of the Corpus Lin-
guistics 2011, Birmingham, UK.
M. G?en?ereux, I. Hendrickx, and A. Mendes. 2012.
A large portuguese corpus on-line: Cleaning and
preprocessing. In PROPOR, volume 7243 of Lec-
ture Notes in Computer Science, pages 113?120.
Springer.
A. Kilgarriff and G. Grefenstette. 2003. Introduction
to the special issue on the web as corpus. Computa-
tional Linguistics, 29(3):333?347.
K. Kipper, A. Korhonen, N. Ryant, and M. Palmer.
2008. A large-scale classification of english verbs.
Journal of LRE, 42:21?40.
A. Lenci, G. Lapesa, and G. Bonansinga. 2012. Lexit:
A computational resource on italian argument struc-
ture. In N. Calzolari, K. Choukri, T. Declerck,
M. U?gur Do?gan, B. Maegaard, J. Mariani, J. Odijk,
and S. Piperidis, editors, Proc. of LREC 2012, pages
3712?3718, Istanbul, Turkey, May. ELRA.
P. Lucisano and M. E. Piemontese. 1988. Gulpease:
una formula per la predizione della difficolt dei testi
in lingua italiana. Scuola e citt`a, 39(3):110?124.
V. Lyding, C. Borghetti, H. Dittmann, L. Nicolas, and
E. Stemle. 2013. Open corpus interface for italian
language learning. In Proc. of the ICT for Language
Learning Conference, 6th Edition, Florence, Italy.
D. McClosky, E. Charniak, and M. Johnson. 2006.
Reranking and self-training for parser adaptation. In
Proc. of ACL 2006, ACL, Sydney.
S. Petrov and R. McDonald. 2012. Overview of the
2012 shared task on parsing the web. In Proc. of
SANCL 2012, First Workshop on Syntactic Analysis
of Non-Canonical Language, Montreal.
G. Rehm, M. Santini, A. Mehler, P. Braslavski,
R. Gleim, A. Stubbe, S. Symonenko, M. Tavosanis,
and V. Vidulin. 2008. Towards a reference corpus of
web genres for the evaluation of genre identification
systems. In Proc. of LREC 2008, pages 351?358,
Marrakech, Morocco.
M. Santini, A. Mehler, and S. Sharoff. 2010. Riding
the Rough Waves of Genre on the Web. Concepts
and Research Questions. In A. Mehler, S. Sharoff,
and M. Santini, editors, Genres on the Web: Compu-
tational Models and Empirical Studies., pages 3?33.
Springer, Dordrecht.
M. Santini. 2005. Genres in formation? an ex-
ploratory study of web pages using cluster analysis.
In Proc. of the 8th Annual Colloquium for the UK
Special Interest Group for Computational Linguis-
tics (CLUK05), Manchester, UK.
S. Sharoff. 2006. Creating General-Purpose Corpora
Using Automated Search Engine Queries. In M. Ba-
roni and S. Bernardini, editors, Wacky! Working
Papers on the Web as Corpus, pages 63?98. Gedit,
Bologna.
S. Sharoff. 2010. Analysing similarities and differ-
ences between corpora. In 7th Language Technolo-
gies Conference, Ljubljana.
J. M. Steger and E. W. Stemle. 2009. KrdWrd ? The
Architecture for Unified Processing of Web Content.
In Proc. Fifth Web as Corpus Work., Donostia-San
Sebastian, Basque Country.
C. A. Thompson, M. E. Califf, and R. J. Mooney. 1999.
Active learning for natural language parsing and in-
formation extraction. In Proc. of ICML99, the Six-
teenth International Conference on Machine Learn-
ing, San Francisco, CA.
43
