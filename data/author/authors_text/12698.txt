Creation of a New Domain and Evaluation of Comparison Generation in a
Natural Language Generation System
Matthew Marge
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
mrmarge@cs.cmu.edu
Amy Isard
ICCS/HCRC
School of Informatics
University of Edinburgh
Amy.Isard@ed.ac.uk
Johanna Moore
ICCS/HCRC
School of Informatics
University of Edinburgh
J.Moore@ed.ac.uk
Abstract
We describe the creation of a new domain for
the Methodius Natural Language Generation
System, and an evaluation of Methodius? pa-
rameterized comparison generation algorithm.
The new domain was based around music and
performers, and texts about the domain were
generated using Methodius. Our evaluation
showed that test subjects learned more from
texts that contained comparisons than from
those that did not. We also established that the
comparison generation algorithm could gener-
alize to the music domain.
1 Introduction
There has been research into tailoring natural lan-
guage to a user?s previous browsing history in a va-
riety of domains such as medicine, museum col-
lections, and animal descriptions (McKeown, 1985;
Milosavljevic, 1997; Dale et al, 1998; O?Donnell
et al, 2001). Another domain in which this could
be applied is automated disc jockeys (DJs) that ac-
company a music stream such as Pandora1 and dis-
cuss interesting trivia or facts about music tracks re-
cently played to the user. User modeling could make
these texts much more natural and less repetitive,
and comparisons and contrasts between music artists
or tracks could also provide users with a novel way
to explore their music collection.
The Methodius system (Isard, 2007) continues
in a line of research which began with ILEX
(O?Donnell et al, 2001) and continued with M-
PIRO (Isard et al, 2003) and now also NaturalOWL
1http://www.pandora.com
(Galanis and Androutsopoulos, 2007). Like these
other systems, Methodius creates customizable de-
scriptions of objects from an database, but it features
a novel algorithm for generating comparisons be-
tween a new object and objects that have previously
been encountered, which stands out from previous
research in this area because it uses several explicit
parameters to choose the most relevant and interest-
ing comparisons given the context (Isard, 2007).
There have been previous evaluations of some of
these systems, including (Cox et al, 1999; Karasi-
mos and Isard, 2004). Karasimos and Isard con-
ducted an evaluation of comparisons and aggrega-
tion in the M-PIRO system. The results showed that
participants learned more and perceived that they
learned more from texts that contained comparisons
and aggregations than they did from texts that did
not. In this study, we investigate whether these re-
sults generalize to our new domain, and we isolate
the effect of comparisons from that of aggregation.
2 Knowledge Base Construction
2.1 Corpus Collection
We collected a small corpus to investigate the type
of facts disc jockeys tend to say about music. We se-
lected two genres where music descriptions between
pieces were common, jazz and classical music. The
programmes we used were broadcast on BBC Ra-
dio Three2. We transcribed sixty-four discussions;
to maintain uniformity, we followed the Linguistic
Data Consortium?s transcription guidelines3. This
2http://www.bbc.co.uk/radio3
3http://projects.ldc.upenn.edu/Transcription/quick-trans
169
was not a thorough corpus collection; the purpose of
collecting examples was to gain a sense of what disc
jockeys tend to discuss and compare.
2.2 Ontology Design
Based on the transcribed examples, we selected and
hand-wrote twelve database entries for music tracks,
using the authoring tool developed by the M-PIRO
project (Androutsopoulos et al, 2007). We trans-
formed the output of this tool into files suitable for
Methodius using an ad-hoc collection of Perl and
XSLT scripts, which also added the necessary infor-
mation to the OpenCCG grammars (White, 2006)
used by Methodius. We discuss future plans in this
area in Section 5.
We created a single-inheritance ontology for a
knowledge base of music pieces. First, we listed
the high-level entity types in the music domain, such
as ?person?, ?instrument?, ?classical music period?,
and ?jazz music period?. We then added attributes
commonly found in our disc jockey transcriptions.
For each entity type, we defined a set of fields. For
example, the classical-period field must contain an
entity which expresses a classical music piece?s time
period. We also specified a microplanning expres-
sion for each field, which provides detail on how the
field?s information should be generated at the sen-
tence level. We then added all the lexical items nec-
essary for the music domain.
2.3 Ontology Population
We populated our domain with six classical music
pieces and six jazz music pieces from the allmu-
sic.com database4. The songs were selected to yield
at least two interesting comparisons when placed in
a specific order. We also added entities linked to the
twelve songs, for example, each song?s album, per-
former, and composer, and information about these
entities. One challenge inherent in selecting these
entities from a publicly available database was to
eliminate as much common knowledge as possi-
ble about the music. In order to decrease back-
ground knowledge as a potential factor in our ex-
periment, we selected songs that primarily did not
contain popular performers, composers, and con-
ductors. We were able to gauge the popularity of
4http://www.allmusic.com
"Avatar" was written by Gary Husband and it was
performed by Billy Cobham, who was influenced
by Miles Davis. Billy Cobham originated from
Panama City, Panama and he played the drums; he
was active from the 1970s to the 1990s and he
participated in the Mahavishnu Orchestra. He
was influenced by Miles Davis. "Avatar" was
written during the Fusion period.
Figure 1: A generated description without comparisons.
Unlike "Fracture" and "A Mystery in Town",
which were written by Eddie "Lockjaw" Davis and
were performed by Fats Navarro, "Avatar" was
written by Gary Husband and it was performed by
Billy Cobham. Cobham originated from Panama
City, Panama and he played the drums; he was
active from the 1970s to the 1990s and he
participated in the Mahavishnu Orchestra. He
was influenced by Miles Davis. "Avatar" was
written during the Fusion period.
Figure 2: A generated description with comparisons to
previously described songs.
artists by their ?popularity rank? in the allmusic.com
database. However, we had to maintain a careful
balance between obscure artists and the ability to
generate interesting comparisons. Obscure artists
had less detailed information in the allmusic.com
database than popular music artists, so were forced
to select a few popular music artists for our exper-
iment, as their music pieces had multiple possible
interesting comparisons.
3 Experiment
We tried to maintain as many conditions from the
previous, similar study (Karasimos and Isard, 2004)
as possible to allow us to directly compare our re-
sults to theirs. The previous study established that
people learned more and perceived that they learned
more from text enriched with comparisons and ag-
gregations of facts than from texts that contained
neither. Our experimental design was similar to
theirs but all conditions of our experiment contained
text generated with aggregations of facts; our aim
was to isolate the effects of comparisons from those
of sentence aggregation.
For jazz texts, comparisons between songs involv-
ing performers, albums, composers, and time peri-
ods were possible. Classical texts could produce
all four of these types of comparisons. In addi-
tion, classical texts could also include comparisons
of conductors. Although the potential similarities
170
for classical and jazz texts were not equal, we de-
cided to include the conductor as a potential com-
parison for classical music. This is because across
both text types, we maintained the same number of
generated comparisons for each text type by limit-
ing Methodius to generating only one comparison
or contrast per paragraph of text. We present exam-
ples of a paragraph of text generated by Methodius
without (Figure 1) and with (Figure 2) comparisons.
In both cases, we assume that the user has already
seen texts about the songs ?Fracture? and ?A Mys-
tery in Town?, which expressed the facts about these
previous songs which are used in the comparisons in
Figure 2; the comparison text does not contain more
new information.
3.1 Evaluation Design
For our user study, we created a web interface using
WebExp2 Experiment Design software5 that con-
tained text generated by Methodius from our music
fact knowledge base. Forty fluent English speak-
ers were recruited and directed to a web page that
gave detailed instructions. After providing some ba-
sic personal information including their name, age,
gender, occupation and native languages, subjects
started with a test page, where they read a sample
paragraph and responded to one factual question, to
make sure that they had understood the interface,
and they then proceeded to the main experiment.
Participants read 6 paragraphs about either jazz
or classical music, and answered 15 factual recall
questions. They then read a further 6 paragraphs
about the other type of music, followed by 15 fac-
tual recall questions on the second set of texts. Fi-
nally they completed a post-experimental survey of
12 Likert Scale questions (Likert, 1932). We used
a within-subjects design, where each subject saw
two sets of texts, one classical and one jazz, one
with and one without comparisons, and the order
in which text sets were presented was controlled.
The multiple choice questions did not change given
the condition; so every participant saw the same
two sets of 15 multiple-choice questions in random-
ized orders. Seven multiple-choice questions of each
fifteen-question set dealt with facts that may be rein-
forced by comparisons. The remaining eight ques-
5http://www.webexp.info
Group Texts with com-
parisons
Texts without
comparisons
A 4.15 (1.814) 3.35 (1.872)
B 4.45 (1.638) 3.10 (1.651)
All 4.30 (1.713) 3.23 (1.747)
Table 1: Mean multiple choice scores with standard devi-
ation in brackets.
tions in each section served as a control for this ex-
periment.
On each page, the interface presented an image of
a paragraph of text generated by Methodius. The
users proceeded to the next paragraph when they
were ready by pressing the ?Next song? or ?Next
piece? button, depending on whether the music type
was jazz or classical. The texts were presented as
images for two reasons: so that the presentation of
stimuli would remain consistent across the differ-
ent computers and to prevent the text from being
selected by the participant, thus discouraging them
from copying the text and placing it into another
window as a reference to answer the factual recall
questions asked later.
4 Results
A summary of the participants? multiple choice
scores are shown in Table 1. Group A read classi-
cal texts with comparisons and jazz texts without,
and Group B read jazz texts with comparisons and
classical texts without.
We performed a 2-way repeated measures
ANOVA on our data and found that participants per-
formed significantly better on questions about the
texts which had comparisons (F (1, 36) = 11.131,
p < .01). There were no ordering or grouping
effects?the performance of participants did not de-
pend on which type of texts they saw first, or on
which type of texts contained comparisons.
In general, the Likert scores showed no signifi-
cant differences between the texts which had com-
parisons and those which did not. Karasimos and
Isard (2004) did find significant differences, but in
their case, texts had either comparisons and sen-
tence aggregations, or neither. In our study, all the
texts had sentence aggregations, so it may be this
factor which contributed to their higher Likert re-
171
sults on questions such as ?I enjoyed reading about
these songs? and the binary ?Which text (quality,
fluency) did you like more? question, for which we
also found no significant difference. Details of re-
sults and statistics can be found in (Marge, 2007).
5 Conclusions and Future Work
We have shown that the Methodius comparison gen-
eration algorithm does generalize to new domains,
and that it is possible to quickly author a new domain
and generate fluent and readable text, using an ap-
propriate authoring tool. We have also confirmed the
findings of previous studies, and showed that the use
of comparisons in texts does significantly improve
participants? recall of the facts which they have read.
In future work, we would like to use the cur-
rent text generation in an automatic DJ system with
streaming music, and perform further user studies in
order to make the texts as interesting and relevant
as possible. We would also like to perform a study
in which we compare the output of the comparison
algorithm using different parameter settings, to see
whether users express a preference.
Since this work was carried out, Methodius has
been adapted to accept ontologies and sentence
plans written in OWL/RDF. These can be created
using the Prote?ge? editor6 with an NLG plugin de-
veloped at the Athens University of Economics and
Business as part of the NaturalOWL generation sys-
tem (Galanis and Androutsopoulos, 2007), which is
available as an open source package7. A more prin-
cipled method for the OpenCCG conversion process
than the one described in Section 2.2 is in develop-
ment, and we hope to publish a paper on this subject.
Acknowledgements
The authors would like to acknowledge the help
and advice given by Colin Matheson, Ellen Bard,
Keith Edwards, Ray Carrick, Frank Keller, and Neil
Mayo and the comments of the anonymous review-
ers. This work was funded in part by a grant from
the Edinburgh-Stanford Link and by the Saint An-
drew?s Society of the State of New York. The music
data in this study was used with the permission of
the All Music Guide.
6http://www.protege.stanford.edu
7http://www.aueb.gr/users/ion/software/NaturalOWL.tar.gz
References
I. Androutsopoulos, J. Oberlander, and V. Karkaletsis.
2007. Source authoring for multilingual generation
of personalised object descriptions. Natural Language
Engineering, 13:191?233.
R. Cox, M. O?Donnell, and J. Oberlander. 1999. Dy-
namic versus static hypermedia in museum education:
an evaluation of ILEX, the intelligent labelling ex-
plorer. In Proceedings of the Artificial Intelligence in
Education conference, Le Mans.
R. Dale, J. Green, M. Milosavljevic, C. Paris, C. Ver-
spoor, and S. Williams. 1998. The realities of gener-
ating natural language from databases. In Proceedings
of the 11th Australian Joint Conference on Artificial
Intelligence, Brisbane, Australia.
D. Galanis and I. Androutsopoulos. 2007. Generating
multilingual descriptions from linguistically annotated
OWL ontologies: the NaturalOWL system. In Pro-
ceedings of ENLG 2007.
A. Isard, J. Oberlander, I. Androutsopoulos, and C. Math-
eson. 2003. Speaking the users? languages. IEEE In-
telligent Systems, 18(1):40?45. Special Issue on Ad-
vances in Natural Language Processing.
A. Isard. 2007. Choosing the best comparison under
the circumstances. In Proceedings of the International
Workshop on Personalization Enhanced Access to Cul-
tural Heritage, Corfu, Greece.
A. Karasimos and A. Isard. 2004. Multi-lingual eval-
uation of a natural language generation system. In
Proceedings of the Fourth International Conference on
Language Resources and Evaluation, Lisbon, Portu-
gal.
R. Likert. 1932. A technique for the measurement of
attitudes. Archives of Psychology, 22(140):1?55.
M. Marge. 2007. An evaluation of comparison genera-
tion in the methodius natural language generation sys-
tem. Master?s thesis, University of Edinburgh.
K. McKeown. 1985. Text Generation: Using Discourse
Strategies and Focus Constraints to Generate Natu-
ral Language Text. Cambridge University Press, New
York, NY, USA.
M. Milosavljevic. 1997. Content selection in compari-
son generation. In 6th European Workshop on Natural
Language Generation), Duisburg, Germany.
M. O?Donnell, C. Mellish, J. Oberlander, and A. Knott.
2001. ILEX: An architecture for a dynamic hypertext
generation system. Natural Language Engineering,
7:225?250.
M.White. 2006. Efficient realization of coordinate struc-
tures in combinatory categorial grammar. Research on
Language and Computation, 4(1):39?75.
172
Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 99?107,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Using the Amazon Mechanical Turk to Transcribe and  
Annotate Meeting Speech for Extractive Summarization 
Matthew Marge   Satanjeev Banerjee   Alexander I. Rudnicky  
School of Computer Science, Carnegie Mellon University 
Pittsburgh, PA 15213, USA 
{mrmarge,banerjee,air}@cs.cmu.edu 
 
Abstract 
Due to its complexity, meeting speech pro-
vides a challenge for both transcription and 
annotation. While Amazon?s Mechanical Turk 
(MTurk) has been shown to produce good re-
sults for some types of speech, its suitability 
for transcription and annotation of spontane-
ous speech has not been established. We find 
that MTurk can be used to produce high-
quality transcription and describe two tech-
niques for doing so (voting and corrective). 
We also show that using a similar approach, 
high quality annotations useful for summari-
zation systems can also be produced. In both 
cases, accuracy is comparable to that obtained 
using trained personnel.  
1 Introduction 
Recently, Amazon?s Mechanical Turk (MTurk) has 
been shown to produce useful transcriptions of 
speech data; Gruenstein et al (2009) have success-
fully used MTurk to correct the transcription out-
put from a speech recognizer, while Novotney and 
Callison-Burch (2010) used MTurk for transcrib-
ing a corpus of conversational speech. These stu-
dies suggest that transcription, formerly considered 
to be an exacting task requiring at least some train-
ing, could be carried out by casual workers. How-
ever, only fairly simple transcription tasks were 
studied.  
 We propose to assess the suitability of MTurk 
for processing more challenging material, specifi-
cally recordings of meeting speech. Spontaneous 
speech can be difficult to transcribe because it may 
contain false starts, disfluencies, mispronunciations 
and other defects. Similarly for annotation, meet-
ing content may be difficult to follow and conven-
tions difficult to apply consistently.  
 Our first goal is to ascertain whether MTurk 
transcribers can accurately transcribe spontaneous 
speech, containing speech errors and of variable 
utterance length.  
 Our second goal is to use MTurk for creating 
annotations suitable for extractive summarization 
research, specifically labeling each utterance as 
either ?in-summary? or ?not in-summary?. Among 
other challenges, this task cannot be decomposed 
into small independent sub-tasks?for example, 
annotators cannot be asked to annotate a single 
utterance independent of other utterances. To our 
knowledge, MTurk has not been previously ex-
plored for the purpose of summarization annota-
tion.  
2 Meeting Speech Transcription Task 
We recently explored the use of MTurk for tran-
scription of short-duration clean speech (Marge et 
al., 2010) and found that combining independent 
transcripts using ROVER yields very close agree-
ment with a gold standard (2.14%, comparable to 
expert agreement). But simply collecting indepen-
dent transcriptions seemed inefficient: the ?easy? 
parts of each utterance are all transcribed the same. 
In the current study our goal is determine whether 
a smaller number of initial transcriptions can be 
used to identify easy- and difficult-to-transcribe 
regions, so that the attention of subsequent tran-
scribers can be focused on the more difficult re-
gions.  
2.1 Procedure 
In this corrective strategy for transcription, we 
have two turkers to independently produce tran-
scripts. A word-level minimum edit distance me-
tric is then used to align the two transcripts and 
locate disagreements. These regions are replaced 
with underscores, and new turkers are asked to 
transcribe those regions.  
Utterances were balanced for transcription dif-
ficulty (measured by the native English back-
99
ground of the speaker and utterance length). For 
the first pass transcription task, four sets of jobs 
were posted for turkers to perform, with each pay-
ing $0.01, $0.02, $0.04, or $0.07 per approved 
transcription. Payment was linearly scaled with the 
length of the utterance to be transcribed at a rate of 
$0.01 per 10 seconds of speech, with an additional 
payment of $0.01 for providing feedback. In each 
job set, there were 12 utterances to be transcribed 
(yielding a total of 24 jobs available given two 
transcribers per utterance). Turkers were free to 
transcribe as many utterances as they could across 
all payment amounts. 
After acquiring two transcriptions, we aligned 
them, identified points of disagreement and re-
posted the transcripts and the audio as part of a 
next round of job sets. Payment amounts were kept 
the same based on utterance length. In this second 
pass of transcriptions, three turkers were recruited 
to correct and amend each transcription. Thus, a 
total of five workers worked on every transcription 
after both iterations of the corrective task. In our 
experiment 23 turkers performed the first phase of 
the task, and 28 turkers the corrective task (4 
workers did both passes).  
2.2 First and Second Pass Instructions 
First-pass instructions asked turkers to listen to 
utterances with an embedded audio player pro-
vided with the HIT. Turkers were instructed to 
transcribe every word heard in the audio and to 
follow guidelines for marking speaker mispronun-
ciations and false starts. Filled pauses (?uh?, ?um?, 
etc.) were not to be transcribed in the first pass. 
Turkers could replay the audio as many times as 
necessary. 
In the second pass, turkers were instructed to 
focus on the portions of the transcript marked with 
underscores, but also to correct any other words 
they thought were incorrect. The instructions also 
asked turkers to identify three types of filler words: 
?uh?, ?um?, and ?lg? (laughter). We selected this 
set since they were the most frequent in the gold 
standard transcripts. Again, turkers could replay 
the audio.  
2.3 Speech Corpus 
The data were sampled from a previously-collected 
corpus of natural meetings (Banerjee and Rud-
nicky, 2007). The material used in this paper 
comes from four speakers, two native English 
speakers and two non-Native English speakers (all 
male). We selected 48 audio clips; 12 from each of 
the four speakers. Within each speaker's set of 
clips, we further divided the material into four 
length categories: ~5, ~10, ~30 and ~60 sec. The 
speech material is conversational in nature; the 
gold standard transcriptions of this data included 
approximately 15 mispronunciations and 125 false 
starts. Table 1 presents word count information 
related to the utterances in each length category. 
 
Utterance 
Length 
Word Count 
(mean) 
Standard  
Deviation 
Utterance 
Count 
5 sec 14  5.58 12  
10 sec 24.5  7.26 12  
30 sec 84  22.09 12  
60 sec 146.6  53.17 12  
 Table 1. Utterance characteristics. 
3 Meeting Transcription Analysis 
Evaluation of first and second pass corrections was 
done by calculating word error rate (WER) with a 
gold standard, obtained using the transcription 
process described in (Bennett and Rudnicky, 
2002). Before doing so, we normalized the candi-
date MTurk transcriptions as follows: spell-
checking (with included domain-specific technical 
terms), and removal of punctuation (periods, com-
mas, etc.). Apostrophes were retained. 
 
Table 2. WER across transcription iterations. 
3.1 First-Pass Transcription Results 
Results from aligning our first-pass transcriptions 
with a gold standard are shown in the second col-
umn of Table 2. Overall error rate was 23.8%, 
which reveals the inadequacy of individual turker 
transcriptions, if no further processing is done. 
(Remember that first-pass transcribers were asked 
to leave out fillers even though the gold standard 
contained them, thus increasing WER). 
Utterance 
Length 
First-Pass 
WER 
Second-Pass 
WER 
ROVER-3 
WER 
5 sec. 31.5% 19.8% 15.3% 
10 sec. 26.7% 20.3% 13.8% 
30 sec. 20.8% 16.9% 15.0% 
60 sec. 24.3% 17.1% 15.4% 
Aggregate 23.8% 17.5% 15.1% 
100
In this first pass, speech from non-native speak-
ers was transcribed more poorly (25.4% WER) 
than speech from native English speakers (21.7% 
WER). In their comments sections, 17% of turkers 
noted the difficulty in transcribing non-native 
speakers, while 13% found native English speech 
difficult. More than 80% of turkers thought the 
amount of work ?about right? for the payment re-
ceived.  
3.2 Second-Pass Transcription Results 
The corrective process greatly improved agreement 
with our expert transcriptions. Aggregate WER 
was reduced from 23.8% to 17.5% (27% relative 
reduction) when turkers corrected initial transcripts 
with highlighted disagreements (third column of 
Table 2). In fact, transcriptions after corrections 
were significantly more accurate than initial tran-
scriptions (F(1, 238) = 13.4, p < 0.05). With re-
spect to duration, the WER of the 5-second utter-
ances had the greatest improvement, a relative re-
duction of WER by 37%.  Transcription alignment  
with the gold standard experienced a 39% im-
provement to 13.3% for native English speech, and 
a 19% improvement to 20.6% for non-native Eng-
lish speech (columns 2 and 3 of Table 3).  
 We found that 30% of turkers indicated that the 
second-pass correction task was difficult, as com-
pared with 15% for the first-pass transcription task. 
Work amount was perceived to be about right 
(85% of the votes) in this phase, similar to the first. 
3.3 Combining Corrected Transcriptions 
In order to improve the transcriptions further, we 
combined the three second-pass transcriptions of 
each utterance using ROVER?s word-level voting 
scheme (Fiscus, 1997). The WER of the resulting 
transcripts are presented in the fourth column of 
Table 2. Aggregate WER was further reduced by 
14% relative to 15.1%. This result is close to typ-
ical disagreement rates of 6-12% reported in the 
literature (Roy and Roy, 2009). The best im-
provements using ROVER were found with the 
transcriptions of the shorter utterances: WER 
from the second-pass of 5-second utterances tran-
scriptions was reduced by 23% to 15.3%. The 10-
second utterance transcriptions experienced the 
best improvement, 32%, to a WER of 13.8%. 
 Although segmenting audio into shorter seg-
ments may yield fast turnaround times, we found 
that utterance length is not a significant factor in 
determining alignment between combined, cor-
rected transcriptions and gold-standard transcrip-
tions (F(3, 44) = 0.16, p = 0.92). We speculate that 
longer utterances show good accuracy due to the 
increased context available to transcribers.  
Table 3. WER across transcription iterations based on 
speaker background. 
3.4 Error Analysis 
Out of 3,281 words (48 merged transcriptions of 
48 utterances), 496 were errors. Among the errors 
were 37 insertions, 315 deletions, and 144 substitu-
tions. Thus the most common error was to miss a 
word.  
 Further analysis revealed that two common cas-
es of errors occurred: the misplacement or exclu-
sion of filler words (even though the second phase 
explicitly instructed turkers to insert filler words) 
and failure to transcribe words considered to be out 
of the range of the transcriber?s vocabulary, such 
as technical terms and foreign names. Filler words 
accounted for 112 errors (23%). Removing fillers 
from both the combined transcripts and the gold 
standard improved WER by 14% relative to 
13.0%. Further, WER for native English speech 
transcriptions was reduced to 8.9%. This difference 
was however not statistically significant (F(1,94) = 
1.64, p = 0.2). 
 Turkers had difficulty transcribing uncommon 
words, technical terms, names, acronyms, etc. 
(e.g., ?Speechalyzer?, ?CTM?, ?PQs?). Investiga-
tion showed that at least 41 errors (8%) could be 
attributed to this out-of-vocabulary problem. It is 
unclear if there is any way to completely eradicate 
such errors, short of asking the original speakers. 
3.5 Comparison to One-Pass Approach 
Although the corrective model provides significant 
gain from individual transcriptions, this approach 
is logistically more complex. We compared it to 
our one-pass approach, in which five turkers inde-
pendently transcribe all utterances (Marge et al, 
2010). Five new transcribers per utterance were 
recruited for this task (yielding 240 transcriptions). 
Speaker 
Background 
First-Pass 
WER 
Second-
Pass WER 
ROVER-3 
WER 
Native  21.7% 13.3% 10.8% 
Non-native 25.4% 20.6% 18.4% 
101
Individual error rate was 24.0%, comparable to the 
overall error rate for the first step of the corrective 
approach (Table 2).  
 After combining all five transcriptions with 
ROVER, we found similar gains to the corrective 
approach: an overall improvement to 15.2% error 
rate. Thus both approaches can effectively produce 
high-quality transcriptions. We speculate that if 
higher accuracy is required, the corrective process 
could be extended to iteratively re-focus effort on 
the regions of greatest disagreement. 
3.6 Latency 
Although payment scaled with the duration of ut-
terances, we observed a consistent disparity in tur-
naround time. All HITs were posted at the same 
time in both iterations (Thursday afternoon, EST). 
Turkers were able to transcribe 48 utterances twice 
in about a day in the first pass for the shorter utter-
ances (5- and 10-second utterances), while it took 
nearly a week to transcribe the 30- and 60-second 
utterances. Turkers were likely discouraged by the 
long duration of the transcriptions compounded 
with the nature of the speech. To increase turna-
round time on lengthy utterances, we speculate that 
it may be necessary to scale payment non-linearly 
with length (or another measure of perceived ef-
fort). 
3.7 Conclusion 
Spontaneous speech, even in long segments, can 
indeed be transcribed on MTurk with a level of 
accuracy that approaches expert agreement rates 
for spontaneous speech. However, we expect seg-
mentation of audio materials into smaller segments 
would yield fast turnaround time, and may keep 
costs low. In addition, we find that ROVER works 
more effectively on shorter segments because 
lengths of candidate transcriptions are less likely to 
have large disparities. Thus, multiple transcriptions 
per utterance can be utilized best when their 
lengths are shorter.  
4 Annotating for Summarization  
4.1 Motivation 
Transcribing audio data into text is the first step 
towards making information contained in audio 
easily accessible to humans. A next step is to con-
dense the information in the raw transcription, and 
produce a short summary that includes the most 
important information. Good summaries can pro-
vide readers with a general sense of the meeting, or 
help them to drill down into the raw transcript (or 
the audio itself) for additional information. 
4.2 Annotation Challenges  
Unfortunately, summary creation is a difficult task 
because ?importance? is inherently subjective and 
varies from consumer to consumer. For example, 
the manager of a project, browsing a summary of a 
meeting, might be interested in all agenda items, 
whereas a project participant may be interested in 
only those parts of the meeting that pertain to his 
portion of the project.  
Despite this subjectivity, the usefulness of a 
summary is clear, and audio summarization is an 
active area of research. Within this field, two kinds 
of human annotations are generally created?
annotators are either asked to write a short sum-
mary of the audio, or they are asked to label each 
transcribed utterance as either ?in summary? or 
?out of summary?. The latter annotation is particu-
larly useful for training and evaluating extractive 
summarization systems?systems that create sum-
maries by selecting a subset of the utterances.  
Due to the subjectivity involved, we find very 
low inter-annotator agreement for this labeling 
task. Liu and Liu (2008) reported Kappa agreement 
scores of between 0.11 and 0.35 across 6 annota-
tors, Penn and Zhu (2008) reported 0.38 on tele-
phone conversation and 0.37 on lecture speech, 
using 3 annotators, and Galley (2006) reported 
0.32 on meeting data. Such low levels of agree-
ment imply that the resulting training data is likely 
to contain a great deal of ?noise??utterances la-
beled ?in summary? or ?out of summary?, when in 
fact they are not good examples of those classes. 
Disagreements arise due to the fact that utter-
ance importance is a spectrum.  While some utter-
ances are clearly important or unimportant, there 
are many utterances that lie between these ex-
tremes. In order to label utterances as either ?in-
summary? or not, annotators must choose an arbi-
trary threshold at which to make this decision. 
Simply asking annotators to provide a continuous 
?importance value? between 0 and 1 is also likely 
to be infeasible as the exact value for a given utter-
ance is difficult to ascertain. 
102
4.3 3-Class Formulation 
One way to alleviate this problem is to redefine the 
task as a 3-class labeling problem. Annotators can 
be asked to label utterances as either ?important?, 
?unimportant? or ?in-between?. Although this for-
mulation creates two decision boundaries, instead 
of the single one in the 2-class formulation, the 
expectation is that a large number of utterances 
with middling importance will simply be assigned 
to the ?in between? class, thus reducing the amount 
of noise in the data. Indeed we have shown (Baner-
jee and Rudnicky, 2009) that in-house annotators 
achieve high inter-annotator agreement when pro-
vided with the 3-class formulation. 
Another way to alleviate the problem of low 
agreement is to obtain annotations from many an-
notators, and identify the utterances that a majority 
of the annotators appear to agree on; such utter-
ances may be considered as good examples of their 
class. Using multiple annotators is typically not 
feasible due to cost. In this paper we investigate 
using MTurk to create 3-class-based summariza-
tion annotations from multiple annotators per 
meeting, and to combine and filter these annota-
tions to create high quality labels. 
5 Using Mechanical Turk for Annotations 
5.1 Challenges of Using Mechanical Turk 
Unlike some other tasks that require little or no 
context in order to perform the annotation, summa-
rization annotation requires a great deal of context. 
It is unlikely that an annotator can determine the 
importance of an utterance without being aware of 
neighboring utterances. Moreover, the appropriate 
length of context for a given utterance is likely to 
vary. Presenting all contiguous utterances that dis-
cuss the same topic might be appropriate, but 
would require manual segmentation of the meeting 
into topics. In this paper we experiment with show-
ing all utterances of a meeting. This is a challenge 
however, because MTurk is typically applied to 
quick low-cost tasks that need little context. It is 
unclear whether turkers would be willing to per-
form such a time-consuming task, even for higher 
payment. 
Another challenge for turkers is being able to 
understand the discussion well enough to perform 
the annotation. We experiment here with meetings 
that include significant technical content. While in-
house annotators can be trained over time to under-
stand the material well enough to perform the task, 
it is impractical to provide turkers with such train-
ing. We investigate the degree to which turkers can 
provide summarization annotation with minimal 
training.  
5.2 Data Used 
We selected 5 recorded meetings for our study. 
These meetings were not scripted?and would 
have taken place even if they weren?t being rec-
orded. They were project meetings containing dis-
cussions about software deliverables, problems, 
resolution plans, etc. The contents included tech-
nical jargon and concepts that non-experts are un-
likely to grasp by reading the meeting transcript 
alone.  
The 5 meetings had 2 to 4 participants each 
(mean: 3.5). For all meetings, the speech from each 
participant was recorded separately using head-
mounted close-talking microphones. We manually 
split these audio streams into utterances?ensuring 
that utterances did not have more than a 0.5 second 
pause in them, and then transcribed them using an 
established process (Bennett and Rudnicky, 2002). 
The meetings varied widely in length from 15 mi-
nutes and 282 utterances to 40 minutes and 948 
utterances (means: 30 minutes, 610 utterances). 
There were 3,052 utterances across the 5 meetings, 
each containing an mean of 7 words. The utter-
ances in the meetings were annotated using the 3-
class formulation by two in-house annotators. 
Their inter-annotator agreement is presented along 
with the rest of the evaluation results in Section 6. 
0
10
20
30
40
50
60
Important Neutral Unimportant
%
 o
f U
tt
er
an
ce
s
In-house Mturk
Figure 1. Label distribution of in-house and MTurk 
annotators. 
 
103
5.3 HIT Design and Instructions 
We instructed turkers to imagine that someone else 
(not them) was going to eventually write a report 
about the meeting, and it was their task to identify 
those utterances that should be included in the re-
port. We asked annotators to label utterances as 
?important? if they should be included in the report 
and ?unimportant? otherwise. In addition, utter-
ances that they thought were of medium impor-
tance and that may or may not need to be included 
in the report were to be labeled as ?neutral?. We 
provided examples of utterances in each of these 
classes. For the ?important? class, for instance, we 
included ?talking about a problem? and ?discuss-
ing future plan of action? as examples. For the ?un-
important? class, we included ?off topic joking?, 
and for the ?neutral? class ?minute details of an 
algorithm? was an example. 
In addition to these instructions and examples, 
we gave turkers a general guideline to the effect 
that in these meetings typically 1/4th of the utter-
ances are ?important?, 1/4th ?neutral? and the rest 
?unimportant?. As we discuss in section 6, it is 
unclear whether most turkers followed this guide-
line. 
Following these instructions, examples and tips, 
we provided the text of the utterances in the form 
of an HTML table. Each row contained a single 
utterance, prefixed with the name of the speaker. 
The row also contained three radio buttons for the 
three classes into which the annotator was asked to 
classify the utterance. Although we did not ensure 
that annotators annotated every utterance before 
submitting their work, we observed that for 95% of 
the utterances every annotator did provide a judg-
ment; we ignore the remaining 5% of the utter-
ances in our evaluation below. 
5.4 Number of Turkers and Payment 
For each meeting, we used 5 turkers and paid each 
one the same. That is, we did not vary the payment 
amount as an experimental variable. We calculated 
the amount to pay for a meeting based on in the 
length of that meeting. Specifically, we multiplied 
the number of utterances by 0.13 US cents to arrive 
at the payment. This resulted in payments ranging 
from 35 cents to $1.25 per meeting (mean 79 
cents). The effective hourly rate (based on how 
much time turkers took to actually finish each job) 
was $0.87. 
6 Annotation Results 
6.1 Label Distribution 
We first examine the average distribution of labels 
across the 3 classes. Figure 1 shows the distribu-
tions (expressed as percentages of the number of 
utterances) for in-house and MTurk annotators, 
averaged across the 5 meetings. Observe that the 
distribution for the in-house annotators is far more 
skewed away from a uniform 33% assignment, 
whereas the label distribution of turkers is less 
skewed. The likely reason for this difference is that 
0.0
0.1
0.2
0.3
0.4
0.5
0.6
In house v 
In house
Turker v 
Turker
In house v 
Turker
Ka
pp
a 
Ag
re
em
en
t
Figure 3. Agreement with in-house annotators when 
turker annotations are merged through voting. 
0.0
0.2
0.4
0.6
0.8
1.0
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40 Fraction of data w
ith agreem
ent criteria
Ka
pp
a 
Ag
re
em
en
t
Agreement Fraction of data
Figure 2. Average kappa agreement between in-house 
annotators, turkers, and in-house annotators and turkers. 
104
turkers have a poorer understanding of the meet-
ings, and are more likely than in-house annotators 
to make arbitrary judgments about utterances. This 
poor understanding perhaps also explains the large 
difference in the percentage of utterances labeled 
as important?for many utterances that are difficult 
to understand, turkers probably play it safe by 
marking it important.  
The error bars represent the standard deviations of 
these averages, and capture the difference in label 
distribution from meeting to meeting. While different 
meetings are likely to inherently have different ratios 
of the 3 classes, observe that the standard deviations 
for the in-house annotators are much lower than those 
for the turkers. For example, the percentage of utter-
ances labeled ?important? by in-house annotators 
varies from 9% to 22% across the 5 meetings, whe-
reas it varies from 30% to 57% for turkers, a much 
wider range. These differences in standard deviation 
persist for each meeting as well?that is, for any giv-
en meeting, the label distribution of the turkers varies 
much more between each other than the distribution 
of the in-house annotators. 
6.2 Inter-Annotator Agreement 
Figure 2 shows the kappa values for pairs of anno-
tators, averaged across the 5 meetings, while the 
error bars represent the standard deviations. The 
kappa between the two in-house annotators. (0.4) 
is well within the range of values reported in the 
summarization literature (see section 4). The kappa 
values range from 0.24 to 0.50 across the 5 meet-
ings. The inter-annotator agreement between pairs 
of turkers, averaged across the 10 possible pairs 
per meeting (5 choose 2), and across the 5 meet-
ings show that turkers tend to agree less between 
each other than in-house annotators, although this 
kappa (0.28) is still within the range of typical 
agreement (this kappa has lower variance because 
the sample size is larger). The kappa between in-
house annotators and turkers1 (0.19) is on the low-
er end of the scale but remains within the range of 
agreement reported in the literature, suggesting 
that Mechanical Turk may be a useful tool for 
summarization.  
                                                          
1
 For each meeting, we measure agreement between every 
possible pair of annotators such that one of the annotators was 
an in-house annotator, and the other a turker. Here we present 
the average agreement across all such pairs, and across all the 
meetings. 
6.3 Agreement after Voting 
We consider merging the annotations from mul-
tiple turkers using a simple voting scheme as fol-
lows. For each utterance, if 3, 4 or 5 annotators 
labeled the utterance with the same class, we la-
beled the utterance with that class. For utterances 
in which 2 annotators voted for one class, 2 for 
another and 1 for the third, we randomly picked 
from one of the classes in which 2 annotators voted 
the same way. We then computed agreement be-
tween this ?voted turker? and each of the two in-
house annotators, and averaged across the 5 meet-
ings. Figure 3 shows these agreement values. The 
left-most point on the ?Kappa Agreement? curve 
shows the average agreement obtained using indi-
vidual turkers (0.19) while the second point shows 
the agreement with the ?voted turker? (0.22). This 
is only a marginal improvement, implying that 
simply voting and using all the data does not im-
prove much over the average agreement of indi-
vidual annotators.  
 The agreement does improve when we consider 
only those utterances that a clear majority of anno-
tators agreed on. The 3rd, 4th and 5th points on the 
?Agreement? curve plot the average agreement 
when considering only those utterances that at least 
3, 4 and 5 turkers agreed on. The ?Fraction of da-
ta? curve plots the fraction of the meeting utter-
ances that fit these agreement criteria. For 
utterances that at least 3 turkers agreed on, the 
kappa agreement value with in-house annotators is 
0.25, and this represents 84% of the data. For about 
50% of the data 4 of 5 turkers agreed, and these 
utterances had a kappa of 0.32. Finally utterances 
for which annotators were unanimous had a kappa 
of 0.37, but represented only 22% of the data. It is 
particularly encouraging to note that although the 
amount of data reduces as we focus on utterances 
that more and more turkers agree on, the utterances 
so labeled are not dominated by any one class. For 
example, among utterances that 4 or more turkers 
agree on, 48% belong to the important class, 48% 
to unimportant class, and the remaining 4% to the 
neutral class. These results show that with voting, 
it is possible to select a subset of utterances that 
have higher agreement rates, implying that they are 
annotated with higher confidence. For future work 
we will investigate whether a summarization sys-
tem trained on only the highly agreed-upon data 
outperforms one trained on all the annotation data. 
105
7 Conclusions 
In this study, we found that MTurk can be used to 
create accurate transcriptions of spontaneous meet-
ing speech when using a two-stage corrective 
process. Our best technique yielded a disagreement 
rate of 15.1%, which is competitive with reported 
disagreement in the literature of 6-12%. We found 
that both fillers and out-of-vocabulary words 
proved troublesome. We also observed that the 
length of the utterance being transcribed wasn?t a 
significant factor in determining WER, but that the 
native language of the speaker was indeed a signif-
icant factor.  
 We also experimented with using MTurk for the 
purpose of labeling utterances for extractive sum-
marization research. We showed that despite the 
lack of training, turkers produce labels with better 
than random agreement with in-house annotators. 
Further, when combined using voting, and with the 
low-agreement utterances filtered out, we can iden-
tify a set of utterances that agree significantly bet-
ter with in-house annotations.  
 In summary, MTurk appears to be a viable re-
source for producing transcription and annotation 
of meeting speech. Producing high-quality outputs, 
however, may require the use of techniques such as 
ensemble voting and iterative correction or refine-
ment that leverage performance of the same task 
by multiple workers. 
References 
 
S. Banerjee and A. I. Rudnicky. 2007. Segmenting 
 meetings into agenda items by extracting implicit 
 supervision from human note-taking. In 
 Proceedings of IUI.  
S. Banerjee and A. I. Rudnicky. 2009. Detecting the 
 noteworthiness of utterances in human meetings.  In 
 Proceedings of SIGDial.  
C. Bennett and A. I. Rudnicky. 2002. The Carnegie 
 Mellon Communicator corpus. In Proceedings of 
 ICSLP.  
J. G. Fiscus. 1997. A post-processing system to yield     
 word error rates: Recognizer Output Voting Error 
 Reduction (ROVER). In Proceedings of ASRU 
 Workshop.  
M. Galley. (2006). A skip-chain conditional ran-
 dom field for ranking meeting utterances by im
 portance. In Proceedings of EMNLP.  
 
 
A. Gruenstein, I. McGraw, and A. Sutherland. 2009. A 
 self-transcribing speech corpus: collecting 
 continuous speech with an online educational game. 
 In Proceedings of SLaTE Workshop.  
F. Liu and Y. Liu. 2008. Correlation between 
 ROUGE and human evaluation of extractive 
 meeting summaries. In Proceedings of ACL-HLT.  
M. Marge, S. Banerjee, and A. I. Rudnicky. 2010.    
 Using the Amazon Mechanical Turk for 
 transcription of spoken language. In Proceedings 
 of ICASSP.  
S. Novotney and C. Callison-Burch. 2010. Cheap, fast 
 and good enough: Automatic speech recognition 
 with non-expert transcription. In Proceedings of 
 NAACL. 
G. Penn and X. Zhu. 2008. A critical reassessment of 
 evaluation baselines for speech  summarization. In 
 Proceedings of ACL-HLT.  
B. Roy and D. Roy. 2009. Fast transcription of un-
 structured audio recordings. In Proceedings of In
 terspeech. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
106
Appendix 
 
Transcription task HIT type 1: 
 
 
 
Transcription task HIT type 2: 
 
 
 
Annotation task HIT: 
 
 
107
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 91?94,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
Towards Improving the Naturalness of
Social Conversations with Dialogue Systems
Matthew Marge, Joa?o Miranda, Alan W Black, Alexander I. Rudnicky
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213
{mrmarge,jmiranda,awb,air}@cs.cmu.edu
Abstract
We describe an approach to improving
the naturalness of a social dialogue sys-
tem, Talkie, by adding disfluencies and
other content-independent enhancements
to synthesized conversations. We investi-
gated whether listeners perceive conversa-
tions with these improvements as natural
(i.e., human-like) as human-human con-
versations. We also assessed their ability
to correctly identify these conversations as
between humans or computers. We find
that these enhancements can improve the
perceived naturalness of conversations for
observers ?overhearing? the dialogues.
1 Introduction
An enduring problem in spoken dialogue systems
research is how to make conversations between
humans and computers approach the naturalness
of human-human conversations. Although this
has been addressed in several goal-oriented dia-
logue systems (e.g., for tutoring, question answer-
ing, etc.), social dialogue systems (i.e., non-task-
oriented) have not significantly advanced beyond
so-called ?chatbots?. Proper social dialogue sys-
tems (Bickmore and Cassell, 2004; Higuchi et
al., 2002) would be able to conduct open con-
versations, without being restricted to particular
domains. Such systems would find use in many
environments (e.g., human-robot interaction, en-
tertainment technology).
This paper presents an approach to improving a
social dialogue system capable of chatting about
the news by adding content-independent enhance-
ments to speech. We hypothesize that enhance-
ments such as explicit acknowledgments (e.g.,
right, so, well) and disfluencies can make human-
computer conversations sound indistinguishable
from those between two humans.
Enhancements to synthesized speech have been
found to influence perception of a synthetic
voice?s hesitation (Carlson et al, 2006) and per-
sonality (Nass and Lee, 2001). Andersson et
al. (2010) used machine learning techniques to
determine where to include conversational phe-
nomena to improve synthesized speech. Adell et
al. (2007) developed methods for inserting filled
pauses into synthesized speech that listeners found
more natural. In these studies, human judges com-
pared utterances in isolation with and without im-
provements. In our study, we focus on a holistic
evaluation of naturalness in dialogues and ask ob-
servers to directly assess the naturalness of con-
versations that they ?overhear?.
2 The Talkie System
Talkie is a spoken dialogue system capable of hav-
ing open conversations about recent topics in the
news. This system was developed for a dialogue
systems course (Lim et al, 2009). Interaction
is intended to be unstructured and free-flowing,
much like social conversations. Talkie initiates a
conversation by mentioning a recent news head-
line and invites the user to comment on it.
The system uses a database of news topics and
human-written comments from the ?most blogged
about articles? of the New York Times (NYT)1.
Comments are divided into single sentences to ap-
proximate the length of a spoken response. Given
a user?s utterance (e.g., keywords related to the
topic), Talkie responds with the comment that
most closely resembles that utterance. Talkie may
access any comment related to the topic under dis-
cussion (without repetition). The user may choose
to switch to a different topic at any time (at which
point Talkie will propose a different topic from its
set).
1http://www.nytimes.com/gst/mostblogged.html
Follow links to each article?s comment section.
91
3 Study
We performed a study to determine if the per-
ceived naturalness of conversations could be im-
proved by using heuristic enhancements to speech
output. Participants ?overheard? conversations
(similar to Walker et al (2004)). Originally typed
interactions, the conversations were later synthe-
sized into speech using the Flite speech synthesis
engine (Black and Lenzo, 2001). For distinctive-
ness, conversations were between one male voice
(rms) and one female voice (slt). The voices were
generated using the CLUSTERGEN statistical para-
metric synthesizer (Black, 2006). All conversa-
tions began with the female voice.
3.1 Dialogue Content
We considered four different conversation types:
(1 & 2) between a human and Talkie (human-
computer and computer-human depending on the
first speaker), (3) between two humans on a
topic in Talkie?s database (human-human), and
(4) between two instances of Talkie (computer-
computer). The human-computer and computer-
human conditions differed from each other by
one utterance; that is, one was a shifted version
of the other by one dialogue turn. The human-
computer conversations were collected from two
people (one native English speaker, one native
Portuguese speaker) interacting with Talkie on
separate occasions. For human-human conversa-
tions, Talkie proposed a topic for discussion. Each
conversation contained ten turns of dialogue. To
remove any potential effects from the start and end
content of the conversations, we selected the mid-
dle three turns for synthesis. Each conversation
type had five conversations, each about one of five
recent headlines (as of May 2010).
3.2 Heuristic Enhancements
We defined a set of rules that added phenomena
observed in human-human spoken conversations.
These included filled pauses, word repetitions, si-
lences, and explicit acknowledgments. Conversa-
tions in this study were enhanced manually by fol-
lowing the set of rules described in Figure 1; an
example is shown in Figure 2.
3.3 Participants and Task
Eighty participants were recruited from Ama-
zon?s Mechanical Turk2 (MTurk) for this between-
2http://www.mturk.com
Category I - Explicit Acknolwedgements
? inserted sparingly at the beginning of sentences
when grammatical (e.g., well, so, you know,
right).
Category II - Filled pauses / repetitions
? no more than three per dialogue
? no more than two in any one sentence
? placed repetitions in places where speaker may
have hesitation (e.g., after ?I think...?)
? utterances may have a filled pause or a repetition
followed by a filled pause.
Category III - Silences
? added sparingly after filled pauses.
Figure 1: Heuristics used for adding enhance-
ments to synthesized dialogue.
subjects study. Only workers with a 95% HIT (hu-
man intelligence task) approval rating could par-
ticipate. There were three experimental dimen-
sions that we counterbalanced across participants:
the four conversation ?types? described in Section
3.1, the order of the five topics, and the presence
or absence of heuristic enhancements to the syn-
thesized speech that participants heard. Each par-
ticipant listened to every topic such that exactly
five participants heard each condition.
Participants made two judgments per conversa-
tion: rating the ?naturalness? of a conversation
based on a four-point Likert scale and ranking
the conversant identities (i.e., as two humans, two
computers, or a computer and a human) from most
to least confident. We used the non-parametric
Kruskal-Wallis one-way analysis of variance to
determine statistically significant differences in
naturalness responses across groups (naturalness
being the dependent measure).
4 Results
We analyzed two measures: (1) naturalness rat-
ings of conversations, and (2) frequencies from
highest-rank to lowest-rank of who the participant
believed to be the speakers in each conversation.
We found no statistically significant ordering ef-
fects or biases due to the topic of conversation.
4.1 Speaker Type Condition Results
Results for naturalness scores are summarized in
Figure 3. As expected, mean naturalness was
highest for the human-human condition and lowest
for the computer-computer condition. We found
92
Human-computer conversation (original, enhancements)
h: Well, that sounds ridiculous, you get taxed proportionally to your paycheck.
c: So, let them retire on social security payments, then you will see changes with <uh>with an overabundance of funds.
h: I don?t see how this would be possible.
c: You know, you don?t have the money yet, and you have just increased your debt by 2.5 trillion dollars.
h: Exactly why should we be raising taxes on the rich?
c: With our current politics of division, I <umm> worry we have the will or determination to address these issues.
Figure 2: Example conversation with heuristic enhancements marked in bold.
1.9
2.5 2.3
2.7
1.5
2.0
2.5
3.0
1.0 cc ch hc hh
Figure 3: Naturalness across the speaker type con-
dition.
no statistically significant difference in naturalness
ratings for the computer-human condition com-
pared to the human-computer condition (H(1) =
2.94; p = 0.09). Also, the computer-computer
condition was significantly different from all other
conditions, suggesting that conversation flow is an
important factor in determining the naturalness of
a conversation (H(3) = 42.49, p < 0.05).
People rated conversations involving a com-
puter and a human similarly to human-human con-
versations (without enhancements). There were
no statistically significant differences between the
three conditions cc, ch, and hc (H(2) = 5.36, p =
0.06). However, a trend indicated that hc natural-
ness ratings differed from those of the ch and hh
conditions. Conversations from the hc condition
had much lower (18%) mean naturalness ratings
compared to their ch counterparts, even though
they were nearly equivalent in content.
4.2 Heuristic Enhancements Results
There were significant differences in naturalness
ratings when heuristic enhancements were present
(H(1) = 17.49, p < 0.05). Figure 4 shows that
the perceived naturalness was on average higher
with heuristic enhancements. Overall, mean natu-
ralness improved by 20%. This result agrees with
findings from Andersson et al (2010).
Computer-computer conversations had the
highest relative improvement (42%) in mean nat-
uralness. Naturalness ratings were significantly
different when comparing these conversations
with and without enhancements (H(1) = 11.77, p
< 0.05). Content-free conversational phenomena
appear to compensate for the lack of logical flow
in these conversations. According to Figure 5,
after enhancements people are no better than
chance at correctly determining the speakers in
a computer-computer conversation. Thus the
heuristic enhancements clearly affect naturalness
judgments.
Even the naturalness of conversations with good
logical flow can improve with heuristic adjust-
ments; there was a 26% relative improvement in
the mean naturalness of human-human conver-
sations. Participant ratings of naturalness were
again significantly different (H(1) = 12.45, p <
0.05). Note that these conversations were origi-
nally typed dialogue. As such, they did not capture
turn-taking properties present in conversational
speech. When enhanced with conversational phe-
nomena, they more closely resembled natural spo-
ken conversations. As shown in Figure 5, people
are more likely than chance to correctly identify
two humans as being the participants in the di-
alogue after these enhancements were applied to
speech.
Conversations with one computer and one hu-
man also benefited from heuristic enhancements.
Improvements in naturalness were marginal, how-
ever. Naturalness scores in the hc condition im-
proved by 16%, but this improvement was only
a trend (H(1) = 3.66, p = 0.06). Improvement
was negligible in the ch condition. Participants
selected the correct speakers in human-computer
dialogues no better than random. We note that
participants tended to avoid ranking conversations
as ?human & computer? with confidence (i.e., the
highest rank). A significant majority (267 out of
400) of second-rank selections were ?human &
computer.? Participants tended to order conditions
93
1.5
2.5 2.1 2.42.2
2.5 2.4
3.0
1.5
2.0
2.5
3.0
3.5
1.0 cc ch hc hh
no_enhance all_enhance
Figure 4: Mean naturalness across enhancement
conditions.
66.0%
34.0%
16.0%
30.0%30.0%
16.0%
44.0% 56.0%
10%20%
30%40%
50%60%
70%
0% cc ch hc hh
no_enhance all_enhance
Figure 5: Percentage of participants? selections of
members of the conversation that were correct.
from all human to all computer or vice-versa.
5 Conclusions
We have shown that content-independent heuris-
tics can be used to improve the perceived natural-
ness of conversations. Our conversations sampled
a variety of interactions using Talkie, a social di-
alogue system that converses about recent news
headlines. An experiment examined the factors
that could influence how external judges rate the
naturalness of these conversations.
We found that without enhancements, people
rated conversations involving a human and a com-
puter similarly to conversations involving two hu-
mans. Adding heuristic enhancements produced
different results, depending on the conversation
type: computer-computer and human-human con-
versations had the best gain in naturalness scores.
Though it remains to be seen if people are always
influenced by such enhancements, they are clearly
useful for improving the naturalness of human-
computer dialogues.
Future work will involve developing methods to
automatically inject enhancements into the synthe-
sized speech output produced by Talkie, as well
as determining whether other types of systems can
benefit from these techniques.
Acknowledgments
We would like to thank Aasish Pappu, Jose-Pablo
Gonzales Brenes, Long Qin, and Daniel Lim for
developing the Talkie dialogue system.
References
J. Adell, A. Bonafonte, and D. Escudero. Filled pauses
in speech synthesis: Towards conversational speech.
In TSD?07, Pilsen, Czech Republic, 2007.
S. Andersson, K. Georgila, D. Traum, M. Aylett, and
R.A.J. Clark. Prediction and realisation of con-
versational characteristics by utilising spontaneous
speech for unit selection. In the 5th International
Conference on Speech Prosody, Chicago, Illinois,
USA, 2010.
T. Bickmore and J. Cassell. Social Dialogue with Em-
bodied Conversational Agents. J. van Kuppevelt, L.
Dybkjaer, and N. Bernsen (eds.), Natural, Intelligent
and Effective Interaction with Multimodal Dialogue
Systems. New York: Kluwer Academic.
A. Black. CLUSTERGEN: A Statistical Parametric
Synthesizer using Trajectory Modeling. In Inter-
speech?06 - ICSLP, Pittsburgh, PA, 2006.
A. Black and K. Lenzo. Flite: a small fast run-time
synthesis engine. In ISCA 4th Speech Synthesis
Workshop, Scotland, 2001.
R. Carlson and K. Gustafson and E. Strangert. Cues for
Hesitation in Speech Synthesis. In Interspeech?06 -
ICSLP, Pittsburgh, PA, 2006.
S. Higuchi, R. Rzepka, and K. Araki. A casual conver-
sation system using modality and word associations
retrieved from the web. In EMNLP?08. Honolulu,
Hawaii, 2008.
D. Lim, A. Pappu, J. Gonzales-Brenes, and L. Qin.
The Talkie Spoken Dialogue System. Unpublished
manuscript, Carnegie Mellon Univeristy, 2009.
C. Nass and K. M. Lee. Does computer-synthesized
speech manifest personality? Experimental tests of
recognition, similarity-attraction, and consistency-
attraction. Journal of Experimental Psychology:
Applied 7 (2001) 171-181.
M. A. Walker, S. J. Whittaker, A. Stent, P. Maloor, J.
Moore, M. Johnston, G. Vasireddy. Generation and
evaluation of user tailored responses in multimodal
dialogue. Cognitive Sci. 28 (2004) 811-840.
94
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 157?164,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
Comparing Spoken Language Route Instructions  for Robots across Environment Representations 
  Matthew Marge School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213 mrmarge@cs.cmu.edu 
Alexander I. Rudnicky School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213 air@cs.cmu.edu     Abstract 
Spoken language interaction between humans and robots in natural environments will neces-sarily involve communication about space and distance. The current study examines people?s close-range route instructions for robots and how the presentation format (schematic, vir-tual or natural) and the complexity of the route affect the content of instructions. We find that people have a general preference for providing metric-based instructions. At the same time, presentation format appears to have less im-pact on the formulation of these instructions. We conclude that understanding of spatial lan-guage requires handling both landmark-based and metric-based expressions. 1 Introduction Spoken language interaction between humans and robots in natural environments will necessar-ily involve communication about space and dis-tance. It is consequently useful to understand the nature of the language that humans would use for this purpose. In the present study we examine this question in the context of formulating route instructions given to robots. For practical pur-poses, we are also interested in understanding how presentation format affects such language. Instructions given in a physical space might dif-fer from those given in a virtual world, which in turn may differ from those given when only a schematic representation (e.g., a map or drawing) is available.  There is general agreement that landmarks play an important role in spatial language (Dan-iel and Denis, 2004; Klippel and Winter, 2005; Lovelace et al, 1999; MacMahon, 2007; Michon and Denis, 2001; Nothegger et al, 2004; Raubal 
and Winter, 2002; Weissensteiner and Winter, 2004). However, landmarks might not necessar-ily be used uniformly in instructions across pres-entation formats. For example, people may use objects in the environment as landmarks more often when they do not have a good sense of dis-tance in the environment. Behaviors related to spatial language may change based on the com-plexity of the route that a robot must take. This could be due to a combination of factors, includ-ing ease of use and personal assessment of a ro-bot?s ability to interpret specific distances over landmarks.   Several studies have investigated written or typed spatial language (e.g., MacMahon et al, 2006; Koulori and Lauria, 2009; Kollar et al, 2010). In addition, Ross (2008) studied models of spoken language interpretation in schematic views of areas.  In the current study we focus on close-range spoken language route instructions.  2 Related Work Interpreting spatial language is an important ca-pability for systems (e.g., mobile robots) that share space with people. Human-human commu-nication of spatial language has been extensively studied. Talmy (1983) proposed that the nature of language places constraints on how people communicate about space with others (i.e., schematization). Spatial descriptions are primar-ily influenced by how reference objects fit along fundamental axes that exhibit clear relationships with the target, and secondly by the salience of references (Carlson and Hill, 2008). People also tend to keep their spatial descriptions consistent after making an initial choice of strategy based on any existing relationships between the target to be described and other references (Vorwerg, 2009).  
157
Studies involving spatial language with robots have thus far focused on scenarios where one robot is moved around an area using spatial prepositions (Stopp et al, 1994; Moratz et al, 2003) and further with landmarks (Skubic et al, 2002; Perzanowski et al, 2003). A number of these approaches, however, were crafted by the designers of the robots themselves and not nec-essarily based on an understanding of what comes naturally to people. Indeed, Shi and Ten-brink (2009) found that a person?s internal lin-guistic representations may differ significantly from what a robot is capable of interpreting. Bugmann et al (2004) motivated the concept of corpus-based robotics, where spontaneous spo-ken commands are collected and in turn used for designing the functionality of robots. They col-lected natural language instructions from people commanding robots in a miniature of a real-world environment. Our approach follows this same reasoning; we explore naturally occurring spatial language through route instructions to robots in three distinct formats (schematic, vir-tual, and natural environments).  3 Method We designed and conducted three experiments using a navigation task that required the partici-pant to ?tell? a robot how to move to a target lo-cation. We varied the presentation formats of the stimuli (two-dimensional schematics, three-dimensional virtual scenes, real-world areas in-person). In each variant, the participant observed a static scene depicting two robots (?Mok? and ?Aki?) and a destination marker. The partici-pant?s task was to move Mok to the target desti-nation using spoken instructions. Participants were told to act as if they were an observer of the 
scene but that were themselves not present in the scene; put otherwise, the robots could hear par-ticipants but not see them (and thus the partici-pant could not figure in the instructions).  The experiment instructions directed partici-pants to assume that Mok would understand natural language and were told to use natural ex-pressions to specify instructions (that is, there was no ?special language? necessary). Partici-pants were told that they could take the orienta-tions of the robots into account when they formu-lated their instructions. They were moreover asked to include all necessary steps in a single utterance (i.e., a turn composed of one or more spatial language commands). The robots did not move in the experiments. Since our aim was to learn about spoken lan-guage route instructions, all participants recorded their requests using a simple recorder interface that could be activated while viewing the scene. A standard headset microphone was used. To avoid self-correction while speaking, the instruc-tions directed participants to think about their instructions before recording. Participants could playback their instructions, and re-record them if they deemed them unsatisfactory. All interface activity was time-stamped and logged.  3.1 General variations In their work, Hayward and Tarr (1995) found that people used spatial language with reference to landmarks most often and found it most suit-able when the objects in a scene were horizon-tally or vertically aligned. We systematically var-ied three elements of the stimuli in this study: the orientations of the two robots, Mok and Aki, and the location of the destination marker. Each ro-bot?s orientation was varied four ways: directly pointing forward, right, left, or backward. The  
 
                 (a)    (b)     (c)  Figure 1. Stimuli from the (a) schematic, (b) virtual, and (c) real-world scene experiments. Each scenario has 2 robots, Mok (left) and Aki (right). Mok is the actor in all scenarios. Outlined are possible destinations for Mok. 
    Mok                 Aki Mok                   Aki  
Mok                       Aki 
158
 Figure 2. Specified are four potential goal desti-nations for Mok, the actor in all scenarios. Only one of the destinations is shown on a particular trial.  destination marker was also varied four ways: directly in front of, behind, right of, or left of Aki. These three dimensions were varied using a factorial design, yielding 64 different configura-tions that were presented in randomized order. Thus each participant produced 64 sets of in-structions. Participants received a break at the halfway point of the session.  3.2 Schematic (2-D) Scene Experiment Participants observed two-dimensional configu-rations of schematics that contained two robots (Mok and Aki) and a destination marker in this experiment. Each participant viewed a single monitor displaying a recording interface overlaid by static slides that contained the stimuli. After each participant was shown the speech recording interface and had tried it out, they proceeded through a randomly ordered slide set. In this ex-periment, participants viewed an overhead per-spective of the scene, with the robots represented as arrows and the destination marked by purple circles (see Figures 1a and 2). The robots were represented by arrows that were meant to indi-cate their orientations in the scene. 3.3 Virtual (3-D) Scene and Distance Awareness Variation Experiment In this experiment, we crafted stimuli with a three-dimensional map builder and USARSim, a virtual simulation platform designed for conduct-ing experiments with robots (Carpin et al, 2007). The map was designed such that trials were ?rooms? in a multi-room environment. Partici-pants did not walk through the environment; they only viewed static configurations. Included in the map were instances of two Pioneer P2AT robots. All visual stimuli were presented at an eye-level view, with eyes at a height of 5?10? (see Figure 
1b). The room was designed such that walls would be too far away to serve as landmarks. Visual stimuli for this experiment required full-screen access to the game engine, so the record-ing interface was moved to an adjoining monitor.  We included an additional condition: inform-ing participants (or not) of the distance between the two robots. We recruited fourteen partici-pants for this study, seven in each of two condi-tions. In one condition (no-dist), participants were not given any information related to the scale of the robots and area in the stimuli. This is equivalent to what participants experienced in the schematic scene experiment. In the second condition (dist), the instructions indicated that the two robots, Mok and Aki, were seven feet apart. However, no scale information (e.g., a ruler) was provided in the scene itself. This would provide the option to cast instructions in terms of absolute distances. The option to use Aki as a landmark reference point remained the same as in the first experiment. We hypothesize that participants that are not given a sense of scale will use landmarks much more often than those participants that are provided distance in-formation.  3.4 Real-World Scene Experiment In natural environments, it can be assumed that people generally have a good sense of scale. In this experiment, participants viewed similar stimuli to the virtual scenarios (eye-level view), but in-person (see Figure 1c). Bins were used to represent the two robots, with two eyes placed on top of each bin to indicate orientation. As in the previous experiments, participants were told to give instructions to one robot (Mok) so that it would arrive at the destination. We recorded par-ticipant instructions for 8 different configurations of the two robots (destination varied four ways, Mok?s orientation varied two ways, right and left; Aki?s orientation did not change). We sim-plified the number of orientations because we found that orientations of Mok and Aki did not influence landmark use in the previous experi-ments. After each instruction, participants were asked to close their eyes as the experimenter changed the orientations. Since they were not at a computer screen for this experiment, only ver-bal instructions were recorded, with no task times. 3.5 Participation A total of 35 participants were recruited for this study, 10 in the schematic scene experiment, 14 
159
in the virtual scene experiment, and 11 in the real-world scene experiment. Participants ranged in age from 19 to 61 (M = 28.4 years, SD = 9.9). Of all participants, 22 were male and 14 were female. All participants were self-reported fluent English speakers. 4 Data The first study (schematic stimuli) yielded a total of 640 route instructions (64 from each of 10 participants). All of these instructions were tran-scribed in-house using the CMU Communicator guidelines (Bennett and Rudnicky, 2002). In ad-dition to the recorded instructions, we also logged participants? interactions with the speech recording interface. Since the experiment instruc-tions ask participants to think about what they plan to say before recording their speech, we as-sessed their ?thinking time? from this logging information. In the second study (virtual stimuli), more par-ticipants were recruited, but they were divided into two conditions (presence/absence of an ex-plicitly stated metric distance between the two robots in the stimuli). A total of 896 route in-structions were collected in the second study (64 
from each of 14 participants). Of the 14 partici-pants recruited for this study, 12 were transcribed using Amazon?s Mechanical Turk (Marge et al, 2010) with the same guidelines as the first study. In the real-world study, 8 route instructions were recorded from 11 participants and transcribed, yielding a total of 88 utterances.  5 Measurements Several outcomes were analyzed in this study, including the time needed to formulate directions to the robot and the number of discrete steps that participants included in their instructions. We analyzed two measures, ?thinking time? and word count. Thinking time represents the time between starting viewing a stimulus and pressing the ?Record? button. We measured utterance length by counting the number of words spoken by participants for each instruction. Utterance-level restarts and mispronunciations were ex-cluded from this count.  We also coded the instructions in terms of the number of discrete ?steps? (see Table 1). We defined a ?step? as any action where motion by Mok (the moving robot) was required to com-plete a sub-goal. For example, ?turn left and  
Environment Type Spoken language route instruction (transcribed with fillers removed) 2-D Mixed Mok turn left / and stop at the right hand side of Aki. 2-D Mixed Turn right about sixty degrees / then go forward until you're in front of Aki. 
3-D no-dist Mixed  Mok turn to your left / move towards Aki when you are pretty close to Aki stop there / turn to your right / continue moving in a straight line path you will find a blue dot to your left at some point stop there / turn to your left / and reach the blue dot which is your destination. 3-D no-dist Relative Go forward half the distance between you and Aki. 
3-D dist Absolute Rotate to your right / move forward about five feet / rotate again to your left / and move forward about seven feet. 
3-D dist Absolute Turn to your right / move forward one foot / turn to your left / move forward ten feet / turn to your left again / move forward one foot. 
Real-world Absolute Okay Mok I want you to go straight ahead for about five feet / then turn to your right forty five degrees / and go ahead and you're gonna hit the spot in about four feet from there. Real-world Mixed Mok move to Aki / turn left / and move forward three feet. 
Table 1. Spoken language route instructions for Mok, the moving robot, were transcribed and di-vided into absolute and relative steps (absolute step / relative step). Absolute steps are explicit in-structions that contain metric or metric-like distances, while relative steps include Aki (the static robot) as a reference.  
160
 Figure 3. Mean proportion of relative steps to absolute steps across distance-na?ve 2-D (sche-matic), distance-na?ve 3-D (virtual), distance-aware 3-D (virtual), and real-world scenarios (with a 1% margin of error).  
 Figure 4. Proportions of instruction types across distance-na?ve 2-D (schematic), distance-na?ve 3-D (virtual), distance-aware 3-D (virtual), and real-world scenarios.  move forward five feet? consists of two steps: (1) a ninety degree turn to the left and (2) a move-ment forward of five feet to get to a new loca-tion. We divided steps into two categories, abso-lute steps and relative steps (similar to Levin-son?s (1996) absolute and intrinsic reference sys-tems). An absolute step is one with explicit in-structions that contain metric or metric-like dis-tances (e.g., ?move forward two feet?, ?turn right ninety degrees?, ?move forward three steps?). We assume that simple turns (e.g., ?turn right?) 
are turns of 90 degrees, and thus are absolute steps. We define a relative step as one that in-cludes Aki, the static robot, in the reference (e.g., ?move forward until you reach Aki?, ?turn right until you face Aki?).  6 Results We conducted analyses based on measures of thinking time, word count, and the number of discrete ?steps? in participants? spoken language route instructions. Among the folds of the data we examined were observations from schematics without distance information (i.e., ?2-D no-dist?), virtual scenes without giving participants distance information (i.e., ?3-D no-dist?), virtual scenes with giving participants initial distance information (i.e., ?3-D dist?), and real-world scenes (i.e., ?realworld?). Since we collected an equal number of route instructions in the two virtual scene conditions (i.e., with and without being told about the distance in the environ-ment), we directly compared properties of these instructions.   In Sections 6.2 and 6.3, absolute steps, rela-tive steps, word count (log-10 transformed), and thinking timing (log-10 transformed) were the dependent measures in mixed-effects models of analysis of variance (for significance testing). ParticipantID was modeled as a random effect. We are interested in the population from which participants were drawn. 6.1 Adjusting Spatial Information Landmark use was affected by participants? awareness of scale. The fewer scale cues avail-able, the greater the number of references to landmarks. Thus, landmarks were most prevalent in instructions generated for schematic scenarios and least prevalent in the condition that explicitly specified a scale. See Figure 3 for the actual pro-portions. We did not inform participants of scale in the real-world condition. Interestingly, their absolute/relative mix was closer to the no-scale conditions even though they were observing an actual scene and could presumably make infer-ences about distances. Figure 4 shows that pres-entation format also affected participants? use of instructions that were entirely absolute in nature. There were fewer mixed instructions (i.e., in-structions where absolute instructions were sup-ported by landmarks) in conditions where par-ticipants had a sense of scale.  Though distances may be self-evident in real-world scenarios, they often are not in virtual en-
58.9% 68.5% 
93.5% 
73.1% 
41.1% 31.5% 
6.5% 
26.9% 
0% 
10% 
20% 
30% 
40% 
50% 
60% 
70% 
80% 
90% 
100% 
2d 3d nodist 3d dist realworld 
Absolute Proportion Relative Proportion 
27.7% 36.0% 
77.6% 
54.7% 
14.4% 7.9% 
0.9% 
16.3% 
58.0% 56.2% 
21.5% 29.1% 
0% 
10% 
20% 
30% 
40% 
50% 
60% 
70% 
80% 
90% 
100% 
2d 3d nodist 3d dist realworld 
Absolute Relative Mixed 
161
vironments. Participants behaved differently from real-world scenarios when we presented a non-trivial indication of scale. Participants? in-structions were dominated by absolute instruc-tions when they had a sense of scale in a virtual environment. This suggests that despite similari-ties in scale awareness, people formulate spatial language instructions differently when they can-not for themselves determine a sense of distance in an environment. 6.2 Sense of Distance in Virtual Stimuli We directly compared participants? spoken lan-guage route instructions with respect to the pres-ence (i.e., ?dist?) or absence (i.e., ?no-dist?) of distance information in the virtual environment. Though participants already had an initial prefer-ence toward using metric-based instructions, these became dominant when participants were aware of the distance in the virtual environment.  Participants that were not given a sense of dis-tance referred to Aki as a landmark much more than when participants were given a sense of dis-tance, confirming our initial hypothesis. We ob-served that the mean number of relative steps in the no-dist condition was nearly four times greater (1.0 relative steps per instruction) than the dist condition (0.2 relative steps per instruc-tion) (F[1, 12] = 4.6, p = 0.05). As expected, par-ticipants used absolute references more in the dist condition, given the lack of landmark use. The mean number of absolute steps was greater in the dist condition (3.3 per instruction) com-pared to the no-dist condition (mean 2.4 absolute steps per instruction) (F[1, 12] = 5.5, p < 0.05).  As shown in Figure 3, the proportions of abso-lute to relative steps in participants? instructions show clear differences in strategy. When partici-pants received distance information, an over-whelming majority of steps were absolute in na-ture (i.e., steps containing metric or metric-like distances). Aki was mentioned in steps only 6.5% of the time in the dist condition (i.e., rela-tive steps). The proportions were more balanced in the no-dist condition, with 68% of steps being absolute. The remaining 32% of steps referred to Aki. The difference between proportions from the no-dist and dist conditions was statistically significant (F[1,12] = 7.5, p < 0.05). From these analyses we can see that distance greatly influ-enced participants? language instructions in vir-tual environments.  We further classified participants? instructions as entirely absolute, relative, or mixed in nature. When participants used landmarks, they tended 
to mix them with absolute steps in their instruc-tions. Participants in the dist condition comprised most instructions with only absolute steps. How-ever, even though 6.5% of steps were absolute in nature, they were distributed among one-fifth of instructions. In the no-dist condition, though relative steps comprised only 31.5% of total steps, they were distributed among a majority of the instructions. These results suggest that se-quences of absolute steps may be sufficient on their own, but relative steps, when used, depend on the presence of some absolute terms. 6.3 Goal Location and Orientation Results Our analysis showed that the goal location in scenarios impacted participants? instructions. For word count, participants used significantly dif-ferent numbers of words based on the goal loca-tion (F[3, 1580] = 252.2, p < 0.0001). Upon fur-ther analysis, across all experiments, when the goal was closest to the Mok, the moving robot, people spoke fewer words (14 fewer words on average) compared to other locations (analysis conducted with a Tukey pairwise comparisons test). Participants also had significantly different thinking times based on the goal location (F[3, 1502] = 6.21, p < 0.05). Thinking time for the destination closest to Mok was lowest overall (on average at least 1.3s lower) and significantly dif-ferent from two of the three remaining goal loca-tions (via a Tukey pairwise comparisons test). There were no significant differences in word count and thinking time when varying Mok?s orientation or Aki?s orientation.  We also observed patterns in the steps people gave in their instructions. A landmark?s place-ment, when directly interfering with a goal, in-creased its reference in spatial language instruc-tions. When the goal location was blocked by Aki, we observed a high proportion of relative steps. For schematic stimuli, participants often required Mok to move past Aki in order to get to the destination. After observing the proportions of absolute steps and relative steps out of the to-tal number of steps across destination, we found that stimuli with this destination yielded an aver-age of 45% relative steps to 55% absolute steps. This is a greater proportion than any of the other destinations (their relative step proportions ranged from 33% to 38%). 7 Summary and Conclusions We presented a study that examines people?s close-range spoken language route instructions 
162
for robots and how the presentation format and the complexity of the route influenced the con-tent of instructions. Across all presentation for-mats, people preferred providing instructions that were absolute in nature (i.e., metric-based). De-spite this preference, landmarks were used on occasion. When they were, participants? use of them was influenced by the presentation format (schematic, virtual or natural). When participants had a general sense of distance in scenes, they were much more acclimated to using specific distances to give route instructions to a robot.     Our results indicate that the goal location can influence participant effort (i.e., time to formu-late) and the pattern (absolute/relative) in spoken language route instructions to robots. Several of these were predictable (e.g., least effort when goal location was closest to moving robot). When participants viewed these configurations in virtual environments, there were clear differ-ences in their instructions based on whether or not they were given a sense of scale.  We compared the natural language instruc-tions from the real-world condition to those from virtual stimuli. Figure 3 shows that in general, real-world participants? instructions contained similar proportions of landmarks to the 3d no-dist (virtual) condition. However, there was a greater preference to use absolute steps in the real-world than in the virtual world; participants apparently access their own sense of scale when formulating these instructions. With respect to spatial language instructions, participants tended to treat virtual environments much like real-world environments. This study provides useful information about methodology in the study of spatial language and also suggests principles for the design of spatial language understanding capabilities for robots in human environments. Specifically, virtual world representations, under suitable conditions, elicit language similar to that found under real-world situations, although the more information people have about the metric properties of the environ-ment the more likely they are to use them. But even in the absence of unambiguous metrics people seem to want to use such language in the instructions that they produce. These observa-tions can be used to inform the design of spatial language understanding for robot systems as well as guide the development of requirements for a spatial reasoning component.     
Acknowledgments This work was supported by the Boeing Com-pany and a National Science Foundation Gradu-ate Research Fellowship. The authors would like to thank Carolyn Ros?, Satanjeev Banerjee, Aasish Pappu, and the anonymous reviewers for their helpful comments on this work. The views and conclusions expressed in this document only represent those of the authors. References  C. Bennett and A. I. Rudnicky. 2002. The Carnegie Mellon Communicator Corpus, ICSLP, 2002.  G. Bugmann, E. Klein, S. Lauria, and T. Kyriacou. 2004. Corpus-based robotics: A route instruction example, Intelligent Autonomous System, pp. 96-103. L. A. Carlson and P. L. Hill. 2008. Processing the presence, placement and properties of a distractor during spatial language tasks, Memory and Cognition, 36, pp. 240-255. S. Carpin, M. Lewis, J. Wang, S. Balakirsky, and C. Scrapper. 2007. USARSim: A Robot Simulator for Research and Education, International Conference on Robotics and Automation, 2007, pp. 1400-1405. M. P. Daniel and M. Denis. 2004. The production of route directions: Investigating conditions that favour conciseness in spatial discourse, Applied Cognitive Psychology, 18, pp. 57-75. W. G. Hayward and M. J. Tarr. 1995. Spatial language and spatial representation, Cognition, 55 (1), pp. 39-84. A. Klippel and S. Winter. 2005. Structural Salience of Landmarks for Route Directions, COSIT 2005, pp. 347-362. T. Kollar, S. Tellex, D. Roy, and N. Roy. 2010. Toward Understanding Natural Language Directions, Human Robot Interaction Conference (HRI-2010), pp. 259-266. T. Koulouri and S. Lauria. 2009. Exploring Miscommunication and Collaborative Behaviour in Human-Robot Interaction, SIGdial 2009, pp. 111-119. S. C. Levinson. 1996. Frames of reference and Molyneux?s question: cross-linguistic evidence, in P. Bloom, M. Peterson, L. Nadel, and M. Garrett (Eds.), Language and space, pp. 109-169. K. Lovelace, M. Hegarty, and D. R. Montello. 1999. Elements of good route directions in familiar and unfamiliar environments, in C. Freksa and D. M. Mark (Eds.), Spatial information theory: Cognitive and computational foundations of geographic information science. Berlin: Springer. M. MacMahon. 2007. Following Natural Language Route Instructions, Ph.D. Thesis, University of Texas at Austin. M. MacMahon, B. Stankiewicz, and B. Kuipers. 2006. Walk the Talk: Connecting Language, Knowledge, and Action in Route Instructions, 21st 
163
National Conf. on Artificial Intelligence (AAAI), 2006, pp. 1475-1482. M. Marge, S. Banerjee, and A. I. Rudnicky. 2010. Using the Amazon Mechanical Turk for Transcription of Spoken Language, IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2010. Dallas, TX. P. E. Michon and M. Denis. 2001. When and why are visual landmarks used in giving directions? in D. R. Montello (Ed.), Spatial information theory: Foundations of geographic information science, pp. 292-305. Berlin: Springer. R. Moratz, T. Tenbrink, J. Bateman, and K. Fischer. 2003. Spatial knowledge representation for human-robot interaction, Spatial Cognition III. Berlin: Springer-Verlag. C. Nothegger, S. Winter, and M. Raubal. 2004. Selection of salient features for route directions, Spatial Cognition and Computation, 4 (2), pp. 113-136. D. Perzanowski, D. Brock, W. Adams, M. Bugajska, A. C. Schultz, and J. G. Trafton. 2003. Finding the FOO: A Pilot Study for a Multimodal Interface, IEEE Systems, Man, and Cybernetics Conference, 2003. Washington, D.C. M. Raubal and S. Winter. 2002. Enriching wayfinding instructions with local landmarks, in M. J. Egenhofer and D. M. Mark (Eds.), Geographic information science, pp. 243-259. Berlin: Springer. R. Ross. 2008. Tiered Models of Spatial Language Interpretation, International Conference on Spatial Cognition, 2008. Freiburg, Germany. H. Shi and T. Tenbrink. 2009. Telling Rolland where to go: HRI dialogues on route navigation, in K. Coventry, T. Tenbrink, and J. Bateman (Eds.), Spatial Language and Dialogue (pp. 177-190). Oxford University Press. M. Skubic, D. Perzanowski, A. Schultz, and W. Adams. 2002. Using Spatial Language in a Human-Robot Dialog, IEEE International Conference on Robotics and Automation, 2002, pp. 4143-4148. Washington, D.C. E. Stopp, K. P. Gapp, G. Herzog, T. Laengle, and T. Lueth. 1994. Utilizing Spatial Relations for Natural Language Access to an Autonomous Mobile Robot, 18th German Annual Conference on Artificial Intelligence, 1994, pp. 39-50. Berlin. L. Talmy. 1983. How language structures space, in H. Pick, and L. Acredolo (Eds.), Spatial Orientation: Theory, Research and Application.  C. Vorwerg. 2009. Consistency in successive spatial utterances, in K. Coventry, T. Tenbrink, and J. Bateman (Eds.), Spatial Language and Dialogue. Oxford University Press. E. Weissensteiner and S. Winter. 2004. Landmarks in the communication of route instructions, in M. Egenhofer, C. Freksa, and H. Miller (Eds.), GIScience. Berlin: Springer.   
164
