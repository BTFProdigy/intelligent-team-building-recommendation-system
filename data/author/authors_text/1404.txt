The Importance of Supertagging for Wide-Coverage CCG Parsing
Stephen Clark
School of Informatics
University of Edinburgh
2 Buccleuch Place, Edinburgh, UK
stephen.clark@ed.ac.uk
James R. Curran
School of Information Technologies
University of Sydney
NSW 2006, Australia
james@it.usyd.edu.au
Abstract
This paper describes the role of supertagging
in a wide-coverage CCG parser which uses a
log-linear model to select an analysis. The
supertagger reduces the derivation space over
which model estimation is performed, reducing
the space required for discriminative training.
It also dramatically increases the speed of the
parser. We show that large increases in speed
can be obtained by tightly integrating the su-
pertagger with the CCG grammar and parser.
This is the first work we are aware of to success-
fully integrate a supertagger with a full parser
which uses an automatically extracted grammar.
We also further reduce the derivation space us-
ing constraints on category combination. The
result is an accurate wide-coverage CCG parser
which is an order of magnitude faster than com-
parable systems for other linguistically moti-
vated formalisms.
1 Introduction
Lexicalised grammar formalisms such as Lexical-
ized Tree Adjoining Grammar (LTAG) and Com-
binatory Categorial Grammar (CCG) assign one or
more syntactic structures to each word in a sentence
which are then manipulated by the parser. Supertag-
ging was introduced for LTAG as a way of increasing
parsing efficiency by reducing the number of struc-
tures assigned to each word (Bangalore and Joshi,
1999). Supertagging has more recently been applied
to CCG (Clark, 2002; Curran and Clark, 2003).
Supertagging accuracy is relatively high for man-
ually constructed LTAGs (Bangalore and Joshi,
1999). However, for LTAGs extracted automati-
cally from the Penn Treebank, performance is much
lower (Chen et al, 1999; Chen et al, 2002). In
fact, performance for such grammars is below that
needed for successful integration into a full parser
(Sarkar et al, 2000). In this paper we demonstrate
that CCG supertagging accuracy is not only suffi-
cient for accurate and robust parsing using an auto-
matically extracted grammar, but also offers several
practical advantages.
Our wide-coverage CCG parser uses a log-linear
model to select an analysis. The model paramaters
are estimated using a discriminative method, that is,
one which requires all incorrect parses for a sen-
tence as well as the correct parse. Since an auto-
matically extracted CCG grammar can produce an
extremely large number of parses, the use of a su-
pertagger is crucial in limiting the total number of
parses for the training data to a computationally
manageable number.
The supertagger is also crucial for increasing the
speed of the parser. We show that spectacular in-
creases in speed can be obtained, without affecting
accuracy or coverage, by tightly integrating the su-
pertagger with the CCG grammar and parser. To
achieve maximum speed, the supertagger initially
assigns only a small number of CCG categories to
each word, and the parser only requests more cate-
gories from the supertagger if it cannot provide an
analysis. We also demonstrate how extra constraints
on the category combinations, and the application
of beam search using the parsing model, can further
increase parsing speed.
This is the first work we are aware of to succes-
fully integrate a supertagger with a full parser which
uses a lexicalised grammar automatically extracted
from the Penn Treebank. We also report signifi-
cantly higher parsing speeds on newspaper text than
any previously reported for a full wide-coverage
parser. Our results confirm that wide-coverage CCG
parsing is feasible for many large-scale NLP tasks.
2 CCG Supertagging
Parsing using CCG can be viewed as a two-stage
process: first assign lexical categories to the words
in the sentence, and then combine the categories to-
gether using CCG?s combinatory rules.1 The first
stage can be accomplished by simply assigning to
each word all categories from the word?s entry in
the lexicon (Hockenmaier, 2003).
1See Steedman (2000) for an introduction to CCG, and see
Clark et al (2002) and Hockenmaier (2003) for an introduction
to wide-coverage parsing using CCG.
The WSJ is a publication that I enjoy reading
NP/N N (S[dcl]\NP)/NP NP/N N (NP\NP)/(S[dcl]/NP) NP (S[dcl]\NP)/(S[ng]\NP) (S[ng]\NP)/NP
Figure 1: Example sentence with CCG lexical categories
frequency # cat types # cat tokens in # sentences in 2-21 # cat tokens in # sentences in 00
cut-off 2-21 not in cat set with missing cat 00 not in cat set with missing cat
1 1 225 0 0 12 (0.03%) 12 (0.6%)
10 409 1 933 (0.2%) 1 712 (4.3%) 79 (0.2%) 69 (3.6%)
Table 1: Statistics for the lexical category set
An alternative is to use a statistical tagging ap-
proach to assign one or more categories. A statisti-
cal model can be used to determine the most likely
categories given the word?s context. The advan-
tage of this supertagging approach is that the num-
ber of categories assigned to each word can be re-
duced, with a correspondingly massive reduction in
the number of derivations.
Bangalore and Joshi (1999) use a standard
Markov model tagger to assign LTAG elementary
trees to words. Here we use the Maximum En-
tropy models described in Curran and Clark (2003).
An advantage of the Maximum Entropy approach
is that it is easy to encode a wide range of poten-
tially useful information as features; for example,
Clark (2002) has shown that POS tags provide use-
ful information for supertagging. The next section
describes the set of lexical categories used by our
supertagger and parser.
2.1 The Lexical Category Set
The set of lexical categories is obtained from CCG-
bank (Hockenmaier and Steedman, 2002; Hocken-
maier, 2003), a corpus of CCG normal-form deriva-
tions derived semi-automatically from the Penn
Treebank. Following Clark (2002), we apply a fre-
quency cutoff to the training set, only using those
categories which appear at least 10 times in sec-
tions 2-21. Figure 1 gives an example sentence su-
pertagged with the correct CCG lexical categories.
Table 1 gives the number of different category
types and shows the coverage on training (seen) and
development (unseen) data (section 00 from CCG-
bank). The table also gives statistics for the com-
plete set containing every lexical category type in
CCGbank.2 These figures show that using a fre-
quency cutoff can significantly reduce the size of
the category set with only a small loss in coverage.
2The numbers differ slightly from those reported in Clark
(2002) since a newer version of CCGbank is being used here.
Clark (2002) compares the size of grammars
extracted from CCGbank with automatically ex-
tracted LTAGs. The grammars of Chen and Vijay-
Shanker (2000) contain between 2,000 and 9,000
tree frames, depending on the parameters used in
the extraction process, significantly more elemen-
tary structures than the number of lexical categories
derived from CCGbank. We hypothesise this is a
key factor in the higher accuracy for supertagging
using a CCG grammar compared with an automati-
cally extracted LTAG.
2.2 The Tagging Model
The supertagger uses probabilities p(y|x) where y is
a lexical category and x is a context. The conditional
probabilities have the following log-linear form:
p(y|x) = 1
Z(x)e
?
i ?i fi(y,x) (1)
where fi is a feature, ?i is the corresponding weight,
and Z(x) is a normalisation constant. The context
is a 5-word window surrounding the target word.
Features are defined for each word in the window
and for the POS tag of each word. Curran and Clark
(2003) describes the model and explains how Gen-
eralised Iterative Scaling, together with a Gaussian
prior for smoothing, can be used to set the weights.
The supertagger in Curran and Clark (2003) finds
the single most probable category sequence given
the sentence, and uses additional features defined
in terms of the previously assigned categories. The
per-word accuracy is between 91 and 92% on un-
seen data in CCGbank; however, Clark (2002)
shows this is not high enough for integration into a
parser since the large number of incorrect categories
results in a significant loss in coverage.
Clark (2002) shows how the models in (1) can
be used to define a multi-tagger which can assign
more than one category to a word. For each word in
the sentence, the multi-tagger assigns all those cat-
? CATS/ ACC SENT ACC SENT
WORD ACC (POS) ACC
0.1 1.4 97.0 62.6 96.4 57.4
0.075 1.5 97.4 65.9 96.8 60.6
0.05 1.7 97.8 70.2 97.3 64.4
0.01 2.9 98.5 78.4 98.2 74.2
0.01k=100 3.5 98.9 83.6 98.6 78.9
0 21.9 99.1 84.8 99.0 83.0
Table 2: Supertagger accuracy on section 00
egories whose probability according to (1) is within
some factor, ?, of the highest probability category
for the word.
We follow Clark (2002) in ignoring the features
based on the previously assigned categories; there-
fore every tagging decision is local and the Viterbi
algorithm is not required. This simple approach has
the advantage of being very efficient, and we find
that it is accurate enough to enable highly accu-
rate parsing. However, a method which used the
forward-backward algorithm to sum over all possi-
ble sequences, or some other method which took
into account category sequence information, may
well improve the results.
For words seen at least k times in the training
data, the tagger can only assign categories appear-
ing in the word?s entry in the tag dictionary. Each
entry in the tag dictionary is a list of all the cate-
gories seen with that word in the training data. For
words seen less than k times, we use an alternative
dictionary based on the word?s POS tag: the tagger
can only assign categories that have been seen with
the POS tag in the training data. A value of k = 20
was used in this work, and sections 2-21 of CCG-
bank were used as training data.
Table 2 gives the per-word accuracy (acc) on sec-
tion 00 for various values of ?, together with the
average number of categories per word. The sent
acc column gives the precentage of sentences whose
words are all supertagged correctly. The figures for
? = 0.01k=100 correspond to a value of 100 for the
tag dictionary parameter k. The set of categories as-
signed to a word is considered correct if it contains
the correct category. The table gives results for gold
standard POS tags and, in the final 2 columns, for
POS tags automatically assigned by the Curran and
Clark (2003) tagger. The drop in accuracy is ex-
pected given the importance of POS tags as features.
The figures for ? = 0 are obtained by assigning
all categories to a word from the word?s entry in the
tag dictionary. For words which appear less than 20
times in the training data, the dictionary based on
the word?s POS tag is used. The table demonstrates
the significant reduction in the average number of
categories that can be achieved through the use of
a supertagger. To give one example, the number of
categories in the tag dictionary?s entry for the word
is is 45 (only considering categories which have ap-
peared at least 10 times in the training data). How-
ever, in the sentence Mr. Vinken is chairman of Else-
vier N.V., the Dutch publishing group., the supertag-
ger correctly assigns 1 category to is for ? = 0.1,
and 3 categories for ? = 0.01.
3 The Parser
The parser is described in detail in Clark and Curran
(2004). It takes POS tagged sentences as input with
each word assigned a set of lexical categories. A
packed chart is used to efficiently represent all of the
possible analyses for a sentence, and the CKY chart
parsing algorithm described in Steedman (2000) is
used to build the chart.
Clark and Curran (2004) evaluate a number of
log-linear parsing models for CCG. In this paper we
use the normal-form model, which defines proba-
bilities with the conditional log-linear form in (1),
where y is a derivation and x is a sentence. Features
are defined in terms of the local trees in the deriva-
tion, including lexical head information and word-
word dependencies. The normal-form derivations in
CCGbank provide the gold standard training data.
The feature set we use is from the best performing
normal-form model in Clark and Curran (2004).
For a given sentence the output of the parser is
a dependency structure corresponding to the most
probable derivation, which can be found using the
Viterbi algorithm. The dependency relations are de-
fined in terms of the argument slots of CCG lexical
categories. Clark et al (2002) and Clark and Curran
(2004) give a detailed description of the dependency
structures.
3.1 Model Estimation
In Clark and Curran (2004) we describe a discrim-
inative method for estimating the parameters of a
log-linear parsing model. The estimation method
maximises the following objective function:
L?(?) = L(?) ?G(?) (2)
= log
m
?
j=1
P?(d j|S j) ?
n
?
i=1
?2i
2?2
The data consists of sentences S 1, . . . , S m, to-
gether with gold standard normal-form derivations,
d1, . . . , dm. L(?) is the log-likelihood of model ?,
and G(?) is a Gaussian prior term used to avoid
overfitting (n is the number of features; ?i is the
weight for feature fi; and ? is a parameter of the
Gaussian). The objective function is optimised us-
ing L-BFGS (Nocedal and Wright, 1999), an itera-
tive algorithm from the numerical optimisation lit-
erature.
The algorithm requires the gradient of the objec-
tive function, and the value of the objective func-
tion, at each iteration. Calculation of these val-
ues requires all derivations for each sentence in
the training data. In Clark and Curran (2004) we
describe efficient methods for performing the cal-
culations using packed charts. However, a very
large amount of memory is still needed to store the
packed charts for the complete training data even
though the representation is very compact; in Clark
and Curran (2003) we report a memory usage of 30
GB. To handle this we have developed a parallel
implementation of the estimation algorithm which
runs on a Beowulf cluster.
The need for large high-performance computing
resources is a disadvantage of our earlier approach.
In the next section we show how use of the supertag-
ger, combined with normal-form constraints on the
derivations, can significantly reduce the memory re-
quirements for the model estimation.
4 Generating Parser Training Data
Since the training data contains the correct lexical
categories, we ensure the correct category is as-
signed to each word when generating the packed
charts for model estimation. Whilst training the
parser, the supertagger can be thought of as supply-
ing a number of plausible but incorrect categories
for each word; these, together with the correct cat-
egories, determine the parts of the parse space that
are used in the estimation process. We would like
to keep the packed charts as small as possible, but
not lose accuracy in the resulting parser. Section 4.2
discusses the use of various settings on the supertag-
ger. The next section describes how normal-form
constraints can further reduce the derivation space.
4.1 Normal-Form Constraints
As well as the supertagger, we use two additional
strategies for reducing the derivation space. The
first, following Hockenmaier (2003), is to only al-
low categories to combine if the combination has
been seen in sections 2-21 of CCGbank. For exam-
ple, NP/NP could combine with NP/NP according
to CCG?s combinatory rules (by forward composi-
tion), but since this particular combination does not
appear in CCGbank the parser does not allow it.
The second strategy is to use Eisner?s normal-
form constraints (Eisner, 1996). The constraints
SUPERTAGGING/PARSING USAGE
CONSTRAINTS DISK MEMORY
? = 0.01 ? 0.05 ? 0.1 17 GB 31 GB
CCGbank constraints 13 GB 23 GB
Eisner constraints 9 GB 16 GB
? = 0.05 ? 0.1 2 GB 4 GB
Table 3: Space requirements for model training data
prevent any constituent which is the result of a for-
ward (backward) composition serving as the pri-
mary functor in another forward (backward) com-
position or a forward (backward) application. Eis-
ner only deals with a grammar without type-raising,
and so the constraints do not guarantee a normal-
form parse when using a grammar extracted from
CCGbank. However, the constraints are still useful
in restricting the derivation space. As far as we are
aware, this is the first demonstration of the utility of
such constraints for a wide-coverage CCG parser.
4.2 Results (Space Requirements)
Table 3 shows the effect of different supertagger set-
tings, and the normal-form constraints, on the size
of the packed charts used for model estimation. The
disk usage is the space taken on disk by the charts,
and the memory usage is the space taken in mem-
ory during the estimation process. The training sen-
tences are parsed using a number of nodes from a
64-node Beowulf cluster.3 The time taken to parse
the training sentences depends on the supertagging
and parsing constraints, and the number of nodes
used, but is typically around 30 minutes.
The first row of the table corresponds to using
the least restrictive ? value of 0.01, and reverting
to ? = 0.05, and finally ? = 0.1, if the chart size
exceeds some threshold. The threshold was set at
300,000 nodes in the chart. Packed charts are cre-
ated for approximately 94% of the sentences in sec-
tions 2-21 of CCGbank. The coverage is not 100%
because, for some sentences, the parser cannot pro-
vide an analysis, and some charts exceed the node
limit even at the ? = 0.1 level. This strategy was
used in our earlier work (Clark and Curran, 2003)
and, as the table shows, results in very large charts.
Note that, even with this relaxed setting on the su-
pertagger, the number of categories assigned to each
word is only around 3 on average. This suggests that
it is only through use of the supertagger that we are
able to estimate a log-linear parsing model on all of
the training data at all, since without it the memory
3The figures in the table are estimates based on a sample of
the nodes in the cluster.
requirements would be far too great, even for the
entire 64-node cluster.4
The second row shows the reduction in size if
the parser is only allowed to combine categories
which have combined in the training data. This sig-
nificantly reduces the number of categories created
using the composition rules, and also prevents the
creation of unlikely categories using rule combina-
tions not seen in CCGbank. The results show that
the memory and disk usage are reduced by approx-
imately 25% using these constraints.
The third row shows a further reduction in size
when using the Eisner normal-form constraints.
Even with the CCGbank rule constraints, the
parser still builds many non-normal-form deriva-
tions, since CCGbank does contain cases of compo-
sition and type-raising. (These are used to analyse
some coordination and extraction cases, for exam-
ple.) The combination of the two types of normal-
form constraints reduces the memory requirements
by 48% over the original approach. In Clark and
Curran (2004) we show that the parsing model re-
sulting from training data generated in this way
produces state-of-the-art CCG dependency recovery:
84.6 F-score over labelled dependencies.
The final row corresponds to a more restrictive
setting on the supertagger, in which a value of ? =
0.05 is used initially and ? = 0.1 is used if the
node limit is exceeded. The two types of normal-
form constraints are also used. In Clark and Curran
(2004) we show that using this more restrictive set-
ting has a small negative impact on the accuracy of
the resulting parser (about 0.6 F-score over labelled
dependencies). However, the memory requirement
for training the model is now only 4 GB, a reduction
of 87% compared with the original approach.
5 Parsing Unseen Data
The previous section showed how to combine the
supertagger and parser for the purpose of creating
training data, assuming the correct category for each
word is known. In this section we describe our
approach to tightly integrating the supertagger and
parser for parsing unseen data.
Our previous approach to parsing unseen data
(Clark et al, 2002; Clark and Curran, 2003) was
to use the least restrictive setting of the supertag-
ger which still allows a reasonable compromise be-
tween speed and accuracy. Our philosophy was to
give the parser the greatest possibility of finding the
correct parse, by giving it as many categories as pos-
sible, while still retaining reasonable efficiency.
4Another possible solution would be to use sampling meth-
ods, e.g. Osborne (2000).
SUPERTAGGING/PARSING TIME SENTS WORDS
CONSTRAINTS SEC /SEC /SEC
? = 0.01? . . .? 0.1 3 523 0.7 16
CCGbank constraints 1 181 2.0 46
Eisner constraints 995 2.4 55
? = 0.1? . . . 0.01k=100 608 3.9 90
CCGbank constraints 124 19.4 440
Eisner constraints 100 24.0 546
Parser beam 67 35.8 814
94% coverage 49 49.0 1 114
Parser beam 46 52.2 1 186
Oracle 18 133.4 3 031
Table 4: Parse times for section 23
The problem with this approach is that, for some
sentences, the number of categories in the chart still
gets extremely large and so parsing is unacceptably
slow. Hence we applied a limit to the number of
categories in the chart, as in the previous section,
and reverted to a more restrictive setting of the su-
pertagger if the limit was exceeded. We first used
a value of ? = 0.01, and then reverted to ? = 0.05,
and finally ? = 0.1.
In this paper we take the opposite approach: we
start with a very restrictive setting of the supertag-
ger, and only assign more categories if the parser
cannot find an analysis spanning the sentence. In
this way the parser interacts much more closely with
the supertagger. In effect, the parser is using the
grammar to decide if the categories provided by the
supertagger are acceptable, and if not the parser re-
quests more categories. The parser uses the 5 levels
given in Table 2, starting with ? = 0.1 and moving
through the levels to ? = 0.01k=100 .
The advantage of this approach is that parsing
speeds are much higher. We also show that our
new approach slightly increases parsing accuracy
over the previous method. This suggests that, given
our current parsing model, it is better to rely largely
on the supertagger to provide the correct categories
rather than use the parsing model to select the cor-
rect categories from a very large derivation space.
5.1 Results (Parse Times)
The results in this section are all using the best per-
forming normal-form model in Clark and Curran
(2004), which corresponds to row 3 in Table 3. All
experiments were run on a 2.8 GHZ Intel Xeon P4
with 2 GB RAM.
Table 4 gives parse times for the 2,401 sentences
in section 23 of CCGbank. The final two columns
give the number of sentences, and the number of
? CATS/ 0.1 FIRST 0.01 FIRST
WORD PARSES % PARSES %
0.1 1.4 1689 88.4 0 0.0
0.075 1.5 43 2.3 7 0.4
0.05 1.7 51 2.7 39 2.0
0.01 2.9 79 4.1 1816 95.1
0.01k=100 3.5 33 1.7 33 1.7
NO SPAN 15 0.8 15 0.8
Table 5: Supertagger ? levels used on section 00
words, parsed per second. For all of the figures re-
ported on section 23, unless stated otherwise, the
parser is able to provide an analysis for 98.5% of the
sentences. The parse times and speeds include the
failed sentences, but do not include the time taken
by the supertagger; however, the supertagger is ex-
tremely efficient, and takes less than 6 seconds to
supertag section 23, most of which consists of load
time for the Maximum Entropy model.
The first three rows correspond to our strategy of
earlier work by starting with the least restrictive set-
ting of the supertagger. The first value of ? is 0.01;
if the parser cannot find a spanning analysis, this is
changed to ? = 0.01k=100; if the node limit is ex-
ceeded (for these experiments set at 1,000,000), ? is
changed to 0.05. If the node limit is still exceeded,
? is changed to 0.075, and finally 0.1. The second
row has the CCGbank rule restriction applied, and
the third row the Eisner normal-form restrictions.
The next three rows correspond to our new strat-
egy of starting with the least restrictive setting of the
supertagger (? = 0.1), and moving through the set-
tings if the parser cannot find a spanning analysis.
The table shows that the normal-form constraints
have a significant impact on the speed, reducing the
parse times for the old strategy by 72%, and reduc-
ing the times for the new strategy by 84%. The
new strategy also has a spectacular impact on the
speed compared with the old strategy, reducing the
times by 83% without the normal-form constraints
and 90% with the constraints.
The 94% coverage row corresponds to using only
the first two supertagging levels; the parser ignores
the sentence if it cannot get an analysis at the ? =
0.05 level. The percentage of sentences without an
analysis is now 6%, but the parser is extremely fast,
processing almost 50 sentences a second. This con-
figuration of the system would be useful for obtain-
ing data for lexical knowledge acquisition, for ex-
ample, for which large amounts of data are required.
The oracle row shows the parser speed when it
is provided with only the correct lexical categories.
The parser is extremely fast, and in Clark and Cur-
ran (2004) we show that the F-score for labelled
dependencies is almost 98%. This demonstrates
the large amount of information in the lexical cat-
egories, and the potential for improving parser ac-
curacy and efficiency by improving the supertagger.
Finally, the first parser beam row corresponds to
the parser using a beam search to further reduce the
derivation space. The beam search works by prun-
ing categories from the chart: a category can only
be part of a derivation if its beam score is within
some factor, ?, of the highest scoring category for
that cell in the chart. Here we simply use the ex-
ponential of the inside score of a category as the
beam score; the inside score for a category c is the
sum over all sub-derivations dominated by c of the
weights of the features in those sub-derivations (see
Clark and Curran (2004).5
The value of ? that we use here reduces the accu-
racy of the parser on section 00 by a small amount
(0.3% labelled F-score), but has a significant impact
on parser speed, reducing the parse times by a fur-
ther 33%. The final parser beam row combines the
beam search with the fast, reduced coverage config-
uration of the parser, producing speeds of over 50
sentences per second.
Table 5 gives the percentage of sentences which
are parsed at each supertagger level, for both the
new and old parsing strategies. The results show
that, for the old approach, most of the sentences are
parsed using the least restrictive setting of the su-
pertagger (? = 0.01); conversely, for the new ap-
proach, most of the sentences are parsed using the
most restrictive setting (? = 0.1).
As well as investigating parser efficiency, we
have also evaluated the accuracy of the parser on
section 00 of CCGbank, using both parsing strate-
gies together with the normal-form constraints. The
new strategy increases the F-score over labelled de-
pendencies by approximately 0.5%, leading to the
figures reported in Clark and Curran (2004).
5.2 Comparison with Other Work
The only other work we are aware of to investigate
the impact of supertagging on parsing efficiency is
the work of Sarkar et al (2000) for LTAG. Sarkar et
al. did find that LTAG supertagging increased pars-
ing speed, but at a significant cost in coverage: only
1,324 sentences out of a test set of 2,250 received a
parse. The parse times reported are also not as good
as those reported here: the time taken to parse the
2,250 test sentences was over 5 hours.
5Multiplying by an estimate of the outside score may im-
prove the efficacy of the beam.
Kaplan et al (2004) report high parsing speeds
for a deep parsing system which uses an LFG gram-
mar: 1.9 sentences per second for 560 sentences
from section 23 of the Penn Treebank. They also re-
port speeds for the publicly available Collins parser
(Collins, 1999): 2.8 sentences per second for the
same set. The best speeds we have reported for the
CCG parser are an order of magnitude faster.
6 Conclusions
This paper has shown that by tightly integrating a
supertagger with a CCG parser, very fast parse times
can be achieved for Penn Treebank WSJ text. As far
as we are aware, the times reported here are an order
of magnitude faster than any reported for compara-
ble systems using linguistically motivated grammar
formalisms. The techniques we have presented in
this paper increase the speed of the parser by a fac-
tor of 77. This makes this parser suitable for large-
scale NLP tasks.
The results also suggest that further improve-
ments can be obtained by improving the supertag-
ger, which should be possible given the simple tag-
ging approach currently being used.
The novel parsing strategy of allowing the gram-
mar to decide if the supertagging is likely to be cor-
rect suggests a number of interesting possibilities.
In particular, we would like to investigate only re-
pairing those areas of the chart that are most likely
to contain errors, rather than parsing the sentence
from scratch using a new set of lexical categories.
This could further increase parsing effficiency.
Acknowledgements
We would like to thank Julia Hockenmaier, whose
work creating the CCGbank made this research pos-
sible, and Mark Steedman for his advice and guid-
ance. This research was supported by EPSRC grant
GR/M96889, and a Commonwealth scholarship and
a Sydney University Travelling scholarship to the
second author.
References
Srinivas Bangalore and Aravind Joshi. 1999. Supertag-
ging: An approach to almost parsing. Computational
Linguistics, 25(2):237?265.
John Chen and K. Vijay-Shanker. 2000. Automated ex-
traction of TAGS from the Penn Treebank. In Pro-
ceedings of IWPT 2000, Trento, Italy.
John Chen, Srinivas Bangalore, and K. Vijay-Shanker.
1999. New models for improving supertag disam-
biguation. In Proceedings of the 9th Meeting of
EACL, Bergen, Norway.
John Chen, Srinivas Bangalore, Michael Collins, and
Owen Rambow. 2002. Reranking an N-gram su-
pertagger. In Proceedings of the TAG+ Workshop,
pages 259?268, Venice, Italy.
Stephen Clark and James R. Curran. 2003. Log-linear
models for wide-coverage CCG parsing. In Proceed-
ings of the EMNLP Conference, pages 97?104, Sap-
poro, Japan.
Stephen Clark and James R. Curran. 2004. Parsing
the WSJ using CCG and log-linear models. In Pro-
ceedings of the 42nd Meeting of the ACL, Barcelona,
Spain.
Stephen Clark, Julia Hockenmaier, and Mark Steedman.
2002. Building deep dependency structures with a
wide-coverage CCG parser. In Proceedings of the
40th Meeting of the ACL, pages 327?334, Philadel-
phia, PA.
Stephen Clark. 2002. A supertagger for Combinatory
Categorial Grammar. In Proceedings of the TAG+
Workshop, pages 19?24, Venice, Italy.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
James R. Curran and Stephen Clark. 2003. Investigating
GIS and smoothing for maximum entropy taggers. In
Proceedings of the 10th Meeting of the EACL, pages
91?98, Budapest, Hungary.
Jason Eisner. 1996. Efficient normal-form parsing for
Combinatory Categorial Grammar. In Proceedings of
the 34th Meeting of the ACL, pages 79?86, Santa
Cruz, CA.
Julia Hockenmaier and Mark Steedman. 2002. Acquir-
ing compact lexicalized grammars from a cleaner tree-
bank. In Proceedings of the Third LREC Conference,
pages 1974?1981, Las Palmas, Spain.
Julia Hockenmaier. 2003. Data and Models for Statis-
tical Parsing with Combinatory Categorial Grammar.
Ph.D. thesis, University of Edinburgh.
Ronald M. Kaplan, Stefan Riezler, Tracy H. King,
John T. Maxwell III, Alexander Vasserman, and
Richard Crouch. 2004. Speed and accuracy in shal-
low and deep stochastic parsing. In Proceedings of
the HLT/NAACL Conference, Boston, MA.
Jorge Nocedal and Stephen J. Wright. 1999. Numerical
Optimization. Springer, New York, USA.
Miles Osborne. 2000. Estimation of stochastic
attribute-value grammars using an informative sam-
ple. In Proceedings of the 18th International Confer-
ence on Computational Linguistics, pages 586?592,
Saarbru?cken, Germany.
Anoop Sarkar, Fei Xia, and Aravind Joshi. 2000. Some
experiments on indicators of parsing complexity for
lexicalized grammars. In Proceedings of the COLING
Workshop on Efficiency in Large-Scale Parsing Sys-
tems, pages 37?42, Luxembourg.
Mark Steedman. 2000. The Syntactic Process. The MIT
Press, Cambridge, MA.
Wide-Coverage Semantic Representations from a CCG Parser
Johan Bos, Stephen Clark, Mark Steedman
School of Informatics, University of Edinburgh
  jbos,stevec,steedman  @inf.ed.ac.uk
James R. Curran
School of Information Technologies, University of Sydney
james@it.usyd.edu.au
Julia Hockenmaier
Institute for Research in Cognitive Science, University of Pennsylvania
juliahr@linc.cis.upenn.edu
Abstract
This paper shows how to construct semantic
representations from the derivations produced
by a wide-coverage CCG parser. Unlike the de-
pendency structures returned by the parser it-
self, these can be used directly for semantic in-
terpretation. We demonstrate that well-formed
semantic representations can be produced for
over 97% of the sentences in unseen WSJ text.
We believe this is a major step towards wide-
coverage semantic interpretation, one of the key
objectives of the field of NLP.
1 Introduction
The levels of accuracy and robustness recently
achieved by statistical parsers (e.g. Collins (1999),
Charniak (2000)) have led to their use in a num-
ber of NLP applications, such as question-answering
(Pasca and Harabagiu, 2001), machine transla-
tion (Charniak et al, 2003), sentence simplifica-
tion (Carroll et al, 1999), and a linguist?s search
engine (Resnik and Elkiss, 2003). Such parsers
typically return phrase-structure trees in the style
of the Penn Treebank, but without traces and co-
indexation. However, the usefulness of this output
is limited, since the underlying meaning (as repre-
sented in a predicate-argument structure or logical
form) is difficult to reconstruct from such skeletal
parse trees.
In this paper we demonstrate how a wide-
coverage statistical parser using Combinatory Cat-
egorial Grammar (CCG) can be used to generate se-
mantic representations. There are a number of ad-
vantages to using CCG for this task. First, CCG
provides ?surface compositional? analysis of certain
syntactic phenomena such as coordination and ex-
traction, allowing the logical form to be obtained for
such cases in a straightforward way. Second, CCG is
a lexicalised grammar, and only uses a small num-
ber of semantically transparent combinatory rules to
combine CCG categories. Hence providing a com-
positional semantics for CCG simply amounts to as-
signing semantic representations to the lexical en-
tries and interpreting the combinatory rules. And
third, there exist highly accurate, efficient and ro-
bust CCG parsers which can be used directly for
this task (Clark and Curran, 2004b; Hockenmaier,
2003).
The existing CCG parsers deliver predicate argu-
ment structures, but not semantic representations
that can be used for inference. The present paper
seeks to extend one of these wide coverage parsers
by using it to build logical forms suitable for use in
various NLP applications that require semantic in-
terpretation.
We show how to construct first-order represen-
tations from CCG derivations using the ?-calculus,
and demonstrate that semantic representations can
be produced for over 97% of the sentences in unseen
WSJ text. The only other deep parser we are aware
of to achieve such levels of robustness for the WSJ
is Kaplan et al (2004). The use of the ?-calculus
is integral to our method. However, first-order rep-
resentations are simply used as a proof-of-concept;
we could have used DRSs (Kamp and Reyle, 1993)
or some other representation more tailored to the ap-
plication in hand.
There is some existing work with a similar mo-
tivation to ours. Briscoe and Carroll (2002) gen-
erate underspecified semantic representations from
their robust parser. Toutanova et al (2002) and Ka-
plan et al (2004) combine statistical methods with a
linguistically motivated grammar formalism (HPSG
and LFG respectively) in an attempt to achieve levels
of robustness and accuracy comparable to the Penn
Treebank parsers (which Kaplan et al do achieve).
However, there is a key difference between these
approaches and ours. In our approach the creation
of the semantic representations forms a completely
It could cost taxpayers 15 million to install and residents 1 million a year to maintain
NP   S  NP  VP     VP  VP NP   NP  NP NP NP VP  NP conj NP NP VP  NP
it  ?p?x  could    p x ?x?y?p?z  cost    p zx  yxz taxpayers  15M  install  ?p?q  q p residents  1Mpa  maintain 
 T  T  T  B  B 	
    VP  VP NP   NP       VP
  VP NP   NP  NP  VP  VP NP  
    VP  VP NP   NP VP   VP  VP NP  VP       VP  VP NP   NP  NP
?q?y?p?z  q taxpayers  y p z ?r?p?z  r 15M  p z ?s?z  s install  z ?l?q?z  l qz  q residents  1Mpa  maintain  z
 B
  VP  VP NP  
      VP  VP NP   NP  NP
?q?p?z  q taxpayers  15M  p z
 B
VP       VP  VP NP   NP  NP
?q?z  q taxpayers  15M  install  z
 ? 	
VP       VP  VP NP   NP  NP
?q?z  q taxpayers  15M  install  z  q residents  1Mpa  maintain  z

VP
?z  cost    install  z taxpayers   15M  taxpayers  z  cost   maintain  z residents   1Mpa  residents  z
	
S  NP
?z  could    cost    install  z taxpayers   15M  taxpayers  z  could    cost    maintain  z residents   1Mpa  residents  z 

S
could    cost    install  it  taxpayers   15M  taxpayers  it   could    cost    maintain  it  residents   1Mpa  residents  it  
Figure 1: An example CCG derivation with a provisional semantics using predicate-argument structures
separate module to the syntax, whereas in the LFG
and HPSG approaches the semantic representation
forms an integral part of the grammar. This means
that, in order for us to work with another seman-
tic formalism, we simply have to modify the lexical
entries with respect to the semantic component.
2 Combinatory Categorial Grammar
We assume familiarity with CCG (Steedman, 2000),
an entirely type-driven lexicalized theory of gram-
mar based on categorial grammar. CCG lexical en-
tries pair a syntactic category (defining syntactic va-
lency and directionality) with a semantic interpre-
tation. For example, one of the categories for the
verb cost can be written as follows, with a provi-
sional Montague-style semantics expressed in terms
of predicate-argument structure:1

VP 

VP  NP  NP  NP :
?x?y?p?z  cost   p zx  yxz
Combinatory rules project such lexical category-
interpretation pairs onto derived category-
interpretation pairs. The specific involvement
in CCG of rules of functional composition (indexed
 B and  B in derivations) and type-raising (in-
dexed  T and  T) allows very free derivation of
non-standard constituents. This results in semantic
interpretations that support the ?surface composi-
tional? analysis of relativization and coordination,
as in Figure 1 for the sentence It could cost tax-
payers ?15 million to install and BPC residents 1
million a year to maintain.2
1This semantic notation uses x  y  z and p  q  r s as variables,
identifies constants with primes and uses concatenation a b to
indicate application of a to b. Application is ?left-associative,?
so abc is equivalent to  ab  c. The order of arguments in the
predication is ?wrapped?, consistent with the facts of reflexive
binding.
2Some details of the derivation and of the semantics of
noun phrases are suppressed, since these are developed be-
While the proliferation of surface constituents al-
lowed by CCG adds to derivational ambiguity (since
the constituent taxpayers ?15 million to install is
also allowed in the non-coordinate sentence It could
cost taxpayers ?15 million to install), previous work
has shown that standard techniques from the statisti-
cal parsing literature can be used for practical wide-
coverage parsing with state-of-the-art performance.
3 The Parser
A number of statistical parsers have recently been
developed for CCG (Clark et al, 2002; Hocken-
maier and Steedman, 2002b; Clark and Curran,
2004b). All of these parsers use a grammar de-
rived from CCGbank (Hockenmaier and Steedman,
2002a; Hockenmaier, 2003), a treebank of normal-
form CCG derivations derived semi-automatically
from the Penn Treebank. In this paper we use the
Clark and Curran (2004b) parser, which uses a log-
linear model of normal-form derivations to select an
analysis.
The parser takes a POS tagged sentence as in-
put with a set of lexical categories assigned to
each word. A CCG supertagger (Clark and Cur-
ran, 2004a) is used to assign the categories. The
supertagger uses a log-linear model of the target
word?s context to decide which categories to assign.
Clark and Curran (2004a) shows how dynamic use
of the supertagger ? starting off with a small num-
ber of categories assigned to each word and gradu-
ally increasing the number until an analysis is found
? can lead to a highly efficient and robust parser.
The lexical category set used by the parser con-
sists of those category types which occur at least 10
times in sections 2-21 of CCGbank, which results
in a set of 409 categories. Clark and Curran (2004a)
demonstrates that this relatively small set has high
coverage on unseen data and can be used to create
low. Some categories and interpretations are split across lines
to save space.
a robust and accurate parser. The relevance of a rel-
atively small category set is that, in order to obtain
semantic representations for a particular formalism,
only 409 categories have to be annotated.
The parser uses the CKY chart-parsing algorithm
from Steedman (2000). The combinatory rules
used by the parser are functional application (for-
ward and backward), generalised forward composi-
tion, backward composition, generalised backward-
crossed composition, and type raising. There is also
a coordination rule which conjoins categories of the
same type.
The parser also uses a number of unary
type-changing rules (Hockenmaier and Steedman,
2002a) and punctuation rules taken from CCGbank.
An example of a type-changing rule used by the
parser is the following, which takes a passive form
of a verb and creates a nominal modifier:
S   pss  NP  NP  NP (1)
This rule is used to create NPs such as the role
played by Kim Cattrall. An example of a comma
rule is the following:
S  S  S  S (2)
This rule takes a sentential modifier followed by a
comma and returns a sentential modifier of the same
type.
Type-raising is applied to the categories NP, PP
and S   adj  NP (adjectival phrase), and is imple-
mented by adding the relevant set of type-raised
categories to the chart whenever an NP, PP or
S   adj  NP is present. The sets of type-raised cate-
gories are based on the most commonly used type-
raising rule instantiations in sections 2-21 of CCG-
bank, and currently contain 8 type-raised categories
for NP and 1 each for PP and S   adj  NP.
For a given sentence, the automatically extracted
grammar can produce a very large number of deriva-
tions. Clark and Curran (2003) and Clark and Cur-
ran (2004b) describe how a packed chart can be used
to efficiently represent the derivation space, and also
efficient algorithms for finding the most probable
derivation. The parser uses a log-linear model over
normal-form derivations.3 Features are defined in
terms of the local trees in the derivation, including
lexical head information and word-word dependen-
cies. The normal-form derivations in CCGbank pro-
vide the gold standard training data.
For a given sentence, the output of the parser is
a set of syntactic dependencies corresponding to the
3A normal-form derivation is one which only uses type-
raising and function composition when necessary.
most probable derivation. However, for this paper
the parser has been modified to simply output the
derivation in the form shown in Figure 2, which is
the input for the semantic component.
4 Building Semantic Representations
4.1 Semantic Formalism
Our method for constructing semantic representa-
tions can be used with many different semantic for-
malisms. In this paper we use formulas of first-order
logic with a neo-Davidsonian analysis of events. We
do not attempt to cover all semantic phenomena;
for example, we do not currently deal with the res-
olution of pronouns and ellipsis; we do not give
a proper analysis of tense and aspect; we do not
distinguish between distributive and collective read-
ings of plural noun phrases; and we do not handle
quantifier scope ambiguities.
The following first-order formula for the sentence
A spokesman had no comment demonstrates the rep-
resentation we use:

x(spokesman(x) 
	 y(comment(y) 


e(have(e)  agent(e,x)  patient(e,y)))).
The tool that we use to build semantic representa-
tions is based on the lambda calculus. It can be used
to mark missing semantic information from natural
language expressions in a principled way using ?,
an operator that binds variables ranging over vari-
ous semantic types. For instance, a noun phrase like
a spokesman can be given the ?-expression
?p.  x(spokesman(x)  (p@x))
where the @ denotes functional application, and the
variable p marks the missing information provided
by the verb phrase. This expression can be com-
bined with the ?-expression for lied, using func-
tional application, yielding the following expres-
sion:
?p.  x(spokesman(x)  (p@x))@
?y.  e(lie(e)  agent(e,y)).
?-conversion is the process of eliminating all oc-
currences of functional application by substituting
the argument for the ?-bound variables in the func-
tor. ?-conversion turns the previous expression into
a first-order translation for A spokesman lied:

x(spokesman(x)   e(lie(e)  agent(e,x))).
The resulting semantic formalism is very sim-
ilar to the type-theoretic language L? (Dowty et
al., 1981). However, we merely use the lambda-
calculus as a tool for constructing semantic rep-
resentations, rather as a formal tool for model-
theoretic interpretation. As already mentioned, we
can use the same method to obtain, for exam-
ple, Discourse Representation Structures (Kuschert,
1999), or underspecified semantic representations
(Bos, 2004) to deal with quantifier scope ambigu-
ities.
4.2 Method and Algorithm
The output of the parser is a tree representing a
CCG derivation, where the leaves are lexical items
and the nodes correspond to one of the CCG com-
binatory rules, a unary type-changing rule, a type-
raising rule, or one of the additional miscellaneous
rules discussed earlier. Mapping the CCG deriva-
tion into a semantic representation consists of the
following tasks:
1. assigning semantic representations to the lexi-
cal items;
2. reformulating the combinatory rules in terms
of functional application;
3. dealing with type-raising and type-changing
rules;
4. applying ?-conversion to the resulting tree
structure.
Lexical items are ordered pairs consisting of the
CCG category and a lemmatised wordform. This in-
formation is used to assign a ?-expression to the leaf
nodes in the tree. For most open-class lexical items
we use the lemma to instantiate the lexical seman-
tics, as illustrated by the following two examples
(intransitive verbs and adjectives):
 
S[dcl]\NP, walk 
?q?u.q@?x.  e(walk(e)  agent(e,x)  u@e)
 
N/N, big 
?p?x.(big(x)  p@x)
For closed-class lexical items, the lexical seman-
tics is spelled out for each lemma individually, as in
the following two examples:
 
(S[X]\NP)\(S[X]\NP), not 
?v?q?f.  ((v@q)@f)
 
NP[nb]/N, all 
?p?q.	 x(p@x  q@x)
The second task deals with the combinatory rules.
The rules we currently use are forward and back-
ward application (FAPP, BAPP), generalised for-
ward composition (FCOMP), backward composition
(BCOMP), and generalised backward-crossed com-
position (BCROSS).
FAPP

x  y 

x@y 
BAPP

x  y 

y@x 
FCOMP

x  y  ?u   x@  u@y 
BCOMP

x  y  ?u   y@  u@x 
BCROSS

x  y  ?u   y@  x@u 
The type-raising and type-changing rules are
dealt with by looking up the specific rule and replac-
ing it with the resulting semantics. For instance, the
rule that raises category NP to S[X]/(S[X]\NP)
converts the semantics as follows:
TYPERAISE(NP, S[X]/(S[X]\NP), x)
= ?v?e.((v@x)@e)
The following type-changing rule applies to the
lexical semantics of categories of type N and con-
verts them to NP:
TYPECHANGE(N, NP, y)
= ?p.  x(y@x  p@x)
Tasks 1?3 are implemented using a recursive al-
gorithm that traverses the derivation and returns a
?-expression. Note that the punctuation rules used
by the parser do not contribute to the compositional
semantics and are therefore ignored.
Task 4 reduces the ?-expression to the target rep-
resentation by applying ?-conversion. In order to
maintain correctness of this operation, the functor
undergoes ?-conversion (renaming all bound vari-
ables for new occurrences) before substitution takes
place. ?-conversion is implemented using the tools
provided by Blackburn and Bos (2003).
4.3 Results
There are a number of possible ways to evaluate the
semantic representations output by our system. The
first is to calculate the coverage ? that is, the per-
centage of syntactic parses which can be given some
analysis by the semantic component. The second is
to evaluate the accuracy of the semantic representa-
tions; the problem is that there is not yet an accepted
evaluation metric which can be applied to such rep-
resentations.
There is, however, an accepted way of evaluat-
ing the syntactic component of the system, namely
to calculate precision and recall figures for labelled
syntactic dependencies (Clark et al, 2002). Given
bapp(?S[dcl]?,
bapp(?NP?,
fapp(?NP[nb]?,
leaf(?NP[nb]/N?,?the?),
fapp(?N?,
leaf(?N/N?,school-board?),
leaf(?N?,?hearing?))),
fapp(?NP\NP?,
bapp(?(NP\NP)/S[dcl]?,
leaf(?(NP\NP)/NP?,?at?),
leaf(?((NP\NP)/S[dcl])\((NP\NP)/NP)?,?which?)),
bapp(?S[dcl]?,
leaf(?NP?,?she?),
fapp(?S[dcl]\NP?,
leaf(?(S[dcl]\NP)/(S[pss]\NP)?,?was?),
leaf(?S[pss]\NP?,?dismissed?)))),
fapp(?S[dcl]\NP?,
leaf(?(S[dcl]\NP)/(S[pss]\NP)?,?was?),
fapp(?S[pss]\NP?,
leaf(?(S[pss]\NP)/PP?,?crowded?),
fapp(?PP?,
leaf(?PP/NP?,?with?),
bapp(?NP?,
lex(?NP?,leaf(?N?,?students?)),
conj(?conj?,?NP?,?NP\NP?,
leaf(?conj?,?and?),
lex(?NP?,leaf(?N?,?teachers?)))))))).
some A ((school-board[A] & hearing[A]) & some B (female[B] & some C
(dismiss[C] & (patient[C,B] & (at[A,C] & some D (crowd[D] & (patient[D,A]
& ((some E (student[E] & with[D,E]) & some F (teacher[F] & with[D,F])) &
event[D]))))))))
Figure 2: Parser output and semantic representation for the example sentence:
The school-board hearing at which she was dismissed was crowded with students and teachers
that the CCG parser produces dependencies which
are essentially predicate-argument dependencies,
the accuracy of the syntactic component should be
a good indication of the accuracy of the semantics,
especially given the transparent interface between
syntax and semantics used by our system. Hence
we report coverage figures in this paper, and repeat
figures for dependency recovery from an earlier pa-
per.
We do not evaluate the accuracy of the system
output directly, but we do have a way of check-
ing the well-formedness of the semantic represen-
tations. (The well-formedness of the representation
does not of course guarantee the correctness of the
output.) If the semantic representation fails to ?-
convert, we know that there are type conflicts re-
sulting from either: incorrect semantics assigned to
some lexical entries; incorrect interpretation of one
of the combinatory rules; or an inconsistency in the
output of the syntactic component.
We assigned lexical semantics to the 245 most
frequent categories from the complete set of 409,
and implemented 4 of the type-raising rules, and the
10 unary type-changing rules, used by the parser.
We used section 00 from CCGbank for development
purposes; section 23 (2,401 sentences) was used as
the test set. The parser provides a syntactic analysis
for 98.6% of the sentences in section 23. The ac-
curacy of the parser is reported in Clark and Curran
(2004b): 84.6% F-score over labelled dependencies
for section 23. Of the sentences the parser analyses,
92.3% were assigned a semantic representation, all
of which were well-formed. The output of the sys-
tem for an example sentence is given in Figure 2.
The reason for the lack of complete coverage is
that we did not assign semantic representations to
the complete set of lexical categories. In future
work we will cover the complete set, but as a simple
remedy we have implemented the following robust-
ness strategy: we assign a semantic template to parts
of the tree that could not be analysed. For example,
the template for the NP category is ?p.  x(p@x).
This was done for the 10 most frequent categories
and results in a coverage of 98.6%.
Although we expect the accuracy of the seman-
tic representations to mirror those of the syntactic
component, and therefore be useful in NLP applica-
tions, there is still a small number of errors arising
from different sources. First, some constructions are
incorrectly analysed in CCGbank; for example, ap-
positives in CCGbank are represented as coordinate
constructions (Hockenmaier, 2003). Second, errors
are introduced by the semantic construction com-
ponent; for example, the non-head nouns in a noun-
noun compound are currently treated as modifiers of
the head noun, in the same way as adjectives. And
finally, the parser introduces errors because of in-
complete coverage of the lexicon, and mistakes due
to the parsing model. We expect general improve-
ments in statistical parsing technology will further
improve the accuracy of the parser, and we will fur-
ther develop the semantic component.
5 Conclusions and Future Work
This paper has demonstrated that we can construct
semantic representations using a wide-coverage
CCG parser, with a coverage of over 97% on un-
seen WSJ sentences. We believe this is a major step
towards wide-coverage semantic interpretation, one
of the key objectives of the field of NLP.
The advantages of our approach derive largely
from the use of CCG. The lexicalised nature of the
formalism means that our system has a high degree
of modularity, with separate syntactic and semantic
components.
We have shown how to construct simple first-
order semantic representations from CCG deriva-
tions. We have not dealt with all semantic phe-
nomena, such as quantifier scope ambiguities and
anaphora resolution. In future work we will in-
vestigate using underspecified semantic representa-
tions. The utility of our system for NLP applications
will be tested by integration with an existing open-
domain Question-Answering system (Leidner et al,
2003).
We will also investigate the construction of a tree-
bank of semantic representations derived automati-
cally from CCGbank. Previous work, such as Li-
akata and Pulman (2002) and Cahill et al (2003),
has attempted to generate semantic representations
from the Penn Treebank. Cahill et al use a transla-
tion of the Treebank to LFG F-structures and quasi-
logical forms. An advantage of our approach is
that our system for constructing semantic represen-
tations, whatever semantic formalism is used, can
be applied directly to the derivations in CCGbank.
Acknowledgements
This research was partially supported by EPSRC
grant GR/M96889.
References
Patrick Blackburn and Johan Bos. 2003. Represen-
tation and Inference for Natural Language. A First
Course in Computational Semantics. Draft available
at http://www.comsem.org, June.
Johan Bos. 2004. Computational semantics in dis-
course: Underspecification, resolution, and inference.
Journal of Logic, Language and Information, 12(2).
Ted Briscoe and John Carroll. 2002. Robust accurate
statistical annotation of general text. In Proceedings
of the 3rd LREC Conference, pages 1499?1504, Las
Palmas, Gran Canaria.
Aoife Cahill, Mairead McCarthy, Josef van Genabith,
and Andy Way. 2003. Quasi-Logical Forms from
F-Structures for the Penn Treebank. In Harry Bunt,
Ielka van der Sluis, and Roser Morante, editors, Pro-
ceedings of the Fifth International Workshop on Com-
putational Semantics (IWCS-5), pages 55?71. Tilburg
University.
J. Carroll, G. Minnen, D. Pearce, Y. Canning, S. De-
vlin, and J. Tait. 1999. Simplifying text for language-
impaired readers. In Proceedings of the 9th Meeting
of EACL, pages 269?270, Bergen, Norway.
Eugene Charniak, Kevin Knight, and Kenji Yamada.
2003. Syntax-based language models for machine
translation. In Proceedings of the MT Summit IX, New
Orleans, Louisiana.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the 1st Meeting of the
NAACL, pages 132?139, Seattle, WA.
Stephen Clark and James R. Curran. 2003. Log-linear
models for wide-coverage CCG parsing. In Proceed-
ings of the EMNLP Conference, pages 97?104, Sap-
poro, Japan.
Stephen Clark and James R. Curran. 2004a. The impor-
tance of supertagging for wide-coverage CCG pars-
ing. In Proceedings of the 20th International Confer-
ence on Computational Linguistics (COLING-04) (to
appear), Geneva, Switzerland.
Stephen Clark and James R. Curran. 2004b. Parsing
the WSJ using CCG and log-linear models. In Pro-
ceedings of the 42nd Meeting of the ACL (to appear),
Barcelona, Spain.
Stephen Clark, Julia Hockenmaier, and Mark Steedman.
2002. Building deep dependency structures with a
wide-coverage CCG parser. In Proceedings of the
40th Meeting of the ACL, pages 327?334, Philadel-
phia, PA.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
David R. Dowty, Robert E. Wall, and Stanley Peters.
1981. Introduction to Montague Semantics. Studies
in Linguistics and Philosophy. D. Reidel Publishing
Company.
Julia Hockenmaier and Mark Steedman. 2002a. Acquir-
ing compact lexicalized grammars from a cleaner tree-
bank. In Proceedings of the Third LREC Conference,
pages 1974?1981, Las Palmas, Spain.
Julia Hockenmaier and Mark Steedman. 2002b. Gen-
erative models for statistical parsing with Combina-
tory Categorial Grammar. In Proceedings of the 40th
Meeting of the ACL, pages 335?342, Philadelphia, PA.
Julia Hockenmaier. 2003. Data and Models for Statis-
tical Parsing with Combinatory Categorial Grammar.
Ph.D. thesis, University of Edinburgh.
Hans Kamp and Uwe Reyle. 1993. From Discourse
to Logic; An Introduction to Modeltheoretic Seman-
tics of Natural Language, Formal Logic and DRT.
Kluwer, Dordrecht.
Ronald M. Kaplan, Stefan Riezler, Tracy H. King,
John T. Maxwell III, Alexander Vasserman, and
Richard Crouch. 2004. Speed and accuracy in shal-
low and deep stochastic parsing. In Proceedings of
the HLT/NAACL Conference, Boston, MA.
Susanna Kuschert. 1999. Dynamic Meaning and Ac-
commodation. Ph.D. thesis, Universita?t des Saarlan-
des.
Jochen L. Leidner, Johan Bos, Tiphaine Dalmas,
James R. Curran, Stephen Clark, Colin J. Bannard,
Mark Steedman, and Bonnie Webber. 2003. The
QED open-domain answer retrieval system for TREC
2003. In Proceedings of the Twelfth Text Retrieval
Conference (TREC 2003), pages 595?599, Gaithers-
burg, MD.
Maria Liakata and Stephen Pulman. 2002. From trees to
predicate-argument structures. In Shu-Chuan Tseng,
editor, COLING 2002. Proceedings of the 19th In-
ternational Conference on Computational Linguistics,
pages 563?569. Taipei, Taiwan.
Marius Pasca and Sanda Harabagiu. 2001. High per-
formance question/answering. In Proceedings of the
ACL SIGIR Conference on Research and Development
in Information Retrieval, pages 366?374, New Or-
leans LA.
Philip Resnik and Aaron Elkiss. 2003. The lin-
guist?s search engine: Getting started guide. Tech-
nical Report LAMP-TR-108/CS-TR-4541/UMIACS-
TR-2003-109, University of Maryland, College Park,
MA.
Mark Steedman. 2000. The Syntactic Process. The MIT
Press, Cambridge, MA.
Kristina Toutanova, Christopher Manning, Stuart
Shieber, Dan Flickinger, and Stephan Oepen. 2002.
Parse disambiguation for a rich HPSG grammar. In
Proceedings of the First Workshop on Treebanks
and Linguistic Theories, pages 253?263, Sozopol,
Bulgaria.
331
332
333
334
335
336
337
338
91
92
93
94
95
96
97
98
c? 2002 Association for Computational Linguistics
Class-Based Probability Estimation Using
a Semantic Hierarchy
Stephen Clark? David Weir?
University of Edinburgh University of Sussex
This article concerns the estimation of a particular kind of probability, namely, the probability
of a noun sense appearing as a particular argument of a predicate. In order to overcome the
accompanying sparse-data problem, the proposal here is to define the probabilities in terms of
senses from a semantic hierarchy and exploit the fact that the senses can be grouped into classes
consisting of semantically similar senses. There is a particular focus on the problem of how
to determine a suitable class for a given sense, or, alternatively, how to determine a suitable
level of generalization in the hierarchy. A procedure is developed that uses a chi-square test to
determine a suitable level of generalization. In order to test the performance of the estimation
method, a pseudo-disambiguation task is used, together with two alternative estimation methods.
Each method uses a different generalization procedure; the first alternative uses the minimum
description length principle, and the second uses Resnik?s measure of selectional preference. In
addition, the performance of our method is investigated using both the standard Pearson chi-
square statistic and the log-likelihood chi-square statistic.
1. Introduction
This article concerns the problem of how to estimate the probabilities of noun senses
appearing as particular arguments of predicates. Such probabilities can be useful for a
variety of natural language processing (NLP) tasks, such as structural disambiguation
and statistical parsing, word sense disambiguation, anaphora resolution, and language
modeling. To see how such knowledge can be used to resolve structural ambiguities,
consider the following prepositional phrase attachment ambiguity:
Example 1
Fred ate strawberries with a spoon.
The ambiguity arises because the prepositional phrase with a spoon can attach to either
strawberries or ate. The ambiguity can be resolved by noting that the correct sense of
spoon is more likely to be an argument of ?ate-with? than ?strawberries-with? (Li and
Abe 1998; Clark and Weir 2000).
The problem with estimating a probability model defined over a large vocabulary
of predicates and noun senses is that this involves a huge number of parameters,
which results in a sparse-data problem. In order to reduce the number of parameters,
we propose to define a probability model over senses in a semantic hierarchy and
? Division of Informatics, University of Edinburgh, 2 Buccleuch Place, Edinburgh, EH8 9LW, UK. E-mail:
stephenc@cogsci.ed.ac.uk.
? School of Cognitive and Computing Sciences, University of Sussex, Brighton, BN1 9QH, UK. E-mail:
david.weir@cogs.susx.ac.uk.
188
Computational Linguistics Volume 28, Number 2
to exploit the fact that senses can be grouped into classes consisting of semantically
similar senses. The assumption underlying this approach is that the probability of a
particular noun sense can be approximated by a probability based on a suitably chosen
class. For example, it seems reasonable to suppose that the probability of (the food
sense of) chicken appearing as an object of the verb eat can be approximated in some
way by a probability based on a class such as FOOD.
There are two elements involved in the problem of using a class to estimate the
probability of a noun sense. First, given a suitably chosen class, how can that class
be used to estimate the probability of the sense? And second, given a particular noun
sense, how can a suitable class be determined? This article offers novel solutions to
both problems, and there is a particular focus on the second question, which can be
thought of as how to find a suitable level of generalization in the hierarchy.1
The semantic hierarchy used here is the noun hierarchy of WordNet (Fellbaum
1998), version 1.6. Previous work has considered how to estimate probabilities us-
ing classes from WordNet in the context of acquiring selectional preferences (Resnik
1998; Ribas 1995; Li and Abe 1998; McCarthy 2000), and this previous work has also
addressed the question of how to determine a suitable level of generalization in the
hierarchy. Li and Abe use the minimum description length principle to obtain a level
of generalization, and Resnik uses a simple technique based on a statistical measure
of selectional preference. (The work by Ribas builds on that by Resnik, and the work
by McCarthy builds on that by Li and Abe.) We compare our estimation method with
those of Resnik and Li and Abe, using a pseudo-disambiguation task. Our method
outperforms these alternatives on the pseudo-disambiguation task, and an analysis of
the results shows that the generalization methods of Resnik and Li and Abe appear
to be overgeneralizing, at least for this task.
Note that the problem being addressed here is the engineering problem of es-
timating predicate argument probabilities, with the aim of producing estimates that
will be useful for NLP applications. In particular, we are not addressing the problem
of acquiring selectional restrictions in the way this is usually construed (Resnik 1993;
Ribas 1995; McCarthy 1997; Li and Abe 1998; Wagner 2000). The purpose of using a
semantic hierarchy for generalization is to overcome the sparse data problem, rather
than find a level of abstraction that best represents the selectional restrictions of some
predicate. This point is considered further in Section 5.
The next section describes the noun hierarchy from WordNet and gives a more
precise description of the probabilities to be estimated. Section 3 shows how a class
from WordNet can be used to estimate the probability of a noun sense. Section 4 shows
how a chi-square test is used as part of the generalization procedure, and Section 5
describes the generalization procedure. Section 6 describes the alternative class-based
estimation methods used in the pseudo-disambiguation experiments, and Section 7
presents those experiments.
2. The Semantic Hierarchy
The noun hierarchy of WordNet consists of senses, or what Miller (1998) calls lexicalized
concepts, organized according to the ?is-a-kind-of? relation. Note that we are using
concept to refer to a lexicalized concept or sense and not to a set of senses; we use class to
refer to a set of senses. There are around 66,000 different concepts in the noun hierarchy
1 A third element of the problem, namely, how to obtain arguments of predicates as training data, is not
considered here. We assume the existence of such data, obtained from a treebank or shallow parser.
189
Clark and Weir Class-Based Probability Estimation
of WordNet version 1.6. A concept in WordNet is represented by a ?synset,? which is
the set of synonymous words that can be used to denote that concept. For example,
the synset for the concept ?cocaine?2 is { cocaine, cocain, coke, snow, C }. Let syn(c) be the
synset for concept c, and let cn(n) = { c |n ? syn(c) } be the set of concepts that can be
denoted by noun n.
The hierarchy has the structure of a directed acyclic graph (although only around
1% of the nodes have more than one parent), where the edges of the graph constitute
what we call the ?direct?isa? relation. Let isa be the transitive, reflexive closure of
direct?isa; then c? isa c implies c? is a kind of c. If c? isa c, then c is a hypernym of c? and
c? is a hyponym of c. In fact, the hierarchy is not a single hierarchy but instead consists of
nine separate subhierarchies, each headed by the most general kind of concept, such as
?entity?, ?abstraction?, ?event?, and ?psychological feature?. For the purposes of this work
we add a common root dominating the nine subhierarchies, which we denote ?root?.
There are some important points that need to be clarified regarding the hierarchy.
First, every concept in the hierarchy has a nonempty synset (except the notional con-
cept ?root?). Even the most general concepts, such as ?entity?, can be denoted by some
noun; the synset for ?entity? is { entity, something }. Second, there is an important distinc-
tion between an individual concept and a set of concepts. For example, the individual
concept ?entity? should not be confused with the set or class consisting of concepts
denoting kinds of entities. To make this distinction clear, we use c = { c? | c? isa c }
to denote the set of concepts dominated by concept c, including c itself. For exam-
ple, ?animal? is the set consisting of those concepts corresponding to kinds of animals
(including ?animal? itself).
The probability of a concept appearing as an argument of a predicate is written p(c |
v, r), where c is a concept in WordNet, v is a predicate, and r is an argument position.3
The focus in this article is on the arguments of verbs, but the techniques discussed
can be applied to any predicate that takes nominal arguments, such as adjectives. The
probability p(c | v, r) is to be interpreted as follows: This is the probability that some
noun n in syn(c), when denoting concept c, appears in position r of verb v (given
v and r). The example used throughout the article is p(?dog? | run, subj), which is
the conditional probability that some noun in the synset of ?dog?, when denoting the
concept ?dog?, appears in the subject position of the verb run. Note that, in practice,
no distinction is made between the different senses of a verb (although the techniques
do allow such a distinction) and that each use of a noun is assumed to correspond to
exactly one concept.4
3. Class-Based Probability Estimation
This section explains how a set of concepts, or class, from WordNet can be used to
estimate the probability of an individual concept. More specifically, we explain how
a set of concepts c?, where c? is some hypernym of concept c, can be used to estimate
p(c | v, r). (Recall that c? denotes the set of concepts dominated by c?, including c? itself.)
One possible approach would be simply to substitute c? for the individual concept c.
This is a poor solution, however, since p(c? | v, r) is the conditional probability that
2 Angled brackets are used to denote concepts in the hierarchy.
3 The term predicate is used loosely here, in that the predicate does not have to be a semantic object but
can simply be a word form.
4 A recent paper that extends the acquisition of selectional preferences to sense-sense relationships is
Agirre and Martinez (2001).
190
Computational Linguistics Volume 28, Number 2
some noun denoting a concept in c? appears in position r of verb v. For example,
p(?animal? | run, subj) is the probability that some noun denoting a kind of animal
appears in the subject position of the verb run. Probabilities of sets of concepts are
obtained by summing over the concepts in the set:
p(c? | v, r) =
?
c???c?
p(c?? | v, r) (1)
This means that p(?animal? | run, subj) is likely to be much greater than p(?dog? |
run, subj) and thus is not a good approximation of p(?dog? | run, subj).
What can be done, though, is to condition on sets of concepts. If it can be shown
that p(v | c?, r), for some hypernym c? of c, is a reasonable approximation of p(v | c, r),
then we have a way of estimating p(c | v, r). The probability p(v | c, r) can be obtained
from p(c | v, r) using Bayes? theorem:
p(c | v, r) = p(v | c, r) p(c | r)
p(v | r) (2)
Since p(c | r) and p(v | r) are conditioned on the argument slot only, we assume
these can be estimated satisfactorily using relative frequency estimates. Alternatively,
a standard smoothing technique such as Good-Turing could be used.5 This leaves p(v |
c, r). Continuing with the ?dog? example, the proposal is to estimate p(run | ?dog?, subj)
using a relative-frequency estimate of p(run | ?animal?, subj) or an estimate based on a
similar, suitably chosen class. Thus, assuming this choice of class, p(?dog? | run, subj)
would be approximated as follows:
p(?dog? | run, subj) ? p(run | ?animal?, subj)p(?dog? | subj)
p(run | subj) (3)
The following derivation shows that if p(v | c?i , r) = k for each child c?i of c?, and
p(v | c?, r) = k, then p(v | c?, r) is also equal to k:
p(v | c?, r) = p(c? | v, r) p(v | r)
p(c? | r)
(4)
=
p(v | r)
p(c? | r)
(
?
i
p(c?i | v, r) + p(c
? | v, r)
)
(5)
=
p(v | r)
p(c? | r)
(
?
i
p(v | c?i , r)
p(c?i | r)
p(v | r) + p(v | c
?, r)
p(c? | r)
p(v | r)
)
(6)
=
1
p(c? | r)
(
?
i
k p(c?i | r) + k p(c
? | r)
)
(7)
=
k
p(c? | r)
(
?
i
p(c?i | r) + p(c
? | r)
)
(8)
= k (9)
5 Unsmoothed estimates were used in this work.
191
Clark and Weir Class-Based Probability Estimation
Note that the proof applies only to a tree, since the proof assumes that c? is partitioned
by c? and the sets of concepts dominated by each of the daughters of c?, which is not
necessarily true for a directed acyclic graph (DAG). WordNet is a DAG but is a close
approximation to a tree, and so we assume this will not be a problem in practice.6
The derivation in (4)?(9) shows how probabilities conditioned on sets of concepts
can remain constant when moving up the hierarchy, and this suggests a way of finding
a suitable set, c?, as a generalization for concept c: Initially set c? equal to c and move
up the hierarchy, changing the value of c?, until there is a significant change in p(v |
c?, r). Estimates of p(v | c?i , r), for each child c?i of c?, can be compared to see whether
p(v | c?, r) has significantly changed. (We ignore the probability p(v | c?, r) and consider
the probabilities p(v | c?i , r) only.) Note that this procedure rests on the assumption that
p(v | c, r) is close to p(v | c, r). (In fact, p(v | c, r) is equal to p(v | c, r) when c is a leaf
node.) So when finding a suitable level for the estimation of p(?sandwich? | eat, obj),
for example, we first assume that p(eat | ?sandwich?, obj) is a good approximation of
p(eat | ?sandwich?, obj) and then apply the procedure to p(eat | ?sandwich?, obj).
A feature of the proposed generalization procedure is that comparing probabilities
of the form p(v | C, r), where C is a class, is closely related to comparing ratios of
probabilities of the form p(C | v, r)/p(C | r) (for a given verb and argument position):
p(v | C, r) = p(C | v, r)
p(C | r) p(v | r) (10)
Note that, for a given verb and argument position, p(v | r) is constant across classes.
Equation (10) is of interest because the ratio p(C | v, r)/p(C | r) can be interpreted as a
measure of association between the verb v and class C. This ratio is similar to point-
wise mutual information (Church and Hanks 1990) and also forms part of Resnik?s
association score, which will be introduced in Section 6. Thus the generalization pro-
cedure can be thought of as one that finds ?homogeneous? areas of the hierarchy,
that is, areas consisting of classes that are associated to a similar degree with the verb
(Clark and Weir 1999).
Finally, we note that the proposed estimation method does not guarantee that the
estimates form a probability distribution over the concepts in the hierarchy, and so a
normalization factor is required:
psc(c | v, r) =
p?(v | [c, v, r], r) p?(c|r)p?(v|r)
?
c??C p?(v | [c?, v, r], r)
p?(c?|r)
p?(v|r)
(11)
We use psc to denote an estimate obtained using our method (since the technique
finds sets of semantically similar senses, or ?similarity classes?) and [c, v, r] to denote
the class chosen for concept c in position r of verb v; p? denotes a relative frequency
estimate, and C denotes the set of concepts in the hierarchy.
Before providing the details of the generalization procedure, we give the relative-
frequency estimates of the relevant probabilities and deal with the problem of am-
6 Li and Abe (1998) also develop a theoretical framework that applies only to a tree and turn WordNet
into a tree by copying each subgraph with multiple parents. One way to extend the experiments in
Section 7 would be to investigate whether this transformation has an impact on the results of those
experiments.
192
Computational Linguistics Volume 28, Number 2
biguous data. The relative-frequency estimates are as follows:
p?(c | r) = f (c,r)f (r) =
?
v??V f (c, v
?, r)
?
v??V
?
c??C f (c
?, v?, r)
(12)
p?(v | r) = f (v,r)f (r) =
?
c??C f (c
?, v, r)
?
v??V
?
c??C f (c
?, v?, r)
(13)
p?(v | c?, r) = f (c?,v,r)
f (c?,r)
=
?
c???c? f (c
??, v, r)
?
v??V
?
c???c? f (c
??, v?, r)
(14)
where f (c, v, r) is the number of (n, v, r) triples in the data in which n is being used to
denote c, and V is the set of verbs in the data. The problem is that the estimates are
defined in terms of frequencies of senses, whereas the data are assumed to be in the
form of (n, v, r) triples: a noun, verb, and argument position. All the data used in this
work have been obtained from the British National Corpus (BNC), using the system
of Briscoe and Carroll (1997), which consists of a shallow-parsing component that is
able to identify verbal arguments.
We take a simple approach to the problem of estimating the frequencies of senses,
by distributing the count for each noun in the data evenly among all senses of the
noun:
f? (c, v, r) =
?
n?syn(c)
f (n, v, r)
|cn(n)| (15)
where f? (c, v, r) is an estimate of the number of times that concept c appears in position
r of verb v, and |cn(n)| is the cardinality of cn(n). This is the approach taken by
Li and Abe (1998), Ribas (1995), and McCarthy (2000).7 Resnik (1998) explains how
this apparently crude technique works surprisingly well. Alternative approaches are
described in Clark and Weir (1999) (see also Clark [2001]), Abney and Light (1999),
and Ciaramita and Johnson (2000).
4. Using a Chi-Square Test to Compare Probabilities
In this section we show how to test whether p(v | c?, r) changes significantly when
considering a node higher in the hierarchy. Consider the problem of deciding whether
p(run | ?canine?, subj) is a good approximation of p(run | ?dog?, subj). (?canine? is the
parent of ?dog? in WordNet.) To do this, the probabilities p(run | c?i , subj) are compared
using a chi-square test, where the c?i are the children of ?canine?. In this case, the null
hypothesis of the test is that the probabilities p(run | ci, subj) are the same for each
child ci. By judging the strength of the evidence against the null hypothesis, how
similar the true probabilities are likely to be can be determined. If the test indicates
that the probabilities are sufficiently unlikely to be the same, then the null hypothesis
is rejected, and the conclusion is that p(run | ?canine?, subj) is not a good approximation
of p(run | ?dog?, subj).
An example contingency table, based on counts obtained from a subset of the BNC
using the system of Briscoe and Carroll, is given in Table 1. (Recall that the frequencies
are estimated by distributing the count for a noun equally among the noun?s senses;
this explains the fractional counts.) One column contains estimates of counts arising
7 Resnik takes a similar approach but divides the count evenly among the noun?s senses and all the
hypernyms of those senses.
193
Clark and Weir Class-Based Probability Estimation
Table 1
Contingency table for the children of ?canine? in the subject position of run.
ci f?(ci, run, subj) f?(ci, subj) ? f?(ci, run, subj) f?(ci, subj) =
?
v?V f?(ci, v, subj)
?bitch? 0.3 (0.5) 26.7 (26.6) 27.0
?dog? 12.8 (10.5) 620.4 (622.7) 633.2
?wolf? 0.3 (0.6) 38.7 (38.4) 39.0
?jackal? 0.0 (0.3) 20.0 (19.7) 20.0
?wild dog? 0.0 (0.0) 3.0 (3.0) 3.0
?hyena? 0.0 (0.2) 10.0 (9.8) 10.0
?fox? 0.0 (1.2) 72.3 (71.1) 72.3
13.4 791.1 804.5
from concepts in ci appearing in the subject position of the verb run: f? (ci, run, subj). A
second column presents estimates of counts arising from concepts in ci appearing in
the subject position of a verb other than run. The figures in brackets are the expected
values if the null hypothesis is true.
There is a choice of which statistic to use in conjunction with the chi-square test.
The usual statistic encountered in textbooks is the Pearson chi-square statistic, de-
noted X2:
X2 =
?
i,j
(oij ? eij)2
eij
(16)
where oij is the observed value for the cell in row i and column j, and eij is the
corresponding expected value. An alternative statistic is the log-likelihood chi-square
statistic, denoted G2:8
G2 = 2
?
i,j
oij loge
oij
eij
(17)
The two statistics have similar values when the counts in the contingency table are
large (Agresti 1996). The statistics behave differently, however, when the table contains
low counts, and, since corpus data are likely to lead to some low counts, the question
of which statistic to use is an important one. Dunning (1993) argues for the use of G2
rather than X2, based on an analysis of the sampling distributions of G2 and X2, and
results obtained when using the statistics to acquire highly associated bigrams. We
consider Dunning?s analysis at the end of this section, and the question of whether to
use G2 or X2 will be discussed further there. For now, we continue with the discussion
of how the chi-square test is used in the generalization procedure.
For Table 1, the value of G2 is 3.8, and the value of X2 is 2.5. Assuming a level of
significance of ? = 0.05, the critical value is 12.6 (for six degrees of freedom). Thus,
for this ? value, the null hypothesis would not be rejected for either statistic, and the
conclusion would be that there is no reason to suppose that p(run | ?canine?, subj) is
not a reasonable approximation of p(run | ?dog?, subj).
8 An alternative formula for G2 is given in Dunning (1993), but the two are equivalent.
194
Computational Linguistics Volume 28, Number 2
Table 2
Contingency table for the children of ?liquid? in the object position of drink.
ci f?(ci, drink, obj) f?(ci, obj) ? f?(ci, drink, obj) f?(ci, obj) =
?
v?V f?(ci, v, obj)
?beverage? 261.0 (238.7) 2,367.7 (2,390.0) 2,628.7
?supernatant? 0.0 (0.1) 1.0 (0.9) 1.0
?alcohol? 11.5 (9.4) 92.0 (94.1) 103.5
?ammonia? 0.0 (0.8) 8.5 (7.7) 8.5
?antifreeze? 0.0 (0.1) 1.0 (0.9) 1.0
?distillate? 0.0 (0.5) 6.0 (5.5) 6.0
?water? 12.0 (31.6) 335.7 (316.1) 347.7
?ink? 0.0 (2.9) 32.0 (29.1) 32.0
?liquor? 0.7 (1.1) 11.6 (11.2) 12.3
285.2 2,855.5 3,140.7
As a further example, Table 2 gives counts for the children of ?liquid? in the object
position of drink. Again, the counts have been obtained from a subset of the BNC
using the system of Briscoe and Carroll. Not all the sets dominated by the children of
?liquid? are shown, as some, such as ?sheep dip?, never appear in the object position
of a verb in the data. This example is designed to show a case in which the null
hypothesis is rejected. The value of G2 for this table is 29.0, and the value of X2 is
21.2. So for G2, even if an ? value as low as 0.0005 were being used (for which the
critical value is 27.9 for eight degrees of freedom), the null hypothesis would still be
rejected. For X2, the null hypothesis is rejected for ? values greater than 0.005. This
seems reasonable, since the probabilities associated with the children of ?liquid? and
the object position of drink would be expected to show a lot of variation across the
children.
A key question is how to select the appropriate value for ?. One solution is to
treat ? as a parameter and set it empirically by taking a held-out test set and choosing
the value of ? that maximizes performance on the relevant task. For example, Clark
and Weir (2000) describes a prepositional phrase attachment algorithm that employs
probability estimates obtained using the WordNet method described here. To set the
value of ?, the performance of the algorithm on a development set could be com-
pared across different values of ?, and the value that leads to the best performance
could be chosen. Note that this approach sets no constraints on the value of ?: The
value could be as high as 0.995 or as low as 0.0005, depending on the particular
application.
There may be cases in which the conditions for the appropriate application of a chi-
square test are not met. One condition that is likely to be violated is the requirement
that expected values in the contingency table not be too small. (A rule of thumb
often found in textbooks is that the expected values should be greater than five.) One
response to this problem is to apply some kind of thresholding and either ignore
counts below the threshold, or apply the test only to tables that do not contain low
counts. Ribas (1995), Li and Abe (1998), McCarthy (2000), and Wagner (2000) all use
some kind of thresholding when dealing with counts in the hierarchy (although not in
the context of a chi-square test). Another approach would be to use Fisher?s exact test
(Agresti 1996; Pedersen 1996), which can be applied to tables regardless of the size of
195
Clark and Weir Class-Based Probability Estimation
the counts they contain. The main problem with this test is that it is computationally
expensive, especially for large contingency tables.
What we have found in practice is that applying the chi-square test to tables dom-
inated by low counts tends to produce an insignificant result, and the null hypothesis
is not rejected. The consequences of this for the generalization procedure are that
low-count tables tend to result in the procedure moving up to the next node in the
hierarchy. But given that the purpose of the generalization is to overcome the sparse-
data problem, moving up a node is desirable, and therefore we do not modify the test
for tables with low counts.
The final issue to consider is which chi-square statistic to use. Dunning (1993)
argues for the use of G2 rather than X2, based on the claim that the sampling distri-
bution of G2 approaches the true chi-square distribution quicker than the sampling
distribution of X2. However, Agresti (1996, page 34) makes the opposite claim: ?The
sampling distributions of X2 and G2 get closer to chi-squared as the sample size n
increases. . . . The convergence is quicker for X2 than G2.?
In addition, Pedersen (2001) questions whether one statistic should be preferred
over the other for the bigram acquisition task and cites Cressie and Read (1984), who
argue that there are some cases where the Pearson statistic is more reliable than the
log-likelihood statistic. Finally, the results of the pseudo-disambiguation experiments
presented in Section 7 are at least as good, if not better, when using X2 rather than G2,
and so we conclude that the question of which statistic to use should be answered on
a per application basis.
5. The Generalization Procedure
The procedure for finding a suitable class, c?, to generalize concept c in position r
of verb v works as follows. (We refer to c? as the ?similarity class? of c with respect
to v and r and the hypernym c? as top(c, v, r), since the chosen hypernym sits at the
?top? of the similarity class.) Initially, concept c is assigned to a variable top. Then,
by working up the hierarchy, successive hypernyms of c are assigned to top, and this
process continues until the probabilities associated with the sets of concepts dominated
by top and the siblings of top are significantly different. Once a node is reached that
results in a significant result for the chi-square test, the procedure stops, and top is
returned as top(c, v, r). In cases where a concept has more than one parent, the parent
is chosen that results in the lowest value of the chi-square statistic, as this indicates
the probabilities are the most similar. The set top(c, v, r) is the similarity class of c for
verb v and position r. Figure 1 gives an algorithm for determining top(c, v, r).
Figure 2 gives an example of the procedure at work. Here, top(?soup?, stir, obj) is
being determined. The example is based on data from a subset of the BNC, with 303
cases of an argument in the object position of stir. The G2 statistic is used, together with
an ? value of 0.05. Initially, top is set to ?soup?, and the probabilities corresponding
to the children of ?dish? are compared: p(stir | ?soup?, obj), p(stir | ?lasagne?, obj), p(stir |
?haggis?, obj), and so on for the rest of the children. The chi-square test results in a G2
value of 14.5, compared to a critical value of 55.8. Since G2 is less than the critical value,
the procedure moves up to the next node. This process continues until a significant
result is obtained, which first occurs at ?substance? when comparing the children of
?object?. Thus ?substance? is the chosen level of generalization.
Now we show how the chosen level of generalization varies with ? and how it
varies with the size of the data set. A note of clarification is required before presenting
the results. In related work on acquiring selectional preferences (Ribas 1995; McCarthy
196
Computational Linguistics Volume 28, Number 2
Algorithm top(c, v, r):
top ? c
sig result ? false
comment parentmin gives lowest G2 value, G2min
while not sig result & top = ?root? do
G2min ? ?
for all parents of top do
calculate G2 for sets dominated by children of parent
if G2 < G2min
then G2min ? G2
parentmin ? parent
end
if chi-square test for parentmin is significant
then sig result ? true
else move up to next node: top ? parentmin
end
return top
Figure 1
An algorithm for determining top(c, v, r).
haggislasagne
dish
nourishment
food
fare beverage
coursemeal
substance
object
uid poison
artifactground
entity
soup
G
2
: 14:5, critical value: 55:8
G
2
: 5:4, crit val: 16:9
G
2
: 5:5, crit val: 16:9
G
2
: 29:9, crit val: 58:1
G
2
: 141:1, crit val: 37:7
Figure 2
An example generalization: Determining top(?soup?, stir, obj).
197
Clark and Weir Class-Based Probability Estimation
1997; Li and Abe 1998; Wagner 2000), the level of generalization is often determined for
a small number of hand-picked verbs and the result compared with the researcher?s
intuition about the most appropriate level for representing a selectional preference.
According to this approach, if ?sandwich? were chosen to represent ?hotdog? in the
object position of eat, this might be considered an undergeneralization, since ?food?
might be considered more appropriate. For this work we argue that such an evaluation
is not appropriate; since the purpose of this work is probability estimation, the most
appropriate level is the one that leads to the most accurate estimate, and this may or
may not agree with intuition. Furthermore, we show in Section 7 that to generalize
unnecessarily can be harmful for some tasks: If we already have lots of data regarding
?sandwich?, why generalize any higher? Thus the purpose of this section is not to show
that the acquired levels are ?correct,? but simply to show how the levels vary with ?
and the sample size.
To show how the level of generalization varies with changes in ?, top(c, v, obj)
was determined for a number of hand-picked (c, v, obj) triples over a range of values
for ?. The triples were chosen to give a range of strongly and weakly selecting verbs
and a range of verb frequencies. The data were again extracted from a subset of the
BNC using the system of Briscoe and Carroll (1997), and the G2 statistic was used in
the chi-square test. The results are shown in Table 3. The number of times the verb
occurred with some object is also given in the table.
The results suggest that the generalization level becomes more specific as ? in-
creases. This is to be expected, since, given a contingency table chosen at random, a
higher value of ? is more likely to lead to a significant result than a lower value of ?.
We also see that, for some cases, the value of ? has little effect on the level. We would
expect there to be less change in the level of generalization for strongly selecting verbs,
such as drink and eat, and a greater range of levels for weakly selecting verbs such
as see. This is because any significant difference in probabilities is likely to be more
marked for a strongly selecting verb, and likely to be significant over a wider range
of ? values. The table only provides anecdotal evidence, but provides some support
to this argument.
To investigate more generally how the level of generalization varies with changes
in ?, and also with changes in sample size, we took 6, 000 (c, v, obj) triples and calcu-
lated the difference in depth between c and top(c, v, r) for each triple. The 6, 000 triples
were taken from the first experimental test set described in Section 7, and the train-
ing data from this experiment were used to provide the counts. (The test set contains
nouns, rather than noun senses, and so the sense of the noun that is most probable
given the verb and object slot was used.) An average difference in depth was then
calculated. To give an example of how the difference in depth was calculated, sup-
pose ?dog? generalized to ?placental mammal? via ?canine? and ?carnivore?; in this case
the difference would be three.
The results for various levels of ? and different sample sizes are shown in Table 4.
The figures in each column arise from using the contingency tables based on the
complete training data, but with each count in the table multiplied by the percentage
at the head of the column. Thus the 50% column is based on contingency tables in
which each original count is multiplied by 50%, which is equivalent to using a sample
one-half the size of the original training set. Reading across a row shows how the
generalization varies with sample size, and reading down a column shows how it
varies with ?. The results show clearly that the extent of generalization decreases
with an increase in the value of ?, supporting the trend observed in Table 3. The
results also show that the extent of generalization increases with a decrease in sample
198
Computational Linguistics Volume 28, Number 2
Table 3
Example levels of generalization for different values of ?.
(c, v, r), f(v, r) ?
(?coffee?, drink, obj) 0.0005 ?coffee??BEVERAGE??food? . . . ?object??entity?
0.05 ?coffee??BEVERAGE??food? . . . ?object??entity?
f (drink, obj) = 849 0.5 ?coffee??BEVERAGE??food? . . . ?object??entity?
0.995 ?coffee??BEVERAGE??food? . . . ?object??entity?
(?hotdog?, eat, obj) 0.0005 ?hotdog??sandwich??snack food??DISH? . . . ?food? . . . ?entity?
0.05 ?hotdog??sandwich??snack food??DISH? . . . ?food? . . . ?entity?
f (eat, obj) = 1,703 0.5 ?hotdog??sandwich??snack food??DISH? . . . ?food? . . . ?entity?
0.995 ?hotdog??SANDWICH??snack food??dish? . . . ?food? . . . ?entity?
(?Socrates?, kiss, obj) 0.0005 ?Socrates? . . . ?person??life form??CAUSAL AGENT??entity?
0.05 ?Socrates? . . . ?person??life form??CAUSAL AGENT??entity?
f (kiss, obj) = 345 0.5 ?Socrates? . . . ?person??life form??CAUSAL AGENT??entity?
0.995 ?Socrates? . . . ?PERSON??life form??causal agent??entity?
(?dream?, remember, obj) 0.0005 ?dream? . . . ?preoccupation??cognitive state??STATE?
0.05 ?dream? . . . ?preoccupation??cognitive state??STATE?
f (remember, obj) = 1,982 0.5 ?dream? . . . ?preoccupation??COGNITIVE STATE??state?
0.995 ?dream? . . . ?PREOCCUPATION??cognitive state??state?
(?man?, see, obj) 0.0005 ?man? . . . ?mammal? . . . ?ANIMAL??life form??entity?
0.05 ?man? . . . ?MAMMAL? . . . ?animal??life form??entity?
f (see, obj) = 16,757 0.5 ?man? . . . ?MAMMAL? . . . ?animal??life form??entity?
0.995 ?MAN? . . . ?mammal? . . . ?animal??life form??entity?
(?belief?, abandon, obj) 0.0005 ?belief??mental object??cognition??PSYCHOLOGICAL FEATURE?
0.05 ?belief??MENTAL OBJECT??cognition??psychological feature?
f (abandon, obj) = 673 0.5 ?BELIEF??mental object??cognition??psychological feature?
0.995 ?BELIEF??mental object??cognition??psychological feature?
(?nightmare?, have, obj) 0.0005 ?nightmare??dreaming??IMAGINATION? . . . ?psychological feature?
0.05 ?nightmare??dreaming??IMAGINATION? . . . ?psychological feature?
f (have, obj) = 93,683 0.5 ?nightmare??DREAMING??imagination? . . . ?psychological feature?
0.995 ?nightmare??DREAMING??imagination? . . . ?psychological feature?
Note: The selected level is shown in upper case.
Table 4
Extent of generalization for different values of ? and sample sizes.
? 100% 50% 10% 1%
0.0005 3.3 3.9 5.0 5.6
0.05 2.8 3.5 4.6 5.6
0.5 2.1 2.9 4.1 5.4
0.995 1.2 1.5 2.6 3.9
size. Again, this is to be expected, since any difference in probability estimates is less
likely to be significant for tables with low counts.
6. Alternative Class-Based Estimation Methods
The approaches used for comparison are that of Resnik (1993, 1998), subsequently
developed by Ribas (1995), and that of Li and Abe (1998), which has been adopted by
McCarthy (2000). These have been chosen because they directly address the question
of how to find a suitable level of generalization in WordNet.
199
Clark and Weir Class-Based Probability Estimation
The first alternative uses the ?association score,? which is a measure of how well
a set of concepts, C, satisfies the selectional preferences of a verb, v, for an argument
position, r:9
A(C, v, r) = p(C | v, r) log2
p(C | v, r)
p(C | r) (18)
An estimate of the association score, A?(C, v, r), can be obtained using relative frequency
estimates of the probabilities. The key question is how to determine a suitable level of
generalization for concept c, or, alternatively, how to find a suitable class to represent
concept c (assuming the choice is from those classes that contain all concepts dom-
inated by some hypernym of c). Resnik?s solution to this problem (which he neatly
refers to as the ?vertical-ambiguity? problem) is to choose the class that maximizes
the association score.
It is not clear that the class with the highest association score is always the most
appropriate level of generalization. For example, this approach does not always gen-
eralize appropriately for arguments that are negatively associated with some verb. To
see why, consider the problem of deciding how well the concept ?location? satisfies the
preferences of the verb eat for its object. Since locations are not the kinds of things that
are typically eaten, a suitable level of generalization would correspond to a class that
has a low association score with respect to eat. However, ?location? is a kind of ?entity?
in WordNet,10 and choosing the class with the highest association score is likely to
produce ?entity? as the chosen class. This is a problem, because the association score
of ?entity? with respect to eat may be too high to reflect the fact that ?location? is a very
unlikely object of the verb.
Note that the solution to the vertical-ambiguity problem presented in the previous
sections is able to generalize appropriately in such cases. Continuing with the eat
?location? example, our generalization procedure is unlikely to get as high as ?entity?
(assuming a reasonable number of examples of eat in the training data), since the
probabilities corresponding to the daughters of ?entity? are likely to be very different
with respect to the object position of eat.
The second alternative uses the minimum description length (MDL) principle.
Li and Abe use MDL to select a set of classes from a hierarchy, together with their
associated probabilities, to represent the selectional preferences of a particular verb.
The preferences and class-based probabilities are then used to estimate probabilities
of the form p(n | v, r), where n is a noun, v is a verb, and r is an argument slot.
Li and Abe?s application of MDL requires the hierarchy to be in the form of a
thesaurus, in which each leaf node represents a noun and internal nodes represent the
class of nouns that the node dominates. The hierarchy is also assumed to be in the
form of a tree. The class-based models consist of a partition of the set of nouns (leaf
nodes) and a probability associated with each class in the partition. The probabilities
are the conditional probabilities of each class, given the relevant verb and argument
position. Li and Abe refer to such a partition as a ?cut? and the cut together with the
probabilities as a ?tree cut model.? The probabilities of the classes in a cut, ?, satisfy
the following constraint:
?
C??
p(C | v, r) = 1 (19)
9 The definition used here is that given by Ribas (1995).
10 For example, the hypernyms of the concept ?Dallas? are as follows: ?city?, ?municipality?,
?urban area?, ?geographical area?, ?region?, ?location?, ?object?, ?entity?.
200
Computational Linguistics Volume 28, Number 2
<abstraction>
<life_form>
<plant>
<object>
<entity>
<substance>
<set>
<root>
<mushroom>
<artifact>
<rope>
<food>
<pizza><lobster>
<fluid><solid>
<animal>
<lobster>
<time><space>
Figure 3
Possible cut returned by MDL.
In order to determine the probability of a noun, the probability of a class is assumed
to be distributed uniformly among the members of that class:
p(n | v, r) = 1|C| p(C | v, r) for all n ? C (20)
Since WordNet is a hierarchy with noun senses, rather than nouns, at the nodes,
Li and Abe deal with the issue of word sense ambiguity using the method described
in Section 3, by dividing the count for a noun equally among the concepts whose
synsets contain the noun. Also, since WordNet is a DAG, Li and Abe turn WordNet
into a tree by copying each subgraph with multiple parents. And so that each noun
in the data appears (in a synset) at a leaf node, Li and Abe remove those parts of the
hierarchy dominated by a noun in the data (but only for that instance of WordNet
corresponding to the relevant verb).
An example cut showing part of the WordNet hierarchy is shown in Figure 3 (based
on an example from Li and Abe [1998]; the dashed lines indicate parts of the hierarchy
that are not shown in the diagram). This is a possible cut for the object position of the
verb eat, and the cut consists of the following classes: ?life form?, ?solid?, ?fluid?, ?food?,
?artifact?, ?space?, ?time?, ?set?. (The particular choice of classes for the cut in this example
is not too important; the example is designed to show how probabilities of senses are
estimated from class probabilities.) Since the class in the cut containing ?pizza? is ?food?,
the probability p(?pizza? | eat, obj) would be estimated as p(?food? | eat, obj)/|?food?|.
Similarly, since the class in the cut containing ?mushroom? is ?life form?, the probability
p(?mushroom? | eat, obj) would be estimated as p(?life form? | eat, obj)/|?life form?|.
The uniform-distribution assumption (20) means that cuts close to the root of the
hierarchy result in a greater smoothing of the probability estimates than cuts near the
leaves. Thus there is a trade-off between choosing a model that has a cut near the
leaves, which is likely to overfit the data, and a more general (simple) model near the
root, which is likely to underfit the data. MDL looks ideally suited to the task of model
selection, since it is designed to deal with precisely this trade-off. The simplicity of a
model is measured using the model description length, which is an information-theoretic
201
Clark and Weir Class-Based Probability Estimation
term and denotes the number of bits required to encode the model. The fit to the data
is measured using the data description length, which is the number of bits required to
encode the data (relative to the model). The overall description length is the sum of
the model description length and the data description length, and the MDL principle
is to select the model with the shortest description length.
We used McCarthy?s (2000) implementation of MDL. So that every noun is repre-
sented at a leaf node, McCarthy does not remove parts of the hierarchy, as Li and Abe
do, but instead creates new leaf nodes for each synset at an internal node. McCarthy
also does not transform WordNet into a tree, which is strictly required for Li and
Abe?s application of MDL. This did create a problem with overgeneralization: Many
of the cuts returned by MDL were overgeneralizing at the ?entity? node. The reason
is that ?person?, which is close to ?entity? and dominated by ?entity?, has two parents:
?life form? and ?causal agent?. This DAG-like property was responsible for the over-
generalization, and so we removed the link between ?person? and ?causal agent?. This
appeared to solve the problem, and the results presented later for the average degree
of generalization do not show an overgeneralization compared with those given in Li
and Abe (1998).
7. Pseudo-Disambiguation Experiments
The task we used to compare the class-based estimation techniques is a decision task
previously used by Pereira, Tishby, and Lee (1993) and Rooth et al (1999). The task is
to decide which of two verbs, v and v?, is more likely to take a given noun, n, as an
object. The test and training data were obtained as follows. A number of verb?direct
object pairs were extracted from a subset of the BNC, using the system of Briscoe and
Carroll. All those pairs containing a noun not in WordNet were removed, and each
verb and argument was lemmatized. This resulted in a data set of around 1.3 million
(v, n) pairs.
To form a test set, 3,000 of these pairs were randomly selected such that each
selected pair contained a fairly frequent verb. (Following Pereira, Tishby, and Lee, only
those verbs that occurred between 500 and 5,000 times in the data were considered.)
Each instance of a selected pair was then deleted from the data to ensure that the test
data were unseen. The remaining pairs formed the training data. To complete the test
set, a further fairly frequent verb, v?, was randomly chosen for each (v, n) pair. The
random choice was made according to the verb?s frequency in the original data set,
subject to the condition that the pair (v?, n) did not occur in the training data. Given
the set of (v, n, v?) triples, the task is to decide whether (v, n) or (v?, n) is the correct
pair.11
We acknowledge that the task is somewhat artificial, but pseudo-disambiguation
tasks of this kind are becoming popular in statistical NLP because of the ease with
which training and test data can be created. We also feel that the pseudo-disambig-
uation task is useful for evaluating the different estimation methods, since it directly
addresses the question of how likely a particular predicate is to take a given noun as
an argument. An evaluation using a PP attachment task was attempted in Clark and
Weir (2000), but the evaluation was limited by the relatively small size of the Penn
Treebank.
11 We note that this procedure does not guarantee that the correct pair is more likely than the incorrect
pair, because of noise in the data from the parser and also because a highly plausible incorrect pair
could be generated by chance.
202
Computational Linguistics Volume 28, Number 2
Table 5
Results for the pseudo-disambiguation task.
Generalization technique % correct av.gen. sd.gen.
Similarity class
? = 0.0005 73.8 3.3 2.0
? = 0.05 73.4 2.8 1.9
? = 0.3 73.0 2.4 1.8
? = 0.75 73.9 1.9 1.6
? = 0.995 73.8 1.2 1.2
Low class 73.6 0.9 1.0
MDL 68.3 4.1 1.9
Assoc 63.9 4.2 2.1
Note: av.gen. is the average number of generalized levels;
sd.gen. is the standard deviation.
Using our approach, the disambiguation decision for each (v, n, v?) triple was made
according to the following procedure:
if max
c?cn(n)
psc(c | v, obj) > max
c?cn(n)
psc(c | v?, obj)
then choose (v, n)
else if max
c?cn(n)
psc(c | v?, obj) > max
c?cn(n)
psc(c | v, obj)
then choose (v?, n)
else choose at random
If n has more than one sense, the sense is chosen that maximizes the relevant prob-
ability estimate; this explains the maximization over cn(n). The probability estimates
were obtained using our class-based method, and the G2 statistic was used for the
chi-square test. This procedure was also used for the MDL alternative, but using the
MDL method to estimate the probabilities.
Using the association score for each test triple, the decision was made according
to the following procedure:
if max
c?cn(n)
max
c??h(c)
A?(c?, v, obj) > max
c?cn(n)
max
c??h(c)
A?(c?, v?, obj)
then choose (v, n)
else if max
c?cn(n)
max
c??h(c)
A?(c?, v?, obj) > max
c?cn(n)
max
c??h(c)
A?(c?, v, obj)
then choose (v?, n)
else choose at random
We use h(c) to denote the set consisting of the hypernyms of c. The inner maximization
is over h(c), assuming c is the chosen sense of n, which corresponds to Resnik?s method
of choosing a set to represent c. The outer maximization is over the senses of n, cn(n),
which determines the sense of n by choosing the sense that maximizes the association
score.
The first set of results is given in Table 5. Our technique is referred to as the
?similarity class? technique, and the approach using the association score is referred
203
Clark and Weir Class-Based Probability Estimation
Table 6
Results for the pseudo-disambiguation task with one-fifth training data.
Generalization technique % correct av.gen. sd.gen.
Similarity class
? = 0.0005 66.7 4.5 1.9
? = 0.05 68.4 4.1 1.9
? = 0.3 70.2 3.7 1.9
? = 0.75 72.3 3.0 1.9
? = 0.995 72.4 1.9 1.6
Low class 71.9 1.1 1.1
MDL 62.9 4.7 1.9
Assoc 62.6 4.1 2.0
Note: av.gen. is the average number of generalized levels;
sd.gen. is the standard deviation.
to as ?Assoc.? The results are given for a range of ? values and demonstrate clearly that
the performance of similarity class varies little with changes in ? and that similarity
class outperforms both MDL and Assoc.12
We also give a score for our approach using a simple generalization procedure,
which we call ?low class.? The procedure is to select the first class that has a count
greater than zero (relative to the verb and argument position), which is likely to return
a low level of generalization, on the whole. The results show that our generalization
technique only narrowly outperforms the simple alternative. Note that, although low
class is based on a very simple generalization method, the estimation method is still
using our class-based technique, by applying Bayes? theorem and conditioning on a
class, as described in Section 3; the difference is in how the class is chosen.
To investigate the results, we calculated the average number of generalized levels
for each approach. The number of generalized levels for a concept c (relative to a
verb v and argument position r) is the difference in depth between c and top(c, v, r),
as explained in Section 5. For each test case, the number of generalized levels for
both verbs, v and v?, was calculated, but only for the chosen sense of n. The results
are given in the third column of Table 5 and demonstrate clearly that both MDL and
Assoc are generalizing to a greater extent than similarity class. (The fourth column
gives a standard deviation figure.) These results suggest that MDL and Assoc are
overgeneralizing, at least for the purposes of this task.
To investigate why the value for ? had no impact on the results, we repeated the
experiment, but with one fifth of the data. A new data set was created by taking every
fifth pair of the original 1.3 million pairs. A test set of 3,000 triples was created from
this new data set, as before, but this time only verbs that occurred between 100 and
1,000 times were considered. The results using these test and training data are given
in Table 6.
These results show a variation in performance across values for ?, with an opti-
mal performance when ? is around 0.75. (Of course, in practice, the value for ? would
need to be optimized on a held-out set.) But even with this variation, similarity class is
still outperforming MDL and Assoc across the whole range of ? values. Note that the
12 The results given for similarity class are different from those given in Clark and Weir (2001) because
the probability estimates used in Clark and Weir (2001) were not normalized.
204
Computational Linguistics Volume 28, Number 2
Table 7
Disambiguation results for G2 and X2.
? value % correct (G2) % correct (X2)
0.0005 73.8 (3.3) 74.1 (3.0)
0.05 73.4 (2.8) 73.8 (2.5)
0.3 73.0 (2.4) 74.1 (2.2)
0.75 73.9 (1.9) 74.3 (1.8)
0.995 73.8 (1.2) 73.3 (1.2)
? values corresponding to the lowest scores lead to a significant amount of general-
ization, which provides additional evidence that MDL and Assoc are overgeneralizing
for this task. The low-class method scores highly for this data set alo, but given that
the task is one that apparently favors a low level of generalization, the high score is
not too surprising.
As a final experiment, we compared the task performance using the X2, rather than
G2, statistic in the chi-square test. The results are given in Table 7 for the complete
data set.13 The figures in brackets give the average number of generalized levels.
The X2 statistic is performing at least as well as G2, and the results show that the
average level of generalization is slightly higher for G2 than X2. This suggests a possible
explanation for the results presented here and those in Dunning (1993): that the X2
statistic provides a less conservative test when counts in the contingency table are
low. (By a conservative test we mean one in which the null hypothesis is not easily
rejected.) A less conservative test is better suited to the pseudo-disambiguation task,
since it results in a lower level of generalization, on the whole, which is good for this
task. In contrast, the task that Dunning considers, the discovery of bigrams, is better
served by a more conservative test.
8. Conclusion
We have presented a class-based estimation method that incorporates a procedure for
finding a suitable level of generalization in WordNet. This method has been shown to
provide superior performance on a pseudo-disambiguation task, compared with two
alternative approaches. An analysis of the results has shown that the other approaches
appear to be overgeneralizing, at least for this task. One of the features of the gener-
alization procedure is the way that ?, the level of significance in the chi-square test,
is treated as a parameter. This allows some control over the extent of generalization,
which can be tailored to particular tasks. We have also shown that the task perfor-
mance is at least as good when using the Pearson chi-square statistic as when using
the log-likelihood chi-square statistic.
There are a number of ways in which this work could be extended. One possibility
would be to use all the classes dominated by the hypernyms of a concept, rather than
just one, to estimate the probability of the concept. An estimate would be obtained for
each hypernym, and the estimates combined in a linear interpolation. An approach
similar to this is taken by Bikel (2000), in the context of statistical parsing.
There is still room for investigation of the hidden-data problem when data are used
that have not been sense disambiguated. In this article, a very simple approach is taken,
13 ?2 performed slightly better than G2 using the smaller data set alo.
205
Clark and Weir Class-Based Probability Estimation
which is to split the count for a noun evenly among the noun?s senses. Abney and Light
(1999) have tried a more motivated approach, using the expectation maximization
algorithm, but with little success. The approach described in Clark and Weir (1999) is
shown in Clark (2001) to have some impact on the pseudo-disambiguation task, but
only with certain values of the ? parameter, and ultimately does not improve on the
best performance.
Finally, an issue that has not been much addressed in the literature (except by
Li and Abe [1996]) is how the accuracy of class-based estimation techniques compare
when automatically acquired classes, as opposed to the manually created classes from
WordNet, are used. The pseudo-disambiguation task described here has also been used
to evaluate clustering algorithms (Pereira, Tishby, and Lee, 1993; Rooth et al, 1999),
but with different data, and so it is difficult to compare the results. A related issue
is how the structure of WordNet affects the accuracy of the probability estimates. We
have taken the structure of the hierarchy for granted, without any analysis, but it may
be that an alternative design could be more conducive to probability estimation.
Acknowledgments
This article is an extended and updated
version of a paper that appeared in the
proceedings of NAACL 2001. The work on
which it is based was carried out while the
first author was a D.Phil. student at the
University of Sussex and was supported by
an EPSRC studentship. We would like to
thank Diana McCarthy for suggesting the
pseudo-disambiguation task and providing
the MDL software, John Carroll for
supplying the data, and Ted Briscoe, Geoff
Sampson, Gerald Gazdar, Bill Keller, Ted
Pedersen, and the anonymous reviewers for
their helpful comments. We would also like
to thank Ted Briscoe for presenting an
earlier version of this article on our behalf
at NAACL 2001.
References
Abney, Steven P. and Marc Light. 1999.
Hiding a semantic hierarchy in a Markov
model. In Proceedings of the ACL Workshop
on Unsupervised Learning in Natural
Language Processing, University of
Maryland, College Park, pages 1?8.
Agirre, Eneko and David Martinez. 2001.
Learning class-to-class selectional
preferences. In Proceedings of the Fifth ACL
Workshop on Computational Language
Learning, Toulouse, France, pages 15?22.
Agresti, Alan. 1996. An Introduction to
Categorical Data Analysis. Wiley.
Bikel, Daniel M. 2000. A statistical model
for parsing and word-sense
disambiguation. In Proceedings of the Joint
SIGDAT Conference on Empirical Methods in
Natural Language Processing and Very Large
Corpora, pages 155?163, Hong Kong.
Briscoe, Ted and John Carroll. 1997.
Automatic extraction of subcategorization
from corpora. In Proceedings of the Fifth
ACL Conference on Applied Natural Language
Processing, pages 356?363, Washington,
DC.
Church, Kenneth W. and Patrick Hanks.
1990. Word association norms, mutual
information, and lexicography.
Computational Linguistics, 16(1):22?29.
Ciaramita, Massimiliano and Mark Johnson.
2000. Explaining away ambiguity:
Learning verb selectional preference with
Bayesian networks. In Proceedings of the
18th International Conference on
Computational Linguistics, pages 187?193,
Saarbrucken, Germany.
Clark, Stephen. 2001. Class-Based Statistical
Models for Lexical Knowledge Acquisition.
Ph.D. dissertation, University of Sussex.
Clark, Stephen and David Weir. 1999. An
iterative approach to estimating
frequencies over a semantic hierarchy. In
Proceedings of the Joint SIGDAT Conference
on Empirical Methods in Natural Language
Processing and Very Large Corpora, pages
258?265, University of Maryland, College
Park.
Clark, Stephen and David Weir. 2000. A
class-based probabilistic approach to
structural disambiguation. In Proceedings
of the 18th International Conference on
Computational Linguistics, pages 194?200,
Saarbrucken, Germany.
Clark, Stephen and David Weir. 2001.
Class-based probability estimation using a
semantic hierarchy. In Proceedings of the
Second Meeting of the North American
Chapter of the Association for Computational
Linguistics, pages 95?102, Pittsburgh.
Cressie, Noel A. C. and Timothy R. C. Read.
1984. Multinomial goodness of fit tests.
206
Computational Linguistics Volume 28, Number 2
Journal of the Royal Statistics Society Series B,
46:440?464.
Dunning, Ted. 1993. Accurate methods for
the statistics of surprise and coincidence.
Computational Linguistics, 19(1):61?74.
Fellbaum, Christiane, editor. 1998. WordNet:
An Electronic Lexical Database. MIT Press.
Li, Hang and Naoki Abe. 1996. Clustering
words with the MDL principle. In
Proceedings of the 16th International
Conference on Computational Linguistics,
pages 4?9, Copenhagen, Denmark.
Li, Hang and Naoki Abe. 1998. Generalizing
case frames using a thesaurus and the
MDL principle. Computational Linguistics,
24(2):217?244.
McCarthy, Diana. 1997. Word sense
disambiguation for acquisition of
selectional preferences. In Proceedings of
the ACL/EACL Workshop on Automatic
Information Extraction and Building of Lexical
Semantic Resources for NLP Applications,
pages 52?61, Madrid.
McCarthy, Diana. 2000. Using semantic
preferences to identify verbal
participation in role switching. In
Proceedings of the First Conference of the
North American Chapter of the Association for
Computational Linguistics, pages 256?263,
Seattle.
Miller, George A. 1998. Nouns in WordNet.
In Christiane Fellbaum, editor, WordNet:
An Electronic Lexical Database. MIT Press,
pages 23?46.
Pedersen, Ted. 1996. Fishing for exactness.
In Proceedings of the South-Central SAS Users
Group Conference, Austin, pages 188?200.
Pedersen, Ted. 2001. A decision tree of
bigrams is an accurate predictor of word
sense. In Proceedings of the Second Meeting
of the North American Chapter of the
Association for Computational Linguistics,
pages 79?86, Pittsburgh.
Pereira, Fernando, Naftali Tishby, and
Lillian Lee. 1993. Distributional clustering
of English words. In Proceedings of the 31st
Annual Meeting of the Association for
Computational Linguistics, pages 183?190,
Columbus, OH.
Resnik, Philip. 1993. Selection and
Information: A Class-Based Approach to
Lexical Relationships. Ph.D. dissertation,
University of Pennsylvania.
Resnik, Philip. 1998. WordNet and
class-based probabilities. In Christiane
Fellbaum, editor, WordNet: An Electronic
Lexical Database. MIT Press, pages 239?263.
Ribas, Francesc. 1995. On learning more
appropriate selectional restrictions. In
Proceedings of the Seventh Conference of the
European Chapter of the Association for
Computational Linguistics, pages 112?118,
Dublin.
Rooth, Mats, Stefan Riezler, Detlef Prescher,
Glenn Carroll, and Franz Beil. 1999.
Inducing a semantically annotated lexicon
via EM-based clustering. In Proceedings of
the 37th Annual Meeting of the Association for
Computational Linguistics, pages 104?111,
University of Maryland, College Park.
Wagner, Andreas. 2000. Enriching a lexical
semantic net with selectional preferences
by means of statistical corpus analysis. In
Proceedings of the ECAI-2000 Workshop on
Ontology Learning, Berlin, pages 37?42.
 	
Example Selection for Bootstrapping Statistical Parsers
Mark Steedman?, Rebecca Hwa?, Stephen Clark?, Miles Osborne?, Anoop Sarkar?
Julia Hockenmaier?, Paul Ruhlen? Steven Baker?, Jeremiah Crim?
?School of Informatics, University of Edinburgh
{steedman,stephenc,julia,osborne}@cogsci.ed.ac.uk
?Institute for Advanced Computer Studies, University of Maryland
hwa@umiacs.umd.edu
?School of Computing Science, Simon Fraser University
anoop@cs.sfu.ca
?Center for Language and Speech Processing, Johns Hopkins University
jcrim@jhu.edu,ruhlen@cs.jhu.edu
?Department of Computer Science, Cornell University
sdb22@cornell.edu
Abstract
This paper investigates bootstrapping for statis-
tical parsers to reduce their reliance on manu-
ally annotated training data. We consider both
a mostly-unsupervised approach, co-training,
in which two parsers are iteratively re-trained
on each other?s output; and a semi-supervised
approach, corrected co-training, in which a
human corrects each parser?s output before
adding it to the training data. The selection of
labeled training examples is an integral part of
both frameworks. We propose several selection
methods based on the criteria of minimizing er-
rors in the data and maximizing training util-
ity. We show that incorporating the utility cri-
terion into the selection method results in better
parsers for both frameworks.
1 Introduction
Current state-of-the-art statistical parsers (Collins, 1999;
Charniak, 2000) are trained on large annotated corpora
such as the Penn Treebank (Marcus et al, 1993). How-
ever, the production of such corpora is expensive and
labor-intensive. Given this bottleneck, there is consider-
able interest in (partially) automating the annotation pro-
cess.
To overcome this bottleneck, two approaches from ma-
chine learning have been applied to training parsers. One
is sample selection (Thompson et al, 1999; Hwa, 2000;
Tang et al, 2002), a variant of active learning (Cohn et al,
1994), which tries to identify a small set of unlabeled sen-
tences with high training utility for the human to label1.
Sentences with high training utility are those most likely
to improve the parser. The other approach, and the fo-
cus of this paper, is co-training (Sarkar, 2001), a mostly-
unsupervised algorithm that replaces the human by hav-
ing two (or more) parsers label training examples for each
other. The goal is for both parsers to improve by boot-
strapping off each other?s strengths. Because the parsers
may label examples incorrectly, only a subset of their out-
put, chosen by some selection mechanism, is used in or-
der to minimize errors. The choice of selection method
significantly affects the quality of the resulting parsers.
We investigate a novel approach of selecting training
examples for co-training parsers by incorporating the idea
of maximizing training utility from sample selection. The
selection mechanism is integral to both sample selection
and co-training; however, because co-training and sam-
ple selection have different goals, their selection methods
focus on different criteria: co-training typically favors se-
lecting accurately labeled examples, while sample selec-
tion typically favors selecting examples with high train-
ing utility, which often are not sentences that the parsers
already label accurately. In this work, we investigate se-
lection methods for co-training that explore the trade-off
between maximizing training utility and minimizing er-
rors.
Empirical studies were conducted to compare selection
methods under both co-training and a semi-supervised
framework called corrected co-training (Pierce and
Cardie, 2001), in which the selected examples are man-
ually checked and corrected before being added to the
1In the context of training parsers, a labeled example is a
sentence with its parse tree. Throughout this paper, we use the
term ?label? and ?parse? interchangeably.
                                                               Edmonton, May-June 2003
                                                             Main Papers , pp. 157-164
                                                         Proceedings of HLT-NAACL 2003
training data. For co-training, we show that the benefit of
selecting examples with high training utility can offset the
additional errors they contain. For corrected co-training,
we show that selecting examples with high training util-
ity reduces the number of sentences the human annotator
has to check. For both frameworks, we show that selec-
tion methods that maximize training utility find labeled
examples that result in better trained parsers than those
that only minimize error.
2 Co-training
Blum and Mitchell (1998) introduced co-training to
bootstrap two classifiers with different views of the data.
The two classifiers are initially trained on a small amount
of annotated seed data; then they label unannotated data
for each other in an iterative training process. Blum and
Mitchell prove that, when the two views are conditionally
independent given the label, and each view is sufficient
for learning the task, co-training can boost an initial
weak learner using unlabeled data.
The theory underlying co-training has been extended
by Dasgupta et al (2002) to prove that, by maximizing
their agreement over the unlabeled data, the two learn-
ers make few generalization errors (under the same in-
dependence assumption adopted by Blum and Mitchell).
Abney (2002) argues that this assumption is extremely
strong and typically violated in the data, and he proposes
a weaker independence assumption.
Goldman and Zhou (2000) show that, through care-
ful selection of newly labeled examples, co-training can
work even when the classifiers? views do not satisfy
the independence assumption. In this paper we investi-
gate methods for selecting labeled examples produced by
two statistical parsers. We do not explicitly maximize
agreement (along the lines of Abney?s algorithm (2002))
because it is too computationally intensive for training
parsers.
The pseudocode for our co-training framework is given
in Figure 1. It consists of two different parsers and a cen-
tral control that interfaces between the two parsers and
the data. At each co-training iteration, a small set of sen-
tences is drawn from a large pool of unlabeled sentences
and stored in a cache. Both parsers then attempt to label
every sentence in the cache. Next, a subset of the newly
labeled sentences is selected to be added to the train-
ing data. The examples added to the training set of one
parser (referred to as the student) are only those produced
by the other parser (referred to as the teacher), although
the methods we use generalize to the case in which the
parsers share a single training set. During selection, one
parser first acts as the teacher and the other as the student,
and then the roles are reversed.
A and B are two different parsers.
M iA and M iB are the models of A and B at step i.
U is a large pool of unlabeled sentences.
U i is a small cache holding a subset of U at step i.
L is the manually labeled seed data.
LiA and LiB are the labeled training examples for A and B
at step i.
Initialize:
L0A ? L
0
B ? L.
M0A ? Train(A,L
0
A)
M0B ? Train(B,L
0
B)
Loop:
U i ? Add unlabeled sentences from U .
M iA and M iB parse the sentences in U i and
assign scores to them according to their scoring
functions fA and fB .
Select new parses {PA} and {PB} according to some
selection method S, which uses the scores
from fA and fB .
Li+1A is L
i
A augmented with {PB}
Li+1B is L
i
B augmented with {PA}
M i+1A ? Train(A,L
i+1
A )
M i+1B ? Train(B,L
i+1
B )
Figure 1: The pseudo-code for the co-training algorithm
3 Selecting Training Examples
In each iteration, selection is performed in two steps.
First, each parser uses some scoring function, f , to assess
the parses it generated for the sentences in the cache.2
Second, the central control uses some selection method,
S, to choose a subset of these labeled sentences (based on
the scores assigned by f ) to add to the parsers? training
data. The focus of this paper is on the selection phase, but
to more fully investigate the effect of different selection
methods we also consider two possible scoring functions.
3.1 Scoring functions
The scoring function attempts to quantify the correctness
of the parses produced by each parser. An ideal scor-
ing function would give the true accuracy rates (e.g., F-
score, the combined labeled precision and recall rates).
In practice, accuracy is approximated by some notion
of confidence. For example, one easy-to-compute scor-
ing function measures the conditional probability of the
(most likely) parse. If a high probability is assigned, the
parser is said to be confident in the label it produced.
In our experimental studies, we considered the selec-
tion methods? interaction with two scoring functions: an
oracle scoring function fF-score that returns the F-score
of the parse as measured against a gold standard, and a
2In our experiments, both parsers use the same scoring func-
tion.
practical scoring function fprob that returns the condi-
tional probability of the parse.3
3.2 Selection methods
Based on the scores assigned by the scoring function,
the selection method chooses a subset of the parser la-
beled sentences that best satisfy some selection criteria.
One such criterion is the accuracy of the labeled exam-
ples, which may be estimated by the teacher parser?s con-
fidence in its labels. However, the examples that the
teacher correctly labeled may not be those that the stu-
dent needs. We hypothesize that the training utility of
the examples for the student parser is another important
criterion.
Training utility measures the improvement a parser
would make if that sentence were correctly labeled and
added to the training set. Like accuracy, the utility of
an unlabeled sentence is difficult to quantify; therefore,
we approximate it with values that can be computed from
features of the sentence. For example, sentences contain-
ing many unknown words may have high training util-
ity; so might sentences that a parser has trouble parsing.
Under the co-training framework, we estimate the train-
ing utility of a sentence for the student by comparing the
score the student assigned to its parse (according to its
scoring function) against the score the teacher assigned
to its own parse.
To investigate how the selection criteria of utility and
accuracy affect the co-training process, we considered a
number of selection methods that satisfy the requirements
of accuracy and training utility to varying degrees. The
different selection methods are shown below. For each
method, a sentence (as labeled by the teacher parser) is
selected if:
? above-n (Sabove-n): the score of the teacher?s parse
(using its scoring function) ? n.
? difference (Sdiff-n): the score of the teacher?s parse
is greater than the score of the student?s parse by
some threshold n.
? intersection (Sint-n): the score of the teacher?s parse
is in the set of the teacher?s n percent highest-
scoring labeled sentences, and the score of the stu-
dent?s parse for the same sentence is in the set of
the student?s n percent lowest-scoring labeled sen-
tences.
Each selection method has a control parameter, n, that
determines the number of labeled sentences to add at each
co-training iteration. It also serves as an indirect control
3A nice property of using conditional probability,
Pr(parse|sentence), as the scoring function is that it
normalizes for sentence length.
of the number of errors added to the training set. For ex-
ample, the Sabove-n method would allow more sentences
to be selected if n was set to a low value (with respect to
the scoring function); however, this is likely to reduce the
accuracy rate of the training set.
The above-n method attempts to maximize the accu-
racy of the data (assuming that parses with higher scores
are more accurate). The difference method attempts to
maximize training utility: as long as the teacher?s label-
ing is more accurate than that of the student, it is cho-
sen, even if its absolute accuracy rate is low. The inter-
section method attempts to maximize both: the selected
sentences are accurately labeled by the teacher and incor-
rectly labeled by the student.
4 Experiments
Experiments were performed to compare the effect of
the selection methods on co-training and corrected co-
training. We consider a selection method, S1, superior
to another, S2, if, when a large unlabeled pool of sen-
tences has been exhausted, the examples selected by S1
(as labeled by the machine, and possibly corrected by the
human) improve the parser more than those selected by
S2. All experiments shared the same general setup, as
described below.
4.1 Experimental Setup
For two parsers to co-train, they should generate com-
parable output but use independent statistical models.
In our experiments, we used a lexicalized context free
grammar parser developed by Collins (1999), and a lex-
icalized Tree Adjoining Grammar parser developed by
Sarkar (2002). Both parsers were initialized with some
seed data. Since the goal is to minimize human annotated
data, the size of the seed data should be small. In this pa-
per we used a seed set size of 1, 000 sentences, taken from
section 2 of the Wall Street Journal (WSJ) Penn Tree-
bank. The total pool of unlabeled sentences was the re-
mainder of sections 2-21 (stripped of their annotations),
consisting of about 38,000 sentences. The cache size is
set at 500 sentences. We have explored using different
settings for the seed set size (Steedman et al, 2003).
The parsers were evaluated on unseen test sentences
(section 23 of the WSJ corpus). Section 0 was used as
a development set for determining parameters. The eval-
uation metric is the Parseval F-score over labeled con-
stituents: F-score = 2?LR?LPLR+LP , where LP and LR
are labeled precision and recall rate, respectively. Both
parsers were evaluated, but for brevity, all results reported
here are for the Collins parser, which received higher Par-
seval scores.
80
80.5
81
81.5
82
82.5
83
83.5
84
0 2000 4000 6000 8000 10000 12000
Pa
rsin
g A
ccu
rac
y o
n T
est
 Da
ta (F
sco
re)
Number of Training Sentences
above-70%diff-10%int-60%No selection(Human annotated)
80
80.5
81
81.5
82
82.5
83
83.5
84
0 2000 4000 6000 8000 10000 12000
Pa
rsin
g A
ccu
rac
y o
n T
est
 Da
ta (F
sco
re)
Number of Training Sentences
above-90%diff-10%int-30%No selection(Human annotated)
(a) (b)
Figure 2: A comparison of selection methods using the oracle scoring function, fF-score, controlling for the label
quality of the training data. (a) The average accuracy rates are about 85%. (b) The average accuracy rates (except for
those selected by Sdiff-10%) are about 95%.
4.2 Experiment 1: Selection Methods and
Co-Training
We first examine the effect of the three selection meth-
ods on co-training without correction (i.e., the chosen
machine-labeled training examples may contain errors).
Because the selection decisions are based on the scores
that the parsers assign to their outputs, the reliability of
the scoring function has a significant impact on the per-
formance of the selection methods. We evaluate the ef-
fectiveness of the selection methods using two scoring
functions. In Section 4.2.1, each parser assesses its out-
put with an oracle scoring function that returns the Par-
seval F-score of the output (as compared to the human
annotated gold-standard). This is an idealized condition
that gives us direct control over the error rate of the la-
beled training data. By keeping the error rates constant,
our goal is to determine which selection method is more
successful in finding sentences with high training utility.
In Section 4.2.2 we replace the oracle scoring function
with fprob, which returns the conditional probability of
the best parse as the score. We compare how the selection
methods? performances degrade under the realistic con-
dition of basing selection decisions on unreliable parser
output assessment scores.
4.2.1 Using the oracle scoring function, fF-score
The goal of this experiment is to evaluate the selection
methods using a reliable scoring function. We therefore
use an oracle scoring function, fF-score, which guaran-
tees a perfect assessment of the parser?s output. This,
however, may be too powerful. In practice, we expect
even a reliable scoring function to sometimes assign high
scores to inaccurate parses. We account for this effect by
adjusting the selection method?s control parameter to af-
fect two factors: the accuracy rate of the newly labeled
training data, and the number of labeled sentences added
at each training iteration. A relaxed parameter setting
adds more parses to the training data, but also reduces
the accuracy of the training data.
Figure 2 compares the effect of the three selection
methods on co-training for the relaxed (left graph) and
the strict (right graph) parameter settings. Each curve in
the two graphs charts the improvement in the parser?s ac-
curacy in parsing the test sentences (y-axis) as it is trained
on more data chosen by its selection method (x-axis).
The curves have different endpoints because the selection
methods chose a different number of sentences from the
same 38K unlabeled pool. For reference, we also plotted
the improvement of a fully-supervised parser (i.e., trained
on human-annotated data, with no selection).
For the more relaxed setting, the parameters are chosen
so that the newly labeled training data have an average
accuracy rate of about 85%:
? Sabove-70% requires the labels to have an F-score ?
70%. It adds about 330 labeled sentences (out of the
500 sentence cache) with an average accuracy rate
of 85% to the training data per iteration.
? Sdiff-10% requires the score difference between the
teacher?s labeling and the student?s labeling to be at
least 10%. It adds about 50 labeled sentences with
an average accuracy rate of 80%.
? Sint-60% requires the teacher?s parse to be in the
top 60% of its output and the student?s parse for the
same sentence to be in its bottom 60%. It adds about
150 labeled sentences with an average accuracy rate
of 85%.
Although none rivals the parser trained on human an-
notated data, the selection method that improves the
parser the most is Sdiff-10%. One interpretation is that
the training utility of the examples chosen by Sdiff-10%
outweighs the cost of errors introduced into the training
data. Another interpretation is that the other two selection
methods let in too many sentences containing errors. In
the right graph, we compare the same Sdiff-10% with the
other two selection methods using stricter control, such
that the average accuracy rate for these methods is now
about 95%:
? Sabove-90% now requires the parses to be at least
90% correct. It adds about 150 labeled sentences
per iteration.
? Sint-30% now requires the teacher?s parse to be in
the top 30% of its output and the student?s parse for
the same sentence in its bottom 30%. It adds about
15 labeled sentences.
The stricter control on Sabove-90% improved the
parser?s performance, but not enough to overtake
Sdiff-10% after all the sentences in the unlabeled pool
had been considered, even though the training data of
Sdiff-10% contained many more errors. Sint-30% has a
faster initial improvement4, closely tracking the progress
of the fully-supervised parser. However, the stringent re-
quirement exhausted the unlabeled data pool before train-
ing the parser to convergence. Sint-30% might continue
to help the parser to improve if it had access to more un-
labeled data, which is easier to acquire than annotated
data5.
Comparing the three selection methods under both
strict and relaxed control settings, the results suggest that
training utility is an important criterion in selecting train-
ing examples, even at the cost of reduced accuracy.
4.2.2 Using the fprob scoring function
To determine the effect of unreliable scores on the se-
lection methods, we replace the oracle scoring function,
fF-score, with fprob, which approximates the accuracy
of a parse with its conditional probability. Although this
is a poor estimate of accuracy (especially when computed
from a partially trained parser), it is very easy to compute.
The unreliable scores also reduce the correlation between
the selection control parameters and the level of errors in
the training data. In this experiment, we set the parame-
ters for all three selection methods so that approximately
4A fast improvement rate is not a central concern here, but
it will be more relevant for corrected co-training.
5This oracle experiment is bounded by the size of the anno-
tated portion of the WSJ corpus.
79.8
80
80.2
80.4
80.6
80.8
81
81.2
1000 1500 2000 2500 3000 3500 4000 4500 5000
Par
sin
g A
ccu
rac
y o
n T
est
 Da
ta (F
scor
e)
Number of Training Sentences
above-70%diff-30%int-30%
Figure 3: A comparison of selection methods using the
conditional probability scoring function, fprob.
30-50 sentences were added to the training data per iter-
ation. The average accuracy rate of the training data for
Sabove-70% was about 85%, and the rate for Sdiff-30%
and Sint-30% was about 75%.
As expected, the parser performances of all three selec-
tion methods using fprob (shown in Figure 3) are lower
than using fF-score (see Figure 2). However, Sdiff-30%
and Sint-30% helped the co-training parsers to improve
with a 5% error reduction (1% absolute difference) over
the parser trained only on the initial seed data. In con-
trast, despite an initial improvement, using Sabove-70%
did not help to improve the parser. In their experiments on
NP identifiers, Pierce and Cardie (2001) observed a sim-
ilar effect. They hypothesize that co-training does not
scale well for natural language learning tasks that require
a huge amount of training data because too many errors
are accrued over time. Our experimental results suggest
that the use of training utility in the selection process can
make co-training parsers more tolerant to these accumu-
lated errors.
4.3 Experiment 2: Selection Methods and
Corrected Co-training
To address the problem of the training data accumulating
too many errors over time, Pierce and Cardie proposed
a semi-supervised variant of co-training called corrected
co-training, which allows a human annotator to review
and correct the output of the parsers before adding it to
the training data. The main selection criterion in their
co-training system is accuracy (approximated by confi-
dence). They argue that selecting examples with nearly
correct labels would require few manual interventions
from the annotator.
We hypothesize that it may be beneficial to consider
the training utility criterion in this framework as well.
We perform experiments to determine whether select-
ing fewer (and possibly less accurately labeled) exam-
80
81
82
83
84
85
86
87
2000 4000 6000 8000 10000 12000
Pa
rsin
g A
ccu
rac
y o
n T
est
 Da
ta (F
sco
re)
Number of Training Sentences
above-90%diff-10%int-30%No selection
80
81
82
83
84
85
86
87
0 5000 10000 15000 20000 25000 30000 35000 40000 45000
Pa
rsin
g A
ccu
rac
y o
n T
est
 Da
ta (F
sco
re)
Number of Constituents to Correct in the Training Data
above-90%diff-10%int-30%No selection
(a) (b)
Figure 4: A comparison of selection methods for corrected co-training using fF-score (a) in terms of the number of
sentences added to the training data; (b) in terms of the number of manually corrected constituents.
ples with higher training utility would require less effort
from the annotator. In our experiments, we simulated
the interactive sample selection process by revealing the
gold standard. As before, we compare the three selection
methods using both fF-score and fprob as scoring func-
tions.6
4.3.1 Using the oracle scoring function, fF-score
Figure 4 shows the effect of the three selection meth-
ods (using the strict parameter setting) on corrected co-
training. As a point of reference, we plot the improve-
ment rate for a fully supervised parser (same as the one
in Figure 2). In addition to charting the parser?s perfor-
mance in terms of the number of labeled training sen-
tences (left graph), we also chart the parser?s performance
in terms of the the number of constituents the machine
mislabeled (right graph). The pair of graphs indicates the
amount of human effort required: the left graph shows
the number of sentences the human has to check, and the
right graph shows the number of constituents the human
has to correct.
Comparing Sabove-90% and Sdiff-10%, we see that
Sdiff-10% trains a better parser than Sabove-90% when all
the unlabeled sentences have been considered. It also im-
proves the parser using a smaller set of training exam-
ples. Thus, for the same parsing performance, it requires
the human to check fewer sentences than Sabove-90% and
the reference case of no selection (Figure 4(a)). On the
other hand, because the labeled sentences selected by
Sdiff-10% contain more mistakes than those selected by
Sabove-90%, Sdiff-10% requires slightly more corrections
6The selection control parameters are the same as the previ-
ous set of experiments, using the strict setting (i.e., Figure 2(b))
for fF-score.
than Sabove-90% for the same level of parsing perfor-
mance; though both require fewer corrections than the
reference case of no selection (Figure 4(b)). Because
the amount of effort spent by the annotator depends on
the number of sentences checked as well as the amount
of corrections made, whether Sdiff-10% or Sabove-90% is
more effort reducing may be a matter of the annotator?s
preference.
The selection method that improves the parser at the
fastest rate is Sint-30%. For the same parser performance
level, it selects the fewest number of sentences for a hu-
man to check and requires the human to make the least
number of corrections. However, as we have seen in the
earlier experiment, very few sentences in the unlabeled
pool satisfy its stringent criteria, so it ran out of data be-
fore the parser was trained to convergence. At this point
we cannot determine whether Sint-30% might continue to
improve the parser if we used a larger set of unlabeled
data.
4.3.2 Using the fprob scoring function
We also consider the effect of unreliable scores in the
corrected co-training framework. A comparison between
the selection methods using fprob is reported in Figure
5. The left graph charts parser performance in terms of
the number of sentences the human must check; the right
charts parser performance in terms of the number of con-
stituents the human must correct. As expected, the unreli-
able scoring function degrades the effectiveness of the se-
lection methods; however, compared to its unsupervised
counterpart (Figure 3), the degradation is not as severe.
In fact, Sdiff-30% and Sint-30% still require fewer train-
ing data than the reference parser. Moreover, consistent
with the other experiments, the selection methods that at-
tempt to maximize training utility achieve better parsing
performance than Sabove-70%. Finally, in terms of reduc-
ing human effort, the three selection methods require the
human to correct comparable amount of parser errors for
the same level of parsing performance, but for Sdiff-30%
and Sint-30%, fewer sentences need to be checked.
4.3.3 Discussion
Corrected co-training can be seen as a form of active
learning, whose goal is to identify the smallest set of un-
labeled data with high training utility for the human to
label. Active learning can be applied to a single learner
(Lewis and Catlett, 1994) and to multiple learners (Fre-
und et al, 1997; Engelson and Dagan, 1996; Ngai and
Yarowsky, 2000). In the context of parsing, all previ-
ous work (Thompson et al, 1999; Hwa, 2000; Tang et
al., 2002) has focussed on single learners. Corrected co-
training is the first application of active learning for mul-
tiple parsers. We are currently investigating comparisons
to the single learner approaches.
Our approach is similar to co-testing (Muslea et al,
2002), an active learning technique that uses two classi-
fiers to find contentious examples (i.e., data for which the
classifiers? labels disagree) for a human to label. There is
a subtle but significant difference, however, in that their
goal is to reduce the total number of labeled training ex-
amples whereas we also wish to reduce the number of
corrections made by the human. Therefore, our selection
methods must take into account the quality of the parse
produced by the teacher in addition to how different its
parse is from the one produced by the student. The inter-
section method precisely aims at selecting sentences that
satisfy both requirements. Exploring different selection
methods is part of our on-going research effort.
5 Conclusion
We have considered three selection methods that have dif-
ferent priorities in balancing the two (often competing)
criteria of accuracy and training utility. We have em-
pirically compared their effect on co-training, in which
two parsers label data for each other, as well as corrected
co-training, in which a human corrects the parser labeled
data before adding it to the training set. Our results sug-
gest that training utility is an important selection criterion
to consider, even at the cost of potentially reducing the ac-
curacy of the training data. In our empirical studies, the
selection method that aims to maximize training utility,
Sdiff-n, consistently finds better examples than the one
that aims to maximize accuracy, Sabove-n. Our results
also suggest that the selection method that aims to maxi-
mize both accuracy and utility, Sint-n, shows promise in
improving co-training parsers and in reducing human ef-
fort for corrected co-training; however, a much larger un-
labeled data set is needed to verify the benefit of Sint-n.
The results of this study indicate the need for scor-
ing functions that are better estimates of the accuracy of
the parser?s output than conditional probabilities. Our
oracle experiments show that, by using effective selec-
tion methods, the co-training process can improve parser
peformance even when the newly labeled parses are
not completely accurate. This suggests that co-training
may still be beneficial when using a practical scoring
function that might only coarsely distinguish accurate
parses from inaccurate parses. Further avenues to ex-
plore include the development of selection methods to
efficiently approximate maximizing the objective func-
tion of parser agreement on unlabeled data, following the
work of Dasgupta et al (2002) and Abney (2002). Also,
co-training might be made more effective if partial parses
were used as training data. Finally, we are conducting ex-
periments to compare corrected co-training with other ac-
tive learning methods. We hope these studies will reveal
ways to combine the strengths of co-training and active
learning to make better use of unlabeled data.
Acknowledgments
This work has been supported, in part, by NSF/DARPA
funded 2002 Human Language Engineering Workshop
at JHU, EPSRC grant GR/M96889, the Department of
Defense contract RD-02-5700, and ONR MURI Con-
tract FCPO.810548265. We would like to thank Chris
Callison-Burch, Michael Collins, John Henderson, Lil-
lian Lee, Andrew McCallum, and Fernando Pereira for
helpful discussions; to Ric Crabbe, Adam Lopez, the par-
ticipants of CS775 at Cornell University, and the review-
ers for their comments on this paper.
References
Steven Abney. 2002. Bootstrapping. In Proceedings of the
40th Annual Meeting of the Association for Computational
Linguistics, pages 360?367, Philadelphia, PA.
Avrim Blum and Tom Mitchell. 1998. Combining labeled
and unlabeled data with co-training. In Proceedings of the
11th Annual Conference on Computational Learning Theory,
pages 92?100, Madison, WI.
Eugene Charniak. 2000. A maximum-entropy-inspired parser.
In Proceedings of the 1st Annual Meeting of the NAACL.
David Cohn, Les Atlas, and Richard Ladner. 1994. Improv-
ing generalization with active learning. Machine Learning,
15(2):201?221.
Michael Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University of Penn-
sylvania.
Sanjoy Dasgupta, Michael Littman, and David McAllester.
2002. PAC generalization bounds for co-training. In T. G.
Dietterich, S. Becker, and Z. Ghahramani, editors, Advances
80
81
82
83
84
85
86
87
2000 4000 6000 8000 10000 12000
Pa
rsin
g A
ccu
rac
y o
n T
est
 Da
ta (F
sco
re)
Number of Training Sentences
above-70%diff-30%int-30%No selection
80
81
82
83
84
85
86
87
0 5000 10000 15000 20000 25000 30000 35000 40000 45000
Pa
rsin
g A
ccu
rac
y o
n T
est
 Da
ta (F
sco
re)
Number of Constituents to Correct in the Training Data
above-70%diff-30%int-30%No selection
(a) (b)
Figure 5: A comparison of selection methods for corrected co-training using fprob (a) in terms of the number of
sentences added to the training data; (b) in terms of the number of manually corrected constituents.
in Neural Information Processing Systems 14, Cambridge,
MA. MIT Press.
Sean P. Engelson and Ido Dagan. 1996. Minimizing manual
annotation cost in supervised training from copora. In Pro-
ceedings of the 34th Annual Meeting of the ACL, pages 319?
326.
Yoav Freund, H. Sebastian Seung, Eli Shamir, and Naftali
Tishby. 1997. Selective sampling using the query by com-
mittee algorithm. Machine Learning, 28(2-3):133?168.
Sally Goldman and Yan Zhou. 2000. Enhancing supervised
learning with unlabeled data. In Proceedings of the 17th In-
ternational Conference on Machine Learning, Stanford, CA.
Rebecca Hwa. 2000. Sample selection for statistical grammar
induction. In Proceedings of the 2000 Joint SIGDAT Confer-
ence on EMNLP and VLC, pages 45?52, Hong Kong, China,
October.
David D. Lewis and Jason Catlett. 1994. Heterogeneous un-
certainty sampling for supervised learning. In Proceedings
of the Eleventh International Conference on Machine Learn-
ing, pages 148?156.
Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguistics, 19(2):313?330.
Ion Muslea, Steve Minton, and Craig Knoblock. 2002. Selec-
tive sampling with redundant views. In Proceedings of the
Seventeenth National Conference on Artificial Intelligence,
pages 621?626.
Grace Ngai and David Yarowsky. 2000. Rule writing or an-
notation: Cost-efficient resource usage for base noun phrase
chunking. In Proceedings of the 38th Annual Meeting of the
ACL, pages 117?125, Hong Kong, China, October.
David Pierce and Claire Cardie. 2001. Limitations of co-
training for natural language learning from large datasets. In
Proceedings of the Empirical Methods in NLP Conference,
Pittsburgh, PA.
Anoop Sarkar. 2001. Applying co-training methods to statisti-
cal parsing. In Proceedings of the 2nd Annual Meeting of the
NAACL, pages 95?102, Pittsburgh, PA.
Anoop Sarkar. 2002. Statistical Parsing Algorithms for Lexi-
calized Tree Adjoining Grammars. Ph.D. thesis, University
of Pennsylvania.
Mark Steedman, Miles Osborne, Anoop Sarkar, Stephen Clark,
Rebecca Hwa, Julia Hockenmaier, Paul Ruhlen, Steven
Baker, and Jeremiah Crim. 2003. Bootstrapping statistical
parsers from small datasets. In The Proceedings of the An-
nual Meeting of the European Chapter of the ACL. To ap-
pear.
Min Tang, Xiaoqiang Luo, and Salim Roukos. 2002. Active
learning for statistical natural language parsing. In Proceed-
ings of the 40th Annual Meeting of the ACL, pages 120?127,
July.
Cynthia A. Thompson, Mary Elaine Califf, and Raymond J.
Mooney. 1999. Active learning for natural language pars-
ing and information extraction. In Proceedings of ICML-99,
pages 406?414, Bled, Slovenia.
Building Deep Dependency Structures with a Wide-Coverage CCG Parser
Stephen Clark, Julia Hockenmaier and Mark Steedman
Division of Informatics
University of Edinburgh
Edinburgh EH8 9LW, UK
 
stephenc, julia, steedman  @cogsci.ed.ac.uk
Abstract
This paper describes a wide-coverage sta-
tistical parser that uses Combinatory Cat-
egorial Grammar (CCG) to derive de-
pendency structures. The parser differs
from most existing wide-coverage tree-
bank parsers in capturing the long-range
dependencies inherent in constructions
such as coordination, extraction, raising
and control, as well as the standard local
predicate-argument dependencies. A set
of dependency structures used for train-
ing and testing the parser is obtained from
a treebank of CCG normal-form deriva-
tions, which have been derived (semi-) au-
tomatically from the Penn Treebank. The
parser correctly recovers over 80% of la-
belled dependencies, and around 90% of
unlabelled dependencies.
1 Introduction
Most recent wide-coverage statistical parsers have
used models based on lexical dependencies (e.g.
Collins (1999), Charniak (2000)). However, the de-
pendencies are typically derived from a context-free
phrase structure tree using simple head percolation
heuristics. This approach does not work well for the
long-range dependencies involved in raising, con-
trol, extraction and coordination, all of which are
common in text such as the Wall Street Journal.
Chiang (2000) uses Tree Adjoining Grammar
as an alternative to context-free grammar, and
here we use another ?mildly context-sensitive? for-
malism, Combinatory Categorial Grammar (CCG,
Steedman (2000)), which arguably provides the
most linguistically satisfactory account of the de-
pendencies inherent in coordinate constructions and
extraction phenomena. The potential advantage
from using such an expressive grammar is to facili-
tate recovery of such unbounded dependencies. As
well as having a potential impact on the accuracy of
the parser, recovering such dependencies may make
the output more useful.
CCG is unlike other formalisms in that the stan-
dard predicate-argument relations relevant to inter-
pretation can be derived via extremely non-standard
surface derivations. This impacts on how best to de-
fine a probability model for CCG, since the ?spuri-
ous ambiguity? of CCG derivations may lead to an
exponential number of derivations for a given con-
stituent. In addition, some of the spurious deriva-
tions may not be present in the training data. One
solution is to consider only the normal-form (Eis-
ner, 1996a) derivation, which is the route taken in
Hockenmaier and Steedman (2002b).1
Another problem with the non-standard surface
derivations is that the standard PARSEVAL per-
formance measures over such derivations are unin-
formative (Clark and Hockenmaier, 2002). Such
measures have been criticised by Lin (1995) and
Carroll et al (1998), who propose recovery of head-
dependencies characterising predicate-argument re-
lations as a more meaningful measure.
If the end-result of parsing is interpretable
predicate-argument structure or the related depen-
dency structure, then the question arises: why build
derivation structure at all? A CCG parser can
directly build derived structures, including long-
1Another, more speculative, possibility is to treat the alter-
native derivations as hidden and apply the EM algorithm.
                Computational Linguistics (ACL), Philadelphia, July 2002, pp. 327-334.
                         Proceedings of the 40th Annual Meeting of the Association for
range dependencies. These derived structures can
be of any form we like?for example, they could
in principle be standard Penn Treebank structures.
Since we are interested in dependency-based parser
evaluation, our parser currently builds dependency
structures. Furthermore, since we want to model
the dependencies in such structures, the probability
model is defined over these structures rather than the
derivation.
The training and testing material for this CCG
parser is a treebank of dependency structures, which
have been derived from a set of CCG deriva-
tions developed for use with another (normal-form)
CCG parser (Hockenmaier and Steedman, 2002b).
The treebank of derivations, which we call CCG-
bank (Hockenmaier and Steedman, 2002a), was in
turn derived (semi-)automatically from the hand-
annotated Penn Treebank.
2 The Grammar
In CCG, most language-specific aspects of the gram-
mar are specified in the lexicon, in the form of syn-
tactic categories that identify a lexical item as either
a functor or argument. For the functors, the category
specifies the type and directionality of the arguments
and the type of the result. For example, the follow-
ing category for the transitive verb bought specifies
its first argument as a noun phrase (NP) to its right
and its second argument as an NP to its left, and its
result as a sentence:
(1) bought :=  S  NP  NP
For parsing purposes, we extend CCG categories
to express category features, and head-word and de-
pendency information directly, as follows:
(2) bought :=  S  dcl  bought  NP1  NP2
The feature  dcl specifies the category?s S result as a
declarative sentence, bought identifies its head, and
the numbers denote dependency relations. Heads
and dependencies are always marked up on atomic
categories (S, N, NP, PP, and conj in our implemen-
tation).
The categories are combined using a small set of
typed combinatory rules, such as functional applica-
tion and composition (see Steedman (2000) for de-
tails). Derivations are written as follows, with under-
lines indicating combinatory reduction and arrows
indicating the direction of the application:
(3) Marks bought Brooks
NPMarks 	 S 
 dcl  bought  NP1  NP2 NPBrooks
S 
 dcl  bought  NP1 
S 
 dcl  bought
Formally, a dependency is defined as a 4-tuple:

h f  f  s  ha  , where h f is the head word of the func-
tor,2 f is the functor category (extended with head
and dependency information), s is the argument slot,
and ha is the head word of the argument?for exam-
ple, the following is the object dependency yielded
by the first step of derivation (3):
(4)  bought

 S  dcl  bought  NP1  NP2  2  Brooks 
Variables can also be used to denote heads, and
used via unification to pass head information from
one category to another. For example, the expanded
category for the control verb persuade is as follows:
(5) persuade :=  S  dcl  persuade  NP1  S  to  2  NPX  NPX,3
The head of the infinitival complement?s subject is
identified with the head of the object, using the vari-
able X. Unification then ?passes? the head of the ob-
ject to the subject of the infinitival, as in standard
unification-based accounts of control.3
The kinds of lexical items that use the head pass-
ing mechanism are raising, auxiliary and control
verbs, modifiers, and relative pronouns. Among the
constructions that project unbounded dependencies
are relativisation and right node raising. The follow-
ing category for the relative pronoun category (for
words such as who, which, that) shows how heads
are co-indexed for object-extraction:
(6) who :=  NPX  NPX,1  S  dcl  2  NPX 
The derivation for the phrase The company that
Marks wants to buy is given in Figure 1 (with the
features on S categories removed to save space, and
the constant heads reduced to the first letter). Type-
raising (  ) and functional composition (  ), along
2Note that the functor does not always correspond to the lin-
guistic notion of a head.
3The extension of CCG categories in the lexicon and the la-
belled data is simplified in the current system to make it entirely
automatic. For example, any word with the same category (5)
as persuade gets the object-control extension. In certain rare
cases (such as promise) this gives semantically incorrect depen-
dencies in both the grammar and the data (promise Brooks to go
has a structure meaning promise Brooks that Brooks will go).
The company that Marks wants to buy
NPx

Nx,1 Nc
	
NPx  NPx,1

	
S2

NPx

NPm
	
Sw  NPx,1

	
S2  NPx

	
Sy  NPx,1

	
Sy,2  NPx

	
Sb  NP1

NP2

 ffParsing the WSJ using CCG and Log-Linear Models
Stephen Clark
School of Informatics
University of Edinburgh
2 Buccleuch Place, Edinburgh, UK
stephen.clark@ed.ac.uk
James R. Curran
School of Information Technologies
University of Sydney
NSW 2006, Australia
james@it.usyd.edu.au
Abstract
This paper describes and evaluates log-linear
parsing models for Combinatory Categorial
Grammar (CCG). A parallel implementation of
the L-BFGS optimisation algorithm is described,
which runs on a Beowulf cluster allowing the
complete Penn Treebank to be used for estima-
tion. We also develop a new efficient parsing
algorithm for CCG which maximises expected
recall of dependencies. We compare models
which use all CCG derivations, including non-
standard derivations, with normal-form models.
The performances of the two models are com-
parable and the results are competitive with ex-
isting wide-coverage CCG parsers.
1 Introduction
A number of statistical parsing models have recently
been developed for Combinatory Categorial Gram-
mar (CCG; Steedman, 2000) and used in parsers ap-
plied to the WSJ Penn Treebank (Clark et al, 2002;
Hockenmaier and Steedman, 2002; Hockenmaier,
2003b). In Clark and Curran (2003) we argued
for the use of log-linear parsing models for CCG.
However, estimating a log-linear model for a wide-
coverage CCG grammar is very computationally ex-
pensive. Following Miyao and Tsujii (2002), we
showed how the estimation can be performed effi-
ciently by applying the inside-outside algorithm to
a packed chart. We also showed how the complete
WSJ Penn Treebank can be used for training by de-
veloping a parallel version of Generalised Iterative
Scaling (GIS) to perform the estimation.
This paper significantly extends our earlier work
in a number of ways. First, we evaluate a number
of log-linear models, obtaining results which are
competitive with the state-of-the-art for CCG pars-
ing. We also compare log-linear models which use
all CCG derivations, including non-standard deriva-
tions, with normal-form models. Second, we find
that GIS is unsuitable for estimating a model of the
size being considered, and develop a parallel ver-
sion of the L-BFGS algorithm (Nocedal and Wright,
1999). And finally, we show that the parsing algo-
rithm described in Clark and Curran (2003) is ex-
tremely slow in some cases, and suggest an efficient
alternative based on Goodman (1996).
The development of parsing and estimation algo-
rithms for models which use all derivations extends
existing CCG parsing techniques, and allows us to
test whether there is useful information in the addi-
tional derivations. However, we find that the perfor-
mance of the normal-form model is at least as good
as the all-derivations model, in our experiments to-
date. The normal-form approach allows the use of
additional constraints on rule applications, leading
to a smaller model, reducing the computational re-
sources required for estimation, and resulting in an
extremely efficient parser.
This paper assumes a basic understanding of
CCG; see Steedman (2000) for an introduction, and
Clark et al (2002) and Hockenmaier (2003a) for an
introduction to statistical parsing with CCG.
2 Parsing Models for CCG
CCG is unusual among grammar formalisms in that,
for each derived structure for a sentence, there can
be many derivations leading to that structure. The
presence of such ambiguity, sometimes referred to
as spurious ambiguity, enables CCG to produce el-
egant analyses of coordination and extraction phe-
nomena (Steedman, 2000). However, the introduc-
tion of extra derivations increases the complexity of
the modelling and parsing problem.
Clark et al (2002) handle the additional deriva-
tions by modelling the derived structure, in their
case dependency structures. They use a conditional
model, based on Collins (1996), which, as the au-
thors acknowledge, has a number of theoretical de-
ficiencies; thus the results of Clark et al provide a
useful baseline for the new models presented here.
Hockenmaier (2003a) uses a model which
favours only one of the derivations leading to a
derived structure, namely the normal-form deriva-
tion (Eisner, 1996). In this paper we compare the
normal-form approach with a dependency model.
For the dependency model, we define the probabil-
ity of a dependency structure as follows:
P(pi|S ) =
?
d??(pi)
P(d, pi|S ) (1)
where pi is a dependency structure, S is a sentence
and ?(pi) is the set of derivations which lead to pi.
This extends the approach of Clark et al (2002)
who modelled the dependency structures directly,
not using any information from the derivations. In
contrast to the dependency model, the normal-form
model simply defines a distribution over normal-
form derivations.
The dependency structures considered in this pa-
per are described in detail in Clark et al (2002)
and Clark and Curran (2003). Each argument slot
in a CCG lexical category represents a dependency
relation, and a dependency is defined as a 5-tuple
?h f , f , s, ha, l?, where h f is the head word of the lex-
ical category, f is the lexical category, s is the argu-
ment slot, ha is the head word of the argument, and
l indicates whether the dependency is long-range.
For example, the long-range dependency encoding
company as the extracted object of bought (as in the
company that IBM bought) is represented as the fol-
lowing 5-tuple:
?bought, (S[dcl]\NP1)/NP2, 2, company, ??
where ? is the category (NP\NP)/(S[dcl]/NP) as-
signed to the relative pronoun. For local dependen-
cies l is assigned a null value. A dependency struc-
ture is a multiset of these dependencies.
3 Log-Linear Parsing Models
Log-linear models (also known as Maximum En-
tropy models) are popular in NLP because of the
ease with which discriminating features can be in-
cluded in the model. Log-linear models have been
applied to the parsing problem across a range of
grammar formalisms, e.g. Riezler et al (2002) and
Toutanova et al (2002). One motivation for using
a log-linear model is that long-range dependencies
which CCG was designed to handle can easily be en-
coded as features.
A conditional log-linear model of a parse ? ? ?,
given a sentence S , is defined as follows:
P(?|S ) = 1
ZS
e?. f (?) (2)
where ?. f (?) = ?i ?i fi(?). The function fi is a
feature of the parse which can be any real-valued
function over the space of parses ?. Each feature
fi has an associated weight ?i which is a parameter
of the model to be estimated. ZS is a normalising
constant which ensures that P(?|S ) is a probability
distribution:
ZS =
?
????(S )
e?. f (??) (3)
where ?(S ) is the set of possible parses for S .
For the dependency model a parse, ?, is a ?d, pi?
pair (as given in (1)). A feature is a count of the
number of times some configuration occurs in d or
the number of times some dependency occurs in pi.
Section 6 gives examples of features.
3.1 The Dependency Model
We follow Riezler et al (2002) in using a discrimi-
native estimation method by maximising the condi-
tional likelihood of the model given the data. For the
dependency model, the data consists of sentences
S 1, . . . , S m, together with gold standard dependency
structures, pi1, . . . , pim. The gold standard structures
are multisets of dependencies, as described earlier.
Section 6 explains how the gold standard structures
are obtained.
The objective function of a model ? is the condi-
tional log-likelihood, L(?), minus a Gaussian prior
term, G(?), used to reduce overfitting (Chen and
Rosenfeld, 1999). Hence, given the definition of the
probability of a dependency structure (1), the objec-
tive function is as follows:
L?(?) = L(?) ?G(?) (4)
= log
m
?
j=1
P?(pi j|S j) ?
n
?
i=1
?2i
2?2i
=
m
?
j=1
log
?
d??(pi j) e
?. f (d,pi j)
?
???(S j) e
?. f (?) ?
n
?
i=1
?2i
2?2i
=
m
?
j=1
log
?
d??(pi j)
e?. f (d,pi j)
?
m
?
j=1
log
?
???(S j)
e?. f (?) ?
n
?
i=1
?2i
2?2i
where n is the number of features. Rather than have
a different smoothing parameter ?i for each feature,
we use a single parameter ?.
We use a technique from the numerical optimisa-
tion literature, the L-BFGS algorithm (Nocedal and
Wright, 1999), to optimise the objective function.
L-BFGS is an iterative algorithm which requires the
gradient of the objective function to be computed at
each iteration. The components of the gradient vec-
tor are as follows:
?L?(?)
??i
=
m
?
j=1
?
d??(pi j)
e?. f (d,pi j) fi(d, pi j)
?
d??(pi j) e
?. f (d,pi j) (5)
?
m
?
j=1
?
???(S j)
e?. f (?) fi(?)
?
???(S j) e
?. f (?) ?
?i
?2i
The first two terms in (5) are expectations of fea-
ture fi: the first expectation is over all derivations
leading to each gold standard dependency struc-
ture; the second is over all derivations for each sen-
tence in the training data. Setting the gradient to
zero yields the usual maximum entropy constraints
(Berger et al, 1996), except that in this case the
empirical values are themselves expectations (over
all derivations leading to each gold standard depen-
dency structure). The estimation process attempts
to make the expectations equal, by putting as much
mass as possible on the derivations leading to the
gold standard structures.1 The Gaussian prior term
penalises any model whose weights get too large in
absolute value.
Calculation of the feature expectations requires
summing over all derivations for a sentence, and
summing over all derivations leading to a gold stan-
dard dependency structure. In both cases there can
be exponentially many derivations, and so enumer-
ating all derivations is not possible (at least for
wide-coverage automatically extracted grammars).
Clark and Curran (2003) show how the sum over
the complete derivation space can be performed ef-
ficiently using a packed chart and a variant of the
inside-outside algorithm. Section 5 shows how the
same technique can also be applied to all derivations
leading to a gold standard dependency structure.
3.2 The Normal-Form Model
The objective function and gradient vector for the
normal-form model are as follows:
L?(?) = L(?) ?G(?) (6)
= log
m
?
j=1
P?(d j|S j) ?
n
?
i=1
?2i
2?2i
?L?(?)
??i
=
m
?
j=1
fi(d j) (7)
?
m
?
j=1
?
d??(S j)
e?. f (d) fi(d)
?
d??(S j) e
?. f (d) ?
?i
?2i
1See Riezler et al (2002) for a similar description in the
context of LFG parsing.
where d j is the the gold standard derivation for sen-
tence S j and ?(S j) is the set of possible derivations
for S j. Note that the empirical expectation in (7) is
simply a count of the number of times the feature
appears in the gold-standard derivations.
4 Packed Charts
The packed charts perform a number of roles: they
are a compact representation of a very large num-
ber of CCG derivations; they allow recovery of the
highest scoring parse or dependency structure with-
out enumerating all derivations; and they represent
an instance of what Miyao and Tsujii (2002) call a
feature forest, which is used to efficiently estimate a
log-linear model. The idea behind a packed chart is
simple: equivalent chart entries of the same type, in
the same cell, are grouped together, and back point-
ers to the daughters indicate how an individual entry
was created. Equivalent entries form the same struc-
tures in any subsequent parsing.
Since the packed charts are used for model es-
timation and recovery of the highest scoring parse
or dependency structure, the features in the model
partly determine which entries can be grouped to-
gether. In this paper we use features from the de-
pendency structure, and features defined on the lo-
cal rule instantiations.2 Hence, any two entries with
identical category type, identical head, and identical
unfilled dependencies are equivalent. Note that not
all features are local to a rule instantiation; for ex-
ample, features encoding long-range dependencies
may involve words which are a long way apart in
the sentence.
For the purposes of estimation and finding the
highest scoring parse or dependency structure, only
entries which are part of a derivation spanning the
whole sentence are relevant. These entries can be
easily found by traversing the chart top-down, start-
ing with the entries which span the sentence. The
entries within spanning derivations form a feature
forest (Miyao and Tsujii, 2002). A feature forest ?
is a tuple ?C,D,R, ?, ?? where:
  C is a set of conjunctive nodes;
  D is a set of disjunctive nodes;
  R ? D is a set of root disjunctive nodes;
  ? : D? 2C is a conjunctive daughter function;
  ? : C ? 2D is a disjunctive daughter function.
The individual entries in a cell are conjunctive
nodes, and the equivalence classes of entries are dis-
2By rule instantiation we mean the local tree arising from
the application of a CCG combinatory rule.
?C,D,R, ?, ?? is a packed chart / feature forest
G is a set of gold standard dependencies
Let c be a conjunctive node
Let d be a disjunctive node
deps(c) is the set of dependencies on node c
cdeps(c) =
{
?1 if, for some ? ? deps(c), ? < G
|deps(c)| otherwise
dmax(c) =
?
?
?
?
?
?
?
?
?
?1 if cdeps(c) = ?1
?1 if dmax(d) = ?1 for some d ? ?(c)
?
d??(c) dmax(d) + cdeps(c) otherwise
dmax(d) = max{dmax(c) | c ? ?(d)}
mark(d):
mark d as a correct node
foreach c ? ?(d)
if dmax(c) = dmax(d)
mark c as a correct node
foreach d? ? ?(c)
mark(d?)
foreach dr ? R such that dmax. (dr) = |G|
mark(dr)
Figure 1: Finding nodes in correct derivations
junctive nodes. The roots of the CCG derivations
represent the root disjunctive nodes.3
5 Efficient Estimation
The L-BFGS algorithm requires the following val-
ues at each iteration: the expected value, and the
empirical expected value, of each feature (to calcu-
late the gradient); and the value of the likelihood
function. For the normal-form model, the empiri-
cal expected values and the likelihood can easily be
obtained, since these only involve the single gold-
standard derivation for each sentence. The expected
values can be calculated using the method in Clark
and Curran (2003).
For the dependency model, the computations of
the empirical expected values (5) and the likelihood
function (4) are more complex, since these require
sums over just those derivations leading to the gold
standard dependency structure. We will refer to
such derivations as correct derivations.
Figure 1 gives an algorithm for finding nodes in
a packed chart which appear in correct derivations.
cdeps(c) is the number of correct dependencies on
conjunctive node c, and takes the value ?1 if there
are any incorrect dependencies on c. dmax(c) is
3A more complete description of CCG feature forests is
given in Clark and Curran (2003).
the maximum number of correct dependencies pro-
duced by any sub-derivation headed by c, and takes
the value ?1 if there are no sub-derivations produc-
ing only correct dependencies. dmax(d) is the same
value but for disjunctive node d. Recursive defini-
tions for calculating these values are given in Fig-
ure 1; the base case occurs when conjunctive nodes
have no disjunctive daughters.
The algorithm identifies all those root nodes
heading derivations which produce just the cor-
rect dependencies, and traverses the chart top-down
marking the nodes in those derivations. The in-
sight behind the algorithm is that, for two conjunc-
tive nodes in the same equivalence class, if one
node heads a sub-derivation producing more cor-
rect dependencies than the other node (and each
sub-derivation only produces correct dependencies),
then the node with less correct dependencies cannot
be part of a correct derivation.
The conjunctive and disjunctive nodes appearing
in correct derivations form a new correct feature for-
est. The correct forest, and the complete forest con-
taining all derivations spanning the sentence, can be
used to estimate the required likelihood value and
feature expectations. Let E?? fi be the expected value
of fi over the forest ? for model ?; then the values
in (5) can be obtained by calculating E? j? fi for the
complete forest ? j for each sentence S j in the train-
ing data (the second sum in (5)), and also E? j? fi for
each forest ? j of correct derivations (the first sum
in (5)):
?L(?)
??i
=
m
?
j=1
(E? j? fi ? E
? j
? fi) (8)
The likelihood in (4) can be calculated as follows:
L(?) =
m
?
j=1
(log Z? j ? log Z? j) (9)
where log Z? is the normalisation constant for ?.
6 Estimation in Practice
The gold standard dependency structures are pro-
duced by running our CCG parser over the
normal-form derivations in CCGbank (Hocken-
maier, 2003a). Not all rule instantiations in CCG-
bank are instances of combinatory rules, and not all
can be produced by the parser, and so gold standard
structures were created for 85.5% of the sentences
in sections 2-21 (33,777 sentences).
The same parser is used to produce the packed
charts. The parser uses a maximum entropy su-
pertagger (Clark and Curran, 2004) to assign lexical
categories to the words in a sentence, and applies the
CKY chart parsing algorithm described in Steedman
(2000). For parsing the training data, we ensure that
the correct category is a member of the set assigned
to each word. The average number of categories as-
signed to each word is determined by a parameter
in the supertagger. For the first set of experiments,
we used a setting which assigns 1.7 categories on
average per word.
The feature set for the dependency model con-
sists of the following types of features: dependency
features (with and without distance measures), rule
instantiation features (with and without a lexical
head), lexical category features, and root category
features. Dependency features are the 5-tuples de-
fined in Section 1. There are also three additional
dependency feature types which have an extra dis-
tance field (and only include the head of the lex-
ical category, and not the head of the argument);
these count the number of words (0, 1, 2 or more),
punctuation marks (0, 1, 2 or more), and verbs (0,
1 or more) between head and dependent. Lexi-
cal category features are word?category pairs at the
leaf nodes, and root features are headword?category
pairs at the root nodes. Rule instantiation features
simply encode the combining categories together
with the result category. There is an additional rule
feature type which also encodes the lexical head of
the resulting category. Additional generalised fea-
tures for each feature type are formed by replacing
words with their POS tags.
The feature set for the normal-form model is
the same except that, following Hockenmaier and
Steedman (2002), the dependency features are de-
fined in terms of the local rule instantiations, by
adding the heads of the combining categories to the
rule instantiation features. Again there are 3 addi-
tional distance feature types, as above, which only
include the head of the resulting category. We had
hoped that by modelling the predicate-argument de-
pendencies produced by the parser, rather than local
rule dependencies, we would improve performance.
However, using the predicate-argument dependen-
cies in the normal-form model instead of, or in ad-
dition to, the local rule dependencies, has not led to
an improvement in parsing accuracy.
Only features which occurred more than once in
the training data were included, except that, for the
dependency model, the cutoff for the rule features
was 9 and the counting was performed across all
derivations, not just the gold-standard derivation.
The normal-form model has 482,007 features and
the dependency model has 984,522 features.
We used 45 machines of a 64-node Beowulf clus-
ter to estimate the dependency model, with an av-
erage memory usage of approximately 550 MB for
each machine. For the normal-form model we were
able to reduce the size of the charts considerably by
applying two types of restriction to the parser: first,
categories can only combine if they appear together
in a rule instantiation in sections 2?21 of CCGbank;
and second, we apply the normal-form restrictions
described in Eisner (1996). (See Clark and Curran
(2004) for a description of the Eisner constraints.)
The normal-form model requires only 5 machines
for estimation, with an average memory usage of
730 MB for each machine.
Initially we tried the parallel version of GIS de-
scribed in Clark and Curran (2003) to perform
the estimation, running over the Beowulf cluster.
However, we found that GIS converged extremely
slowly; this is in line with other recent results in the
literature applying GIS to globally optimised mod-
els such as conditional random fields, e.g. Sha and
Pereira (2003). As an alternative to GIS, we have
implemented a parallel version of our L-BFGS code
using the Message Passing Interface (MPI) standard.
L-BFGS over forests can be parallelised, using the
method described in Clark and Curran (2003) to cal-
culate the feature expectations. The L-BFGS algo-
rithm, run to convergence on the cluster, takes 479
iterations and 2 hours for the normal-form model,
and 1,550 iterations and roughly 17 hours for the
dependency model.
7 Parsing Algorithm
For the normal-form model, the Viterbi algorithm is
used to find the most probable derivation. For the
dependency model, the highest scoring dependency
structure is required. Clark and Curran (2003) out-
lines an algorithm for finding the most probable de-
pendency structure, which keeps track of the high-
est scoring set of dependencies for each node in
the chart. For a set of equivalent entries in the
chart (a disjunctive node), this involves summing
over all conjunctive node daughters which head sub-
derivations leading to the same set of high scoring
dependencies. In practice large numbers of such
conjunctive nodes lead to very long parse times.
As an alternative to finding the most probable
dependency structure, we have developed an algo-
rithm which maximises the expected labelled re-
call over dependencies. Our algorithm is based on
Goodman?s (1996) labelled recall algorithm for the
phrase-structure PARSEVAL measures.
Let Lpi be the number of correct dependencies in
pi with respect to a gold standard dependency struc-
ture G; then the dependency structure, pimax, which
maximises the expected recall rate is:
pimax = arg maxpi E(Lpi/|G|) (10)
= arg max
pi
?
pii
P(pii|S )|pi ? pii|
where S is the sentence for gold standard depen-
dency structure G and pii ranges over the depen-
dency structures for S . This expression can be ex-
panded further:
pimax = arg maxpi
?
pii
P(pii|S )
?
??pi
1 if ? ? pii
= arg max
pi
?
??pi
?
pi? |??pi?
P(pi?|S )
= arg max
pi
?
??pi
?
d??(pi?)|??pi?
P(d|S ) (11)
The final score for a dependency structure pi is a
sum of the scores for each dependency ? in pi; and
the score for a dependency ? is the sum of the proba-
bilities of those derivations producing ?. This latter
sum can be calculated efficiently using inside and
outside scores:
pimax = arg maxpi
?
??pi
1
ZS
?
c?C
?c?c if ? ? deps(c)
(12)
where ?c is the inside score and ?c is the outside
score for node c (see Clark and Curran (2003)); C
is the set of conjunctive nodes in the packed chart
for sentence S and deps(c) is the set of dependen-
cies on conjunctive node c. The intuition behind
the expected recall score is that a dependency struc-
ture scores highly if it has dependencies produced
by high scoring derivations.4
The algorithm which finds pimax is a simple vari-
ant on the Viterbi algorithm, efficiently finding a
derivation which produces the highest scoring set of
dependencies.
8 Experiments
Gold standard dependency structures were derived
from section 00 (for development) and section 23
(for testing) by running the parser over the deriva-
tions in CCGbank, some of which the parser could
not process. In order to increase the number of test
sentences, and to allow a fair comparison with other
CCG parsers, extra rules were encoded in the parser
(but we emphasise these were only used to obtain
4Coordinate constructions can create multiple dependencies
for a single argument slot; in this case the score for the multiple
dependencies is the average of the individual scores.
LP LR UP UR cat
Dep model 86.7 85.6 92.6 91.5 93.5
N-form model 86.4 86.2 92.4 92.2 93.6
Table 1: Results on development set; labelled and unla-
belled precision and recall, and lexical category accuracy
Features LP LR UP UR cat
RULES 82.6 82.0 89.7 89.1 92.4
+HEADS 83.6 83.3 90.2 90.0 92.8
+DEPS 85.5 85.3 91.6 91.3 93.5
+DISTANCE 86.4 86.2 92.4 92.2 93.6
FINAL 87.0 86.8 92.7 92.5 93.9
Table 2: Results on development set for the normal-
form models
the section 23 test data; they were not used to parse
unseen data as part of the testing). This resulted in
2,365 dependency structures for section 23 (98.5%
of the full section), and 1,825 (95.5%) dependency
structures for section 00.
The first stage in parsing the test data is to apply
the supertagger. We use the novel strategy devel-
oped in Clark and Curran (2004): first assign a small
number of categories (approximately 1.4) on aver-
age to each word, and increase the number of cate-
gories if the parser fails to find an analysis. We were
able to parse 98.9% of section 23 using this strategy.
Clark and Curran (2004) shows that this supertag-
ging method results in a highly efficient parser.
For the normal-form model we returned the de-
pendency structure for the most probable derivation,
applying the two types of normal-form constraints
described in Section 6. For the dependency model
we returned the dependency structure with the high-
est expected labelled recall score.
Following Clark et al (2002), evaluation is by
precision and recall over dependencies. For a la-
belled dependency to be correct, the first 4 elements
of the dependency tuple must match exactly. For
an unlabelled dependency to be correct, the heads
of the functor and argument must appear together
in some relation in the gold standard (in any order).
The results on section 00, using the feature sets de-
scribed earlier, are given in Table 1, with similar
results overall for the normal-form model and the
dependency model. Since experimentation is easier
with the normal-form model than the dependency
model, we present additional results for the normal-
form model.
Table 2 gives the results for the normal-form
model for various feature sets. The results show
that each additional feature type increases perfor-
LP LR UP UR cat
Clark et al 2002 81.9 81.8 90.1 89.9 90.3
Hockenmaier 2003 84.3 84.6 91.8 92.2 92.2
Log-linear 86.6 86.3 92.5 92.1 93.6
Hockenmaier(POS) 83.1 83.5 91.1 91.5 91.5
Log-linear (POS) 84.8 84.5 91.4 91.0 92.5
Table 3: Results on the test set
mance. Hockenmaier also found the dependencies
to be very beneficial ? in contrast to recent results
from the lexicalised PCFG parsing literature (Gildea,
2001) ? but did not gain from the use of distance
measures. One of the advantages of a log-linear
model is that it is easy to include additional infor-
mation, such as distance, as features.
The FINAL result in Table 2 is obtained by us-
ing a larger derivation space for training, created
using more categories per word from the supertag-
ger, 2.9, and hence using charts containing more
derivations. (15 machines were used to estimate this
model.) More investigation is needed to find the op-
timal chart size for estimation, but the results show
a gain in accuracy.
Table 3 gives the results of the best performing
normal-form model on the test set. The results
of Clark et al (2002) and Hockenmaier (2003a)
are shown for comparison. The dependency set
used by Hockenmaier contains some minor differ-
ences to the set used here, but ?evaluating? our test
set against Hockenmaier?s gives an F-score of over
97%, showing the test sets to be very similar. The
results show that our parser is performing signifi-
cantly better than that of Clark et al, demonstrating
the benefit of derivation features and the use of a
sound statistical model.
The results given so far have all used gold stan-
dard POS tags from CCGbank. Table 3 also gives the
results if automatically assigned POS tags are used
in the training and testing phases, using the C&C
POS tagger (Curran and Clark, 2003). The perfor-
mance reduction is expected given that the supertag-
ger relies heavily on POS tags as features.
More investigation is needed to properly com-
pare our parser and Hockenmaier?s, since there are
a number of differences in addition to the models
used: Hockenmaier effectively reads a lexicalised
PCFG off CCGbank, and is able to use all of the
available training data; Hockenmaier does not use
a supertagger, but does use a beam search.
Parsing the 2,401 sentences in section 23 takes
1.6 minutes using the normal-form model, and 10.5
minutes using the dependency model. The differ-
ence is due largely to the normal-form constraints
used by the normal-form parser. Clark and Curran
(2004) shows that the normal-form constraints sig-
nificantly increase parsing speed and, in combina-
tion with adaptive supertagging, result in a highly
efficient wide-coverage parser.
As a final oracle experiment we parsed the sen-
tences in section 00 using the correct lexical cate-
gories from CCGbank. Since the parser uses only a
subset of the lexical categories in CCGbank, 7% of
the sentences could not be parsed; however, the la-
belled F-score for the parsed sentences was almost
98%. This very high score demonstrates the large
amount of information in lexical categories.
9 Conclusion
A major contribution of this paper has been the de-
velopment of a parsing model for CCG which uses
all derivations, including non-standard derivations.
Non-standard derivations are an integral part of the
CCG formalism, and it is an interesting question
whether efficient estimation and parsing algorithms
can be defined for models which use all derivations.
We have answered this question, and in doing so
developed a new parsing algorithm for CCG which
maximises expected recall of dependencies.
We would like to extend the dependency model,
by including the local-rule dependencies which are
used by the normal-form model, for example. How-
ever, one of the disadvantages of the dependency
model is that the estimation process is already using
a large proportion of our existing resources, and ex-
tending the feature set will further increase the exe-
cution time and memory requirement of the estima-
tion algorithm.
We have also shown that a normal-form model
performs as well as the dependency model. There
are a number of advantages to the normal-form
model: it requires less space and time resources
for estimation and it produces a faster parser. Our
normal-form parser significantly outperforms the
parser of Clark et al (2002) and produces results
at least as good as the current state-of-the-art for
CCG parsing. The use of adaptive supertagging and
the normal-form constraints result in a very efficient
wide-coverage parser. Our system demonstrates
that accurate and efficient wide-coverage CCG pars-
ing is feasible.
Future work will investigate extending the feature
sets used by the log-linear models with the aim of
further increasing parsing accuracy. Finally, the ora-
cle results suggest that further experimentation with
the supertagger will significantly improve parsing
accuracy, efficiency and robustness.
Acknowledgements
We would like to thank Julia Hockenmaier for
the use of CCGbank and helpful comments, and
Mark Steedman for guidance and advice. Jason
Baldridge, Frank Keller, Yuval Krymolowski and
Miles Osborne provided useful feedback. This work
was supported by EPSRC grant GR/M96889, and a
Commonwealth scholarship and a Sydney Univer-
sity Travelling scholarship to the second author.
References
Adam Berger, Stephen Della Pietra, and Vincent Della
Pietra. 1996. A maximum entropy approach to nat-
ural language processing. Computational Linguistics,
22(1):39?71.
Stanley Chen and Ronald Rosenfeld. 1999. A Gaussian
prior for smoothing maximum entropy models. Tech-
nical report, Carnegie Mellon University, Pittsburgh,
PA.
Stephen Clark and James R. Curran. 2003. Log-linear
models for wide-coverage CCG parsing. In Proceed-
ings of the EMNLP Conference, pages 97?104, Sap-
poro, Japan.
Stephen Clark and James R. Curran. 2004. The impor-
tance of supertagging for wide-coverage CCG pars-
ing. In Proceedings of COLING-04, Geneva, Switzer-
land.
Stephen Clark, Julia Hockenmaier, and Mark Steedman.
2002. Building deep dependency structures with a
wide-coverage CCG parser. In Proceedings of the
40th Meeting of the ACL, pages 327?334, Philadel-
phia, PA.
Michael Collins. 1996. A new statistical parser based on
bigram lexical dependencies. In Proceedings of the
34th Meeting of the ACL, pages 184?191, Santa Cruz,
CA.
James R. Curran and Stephen Clark. 2003. Investigating
GIS and smoothing for maximum entropy taggers. In
Proceedings of the 10th Meeting of the EACL, pages
91?98, Budapest, Hungary.
Jason Eisner. 1996. Efficient normal-form parsing for
Combinatory Categorial Grammar. In Proceedings of
the 34th Meeting of the ACL, pages 79?86, Santa
Cruz, CA.
Daniel Gildea. 2001. Corpus variation and parser per-
formance. In Proceedings of the EMNLP Conference,
pages 167?202, Pittsburgh, PA.
Joshua Goodman. 1996. Parsing algorithms and metrics.
In Proceedings of the 34th Meeting of the ACL, pages
177?183, Santa Cruz, CA.
Julia Hockenmaier and Mark Steedman. 2002. Gen-
erative models for statistical parsing with Combina-
tory Categorial Grammar. In Proceedings of the 40th
Meeting of the ACL, pages 335?342, Philadelphia, PA.
Julia Hockenmaier. 2003a. Data and Models for Statis-
tical Parsing with Combinatory Categorial Grammar.
Ph.D. thesis, University of Edinburgh.
Julia Hockenmaier. 2003b. Parsing with generative
models of predicate-argument structure. In Proceed-
ings of the 41st Meeting of the ACL, pages 359?366,
Sapporo, Japan.
Yusuke Miyao and Jun?ichi Tsujii. 2002. Maximum en-
tropy estimation for feature forests. In Proceedings
of the Human Language Technology Conference, San
Diego, CA.
Jorge Nocedal and Stephen J. Wright. 1999. Numerical
Optimization. Springer, New York, USA.
Stefan Riezler, Tracy H. King, Ronald M. Kaplan,
Richard Crouch, John T. Maxwell III, and Mark John-
son. 2002. Parsing the Wall Street Journal using a
Lexical-Functional Grammar and discriminative esti-
mation techniques. In Proceedings of the 40th Meet-
ing of the ACL, pages 271?278, Philadelphia, PA.
Fei Sha and Fernando Pereira. 2003. Shallow parsing
with conditional random fields. In Proceedings of the
HLT/NAACL Conference, pages 213?220, Edmonton,
Canada.
Mark Steedman. 2000. The Syntactic Process. The MIT
Press, Cambridge, MA.
Kristina Toutanova, Christopher Manning, Stuart
Shieber, Dan Flickinger, and Stephan Oepen. 2002.
Parse disambiguation for a rich HPSG grammar. In
Proceedings of the First Workshop on Treebanks
and Linguistic Theories, pages 253?263, Sozopol,
Bulgaria.
Bootstrapping POS taggers using Unlabelled Data
Stephen Clark, James R. Curran and Miles Osborne
School of Informatics
University of Edinburgh
2 Buccleuch Place, Edinburgh. EH8 9LW
fstephenc,jamesc,osborneg@cogsci.ed.ac.uk
Abstract
This paper investigates booststrapping part-of-
speech taggers using co-training, in which two
taggers are iteratively re-trained on each other?s
output. Since the output of the taggers is noisy,
there is a question of which newly labelled ex-
amples to add to the training set. We investi-
gate selecting examples by directly maximising
tagger agreement on unlabelled data, a method
which has been theoretically and empirically
motivated in the co-training literature. Our
results show that agreement-based co-training
can significantly improve tagging performance
for small seed datasets. Further results show
that this form of co-training considerably out-
performs self-training. However, we find that
simply re-training on all the newly labelled data
can, in some cases, yield comparable results to
agreement-based co-training, with only a frac-
tion of the computational cost.
1 Introduction
Co-training (Blum and Mitchell, 1998), and several vari-
ants of co-training, have been applied to a number of
NLP problems, including word sense disambiguation
(Yarowsky, 1995), named entity recognition (Collins
and Singer, 1999), noun phrase bracketing (Pierce and
Cardie, 2001) and statistical parsing (Sarkar, 2001;
Steedman et al, 2003). In each case, co-training was
used successfully to bootstrap a model from only a small
amount of labelled data and a much larger pool of un-
labelled data. Previous co-training approaches have typ-
ically used the score assigned by the model as an indi-
cator of the reliability of a newly labelled example. In
this paper we take a different approach, based on theoret-
ical work by Dasgupta et al (2002) and Abney (2002), in
which newly labelled training examples are selected us-
ing a greedy algorithm which explicitly maximises the
POS taggers? agreement on unlabelled data.
We investigate whether co-training based upon di-
rectly maximising agreement can be successfully ap-
plied to a pair of part-of-speech (POS) taggers: the
Markov model TNT tagger (Brants, 2000) and the max-
imum entropy C&C tagger (Curran and Clark, 2003).
There has been some previous work on boostrap-
ping POS taggers (e.g., Zavrel and Daelemans (2000) and
Cucerzan and Yarowsky (2002)), but to our knowledge
no previous work on co-training POS taggers.
The idea behind co-training the POS taggers is very
simple: use output from the TNT tagger as additional
labelled data for the maximum entropy tagger, and vice
versa, in the hope that one tagger can learn useful infor-
mation from the output of the other. Since the output of
both taggers is noisy, there is a question of which newly
labelled examples to add to the training set. The addi-
tional data should be accurate, but also useful, providing
the tagger with new information. Our work differs from
the Blum and Mitchell (1998) formulation of co-training
by using two different learning algorithms rather than two
independent feature sets (Goldman and Zhou, 2000).
Our results show that, when using very small amounts
of manually labelled seed data and a much larger amount
of unlabelled material, agreement-based co-training can
significantly improve POS tagger accuracy. We also show
that simply re-training on all of the newly labelled data
is surprisingly effective, with performance depending on
the amount of newly labelled data added at each itera-
tion. For certain sizes of newly labelled data, this sim-
ple approach is just as effective as the agreement-based
method. We also show that co-training can still benefit
both taggers when the performance of one tagger is ini-
tially much better than the other.
We have also investigated whether co-training can im-
prove the taggers already trained on large amounts of
                                                               Edmonton, May-June 2003
                                                    held at HLT-NAACL 2003 , pp. 49-55
                                            Proceeings of the Seventh CoNLL conference
manually annotated data. Using standard sections of the
WSJ Penn Treebank as seed data, we have been unable
to improve the performance of the taggers using self-
training or co-training.
Manually tagged data for English exists in large quan-
tities, which means that there is no need to create taggers
from small amounts of labelled material. However, our
experiments are relevant for languages for which there
is little or no annotated data. We only perform the ex-
periments in English for convenience. Our experiments
can also be seen as a vehicle for exploring aspects of co-
training.
2 Co-training
Given two (or more) ?views? (as described in
Blum and Mitchell (1998)) of a classification task,
co-training can be informally described as follows:
 Learn separate classifiers for each view using a
small amount of labelled seed data.
 Use each classifier to label some previously unla-
belled data.
 For each classifier, add some subset of the newly la-
belled data to the training data.
 Retrain the classifiers and repeat.
The intuition behind the algorithm is that each classi-
fier is providing extra, informative labelled data for the
other classifier(s). Blum and Mitchell (1998) derive PAC-
like guarantees on learning by assuming that the two
views are individually sufficient for classification and the
two views are conditionally independent given the class.
Collins and Singer (1999) present a variant of the
Blum and Mitchell algorithm, which directly maximises
an objective function that is based on the level of
agreement between the classifiers on unlabelled data.
Dasgupta et al (2002) provide a theoretical basis for this
approach by providing a PAC-like analysis, using the
same independence assumption adopted by Blum and
Mitchell. They prove that the two classifiers have low
generalisation error if they agree on unlabelled data.
Abney (2002) argues that the Blum and Mitchell in-
dependence assumption is very restrictive and typically
violated in the data, and so proposes a weaker indepen-
dence assumption, for which the Dasgupta et al (2002)
results still hold. Abney also presents a greedy algorithm
that maximises agreement on unlabelled data, which pro-
duces comparable results to Collins and Singer (1999) on
their named entity classification task.
Goldman and Zhou (2000) show that, if the newly la-
belled examples used for re-training are selected care-
fully, co-training can still be successful even when the
views used by the classifiers do not satisfy the indepen-
dence assumption.
In remainder of the paper we present a practical
method for co-training POS taggers, and investigate the
extent to which example selection based on the work of
Dasgupta et al and Abney can be effective.
3 The POS taggers
The two POS taggers used in the experiments are TNT, a
publicly available Markov model tagger (Brants, 2000),
and a reimplementation of the maximum entropy (ME)
tagger MXPOST (Ratnaparkhi, 1996). The ME tagger,
which we refer to as C&C, uses the same features as MX-
POST, but is much faster for training and tagging (Cur-
ran and Clark, 2003). Fast training and tagging times
are important for the experiments performed here, since
the bootstrapping process can require many tagging and
training iterations.
The model used by TNT is a standard tagging Markov
model, consisting of emission probabilities, and transi-
tion probabilities based on trigrams of tags. It also deals
with unknown words using a suffix analysis of the target
word (the word to be tagged). TNT is very fast for both
training and tagging.
The C&C tagger differs in a number of ways from
TNT. First, it uses a conditional model of a tag sequence
given a string, rather than a joint model. Second, ME
models are used to define the conditional probabilities of
a tag given some context. The advantage of ME mod-
els over the Markov model used by TNT is that arbitrary
features can easily be included in the context; so as well
as considering the target word and the previous two tags
(which is the information TNT uses), the ME models also
consider the words either side of the target word and, for
unknown and infrequent words, various properties of the
string of the target word.
A disadvantage is that the training times for ME mod-
els are usually relatively slow, especially with iterative
scaling methods (see Malouf (2002) for alternative meth-
ods). Here we use Generalised Iterative Scaling (Dar-
roch and Ratcliff, 1972), but our implementation is much
faster than Ratnaparkhi?s publicly available tagger. The
C&C tagger trains in less than 7 minutes on the 1 million
words of the Penn Treebank, and tags slightly faster than
TNT.
Since the taggers share many common features, one
might think they are not different enough for effective
co-training to be possible. In fact, both taggers are suffi-
ciently different for co-training to be effective. Section 4
shows that both taggers can benefit significantly from the
information contained in the other?s output.
The performance of the taggers on section 00 of the
WSJ Penn Treebank is given in Table 1, for different seed
set sizes (number of sentences). The seed data is taken
Tagger 50 seed 500 seed ? 40,000 seed
TNT 81.3 91.0 96.5
C&C 73.2 88.3 96.8
Table 1: Tagger performance for different seed sets
from sections 2?21 of the Treebank. The table shows that
the performance of TNT is significantly better than the
performance of C&C when the size of the seed data is
very small.
4 Experiments
The co-training framework uses labelled examples from
one tagger as additional training data for the other. For
the purposes of this paper, a labelled example is a tagged
sentence. We chose complete sentences, rather than
smaller units, because this simplifies the experiments and
the publicly available version of TNT requires complete
tagged sentences for training. It is possible that co-
training with sub-sentential units might be more effective,
but we leave this as future work.
The co-training process is given in Figure 1. At
each stage in the process there is a cache of unla-
belled sentences (selected from the total pool of un-
labelled sentences) which is labelled by each tagger.
The cache size could be increased at each iteration,
which is a common practice in the co-training litera-
ture. A subset of those sentences labelled by TNT is
then added to the training data for C&C, and vice versa.
Blum and Mitchell (1998) use the combined set of newly
labelled examples for training each view, but we fol-
low Goldman and Zhou (2000) in using separate labelled
sets. In the remainder of this section we consider two pos-
sible methods for selecting a subset. The cache is cleared
after each iteration.
There are various ways to select the labelled examples
for each tagger. A typical approach is to select those ex-
amples assigned a high score by the relevant classifier,
under the assumption that these examples will be the most
reliable. A score-based selection method is difficult to
apply in our experiments, however, since TNT does not
provide scores for tagged sentences.
We therefore tried two alternative selection methods.
The first is to simply add all of the cache labelled by one
tagger to the training data of the other. We refer to this
method as naive co-training. The second, more sophisti-
cated, method is to select that subset of the labelled cache
which maximises the agreement of the two taggers on un-
labelled data. We call this method agreement-based co-
training. For a large cache the number of possible subsets
makes exhaustive search intractable, and so we randomly
sample the subsets.
S is a seed set of labelled sentences
LT is labelled training data for TNT
LC is labelled training data for C&C
U is a large set of unlabelled sentences
C is a cache holding a small subset of U
initialise:
LT ? LC ? S
Train TNT and C&C on S
loop:
Partition U into the disjoint sets C and U?.
Label C with TNT and C&C
Select sentences labelled by TNT and add to LC
Train C&C on LC
Select sentences labelled by C&C and add to LT
Train TNT on LT
U = U?.
Until U is empty
Figure 1: The general co-training process
C is a cache of sentences labelled by the other tagger
U is a set of sentences, used for measuring agreement
initialise:
cmax ? ?; Amax ? 0
Repeat n times:
Randomly sample c ? C
Retrain current tagger using c as additional data
if new agreement rate, A, on U > Amax
Amax ? A; cmax ? c
return cmax
Figure 2: Agreement-based example selection
The pseudo-code for the agreement-based selection
method is given in Figure 2. The current tagger is the
one being retrained, while the other tagger is kept static.
The co-training process uses the selection method for se-
lecting sentences from the cache (which has been labelled
by one of the taggers). Note that during the selection pro-
cess, we repeatedly sample from all possible subsets of
the cache; this is done by first randomly choosing the
size of the subset and then randomly choosing sentences
based on the size. The number of subsets we consider is
determined by the number of times the loop is traversed
in Figure 2.
If TNT is being trained on the output of C&C, then the
most recent version of C&C is used to measure agreement
(and vice versa); so we first attempt to improve one tag-
ger, then the other, rather than both at the same time. The
agreement rate of the taggers on unlabelled sentences is
the per-token agreement rate; that is, the number of times
each word in the unlabelled set of sentences is assigned
the same tag by both taggers.
For the small seed set experiments, the seed data was
an arbitrarily chosen subset of sections 10?19 of the
WSJ Penn Treebank; the unlabelled training data was
taken from 50, 000 sentences of the 1994 WSJ section
of the North American News Corpus (NANC); and the
unlabelled data used to measure agreement was around
10, 000 sentences from sections 1?5 of the Treebank.
Section 00 of the Treebank was used to measure the ac-
curacy of the taggers. The cache size was 500 sentences.
4.1 Self-Training and Agreement-based Co-training
Results
Figure 3 shows the results for self-training, in which each
tagger is simply retrained on its own labelled cache at
each round. (By round we mean the re-training of a sin-
gle tagger, so there are two rounds per co-training itera-
tion.) TNT does improve using self-training, from 81.4%
to 82.2%, but C&C is unaffected. Re-running these ex-
periments using a range of unlabelled training sets, from
a variety of sources, showed similar behaviour.
0.73
0.74
0.75
0.76
0.77
0.78
0.79
0.8
0.81
0.82
0.83
0 5 10 15 20 25 30 35 40 45 50
Ac
cu
ra
cy
Number of rounds
TnT
C&C
Figure 3: Self-training TNT and C&C (50 seed sen-
tences). The upper curve is for TNT; the lower curve is
for C&C.
Figure 4 gives the results for the greedy agreement co-
training, using a cache size of 500 and searching through
100 subsets of the labelled cache to find the one that max-
imises agreement. Co-training improves the performance
of both taggers: TNT improves from 81.4% to 84.9%,
and C&C improves from 73.2% to 84.3% (an error re-
duction of over 40%).
Figures 5 and 6 show the self-training results and
agreement-based results when a larger seed set, of 500
sentences, is used for each tagger. In this case, self-
training harms TNT and C&C is again unaffected. Co-
training continues to be beneficial.
Figure 7 shows how the size of the labelled data set (the
number of sentences) grows for each tagger per round.
0.72
0.74
0.76
0.78
0.8
0.82
0.84
0.86
0 5 10 15 20 25 30 35 40 45 50
Ac
cu
ra
cy
Number of rounds
TnT
C&C
Figure 4: Agreement-based co-training between
TNT and C&C (50 seed sentences). The curve that
starts at a higher value is for TNT.
0.88
0.885
0.89
0.895
0.9
0.905
0.91
0.915
0 5 10 15 20 25 30 35 40 45 50
Ac
cu
ra
cy
Number of rounds
TnT
C&C
Figure 5: Self-training TNT and C&C (500 seed sen-
tences). The upper curve is for TNT; the lower curve is
for C&C.
Towards the end of the co-training run, more material is
being selected for C&C than TNT. The experiments us-
ing a seed set size of 50 showed a similar trend, but the
difference between the two taggers was less marked. By
examining the subsets chosen from the labelled cache at
each round, we also observed that a large proportion of
the cache was being selected for both taggers.
4.2 Naive Co-training Results
Agreement-based co-training for POS taggers is effective
but computationally demanding. The previous two agree-
ment maximisation experiments involved retraining each
tagger 2, 500 times. Given this, and the observation that
maximisation generally has a preference for selecting a
large proportion of the labelled cache, we looked at naive
co-training: simply retraining upon all available mate-
0.88
0.885
0.89
0.895
0.9
0.905
0.91
0.915
0.92
0 5 10 15 20 25 30 35 40 45 50
Ac
cu
ra
cy
Number of rounds
TnT
C&C
Figure 6: Agreement-based co-training between
TNT and C&C (500 seed sentences). The curve that
starts at a higher value is for TNT.
0
2000
4000
6000
8000
10000
12000
0 5 10 15 20 25 30 35 40 45 50
C&C
tntTnT
Figure 7: Growth in training-set sizes for co-training
TNT and C&C (500 seed sentences). The upper curve
is for C&C.
rial (i.e. the whole cache) at each round. Table 2 shows
the naive co-training results after 50 rounds of co-training
when varying the size of the cache. 50 manually labelled
sentences were used as the seed material. Table 3 shows
results for the same experiment, but this time with a seed
set of 500 manually labelled sentences.
We see that naive co-training improves as the cache
size increases. For a large cache, the performance lev-
els for naive co-training are very similar to those pro-
duced by our agreement-based co-training method. Af-
ter 50 rounds of co-training using 50 seed sentences,
the agreement rates for naive and agreement-based co-
training were very similar: from an initial value of 73%
to 97% agreement.
Naive co-training is more efficient than agreement-
based co-training. For the parameter settings used in
Amount added TNT C&C
0 81.3 73.2
50 82.9 82.7
100 83.5 83.3
150 84.4 84.3
300 85.0 84.9
500 85.3 85.1
Table 2: Naive co-training accuracy results when varying
the amount added after each round (50 seed sentences)
Amount added TNT C&C
0 91.0 88.3
100 92.0 91.9
300 92.0 91.9
500 92.1 92.0
1000 92.0 91.9
Table 3: Naive co-training accuracy results when varying
the amount added after each round (500 seed sentences)
the previous experiments, agreement-based co-training
required the taggers to be re-trained 10 to 100 times
more often then naive co-training. There are advan-
tages to agreement-based co-training, however. First,
the agreement-based method dynamically selects the best
sample at each stage, which may not be the whole cache.
In particular, when the agreement rate cannot be im-
proved upon, the selected sample can be rejected. For
naive co-training, new samples will always be added,
and so there is a possibility that the noise accumulated
at later stages will start to degrade performance (see
Pierce and Cardie (2001)). Second, for naive co-training,
the optimal amount of data to be added at each round (i.e.
the cache size) is a parameter that needs to be determined
on held out data, whereas the agreement-based method
determines this automatically.
4.3 Larger-Scale Experiments
We also performed a number of experiments using much
more unlabelled training material than before. Instead
of using 50, 000 sentences from the 1994 WSJ section of
the North American News Corpus, we used 417, 000 sen-
tences (from the same section) and ran the experiments
until the unlabelled data had been exhausted.
One experiment used naive co-training, with 50 seed
sentences and a cache of size 500. This led to an agree-
ment rate of 99%, with performance levels of 85.4% and
85.4% for TNT and C&C respectively. 230, 000 sen-
tences (? 5 million words) had been processed and were
used as training material by the taggers. The other ex-
periment used our agreement-based co-training approach
(50 seed sentences, cache size of 1, 000 sentences, explor-
ing at most 10 subsets in the maximisation process per
round). The agreement rate was 98%, with performance
levels of 86.0% and 85.9% for both taggers. 124, 000
sentences had been processed, of which 30, 000 labelled
sentences were selected for training TNT and 44, 000 la-
belled sentences were selected for training C&C.
Co-training using this much larger amount of unla-
belled material did improve our previously mentioned re-
sults, but not by a large margin.
4.4 Co-training using Imbalanced Views
It is interesting to consider what happens when one view
is initially much more accurate than the other view. We
trained one of the taggers on much more labelled seed
data than the other, to see how this affects the co-training
process. Both taggers were initialised with either 500 or
50 seed sentences, and agreement-based co-training was
applied, using a cache size of 500 sentences. The results
are shown in Table 4.
Seed material Initial Perf Final Perf
TNT C&C TNT C&C TNT C&C
50 500 81.3 88.3 90.0 89.4
500 50 91.0 73.2 91.3 91.3
Table 4: Co-training Results for Imbalanced Views
Co-training continues to be effective, even when the
two taggers are imbalanced. Also, the final performance
of the taggers is around the same value, irrespective of
the direction of the imbalance.
4.5 Large Seed Experiments
Although bootstrapping from unlabelled data is particu-
larly valuable when only small amounts of training ma-
terial are available, it is also interesting to see if self-
training or co-training can improve state of the art POS
taggers.
For these experiments, both C&C and TNT were ini-
tially trained on sections 00?18 of the WSJ Penn Tree-
bank, and sections 19?21 and 22?24 were used as the
development and test sets. The 1994?1996 WSJ text
from the NANC was used as unlabelled material to fill the
cache.
The cache size started out at 8000 sentences and in-
creased by 10% in each round to match the increasing
labelled training data. In each round of self-training or
naive co-training 10% of the cache was randomly se-
lected and added to the labelled training data. The ex-
periments ran for 40 rounds.
The performance of the different training regimes is
listed in Table 5. These results show no significant im-
provement using either self-training or co-training with
very large seed datasets. Self-training shows only a slight
Method WSJ19?21 WSJ22?24
C&C TNT C&C TNT
Initial 96.71 96.50 96.78 96.46
Self-train 96.77 96.45 96.87 96.42
Naive co-train 96.74 96.48 96.76 96.46
Table 5: Performance with large seed sets
improvement for C&C1 while naive co-training perfor-
mance is always worse.
5 Conclusion
We have shown that co-training is an effective technique
for bootstrapping POS taggers trained on small amounts
of labelled data. Using unlabelled data, we are able to
improve TNT from 81.3% to 86.0%, whilst C&C shows
a much more dramatic improvement of 73.2% to 85.9%.
Our agreement-based co-training results support
the theoretical arguments of Abney (2002) and
Dasgupta et al (2002), that directly maximising the
agreement rates between the two taggers reduces gen-
eralisation error. Examination of the selected subsets
showed a preference for a large proportion of the cache.
This led us to propose a naive co-training approach,
which significantly reduced the computational cost
without a significant performance penalty.
We also showed that naive co-training was unable to
improve the performance of the taggers when they had
already been trained on large amounts of manually anno-
tated data. It is possible that agreement-based co-training,
using more careful selection, would result in an improve-
ment. We leave these experiments to future work, but
note that there is a large computational cost associated
with such experiments.
The performance of the bootstrapped taggers is still
a long way behind a tagger trained on a large amount
of manually annotated data. This finding is in accord
with earlier work on bootstrapping taggers using EM (El-
worthy, 1994; Merialdo, 1994). An interesting question
would be to determine the minimum number of manually
labelled examples that need to be used to seed the sys-
tem before we can achieve comparable results as using
all available manually labelled sentences.
For our experiments, co-training never led to a de-
crease in performance, regardless of the number of itera-
tions. The opposite behaviour has been observed in other
applications of co-training (Pierce and Cardie, 2001).
Whether this robustness is a property of the tagging prob-
lem or our approach is left for future work.
1This is probably by chance selection of better subsets.
Acknowledgements
This work has grown out of many fruitful discus-
sions with the 2002 JHU Summer Workshop team that
worked on weakly supervised bootstrapping of statistical
parsers. The first author was supported by EPSRC grant
GR/M96889, and the second author by a Commonwealth
scholarship and a Sydney University Travelling scholar-
ship. We would like to thank the anonymous reviewers
for their helpful comments, and also Iain Rae for com-
puter support.
References
Steven Abney. 2002. Bootstrapping. In Proceedings of
the 40th Annual Meeting of the Association for Compu-
tational Linguistics, pages 360?367, Philadelphia, PA.
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In Proceed-
ings of the 11th Annual Conference on Computational
Learning Theory, pages 92?100, Madisson, WI.
Thorsten Brants. 2000. TnT - a statistical part-of-speech
tagger. In Proceedings of the 6th Conference on Ap-
plied Natural Language Processing, pages 224?231.
Michael Collins and Yoram Singer. 1999. Unsupervised
models for named entity classification. In Proceedings
of the Empirical Methods in NLP Conference, pages
100?110, University of Maryland, MD.
Silviu Cucerzan and David Yarowsky. 2002. Boot-
strapping a multilingual part-of-speech tagger in one
person-day. In Proceedings of the 6th Workshop on
Computational Language Learning, Taipei, Taiwan.
James R. Curran and Stephen Clark. 2003. Investigating
GIS and Smoothing for Maximum Entropy Taggers.
In Proceedings of the 11th Annual Meeting of the Eu-
ropean Chapter of the Association for Computational
Linguistics, Budapest, Hungary. (to appear).
J. N. Darroch and D. Ratcliff. 1972. Generalized itera-
tive scaling for log-linear models. The Annals of Math-
ematical Statistics, 43(5):1470?1480.
Sanjoy Dasgupta, Michael Littman, and David
McAllester. 2002. PAC generalization bounds
for co-training. In T. G. Dietterich, S. Becker,
and Z. Ghahramani, editors, Advances in Neural
Information Processing Systems 14, pages 375?382,
Cambridge, MA. MIT Press.
D. Elworthy. 1994. Does Baum-Welch re-estimation
help taggers? In Proceedings of the 4th Conference
on Applied Natural Language Processing, pages 53?
58, Stuttgart, Germany.
Sally Goldman and Yan Zhou. 2000. Enhancing super-
vised learning with unlabeled data. In Proceedings of
the 17th International Conference on Machine Learn-
ing, Stanford, CA.
Robert Malouf. 2002. A comparison of algorithms
for maximum entropy parameter estimation. In Pro-
ceedings of the Sixth Workshop on Natural Language
Learning, pages 49?55, Taipei, Taiwan.
Bernard Merialdo. 1994. Tagging English text with
a probabilistic model. Computational Linguistics,
20(2):155?171.
David Pierce and Claire Cardie. 2001. Limitations of
co-training for natural language learning from large
datasets. In Proceedings of the Empirical Methods in
NLP Conference, Pittsburgh, PA.
Adwait Ratnaparkhi. 1996. A maximum entropy part-
of-speech tagger. In Proceedings of the EMNLP Con-
ference, pages 133?142, Philadelphia, PA.
Anoop Sarkar. 2001. Applying co-training methods to
statistical parsing. In Proceedings of the 2nd Annual
Meeting of the NAACL, pages 95?102, Pittsburgh, PA.
Mark Steedman, Miles Osborne, Anoop Sarkar, Stephen
Clark, Rebecca Hwa, Julia Hockenmaier, Paul Ruhlen,
Steven Baker, and Jeremiah Crim. 2003. Bootstrap-
ping statistical parsers from small datasets. In Pro-
ceedings of the 11th Annual Meeting of the European
Chapter of the Association for Computational Linguis-
tics, Budapest, Hungary. (to appear).
David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In Proceed-
ings of the 33rd Annual Meeting of the Association
for Computational Linguistics, pages 189?196, Cam-
bridge, MA.
Jakub Zavrel and Walter Daelemans. 2000. Bootstrap-
ping a tagged corpus through combination of exist-
ing heterogeneous taggers. In Proceedings of the 2nd
International Conference on Language Resources and
Evaluation, pages 17?20, Athens, Greece.
Language Independent NER using a Maximum Entropy Tagger
James R. Curran and Stephen Clark
School of Informatics
University of Edinburgh
2 Buccleuch Place, Edinburgh. EH8 9LW
{jamesc,stephenc}@cogsci.ed.ac.uk
Abstract
Named Entity Recognition (NER) systems need
to integrate a wide variety of information for
optimal performance. This paper demonstrates
that a maximum entropy tagger can effectively
encode such information and identify named
entities with very high accuracy. The tagger
uses features which can be obtained for a vari-
ety of languages and works effectively not only
for English, but also for other languages such
as German and Dutch.
1 Introduction
Named Entity Recognition1 (NER) can be treated as a
tagging problem where each word in a sentence is as-
signed a label indicating whether it is part of a named
entity and the entity type. Thus methods used for part
of speech (POS) tagging and chunking can also be used
for NER. The papers from the CoNLL-2002 shared
task which used such methods (e.g. Malouf (2002),
Burger et al (2002)) reported results significantly lower
than the best system (Carreras et al, 2002). However,
Zhou and Su (2002) have reported state of the art results
on the MUC-6 and MUC-7 data using a HMM-based tagger.
Zhou and Su (2002) used a wide variety of features,
which suggests that the relatively poor performance of the
taggers used in CoNLL-2002 was largely due to the fea-
ture sets used rather than the machine learning method.
We demonstrate this to be the case by improving on the
best Dutch results from CoNLL-2002 using a maximum
entropy (ME) tagger. We report reasonable precision and
recall (84.9 F-score) for the CoNLL-2003 English test
data, and an F-score of 68.4 for the CoNLL-2003 Ger-
man test data.
1We assume that NER involves assigning the correct label to
an entity as well as identifying its boundaries.
Incorporating a diverse set of overlapping features
in a HMM-based tagger is difficult and complicates the
smoothing typically used for such taggers. In contrast, a
ME tagger can easily deal with diverse, overlapping fea-
tures. We also use a Gaussian prior on the parameters for
effective smoothing over the large feature space.
2 The ME Tagger
The ME tagger is based on Ratnaparkhi (1996)?s POS tag-
ger and is described in Curran and Clark (2003) . The
tagger uses models of the form:
p(y|x) =
1
Z(x)
exp
(
n?
i=1
?ifi(x, y)
)
(1)
where y is the tag, x is the context and the fi(x, y) are
the features with associated weights ?i. The probability
of a tag sequence y1 . . . yn given a sentence w1 . . . wn is
approximated as follows:
p(y1 . . . yn|w1 . . . wn) ?
n?
i=1
p(yi|xi) (2)
where xi is the context for word wi. The tagger uses
beam search to find the most probable sequence given the
sentence.
The features are binary valued functions which pair a
tag with various elements of the context; for example:
fj(x, y) =
{
1 if word(x) = Moody & y = I-PER
0 otherwise
(3)
word(x) = Moody is an example of a contextual predi-
cate.
Generalised Iterative Scaling (GIS) is used to estimate
the values of the weights. The tagger uses a Gaussian
prior over the weights (Chen et al, 1999) which allows a
large number of rare, but informative, features to be used
without overfitting.
Condition Contextual predicate
freq(wi) < 5 X is prefix of wi, |X| ? 4
X is suffix of wi, |X| ? 4
wi contains a digit
wi contains uppercase character
wi contains a hyphen
?wi wi = X
wi?1 = X , wi?2 = X
wi+1 = X , wi+2 = X
?wi POSi = X
POSi?1 = X , POSi?2 = X
POSi+1 = X , POSi+2 = X
?wi NEi?1 = X
NEi?2NEi?1 = XY
Table 1: Contextual predicates in baseline system
3 The Data
We used three data sets: the English and German data
for the CoNLL-2003 shared task (Tjong Kim Sang and
De Meulder, 2003) and the Dutch data for the CoNLL-
2002 shared task (Tjong Kim Sang, 2002). Each word in
the data sets is annotated with a named entity tag plus POS
tag, and the words in the German and English data also
have a chunk tag. Our system does not currently exploit
the chunk tags.
There are 4 types of entities to be recognised: persons,
locations, organisations, and miscellaneous entities not
belonging to the other three classes. The 2002 data uses
the IOB-2 format in which a B-XXX tag indicates the first
word of an entity of type XXX and I-XXX is used for sub-
sequent words in an entity of type XXX. The tag O in-
dicates words outside of a named entity. The 2003 data
uses a variant of IOB-2, IOB-1, in which I-XXX is used
for all words in an entity, including the first word, unless
the first word separates contiguous entities of the same
type, in which case B-XXX is used.
4 The Feature Set
Table 1 lists the contextual predicates used in our base-
line system, which are based on those used in the
Curran and Clark (2003) CCG supertagger. The first set
of features apply to rare words, i.e. those which appear
less than 5 times in the training data. The first two kinds
of features encode prefixes and suffixes less than length 5,
and the remaining rare word features encode other mor-
phological characteristics. These features are important
for tagging unknown and rare words. The remaining fea-
tures are the word, POS tag, and NE tag history features,
using a window size of 2. Note that the NEi?2NEi?1
feature is a composite feature of both the previous and
previous-previous NE tags.
Condition Contextual predicate
freq(wi) < 5 wi contains period
wi contains punctuation
wi is only digits
wi is a number
wi is {upper,lower,title,mixed} case
wi is alphanumeric
length of wi
wi has only Roman numerals
wi is an initial (X.)
wi is an acronym (ABC, A.B.C.)
?wi memory NE tag for wi
unigram tag of wi+1
unigram tag of wi+2
?wi wi in a gazetteer
wi?1 in a gazetteer
wi+1 in a gazetteer
?wi wi not lowercase and flc > fuc
?wi unigrams of word type
bigrams of word types
trigrams of word types
Table 2: Contextual predicates in final system
Table 2 lists the extra features used in our final
system. These features have been shown to be use-
ful in other NER systems. The additional ortho-
graphic features have proved useful in other systems,
for example Carreras et al (2002), Borthwick (1999) and
Zhou and Su (2002). Some of the rows in Table 2 de-
scribe sets of contextual predicates. The wi is only digits
predicates apply to words consisting of all digits. They
encode the length of the digit string with separate pred-
icates for lengths 1?4 and a single predicate for lengths
greater than 4. Titlecase applies to words with an ini-
tial uppercase letter followed by all lowercase (e.g. Mr).
Mixedcase applies to words with mixed lower- and up-
percase (e.g. CityBank). The length predicates encode
the number of characters in the word from 1 to 15, with a
single predicate for lengths greater than 15.
The next set of contextual predicates encode extra in-
formation about NE tags in the current context. The
memory NE tag predicate (see e.g. Malouf (2002))
records the NE tag that was most recently assigned to
the current word. The use of beam-search tagging
means that tags can only be recorded from previous
sentences. This memory is cleared at the beginning
of each document. The unigram predicates (see e.g.
Tsukamoto et al (2002)) encode the most probable tag
for the next words in the window. The unigram probabil-
ities are relative frequencies obtained from the training
data. This feature enables us to know something about
the likely NE tag of the next word before reaching it.
Most systems use gazetteers to encode information
about personal and organisation names, locations and
trigger words. There is considerable variation in the size
of the gazetteers used. Some studies found that gazetteers
did not improve performance (e.g. Malouf (2002)) whilst
others gained significant improvement using gazetteers
and triggers (e.g. Carreras et al (2002)). Our system in-
corporates only English and Dutch first name and last
name gazetteers as shown in Table 6. These gazetteers
are used for predicates applied to the current, previous
and next word in the window.
Collins (2002) includes a number of interesting con-
textual predicates for NER. One feature we have adapted
encodes whether the current word is more frequently seen
lowercase than uppercase in a large external corpus. This
feature is useful for disambiguating beginning of sen-
tence capitalisation and tagging sentences which are all
capitalised. The frequency counts have been obtained
from 1 billion words of English newspaper text collected
by Curran and Osborne (2002).
Collins (2002) also describes a mapping from words to
word types which groups words with similar orthographic
forms into classes. This involves mapping characters to
classes and merging adjacent characters of the same type.
For example, Moody becomes Aa, A.B.C. becomes
A.A.A. and 1,345.05 becomes 0,0.0. The classes
are used to define unigram, bigram and trigram contex-
tual predicates over the window.
We have also defined additional composite features
which are a combination of atomic features; for exam-
ple, a feature which is active for mid-sentence titlecase
words seen more frequently as lowercase than uppercase
in a large external corpus.
5 Results
The baseline development results for English using the
supertagger features only are given in Table 3. The full
system results for the English development data are given
in Table 7. Clearly the additional features have a signifi-
cant impact on both precision and recall scores across all
entities. We have found that the word type features are
particularly useful, as is the memory feature. The perfor-
mance of the final system drops by 1.97% if these fea-
tures are removed. The performance of the system if the
gazetteer features are removed is given in Table 4. The
sizes of our gazetteers are given in Table 6. We have
experimented with removing the other contextual pred-
icates but each time performance was reduced, except for
the next-next unigram tag feature which was switched off
for all final experiments.
The results for the Dutch test data are given in Table 5.
These improve upon the scores of the best performing
system at CoNLL-2002 (Carreras et al, 2002).
English DEV PRECISION RECALL F?=1
LOCATION 90.78% 90.58% 90.68
MISC 85.80% 81.24% 83.45
ORGANISATION 82.24% 80.09% 81.15
PERSON 92.02% 92.67% 92.35
OVERALL 88.53% 87.41% 87.97
Table 3: Baseline C&C results for English DEV data
English DEV PRECISION RECALL F?=1
LOCATION 91.69% 93.14% 92.41
MISC 88.15% 83.08% 85.54
ORGANISATION 83.48% 85.53% 84.49
PERSON 94.40% 95.11% 94.75
OVERALL 90.13% 90.47% 90.30
Table 4: No external resources results for Eng. DEV data
Dutch TEST PRECISION RECALL F?=1
LOCATION 84.42% 81.91% 83.15
MISC 78.46% 74.89% 76.64
ORGANISATION 77.35% 68.93% 72.90
PERSON 80.13% 90.71% 85.09
OVERALL 79.91% 79.35% 79.63
Table 5: Results for the Dutch TEST data
Gazetteer ENTRIES
FIRST NAME 6,673
LAST NAME 89,836
freqLC > freqUC LIST 778,791
Table 6: Size of Gazetteers
The final results for the English test data are given in
Table 7. These are significantly lower than the results for
the development data. The results for the German devel-
opment and test sets are given in Table 7. For the German
NER we removed the lowercase more frequent than up-
percase feature. Apart from this change, the system was
identical. We did not add any extra gazetteer information
for German.
6 Conclusion
Our NER system demonstrates that using a large variety
of features produces good performance. These features
can be defined and extracted in a language independent
manner, as our results for German, Dutch and English
show. Maximum entropy models are an effective way
of incorporating diverse and overlapping features. Our
maximum entropy tagger employs Gaussian smoothing
which allows a large number of sparse, but informative,
features to be used without overfitting.
Using a wider context window than 2 words may im-
prove performance; a reranking phase using global fea-
tures may also improve performance (Collins, 2002).
Acknowledgements
We would like to thank Jochen Leidner for help collecting
the Gazetteers. This research was supported by a Com-
monwealth scholarship and a Sydney University Trav-
elling scholarship to the first author, and EPSRC grant
GR/M96889.
References
Andrew Borthwick. 1999. A Maximum Entropy Ap-
proach to Named Entity Recognition. Ph.D. thesis,
New York University.
John D. Burger, John C. Henderson, and William T. Mor-
gan. 2002. Statistical Named Entity Recogizer Adap-
tation. In Proceedings of the 2002 CoNLL Workshop,
pages 163?166, Taipei, Taiwan.
Xavier Carreras, Lluis Ma`rquez, and Lluis Padro?. 2002.
Named Entity Recognition using AdaBoost. In Pro-
ceedings of the 2002 CoNLL Workshop, pages 167?
170, Taipei, Taiwan.
John Chen, Srinivas Bangalore, and K. Vijay-Shanker.
1999. New Models for Improving Supertag Disam-
biguation. In Proceedings of the 9th Meeting of EACL,
Bergen, Norway.
Michael Collins. 2002. Ranking Algorithms for Named-
Entity Extraction: Boosting and the Voted Perceptron.
In Proceedings of the 40th Meeting of the ACL, pages
489?496, Philadelphia, PA.
James R. Curran and Stephen Clark. 2003. Investigating
GIS and Smoothing for Maximum Entropy Taggers.
In Proceedings of the 11th Meeting of the European
Chapter of the ACL, Budapest, Hungary.
James R. Curran and Miles Osborne. 2002. A very very
large corpus doesn?t always yield reliable estimates. In
Proceedings of the 2002 CoNLL Workshop, pages 126?
131, Taipei, Taiwan.
Robert Malouf. 2002. Markov models for language-
independent named entity recognition. In Proceedings
of the 2002 CoNLL Workshop, pages 187?190, Taipei,
Taiwan.
Adwait Ratnaparkhi. 1996. A Maximum Entropy Part-
Of-Speech Tagger. In Proceedings of the EMNLP
Conference, pages 133?142, Philadelphia, PA.
English devel. Precision Recall F?=1
LOC 91.75% 93.20% 92.47
MISC 88.34% 82.97% 85.57
ORG 83.54% 85.53% 84.52
PER 94.26% 95.39% 94.82
Overall 90.15% 90.56% 90.35
English test Precision Recall F?=1
LOC 84.97% 90.53% 87.66
MISC 76.77% 75.78% 76.27
ORG 79.60% 79.41% 79.51
PER 91.64% 90.79% 91.21
Overall 84.29% 85.50% 84.89
German devel. Precision Recall F?=1
LOC 67.59% 70.11% 68.83
MISC 71.87% 48.81% 58.14
ORG 71.85% 50.60% 59.39
PER 81.69% 64.03% 71.79
Overall 73.29% 58.89% 65.31
German test Precision Recall F?=1
LOC 70.91% 71.11% 71.01
MISC 68.51% 46.12% 55.13
ORG 68.43% 50.19% 57.91
PER 88.04% 72.05% 79.25
Overall 75.61% 62.46% 68.41
Table 7: Full system results for the English and German
development and test data sets.
Erik F. Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the CoNLL-2003 Shared Task:
Language-Independent Named Entity Recognition. In
Walter Daelemans and Miles Osborne, editors, Pro-
ceedings of CoNLL-2003, Edmonton, Canada.
Erik F. Tjong Kim Sang. 2002. Introduction to the
CoNLL-2002 Shared Task: Language-Independent
Named Entity Recognition. In Proceedings of CoNLL-
2002, Taipei, Taiwan.
Koji Tsukamoto, Yutaka Mitsuishi, and Manabu Sassano.
2002. Learning with Multiple Stacking for Named En-
tity Recognition. In Proceedings of the 2002 CoNLL
Workshop, pages 191?194, Taipei, Taiwan.
GuoDong Zhou and Jian Su. 2002. Named Entity
Recognition using an HMM-based Chunk Tagger. In
Proceedings of the 40th Annual Meeting of the ACL,
pages 473?480, Philadelphia, PA.
Log-Linear Models for Wide-Coverage CCG Parsing
Stephen Clark and James R. Curran
School of Informatics
University of Edinburgh
2 Buccleuch Place, Edinburgh. EH8 9LW
fstephenc,jamescg@cogsci.ed.ac.uk
Abstract
This paper describes log-linear pars-
ing models for Combinatory Categorial
Grammar (CCG). Log-linear models can
easily encode the long-range dependen-
cies inherent in coordination and extrac-
tion phenomena, which CCG was designed
to handle. Log-linear models have pre-
viously been applied to statistical pars-
ing, under the assumption that all possible
parses for a sentence can be enumerated.
Enumerating all parses is infeasible for
large grammars; however, dynamic pro-
gramming over a packed chart can be used
to efficiently estimate the model parame-
ters. We describe a parellelised implemen-
tation which runs on a Beowulf cluster and
allows the complete WSJ Penn Treebank
to be used for estimation.
1 Introduction
Statistical parsing models have recently been de-
veloped for Combinatory Categorial Grammar
(CCG, Steedman (2000)) and used in wide-coverage
parsers applied to the WSJ Penn Treebank (Clark et
al., 2002; Hockenmaier and Steedman, 2002). An
attraction of CCG is its elegant treatment of coor-
dination and extraction, allowing recovery of the
long-range dependencies inherent in these construc-
tions. We would like the parsing model to include
long-range dependencies, but this introduces prob-
lems for generative parsing models similar to those
described by Abney (1997) for attribute-value gram-
mars; hence Hockenmaier and Steedman do not in-
clude such dependencies in their model, and Clark
et al include the dependencies but use an incon-
sistent model. Following Abney, we propose a log-
linear framework which incorporates long-range de-
pendencies as features without loss of consistency.
Log-linear models have previously been ap-
plied to statistical parsing (Johnson et al, 1999;
Toutanova et al, 2002; Riezler et al, 2002; Os-
borne, 2000). Typically, these approaches have enu-
merated all possible parses for model estimation
and finding the most probable parse. For gram-
mars extracted from the Penn Treebank (in our case
CCGbank (Hockenmaier, 2003)), enumerating all
parses is infeasible. One approach to this prob-
lem is to sample the parse space for estimation, e.g.
Osborne (2000). In this paper we use a dynamic pro-
gramming technique applied to a packed chart, simi-
lar to those proposed by Geman and Johnson (2002)
and Miyao and Tsujii (2002), which efficiently esti-
mates the model parameters over the complete space
without enumerating parses. The estimation method
is similar to the inside-outside algorithm used for es-
timating a PCFG (Lari and Young, 1990).
Miyao and Tsujii (2002) apply their estimation
technique to an automatically extracted Tree Adjoin-
ing Grammar using Improved Iterative Scaling (IIS,
Della Pietra et al (1997)). However, their model
has significant memory requirements which limits
them to using 868 sentences as training data. We use
a parallelised version of Generalised Iterative Scal-
ing (GIS, Darroch and Ratcliff (1972)) on a Beowulf
cluster which allows the complete WSJ Penn Tree-
bank to be used as training data.
This paper assumes a basic knowledge of CCG;
see Steedman (2000) and Clark et al (2002) for an
introduction.
2 The Grammar
Following Clark et al (2002), we augment CCG lex-
ical categories with head and dependency informa-
tion. For example, the extended category for per-
suade is as follows:
persuade :=((S[dcl]persuade\NP1)=(S[to]2\NPX))=NPX,3 (1)
The feature [dcl] indicates a declarative sentence; the
resulting S[dcl] is headed by persuade; and the num-
bers indicate dependency relations. The variable X
denotes a head, identifying the head of the infiniti-
val complement?s subject with the head of the ob-
ject, thus capturing the object control relation. For
example, in Microsoft persuades IBM to buy Lotus,
IBM fills the subject slot of buy.
Formally, a dependency is defined as a 5-tuple:
?hf ; f ; s; ha; l?, where hf is the head word of the
functor, f is the functor category (extended with
head and dependency information), s is the argu-
ment slot, and ha is the head word of the argument.
The l is an additional field used to encode whether
the dependency is long-range. For example, the de-
pendency encoding Lotus as the object of bought (as
in IBM bought Lotus) is represented as follows:
?bought; (S[dcl]bought\NP1)=NP2; 2; Lotus; null? (2)
If the object has been extracted using a relative pro-
noun with the category (NP\NP)=(S[dcl]=NP) (as in
the company that IBM bought), the dependency is as
follows:
?bought; (S[dcl]bought\NP1)=NP2; 2; company; ?? (3)
where ? is the category (NP\NP)=(S[dcl]=NP) as-
signed to the relative pronoun. A dependency struc-
ture is simply a set of these dependencies.
Every argument in every lexical category is en-
coded as a dependency. Unlike Clark et al, we do
not require dependencies to be always marked on
atomic categories. For example, the marked up cat-
egory for about (as in about 5,000 pounds) is:
(NX=NX)Y=(N=N)Y,1 (4)
If 5,000 has the category (NX=NX)5,000, the depen-
dency relation marked on the (N=N)Y,1 argument in
(4) allows the dependency between about and 5,000
to be captured.
Clark et al (2002) give examples showing how
heads can fill dependency slots during a derivation,
and how long-range dependencies can be recovered
through unification of co-indexed head variables.
3 Log-Linear Models for CCG
Previous parsing models for CCG include a genera-
tive model over normal-form derivations (Hocken-
maier and Steedman, 2002) and a conditional model
over dependency structures (Clark et al, 2002).
We follow Clark et al in modelling dependency
structures, but, unlike Clark et al, do so in terms
of derivations. An advantage of our approach is
that the model can potentially include derivation-
specific features in addition to dependency informa-
tion. Also, modelling derivations provides a close
link between the model and the parsing algorithm,
which makes it easier to define dynamic program-
ming techniques for efficient model estimation and
decoding1, and also apply beam search to reduce the
search space.
The probability of a dependency structure,  ? ,
given a sentence, S , is defined as follows:
P(|S ) =
?
d?(;S )
P(d; |S ) (5)
where (; S ) is the set of derivations for S which
lead to  and  is the set of dependency structures.
Note that (; S ) includes the non-standard deriva-
tions allowed by CCG. This model allows the pos-
sibility of including features from the non-standard
derivations, such as features encoding the use of
type-raising or function composition.
A log-linear model of a parse, ! ? ?, given a
sentence S , is defined as follows:
P(!|S ) = 1
ZS
?
i

fi(!)
i (6)
This model can be applied to any kind of parse, but
for this paper a parse, !, is a ?d; ? pair (as given
in (5)). The function fi is a feature of the parse
1We use the term decoding to refer to the process of finding
the most probable dependency structure from a packed chart.
which can be any real-valued function over the space
of parses?. In this paper fi(!) is a count of the num-
ber of times some dependency occurs in !. Each
feature fi has an associated weight i which is a pa-
rameter of the model to be estimated. ZS is a normal-
ising constant which ensures that P(!|S ) is a proba-
bility distribution:
ZS =
?
!
?
?Object-Extraction and Question-Parsing using CCG
Stephen Clark and Mark Steedman
School of Informatics
University of Edinburgh
2 Buccleuch Place, Edinburgh, UK
 
stevec,steedman  @inf.ed.ac.uk
James R. Curran
School of Information Technologies
University of Sydney
NSW 2006, Australia
james@it.usyd.edu.au
Abstract
Accurate dependency recovery has recently
been reported for a number of wide-coverage
statistical parsers using Combinatory Categorial
Grammar (CCG). However, overall figures give
no indication of a parser?s performance on spe-
cific constructions, nor how suitable a parser is
for specific applications. In this paper we give
a detailed evaluation of a CCG parser on ob-
ject extraction dependencies found in WSJ text.
We also show how the parser can be used to
parse questions for Question Answering. The
accuracy of the original parser on questions is
very poor, and we propose a novel technique for
porting the parser to a new domain, by creating
new labelled data at the lexical category level
only. Using a supertagger to assign categories
to words, trained on the new data, leads to a dra-
matic increase in question parsing accuracy.
1 Introduction
Several wide-coverage statistical parsers have re-
cently been developed for Combinatory Categorial
Grammar (CCG; Steedman, 2000) and applied to
the WSJ Penn Treebank (Clark et al, 2002; Hock-
enmaier and Steedman, 2002; Hockenmaier, 2003b;
Clark and Curran, 2004b). One motivation for using
CCG is the recovery of the long-range dependencies
inherent in phenomena such as coordination and ex-
traction. Recovery of these dependencies is impor-
tant for NLP tasks which require semantic interpre-
tation and for processing text which contains a high
frequency of such cases, e.g. Wh-questions fed to a
Question Answering (QA) system.
One shortcoming of treebank parsers such as
Collins (1999) and Charniak (2000) is that they typi-
cally produce phrase-structure trees containing only
local syntactic information. Johnson (2002) uses
post-processing methods to insert ?empty? nodes
into the trees, and Dienes and Dubey (2003) use pre-
processing methods to determine where discontinu-
ities are likely to appear in the sentence. In contrast,
the CCG parsers detect long-range dependencies as
an integral part of the parsing process.
The CCG parser used here (Clark and Curran,
2004b) is highly accurate and efficient, recovering
labelled dependencies with an overall F-score of
over 84% on WSJ text, and parsing up to 50 sen-
tences per second. Thus the parser should be useful
for large-scale NLP tasks. However, the overall ac-
curacy figures give no indication of the parser?s per-
formance on specific constructions, nor how suit-
able the parser is for specific applications. In this
paper we give a detailed evaluation for object ex-
traction dependencies and show how the parser can
be used to parse questions for QA.
We find that the parser performs well on the ob-
ject extraction cases found in the Penn Treebank,
given the difficulty of the task. In contrast, the
parser performs poorly on questions from TREC,
due to the small number of questions in the Penn
Treebank. This motivates the remainder of the pa-
per, in which we describe the creation of new train-
ing data consisting of labelled questions. Crucially,
the questions are labelled at the lexical category
level only, and not at the derivation level, making
the creation of new labelled data relatively easy.
The parser uses a supertagger to assign lexical
categories to words, and the supertagger can be
adapted to the new question domain by training on
the newly created data. We find that using the orig-
inal parsing model with the new supertagger model
dramatically increases parsing accuracy on TREC
questions, producing a parser suitable for use in a
QA system. For evaluation we focus on What ques-
tions used in the TREC competitions. As well as
giving an overall evaluation on this test set, we also
consider a number of object extraction cases.
The creation of new training data at the lexical
category level alone is a technique which could be
used to rapidly port the parser to other domains.
This technique may also be applicable to other lex-
icalised grammar formalisms, such as Tree Adjoin-
ing Grammar (Bangalore and Joshi, 1999).1
1Doran et al (1997) propose using a supertagger for semi-
automatically porting the XTAG grammar to a new domain.
2 The Parser
The parser used in this paper is described in Clark
and Curran (2004b). It takes as input a POS tagged
sentence with a set of lexical categories assigned to
each word. The CCG combinatory rules are used to
combine the categories. A packed chart efficiently
represents all of the possible analyses for a sentence,
and the CKY chart parsing algorithm described in
Steedman (2000) is used to build the chart.
A Maximum Entropy CCG supertagger (Clark
and Curran, 2004a) is used to assign the categories.
The lexical category set is obtained from CCGbank
(Hockenmaier, 2003a), a treebank of normal-form
CCG derivations derived from the Penn Treebank.
CCGbank is also used for learning the parameters
of the supertagger and parsing models.
2.1 The Supertagger
The supertagger uses a log-linear model to define
a distribution for each word over the lexical cate-
gory set. Model features are defined by the words
and POS tags in the 5-word window surrounding the
target word. The supertagger selects the most prob-
able categories locally rather than maximising the
sequence probability, assigning all categories whose
probability is within some factor, ?, of the highest
probability category. For a word seen frequently in
the training data, the supertagger can only assign
categories from the word?s entry in the tag dictio-
nary, which lists the categories each word has been
seen with in the data.
In Clark et al?s (2002) parser, a supertagger is
used as follows: first around 4 lexical categories are
assigned to each word, on average; if the chart gets
too big or parsing takes too long, the number of cat-
egories is reduced until the sentence can be parsed.
In this paper we use our more recent approach
(Clark and Curran, 2004a): first a small number of
categories is assigned to each word, e.g. 1.5, and the
parser requests more categories if a spanning analy-
sis cannot be found. This method relies on the gram-
mar being constraining enough to decide whether
the categories provided by the supertagger are likely
to contain the correct sequence. Section 6 shows
that this approach works well for parsing questions.
2.2 Parsing Model
In Clark and Curran (2004b) we investigate several
log-linear parsing models for CCG. In this paper we
use the following conditional model:
p(y|x) = 1
Z(x)e
?
i ?i fi(y) (1)
where y is a normal-form derivation and x is a sen-
tence. (A normal-form derivation is one where com-
position and type-raising are used only when neces-
sary.) There are various features, fi, used by the
model: rule instantiation features which count the
number of times a local tree occurs in a derivation;
features defined by the root category of a deriva-
tion; and features defined by the lexical categories
at the leaves. Each feature type has unlexicalised
and head-lexicalised versions.
The remaining features capture word-word de-
pendencies, which significantly improve accuracy.
The best-performing model encodes word-word de-
pendencies in terms of the local rule instantiations,
as in Hockenmaier and Steedman (2002). We have
also tried predicate-argument dependencies, includ-
ing long-range dependencies, but these have not im-
proved performance. Note we still recover long-
range dependencies, even if modelling them does
not improve performance.
The parser returns a derived structure correspond-
ing to the most probable derivation. For evalua-
tion the parser returns dependency structures, but
we have also developed a module which builds first-
order semantic representations from the derivations,
which can be used for inference (Bos et al, 2004).
3 Object Extraction
Steedman (1996) presents a detailed study of vari-
ous extraction phenomena. Here we focus on object
extraction, since the dependencies in such cases are
unbounded, and CCG has been designed to handle
these cases. Correct dependency recovery for object
extraction is also difficult for shallow methods such
as Johnson (2002) and Dienes and Dubey (2003).
We consider three types of object extraction: ob-
ject relative clauses, free object relatives, and tough-
adjectives (Hockenmaier, 2003a). Examples of the
first two from CCGbank are given in Figures 1
and 2, together with the normal-form derivation.
The caption gives the number of sentences contain-
ing such a case in Sections 2-21 of CCGbank (the
training data) and Section 00 (development data).
The pattern of the two derivations is similar:
the subject of the verb phrase missing an object
is type-raised (T); the type-raised subject com-
poses (B) with the verb-phrase; and the category
for the relative pronoun ((NP\NP)/(S[dcl]/NP) or
NP/(S[dcl]/NP)) applies to the sentence-missing-
its-object (S[dcl]/NP). Clark et al (2002) show
how the dependency between the verb and object
can be captured by co-indexing the heads of the NPs
in the relative pronoun category.
Figure 3 gives the derivation for a tough-
adjective. The dependency between take and That
can be recovered by co-indexing the heads of NPs in
an excellent publication that I enjoy reading
NP/N N/N N (NP\NP)/(S[dcl]/NP) NP (S[dcl]\NP)/(S[ng]\NP) (S[ng]\NP)/NP
> >T >B
N S/(S\NP) (S[dcl]\NP)/NP)
> >B
NP S[dcl]/NP
>
NP\NP
<
NP
Figure 1: Extraction from object relative clause; 431 sentences in Sections 2-21, 20 in Section 00
he believes in what he plays
NP (S[dcl]\NP)/PP PP/NP NP/(S[dcl]/NP) NP (S[dcl]\NP)/NP)
>B >T(S[dcl]\NP)/NP) S/(S\NP)
>B
S[dcl]/NP
>
NP
>
S[dcl]\NP
<
S[dcl]
Figure 2: Free object relative example; 269 sentences in Sections 2-21, 16 sentences in Section 00
That got hard to take
NP (S[dcl]\NP)/(S[adj]\NP) (S[adj]\NP)/((S[to]\NP)/NP) (S[to]\NP)/(S[b]\NP) (S[b]\NP)/NP)
>B(S[to]\NP)/NP)
>
S[adj]\NP
>
S[dcl]\NP
<
S[dcl]
Figure 3: tough-adjective example; 52 sentences in Sections 2-21, 2 sentences in Section 00
the categories for hard and got. These cases are rela-
tively rare, with around 50 occurring in the whole of
the treebank, and only two in the development set;
the parser correctly recovers one of the two object
dependencies for the tough-adjective cases in 00.
For the free object relative cases in Section 00,
the parser recovers 14 of the 17 gold-standard de-
pendencies2 between the relative pronoun and the
head of the relative clause. The precision is 14/15.
For the three gold standard cases that are misanal-
ysed, the category NP/S[dcl] is assigned to the rel-
ative pronoun, rather than NP/(S[dcl]/NP).
For the cases involving object relative clauses the
parser provides a range of errors for which it is use-
ful to give a detailed analysis.
3.1 Analysis of Object Extraction Cases
Figure 4 gives the 20 sentences in Section 00
which contain a relative pronoun with the category
(NP\NP)/(S[dcl]/NP). There are 24 object depen-
dencies in total, since some sentences contain more
than one extraction (11), and some extractions in-
volve more than one head (8, 18, 19). For evalua-
tion, we determined whether the parser correctly re-
2One of the 16 sentences contains two such dependencies.
covered the dependency between the head of the ex-
tracted object and the verb. For example, to get the
two dependencies in sentence 18 correct, the parser
would have to assign the correct lexical category to
had, and return respect and confidence as objects.
The parser correctly recovers 15 of the 24 object
dependencies.3 Overall the parser hypothesises 20
extracted object dependencies, giving a precision of
15/20. Hockenmaier (2003a) reports similar results
for a CCG parser using a generative model: 14/24
recall and 14/21 precision. The results here are a
significant improvement over those in Clark et al
(2002), in which only 10 of the 24 dependencies
were recovered correctly. Below is a detailed anal-
ysis of the mistakes made by the parser.
For Sentence 1 the parser cannot provide any
analysis. This is because the correct category for es-
timated, ((S[pt]\NP)/PP)/NP, is not in the tag dic-
tionary?s entry for estimated. Since estimated oc-
curs around 200 times in the data, the supertagger
only considers categories from the tag dictionary
entry, and thus cannot provide the correct category
as an option.
3Unless stated otherwise the parser uses automatically as-
signed, rather than gold standard, POS tags.
1. Commonwealth Edison now faces an additional court-ordered refund on its summer/winter rate differential collections that the Illinois Appellate
Court has estimated at $140 million.
2. Mrs. Hills said many of the 25 countries that she placed under varying degrees of scrutiny have made genuine progress on this touchy issue.?
3. It?s the petulant complaint of an impudent American whom Sony hosted for a year while he was on a Luce Fellowship in Tokyo ? to the regret of
both parties.?
4. It said the man, whom it did not name, had been found to have the disease after hospital tests.
5. Democratic Lt. Gov. Douglas Wilder opened his gubernatorial battle with Republican Marshall Coleman with an abortion commercial produced
by Frank Greer that analysts of every political persuasion agree was a tour de force.
6. Against a shot of Monticello superimposed on an American flag, an announcer talks about the strong tradition of freedom and individual liberty
that Virginians have nurtured for generations.?
7. Interviews with analysts and business people in the U.S. suggest that Japanese capital may produce the economic cooperation that Southeast
Asian politicians have pursued in fits and starts for decades.
8. Another was Nancy Yeargin, who came to Greenville in 1985, full of the energy and ambitions that reformers wanted to reward.
9. Mostly, she says, she wanted to prevent the damage to self-esteem that her low-ability students would suffer from doing badly on the test.?
10. Mrs. Ward says that when the cheating was discovered, she wanted to avoid the morale-damaging public disclosure that a trial would bring.?
11. In CAT sections where students? knowledge of two-letter consonant sounds is tested, the authors noted that Scoring High concentrated on the
same sounds that the test does ? to the exclusion of other sounds that fifth graders should know.?
12. Interpublic Group said its television programming operations ? which it expanded earlier this year ? agreed to supply more than 4,000 hours of
original programming across Europe in 1990.
13. Interpublic is providing the programming in return for advertising time, which it said will be valued at more than $75 million in 1990 and $150
million in 1991.?
14. Mr. Sherwood speculated that the leeway that Sea Containers has means that Temple would have to substantially increase their bid if they?re
going to top us.?
15. The Japanese companies bankroll many small U.S. companies with promising products or ideas, frequently putting their money behind projects
that commercial banks won?t touch.?
16. In investing on the basis of future transactions, a role often performed by merchant banks, trading companies can cut through the logjam that
small-company owners often face with their local commercial banks.
17. A high-balance customer that banks pine for, she didn?t give much thought to the rates she was receiving, nor to the fees she was paying.?
18. The events of April through June damaged the respect and confidence which most Americans previously had for the leaders of China.?
19. He described the situation as an escrow problem, a timing issue, which he said was rapidly rectified, with no losses to customers.?
20. But Rep. Marge Roukema (R., N.J.) instead praised the House?s acceptance of a new youth training wage, a subminimum that GOP
administrations have sought for many years.
Figure 4: Cases of object extraction from a relative clause in 00; the extracted object, relative pronoun and verb are in
italics; for sentences marked with a
?
the parser correctly recovers all dependencies involved in the object extraction.
For Sentence 2 the correct category is assigned
to the relative pronoun that, but a wrong attachment
results in many as the object of placed rather than
countries.
In Sentence 5 the incorrect lexical category
((S\NP)\(S\NP))/S[dcl] is assigned to the relative
pronoun that. In fact, the correct category is pro-
vided as an option by the supertagger, but the parser
is unable to select it. This is because the category
for agree is incorrect, since again the correct cat-
egory, ((S[dcl]\NP)/NP)/(S[dcl]\NP), is not in the
verb?s entry in the tag dictionary.
In Sentence 6 the correct category is assigned to
the relative pronoun, but a number of mistakes else-
where result in the wrong noun attachment.
In Sentences 8 and 9 the complementizer cate-
gory S[em]/S[dcl] is incorrectly assigned to the rel-
ative pronoun that. For Sentence 8 the correct anal-
ysis is available but the parsing model chose in-
correctly. For Sentence 9 the correct analysis is
unavailable because the correct category for suffer,
((S[b]\NP)/PP)/NP, is not in the verb?s entry in the
tag dictionary.
In Sentence 13 the correct category is again as-
signed to the relative pronoun, but a wrong attach-
ment results in return being the object of placed,
rather than time.
In Sentence 17 the wrong category S[em]/S[b] is
assigned to the relative pronoun that. Again the
problem is with the category for the verb, but for
a different reason: the POS tagger incorrectly tags
pine as a base form (VB), rather than VBP, which
completely misleads the supertagger.
This small study only provides anecdotal evi-
dence for the reasons the parser is unable to recover
some long-range object dependencies. However, the
analysis suggests that the parser fails largely for
the same reasons it fails on other WSJ sentences:
wrong attachment decisions are being made; the
lexical coverage of the supertagger is lacking for
some verbs; the model is sometimes biased towards
incorrect lexical categories; and the supertagger is
occasionally led astray by incorrect POS tags.
Note that the recovery of these dependencies is a
difficult problem, since the parser must assign the
correct categories to the relative pronoun and verb,
and make two attachment decisions: one attaching
the relative pronoun to the verb, and one attaching
it to the noun phrase. The recall figures for the in-
dividual dependencies in the relative pronoun cate-
gory are 16/21 for the verb attachment and 15/24 for
the noun attachment.
In conclusion, the kinds of errors made by the
parser suggest that general improvements in the
coverage of the lexicon and parsing models based
on CCGbank will lead to better recovery of long-
range object dependencies.
4 Parsing Questions
Wide-coverage parsers are now being successfully
used as part of open-domain QA systems, e.g. Pasca
and Harabagiu (2001). The speed and accuracy of
our CCG parser suggests that it could be used to
parse answer candidates, and we are currently in-
tegrating the parser into a QA system. We would
also like to apply the parser to the questions, for
two reasons: the use of CCG allows the parser to
deal with extraction cases, which occur relatively
frequently in questions; and the comparison of po-
tential answers with the question, performed by the
answer extraction component, is simplified if the
same parser is used for both.
Initially we tried some experiments applying the
parser to questions from previous TREC competi-
tions. The results were extremely poor, largely be-
cause the questions contain constructions which ap-
pear very infrequently, if at all, in CCGbank.4 For
example, there are no What questions with the gen-
eral form of What President became Chief Justice
after his precidency? in CCGbank, but this is a very
common form of Wh-question. (There is a very
small number (3) of similar question types begin-
ning How or Which in Sections 2?21.)
One solution is to create new annotated question
data and retrain the parser, perhaps combining the
data with CCGbank. However, the creation of gold-
standard derivation trees is very expensive.
A novel alternative, which we pursue here, is to
annotate questions at the lexical category level only.
Annotating sentences with lexical categories is sim-
pler than annotating with derivations, and can be
done with the tools and resources we have avail-
able. The key question is whether training only the
supertagger on new question data is enough to give
high parsing accuracy; in Section 6 we show that it
is. The next Section describes the creation of the
question corpus.
5 A What-Question Corpus
We have created a corpus consisting of 1,171 ques-
tions beginning with the word What, taken from the
TREC 9?12 competitions (2000?2003). We chose to
focus on What-questions because these are a com-
4An earlier version of our QA system used RASP (Briscoe
and Carroll, 2002) to parse the questions, but this parser also
performed extremely poorly on some question types.
1. What are Cushman and Wakefield known for?
2. What are pomegranates?
3. What is hybridization?
4. What is Martin Luther King Jr.?s real birthday?
5. What is one of the cities that the University of Minnesota is located in?
6. What do penguins eat?
7. What amount of folic acid should an expectant mother take daily?
8. What city did the Flintstones live in?
9. What instrument is Ray Charles best known for playing?
10. What state does Martha Stewart live in?
11. What kind of a sports team is the Wisconsin Badgers?
12. What English word contains the most letters?
13. What king signed the Magna Carta?
14. What caused the Lynmouth floods?
Figure 5: Examples from the What-question corpus
CATEGORY FOR What FREQ %
S[wq]/(S[q]/NP) 728 62.2
(S[wq]/(S[q]/NP))/N 221 18.9
(S[wq]/(S[dcl]\NP))/N 207 17.7
S[wq]/(S[dcl]\NP) 15 1.3
Table 1: Distribution of What categories in questions
mon form of question, and many contain cases of
extraction, including some unbounded object ex-
traction. A sample of questions from the corpus is
given in Figure 5.
The questions were tokenised according to the
Penn Treebank convention and automatically POS
tagged. Some of the obvious errors made by the
tagger were manually corrected. The first author
then manually labelled 500 questions with lexi-
cal categories. The supertagger was trained on
the annotated questions, and used to label the re-
maining questions, which were then manually cor-
rected. The performance of the supertagger was
good enough at this stage to significantly reduce the
effort required for annotation. The second author
has verified a subset of the annotated sentences. The
question corpus took less than a week to create.
Figure 6 gives the derivations for some exam-
ple questions. The lexical categories, which make
up the annotation in the question corpus, are in
bold. Note the first example contains an un-
bounded object extraction, indicated by the ques-
tion clause missing an object (S[q]/NP) which is
an argument of What. Table 1 gives the distribu-
tion of categories assigned to the first word What
in each question in the corpus. The first row gives
the category of object question What. The sec-
ond row is the object question determiner. The
third row is the subject question determiner. And
What Cruise Line does Kathie Gifford advertise for ?
(S[wq]/(S[q]/NP))/N N/N N (S[q]/(S[b]\NP))/NP N/N N (S[b]\NP)/PP PP/NP .
> > >B
N N (S[b]\NP)/NP)
>
S[wq]/(S[q]/NP) NP
>
S[q]/(S[b]\NP)
>B
S[q]/NP
>
S[wq]
S[wq]
What English word contains the most letters ?
(S[wq]/(S[dcl]\NP))/N N/N N (S[dcl]\NP)/NP NP/N N/N N .
> >
N N
> >
S[wq]/(S[dcl]\NP) NP
>
S[dcl]\NP
>S[wq]
S[wq]
Figure 6: Derivations for example What-questions; lexical categories are in bold
the final row is the root subject question What.
For the examples in Figure 5, S[wq]/(S[q]/NP)
appears in questions 1?6, (S[wq]/(S[q]/NP))/N
in 7?11, (S[wq]/(S[dcl]\NP))/N in 12?13, and
S[wq]/(S[dcl]\NP) in 14.
6 Evaluation
A development set was created by randomly select-
ing 171 questions. For development purposes the
remaining 1,000 questions were used for training;
these were also used as a final cross-validation train-
ing/test set. The average length of the tokenised
questions in the whole corpus is 8.6 tokens.
The lexical category set used by the parser con-
tains all categories which occur at least 10 times
in CCGbank, giving a set of 409 categories. In
creating the question corpus we used a small num-
ber of new category types, of which 3 were needed
to cover common question constructions. One of
these, (S[wq]/(S[dcl]\NP))/N , applies to What, as
in the second example in Figure 6. This category
does appear in CCGbank, but so infrequently that
it is not part of the parser?s lexical category set.
Two more apply to question words like did and is;
for example, (S[q]/(S[pss]\NP))/NP applies to is
in What instrument is Ray Charles best known for
playing?, and (S[q]/PP)/NP applies to is in What
city in Florida is Sea World in?.
6.1 Supertagger Accuracy
As an initial evaluation we tested the accuracy of
just the supertagger on the development data. The
supertagger was run in two modes: one in which
a single category was assigned to each word, and
one in which 1.5 categories were assigned to each
1 CAT 1.5 CATS
ACCURACY: WORD SENT WORD SENT
MODEL
CCGbank 72.0 2 84.8 11
Qs 92.3 67 96.6 81
Qs+CCGbank 93.1 61 98.1 87
10Qs+CCGbank 93.6 67 97.9 83
Table 2: Accuracy of supertagger on dev question data
word, on average. Table 2 gives the per-word accu-
racy on the development question data for a num-
ber of supertagging models; SENT accuracy gives
the percentage of sentences for which every word
is assigned the correct category. Four supertagging
models were used: one trained on CCGbank only;
one trained on the 1,000 questions; one trained on
the 1,000 questions plus CCGbank; and one trained
on 10 copies of the 1,000 questions plus CCGbank.
The supertagger performs well when trained on
the question data, and benefits from a combination
of the questions and CCGbank. To increase the in-
fluence of the questions, we tried adding 10 copies
of the question data to CCGbank, but this had little
impact on accuracy. However, the supertagger per-
forms extremely poorly when trained only on CCG-
bank. One reason for the very low SENT accuracy
figure is that many of the questions contain lexical
categories which are not in the supertagger?s cate-
gory set derived from CCGbank: 56 of the 171 de-
velopment questions have this property.
The parsing results in Clark and Curran (2004b)
rely on a supertagger per-word accuracy of at least
97%, and a sentence accuracy of at least 60% (for
1.5 categories per word). Thus the sentence accu-
SUPERTAGGING / ACCURACY
PARSING METHOD WORD SENT WHAT
Increasing av. cats 94.6 82 91
Decreasing av. cats 89.7 65 80
Increasing cats (rand) 93.4 79 88
Decreasing cats (rand) 64.0 9 21
Baseline 68.5 0 61
Table 3: Parser category accuracy on dev data
racy of 11% confirms that our parsing system based
only on CCGbank is quite inadequate for accurate
question parsing.
6.2 Parser Accuracy
Since the gold-standard question data is only la-
belled at the lexical category level, we are only able
to perform a full evaluation at that level. However,
the scores in Clark and Curran (2004b) give an in-
dication of how supertagging accuracy corresponds
to overall dependency recovery. In addition, in Sec-
tion 6.3 we present an evaluation on object extrac-
tion dependencies in the development data.
We applied the parser to the 171 questions in
the development data, using the supertagger model
from the third row in Table 2, together with a log-
linear parsing model trained on CCGbank. We
used the supertagging approach described in Sec-
tion 2.1, in which a small number of categories is
initially assigned to each word, and the parser re-
quests more categories if a spanning analysis can-
not be found. We used 4 different values for the
parameter ? (which determines the average number
of categories per word): 0.5, 0.25, 0.075 and 0.01.
The average number of categories at each level for
the development data is 1.1, 1.2, 1.6 and 3.8. The
parser provided an analysis for all but one of the
171 questions.
The first row of Table 3 gives the per-word, and
sentence, category accuracy for the parser output.
Figures are also given for the accuracy of the cate-
gories assigned to the first word What. The figures
show that the parser is more accurate at supertag-
ging than the single-category supertagger.
The second row gives the results if the original
supertagging approach of Clark et al (2002) is
used, i.e. starting with a high number of categories
per word, and reducing the number if the sentence
cannot be parsed within reasonable space and time
constraints. The third row corresponds to our new
supertagging approach, but chooses a derivation at
random, by randomly traversing the packed chart
representation used by the parser. The fourth row
corresponds to the supertagging approach of Clark
et al (2002), together with a random selection of
SUPERTAGGING / ACCURACY
PARSING METHOD WORD SENT WHAT
Increasing av. cats 94.4 79 92
Decreasing av. cats 89.5 64 81
Table 4: Cross-validation results
the derivation. The baseline method in the fifth row
assigns to a word the category most frequently seen
with it in the data; for unseen words N is assigned.
The results in Table 3 demonstrate that our new
supertagging approach is very effective. The reason
is that the parser typically uses the first supertag-
ger level, where the average number of categories
per word is only 1.1, and the per-word/sentence cat-
egory accuracies are 95.5 and 70.8%, repsectively.
136 of the 171 questions (79.5%) are parsed at this
level. Since the number of categories per word is
very small, the parser has little work to do in com-
bining the categories; the supertagger is effectively
an almost-parser (Bangalore and Joshi, 1999). Thus
the parsing model, which is not tuned for questions,
is hardly used by the parser. This interpretation is
supported by the high scores for the random method
in row 3 of the table.
In contrast, the previous supertagging method of
Clark et al (2002) results in a large derivation
space, which must be searched using the parsing
model. Thus the accuracy of the parser is greatly
reduced, as shown in rows 2 and 4.
As a final test of the robustness of our results,
we performed a cross-validation experiment using
the 1,000 training questions. The 1,000 questions
were randomly split into 10 chunks. Each chunk
was used as a test set in a separate run, with the
remaining chunks as training data plus CCGbank.
Table 4 gives the results averaged over the 10 runs
for the two supertagging approaches.
6.3 Object Extraction in Questions
For the object extraction evaluation we considered
the 36 questions in the development data which
have the category (S[wq]/(S[q]/NP))/N assigned to
What. Table 7 gives examples of the questions. We
assume these are fairly representative of the kinds
of object extraction found in other question types,
and thus present a useful test set.
We parsed the questions using the best perform-
ing configuration from the previous section. All but
one of the sentences was given an analysis. The per-
word/sentence category accuracies were 90.2% and
71.4%, respectively. These figures are lower than
for the corpus as a whole, suggesting these object
extraction questions are more difficult than average.
What amount of folic acid should an expectant mother take daily?
What movie did Madilyn Kahn star in with Gene Wilder?
What continent is Egypt on?
What year was Ebbets Field, home of Brooklyn Dodgers, built?
What body of water does the Colorado River empty into?
Figure 7: Examples of object extraction questions
We inspected the output to see if the object de-
pendencies had been recovered correctly. To get the
object dependency correct in the first question in Ta-
ble 7, for example, the parser would need to assign
the correct category to take and return amount as the
object of take. Of the 37 extracted object dependen-
cies (one question had two such dependencies), 29
(78.4%) were recovered correctly. Given that the
original parser trained on CCGbank performs ex-
tremely poorly on such questions, we consider this
to be a highly promising result.
7 Conclusion
We have presented a detailed evaluation of a CCG
parser on object extraction dependencies in WSJ
text. Given the difficulty of the task, the accuracy
of the parser is encouraging. The errors made by
the parser suggest that general improvements in the
coverage of the lexicon and parsing models derived
from CCGbank will lead to improved recovery of
long-range object dependencies.
In contrast, we have suggested that general im-
provements in CCGbank parsing models will not
lead to satisfactory performance on question pars-
ing. The reason is that the Wh-question domain
is syntactically distinct from WSJ text. We have
presented a novel method for porting the parser to
the question domain, which has led to good perfor-
mance on question parsing. This has also demon-
strated the close integration of the supertagger and
the CCG parser on which our method depends.
One of the major drawbacks of current NLP tech-
nology is that in general it performs very poorly
outside of the training data domain. Our porting
method only requires lexical category data, which is
far easier to produce than full parse trees. This is an
efficient method for porting the parser to other do-
mains. The method may also be applicable to other
lexicalised grammar formalisms.
We will extend the question corpus to other ques-
tion types. We are also continuing to develop the
supertagger, which we have demonstrated is central
to efficient portable wide-coverage CCG parsing.
References
Srinivas Bangalore and Aravind Joshi. 1999. Supertag-
ging: An approach to almost parsing. Computational
Linguistics, 25(2):237?265.
Johan Bos, Stephen Clark, Mark Steedman, James R.
Curran, and Julia Hockenmaier. 2004. Wide-
coverage semantic representations from a CCG parser.
In Proceedings of COLING-04, Geneva, Switzerland.
Ted Briscoe and John Carroll. 2002. Robust accurate
statistical annotation of general text. In Proceedings
of the 3rd LREC Conference, pages 1499?1504, Las
Palmas, Gran Canaria.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the 1st Meeting of the
NAACL, pages 132?139, Seattle, WA.
Stephen Clark and James R. Curran. 2004a. The impor-
tance of supertagging for wide-coverage CCG pars-
ing. In Proceedings of COLING-04, Geneva, Switzer-
land.
Stephen Clark and James R. Curran. 2004b. Parsing
the WSJ using CCG and log-linear models. In Pro-
ceedings of the 42nd Meeting of the ACL, Barcelona,
Spain.
Stephen Clark, Julia Hockenmaier, and Mark Steedman.
2002. Building deep dependency structures with a
wide-coverage CCG parser. In Proceedings of the
40th Meeting of the ACL, pages 327?334, Philadel-
phia, PA.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
Peter Dienes and Amit Dubey. 2003. Deep syntactic
processing by combining shallow methods. In Pro-
ceedings of the EMNLP Conference, pages 431?438,
Sapporo, Japan.
C. Doran, B. Hockey, P. Hopely, J. Rosenzweig,
A. Sarkar, B. Srinivas, F. Xia, A. Nasr, and O. Ram-
bow. 1997. Maintaining the forest and burning out
the underbrush in XTAG. In Proceedings of the EN-
VGRAM Workshop, Madrid, Spain.
Julia Hockenmaier and Mark Steedman. 2002. Gen-
erative models for statistical parsing with Combina-
tory Categorial Grammar. In Proceedings of the 40th
Meeting of the ACL, pages 335?342, Philadelphia, PA.
Julia Hockenmaier. 2003a. Data and Models for Statis-
tical Parsing with Combinatory Categorial Grammar.
Ph.D. thesis, University of Edinburgh.
Julia Hockenmaier. 2003b. Parsing with generative
models of predicate-argument structure. In Proceed-
ings of the 41st Meeting of the ACL, pages 359?366,
Sapporo, Japan.
Mark Johnson. 2002. A simple pattern-matching al-
gorithm for recovering empty nodes and their an-
tecedents. In Proceedings of the 40th Meeting of the
ACL, pages 136?143, Philadelphia, PA.
Marius Pasca and Sanda Harabagiu. 2001. High per-
formance question/answering. In Proceedings of the
ACL SIGIR Conference on Research and Development
in Information Retrieval, pages 366?374, New Or-
leans LA.
Mark Steedman. 1996. Surface Structure and Interpre-
tation. The MIT Press, Cambridge, MA.
Mark Steedman. 2000. The Syntactic Process. The MIT
Press, Cambridge, MA.
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 475?484,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Adapting a Lexicalized-Grammar Parser to Contrasting Domains
Laura Rimell and Stephen Clark
Oxford University Computing Laboratory
Wolfson Building, Parks Road
Oxford, OX1 3QD, UK
{laura.rimell,stephen.clark}@comlab.ox.ac.uk
Abstract
Most state-of-the-art wide-coverage parsers
are trained on newspaper text and suffer a
loss of accuracy in other domains, making
parser adaptation a pressing issue. In this
paper we demonstrate that a CCG parser can
be adapted to two new domains, biomedical
text and questions for a QA system, by us-
ing manually-annotated training data at the
POS and lexical category levels only. This ap-
proach achieves parser accuracy comparable
to that on newspaper data without the need
for annotated parse trees in the new domain.
We find that retraining at the lexical category
level yields a larger performance increase for
questions than for biomedical text and analyze
the two datasets to investigate why different
domains might behave differently for parser
adaptation.
1 Introduction
Most state-of-the-art wide-coverage parsers are
based on the Penn Treebank (Marcus et al, 1993),
making such parsers highly tuned to newspaper text.
A pressing question facing the parsing community
is how to adapt these parsers to other domains, such
as biomedical research papers and web pages. A re-
lated question is how to improve the performance
of these parsers on constructions that are rare in the
Penn Treebank, such as questions. Questions are
particularly important since a question parser is a
component in most Question Answering (QA) sys-
tems (Harabagiu et al, 2001).
In this paper we investigate parser adaptation in
the context of lexicalized grammars, by using a
parser based on Combinatory Categorial Grammar
(CCG) (Steedman, 2000). A key property of CCG is
that it is lexicalized, meaning that each word in a
sentence is associated with an elementary syntactic
structure. In the case of CCG this is a lexical cate-
gory expressing subcategorization information. We
exploit this property of CCG by performing manual
annotation in the new domain, but only up to this
level of representation, where the annotation can be
carried out relatively quickly. Since CCG lexical cat-
egories are so expressive, many of the syntactic char-
acteristics of a domain are captured at this level.
The two domains we consider are the biomedical
domain and questions for a QA system. We use the
term ?domain? somewhat loosely here, since ques-
tions are best described as a particular set of syn-
tactic constructions, rather than a set of documents
about a particular topic. However, we consider ques-
tion data to be interesting in the context of domain
adaptation for the following reasons: 1) there are
few examples in the Penn Treebank (PTB) and so
PTB parsers typically perform poorly on them; 2)
questions form a fairly homogeneous set with re-
spect to the syntactic constructions employed, and
it is an interesting question how easy it is to adapt a
parser to such data; and 3) QA is becoming an impor-
tant example of NLP technology, and question pars-
ing is an important task for QA systems.
The CCG parser we use (Clark and Curran, 2007b)
makes use of three levels of representation: one, a
POS tag level based on the fairly coarse-grained POS
tags in the Penn Treebank; two, a lexical category
level based on the more fine-grained CCG lexical cat-
egories, which are assigned to words by a CCG su-
475
pertagger; and three, a hierarchical level consisting
of CCG derivations. A key idea in this paper, follow-
ing a pilot study in Clark et al (2004), is to perform
manual annotation only at the first two levels. Since
the lexical category level consists of sequences of
tags, rather than hierarchical derivations, the anno-
tation can be performed relatively quickly.
For the biomedical and question domains we
manually annotated approximately 1,000 and 2,000
sentences, respectively, with CCG lexical categories.
We also created a gold standard set of grammati-
cal relations (GR) in the Stanford format (de Marn-
effe et al, 2006), using 500 of the questions. For
the biomedical domain we used the BioInfer corpus
(Pyysalo et al, 2007a), an existing gold-standard GR
resource also in the Stanford format. We evaluated
the parser on both lexical category assignment and
recovery of GRs.
The results show that the domain adaptation ap-
proach used here is successful in two very different
domains, achieving parsing accuracy comparable to
state-of-the-art accuracy for newspaper text. The re-
sults also show, however, that the two domains have
different profiles with regard to the levels of repre-
sentation used by the parser. We find that simply re-
training the POS tagger used by the parser leads to a
large improvement in performance for the biomed-
ical domain, and that retraining the CCG supertag-
ger on the annotated biomedical data improves the
performance further. For the question data, retrain-
ing just the POS tagger also improves parser perfor-
mance, but retraining the supertagger has a much
greater effect. We perform some analysis of the two
datasets in order to explain the different behaviours
with regard to porting the CCG parser.
2 The CCG Parser
The CCG parser is described in detail in Clark and
Curran (2007b) and so we provide only a brief de-
scription. The stages in the CCG parsing pipeline are
as follows. First, a maximum entropy POS tagger
assigns a single POS tag to each word in a sentence.
POS tags are fairly coarse-grained grammatical la-
bels indicating part-of-speech; the Penn Treebank
set, used here, contains approximately 50 labels.
Second, a maximum entropy supertagger assigns
CCG lexical categories to the words in the sentence.
Lexical categories can be thought of as fine-grained
POS tags expressing subcategorization information,
i.e. information about the argument frame of the
word. There are 425 categories in the set used by the
CCG parser. Supertagging was originally developed
for Lexicalized Tree Adjoining Grammar (Banga-
lore and Joshi, 1999), but has been particularly suc-
cessful for wide-coverage CCG parsing (Clark and
Curran, 2007b). Rather than assign a single category
to each word, the supertagger operates as a multi-
tagger, sometimes assigning more than one category
if the context is not sufficiently discriminating to
suggest a single tag (Curran et al, 2006). Since
the taggers have linear time complexity, the first two
stages can be performed extremely quickly.
Finally, the parsing stage combines the lexical cat-
egories, using a small set of combinatory rules that
are part of the grammar of CCG, and builds a packed
chart representation containing all the derivations
which can be built from the lexical categories. The
Viterbi algorithm efficiently finds the highest scor-
ing derivation from the packed chart, using a log-
linear model to score the derivations. The grammar
and training data for the newspaper version of the
CCG parser are obtained from CCGbank (Hocken-
maier and Steedman, 2007), a CCG version of the
Penn Treebank.
The aspect of the pipeline which is most relevant
to this paper is the supertagging phase. Figure 1
gives an example sentence from each target domain,
with the CCG lexical category assigned to each word
shown below the word, and the POS tag to the right.
Note that the categories contain a significant amount
of grammatical information, in particular subcatego-
rization information. The verb acts in the biomedi-
cal sentence, for example, looks for a prepositional
phrase (PP, as a linkage protein) to its right and a
noun phrase (NP, Talin) to its left, with the resulting
category a declarative sentence (S[dcl]).
Bangalore and Joshi (1999) refer to supertagging
as almost parsing, because once the correct lexical
categories have been assigned, the parser is left with
much less work to do. The CCG supertagger is not
able to assign a single category to each word with
extremely high accuracy ? hence the need for it to
operate as a multi-tagger ? but even in multi-tagger
mode it dramatically reduces the ambiguity passed
through to the parser (Clark and Curran, 2007b).
476
Talin|NN perhaps|RB acts|VBZ as|IN a|DT linkage|NN protein|NN .|.
NP (S\NP)/(S\NP) (S [dcl ]\NP)/PP PP/NP NP [nb]/N N /N N .
What|WDT king|NN signed|VBD the|DT Magna|NNP Carta|NNP ?|.
(S [wq ]/(S [dcl ]\NP))/N N (S [dcl ]\NP)/NP NP [nb]/N N /N N .
Figure 1: Example sentences with lexical category assignment.
The parser has been evaluated on DepBank (King
et al, 2003), using the GR scheme of Briscoe et
al. (2006), and it scores 82.4% labelled precision
and 81.2% labelled recall overall (Clark and Curran,
2007a). Section 4.4 describes how the CCG depen-
dencies can be mapped into the Stanford GR scheme
(de Marneffe et al, 2006) and gives the results of
evaluating the parser on biomedical and question GR
resources.
The CCG parser is particularly well suited to the
biomedical and question domains. First, use of CCG
allows recovery of long-distance dependencies. In
the sentence What does target heart rate mean?, the
word What is an underlying object of the verb mean.
The parser recovers this information despite the dis-
tance between the two words. This capability is
crucial for question parsing, and also useful in the
biomedical field for extraction of relationships be-
tween biological entities. Additionally, the speed of
the parser (tens of sentences per second) is useful
for the large volumes of biomedical data that require
processing for biomedical text mining.
3 Approach
Our approach to domain adaptation is to target the
coarser-grained, less syntactically complex, levels of
representation used by the parser, and to train new
models with manually annotated data at these levels.
The motivation for this approach is twofold. First,
accuracy at each stage of the pipeline depends on ac-
curacy at the earlier stages. If the POS tagger assigns
incorrect tags, it is unlikely that the supertagger will
be able to recover and produce the correct lexical
categories, since it relies heavily on POS tags as fea-
tures. Without the correct categories, the parser in
turn will be unable to find a correct parse.
In the sentence What year did the Vietnam War
end?, the newspaper-trained POS tagger incorrectly
assigns the POS tag NN (common noun) to the verb
end, since verb-final sentences are atypical for the
PTB. As a result, the supertagger is virtually cer-
tain (greater than 99% probability) that the correct
CCG lexical category for end is N (noun). The parser
then assigns the Vietnam War end the structure of a
noun phrase, and chooses an unusual subcategoriza-
tion frame for did in which it takes three arguments:
What, year, and the Vietnam War end.
In the sentence How many siblings does
she have?, on the other hand, the supertag-
ger assigns an incorrect category to the word
How despite it having the correct POS tag
(WRB for wh-adverb). The correct category is
((S [wq ]/(S [q ]/NP))/N )/(NP/N ), which takes
many (category NP/N ) and siblings (category N )
as arguments. Instead it is tagged as S [wq ]/S [q ],
the category for a sentential adverb (i.e. the man-
ner reading of how), which prevents a correct parse.
Our intention was that creating new training data at
the lower levels of representation would improve the
accuracy of the POS tagger and supertagger in the
target domains, thereby improving the accuracy of
later stages in the pipeline as well.
The second motivation for our approach is to re-
duce annotation overhead. Full syntactic deriva-
tions are costly to produce by hand. POS tags, how-
ever, are relatively easy to annotate; even an out-
of-domain tagger will provide a good starting point,
and manual correction is quick, especially in a do-
main without much unfamiliar vocabulary. CCG lex-
ical categories require more expertise, but our ex-
perience shows that an out-of-domain supertagger
can again provide a starting point for correction, and
since the annotation is flat rather than hierarchical,
we hypothesize that it is not as difficult or time-
consuming as annotation of full derivations.
Our adaptation approach has been partially ex-
plored in previous work which targets one or another
of the different levels of representation.
477
Lease and Charniak (2005) obtained an improve-
ment in the accuracy of the Charniak (2000) parser,
as well as POS tagging accuracy, when applied to
the biomedical domain, by training a new POS tag-
ger model with a combination of newspaper and
biomedical data. The parser improvement was due
solely to the new POS tagger, without retraining the
parser model. Since the Charniak parser does not
use a lexicalized grammar with an intermediate level
of representation, any further improvements would
have to come from the parser model itself.
Clark et al (2004) obtained an improvement in
CCG supertagging accuracy for What-questions by
training a new supertagger model with a combina-
tion of newspaper and question data annotated with
CCG lexical categories. Because a question resource
annotated with GRs was not available, they did not
perform a parser evaluation, and the effects of the
POS tagging level were not compared to the lexi-
cal category level. In this paper, we extend the pi-
lot experiments performed by Clark et al (2004) in
four ways. First, we use a larger corpus of TREC
questions covering additional question types, thus
extending the experiments to the question domain
more broadly, as well as to the biomedical domain.
Second, we create a gold standard GR resource en-
abling a full parser evaluation on question data.
Third, we show that the POS level is important for
adaptation, reinforcing the work of Lease and Char-
niak (2005). A key finding of the present paper is
that the combination of retraining at the POS tag and
lexical category levels provides additional improve-
ments beyond those gained by retraining at a single
level. Finally, we provide analysis comparing the
adaptation methodology for question and biomedi-
cal data.
Hara et al (2007) followed a similar approach to
Clark et al (2004), using the parser of Ninomiya
et al (2006), a version of the Enju parser (Miyao
and Tsujii, 2005). Enju is based on HPSG, a lex-
icalized grammar formalism. They obtained an im-
provement in parsing accuracy in the biomedical do-
main by training a new probabilistic model of lexi-
cal entry assignments on a combination of newspa-
per and biomedical data without changing the orig-
inal newspaper-trained parsing model. Hara et al
(2007) did not consider the role of POS tagging. The
lexical category data in Hara et al (2007) was de-
rived from a gold standard treebank, while the an-
notation of lexical categories in this paper was per-
formed without reference to gold standard syntactic
derivations.
Judge et al (2006) produced a corpus of 4,000
questions annotated with syntactic trees, and ob-
tained an improvement in parsing accuracy for
Bikel?s reimplementation of the Collins parser
(Collins, 1997) by training a new parser model with
a combination of newspaper and question data. Our
approach differs in retraining only at the levels of
representation below parse trees.
4 Experiments and Results
4.1 Resources
We have used a combination of existing resources
and new, manually annotated data. The baseline POS
tagger, supertagger, and parser are trained on WSJ
Sections 02-21 of CCGbank. The baseline perfor-
mance at each level of representation is on WSJ Sec-
tion 00 of CCGbank, which contains 1913 sentences
and approximately 45,000 words.
For the biomedical domain, we trained the POS
tagger on gold-standard POS tags from GENIA (Kim
et al, 2003), a corpus of 2,000 MEDLINE abstracts
containing a total of approximately 18,500 sentences
and 440,000 words. We also annotated the first
1,000 sentences of GENIA with CCG lexical cate-
gories. This set of 1,000 sentences, containing ap-
proximately 27,000 words, was used for POS tagger
evaluation and for development and evaluation of a
new supertagger model. For parser evaluation, we
used BioInfer (Pyysalo et al, 2007a), a corpus of
MEDLINE abstracts (on a different topic from those
in GENIA) containing 1,100 sentences, and with syn-
tactic dependencies encoded as grammatical rela-
tions in the Stanford GR format. We used the same
evaluation set of 500 sentences as in Pyysalo et al
(2007b), and the remaining 600 for development of
the mapping to Stanford format. Two parsers have
already been evaluated on BioInfer, which makes it
a useful resource for comparative evaluation.
For the question domain, we extended the dataset
described in Clark et al (2004). That dataset con-
tained 1,171 questions beginning with the word
What, from the TREC 9-12 competitions (2000-
2003), manually POS tagged and annotated with
478
CCG lexical categories. We annotated all the addi-
tional TREC question types and improved the exist-
ing annotation, for a total of 1,828 sentences. We ad-
ditionally annotated a random subset of 500 of these
with GRs in the Stanford format. This subset served
as our evaluation set at all levels of representation. It
contains approximately 4,000 words, fewer than the
other domains because of the significantly shorter
sentence lengths of typical questions. The remain-
ing 1,328 sentences were used as training data. A
set of about a dozen sentences from the evaluation
and training sets were used to develop the mapping
to Stanford format for lexical categories not occur-
ring in the biomedical data.
4.2 POS tagger
We began by training new models at the POS tag
level of representation. All datasets use the PTB
tagset. As a baseline, we used the original WSJ 02-
21 model on the biomedical and question datasets.
For comparison we also evaluated on Section 00 us-
ing the WSJ-trained model.
For the question data, the new POS tagger was
trained on CCGbank Sections 02-21 plus ten copies
of the 1,328 training sentences. The WSJ data pro-
vides additional robustness and wide grammatical
coverage, and the weighting factor of ten was chosen
in preliminary experiments to prevent the newspaper
data from ?overwhelming? the question data. For
the biomedical data, the new POS tagger was trained
on the full GENIA corpus, minus the first 1,000 sen-
tences. GENIA is large enough that combination with
the newspaper data was not needed.
Table 1 gives the results. For both of the new do-
mains the performance of the WSJ model decreased
compared to Section 00, but the retrained model per-
formed at least as well as the WSJ model did on 00.1
Improving the POS tagger performance has a posi-
tive effect on the performance of the supertagger and
parser, which will be discussed in Sections 4.3-4.4.
1Since GENIA does not use the proper noun tag, NNP, for
names of genes and other biomedical entities, all figures in
this paper collapse the NNP-NN distinction where relevant for
biomedical data. The question data uses NNP and the distinc-
tion is not collapsed.
WSJ 02-21 Retrained
Sec. 00 96.7 ?
Qus 92.2 97.1
Bio 93.4 98.7
Table 1: POS tagger accuracy (%) for original and re-
trained models.
Orig
pipeline
Retrained
POS
Retrained
POS and
super
Sec. 00 91.5 ? ?
Qus 71.6 74.0 92.1
Bio 89.0 91.2 93.0
Table 2: Supertagging accuracy (%) and the effect of re-
training the POS model and the supertagger model.
4.3 Supertagger
We next trained new models at the CCG lexical cat-
egory level. The training data consisted of manu-
ally annotated biomedical and question sentences;
specifically, lexical categories were automatically
assigned by the original parsing pipeline and then
manually corrected. Whenever possible we used
categories from the parser?s original set of 425, al-
though occasionally it was necessary to use a new
category for a syntactic construction not occurring
in CCGbank Sections 02-21. (The parser can be con-
figured to recognize additional categories.) Question
data in particular requires the use of categories that
are rare or unseen in CCGbank.
For the questions, the new supertagger model,
like the POS tagger, was trained on WSJ 02-21 plus
ten copies of the 1,328 training sentences. For the
biomedical data, a ten-fold cross-validation was per-
formed, training each supertagger model on WSJ 02-
21 plus ten copies of 90% of the 1,000 annotated
sentences. Table 2 gives the supertagger accuracy
with and without the retrained POS and supertagger
models. The figure for the retrained biomedical su-
pertagger is the average of the ten-fold split.
The results show an improvement in accuracy of
lexical category assignment solely from retraining
the POS tagger, and an additional improvement from
retraining the supertagger. Supertagger accuracy for
the two domains with a retrained supertagger was
comparable, and in both cases was at least as high
479
What car company invented the Edsel?
(nsubj invented company)
(det Edsel the)
(dobj invented Edsel)
(det company What)
(nn company car)
Figure 2: Example of grammatical relations in the Stan-
ford grammatical relation format.
as for the original pipeline on Section 00. The ques-
tion data started from a much lower baseline figure,
however.
4.4 Parser
We evaluated the parser on the 500 questions anno-
tated with Stanford GRs and on the 500 evaluation
sentences from the BioInfer corpus. We used the
original newspaper pipeline, a pipeline with a re-
trained POS tagger, and a pipeline with both a re-
trained POS tagger and supertagger.
In order to perform these evaluations we devel-
ooped a mapping from the parser?s native CCG syn-
tactic dependencies to GRs in the Stanford format.
The mapping was based on the same principles as
the mapping that produces GR output in the style
of Briscoe et al (2006). These principles are dis-
cussed in detail in Clark and Curran (2007a); in
summary, the argument slots in the CCG dependen-
cies are mapped to argument slots in Stanford GRs,
a fairly complex, many-to-many mapping. An ad-
ditional post-processing script applies some manu-
ally developed rules to bring the output closer to the
Stanford format. Figure 2 gives an example of Stan-
ford GRs, where the label of the relation is followed
by two arguments, head and dependent.
Table 3 gives the results of the parser evaluation
on GRs. Since the parser model was not retrained,
the improvements in accuracy are due solely to the
new POS and supertaggers. The results are given as
an F-score over labelled GRs.2
The F-scores given in Table 3 are only for sen-
tences for which a parse was found. However, there
were also improvements in coverage with the re-
trained models. For the question data, parser cov-
2Only GRs at the lowest level of the Stanford hierarchy were
considered in the evaluation; more generic relations such as de-
pendent were not considered.
Orig POS
and super
New POS New POS
and super
Qus 64.4 69.4 86.6
BioInfer 76.0 80.4 81.5
Table 3: Parser F-score on grammatical relations and the
effect of retraining the POS and supertagger models.
erage was 94% for the original pipeline and the
pipeline with just the retrained POS tagger, and
99.6% with the retrained POS and supertaggers. For
the biomedical data, coverage was 97.2% for the
original pipeline, 99.0% for the pipeline with the re-
trained POS tagger, and 99.8% for the pipeline with
the retrained POS and supertaggers.
The final accuracy for both domains is in the same
range as that of the original parser on newspaper
data (81.8%) (Clark and Curran, 2007b), although
the results are not directly comparable, since the
newspaper resource uses a different GR scheme. For
the BioInfer corpus, the final accuracy is also in
line with results reported in the literature for other
parsers (Pyysalo et al, 2007b). (No comparable GR
results are available for questions.) A score in this
range is thought to be near the upper bound when
evaluating a CCG parser on GRs, since some loss is
inherent in the mapping to GRs (Clark and Curran,
2007a).
5 Analysis
Although domain adaptation was successful for both
of our target domains, the impact of the different
levels of representation on parsing accuracy was not
uniform. Table 3 shows that retraining the POS tag-
ger accounted for a greater proportion of the im-
provement on biomedical data, while retraining the
supertagger accounted for a much greater proportion
on questions. In this section we discuss some of the
differences between the domains which may have
contributed to their behaviour in this regard, with
the intention of highlighting attributes that may be
relevant for domain adaptation in general.
Informally, we believe that the main difference
between newspaper and biomedical text is vocabu-
lary, and that their syntactic structures are essentially
similar (with some isolated exceptions, such as more
frequent use of parentheses and comma-separated
480
Tag Errors Freq confused
Qus
WDT 129 WP
VB 46 NN, VBP
NNP 33 JJ, NN
NN 32 JJ, NNP
Bio
NN 801 JJ, CD
JJ 268 NN, VBN
VBN 113 JJ, VBD
FW 95 NN, IN
Table 4: Tags with the most frequent errors by the
newspaper-trained POS tagger and the tags they were
most frequently confused with.
lists in biomedical text). Once the POS tagger had
been retrained for biomedical text, accounting for
unfamiliar vocabulary, the original supertagger al-
ready performed well. The main difference between
newspaper and question data, on the other hand, is
syntactic. Retraining the POS tagger for questions
therefore had less effect; even with the correct POS
tags the supertagger was unable to assign the correct
lexical categories. Since lexical categories encode
syntactic information, the domain with the more di-
vergent syntax is likely to benefit most from new
training data at the lexical category level.
5.1 POS tagger
Table 1 showed that the accuracy of the newspaper-
trained POS tagger was in the same range for both
biomedical and question data. However, the distri-
bution of errors was different. Table 4 shows the tags
with the most frequent errors, accounting for about
75% of all POS tag errors in each domain, and the
tags that they were most frequently confused with.
For the question data, the most frequent error was
tagging a wh-determiner (WDT) as a wh-pronoun
(WP). A determiner combines with a noun to form
a noun phrase, as in the sentence What Liverpool
club spawned the Beatles?. A pronoun, on the other
hand, is a noun phrase in its own right, as in What
are the colors of the German flag?. This tagger er-
ror arises from the fact that the word What occurs
only once in WSJ 02-21 with a WDT tag. The sec-
ond most common error was on bare verbs (VB), be-
cause the newspaper model gives a low probability
of bare verbs occurring in sentence-final position, or
not directly following an auxiliary.
Unknown word
rate
Unknown
word-POS rate
Sec. 00 3.8 4.4
Qus 7.5 8.3
Bio 23.6 25.3
Table 5: Unknown word rate and word-POS tag pair rate
(%) compared to WSJ 02-21 (by token).
For the biomedical data, the most frequent errors
by far were confusions of noun (NN) and adjective
(JJ). This is most likely due to the prevalence of long
noun phrases in the biomedical data, such as major
histocompatibility complex class II molecules. Al-
though the words preceding the head noun are rec-
ognized as nominal modifiers, the classification into
noun and adjective is difficult, especially when the
word is previously unseen. There were also prob-
lems distinguishing verbal past participles (VBN)
from adjectives (JJ) and identifying foreign words
(FW), for example the phrase in vitro.
The fact that the newspaper-trained POS tagger
performed comparably in the two target domains
(Table 1) is surprising, since their lexical profiles
are quite different. Lease and Charniak (2005) dis-
cussed unknown word rate as a predictor of POS
tagger accuracy. However, the unknown word rate
compared with WSJ 02-21 is much higher for the
biomedical data than for the question data, as seen
in Table 5. (The unknown word rate for the question
data is still higher than that for WSJ 00, which may
be due to the high proportion of proper nouns in the
question data.)
Some POS tagging errors can be attributed, not
to an unknown word, but to the use of a known
word with an unfamiliar tag (as in the WDT exam-
ple above). However, it is not the case that the ques-
tion data contains many known words with unknown
tags, since the rate of unknown word-tag pairs is also
much higher for biomedical than for question data,
as seen in the rightmost column of Table 5.
We do know that the newspaper-trained POS tag-
ger performs better on unknown words for biomedi-
cal (84.7%) than for question data (80.4%). We hy-
pothesize that the syntactic context of the biomed-
ical data, being more similar to newspaper data,
provides more information for the POS tagger in
481
WSJ 02-21 New train-
ing sets
3-grams
Sec. 00 0.4 ?
Qus 3.6 0.7
Bio 0.7 0.5
5-grams
Sec. 00 12.1 ?
Qus 22.0 7.4
Bio 10.9 9.2
Table 6: Unknown POS n-gram rate (%) compared to WSJ
02-21, and when in-domain data is added (by token).
biomedical than in question data. Syntactic differ-
ences are discussed in the next section.
5.2 Supertagger
To quantify the syntactic distance between domains,
we propose using the unknown POS n-gram rate
compared to WSJ Sections 02-21. In the absence of
parse trees, POS n-grams can serve as a rough proxy
for the syntactic characteristics of a domain, reflect-
ing local word order configurations. POS n-grams
have been used in document modeling for text cate-
gorization (Baayen et al, 1996; Argamon-Engelson
et al, 1998), but we believe our proposed use of the
unknown POS n-gram rate is novel.
The leftmost column of Table 6 gives the un-
known POS trigram and 5-gram rates compared to
WSJ Sections 02-21. The rates for the biomedical
data are quite similar to those for Section 00. The
question data, however, shows higher rates of un-
known POS n-grams.
For both biomedical and question data, adding in-
domain data to the training set makes its syntactic
profile more like that of the evaluation set. The right-
most column of Table 6 shows the unknown POS n-
gram rates compared to the datasets used for training
the new supertagger models, consisting of WSJ 02-
21 plus annotated question or biomedical data. (For
the biomedical data, the figures are averages of the
same ten-fold split used for evaluation). It can be
seen that adding in-domain data reduces the rate of
unknown POS n-grams to about the same level ob-
served for newspaper text.
The unknown POS n-gram rate requires POS
tagged data for a new domain and thus cannot be
3-grams 5-grams
Sec. 00 18 19
Qus 8 5
Bio 16 13
Table 7: Number of the 20 most frequent POS n-grams
that are also in the 20 most frequent POS n-grams of WSJ
Sections 02-21.
WSJ 02-21 Bio Qus
. ? ? JJ NN NN ? ? WP
IN DT NN IN JJ NN ? WP VBZ
NN . ? NN IN JJ ? ? WDT
DT JJ NN NNS IN NN WP VBZ DT
Table 8: Four most frequent POS trigrams for WSJ 02-
21; four most frequent POS trigrams for biomedical and
question data that are not in the 20 most frequent for WSJ
02-21. The dash represents the sentence boundary.
used with unlabelled data. However, since POS tag-
ging is relatively inexpensive, it might be possible to
use this rate as one measure of syntactic distance be-
tween a training corpus and a target domain, prior to
undertaking parser domain adaptation. The measure
does not capture all aspects of syntactic distance,
however. As pointed out by an anonymous reviewer,
if the syntactic tree structures are similar across do-
mains but lexical distributions are different ? e.g. a
large number of words with unfamiliar categories in
the new domain ? this measure will not be sensitive
to the difference.
Another useful measure for comparing domain
adaptation in the biomedical and question domains
is frequent POS n-grams. Table 7 shows how many
of the 20 most frequent POS n-grams in each dataset
overlap with the 20 most frequent POS n-grams in
WSJ 02-21. It can be seen that the overlap is the
highest for Section 00, but much lower for the ques-
tion data than for the biomedical data, again demon-
strating that the question data makes frequent use of
syntactic constructions which are rare in the PTB.
Table 8 shows the four most frequent POS tri-
grams in WSJ Sections 02-21,3 and the four most
frequent POS trigrams in the biomedical and ques-
tion data that are not among the 20 most frequent
3Collapsing the NNP-NN distinction yields a slightly differ-
ent set.
482
for WSJ 02-21. The frequent question trigrams in-
clude two sentence-initial question words as well as
the pattern ? WP VBZ, occurring in sentences be-
ginning with e.g. What is or Who is. Though not
among the top four, the pattern VB . ?, represent-
ing a sentence-final bare verb, is also frequent. The
most frequent biomedical POS trigrams are not dra-
matically different from the newspaper trigrams, but
do appear to reflect the prevalence of NPs and PPs
in the data.
One final measure of syntactic distance is the
frequency with which CCG lexical categories that
are rare or unseen in CCGbank are used in a do-
main. It is typical to use a few such categories,
even for in-domain data, for unusual syntactic con-
structions, but each one is usually used only a hand-
ful of times. The question data is unique in the
frequency with which previously rare or unseen
categories are required. For example, the unseen
category (S [wq ]/S [q ])/N , representing the word
What in a question such as What day did Nintendo
64 come out? is used 11 times in the evaluation
set; the rare category (S [wq ]/(S [dcl ]\NP))/N ,
used in subject questions like Which river runs
through Dublin?, is used 61 times; and the rare cat-
egory (S [q ]/(S [pss]\NP))/NP , representing pas-
sive verbs in sentences like What is Jane Goodall
known for?, is used 59 times.
6 Conclusion
We have targeted lower levels of representation in
order to adapt a lexicalized-grammar parser to two
new domains, biomedical text and questions. Al-
though each of the lower levels has been targeted in-
dependently in previous work, this is the first study
that examines both levels together to determine how
they affect parsing accuracy. We achieved an accu-
racy on grammatical relations in the same range as
that of the original parser for newspaper text, with-
out requiring costly annotation of full parse trees.
Both biomedical and question data are domains in
which there is an immediate need for accurate pars-
ing. The question dataset is in some ways an ex-
treme example for domain adaptation, since the sen-
tences are syntactically uniform; on the other hand,
it is of interest as a set of constructions where the
parser initially performed poorly, and is a realistic
parsing challenge in the context of QA systems.
Interestingly, although an increase in accuracy at
each stage of the pipeline did yield an increase at
the following stage, these increases were not uni-
form across the two domains. The new POS tagger
model was responsible for most of the improvement
in parsing for the biomedical domain, while the new
supertagger model was necessary to see a large im-
provement in the question domain. We attribute this
to the fact that question syntax is significantly differ-
ent from newspaper syntax. We expect these consid-
erations to apply to any lexicalized-grammar parser.
Of course, it would be useful to have a way of
predicting which level of annotation would be most
effective for adapting to a new domain before the an-
notation begins. The utility of measures such as un-
known word rate (which can be performed with un-
labelled data) and unknown POS n-gram rate (which
can be performed with only POS tags) is not yet suffi-
ciently clear to rely on them as predictive measures,
but it seems a fruitful avenue for future work to in-
vestigate the importance of such measures for parser
domain adaptation.
Acknowledgments
We would like to thank Marie-Catherine de Marn-
effe for advice on the use of the Stanford GR for-
mat, Sampo Pyysalo for sharing information about
the BioInfer corpus, and Mark Steedman for advice
on encoding question data in CCG. We would also
like to thank three anonymous reviewers for their
suggestions. This work was supported by EPSRC
grant EP/E035698/1: Accurate and Efficient Parsing
of Biomedical Text.
References
Shlomo Argamon-Engelson, Moshe Koppel, and Galit
Avneri. 1998. Style-based text categorization: What
newspaper am I reading? In Proceedings of AAAI
Workshop on Learning for Text Categorization, pages
1?4.
Harald Baayen, Hans Van Halteren, and Fiona Tweedie.
1996. Outside the cave of shadows: Using syntactic
annotation to enhance authorship attribution. Literary
and Linguistic Computing, 11(3):121?131.
Srinivas Bangalore and Aravind Joshi. 1999. Supertag-
ging: An approach to almost parsing. Computational
Linguistics, 25(2):237?265.
483
Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The second release of the RASP system. In Proceed-
ings of the Interactive Demo Session of the Joint Con-
ference of the International Committee on Computa-
tional Linguistics and the Association for Computa-
tional Linguistics (COLING/ACL-06), Sydney, Aus-
trailia.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the 1st Meeting of the
NAACL, pages 132?139, Seattle, WA.
Stephen Clark and James R. Curran. 2007a. Formalism-
independent parser evaluation with CCG and Dep-
Bank. In Proceedings of the 45th Meeting of the ACL,
pages 248?255, Prague, Czech Republic.
Stephen Clark and James R. Curran. 2007b. Wide-
coverage efficient statistical parsing with CCG and
log-linear models. Computational Linguistics,
33(4):493?552.
Stephen Clark, Mark Steedman, and James R. Curran.
2004. Object-extraction and question-parsing using
CCG. In Proceedings of the EMNLP Conference,
pages 111?118, Barcelona, Spain.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of the
35th Meeting of the ACL, pages 16?23, Madrid, Spain.
James R. Curran, Stephen Clark, and David Vadas.
2006. Multi-tagging for lexicalized-grammar pars-
ing. In Proceedings of the Joint Conference of the
International Committee on Computational Linguis-
tics and the Association for Computational Linguis-
tics (COLING/ACL-06), pages 697?704, Sydney, Aus-
trailia.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In Pro-
ceedings of the 5th LREC Conference, pages 449?454,
Genoa, Italy.
Tadayoshi Hara, Yusuke Miyao, and Jun?ichi Tsujii.
2007. Evaluating impact of re-training a lexical dis-
ambiguation model on domain adaptation of an HPSG
parser. In Proceedings of IWPT, pages 11?22, Prague,
Czech Republic.
Sanda Harabagiu, Dan Moldovan, Marius Pasca, Rada
Mihalcea, Mihai Surdeanu, Razvan Bunescu, Roxana
Girju, Vasile Rus, and Paul Morarescu. 2001. The
role of lexico-semantic feedback in open-domain tex-
tual question-answering. In Proceedings of the 39th
Meeting of the ACL, pages 274?281, Toulose, France.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: a corpus of CCG derivations and dependency
structures extracted from the Penn Treebank. Compu-
tational Linguistics, 33(3):355?396.
John Judge, Aoife Cahill, and Josef van Genabith. 2006.
Questionbank: Creating a corpus of parse-annotated
questions. In Proceedings of the Joint Conference of
the International Committee on Computational Lin-
guistics and the Association for Computational Lin-
guistics, pages 497?504, Sydney, Australia.
Jin-Dong Kim, Tomoko Ohta, Yuka Teteisi, and Jun?ichi
Tsujii. 2003. GENIA corpus ? a semantically an-
notated corpus for bio-textmining. Bioinformatics,
19:i180?i182.
Tracy H. King, Richard Crouch, Stefan Riezler, Mary
Dalrymple, and Ronald M. Kaplan. 2003. The PARC
700 Dependency Bank. In Proceedings of the 4th
International Workshop on Linguistically Interpreted
Corpora, Budapest, Hungary.
Matthew Lease and Eugene Charniak. 2005. Parsing
biomedical literature. In Proceedings of the Second
International Joint Conference on Natural Language
Processing (IJCNLP-05), Jeju Island, Korea.
Mitchell Marcus, Beatrice Santorini, and Mary
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313?330.
Yusuke Miyao and Jun?ichi Tsujii. 2005. Probabilistic
disambiguation models for wide-coverage HPSG pars-
ing. In Proceedings of the 43rd meeting of the ACL,
pages 83?90, University of Michigan, Ann Arbor.
Takashi Ninomiya, Takuya Matsuzaki, Yoshimasa Tsu-
ruoka, Yusuke Miyao, and Jun?ichi Tsujii. 2006. Ex-
tremely lexicalized models for accurate and fast HPSG
parsing. In Proceedings of the EMNLP Conference.
Sampo Pyysalo, Filip Ginter, Juho Heimonen, Jari
Bjo?rne, Jorma Boberg, Jouni Ja?rvinen, and Tapio
Salakoski. 2007a. BioInfer: A corpus for information
extraction in the biomedical domain. BMC Bioinfor-
matics, 8:50.
Sampo Pyysalo, Filip Ginter, Veronika Laippala, Ka-
tri Haverinen, Juho Heimonen, and Tapio Salakoski.
2007b. On the unification of syntactic annotations un-
der the stanford dependency scheme: A case study on
BioInfer and GENIA. In ACL?07 workshop on Biolog-
ical, translational, and clinical language processing,
pages 25?32, Prague, Czech Republic.
Mark Steedman. 2000. The Syntactic Process. The MIT
Press, Cambridge, MA.
484
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 562?571,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
A Tale of Two Parsers: investigating and combining graph-based and
transition-based dependency parsing using beam-search
Yue Zhang and Stephen Clark
Oxford University Computing Laboratory
Wolfson Building, Parks Road
Oxford OX1 3QD, UK
{yue.zhang,stephen.clark}@comlab.ox.ac.uk
Abstract
Graph-based and transition-based approaches
to dependency parsing adopt very different
views of the problem, each view having its
own strengths and limitations. We study both
approaches under the framework of beam-
search. By developing a graph-based and a
transition-based dependency parser, we show
that a beam-search decoder is a competitive
choice for both methods. More importantly,
we propose a beam-search-based parser that
combines both graph-based and transition-
based parsing into a single system for train-
ing and decoding, showing that it outper-
forms both the pure graph-based and the pure
transition-based parsers. Testing on the En-
glish and Chinese Penn Treebank data, the
combined system gave state-of-the-art accura-
cies of 92.1% and 86.2%, respectively.
1 Introduction
Graph-based (McDonald et al, 2005; McDon-
ald and Pereira, 2006; Carreras et al, 2006) and
transition-based (Yamada and Matsumoto, 2003;
Nivre et al, 2006) parsing algorithms offer two dif-
ferent approaches to data-driven dependency pars-
ing. Given an input sentence, a graph-based algo-
rithm finds the highest scoring parse tree from all
possible outputs, scoring each complete tree, while
a transition-based algorithm builds a parse by a se-
quence of actions, scoring each action individually.
The terms ?graph-based? and ?transition-based?
were used by McDonald and Nivre (2007) to de-
scribe the difference between MSTParser (McDon-
ald and Pereira, 2006), which is a graph-based parser
with an exhaustive search decoder, and MaltParser
(Nivre et al, 2006), which is a transition-based
parser with a greedy search decoder. In this paper,
we do not differentiate graph-based and transition-
based parsers by their search algorithms: a graph-
based parser can use an approximate decoder while
a transition-based parser is not necessarily determin-
istic. To make the concepts clear, we classify the two
types of parser by the following two criteria:
1. whether or not the outputs are built by explicit
transition-actions, such as ?Shift? and ?Reduce?;
2. whether it is dependency graphs or transition-
actions that the parsing model assigns scores to.
By this classification, beam-search can be applied
to both graph-based and transition-based parsers.
Representative of each method, MSTParser and
MaltParser gave comparable accuracies in the
CoNLL-X shared task (Buchholz and Marsi, 2006).
However, they make different types of errors, which
can be seen as a reflection of their theoretical differ-
ences (McDonald and Nivre, 2007). MSTParser has
the strength of exact inference, but its choice of fea-
tures is constrained by the requirement of efficient
dynamic programming. MaltParser is deterministic,
yet its comparatively larger feature range is an ad-
vantage. By comparing the two, three interesting re-
search questions arise: (1) how to increase the flex-
ibility in defining features for graph-based parsing;
(2) how to add search to transition-based parsing;
and (3) how to combine the two parsing approaches
so that the strengths of each are utilized.
In this paper, we study these questions under one
framework: beam-search. Beam-search has been
successful in many NLP tasks (Koehn et al, 2003;
562
Inputs: training examples (xi, yi)
Initialization: set ~w = 0
Algorithm:
// R training iterations; N examples
for t = 1..R, i = 1..N :
zi = argmaxy?GEN(xi) ?(y) ? ~w
if zi 6= yi:
~w = ~w + ?(yi)? ?(zi)
Outputs: ~w
Figure 1: The perceptron learning algorithm
Collins and Roark, 2004), and can achieve accuracy
that is close to exact inference. Moreover, a beam-
search decoder does not impose restrictions on the
search problem in the way that an exact inference
decoder typically does, such as requiring the ?op-
timal subproblem? property for dynamic program-
ming, and therefore enables a comparatively wider
range of features for a statistical system.
We develop three parsers. Firstly, using the same
features as MSTParser, we develop a graph-based
parser to examine the accuracy loss from beam-
search compared to exact-search, and the accuracy
gain from extra features that are hard to encode
for exact inference. Our conclusion is that beam-
search is a competitive choice for graph-based pars-
ing. Secondly, using the transition actions from
MaltParser, we build a transition-based parser and
show that search has a positive effect on its accuracy
compared to deterministic parsing. Finally, we show
that by using a beam-search decoder, we are able
to combine graph-based and transition-based pars-
ing into a single system, with the combined system
significantly outperforming each individual system.
In experiments with the English and Chinese Penn
Treebank data, the combined parser gave 92.1% and
86.2% accuracy, respectively, which are comparable
to the best parsing results for these data sets, while
the Chinese accuracy outperforms the previous best
reported by 1.8%. In line with previous work on de-
pendency parsing using the Penn Treebank, we fo-
cus on projective dependency parsing.
2 The graph-based parser
Following MSTParser (McDonald et al, 2005; Mc-
Donald and Pereira, 2006), we define the graph-
Variables: agenda ? the beam for state items
item ? partial parse tree
output ? a set of output items
index, prev ? word indexes
Input: x ? POS-tagged input sentence.
Initialization: agenda = [??]
Algorithm:
for index in 1..x.length():
clear output
for item in agenda:
// for all prev words that can be linked with
// the current word at index
prev = index ? 1
while prev 6= 0: // while prev is valid
// add link making prev parent of index
newitem = item // duplicate item
newitem.link(prev, index) // modify
output.append(newitem) // record
// if prev does not have a parent word,
// add link making index parent of prev
if item.parent(prev) == 0:
item.link(index, prev) // modify
output.append(item) // record
prev = the index of the first word before
prev whose parent does not exist
or is on its left; 0 if no match
clear agenda
put the best items from output to agenda
Output: the best item in agenda
Figure 2: A beam-search decoder for graph-based pars-
ing, developed from the deterministic Covington algo-
rithm for projective parsing (Covington, 2001).
based parsing problem as finding the highest scoring
tree y from all possible outputs given an input x:
F (x) = argmax
y?GEN(x)
Score(y)
where GEN(x) denotes the set of possible parses for
the input x. To repeat our earlier comments, in this
paper we do not consider the method of finding the
argmax to be part of the definition of graph-based
parsing, only the fact that the dependency graph it-
self is being scored, and factored into scores at-
tached to the dependency links.
The score of an output parse y is given by a linear
model:
Score(y) = ?(y) ? ~w
563
where ?(y) is the global feature vector from y and
~w is the weight vector of the model.
We use the discriminative perceptron learning al-
gorithm (Collins, 2002; McDonald et al, 2005) to
train the values of ~w. The algorithm is shown in Fig-
ure 1. Averaging parameters is a way to reduce over-
fitting for perceptron training (Collins, 2002), and is
applied to all our experiments.
While the MSTParser uses exact-inference (Eis-
ner, 1996), we apply beam-search to decoding. This
is done by extending the deterministic Covington
algorithm for projective dependency parsing (Cov-
ington, 2001). As shown in Figure 2, the decoder
works incrementally, building a state item (i.e. par-
tial parse tree) word by word. When each word is
processed, links are added between the current word
and its predecessors. Beam-search is applied by
keeping the B best items in the agenda at each pro-
cessing stage, while partial candidates are compared
by scores from the graph-based model, according to
partial graph up to the current word.
Before decoding starts, the agenda contains an
empty sentence. At each processing stage, existing
partial candidates from the agenda are extended in
all possible ways according to the Covington algo-
rithm. The top B newly generated candidates are
then put to the agenda. After all input words are pro-
cessed, the best candidate output from the agenda is
taken as the final output.
The projectivity of the output dependency trees
is guaranteed by the incremental Covington process.
The time complexity of this algorithm is O(n2),
where n is the length of the input sentence.
During training, the ?early update? strategy of
Collins and Roark (2004) is used: when the correct
state item falls out of the beam at any stage, parsing
is stopped immediately, and the model is updated
using the current best partial item. The intuition is
to improve learning by avoiding irrelevant informa-
tion: when all the items in the current agenda are
incorrect, further parsing steps will be irrelevant be-
cause the correct partial output no longer exists in
the candidate ranking.
Table 1 shows the feature templates from the
MSTParser (McDonald and Pereira, 2006), which
are defined in terms of the context of a word, its
parent and its sibling. To give more templates, fea-
tures from templates 1 ? 5 are also conjoined with
1 Parent word (P) Pw; Pt; Pwt
2 Child word (C) Cw; Ct; Cwt
3 P and C PwtCwt; PwtCw;
PwCwt; PwtCt;
PtCwt; PwCw; PtCt
4 A tag Bt PtBtCt
between P, C
5 Neighbour words PtPLtCtCLt;
of P, C, PtPLtCtCRt;
left (PL/CL) PtPRtCtCLt;
and right (PR/CR) PtPRtCtCRt;
PtPLtCLt; PtPLtCRt;
PtPRtCLt; PtPRtCRt;
PLtCtCLt; PLtCtCRt;
PRtCtCLt; PRtCtCRt;
PtCtCLt; PtCtCRt;
PtPLtCt; PtPRtCt
6 sibling (S) of C CwSw; CtSt;
CwSt; CtSw;
PtCtSt;
Table 1: Feature templates from MSTParser
w ? word; t ? POS-tag.
1 leftmost (CLC) and PtCtCLCt;
rightmost (CRC) PtCtCRCt
children of C
2 left (la) and right (ra) Ptla; Ptra;
arity of P Pwtla; Pwtra
Table 2: Additional feature templates for the graph-based
parser
the link direction and distance, while features from
template 6 are also conjoined with the direction and
distance between the child and its sibling. Here
?distance? refers to the difference between word in-
dexes. We apply all these feature templates to the
graph-based parser. In addition, we define two extra
feature templates (Table 2) that capture information
about grandchildren and arity (i.e. the number of
children to the left or right). These features are not
conjoined with information about direction and dis-
tance. They are difficult to include in an efficient
dynamic programming decoder, but easy to include
in a beam-search decoder.
564
Figure 3: Feature context for the transition-based algo-
rithm
3 The transition-based parser
We develop our transition-based parser using the
transition model of the MaltParser (Nivre et al,
2006), which is characterized by the use of a stack
and four transition actions: Shift, ArcRight, ArcLeft
and Reduce. An input sentence is processed from
left to right, with an index maintained for the current
word. Initially empty, the stack is used throughout
the parsing process to store unfinished words, which
are the words before the current word that may still
be linked with the current or a future word.
The Shift action pushes the current word to the
stack and moves the current index to the next word.
The ArcRight action adds a dependency link from
the stack top to the current word (i.e. the stack top
becomes the parent of the current word), pushes the
current word on to the stack, and moves the current
index to the next word. The ArcLeft action adds a
dependency link from the current word to the stack
top, and pops the stack. The Reduce action pops the
stack. Among the four transition actions, Shift and
ArcRight push a word on to the stack while ArcLeft
and Reduce pop the stack; Shift and ArcRight read
the next input word while ArcLeft and ArcRight add
a link to the output. By repeated application of these
actions, the parser reads through the input and builds
a parse tree.
The MaltParser works deterministically. At each
step, it makes a single decision and chooses one of
the four transition actions according to the current
context, including the next input words, the stack
and the existing links. As illustrated in Figure 3, the
contextual information consists of the top of stack
(ST), the parent (STP) of ST, the leftmost (STLC) and
rightmost child (STRC) of ST, the current word (N0),
the next three words from the input (N1, N2, N3) and
the leftmost child of N0 (N0LC). Given the context
s, the next action T is decided as follows:
T (s) = argmax
T?ACTION
Score(T, s)
where ACTION = {Shift, ArcRight, ArcLeft,
Reduce}.
One drawback of deterministic parsing is error
propagation, since once an incorrect action is made,
the output parse will be incorrect regardless of the
subsequent actions. To reduce such error propa-
gation, a parser can keep track of multiple candi-
date outputs and avoid making decisions too early.
Suppose that the parser builds a set of candidates
GEN(x) for the input x, the best output F (x) can
be decided by considering all actions:
F (x) = argmax
y?GEN(x)
?
T ??act(y) Score(T ?, sT ?)
Here T ? represents one action in the sequence
(act(y)) by which y is built, and sT ? represents the
corresponding context when T ? is taken.
Our transition-based algorithm keeps B different
sequences of actions in the agenda, and chooses the
one having the overall best score as the final parse.
Pseudo code for the decoding algorithm is shown
in Figure 4. Here each state item contains a partial
parse tree as well as a stack configuration, and state
items are built incrementally by transition actions.
Initially the stack is empty, and the agenda contains
an empty sentence. At each processing stage, one
transition action is applied to existing state items as
a step to build the final parse. Unlike the MaltParser,
which makes a decision at each stage, our transition-
based parser applies all possible actions to each ex-
isting state item in the agenda to generate new items;
then from all the newly generated items, it takes the
B with the highest overall score and puts them onto
the agenda. In this way, some ambiguity is retained
for future resolution.
Note that the number of transition actions needed
to build different parse trees can vary. For exam-
ple, the three-word sentence ?A B C? can be parsed
by the sequence of three actions ?Shift ArcRight
ArcRight? (B modifies A; C modifies B) or the
sequence of four actions ?Shift ArcLeft Shift Ar-
cRight? (both A and C modifies B). To ensure that
all final state items are built by the same number
of transition actions, we require that the final state
565
Variables: agenda ? the beam for state items
item ? (partial tree, stack config)
output ? a set of output items
index ? iteration index
Input: x ? POS-tagged input sentence.
Initialization: agenda = [(??, [])]
Algorithm:
for index in 1 .. 2? x.length() ?1:
clear output
for item in agenda:
// when all input words have been read, the
// parse tree has been built; only pop.
if item.length() == x.length():
if item.stacksize() > 1:
item.Reduce()
output.append(item)
// when some input words have not been read
else:
if item.lastaction() 6= Reduce:
newitem = item
newitem.Shift()
output.append(newitem)
if item.stacksize() > 0:
newitem = item
newitem.ArcRight()
output.append(newitem)
if (item.parent(item.stacktop())==0):
newitem = item
newitem.ArcLeft()
output.append(newitem)
else:
newitem = item
newitem.Reduce()
output.append(newitem)
clear agenda
transfer the best items from output to agenda
Output: the best item in agenda
Figure 4: A beam-search decoding algorithm for
transition-based parsing
items must 1) have fully-built parse trees; and 2)
have only one root word left on the stack. In this
way, popping actions should be made even after a
complete parse tree is built, if the stack still contains
more than one word.
Now because each word excluding the root must
be pushed to the stack once and popped off once
during the parsing process, the number of actions
Inputs: training examples (xi, yi)
Initialization: set ~w = 0
Algorithm:
// R training iterations; N examples
for t = 1..R, i = 1..N :
zi = argmaxy?GEN(xi)
?
T ??act(yi) ?(T
?, c?) ? ~w
if zi 6= yi:
~w = ~w + ?T ??act(yi) ?(T
?, cT ?)
??T ??act(zi) ?(T
?, cT ?)
Outputs: ~w
Figure 5: the perceptron learning algorithm for the
transition-based parser
1 stack top STwt; STw; STt
2 current word N0wt; N0w; N0t
3 next word N1wt; N1w; N1t
4 ST and N0 STwtN0wt; STwtN0w;
STwN0wt; STwtN0t;
STtN0wt; STwN0w; STtN0t
5 POS bigram N0tN1t
6 POS trigrams N0tN1tN2t; STtN0tN1t;
STPtSTtN0t; STtSTLCtN0t;
STtSTRCtN0t; STtN0tN0LCt
7 N0 word N0wN1tN2t; STtN0wN1t;
STPtSTtN0w; STtSTLCtN0w;
STtSTRCtN0w; STtN0wN0LCt
Table 3: Feature templates for the transition-based parser
w ? word; t ? POS-tag.
needed to parse a sentence is always 2n ? 1, where
n is the length of the sentence. Therefore, the de-
coder has linear time complexity, given a fixed beam
size. Because the same transition actions as the
MaltParser are used to build each item, the projec-
tivity of the output dependency tree is ensured.
We use a linear model to score each transition ac-
tion, given a context:
Score(T, s) = ?(T, s) ? ~w
?(T, s) is the feature vector extracted from the ac-
tion T and the context s, and ~w is the weight vec-
tor. Features are extracted according to the templates
shown in Table 3, which are based on the context in
Figure 3. Note that our feature definitions are sim-
ilar to those used by MaltParser, but rather than us-
ing a kernel function with simple features (e.g. STw,
566
N0t, but not STwt or STwN0w), we combine features
manually.
As with the graph-based parser, we use the dis-
criminative perceptron (Collins, 2002) to train the
transition-based model (see Figure 5). It is worth
noticing that, in contrast to MaltParser, which trains
each action decision individually, our training algo-
rithm globally optimizes all action decisions for a
parse. Again, ?early update? and averaging parame-
ters are applied to the training process.
4 The combined parser
The graph-based and transition-based approaches
adopt very different views of dependency parsing.
McDonald and Nivre (2007) showed that the MST-
Parser and MaltParser produce different errors. This
observation suggests a combined approach: by using
both graph-based information and transition-based
information, parsing accuracy can be improved.
The beam-search framework we have developed
facilitates such a combination. Our graph-based
and transition-based parsers share many similarities.
Both build a parse tree incrementally, keeping an
agenda of comparable state items. Both rank state
items by their current scores, and use the averaged
perceptron with early update for training. The key
differences are the scoring models and incremental
parsing processes they use, which must be addressed
when combining the parsers.
Firstly, we combine the graph-based and the
transition-based score models simply by summation.
This is possible because both models are global and
linear. In particular, the transition-based model can
be written as:
ScoreT(y) =
?
T ??act(y) Score(T ?, sT ?)
= ?T ??act(y) ?(T ?, sT ?) ? ~wT
= ~wT ?
?
T ??act(y) ?(T ?, sT ?)
If we take
?
T ??act(y) ?(T ?, sT ?) as the global fea-
ture vector ?T(y), we have:
ScoreT(y) = ?T(y) ? ~wT
which has the same form as the graph-based model:
ScoreG(y) = ?G(y) ? ~wG
Sections Sentences Words
Training 2?21 39,832 950,028
Dev 22 1,700 40,117
Test 23 2,416 56,684
Table 4: The training, development and test data from
PTB
We therefore combine the two models to give:
ScoreC(y) = ScoreG(y) + ScoreT(y)
= ?G(y) ? ~wG + ?T(y) ? ~wT
Concatenating the feature vectors ?G(y) and ?T(y)
to give a global feature vector ?C(y), and the weight
vectors ~wG and ~wT to give a weight vector ~wC, the
combined model can be written as:
ScoreC(y) = ?C(y) ? ~wC
which is a linear model with exactly the same form
as both sub-models, and can be trained with the per-
ceptron algorithm in Figure 1. Because the global
feature vectors from the sub models are concate-
nated, the feature set for the combined model is the
union of the sub model feature sets.
Second, the transition-based decoder can be used
for the combined system. Both the graph-based de-
coder in Figure 2 and the transition-based decoder in
Figure 4 construct a parse tree incrementally. How-
ever, the graph-based decoder works on a per-word
basis, adding links without using transition actions,
and so is not appropriate for the combined model.
The transition-based algorithm, on the other hand,
uses state items which contain partial parse trees,
and so provides all the information needed by the
graph-based parser (i.e. dependency graphs), and
hence the combined system.
In summary, we build the combined parser by
using a global linear model, the union of feature
templates and the decoder from the transition-based
parser.
5 Experiments
We evaluate the parsers using the English and Chi-
nese Penn Treebank corpora. The English data
is prepared by following McDonald et al (2005).
Bracketed sentences from the Penn Treebank (PTB)
3 are split into training, development and test sets
567
Figure 6: The influence of beam size on the transition-
based parser, using the development data
X-axis: number of training iterations
Y-axis: word precision
as shown in Table 4, and then translated into depen-
dency structures using the head-finding rules from
Yamada and Matsumoto (2003).
Before parsing, POS tags are assigned to the in-
put sentence using our reimplementation of the POS-
tagger from Collins (2002). Like McDonald et al
(2005), we evaluate the parsing accuracy by the
precision of lexical heads (the percentage of input
words, excluding punctuation, that have been as-
signed the correct parent) and by the percentage
of complete matches, in which all words excluding
punctuation have been assigned the correct parent.
5.1 Development experiments
Since the beam size affects all three parsers, we
study its influence first; here we show the effect on
the transition-based parser. Figure 6 shows different
accuracy curves using the development data, each
with a different beam size B. The X-axis represents
the number of training iterations, and the Y-axis the
precision of lexical heads.
The parsing accuracy generally increases as the
beam size increases, while the quantity of increase
becomes very small when B becomes large enough.
The decoding times after the first training iteration
are 10.2s, 27.3s, 45.5s, 79.0s, 145.4s, 261.3s and
469.5s, respectively, when B = 1, 2, 4, 8, 16, 32, 64.
Word Complete
MSTParser 1 90.7 36.7
Graph [M] 91.2 40.8
Transition 91.4 41.8
Graph [MA] 91.4 42.5
MSTParser 2 91.5 42.1
Combined [TM] 92.0 45.0
Combined [TMA] 92.1 45.4
Table 5: Accuracy comparisons using PTB 3
In the rest of the experiments, we set B = 64 in
order to obtain the highest possible accuracy.
When B = 1, the transition-based parser be-
comes a deterministic parser. By comparing the
curves when B = 1 and B = 2, we can see that,
while the use of search reduces the parsing speed, it
improves the quality of the output parses. Therefore,
beam-search is a reasonable choice for transition-
based parsing.
5.2 Accuracy comparisons
The test accuracies are shown in Table 5, where each
row represents a parsing model. Rows ?MSTParser
1/2? show the first-order (using feature templates 1 ?
5 from Table 1) (McDonald et al, 2005) and second-
order (using all feature templates from Table 1)
(McDonald and Pereira, 2006) MSTParsers, as re-
ported by the corresponding papers. Rows ?Graph
[M]? and ?Graph [MA]? represent our graph-based
parser using features from Table 1 and Table 1 + Ta-
ble 2, respectively; row ?Transition? represents our
transition-based parser; and rows ?Combined [TM]?
and ?Combined [TMA]? represent our combined
parser using features from Table 3 + Table 1 and Ta-
ble 3 + Table 1 + Table 2, respectively. Columns
?Word? and ?Complete? show the precision of lexi-
cal heads and complete matches, respectively.
As can be seen from the table, beam-search re-
duced the head word accuracy from 91.5%/42.1%
(?MSTParser 2?) to 91.2%/40.8% (?Graph [M]?)
with the same features as exact-inference. How-
ever, with only two extra feature templates from
Table 2, which are not conjoined with direction or
distance information, the accuracy is improved to
91.4%/42.5% (?Graph [MA]?). This improvement
can be seen as a benefit of beam-search, which al-
lows the definition of more global features.
568
Sections Sentences Words
Training 001?815; 16,118 437,859
1001?1136
Dev 886?931; 804 20,453
1148?1151
Test 816?885; 1,915 50,319
1137?1147
Table 6: Training, development and test data from CTB
Non-root Root Comp.
Graph [MA] 83.86 71.38 29.82
Duan 2007 84.36 73.70 32.70
Transition 84.69 76.73 32.79
Combined [TM] 86.13 77.04 35.25
Combined [TMA] 86.21 76.26 34.41
Table 7: Test accuracies with CTB 5 data
The combined parser is tested with various sets
of features. Using only graph-based features in Ta-
ble 1, it gave 88.6% accuracy, which is much lower
than 91.2% from the graph-based parser using the
same features (?Graph [M]?). This can be explained
by the difference between the decoders. In particu-
lar, the graph-based model is unable to score the ac-
tions ?Reduce? and ?Shift?, since they do not mod-
ify the parse tree. Nevertheless, the score serves as a
reference for the effect of additional features in the
combined parser.
Using both transition-based features and graph-
based features from the MSTParser (?Combined
[TM]?), the combined parser achieved 92.0% per-
word accuracy, which is significantly higher than the
pure graph-based and transition-based parsers. Ad-
ditional graph-based features further improved the
accuracy to 92.1%/45.5%, which is the best among
all the parsers compared.1
5.3 Parsing Chinese
We use the Penn Chinese Treebank (CTB) 5 for ex-
perimental data. Following Duan et al (2007), we
1A recent paper, Koo et al (2008) reported parent-prediction
accuracy of 92.0% using a graph-based parser with a different
(larger) set of features (Carreras, 2007). By applying separate
word cluster information, Koo et al (2008) improved the accu-
racy to 93.2%, which is the best known accuracy on the PTB
data. We excluded these from Table 5 because our work is not
concerned with the use of such additional knowledge.
split the corpus into training, development and test
data as shown in Table 6, and use the head-finding
rules in Table 8 in the Appendix to turn the bracketed
sentences into dependency structures. Most of the
head-finding rules are from Sun and Jurafsky (2004),
while we added rules to handle NN and FRAG, and
a default rule to use the rightmost node as the head
for the constituent that are not listed.
Like Duan et al (2007), we use gold-standard
POS-tags for the input. The parsing accuracy is eval-
uated by the percentage of non-root words that have
been assigned the correct head, the percentage of
correctly identified root words, and the percentage
of complete matches, all excluding punctuation.
The accuracies are shown in Table 7. Rows
?Graph [MA]?, ?Transition?, ?Combined [TM]? and
?Combined [TMA]? show our models in the same
way as for the English experiments from Section 5.2.
Row ?Duan 2007? represents the transition-based
model from Duan et al (2007), which applies beam-
search to the deterministic model from Yamada and
Matsumoto (2003), and achieved the previous best
accuracy on the data.
Our observations on parsing Chinese are essen-
tially the same as for English. Our combined parser
outperforms both the pure graph-based and the pure
transition-based parsers. It gave the best accuracy
we are aware of for dependency parsing using CTB.
6 Related work
Our graph-based parser is derived from the work
of McDonald and Pereira (2006). Instead of per-
forming exact inference by dynamic programming,
we incorporated the linear model and feature tem-
plates from McDonald and Pereira (2006) into our
beam-search framework, while adding new global
features. Nakagawa (2007) and Hall (2007) also
showed the effectiveness of global features in im-
proving the accuracy of graph-based parsing, us-
ing the approximate Gibbs sampling method and a
reranking approach, respectively.
Our transition-based parser is derived from the
deterministic parser of Nivre et al (2006). We
incorporated the transition process into our beam-
search framework, in order to study the influence
of search on this algorithm. Existing efforts to
add search to deterministic parsing include Sagae
569
and Lavie (2006b), which applied best-first search
to constituent parsing, and Johansson and Nugues
(2006) and Duan et al (2007), which applied beam-
search to dependency parsing. All three methods es-
timate the probability of each transition action, and
score a state item by the product of the probabilities
of all its corresponding actions. But different from
our transition-based parser, which trains all transi-
tions for a parse globally, these models train the
probability of each action separately. Based on the
work of Johansson and Nugues (2006), Johansson
and Nugues (2007) studied global training with an
approximated large-margin algorithm. This model
is the most similar to our transition-based model,
while the differences include the choice of learning
and decoding algorithms, the definition of feature
templates and our application of the ?early update?
strategy.
Our combined parser makes the biggest contribu-
tion of this paper. In contrast to the models above,
it includes both graph-based and transition-based
components. An existing method to combine mul-
tiple parsing algorithms is the ensemble approach
(Sagae and Lavie, 2006a), which was reported to
be useful in improving dependency parsing (Hall et
al., 2007). A more recent approach (Nivre and Mc-
Donald, 2008) combined MSTParser and MaltParser
by using the output of one parser for features in the
other. Both Hall et al (2007) and Nivre and McDon-
ald (2008) can be seen as methods to combine sep-
arately defined models. In contrast, our parser com-
bines two components in a single model, in which
all parameters are trained consistently.
7 Conclusion and future work
We developed a graph-based and a transition-based
projective dependency parser using beam-search,
demonstrating that beam-search is a competitive
choice for both parsing approaches. We then com-
bined the two parsers into a single system, using dis-
criminative perceptron training and beam-search de-
coding. The appealing aspect of the combined parser
is the incorporation of two largely different views of
the parsing problem, thus increasing the information
available to a single statistical parser, and thereby
significantly increasing the accuracy. When tested
using both English and Chinese dependency data,
the combined parser was highly competitive com-
pared to the best systems in the literature.
The idea of combining different approaches to
the same problem using beam-search and a global
model could be applied to other parsing tasks, such
as constituent parsing, and possibly other NLP tasks.
Acknowledgements
This work is supported by the ORS and Clarendon
Fund. We thank the anonymous reviewers for their
detailed comments.
Appendix
Constituent Rules
ADJP r ADJP JJ AD; r
ADVP r ADVP AD CS JJ NP PP P VA VV; r
CLP r CLP M NN NP; r
CP r CP IP VP; r
DNP r DEG DNP DEC QP; r
DP r M; l DP DT OD; l
DVP r DEV AD VP; r
FRAG r VV NR NN NT; r
IP r VP IP NP; r
LCP r LCP LC; r
LST r CD NP QP; r
NP r NP NN IP NR NT; r
NN r NP NN IP NR NT; r
PP l P PP; l
PRN l PU; l
QP r QP CLP CD; r
UCP l IP NP VP; l
VCD l VV VA VE; l
VP l VE VC VV VNV VPT VRD VSB
VCD VP; l
VPT l VA VV; l
VRD l VVI VA; l
VSB r VV VE; r
default r
Table 8: Head-finding rules to extract dependency data
from CTB
570
References
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of CoNLL, pages 149?164, New York
City, USA, June.
Xavier Carreras, Mihai Surdeanu, and Lluis Marquez.
2006. Projective dependency parsing with perceptron.
In Proceedings of CoNLL, New York City, USA, June.
Xavier Carreras. 2007. Experiments with a higher-order
projective dependency parser. In Proceedings of the
CoNLL Shared Task Session of EMNLP/CoNLL, pages
957?961, Prague, Czech Republic, June.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceedings
of ACL, pages 111?118, Barcelona, Spain, July.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
EMNLP, pages 1?8, Philadelphia, USA, July.
Michael A. Covington. 2001. A fundamental algorithm
for dependency parsing. In Proceedings of the ACM
Southeast Conference, Athens, Georgia, March.
Xiangyu Duan, Jun Zhao, and Bo Xu. 2007. Probabilis-
tic models for action-based chinese dependency pars-
ing. In Proceedings of ECML/ECPPKDD, Warsaw,
Poland, September.
Jason Eisner. 1996. Three new probabilistic models for
dependency parsing: An exploration. In Proceedings
of COLING, pages 340?345, Copenhagen, Denmark,
August.
Johan Hall, Jens Nilsson, Joakim Nivre, Gu?lsen Eryigit,
Bea?ta Megyesi, Mattias Nilsson, and Markus Saers.
2007. Single malt or blended? a study in multilingual
parser optimization. In Proceedings of the CoNLL
Shared Task Session of EMNLP/CoNLL, pages 933?
939, Prague, Czech Republic, June.
Keith Hall. 2007. K-best spanning tree parsing. In Pro-
ceedings of ACL, Prague, Czech Republic, June.
Richard Johansson and Pierre Nugues. 2006. Investigat-
ing multilingual dependency parsing. In Proceedings
of CoNLL, pages 206?210, New York City, USA, June.
Richard Johansson and Pierre Nugues. 2007. Incremen-
tal dependency parsing using online learning. In Pro-
ceedings of the CoNLL/EMNLP, pages 1134?1138,
Prague, Czech Republic.
Philip Koehn, Franz Och, and Daniel Marcu. 2003. Sta-
tistical phrase-based translation. In Proceedings of
NAACL/HLT, Edmonton, Canada, May.
Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple semi-supervised dependency parsing. In Pro-
ceedings of ACL/HLT, pages 595?603, Columbus,
Ohio, June.
Ryan McDonald and Joakim Nivre. 2007. Characteriz-
ing the errors of data-driven dependency parsing mod-
els. In Proceedings of EMNLP/CoNLL, pages 122?
131, Prague, Czech Republic, June.
R McDonald and F Pereira. 2006. Online learning of ap-
proximate dependency parsing algorithms. In In Proc.
of EACL, pages 81?88, Trento, Italy, April.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proceedings of ACL, pages 91?98, Ann
Arbor, Michigan, June.
Tetsuji Nakagawa. 2007. Multilingual dependency
parsing using global features. In Proceedings of the
CoNLL Shared Task Session of EMNLP/CoNLL, pages
952?956, Prague, Czech Republic, June.
Joakim Nivre and Ryan McDonald. 2008. Integrating
graph-based and transition-based dependency parsers.
In Proceedings of ACL/HLT, pages 950?958, Colum-
bus, Ohio, June.
Joakim Nivre, Johan Hall, Jens Nilsson, Gu?ls?en Eryig?it,
and Svetoslav Marinov. 2006. Labeled pseudo-
projective dependency parsing with support vector ma-
chines. In Proceedings of CoNLL, pages 221?225,
New York City, USA, June.
K Sagae and A Lavie. 2006a. Parser combination by
reparsing. In In Proc. HLT/NAACL, pages 129?132,
New York City, USA, June.
Kenji Sagae and Alon Lavie. 2006b. A best-first prob-
abilistic shift-reduce parser. In Proceedings of COL-
ING/ACL (poster), pages 691?698, Sydney, Australia,
July.
Honglin Sun and Daniel Jurafsky. 2004. Shallow
semantic parsing of Chinese. In Proceedings of
NAACL/HLT, Boston, USA, May.
H Yamada and Y Matsumoto. 2003. Statistical depen-
dency analysis using support vector machines. In Pro-
ceedings of IWPT, Nancy, France, April.
571
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 144?151,
New York, June 2006. c?2006 Association for Computational Linguistics
Partial Training for a Lexicalized-Grammar Parser
Stephen Clark
Oxford University Computing Laboratory
Wolfson Building, Parks Road
Oxford, OX1 3QD, UK
stephen.clark@comlab.ox.ac.uk
James R. Curran
School of Information Technologies
University of Sydney
NSW 2006, Australia
james@it.usyd.edu.au
Abstract
We propose a solution to the annotation
bottleneck for statistical parsing, by ex-
ploiting the lexicalized nature of Combi-
natory Categorial Grammar (CCG). The
parsing model uses predicate-argument
dependencies for training, which are de-
rived from sequences of CCG lexical cate-
gories rather than full derivations. A sim-
ple method is used for extracting depen-
dencies from lexical category sequences,
resulting in high precision, yet incomplete
and noisy data. The dependency parsing
model of Clark and Curran (2004b) is ex-
tended to exploit this partial training data.
Remarkably, the accuracy of the parser
trained on data derived from category se-
quences alone is only 1.3% worse in terms
of F-score than the parser trained on com-
plete dependency structures.
1 Introduction
State-of-the-art statistical parsers require large
amounts of hand-annotated training data, and are
typically based on the Penn Treebank, the largest
treebank available for English. Even robust parsers
using linguistically sophisticated formalisms, such
as TAG (Chiang, 2000), CCG (Clark and Curran,
2004b; Hockenmaier, 2003), HPSG (Miyao et al,
2004) and LFG (Riezler et al, 2002; Cahill et al,
2004), often use training data derived from the Penn
Treebank. The labour-intensive nature of the tree-
bank development process, which can take many
years, creates a significant barrier for the develop-
ment of parsers for new domains and languages.
Previous work has attempted parser adaptation
without relying on treebank data from the new do-
main (Steedman et al, 2003; Lease and Charniak,
2005). In this paper we propose the use of anno-
tated data in the new domain, but only partially an-
notated data, which reduces the annotation effort re-
quired (Hwa, 1999). We develop a parsing model
which can be trained using partial data, by exploiting
the properties of lexicalized grammar formalisms.
The formalism we use is Combinatory Categorial
Grammar (Steedman, 2000), together with a parsing
model described in Clark and Curran (2004b) which
we adapt for use with partial data.
Parsing with Combinatory Categorial Grammar
(CCG) takes place in two stages: first, CCG lexical
categories are assigned to the words in the sentence,
and then the categories are combined by the parser
(Clark and Curran, 2004a). The lexical categories
can be thought of as detailed part of speech tags and
typically express subcategorization information. We
exploit the fact that CCG lexical categories contain
a lot of syntactic information, and can therefore be
used for training a full parser, even though attach-
ment information is not explicitly represented in a
category sequence. Our partial training regime only
requires sentences to be annotated with lexical cate-
gories, rather than full parse trees; therefore the data
can be produced much more quickly for a new do-
main or language (Clark et al, 2004).
The partial training method uses the log-linear
dependency model described in Clark and Curran
(2004b), which uses sets of predicate-argument de-
144
pendencies, rather than derivations, for training. Our
novel idea is that, since there is so much informa-
tion in the lexical category sequence, most of the
correct dependencies can be easily inferred from the
categories alone. More specifically, for a given sen-
tence and lexical category sequence, we train on
those predicate-argument dependencies which occur
in k% of the derivations licenced by the lexical cat-
egories. By setting the k parameter high, we can
produce a set of high precision dependencies for
training. A similar idea is proposed by Carroll and
Briscoe (2002) for producing high precision data for
lexical acquisition.
Using this procedure we are able to produce de-
pendency data with over 99% precision and, re-
markably, up to 86% recall, when compared against
the complete gold-standard dependency data. The
high recall figure results from the significant amount
of syntactic information in the lexical categories,
which reduces the ambiguity in the possible depen-
dency structures. Since the recall is not 100%, we
require a log-linear training method which works
with partial data. Riezler et al (2002) describe a
partial training method for a log-linear LFG parsing
model in which the ?correct? LFG derivations for a
sentence are those consistent with the less detailed
gold standard derivation from the Penn Treebank.
We use a similar method here by treating a CCG
derivation as correct if it is consistent with the high-
precision partial dependency structure. Section 3 ex-
plains what we mean by consistency in this context.
Surprisingly, the accuracy of the parser trained on
partial data approaches that of the parser trained on
full data: our best partial-data model is only 1.3%
worse in terms of dependency F-score than the full-
data model, despite the fact that the partial data does
not contain any explicit attachment information.
2 The CCG Parsing Model
Clark and Curran (2004b) describes two log-linear
parsing models for CCG: a normal-form derivation
model and a dependency model. In this paper we
use the dependency model, which requires sets of
predicate-argument dependencies for training.1
1Hockenmaier and Steedman (2002) describe a generative
model of normal-form derivations; one possibility for training
this model on partial data, which has not been explored, is to
use the EM algorithm (Pereira and Schabes, 1992).
The predicate-argument dependencies are repre-
sented as 5-tuples: ?hf , f, s, ha, l?, where hf is the
lexical item of the lexical category expressing the
dependency relation; f is the lexical category; s is
the argument slot; ha is the head word of the ar-
gument; and l encodes whether the dependency is
non-local. For example, the dependency encoding
company as the object of bought (as in IBM bought
the company) is represented as follows:
?bought2, (S\NP1 )/NP2 , 2, company4,?? (1)
CCG dependency structures are sets of predicate-
argument dependencies. We define the probability
of a dependency structure as the sum of the probabil-
ities of all those derivations leading to that structure
(Clark and Curran, 2004b). ?Spurious ambiguity? in
CCG means that there can be more than one deriva-
tion leading to any one dependency structure. Thus,
the probability of a dependency structure, pi, given a
sentence, S, is defined as follows:
P (pi|S) =
?
d??(pi)
P (d, pi|S) (2)
where ?(pi) is the set of derivations which lead to pi.
The probability of a ?d, pi? pair, ?, conditional on
a sentence S, is defined using a log-linear form:
P (?|S) = 1ZS
e?.f (?) (3)
where ?.f(?) =?i ?ifi(?). The function fi is the
integer-valued frequency function of the ith feature;
?i is the weight of the ith feature; and ZS is a nor-
malising constant.
Clark and Curran (2004b) describes the training
procedure for the dependency model, which uses a
discriminative estimation method by maximising the
conditional likelihood of the model given the data
(Riezler et al, 2002). The optimisation of the objec-
tive function is performed using the limited-memory
BFGS numerical optimisation algorithm (Nocedal
and Wright, 1999; Malouf, 2002), which requires
calculation of the objective function and the gradi-
ent of the objective function at each iteration.
The objective function is defined below, where
L(?) is the likelihood and G(?) is a Gaussian prior
term for smoothing.
145
He anticipates growth for the auto maker
NP (S [dcl ]\NP)/NP NP (NP\NP)/NP NP [nb]/N N /N N
Figure 1: Example sentence with CCG lexical categories
L?(?) = L(?) ?G(?) (4)
=
m
?
j=1
log
?
d??(pij)
e?.f (d,pij)
?
m
?
j=1
log
?
???(Sj)
e?.f (?) ?
n
?
i=1
?2i
2?2
S1, . . . , Sm are the sentences in the training data;
pi1, . . . , pim are the corresponding gold-standard de-
pendency structures; ?(S) is the set of possible
?derivation, dependency-structure? pairs for S; ? is
a smoothing parameter; and n is the number of fea-
tures. The components of the gradient vector are:
?L?(?)
??i
=
m
?
j=1
?
d??(pij)
e?.f (d,pij)fi(d, pij)
?
d??(pij) e?.f (d,pij)
(5)
?
m
?
j=1
?
???(Sj)
e?.f (?)fi(?)
?
???(Sj) e?.f (?)
? ?i?2
The first two terms of the gradient are expecta-
tions of feature fi: the first expectation is over
all derivations leading to each gold-standard depen-
dency structure, and the second is over all deriva-
tions for each sentence in the training data. The es-
timation process attempts to make the expectations
in (5) equal (ignoring the Gaussian prior term). An-
other way to think of the estimation process is that
it attempts to put as much mass as possible on the
derivations leading to the gold-standard structures
(Riezler et al, 2002).
Calculation of the feature expectations requires
summing over all derivations for a sentence, and
summing over all derivations leading to a gold-
standard dependency structure. Clark and Cur-
ran (2003) shows how the sum over the complete
derivation space can be performed efficiently using
a packed chart and the inside-outside algorithm, and
Clark and Curran (2004b) extends this method to
sum over all derivations leading to a gold-standard
dependency structure.
3 Partial Training
The partial data we use for training the dependency
model is derived from CCG lexical category se-
quences only. Figure 1 gives an example sentence
adapted from CCGbank (Hockenmaier, 2003) to-
gether with its lexical category sequence. Note that,
although the attachment of the prepositional phrase
to the noun phrase is not explicitly represented, it
can be inferred in this example because the lexical
category assigned to the preposition has to combine
with a noun phrase to the left, and in this example
there is only one possibility. One of the key insights
in this paper is that the significant amount of syntac-
tic information in CCG lexical categories allows us
to infer attachment information in many cases.
The procedure we use for extracting dependencies
from a sequence of lexical categories is to return all
those dependencies which occur in k% of the deriva-
tions licenced by the categories. By giving the k pa-
rameter a high value, we can extract sets of depen-
dencies with very high precision; in fact, assuming
that the correct lexical category sequence licences
the correct derivation, setting k to 100 must result in
100% precision, since any dependency which occurs
in every derivation must occur in the correct deriva-
tion. Of course the recall is not guaranteed to be
high; decreasing k has the effect of increasing recall,
but at the cost of decreasing precision.
The training method described in Section 2 can
be adapted to use the (potentially incomplete) sets
of dependencies returned by our extraction proce-
dure. In Section 2 a derivation was considered cor-
rect if it produced the complete set of gold-standard
dependencies. In our partial-data version a deriva-
tion is considered correct if it produces dependen-
cies which are consistent with the dependencies re-
turned by our extraction procedure. We define con-
sistency as follows: a set of dependencies D is con-
sistent with a set G if G is a subset of D. We also
say that a derivation d is consistent with dependency
set G if G is a subset of the dependencies produced
by d.
146
This definition of ?correct derivation? will intro-
duce some noise into the training data. Noise arises
from sentences where the recall of the extracted de-
pendencies is less than 100%, since some of the
derivations which are consistent with the extracted
dependencies for such sentences will be incorrect.
Noise also arises from sentences where the preci-
sion of the extracted dependencies is less than 100%,
since for these sentences every derivation which is
consistent with the extracted dependencies will be
incorrect. The hope is that, if an incorrect derivation
produces mostly correct dependencies, then it can
still be useful for training. Section 4 shows how the
precision and recall of the extracted dependencies
varies with k and how this affects parsing accuracy.
The definitions of the objective function (4) and
the gradient (5) for training remain the same in the
partial-data case; the only differences are that ?(pi)
is now defined to be those derivations which are con-
sistent with the partial dependency structure pi, and
the gold-standard dependency structures pij are the
partial structures extracted from the gold-standard
lexical category sequences.2
Clark and Curran (2004b) gives an algorithm for
finding all derivations in a packed chart which pro-
duce a particular set of dependencies. This algo-
rithm is required for calculating the value of the ob-
jective function (4) and the first feature expectation
in (5). We adapt this algorithm for finding all deriva-
tions which are consistent with a partial dependency
structure. The new algorithm is shown in Figure 2.
The algorithm relies on the definition of a packed
chart, which is an instance of a feature forest (Miyao
and Tsujii, 2002). The idea behind a packed chart is
that equivalent chart entries of the same type and in
the same cell are grouped together, and back point-
ers to the daughters indicate how an individual entry
was created. Equivalent entries form the same struc-
tures in any subsequent parsing.
A feature forest is defined in terms of disjunctive
and conjunctive nodes. For a packed chart, the indi-
vidual entries in a cell are conjunctive nodes, and the
equivalence classes of entries are disjunctive nodes.
The definition of a feature forest is as follows:
A feature forest ? is a tuple ?C,D,R, ?, ?? where:
2Note that the procedure does return all the gold-standard
dependencies for some sentences.
?C,D,R, ?, ?? is a packed chart / feature forest
G is a set of dependencies returned by the extraction procedure
Let c be a conjunctive node
Let d be a disjunctive node
deps(c) is the set of dependencies on node c
cdeps(c) = |deps(c) ? G|
dmax(c) =
?
d??(c) dmax(d) + cdeps(c)
dmax(d) = max{dmax(c) | c ? ?(d)}
mark(d):
mark d as a correct node
foreach c ? ?(d)
if dmax(c) == dmax(d)
mark c as a correct node
foreach d? ? ?(c)
mark(d?)
foreach dr ? R such that dmax. (dr) = |G|
mark(dr)
Figure 2: Finding nodes in derivations consistent
with a partial dependency structure
? C is a set of conjunctive nodes;
? D is a set of disjunctive nodes;
? R ? D is a set of root disjunctive nodes;
? ? : D ? 2C is a conjunctive daughter function;
? ? : C ? 2D is a disjunctive daughter function.
Dependencies are associated with conjunctive
nodes in the feature forest. For example, if the
disjunctive nodes (equivalence classes of individual
entries) representing the categories NP and S\NP
combine to produce a conjunctive node S , the re-
sulting S node will have a verb-subject dependency
associated with it.
In Figure 2, cdeps(c) is the number of dependen-
cies on conjunctive node c which appear in partial
structure G; dmax(c) is the maximum number of
dependencies in G produced by any sub-derivation
headed by c; dmax(d) is the same value for disjunc-
tive node d. Recursive definitions for calculating
these values are given; the base case occurs when
conjunctive nodes have no disjunctive daughters.
The algorithm identifies all those root nodes head-
ing derivations which are consistent with the partial
dependency structure G, and traverses the chart top-
down marking the nodes in those derivations. The
insight behind the algorithm is that, for two con-
junctive nodes in the same equivalence class, if one
node heads a sub-derivation producing more depen-
dencies in G than the other node, then the node with
147
less dependencies inG cannot be part of a derivation
consistent with G.
The conjunctive and disjunctive nodes appearing
in derivations consistent with G form a new ?gold-
standard? feature forest. The gold-standard forest,
and the complete forest containing all derivations
spanning the sentence, can be used to estimate the
likelihood value and feature expectations required
by the estimation algorithm. Let E??fi be the ex-
pected value of fi over the forest ? for model ?;
then the values in (5) can be obtained by calculating
E?j? fi for the complete forest ?j for each sentence
Sj in the training data (the second sum in (5)), and
also E?j? fi for each forest ?j of derivations consis-
tent with the partial gold-standard dependency struc-
ture for sentence Sj (the first sum in (5)):
?L(?)
??i
=
m
?
j=1
(E?j? fi ?E
?j
? fi) (6)
The likelihood in (4) can be calculated as follows:
L(?) =
m
?
j=1
(logZ?j ? logZ?j ) (7)
where logZ? is the normalisation constant for ?.
4 Experiments
The resource used for the experiments is CCGbank
(Hockenmaier, 2003), which consists of normal-
form CCG derivations derived from the phrase-
structure trees in the Penn Treebank. It also contains
predicate-argument dependencies which we use for
development and final evaluation.
4.1 Accuracy of Dependency Extraction
Sections 2-21 of CCGbank were used to investigate
the accuracy of the partial dependency structures re-
turned by the extraction procedure. Full, correct de-
pendency structures for the sentences in 2-21 were
created by running our CCG parser (Clark and Cur-
ran, 2004b) over the gold-standard derivation for
each sentence, outputting the dependencies. This re-
sulted in full dependency structures for 37,283 of the
sentences in sections 2-21.
Table 1 gives precision and recall values for the
dependencies obtained from the extraction proce-
dure, for the 37,283 sentences for which we have
k Precision Recall SentAcc
0.99999 99.76 74.96 13.84
0.9 99.69 79.37 16.52
0.85 99.65 81.30 18.40
0.8 99.57 82.96 19.51
0.7 99.09 85.87 22.46
0.6 98.00 88.67 26.28
Table 1: Accuracy of the Partial Dependency Data
complete dependency structures. The SentAcc col-
umn gives the percentage of training sentences for
which the partial dependency structures are com-
pletely correct. For a given sentence, the extrac-
tion procedure returns all dependencies occurring in
at least k% of the derivations licenced by the gold-
standard lexical category sequence. The lexical cat-
egory sequences for the sentences in 2-21 can easily
be read off the CCGbank derivations.
The derivations licenced by a lexical category se-
quence were created using the CCG parser described
in Clark and Curran (2004b). The parser uses a small
number of combinatory rules to combine the cate-
gories, along with the CKY chart-parsing algorithm
described in Steedman (2000). It also uses some
unary type-changing rules and punctuation rules ob-
tained from the derivations in CCGbank.3 The parser
builds a packed representation, and counting the
number of derivations in which a dependency occurs
can be performed using a dynamic programming al-
gorithm similar to the inside-outside algorithm.
Table 1 shows that, by varying the value of k, it
is possible to get the recall of the extracted depen-
dencies as high as 85.9%, while still maintaining a
precision value of over 99%.
4.2 Accuracy of the Parser
The training data for the dependency model was cre-
ated by first supertagging the sentences in sections
2-21, using the supertagger described in Clark and
Curran (2004b).4 The average number of categories
3Since our training method is intended to be applicable in
the absence of derivation data, the use of such rules may appear
suspect. However, we argue that the type-changing and punc-
tuation rules could be manually created for a new domain by
examining the lexical category data.
4An improved version of the supertagger was used for this
paper in which the forward-backward algorithm is used to cal-
culate the lexical category probability distributions.
148
assigned to each word is determined by a parameter,
?, in the supertagger. A category is assigned to a
word if the category?s probability is within ? of the
highest probability category for that word.
For these experiments, we used a ? value of 0.01,
which assigns roughly 1.6 categories to each word,
on average; we also ensured that the correct lexi-
cal category was in the set assigned to each word.
(We did not do this when parsing the test data.) For
some sentences, the packed charts can become very
large. The supertagging approach we adopt for train-
ing differs to that used for testing: if the size of the
chart exceeds some threshold, the value of ? is in-
creased, reducing ambiguity, and the sentence is su-
pertagged and parsed again. The threshold which
limits the size of the charts was set at 300 000 indi-
vidual entries. Two further values of ? were used:
0.05 and 0.1.
Packed charts were created for each sentence and
stored in memory. It is essential that the packed
charts for each sentence contain at least one deriva-
tion leading to the gold-standard dependency struc-
ture. Not all rule instantiations in CCGbank can be
produced by our parser; hence it is not possible to
produce the gold standard for every sentence in Sec-
tions 2-21. For the full-data model we used 34 336
sentences (86.7% of the total). For the partial-data
models we were able to use slightly more, since the
partial structures are easier to produce. Here we
used 35,709 sentences (k = 0.85).
Since some of the packed charts are very large,
we used an 18-node Beowulf cluster, together with
a parallel version of the BFGS training algorithm.
The training time and number of iterations to con-
vergence were 172 minutes and 997 iterations for the
full-data model, and 151 minutes and 861 iterations
for the partial-data model (k = 0.85). Approximate
memory usage in each case was 17.6 GB of RAM.
The dependency model uses the same set of fea-
tures described in Clark and Curran (2004b): de-
pendency features representing predicate-argument
dependencies (with and without distance measures);
rule instantiation features encoding the combining
categories together with the result category (with
and without a lexical head); lexical category fea-
tures, consisting of word?category pairs at the leaf
nodes; and root category features, consisting of
headword?category pairs at the root nodes. Further
k LP LR F CatAcc
0.99999 85.80 84.51 85.15 93.77
0.9 85.86 84.51 85.18 93.78
0.85 85.89 84.50 85.19 93.71
0.8 85.89 84.45 85.17 93.70
0.7 85.52 84.07 84.79 93.72
0.6 84.99 83.70 84.34 93.65
FullData 87.16 85.84 86.50 93.79
Random 74.63 72.53 73.57 89.31
Table 2: Accuracy of the Parser on Section 00
generalised features for each feature type are formed
by replacing words with their POS tags.
Only features which occur more than once in the
training data are included, except that the cutoff
for the rule features is 10 or more and the count-
ing is performed across all derivations licenced by
the gold-standard lexical category sequences. The
larger cutoff was used since the productivity of the
grammar can lead to large numbers of these features.
The dependency model has 548 590 features. In or-
der to provide a fair comparison, the same feature set
was used for the partial-data and full-data models.
The CCG parsing consists of two phases: first the
supertagger assigns the most probable categories to
each word, and then the small number of combina-
tory rules, plus the type-changing and punctuation
rules, are used with the CKY algorithm to build a
packed chart.5 We use the method described in Clark
and Curran (2004b) for integrating the supertagger
with the parser: initially a small number of cat-
egories is assigned to each word, and more cate-
gories are requested if the parser cannot find a span-
ning analysis. The ?maximum-recall? algorithm de-
scribed in Clark and Curran (2004b) is used to find
the highest scoring dependency structure.
Table 2 gives the accuracy of the parser on Section
00 of CCGbank, evaluated against the predicate-
argument dependencies in CCGbank.6 The table
gives labelled precision, labelled recall and F-score,
and lexical category accuracy. Numbers are given
for the partial-data model with various values of k,
and for the full-data model, which provides an up-
5Gold-standard POS tags from CCGbank were used for all
the experiments in this paper.
6There are some dependency types produced by our parser
which are not in CCGbank; these were ignored for evaluation.
149
LP LR F CatAcc
k = 0.85 86.21 85.01 85.60 93.90
FullData 87.50 86.37 86.93 94.01
Table 3: Accuracy of the Parser on Section 23
k Precision Recall SentAcc
0.99999 99.71 80.16 17.48
0.9999 99.68 82.09 19.13
0.999 99.49 85.18 22.18
0.99 99.00 88.95 27.69
0.95 98.34 91.69 34.95
0.9 97.82 92.84 39.18
Table 4: Accuracy of the Partial Dependency Data
using Inside-Outside Scores
per bound for the partial-data model. We also give a
lower bound which we obtain by randomly travers-
ing a packed chart top-down, giving equal proba-
bility to each conjunctive node in an equivalence
class. The precision and recall figures are over those
sentences for which the parser returned an analysis
(99.27% of Section 00).
The best result is obtained for a k value of 0.85,
which produces partial dependency data with a pre-
cision of 99.7 and a recall of 81.3. Interestingly, the
results show that decreasing k further, which results
in partial data with a higher recall and only a slight
loss in precison, harms the accuracy of the parser.
The Random result also dispels any suspicion that
the partial-model is performing well simply because
of the supertagger; clearly there is still much work
to be done after the supertagging phase.
Table 3 gives the accuracy of the parser on Sec-
tion 23, using the best performing partial-data model
on Section 00. The precision and recall figures are
over those sentences for which the parser returned
an analysis (99.63% of Section 23). The results
show that the partial-data model is only 1.3% F-
score short of the upper bound.
4.3 Further Experiments with Inside-Outside
In a final experiment, we attempted to exploit the
high accuracy of the partial-data model by using it
to provide new training data. For each sentence in
Section 2-21, we parsed the gold-standard lexical
category sequences and used the best performing
partial-data model to assign scores to each depen-
dency in the packed chart. The score for a depen-
dency was the sum of the probabilities of all deriva-
tions producing that dependency, which can be cal-
culated using the inside-outside algorithm. (This is
the score used by the maximum-recall parsing algo-
rithm.) Partial dependency structures were then cre-
ated by returning all dependencies whose score was
above some threshold k, as before. Table 4 gives the
accuracy of the data created by this procedure. Note
how these values differ to those reported in Table 1.
We then trained the dependency model on this
partial data using the same method as before. How-
ever, the peformance of the parser on Section 00 us-
ing these new models was below that of the previous
best performing partial-data model for all values of
k. We report this negative result because we had hy-
pothesised that using a probability model to score
the dependencies, rather than simply the number of
derivations in which they occur, would lead to im-
proved performance.
5 Conclusions
Our main result is that it is possible to train a CCG
dependency model from lexical category sequences
alone and still obtain parsing results which are only
1.3% worse in terms of labelled F-score than a
model trained on complete data. This is a notewor-
thy result and demonstrates the significant amount
of information encoded in CCG lexical categories.
The engineering implication is that, since the de-
pendency model can be trained without annotating
recursive structures, and only needs sequence in-
formation at the word level, then it can be ported
rapidly to a new domain (or language) by annotating
new sequence data in that domain.
One possible response to this argument is that,
since the lexical category sequence contains so
much syntactic information, then the task of anno-
tating category sequences must be almost as labour
intensive as annotating full derivations. To test this
hypothesis fully would require suitable annotation
tools and subjects skilled in CCG annotation, which
we do not currently have access to.
However, there is some evidence that annotat-
ing category sequences can be done very efficiently.
Clark et al (2004) describes a porting experiment
150
in which a CCG parser is adapted for the ques-
tion domain. The supertagger component of the
parser is trained on questions annotated at the lex-
ical category level only. The training data consists
of over 1,000 annotated questions which took less
than a week to create. This suggests, as a very
rough approximation, that 4 annotators could an-
notate 40,000 sentences with lexical categories (the
size of the Penn Treebank) in a few months.
Another advantage of annotating with lexical cat-
egories is that a CCG supertagger can be used to per-
form most of the annotation, with the human an-
notator only required to correct the mistakes made
by the supertagger. An accurate supertagger can be
bootstrapped quicky, leaving only a small number of
corrections for the annotator. A similar procedure is
suggested by Doran et al (1997) for porting an LTAG
grammar to a new domain.
We have a proposed a novel solution to the an-
notation bottleneck for statistical parsing which ex-
ploits the lexicalized nature of CCG, and may there-
fore be applicable to other lexicalized grammar for-
malisms such as LTAG.
References
A. Cahill, M. Burke, R. O?Donovan, J. van Genabith, and
A. Way. 2004. Long-distance dependency resolution in au-
tomatically acquired wide-coverage PCFG-based LFG ap-
proximations. In Proceedings of the 42nd Meeting of the
ACL, pages 320?327, Barcelona, Spain.
John Carroll and Ted Briscoe. 2002. High precision extrac-
tion of grammatical relations. In Proceedings of the 19th In-
ternational Conference on Computational Linguistics, pages
134?140, Taipei, Taiwan.
David Chiang. 2000. Statistical parsing with an automatically-
extracted Tree Adjoining Grammar. In Proceedings of the
38th Meeting of the ACL, pages 456?463, Hong Kong.
Stephen Clark and James R. Curran. 2003. Log-linear mod-
els for wide-coverage CCG parsing. In Proceedings of the
EMNLP Conference, pages 97?104, Sapporo, Japan.
Stephen Clark and James R. Curran. 2004a. The importance of
supertagging for wide-coverage CCG parsing. In Proceed-
ings of COLING-04, pages 282?288, Geneva, Switzerland.
Stephen Clark and James R. Curran. 2004b. Parsing the WSJ
using CCG and log-linear models. In Proceedings of the
42nd Meeting of the ACL, pages 104?111, Barcelona, Spain.
Stephen Clark, Mark Steedman, and James R. Curran. 2004.
Object-extraction and question-parsing using CCG. In
Proceedings of the EMNLP Conference, pages 111?118,
Barcelona, Spain.
C. Doran, B. Hockey, P. Hopely, J. Rosenzweig, A. Sarkar,
B. Srinivas, F. Xia, A. Nasr, and O. Rambow. 1997. Main-
taining the forest and burning out the underbrush in XTAG.
In Proceedings of the ENVGRAM Workshop, Madrid, Spain.
Julia Hockenmaier and Mark Steedman. 2002. Generative
models for statistical parsing with Combinatory Categorial
Grammar. In Proceedings of the 40th Meeting of the ACL,
pages 335?342, Philadelphia, PA.
Julia Hockenmaier. 2003. Data and Models for Statistical
Parsing with Combinatory Categorial Grammar. Ph.D. the-
sis, University of Edinburgh.
Rebbeca Hwa. 1999. Supervised grammar induction using
training data with limited constituent information. In Pro-
ceedings of the 37th Meeting of the ACL, pages 73?79, Uni-
versity of Maryland, MD.
Matthew Lease and Eugene Charniak. 2005. Parsing biomed-
ical literature. In Proceedings of the Second Interna-
tional Joint Conference on Natural Language Processing
(IJCNLP-05), Jeju Island, Korea.
Robert Malouf. 2002. A comparison of algorithms for max-
imum entropy parameter estimation. In Proceedings of the
Sixth Workshop on Natural Language Learning, pages 49?
55, Taipei, Taiwan.
Yusuke Miyao and Jun?ichi Tsujii. 2002. Maximum entropy
estimation for feature forests. In Proceedings of the Human
Language Technology Conference, San Diego, CA.
Yusuke Miyao, Takashi Ninomiya, and Jun?ichi Tsujii. 2004.
Corpus-oriented grammar development for acquiring a head-
driven phrase structure grammar from the Penn Treebank. In
Proceedings of the First International Joint Conference on
Natural Language Processing (IJCNLP-04), pages 684?693,
Hainan Island, China.
Jorge Nocedal and Stephen J. Wright. 1999. Numerical Opti-
mization. Springer, New York, USA.
Fernando Pereira and Yves Schabes. 1992. Inside-outside rees-
timation from partially bracketed corpora. In Proceedings of
the 30th Meeting of the ACL, pages 128?135, Newark, DE.
Stefan Riezler, Tracy H. King, Ronald M. Kaplan, Richard
Crouch, John T. Maxwell III, and Mark Johnson. 2002.
Parsing the Wall Street Journal using a Lexical-Functional
Grammar and discriminative estimation techniques. In Pro-
ceedings of the 40th Meeting of the ACL, pages 271?278,
Philadelphia, PA.
Mark Steedman, Miles Osborne, Anoop Sarkar, Stephen Clark,
Rebecca Hwa, Julia Hockenmaier, Paul Ruhlen, Steve Baker,
and Jeremiah Crim. 2003. Bootstrapping statistical parsers
from small datasets. In Proceedings of the 11th Conference
of the European Association for Computational Linguistics,
Budapest, Hungary.
Mark Steedman. 2000. The Syntactic Process. The MIT Press,
Cambridge, MA.
151
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 697?704,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Multi-Tagging for Lexicalized-Grammar Parsing
James R. Curran
School of IT
University of Sydney
NSW 2006, Australia
james@it.usyd.edu.au
Stephen Clark
Computing Laboratory
Oxford University
Wolfson Building
Parks Road
Oxford, OX1 3QD, UK
sclark@comlab.ox.ac.uk
David Vadas
School of IT
University of Sydney
NSW 2006, Australia
dvadas1@it.usyd.edu.au
Abstract
With performance above 97% accuracy for
newspaper text, part of speech (POS) tag-
ging might be considered a solved prob-
lem. Previous studies have shown that
allowing the parser to resolve POS tag
ambiguity does not improve performance.
However, for grammar formalisms which
use more fine-grained grammatical cate-
gories, for example TAG and CCG, tagging
accuracy is much lower. In fact, for these
formalisms, premature ambiguity resolu-
tion makes parsing infeasible.
We describe a multi-tagging approach
which maintains a suitable level of lexical
category ambiguity for accurate and effi-
cient CCG parsing. We extend this multi-
tagging approach to the POS level to over-
come errors introduced by automatically
assigned POS tags. Although POS tagging
accuracy seems high, maintaining some
POS tag ambiguity in the language pro-
cessing pipeline results in more accurate
CCG supertagging.
1 Introduction
State-of-the-art part of speech (POS) tagging ac-
curacy is now above 97% for newspaper text
(Collins, 2002; Toutanova et al, 2003). One pos-
sible conclusion from the POS tagging literature
is that accuracy is approaching the limit, and any
remaining improvement is within the noise of the
Penn Treebank training data (Ratnaparkhi, 1996;
Toutanova et al, 2003).
So why should we continue to work on the POS
tagging problem? Here we give two reasons. First,
for lexicalized grammar formalisms such as TAG
and CCG, the tagging problem is much harder.
Second, any errors in POS tagger output, even at
97% acuracy, can have a significant impact on
components further down the language processing
pipeline. In previous work we have shown that us-
ing automatically assigned, rather than gold stan-
dard, POS tags reduces the accuracy of our CCG
parser by almost 2% in dependency F-score (Clark
and Curran, 2004b).
CCG supertagging is much harder than POS tag-
ging because the CCG tag set consists of fine-
grained lexical categories, resulting in a larger tag
set ? over 400 CCG lexical categories compared
with 45 Penn Treebank POS tags. In fact, using
a state-of-the-art tagger as a front end to a CCG
parser makes accurate parsing infeasible because
of the high supertagging error rate.
Our solution is to use multi-tagging, in which
a CCG supertagger can potentially assign more
than one lexical category to a word. In this
paper we significantly improve our earlier ap-
proach (Clark and Curran, 2004a) by adapting the
forward-backward algorithm to a Maximum En-
tropy tagger, which is used to calculate a proba-
bility distribution over lexical categories for each
word. This distribution is used to assign one or
more categories to each word (Charniak et al,
1996). We report large increases in accuracy over
single-tagging at only a small cost in increased
ambiguity.
A further contribution of the paper is to also
use multi-tagging for the POS tags, and to main-
tain some POS ambiguity in the language process-
ing pipeline. In particular, since POS tags are im-
portant features for the supertagger, we investigate
how supertagging accuracy can be improved by
not prematurely committing to a POS tag decision.
Our results first demonstrate that a surprising in-
697
crease in POS tagging accuracy can be achieved
with only a tiny increase in ambiguity; and second
that maintaining some POS ambiguity can signifi-
cantly improve the accuracy of the supertagger.
The parser uses the CCG lexical categories to
build syntactic structure, and the POS tags are
used by the supertagger and parser as part of their
statisical models. We show that using a multi-
tagger for supertagging results in an effective pre-
processor for CCG parsing, and that using a multi-
tagger for POS tagging results in more accurate
CCG supertagging.
2 Maximum Entropy Tagging
The tagger uses conditional probabilities of the
form P (y|x) where y is a tag and x is a local
context containing y. The conditional probabili-
ties have the following log-linear form:
P (y|x) =
1
Z(x)
e
?
i
?ifi(x,y) (1)
where Z(x) is a normalisation constant which en-
sures a proper probability distribution for each
context x.
The feature functions fi(x, y) are binary-
valued, returning either 0 or 1 depending on the
tag y and the value of a particular contextual pred-
icate given the context x. Contextual predicates
identify elements of the context which might be
useful for predicting the tag. For example, the fol-
lowing feature returns 1 if the current word is the
and the tag is DT; otherwise it returns 0:
fi(x, y) =
{
1 if word(x) = the & y = DT
0 otherwise
(2)
word(x) = the is an example of a contextual
predicate. The POS tagger uses the same con-
textual predicates as Ratnaparkhi (1996); the su-
pertagger adds contextual predicates correspond-
ing to POS tags and bigram combinations of POS
tags (Curran and Clark, 2003).
Each feature fi has an associated weight ?i
which is determined during training. The training
process aims to maximise the entropy of the model
subject to the constraints that the expectation of
each feature according to the model matches the
empirical expectation from the training data. This
can be also thought of in terms of maximum like-
lihood estimation (MLE) for a log-linear model
(Della Pietra et al, 1997). We use the L-BFGS op-
timisation algorithm (Nocedal and Wright, 1999;
Malouf, 2002) to perform the estimation.
MLE has a tendency to overfit the training data.
We adopt the standard approach of Chen and
Rosenfeld (1999) by introducing a Gaussian prior
term to the objective function which penalises fea-
ture weights with large absolute values. A param-
eter defined in terms of the standard deviation of
the Gaussian determines the degree of smoothing.
The conditional probability of a sequence of
tags, y1, . . . , yn, given a sentence, w1, . . . , wn, is
defined as the product of the individual probabili-
ties for each tag:
P (y1, . . . , yn|w1, . . . , wn) =
n?
i=1
P (yi|xi) (3)
where xi is the context for word wi. We use the
standard approach of Viterbi decoding to find the
highest probability sequence.
2.1 Multi-tagging
Multi-tagging ? assigning one or more tags to a
word ? is used here in two ways: first, to retain
ambiguity in the CCG lexical category sequence
for the purpose of building parse structure; and
second, to retain ambiguity in the POS tag se-
quence. We retain ambiguity in the lexical cate-
gory sequence since a single-tagger is not accurate
enough to serve as a front-end to a CCG parser, and
we retain some POS ambiguity since POS tags are
used as features in the statistical models of the su-
pertagger and parser.
Charniak et al (1996) investigated multi-POS
tagging in the context of PCFG parsing. It was
found that multi-tagging provides only a minor
improvement in accuracy, with a significant loss
in efficiency; hence it was concluded that, given
the particular parser and tagger used, a single-tag
POS tagger is preferable to a multi-tagger. More
recently, Watson (2006) has revisited this question
in the context of the RASP parser (Briscoe and Car-
roll, 2002) and found that, similar to Charniak et
al. (1996), multi-tagging at the POS level results in
a small increase in parsing accuracy but at some
cost in efficiency.
For lexicalized grammars, such as CCG and
TAG, the motivation for using a multi-tagger to as-
sign the elementary structures (supertags) is more
compelling. Since the set of supertags is typ-
ically much larger than a standard POS tag set,
the tagging problem becomes much harder. In
698
fact, when using a state-of-the-art single-tagger,
the per-word accuracy for CCG supertagging is so
low (around 92%) that wide coverage, high ac-
curacy parsing becomes infeasible (Clark, 2002;
Clark and Curran, 2004a). Similar results have
been found for a highly lexicalized HPSG grammar
(Prins and van Noord, 2003), and also for TAG.
As far as we are aware, the only approach to suc-
cessfully integrate a TAG supertagger and parser is
the Lightweight Dependency Analyser of Banga-
lore (2000). Hence, in order to perform effective
full parsing with these lexicalized grammars, the
tagger front-end must be a multi-tagger (given the
current state-of-the-art).
The simplest approach to CCG supertagging is
to assign all categories to a word which the word
was seen with in the data. This leaves the parser
the task of managing the very large parse space re-
sulting from the high degree of lexical category
ambiguity (Hockenmaier and Steedman, 2002;
Hockenmaier, 2003). However, one of the orig-
inal motivations for supertagging was to signifi-
cantly reduce the syntactic ambiguity before full
parsing begins (Bangalore and Joshi, 1999). Clark
and Curran (2004a) found that performing CCG
supertagging prior to parsing can significantly in-
crease parsing efficiency with no loss in accuracy.
Our multi-tagging approach follows that of
Clark and Curran (2004a) and Charniak et al
(1996): assign all categories to a word whose
probabilities are within a factor, ?, of the proba-
bility of the most probable category for that word:
Ci = {c | P (Ci = c|S) > ? P (Ci = cmax|S)}
Ci is the set of categories assigned to the ith word;
Ci is the random variable corresponding to the cat-
egory of the ith word; cmax is the category with the
highest probability of being the category of the ith
word; and S is the sentence. One advantage of this
adaptive approach is that, when the probability of
the highest scoring category is much greater than
the rest, no extra categories will be added.
Clark and Curran (2004a) propose a simple
method for calculating P (Ci = c|S): use the
word and POS features in the local context to cal-
culate the probability and ignore the previously
assigned categories (the history). However, it is
possible to incorporate the history in the calcula-
tion of the tag probabilities. A greedy approach is
to use the locally highest probability history as a
feature, which avoids any summing over alterna-
tive histories. Alternatively, there is a well-known
dynamic programming algorithm ? the forward
backward algorithm ? which efficiently calcu-
lates P (Ci = c|S) (Charniak et al, 1996).
The multitagger uses the following conditional
probabilities:
P (yi|w1,n) =
?
y1,i?1,yi+1,n
P (yi, y1,i?1, yi+1,n|w1,n)
where xi,j = xi, . . . xj . Here yi is to be thought of
as a fixed category, whereas yj (j 6= i) varies over
the possible categories for word j. In words, the
probability of category yi, given the sentence, is
the sum of the probabilities of all sequences con-
taining yi. This sum is calculated efficiently using
the forward-backward algorithm:
P (Ci = c|S) = ?i(c)?i(c) (4)
where ?i(c) is the total probability of all the cate-
gory sub-sequences that end at position i with cat-
egory c; and ?i(c) is the total probability of all the
category sub-sequences through to the end which
start at position i with category c.
The standard description of the forward-
backward algorithm, for example Manning and
Schutze (1999), is usually given for an HMM-style
tagger. However, it is straightforward to adapt the
algorithm to the Maximum Entropy models used
here. The forward-backward algorithm we use is
similar to that for a Maximum Entropy Markov
Model (Lafferty et al, 2001).
POS tags are very informative features for the
supertagger, which suggests that using a multi-
POS tagger may benefit the supertagger (and ulti-
mately the parser). However, it is unclear whether
multi-POS tagging will be useful in this context,
since our single-tagger POS tagger is highly accu-
rate: over 97% for WSJ text (Curran and Clark,
2003). In fact, in Clark and Curran (2004b) we re-
port that using automatically assigned, as opposed
to gold-standard, POS tags as features results in a
2% loss in parsing accuracy. This suggests that re-
taining some ambiguity in the POS sequence may
be beneficial for supertagging and parsing accu-
racy. In Section 4 we show this is the case for
supertagging.
3 CCG Supertagging and Parsing
Parsing using CCG can be viewed as a two-stage
process: first assign lexical categories to the words
in the sentence, and then combine the categories
699
The WSJ is a paper that I enjoy reading
NP/N N (S [dcl ]\NP)/NP NP/N N (NP\NP)/(S [dcl ]/NP) NP (S [dcl ]\NP)/(S [ng ]\NP) (S [ng ]\NP)/NP
Figure 1: Example sentence with CCG lexical categories.
together using CCG?s combinatory rules.1 We per-
form stage one using a supertagger.
The set of lexical categories used by the su-
pertagger is obtained from CCGbank (Hocken-
maier, 2003), a corpus of CCG normal-form
derivations derived semi-automatically from the
Penn Treebank. Following our earlier work, we
apply a frequency cutoff to the training set, only
using those categories which appear at least 10
times in sections 02-21, which results in a set of
425 categories. We have shown that the resulting
set has very high coverage on unseen data (Clark
and Curran, 2004a). Figure 1 gives an example
sentence with the CCG lexical categories.
The parser is described in Clark and Curran
(2004b). It takes POS tagged sentences as input
with each word assigned a set of lexical categories.
A packed chart is used to efficiently represent
all the possible analyses for a sentence, and the
CKY chart parsing algorithm described in Steed-
man (2000) is used to build the chart. A log-linear
model is used to score the alternative analyses.
In Clark and Curran (2004a) we described a
novel approach to integrating the supertagger and
parser: start with a very restrictive supertagger set-
ting, so that only a small number of lexical cate-
gories is assigned to each word, and only assign
more categories if the parser cannot find a span-
ning analysis. This strategy results in an efficient
and accurate parser, with speeds up to 35 sen-
tences per second. Accurate supertagging at low
levels of lexical category ambiguity is therefore
particularly important when using this strategy.
We found in Clark and Curran (2004b) that a
large drop in parsing accuracy occurs if automat-
ically assigned POS tags are used throughout the
parsing process, rather than gold standard POS
tags (almost 2% F-score over labelled dependen-
cies). This is due to the drop in accuracy of the
supertagger (see Table 3) and also the fact that
the log-linear parsing model uses POS tags as fea-
tures. The large drop in parsing accuracy demon-
strates that improving the performance of POS tag-
1See Steedman (2000) for an introduction to CCG, and
see Hockenmaier (2003) for an introduction to wide-coverage
parsing using CCG.
TAGS/WORD ? WORD ACC SENT ACC
1.00 1 96.7 51.8
1.01 0.8125 97.1 55.4
1.05 0.2969 98.3 70.7
1.10 0.1172 99.0 80.9
1.20 0.0293 99.5 89.3
1.30 0.0111 99.6 91.7
1.40 0.0053 99.7 93.2
4.23 0 99.8 94.8
Table 1: POS tagging accuracy on Section 00 for
different levels of ambiguity.
gers is still an important research problem. In this
paper we aim to reduce the performance drop of
the supertagger by maintaing some POS ambiguity
through to the supertagging phase. Future work
will investigate maintaining some POS ambiguity
through to the parsing phase also.
4 Multi-tagging Experiments
We performed several sets of experiments for
POS tagging and CCG supertagging to explore the
trade-off between ambiguity and tagging accuracy.
For both POS tagging and supertagging we varied
the average number of tags assigned to each word,
to see whether it is possible to significantly in-
crease tagging accuracy with only a small increase
in ambiguity. For CCG supertagging, we also com-
pared multi-tagging approaches, with a fixed cate-
gory ambiguity of 1.4 categories per word.
All of the experiments used Section 02-21 of
CCGbank as training data, Section 00 as develop-
ment data and Section 23 as final test data. We
evaluate both per-word tag accuracy and sentence
accuracy, which is the percentage of sentences for
which every word is tagged correctly. For the
multi-tagging results we consider the word to be
tagged correctly if the correct tag appears in the
set of tags assigned to the word.
4.1 Results
Table 1 shows the results for multi-POS tagging
for different levels of ambiguity. The row corre-
sponding to 1.01 tags per word shows that adding
700
METHOD GOLD POS AUTO POS
WORD SENT WORD SENT
single 92.6 36.8 91.5 32.7
noseq 96.2 51.9 95.2 46.1
best hist 97.2 63.8 96.3 57.2
fwdbwd 97.9 72.1 96.9 64.8
Table 2: Supertagging accuracy on Section 00 us-
ing different approaches with multi-tagger ambi-
guity fixed at 1.4 categories per word.
TAGS/ GOLD POS AUTO POS
WORD ? WORD SENT WORD SENT
1.0 1 92.6 36.8 91.5 32.7
1.2 0.1201 96.8 63.4 95.8 56.5
1.4 0.0337 97.9 72.1 96.9 64.8
1.6 0.0142 98.3 76.4 97.5 69.3
1.8 0.0074 98.4 78.3 97.7 71.0
2.0 0.0048 98.5 79.4 97.9 72.5
2.5 0.0019 98.7 80.6 98.1 74.3
3.0 0.0009 98.7 81.4 98.3 75.6
12.5 0 98.9 82.3 98.8 80.1
Table 3: Supertagging accuracy on Section 00 for
different levels of ambiguity.
even a tiny amount of ambiguity (1 extra tag in ev-
ery 100 words) gives a reasonable improvement,
whilst adding 1 tag in 20 words, or approximately
one extra tag per sentence on the WSJ, gives a sig-
nificant boost of 1.6% word accuracy and almost
20% sentence accuracy.
The bottom row of Table 1 gives an upper bound
on accuracy if the maximum ambiguity is allowed.
This involves setting the ? value to 0, so all feasi-
ble tags are assigned. Note that the performance
gain is only 1.6% in sentence accuracy, compared
with the previous row, at the cost of a large in-
crease in ambiguity.
Our first set of CCG supertagging experiments
compared the performance of several approaches.
In Table 2 we give the accuracies when using gold
standard POS tags, and also POS tags automatically
assigned by our POS tagger described above. Since
POS tags are important features for the supertagger
maximum entropy model, erroneous tags have a
significant impact on supertagging accuracy.
The singlemethod is the single-tagger supertag-
ger, which at 91.5% per-word accuracy is too inac-
curate for use with the CCG parser. The remaining
rows in the table give multi-tagger results for a cat-
egory ambiguity of 1.4 categories per word. The
noseq method, which performs significantly better
than single, does not take into account the previ-
ously assigned categories. The best hist method
gains roughly another 1% in accuracy over noseq
by taking the greedy approach of using only the
two most probable previously assigned categories.
Finally, the full forward-backward approach de-
scribed in Section 2.1 gains roughly another 0.6%
by considering all possible category histories. We
see the largest jump in accuracy just by returning
multiple categories. The other more modest gains
come from producing progressively better models
of the category sequence.
The final set of supertagging experiments in Ta-
ble 3 demonstrates the trade-off between ambigu-
ity and accuracy. Note that the ambiguity levels
need to be much higher to produce similar perfor-
mance to the POS tagger and that the upper bound
case (? = 0) has a very high average ambiguity.
This is to be expected given the much larger CCG
tag set.
5 Tag uncertainty thoughout the pipeline
Tables 2 and 3 show that supertagger accuracy
when using gold-standard POS tags is typically
1% higher than when using automatically assigned
POS tags. Clearly, correct POS tags are important
features for the supertagger.
Errors made by the supertagger can multiply
out when incorrect lexical categories are passed
to the parser, so a 1% increase in lexical category
error can become much more significant in the
parser evaluation. For example, when using the
dependency-based evaluation in Clark and Curran
(2004b), getting the lexical category wrong for a
ditransitive verb automatically leads to three de-
pendencies in the output being incorrect.
We have shown that multi-tagging can signif-
icantly increase the accuracy of the POS tagger
with only a small increase in ambiguity. What
we would like to do is maintain some degree of
POS tag ambiguity and pass multiple POS tags
through to the supertagging stage (and eventually
the parser). There are several ways to encode mul-
tiple POS tags as features. The simplest approach
is to treat all of the POS tags as binary features,
but this does not take into account the uncertainty
in each of the alternative tags. What we need is a
way of incorporating probability information into
the Maximum Entropy supertagger.
701
6 Real-values in ME models
Maximum Entropy (ME) models, in the NLP lit-
erature, are typically defined with binary features,
although they do allow real-valued features. The
only constraint comes from the optimisation algo-
rithm; for example, GIS only allows non-negative
values. Real-valued features are commonly used
with other machine learning algorithms.
Binary features suffer from certain limitations
of the representation, which make them unsuitable
for modelling some properties. For example, POS
taggers have difficulty determining if capitalised,
sentence initial words are proper nouns. A useful
way to model this property is to determine the ra-
tio of capitalised and non-capitalised instances of
a particular word in a large corpus and use a real-
valued feature which encodes this ratio (Vadas and
Curran, 2005). The only way to include this fea-
ture in a binary representation is to discretize (or
bin) the feature values. For this type of feature,
choosing appropriate bins is difficult and it may be
hard to find a discretization scheme that performs
optimally.
Another problem with discretizing feature val-
ues is that it imposes artificial boundaries to define
the bins. For the example above, we may choose
the bins 0 ? x < 1 and 1 ? x < 2, which sepa-
rate the values 0.99 and 1.01 even though they are
close in value. At the same time, the model does
not distinguish between 0.01 and 0.99 even though
they are much further apart.
Further, if we have not seen cases for the bin
2 ? x < 3, then the discretized model has no evi-
dence to determine the contribution of this feature.
But for the real-valued model, evidence support-
ing 1 ? x < 2 and 3 ? x < 4 provides evidence
for the missing bin. Thus the real-valued model
generalises more effectively.
One issue that is not addressed here is the inter-
action between the Gaussian smoothing parameter
and real-valued features. Using the same smooth-
ing parameter for real-valued features with vastly
different distributions is unlikely to be optimal.
However, for these experiments we have used the
same value for the smoothing parameter on all
real-valued features. This is the same value we
have used for the binary features.
7 Multi-POS Supertagging Experiments
We have experimented with four different ap-
proaches to passing multiple POS tags as features
through to the supertagger. For the later exper-
iments, this required the existing binary-valued
framework to be extended to support real values.
The level of POS tag ambiguity was varied be-
tween 1.05 and 1.3 POS tags per word on average.
These results are shown in Table 4.
The first approach is to treat the multiple POS
tags as binary features (bin). This simply involves
adding the multiple POS tags for each word in
both the training and test data. Every assigned
POS tag is treated as a separate feature and con-
sidered equally important regardless of its uncer-
tainty. Here we see a minor increase in perfor-
mance over the original supertagger at the lower
levels of POS ambiguity. However, as the POS
ambiguity is increased, the performance of the
binary-valued features decreases and is eventually
worse than the original supertagger. This is be-
cause at the lowest levels of ambiguity the extra
POS tags can be treated as being of similar reli-
ability. However, at higher levels of ambiguity
many POS tags are added which are unreliable and
should not be trusted equally.
The second approach (split) uses real-valued
features to model some degree of uncertainty in
the POS tags, dividing the POS tag probability mass
evenly among the alternatives. This has the ef-
fect of giving smaller feature values to tags where
many alternative tags have been assigned. This
produces similar results to the binary-valued fea-
tures, again performing best at low levels of ambi-
guity.
The third approach (invrank) is to use the in-
verse rank of each POS tag as a real-valued feature.
The inverse rank is the reciprocal of the tag?s rank
ordered by decreasing probability. This method
assumes the POS tagger correctly orders the alter-
native tags, but does not rely on the probability
assigned to each tag. Overall, invrank performs
worse than split.
The final and best approach is to use the prob-
abilities assigned to each alternative tag as real-
valued features:
fi(x, y) =
{
p(POS(x) = NN) if y = NP
0 otherwise
(5)
This model gives the best performance at 1.1 POS
tags per-word average ambiguity. Note that, even
when using the probabilities as features, only a
small amount of additional POS ambiguity is re-
quired to significantly improve performance.
702
METHOD POS AMB WORD SENT
orig 1.00 96.9 64.8
bin 1.05 97.3 67.7
1.10 97.3 66.3
1.20 97.0 63.5
1.30 96.8 62.1
split 1.05 97.4 68.5
1.10 97.4 67.9
1.20 97.3 67.0
1.30 97.2 65.1
prob 1.05 97.5 68.7
1.10 97.5 69.1
1.20 97.5 68.7
1.30 97.5 68.7
invrank 1.05 97.3 68.0
1.10 97.4 68.0
1.20 97.3 67.1
1.30 97.3 67.1
gold - 97.9 72.1
Table 4: Multi-POS supertagging on Section 00
with different levels of POS ambiguity and using
different approaches to POS feature encoding.
Table 5 shows our best performance figures for
the multi-POS supertagger, against the previously
described method using both gold standard and au-
tomatically assigned POS tags.
Table 6 uses the Section 23 test data to
demonstrate the improvement in supertagging
when moving from single-tagging (single) to sim-
ple multi-tagging (noseq); from simple multi-
tagging to the full forward-backward algorithm
(fwdbwd); and finally when using the probabilities
of multiply-assigned POS tags as features (MULTI-
POS column). All of these multi-tagging experi-
ments use an ambiguity level of 1.4 categories per
word and the last result uses POS tag ambiguity of
1.1 tags per word.
8 Conclusion
The NLP community may consider POS tagging to
be a solved problem. In this paper, we have sug-
gested two reasons why this is not the case. First,
tagging for lexicalized-grammar formalisms, such
as CCG and TAG, is far from solved. Second,
even modest improvements in POS tagging accu-
racy can have a large impact on the performance of
downstream components in a language processing
pipeline.
TAGS/ AUTO POS MULTI POS GOLD POS
WORD WORD SENT WORD SENT WORD SENT
1.0 91.5 32.7 91.9 34.3 92.6 36.8
1.2 95.8 56.5 96.3 59.2 96.8 63.4
1.4 96.9 64.8 97.5 67.0 97.9 72.1
1.6 97.5 69.3 97.9 73.3 98.3 76.4
1.8 97.7 71.0 98.2 76.1 98.4 78.3
2.0 97.9 72.5 98.4 77.4 98.5 79.4
2.5 98.1 74.3 98.5 78.7 98.7 80.6
3.0 98.3 75.6 98.6 79.7 98.7 81.4
Table 5: Best multi-POS supertagging accuracy on
Section 00 using POS ambiguity of 1.1 and the
probability real-valued features.
METHOD AUTO POS MULTI POS GOLD POS
single 92.0 - 93.3
noseq 95.4 - 96.6
fwdbwd 97.1 97.7 98.2
Table 6: Final supertagging results on Section 23.
We have developed a novel approach to main-
taining tag ambiguity in language processing
pipelines which avoids premature ambiguity res-
olution. The tag ambiguity is maintained by using
the forward-backward algorithm to calculate indi-
vidual tag probabilities. These probabilities can
then be used to select multiple tags and can also
be encoded as real-valued features in subsequent
statistical models.
With this new approach we have increased POS
tagging accuracy significantly with only a tiny am-
biguity penalty and also significantly improved on
previous CCG supertagging results. Finally, us-
ing POS tag probabilities as real-valued features in
the supertagging model, we demonstrated perfor-
mance close to that obtained with gold-standard
POS tags. This will significantly improve the ro-
bustness of the parser on unseen text.
In future work we will investigate maintaining
tag ambiguity further down the language process-
ing pipeline and exploiting the uncertainty from
previous stages. In particular, we will incorporate
real-valued POS tag and lexical category features
in the statistical parsing model. Another possibil-
ity is to investigate whether similar techniques can
improve other tagging tasks, such as Named Entity
Recognition.
This work can be seen as part of the larger
goal of maintaining ambiguity and exploiting un-
703
certainty throughout language processing systems
(Roth and Yih, 2004), which is important for cop-
ing with the compounding of errors that is a sig-
nificant problem in language processing pipelines.
Acknowledgements
We would like to thank the anonymous reviewers
for their helpful feedback. This work has been
supported by the Australian Research Council un-
der Discovery Project DP0453131.
References
Srinivas Bangalore and Aravind Joshi. 1999. Supertagging:
An approach to almost parsing. Computational Linguis-
tics, 25(2):237?265.
Srinivas Bangalore. 2000. A lightweight dependency anal-
yser for partial parsing. Natural Language Engineering,
6(2):113?138.
Ted Briscoe and John Carroll. 2002. Robust accurate statis-
tical annotation of general tex. In Proceedings of the 3rd
LREC Conference, pages 1499?1504, Las Palmas, Gran
Canaria.
Eugene Charniak, Glenn Carroll, John Adcock, Anthony
Cassandra, Yoshihiko Gotoh, Jeremy Katz, Michael
Littman, and John McCann. 1996. Taggers for parsers.
Artificial Intelligence, 85:45?57.
Stanley Chen and Ronald Rosenfeld. 1999. A Gaussian prior
for smoothing maximum entropy models. Technical re-
port, Carnegie Mellon University, Pittsburgh, PA.
Stephen Clark and James R. Curran. 2004a. The impor-
tance of supertagging for wide-coverage CCG parsing.
In Proceedings of COLING-04, pages 282?288, Geneva,
Switzerland.
Stephen Clark and James R. Curran. 2004b. Parsing the
WSJ using CCG and log-linear models. In Proceedings of
the 42nd Meeting of the ACL, pages 104?111, Barcelona,
Spain.
Stephen Clark. 2002. A supertagger for Combinatory Cate-
gorial Grammar. In Proceedings of the TAG+ Workshop,
pages 19?24, Venice, Italy.
Michael Collins. 2002. Discriminative training methods for
Hidden Markov Models: Theory and experiments with
perceptron algorithms. In Proceedings of the EMNLP
Conference, pages 1?8, Philadelphia, PA.
James R. Curran and Stephen Clark. 2003. Investigating GIS
and smoothing for maximum entropy taggers. In Proceed-
ings of the 10th Meeting of the EACL, pages 91?98, Bu-
dapest, Hungary.
Stephen Della Pietra, Vincent Della Pietra, and John Laf-
ferty. 1997. Inducing features of random fields. IEEE
Transactions Pattern Analysis and Machine Intelligence,
19(4):380?393.
Julia Hockenmaier and Mark Steedman. 2002. Generative
models for statistical parsing with Combinatory Categorial
Grammar. In Proceedings of the 40th Meeting of the ACL,
pages 335?342, Philadelphia, PA.
Julia Hockenmaier. 2003. Data and Models for Statistical
Parsing with Combinatory Categorial Grammar. Ph.D.
thesis, University of Edinburgh.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models for
segmenting and labeling sequence data. In Proceedings of
the 18th International Conference on Machine Learning,
pages 282?289, Williams College, MA.
Robert Malouf. 2002. A comparison of algorithms for max-
imum entropy parameter estimation. In Proceedings of
the Sixth Workshop on Natural Language Learning, pages
49?55, Taipei, Taiwan.
Christopher Manning and Hinrich Schutze. 1999. Foun-
dations of Statistical Natural Language Processing. The
MIT Press, Cambridge, Massachusetts.
Jorge Nocedal and Stephen J. Wright. 1999. Numerical Op-
timization. Springer, New York, USA.
Robbert Prins and Gertjan van Noord. 2003. Reinforcing
parser preferences through tagging. Traitement Automa-
tique des Langues, 44(3):121?139.
Adwait Ratnaparkhi. 1996. A maximum entropy part-of-
speech tagger. In Proceedings of the EMNLP Conference,
pages 133?142, Philadelphia, PA.
D. Roth and W. Yih. 2004. A linear programming for-
mulation for global inference in natural language tasks.
In Hwee Tou Ng and Ellen Riloff, editors, Proc. of the
Annual Conference on Computational Natural Language
Learning (CoNLL), pages 1?8. Association for Computa-
tional Linguistics.
Mark Steedman. 2000. The Syntactic Process. The MIT
Press, Cambridge, MA.
Kristina Toutanova, Dan Klein, Christopher Manning, and
Yoram Singer. 2003. Feature-rich part-of-speech tag-
ging with a cyclic dependency network. In Proceedings
of the HLT/NAACL conference, pages 252?259, Edmon-
ton, Canada.
David Vadas and James R. Curran. 2005. Tagging un-
known words with raw text features. In Proceedings of the
Australasian Language Technology Workshop 2005, pages
32?39, Sydney, Australia.
Rebecca Watson. 2006. Part-of-speech tagging models for
parsing. In Proceedings of the Computaional Linguistics
in the UK Conference (CLUK-06), Open University, Mil-
ton Keynes, UK.
704
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 248?255,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Formalism-Independent Parser Evaluation with CCG and DepBank
Stephen Clark
Oxford University Computing Laboratory
Wolfson Building, Parks Road
Oxford, OX1 3QD, UK
stephen.clark@comlab.ox.ac.uk
James R. Curran
School of Information Technologies
University of Sydney
NSW 2006, Australia
james@it.usyd.edu.au
Abstract
A key question facing the parsing commu-
nity is how to compare parsers which use
different grammar formalisms and produce
different output. Evaluating a parser on the
same resource used to create it can lead
to non-comparable accuracy scores and an
over-optimistic view of parser performance.
In this paper we evaluate a CCG parser on
DepBank, and demonstrate the difficulties
in converting the parser output into Dep-
Bank grammatical relations. In addition we
present a method for measuring the effec-
tiveness of the conversion, which provides
an upper bound on parsing accuracy. The
CCG parser obtains an F-score of 81.9%
on labelled dependencies, against an upper
bound of 84.8%. We compare the CCG
parser against the RASP parser, outperform-
ing RASP by over 5% overall and on the ma-
jority of dependency types.
1 Introduction
Parsers have been developed for a variety of gram-
mar formalisms, for example HPSG (Toutanova et
al., 2002; Malouf and van Noord, 2004), LFG (Ka-
plan et al, 2004; Cahill et al, 2004), TAG (Sarkar
and Joshi, 2003), CCG (Hockenmaier and Steed-
man, 2002; Clark and Curran, 2004b), and variants
of phrase-structure grammar (Briscoe et al, 2006),
including the phrase-structure grammar implicit in
the Penn Treebank (Collins, 2003; Charniak, 2000).
Different parsers produce different output, for ex-
ample phrase structure trees (Collins, 2003), depen-
dency trees (Nivre and Scholz, 2004), grammati-
cal relations (Briscoe et al, 2006), and formalism-
specific dependencies (Clark and Curran, 2004b).
This variety of formalisms and output creates a chal-
lenge for parser evaluation.
The majority of parser evaluations have used test
sets drawn from the same resource used to develop
the parser. This allows the many parsers based on
the Penn Treebank, for example, to be meaningfully
compared. However, there are two drawbacks to this
approach. First, parser evaluations using different
resources cannot be compared; for example, the Par-
seval scores obtained by Penn Treebank parsers can-
not be compared with the dependency F-scores ob-
tained by evaluating on the Parc Dependency Bank.
Second, using the same resource for development
and testing can lead to an over-optimistic view of
parser performance.
In this paper we evaluate a CCG parser (Clark
and Curran, 2004b) on the Briscoe and Carroll ver-
sion of DepBank (Briscoe and Carroll, 2006). The
CCG parser produces head-dependency relations, so
evaluating the parser should simply be a matter of
converting the CCG dependencies into those in Dep-
Bank. Such conversions have been performed for
other parsers, including parsers producing phrase
structure output (Kaplan et al, 2004; Preiss, 2003).
However, we found that performing such a conver-
sion is a time-consuming and non-trivial task.
The contributions of this paper are as follows.
First, we demonstrate the considerable difficulties
associated with formalism-independent parser eval-
uation, highlighting the problems in converting the
248
output of a parser from one representation to an-
other. Second, we develop a method for measur-
ing how effective the conversion process is, which
also provides an upper bound for the performance of
the parser, given the conversion process being used;
this method can be adapted by other researchers
to strengthen their own parser comparisons. And
third, we provide the first evaluation of a wide-
coverage CCG parser outside of CCGbank, obtaining
impressive results on DepBank and outperforming
the RASP parser (Briscoe et al, 2006) by over 5%
overall and on the majority of dependency types.
2 Previous Work
The most common form of parser evaluation is to ap-
ply the Parseval metrics to phrase-structure parsers
based on the Penn Treebank, and the highest re-
ported scores are now over 90% (Bod, 2003; Char-
niak and Johnson, 2005). However, it is unclear
whether these high scores accurately reflect the per-
formance of parsers in applications. It has been ar-
gued that the Parseval metrics are too forgiving and
that phrase structure is not the ideal representation
for a gold standard (Carroll et al, 1998). Also, us-
ing the same resource for training and testing may
result in the parser learning systematic errors which
are present in both the training and testing mate-
rial. An example of this is from CCGbank (Hock-
enmaier, 2003), where all modifiers in noun-noun
compound constructions modify the final noun (be-
cause the Penn Treebank, from which CCGbank is
derived, does not contain the necessary information
to obtain the correct bracketing). Thus there are non-
negligible, systematic errors in both the training and
testing material, and the CCG parsers are being re-
warded for following particular mistakes.
There are parser evaluation suites which have
been designed to be formalism-independent and
which have been carefully and manually corrected.
Carroll et al (1998) describe such a suite, consisting
of sentences taken from the Susanne corpus, anno-
tated with Grammatical Relations (GRs) which spec-
ify the syntactic relation between a head and depen-
dent. Thus all that is required to use such a scheme,
in theory, is that the parser being evaluated is able
to identify heads. A similar resource ? the Parc
Dependency Bank (DepBank) (King et al, 2003)
? has been created using sentences from the Penn
Treebank. Briscoe and Carroll (2006) reannotated
this resource using their GRs scheme, and used it to
evaluate the RASP parser.
Kaplan et al (2004) compare the Collins (2003)
parser with the Parc LFG parser by mapping LFG F-
structures and Penn Treebank parses into DepBank
dependencies, claiming that the LFG parser is con-
siderably more accurate with only a slight reduc-
tion in speed. Preiss (2003) compares the parsers of
Collins (2003) and Charniak (2000), the GR finder
of Buchholz et al (1999), and the RASP parser, us-
ing the Carroll et al (1998) gold-standard. The Penn
Treebank trees of the Collins and Charniak parsers,
and the GRs of the Buchholz parser, are mapped into
the required GRs, with the result that the GR finder
of Buchholz is the most accurate.
The major weakness of these evaluations is that
there is no measure of the difficultly of the conver-
sion process for each of the parsers. Kaplan et al
(2004) clearly invested considerable time and ex-
pertise in mapping the output of the Collins parser
into the DepBank dependencies, but they also note
that ?This conversion was relatively straightforward
for LFG structures . . . However, a certain amount of
skill and intuition was required to provide a fair con-
version of the Collins trees?. Without some measure
of the difficulty ? and effectiveness ? of the con-
version, there remains a suspicion that the Collins
parser is being unfairly penalised.
One way of providing such a measure is to con-
vert the original gold standard on which the parser
is based and evaluate that against the new gold stan-
dard (assuming the two resources are based on the
same corpus). In the case of Kaplan et al (2004), the
testing procedure would include running their con-
version process on Section 23 of the Penn Treebank
and evaluating the output against DepBank. As well
as providing some measure of the effectiveness of
the conversion, this method would also provide an
upper bound for the Collins parser, giving the score
that a perfect Penn Treebank parser would obtain on
DepBank (given the conversion process).
We perform such an evaluation for the CCG parser,
with the surprising result that the upper bound on
DepBank is only 84.8%, despite the considerable ef-
fort invested in developing the conversion process.
249
3 The CCG Parser
Clark and Curran (2004b) describes the CCG parser
used for the evaluation. The grammar used by the
parser is extracted from CCGbank, a CCG version of
the Penn Treebank (Hockenmaier, 2003). The gram-
mar consists of 425 lexical categories ? expressing
subcategorisation information ? plus a small num-
ber of combinatory rules which combine the cate-
gories (Steedman, 2000). A supertagger first assigns
lexical categories to the words in a sentence, which
are then combined by the parser using the combi-
natory rules and the CKY algorithm. A log-linear
model scores the alternative parses. We use the
normal-form model, which assigns probabilities to
single derivations based on the normal-form deriva-
tions in CCGbank. The features in the model are
defined over local parts of the derivation and include
word-word dependencies. A packed chart represen-
tation allows efficient decoding, with the Viterbi al-
gorithm finding the most probable derivation.
The parser outputs predicate-argument dependen-
cies defined in terms of CCG lexical categories.
More formally, a CCG predicate-argument depen-
dency is a 5-tuple: ?hf , f, s, ha, l?, where hf is the
lexical item of the lexical category expressing the
dependency relation; f is the lexical category; s is
the argument slot; ha is the head word of the ar-
gument; and l encodes whether the dependency is
long-range. For example, the dependency encoding
company as the object of bought (as in IBM bought
the company) is represented as follows:
?bought, (S\NP1 )/NP2 , 2, company, ?? (1)
The lexical category (S\NP1 )/NP2 is the cate-
gory of a transitive verb, with the first argument slot
corresponding to the subject, and the second argu-
ment slot corresponding to the direct object. The
final field indicates the nature of any long-range de-
pendency; in (1) the dependency is local.
The predicate-argument dependencies ? includ-
ing long-range dependencies ? are encoded in the
lexicon by adding head and dependency annota-
tion to the lexical categories. For example, the
expanded category for the control verb persuade
is (((S [dcl]persuade\NP 1)/(S [to]2\NP X))/NP X,3). Nu-
merical subscripts on the argument categories rep-
resent dependency relations; the head of the final
declarative sentence is persuade; and the head of the
infinitival complement?s subject is identified with
the head of the object, using the variable X, as in
standard unification-based accounts of control.
Previous evaluations of CCG parsers have used the
predicate-argument dependencies from CCGbank as
a test set (Hockenmaier and Steedman, 2002; Clark
and Curran, 2004b), with impressive results of over
84% F-score on labelled dependencies. In this paper
we reinforce the earlier results with the first evalua-
tion of a CCG parser outside of CCGbank.
4 Dependency Conversion to DepBank
For the gold standard we chose the version of Dep-
Bank reannotated by Briscoe and Carroll (2006),
consisting of 700 sentences from Section 23 of the
Penn Treebank. The B&C scheme is similar to the
original DepBank scheme (King et al, 2003), but
overall contains less grammatical detail; Briscoe and
Carroll (2006) describes the differences. We chose
this resource for the following reasons: it is pub-
licly available, allowing other researchers to com-
pare against our results; the GRs making up the an-
notation share some similarities with the predicate-
argument dependencies output by the CCG parser;
and we can directly compare our parser against a
non-CCG parser, namely the RASP parser. We chose
not to use the corpus based on the Susanne corpus
(Carroll et al, 1998) because the GRs are less like
the CCG dependencies; the corpus is not based on
the Penn Treebank, making comparison more diffi-
cult because of tokenisation differences, for exam-
ple; and the latest results for RASP are on DepBank.
The GRs are described in Briscoe and Carroll
(2006) and Briscoe et al (2006). Table 1 lists the
GRs used in the evaluation. As an example, the sen-
tence The parent sold Imperial produces three GRs:
(det parent The), (ncsubj sold parent ) and
(dobj sold Imperial). Note that some GRs ? in
this example ncsubj ? have a subtype slot, giving
extra information. The subtype slot for ncsubj is
used to indicate passive subjects, with the null value
? ? for active subjects and obj for passive subjects.
Other subtype slots are discussed in Section 4.2.
The CCG dependencies were transformed into
GRs in two stages. The first stage was to create
a mapping between the CCG dependencies and the
250
GR description
conj coordinator
aux auxiliary
det determiner
ncmod non-clausal modifier
xmod unsaturated predicative modifier
cmod saturated clausal modifier
pmod PP modifier with a PP complement
ncsubj non-clausal subject
xsubj unsaturated predicative subject
csubj saturated clausal subject
dobj direct object
obj2 second object
iobj indirect object
pcomp PP which is a PP complement
xcomp unsaturated VP complement
ccomp saturated clausal complement
ta textual adjunct delimited by punctuation
Table 1: GRs in B&C?s annotation of DepBank
GRs. This involved mapping each argument slot in
the 425 lexical categories in the CCG lexicon onto
a GR. In the second stage, the GRs created from the
parser output were post-processed to correct some of
the obvious remaining differences between the CCG
and GR representations.
In the process of performing the transformation
we encountered a methodological problem: with-
out looking at examples it was difficult to create
the mapping and impossible to know whether the
two representations were converging. Briscoe et al
(2006) split the 700 sentences in DepBank into a test
and development set, but the latter only consists of
140 sentences which was not enough to reliably cre-
ate the transformation. There are some development
files in the RASP release which provide examples of
the GRs, which were used when possible, but these
only cover a subset of the CCG lexical categories.
Our solution to this problem was to convert the
gold standard dependencies from CCGbank into
GRs and use these to develop the transformation. So
we did inspect the annotation in DepBank, and com-
pared it to the transformed CCG dependencies, but
only the gold-standard CCG dependencies. Thus the
parser output was never used during this process.
We also ensured that the dependency mapping and
the post processing are general to the GRs scheme
and not specific to the test set or parser.
4.1 Mapping the CCG dependencies to GRs
Table 2 gives some examples of the mapping; %l in-
dicates the word associated with the lexical category
CCG lexical category slot GR
(S [dcl ]\NP1 )/NP2 1 (ncsubj %l %f )
(S [dcl ]\NP1 )/NP2 2 (dobj %l %f)
(S\NP)/(S\NP)1 1 (ncmod %f %l)
(NP\NP1 )/NP2 1 (ncmod %f %l)
(NP\NP1 )/NP2 2 (dobj %l %f)
NP [nb]/N1 1 (det %f %l)
(NP\NP1 )/(S [pss]\NP)2 1 (xmod %f %l)
(NP\NP1 )/(S [pss]\NP)2 2 (xcomp %l %f)
((S\NP)\(S\NP)1 )/S [dcl ]2 1 (cmod %f %l)
((S\NP)\(S\NP)1 )/S [dcl ]2 2 (ccomp %l %f)
((S [dcl ]\NP1 )/NP2 )/NP3 2 (obj2 %l %f)
(S [dcl ]\NP1 )/(S [b]\NP)2 2 (aux %f %l)
Table 2: Examples of the dependency mapping
and %f is the head of the constituent filling the argu-
ment slot. Note that the order of %l and %f varies ac-
cording to whether the GR represents a complement
or modifier, in line with the Briscoe and Carroll an-
notation. For many of the CCG dependencies, the
mapping into GRs is straightforward. For example,
the first two rows of Table 2 show the mapping for
the transitive verb category (S [dcl ]\NP1 )/NP2 : ar-
gument slot 1 is a non-clausal subject and argument
slot 2 is a direct object.
Creating the dependency transformation is more
difficult than these examples suggest. The first prob-
lem is that the mapping from CCG dependencies to
GRs is many-to-many. For example, the transitive
verb category (S [dcl ]\NP)/NP applies to the cop-
ula in sentences like Imperial Corp. is the parent
of Imperial Savings & Loan. With the default anno-
tation, the relation between is and parent would be
dobj, whereas in DepBank the argument of the cop-
ula is analysed as an xcomp. Table 3 gives some ex-
amples of how we attempt to deal with this problem.
The constraint in the first example means that, when-
ever the word associated with the transitive verb cat-
egory is a form of be, the second argument is xcomp,
otherwise the default case applies (in this case dobj).
There are a number of categories with similar con-
straints, checking whether the word associated with
the category is a form of be.
The second type of constraint, shown in the third
line of the table, checks the lexical category of the
word filling the argument slot. In this example, if the
lexical category of the preposition is PP/NP , then
the second argument of (S [dcl ]\NP)/PP maps to
iobj; thus in The loss stems from several fac-
tors the relation between the verb and preposition
is (iobj stems from). If the lexical category of
251
CCG lexical category slot GR constraint example
(S [dcl ]\NP1 )/NP2 2 (xcomp %l %f) word=be The parent is Imperial
(dobj %l %f) The parent sold Imperial
(S [dcl ]\NP1 )/PP2 2 (iobj %l %f) cat=PP/NP The loss stems from several factors
(xcomp %l %f) cat=PP/(S [ng ]\NP) The future depends on building ties
(S [dcl ]\NP1 )/(S [to]\NP)2 2 (xcomp %f %l %k) cat=(S [to]\NP)/(S [b]\NP) wants to wean itself away from
Table 3: Examples of the many-to-many nature of the CCG dependency to GRs mapping, and a ternary GR
the preposition is PP/(S [ng ]\NP), then the GR
is xcomp; thus in The future depends on building
ties the relation between the verb and preposition
is (xcomp depends on). There are a number of
CCG dependencies with similar constraints, many of
them covering the iobj/xcomp distinction.
The second difficulty is that not all the GRs are bi-
nary relations, whereas the CCG dependencies are all
binary. The primary example of this is to-infinitival
constructions. For example, in the sentence The
company wants to wean itself away from expensive
gimmicks, the CCG parser produces two dependen-
cies relating wants, to and wean, whereas there is
only one GR: (xcomp to wants wean). The fi-
nal row of Table 3 gives an example. We im-
plement this constraint by introducing a %k vari-
able into the GR template which denotes the ar-
gument of the category in the constraint column
(which, as before, is the lexical category of the
word filling the argument slot). In the example, the
current category is (S [dcl ]\NP1 )/(S [to]\NP)2 ,
which is associated with wants; this combines with
(S [to]\NP)/(S [b]\NP), associated with to; and
the argument of (S [to]\NP)/(S [b]\NP) is wean.
The %k variable allows us to look beyond the argu-
ments of the current category when creating the GRs.
A further difficulty is that the head passing con-
ventions differ between DepBank and CCGbank. By
head passing we mean the mechanism which de-
termines the heads of constituents and the mecha-
nism by which words become arguments of long-
range dependencies. For example, in the sentence
The group said it would consider withholding roy-
alty payments, the DepBank and CCGbank annota-
tions create a dependency between said and the fol-
lowing clause. However, in DepBank the relation
is between said and consider, whereas in CCGbank
the relation is between said and would. We fixed this
problem by defining the head of would consider to
be consider rather than would, by changing the an-
notation of all the relevant lexical categories in the
CCG lexicon (mainly those creating aux relations).
There are more subject relations in CCGbank than
DepBank. In the previous example, CCGbank has a
subject relation between it and consider, and also it
and would, whereas DepBank only has the relation
between it and consider. In practice this means ig-
noring a number of the subject dependencies output
by the CCG parser.
Another example where the dependencies differ
is the treatment of relative pronouns. For example,
in Sen. Mitchell, who had proposed the streamlin-
ing, the subject of proposed is Mitchell in CCGbank
but who in DepBank. Again, we implemented this
change by fixing the head annotation in the lexical
categories which apply to relative pronouns.
4.2 Post processing of the GR output
To obtain some idea of whether the schemes were
converging, we performed the following oracle ex-
periment. We took the CCG derivations from
CCGbank corresponding to the sentences in Dep-
Bank, and forced the parser to produce gold-
standard derivations, outputting the newly created
GRs. Treating the DepBank GRs as a gold-standard,
and comparing these with the CCGbank GRs, gave
precision and recall scores of only 76.23% and
79.56% respectively (using the RASP evaluation
tool). Thus given the current mapping, the perfect
CCGbank parser would achieve an F-score of only
77.86% when evaluated against DepBank.
On inspecting the output, it was clear that a
number of general rules could be applied to bring
the schemes closer together, which was imple-
mented as a post-processing script. The first set
of changes deals with coordination. One sig-
nificant difference between DepBank and CCG-
bank is the treatment of coordinations as argu-
ments. Consider the example The president and
chief executive officer said the loss stems from sev-
eral factors. For both DepBank and the trans-
formed CCGbank there are two conj GRs arising
252
from the coordination: (conj and president) and
(conj and officer). The difference arises in the
subject of said: in DepBank the subject is and:
(ncsubj said and ), whereas in CCGbank there
are two subjects: (ncsubj said president ) and
(ncsubj said officer ). We deal with this dif-
ference by replacing any pairs of GRs which differ
only in their arguments, and where the arguments
are coordinated items, with a single GR containing
the coordination term as the argument.
Ampersands are a frequently occurring problem
in WSJ text. For example, the CCGbank analysis
of Standard & Poor?s index assigns the lexical cat-
egory N /N to both Standard and &, treating them
as modifiers of Poor, whereas DepBank treats & as
a coordinating term. We fixed this by creating conj
GRs between any & and the two words either side;
removing the modifier GR between the two words;
and replacing any GRs in which the words either side
of the & are arguments with a single GR in which &
is the argument.
The ta relation, which identifies text adjuncts de-
limited by punctuation, is difficult to assign cor-
rectly to the parser output. The simple punctuation
rules used by the parser do not contain enough in-
formation to distinguish between the various cases
of ta. Thus the only rule we have implemented,
which is somewhat specific to the newspaper genre,
is to replace GRs of the form (cmod say arg)
with (ta quote arg say), where say can be any
of say, said or says. This rule applies to only a small
subset of the ta cases but has high enough precision
to be worthy of inclusion.
A common source of error is the distinction be-
tween iobj and ncmod, which is not surprising given
the difficulty that human annotators have in distin-
guishing arguments and adjuncts. There are many
cases where an argument in DepBank is an adjunct
in CCGbank, and vice versa. The only change we
have made is to turn all ncmod GRs with of as the
modifier into iobj GRs (unless the ncmod is a par-
titive predeterminer). This was found to have high
precision and applies to a large number of cases.
There are some dependencies in CCGbank which
do not appear in DepBank. Examples include any
dependencies in which a punctuation mark is one of
the arguments; these were removed from the output.
We attempt to fill the subtype slot for some GRs.
The subtype slot specifies additional information
about the GR; examples include the value obj in a
passive ncsubj, indicating that the subject is an un-
derlying object; the value num in ncmod, indicating a
numerical quantity; and prt in ncmod to indicate a
verb particle. The passive case is identified as fol-
lows: any lexical category which starts S [pss]\NP
indicates a passive verb, and we also mark any verbs
POS tagged VBN and assigned the lexical category
N /N as passive. Both these rules have high preci-
sion, but still leave many of the cases in DepBank
unidentified. The numerical case is identified using
two rules: the num subtype is added if any argument
in a GR is assigned the lexical category N /N [num],
and if any of the arguments in an ncmod is POS
tagged CD. prt is added to an ncmod if the modi-
fiee has any of the verb POS tags and if the modifier
has POS tag RP.
The final columns of Table 4 show the accuracy
of the transformed gold-standard CCGbank depen-
dencies when compared with DepBank; the sim-
ple post-processing rules have increased the F-score
from 77.86% to 84.76%. This F-score is an upper
bound on the performance of the CCG parser.
5 Results
The results in Table 4 were obtained by parsing the
sentences from CCGbank corresponding to those
in the 560-sentence test set used by Briscoe et al
(2006). We used the CCGbank sentences because
these differ in some ways to the original Penn Tree-
bank sentences (there are no quotation marks in
CCGbank, for example) and the parser has been
trained on CCGbank. Even here we experienced
some unexpected difficulties, since some of the to-
kenisation is different between DepBank and CCG-
bank and there are some sentences in DepBank
which have been significantly shortened compared
to the original Penn Treebank sentences. We mod-
ified the CCGbank sentences ? and the CCGbank
analyses since these were used for the oracle ex-
periments ? to be as close to the DepBank sen-
tences as possible. All the results were obtained us-
ing the RASP evaluation scripts, with the results for
the RASP parser taken from Briscoe et al (2006).
The results for CCGbank were obtained using the
oracle method described above.
253
RASP CCG parser CCGbank
Relation Prec Rec F Prec Rec F Prec Rec F # GRs
aux 93.33 91.00 92.15 94.20 89.25 91.66 96.47 90.33 93.30 400
conj 72.39 72.27 72.33 79.73 77.98 78.84 83.07 80.27 81.65 595
ta 42.61 51.37 46.58 52.31 11.64 19.05 62.07 12.59 20.93 292
det 87.73 90.48 89.09 95.25 95.42 95.34 97.27 94.09 95.66 1 114
ncmod 75.72 69.94 72.72 75.75 79.27 77.47 78.88 80.64 79.75 3 550
xmod 53.21 46.63 49.70 43.46 52.25 47.45 56.54 60.67 58.54 178
cmod 45.95 30.36 36.56 51.50 61.31 55.98 64.77 69.09 66.86 168
pmod 30.77 33.33 32.00 0.00 0.00 0.00 0.00 0.00 0.00 12
ncsubj 79.16 67.06 72.61 83.92 75.92 79.72 88.86 78.51 83.37 1 354
xsubj 33.33 28.57 30.77 0.00 0.00 0.00 50.00 28.57 36.36 7
csubj 12.50 50.00 20.00 0.00 0.00 0.00 0.00 0.00 0.00 2
dobj 83.63 79.08 81.29 87.03 89.40 88.20 92.11 90.32 91.21 1 764
obj2 23.08 30.00 26.09 65.00 65.00 65.00 66.67 60.00 63.16 20
iobj 70.77 76.10 73.34 77.60 70.04 73.62 83.59 69.81 76.08 544
xcomp 76.88 77.69 77.28 76.68 77.69 77.18 80.00 78.49 79.24 381
ccomp 46.44 69.42 55.55 79.55 72.16 75.68 80.81 76.31 78.49 291
pcomp 72.73 66.67 69.57 0.00 0.00 0.00 0.00 0.00 0.00 24
macroaverage 62.12 63.77 62.94 65.61 63.28 64.43 71.73 65.85 68.67
microaverage 77.66 74.98 76.29 82.44 81.28 81.86 86.86 82.75 84.76
Table 4: Accuracy on DepBank. F-score is the balanced harmonic mean of precision (P ) and recall (R):
2PR/(P + R). # GRs is the number of GRs in DepBank.
The CCG parser results are based on automati-
cally assigned POS tags, using the Curran and Clark
(2003) tagger. The coverage of the parser on Dep-
Bank is 100%. For a GR in the parser output to be
correct, it has to match the gold-standard GR exactly,
including any subtype slots; however, it is possible
for a GR to be incorrect at one level but correct at
a subsuming level.1 For example, if an ncmod GR is
incorrectly labelled with xmod, but is otherwise cor-
rect, it will be correct for all levels which subsume
both ncmod and xmod, for example mod. The micro-
averaged scores are calculated by aggregating the
counts for all the relations in the hierarchy, including
the subsuming relations; the macro-averaged scores
are the mean of the individual scores for each rela-
tion (Briscoe et al, 2006).
The results show that the performance of the CCG
parser is higher than RASP overall, and also higher
on the majority of GR types (especially the more
frequent types). RASP uses an unlexicalised pars-
ing model and has not been tuned to newspaper text.
On the other hand it has had many years of develop-
ment; thus it provides a strong baseline for this test
set. The overall F-score for the CCG parser, 81.86%,
is only 3 points below that for CCGbank, which pro-
1The GRs are arranged in a hierarchy, with those in Table 1 at
the leaves; a small number of more general GRs subsume these
(Briscoe and Carroll, 2006).
vides an upper bound for the CCG parser (given the
conversion process being used).
6 Conclusion
A contribution of this paper has been to high-
light the difficulties associated with cross-formalism
parser comparison. Note that the difficulties are not
unique to CCG, and many would apply to any cross-
formalism comparison, especially with parsers using
automatically extracted grammars. Parser evalua-
tion has improved on the original Parseval measures
(Carroll et al, 1998), but the challenge remains to
develop a representation and evaluation suite which
can be easily applied to a wide variety of parsers
and formalisms. Despite the difficulties, we have
given the first evaluation of a CCG parser outside of
CCGbank, outperforming the RASP parser by over
5% overall and on the majority of dependency types.
Can the CCG parser be compared with parsers
other than RASP? Briscoe and Carroll (2006) give a
rough comparison of RASP with the Parc LFG parser
on the different versions of DepBank, obtaining sim-
ilar results overall, but they acknowledge that the re-
sults are not strictly comparable because of the dif-
ferent annotation schemes used. Comparison with
Penn Treebank parsers would be difficult because,
for many constructions, the Penn Treebank trees and
254
CCG derivations are different shapes, and reversing
the mapping Hockenmaier used to create CCGbank
would be very difficult. Hence we challenge other
parser developers to map their own parse output into
the version of DepBank used here.
One aspect of parser evaluation not covered in this
paper is efficiency. The CCG parser took only 22.6
seconds to parse the 560 sentences in DepBank, with
the accuracy given earlier. Using a cluster of 18 ma-
chines we have also parsed the entire Gigaword cor-
pus in less than five days. Hence, we conclude that
accurate, large-scale, linguistically-motivated NLP is
now practical with CCG.
Acknowledgements
We would like to thanks the anonymous review-
ers for their helpful comments. James Curran was
funded under ARC Discovery grants DP0453131
and DP0665973.
References
Rens Bod. 2003. An efficient implementation of a new DOP
model. In Proceedings of the 10th Meeting of the EACL,
pages 19?26, Budapest, Hungary.
Ted Briscoe and John Carroll. 2006. Evaluating the accuracy
of an unlexicalized statistical parser on the PARC DepBank.
In Proceedings of the Poster Session of COLING/ACL-06,
Sydney, Australia.
Ted Briscoe, John Carroll, and Rebecca Watson. 2006. The
second release of the RASP system. In Proceedings of
the Interactive Demo Session of COLING/ACL-06, Sydney,
Australia.
Sabine Buchholz, Jorn Veenstra, and Walter Daelemans. 1999.
Cascaded grammatical relation assignment. In Proceedings
of EMNLP/VLC-99, pages 239?246, University of Mary-
land, June 21-22.
A. Cahill, M. Burke, R. O?Donovan, J. van Genabith, and
A. Way. 2004. Long-distance dependency resolution in au-
tomatically acquired wide-coverage PCFG-based LFG ap-
proximations. In Proceedings of the 42nd Meeting of the
ACL, pages 320?327, Barcelona, Spain.
John Carroll, Ted Briscoe, and Antonio Sanfilippo. 1998.
Parser evaluation: a survey and a new proposal. In Proceed-
ings of the 1st LREC Conference, pages 447?454, Granada,
Spain.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-fine n-
best parsing and maxent discriminative reranking. In Pro-
ceedings of the 43rd Annual Meeting of the ACL, University
of Michigan, Ann Arbor.
Eugene Charniak. 2000. A maximum-entropy-inspired parser.
In Proceedings of the 1st Meeting of the NAACL, pages 132?
139, Seattle, WA.
Stephen Clark and James R. Curran. 2004a. The importance of
supertagging for wide-coverage CCG parsing. In Proceed-
ings of COLING-04, pages 282?288, Geneva, Switzerland.
Stephen Clark and James R. Curran. 2004b. Parsing the WSJ
using CCG and log-linear models. In Proceedings of the
42nd Meeting of the ACL, pages 104?111, Barcelona, Spain.
Michael Collins. 2003. Head-driven statistical models
for natural language parsing. Computational Linguistics,
29(4):589?637.
James R. Curran and Stephen Clark. 2003. Investigating GIS
and smoothing for maximum entropy taggers. In Proceed-
ings of the 10th Meeting of the EACL, pages 91?98, Bu-
dapest, Hungary.
Julia Hockenmaier and Mark Steedman. 2002. Generative
models for statistical parsing with Combinatory Categorial
Grammar. In Proceedings of the 40th Meeting of the ACL,
pages 335?342, Philadelphia, PA.
Julia Hockenmaier. 2003. Data and Models for Statistical
Parsing with Combinatory Categorial Grammar. Ph.D. the-
sis, University of Edinburgh.
Ron Kaplan, Stefan Riezler, Tracy H. King, John T. Maxwell
III, Alexander Vasserman, and Richard Crouch. 2004.
Speed and accuracy in shallow and deep stochastic parsing.
In Proceedings of the HLT Conference and the 4th NAACL
Meeting (HLT-NAACL?04), Boston, MA.
Tracy H. King, Richard Crouch, Stefan Riezler, Mary Dalrym-
ple, and Ronald M. Kaplan. 2003. The PARC 700 Depen-
dency Bank. In Proceedings of the LINC-03 Workshop, Bu-
dapest, Hungary.
Robert Malouf and Gertjan van Noord. 2004. Wide coverage
parsing with stochastic attribute value grammars. In Pro-
ceedings of the IJCNLP-04 Workshop: Beyond shallow anal-
yses - Formalisms and statistical modeling for deep analyses,
Hainan Island, China.
Joakim Nivre and Mario Scholz. 2004. Deterministic depen-
dency parsing of English text. In Proceedings of COLING-
2004, pages 64?70, Geneva, Switzerland.
Judita Preiss. 2003. Using grammatical relations to compare
parsers. In Proceedings of the 10th Meeting of the EACL,
pages 291?298, Budapest, Hungary.
Anoop Sarkar and Aravind Joshi. 2003. Tree-adjoining gram-
mars and its application to statistical parsing. In Rens Bod,
Remko Scha, and Khalil Sima?an, editors, Data-oriented
parsing. CSLI.
Mark Steedman. 2000. The Syntactic Process. The MIT Press,
Cambridge, MA.
Kristina Toutanova, Christopher Manning, Stuart Shieber, Dan
Flickinger, and Stephan Oepen. 2002. Parse disambiguation
for a rich HPSG grammar. In Proceedings of the First Work-
shop on Treebanks and Linguistic Theories, pages 253?263,
Sozopol, Bulgaria.
255
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 840?847,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Chinese Segmentation with a Word-Based Perceptron Algorithm
Yue Zhang and Stephen Clark
Oxford University Computing Laboratory
Wolfson Building, Parks Road
Oxford OX1 3QD, UK
{yue.zhang,stephen.clark}@comlab.ox.ac.uk
Abstract
Standard approaches to Chinese word seg-
mentation treat the problem as a tagging
task, assigning labels to the characters in
the sequence indicating whether the char-
acter marks a word boundary. Discrimina-
tively trained models based on local char-
acter features are used to make the tagging
decisions, with Viterbi decoding finding the
highest scoring segmentation. In this paper
we propose an alternative, word-based seg-
mentor, which uses features based on com-
plete words and word sequences. The gener-
alized perceptron algorithm is used for dis-
criminative training, and we use a beam-
search decoder. Closed tests on the first and
second SIGHAN bakeoffs show that our sys-
tem is competitive with the best in the litera-
ture, achieving the highest reported F-scores
for a number of corpora.
1 Introduction
Words are the basic units to process for most NLP
tasks. The problem of Chinese word segmentation
(CWS) is to find these basic units for a given sen-
tence, which is written as a continuous sequence of
characters. It is the initial step for most Chinese pro-
cessing applications.
Chinese character sequences are ambiguous, of-
ten requiring knowledge from a variety of sources
for disambiguation. Out-of-vocabulary (OOV) words
are a major source of ambiguity. For example, a
difficult case occurs when an OOV word consists
of characters which have themselves been seen as
words; here an automatic segmentor may split the
OOV word into individual single-character words.
Typical examples of unseen words include Chinese
names, translated foreign names and idioms.
The segmentation of known words can also be
ambiguous. For example, ???b? should be ??
? (here)b (flour)? in the sentence ???b?s?
5? (flour and rice are expensive here) or ?? (here)
?b (inside)? in the sentence ???b??? (it?s
cold inside here). The ambiguity can be resolved
with information about the neighboring words. In
comparison, for the sentences ?=Proceedings of the ACL 2007 Demo and Poster Sessions, pages 33?36,
Prague, June 2007. c?2007 Association for Computational Linguistics
Linguistically Motivated Large-Scale NLP with C&C and Boxer
James R. Curran
School of Information Technologies
University of Sydney
NSW 2006, Australia
james@it.usyd.edu.au
Stephen Clark
Computing Laboratory
Oxford University
Wolfson Building, Parks Road
Oxford, OX1 3QD, UK
stephen.clark@comlab.ox.ac.uk
Johan Bos
Dipartimento di Informatica
Universita` di Roma ?La Sapienza?
via Salaria 113
00198 Roma, Italy
bos@di.uniroma1.it
1 Introduction
The statistical modelling of language, together with
advances in wide-coverage grammar development,
have led to high levels of robustness and efficiency
in NLP systems and made linguistically motivated
large-scale language processing a possibility (Mat-
suzaki et al, 2007; Kaplan et al, 2004). This pa-
per describes an NLP system which is based on syn-
tactic and semantic formalisms from theoretical lin-
guistics, and which we have used to analyse the en-
tire Gigaword corpus (1 billion words) in less than
5 days using only 18 processors. This combination
of detail and speed of analysis represents a break-
through in NLP technology.
The system is built around a wide-coverage Com-
binatory Categorial Grammar (CCG) parser (Clark
and Curran, 2004b). The parser not only recovers
the local dependencies output by treebank parsers
such as Collins (2003), but also the long-range dep-
dendencies inherent in constructions such as extrac-
tion and coordination. CCG is a lexicalized gram-
mar formalism, so that each word in a sentence is
assigned an elementary syntactic structure, in CCG?s
case a lexical category expressing subcategorisation
information. Statistical tagging techniques can as-
sign lexical categories with high accuracy and low
ambiguity (Curran et al, 2006). The combination of
finite-state supertagging and highly engineered C++
leads to a parser which can analyse up to 30 sen-
tences per second on standard hardware (Clark and
Curran, 2004a).
The C&C tools also contain a number of Maxi-
mum Entropy taggers, including the CCG supertag-
ger, a POS tagger (Curran and Clark, 2003a), chun-
ker, and named entity recogniser (Curran and Clark,
2003b). The taggers are highly efficient, with pro-
cessing speeds of over 100,000 words per second.
Finally, the various components, including the
morphological analyser morpha (Minnen et al,
2001), are combined into a single program. The out-
put from this program ? a CCG derivation, POS tags,
lemmas, and named entity tags ? is used by the
module Boxer (Bos, 2005) to produce interpretable
structure in the form of Discourse Representation
Structures (DRSs).
2 The CCG Parser
The grammar used by the parser is extracted from
CCGbank, a CCG version of the Penn Treebank
(Hockenmaier, 2003). The grammar consists of 425
lexical categories, expressing subcategorisation in-
formation, plus a small number of combinatory rules
which combine the categories (Steedman, 2000). A
Maximum Entropy supertagger first assigns lexical
categories to the words in a sentence (Curran et al,
2006), which are then combined by the parser using
the combinatory rules and the CKY algorithm.
Clark and Curran (2004b) describes log-linear
parsing models for CCG. The features in the models
are defined over local parts of CCG derivations and
include word-word dependencies. A disadvantage
of the log-linear models is that they require clus-
ter computing resources for practical training (Clark
and Curran, 2004b). We have also investigated per-
ceptron training for the parser (Clark and Curran,
2007b), obtaining comparable accuracy scores and
similar training times (a few hours) compared with
the log-linear models. The significant advantage of
33
the perceptron training is that it only requires a sin-
gle processor. The training is online, updating the
model parameters one sentence at a time, and it con-
verges in a few passes over the CCGbank data.
A packed chart representation allows efficient de-
coding, with the same algorithm ? the Viterbi al-
gorithm ? finding the highest scoring derivation for
the log-linear and perceptron models.
2.1 The Supertagger
The supertagger uses Maximum Entropy tagging
techniques (Section 3) to assign a set of lexical cate-
gories to each word (Curran et al, 2006). Supertag-
ging has been especially successful for CCG: Clark
and Curran (2004a) demonstrates the considerable
increases in speed that can be obtained through use
of a supertagger. The supertagger interacts with the
parser in an adaptive fashion: initially it assigns
a small number of categories, on average, to each
word in the sentence, and the parser attempts to cre-
ate a spanning analysis. If this is not possible, the
supertagger assigns more categories, and this pro-
cess continues until a spanning analysis is found.
2.2 Parser Output
The parser produces various types of output. Fig-
ure 1 shows the dependency output for the exam-
ple sentence But Mr. Barnum called that a worst-
case scenario. The CCG dependencies are defined in
terms of the arguments within lexical categories; for
example, ?(S [dcl ]\NP1 )/NP2 , 2? represents the di-
rect object of a transitive verb. The parser also
outputs grammatical relations (GRs) consistent with
Briscoe et al (2006). The GRs are derived through a
manually created mapping from the CCG dependen-
cies, together with a python post-processing script
which attempts to remove any differences between
the two annotation schemes (for example the way in
which coordination is analysed).
The parser has been evaluated on the predicate-
argument dependencies in CCGbank, obtaining la-
belled precision and recall scores of 84.8% and
84.5% on Section 23. We have also evaluated the
parser on DepBank, using the Grammatical Rela-
tions output. The parser scores 82.4% labelled pre-
cision and 81.2% labelled recall overall. Clark and
Curran (2007a) gives precison and recall scores bro-
ken down by relation type and also compares the
Mr._2 N/N_1 1 Barnum_3
called_4 ((S[dcl]\NP_1)/NP_2)/NP_3 3 that_5
worst-case_7 N/N_1 1 scenario_8
a_6 NP[nb]/N_1 1 scenario_8
called_4 ((S[dcl]\NP_1)/NP_2)/NP_3 2 scenario_8
called_4 ((S[dcl]\NP_1)/NP_2)/NP_3 1 Barnum_3
But_1 S[X]/S[X]_1 1 called_4
(ncmod _ Barnum_3 Mr._2)
(obj2 called_4 that_5)
(ncmod _ scenario_8 worst-case_7)
(det scenario_8 a_6)
(dobj called_4 scenario_8)
(ncsubj called_4 Barnum_3 _)
(conj _ called_4 But_1)
Figure 1: Dependency output in the form of CCG
dependencies and grammatical relations
performance of the CCG parser with the RASP parser
(Briscoe et al, 2006).
3 Maximum Entropy Taggers
The taggers are based on Maximum Entropy tag-
ging methods (Ratnaparkhi, 1996), and can all be
trained on new annotated data, using either GIS or
BFGS training code.
The POS tagger uses the standard set of grammat-
ical categories from the Penn Treebank and, as well
as being highly efficient, also has state-of-the-art ac-
curacy on unseen newspaper text: over 97% per-
word accuracy on Section 23 of the Penn Treebank
(Curran and Clark, 2003a). The chunker recognises
the standard set of grammatical ?chunks?: NP, VP,
PP, ADJP, ADVP, and so on. It has been trained on
the CoNLL shared task data.
The named entity recogniser recognises the stan-
dard set of named entities in text: person, loca-
tion, organisation, date, time, monetary amount. It
has been trained on the MUC data. The named en-
tity recogniser contains many more features than the
other taggers; Curran and Clark (2003b) describes
the feature set.
Each tagger can be run as a ?multi-tagger?, poten-
tially assigning more than one tag to a word. The
multi-tagger uses the forward-backward algorithm
to calculate a distribution over tags for each word in
the sentence, and a parameter determines how many
tags are assigned to each word.
4 Boxer
Boxer is a separate component which takes a CCG
derivation output by the C&C parser and generates a
semantic representation. Boxer implements a first-
order fragment of Discourse Representation Theory,
34
DRT (Kamp and Reyle, 1993), and is capable of
generating the box-like structures of DRT known as
Discourse Representation Structures (DRSs). DRT is
a formal semantic theory backed up with a model
theory, and it demonstrates a large coverage of lin-
guistic phenomena. Boxer follows the formal the-
ory closely, introducing discourse referents for noun
phrases and events in the domain of a DRS, and their
properties in the conditions of a DRS.
One deviation with the standard theory is the
adoption of a Neo-Davidsonian analysis of events
and roles. Boxer also implements Van der Sandt?s
theory of presupposition projection treating proper
names and defininite descriptions as anaphoric ex-
pressions, by binding them to appropriate previously
introduced discourse referents, or accommodating
on a suitable level of discourse representation.
4.1 Discourse Representation Structures
DRSs are recursive data structures ? each DRS com-
prises a domain (a set of discourse referents) and a
set of conditions (possibly introducing new DRSs).
DRS-conditions are either basic or complex. The ba-
sic DRS-conditions supported by Boxer are: equal-
ity, stating that two discourse referents refer to the
same entity; one-place relations, expressing proper-
ties of discourse referents; two place relations, ex-
pressing binary relations between discourse refer-
ents; and names and time expressions. Complex
DRS-conditions are: negation of a DRS; disjunction
of two DRSs; implication (one DRS implying an-
other); and propositional, relating a discourse ref-
erent to a DRS.
Nouns, verbs, adjectives and adverbs introduce
one-place relations, whose meaning is represented
by the corresponding lemma. Verb roles and prepo-
sitions introduce two-place relations.
4.2 Input and Output
The input for Boxer is a list of CCG derivations deco-
rated with named entities, POS tags, and lemmas for
nouns and verbs. By default, each CCG derivation
produces one DRS. However, it is possible for one
DRS to span several CCG derivations; this enables
Boxer to deal with cross-sentential phenomena such
as pronouns and presupposition.
Boxer provides various output formats. The de-
fault output is a DRS in Prolog format, with dis-
______________________
| x0 x1 x2 x3 |
|______________________|
| named(x0,barnum,per) |
| named(x0,mr,ttl) |
| thing(x1) |
| worst-case(x2) |
| scenario(x2) |
| call(x3) |
| but(x3) |
| event(x3) |
| agent(x3,x0) |
| patient(x3,x1) |
| theme(x3,x2) |
|______________________|
Figure 2: Easy-to-read output format of Boxer
course referents represented as Prolog variables.
Other output options include: a flat structure, in
which the recursive structure of a DRS is unfolded by
labelling each DRS and DRS-condition; an XML for-
mat; and an easy-to-read box-like structure as found
in textbooks and articles on DRT. Figure 2 shows the
easy-to-read output for the sentence But Mr. Barnum
called that a worst-case scenario.
The semantic representations can also be output
as first-order formulas. This is achieved using the
standard translation from DRS to first-order logic
(Kamp and Reyle, 1993), and allows the output
to be pipelined into off-the-shelf theorem provers
or model builders for first-order logic, to perform
consistency or informativeness checking (Blackburn
and Bos, 2005).
5 Usage of the Tools
The taggers (and therefore the parser) can accept
many different input formats and produce many dif-
ferent output formats. These are described using a
?little language? similar to C printf format strings.
For example, the input format %w|%p \n indicates
that the program expects word (%w) and POS tag
(%p) pairs as input, where the words and POS tags
are separated by pipe characters, and each word-POS
tag pair is separated by a single space, and whole
sentences are separated by newlines (\n). Another
feature of the input/output is that other fields can be
read in which are not used in the tagging process,
and also form part of the output.
The C&C tools use a configuration management
system which allows the user to override all of the
default parameters for training and running the tag-
gers and parser. All of the tools can be used as stand-
alone components. Alternatively, a pipeline of the
35
tools is provided which supports two modes: local
file reading/writing or SOAP server mode.
6 Applications
We have developed an open-domain QA system built
around the C&C tools and Boxer (Ahn et al, 2005).
The parser is well suited to analysing large amounts
of text containing a potential answer, because of
its efficiency. The grammar is also well suited to
analysing questions, because of CCG?s treatment of
long-range dependencies. However, since the CCG
parser is based on the Penn Treebank, which con-
tains few examples of questions, the parser trained
on CCGbank is a poor analyser of questions. Clark
et al (2004) describes a porting method we have de-
veloped which exploits the lexicalized nature of CCG
by relying on rapid manual annotation at the lexi-
cal category level. We have successfully applied this
method to questions.
The robustness and efficiency of the parser; its
ability to analyses questions; and the detailed out-
put provided by Boxer make it ideal for large-scale
open-domain QA.
7 Conclusion
Linguistically motivated NLP can now be used
for large-scale language processing applications.
The C&C tools plus Boxer are freely available
for research use and can be downloaded from
http://svn.ask.it.usyd.edu.au/trac/candc/wiki.
Acknowledgements
James Curran was funded under ARC Discovery grants
DP0453131 and DP0665973. Johan Bos is supported by a ?Ri-
entro dei Cervelli? grant (Italian Ministry for Research).
References
Kisuh Ahn, Johan Bos, James R. Curran, Dave Kor, Malvina
Nissim, and Bonnie Webber. 2005. Question answering
with QED at TREC-2005. In Proceedings of TREC-2005.
Patrick Blackburn and Johan Bos. 2005. Representation and
Inference for Natural Language. A First Course in Compu-
tational Semantics. CSLI.
Johan Bos. 2005. Towards wide-coverage semantic interpreta-
tion. In Proceedings of IWCS-6, pages 42?53, Tilburg, The
Netherlands.
Ted Briscoe, John Carroll, and Rebecca Watson. 2006. The
second release of the RASP system. In Proceedings of the
Interactive Demo Session of COLING/ACL-06, Sydney.
Stephen Clark and James R. Curran. 2004a. The importance of
supertagging for wide-coverage CCG parsing. In Proceed-
ings of COLING-04, pages 282?288, Geneva, Switzerland.
Stephen Clark and James R. Curran. 2004b. Parsing the WSJ
using CCG and log-linear models. In Proceedings of ACL-
04, pages 104?111, Barcelona, Spain.
Stephen Clark and James R. Curran. 2007a. Formalism-
independent parser evaluation with CCG and DepBank. In
Proceedings of the 45th Annual Meeting of the ACL, Prague,
Czech Republic.
Stephen Clark and James R. Curran. 2007b. Perceptron train-
ing for a wide-coverage lexicalized-grammar parser. In Pro-
ceedings of the ACL Workshop on Deep Linguistic Process-
ing, Prague, Czech Republic.
Stephen Clark, Mark Steedman, and James R. Curran. 2004.
Object-extraction and question-parsing using CCG. In
Proceedings of the EMNLP Conference, pages 111?118,
Barcelona, Spain.
Michael Collins. 2003. Head-driven statistical models
for natural language parsing. Computational Linguistics,
29(4):589?637.
James R. Curran and Stephen Clark. 2003a. Investigating GIS
and smoothing for maximum entropy taggers. In Proceed-
ings of the 10th Meeting of the EACL, pages 91?98, Bu-
dapest, Hungary.
James R. Curran and Stephen Clark. 2003b. Language inde-
pendent NER using a maximum entropy tagger. In Proceed-
ings of CoNLL-03, pages 164?167, Edmonton, Canada.
James R. Curran, Stephen Clark, and David Vadas. 2006.
Multi-tagging for lexicalized-grammar parsing. In Proceed-
ings of COLING/ACL-06, pages 697?704, Sydney.
Julia Hockenmaier. 2003. Data and Models for Statistical
Parsing with Combinatory Categorial Grammar. Ph.D. the-
sis, University of Edinburgh.
H. Kamp and U. Reyle. 1993. From Discourse to Logic; An
Introduction to Modeltheoretic Semantics of Natural Lan-
guage, Formal Logic and DRT. Kluwer, Dordrecht.
Ron Kaplan, Stefan Riezler, Tracy H. King, John T. Maxwell
III, Alexander Vasserman, and Richard Crouch. 2004.
Speed and accuracy in shallow and deep stochastic pars-
ing. In Proceedings of HLT and the 4th Meeting of NAACL,
Boston, MA.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii. 2007.
Efficient HPSG parsing with supertagging and CFG-
filtering. In Proceedings of IJCAI-07, Hyderabad, India.
Guido Minnen, John Carroll, and Darren Pearce. 2001. Ap-
plied morphological processing of English. Natural Lan-
guage Engineering, 7(3):207?223.
Adwait Ratnaparkhi. 1996. A maximum entropy part-of-
speech tagger. In Proceedings of the EMNLP Conference,
pages 133?142, Philadelphia, PA.
Mark Steedman. 2000. The Syntactic Process. The MIT Press,
Cambridge, MA.
36
Proceedings of ACL-08: HLT, pages 888?896,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Joint Word Segmentation and POS Tagging using a Single Perceptron
Yue Zhang and Stephen Clark
Oxford University Computing Laboratory
Wolfson Building, Parks Road
Oxford OX1 3QD, UK
{yue.zhang,stephen.clark}@comlab.ox.ac.uk
Abstract
For Chinese POS tagging, word segmentation
is a preliminary step. To avoid error propa-
gation and improve segmentation by utilizing
POS information, segmentation and tagging
can be performed simultaneously. A challenge
for this joint approach is the large combined
search space, which makes efficient decod-
ing very hard. Recent research has explored
the integration of segmentation and POS tag-
ging, by decoding under restricted versions of
the full combined search space. In this paper,
we propose a joint segmentation and POS tag-
ging model that does not impose any hard con-
straints on the interaction between word and
POS information. Fast decoding is achieved
by using a novel multiple-beam search algo-
rithm. The system uses a discriminative sta-
tistical model, trained using the generalized
perceptron algorithm. The joint model gives
an error reduction in segmentation accuracy of
14.6% and an error reduction in tagging ac-
curacy of 12.2%, compared to the traditional
pipeline approach.
1 Introduction
Since Chinese sentences do not contain explicitly
marked word boundaries, word segmentation is a
necessary step before POS tagging can be performed.
Typically, a Chinese POS tagger takes segmented in-
puts, which are produced by a separate word seg-
mentor. This two-step approach, however, has an
obvious flaw of error propagation, since word seg-
mentation errors cannot be corrected by the POS tag-
ger. A better approach would be to utilize POS in-
formation to improve word segmentation. For ex-
ample, the POS-word pattern ?number word? + ??
(a common measure word)? can help in segmenting
the character sequence ??|? into the word se-
quence ? (one) ? (measure word) | (person)?
instead of ? (one) ?| (personal; adj)?. More-
over, the comparatively rare POS pattern ?number
word? + ?number word? can help to prevent seg-
menting a long number word into two words.
In order to avoid error propagation and make use
of POS information for word segmentation, segmen-
tation and POS tagging can be viewed as a single
task: given a raw Chinese input sentence, the joint
POS tagger considers all possible segmented and
tagged sequences, and chooses the overall best out-
put. A major challenge for such a joint system is
the large search space faced by the decoder. For a
sentence with n characters, the number of possible
output sequences is O(2n?1 ? Tn), where T is the
size of the tag set. Due to the nature of the com-
bined candidate items, decoding can be inefficient
even with dynamic programming.
Recent research on Chinese POS tagging has
started to investigate joint segmentation and tagging,
reporting accuracy improvements over the pipeline
approach. Various decoding approaches have been
used to reduce the combined search space. Ng and
Low (2004) mapped the joint segmentation and POS
tagging task into a single character sequence tagging
problem. Two types of tags are assigned to each
character to represent its segmentation and POS. For
example, the tag ?b NN? indicates a character at
the beginning of a noun. Using this method, POS
features are allowed to interact with segmentation.
888
Since tagging is restricted to characters, the search
space is reduced to O((4T )n), and beam search de-
coding is effective with a small beam size. How-
ever, the disadvantage of this model is the difficulty
in incorporating whole word information into POS
tagging. For example, the standard ?word + POS
tag? feature is not explicitly applicable. Shi and
Wang (2007) introduced POS information to seg-
mentation by reranking. N -best segmentation out-
puts are passed to a separately-trained POS tagger,
and the best output is selected using the overall POS-
segmentation probability score. In this system, the
decoding for word segmentation and POS tagging
are still performed separately, and exact inference
for both is possible. However, the interaction be-
tween POS and segmentation is restricted by rerank-
ing: POS information is used to improve segmenta-
tion only for the N segmentor outputs.
In this paper, we propose a novel joint model
for Chinese word segmentation and POS tagging,
which does not limiting the interaction between
segmentation and POS information in reducing the
combined search space. Instead, a novel multiple
beam search algorithm is used to do decoding effi-
ciently. Candidate ranking is based on a discrimina-
tive joint model, with features being extracted from
segmented words and POS tags simultaneously. The
training is performed by a single generalized percep-
tron (Collins, 2002). In experiments with the Chi-
nese Treebank data, the joint model gave an error
reduction of 14.6% in segmentation accuracy and
12.2% in the overall segmentation and tagging accu-
racy, compared to the traditional pipeline approach.
In addition, the overall results are comparable to the
best systems in the literature, which exploit knowl-
edge outside the training data, even though our sys-
tem is fully data-driven.
Different methods have been proposed to reduce
error propagation between pipelined tasks, both in
general (Sutton et al, 2004; Daume? III and Marcu,
2005; Finkel et al, 2006) and for specific problems
such as language modeling and utterance classifica-
tion (Saraclar and Roark, 2005) and labeling and
chunking (Shimizu and Haas, 2006). Though our
model is built specifically for Chinese word segmen-
tation and POS tagging, the idea of using the percep-
tron model to solve multiple tasks simultaneously
can be generalized to other tasks.
1 word w
2 word bigram w1w2
3 single-character word w
4 a word of length l with starting character c
5 a word of length l with ending character c
6 space-separated characters c1 and c2
7 character bigram c1c2 in any word
8 the first / last characters c1 / c2 of any word
9 word w immediately before character c
10 character c immediately before word w
11 the starting characters c1 and c2 of two con-
secutive words
12 the ending characters c1 and c2 of two con-
secutive words
13 a word of length l with previous word w
14 a word of length l with next word w
Table 1: Feature templates for the baseline segmentor
2 The Baseline System
We built a two-stage baseline system, using the per-
ceptron segmentation model from our previous work
(Zhang and Clark, 2007) and the perceptron POS tag-
ging model from Collins (2002). We use baseline
system to refer to the system which performs seg-
mentation first, followed by POS tagging (using the
single-best segmentation); baseline segmentor to re-
fer to the segmentor from (Zhang and Clark, 2007)
which performs segmentation only; and baseline
POStagger to refer to the Collins tagger which per-
forms POS tagging only (given segmentation). The
features used by the baseline segmentor are shown in
Table 1. The features used by the POS tagger, some
of which are different to those from Collins (2002)
and are specific to Chinese, are shown in Table 2.
The word segmentation features are extracted
from word bigrams, capturing word, word length
and character information in the context. The word
length features are normalized, with those more than
15 being treated as 15.
The POS tagging features are based on contex-
tual information from the tag trigram, as well as the
neighboring three-word window. To reduce overfit-
ting and increase the decoding speed, templates 4, 5,
6 and 7 only include words with less than 3 charac-
ters. Like the baseline segmentor, the baseline tag-
ger also normalizes word length features.
889
1 tag t with word w
2 tag bigram t1t2
3 tag trigram t1t2t3
4 tag t followed by word w
5 word w followed by tag t
6 word w with tag t and previous character c
7 word w with tag t and next character c
8 tag t on single-character word w in charac-
ter trigram c1wc2
9 tag t on a word starting with char c
10 tag t on a word ending with char c
11 tag t on a word containing char c (not the
starting or ending character)
12 tag t on a word starting with char c0 and
containing char c
13 tag t on a word ending with char c0 and
containing char c
14 tag t on a word containing repeated char cc
15 tag t on a word starting with character cat-
egory g
16 tag t on a word ending with character cate-
gory g
Table 2: Feature templates for the baseline POS tagger
Templates 15 and 16 in Table 2 are inspired by the
CTBMorph feature templates in Tseng et al (2005),
which gave the most accuracy improvement in their
experiments. Here the category of a character is
the set of tags seen on the character during train-
ing. Other morphological features from Tseng et al
(2005) are not used because they require extra web
corpora besides the training data.
During training, the baseline POS tagger stores
special word-tag pairs into a tag dictionary (Ratna-
parkhi, 1996). Such information is used by the de-
coder to prune unlikely tags. For each word occur-
ring more than N times in the training data, the de-
coder can only assign a tag the word has been seen
with in the training data. This method led to im-
provement in the decoding speed as well as the out-
put accuracy for English POS tagging (Ratnaparkhi,
1996). Besides tags for frequent words, our base-
line POS tagger also uses the tag dictionary to store
closed-set tags (Xia, 2000) ? those associated only
with a limited number of Chinese words.
3 Joint Segmentation and Tagging Model
In this section, we build a joint word segmentation
and POS tagging model that uses exactly the same
source of information as the baseline system, by ap-
plying the feature templates from the baseline word
segmentor and POS tagger. No extra knowledge is
used by the joint model. However, because word
segmentation and POS tagging are performed simul-
taneously, POS information participates in word seg-
mentation.
3.1 Formulation of the joint model
We formulate joint word segmentation and POS tag-
ging as a single problem, which maps a raw Chi-
nese sentence to a segmented and POS tagged output.
Given an input sentence x, the output F (x) satisfies:
F (x) = argmax
y?GEN(x)
Score(y)
where GEN(x) represents the set of possible outputs
for x.
Score(y) is computed by a feature-based linear
model. Denoting the global feature vector for the
tagged sentence y with ?(y), we have:
Score(y) = ?(y) ? ~w
where ~w is the parameter vector in the model. Each
element in ~w gives a weight to its corresponding el-
ement in ?(y), which is the count of a particular
feature over the whole sentence y. We calculate the
~w value by supervised learning, using the averaged
perceptron algorithm (Collins, 2002), given in Fig-
ure 1. 1
We take the union of feature templates from the
baseline segmentor (Table 1) and POS tagger (Ta-
ble 2) as the feature templates for the joint system.
All features are treated equally and processed to-
gether according to the linear model, regardless of
whether they are from the baseline segmentor or tag-
ger. In fact, most features from the baseline POS
tagger, when used in the joint model, represent seg-
mentation patterns as well. For example, the afore-
mentioned pattern ?number word? + ???, which is
1In order to provide a comparison for the perceptron algo-
rithm we also tried SVMstruct (Tsochantaridis et al, 2004) for
parameter estimation, but this training method was prohibitively
slow.
890
Inputs: training examples (xi, yi)
Initialization: set ~w = 0
Algorithm:
for t = 1..T , i = 1..N
calculate zi = argmaxy?GEN(xi) ?(y) ? ~w
if zi 6= yi
~w = ~w + ?(yi) ? ?(zi)
Outputs: ~w
Figure 1: The perceptron learning algorithm
useful only for the POS ?number word? in the base-
line tagger, is also an effective indicator of the seg-
mentation of the two words (especially ???) in the
joint model.
3.2 The decoding algorithm
One of the main challenges for the joint segmenta-
tion and POS tagging system is the decoding algo-
rithm. The speed and accuracy of the decoder is
important for the perceptron learning algorithm, but
the system faces a very large search space of com-
bined candidates. Given the linear model and feature
templates, exact inference is very hard even with dy-
namic programming.
Experiments with the standard beam-search de-
coder described in (Zhang and Clark, 2007) resulted
in low accuracy. This beam search algorithm pro-
cesses an input sentence incrementally. At each
stage, the incoming character is combined with ex-
isting partial candidates in all possible ways to gen-
erate new partial candidates. An agenda is used to
control the search space, keeping only the B best
partial candidates ending with the current charac-
ter. The algorithm is simple and efficient, with a
linear time complexity of O(BTn), where n is the
size of input sentence, and T is the size of the tag
set (T = 1 for pure word segmentation). It worked
well for word segmentation alone (Zhang and Clark,
2007), even with an agenda size as small as 8, and
a simple beam search algorithm also works well for
POS tagging (Ratnaparkhi, 1996). However, when
applied to the joint model, it resulted in a reduction
in segmentation accuracy (compared to the baseline
segmentor) even with B as large as 1024.
One possible cause of the poor performance of the
standard beam search method is the combined nature
of the candidates in the search space. In the base-
Input: raw sentence sent ? a list of characters
Variables: candidate sentence item ? a list of
(word, tag) pairs;
maximum word-length record
maxlen for each tag;
the agenda list agendas;
the tag dictionary tagdict;
start index for current word;
end index for current word
Initialization: agendas[0] = [??],
agendas[i] = [] (i! = 0)
Algorithm:
for end index = 1 to sent.length:
foreach tag:
for start index =
max(1, end index ? maxlen[tag] + 1)
to end index:
word = sent[start index..end index]
if (word, tag) consistent with tagdict:
for item ? agendas[start index ? 1]:
item1 = item
item1.append((word,tag))
agendas[end index].insert(item1)
Outputs: agendas[sent.length].best item
Figure 2: The decoding algorithm for the joint word seg-
mentor and POS tagger
line POS tagger, candidates in the beam are tagged
sequences ending with the current word, which can
be compared directly with each other. However, for
the joint problem, candidates in the beam are seg-
mented and tagged sequences up to the current char-
acter, where the last word can be a complete word or
a partial word. A problem arises in whether to give
POS tags to incomplete words. If partial words are
given POS tags, it is likely that some partial words
are ?justified? as complete words by the current POS
information. On the other hand, if partial words are
not given POS tag features, the correct segmentation
for long words can be lost during partial candidate
comparison (since many short completed words with
POS tags are likely to be preferred to a long incom-
plete word with no POS tag features).2
2We experimented with both assigning POS features to par-
tial words and omitting them; the latter method performed better
but both performed significantly worse than the multiple beam
search method described below.
891
Another possible cause is the exponential growth
in the number of possible candidates with increasing
sentence size. The number increases from O(Tn)
for the baseline POS tagger to O(2n?1Tn) for the
joint system. As a result, for an incremental decod-
ing algorithm, the number of possible candidates in-
creases exponentially with the current word or char-
acter index. In the POS tagging problem, a new in-
coming word enlarges the number of possible can-
didates by a factor of T (the size of the tag set).
For the joint problem, however, the enlarging fac-
tor becomes 2T with each incoming character. The
speed of search space expansion is much faster, but
the number of candidates is still controlled by a sin-
gle, fixed-size beam at any stage. If we assume
that the beam is not large enough for all the can-
didates at at each stage, then, from the newly gen-
erated candidates, the baseline POS tagger can keep
1/T for the next processing stage, while the joint
model can keep only 1/2T , and has to discard the
rest. Therefore, even when the candidate compar-
ison standard is ignored, we can still see that the
chance for the overall best candidate to fall out of
the beam is largely increased. Since the search space
growth is exponential, increasing the fixed beam size
is not effective in solving the problem.
To solve the above problems, we developed a mul-
tiple beam search algorithm, which compares candi-
dates only with complete tagged words, and enables
the size of the search space to scale with the input
size. The algorithm is shown in Figure 2. In this
decoder, an agenda is assigned to each character in
the input sentence, recording the B best segmented
and tagged partial candidates ending with the char-
acter. The input sentence is still processed incremen-
tally. However, now when a character is processed,
existing partial candidates ending with any previous
characters are available. Therefore, the decoder enu-
merates all possible tagged words ending with the
current character, and combines each word with the
partial candidates ending with its previous charac-
ter. All input characters are processed in the same
way, and the final output is the best candidate in the
final agenda. The time complexity of the algorithm
is O(WTBn), with W being the maximum word
size, T being the total number of POS tags and n the
number of characters in the input. It is also linear
in the input size. Moreover, the decoding algorithm
gives competent accuracy with a small agenda size
of B = 16.
To further limit the search space, two optimiza-
tions are used. First, the maximum word length
for each tag is recorded and used by the decoder
to prune unlikely candidates. Because the major-
ity of tags only apply to words with length 1 or
2, this method has a strong effect. Development
tests showed that it improves the speed significantly,
while having a very small negative influence on the
accuracy. Second, like the baseline POS tagger, the
tag dictionary is used for Chinese closed set tags and
the tags for frequent words. To words outside the tag
dictionary, the decoder still tries to assign every pos-
sible tag.
3.3 Online learning
Apart from features, the decoder maintains other
types of information, including the tag dictionary,
the word frequency counts used when building the
tag dictionary, the maximum word lengths by tag,
and the character categories. The above data can
be collected by scanning the corpus before training
starts. However, in both the baseline tagger and the
joint POS tagger, they are updated incrementally dur-
ing the perceptron training process, consistent with
online learning.3
The online updating of word frequencies, max-
imum word lengths and character categories is
straightforward. For the online updating of the tag
dictionary, however, the decision for frequent words
must be made dynamically because the word fre-
quencies keep changing. This is done by caching
the number of occurrences of the current most fre-
quent word M , and taking all words currently above
the threshold M/5000 + 5 as frequent words. 5000
is a rough figure to control the number of frequent
words, set according to Zipf?s law. The parameter
5 is used to force all tags to be enumerated before a
word is seen more than 5 times.
4 Related Work
Ng and Low (2004) and Shi and Wang (2007) were
described in the Introduction. Both models reduced
3We took this approach because we wanted the whole train-
ing process to be online. However, for comparison purposes,
we also tried precomputing the above information before train-
ing and the difference in performance was negligible.
892
the large search space by imposing strong restric-
tions on the form of search candidates. In particu-
lar, Ng and Low (2004) used character-based POS
tagging, which prevents some important POS tag-
ging features such as word + POS tag; Shi and Wang
(2007) used an N -best reranking approach, which
limits the influence of POS tagging on segmentation
to the N -best list. In comparison, our joint model
does not impose any hard limitations on the inter-
action between segmentation and POS information.4
Fast decoding speed is achieved by using a novel
multiple-beam search algorithm.
Nakagawa and Uchimoto (2007) proposed a hy-
brid model for word segmentation and POS tagging
using an HMM-based approach. Word information is
used to process known-words, and character infor-
mation is used for unknown words in a similar way
to Ng and Low (2004). In comparison, our model
handles character and word information simultane-
ously in a single perceptron model.
5 Experiments
The Chinese Treebank (CTB) 4 is used for the exper-
iments. It is separated into two parts: CTB 3 (420K
characters in 150K words / 10364 sentences) is used
for the final 10-fold cross validation, and the rest
(240K characters in 150K words / 4798 sentences)
is used as training and test data for development.
The standard F-scores are used to measure both
the word segmentation accuracy and the overall seg-
mentation and tagging accuracy, where the overall
accuracy is TF = 2pr/(p + r), with the precision
p being the percentage of correctly segmented and
tagged words in the decoder output, and the recall r
being the percentage of gold-standard tagged words
that are correctly identified by the decoder. For di-
rect comparison with Ng and Low (2004), the POS
tagging accuracy is also calculated by the percentage
of correct tags on each character.
5.1 Development experiments
The learning curves of the baseline and joint models
are shown in Figure 3, Figure 4 and Figure 5, respec-
tively. These curves are used to show the conver-
4Apart from the beam search algorithm, we do impose some
minor limitations on the search space by methods such as the tag
dictionary, but these can be seen as optional pruning methods
for optimization.
0.88
0.89
0.9
0.91
0.92
1 2 3 4 5 6 7 8 9 10
Number of training iterations
F-
sc
o
re
Figure 3: The learning curve of the baseline segmentor
0.86
0.87
0.88
0.89
0.9
1 2 3 4 5 6 7 8 9 10
Number of training iterations
F-
sc
o
re
Figure 4: The learning curve of the baseline tagger
0.8
0.82
0.84
0.86
0.88
0.9
0.92
1 2 3 4 5 6 7 8 9 10
Number of training iterations
F-
sc
o
re
segmentation accuracy
overall accuracy
Figure 5: The learning curves of the joint system
gence of perceptron and decide the number of train-
ing iterations for the test. It should be noticed that
the accuracies from Figure 4 and Figure 5 are not
comparable because gold-standard segmentation is
used as the input for the baseline tagger. Accord-
ing to the figures, the number of training iterations
893
Tag Seg NN NR VV AD JJ CD
NN 20.47 ? 0.78 4.80 0.67 2.49 0.04
NR 5.95 3.61 ? 0.19 0.04 0.07 0
VV 12.13 6.51 0.11 ? 0.93 0.56 0.04
AD 3.24 0.30 0 0.71 ? 0.33 0.22
JJ 3.09 0.93 0.15 0.26 0.26 ? 0.04
CD 1.08 0.04 0 0 0.07 0 ?
Table 3: Error analysis for the joint model
for the baseline segmentor, POS tagger, and the joint
system are set to 8, 6, and 7, respectively for the re-
maining experiments.
There are many factors which can influence the
accuracy of the joint model. Here we consider the
special character category features and the effect of
the tag dictionary. The character category features
(templates 15 and 16 in Table 2) represent a Chinese
character by all the tags associated with the charac-
ter in the training data. They have been shown to im-
prove the accuracy of a Chinese POS tagger (Tseng
et al, 2005). In the joint model, these features also
represent segmentation information, since they con-
cern the starting and ending characters of a word.
Development tests showed that the overall tagging
F-score of the joint model increased from 84.54% to
84.93% using the character category features. In the
development test, the use of the tag dictionary im-
proves the decoding speed of the joint model, reduc-
ing the decoding time from 416 seconds to 256 sec-
onds. The overall tagging accuracy also increased
slightly, consistent with observations from the pure
POS tagger.
The error analysis for the development test is
shown in Table 3. Here an error is counted when
a word in the standard output is not produced by the
decoder, due to incorrect segmentation or tag assign-
ment. Statistics about the six most frequently mis-
taken tags are shown in the table, where each row
presents the analysis of one tag from the standard
output, and each column gives a wrongly assigned
value. The column ?Seg? represents segmentation
errors. Each figure in the table shows the percentage
of the corresponding error from all the errors.
It can be seen from the table that the NN-VV and
VV-NN mistakes were the most commonly made by
the decoder, while the NR-NN mistakes are also fre-
Baseline Joint
# SF TF TA SF TF TA
1 96.98 92.91 94.14 97.21 93.46 94.66
2 97.16 93.20 94.34 97.62 93.85 94.79
3 95.02 89.53 91.28 95.94 90.86 92.38
4 95.51 90.84 92.55 95.92 91.60 93.31
5 95.49 90.91 92.57 96.06 91.72 93.25
6 93.50 87.33 89.87 94.56 88.83 91.14
7 94.48 89.44 91.61 95.30 90.51 92.41
8 93.58 88.41 90.93 95.12 90.30 92.32
9 93.92 89.15 91.35 94.79 90.33 92.45
10 96.31 91.58 93.01 96.45 91.96 93.45
Av. 95.20 90.33 92.17 95.90 91.34 93.02
Table 4: The accuracies by 10-fold cross validation
SF ? segmentation F-score,
TF ? overall F-score,
TA ? tagging accuracy by character.
quent. These three types of errors significantly out-
number the rest, together contributing 14.92% of all
the errors. Moreover, the most commonly mistaken
tags are NN and VV, while among the most frequent
tags in the corpus, PU, DEG and M had compara-
tively less errors. Lastly, segmentation errors con-
tribute around half (51.47%) of all the errors.
5.2 Test results
10-fold cross validation is performed to test the ac-
curacy of the joint word segmentor and POS tagger,
and to make comparisons with existing models in the
literature. Following Ng and Low (2004), we parti-
tion the sentences in CTB 3, ordered by sentence ID,
into 10 groups evenly. In the nth test, the nth group
is used as the testing data.
Table 4 shows the detailed results for the cross
validation tests, each row representing one test. As
can be seen from the table, the joint model outper-
forms the baseline system in each test.
Table 5 shows the overall accuracies of the base-
line and joint systems, and compares them to the rel-
evant models in the literature. The accuracy of each
model is shown in a row, where ?Ng? represents the
models from Ng and Low (2004) and ?Shi? repre-
sents the models from Shi and Wang (2007). Each
accuracy measure is shown in a column, including
the segmentation F-score (SF ), the overall tagging
894
Model SF TF TA
Baseline+ (Ng) 95.1 ? 91.7
Joint+ (Ng) 95.2 ? 91.9
Baseline+* (Shi) 95.85 91.67 ?
Joint+* (Shi) 96.05 91.86 ?
Baseline (ours) 95.20 90.33 92.17
Joint (ours) 95.90 91.34 93.02
Table 5: The comparison of overall accuracies by 10-fold
cross validation using CTB
+ ? knowledge about sepcial characters,
* ? knowledge from semantic net outside CTB.
F-score (TF ) and the tagging accuracy by characters
(TA). As can be seen from the table, our joint model
achieved the largest improvement over the baseline,
reducing the segmentation error by 14.58% and the
overall tagging error by 12.18%.
The overall tagging accuracy of our joint model
was comparable to but less than the joint model of
Shi and Wang (2007). Despite the higher accuracy
improvement from the baseline, the joint system did
not give higher overall accuracy. One likely reason
is that Shi and Wang (2007) included knowledge
about special characters and semantic knowledge
from web corpora (which may explain the higher
baseline accuracy), while our system is completely
data-driven. However, the comparison is indirect be-
cause our partitions of the CTB corpus are different.
Shi and Wang (2007) also chunked the sentences be-
fore doing 10-fold cross validation, but used an un-
even split. We chose to follow Ng and Low (2004)
and split the sentences evenly to facilitate further
comparison.
Compared with Ng and Low (2004), our baseline
model gave slightly better accuracy, consistent with
our previous observations about the word segmen-
tors (Zhang and Clark, 2007). Due to the large ac-
curacy gain from the baseline, our joint model per-
formed much better.
In summary, when compared with existing joint
word segmentation and POS tagging systems in the
literature, our proposed model achieved the best ac-
curacy boost from the cascaded baseline, and com-
petent overall accuracy.
6 Conclusion and Future Work
We proposed a joint Chinese word segmentation and
POS tagging model, which achieved a considerable
reduction in error rate compared to a baseline two-
stage system.
We used a single linear model for combined word
segmentation and POS tagging, and chose the gen-
eralized perceptron algorithm for joint training. and
beam search for efficient decoding. However, the
application of beam search was far from trivial be-
cause of the size of the combined search space. Mo-
tivated by the question of what are the compara-
ble partial hypotheses in the space, we developed
a novel multiple beam search decoder which effec-
tively explores the large search space. Similar tech-
niques can potentially be applied to other problems
involving joint inference in NLP.
Other choices are available for the decoding of
a joint linear model, such as exact inference with
dynamic programming, provided that the range of
features allows efficient processing. The baseline
feature templates for Chinese segmentation and POS
tagging, when added together, makes exact infer-
ence for the proposed joint model very hard. How-
ever, the accuracy loss from the beam decoder, as
well as alternative decoding algorithms, are worth
further exploration.
The joint system takes features only from the
baseline segmentor and the baseline POS tagger to
allow a fair comparison. There may be additional
features that are particularly useful to the joint sys-
tem. Open features, such as knowledge of numbers
and European letters, and relationships from seman-
tic networks (Shi and Wang, 2007), have been re-
ported to improve the accuracy of segmentation and
POS tagging. Therefore, given the flexibility of the
feature-based linear model, an obvious next step is
the study of open features in the joint segmentor and
POS tagger.
Acknowledgements
We thank Hwee-Tou Ng and Mengqiu Wang for
their helpful discussions and sharing of experimen-
tal data, and the anonymous reviewers for their sug-
gestions. This work is supported by the ORS and
Clarendon Fund.
895
References
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
the EMNLP conference, pages 1?8, Philadelphia, PA.
Hal Daume? III and Daniel Marcu. 2005. Learning as
search optimization: Approximate large margin meth-
ods for structured prediction. In Proceedings of the
ICML Conference, pages 169?176, Bonn, Germany.
Jenny Rose Finkel, Christopher D. Manning, and An-
drew Y. Ng. 2006. Solving the problem of cascading
errors: Approximate Bayesian inference for linguistic
annotation pipelines. In Proceedings of the EMNLP
Conference, pages 618?626, Sydney, Australia.
Tetsuji Nakagawa and Kiyotaka Uchimoto. 2007. A
hybrid approach to word segmentation and pos tag-
ging. In Proceedings of ACL Demo and Poster Ses-
sion, pages 217?220, Prague, Czech Republic.
Hwee Tou Ng and Jin Kiat Low. 2004. Chinese
part-of-speech tagging: One-at-a-time or all-at-once?
Word-based or character-based? In Proceedings of
the EMNLP Conference, pages 277?284, Barcelona,
Spain.
Adwait Ratnaparkhi. 1996. A maximum entropy model
for part-of-speech tagging. In Proceedings of the
EMNLP Conference, pages 133?142, Philadelphia,
PA.
Murat Saraclar and Brian Roark. 2005. Joint discrimi-
native language modeling and utterance classification.
In Proceedings of the ICASSP Conference, volume 1,
Philadelphia, USA.
Yanxin Shi and Mengqiu Wang. 2007. A dual-layer CRF
based joint decoding method for cascade segmentation
and labelling tasks. In Proceedings of the IJCAI Con-
ference, Hyderabad, India.
Nobuyuki Shimizu and Andrew Haas. 2006. Exact de-
coding for jointly labeling and chunking sequences. In
Proceedings of the COLING/ACL Conference, Poster
Sessions, Sydney, Australia.
Charles Sutton, Khashayar Rohanimanesh, and Andrew
McCallum. 2004. Dynamic conditional random
fields: Factorized probabilistic models for labeling
and segmenting sequence data. In Proceedings of the
ICML Conference, Banff, Canada.
Huihsin Tseng, Daniel Jurafsky, and Christopher Man-
ning. 2005. Morphological features help POS tagging
of unknown words across language varieties. In Pro-
ceedings of the Fourth SIGHAN Workshop, Jeju Island,
Korea.
I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun.
2004. Support vector machine learning for interdepen-
dent and structured output spaces. In Proceedings of
the ICML Conference, Banff, Canada.
Fei Xia. 2000. The part-of-speech tagging guidelines for
the Chinese Treebank (3.0). IRCS Report, University
of Pennsylvania.
Yue Zhang and Stephen Clark. 2007. Chinese segmen-
tation with a word-based perceptron algorithm. In
Proceedings of the ACL Conference, pages 840?847,
Prague, Czech Republic.
896
Proceedings of the 5th Workshop on Important Unresolved Matters, pages 9?16,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Perceptron Training for a Wide-Coverage Lexicalized-Grammar Parser
Stephen Clark
Oxford University Computing Laboratory
Wolfson Building, Parks Road
Oxford, OX1 3QD, UK
stephen.clark@comlab.ox.ac.uk
James R. Curran
School of Information Technologies
University of Sydney
NSW 2006, Australia
james@it.usyd.edu.au
Abstract
This paper investigates perceptron training
for a wide-coverage CCG parser and com-
pares the perceptron with a log-linear model.
The CCG parser uses a phrase-structure pars-
ing model and dynamic programming in the
form of the Viterbi algorithm to find the
highest scoring derivation. The difficulty in
using the perceptron for a phrase-structure
parsing model is the need for an efficient de-
coder. We exploit the lexicalized nature of
CCG by using a finite-state supertagger to
do much of the parsing work, resulting in
a highly efficient decoder. The perceptron
performs as well as the log-linear model; it
trains in a few hours on a single machine;
and it requires only a few hundred MB of
RAM for practical training compared to 20
GB for the log-linear model. We also inves-
tigate the order in which the training exam-
ples are presented to the online perceptron
learner, and find that order does not signifi-
cantly affect the results.
1 Introduction
A recent development in data-driven parsing is the
use of discriminative training methods (Riezler et
al., 2002; Taskar et al, 2004; Collins and Roark,
2004; Turian and Melamed, 2006). One popular ap-
proach is to use a log-linear parsing model and max-
imise the conditional likelihood function (Johnson
et al, 1999; Riezler et al, 2002; Clark and Curran,
2004b; Malouf and van Noord, 2004; Miyao and
Tsujii, 2005). Maximising the likelihood involves
calculating feature expectations, which is computa-
tionally expensive. Dynamic programming (DP) in
the form of the inside-outside algorithm can be used
to calculate the expectations, if the features are suf-
ficiently local (Miyao and Tsujii, 2002); however,
the memory requirements can be prohibitive, es-
pecially for automatically extracted, wide-coverage
grammars. In Clark and Curran (2004b) we use clus-
ter computing resources to solve this problem.
Parsing research has also begun to adopt discrim-
inative methods from the Machine Learning litera-
ture, such as the perceptron (Freund and Schapire,
1999; Collins and Roark, 2004) and the large-
margin methods underlying Support Vector Ma-
chines (Taskar et al, 2004; McDonald, 2006).
Parser training involves decoding in an iterative pro-
cess, updating the model parameters so that the de-
coder performs better on the training data, accord-
ing to some training criterion. Hence, for efficient
training, these methods require an efficient decoder;
in fact, for methods like the perceptron, the update
procedure is so trivial that the training algorithm es-
sentially is decoding.
This paper describes a decoder for a lexicalized-
grammar parser which is efficient enough for prac-
tical discriminative training. We use a lexicalized
phrase-structure parser, the CCG parser of Clark and
Curran (2004b), together with a DP-based decoder.
The key idea is to exploit the properties of lexi-
calized grammars by using a finite-state supertag-
ger prior to parsing (Bangalore and Joshi, 1999;
Clark and Curran, 2004a). The decoder still uses
the CKY algorithm, so the worst case complexity of
9
the parsing is unchanged; however, by allowing the
supertagger to do much of the parsing work, the effi-
ciency of the decoder is greatly increased in practice.
We chose the perceptron for the training algo-
rithm because it has shown good performance on
other NLP tasks; in particular, Collins (2002) re-
ported good performance for a perceptron tagger
compared to a Maximum Entropy tagger. Like
Collins (2002), the decoder is the same for both the
perceptron and the log-linear parsing models; the
only change is the method for setting the weights.
The perceptron model performs as well as the log-
linear model, but is considerably easier to train.
Another contribution of this paper is to advance
wide-coverage CCG parsing. Previous discrimina-
tive models for CCG (Clark and Curran, 2004b) re-
quired cluster computing resources to train. In this
paper we reduce the memory requirements from 20
GB of RAM to only a few hundred MB, but with-
out greatly increasing the training time or reducing
parsing accuracy. This provides state-of-the-art CCG
parsing with a practical development environment.
More generally, this work provides a practical
environment for experimenting with discriminative
models for phrase-structure parsing; because the
training time for the CCG parser is relatively short
(a few hours), experiments such as comparing alter-
native feature sets can be performed. As an example,
we investigate the order in which the training exam-
ples are presented to the perceptron learner. Since
the perceptron training is an online algorithm ? up-
dating the weights one training sentence at a time
? the order in which the data is processed affects
the resulting model. We consider random ordering;
presenting the shortest sentences first; and present-
ing the longest sentences first; and find that the order
does not significantly affect the final results.
We also use the random orderings to investigate
model averaging. We produced 10 different models,
by randomly permuting the data, and averaged the
weights. Again the averaging was found to have no
impact on the results, showing that the perceptron
learner ? at least for this parsing task ? is robust
to the order of the training examples.
The contributions of this paper are as follows.
First, we compare perceptron and log-linear parsing
models for a wide-coverage phrase-structure parser,
the first work we are aware of to do so. Second,
we provide a practical framework for developing
discriminative models for CCG, reducing the mem-
ory requirements from over 20 GB to a few hundred
MB. And third, given the significantly shorter train-
ing time compared to other discriminative parsing
models (Taskar et al, 2004), we provide a practical
framework for investigating discriminative training
methods more generally.
2 The CCG Parser
Clark and Curran (2004b) describes the CCG parser.
The grammar used by the parser is extracted from
CCGbank, a CCG version of the Penn Treebank
(Hockenmaier, 2003). The grammar consists of 425
lexical categories, expressing subcategorisation in-
formation, plus a small number of combinatory rules
which combine the categories (Steedman, 2000). A
Maximum Entropy supertagger first assigns lexical
categories to the words in a sentence, which are
then combined by the parser using the combinatory
rules and the CKY algorithm. A log-linear model
scores the alternative parses. We use the normal-
form model, which assigns probabilities to single
derivations based on the normal-form derivations in
CCGbank. The features in the model are defined
over local parts of the derivation and include word-
word dependencies. A packed chart representation
allows efficient decoding, with the Viterbi algorithm
finding the most probable derivation.
The supertagger is a key part of the system. It
uses a log-linear model to define a distribution over
the lexical category set for each word and the previ-
ous two categories (Ratnaparkhi, 1996) and the for-
ward backward algorithm efficiently sums over all
histories to give a distibution for each word. These
distributions are then used to assign a set of lexical
categories to each word (Curran et al, 2006).
Supertagging was first defined for LTAG (Banga-
lore and Joshi, 1999), and was designed to increase
parsing speed for lexicalized grammars by allow-
ing a finite-state tagger to do some of the parsing
work. Since the elementary syntactic units in a lexi-
calized grammar ? in LTAG?s case elementary trees
and in CCG?s case lexical categories ? contain a sig-
nificant amount of grammatical information, com-
bining them together is easier than the parsing typi-
cally performed by phrase-structure parsers. Hence
10
Bangalore and Joshi (1999) refer to supertagging as
almost parsing.
Supertagging has been especially successful for
CCG: Clark and Curran (2004a) demonstrates the
considerable increases in speed that can be obtained
through use of a supertagger. The supertagger in-
teracts with the parser in an adaptive fashion. Ini-
tially the supertagger assigns a small number of cat-
egories, on average, to each word in the sentence,
and the parser attempts to create a spanning analysis.
If this is not possible, the supertagger assigns more
categories, and this process continues until a span-
ning analysis is found. The number of categories as-
signed to each word is determined by a parameter ?
in the supertagger: all categories are assigned whose
forward-backward probabilities are within ? of the
highest probability category (Curran et al, 2006).
Clark and Curran (2004a) also shows how the su-
pertagger can reduce the size of the packed charts to
allow discriminative log-linear training. However,
even with the use of a supertagger, the packed charts
for the complete CCGbank require over 20 GB of
RAM. Reading the training instances into memory
one at a time and keeping a record of the relevant
feature counts would be too slow for practical de-
velopment, since the log-linear model requires hun-
dreds of iterations to converge. Hence the packed
charts need to be stored in memory. In Clark and
Curran (2004b) we use a cluster of 45 machines, to-
gether with a parallel implementation of the BFGS
training algorithm, to solve this problem.
The need for cluster computing resources presents
a barrier to the development of further CCG pars-
ing models. Hockenmaier and Steedman (2002) de-
scribe a generative model for CCG, which only re-
quires a non-iterative counting process for training,
but it is generally acknowledged that discrimina-
tive models provide greater flexibility and typically
higher performance. In this paper we propose the
perceptron algorithm as a solution. The perceptron
is an online learning algorithm, and so the param-
eters are updated one training instance at a time.
However, the key difference compared with the log-
linear training is that the perceptron converges in
many fewer iterations, and so it is practical to read
the training instances into memory one at a time.
The difficulty in using the perceptron for training
phrase-structure parsing models is the need for an
efficient decoder (since perceptron training essen-
tially is decoding). Here we exploit the lexicalized
nature of CCG by using the supertagger to restrict the
size of the charts over which Viterbi decoding is per-
formed, resulting in an extremely effcient decoder.
In fact, the decoding is so fast that we can estimate a
state-of-the-art discriminative parsing model in only
a few hours on a single machine.
3 Perceptron Training
The parsing problem is to find a mapping from a set
of sentences x ? X to a set of parses y ? Y . We
assume that the mapping F is represented through a
feature vector ?(x, y) ? Rd and a parameter vector
? ? Rd in the following way (Collins, 2002):
F (x) = argmax
y?GEN(x)
?(x, y) ? ? (1)
where GEN(x) denotes the set of possible parses for
sentence x and ?(x, y) ? ? =
?
i ?i?i(x, y) is the
inner product. The learning task is to set the parame-
ter values (the feature weights) using the training set
as evidence, where the training set consists of ex-
amples (xi, yi) for 1 ? i ? N . The decoder is an
algorithm which finds the argmax in (1).
In this paper, Y is the set of possible CCG deriva-
tions and GEN(x) enumerates the set of derivations
for sentence x. We use the same feature representa-
tion ?(x, y) as in Clark and Curran (2004b), to allow
comparison with the log-linear model. The features
are defined in terms of local subtrees in the deriva-
tion, consisting of a parent category plus one or
two children. Some features are lexicalized, encod-
ing word-word dependencies. Features are integer-
valued, counting the number of times some configu-
ration occurs in a derivation.
GEN(x) is defined by the CCG grammar, plus the
supertagger, since the supertagger determines how
many lexical categories are assigned to each word
in x (through the ? parameter). Rather than try to
recreate the adaptive supertagging described in Sec-
tion 2 for training, we simply fix the the value of ? so
that GEN(x) is the set of derivations licenced by the
grammar for sentence x, given that value. ? is now
a parameter of the training process which we deter-
mine experimentally using development data. The ?
parameter can be thought of as determining the set
of incorrect derivations which the training algorithm
11
uses to ?discriminate against?, with a smaller value
of ? resulting in more derivations.
3.1 Feature Forests
The same decoder is used for both training and test-
ing: the Viterbi algorithm. However, the packed
representation of GEN(x) in each case is different.
When running the parser, a lot of grammatical in-
formation is stored in order to produce linguistically
meaningful output. For training, all that is required
is a packed representation of the features on each
derivation in GEN(x) for each sentence in the train-
ing data. The feature forests described in Miyao and
Tsujii (2002) provide such a representation.
Clark and Curran (2004b) describe how a set of
CCG derivations can be represented as a feature for-
est. The feature forests are created by first building
packed charts for the training sentences, and then
extracting the feature information. Packed charts
group together equivalent chart entries. Entries are
equivalent when they interact in the same manner
with both the generation of subsequent parse struc-
ture and the numerical parse selection. In prac-
tice, this means that equivalent entries have the same
span, and form the same structures and generate the
same features in any further parsing of the sentence.
Back pointers to the daughters indicate how an indi-
vidual entry was created, so that any derivation can
be recovered from the chart.
A feature forest is essentially a packed chart with
only the feature information retained (see Miyao and
Tsujii (2002) and Clark and Curran (2004b) for the
details). Dynamic programming algorithms can be
used with the feature forests for efficient estimation.
For the log-linear parsing model in Clark and Cur-
ran (2004b), the inside-outside algorithm is used to
calculate feature expectations, which are then used
by the BFGS algorithm to optimise the likelihood
function. For the perceptron, the Viterbi algorithm
finds the features corresponding to the highest scor-
ing derivation, which are then used in a simple addi-
tive update process.
3.2 The Perceptron Algorithm
The training algorithm initializes the parameter vec-
tor as all zeros, and updates the vector by decoding
the examples. Each feature forest is decoded with
the current parameter vector. If the output is incor-
Inputs: training examples (xi, yi)
Initialisation: set ? = 0
Algorithm:
for t = 1..T , i = 1..N
calculate zi = argmaxy?GEN(xi) ?(xi, y) ? ?
if zi 6= yi
? = ? +?(xi, yi)? ?(xi, zi)
Outputs: ?
Figure 1: The perceptron training algorithm
rect, the parameter vector is updated by adding the
feature vector of the correct derivation and subtract-
ing the feature vector of the decoder output. Train-
ing typically involves multiple passes over the data.
Figure 1 gives the algorithm, where N is the number
of training sentences and T is the number of itera-
tions over the data.
For all the experiments in this paper, we used the
averaged version of the perceptron. Collins (2002)
introduced the averaged perceptron, as a way of re-
ducing overfitting, and it has been shown to perform
better than the non-averaged version on a number of
tasks. The averaged parameters are defined as fol-
lows: ?s =
?
t=1...T,i=1...N ?
t,i
s /NT where ?t,is is
the value of the sth feature weight after the tth sen-
tence has been processed in the ith iteration.
A naive implementation of the averaged percep-
tron updates the accumulated weight for each fea-
ture after each example. However, the number of
features whose values change for each example is a
small proportion of the total. Hence we use the al-
gorithm described in Daume III (2006) which avoids
unnecessary calculations by only updating the accu-
mulated weight for a feature fs when ?s changes.
4 Experiments
The feature forests were created as follows. First,
the value of the ? parameter for the supertagger was
fixed (for the first set of experiments at 0.004). The
supertagger was then run over the sentences in Sec-
tions 2-21 of CCGbank. We made sure that ev-
ery word was assigned the correct lexical category
among its set (we did not do this for testing). Then
the parser was run on the supertagged sentences, us-
ing the CKY algorithm and the CCG combinatory
rules. We applied the same normal-form restrictions
used in Clark and Curran (2004b): categories can
12
only combine if they have been seen to combine in
Sections 2-21 of CCGbank, and only if they do not
violate the Eisner (1996a) normal-form constraints.
This part of the process requires a few hundred MB
of RAM to run the parser, and takes a few hours for
Sections 2-21 of CCGbank. Any further training
times or memory requirements reported do not in-
clude the resources needed to create the forests.
The feature forests are extracted from the packed
chart representation used in the parser. We only use
a feature forest for training if it contains the correct
derivation (according to CCGbank). Some forests
do not have the correct derivation, even though we
ensure the correct lexical categories are present, be-
cause the grammar used by the parser is missing
some low-frequency rules in CCGbank. The to-
tal number of forests used for the experiments was
35,370 (89% of Sections 2-21) . Only features which
occur at least twice in the training data were used,
of which there are 477,848. The complete set of
forests used to obtain the final perceptron results in
Section 4.1 require 21 GB of disk space.
The perceptron is an online algorithm, updating
the weights after each forest is processed. Each for-
est is read into memory one at a time, decoding is
performed, and the weight values are updated. Each
forest is discarded from memory after it has been
used. Constantly reading forests off disk is expen-
sive, but since the perceptron converges in so few
iterations the training times are reasonable.
In contrast, log-linear training takes hundreds of
iterations to converge, and so it would be impractical
to keep reading the forests off disk. Also, since log-
linear training uses a batch algorithm, it is more con-
venient to keep the forests in memory at all times.
In Clark and Curran (2004b) we use a cluster of 45
machines, together with a parallel implementation
of BFGS, to solve this problem, but need up to 20 GB
of RAM.
The feature forest representation, and our imple-
mentation of it, is so compact that the perceptron
training requires only 20 MB of RAM. Since the su-
pertagger has already removed much of the practical
parsing complexity, decoding one of the forests is
extremely quick, and much of the training time is
taken with continually reading the forests off disk.
However, the training time for the perceptron is still
only around 5 hours for 10 iterations.
model RAM iterations time (mins)
perceptron 20 MB 10 312
log-linear 19 GB 475 91
Table 1: Training requirements for the perceptron
and log-linear models
Table 1 compares the training for the perceptron
and log-linear models. The perceptron was run for
10 iterations and the log-linear training was run to
convergence. The training time for 10 iterations of
the perceptron is longer than the log-linear training,
although the results in Section 4.1 show that the per-
ceptron typically converges in around 4 iterations.
The striking result in the table is the significantly
smaller memory requirement for the perceptron.
4.1 Results
Table 2 gives the first set of results for the averaged
perceptron model. These were obtained using Sec-
tion 00 of CCGbank as development data. Gold-
standard POS tags from CCGbank were used for all
the experiments. The parser provides an analysis for
99.37% of the sentences in Section 00. The F-scores
are based only on the sentences for which there is
an analysis. Following Clark and Curran (2004b),
accuracy is measured using F-score over the gold-
standard predicate-argument dependencies in CCG-
bank. The table shows that the accuracy increases
initially with the number of iterations, but converges
quickly after only 4 iterations. The accuracy after
only one iteration is also surprisingly high.
Table 3 compares the accuracy of the perceptron
and log-linear models on the development data. LP
is labelled precision, LR is labelled recall, and CAT
is the lexical category accuracy. The same feature
forests were used for training the perceptron and
log-linear models, and the same parser and decoding
algorithm were used for testing, so the results for the
two models are directly comparable. The only dif-
ference in each case was the weights file used.1
The table also gives the accuracy for the percep-
tron model (after 6 iterations) when a smaller value
of the supertagger ? parameter is used during the
1Both of these models have parameters which have been
optimised on the development data, in the log-linear case the
Gaussian smoothing parameter and in the perceptron case the
number of training iterations.
13
iteration 1 2 3 4 5 6 7 8 9 10
F-score 85.87 86.28 86.33 86.49 86.46 86.51 86.47 86.52 86.53 86.54
Table 2: Accuracy on the development data for the averaged perceptron (? = 0.004)
model LP LR F CAT
log-linear?=0.004 87.02 86.07 86.54 93.99
perceptron?=0.004 87.11 85.98 86.54 94.03
perceptron?=0.002 87.25 86.20 86.72 94.08
Table 3: Comparison of the perceptron and log-
linear models on the development data
forest creation (with the number of training itera-
tions again optimised on the development data). A
smaller ? value results in larger forests, giving more
incorrect derivations for the training algorithm to
?discriminate against?. Increasing the size of the
forests is no problem for the perceptron, since the
memory requirements are so modest, but this would
cause problems for the log-linear training which is
already highly memory intensive. The table shows
that increasing the number of incorrect derivations
gives a small improvement in performance for the
perceptron.
Table 4 gives the accuracies for the two models
on the test data, Section 23 of CCGbank. Here the
coverage of the parser is 99.63%, and again the ac-
curacies are computed only for the sentences with
an analysis. The figures for the averaged perceptron
were obtained using 6 iterations, with ? = 0.002.
The perceptron slightly outperforms the log-linear
model (although we have not carried out signifi-
cance tests). We justify the use of different ? values
for the two models by arguing that the perceptron is
much more flexible in terms of the size of the train-
ing forests it can handle.
Note that the important result here is that the per-
ceptron model performs at least as well as the log-
linear model. Since the perceptron is considerably
easier to train, this is a useful finding. Also, since
the log-linear parsing model is a Conditional Ran-
dom Field (CRF), the results suggest that the percep-
tron should be compared with a CRF for other tasks
for which the CRF is considered to give state-of-the-
art results.
model LP LR F CAT
log-linear?=0.004 87.39 86.51 86.95 94.07
perceptron?=0.002 87.50 86.62 87.06 94.08
Table 4: Comparison of the perceptron and log-
linear models on the test data
5 Order of Training Examples
As an example of the flexibility of our discrimina-
tive training framework, we investigated the order in
which the training examples are presented to the on-
line perceptron learner. These experiments were par-
ticularly easy to carry out in our framework, since
the 21 GB file containing the complete set of training
forests can be sampled from directly. We stored the
position on disk of each of the forests, and selected
the forests one by one, according to some order.
The first set of experiments investigated ordering
the training examples by sentence length. Buttery
(2006) found that a psychologically motivated Cate-
gorial Grammar learning system learned faster when
the simplest linguistic examples were presented first.
Table 5 shows the results both when the shortest sen-
tences are presented first and when the longest sen-
tences are presented first. Training on the longest
sentences first provides the best performance, but is
no better than the standard ordering.
For the random ordering experiments, forests
were randomly sampled from the complete 21 GB
training file on disk, without replacement. The
new forests file was then used for the averaged-
perceptron training, and this process was repeated
9 times.
The number of iterations for each training run was
optimised in terms of the accuracy of the resulting
model on the development data. There was little
variation among the models, with the best model
scoring 86.84% F-score on the development data
and the worst scoring 86.63%. Table 6 shows that
the performance of this best model on the test data
is only slightly better than the model trained using
the CCGbank ordering.
14
iteration 1 2 3 4 5 6
Standard order 86.14 86.30 86.53 86.61 86.69 86.72
Shortest first 85.98 86.41 86.57 86.56 86.54 86.53
Longest first 86.25 86.48 86.66 86.72 86.74 86.75
Table 5: F-score of the averaged perceptron on the development data for different data orderings (? = 0.002)
perceptron model LP LR F CAT
standard order 87.50 86.62 87.06 94.08
best random order 87.52 86.72 87.12 94.12
averaged 87.53 86.67 87.10 94.09
Table 6: Comparison of various perceptron models
on the test data
Finally, we used the 10 models (including the
model from the original training set) to investigate
model averaging. Corston-Oliver et al (2006) mo-
tivate model averaging for the perceptron in terms
of Bayes Point Machines. The averaged percep-
tron weights resulting from each permutation of the
training data were simply averaged to produce a new
model. Table 6 shows that the averaged model again
performs only marginally better than the original
model, and not as well as the best-performing ?ran-
dom? model, which is perhaps not surprising given
the small variation among the performances of the
component models.
In summary, the perceptron learner appears highly
robust to the order of the training examples, at least
for this parsing task.
6 Comparison with Other Work
Taskar et al (2004) investigate discriminative train-
ing methods for a phrase-structure parser, and also
use dynamic programming for the decoder. The key
difference between our work and theirs is that they
are only able to train on sentences of 15 words or
less, because of the expense of the decoding.
There is work on discriminative models for de-
pendency parsing (McDonald, 2006); since there
are efficient decoding algorithms available (Eisner,
1996b), complete resources such as the Penn Tree-
bank can used for estimation, leading to accurate
parsers. There is also work on discriminative mod-
els for parse reranking (Collins and Koo, 2005). The
main drawback with this approach is that the correct
parse may get lost in the first phase.
The existing work most similar to ours is Collins
and Roark (2004). They use a beam-search decoder
as part of a phrase-structure parser to allow practical
estimation. The main difference is that we are able
to store the complete forests for training, and can
guarantee that the forest contains the correct deriva-
tion (assuming the grammar is able to generate it
given the correct lexical categories). The downside
of our approach is the restriction on the locality of
the features, to allow dynamic programming. One
possible direction for future work is to compare the
search-based approach of Collins and Roark with
our DP-based approach.
In the tagging domain, Collins (2002) compared
log-linear and perceptron training for HMM-style
tagging based on dynamic programming. Our work
could be seen as extending that of Collins since we
compare log-linear and perceptron training for a DP-
based wide-coverage parser.
7 Conclusion
Investigation of discriminative training methods is
one of the most promising avenues for breaking
the current bottleneck in parsing performance. The
drawback of these methods is the need for an effi-
cient decoder. In this paper we have demonstrated
how the lexicalized nature of CCG can be used to
develop a very efficient decoder, which leads to a
practical development environment for discrimina-
tive training.
We have also provided the first comparison of a
perceptron and log-linear model for a wide-coverage
phrase-structure parser. An advantage of the percep-
tron over the log-linear model is that it is consider-
ably easier to train, requiring 1/1000th of the mem-
ory requirements and converging in only 4 iterations.
Given that the global log-linear model used here
(CRF) is thought to provide state-of-the-art perfor-
mance for many NLP tasks, it is perhaps surprising
15
that the perceptron performs as well. The evalua-
tion in this paper was based solely on CCGbank, but
we have shown in Clark and Curran (2007) that the
CCG parser gives state-of-the-art performance, out-
performing the RASP parser (Briscoe et al, 2006)
by over 5% on DepBank. This suggests the need for
more comparisons of CRFs and discriminative meth-
ods such as the perceptron for other NLP tasks.
Acknowledgements
James Curran was funded under ARC Discovery
grants DP0453131 and DP0665973.
References
Srinivas Bangalore and Aravind Joshi. 1999. Supertagging:
An approach to almost parsing. Computational Linguistics,
25(2):237?265.
Ted Briscoe, John Carroll, and Rebecca Watson. 2006. The
second release of the RASP system. In Proceedings of
the Interactive Demo Session of COLING/ACL-06, Sydney,
Austrailia.
Paula Buttery. 2006. Computational models for first language
acquisition. Technical Report UCAM-CL-TR-675, Univer-
sity of Cambridge Computer Laboratory.
Stephen Clark and James R. Curran. 2004a. The importance of
supertagging for wide-coverage CCG parsing. In Proceed-
ings of COLING-04, pages 282?288, Geneva, Switzerland.
Stephen Clark and James R. Curran. 2004b. Parsing the WSJ
using CCG and log-linear models. In Proceedings of the
42nd Meeting of the ACL, pages 104?111, Barcelona, Spain.
Stephen Clark and James R. Curran. 2007. Formalism-
independent parser evaluation with CCG and DepBank. In
Proceedings of the 45th Annual Meeting of the ACL, Prague,
Czech Republic.
Michael Collins and Terry Koo. 2005. Discriminative rerank-
ing for natural language parsing. Computational Linguistics,
31(1):25?69.
Michael Collins and Brian Roark. 2004. Incremental parsing
with the perceptron algorithm. In Proceedings of the 42nd
Meeting of the ACL, pages 111?118, Barcelona, Spain.
Michael Collins. 2002. Discriminative training methods for
hidden markov models: Theory and experiments with per-
ceptron algorithms. In Proceedings of the 40th Meeting of
the ACL, Philadelphia, PA.
S. Corston-Oliver, A. Aue, K. Duh, and E. Ringger. 2006. Mul-
tilingual dependency parsing using bayes point machines. In
Proceedings of HLT/NAACL-06, New York.
James R. Curran, Stephen Clark, and David Vadas. 2006.
Multi-tagging for lexicalized-grammar parsing. In Proceed-
ings of COLING/ACL-06, pages 697?704, Sydney, Aus-
trailia.
Jason Eisner. 1996a. Efficient normal-form parsing for Com-
binatory Categorial Grammar. In Proceedings of the 34th
Meeting of the ACL, pages 79?86, Santa Cruz, CA.
Jason Eisner. 1996b. Three new probabilistic models for de-
pendency parsing: An exploration. In Proceedings of the
16th COLING Conference, pages 340?345, Copenhagen,
Denmark.
Yoav Freund and Robert E. Schapire. 1999. Large margin clas-
sification using the perceptron algorithm. Machine Learn-
ing, 37(3):277?296.
Julia Hockenmaier and Mark Steedman. 2002. Generative
models for statistical parsing with Combinatory Categorial
Grammar. In Proceedings of the 40th Meeting of the ACL,
pages 335?342, Philadelphia, PA.
Julia Hockenmaier. 2003. Data and Models for Statistical
Parsing with Combinatory Categorial Grammar. Ph.D. the-
sis, University of Edinburgh.
Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi Chi, and
Stefan Riezler. 1999. Estimators for stochastic ?unification-
based? grammars. In Proceedings of the 37th Meeting of the
ACL, pages 535?541, University of Maryland, MD.
Robert Malouf and Gertjan van Noord. 2004. Wide coverage
parsing with stochastic attribute value grammars. In Pro-
ceedings of the IJCNLP-04 Workshop: Beyond shallow anal-
yses - Formalisms and statistical modeling for deep analyses,
Hainan Island, China.
Ryan McDonald. 2006. Discriminative Training and Spanning
Tree Algorithms for Dependency Parsing. Ph.D. thesis, Uni-
versity of Pennsylvania.
Yusuke Miyao and Jun?ichi Tsujii. 2002. Maximum entropy
estimation for feature forests. In Proceedings of the Human
Language Technology Conference, San Diego, CA.
Yusuke Miyao and Jun?ichi Tsujii. 2005. Probabilistic dis-
ambiguation models for wide-coverage HPSG parsing. In
Proceedings of the 43rd meeting of the ACL, pages 83?90,
University of Michigan, Ann Arbor.
Adwait Ratnaparkhi. 1996. A maximum entropy part-of-
speech tagger. In Proceedings of the EMNLP Conference,
pages 133?142, Philadelphia, PA.
Stefan Riezler, Tracy H. King, Ronald M. Kaplan, Richard
Crouch, John T. Maxwell III, and Mark Johnson. 2002.
Parsing the Wall Street Journal using a Lexical-Functional
Grammar and discriminative estimation techniques. In Pro-
ceedings of the 40th Meeting of the ACL, pages 271?278,
Philadelphia, PA.
Mark Steedman. 2000. The Syntactic Process. The MIT Press,
Cambridge, MA.
B. Taskar, D. Klein, M. Collins, D. Koller, and C. Manning.
2004. Max-margin parsing. In Proceedings of the EMNLP
conference, pages 1?8, Barcelona, Spain.
Joseph Turian and I. Dan Melamed. 2006. Advances in dis-
criminative parsing. In Proceedings of COLING/ACL-06,
pages 873?880, Sydney, Australia.
16
Proceedings of the 10th Conference on Parsing Technologies, pages 39?47,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Improving the Efficiency of a Wide-Coverage CCG Parser
Bojan Djordjevic and James R. Curran
School of Information Technologies
University of Sydney
NSW 2006, Australia
{bojan,james}@it.usyd.edu.au
Stephen Clark
Computing Laboratory
Oxford University
Wolfson Building, Parks Road
Oxford, OX1 3QD, UK
stephen.clark@comlab.ox.ac.uk
Abstract
The C&C CCG parser is a highly efficient
linguistically motivated parser. The effi-
ciency is achieved using a tightly-integrated
supertagger, which assigns CCG lexical cat-
egories to words in a sentence. The integra-
tion allows the parser to request more cat-
egories if it cannot find a spanning anal-
ysis. We present several enhancements to
the CKY chart parsing algorithm used by the
parser. The first proposal is chart repair,
which allows the chart to be efficiently up-
dated by adding lexical categories individu-
ally, and we evaluate several strategies for
adding these categories. The second pro-
posal is to add constraints to the chart which
require certain spans to be constituents. Fi-
nally, we propose partial beam search to fur-
ther reduce the search space. Overall, the
parsing speed is improved by over 35% with
negligible loss of accuracy or coverage.
1 Introduction
A recent theme in parsing research has been the
application of statistical methods to linguistically
motivated grammars, for example LFG (Kaplan et
al., 2004; Cahill et al, 2004), HPSG (Toutanova
et al, 2002; Malouf and van Noord, 2004), TAG
(Sarkar and Joshi, 2003) and CCG (Hockenmaier
and Steedman, 2002; Clark and Curran, 2004b). The
attraction of linguistically motivated parsers is the
potential to produce rich output, in particular the
predicate-argument structure representing the under-
lying meaning of a sentence. The disadvantage of
such parsers is that they are typically not very effi-
cient, parsing a few sentences per second on com-
modity hardware (Kaplan et al, 2004). The C&C
CCG parser (Clark and Curran, 2004b) is an order
of magnitude faster, but is still limited to around 25
sentences per second.
The key to efficient CCG parsing is a finite-state
supertagger which performs much of the parsing
work (Bangalore and Joshi, 1999). CCG is a lex-
icalised grammar formalism, in which elementary
syntactic structures ? in CCG?s case lexical cate-
gories expressing subcategorisation information ?
are assigned to the words in a sentence. CCG su-
pertagging can be performed accurately and effi-
ciently by a Maximum Entropy tagger (Clark and
Curran, 2004a). Since the lexical categories contain
so much grammatical information, assigning them
with low average ambiguity leaves the parser, which
combines them together, with much less work to do
at parse time. Hence Bangalore and Joshi (1999), in
the context of LTAG parsing, refer to supertagging as
almost parsing.
Clark and Curran (2004a) presents a novel
method of integrating the supertagger and parser:
initially only a small number of categories, on av-
erage, is assigned to each word, and the parser at-
tempts to find a spanning analysis using the CKY
chart-parsing algorithm. If one cannot be found, the
parser requests more categories from the supertagger
and builds the chart again from scratch. This process
repeats until the parser is able to build a chart con-
taining a spanning analysis.1
1Tsuruoka and Tsujii (2004) investigate a similar idea in the
context of the CKY algorithm for a PCFG.
39
The supertagging accuracy is high enough that
the parser fails to find a spanning analysis using the
initial category assignment in approximately 4% of
Wall Street Journal sentences (?). However, parsing
this 4%, which largely consists of the longer sen-
tences, is disproportionately expensive.
This paper describes several modifications to the
C&C parser which improve parsing efficiency with-
out reducing accuracy or coverage by reducing the
impact of the longer sentences. The first involves
chart repair, where the CKY chart is repaired when
extra lexical categories are added (according to the
scheme described above), instead of being rebuilt
from scratch. This allows an even tighter integra-
tion of the supertagger, in that the parser is able to
request individual categories. We explore methods
for choosing which individual categories to add, re-
sulting in an 11% speed improvement.
The next modification involves parsing with con-
straints, so that certain spans are required to be con-
stituents. This reduces the search space consider-
ably by eliminating a large number of constituents
which cross the boundaries of these spans. The best
set of constraints results in a 10% speed improve-
ment over the original parser. These constraints are
general enough that they could be applied to any
constituency-based parser. Finally, we experiment
with several beam strategies to reduce the search
space, finding that a partial beam which operates on
part of the chart is most effective, giving a further
6.1% efficiency improvement.
The chart repair and constraints interact in an in-
teresting, and unexpected, manner when combined,
giving a 35.7% speed improvement overall without
any loss in accuracy or coverage. This speed im-
provement is particularly impressive because it in-
volves techniques which only apply to 4% of Wall
Street Journal sentences.
2 The CCG Parser
Clark and Curran (2004b) describes the CCG parser.
The grammar used by the parser is extracted from
CCGbank, a CCG version of the Penn Treebank
(Hockenmaier, 2003). The grammar consists of 425
lexical categories plus a small number of combi-
natory rules which combine the categories (Steed-
man, 2000). A Maximum Entropy supertagger first
assigns lexical categories to the words in a sen-
tence, which are then combined by the parser using
the combinatory rules. A log-linear model scores
the alternative parses. We use the normal-form
model, which assigns probabilities to single deriva-
tions based on the normal-form derivations in CCG-
bank. The features in the model are defined over
local parts of the derivation and include word-word
dependencies. A packed chart representation allows
efficient decoding, with the Viterbi algorithm find-
ing the most probable derivation.
The supertagger uses a log-linear model to de-
fine a distribution over the lexical category set for
each word and the previous two categories (Ratna-
parkhi, 1996) and the forward backward algorithm
efficiently sums over all histories to give a distribu-
tion for each word. These distributions are then used
to assign a set of lexical categories to each word (?).
The number of categories in each set is determined
by a parameter ?: all categories are assigned whose
forward-backward probabilities are within ? of the
highest probability category (?). If the parser can-
not then find a spanning analysis, the value of ? is
reduced ? so that more lexical categories are as-
signed ? and the parser tries again. This process re-
peats until an analysis spanning the whole sentence
is found.
In our previous work, when the parser was unable
to find a spanning analysis, the chart was destroyed
and then rebuilt from scratch with more lexical cate-
gories assigned to each word. However, this rebuild-
ing process is wasteful because the new chart is al-
ways a superset of the old one and could be created
by just updating the previous chart. We describe the
chart repair process in Section 3 which allows addi-
tional categories to be assigned to an existing chart
and the CKY algorithm run over just those parts of
the chart which require modification.
2.1 Chart Parsing
The parser uses the CKY chart parsing algorithm
(Kasami, 1965; Younger, 1967) described in Steed-
man (2000). The CKY algorithm applies naturally to
CCG since the grammar is binary. It builds the chart
bottom-up, starting with the lexical categories span-
ning single words, incrementally increasing the span
until the whole sentence is covered. Since the con-
stituents are built in order of span size, at every stage
40
all the sub-constituents which could be used to cre-
ate a particular new constituent are already present
in the chart.
The charts are packed by grouping together equiv-
alent chart entries, which allows a large number of
derivations to be represented efficiently. Entries are
equivalent when they interact in the same manner
with both the generation of subsequent parse struc-
ture and the statistical parse selection. In practice,
this means that equivalent entries have the same
span; form the same structures, i.e. the remain-
ing derivation plus dependencies, in any subsequent
parsing; and generate the same features in any sub-
sequent parsing.
The Viterbi algorithm is used to find the most
probable derivation from a packed chart. For each
equivalence class of individual entries, we record the
entry at the root of the subderivation which has the
highest score for the class. The equivalence classes
are defined so that any other individual entry can-
not be part of the highest scoring derivation for the
sentence. The highest-scoring subderivations can
be calculated recursively using the highest-scoring
equivalence classes that were combined to create the
individual entry.
Given a sentence of n words, we define pos ?
{0, . . . , n ? 1} to be the starting position of an en-
try in the chart (represented by a CCG category) and
span ? {1, . . . , n} its length. Let cell(pos, span)
be the set of categories which span the sentence from
pos to pos + span. These will be combinations of
categories in cell(pos, k) and cell(pos+k, span?k)
for all k ? {1, . . . , span? 1}. The chart is a two di-
mensional array indexed by pos and span. The valid
(pos, span) pairs correspond to pos + span ? n,
that is, to spans that do not extend beyond the end
of the sentence. The squares represent valid cells in
Figure 1. The span from position 3 with length 4,
i.e. cell(3, 4), is marked with a diamond in Figure 2.
3 Chart Repair
The parser interacts with the supertagger by decreas-
ing the value of the ? parameter when a spanning
analysis cannot be found for a sentence. This has
the effect of adding more lexical categories to the
chart. Instead of rebuilding the chart from scratch
when new categories are added, it can be repaired
affected cells
cell with a new
category added10
5
0 1 2 3 4 5 6 7 8 9
1
span
pos
2
3
4
6
7
8
9
Figure 1: Cells affected by chart repair.
by modifying cells that are affected by the new cat-
egories. Considering the case where a single lexical
category is added to the ith word in an n word sen-
tence, the new category can only affect the cells that
satisfy pos ? i and pos+ span > i. These cells are
shown in Figure 1 for the word at position 3.
The number of affected cells is (n?pos)(pos+1),
and so the average over the sentence is approxi-
mately 1n
? n?1
0 (n ? p)(p + 1) dp ?
n2
6 cells. The
total number of cells in the chart is n(n+1)2 . The chart
can therefore be repaired bottom up, in CKY order,
by updating a third of the cells on average.
Additional lexical categories for a word are in-
serted into the corresponding cell in the bottom row,
with the additional categories being marked as new.
For each cell C in the second row, each pair of cells
A and B is considered whose spans combine to cre-
ate the span of C. In the original CKY, all categories
from A are combined with all categories from B. In
chart repair, categories are only combined if at least
one of them is new, because otherwise the result is
already in C. The categories added to C are marked,
and the process is repeated for all affected cells in
CKY order.
Chart repair speeds up parsing for two reasons.
First, it reuses previous computations and eliminates
wasteful rebuilding of the chart. Second, it allows
lexical categories to be added to the chart one at a
41
affected cells
invalid cells
required cell
6 7 8 9
1
span
pos
2
3
4
6
7
8
9
10
5
0 1 2 3 4 5
Figure 2: Cells affected by adding a constraint.
time until a spanning derivation is found. In the orig-
inal approach extra categories were added in bulk by
changing the ? level, which significantly increased
the average ambiguity. Chart repair allows the min-
imum amount of ambiguity to be added for a span-
ning derivation to be found.
The C&C parser has a predefined limit on the num-
ber of categories in the chart. If this is exceeded
before a spanning analysis is found then the parser
fails on the sentence. Our new strategy allows a
chart containing a spanning analysis to be built with
the minimum number of categories possible. This
means that some sentences can now be parsed that
would have previously exceeded the limit, slightly
increasing coverage.
3.1 Category selection
The order in which lexical categories are added to
the chart will impact on parsing speed and accu-
racy, and so we evaluate several alternatives. The
first ordering (? VALUE) is by decreasing ? value,
where the ? value is the ratio between the probabil-
ity of the most likely category and the probability of
the given category for that word.2 The second or-
dering (PROB) is by decreasing category probability
2We are overloading the use of ? for convenience. Here, ?
refers to the variable ratio dependent on the particular category,
whereas the ? value used in supertagging is a cutoff applied to
the variable ratio.
as assigned by the supertagger using the forward-
backward algorithm.
We also investigated ordering categories using in-
formation from the chart. Examining the sentences
which required chart repair showed that, when a
word is missing the correct category, the cells af-
fected (as defined in Section 3) by the cell are often
empty. The CHART ordering uses this observation to
select the next lexical category to assign. It selects
the word corresponding to the cell with the high-
est number of empty affected cells, and then adds
the highest probability category not in the chart for
that word. Finally, we included a RANDOM ordering
baseline for comparison purposes.
4 Constraints
The set of possible derivations can be constrained
if we know in advance that a particular span is re-
quired to be the yield of a single constituent in the
correct parse. A constraint on span p reduces the
search space because p must be the yield of a single
cell. This means that cells with yields that cross the
boundary of p cannot be part of a correct derivation,
and do not need to be considered (the grey cells in
Figure 2). In addition, if a cell yields p as a prefix or
suffix (the hashed cells in Figure 2) then it also has
constraints on how it can be created.
Figure 2 shows an example constraint requiring
words 3?6 to be a constituent, which corresponds to
p = cell(3, 4). Consider cell(3, 7): it yields words
3?9 and so contains p as the prefix. Normally it can
be created by combining cell(3, 1) with cell(4, 6),
or cell(3, 2)with cell(5, 5), and so on up to cell(3, 6)
with cell(9, 1). However the first three combinations
are not allowed because the second child crosses the
boundary of p. This gives a lower limit for the span
of the left child. Similarly, if p is the suffix of the
span of a cell then there is a lower limit on the span
of the right child.
As the example demonstrates, a single constraint
can eliminate many combinations, reducing the
search space significantly, and thus improving pars-
ing efficiency.
4.1 Creating Constraints
How can we know in advance that the correct deriva-
tion must yield specific spans, since this appears to
require knowledge of the parse itself? We have ex-
42
plored constraints derived from shallow parsing and
from the raw sentence. Our results demonstrate that
simple constraints can reduce parsing time signifi-
cantly without loss of coverage or accuracy.
Chunk tags were used to create constraints. We
experimented with both gold standard chunks from
the Penn Treebank and also chunker output from the
C&C chunk tagger. The tagger is very similar to the
Maximum Entropy POS tagger described in Curran
and Clark (2003). Only NP chunks were used be-
cause the accuracy of the tagger for other chunks is
lower. The Penn Treebank chunks required modi-
fication because CCGbank analyses some construc-
tions differently. We also created longer NPs by con-
catenating adjacent base NPs, for example in the case
of possessives.
A number of punctuation constraints were used
and had a significant impact especially for longer
sentences. There are a number of punctuation rules
in CCGbank which absorb a punctuation mark by
combining it with a category and returning a cate-
gory of the same type. These rules are very produc-
tive, combining with many constituent types. How-
ever, in CCGbank the sentence final punctuation is
always attached at the root. A constraint on the first
n ? 1 words was added to force the parser to only
attach the sentence final punctuation once the rest of
the sentence has been parsed.
Constraints are placed around parenthesised and
quoted phrases that usually form constituents be-
fore attaching elsewhere. Constraints are also placed
around phrases bound by colons, semicolons, or hy-
phens. These constraints are especially effective
for long sentences with many clauses separated by
semicolons, reducing the sentence to a number of
smaller units which significantly improves parsing
efficiency.
In some instances, adding constraints can be
harmful to parsing efficiency and/or accuracy. Lack
of precision in the constraints can come from noisy
output from NLP components, e.g. the chunker, or
from rules which are not always applicable, e.g.
punctuation constraints. We find that the punctua-
tion constraints are particularly effective while the
gold standard chunks are required to gain any ben-
efit for the NP constraints. Adding constraints also
has the potential to increase coverage because the re-
duced search space means that longer sentences can
be parsed without exceeding the pre-defined limits
on chart size.
5 Selective Beam Search
Beam search involves greedy elimination of low
probability partial derivations before they can form
complete derivations. It is used in many parsers to
reduce the search space, for example Collins (2003).
We use a variable width beam where all categories
c in a particular cell C that satisfy score(c) <
max{score(x)|x ? C} ? B, for some beam cut-
off B, are removed. The category scores score(c)
are log probabilities.
In the C&C parser, the entire packed chart is con-
structed first and then the spanning derivations are
marked. Only the partial derivations that form part
of spanning derivations are scored to select the best
parse, which is a small fraction of the categories in
the chart. Because the categories are scored with
a complex statistical model with a large number of
features, the time spent calculating scores is signif-
icant. We found that applying a beam to every cell
during the construction of the chart was more expen-
sive than not using the beam at all. When the beam
was made harsh enough to be worthwhile, it reduced
accuracy and coverage significantly.
We propose selective beam search where the
beam is only applied to spans of particular lengths.
The shorter spans are most important to cull because
there are many more of them and removing them has
the largest impact in terms of reducing the search
space. However, the supertagger already acts like
a beam at the lexical category level and the parser
model has fewer features at this level, so the beam
may be more accurate for longer spans. We there-
fore expect the beam to be most effective for spans
of intermediate length.
6 Experiments
The parser was trained on CCGbank sections 02-21
and section 00 was used for development. The per-
formance is measured in terms of coverage, F-score
and parsing time. The F-score is for labelled depen-
dencies compared against the predicate-argument
dependencies in CCGbank. The time reported in-
cludes loading the grammar and statistical model,
which takes around 5 seconds, and parsing the 1913
43
sentences in section 00.
The failure rate (opposite of coverage) is broken
down into sentences with length ? 40 and > 40
because longer sentences are more difficult to parse
and the C&C parser already has very high coverage
on shorter sentences. There are 1784 1-40 word sen-
tences and 129 41+ word sentences. The average
length and standard deviation in the 41+ set are 50.8
and 31.5 respectively.
All experiments used gold standard POS tags.
Original and REPAIR do not use constraints. The
NP(GOLD) experiments use Penn Treebank gold
standard NP chunks to determine an upper bound
on the utility of chunk constraints. The times re-
ported for NP(C&C) using the C&C chunker include
the time to load the chunker model and run the chun-
ker (around 1.3 seconds). PUNCT adds all of the
punctuation constraints.
Finally the best system was compared against the
original parser on section 23, which has 2257 sen-
tences of length 1-40 and 153 of length 41+. The
maximum length is only 65, which explains the high
coverage for the 41+ section.
6.1 Chart Repair Results
The results in Table 1 show that chart repair gives
an immediate 11.1% improvement in speed and a
small 0.21% improvement in accuracy. 96.1% of
sentences do not require chart repair because they
are successfully parsed using the initial set of lexi-
cal categories supplied by the supertagger. Hence,
11% is a significant improvement for less than 4%
of the sentences.
We believe the accuracy was improved (on top of
the efficiency) because of the way the repair process
adds new categories. Adding categories individually
allows the parser to be influenced by the probabil-
ities which the supertagger assigns, which are not
directly modelled in the parser. If we were to add
this information from the supertagger into the parser
statistical model directly we would expect almost
no accuracy difference between the original method
and chart repair.
Table 2 shows the impact of different category
ordering approaches for chart repair (with PUNCT
constraints). The most effective approach is to use
the information from the chart about the proportion
of empty cells, which adds half as many categories
METHOD secs % F-SCORE CATS
RANDOM 70.2 -16.2 86.57 23.1
? VALUE 60.4 ? 86.66 15.7
PROB 60.1 0.5 86.65 14.3
CHART 57.2 5.3 86.61 7.0
Table 2: Category ordering for chart repair.
on average as the ? value and probability based ap-
proaches. All of our approaches significantly out-
perform randomly selecting extra categories. The
CHART category ordering is used for the remaining
experiments.
6.2 Constraints Results
The results in Table 1 show that, without chart re-
pair, using gold standard noun phrases does not im-
prove efficiency, while using noun phrases identi-
fied by the C&C chunker decreases speed by 10.8%.
They both also slightly reduce parsing accuracy.
The number of times the parsing process had to be
restarted with the constraints removed, was more
costly than the reduction of the search space. This
is unsurprising because the chunk data was not ob-
tained from CCGbank and the chunker is not ac-
curate enough for the constraints to improve pars-
ing efficiency. The most frequent inconsistencies
between CCGbank and chunks extracted from the
Penn Treebank were fixed in a preprocessing step as
explained in Section 4.1, but the less frequent con-
structions are still problematic.
The best results for parsing with constraints (with-
out repair) were with both punctuation and gold
standard noun phrase constraints, with 20.5% im-
provement in speed and 0.42% in coverage, but an
F-score penalty of 0.3%. This demonstrates the pos-
sible efficiency gain with a perfect chunker ? the
corresponding results with the C&C chunker are still
worse than without constraints. The best results
without a decrease in accuracy use only punctuation
constraints, with 10.4% increase in speed and 0.37%
in coverage. The punctuation constraints also have
the advantage of being simple to implement.
The best overall efficiency gain was obtained
when punctuation and gold standard noun phrases
were used with chart repair, with a 45.4% improve-
ment in speed and 0.63% in coverage, and a 0.4%
drop in accuracy. The best results without a drop in
44
METHOD secs % F-SCORE COVER n ? 40 n > 40
Original 88.3 ? 86.54 98.85 0.392 11.63
REPAIR 78.5 11.1 86.75 99.01 0.336 10.08
NP(GOLD) 88.4 -0.1 86.27 99.06 0.224 10.85
NP(C&C) 97.8 -10.8 86.31 99.16 0.224 9.30
PUNCT 79.1 10.4 86.56 99.22 0.168 9.30
NP(GOLD) + PUNCT 69.8 20.5 86.24 99.27 0.168 8.53
NP(C&C) + PUNCT 97.0 -9.9 86.31 99.16 0.168 10.08
NP(GOLD) + REPAIR 65.0 26.4 86.04 99.37 0.224 6.20
NP(C&C) + REPAIR 77.5 12.2 86.35 99.37 0.224 6.20
PUNCT + REPAIR 57.2 35.2 86.61 99.48 0.168 5.43
NP(GOLD) + PUNCT + REPAIR 48.2 45.4 86.14 99.48 0.168 5.43
NP(C&C) + PUNCT + REPAIR 63.2 28.4 86.43 99.53 0.163 3.88
Table 1: Parsing performance on section 00 with constraints and chart repair
METHOD secs % F-SCORE COVER n ? 40 n > 40
Original 88.3 ? 86.54 98.85 0.392 11.63
PUNCT 79.1 10.4 86.56 99.22 0.168 9.30
REPAIR 78.5 11.1 86.75 99.01 0.336 10.08
PUNCT + REPAIR 57.2 35.2 86.61 99.48 0.168 5.43
PUNCT + REPAIR + BEAM 52.4 40.7 86.56 99.48 0.168 5.43
Table 3: Best performance on Section 00
accuracy were with only punctuation constraints and
chart repair, with improvements of 35.2% speed and
0.63% coverage. Coverage on both short and long
sentences is improved ? the best results show a 43%
and 67% decrease in failure rate for sentence lengths
in the ranges 1-40 and 41+ respectively.
6.3 Partial Beam Results
We found that using the selective beam on 1?2 word
spans had negligible impact on speed and accuracy.
Using the beam on 3?4 word spans had the most im-
pact without accuracy penalty, improving efficiency
by another ?5%. Experiments with the selective
beam on longer spans continued to improve effi-
ciency, but with a much greater penalty in F-score,
e.g. a further ?5% at a cost of 0.5% F-score for 3?6
word spans. However, we are interested in efficiency
improvements with negligible cost to accuracy.
6.4 Overall Results
Table 3 summarises the results for section 00. The
chart repair and punctuation constraints individually
increase parsing efficiency by around 10%. How-
ever, the most interesting result is that in combina-
tion they increase efficiency by over 35%. This is
because the cost of rebuilding the chart when the
constraints are incorrect has been significantly re-
duced by chart repair. Finally, the use of the selec-
tive beam gives modest improvement of 5.5%. The
overall efficiency gain on section 00 is 40.7% with
an additional 0.5% coverage, halving both the num-
ber of short and long sentences that fail to be parsed.
Table 4 shows the performance of the punctuation
constraints, chart repair and selective beam system
on section 23. The results are consistent with sec-
tion 00, showing a 30.9% improvement in speed and
0.29% in coverage, with accuracy staying at roughly
the same level. The results show a consistent 35-
40% reduction in parsing time and a 40-65% reduc-
tion in parse failure rate.
7 Conclusion
We have introduced several modifications to CKY
parsing for CCG that significantly increase parsing
efficiency without an accuracy or coverage penalty.
45
METHOD secs % F-SCORE COVER n ? 40 n > 40
Original 91.3 ? 86.92 99.29 0.621 1.961
PUNCT + REPAIR + BEAM 58.7 35.7 86.82 99.58 0.399 0.654
Table 4: Best performance on Section 23
Chart repair improves efficiency by reusing the
chart from the previous parse attempts. This allows
us to further tighten the parser-supertagger integra-
tion by adding one lexical category at a time until a
spanning derivation is found. We have also explored
several approaches to selecting which category to
add next. We intend to further explore strategies
for determining which category to add next when a
parse fails. This includes combining chart and prob-
ability based orderings. Chart repair alone gives an
11.1% efficiency improvement.
Constraints improve efficiency by avoiding the
construction of sub-derivations that will not be used.
They have a significant impact on parsing speed and
coverage without reducing the accuracy, provided
the constraints are identified with sufficient preci-
sion. Punctuation constraints give a 10.4% improve-
ment, but NP constraints require higher precision NP
chunking than is currently available for CCGbank.
Constraints and chart repair both manipulate the
chart for more efficient parsing. Adding categories
one at a time using chart repair is almost a form of
agenda-based parsing. We intend to explore other
methods for pruning the space and agenda-based
parsing, in particular A* parsing (Klein and Man-
ning, 2003), which will allow only the most proba-
ble parts of the chart to be built, improving efficiency
while still ensuring the optimal derivation is found.
When all of our modifications are used parsing
speed increases by 35-40% and the failure rate de-
creases by 40-65%, both for sentences of length 1-40
and 41+, with a negligible accuracy penalty. The re-
sult is an even faster state-of-the-art wide-coverage
CCG parser.
Acknowledgements
We would like to thank the anonymous reviewers
for their feedback. James Curran was funded under
ARCDiscovery grants DP0453131 and DP0665973.
References
Srinivas Bangalore and Aravind Joshi. 1999. Supertag-
ging: An approach to almost parsing. Computational
Linguistics, 25(2):237?265.
A. Cahill, M. Burke, R. O?Donovan, J. van Genabith,
and A. Way. 2004. Long-distance dependency resolu-
tion in automatically acquired wide-coverage PCFG-
based LFG approximations. In Proceedings of the
42nd Meeting of the ACL, pages 320?327, Barcelona,
Spain.
Stephen Clark and James R. Curran. 2004a. The impor-
tance of supertagging for wide-coverage CCG pars-
ing. In Proceedings of COLING-04, pages 282?288,
Geneva, Switzerland.
Stephen Clark and James R. Curran. 2004b. Parsing the
WSJ using CCG and log-linear models. In Proceed-
ings of ACL-04, pages 104?111, Barcelona, Spain.
Michael Collins. 2003. Head-driven statistical models
for natural language parsing. Computational Linguis-
tics, 29(4):589?637.
James R. Curran and Stephen Clark. 2003. Investigating
GIS and smoothing for maximum entropy taggers. In
Proceedings of the 10th Meeting of the EACL, pages
91?98, Budapest, Hungary.
James R. Curran, Stephen Clark, and David Vadas.
2006. Multi-tagging for lexicalized-grammar parsing.
In Proceedings of COLING/ACL-06, pages 697?704,
Sydney, Austrailia.
Julia Hockenmaier and Mark Steedman. 2002. Gener-
ative models for statistical parsing with Combinatory
Categorial Grammar. In Proceedings of the 40th Meet-
ing of the ACL, pages 335?342, Philadelphia, PA.
Julia Hockenmaier. 2003. Data and Models for Statis-
tical Parsing with Combinatory Categorial Grammar.
Ph.D. thesis, University of Edinburgh.
Ron Kaplan, Stefan Riezler, Tracy H. King, John
T. Maxwell III, Alexander Vasserman, and Richard
Crouch. 2004. Speed and accuracy in shallow and
deep stochastic parsing. In Proceedings of the Human
Language Technology Conference and the 4th Meeting
of the North American Chapter of the Association for
Computational Linguistics (HLT-NAACL?04), Boston,
MA.
46
J. Kasami. 1965. An efficient recognition and syntax
analysis algorithm for context-free languages. Techni-
cal Report AFCRL-65-758, Air Force Cambridge Re-
search Laboratory, Bedford, MA.
Dan Klein and Christopher D. Manning. 2003. A* pars-
ing: Fast exact Viterbi parse selection. In Proceed-
ings of Human Language Technology and the North
American Chapter of the Association for Computa-
tional Linguistics Conference, pages 119?126, Ed-
mond, Canada.
Robert Malouf and Gertjan van Noord. 2004. Wide
coverage parsing with stochastic attribute value gram-
mars. In Proceedings of the IJCNLP-04 Workshop:
Beyond shallow analyses - Formalisms and statistical
modeling for deep analyses, Hainan Island, China.
Adwait Ratnaparkhi. 1996. A maximum entropy part-
of-speech tagger. In Proceedings of the EMNLP Con-
ference, pages 133?142, Philadelphia, PA.
Anoop Sarkar and Aravind Joshi. 2003. Tree-adjoining
grammars and its application to statistical parsing. In
Rens Bod, Remko Scha, and Khalil Sima?an, editors,
Data-oriented parsing. CSLI.
Mark Steedman. 2000. The Syntactic Process. The MIT
Press, Cambridge, MA.
Kristina Toutanova, Christopher Manning, Stuart
Shieber, Dan Flickinger, and Stephan Oepen. 2002.
Parse disambiguation for a rich HPSG grammar. In
Proceedings of the First Workshop on Treebanks
and Linguistic Theories, pages 253?263, Sozopol,
Bulgaria.
Yoshimasa Tsuruoka and Jun?ichi Tsujii. 2004. Iterative
cky parsing for probabilistic context-free grammars.
In Proceedings of the IJCNLP conference, pages 52?
60, Hainan Island, China.
D. Younger. 1967. Recognition and parsing of context-
free languages in time n3. Information and Control,
10(2):189?208.
47
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 813?821,
Singapore, 6-7 August 2009.
c?2009 ACL and AFNLP
Unbounded Dependency Recovery for Parser Evaluation
Laura Rimell and Stephen Clark
University of Cambridge
Computer Laboratory
laura.rimell@cl.cam.ac.uk
stephen.clark@cl.cam.ac.uk
Mark Steedman
University of Edinburgh
School of Informatics
steedman@inf.ed.ac.uk
Abstract
This paper introduces a new parser eval-
uation corpus containing around 700 sen-
tences annotated with unbounded depen-
dencies, from seven different grammatical
constructions. We run a series of off-the-
shelf parsers on the corpus to evaluate how
well state-of-the-art parsing technology is
able to recover such dependencies. The
overall results range from 25% accuracy
to 59%. These low scores call into ques-
tion the validity of using Parseval scores
as a general measure of parsing capability.
We discuss the importance of parsers be-
ing able to recover unbounded dependen-
cies, given their relatively low frequency
in corpora. We also analyse the various er-
rors made on these constructions by one of
the more successful parsers.
1 Introduction
Statistical parsers are now obtaining Parseval
scores of over 90% on the WSJ section of the Penn
Treebank (Bod, 2003; Petrov and Klein, 2007;
Huang, 2008; Carreras et al, 2008). McClosky et
al. (2006) report an F-score of 92.1% using self-
training applied to the reranker of Charniak and
Johnson (2005). Such scores, in isolation, may
suggest that statistical parsing is close to becom-
ing a solved problem, and that further incremental
improvements will lead to parsers becoming as ac-
curate as POS taggers.
A single score in isolation can be misleading,
however, for a number of reasons. First, the single
score is an aggregate over a highly skewed distri-
bution of all constituent types; evaluations which
look at individual constituent or dependency types
show that the accuracies on some, semantically
important, constructions, such as coordination and
PP-attachment, are much lower (Collins, 1999).
Second, it is well known that the accuracy of
parsers trained on the Penn Treebank degrades
when they are applied to different genres and do-
mains (Gildea, 2001). Finally, some researchers
have argued that the Parseval metrics (Black et al,
1991) are too forgiving with respect to certain er-
rors and that an evaluation based on syntactic de-
pendencies, for which scores are typically lower,
is a better test of parser performance (Lin, 1995;
Carroll et al, 1998).
In this paper we focus on the first issue, that the
performance of parsers on some constructions is
much lower than the overall score. The construc-
tions that we focus on are various unbounded de-
pendency constructions. These are interesting for
parser evaluation for the following reasons: one,
they provide a strong test of the parser?s knowl-
edge of the grammar of the language, since many
instances of unbounded dependencies are diffi-
cult to recover using shallow techniques in which
the grammar is only superficially represented; and
two, recovering these dependencies is necessary
to completely represent the underlying predicate-
argument structure of a sentence, useful for appli-
cations such as Question Answering and Informa-
tion Extraction.
To give an example of the sorts of constructions
we are considering, and the (in)ability of parsers
to recover the corresponding unbounded depen-
dencies, none of the parsers that we have tested
were able to recover the dependencies shown in
bold from the following sentences:
We have also developed techniques for recognizing and
locating underground nuclear tests through the waves in the
ground which they generate.
By Monday , they hope to have a sheaf of documents both
sides can trust.
By means of charts showing wave-travel times and depths
in the ocean at various locations , it is possible to estimate
the rate of approach and probable time of arrival at Hawaii
of a tsunami getting under way at any spot in the Pacific .
813
The contributions of this paper are as follows.
First, we present the first set of results for the
recovery of a variety of unbounded dependen-
cies, for a range of existing parsers. Second, we
describe the creation of a publicly available un-
bounded dependency test suite, and give statistics
summarising properties of these dependencies in
naturally occurring text. Third, we demonstrate
that performing the evaluation is surprisingly dif-
ficult, because of different conventions across the
parsers as to how the underlying grammar is rep-
resented. Fourth, we show that current parsing
technology is very poor at representing some im-
portant elements of the argument structure of sen-
tences, and argue for a more focused construction-
based parser evaluation as a complement to exist-
ing grammatical relation-based evaluations. We
also perform an error-analysis for one of the more
successful parsers.
There has been some prior work on evaluating
parsers on long-range dependencies, but no work
we are aware of that has the scope and focus of
this paper. Clark et al (2004) evaluated a CCG
parser on a small corpus of object extraction cases.
Johnson (2002) began the body of work on insert-
ing traces into the output of Penn Treebank (PTB)
parsers, followed by Levy and Manning (2004),
among others. This PTB work focused heavily
on the representation in the Treebank, evaluat-
ing against patterns in the trace annotation. In
this paper we have tried to be more ?formalism-
independent? and construction focused.
2 Unbounded Dependency Corpus
2.1 The constructions
An unbounded dependency construction contains
a word or phrase which appears to have been
moved, while being interpreted in the position
of the resulting ?gap?. An unlimited number
of clause boundaries may intervene between the
moved element and the gap (hence ?unbounded?).
The seven constructions in our corpus were cho-
sen for being relatively frequent in text, compared
to other unbounded dependency types, and rela-
tively easy to identify. An example of each con-
struction, along with its associated dependencies,
is shown in Table 1. Here we give a brief descrip-
tion of each construction.
Object extraction from a relative clause is
characterised by a relative pronoun (a wh-word or
that) introducing a clause from which an argument
in object position has apparently been extracted:
the paper which I wrote. Our corpus includes
cases where the extracted word is (semantically)
the object of a preposition in the verb phrase: the
agency that I applied to.
Object extraction from a reduced relative
clause is essentially the same, except that there is
no overt relative pronoun: the paper I wrote; the
agency I applied to. We did not include participial
reduced relatives such as the paper written by the
professor.
Subject extraction from a relative clause is
characterised by the apparent extraction of an ar-
gument from subject position: the instrument that
measures depth. A relative pronoun is obligatory
in this construction. Our corpus includes passive
subjects: the instrument which was used by the
professor.
Free relatives contain relative pronouns with-
out antecedents: I heard what she said, where
what does not refer to any other noun in the sen-
tence. Free relatives can usually be paraphrased by
noun phrases such as the thing she said (a standard
diagnostic for distinguishing them from embedded
interrogatives like I wonder what she said). The
majority of sentences in our corpus are object free
relatives, but we also included some adverbial free
relatives: She told us how to do it.
Objectwh-questions are questions in which the
wh-word is the semantic object of the verb: What
did you eat?. Objects of prepositions are included:
What city does she live in?. Also included are a
few cases where the wh-word is arguably adver-
bial, but is selected for by the verb: Where is the
park located?.
Right node raising (RNR) is characterised by
coordinated phrases from which a shared element
apparently moves to the right: Mary saw and Su-
san bought the book. This construction is unique
within our corpus in that the ?raised? element can
have a wide variety of grammatical functions. Ex-
amples include: noun phrase object of verb, noun
phrase object of preposition (material about or
messages from the communicator), a combination
of the two (applied for and won approval), prepo-
sitional phrase modifier (president and chief exec-
utive of the company), infinitival modifier (the will
and the capacity to prevent the event), and modi-
fied noun (a good or a bad decision).
Subject extraction from an embedded clause
is characterised by a semantic subject which is ap-
814
Object extraction from a relative clause
Each must match Wisman?s ?pie? with the fragment that he carries with him.
dobj(carries, fragment)
Object extraction from a reduced relative clause
Put another way, the decline in the yield suggests stocks have gotten pretty rich in price relative to the
dividends they pay, some market analysts say.
dobj(pay, dividends)
Subject extraction from a relative clause
It consists of a series of pipes and a pressure-measuring chamber which record the rise and fall of the
water surface.
nsubj(record, series)
nsubj(record, chamber)
Free relative
He tried to ignore what his own common sense told him, but it wasn?t possible; her motives were too
blatant.
dobj(told, what)
Object wh-question
What city does the Tour de France end in?
pobj(in, city)
Right node raising
For the third year in a row, consumers voted Bill Cosby first and James Garner second in persuasiveness
as spokesmen in TV commercials, according to Video Storyboard Tests, New York.
prep(first, in)
prep(second, in)
Subject extraction from an embedded clause
In assigning to God the responsibility which he learned could not rest with his doctors, Eisenhower
gave evidence of that weakening of the moral intuition which was to characterize his administration
in the years to follow.
nsubj(rest, responsibility)
Table 1: Examples of the seven constructions in the unbounded dependency corpus.
parently extracted across two clause boundaries,
as shown in the following bracketing (where ?
marks the origin of the extracted element): the
responsibility which [the government said [? lay
with the voters]]. Our corpus includes sentences
where the embedded clause is a so-called small
clause, i.e. one with a null copula verb: the plan
that she considered foolish, where plan is the se-
mantic subject of foolish.
2.2 The data
The corpus consists of approximately 100 sen-
tences for each of the seven constructions; 80 of
these were reserved for each construction for test-
ing, giving a test set of 560 sentences in total, and
the remainder were used for initial experimenta-
tion (for example to ensure that default settings for
the various parsers were appropriate for this data).
We did not annotate the full sentences, since we
are only interested in the unbounded dependencies
and full annotation of such a corpus would be ex-
tremely time-consuming.
With the exception of the question construc-
tion, all sentences were taken from the PTB, with
roughly half from the WSJ sections (excluding
2-21 which provided the training data for many
815
of the parsers in our set) and half from Brown
(roughly balanced across the different sections).
The questions were taken from the question data
in Rimell and Clark (2008), which was obtained
from various years of the TREC QA track. We
chose to use the PTB as the main source because
the use of traces in the PTB annotation provides a
starting point for the identification of unbounded
dependencies.
Sentences were selected for the corpus by a
combination of automatic and manual processes.
A regular expression applied to PTB trees, search-
ing for appropriate traces for a particular con-
struction, was first used to extract a set of can-
didate sentences. All candidates were manually
reviewed and, if selected, annotated with one or
more grammatical relations representing the rel-
evant unbounded dependencies in the sentence.
Some of the annotation in the treebank makes
identification of some constructions straightfor-
ward; for example right node raising is explicitly
represented as RNR. Indeed it may have been pos-
sible to fully automate this process with use of
the tgrep search tool. However, in order to ob-
tain reliable statistics regarding frequency of oc-
currence, and to ensure a high-quality resource,
we used fairly broad regular expressions to iden-
tify the original set followed by manual review.
We chose to represent the dependencies as
grammatical relations (GRs) since this format
seemed best suited to represent the kind of seman-
tic relationship we are interested in. GRs are head-
based dependencies that have been suggested as a
more appropriate representation for general parser
evaluation than phrase-structure trees (Carroll et
al., 1998). Table 1 gives examples of how GRs
are used to represent the relevant dependencies.
The particular GR scheme we used was based on
the Stanford scheme (de Marneffe et al, 2006);
however, the specific GR scheme is not too crucial
since the whole sentence is not being represented
in the corpus, only the unbounded dependencies.
3 Experiments
The five parsers described in Section 3.2 were used
to parse the test sentences in the corpus, and the
percentage of dependencies in the test set recov-
ered by each parser for each construction was cal-
culated. The details of how the parsers were run
and how the parser output was matched against
the gold standard are given in Section 3.3. This
Construction WSJ Brown Overall
Obj rel clause 2.3 1.1 1.4
Obj reduced rel 2.7 2.8 2.8
Sbj rel clause 10.1 5.7 7.4
Free rel 2.6 0.9 1.3
RNR 2.2 0.9 1.2
Sbj embedded 2.0 0.3 0.4
Table 2: Frequency of constructions in the PTB
(percentage of sentences).
is essentially a recall evaluation, and so is open
to abuse; for example, a program which returns all
the possible word pairs in a sentence, together with
all possible labels, would score 100%. However,
this is easily guarded against: we simply assume
that each parser is being run in a ?standard? mode,
and that each parser has already been evaluated on
a full corpus of GRs in order to measure precision
and recall across all dependency types. (Calculat-
ing precision for the unbounded dependency eval-
uation would be difficult since that would require
us to know howmany incorrect unbounded depen-
dencies were returned by each parser.)
3.1 Statistics relating to the constructions
Table 2 shows the percentage of sentences in the
PTB, from those sections that were examined,
which contain an example of each type of un-
bounded dependency. Perhaps not surprisingly,
root subject extractions from relative clauses are
by far the most common, with the remaining con-
structions occurring in roughly between 1 and 2%
of sentences. Note that, although examples of
each individual construction are relatively rare, the
combined total is over 10% (assuming that each
construction occurs independently). Section 6
contains a discussion regarding the frequency of
occurrence of these events and the consequences
of this for parser performance.
Table 3 shows the average and maximum dis-
tance between head and dependent for each con-
struction, as measured by the difference between
word indices. This is a fairly crude measure of
distance but gives some indication of how ?long-
range? the dependencies are for each construc-
tion. The cases of object extraction from a relative
clause and subject extraction from an embedded
clause provide the longest dependencies, on aver-
age. The following sentence gives an example of
how far apart the head and dependent can be in a
816
Construction Avg Dist Max Dist
Obj rel clause 6.8 21
Obj reduced rel 3.4 8
Sbj rel clause 4.4 18
Free rel 3.4 16
Obj wh-question 4.8 9
RNR 4.8 23
Sbj embedded 7.0 21
Table 3: Distance between head and dependent.
subject embedded construction:
the same stump which had impaled the car of
many a guest in the past thirty years and which he
refused to have removed.
3.2 The parsers
The parsers that we chose to evaluate are the C&C
CCG parser (Clark and Curran, 2007), the Enju
HPSG parser (Miyao and Tsujii, 2005), the RASP
parser (Briscoe et al, 2006), the Stanford parser
(Klein and Manning, 2003), and the DCU post-
processor of PTB parsers (Cahill et al, 2004),
based on LFG and applied to the output of the
Charniak and Johnson reranking parser. Of course
we were unable to evaluate every publicly avail-
able parser, but we believe these are representative
of current wide-coverage robust parsing technol-
ogy.
1
The C&C parser is based on CCGbank (Hock-
enmaier and Steedman, 2007), a CCG version of
the Penn Treebank. It is ideally suited for this eval-
uation because CCG was designed to capture the
unbounded dependencies being considered. The
Enju parser was designed with a similar motiva-
tion to C&C, and is also based on an automat-
ically extracted grammar derived from the PTB,
but the grammar formalism is HPSG rather than
CCG. Both parsers produce head-word dependen-
cies reflecting the underlying predicate-argument
structure of a sentence, and so in theory should be
straightforward to evaluate.
The RASP parser is based on a manually con-
structed POS tag-sequence grammar, with a sta-
tistical parse selection component and a robust
1
One obvious omission is any form of dependency parser
(McDonald et al, 2005; Nivre and Scholz, 2004). However,
the dependencies returned by these parsers are local, and it
would be non-trivial to infer from a series of links whether a
long-range dependency had been correctly represented. Also,
dependency parsers are not significantly better at recovering
head-based dependencies than constituent parsers based on
the PTB (McDonald et al, 2005).
partial-parsing technique which allows it to re-
turn output for sentences which do not obtain a
full spanning analysis according to the grammar.
RASP has not been designed to capture many of the
dependencies in our corpus; for example, the tag-
sequence grammar has no explicit representation
of verb subcategorisation, and so may not know
that there is a missing object in the case of extrac-
tion from a relative clause (though it does recover
some of these dependencies). However, RASP is
a popular parser used in a number of applications,
and it returns dependencies in a suitable format for
evaluation, and so we considered it to be an appro-
priate and useful member of our parser set.
The Stanford parser is representative of a large
number of PTB parsers, exemplified by Collins
(1997) and Charniak (2000). The Parseval scores
reported for the Stanford parser are not the highest
in the literature, but are competitive enough for our
purposes. The advantage of the Stanford parser is
that it returns dependencies in a suitable format for
our evaluation. The dependencies are obtained by
a set of manually defined rules operating over the
phrase-structure trees returned by the parser (de
Marneffe et al, 2006). Like RASP, the Stanford
parser has not been designed to capture unbounded
dependencies; in particular it does not make use of
any of the trace information in the PTB. However,
we wanted to include a ?standard? PTB parser in
our set to see which of the unbounded dependency
constructions it is able to deal with.
Finally, there is a body of work on inserting
trace information into the output of PTB parsers
(Johnson, 2002; Levy and Manning, 2004), which
is the annotation used in the PTB for representing
unbounded dependencies. The work which deals
with the PTB representation directly, such as John-
son (2002), is difficult for us to evaluate because it
does not produce explicit dependencies. However,
the DCU post-processor is ideal because it does
produce dependencies in a GR format. It has also
obtained competitive scores on general GR evalu-
ation corpora (Cahill et al, 2004).
3.3 Parser evaluation
The parsers were run essentially out-of-the-box
when parsing the test sentences. The one excep-
tion was C&C, which required some minor adjust-
ing of parameters, as described in the parser doc-
umentation, to obtain close to full coverage on the
data. In addition, the C&C parser comes with a
817
Obj RC Obj Red Sbj RC Free Obj Q RNR Sbj Embed Total
C&C 59.3 62.6 80.0 72.6 (81.2) 27.5 49.4 22.4 (59.7) 53.6
Enju 47.3 65.9 82.1 76.2 32.5 47.1 32.9 54.4
DCU 23.1 41.8 56.8 46.4 27.5 40.8 5.9 35.7
Rasp 16.5 1.1 53.7 17.9 27.5 34.5 15.3 25.3
Stanford 22.0 1.1 74.7 64.3 41.2 45.4 10.6 38.1
Table 4: Parser accuracy on the unbounded dependency corpus; the highest score for each construction
is in bold; the figures in brackets for C&C derive from the use of a separate question model.
specially designed question model, and so we ap-
plied both this and the standard model to the object
wh-question cases.
The parser output was evaluated against each
dependency in the corpus. Due to the various GR
schemes used by the parsers, an exact match on the
dependency label could not always be expected.
We considered a correctly recovered dependency
to be one where the gold-standard head and depen-
dent were correctly identified, and the label was
an ?acceptable match? to the gold-standard label.
To be an acceptable match, the label had to indi-
cate the grammatical function of the extracted el-
ement at least to the level of distinguishing active
subjects, passive subjects, objects, and adjuncts.
For example, we allowed an obj (object) relation
as a close enough match for dobj (direct object)
in the corpus, even though obj does not distin-
guish different kinds of objects, but we did not al-
low generic ?relative pronoun? relations that are
underspecified for the grammatical role of the ex-
tracted element.
The differences in GR schemes were such that
we ended up performing a time-consuming largely
manual evaluation. We list here some of the key
differences that made the evaluation difficult.
In some cases, the parser?s set of labels was less
fine-grained than the gold standard. For example,
RASP represents the direct objects of both verbs
and prepositions as dobj (direct object), whereas
the gold-standard uses pobj for the preposition
case. We counted the RASP output as correctly
matching the gold standard.
In other cases, the label on the dependency
containing the gold-standard head and depen-
dent was too underspecified to be acceptable by
itself. For example, where the gold-standard
relation was dobj(placed,buckets), DCU
produced relmod(buckets,placed) with
a generic ?relative modifier? label. However,
the correct label could be recovered from else-
where in the parser output, specifically a com-
bination of relpro(buckets,which) and
obj(placed,which). In this case we counted
the DCU output as correctly matching the gold
standard.
In some constructions the Stanford scheme,
upon which the gold-standard was based, makes
different choices about heads than other schemes.
For example, in the the phrase Honolulu, which is
the center of the warning system, the corpus con-
tains a subject dependency with center as the head:
nsubj(center,Honolulu). Other schemes,
however, treat the auxiliary verb is as the head of
the dependency, rather than the predicate nominal
center. As long as the difference in head selec-
tion was due solely to the idiosyncracies of the GR
schemes involved, we counted the relation as cor-
rect.
Finally, the different GR schemes treat coordi-
nation differently. In the corpus, coordinated ele-
ments are always represented with two dependen-
cies. Thus the phrase they may half see and half
imagine the old splendor has two gold-standard
dependencies: dobj(see,splendor) and
dobj(imagine,splendor). If a parser pro-
duced only the former dependency, but appeared
to have the coordination correct, then we awarded
two marks, even though the second dependency
was not explicitly represented.
4 Results
Accuracies for the various parsers are shown in Ta-
ble 4, with the highest score for each construction
in bold. Enju and C&C are the top performers,
operating at roughly the same level of accuracy
across most of the constructions. Use of the C&C
question model made a huge difference for the wh-
object construction (81.2% vs. 27.5%), showing
that adaptation techniques specific to a particular
818
construction can be successful (Rimell and Clark,
2008).
In order to learn more from these results, in Sec-
tion 5 we analyse the various errors made by the
C&C parser on each construction. The conclusions
that we arrive at for the C&C parser we would also
expect to apply to Enju, on the whole, since the de-
sign of the two parsers is so similar. In fact, some
of the recommendations for improvement on this
corpus, such as the need for a better parsing model
to make better attachment decisions, are parser in-
dependent.
The poor performance of RASP on this corpus
is clearly related to a lack of subcategorisation in-
formation, since this is crucial for recovering ex-
tracted arguments. For Stanford, incorporating the
trace information from the PTB into the statistical
model in some way is likely to help. The C&C and
Enju parsers do this through their respective gram-
mar formalisms. Our informal impression of the
DCU post-processor is that it has much of the ma-
chinery available to recover the dependencies that
the Enju and C&C parsers do, but for some reason
which is unclear to us it performs much worse.
5 Analysis of the C&C Parser
We categorised the errors made by the C&C parser
on the development data for each construction. We
chose the C&C parser for the analysis because it
was one of the top performers and we have more
knowledge of its workings than those of Enju.
The C&C parser first uses a supertagger to as-
sign a small number of CCG lexical categories (es-
sentially subcategorisation frames) to each word in
the sentence. These categories are then combined
using a set of combinatory rules to build a CCG
derivation. The parser uses a log-linear probabil-
ity model to select the highest-scoring derivation
(Clark and Curran, 2007). In general, errors in de-
pendency recovery may occur if the correct lexical
category is not assigned by the supertagger for one
or more of the words in a sentence, or if an incor-
rect derivation is chosen by the parsing model.
For unbounded dependency recovery, one
source of errors (labeled type 1 in Table 5) is the
wrong lexical category being assigned to the word
(normally a verb or preposition) governing the ex-
traction site. In these testaments that I would sub-
mit here, if submit is assigned a category for an
intransitive rather than transitive verb, the verb-
object relation will not be recovered.
1a 1b 1c 1d 2 3 Errs Tot
ObjRC 6 5 2 13 20
ObjRed 2 1 1 1 3 8 23
SbjRC 8 1 9 43
Free 1 1 2 22
ObjQ 2 2 4 25
RNR 2 1 7 3 13 28
SbjEmb 3 2 1 4 10 13
Subtotal 6 2 12 4
Total 24 21 14 59 174
Table 5: Error analysis for C&C. Errs is the to-
tal number of errors for a construction, Tot is the
number of dependencies of that type in the devel-
opment data.
There are a number of reasons why the wrong
category may be assigned. First, the lexicon may
not contain enough information about possible
categories for the word (1a), or the necessary cat-
egory may not exist in the parser?s grammar at all
(1b). Even if the grammar contains the correct cat-
egory and the lexicon makes it available, the pars-
ing model may not choose it (1c). Finally, a POS-
tagging error on the word may mislead the parser
into assigning the wrong category (1d).
2
A second source of errors (type 2) is attach-
ment decisions that the parser makes indepen-
dently of the unbounded dependency. In Morgan
. . . carried in several buckets of water from the
spring which he poured into the copper boiler, the
parser assigns the correct categories for the rela-
tive pronoun and verb, but chooses spring rather
than buckets as the head of the relativized NP (i.e.
the object of pour). Most attachment errors in-
volve prepositional phrases (PPs) and coordina-
tion, which have long been known to be areas
where parsers need improvement.
Finally, errors in unbounded dependency recov-
ery may be due to complex errors in the surround-
ing parse context (type 3). We will not comment
more on these cases since they do not tell us much
about unbounded dependencies in particular.
Table 5 shows the distribution of error types
across constructions for the C&C parser. Subject
relative clauses, for example, did not have any er-
rors of type 1, because a verb with an extracted
2
We considered an error to be type 1 only when the cate-
gory error occurred on the word governing the extraction site,
except in the subject embedded sentences, where we also in-
cluded the embedding verb, since the category of this verb is
key to dependency recovery.
819
subject does not require a special lexical category.
Most of the errors here are of type 2. For exam-
ple, in a series of pipes and a pressure-measuring
chamber which record the rise and fall of the wa-
ter surface, the parser attaches the relative clause
to chamber but not to series.
Subject embedded sentences show a different
pattern. Many of the errors can be attributed to
problems with the lexicon and grammar (1a and
1b). For example, in shadows that they imagined
were Apaches, the word imagined never appears in
the training data with the correct category, and so
the required entry is missing from the lexicon.
Object extraction from a relative clause had
a higher number of errors involving the parsing
model (1c). In the first carefree, dreamless sleep
that she had known, the transitive category is
available for known, but not selected by the model.
The majority of the errors made by the parser
are due to insufficient grammar coverage or weak-
ness in the parsing model due to sparsity of head
dependency data, the same fundamental problems
that have dogged automatic parsing since its in-
ception. Hence one view of statistical parsing is
that it has allowed us to solve the easy problems,
but we are still no closer to a general solution for
the recovery of the ?difficult? dependencies. One
possibility is to create more training data target-
ing these constructions ? effectively ?active learn-
ing by construction? ? in the way that Rimell and
Clark (2008) were able to build a question parser.
We leave this idea for future work.
6 Discussion
Unbounded dependencies are rare events, out in
the Zipfian ?long tail?. They will always consti-
tute a fraction of a percent of the overall total of
head-dependencies in any corpus, a proportion too
small to make a significant impact on global mea-
sures of parser accuracy, when expressive parsers
are compared to those that merely approximate
human grammar using finite-state or context-free
covers. This will remain the case even when such
measures are based on dependencies, rather than
on parse trees.
Nevertheless, unbounded dependencies remain
highly significant in a much more important sense.
They support the constructions that are central to
those applications of parsing technology for which
precision is as important as recall, such as open-
domain question-answering. As low-power ap-
proximate parsing methods improve (as they must
if they are ever to be usable at all for such tasks),
we predict that the impact of the constructions we
examine here will become evident. No matter how
infrequent object questions like ?What do frogs
eat?? are, if they are answered as if they were sub-
ject questions (?Herons?), users will rightly reject
any excuse in terms of the overall statistical distri-
bution of related bags of words.
Whether such improvements in parsers come
from the availability of more human-labeled data,
or from a breakthrough in unsupervised machine
learning, we predict an imminent ?Uncanny Val-
ley? in parsing applications, due to the inability of
parsers to recover certain semantically important
dependencies, of the kind familiar from humanoid
robotics and photorealistic animation. In such ap-
plications, the closer the superficial resemblance
to human behavior gets, the more disturbing sub-
tle departures become, and the more they induce
mistrust and revulsion in the user.
7 Conclusion
In this paper we have demonstrated that current
parsing technology is poor at recovering some
of the unbounded dependencies which are crucial
for fully representing the underlying predicate-
argument structure of a sentence. We have also
argued that correct recovery of such dependen-
cies will become more important as parsing tech-
nology improves, despite the relatively low fre-
quency of occurrence of the corresponding gram-
matical constructions. We also see this more fo-
cused parser evaluation methodology ? in this
case construction-focused ? as a way of improv-
ing parsing technology, as an alternative to the
exclusive focus on incremental improvements in
overall accuracy measures such as Parseval.
Acknowledgments
Laura Rimell and Stephen Clark were supported
by EPSRC grant EP/E035698/1. Mark Steed-
man was supported by EU IST Cognitive Systems
grant IP FP6-2004-IST-4-27657 (PACO-PLUS).
We would like to thank Aoife Cahill for produc-
ing the DCU data.
820
References
E. Black, S. Abney, D. Flickenger, C. Gdaniec, R. Gr-
ishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek,
J. Klavans, M. Liberman, M. Marcus, S. Roukos,
B. Santorini, and T. Strzalkowski. 1991. A proce-
dure for quantitatively comparing the syntactic cov-
erage of English grammars. In HLT ?91: Proceed-
ings of the Workshop on Speech and Natural Lan-
guage, pages 306?311.
Rens Bod. 2003. An efficient implementation of a new
DOP model. In Proceedings of the 10th Meeting of
the EACL, pages 19?26, Budapest, Hungary.
Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The second release of the RASP system. In
Proceedings of the Interactive Demo Session of
COLING/ACL-06, Sydney, Australia.
A. Cahill, M. Burke, R. O?Donovan, J. van Genabith,
and A. Way. 2004. Long-distance dependency
resolution in automatically acquired wide-coverage
PCFG-based LFG approximations. In Proceedings
of the 42nd Meeting of the ACL, pages 320?327,
Barcelona, Spain.
Xavier Carreras, Michael Collins, and Terry Koo.
2008. Dynamic programming and the perceptron for
efficient, feature-rich parsing. In Proceedings of the
Twelfth Conference on Natural Language Learning
(CoNLL-08), pages 9?16, Manchester, UK.
John Carroll, Ted Briscoe, and Antonio Sanfilippo.
1998. Parser evaluation: a survey and a new pro-
posal. In Proceedings of the 1st LREC Conference,
pages 447?454, Granada, Spain.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proceedings of the 43rd Meeting of
the ACL, pages 173?180, Michigan, Ann Arbor.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the 1st Meeting
of the NAACL, pages 132?139, Seattle, WA.
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG
and log-linear models. Computational Linguistics,
33(4):493?552.
Stephen Clark, Mark Steedman, and James R. Curran.
2004. Object-extraction and question-parsing using
CCG. In Proceedings of the EMNLP Conference,
pages 111?118, Barcelona, Spain.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of
the 35th Meeting of the ACL, pages 16?23, Madrid,
Spain.
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of the 5th LREC Conference, Genoa,
Italy.
Daniel Gildea. 2001. Corpus variation and parser per-
formance. In Proceedings of the 2001 EMNLP Con-
ference, Pittsburgh, PA.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: a corpus of CCG derivations and dependency
structures extracted from the Penn Treebank. Com-
putational Linguistics, 33(3):355?396.
Liang Huang. 2008. Forest reranking: Discrimina-
tive parsing with non-local features. In Proceed-
ings of the 46th Meeting of the ACL, pages 586?594,
Columbus, Ohio.
Mark Johnson. 2002. A simple pattern-matching al-
gorithm for recovering empty nodes and their an-
tecedents. In Proceedings of the 40th Meeting of the
ACL, pages 136?143, Philadelphia, PA.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting of Association for Computa-
tional Linguistics, pages 423?430, Sapporo, Japan.
Roger Levy and Christopher Manning. 2004. Deep de-
pendencies from context-free statistical parsers: cor-
recting the surface dependency approximation. In
Proceedings of the 42nd Meeting of the ACL, pages
328?335, Barcelona, Spain.
Dekang Lin. 1995. A dependency-based method for
evaluating broad-coverage parsers. In Proceedings
of IJCAI-95, pages 1420?1425, Montreal, Canada.
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Effective self-training for parsing. In
Proceedings of the North American Chapter of the
Association for Computational Linguistics Confer-
ence, pages 152?159, Brooklyn, NY.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proceedings of the 43rd Meet-
ing of the ACL, pages 91?98, Michigan, Ann Arbor.
Yusuke Miyao and Jun?ichi Tsujii. 2005. Probabilis-
tic disambiguation models for wide-coverage HPSG
parsing. In Proceedings of the 43rd Meeting of the
ACL, pages 83?90, Michigan, Ann Arbor.
J. Nivre and M. Scholz. 2004. Deterministic depen-
dency parsing of English text. In Proceedings of
COLING-04, pages 64?70, Geneva, Switzerland.
Slav Petrov and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In Proceedings of
the HLT/NAACL Conference, Rochester, NY.
Laura Rimell and Stephen Clark. 2008. Adapting a
lexicalized-grammar parser to contrasting domains.
In Proceedings of the 2008 EMNLP Conference,
pages 475?484, Honolulu, Hawai?i.
821
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 53?56,
Suntec, Singapore, 4 August 2009.
c
?2009 ACL and AFNLP
Comparing the Accuracy of CCG and Penn Treebank Parsers
Stephen Clark
University of Cambridge
Computer Laboratory
15 JJ Thomson Avenue, Cambridge, UK
stephen.clark@cl.cam.ac.uk
James R. Curran
School of Information Technologies
University of Sydney
NSW 2006, Australia
james@it.usyd.edu.au
Abstract
We compare the CCG parser of Clark and
Curran (2007) with a state-of-the-art Penn
Treebank (PTB) parser. An accuracy com-
parison is performed by converting the
CCG derivations into PTB trees. We show
that the conversion is extremely difficult to
perform, but are able to fairly compare the
parsers on a representative subset of the
PTB test section, obtaining results for the
CCG parser that are statistically no differ-
ent to those for the Berkeley parser.
1 Introduction
There are a number of approaches emerging in sta-
tistical parsing. The first approach, which began in
the mid-90s and now has an extensive literature, is
based on the Penn Treebank (PTB) parsing task:
inferring skeletal phrase-structure trees for unseen
sentences of the WSJ, and evaluating accuracy ac-
cording to the Parseval metrics. Collins (1999) is a
seminal example. The second approach is to apply
statistical methods to parsers based on linguistic
formalisms, such as HPSG, LFG, TAG, and CCG,
with the grammar being defined manually or ex-
tracted from a formalism-specific treebank. Evalu-
ation is typically performed by comparing against
predicate-argument structures extracted from the
treebank, or against a test set of manually anno-
tated grammatical relations (GRs). Examples of
this approach include Riezler et al (2002), Miyao
and Tsujii (2005), Briscoe and Carroll (2006), and
Clark and Curran (2007).
1
Despite the many examples from both ap-
proaches, there has been little comparison across
the two groups, which we refer to as PTB parsing
and formalism-based parsing, respectively. The
1
A third approach is dependency parsing, but we restrict
the comparison in this paper to phrase-structure parsers.
PTB parser we use for comparison is the pub-
licly available Berkeley parser (Petrov and Klein,
2007). The formalism-based parser we use is the
CCG parser of Clark and Curran (2007), which
is based on CCGbank (Hockenmaier and Steed-
man, 2007), a CCG version of the Penn Treebank.
We compare this parser with a PTB parser because
both are derived from the same original source,
and both produce phrase-structure in some form
or another; the interesting question is whether any-
thing is gained by converting the PTB into CCG.
2
The comparison focuses on accuracy and is per-
formed by converting CCG derivations into PTB
phrase-structure trees. A contribution of this paper
is to demonstrate the difficulty of mapping from a
grammatical resource based on the PTB back to the
PTB, and we also comment on the (non-)suitability
of the PTB as a general formalism-independent
evaluation resource. A second contribution is to
provide the first accuracy comparison of the CCG
parser with a PTB parser, obtaining competitive
scores for the CCG parser on a representative sub-
set of the PTB test sections. It is important to note
that the purpose of this evaluation is comparison
with a PTB parser, rather than evaluation of the
CCG parser per se. The CCG parser has been ex-
tensively evaluated elsewhere (Clark and Curran,
2007), and arguably GRs or predicate-argument
structures provide a more suitable test set for the
CCG parser than PTB phrase-structure trees.
2 The CCG to PTB Conversion
There has been much recent work in attempt-
ing to convert native parser output into alterna-
tive representations for evaluation purposes, e.g.
(Clark and Curran, 2007; Matsuzaki and Tsujii,
2008). The conclusion is that such conversions
are surprisingly difficult. Clark and Curran (2007)
2
Since this short paper reports a small, focused research
contribution, we refer readers to Clark and Curran (2007) and
Petrov and Klein (2007) for details of the two parsers.
53
shows that converting gold-standard CCG deriva-
tions into the GRs in DepBank resulted in an F-
score of only 85%; hence the upper bound on the
performance of the CCG parser, using this evalua-
tion scheme, was only 85%. Given that the current
best scores for the PTB parsing task are over 90%,
any loss from the conversion process needs to be
considered carefully if a fair comparison with PTB
parsers is to be achieved.
CCGbank was derived from the PTB, and so
it might be considered that converting back to
the PTB would be a relatively easy task, by es-
sentially reversing the mapping Hockenmaier and
Steedman (2007) used to create CCGbank. How-
ever, there are a number of differences between
the two treebanks which make the conversion back
far from trivial. First, the corresponding deriva-
tions in the treebanks are not isomorphic: a CCG
derivation is not simply a relabelling of the nodes
in the PTB tree; there are many constructions, such
as coordination and control structures, where the
trees are a different shape, as well as having differ-
ent labels. It is important to realise that Hocken-
maier and Steedman (2007) invested a significant
amount of time and effort in creating the mapping.
Second, some of the labels in the PTB do not ap-
pear in CCGbank, for example the QP label, and
these must be added back in; however, developing
rules to insert these labels in the right places is a
far from trivial task.
There were two approaches we considered for
the conversion. One possibility is to associate PTB
tree structures with CCG lexical categories, and
combine the trees together in step with the cate-
gory combinations in a CCG derivation ? in much
the same way that an LTAG has elementary trees
in the lexicon which are combined using the sub-
stitution and adjunction rules of TAG. The second
approach is to associate conversion rules with each
local tree ? i.e. a parent and one or two child nodes
? which appears in the CCGbank data.
3
In this pa-
per we took the second approach.
2.1 Conversion Schemas
There are three types of conversion schema:
schemas which introduce nodes for lexical items;
schemas which insert or elide PTB nodes for unary
3
Another possible approach has been taken by Matsuzaki
and Tsujii (2008), who convert HPSG analyses from a gram-
mar automatically extracted from the PTB back into the PTB.
They treat the problem as one of translation, learning a syn-
chronous grammar to perform the mapping.
TYPE RULE SCHEMA
lexical NP NP
lexical NP [nb]/N ?
lexical (S [dcl ]\NP)/NP VP
unary S [dcl ]? NP\NP (SBAR l)
type- PP ? l
raising (S\NP)\((S\NP)/PP)
binary NP [nb]/N N ? NP [nb] >
binary NP S [dcl ]\NP ? S [dcl ] (S l r)
binary NP/(S [dcl ]\NP) (SBAR
S [dcl ]\NP ? NP l (S r))
Table 1: Example conversion schemas
rules and type-raising; and schemas which can
perform arbitrary manipulation of generated PTB
subtrees for binary CCG rule instances. Examples
of these schemas are shown in Table 1. The pri-
mary operations in the binary schema are inserting
and attaching. Inserting a new node, for example
using the schema (S l r), creates a new S node
dominating both the left and right children of a bi-
nary rule. The attaching schema can attach the left
node under the right node (>); or the right node
under the left node (<).
The lexical categories NP and
(S [dcl ]\NP)/NP (shown in Table 1) intro-
duce the PTB nodes NP and VP, respectively,
while other lexical categories such as NP [nb]/N
introduce no extra nodes. Some unary rules
introduce nodes, such as SBAR for the reduced
relative case, whilst others, such as the type-raised
PP , do not. Finally, binary schemas may create
no new nodes (e.g. when a determiner is attached
to an existing NP), or one or more nodes (e.g. an
extra S node is created when a verb phrase finds
its subject).
A PTB tree is built from a CCG derivation by
running over the derivation in a bottom-up fashion
and applying these schemas to the local trees in
the derivation.
2.2 Schema development
The schemas were developed by manual inspec-
tion using section 00 of CCGbank and the PTB as
a development set, following the oracle method-
ology of Clark and Curran (2007), in which gold-
standard derivations from CCGbank are converted
to the new representation and compared with the
gold standard for that representation. As well as
giving an idea of the difficulty, and success, of the
conversion, the resulting numbers provide an up-
54
SECTION P R F COMP
00 (all) 93.37 95.15 94.25 39.68
00 (len ? 40) 94.11 95.65 94.88 42.11
23 (all) 93.68 95.13 94.40 39.93
23 (len ? 40) 93.75 95.23 94.48 42.15
Table 2: Oracle conversion evaluation
per bound on the performance of the CCG parser.
The test set, section 23, was not inspected at any
stage in the development of the schemas.
In total, we annotated 32 unary and 776 binary
rule instances (of the possible 2853 instances) with
conversion schemas, and 162 of the 425 lexical
categories. We also implemented a small num-
ber of default catch-all cases for the general CCG
combinatory rules and for the rules dealing with
punctuation, which allowed most of the 2853 rule
instances to be covered. Considerable time and ef-
fort was invested in the creation of these schemas.
The oracle conversion results from the gold
standard CCGbank to the PTB for section 00 and
23 are shown in Table 2. The numbers are brack-
eting precision, recall, F-score and complete sen-
tence matches, using the EVALB evaluation script.
Note that these figures provide an upper bound on
the performance of the CCG parser using EVALB,
given the current conversion process.
The importance of this upper bound should not
be underestimated, when the evaluation frame-
work is such that incremental improvements of a
few tenths of a percent are routinely presented as
improving the state-of-the-art, as is the case with
the Parseval metrics. The fact that the upper bound
here is less than 95% shows that it is not possi-
ble to fairly evaluate the CCG parser on the com-
plete test set. Even an upper bound of around 98%,
which is achieved by Matsuzaki and Tsujii (2008),
is not sufficient, since this guarantees a loss of at
least 2%.
4
3 Evaluation
The Berkeley parser (Petrov and Klein, 2007) pro-
vides performance close to the state-of-the-art for
the PTB parsing task, with reported F-scores of
around 90%. Since the oracle score for CCGbank
is less than 95%, it would not be a fair comparison
4
The higher upper bound achieved by Matsuzaki and Tsu-
jii (2008) could be due to the fact that their extracted HPSG
grammars are closer to the PTB than CCGbank, or due to their
conversion method. We leave the application of their method
to the CCG parser for future work.
to use the complete test set. However, there are a
number of sentences which are correct, or almost
correct, according to EVALB after the conversion,
and we are able to use those for a fair comparison.
Table 3 gives the EVALB results for the CCG
parser on various subsets of section 00 of the
PTB. The first row shows the results on only
those sentences which the conversion process can
convert sucessfully (as measured by converting
gold-standard CCGbank derivations and compar-
ing with PTB trees; although, to be clear, the scores
are for the CCG parser on those sentences). As can
be seen from the scores, these sentences form a
slightly easier subset than the full section 00, but
this is a subset which can be used for a fair com-
parison against the Berkeley parser, since the con-
version process is not lossy for this subset.
The second row shows the scores on those sen-
tences for which the conversion process was some-
what lossy, but when the gold-standard CCGbank
derivations are converted, the oracle F-measure is
greater than 95%. The third row is similar, but for
sentences for which the oracle F-score is geater
than 92%. The final row is for the whole of sec-
tion 00. The UB column gives the upper bound on
the accuracy of the CCG parser. Results are calcu-
lated using both gold standard and automatically
assigned POS tags; # is the number of sentences
in the sample, and the % column gives the sample
size as a percentage of the whole section.
We compare the CCG parser to the Berkeley
parser using the accurate mode of the Berke-
ley parser, together with the model supplied with
the publicly available version. Table 3 gives the
results for Section 23, comparing the CCG and
Berkeley parsers. The projected columns give
the projected scores for the CCG parser, if it per-
formed at the same accuracy level for those sen-
tences which could not be converted successfully.
The purpose of this column is to obtain an ap-
proximation of the CCG parser score for a perfect
conversion process.
5
The results in bold are those
which we consider to be a fair comparison against
the Berkeley parser. The difference in scores is
not statistically significant at p=0.05 (using Dan
Bikel?s stratified shuffling test).
One possible objection to this comparison is
that the subset for which we have a fair compar-
5
This is likely to be an upper bound on the performance
of the CCG parser, since the larger test sets contain sentences
which were harder to convert, and hence are likely to be more
difficult to parse.
55
SAMPLE # % UB actual F projected F
gold auto gold auto
00 (F=100) 759 39.7 100.00 94.19 93.41 ? ?
00 (F?95) 1164 60.8 98.49 91.08 89.93 92.46 91.29
00 (F?92) 1430 74.6 97.41 89.73 88.47 92.05 90.76
00 (all) 1913 100.0 94.25 87.00 85.60 92.00 90.52
Table 3: Results on the development set (CCG parser only)
SAMPLE # % UB Berkeley F actual F projected F
gold auto gold auto gold auto
23 (F=100) 961 39.9 100.0 93.38 93.37 93.83 92.86 ? ?
23 (F?95) 1401 58.2 98.61 91.66 91.63 90.82 89.84 92.08 91.09
23 (F?92) 1733 72.0 97.44 91.01 90.88 89.53 88.54 91.82 90.81
23 (all) 2407 100.0 94.40 89.67 89.47 86.36 85.50 91.20 90.29
Table 4: Results on the test set (CCG parser and Berkeley)
ison is likely to be an easy subset consisting of
shorter sentences, and so the most that can be
said is that the CCG parser performs as well as
the Berkeley parser on short sentences. In fact,
the subset for which we perform a perfect conver-
sion contains sentences with an average length of
18.1 words, compared to 21.4 for sentences with
40 words or less (a standard test set for reporting
Parseval figures). Hence we do consider the com-
parison to be highly informative.
4 Conclusion
One question that is often asked of the CCG
parsing work is ?Why not convert back into the
PTB representation and perform a Parseval eval-
uation?? By showing how difficult the conver-
sion is, we believe that we have finally answered
this question, as well as demonstrating compara-
ble performance with the Berkeley parser. In addi-
tion, we have thrown further doubt on the possible
use of the PTB for cross-framework parser evalua-
tion, as recently suggested by Matsuzaki and Tsu-
jii (2008). Even the smallest loss due to mapping
across representations is significant when a few
tenths of a percentage point matter. Whether PTB
parsers could be competitive on alternative parser
evaluations, such as those using GR schemes, for
which the CCG parser performs very well, is an
open question.
Acknowledgements
James Curran was funded under Australian Re-
search Council Discovery grant DP0665973.
Stephen Clark was funded under EPSRC grant
EP/E035698/1.
References
Ted Briscoe and John Carroll. 2006. Evaluating the accu-
racy of an unlexicalized statistical parser on the PARC
DepBank. In Proceedings of the Poster Session of
COLING/ACL-06, Sydney, Austrailia.
Stephen Clark and James R. Curran. 2007. Wide-coverage
efficient statistical parsing with CCG and log-linear mod-
els. Computational Linguistics, 33(4):493?552.
Michael Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University of
Pennsylvania.
Julia Hockenmaier and Mark Steedman. 2007. CCGbank:
a corpus of CCG derivations and dependency structures
extracted from the Penn Treebank. Computational Lin-
guistics, 33(3):355?396.
Takuya Matsuzaki and Jun?ichi Tsujii. 2008. Comparative
parser performance analysis across grammar frameworks
through automatic tree conversion using synchronous
grammars. In Proceedings of COLING-08, pages 545?
552, Manchester, UK.
Yusuke Miyao and Jun?ichi Tsujii. 2005. Probabilistic dis-
ambiguation models for wide-coverage HPSG parsing. In
Proceedings of the 43rd meeting of the ACL, pages 83?90,
University of Michigan, Ann Arbor.
Slav Petrov and Dan Klein. 2007. Improved inference for
unlexicalized parsing. In Proceedings of the HLT/NAACL
conference, Rochester, NY.
Stefan Riezler, Tracy H. King, Ronald M. Kaplan, Richard
Crouch, John T. Maxwell III, and Mark Johnson. 2002.
Parsing the Wall Street Journal using a Lexical-Functional
Grammar and discriminative estimation techniques. In
Proceedings of the 40th Meeting of the ACL, pages 271?
278, Philadelphia, PA.
56
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 162?171,
Paris, October 2009. c?2009 Association for Computational Linguistics
Transition-Based Parsing of the Chinese Treebank using a Global
Discriminative Model
Yue Zhang
Oxford University
Computing Laboratory
yue.zhang@comlab.ox.ac.uk
Stephen Clark
Cambridge University
Computer Laboratory
stephen.clark@cl.cam.ac.uk
Abstract
Transition-based approaches have shown
competitive performance on constituent
and dependency parsing of Chinese. State-
of-the-art accuracies have been achieved
by a deterministic shift-reduce parsing
model on parsing the Chinese Treebank 2
data (Wang et al, 2006). In this paper,
we propose a global discriminative model
based on the shift-reduce parsing process,
combined with a beam-search decoder, ob-
taining competitive accuracies on CTB2.
We also report the performance of the
parser on CTB5 data, obtaining the highest
scores in the literature for a dependency-
based evaluation.
1 Introduction
Transition-based statistical parsing associates
scores with each decision in the parsing process,
selecting the parse which is built by the highest
scoring sequence of decisions (Briscoe and Car-
roll, 1993; Nivre et al, 2006). The parsing algo-
rithm is typically some form of bottom-up shift-
reduce algorithm, so that scores are associated
with actions such as shift and reduce. One ad-
vantage of this approach is that the parsing can be
highly efficient, for example by pursuing a greedy
strategy in which a single action is chosen at each
decision point.
The alternative approach, exemplified by
Collins (1997) and Charniak (2000), is to use
a chart-based algorithm to build the space of
possible parses, together with pruning of low-
probability constituents and the Viterbi algorithm
to find the highest scoring parse. For English de-
pendency parsing, the two approaches give similar
results (McDonald et al, 2005; Nivre et al, 2006).
For English constituent-based parsing using the
Penn Treebank, the best performing transition-
based parser lags behind the current state-of-the-
art (Sagae and Lavie, 2005). In contrast, for Chi-
nese, the best dependency parsers are currently
transition-based (Duan et al, 2007; Zhang and
Clark, 2008). For constituent-based parsing using
the Chinese Treebank (CTB), Wang et al (2006)
have shown that a shift-reduce parser can give
competitive accuracy scores together with high
speeds, by using an SVM to make a single decision
at each point in the parsing process.
In this paper we describe a global discrimina-
tive model for Chinese shift-reduce parsing, and
compare it with Wang et al?s approach. We ap-
ply the same shift-reduce procedure as Wang et
al. (2006), but instead of using a local classifier
for each transition-based action, we train a gener-
alized perceptron model over complete sequences
of actions, so that the parameters are learned in
the context of complete parses. We apply beam
search to decoding instead of greedy search. The
parser still operates in linear time, but the use of
beam-search allows the correction of local deci-
sion errors by global comparison. Using CTB2,
our model achieved Parseval F-scores comparable
to Wang et al?s approach. We also present accu-
racy scores for the much larger CTB5, using both
a constituent-based and dependency-based evalu-
ation. The scores for the dependency-based eval-
uation were higher than the state-of-the-art depen-
dency parsers for the CTB5 data.
2 The Shift-Reduce Parsing Process
The shift-reduce process used by our beam-search
decoder is based on the greedy shift-reduce parsers
of Sagae and Lavie (2005) and Wang et al (2006).
162
The process assumes binary-branching trees; sec-
tion 2.1 explains how these are obtained from the
arbitrary-branching trees in the Chinese Treebank.
The input is assumed to be segmented and POS
tagged, and the word-POS pairs waiting to be pro-
cessed are stored in a queue. A stack holds the
partial parse trees that are built during the parsing
process. A parse state is defined as a ?stack,queue?
pair. Parser actions, including SHIFT and various
kinds of REDUCE, define functions from states to
states by shifting word-POS pairs onto the stack
and building partial parse trees.
The actions used by the parser are:
? SHIFT, which pushes the next word-POS pair
in the queue onto the stack;
? REDUCE?unary?X, which makes a new
unary-branching node with label X; the stack
is popped and the popped node becomes the
child of the new node; the new node is pushed
onto the stack;
? REDUCE?binary?{L/R}?X, which makes a
new binary-branching node with label X; the
stack is popped twice, with the first popped
node becoming the right child of the new
node and the second popped node becoming
the left child; the new node is pushed onto the
stack;
? TERMINATE, which pops the root node off
the stack and ends parsing. This action
is novel in our parser. Sagae and Lavie
(2005) and Wang et al (2006) only used the
first three transition actions, setting the fi-
nal state as all incoming words having been
processed, and the stack containing only one
node. However, there are a small number of
sentences (14 out of 3475 from the training
data) that have unary-branching roots. For
these sentences, Wang?s parser will be unable
to produce the unary-branching roots because
the parsing process terminates as soon as the
root is found. We define a separate action to
terminate parsing, allowing unary reduces to
be applied to the root item before parsing fin-
ishes.
The trees built by the parser are lexicalized, us-
ing the head-finding rules from Zhang and Clark
(2008). The left (L) and right (R) versions of the
REDUCE-binary rules indicate whether the head of
for node Y = X1..Xm ? T :
if m > 2 :
find the head node Xk(1 ? k ? m) of Y
m? = m
while m? > k and m? > 2 :
new node Y ? = X1..Xm??1
Y ? Y ?Xm?
m? = m? ? 1
n? = 1
while n? < k and k ? n? > 1 :
new node Y ? = Xn? ..Xk
Y ? Xn?Y ?
n? = n? + 1
Figure 2: the binarization algorithm with input T
the new node is to be taken from the left or right
child. Note also that, since the parser is building
binary trees, the X label in the REDUCE rules can
be one of the temporary constituent labels, such
as NP?, which are needed for the binarization pro-
cess described in Section 2.1. Hence the number
of left and right binary reduce rules is the number
of constituent labels in the binarized grammar.
Wang et al (2006) give a detailed example
showing how a segmented and POS-tagged sen-
tence can be incrementally processed using the
shift-reduce actions to produce a binary tree. We
show this example in Figure 1.
2.1 The binarization process
The algorithm in Figure 2 is used to map CTB
trees into binarized trees, which are required by
the shift-reduce parsing process. For any tree node
with more than two child nodes, the algorithm
works by first finding the head node, and then pro-
cessing its right-hand-side and left-hand-side, re-
spectively. The head-finding rules are taken from
Zhang and Clark (2008). Y = X1..Xm represents
a tree node Y with child nodes X1...Xm(m ? 1).
The label of the newly generated node Y ? is
based on the constituent label of the original node
Y , but marked with an asterix. Hence binariza-
tion enlarges the set of constituent labels. We
call the constituents marked with ? temporary con-
stituents. The binarization process is reversible, in
that output from the shift-reduce parser can be un-
binarized into CTB format, which is required for
evaluation.
163
Figure 1: An example shift-reduce parsing process, adopted from Wang et al (2006)
2.2 Restrictions on the sequence of actions
Not all sequences of actions produce valid bina-
rized trees. In the deterministic parser of Wang et
al. (2006), the highest scoring action predicted by
the classifier may prevent a valid binary tree from
being built. In this case, Wang et al simply return
a partial parse consisting of all the subtrees on the
stack.
In our parser a set of restrictions is applied
which guarantees a valid parse tree. For example,
two simple restrictions are that a SHIFT action can
only be applied if the queue of incoming words
164
Variables: state item item = (S,Q), where
S is stack and Q is incoming queue;
the agenda agenda;
list of state items next;
Algorithm:
for item ? agenda:
if item.score = agenda.bestScore and
item.isFinished:
rval = item
break
next = []
for move ? item.legalMoves:
next.push(item.TakeAction(move))
agenda = next.getBBest()
Outputs: rval
Figure 3: the beam-search decoding algorithm
is non-empty, and the binary reduce actions can
only be performed if the stack contains at least two
nodes. Some of the restrictions are more complex
than this; the full set is listed in the Appendix.
3 Decoding with Beam Search
Our decoder is based on the incremental shift-
reduce parsing process described in Section 2. We
apply beam-search, keeping the B highest scoring
state items in an agenda during the parsing pro-
cess. The agenda is initialized with a state item
containing the starting state, i.e. an empty stack
and a queue consisting of all word-POS pairs from
the sentence.
At each stage in the decoding process, existing
items from the agenda are progressed by applying
legal parsing actions. From all newly generated
state items, the B highest scoring are put back on
the agenda. The decoding process is terminated
when the highest scored state item in the agenda
reaches the final state. If multiple state items have
the same highest score, parsing terminates if any
of them are finished. The algorithm is shown in
Figure 3.
4 Model and Learning Algorithm
We use a linear model to score state items. Recall
that a parser state is a ?stack,queue? pair, with the
stack holding subtrees and the queue holding in-
coming words waiting to be processed. The score
Inputs: training examples (xi, yi)
Initialization: set ~w = 0
Algorithm:
for t = 1..T , i = 1..N :
zi = parse(xi, ~w)
if zi 6= yi:
~w = ~w +?(yi)? ?(zi)
Outputs: ~w
Figure 4: the perceptron learning algorithm
for state item Y is defined by:
Score(Y ) = ~w ? ?(Y ) =?
i
?i fi(Y )
where ?(Y ) is the global feature vector from Y ,
and ~w is the weight vector defined by the model.
Each element from ?(Y ) represents the global
count of a particular feature from Y . The feature
set consists of a large number of features which
pick out various configurations from the stack and
queue, based on the words and subtrees in the state
item. The features are described in Section 4.1.
The weight values are set using the generalized
perceptron algorithm (Collins, 2002).
The perceptron algorithm is shown in Figure 4.
It initializes weight values as all zeros, and uses
the current model to decode training examples (the
parse function in the pseudo-code). If the output
is correct, it passes on to the next example. If
the output is incorrect, it adjusts the weight val-
ues by adding the feature vector from the gold-
standard output and subtracting the feature vector
from the parser output. Weight values are updated
for each example (making the process online) and
the training data is iterated over T times. In or-
der to avoid overfitting we used the now-standard
averaged version of this algorithm (Collins, 2002).
We also apply the early update modification
from Collins and Roark (2004). If the agenda, at
any point during the decoding process, does not
contain the correct partial parse, it is not possible
for the decoder to produce the correct output. In
this case, decoding is stopped early and the weight
values are updated using the highest scoring par-
tial parse on the agenda.
4.1 Feature set
Table 1 shows the set of feature templates for the
model. Individual features are generated from
165
Description Feature templates
Unigrams S0tc, S0wc, S1tc, S1wc,
S2tc, S2wc, S3tc, S3wc,
N0wt, N1wt, N2wt, N3wt,
S0lwc, S0rwc, S0uwc,
S1lwc, S1rwc, S1uwc,
Bigrams S0wS1w, S0wS1c, S0cS1w, S0cS1c,
S0wN0w, S0wN0t, S0cN0w, S0cN0t,
N0wN1w, N0wN1t, N0tN1w, N0tN1t
S1wN0w, S1wN0t, S1cN0w, S1cN0t,
Trigrams S0cS1cS2c, S0wS1cS2c,
S0cS1wS2c, S0cS1cS2w,
S0cS1cN0t, S0wS1cN0t,
S0cS1wN0t, S0cS1cN0w
Bracket S0wb, S0cb
S0wS1cb, S0cS1wb, S0cS1cb
S0wN0tb, S0cN0wb, S0cN0tb
Separator S0wp, S0wcp, S0wq, S0wcq,
S1wp, S1wcp, S1wq, S1wcq
S0cS1cp, S0cS1cq
Table 1: Feature templates
these templates by first instantiating a template
with particular labels, words and tags, and then
pairing the instantiated template with a particu-
lar action. In the table, the symbols S0, S1, S2,
and S3 represent the top four nodes on the stack,
and the symbols N0, N1, N2 and N3 represent the
first four words in the incoming queue. S0L, S0R
and S0U represent the left and right child for bi-
nary branching S0, and the single child for unary
branching S0, respectively; w represents the lex-
ical head token for a node; c represents the label
for a node. When the corresponding node is a ter-
minal, c represents its POS-tag, whereas when the
corresponding node is non-terminal, c represents
its constituent label; t represents the POS-tag for a
word.
The context S0, S1, S2, S3 and N0, N1, N2, N3
for the feature templates is taken from Wang et al
(2006). However, Wang et al (2006) used a poly-
nomial kernel function with an SVM and did not
manually create feature combinations. Since we
used the linear perceptron algorithm we manually
combined Unigram features into Bigram and Tri-
gram features.
The ?Bracket? row shows bracket-related fea-
tures, which were inspired by Wang et al (2006).
Here brackets refer to left brackets including ???,
??? and ??? and right brackets including ???,
??? and ???. In the table, b represents the
matching status of the last left bracket (if any)
on the stack. It takes three different values:
1 (no matching right bracket has been pushed
onto stack), 2 (a matching right bracket has been
pushed onto stack) and 3 (a matching right bracket
has been pushed onto stack, but then popped off).
The ?Separator? row shows features that in-
clude one of the separator punctuations (i.e. ???,
???, ??? and ???) between the head words of
S0 and S1. These templates apply only when
the stack contains at least two nodes; p repre-
sents a separator punctuation symbol. Each unique
separator punctuation between S0 and S1 is only
counted once when generating the global feature
vector. q represents the count of any separator
punctuation between S0 and S1.
Whenever an action is being considered at each
point in the beam-search process, templates from
Table 1 are matched with the context defined by
the parser state and combined with the action to
generate features. Negative features, which are the
features from incorrect parser outputs but not from
any training example, are included in the model.
There are around a million features in our experi-
ments with the CTB2 dataset.
Wang et al (2006) used a range of other fea-
tures, including rhythmic features of S0 and S1
(Sun and Jurafsky, 2003), features from the most
recently found node that is to the left or right of S0
and S1, the number of words and the number of
punctuations in S0 and S1, the distance between
S0 and S1 and so on. We did not include these
features in our parser, because they did not lead to
improved performance during development exper-
iments.
5 Experiments
The experiments were performed using the Chi-
nese Treebank 2 and Chinese Treebank 5 data.
Standard data preparation was performed before
the experiments: empty terminal nodes were re-
moved; any non-terminal nodes with no children
were removed; any unary X ? X nodes resulting
from the previous steps were collapsed into one X
node.
For all experiments, we used the EVALB tool1
for evaluation, and used labeled recall (LR), la-
beled precision (LP ) and F1 score (which is the
1http://nlp.cs.nyu.edu/evalb/
166
Figure 5: The influence of beam-size
Sections Sentences Words
Training 001?270 3475 85,058
Development 301?325 355 6,821
Test 271?300 348 8,008
Table 2: The standard split of CTB2 data
harmonic mean of LR and LP ) to measure pars-
ing accuracy.
5.1 The influence of beam-size
Figure 5 shows the accuracy curves using differ-
ent beam-sizes for the decoder. The number of
training iterations is on the x-axis with F -score
on the y-axis. The tests were performed using
the development test data and gold-standard POS-
tags. The figure shows the benefit of using a beam
size greater than 1, with comparatively little accu-
racy gain being obtained beyond a beam size of 8.
Hence we set the beam size to 16 for the rest of the
experiments.
5.2 Test results on CTB2
The experiments in this section were performed
using CTB2 to allow comparison with previous
work, with the CTB2 data extracted from Chinese
Treebank 5 (CTB5). The data was split into train-
ing, development test and test sets, as shown in Ta-
ble 2, which is consistent with Wang et al (2006)
and earlier work. The tests were performed us-
ing both gold-standard POS-tags and POS-tags au-
tomatically assigned by a POS-tagger. We used our
Model LR LP F1
Bikel Thesis 80.9% 84.5% 82.7%
Wang 2006 SVM 87.2% 88.3% 87.8%
Wang 2006 Stacked 88.3% 88.1% 88.2%
Our parser 89.4% 90.1% 89.8%
Table 3: Accuracies on CTB2 with gold-standard
POS-tags
own implementation of the perceptron-based tag-
ger from Collins (2002).
The results of various models measured using
sentences with less than 40 words and using gold-
standard POS-tags are shown in Table 3. The
rows represent the model from Bikel and Chiang
(2000), Bikel (2004), the SVM and ensemble mod-
els from Wang et al (2006), and our parser, re-
spectively. The accuracy of our parser is competi-
tive using this test set.
The results of various models using automati-
cally assigned POS-tags are shown in Table 4. The
rows in the table represent the models from Bikel
and Chiang (2000), Levy and Manning (2003),
Xiong et al (2005), Bikel (2004), Chiang and
Bikel (2002), the SVM model from Wang et al
(2006) and the ensemble system from Wang et
al. (2006), and the parser of this paper, respec-
tively. Our parser gave comparable accuracies to
the SVM and ensemble models from Wang et al
(2006). However, comparison with Table 3 shows
that our parser is more sensitive to POS-tagging er-
rors than some of the other models. One possible
reason is that some of the other parsers, e.g. Bikel
(2004), use the parser model itself to resolve tag-
ging ambiguities, whereas we rely on a POS tag-
ger to accurately assign a single tag to each word.
In fact, for the Chinese data, POS tagging accu-
racy is not very high, with the perceptron-based
tagger achieving an accuracy of only 93%. The
beam-search decoding framework we use could
accommodate joint parsing and tagging, although
the use of features based on the tags of incom-
ing words complicates matters somewhat, since
these features rely on tags having been assigned to
all words in a pre-processing step. We leave this
problem for future work.
In a recent paper, Petrov and Klein (2007) re-
ported LR and LP of 85.7% and 86.9% for sen-
tences with less than 40 words and 81.9% and
84.8% for all sentences on the CTB2 test set, re-
167
? 40 words ? 100 words Unlimited
LR LP F1 POS LR LP F1 POS LR LP F1 POS
Bikel 2000 76.8% 77.8% 77.3% - 73.3% 74.6% 74.0% - - - - -
Levy 2003 79.2% 78.4% 78.8% - - - - - - - - -
Xiong 2005 78.7% 80.1% 79.4% - - - - - - - - -
Bikel Thesis 78.0% 81.2% 79.6% - 74.4% 78.5% 76.4% - - - - -
Chiang 2002 78.8% 81.1% 79.9% - 75.2% 78.0% 76.6% - - - - -
Wang 2006 SVM 78.1% 81.1% 79.6% 92.5% 75.5% 78.5% 77.0% 92.2% 75.0% 78.0% 76.5% 92.1%
Wang 2006 Stacked 79.2% 81.1% 80.1% 92.5% 76.7% 78.4% 77.5% 92.2% 76.2% 78.0% 77.1% 92.1%
Our parser 80.2% 80.5% 80.4% 93.5% 76.5% 77.7% 77.1% 93.1% 76.1% 77.4% 76.7% 93.0%
Table 4: Accuracies on CTB2 with automatically assigned tags
? 40 words Unlimited
LR LP F1 POS LR LP F1 POS
87.9% 87.5% 87.7% 100% 86.9% 86.7% 86.8% 100%
80.2% 79.1% 79.6% 94.1% 78.6% 78.0% 78.3% 93.9%
Table 5: Accuracies on CTB5 using gold-standard and automatically assigned POS-tags
Sections Sentences Words
Set A 001?270 3,484 84,873
Set B Set A; 400?699 6,567 161,893
Set C Set B; 700?931 9,707 236,051
Table 6: Training sets with different sizes
spectively. These results are significantly better
than any model from Table 4. However, we did
not include their scores in the table because they
used a different training set from CTB5, which is
much larger than the CTB2 training set used by all
parsers in the table. In order to make a compari-
son, we split the data in the same way as Petrov
and Klein (2007) and tested our parser using auto-
matically assigned POS-tags. It gave LR and LP
of 82.0% and 80.9% for sentences with less than
40 words and 77.8% and 77.4% for all sentences,
significantly lower than Petrov and Klein (2007),
which we partly attribute to the sensitivity of our
parser to pos tag errors (see Table 5).
5.3 The effect of training data size
CTB2 is a relatively small corpus, and so we in-
vestigated the effect of adding more training data
from CTB5. Intuitively, more training data leads
to higher parsing accuracy. By using increased
amount of training sentences (Table 6) from CTB5
with the same development test data (Table 2),
we draw the accuracy curves with different num-
ber of training iterations (Figure 6). This exper-
iment confirmed that the accuracy increases with
the amount of training data.
Figure 6: The influence of the size of training data
Another motivation for us to use more training
data is to reduce overfitting. We invested consid-
erable effort into feature engineering using CTB2,
and found that a small variation of feature tem-
plates (e.g. changing the feature template S0cS1c
from Table 1 to S0tcS1tc) can lead to a compar-
atively large change (up to 1%) in the accuracy.
One possible reason for this variation is the small
size of the CTB2 training data. When performing
experiments using the larger set B from Table 6,
we observed improved stability relative to small
feature changes.
168
Sections Sentences Words
Training 001?815; 16,118 437,8591001?1136
Dev 886?931; 804 20,4531148?1151
Test 816?885; 1,915 50,3191137?1147
Table 7: Standard split of CTB5 data
Non-root Root Complete
Zhang 2008 86.21% 76.26% 34.41%
Our parser 86.95% 79.19% 36.08%
Table 8: Comparison with state-of-the-art depen-
dency parsing using CTB5 data
5.4 Test accuracy using CTB5
Table 5 presents the performance of the parser on
CTB5. We adopt the data split from Zhang and
Clark (2008), as shown in Table 7. We used the
same parser configurations as Section 5.2.
As an additional evaluation we also produced
dependency output from the phrase-structure
trees, using the head-finding rules, so that we
can also compare with dependency parsers, for
which the highest scores in the literature are cur-
rently from our previous work in Zhang and Clark
(2008). We compare the dependencies read off our
constituent parser using CTB5 data with the depen-
dency parser from Zhang and Clark (2008). The
same measures are taken and the accuracies with
gold-standard POS-tags are shown in Table 8. Our
constituent parser gave higher accuracy than the
dependency parser. It is interesting that, though
the constituent parser uses many fewer feature
templates than the dependency parser, the features
do include constituent information, which is un-
available to dependency parsers.
6 Related work
Our parser is based on the shift-reduce parsing
process from Sagae and Lavie (2005) and Wang
et al (2006), and therefore it can be classified
as a transition-based parser (Nivre et al, 2006).
An important difference between our parser and
the Wang et al (2006) parser is that our parser
is based on a discriminative learning model with
global features, whilst the parser from Wang et al
(2006) is based on a local classifier that optimizes
each individual choice. Instead of greedy local de-
coding, we used beam search in the decoder.
An early work that applies beam search to con-
stituent parsing is Ratnaparkhi (1999). The main
difference between our parser and Ratnaparkhi?s is
that we use a global discriminative model, whereas
Ratnaparkhi?s parser has separate probabilities of
actions chained together in a conditional model.
Both our parser and the parser from Collins and
Roark (2004) use a global discriminative model
and an incremental parsing process. The major
difference is the use of different incremental pars-
ing processes. To achieve better performance for
Chinese parsing, our parser is based on the shift-
reduce parsing process. In addition, we did not in-
clude a generative baseline model in the discrimi-
native model, as did Collins and Roark (2004).
Our parser in this paper shares similarity
with our transition-based dependency parser from
Zhang and Clark (2008) in the use of a discrimina-
tive model and beam search. The main difference
is that our parser in this paper is for constituent
parsing. In fact, our parser is one of only a few
constituent parsers which have successfully ap-
plied global discriminative models, certainly with-
out a generative baseline as a feature, whereas
global models for dependency parsing have been
comparatively easier to develop.
7 Conclusion
The contributions of this paper can be summarized
as follows. First, we defined a global discrimina-
tive model for Chinese constituent-based parsing,
continuing recent work in this area which has fo-
cused on English (Clark and Curran, 2007; Car-
reras et al, 2008; Finkel et al, 2008). Second, we
showed how such a model can be applied to shift-
reduce parsing and combined with beam search,
resulting in an accurate linear-time parser. In stan-
dard tests using CTB2 data, our parser achieved
comparable Parseval F-score to the state-of-the-
art systems. Moreover, we observed that more
training data lead to improvements on both accu-
racy and stability against feature variations, and
reported performance of the parser using CTB5
data. By converting constituent-based output to
dependency relations using standard head-finding
rules, our parser also obtained the highest scores
for a CTB5 dependency evaluation in the literature.
Due to the comparatively low accuracy for Chi-
nese POS-tagging, the parsing accuracy dropped
169
significantly when using automatically assigned
POS-tags rather than gold-standard POS-tags. In
our further work, we plan to investigate possible
methods of joint POS-tagging and parsing under
the discriminative model and beam-search frame-
work.
A discriminative model allows consistent train-
ing of a wide range of different features. We
showed in Zhang and Clark (2008) that it was pos-
sible to combine graph and transition-based de-
pendency parser into the same global discrimina-
tive model. Our parser framework in this paper
allows the same integration of graph-based fea-
tures. However, preliminary experiments with fea-
tures based on graph information did not show
accuracy improvements for our parser. One pos-
sible reason is that the transition actions for the
parser in this paper already include graph infor-
mation, such as the label of the newly gener-
ated constituent, while for the dependency parser
in Zhang and Clark (2008), transition actions do
not contain graph information, and therefore the
use of transition-based features helped to make
larger improvements in accuracy. The integration
of graph-based features for our shift-reduce con-
stituent parser is worth further study.
The source code of our parser is publicly avail-
able at http://www.sourceforge.net/projects/zpar.2
Appendix
The set of restrictions which ensures a valid binary
tree is shown below. The restriction on the num-
ber of consecutive unary rule applications is taken
from Sagae and Lavie (2005); it prevents infinite
running of the parser by repetitive use of unary re-
duce actions, and ensures linear time complexity
in the length of the sentence.
? the shift action can only be performed when
the queue of incoming words is not empty;
? when the node on top of the stack is tempo-
rary and its head word is from the right child,
no shift action can be performed;
? the unary reduce actions can be performed
only when the stack is not empty;
? a unary reduce with the same constituent la-
bel (Y ? Y ) is not allowed;
? no more than three unary reduce actions can
be performed consecutively;
2The make target for the parser in this paper is chi-
nese.conparser.
? the binary reduce actions can only be per-
formed when the stack contains at least two
nodes, with at least one of the two nodes on
top of stack (with R being the topmost and L
being the second) being non-temporary;
? if L is temporary with label X?, the result-
ing node must be labeled X or X? and left-
headed (i.e. to take the head word from L);
similar restrictions apply when R is tempo-
rary;
? when the incoming queue is empty and the
stack contains only two nodes, binary reduce
can be applied only if the resulting node is
non-temporary;
? when the stack contains only two nodes, tem-
porary resulting nodes from binary reduce
must be left-headed;
? when the queue is empty and the stack con-
tains more than two nodes, with the third
node from the top being temporary, binary re-
duce can be applied only if the resulting node
is non-temporary;
? when the stack contains more than two nodes,
with the third node from the top being tempo-
rary, temporary resulting nodes from binary
reduce must be left-headed;
? the terminate action can be performed when
the queue is empty, and the stack size is one.
170
References
Daniel M. Bikel and David Chiang. 2000. Two sta-
tistical parsing models applied to the Chinese Tree-
bank. In Proceedings of SIGHAN Workshop, pages
1?6, Morristown, NJ, USA.
Daniel M. Bikel. 2004. On the Parameter Space of
Generative Lexicalized Statistical Parsing Models.
Ph.D. thesis, University of Pennsylvania.
Ted Briscoe and John Carroll. 1993. Generalized prob-
abilistic LR parsing of natural language (corpora)
with unification-based grammars. Computational
Linguistics, 19(1):25?59.
Xavier Carreras, Michael Collins, and Terry Koo.
2008. Tag, dynamic programming, and the percep-
tron for efficient, feature-rich parsing. In Proceed-
ings of CoNLL, pages 9?16, Manchester, England,
August.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of NAACL, pages
132?139, Seattle, WA.
David Chiang and Daniel M. Bikel. 2002. Recovering
latent information in treebanks. In Proceedings of
COLING.
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG
and log-linear models. Computational Linguistics,
33(4):493?552.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceed-
ings of ACL, pages 111?118, Barcelona, Spain, July.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of
the 35th Meeting of the ACL, pages 16?23, Madrid,
Spain.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of EMNLP, pages 1?8, Philadelphia, USA, July.
Xiangyu Duan, Jun Zhao, and Bo Xu. 2007. Proba-
bilistic models for action-based Chinese dependency
parsing. In Proceedings of ECML/ECPPKDD, War-
saw, Poland, September.
Jenny Rose Finkel, Alex Kleeman, and Christopher D.
Manning. 2008. Efficient, feature-based, con-
ditional random field parsing. In Proceedings of
ACL/HLT, pages 959?967, Columbus, Ohio, June.
Association for Computational Linguistics.
Roger Levy and Christopher D. Manning. 2003. Is it
harder to parse Chinese, or the Chinese Treebank?
In Proceedings of ACL, pages 439?446, Sapporo,
Japan, July.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proceedings of ACL, pages 91?
98, Ann Arbor, Michigan, June.
Joakim Nivre, Johan Hall, Jens Nilsson, Gu?ls?en
Eryig?it, and Svetoslav Marinov. 2006. Labeled
pseudo-projective dependency parsing with support
vector machines. In Proceedings of CoNLL, pages
221?225, New York City, June.
Slav Petrov and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In Proceedings of
HLT/NAACL, pages 404?411, Rochester, New York,
April. Association for Computational Linguistics.
Adwait Ratnaparkhi. 1999. Learning to parse natural
language with maximum entropy models. Machine
Learning, 34(1-3):151?175.
Kenji Sagae and Alon Lavie. 2005. A classifier-based
parser with linear run-time complexity. In Proceed-
ings of IWPT, pages 125?132, Vancouver, British
Columbia, October.
Honglin Sun and Daniel Jurafsky. 2003. The effect of
rhythm on structural disambiguation in Chinese. In
Proceedings of SIGHAN Workshop.
Mengqiu Wang, Kenji Sagae, and Teruko Mitamura.
2006. A fast, accurate deterministic parser for Chi-
nese. In Proceedings of COLING/ACL, pages 425?
432, Sydney, Australia.
Deyi Xiong, Shuanglong Li, Qun Liu, Shouxun Lin,
and Yueliang Qian. 2005. Parsing the Penn Chinese
Treebank with semantic knowledge. In Proceedings
of IJCNLP.
Yue Zhang and Stephen Clark. 2008. A tale of
two parsers: investigating and combining graph-
based and transition-based dependency parsing us-
ing beam-search. In Proceedings of EMNLP, pages
562?571, Hawaii, USA, October.
171
A Class-based Probabil ist ic approach to Structural 
Disambiguat ion 
Stephen Clark and David Weir 
School of Cognit ive and Comput ing  Sciences 
University of Sussex 
Brighton, BN1 9IIQ, UK 
{ st ephec:l_, david~r}Ocogs, usx. ac. uk 
Abstract  
Knowledge of which words are able to fill p~rtic- 
ular argum.ent slots of a predicate can be used 
tbr structural disambiguation. This paper de- 
scribes a proposal :for acquiring such knowledge, 
and in line with much of the recent work in this 
area, a probabilistic approach is taken. We de- 
velop a novel way of using a semantic hierar- 
chy to estimate the probabilities, and demon- 
strate the general approach using a preposi- 
tional phrase atta.chment experiment. 
1 Introduction 
Knowledge of which words are able to fill 
particular a.rgument slots of a. l?redlca.te ca,n 
be used tbr structural disa.mbiguation. In 
the following example (Charnial~, 1993), the 
fact that dog, rather than prize, is often 
the su.1)ject of r'lm, can t)e used to decide 
on the attachment site of the relative clause: 
Fred awarded a prize for the dog that ran the fastest 
We describe a proposal for acquiring such 
knowledge, and as in other recent work in this 
area (Resnik, 1993; l,i and Abe, t998), a prob- 
abilistic approach is taken. Using probabilities 
accords with the intuition that there are no ab- 
solute constraints on the arguments of predi- 
cates, bu.t rather that constraints are satisfied 
to a certain degree (Resnik, 1993). Unfortu- 
nately, defining probabilities in terms of words 
leads to a model with a vast number of param- 
eters, resulting in a sparse data problem. To 
overcome this, we propose to define a probabil- 
ity model in terms of senses from a semantic hi- 
erarchy, exploiting the fact that senses of nouns 
can be grouped together into semantically sim- 
ilar classes. 
We use the semantic hierarchy of noun senses 
in WordNet (Fellbamn, 1.998), which consists of 
qexicalised concepts' related by the qs-a-kind- 
of' relation. If c' is a kind of c, then c is a hy- 
pcrnym of c', and c' a hyponym of c. Counts are 
passed u.p the hierarchy fl'om the senses of nouns 
appearing in the data. Thus if cat chicl~cn a.p- 
pears in the data, th.e count for this item passes 
u\]) to (meat}, (good}, and all the other hyper- 
nyms of that sense of chicken. 1 In. order to es- 
timate the probability that a sense of chichcn 
~tppea.rs as the object of the verb cat, we repre- 
sent (chicken} using a. suitable hypern3qn , such 
as (:eood), and base our probability estimate on 
that instead. The level at which (chicken) is 
represented is cruciah it should be high enough 
for adequate counts to have accumulated, but 
not too high so that the hypernym is no longer 
representat ive  of (chicken}.  An exanlp le  of a 
hypernym whidl would be too high is (e r t t i ty ) ,  
as not all entities are semantically similar with 
respect o the object position ot7 cat. 
The problem of choosing an appropria.te l vel 
in the h.ierarchy at which to represent a par- 
ticular noun sense (given a predicate and argu- 
ment position) has been investigated by Resnik 
(1993), Li and Abe (1998) and ll,iba,s (1995). 
The learning mechanism presented \]lore is a 
novel approach based on tinding semantically 
similar sets of concepts in a hierarchy. We 
demonstrate the effectiveness of our approach 
using a PP-attachment experiment. 
2 The Input Data  and Semant ic  
Hierarchy 
The data used to estimate the probal)ilities is 
a multiset of 'co-occurrence triples': a noun 
IWe use italics when referring to words, ~Lnd angled 
bra.ckets for concepts. This notation does not alwa.ys 
pick out a concept uniquely, but the context should make 
cle~n: the concept being referred to. 
194 
\]enltna., verb len, ina, and argunien/, i)osition. 2 
l,et the li;ilivc.rso of verbs~ argll l i lent posi- 
tions aud ,lOtlBS tha.t call appear in the in- 
put data. be denoted "l) = { "Vl~... ~ 'vkv }~ "\]~., : 
{ , , . , , . . . ,  ,.,,~ } a,d N = { ' ,~, , . . . ,  ",J,H }, ~'esi,e,- 
tiw:ly. Sucll data c~ui I)e obtMned fro,,, a. tree- 
ban,s, or from a. shallow pa:rser. Note tha.t we 
do l ie, distinguish I)etwee, i a.lternative seiises of 
ve,'\])s~ a,lld assUIfle tha.t each \]llsta.llCe O\[ a. no,i l l  
i,I t i le data, refel?s to exactly olle conc, ept .  
The sei-i-la.ii{ic hiel:a.rchy used is the tie,Ill hy- 
\[)efltylIl ta.xo:nonly of \/Vo,'dNet (vel'SiOll \]..6). :3 
l,e~ (7,' = { e l , . . . ,  Chc } be tile sot; of concepts 
in WordNet (lq: ,-~ 66,000). A concel,t is rel)re- 
sented in \?ordNet 1,y a synset: a sel o1' syilolly- 
,nous words which cat, I)<7: used to (lenotc lha.1 
c.oncei)t. I!kil7 exa.nll)iO ~ the COliC;el)l, ~co('.a.iile~ 
a.s iii the (\[rtlg~ iS represented l)y l.he following 
synset: {cocaine, co(-ai't~, coD(;, .,no'w, C}. l,et 
syn(c) C ;V l,e the syllscl; I'or the (:olicel)t c, 
a.d  let c,,(,,.) - { c I"" m sy,,(+:) } I,e the. set of 
concepts that  (;a, ii be denoted by the llO,lli 17.. 
The \]liera.rc\] U has the stl'llCtlll'e O\[ a directed 
acyciic gi'a.l,\]i : althougli the nunll)er of nodes in 
the gra.ph wil;h lllO,'e 1,ha.l/ olle \])al'elll. is only 
a,rOtllid cite i)er(tent o f  the  tota l .  The  edges ill 
the graph \['orni what we call the dii'ecl.-isa rela-- 
Lion (dire('.t-is;,. C C' x {'). l,et isa = dii'ect-isa. X 
be ,lie tI'a, llSitivc~ re\[lexive (;\]OStlre Of (lirect-isa, 
so t\]iat (c/, c) ~ \]sa :=> c is a \] iy\])ernynl o;\[' (/; a.nd 
let ?~ = { c' \[(c' ,c) misa. } I)e the set consisting 
of the concept c and all of its hyponynis. Thus, 
the set (*ood) conta.ins all the concel)ts wtiich 
:q,re kinds of food, inclllditig (food). 
Note tha.t words in the data can al)pear in 
SyllSCts a, liy\vh01'0 ill the  hiel;archy. Even  COll- 
cel)ts Sllch a.s (entity)~ which a,l)pe.ar 1,ear t\]ie 
l:OOt el" the hierarchy, have synsets containing 
words which may a.pl)ear hi the da.ta. The 
synset for (entity) is {c'ntitg, something}, and 
the words cntit;q a.nd something (';/,ll apl)ear in 
the a.rgulnent positions of verbs in the data. 
3 Probab i l i ty  Es t imat ion  
The problem being a.ddressed in this section is 
to est imate p(civ, r), for c C C, v < P, and 
2Only verbs a.re considered here, but this work applies 
to other predicates which take a.rgunmnts that can bc 
orga.nised into a semantic hierarchy. 
3When wc refer (.o concepts ill \'VoMNct, wc nlea.n 
concepts ill WordNet's nomi ta, xonomy.  
r C 'R.. The I~roba.bility p(eiv , r )  is the 1)rob- 
ability tha.t some lie\ill in syn(c)~ when dellOf 
itlg coneet)t c, appears in position 'r of" verl) 'v 
(given r a.nd v). Using the relative clause ex- 
anlpie fl'Otil tile hi,reduction> the p,'obal)ilities 
p((dog}lru,z,subj ) a, nd p((prize)lrv.',z,sub.i ) ca.n 
be toni_pared to decide on the attachluent site 
f l  i,i I red awa'rded a p?'izc Jot iitc dog t/tat ran 
~l,; f,.~:~;.,.:. We expe~t 1'((dog) l""', ~m,.i) to 1,e 
grea.ter than  p((pr?ze) l'l'zt'~z,subj). Al thougt ,  the 
\['OCllS iS O11 7,(c1~,,,), the  tochniq l ies  descr ibed  
here can be used to estimat, e other lm)babilities, 
such a.s p(c, f ly  ). (in fa.ct, the latter prol)alfii- 
il, y is used hi the Pl>-a?ta.clinient CXl)erhnents 
de.qcril)ed in Section 5.) 
Using n, axinlun/ likelihood to cstinia.tc 
\])(C\['V, '/')iS i iot  via.hie 1)eca.use of the l iugc nui l l -  
t)er of I)al'al,l or.ors i\]i vo\] red. ~/lal ly COl IIt,i II a.tiolls 
O\[ C, 'l) a.lld 7' will / lot OCCIII" in the data .  '1'o re- 
duce  the ntii,il)ei' of i)a.ranletcrs which need to 
\])e estima.ted~ we utilise tile fa.ct tha.t COllCepts 
Call be grouped ini;o cla.sses, a.nd \ ] 'epresent  C IlS- 
ing a class (/, for some hypernynl c' of c. Ilow- 
ever, p(c'lv , r)ca.nnot be used as a.n estiniate of 
v(( l , , ,  ,'), as V((:I"', "') is give.  l,y the foliowi,,g: 
E . (  c'l,,, .,.) = v( ( : ' l . ,  ,) 
<:"C~ 
The probal)ility ~)(("l'~") i,,c,'eases as c' 
moves up the hiera.rchy. For example,  
s)((eooa)l,;aZ,oi,.i ) is ,,or a good estiu,a,te of 
1,((chicken)leat,obj ). What can be done 
though, is to condition on sets of concepts, and 
use the probabil ity p(v\[c', r). If it ca:n be shown 
tha.t p(v\[c', r), for some hypernym c' of c, is a. 
reasoliable esti lnate o1' v(vlc, v), then we have a. 
wa.y of estiniati,ig p(clv, r). To get \])(vie; , r ) f rom 
l ,(dv, ,') i~ayes ,',ie is ,,sed: 
p(4*,,,') p(vl~, "'v(cl'') = 7 ) ~  
The prol)abilities p(clr ) and p(v\[r) cm~ be esti- 
mated using maximum likelihood esti,n~tes, a.s 
the conditioning event is likely to occur often 
enough for sp;tl'se data not to be a problem. 
(Alternatively ()tie could ha.ok-off to p(c) and 
ply) respectively, or use a. linear combhia.tion 
of p(d' , )a. ,d \],(c), ,.,d P0,1v)a,d V('0, ,'espoc- 
tively.) The formula.e for these est imates will 
I,e give,, shortly. This only leaves plY\[c, r). The 
195 
proposM is to estilnate P(eatl(~h?cken>, oh j) us- 
ing  p(eat\](food), oh j), or something similar. The 
following proposition shows that if p(vlc" , r) is 
I 
the same for each c" in c ' ,  where c' is some 
hypernym of c, then p(v\]c', r) will be equal to 
p(v\[c, r): 
- -  I 
\],(~1c",7.) :/~ for all c" ~ c' ~ \],(vlc',7.)= 
The proo f  is as fo l lows:  
,\],(~17") \],(vl7,7") = \ ] , (71~, , )~ 
- ~'(L 17") ~ p(c"lv,7,) 
z,(c'l,') - clinG 1 
_ z,(~l~') V" \],(~ d', 7.~ \]'(c'17) 
z,(c~lT,) ~ ' ' p(~lT) 
_ 2 a, ~ p(c"l,0 
p(c'lT,) _ 
Cl ing I 
= /~ 
So in order to estimate p(v\[c,r), we need a 
way of searching for a set c', where c' is a hy- 
pernym of c, which consists of concepts c" which 
have similar p(v\]c", r). Of conrse we cannot ex- 
pect to find a set consisting of concepts which 
have identical p(vlc", r), which the proposition 
strictly requires, but if the p(vlc" , 7") are simila.r, 
then we can expect p(vld , r) to be a. reasonable 
estimate of p(vlc , 7"). We refer to the set c' as 
the %imilarity-class' of c, and the suitable hy- 
pernym, c l, as top(c, v, r). The next section ex- 
plains how we determine similarity classes. The 
maxim.urn likelihood estimates for the relevant 
probabilities m:e given in Ta.ble 1.4 
4 F ind ing  S imi la r i ty -c lasses  
First we explain how we determine if a set of 
concepts has similar p(vlc", r) for each concept 
c" in the set. Then we explain how we determine 
top(c, v, r). 
4Since we are a.ssuming the data. is not sense dis- 
a.mbiguated, f,:eq(c, v, r) cannot be obtained by sim- 
ply counting senses. The standard approach, which is 
adopted here, is to estimate fl'eq(c, v, r) by distributing 
the count tor each noun n in syn(c) evenly among all 
senses of the noun. Yarowsky (1992) and \]{esnik (1993) 
explain how the noise introduced by this technique tends 
to dissipate as counts are passed up the hierarchy. 
Table 1: Maximum Likelihood Esti:lnates 
freq(c, v, r) is the number of (n, v, r) triples in 
the data in which n is being used to denote c. 
fl 'eq(c,r) Ev'EV freq(c,v',r) 
P(CI? ' )  : " frcq(r)  - -  Ev 'EVEdcc f rcq(c ' ,v ' , r )  
freq(v,r) Ec'Ec freq(ct,v,r) 
/ ) (VlT")-  freq(r) : Zv ,EVZc ,  ccfreq(c',v',r) 
\]}(vie w, 7") -- freq(c-i"v'r) Z~"c~77freq(c't'v'r) 
rreq(d,,-) = Ev,evE~,,~Tf,-eq(~",'~,',,-) 
Tile method used for comparing the p(vlc" , r) 
for c" in some set c', is based on the technique 
ill Clark and Weir (1999) used for tinding homo- 
geneous ets of concepts in the WordNet noun 
hierarchy. Rather than directly compare esti- 
mates ofp(vlc" , r), which are likely to be unreli- 
able, we consider the children of c', and use esti- 
mates based on counts which have accumulated 
I , /  , /  at the children. If c' has children Q,%, . . . ,  c,,,,, 
I 
we compare e'(~l<, ") for each i. Th~s is an 
I 
a.pproximation, but if the p(vlc}, r) arc similar, 
I 
then we assume that the p(vlc" ,r)  for c" in c' 
are similar too. 
To deterlnine whether the children of some 
? . , ./is the hyperny,~ c' have simila,' \]'('~'14) where c~ 
ith child, we apply a X 2 test to a contingency 
tM)le of frequency counts. Table 2 shows some 
e?a.mple frequencies for c' equM to (nutriment), 
in the ol)ject position of cat. The figures in 
brackets are the expected values, based on the 
marginal totMs in the table. The null hypoth- 
esis of the test is that p(vl@ r ) i s  the same for 
each i. libr TM)1e 2 the null hypothesis is tlmt 
,I tbr every child, ci, of (nutr?ment}, the probabil- 
ity p(catlc~, obj) is the same. 
The log-likelihood X 2 statistic corresponding 
to TM)le 2 is 4.8. The log-likelihood X 2 statistic 
is used rather than the Pearson's X 2 statistic 
because it is thought to be more appropriate 
when the counts in the contingency table are 
low (\])unning, 1993). This tends to occur when 
the test is being applied to a set of concepts 
near the foot of the hierarchy, s We compared 
5Fisher,s exa.ct test could be used for tables with low 
counts, but we do not do so because tables dolninated 
by low counts are likely to have a. high percentage of 
noise, due to the way counts for a noun are split ~unong 
196 
ridable 2: Contingency tal)le for children of (nutriment) 
ci 
milk) 
<meal) 
(course) 
(d?s~) 
(del?cacy) 
f r \ [N(~,  cat, oh.i) 
o.o (o.(~) 
J.a (l.r) 
s.a (s.r) 
o.a (~ .s) 
\] 5.d 
r,.~,q(~, oh.i)- 
l't'(x I(~, cal, oh j) 
9.0 (s..,~) 
rs.o (so..o) 
24.r (24.a) 
s2.a (s~ ..0) 
2r.4 (~s.9) 
221.4 
r,4q(~, oh.i) = 
E, ,~v  r,.~,q(W, . ,, oh.i) 
9.0 
86.5 
26.0 
87.6 
27.7 
the l)erformance of log-likelihood X 2 and Pear- 
son's X ~2 using the l>P-~tttaehment experhnent 
described in Section 5. It was found that the 
log-likelihood ~2 test; did perform slightly bet- 
t('r. \]"or a signitic~nce l w;I ot' 0.05 (which is the 
level used in the exl)eriments), with 4 degrees 
of freedom, the critical wdue is 1,1.86 (llowell, 
;1!197). Thus in this ca.se, tlle null hyl~othesis 
would not be rejected. 
In order to determine top(c, v, r), we conlpare 
l,(vl~7, v) re,: the children of the hypernyms of 
c. hlitially top(c, 'v, r) ix assigned to I)e the con- 
eet)t c itself. Then, l>y worldng Ull the hierarrclly, 
top((:, 'V, r) is reassigned to I)(' successive hyl)er- 
nyms of c until the siblings of tol)(C , ~7+ 7')have 
siglfifi(:a.ntly different prol)abilities. In cases 
where a. concept has more than one I)a.J'ent, the 
parent is chosen which results in tile lowest :\~2 
wflue as this indicates the p(v\[U,r) are more 
simila.r. The set top(c ,v , r )  is the sinfi\]a.rity- 
cla.ss of c t'or verb v and position r. 
Th(; next section provides evidence that tile 
technique for choosing lOl)(C , v, r), which we call 
the 'simihu'ity-class' technique, does select an 
appropriate level of generalisation. 
5 Exper iments  us ing  PP -a t tachment  
ambigu i ty  
The l>P-atta.chme:nt problem we address con- 
siders 4-tuples of the form v,:,t,,pr, n2, and 
the l)robleln is to decide wllether tile prel)o- 
sitional phrase pr n2 attaches to the verl> v 
or the 71oun nl. For exatnl)le, in the fol- 
lowing cas(; tim l)rol)lent is to decide whether 
a l ternat ive  senses.  YVe rely on the  log- l ikel ihood X ,2 test  
re turn ing  a, non-s ign i f i cant  result  in these cases. 
J)'om minister attaches to awaii or approvah 
a.wait apt)7'owd from minister 
We chose the l~P-attachn~ent l)roblenl beca.use 
P l>-attaehment is a perw,.sive form of ambiguity, 
and there exist sta.ndard training and text da.ta~ 
which ma.kes for easy comparisons with other 
a.pproache~s. This p7'oblenl has been tackled by a 
nu nlber of resea.rehers, lh'ill and Resnik (1994), 
Ratnal)arkhi et al (\]994), Collins (1995), Za- 
w:el and l)aelemans (\] 997) all report results be- 
tween 81% and 85%, with Stetina. and Nagao 
(\] 997) tel)erring a result of 88%, which matches 
lhe hunm,t+ l>erf'ornlan(;e on this task rel)orted by 
Ratnal>arkhi (% al. (199.'1). 
Althougll th(' l)l)-attachnwnt l)roblem has 
chara('teristics that n,a.ke it suita.ble for ('valua.- 
t;ion, it; I)resents a inuch bigger sparse data. t)\]:ol)- 
le, m tlla.n would 1)e exl)ected in other l)roblems 
such as relative (:lausc atSadlment. The reason 
for this is that we need 1;(7 cot,sider how ~l C()l~ - 
Cel)t is associated with combi~zations of predi- 
cates and prel)ositions. T\]le al)proach described 
11(;7"(; uses prolml)ilities of the Ibrnl p(c, prlv ) 
,u,d ~,,(c.z,,l,,.,), who,;o ,~ ~ ~,l(,+~). Th is  . lea, is  
that for many predicate/prel)osition combina- 
tions which occur infl'equently in the d~ta., there 
are few examples of n2 which ca.n be used lot 
populating Wo7'dNet in these cases. Despite 
this, we were still able to carry out an ewl.lu- 
ation by considering subsets of the test (ta.ta for 
which the relewmt predicate~preposition com- 
I)inations did occur frequently in tit(; training 
d at a,. 
We deckle on tile a.tta('hnmnt site by compar- 
197 
ing p(c~, pr\[v) and p(c,~,, p,'\],q), where 
= a rg n ax l,(c,p,'lv) 
c,z 1 = arg max p(c, prlTq ) 
The sense of n2 is chosen which maximises 
the relevant probability in each potential at- 
tachment case. If p(c,,,p,jv)is greater than 
1)(%, :m'l~l), the attachment is made to v, oth- 
erwise to nl. If n2 is not in WordNet we com- 
pare p(prlv ) and p(prl~t~). Probabilities of the 
form p(c, prlv ) and p(c, prl~tl ) are used rather 
than p(clv,pr ) and p(cl~l,p,j, because the as- 
sociation between the preposition and v and ~q 
contains useful information. In fact, for a lot 
of cases this intbrmation alone can be used to 
decide on the correct attachment site_ The orig- 
inal corpus-based method of \]Jindle and ll.ooth 
(1993) used exactly this information. Thus the 
method described here can be thought of as Hin- 
dle and Rooth's method with additional class- 
based information about n2. 
In order to estimate p(c,, ,pr lv)(and 
p(C,~l,ln'l,,,,)) we apply the same procedure 
as described in Section 3, first rewriting the 
probability using Bayes' rule: 
p,,.)p(c,,, p,.) p (c , , , j , , l v ) - -  p(vlcv, v(v) 
p,.) !'(P'q c,, ) 
: p(dc,,, l,(v) 
The probabilities p(c.~) and p(v) can be es- 
timated using maximum likelihood estimates, 
a.nd p(vlcv, p,' ) and j,(p,'lc,) can be esti- 
m.ated using maximum likelihood estimates of 
p(vltop(c~ ,v,p,'),pr) and p(prltop(%,pr)) re- 
spectively. 6 
We used the training and test data described 
in l/.atn.aparkhi et al (1994:), which, was taken 
Doln the Penn %:eebank and has now become 
the standard data set for this task. The data 
set consists of tuples of the form (v, ~zl, p~', n2), 
together with the attachment site for each tu- 
ple. There is also a development set to prevent 
implicit training on the test set during develop- 
ment. \~e extracted (v, pr, '~2) and (hi, pr, ,z2) 
~ln Section 4 we only gave the procedure for deter- 
mining top(c~, v, pr), but top(c~, pr) can be determined 
in an analogous fashion. 
triples from the training set, and in order to in- 
crease the number of training triples, we also 
extracted triples Kern unambiguous cases of at- 
tachlnent in the Penn %'eebank. We prepro- 
cessed the training and test data by \]emmatising 
the words, replacing numerical amounts with 
the words ~definite_quantity', replacing mone- 
tary amounts with the words 'sum_olLmoney' 
etc. We then ignored those triples in the re- 
sulting training set (but not test set) for which 
7z2 was not in WordNet, which left a total of 
66,881 triples of training data.. The test set 
contains 3,097 examples. 
Table 3 gives seine examples of the ex- 
tent to which the similarity-class technique 
is generalising, using the training data just 
described, and a significance level of 0.05. 
The chosen hypernym is shown in Ul)per 
case. Note that the WordNet hierarchy con- 
sists of nine separate sub-hierarchies, headed 
by such concepts  as (ent i ty>,  (abst rac t ion) ,  
(psycho log ica l~eature) ,  bnt  we assume the ex- 
istence of a single root which dominates each 
of the sub-hierarchies, which is referred to as 
(root>. In cases where WordNet is very sparsely 
populated, it is preferable to go to (root), 
rather than stay at the root of one of the sub- 
hierarchies where the data may be noisy or too 
sparse to be o\[' any use. The table shows that 
with the amount of data ava.ilable from the Tree- 
bank, the similarity-class technique is selecting 
a. level at or close to (root> in many cases. 
We compared the similarity-class technique 
with fixing the level of generalisation. Two tixed 
levels were used: the root of the entire hieraJ'- 
chy ((root>), and the set consisting of the roots 
of each of the 9 sul>hierarchies. The procedure 
which always selects (root} ignores any informa- 
tion about ~z2, and is equivalent o comparing 
p(prlv ) and p(prl ,h),  which is the ltindle and 
Rooth approach. The results on the 3,097 test 
cases are shown in Table 4. We used a. signifi- 
cance level a of 0.05 tbr the X 2 test. r
As the table shows, the disambiguation ac- 
curacy is below the state of the art. However, 
the results are comparable with those of l,i and 
rSimilar results were obtained using alternative l vels 
of signifiea.nce. Rather than simply selecting a value for 
a, such as 0.05, a' can be tree,ted as a parameter of the 
model, whose optimum value caJl be obtained by running 
the disambiguation method on some held-out supervised 
data. 
198 
'l'al)le 3: Ilow the simila.rity-cla.ss technique chooses top(c, v, pr)a.lld top(c, nq, pr) 
(?Zl, \])?', C) I Iypernyms of c 
( bid,for,(company) ) 
( ~i~io,,,,i,,,,<c~sh> ) 
(v, l", c) 
('l,ol, i,J!q, O J; <t tans act ion>) 
( clo.5",8, (t\[,, <def i n i t  e_quant  ity>) 
( , ,~ ,  wiU,,,<oeeioia~> ) 
< company> <establ ishment> (or ganisat  ion)<social _group> (GROUP) <root> 
(risk) <venture> (task)(+york> (activity)<act> (ROOT> 
( cash>( curt ency)(monetary mystem) (asset)(POSSESS I ON)(root ) 
<transact i on)<group_act ion) <act >(ROOT) 
<D RF II~ I TE_QUAN= TY> <mea~ur e> <abst tact ion> <root> 
<o~i~ial><adjudicator><perso~><li~e~orm><CAUSA~,~G~,NT><e~tity><root> 
'l'able <1: ( ,o~) le te  test set :~()97 test cases 
(~eneralisation technique % co:red. 
SiJnila.rity-cla.ss 80.3 
Select root of sub-hiera.rchy 77.9 
Alwa,ys select (root> 79.0 
Table 5: (root> 1)eing selected for 1)oth a.ttach- 
nlent 1)oints \] 713 test cases 
(hmeralisa+tion technique 
Simila.rity-cla.ss 
Select root of sub-hierarchy 
Ahva.ys select (root> 
% tort'cot 
90.3 
811.4 
79.6 
Abe (119!)8) who a.dol)t a similar a.l>proa(:h us- 
i:ng \VorclNet, but with a, differ<rot raining and 
test set. I,i a.nd Abe iml>rOVed on the l\[\]n- 
die and Rooth techni(lue l)y 1.5%, whh;h is i, 
line with our results. As a.n evahla.tion of the 
simibu'ity-class tec\]lnique, the result is incon- 
clusive. The rca.son for this is tha.t when the 
technique wa,s being used to estima.te \])( vlc,,, \])r ) 
a.Hd P(?~.:I \[c.,zl, I)?'), in many cases t i le root  o1" 1lie 
hiera.rchy wa.s being chosen as the apl>rOl)riat;e 
level of genera.lisa.tion, due to a. sparsely popu- 
la.ted WordNet in tha.t insta.nce. Recall that this 
is la.rgely due to tit<', fa.ct that we a.rc a.ttemltt- 
ing to popula.te WordNet fbr comltina.tions of 
predic~tes ~md prepositions. In such cases tile 
sinlil~u'ity-elass technique is not helping because 
there is very little or no informa.tion a.1)otlt ~,2. s
aln an effort to obtahl more do.to, we a, pplicd the ex- 
traction heuristic of lla.tna.parkhi (1998) to \?all Street 
Journa.l text, which increased the nuntl)er of training 
triples by ~L factor of 111. '\['his only a.chievcd comparable 
results, however, presumably boca.use the high volume 
of noise in the dat~ outweighs the benefit of the increase 
in da.ta size. \]{.atnaparkhi reports only 69% a.ccuracy tot 
Table 6: (root> being select(,(I for at most one 
o1' ill(, a.tl.a('hnmnt points 1032 i.cst ('asc.~ 
(l(,,eralisaCioll techniqu(,~ % ('ol:re('t 
Sitnilaril.y-cla.ss 88. I 
Select root of sul)-hierar(:hy 85.5 
Alwa.ys select ( root )  8,5.6 
In order to eva.lua.te the similarity-class tech- 
nique further, we took those test cases for which 
tile root wa, s not being selected when estima.ting 
bet:t, J,(,,I,,~. J,') .+,,d \])('/,.1 I~,,. pv). n:\],is .pplied 
to 113 c~ses. The results ~u;e given in Table 5. 
We a.lso took those test cases for which the root 
was I)eing selected when estimating +~t most one 
of p(v\[c+,,pr) a.nd p(,q \[c,~, pr). This a.pplie, d to 
\]032 test ca.sos. The results a.re shown in %> 
ble 6. 
the extraction heuristic when applied to the \]%nn Tree- 
ba.nk (excluding cases where the ln:eposition is of). 
199 
6 Conc lus ions  
We have shown that when instances of Word- 
Net are well populated with examples of 
n2, the method described here for solving 
P1)-attachment ambiguities is highly accurate. 
When WordNet is sparsely populated, the 
method automatically resorts to comparing just 
the preposition and each of the potential attach- 
ment sites, as the similarity-class technique will 
select {root} as the appropriate l vel of general- 
\]sat\]on for n2 in such cases. We have also shown 
the similarity-class technique to be superior to 
using a fixed level of general\]sat\]on in WordNet. 
Further work will look at how to integrate 
probabilities uch as p(clv, r) into a model of 
dependency structure, similar to that of Collins 
(1996) and Collins (1997), which can be used 
\['or parse selection. However, knowledge of se- 
\]ectional preferences cannot by itself solve the 
problem of structural disambiguation, and this 
further work will also look at using additional 
knowledge, such a.s subcategorisation informa- 
tion. 
Re ferences  
Eric Brill a.nd Philip Resnik. 1994. A rule-based 
approach to prel)ositional phrase a.tta.chment 
disanfl)iguation, in 1)~vcccdi'ngs of the .\[iJ- 
Icc~th International Co~@rcncc on C'ompu- 
rational Linguistics. 
Eugene (~harnia.k. 1993. ,5'tali.slical Language 
Lcarni'ng. The MIT Press. 
Stephen Clark and l)avid Weir. 1999. An it- 
erative approach to estimating frequencies 
over a semantic hierarchy. In P'lvcccdinqs of 
the Joint ,57GDA 2' ConJ~rcncc on Empirical 
Methods in Natural Language Proccssi'ng and 
Very Large Co17~ora , \])ages 258 265. 
Michael Collins. 11995. Prepositional phrase 
attachment through a backed-off model. In 
Proceedings of the Third l?orhshop on Very 
Large Cou)ora , pages 27-38, Cambridge, 
Massachusetts. 
Michael Collins. 1996. A new statistical parser 
based on bigram lexical dependencies. In 
P~vcecdings of the 3/tth Annual Meeting of the 
A CL, pages 184-1911. 
Michael Collins. 1997. Three generative, lexi- 
calised models for statistical parsing. In Pro- 
cccdings of the 351h A~mual Mccti'ng of the 
Association for Computational Linguistics, 
pages 16-23. 
Ted \])unning. 1993. Accurate luethods Ibr the 
statistics of surprise and coincidence. Com- 
putational Linguistics, 19(1):61-74. 
Christiane Fellbaum, editor. 1998. WordNct 
An l'2lcctronic Lcxical Database. The MIT 
Press. 
Donald ltindle and Mats Rooth. 1993. Struc- 
tural ambiguity and lexical relations. Com- 
putational Linguistics, \] 9(1): 103-120. 
l)avid Howell. 11997. Statistical Methods for 
Psychology: ~th cd. Duxbury Press. 
Hang Li and Naoki Abe. 11998. Genera.liz- 
ing case frames using a thesaurus and the 
MI)L principle. Computational Linguislics, 
24(2): 17-244. 
Adwait l{a.t:na.parldli, Jeff Reynar, and Saliln 
Roukos. 1994. A maximum entropy model 
fbr prepositional phrase attachment. In P,v- 
cccdings of l, hc A RI)A Human Language "l~ch- 
nology Workshop, pages 250-255. 
Adwait \]/.atnaparkhi. 1998. Unsupervised sta- 
tistical models tbr prepositional phrase at- 
tachment. In P~vcccdings of thc ,5'cvcnlccnth 
hzicrnalionol ConJE'rencc on Computational 
Linguistics, Montreal, Canada, Aug. 
Pllilip Rcsnik. 71993. ,5'clcction a~zd hdbrma- 
lion: A Class-Based Approach to l, czical Re- 
lationships. Ph.l). thesis, University of Penn- 
sylvania. 
Francesc l{ibas. 1995. On learning more appro- 
priate selectional restrictions. In Procccdings 
of the ,5'cvcnth ConJ~rcncc of the IJuropcan 
Chapter of the Association for Computational 
I,i~tguistics, l) U blin, Irelal, d. 
;liri Stetina and Makoto Na.gao. 1997. Corpus 
based PP attachment ambiguity resolution 
with a semantic dictionary. In Proceedings of 
thc Fiflh I?orteshop on Very Large Corpora, 
pages 66-80, Beijing and ltong Kong. 
David Yarowsky. 11992. Word-sense disam- 
biguation using statistical models of Roger's 
categories trained on large corpora. \]n P,v- 
cccdin.qs of COLING-92, pages 454-460. 
Jakub Zaw:el and Walter l)aelemans. 1997. 
Melnory-based learning: Using similarity for 
smoothing. In Proceedings of A CL/EACL- 
97, Madrid, Spain. 
200 
Coling 2010: Poster Volume, pages 1471?1479,
Beijing, August 2010
Chart Pruning for Fast Lexicalised-Grammar Parsing
Yue Zhanga? Byung-Gyu Ahn b? Stephen Clarka? Curt Van Wyk c
James R. Currand Laura Rimella
Computer Laboratorya Computer Scienceb Computer Sciencec School of ITd
Cambridge Johns Hopkins Northwestern College Sydney
{yue.zhang,stephen.clark}@cl.cam.ac.uka? bahn@jhu.edu b?
Abstract
Given the increasing need to process mas-
sive amounts of textual data, efficiency of
NLP tools is becoming a pressing concern.
Parsers based on lexicalised grammar for-
malisms, such as TAG and CCG, can be
made more efficient using supertagging,
which for CCG is so effective that every
derivation consistent with the supertagger
output can be stored in a packed chart.
However, wide-coverage CCG parsers still
produce a very large number of deriva-
tions for typical newspaper or Wikipedia
sentences. In this paper we investigate
two forms of chart pruning, and develop a
novel method for pruning complete cells
in a parse chart. The result is a wide-
coverage CCG parser that can process al-
most 100 sentences per second, with lit-
tle or no loss in accuracy over the baseline
with no pruning.
1 Introduction
Many NLP tasks and applications require the pro-
cessing of massive amounts of textual data. For
example, knowledge acquisition efforts can in-
volve processing billions of words of text (Cur-
ran, 2004). Also, the increasing need to process
large amounts of web data places an efficiency
demand on existing NLP tools. TextRunner, for
example, is a system that performs open infor-
mation extraction on the web (Lin et al, 2009).
However, the text processing that is performed by
TextRunner, in particular the parsing, is rudimen-
tary: finite-state shallow parsing technology that
is now decades old. TextRunner uses this technol-
ogy largely for efficiency reasons.
Many of the popular wide-coverage parsers
available today operate at around one newspa-
per sentence per second (Collins, 1999; Charniak,
2000; Petrov and Klein, 2007). There are de-
pendency parsers that operate orders of magni-
tude faster, by exploiting the fact that accurate
dependency parsing can be achieved by using a
shift-reduce linear-time process which makes a
single decision at each point in the parsing pro-
cess (Nivre and Scholz, 2004).
In this paper we focus on the Combinatory Cat-
egorial Grammar (CCG) parser of Clark and Cur-
ran (2007). One advantage of the CCG parser is
that it is able to assign rich structural descriptions
to sentences, from a variety of representations,
e.g. CCG derivations, CCG dependency structures,
grammatical relations (Carroll et al, 1998), and
first-order logical forms (Bos et al, 2004). One
of the properties of the grammar formalism is
that it is lexicalised, associating CCG lexical cate-
gories, or CCG supertags, with the words in a sen-
tence (Steedman, 2000). Clark and Curran (2004)
adapt the technique of supertagging (Bangalore
and Joshi, 1999) to CCG, using a standard max-
imum entropy tagger to assign small sets of su-
pertags to each word. The reduction in ambiguity
resulting from the supertagging stage results in a
surprisingly efficient parser, given the rich struc-
tural output, operating at tens of newspaper sen-
tences per second.
In this paper we demonstrate that the CCG
parser can be made more than twice as fast, with
little or no loss in accuracy. A noteworthy feature
of the CCG parser is that, after the supertagging
1471
stage, the parser builds a complete packed chart,
storing all sentences consistent with the assigned
supertags and the parser?s CCG combinatory rules,
with no chart pruning whatsoever. The use of
chart pruning techniques, typically some form of
beam search, is essential for practical parsing us-
ing Penn Treebank parsers (Collins, 1999; Petrov
and Klein, 2007; Charniak and Johnson, 2005), as
well as practical parsers based on linguistic for-
malisms, such as HPSG (Ninomiya et al, 2005)
and LFG (Kaplan et al, 2004). However, in the
CCG case, the use of the supertagger means that
enough ambiguity has already been resolved to al-
low the complete chart to be represented.
Despite the effectiveness of the supertagging
stage, the number of derivations stored in a packed
chart can still be enormous for typical newspa-
per sentences. Hence it is an obvious question
whether chart pruning techniques can be prof-
itably applied to the CCG parser. Some previous
work (Djordjevic et al, 2007) has investigated this
question but with little success.
In this paper we investigate two types of chart
pruning: a standard beam search, similar to that
used in the Collins parser (Collins, 1999), and a
more aggressive strategy in which complete cells
are pruned, following Roark and Hollingshead
(2009). Roark and Hollingshead use a finite-state
tagger to decide which words in a sentence can
end or begin constituents, from which whole cells
in the chart can be removed. We develop a novel
extension to this approach, in which a tagger is
trained to infer the maximum length constituent
that can begin or end at a particular word. These
lengths can then be used in a more agressive prun-
ing strategy which we show to be significantly
more effective than the basic approach.
Both beam search and cell pruning are highly
effective, with the resulting CCG parser able to
process almost 100 sentences per second using
a single CPU, for both newspaper and Wikipedia
data, with little or no loss in accuracy.
2 The CCG Parser
The parser is described in detail in Clark and Cur-
ran (2007). It is based on CCGbank, a CCG ver-
sion of the Penn Treebank developed by Hocken-
maier and Steedman (2007).
The stages in the parsing pipeline are as fol-
lows. First, a POS tagger assigns a single POS tag
to each word in a sentence. Second, a CCG su-
pertagger assigns lexical categories to the words
in the sentence. Third, the parsing stage combines
the categories, using CCG?s combinatory rules,
and builds a packed chart representation contain-
ing all the derivations which can be built from
the lexical categories. Finally, the Viterbi algo-
rithm finds the highest scoring derivation from
the packed chart, using the normal-form log-linear
model described in Clark and Curran (2007).
Sometimes the parser is unable to build an anal-
ysis which spans the whole sentence. When this
happens the parser and supertagger interact us-
ing the adaptive supertagging strategy described
in Clark and Curran (2004): the parser effectively
asks the supertagger to provide more lexical cate-
gories for each word. This potentially continues
for a number of iterations until the parser does
create a spanning analysis, or else it gives up and
moves to the next sentence.
The parser uses the CKY algorithm (Kasami,
1965; Younger, 1967) described in Steedman
(2000) to create a packed chart. The CKY al-
gorithm applies naturally to CCG since the gram-
mar is binary. It builds the chart bottom-up, start-
ing with two-word constituents (assuming the su-
pertagging phase has been completed), incremen-
tally increasing the span until the whole sentence
is covered. The chart is packed in the standard
sense that any two equivalent constituents created
during the parsing process are placed in the same
equivalence class, with pointers to the children
used in the creation. Equivalence is defined in
terms of the category and head of the constituent,
to enable the Viterbi algorithm to efficiently find
the highest scoring derivation.1 A textbook treat-
ment of CKY applied to statistical parsing is given
in Jurafsky and Martin (2000).
3 Data and Evaluation Metrics
We performed efficiency and accuracy tests on
newspaper and Wikipedia data. For the newspa-
per data, we used the standard test sections from
1Use of the Viterbi algorithm in this way requires the fea-
tures in the parser model to be local to a single rule applica-
tion; Clark and Curran (2007) has more discussion.
1472
(ncmod num hundred 1 Seven 0)
(conj and 2 sixty-one 3)
(conj and 2 hundred 1)
(dobj in 6 total 7)
(ncmod made 5 in 6)
(aux made 5 were 4)
(ncsubj made 5 and 2 obj)
(passive made 5)
Seven hundred and sixty-one were made in
total.
Figure 1: Example Wikipedia test sentence anno-
tated with grammatical relations.
CCGbank. Following Clark and Curran (2007) we
used the CCG dependencies for accuracy evalua-
tion, comparing those output by the parser with
the gold-standard dependencies in CCGbank. Un-
like Clark and Curran, we calculated recall scores
over all sentences, including those for which the
parser did not find an analysis. For the WSJ data
the parser fails on a small number of sentences
(less than 1%), but the chart pruning has the effect
of reducing this failure rate further, and we felt
that this should be factored into the calculation of
recall and hence F-score.
In order to test the parser on Wikipedia text,
we created two test sets. The first, Wiki 300, for
testing accuracy, consists of 300 sentences man-
ually annotated with grammatical relations (GRs)
in the style of Briscoe and Carroll (2006). An
example sentence is given in Figure 1. The data
was created by manually correcting the output of
the parser on these sentences, with the annotation
being performed by Clark and Rimell, including
checks on a subset of these cases to ensure con-
sistency across the two annotators. For the ac-
curacy evaluation, we calculated precision, recall
and balanced F-measure over the GRs in the stan-
dard way.
For testing speed on Wikipedia, we used a cor-
pus of 2500 randomly chosen sentences, Wiki
2500. For all speed tests we measured the num-
ber of sentences per second, using a single CPU
and standard hardware.
4 Beam Search
The beam search approach used in our exper-
iments prunes all constituents in a cell having
scores below a multiple (?) of the score of the
? Speed Gain F-score Gain
Baseline 43.0 85.55
0.001 48.6 13% 85.82 0.27
0.002 54.2 26% 85.88 0.33
0.005 59.0 37% 85.73 0.18
0.01 66.7 55% 85.53 -0.02
Table 1: Accuracy and speed results using differ-
ent beam values ?.
? Speed Gain F-score Gain
Baseline 43.0 85.55
10 60.1 39% 85.55 0.00
20 70.6 64% 85.66 0.11
30 72.3 68% 85.65 0.10
40 76.4 77% 85.63 0.08
50 76.7 78% 85.62 0.07
60 74.5 73% 85.71 0.16
80 68.4 59% 85.71 0.16
100 62.0 44% 85.73 0.18
None 59.0 37% 85.73 0.18
Table 2: Accuracy and speed results for different
values of ? where ? = 0.005.
highest scoring constituent for that cell.2 The
scores for a constituent are calculated using the
same model used to find the highest scoring
derivation. We consider two scores: the Viterbi
score, which is the score of the highest scoring
sub-derivation for that constituent; and the inside
score, which is the sum over all sub-derviations
for that constituent. We investigated the follow-
ing: the trade-off between the aggressiveness of
the beam search and accuracy; the comparison be-
tween the Viterbi and inside scores; and whether
applying the beam to only certain cells in the chart
can improve performance.
Table 1 shows results on Section 00 of CCG-
bank, using the Viterbi score to prune. As ex-
pected, the parsing speed increases as the value
of ? increases, since more constituents are pruned
with a higher ? value. The pruning is effective,
with a ? value of 0.01 giving a 55% speed increase
with neglible loss in accuracy.3
2One restriction we apply in practice is that only con-
stituents resulting from the application of a CCG binary rule,
rather than a unary rule, are pruned.
3The small accuracy increase for some ? values could be
attributable to two factors: one, the parser may select a lower
1473
Speed F-score
Dataset Baseline Beam Gain Baseline Beam Gain
WSJ 00 43.0 76.4 77% 85.55 85.63 0.08
WSJ 02-21 53.4 99.4 86% 93.61 93.27 -0.34
WSJ 23 55.0 107.0 94% 87.12 86.90 -0.22
Wiki 300 35.5 80.3 126% 84.23 85.06 0.83
Wiki 2500 47.6 90.3 89%
Table 4: Beam search results on WSJ 00, 02-21, 23 and Wikipedia texts with ? = 0.005 and ? = 40.
? ? Speed F-score
Baseline 24.7 85.55
inside scores
0.01 37.7 85.52
0.001 25.3 85.79
0.005 10 33.4 85.54
0.005 20 39.5 85.64
0.005 50 42.9 85.58
Viterbi scores
0.01 38.1 85.53
0.001 28.2 85.82
0.005 10 33.6 85.55
0.005 20 39.4 85.66
0.005 50 43.1 85.62
Table 3: Comparison between using Viterbi scores
and inside scores as beam scores.
We also studied the effect of the beam search
at different levels of the chart. We applied a selec-
tive beam in which pruning is only applied to con-
stituents of length less than or equal to a threshold
?. For example, if ? = 20, pruning is applied only
to constituents spanning 20 words or less. The re-
sults are shown in Table 2. The selective beam
is also highly effective, showing speed gains over
the baseline (which does not use a beam) with no
loss in F-score. For a ? value of 50 the speed in-
crease is 78% with no loss in accuracy.
Note that for ? greater than 50, the speed re-
duces. We believe that this is due to the cost
of calculating the beam scores and the reduced
effectiveness of pruning for cells with longer
spans (since pruning shorter constituents early in
the chart-parsing process prevents the creation of
many larger, low-scoring constituents later).
Table 3 shows the comparison between the in-
scoring but more accurate derivation; and two, a possible in-
crease in recall, discussed in Section 3, can lead to a higher
F-score.
side and Viterbi scores. The results are similar,
with Viterbi marginally outperforming the inside
score in most cases. The interesting result from
these experiments is that the summing used in cal-
culating the inside score does not improve perfor-
mance over the max operator used by Viterbi.
Table 4 gives results on Wikipedia text, com-
pared with a number of sections from CCGbank.
(Sections 02-21 provide the training data for the
parser which explains the high accuracy results
on these sections.) Despite the fact that the prun-
ing model is derived from CCGbank and based on
WSJ text, the speed improvements for Wikipedia
were even greater than for WSJ text, with param-
eters ? = 0.005 and ? = 40 leading to almost a
doubling of speed on the Wiki 2500 set, with the
parser operating at 90 sentences per second.
5 Cell Pruning
Whole cells can be pruned from the chart by tag-
ging words in a sentence. Roark and Hollingshead
(2009) used a binary tagging approach to prune a
CFG CKY chart, where tags are assigned to input
words to indicate whether they can be the start or
end of multiple-word constituents. We adapt their
method to CCG chart pruning. We also show the
limitation of binary tagging, and propose a novel
tagging method which leads to increased speeds
and accuracies over the binary taggers.
5.1 Binary tagging
Following Roark and Hollingshead (2009), we as-
sign the binary begin and end tags separately us-
ing two independent taggers. Given the input
?We like playing cards together?, the pruning ef-
fects of each type of tag on the CKY chart are
shown in Figure 2. In this chart, rows repre-
1474
XWe like playing cards together
1 2 3 4 5
1
2
4
5
3
1 1 1 0 0
X X
X
We like playing cards together
1 2 3 4 5
1
2
4
5
3
0 0 0 1 1
Figure 2: The pruning effect of begin (top) and
end (bottom) tags; X indicates a removed cell.
sent consituent sizes and columns represent initial
words of constituents. No cell in the first row of
the chart is pruned, since these cells correspond
to single words, and are necessary for finding a
parse. The begin tag for the input word ?cards? is
0, which means that it cannot begin a multi-word
constituent. Therefore, no cell in column 4 can
contain any constituent. The pruning effect of a
binary begin tag is to cross out a column of chart
cells (ignoring the first row) when the tag value
is zero. Similarly, the end tag of the word ?play-
ing? is 0, which means that it cannot be the end
of a multi-word constituent. Consequently cell (2,
2), which contains constituents for ?like playing?,
and cell (1, 3), which contains constituents for
?We like playing?, must be empty. The pruning
effect of a binary end tag is to cross out a diagonal
of cells (ignoring the first row) when the tag value
is zero.
We use a maximum entropy trigram tagger
(Ratnaparkhi, 1996; Curran and Clark, 2003) to
Model Speed F-score
baseline 25.10 84.89
begin only 27.49 84.71
end only 30.33 84.56
both 33.90 84.60
oracle 33.60 85.67
Table 5: Accuracy and speed results for the binary
taggers on Section 00 of CCGbank.
assign the begin and end tags. Features based on
the words and POS in a 5-word window, plus the
two previously assigned tags, are extracted from
the trigram ending with the current tag and the
five-word window with the current word in the
middle. In our development experiments, both the
begin and the end taggers gave a per-word accu-
racy of around 96%, similar to the accuracy re-
ported in Roark and Hollingshead (2009).
Table 5 shows accuracy and speed results for
the binary taggers.4 Using begin or end tags alone,
the parser achieved speed increases with a small
loss in accuracy. When both begin and end tags
are applied, the parser achieved further speed in-
creases, with no loss in accuracy compared to the
end tag alone. Row ?oracle? shows what happens
using the perfect begin and end taggers, by using
gold-standard constituent information from CCG-
bank. The F-score is higher, since the parser is
being guided away from incorrect derivations, al-
though the speed is no higher than when using au-
tomatically assigned tags.
5.2 Level tagging
A binary tag cannot take effect when there is any
chart cell in the corresponding column or diagonal
that contains constituents. For example, the begin
tag for the word ?card? in Figure 3 cannot be 0 be-
cause ?card? begins a two-word constituent ?card
games?. Hence none of the cells in the column can
be pruned using the binary begin tag, even though
all the cells from the third row above are empty.
We propose what we call a level tagging approach
to address this problem.
Instead of taking a binary value that indicates
4The baseline differs slightly to the previous section be-
cause gold-standard POS tags were used for the beam-search
experiments.
1475
1 2 3 4 5
1
2
4
5
3
Playing card games is fun
Figure 3: The limitation of binary begin tags.
whether a whole column or diagonal of cells can
be pruned, a level tag (begin or end) takes an in-
teger value which indicates the row from which
a column or diagonal can be pruned in the up-
ward direction. For example, a level begin tag
with value 2 allows the column of chart cells for
the word ?card? in Figure 3 to be pruned from the
third row upwards. A level tag (begin or end) with
value 1 prunes the corresponding row or diago-
nal from the second row upwards; it has the same
pruning effect as a binary tag with value 0. For
convenience, value 0 for a level tag means that the
corresponding word can be the beginning or end
of any constituent, which is the same as a binary
tag value 1.
A comparison of the pruning effect of binary
and level tags for the sentence ?Playing card
games is fun? is shown in Figure 4. With a level
begin tag, more cells can be pruned from the col-
umn for ?card?. Therefore, level tags are poten-
tially more powerful for pruning.
We now need a method for assigning level tags
to words in a sentence. However, we cannot
achieve this with a straighforward classifier since
level tags are related; for example, a level tag (be-
gin or end) with value 2 implies level tags with
values 3 and above. We develop a novel method
for calculating the probability of a level tag for
a particular word. Our mechanism for calculat-
ing these probabilities uses what we call maxspan
tags, which can be assigned using a maximum en-
tropy tagger.
Maxspan tags take the same values as level tags.
However, the meanings of maxspan tags and level
X
XX
X
1 2 3 4 5
1
2
4
5
3
Playing card games is fun
X
XX
X
Playing card games is fun
1 2 3 4 5
1
2
4
5
3
Figure 4: The pruning effect of binary (top) and
level (bottom) tags.
tags are different. While a level tag indicates the
row from which a column or diagonal of cells is
pruned, a maxspan tag represents the size of the
largest constituent a word begins or ends. For ex-
ample, in Figure 3, the level end tag for the word
?games? has value 3, since the largest constituent
this words ends spans ?playing card games?.
We use the standard maximum entropy trigram
tagger for maxspan tagging, where features are
extracted from tag trigrams and surrounding five-
word windows, as for the binary taggers. Parse
trees can be turned directly into training data for
a maxspan tagger. Since the level tag set is fi-
nite, we a require a maximum value N that a level
tag can take. We experimented with N = 2 and
N = 4, which reflects the limited range of the
features used by the taggers.5
During decoding, the maxspan tagger uses the
forward-backward algorithm to compute the prob-
ability of maxspan tag values for each word in the
5Higher values of N did not lead to improvements during
development experiments.
1476
Model Speed F-score
baseline 25.10 84.89
binary 33.90 84.60
binary oracle 33.60 85.67
level N = 2 32.79 84.92
level N = 4 34.91 84.95
level N = 4 oracle 47.45 86.49
Table 6: Accuracy and speed results for the level
taggers on Section 00 of CCGbank.
input. Then for each word, the probability of its
level tag tl having value x is the sum of the prob-
abilities of its maxspan tm tag having values 1..x:
P (tl = x) =
x?
i=1
P (tm = i)
Maxspan tag values i from 1 to x represent dis-
joint events in which the largest constituent that
the corresponding word begins or ends has size i.
Summing the probabilities of these disjoint events
gives the probability that the largest constituent
the word begins or ends has a size between 1 and
x, inclusive. That is also the probability that all
the constituents the word begins or ends are in the
range of cells from rows 1 to row x in the corre-
sponding column or diagonal. And therefore that
is also the probability that the chart cells above
row x in the corresponding column or diagonal
do not contain any constituents, which means that
the column and diagonal can be pruned from row
x upward. Therefore, it is also the probability of a
level tag with value x.
The probability of a level tag having value x
increases as x increases from 1 to N . We set a
probability threshold Q and choose the smallest
level tag value x with probability P (tl = x) ? Q
as the level tag for a word. If P (tl = N) < Q, we
set the level tag to 0 and do not prune the column
or diagonal. The threshold value determines a bal-
ance between pruning power and accuracy, with a
higher value pruning more cells but increasing the
risk of incorrectly pruning a cell. During devel-
opment we arrived at a threshold value of 0.8 as
providing a suitable compromise between pruning
power and accuracy.
Table 6 shows accuracy and speed results for
the level tagger, using a threshold value of 0.8.
Model Speed F-score
baseline 36.64 84.23
binary gold 49.59 84.36
binary self 40K 48.79 83.64
binary self 200K 51.51 83.71
binary self 1M 47.78 83.75
level gold 58.23 84.12
level self 40K 54.76 83.83
level self 200K 48.57 83.39
level self 1M 52.54 83.71
Table 7: Accuracy tests on Wiki 300 comparing
gold training (gold) with self training (self) for
different sizes of parser output for self-training.
We compare the effect of the binary tagger and
level taggers with N = 2 and N = 4. The accu-
racies with the level taggers are higher than those
with the binary tagger; they are also higher than
the baseline parsing accuracy. The parser achieves
the highest speed and accuracy when pruned with
the N = 4 level tagger. Comparing the oracle
scores, the level taggers lead to higher speeds than
the binary tagger, reflecting the increased pruning
power of the level taggers compared with the bi-
nary taggers.
5.2.1 Final experiments using gold training
and self training
In this section we report our final tests using
Wikipedia data. We used two methods to derive
training data for the taggers. The first is the stan-
dard method, which is to transform gold-standard
parse trees into begin and end tag sequences. This
method is the method that we used for all previ-
ous experiments, and we call it ?gold training?.
In addition to gold training, we also investigate
an alternative method, which is to obtain training
data for the taggers from the output of the parser
itself, in a form of self-training (McClosky et al,
2006). The intuition is that the tagger will learn
what constituents a trained parser will eventually
choose, and as long as the constituents favoured
by the parsing model are not pruned, no reduction
in accuracy can occur. There is the potential for
an increase in speed, however, due to the pruning
effect.
For gold training, we used sections 02-21 of
1477
Model Speed
baseline 47.6
binary gold 80.8
binary 40K 75.5
binary 200K 77.4
binary 1M 78.6
level gold 93.7
level 40K 92.8
level 200K 92.5
level 1M 96.6
Table 8: Speed tests with gold and self-training on
Wiki 2500.
CCGBank (which consists of about 40K training
sentences) to derive training data. For self train-
ing, we trained the parser on sections 02-21 of
CCGBank, and used the parser to parse 40 thou-
sand, 200 thousand and 1 million sentences from
Wikipedia, respectively. Then we derive three sets
of self training data from the three sets of parser
outputs. We then used our Wiki 300 set to test the
accuracy, and the Wiki 2500 set to test the speed
of the parser.
The results are shown in Tables 7 and 8, where
each row represents a training data set. Rows ?bi-
nary gold? and ?level gold? represent binary and
level taggers trained using gold training. Rows
?binary self X? and ?level self X? represent bi-
nary and level taggers trained using self training,
with the size of the training data being X sen-
tences.
It can be seen from the Tables that the accuracy
loss with self-trained binary or level taggers was
not large (in the worst case, the accuracy dropped
from 84.23% to 83.39%), while the speed was
significantly improved. Using binary taggers, the
largest speed improvement was from 47.6 sen-
tences per second to 80.8 sentences per second
(a 69.7% relative increase). Using level taggers,
the largest speed improvement was from 47.6 sen-
tences per second to 96.6 sentences per second (a
103% relative increase).
A potential advantage of self-training is the
availability of large amounts of training data.
However, our results are somewhat negative in
this regard, in that we find training the tagger on
more than 40,000 parsed sentences (the size of
CCGbank) did not improve the self-training re-
sults. We did see the usual speed improvements
from using the self-trained taggers, however, over
the baseline parser with no pruning.
6 Conclusion
Using our novel method of level tagging for prun-
ing complete cells in a CKY chart, the CCG parser
was able to process almost 100 Wikipedia sen-
tences per second, using both CCGbank and the
output of the parser to train the taggers, with little
or no loss in accuracy. This was a 103% increase
over the baseline with no pruning.
We also demonstrated that standard beam
search is highly effective in increasing the speed
of the CCG parser, despite the fact that the su-
pertagger has already had a significant pruning
effect. In future work we plan to investigate the
gains that can be achieved from combining the
two pruning methods, as well as other pruning
methods such as the self-training technique de-
scribed in Kummerfeld et al (2010) which re-
duces the number of lexical categories assigned
by the supertagger (leading to a speed increase).
Since these methods are largely orthogonal, we
expect to achieve further gains, leading to a re-
markably fast wide-coverage parser outputting
complex linguistic representations.
Acknowledgements
This work was largely carried out at the Johns
Hopkins University Summer Workshop and (par-
tially) supported by National Science Founda-
tion Grant Number IIS-0833652. Yue Zhang and
Stephen Clark are supported by the European
Union Seventh Framework Programme (FP7-ICT-
2009-4) under grant agreement no. 247762.
References
Bangalore, Srinivas and Aravind Joshi. 1999. Su-
pertagging: An approach to almost parsing. Com-
putational Linguistics, 25(2):237?265.
Bos, Johan, Stephen Clark, Mark Steedman, James R.
Curran, and Julia Hockenmaier. 2004. Wide-
coverage semantic representations from a CCG
parser. In Proceedings of COLING-04, pages 1240?
1246, Geneva, Switzerland.
1478
Briscoe, Ted and John Carroll. 2006. Evaluating
the accuracy of an unlexicalized statistical parser on
the PARC DepBank. In Proceedings of the Poster
Session of COLING/ACL-06, pages 41?48, Sydney,
Australia.
Carroll, John, Ted Briscoe, and Antonio Sanfilippo.
1998. Parser evaluation: a survey and a new pro-
posal. In Proceedings of the 1st LREC Conference,
pages 447?454, Granada, Spain.
Charniak, Eugene and Mark Johnson. 2005. Coarse-
to-fine N-best parsing and maxent discriminative
reranking. In Proceedings of the 43rd Meeting of
the ACL, pages 173?180, Michigan, Ann Arbor.
Charniak, Eugene. 2000. A maximum-entropy-
inspired parser. In Proceedings of the 1st Meeting
of the NAACL, pages 132?139, Seattle, WA.
Clark, Stephen and James R. Curran. 2004. The im-
portance of supertagging for wide-coverage CCG
parsing. In Proceedings of COLING-04, pages 282?
288, Geneva, Switzerland.
Clark, Stephen and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG
and log-linear models. Computational Linguistics,
33(4):493?552.
Collins, Michael. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania.
Curran, James R. and Stephen Clark. 2003. Inves-
tigating GIS and smoothing for maximum entropy
taggers. In Proceedings of the 10th Meeting of the
EACL, pages 91?98, Budapest, Hungary.
Curran, James R. 2004. From Distributional to Se-
mantic Similarity. Ph.D. thesis, University of Edin-
burgh.
Djordjevic, Bojan, James R. Curran, and Stephen
Clark. 2007. Improving the efficiency of a wide-
coverage CCG parser. In Proceedings of IWPT-07,
pages 39?47, Prague, Czech Republic.
Hockenmaier, Julia and Mark Steedman. 2007. CCG-
bank: a corpus of CCG derivations and dependency
structures extracted from the Penn Treebank. Com-
putational Linguistics, 33(3):355?396.
Jurafsky, Daniel and James H. Martin. 2000. Speech
and Language Processing: An Introduction to Nat-
ural Language Processing, Computational Linguis-
tics and Speech Recognition. Prentice Hall, New
Jersey.
Kaplan, Ron, Stefan Riezler, Tracy H. King, John
T. Maxwell III, Alexander Vasserman, and Richard
Crouch. 2004. Speed and accuracy in shallow and
deep stochastic parsing. In Proceedings of HLT-
NAACL?04, Boston, MA.
Kummerfeld, Jonathan K., Jessika Roesner, Tim
Dawborn, James Haggerty, James R. Curran, and
Stephen Clark. 2010. Faster parsing by supertag-
ger adaptation. In Proceedings of ACL-10, Uppsala,
Sweden.
Lin, Thomas, Oren Etzioni, and James Fogarty. 2009.
Identifying interesting assertions from the web. In
Proceedings of the 18th Conference on Information
and Knowledge Management (CIKM 2009), Hong
Kong.
McClosky, David, Eugene Charniak, and Mark John-
son. 2006. Effective self-training for parsing. In
Proceedings of NAACL-06, pages 152?159, Brook-
lyn, NY.
Ninomiya, Takashi, Yoshimasa Tsuruoka, Yusuke
Miyao, and Jun?ichi Tsujii. 2005. Efficacy of beam
thresholding, unification filtering and hybrid pars-
ing in probabilistic HPSG parsing. In Proceedings
of IWPT-05, pages 103?114, Vancouver, Canada.
Nivre, J. and M. Scholz. 2004. Deterministic depen-
dency parsing of English text. In Proceedings of
COLING-04, pages 64?70, Geneva, Switzerland.
Petrov, Slav and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In Proceedings of
the HLT/NAACL conference, Rochester, NY.
Ratnaparkhi, Adwait. 1996. A maximum entropy
model for part-of-speech tagging. In Proceedings
of EMNLP-96, pages 133?142, Somerset, New Jer-
sey.
Roark, Brian and Kristy Hollingshead. 2009. Lin-
ear complexity context-free parsing pipelines via
chart constraints. In Proceedings of HLT/NAACL-
09, pages 647?655, Boulder, Colorado.
Steedman, Mark. 2000. The Syntactic Process. The
MIT Press, Cambridge, MA.
1479
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 843?852,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
A Fast Decoder for Joint Word Segmentation and POS-Tagging Using a
Single Discriminative Model
Yue Zhang and Stephen Clark
University of Cambridge Computer Laboratory
William Gates Building,
15 JJ Thomson Avenue,
Cambridge CB3 0FD, UK
{yue.zhang, stephen.clark}@cl.cam.ac.uk
Abstract
We show that the standard beam-search al-
gorithm can be used as an efficient decoder
for the global linear model of Zhang and
Clark (2008) for joint word segmentation and
POS-tagging, achieving a significant speed im-
provement. Such decoding is enabled by:
(1) separating full word features from par-
tial word features so that feature templates
can be instantiated incrementally, according to
whether the current character is separated or
appended; (2) deciding the POS-tag of a poten-
tial word when its first character is processed.
Early-update is used with perceptron training
so that the linear model gives a high score to a
correct partial candidate as well as a full out-
put. Effective scoring of partial structures al-
lows the decoder to give high accuracy with a
small beam-size of 16. In our 10-fold cross-
validation experiments with the Chinese Tree-
bank, our system performed over 10 times as
fast as Zhang and Clark (2008) with little ac-
curacy loss. The accuracy of our system on
the standard CTB 5 test was competitive with
the best in the literature.
1 Introduction and Motivation
Several approaches have been proposed to solve
word segmentation and POS-tagging jointly, includ-
ing the reranking approach (Shi and Wang, 2007;
Jiang et al, 2008b), the hybrid approach (Nakagawa
and Uchimoto, 2007; Jiang et al, 2008a), and the
single-model approach (Ng and Low, 2004; Zhang
and Clark, 2008; Kruengkrai et al, 2009). These
methods led to accuracy improvements over the tra-
ditional, pipelined segmentation and POS-tagging
baseline by avoiding segmentation error propagation
and making use of part-of-speech information to im-
prove segmentation.
The single-model approach to joint segmentation
and POS-tagging offers consistent training of all in-
formation, concerning words, characters and parts-
of-speech. However, exact inference with dynamic
programming can be infeasible if features are de-
fined over a large enough range of the output, such
as over a two-word history. In our previous work
(Zhang and Clark, 2008), which we refer to as
Z&C08 from now on, we used an approximate de-
coding algorithm that keeps track of a set of partially
built structures for each character, which can be seen
as a dynamic programming chart which is greatly re-
duced by pruning.
In this paper we follow the line of single-model
research, in particular the global linear model of
Z&C08. We show that effective decoding can be
achieved with standard beam-search, which gives
significant speed improvements compared to the de-
coding algorithm of Z&C08, and achieves accura-
cies that are competitive with the state-of-the-art.
Our research is also in line with recent research on
improving the speed of NLP systems with little or
no accuracy loss (Charniak et al, 2006; Roark and
Hollingshead, 2008).
Our speed improvement is achieved by the use
of a single-beam decoder. Given an input sentence,
candidate outputs are built incrementally, one char-
acter at a time. When each character is processed,
it is combined with existing candidates in all possi-
ble ways to generate new candidates, and an agenda
is used to keep the N -best candidate outputs from
843
the begining of the sentence to the current character.
Compared to the multiple-beam search algorithm of
Z&C08, the use of a single beam can lead to an order
of magnitude faster decoding speed.
1.1 The processing of partial words
An important problem that we solve in this paper
is the handling of partial words with a single beam
decoder for the global model. As we pointed out
in Z&C08, it is very difficult to score partial words
properly when they are compared with full words,
although such comparison is necessary for incre-
mental decoding with a single-beam. To allow com-
parisons with full words, partial words can either be
treated as full words, or handled differently.
We showed in Z&C08 that a naive single-beam
decoder which treats partial words in the same way
as full words failed to give a competitive accu-
racy. An important reason for the low accuracy is
over-segmentation during beam-search. Consider
the three characters ???? (tap water)?. The first
two characters do not make sense when put together
as a single word. Rather, when treated as two single-
character words, they can make sense in a sentence
such as ?? (please)? (self)? (come)? (take)?.
Therefore, when using single-beam search to pro-
cess ???? (tap water)?, the two-character word
candidate ???? is likely to have been thrown off
the agenda before the third character ??? is con-
sidered, leading to an unrecoverable segmentation
error.
This problem is even more severe for a joint seg-
mentor and POS-tagger than for a pure word seg-
mentor, since the POS-tags and POS-tag bigram of
??? and ??? further supports them being separated
when ??? is considered. The multiple-beam search
decoder we proposed in Z&C08 can be seen as a
means to ensure that the three characters ?????
always have a chance to be considered as a single
word. It explores candidate segmentations from the
beginning of the sentence until each character, and
avoids the problem of processing partial words by
considering only full words. However, since it ex-
plores a larger part of the search space than a single-
beam decoder, its time complexity is correspond-
ingly higher.
In this paper, we treat partial words differently
from full words, so that in the previous example,
the decoder can take the first two characters in ??
?? (tap water)? as a partial word, and keep it
in the beam before the third character is processed.
One challenge is the representation of POS-tags for
partial words. The POS of a partial word is unde-
fined without the corresponding full word informa-
tion. Though a partial word can make sense with
a particular POS-tag when it is treated as a com-
plete word, this POS-tag is not necessarily the POS of
the full word which contains the partial word. Take
the three-character sequence ????? as an exam-
ple. The first character ??? represents a single-
character word ?below?, for which the POS can be
LC or VV. The first two characters ???? repre-
sent a two-character word ?rain?, for which the POS
can be VV. Moreover, all three characters when put
together make the word ?rainy day?, for which the
POS is NN. As discussed above, assigning POS tags
to partial words as if they were full words leads to
low accuracy.
An obvious solution to the above problem is not to
assign a POS to a partial word until it becomes a full
word. However, lack of POS information for partial
words makes them less competitive compared to full
words in the beam, since the scores of full words are
futher supported by POS and POS ngram informa-
tion. Therefore, not assigning POS to partial words
potentially leads to over segmentation. In our exper-
iments, this method did not give comparable accura-
cies to our Z&C08 system.
In this paper, we take a different approach, and
assign a POS-tag to a partial word when its first char-
acter is separated from the final character of the pre-
vious word. When more characters are appended to
a partial word, the POS is not changed. The idea is
to use the POS of a partial word as the predicted POS
of the full word it will become. Possible predictions
are made with the first character of the word, and the
likely ones will be kept in the beam for the next pro-
cessing steps. For example, with the three characters
?????, we try to keep two partial words (besides
full words) in the beam when the first word ??? is
processed, with the POS being VV and NN, respec-
tively. The first POS predicts the two-character word
?????and the second the three-character word
?????. Now when the second character is pro-
cessed, we still need to maintain the possible POS
NN in the agenda, which predicts the three-character
844
word ?????.
As a main contribution of this paper, we show that
the mechanism of predicting the POS at the first char-
acter gives competitive accuracy. This mechanism
can be justified theoretically. Unlike alphabetical
languages, each Chinese character represents some
specific meanings. Given a character, it is natural for
a human speaker to know immediately what types
of words it can start. The allows the knowledge of
possible POS-tags of words that a character can start,
using information about the character from the train-
ing data. Moreover, the POS of the previous words to
the current word are also useful in deciding possible
POS for the word.1
The mechanism of first-character decision of POS
also boosts the efficiency, since the enumeration of
POS is unecessary when a character is appended to
the end of an existing word. As a result, the com-
plexity of each processing step is reduce by half
compared to a method without POS prediction.
Finally, an intuitive way to represent the status of
a partial word is using a flag explicitly, which means
an early decision of the segmentation of the next in-
coming character. We take a simpler alternative ap-
proach, and treat every word as a partial word un-
til the next incoming character is separated from the
last character of this word. Before a word is con-
firmed as a full word, we only apply to it features
that represent its current partial status, such as char-
acter bigrams, its starting character and its part-of-
speech, etc. Full word features, including the first
and last characters of a word, are applied immedi-
ately after a word is confirmed as complete.
An important component for our proposed system
is the training process, which needs to ensure that
the model scores a partial word with predicted POS
properly. We use the averaged perceptron (Collins,
2002) for training, together with the ?early update?
mechanism of Collins and Roark (2004). Rather
than updating the parameters after decoding is com-
plete, the modified algorithm updates parameters at
any processing step if the correct partial candidate
falls out of the beam.
In our experiments using the Chinese Treebank
1The next incoming characters are also a useful source
of information for predicting the POS. However, our system
achieved competitive accuracy with Z&C08 without such char-
acter lookahead features.
data, our system ran an order of magnitude faster
than our Z&C08 system with little loss of accuracy.
The accuracy of our system was competitive with
other recent models.
2 Model and Feature Templates
We use a linear model to score both partial and full
candidate outputs. Given an input x, the score of a
candidate output y is computed as:
Score(y) = ?(y) ? ~w,
where ?(y) is the global feature vector extracted
from y, and ~w is the parameter vector of the model.
Figure 1 shows the feature templates for the
model, where templates 1 ? 14 contain only seg-
mentation information and templates 15 ? 29 contain
both segmentation and POS information. Each tem-
plate is instantiated according to the current charac-
ter in the decoding process. Row ?For? shows the
conditions for template instantiation, where ?s? in-
dicates that the corresponding template is instanti-
ated when the current character starts a new word,
and ?a? indicates that the corresponding template is
instantiated when the current character does not start
a new word. In the row for feature templates, w, t
and c are used to represent a word, a POS-tag and
a character, respectively. The subscripts are based
on the current character, where w?1 represents the
first word to the left of the current character, and
p?2 represents the POS-tag on the second word to
the left of the current character, and so on. As an
example, feature template 1 is instantiated when the
current character starts a new word, and the resulting
feature value is the word to the left of this charac-
ter. start(w), end(w) and len(w) represent the first
character, the last character and the length of word
w, respectively. The length of a word is normalized
to 16 if it is larger than 16. cat(c) represents the POS
category of character c, which is the set of POS-tags
seen on character c, as we used in Z&C08.
Given a partial or complete candidate y, its global
feature vector ?(y) is computed by instantiating all
applicable feature templates from Table 1 for each
character in y, according to whether or not the char-
acter is separated from the previous character.
The feature templates are mostly taken from, or
inspired by, the feature templates of Z&C08. Tem-
plates 1, 2, 3, 4, 5, 8, 10, 12, 13, 14, 15, 19, 20,
845
Feature template For
1 w?1 s
2 w?1w?2 s
3 w?1, where len(w?1) = 1 s
4 start(w?1)len(w?1) s
5 end(w?1)len(w?1) s
6 end(w?1)c0 s
7 c?1c0 a
8 begin(w?1)end(w?1) s
9 w?1c0 s
10 end(w?2)w?1 s
11 start(w?1)c0 s
12 end(w?2)end(w?1) s
13 w?2len(w?1) s
14 len(w?2)w?1 s
15 w?1t?1 s
16 t?1t0 s
17 t?2t?1t0 s
18 w?1t0 s
19 t?2w?1 s
20 w?1t?1end(w?2) s
21 w?1t?1c0 s
22 c?2c?1c0t?1, s
where len(w?1) = 1
23 start(w0)t0 s
24 t?1start(w?1) s
25 t0c0 s, a
26 c0t0start(w0) a
27 ct?1end(w?1), s
where c ? w?1 and c 6= end(w?1)
28 c0t0cat(start(w0)) s
29 ct?1cat(end(w?1)), s
where c ? w?1 and c 6= end(w?1)
30 c0t0c?1t?1 s
31 c0t0c?1 a
Table 1: Feature templates.
24, 27 and 29 concern complete word information,
and they are used in the model to differentiate cor-
rect and incorrect output structures in the same way
as our Z&C08 model. Templates 6, 7, 9, 16, 17,
18, 21, 22, 23, 25, 26 and 28 concern partial word
information, whose role in the model is to indicate
the likelihood that the partial word including the cur-
rent character will become a correct full word. They
act as guidance for the action to take for the cur-
function DECODE(sent, agenda):
CLEAR(agenda)
ADDITEM(agenda, ??)
for index in [0..LEN(sent)]:
for cand in agenda:
new? APPEND(cand, sent[index])
ADDITEM(agenda, new)
for pos in TAGSET():
new? SEP(cand, sent[index], pos)
ADDITEM(agenda, new)
agenda? N-BEST(agenda)
return BEST(agenda)
Figure 1: The incremental beam-search decoder.
rent character according to the context, and are the
crucial reason for the effectiveness of the algorithm
with a small beam-size.
2.1 Decoding
The decoding algorithm builds an output candidate
incrementally, one character at a time. Each char-
acter can either be attached to the current word or
separated as the start a new word. When the current
character starts a new word, a POS-tag is assigned to
the new word. An agenda is used by the decoder to
keep the N -best candidates during the incremental
process. Before decoding starts, the agenda is ini-
tialized with an empty sentence. When a character is
processed, existing candidates are removed from the
agenda and extended with the current character in all
possible ways, and the N -best newly generated can-
didates are put back onto the agenda. After all input
characters have been processed, the highest-scored
candidate from the agenda is taken as the output.
Pseudo code for the decoder is shown in Figure
1. CLEAR removes all items from the agenda, AD-
DITEM adds a new item onto the agenda, N-BEST
returns the N highest-scored items from the agenda,
and BEST returns the highest-scored item from the
agenda. LEN returns the number of characters in a
sentence, and sent[i] returns the ith character from
the sentence. APPEND appends a character to the
last word in a candidate, and SEP joins a character
as the start of a new word in a candidate, assigning
a POS-tag to the new word.
846
Both our decoding algorithm and the decoding al-
gorithm of Z&C08 run in linear time. However, in
order to generate possible candidates for each char-
acter, Z&C08 uses an extra loop to search for pos-
sible words that end with the current character. A
restriction to the maximum word length is applied
to limit the number of iterations in this loop, with-
out which the algorithm would have quadratic time
complexity. In contrast, our decoder does not search
backword for the possible starting character of any
word. Segmentation ambiguities are resolved by bi-
nary choices between the actions append or sepa-
rate for each character, and no POS enumeration is
required when the character is appended. This im-
proves the speed by a significant factor.
2.2 Training
The learning algorithm is based on the generalized
perceptron (Collins, 2002), but parameter adjust-
ments can be performed at any character during the
decoding process, using the ?early update? mecha-
nism of Collins and Roark (2004).
The parameter vector of the model is initialized as
all zeros before training, and used to decode training
examples. Each training example is turned into the
raw input format, and processed in the same way as
decoding. After each character is processed, partial
candidates in the agenda are compared to the cor-
responding gold-standard output for the same char-
acters. If none of the candidates in the agenda are
correct, the decoding is stopped and the parameter
vector is updated by adding the global feature vector
of the gold-standard partial output and subtracting
the global feature vector of the highest-scored par-
tial candidate in the agenda. The training process
then moves on to the next example. However, if any
item in the agenda is the same as the correspond-
ing gold-standard, the decoding process moves to
the next character, without any change to the pa-
rameter values. After all characters are processed,
the decoder prediction is compared with the training
example. If the prediction is correct, the parame-
ter vector is not changed; otherwise it is updated by
adding the global feature vector of the training ex-
ample and subtracting the global feature vector of
the decoder prediction, just as the perceptron algo-
rithm does. The same training examples can be used
to train the model for multiple iterations. We use
the averaged parameter vector (Collins, 2002) as the
final model.
Pseudocode for the training algorithm is shown in
Figure 2. It is based on the decoding algorithm in
Figure 1, and the main differences are: (1) the train-
ing algorithm takes the gold-standard output and the
parameter vector as two additional arguments; (2)
the training algorithm does not return a prediction,
but modifies the parameter vector when necessary;
(3) lines 11 to 20 are additional lines of code for pa-
rameter updates.
Without lines 11 to 16, the training algorithm is
exactly the same as the generalized perceptron al-
gorithm. These lines are added to ensure that the
agenda contains highly probable candidates during
the whole beam-search process, and they are crucial
to the high accuracy of the system. As stated earlier,
the decoder relies on proper scoring of partial words
to maintain a set of high quality candidates in the
agenda. Updating the value of the parameter vector
for partial outputs can be seen as a means to ensure
correct scoring of partial candidates at any character.
2.3 Pruning
We follow Z&C08 and use several pruning methods,
most of which serve to to improve the accuracy by
removing irrelevant candidates from the beam. First,
the system records the maximum number of charac-
ters that a word with a particular POS-tag can have.
For example, from the Chinese Treebank that we
used for our experiments, most POS are associated
with only with one- or two-character words. The
only POS-tags that are seen with words over ten char-
acters long are NN (noun), NR (proper noun) and
CD (numbers). The maximum word length informa-
tion is initialized as all ones, and updated according
to each training example before it is processed.
Second, a tag dictionary is used to record POS-
tags associated with each word. During decoding,
frequent words and words with ?closed set? tags2
are only allowed POS-tags according to the tag dic-
tionary, while other words are allowed every POS-tag
to make candidate outputs. Whether a word is a fre-
quent word is decided by the number of times it has
been seen in the training process. Denoting the num-
2
?Closed set? tags are the set of POS-tags which are only
associated with a fixed set of words, according to the Penn Chi-
nese Treebank specifications (Xia, 2000).
847
function TRAIN(sent, agenda, gold-standard, ~w):
01: CLEAR(agenda)
02: ADDITEM(agenda, ??)
03: for index in [0..LEN(sent)]:
04: for cand in agenda:
05: new? APPEND(cand, sent[index])
06: ADDITEM(agenda, new)
07: for pos in TAGSET():
08: new? SEP(cand, sent[index], pos)
09: ADDITEM(agenda, new)
10: agenda? N-BEST(agenda)
11: for cand in agenda:
12: if cand = gold-standard[0:index]:
13: CONTINUE
14: ~w? ~w + ?(gold-standard[0:index])
15: ~w? ~w - ?(BEST(agenda))
16: return
17: if BEST(agenda) 6= gold-standard:
18: ~w? ~w + ?(gold-standard)
19: ~w? ~w - ?(BEST(agenda))
20: return
21: return
Figure 2: The incremental learning function.
ber of times the most frequent word has been seen
with M , a word is a frequent word if it has been
seen more than M/5000 + 5 times. The threshold
value is taken from Z&C08, and we did not adjust
it during development. Word frequencies are initial-
ized as zeros and updated according to each training
example before it is processed; the tag dictionary is
initialized as empty and updated according to each
training example before it is processed.
Third, we make an additional record of the initial
characters for words with ?closed set? tags. During
decoding, when the current character is added as the
start of a new word, ?closed set? tags are only as-
signed to the word if it is consistent with the record.
This type of pruning is used in addition to the tag
dictionary to prune invalid partial words, while the
tag dictionary is used to prune complete words. The
record for initial character and POS is initially empty,
and udpated according to each training example be-
fore it is processed.
Finally, at any decoding step, we group partial
candidates that are generated by separating the cur-
rent character as the start of a new word by the sig-
nature p0p?1w?1, and keep only the best among
those having the same p0p?1w?1. The signature
p0p?1w?1 is decided by the feature templates we
use: it can be shown that if two candidates cand1
and cand2 generated at the same step have the same
signature, and the score of cand1 is higher than the
score of cand2, then at any future step, the highest
scored candidate generated from cand1 will always
have a higher score than the highest scored candidate
generated from cand2.
From the above pruning methods, only the third
was not used by Z&C08. It can be seen as an extra
mechanism to help keep likely partial words in the
agenda and improve the accuracy, but which does
not give our system a speed advantage over Z&C08.
3 Experiments
We used the Chinese Treebank (CTB) data to per-
form one set of development tests and two sets of fi-
848
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 5  10  15  20  25  30
F-
m
ea
su
re
Training iteration
beam=1
beam=2
beam=4
beam=8
beam=16
beam=32
Figure 3: The influence of beam-sizes, and the conver-
gence of the perceptron.
nal tests. The CTB 4 was split into two parts, with the
CTB 3 being used for a 10-fold cross validation test
to compare speed and accuracies with Z&C08, and
the rest being used for development. The CTB 5 was
used to perform the additional set of experiments to
compare accuracies with other recent work.
We use the standard F-measure to evaluate output
accuracies. For word segmentation, precision is de-
fined as the number of correctly segmented words
divided by the total number of words in the output,
and recall is defined as the number of correctly seg-
mented words divided by the total number of words
in the gold-standard output. For joint segmentation
and POS-tagging, precision is defined as the num-
ber of correctly segmented and POS-tagged words
divided by the total number of words from the out-
put, and recall is defined as the correctly segmented
and POS-tagged words divided by the total number
of words in the gold-standard output.
All our experiments were performed on a Linux
platform, and a single 2.66GHz Intel Core 2 CPU.
3.1 Development tests
Our development data consists of 150K words in
4798 sentences. 80% of the data were randomly
chosen as the development training data, while the
rest were used as the development test data. Our de-
velopment tests were mainly used to decide the size
of the beam, the number of training iterations, the ef-
fect of partial features in beam-search decoding, and
the effect of incremental learning (i.e. early update).
Figure 3 shows the accuracy curves for joint seg-
mentation and POS-tagging by the number of train-
ing iterations, using different beam sizes. With the
size of the beam increasing from 1 to 32, the accura-
cies generally increase, while the amount of increase
becomes small when the size of the beam becomes
16. After the 10th iteration, a beam size of 32 does
not always give better accuracies than a beam size
of 16. We therefore chose 16 as the size of the beam
for our system.
The testing times for each beam size between 1
and 32 are 7.16s, 11.90s, 18.42s, 27.82s, 46.77s
and 89.21s, respectively. The corresponding speeds
in the number of sentences per second are 111.45,
67.06, 43.32, 28.68, 17.06 and 8.95, respectively.
Figure 3 also shows that the accuracy increases
with an increased number of training iterations, but
the amount of increase becomes small after the 25th
iteration. We chose 29 as the number of iterations to
train our system.
The effect of incremental training: We compare
the accuracies by incremental training using early
update and normal perceptron training. In the nor-
mal perceptron training case, lines 11 to 16 are taken
out of the training algorithm in Figure 2. The algo-
rithm reached the best performance at the 22nd iter-
ation, with the segmentation F-score being 90.58%
and joint F-score being 83.38%. In the incremental
training case, the algorithm reached the best accu-
racy at the 30th training iteration, obtaining a seg-
mentation F-score of 91.14% and a joint F-score of
84.06%.
3.2 Final tests using CTB 3
CTB 3 consists of 150K words in 10364 sentences.
We follow Z&C08 and split it into 10 equal-sized
parts. In each test, one part is taken as the test
data and the other nine are combined together as
the training data. We compare the speed and accu-
racy with the joint segmentor and tagger of Z&C08,
which is publicly available as the ZPar system, ver-
sion 0.23.
The results are shown in Table 2, where each row
shows one cross validation test. The column head-
ings ?sf?, ?jf?, ?time? and ?speed? refer to segmen-
tation F-measure, joint F-measure, testing time (in
3http://www.sourceforge.net/projects/zpar
849
Z&C08 this paper
# sf jf time speed sf jf time speed
1 97.18 93.27 557.97 1.86 97.25 93.51 44.20 23.44
2 97.65 93.81 521.63 1.99 97.66 93.97 42.07 24.26
3 96.08 91.04 444.69 2.33 95.55 90.65 39.23 26.41
4 96.31 91.93 431.04 2.40 96.37 92.15 39.54 26.20
5 96.35 91.94 508.39 2.04 95.84 91.51 43.30 23.93
6 94.48 88.63 482.78 2.15 94.25 88.53 43.77 23.67
7 95.27 90.52 361.95 2.86 95.10 90.42 41.76 24.81
8 94.98 90.01 418.54 2.47 94.87 90.30 39.81 26.02
9 95.23 90.84 471.3 2.20 95.21 90.55 42.03 26.65
10 96.49 92.11 500.72 2.08 96.33 92.12 43.12 24.03
average 96.00 91.41 469.90 2.24 95.84 91.37 41.88 24.94
Table 2: Speed and acccuracy comparisons with Z&C08 by 10-fold cross validation.
seconds) and testing speed (in the number of sen-
tences per second), respectively.
Our system gave a joint segmentation and POS-
tagging F-score of 91.37%, which is only 0.04%
lower than that of ZPar 0.2. The speed of our system
was over 10 times as fast as ZPar 0.2.
3.3 Final tests using CTB 5
We follow Kruengkrai et al (2009) and split the CTB
5 into training, development testing and testing sets,
as shown in Table 3. We ignored the development
test data since our system had been developed in pre-
vious experiments.
Kruengkrai et al (2009) made use of character
type knowledge for spaces, numerals, symbols, al-
phabets, Chinese and other characters. In the previ-
ous experiments, our system did not use any knowl-
edge beyond the training data. To make the compar-
ison fairer, we included knowledge of English let-
ters and Arabic numbers in this experiment. During
both training and decoding, English letters and Ara-
bic numbers are segmented using simple rules, treat-
ing consecutive English letters or Arabic numbers as
a single word.
The results are shown in Table 4, where row
?N07? refers to the model of Nakagawa and Uchi-
moto (2007), rows ?J08a? and ?b? refer to the mod-
els of Jiang et al (2008a) and Jiang et al (2008b),
and row ?K09? refers to the models of Kruengkrai et
al. (2009). Columns ?sf? and ?jf? refer to segmen-
tation and joint accuracies, respectively. Our system
Sections Sentences Words
Training 1?270 18,085 493,892
400?931
1001?1151
Dev 301?325 350 6,821
Test 271?300 348 8,008
Table 3: Training, development and test data on CTB 5.
sf jf
K09 (error-driven) 97.87 93.67
our system 97.78 93.67
K09 (baseline) 97.79 93.60
J08a 97.85 93.41
J08b 97.74 93.37
N07 97.83 93.32
Table 4: Accuracy comparisons with recent studies on
CTB 5.
gave comparable accuracies to these recent works,
obtaining the best (same as the error-driven version
of K09) joint F-score.
4 Related Work
The effectiveness of our beam-search decoder
showed that the joint segmentation and tagging
problem may be less complex than previously per-
ceived (Zhang and Clark, 2008; Jiang et al, 2008a).
At the very least, the single model approach with a
simple decoder achieved competitive accuracies to
what has been achieved so far by the reranking (Shi
850
and Wang, 2007; Jiang et al, 2008b) models and
an ensemble model using machine-translation tech-
niques (Jiang et al, 2008a). This may shed new light
on joint segmentation and POS-tagging methods.
Kruengkrai et al (2009) and Zhang and Clark
(2008) are the most similar to our system among
related work. Both systems use a discriminatively
trained linear model to score candidate outputs. The
work of Kruengkrai et al (2009) is based on Nak-
agawa and Uchimoto (2007), which separates the
processing of known words and unknown words,
and uses a set of segmentation tags to represent the
segmentation of characters. In contrast, our model
is conceptually simpler, and does not differentiate
known words and unknown words. Moreover, our
model is based on our previous work, in line with
Zhang and Clark (2007), which does not treat word
segmentation as character sequence labeling.
Our learning and decoding algorithms are also
different from Kruengkrai et al (2009). While Kru-
engkrai et al (2009) perform dynamic programming
and MIRA learning, we use beam-search to perform
incremental decoding, and the early-update version
of the perceptron algorithm to train the model. Dy-
namic programming is exact inference, for which
the time complexity is decided by the locality of
feature templates. In contrast, beam-search is ap-
proximate and can run in linear time. The param-
eter updating for our algorithm is conceptually and
computationally simpler than MIRA, though its per-
formance can be slightly lower. However, the early-
update mechanism we use is consistent with our in-
cremental approach, and improves the learning of
the beam-search process.
5 Conclusion
We showed that a simple beam-search decoding al-
gorithm can be effectively applied to the decoding
problem for a global linear model for joint word
segmentation and POS-tagging. By guiding search
with partial word information and performing learn-
ing for partial candidates, our system achieved sig-
nificantly faster speed with little accuracy loss com-
pared to the system of Z&C08.
The source code of our joint segmentor and POS-
tagger can be found at:
www.sourceforge.net/projects/zpar, version 0.4.
Acknowledgements
We thank Canasai Kruengkrai for discussion on effi-
ciency issues, and the anonymous reviewers for their
suggestions. Yue Zhang and Stephen Clark are sup-
ported by the European Union Seventh Framework
Programme (FP7-ICT-2009-4) under grant agree-
ment no. 247762.
References
Eugene Charniak, Mark Johnson, Micha Elsner, Joseph
Austerweil, David Ellis, Isaac Haxton, Catherine Hill,
R. Shrivaths, Jeremy Moore, Michael Pozar, and
Theresa Vu. 2006. Multilevel coarse-to-fine PCFG
parsing. In Proceedings of HLT/NAACL, pages 168?
175, New York City, USA, June. Association for Com-
putational Linguistics.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceedings
of ACL, pages 111?118, Barcelona, Spain, July.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
EMNLP, pages 1?8, Philadelphia, USA, July.
Wenbin Jiang, Liang Huang, Qun Liu, and Yajuan Lu?.
2008a. A cascaded linear model for joint Chinese
word segmentation and part-of-speech tagging. In
Proceedings of ACL/HLT, pages 897?904, Columbus,
Ohio, June.
Wenbin Jiang, Haitao Mi, and Qun Liu. 2008b. Word
lattice reranking for Chinese word segmentation and
part-of-speech tagging. In Proceedings of COLING,
pages 385?392, Manchester, UK, August.
Canasai Kruengkrai, Kiyotaka Uchimoto, Jun?ichi
Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi
Isahara. 2009. An error-driven word-character hybrid
model for joint Chinese word segmentation and POS
tagging. In Proceedings of ACL/AFNLP, pages 513?
521, Suntec, Singapore, August.
Tetsuji Nakagawa and Kiyotaka Uchimoto. 2007. A
hybrid approach to word segmentation and POS tag-
ging. In Proceedings of ACL Demo and Poster Ses-
sion, Prague, Czech Republic, June.
Hwee Tou Ng and Jin Kiat Low. 2004. Chinese part-of-
speech tagging: One-at-a-time or all-at-once? word-
based or character-based? In Proceedings of EMNLP,
Barcelona, Spain.
Brian Roark and Kristy Hollingshead. 2008. Classify-
ing chart cells for quadratic complexity context-free
inference. In Proceedings of COLING, pages 745?
752, Manchester, UK, August. Coling 2008 Organiz-
ing Committee.
851
Yanxin Shi and Mengqiu Wang. 2007. A dual-layer CRF
based joint decoding method for cascade segmentation
and labelling tasks. In Proceedings of IJCAI, Hyder-
abad, India.
Fei Xia, 2000. The part-of-speech tagging guidelines for
the Chinese Treebank (3.0).
Yue Zhang and Stephen Clark. 2007. Chinese segmenta-
tion with a word-based perceptron algorithm. In Pro-
ceedings of ACL, pages 840?847, Prague, Czech Re-
public, June.
Yue Zhang and Stephen Clark. 2008. Joint word segmen-
tation and POS tagging using a single perceptron. In
Proceedings of ACL/HLT, pages 888?896, Columbus,
Ohio, June.
852
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1194?1203,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Practical Linguistic Steganography using Contextual Synonym Substitution
and Vertex Colour Coding
Ching-Yun Chang
University of Cambridge
Computer Laboratory
Ching-Yun.Chang@cl.cam.ac.uk
Stephen Clark
University of Cambridge
Computer Laboratory
Stephen.Clark@cl.cam.ac.uk
Abstract
Linguistic Steganography is concerned with
hiding information in natural language text.
One of the major transformations used in Lin-
guistic Steganography is synonym substitu-
tion. However, few existing studies have stud-
ied the practical application of this approach.
In this paper we propose two improvements
to the use of synonym substitution for encod-
ing hidden bits of information. First, we use
the Web 1T Google n-gram corpus for check-
ing the applicability of a synonym in context,
and we evaluate this method using data from
the SemEval lexical substitution task. Second,
we address the problem that arises from words
with more than one sense, which creates a po-
tential ambiguity in terms of which bits are
encoded by a particular word. We develop a
novel method in which words are the vertices
in a graph, synonyms are linked by edges, and
the bits assigned to a word are determined by
a vertex colouring algorithm. This method
ensures that each word encodes a unique se-
quence of bits, without cutting out large num-
ber of synonyms, and thus maintaining a rea-
sonable embedding capacity.
1 Introduction
Steganography is concerned with hiding informa-
tion in a cover medium, in order to facilitate covert
communication, such that the presence of the infor-
mation is imperceptible to a user (human or com-
puter). Much of the existing research in steganog-
raphy has used images as cover media; however,
given the ubiquitous nature of electronic text, inter-
est is growing in using natural language as the cover
medium. Linguistic Steganography?lying at the in-
tersection of Computational Linguistics and Com-
puter Security?is concerned with making changes
to a cover text in order to embed information, in such
a way that the changes do not result in ungrammati-
cal or unnatural text.
A related area is natural language watermarking,
in which changes are made to a text in order to iden-
tify it, for example for copyright purposes. An inter-
esting watermarking application is ?traitor tracing?,
in which documents are changed in order to embed
individual watermarks. These marks can then be
used to later identify particular documents, for ex-
ample if a set of documents?identical except for
the changes used to embed the watermarks? have
been sent to a group of individuals, and one of the
documents has been leaked to a newspaper.
In terms of security, a linguistic stegosystem
should impose minimum embedding distortion to
the cover text so that the resulting stegotext in which
a message is camouflaged is inconspicuous, result-
ing in high imperceptibility. In addition, since
steganography aims at covert communication, a lin-
guistic stegosystem should allow sufficient embed-
ding capacity, known as the payload. There is a fun-
damental tradeoff between imperceptibility and pay-
load, since any attempt to embed more information
via changes to the cover text increases the chance
of introducing anomalies into the text and therefore
raising the suspicion of an observer.
A linguistic transformation is required to em-
bed information. Transformations studied in pre-
vious work include lexical substitution (Chapman
and Davida, 1997; Bolshakov, 2004; Taskiran et al,
2006; Topkara et al, 2006b), phrase paraphrasing
(Chang and Clark, 2010), sentence structure manip-
ulations (Atallah et al, 2001a; Atallah et al, 2001b;
1194
Liu et al, 2005; Meral et al, 2007; Murphy, 2001;
Murphy and Vogel, 2007; Topkara et al, 2006a) and
semantic transformations (Atallah et al, 2002; Vy-
bornova and Macq, 2007). Many of these transfor-
mations require some sophisticated NLP tools; for
example, in order to perform semantic transforma-
tions on text, word sense disambiguation, seman-
tic role parsing and anaphora resolution tools may
be required. However, the current state-of-the-art in
language technology is arguably not good enough
for secure linguistic steganography based on sophis-
ticated semantic transformations, and the level of ro-
bustness required to perform practical experiments
has only just become available. Hence many exist-
ing linguistic stegosystems are proof-of-concept im-
plementations with little practical evaluation of the
imperceptibility or payload.
1.1 Synonym substitution
Synonym substitution is a relatively straightforward
linguistic steganography method. It substitutes se-
lected words with the same part of speech (PoS) syn-
onyms, and does not involve operating on the sen-
tence structure so the modification can be guaran-
teed to be grammatical. Another advantage of this
method is that many languages are profuse in syn-
onyms, and so there is a rich source of information
carriers compared with other text transformations.
There are two practical difficulties associated with
hiding bits using synonym subsitution. The first is
that words can have more than one sense. In terms of
WordNet (Fellbaum, 1998), which is the electronic
dictionary we use, words can appear in more than
one synset. This is a problem because a word may be
assigned different bit strings in the different synsets,
and the receiver does not know which of the senses
to use, and hence does not know which hidden bit
string to recover. Our solution to this problem is a
novel vertex colouring method which ensures that
words are always assigned the same bit string, even
when they appear in different synsets.
The second problem is that many synonyms are
only applicable in certain contexts. For example, the
words in the WordNet synset {bridge, span} share
the meaning of ?a structure that allows people or ve-
hicles to cross an obstacle such as a river or canal
or railway etc.?. However, bridge and span cannot
be substutited for each other in the sentence ?sus-
Figure 1: An example of the basic algorithm
pension bridges are typically ranked by the length
of their main span?, and doing so would likely raise
the suspicion of an observer due to the resulting
anomaly in the text.
Our solution to this problem is to perform a con-
textual check which utilises the Web 1T n-gram
Google n-gram corpus.1 We evaluate the method
using the data from the English Lexical Substitu-
tion task for SemEval-2007.2 The resulting preci-
sion of our lexical substitution system can be seen
as an indirect measure of the imperceptibility of the
stegosystem, whereas the recall can be seen as an
indirect measure of the payload.
The paper is organised so that the contextual
check is described first, and this is evaluated inde-
pendently of the steganographic application. Then
the vertex colouring method is presented, and finally
we show how the contextual check can be integrated
with the vertex colouring coding method to give a
complete stegsosystem. For readers unfamiliar with
linguistic steganogaphy, Section 2 has some exam-
ples of how bits can be hidden using textual trans-
formations. Also, Chang and Clark (2010) is a re-
cent NLP paper which describes the general linguis-
tic steganography framework.
2 Related Work
In the original work on linguistic steganography in
the late 1990s, Winstein proposed an information
hiding algorithm using a block coding method to en-
code synonyms, so that the selection of a word from
a synset directly associates with part of the secret
bitstring (Bergmair, 2007). Figure 1 illustrates the
embedding procedure of this approach. In this ex-
ample, the bitstring to be embedded is 010, which
1www.ldc.upenn.edu/Catalog/docs/LDC2006T13/readme.txt
2http://www.dianamccarthy.co.uk/task10index.html
1195
Figure 2: An example of applying the basic algorithm to
overlapping synsets
can be divided into two codewords, 0 and 10, and the
information carriers in the cover text are the words
finished and project. According to the encoding dic-
tionary, complete represents 0, and task represents
10; hence these words are chosen and replace the
original words in the cover text (with suitable suffix-
ation). The stego sentence ?He completed the task?
is then sent to the receiver. In order to recover the
message, the receiver only needs a copy of the en-
coding dictionary, and the decoding algorithm sim-
ply reverses the process used to encode the hidden
bits. Note that the receiver does not need the origi-
nal cover text to recover the information.
This algorithm requires synonym sets to be dis-
joint; i.e. no word may appear in more than one syn-
onym set, since overlapping synsets may cause am-
biguities during the decoding stage. Figure 2 shows
what happens when the basic algorithm is applied to
two overlapping synonym sets. As can be seen from
the example, composition is represented by two dif-
ferent codewords and thus the secret bitstring can-
not be reliably recovered, since the receiver does not
know the original cover word or the sense of the
word. In order to solve this problem, we propose
a novel coding method based on vertex colouring,
described in Section 4.
In addition to the basic algorithm, Winstein pro-
posed the T-Lex system using synonym substitution
as the text transformation. In order to solve the
problem of words appearing in more than one syn-
onym set, Winstein defines interchangeable words
as words that belong to the same synsets, and only
uses these words for substitution. Any words that are
not interchangeable are discarded and not available
for carrying information. The advantage in this ap-
proach is that interchangeable words always receive
the same codeword. The disadvantage is that many
synonyms need to be discarded in order to achieve
this property. Winstein calculates that only 30% of
WordNet can be used in such a system.
Another stegosystem based on synonym substi-
tution was proposed by Bolshakov (2004). In or-
der to ensure both sender and receiver use the
same synsets, Bolshakov applied transitive closure
to overlapping synsets to avoid the decoding ambi-
guity. Applying transitive closure leads to a merger
of all the overlapping synsets into one set which is
then seen as the synset of a target word. Consider
the overlapping synsets in Figure 2 as an example.
After applying transitive closure, the resulting set
is {?authorship?, ?composition?, ?paper?, ?penning?,
?report?, ?theme?, ?writing?}.
Bolshakov (2004) also uses a method to deter-
mine whether a substitution is applicable in context,
using a collocation-based test. Finally, the colloca-
tionally verified synonyms are encoded by using the
block coding method. This is similar to our use of
the Google n-gram data to check for contextual ap-
plicability.
The disadvantage of Bolshakov?s system is that
all words in a synonym transitive closure chain need
to be considered, which can lead to very large sets
of synonyms, and many which are not synonymous
with the original target word. In contrast, our pro-
posed method operates on the original synonym sets
without extending them unnecessarily.
3 Proposed Method and Experiments
3.1 Resources
We use WordNet (Fellbaum, 1998) to provide sets
of synonyms (synsets) for nouns, verbs, adjectives
and adverbs. Since the purpose of using WordNet is
to find possible substitutes for a target word, those
synsets containing only one entry are not useful and
are ignored by our stegosystem. In addition, our
stegosystem only takes single word substitution into
consideration in order to avoid the confusion of find-
ing information-carrying words during the decoding
phase. For example, if the cover word ?complete? is
replaced by ?all over?, the receiver would not know
whether the secret message is embedded in the word
?over? or the phrase ?all over?. Table 1 shows the
statistics of synsets used in our stegosystem.
1196
noun verb adj adv
# of synsets 16,079 4,529 6,655 964
# of entries 30,933 6,495 14,151 2,025
average set size 2.56 2.79 2.72 2.51
max set size 25 16 21 8
Table 1: Statistics of synsets used in our stegosystem
For the contextual check we use the Google Web
1T 5-gram Corpus (Brants and Franz, 2006) which
contains counts for n-grams from unigrams through
to five-grams obtained from over 1 trillion word to-
kens of English Web text. The corpus has been used
for many tasks such as spelling correction (Islam and
Inkpen, 2009; Carlson et al, 2008) and multi-word
expression classification (Kummerfeld and Curran,
2008). Moreover, for the SemEval-2007 English
Lexical Substitution Task, which is similar to our
substitution task, six out of ten participating teams
utilised the Web 1T corpus.
3.2 Synonym Checking Method
In order to measure the degree of acceptability in a
substitution, the proposed filter calculates a substi-
tution score for a synonym by using the observed
frequency counts in the Web n-gram corpus. The
method first extracts contextual n-grams around the
synonym and queries the n-gram frequency counts
from the corpus. For each n, the total count fn is cal-
culated by summing up individual n-gram frequen-
cies, for every contextual n-gram containing the tar-
get word. We define a count function Count(w) =
?5
n=2 log(fn) where log(0) is defined as zero. If
Count(w) = 0, we assume the word w is unrelated
to the context and therefore is eliminated from con-
sideration. We then find the maximum Count(w)
called max from the remaining words. The main
purpose of having max is to score each word rela-
tive to the most likely synonym in the group, so even
in less frequent contexts which lead to smaller fre-
quency counts, the score of each synonym can still
indicate the degree of feasibility. The substitution
score is defined as Score(w) = Count(w)?max.
The hypothesis is that a word with a high score is
more suitable for the context, and we apply a thresh-
old so that synonyms having a score lower than the
threshold are discarded.
Figure 3 demonstrates an example of calculat-
f2=525,856 high pole 3,544
pole . 522,312
f3=554 very high pole 84
high pole . 470
f4=72 a very high pole 72
very high pole . 0
f5=0 not a very high pole 0
a very high pole . 0
Count(?pole?)=log(f2)+log(f3)+log(f4)+log(f5)=23
Score(?pole?)=Count(?pole?)/max=0.44>0.37
Figure 3: An example of using the proposed synonym
checking method
ing the substitution score for the synonym ?pole?
given the cover sentence ?This is not a very high
bar.? First of all, various contextual n-grams are ex-
tracted from the sentence and the Web n-gram cor-
pus is consulted to obtain their frequency counts.
Count(?pole?) is then calculated using the n-gram
frequencies. Suppose the threshold is 0.37, and the
max score is 52. Since Count(?pole?) is greater
than zero and the substitution score Score(?pole?)
is 0.44, the word ?pole? is determined as acceptable
for this context (even though it may not be, depend-
ing on the meaning of ?bar? in this case).
3.3 Evaluation Data
In order to evaluate the proposed synonym check-
ing method, we need some data to test whether our
method can pick out acceptable substitutions. The
English Lexical Substitution task for SemEval-2007
has created human-annotated data for developing
systems that can automatically find feasible substi-
tutes given a target word in context. This data com-
prises 2010 sentences selected from the English In-
ternet Corpus3, and consists of 201 words: nouns,
verbs, adjectives and adverbs each with ten sen-
tences containing that word. The five annotators
were asked to provide up to three substitutes for a
target word in the context of a sentence, and were
permitted to consult a dictionary or thesaurus of
their choosing.
We use the sentences in this gold standard as the
cover text in our experiments so that the substi-
tutes provided by the annotators can be the positive
data for evaluating the proposed synonym check-
3http://corpus.leeds.ac.uk/internet.html
1197
noun verb adj adv
# of target words 59 54 57 35
# of sentences 570 527 558 349
# of positives 2,343 2,371 2,708 1,269
# of negatives 1,914 1,715 1,868 884
Table 2: Statistics of experimental data
ing methods. Since we only take into considera-
tion the single word substitutions for the reason de-
scribed earlier, multi-word substitutes are removed
from the positive data. Moreover, we use Word-
Net as the source of providing candidate substitutes
in our stegosystem, so if a human-provided sub-
stitute does not appear in any synsets of its target
word in WordNet, there is no chance for our sys-
tem to replace the target word with the substitute and
therefore, the substitute can be eliminated. Table 2
presents the statistics of the positive data for our ex-
periments.
Apart from positive data, we also need some neg-
ative data to test whether our method has the ability
to filter out bad substitutions. Since the annotators
were allowed to refer to a dictionary or thesaurus,
we assume that annotators used WordNet as one of
the reference resources while generating candidates.
Hence we assume that, if a word in the correct synset
for a target word is not in the set produced by the hu-
man annotators, then it is inappropriate for that con-
text and a suitable negative example. This method is
appropriate because our steganography system has
to distinguish between good and bad synonyms from
WordNet, given a particular context.
For the above reasons, we extract the negative
data for our experiments by first matching positive
substitutes of a target word to all the synsets that
contain the target word in WordNet. The synset
that includes the most positive substitutes is used
to represent the meaning of the target word. If
there is more than one synset containing the high-
est number of positives, all the synsets are taken
into consideration. We then randomly select up to
six single-word synonyms other than positive substi-
tutes from the chosen synset(s) as negative instances
of the target word. Figure 4 shows an example of
automatically collected negative data from WordNet
given a target word and its positive substitutes. The
synset {?remainder?, ?balance?, ?residual?, ?residue?,
Figure 4: An example of automatic negative data
noun verb adj adv
# of true negatives 234 201 228 98
# of false negatives 9 20 28 16
Table 3: Annotation results for negative data
?residuum?, ?rest?} is selected for negative data col-
lection since it contains one of the positives while
the other synsets do not. We assume the selected
synset represents the meaning of the original word,
and those synonyms in the synset which are not an-
notated as positives must have a certain degree of
mismatch to the context. Therefore, from this exam-
ple, ?balance?, ?residue?, ?residuum? and ?rest? are
extracted as negatives to test whether our synonym
checking method can pick out bad substitutions from
a set of words sharing similar or the same meaning.
In order to examine whether the automatically
collected instances are true negatives and hence
form a useful test set, a sample of automatically gen-
erated negatives was selected for human evaluation.
For each PoS one sentence of each different target
word is selected, which results in roughly 13% of
the collected negative data, and every negative sub-
stitute of the selected sentences was judged by the
second author. As can be seen from the annota-
tion results shown in Table 3, most of the instances
are true negatives, and only a few cases are incor-
rectly chosen as false negatives. Since the main pur-
pose of the data set is to test whether the proposed
synonym checking method can guard against inap-
propriate synonym substitutions and be integrated
in the stegosystem, it is reasonable to have a few
false negatives in our experimental data. Also, it
is more harmless to rule out a permissible substitu-
1198
PoS Acc% P% R% F% Threshold
noun 70.2 70.0 80.2 74.7 0.58
verb 68.1 69.7 79.5 74.3 0.56
adj 72.5 72.7 85.7 78.7 0.48
adv 73.7 76.4 80.1 78.2 0.54
Table 4: Performance of the synonym checking method
tion than including an inappropriate replacement for
a stegosystem in terms of the security. Table 2 gives
the statistics of the automatically collected negative
data for our experiments.
Note that, although we use the data from the lex-
ical substitution task, our task is different: the pos-
sible substitutions for a target word need to be fixed
in advance for linguistic steganography (in order for
the receiver to be able to recover the hidden bits),
whereas for the lexical substitution task participants
were asked to discover possible replacements.
3.4 Results
The performance of the proposed checking method
is evaluated in terms of accuracy, precision, recall
and balanced F-measure. Accuracy represents the
percentage of correct judgements over all accept-
able and unacceptable substitutions. Precision is the
percentage of substitutions judged acceptable by the
method which are determined to be suitable syn-
onyms by the human judges. Recall is the percent-
age of substitutions determined to be feasible by the
human annotators which are also judged acceptable
by the method. The interpretation of the measures
for a stegosystem is that a higher precision value im-
plies a better security level since good substitutions
are less likely to be seen as suspicious by the ob-
server; whereas a larger recall value means a greater
payload capacity since words are being substituted
where possible and therefore embedding as much in-
formation as possible.
In order to derive sensible threshold values for
each PoS, 5-fold cross-validation was implemented
to conduct the experiments. For each fold, 80% of
the data is used to find the threshold value which
maximises the accuracy, and that threshold is then
applied to the remaining 20% to get the final result.
Table 4 gives the results for the synonym checking
method and the average threshold values over the 5
folds. In addition, we are interested in the effect of
Figure 5: System performance under various thresholds
various thresholds on the system performance. Fig-
ure 5 shows the precision and recall values with re-
spect to different thresholds for each PoS. From the
graphs we can clearly see the trade-off between pre-
cision and recall. Although a higher precision can
be achieved by using a higher threshold value, for
example noun?s substitutions almost reach 90% pre-
cision with threshold equal to 0.9, the large drop in
recall means many applicable synonyms are being
eliminated. In other words, the trade-off between
precision and recall implies the trade-off between
imperceptibility and payload capacity for linguistic
steganography. Therefore, the practical threshold
setting would depend on how steganography users
want to trade off imperceptibility for payload.
1199
Figure 6: An example of coloured synonym graph
4 Proposed Stegosystem
4.1 The Vertex Coloring Coding Method
In this section, we propose a novel coding method
based on vertex colouring by which each synonym is
assigned a unique codeword so the usage of overlap-
ping synsets is not problematic for data embedding
and extracting. A vertex colouring is a labelling of
the graph?s vertices with colours subject to the con-
dition that no two adjacent vertices share the same
colour. The smallest number of colours required
to colour a graph G is called its chromatic num-
ber ?(G), and a graph G having chromatic number
?(G) = k is called a k-chromatic graph. The main
idea of the proposed coding method is to represent
overlapping synsets as an undirected k-chromatic
graph called a synonym graph which has a vertex
for each word and an edge for every pair of words
that share the same meaning. A synonym is then
encoded by a codeword that represents the colour
assigned by the vertex colouring of the synonym
graph. Figure 6 shows the use of four different
colours, represented by ?00?, ?01?, ?10? and ?11?, to
colour the 4-chromatic synonym graph of the two
overlapping synsets in Figure 2. Now, the over-
lapped word ?composition? receives a unique code-
word no matter which synset is considered, which
means the replacement of ?paper? to ?composition?
in Figure 2 will not cause an ambiguity since the re-
ceiver can apply the same coding method to derive
identical codewords used by the sender.
99.6% of synsets in WordNet have size less than
8, which means most of the synsets cannot exhaust
more than a 2-bit coding space (i.e. we can only
encode at most 2 bits using a typical synset). There-
fore, we restrict the chromatic number of a synonym
graph G to 1 < ?(G) ? 4, which implies the max-
imum size of a synset is 4. When ?(G) = 2, each
Figure 7: Examples of 2,3,4-chromatic synonym graphs
vertex is assigned a single-bit codeword either ?0?
or ?1? as shown in Figure 7(a). When ?(G) = 3,
the overlapping set?s size is either 2 or 3, which can-
not exhaust the 2-bit coding space although code-
words ?00?, ?01? and ?10? are initially assigned to
each vertex. Therefore, only the most significant
bits are used to represent the synonyms, which we
call codeword reduction. After the codeword reduc-
tion, if a vertex has the same codeword, say ?0?, as
all of its neighbors, the vertex?s codeword must be
changed to ?1? so that the vertex would be able to ac-
commodate either secret bit ?0? or ?1?, which we call
codeword correction. Figure 7(b) shows an example
of the process of codeword reduction and codeword
correction for ?(G) = 3. For the case of ?(G) = 4,
codeword reduction is applied to those vertices that
themselves or their neighboring vertices have no ac-
cess to all the codewords ?00?, ?01?, ?10? and ?11?.
For example, vertices a, b, c, e and f in Figure 7(c)
meet the requirement of needing codeword reduc-
tion. The codeword correction process is then fur-
ther applied to vertex f to rectify its accessibility.
1200
Figure 8 describes a greedy algorithm for con-
structing a coded synonym graph using at most
4 colours, given n synonyms w1, w2,. . . , wn in
the overlapping synsets. Let us define a function
E(wi, wj) which returns an edge between wi and
wj if wi and wj are in the same synset; otherwise
returns false. Another function C(wi) returns the
colour of the synonym wi. The procedure loops
through all the input synonyms. For each iteration,
the procedure first finds available colours for the tar-
get synonym wi. If there is no colour available,
namely all the four colours have already been given
to wi?s neighbors, wi is randomly assigned one of
the four colours; otherwise, wi is assigned one of
the available colours. After adding wi to the graph
G, the procedure checks whether adding an edge of
wi to graph G would violate the vertex colouring.
After constructing the coloured graph, codeword re-
duction and codeword correction as previously de-
scribed are applied to revise improper codewords.
4.2 Proposed Lexical Stegosystem
Figure 9 illustrates the framework of our lexical
stegosystem. Note that we have preprocessed Word-
Net by excluding multi-word synonyms and single-
entry synsets. A possible information carrier is first
found in the cover sentence. We define a possi-
ble information carrier as a word in the cover sen-
tence that belongs to at least one synset in Word-
Net. The synsets containing the target word, and all
other synsets which can be reached via the synonym
relation, are then extracted from WordNet (i.e. we
build the connected component of WordNet which
contains the target word according to the synonym
relation). Words in these sets are then examined
by the Google n-gram contextual checking method
to eliminate inappropriate substitutions. If there is
more than one word left and if words which pass the
filter all belong to the same synset, the block cod-
ing method is used to encode the words; otherwise
the vertex colouring coding is applied. Finally, ac-
cording to the secret bitstring, the system selects the
synonym that shares an edge with the target word
and has as its codeword the longest potential match
with the secret bitstring.
We use the connected component of WordNet
containing the target word as a simple method to en-
sure that both sender and receiver colour-code the
INPUT: a synonym list w1, w2,. . . , wn and an
empty graph G
OUTPUT: a coded synonym graph G using at
most four colours
FOR every synonym wi in the input list
initialize four colours as available for wi
FOR every wj in graph G
IF E(wi, wj) THEN
set C(wj) as unavailable
END IF
END FOR
IF there is a colour available THEN
assign one of the available colours
to wi
ELSE
assign one of the four colours to wi
END IF
ADD wi to graph G
FOR every wj in graph G
IF E(wi, wj) and C(wi) is not
equal to C(wj) THEN
ADD edge E(wi, wj) to G
END IF
END FOR
END FOR
codeword reduction
codeword correction
OUTPUT graph G
Figure 8: Constructing a coloured synonym graph
same graph. It is important to note, however, that
the sender only considers the synonyms of the target
word as potential substitutes; the connected compo-
nent is only used to consistently assign the codes.
For the decoding process, the receiver does not
need the original text for extracting secret data. An
information carrier can be found in the stegotext by
referring to WordNet in which related synonyms are
extracted. Those words in the related sets undergo
the synonym checking method and then are encoded
by either block coding or vertex colouring coding
scheme depending on whether the remaining words
are in the same synset. Finally, the secret bitstring is
implicit in the codeword of the information carrier
and therefore can be extracted.
We demonstrate how to embed secret bit 1 in the
1201
Figure 9: Framework of the proposed lexical stegosystem
sentence ?it is a shame that we could not reach the
next stage.? A possible information carrier ?shame?
is first found in the sentence. Table 5 lists the re-
lated synsets extracted from WordNet. The score
of each word calculated by the synonym checking
method using the Web 1T Corpus is given as a sub-
script. Assume the threshold score is 0.27. The out-
put of the synonym checking method is shown at the
right side of Table 5. Since the remaining words do
not belong to the same synset, the vertex colouring
coding method is then used to encode the words.
Figure 10(a) is the original synset graph in which
each vertex is assigned one of the four colours; Fig-
ure 10(b) is the graph after applying codeword re-
duction. Although both ?disgrace? and ?pity? are en-
coded by ?1?, ?pity? is chosen to replace the cover
word since it has a higher score. Finally, the stego-
text is generated, ?it is a pity that we could not reach
the next stage.?
As a rough guide to the potential payload with
this approach, we estimate that, with a threshold of
0.5 for the contextual check, the payload would be
slightly higher than 1 bit per newspaper sentence.
5 Conclusions
One of the contributions of this paper is to develop a
novel lexical stegosystem based on vertex colouring
cover sentence:
It is a shame that we could not reach the next stage
original synsets retained synsets
{commiseration.28, {commiseration,
pity.97, ruth.13, pathos.31} pity, pathos}
{pity.97, shame1} {pity, shame}
{compassion.49, pity.97} {compassion, pity}
{condolence.27, {commiseration}
commiseration.28} {pathos, poignancy}
{pathos.31, poignancy.31} {shame, disgrace}
{shame1, disgrace.84, {compassion}
ignominy.24} {poignancy}
{compassion.49,
compassionateness0}
{poignance.12,
poignancy.31}
Table 5: Synsets of ?shame? before and after applying the
synonym checking method
Figure 10: Synonym graph of ?shame?
coding which improves the data embedding capacity
compared to existing systems. The vertex colouring
coding method represents synonym substitution as a
synonym graph so the relations between words can
be clearly observed. In addition, an automatic sys-
tem for checking synonym acceptability in context is
integrated in our stegosystem to ensure information
security. For future work, we would like to explore
more linguistic transformations that can meet the re-
quirements of linguistic steganography ? retaining
the meaning, grammaticality and style of the origi-
nal text. In addition, it is crucial to have a full eval-
uation of the linguistic stegosystem in terms of im-
perceptibility and payload capacity so we can know
how much data can be embedded before the cover
text reaches its maximum distortion which is toler-
ated by a human judge.
1202
References
Mikhail J. Atallah, Craig J. McDonough, Victor Raskin,
and Sergei Nirenburg. 2001a. Natural language pro-
cessing for information assurance and security: an
overview and implementations. In Proceedings of the
2000 workshop on New security paradigms, pages 51?
65, Ballycotton, County Cork, Ireland.
Mikhail J. Atallah, Victor Raskin, Michael C. Crogan,
Christian Hempelmann, Florian Kerschbaum, Dina
Mohamed, and Sanket Naik. 2001b. Natural lan-
guage watermarking: design, analysis, and a proof-
of-concept implementation. In Proceedings of the 4th
International Information Hiding Workshop, volume
2137, pages 185?199, Pittsburgh, Pennsylvania.
Mikhail J. Atallah, Victor Raskin, Christian F. Hempel-
mann, Mercan Karahan, Umut Topkara, Katrina E.
Triezenberg, and Radu Sion. 2002. Natural language
watermarking and tamperproofing. In Proceedings of
the 5th International Information Hiding Workshop,
pages 196?212, Noordwijkerhout, The Netherlands.
Richard Bergmair. 2007. A comprehensive bibliogra-
phy of linguistic steganography. In Proceedings of the
SPIE Conference on Security, Steganography, and Wa-
termarking of Multimedia Contents, volume 6505.
Igor A. Bolshakov. 2004. A method of linguistic
steganography based on collocationally-verified syn-
onym. In Information Hiding: 6th International Work-
shop, volume 3200, pages 180?191, Toronto, Canada.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-
gram corpus version 1.1. Technical report, Google Re-
search.
Andrew Carlson, Tom M. Mitchell, and Ian Fette. 2008.
Data analysis project: Leveraging massive textual cor-
pora using n-gram statistics. Technical report, School
of Computer Science, Carnegie Mellon University.
Ching-Yun Chang and Stephen Clark. 2010. Linguis-
tic steganography using automatically generated para-
phrases. In Human Language Technologies: The 2010
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
591?599, Los Angeles, California, June. Association
for Computational Linguistics.
Mark Chapman and George I. Davida. 1997. Hiding the
hidden: A software system for concealing ciphertext
as innocuous text. In Proceedings of the First Interna-
tional Conference on Information and Communication
Security, volume 1334, pages 335?345, Beijing.
Christiane Fellbaum. 1998. WordNet: An electronic lex-
ical database. MIT Press, first edition.
Aminul Islam and Diana Inkpen. 2009. Real-word
spelling correction using Google Web IT 3-grams. In
EMNLP ?09: Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing,
pages 1241?1249, Morristown, USA. Association for
Computational Linguistics.
Jonathan K Kummerfeld and James R Curran. 2008.
Classification of verb particle constructions with the
Google Web 1T Corpus. In Proceedings of the Aus-
tralasian Language Technology Association Workshop
2008, pages 55?63, Hobart, Australia, December.
Yuling Liu, Xingming Sun, and Yong Wu. 2005. A nat-
ural language watermarking based on Chinese syntax.
In Advances in Natural Computation, volume 3612,
pages 958?961, Changsha, China.
Diana McCarthy and Roberto Navigli. 2007. Semeval-
2007 task 10: English lexical substitution task. In Pro-
ceedings of the 4th International Workshop on Seman-
tic Evaluations, pages 48?53, Prague, Czech Republic.
Hasan M. Meral, Emre Sevinc, Ersin Unkar, Bulent
Sankur, A. Sumru Ozsoy, and Tunga Gungor. 2007.
Syntactic tools for text watermarking. In Proceed-
ings of the SPIE Conference on Security, Steganogra-
phy, and Watermarking of Multimedia Contents, vol-
ume 6505, San Jose, CA.
Brian Murphy and Carl Vogel. 2007. The syntax of con-
cealment: reliable methods for plain text information
hiding. In Proceedings of the SPIE Conference on Se-
curity, Steganography, and Watermarking of Multime-
dia Contents, volume 6505, San Jose, CA.
Brian Murphy. 2001. Syntactic information hiding in
plain text. Master?s thesis, Trinity College Dublin.
Cuneyt M. Taskiran, Mercan Topkara, and Edward J.
Delp. 2006. Attacks on linguistic steganography sys-
tems using text analysis. In Proceedings of the SPIE
Conference on Security, Steganography, and Water-
marking of Multimedia Contents, volume 6072, pages
97?105, San Jose, CA.
Mercan Topkara, Umut Topkara, and Mikhail J. Atallah.
2006a. Words are not enough: sentence level natural
language watermarking. In Proceedings of the ACM
Workshop on Content Protection and Security, pages
37?46, Santa Barbara, CA.
Umut Topkara, Mercan Topkara, and Mikhail J. Atal-
lah. 2006b. The hiding virtues of ambiguity: quan-
tifiably resilient watermarking of natural language text
through synonym substitutions. In Proceedings of the
8th Workshop on Multimedia and Security, pages 164?
174, Geneva, Switzerland.
M. Olga Vybornova and Benoit Macq. 2007. A
method of text watermarking using presuppositions.
In Proceedings of the SPIE Conference on Secu-
rity, Steganography, and Watermarking of Multimedia
Contents, volume 6505, San Jose, CA.
1203
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1147?1157,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Syntax-Based Grammaticality Improvement using CCG and Guided Search
Yue Zhang
University of Cambridge
Computer Laboratory
yue.zhang@cl.cam.ac.uk
Stephen Clark
University of Cambridge
Computer Laboratory
stephen.clark@cl.cam.ac.uk
Abstract
Machine-produced text often lacks grammat-
icality and fluency. This paper studies gram-
maticality improvement using a syntax-based
algorithm based on CCG. The goal of the
search problem is to find an optimal parse tree
among all that can be constructed through se-
lection and ordering of the input words. The
search problem, which is significantly harder
than parsing, is solved by guided learning for
best-first search. In a standard word order-
ing task, our system gives a BLEU score of
40.1, higher than the previous result of 33.7
achieved by a dependency-based system.
1 Introduction
Machine-produced text, such as SMT output, often
lacks grammaticality and fluency, especially when
using n-gram language modelling (Knight, 2007).
Recent efforts have been made to improve grammat-
icality using local language models (Blackwood et
al., 2010) and global dependency structures (Wan et
al., 2009). We study grammaticality improvement
using a syntax-based system.
The task is effectively a text-to-text generation
problem where the goal is to produce a grammati-
cal sentence from an ungrammatical and fragmen-
tary input. The input can range from a bag-of-
words (Wan et al, 2009) to a fully-ordered sentence
(Blackwood et al, 2010). A general form of the
problem is to construct a grammatical sentence from
a set of un-ordered input words. However, in cases
where the base system produces fluent subsequences
within the sentence, constraints on the choice and
order of certain words can be fed to the grammati-
cality improvement system. The input may also in-
clude words beyond the output of the base system,
e.g. extra words from the SMT lattice, so that con-
tent word insertion and deletion can be performed
implicity via word selection.
We study the above task using CCG (Steedman,
2000). The main challenge is the search problem,
which is to find an optimal parse tree among all that
can be constructed with any word choice and order
from the set of input words. We use an approximate
best-first algorithm, guided by learning, to tackle
the more-than-factorial complexity. Beam-search is
used to control the volume of accepted hypotheses,
so that only a very small portion of the whole search
space is explored. The search algorithm is guided by
perceptron training, which ensures that the explored
path in the search space consists of highly proba-
ble hypotheses. This framework of best-first search
guided by learning is a general contribution of the
paper, which could be applied to problems outside
grammaticality improvement.
We evaluate our system using the generation task
of word-order recovery, which is to recover the orig-
inal word order of a fully scrambled input sentence
(Bangalore et al, 2000; Wan et al, 2009). This
problem is an instance of our general task formu-
lation, but without any input constraints, or con-
tent word selection (since all input words are used).
It is straightforward to use this task to evaluate
our system and compare with existing approaches.
Our system gave 40.1 BLEU score, higher than the
dependency-based system of Wan et al (2009), for
which a BLEU score of 33.7 was reported.
1147
2 The Grammar
Combinatory Categorial Grammar (CCG; Steedman
(2000)) is a lexicalized grammar formalism, which
associates words with lexical categories. Lexical
categories are detailed grammatical labels, typically
expressing subcategorisation information. CCG, and
parsing with CCG, has been described in detail
elsewhere (Clark and Curran, 2007; Hockenmaier,
2003); here we provide only a short description.
During CCG parsing, adjacent categories are com-
bined using CCG?s combinatory rules. For example,
a verb phrase in English (S\NP ) can combine with
an NP to its left:
NP S\NP ? S
In addition to binary rule instances, such as the
one above, there are also unary rules which operate
on a single category in order to change its type. For
example, forward type-raising can change a subject
NP into a complex category looking to the right for
a verb phrase:
NP ? S/(S\NP)
Following Hockenmaier (2003), we extract the
grammar by reading rule instances directly from the
derivations in CCGbank (Hockenmaier and Steed-
man, 2007), rather than defining the combinatory
rule schema manually as in Clark and Curran (2007).
3 The Search Algorithm
The input to the search algorithm is a set of words,
each word having a count that specifies the maxi-
mum number of times it can appear in the output.
Typically, most input words can occur only once in
the output. However, punctuation marks and func-
tion words can be given a higher count. Depending
on the fluency of the base output (e.g. the output
of the base SMT system), some constraints can be
given to specific input words, limiting their order or
identifying them as an atomic phrase, for example.
The goal of the search algorithm is to find an op-
timal parse tree (including the surface string) among
all that can be constructed via selecting and ordering
a subset of words from the input multiset. The com-
plexity of this problem is much higher than a typical
parsing problem, since there is an exponential num-
ber of word choices for the output sentence, each
with a factorial number of orderings. Moreover, dy-
namic programming packing for parsers, such as a
CKY chart, is not applicable, because of the lack of
a fixed word order.
We perform approximate search using a best-
first algorithm. Starting from single words, candi-
date parses are constructed bottom-up. Similar to a
best-first parser (Caraballo and Charniak, 1998), the
highest scored hypothesis is expanded first. A hy-
pothesis is expanded by applying CCG unary rules
to the hypothesis, or by combining the hypothesis
with existing hypotheses using CCG binary rules.
We use beam search to control the number of ac-
cepted hypotheses, so that the computational com-
plexity of expanding each hypothesis is linear in the
size of the beam. Since there is no guarantee that a
goal hypothesis will be found in polynomial time,
we apply a robustness mechanism (Riezler et al,
2002; White, 2004), and construct a default output
when no goal hypothesis is found within a time limit.
3.1 Data Structures
Edges are the basic structures that represent hy-
potheses. Each edge is a CCG constituent, spanning
a sequence of words. Similar to partial parses in a
typical chart parser, edges have recursive structures.
Depending on the number of subedges, edges can
be classified into leaf edges, unary edges and binary
edges. Leaf edges, which represent input words,
are constructed first in the search process. Existing
edges are expanded to generate new edges via unary
and binary CCG rules. An edge that meets the output
criteria is called a goal edge. In the experiments of
this paper, we define a goal edge as one that includes
all input words the correct number of times.
The signature of an edge consists of the cate-
gory label, surface string and head word of the con-
stituent. Two edges are equivalent if they share
the same signature. Given our feature definitions,
a lower scoring edge with the same signature as a
higher scoring edge cannot be part of the highest
scoring derivation.
The number of words in the surface string of an
edge is called the size of the edge. Other important
substructures of an edge include a bitvector and an
array, which stores the indices of the input words
that the edge contains. Before two edges are com-
bined using a binary CCG rule, an input check is per-
1148
formed to make sure that the total count for a word
from the two edges does not exceed the count for
that word in the input. Intuitively, an edge can record
the count of each unique input word it contains,
and perform the input check in linear time. How-
ever, since most input words typically occur once,
they can be indexed and represented by a bitvector,
which allows a constant time input check. The few
multiple-occurrence words are stored in a count ar-
ray.
In the best-first process, edges to be expanded are
ordered by their scores, and stored in an agenda.
Edges that have been expanded are stored in a chart.
There are many ways in which edges could be or-
dered and compared. Here the chart is organised as
a set of beams, each containing a fixed number of
edges with a particular size. This is similar to typical
decoding algorithms for phrase-based SMT (Koehn,
2010). In each beam, edges are ordered by their
scores, and low score edges are pruned. In addition
to pruning by the beam, only the highest scored edge
is kept among all that share the same signature.
3.2 The Search Process
Figure 1 shows pseudocode for the search algorithm.
During initialization, the agenda (a) and chart (c)
are cleared. All candidate lexical categories are as-
signed to each input word, and the resulting leaf
edges are put onto the agenda.
In the main loop, the best edge (e) is popped from
the agenda. If e is a goal hypothesis, it is appended
to a list of goals (goal), and the loop is continued
without e being expanded. If e or any equivalent
edge e? of e is already in the chart, the loop continues
without expanding e. It can be proved that any edge
in the chart must have been combined with e?, and
therefore the expansion of e is unnecessary.
Edge e is first expanded by applying unary rules,
and any new edges are put into a list (new). Next, e
is matched against each existing edge e? in the chart.
e and e? can be combined if they pass the input check,
and there is a binary rule in which the constituents
are combined. e and e? are combined in both possible
orders, and any resulting edge is added to new.
At the end of each loop, edges from new are added
to the agenda, and new is cleared. The loop contin-
ues until a stopping criterion is met. A typical stop-
ping condition is that goal contains N goal edges.
a? INITAGENDA(input)
c? INITCHART()
new? []
goal? []
while not STOP(goal, time):
e? POPBEST(a)
if GOALTEST(e)
APPEND(goal, e)
continue
for e? in c:
if EQUIV(e?, e):
continue
for e? in UNARY(e, grammar):
APPEND(new, e?)
for e? in c:
if CANCOMBINE(e, e?):
e? ? BINARY(e, e?, grammar)
APPEND(new, e?)
if CANCOMBINE(e?, e):
e? ? BINARY(e?, e, grammar)
APPEND(new, e?)
for e? in new:
ADD(a, e?)
ADD(c, e)
new? []
Figure 1: The search algorithm.
We set N to 1 in our experiments. For practical
reasons we also include a timeout stopping condi-
tion. If no goal edges are found before the timeout
is reached, a default output is constructed by the fol-
lowing procedure. First, if any two edges in the chart
pass the input check, and the words they contain
constitute the full input set, they are concatenated to
form an output string. Second, when no two edges in
the chart meet the above condition, the largest edge
e? in the chart is chosen. Then edges in the chart are
iterated over in the larger first order, with any edge
that passes the input check with e? concatenated with
e? and e? updated. The final e?, which can be shorter
than the input, is taken as the default output.
4 Model and Features
We use a discriminative linear model to score edges,
where the score of an edge e is calculated using the
global feature vector ?(e) and the parameter vector
1149
~w of the model.
SCORE(e) = ?(e) ? ~w
?(e) represents the counts of individual features
of e. It is computed incrementally as the edge is
built. At each constituent level, the incremental fea-
ture vector is extracted according to the feature tem-
plates from Table 1, and we use the term constituent
level vector ? to refer to it. So for any edge e, ?(e)
consists of features from the top rule of the hierar-
chical structure of e. ?(e) can be written as the sum
of ?(e?) of all recursive subedges e? of e, including
e itself:
?(e) =
?
e??e
?(e?)
The parameter update in Section 5 is in terms of con-
stituent level vectors.
The features in Table 1 represent patterns in-
cluding the constituent label; the head word of the
constituent; the size of the constituent; word, POS
and lexical category N-grams resulting from a bi-
nary combination; and the unary and binary rules
by which the constituent is constructed. They can
be classified roughly into ?parsing? features (those
about the parse structure, such as the binary rule)
and ?generation features? (those about the surface
string, such as word bigrams), although some fea-
tures, such as ?rule + head word + non-head word?,
contain both types of information.
5 The Learning Algorithm
The statistical model plays two important roles in
our system. First, as in typical statistical systems, it
is expected to give a higher score to a more correct
hypothesis. Second, it is also crucial to the speed of
the search algorithm, since the best-first mechanism
relies on a model to find goal hypotheses efficiently.
As an indication of the impact of the model on effi-
ciency, if the model parameters are set to all zeros,
the search algorithm cannot find a result for the first
sentence in the development data within two hours.
We perform training on a corpus of CCG deriva-
tions, where constituents in a gold-standard deriva-
tion serve as gold edges. The training algorithm
runs the decoder on each training example, updat-
ing the model when necessary, until the gold goal
condition feature
constituent + size
all edges constituent + head word
constituent + size + head word
constituent + head POS
constituent + leftmost word
constituent + rightmost word
size > 1 consti. + leftmost POS bigram
consti. + rightmost POS bigram
consti. + lmost POS + rmost POS
the binary rule
the binary rule + head word
rule + head word + non-head word
bigrams resulting from combination
binary POS bigrams resulting from combi.
edges word trigrams resulting from combi.
POS trigrams resulting from combi.
resulting lexical categary trigrams
resulting word + POS bigrams
resulting POS + word bigrams
resulting POS + word + POS trigrams
unary unary rule
edges unary rule + headw
Table 1: Feature template definitions.
edge is recovered. We use the perceptron (Rosen-
blatt, 1958) to perform parameter updates. The tra-
ditional perceptron has been adapted to structural
prediction (Collins, 2002) and search optimization
problems (Daume? III and Marcu, 2005; Shen et al,
2007). Our training algorithm can be viewed as an
adaptation of the perceptron to our best-first frame-
work for search efficiency and accuracy.
We choose to update parameters as soon as the
best edge from the agenda is not a gold-standard
edge. The intuition is that all gold edges are forced
to be above all non-gold edges on the agenda. This
is a strong precondition for parameter updates. An
alternative is to update when a gold-standard edge
falls off the chart, which corresponds to the pre-
condition for parameter updates of Daume? III and
Marcu (2005). However, due to the complexity of
our search task, we found that reasonable training
efficiency cannot be achieved by the weaker alterna-
tives. Our updates lead both to correctness (edges in
the chart are correct) and efficiency (correct edges
are found at the first possible opportunity).
1150
During a perceptron update, an incorrect predic-
tion, corresponding to the current best edge in the
agenda, is penalized, and the corresponding gold
edge is rewarded. However, in our scenario it is not
obvious what the corresponding gold edge should
be, and there are many ways in which the gold
edge could be defined. We investigated a number
of alternatives, for example trying to find the ?best
match? for the incorrect prediction. In practice we
found that the simple strategy of selecting the lowest
scored gold-standard edge in the agenda was effec-
tive, and the results presented in this paper are based
on this method.
After an update, there are at least two alterna-
tive methods to continue. The first is to reinitial-
ize the agenda and chart using the new model, and
continue until the current training example is cor-
rectly predicted. This method is called aggressive
training (Shen et al, 2007). In order to achieve
reasonable efficiency, we adopt a second approach,
which is to continue training without reinitializing
the agenda and chart. Instead, only edges from the
top of the agenda down to the lowest-scoring gold-
standard edge are given new scores according to the
new parameters.
Figure 2 shows pseudocode for the learning al-
gorithm applied to one training example. The ini-
tialization is identical to the test search, except that
the list of goal edges is not maintained. In the main
loop, the best edge e is popped off the agenda. If it
is the gold goal edge, the training for this sentence
finishes. If e is not a gold edge, parameter updates
are performed and the loop is continued with e be-
ing discarded. Only gold edges are pushed onto the
chart throughout the training process.
When updating parameters, the current non-gold
edge (e) is used as the negative example, and the
smallest gold edge in the agenda (minGold) is used
as the corresponding positive example. The model
parameters are updated by adding the constituent
level feature vector (see Section 4) of minGold, and
subtracting the constituent level feature vector of e.
Note that we do not use the global feature vector in
the update, since only the constituent level param-
eter vectors are compatible for edges with different
sizes. After a parameter update, edges are rescored
from the top of the agenda down to minGold.
The training algorithm iterates through all train-
a? INITAGENDA(input)
c? INITCHART()
new? []
while true:
e? POPBEST(a)
if GOLD(e) and GOALTEST(e):
return
if not GOLD(e):
popped? []
n? 0
while n < GOLDCOUNT(a):
e?? POPBEST(a)
APPEND(popped, e?)
if GOLD(e?):
minGold? e?
n? n+ 1
~w? ~w ? ?(e) + ?(minGold)
for e? in popped:
RECOMPUTESCORE(e?)
ADD(a, e?)
for e? in c:
RECOMPUTESCORE(e?)
continue
for e? in UNARY(e, grammar):
APPEND(new, e?)
for e? in c:
if CANCOMBINE(e, e?):
e? ? BINARY(e, e?, grammar)
APPEND(new, e?)
if CANCOMBINE(e?, e):
e? ? BINARY(e?, e, grammar)
APPEND(new, e?)
for e? in new:
ADD(a, e?)
ADD(c, e)
new? []
Figure 2: The learning algorithm.
ing examples N times, and the final parameter vec-
tor is used as the model. In our experiments, N is
chosen according to results on development data.
6 Experiments
We use CCGBank (Hockenmaier and Steedman,
2007) for experimental data. CCGbank is the CCG
version of the Penn Treebank. Sections 02?21 are
1151
used for training, section 00 is used for development
and section 23 for the final test.
Original sentences from CCGBank are trans-
formed into bags of words, with sequence informa-
tion removed, and passed to our system as input
data. The system outputs are compared to the orig-
inal sentences for evaluation. Following Wan et al
(2009), we use the BLEU metric (Papineni et al,
2002) for string comparison. Whilst BLEU is not
an ideal measure of fluency or grammaticality, be-
ing based on n-gram precision, it is currently widely
used for automatic evaluation and allows us to com-
pare directly with existing work (Wan et al, 2009).
In addition to the surface string, our system also
produces the CCG parse given an input bag of words.
The quality of the parse tree can reflect both the
grammaticality of the surface string and the quality
of the trained grammar model. However, there is
no direct way to automatically evaluate parse trees
since output word choice and order can be differ-
ent from the gold-standard. Instead, we indirectly
measure parse quality by calculating the precision of
CCG lexical categories. Since CCG lexical categories
contain so much syntactic information, they provide
a useful measure of parse quality. Again because the
word order can be different, we turn both the output
and the gold-standard into a bag of word/category
pairs, and calculate the percentage of matched pairs
as the lexical category precision.
For fair comparison with Wan et al (2009), we
keep base NPs as atomic units when preparing the
input. Wan et al (2009) used base NPs from Penn
Treebank annotation, while we extract base NPs
from the CCGBbank by taking as base NPs the NPs
that do not recursively contain other NPs. These
base NPs mostly correspond to the base NPs from
the Penn Treebank. In the training data, there are
242,813 Penn Treebank base NPs with an average
size of 1.09, and 216,670 CCGBank base NPs with
an average size of 1.19.
6.1 Development Tests
Table 2 shows a set of development experiment re-
sults after one training iteration. Three different
methods of assigning lexical categories are used.
The first (?dictionary?) is to assign all possible lex-
ical categories to each input word from the dictio-
nary. The lexical category dictionary is built using
Length Timeout
Method Timeout BLEU ratio ratio
0.5s 34.98 84.02 62.26
1s 35.40 85.66 57.87
dictionary 5s 36.27 89.05 45.79
10s 36.45 89.13 42,13
50s 37.07 92.52 32.41
0.5s 36.54 84.26 66.07
1s 37.50 86.69 58.22
? = 0.0001 5s 38.75 90.15 43.23
10s 39.14 91.35 38.36
50s 39.58 93.09 30.53
0.5s 40.87 85.66 61.27
1s 42.04 87.99 53.11
? = 0.075 5s 43.99 91.20 40.30
10s 44.23 92.14 35.70
50s 45.08 93.70 29.43
Table 2: Development tests using various levels of lexical
categories and timeouts, after one training iteration.
the training sections of CCGBank. For each word
occurring more than 20 times in the corpus, the dic-
tionary has an entry with all lexical categories the
word has been seen with. For the rest of the words,
the dictionary maintains an entry for each POS which
contains all lexical categories it has been seen with.
There are on average 26.8 different categories for
each input word by this method.
In practice, it is often unnecessary to leave lexi-
cal category disambiguation completely to the gram-
maticality improvement system. When it is reason-
able to assume that the input sentence for the gram-
maticality improvement system is sufficiently fluent,
a list of candidate lexical categories can be assigned
automatically to each word via supertagging (Clark
and Curran, 2007) on the input sequence. We use
the C&C supertagger1 to assign a set of probable
lexical categories to each input word using the gold-
standard order. When the input is noisy, the accuracy
of a supertagger tends to be lower than when the in-
put is grammatical. One way to address this problem
is to allow the supertagger to produce a larger list
of possible supertags for each input word, and leave
the ambiguity to the grammatical improvement sys-
tem. We simulate the noisy input situation by using
1http://svn.ask.it.usyd.edu.au/trac/candc/wiki/Download.
1152
Precision
dictionary 58.5%
? = 0.0001 59.7%
? = 0.075% 77.0%
Table 3: Lexical category accuracies. Timeout = 5s. 1
training iteration.
a small probability cutoff (?) value in the supertag-
ger, and supertag correctly ordered input sentences
before breaking them into bags of words. With a ?
value of 0.0001, there are 5.4 lexical categories for
each input word in the development test (which is
smaller than the dictionary case).
The average number of lexical categories per
word drops to 1.3 when ? equals 0.075, which is the
value used for parsing newspaper text in Clark and
Curran (2007). We include this ? in our experiments
to compare the effect of different ? levels.
The table shows that the BLEU score of the gram-
maticality improvement system is higher when a su-
per tagger is used, and the higher the ? value, the
better the BLEU score. In practice, the ? value
should be set in accordance with the lack of gram-
maticality and fluency in the input. The dictionary
method can be used when the output is extremely
unreliable, while a small beta value can be used if
the output is almost fluent.
Due to the default output mechanism on timeout,
the system can sometimes fail to produce sentences
that cover all input words. We choose five different
timeout settings between 0.5s to 50s, and compare
the speed/quality tradeoff. In addition to BLEU, we
report the percentage of timeouts and the ratio of the
sum of all output sentence lengths to the sum of all
input sentence lengths.
When the timeout value increases, the BLEU
score generally increases. The main effect of a larger
timeout is the increased possibility of a complete
sentence being found. As the time increases from
0.5s to 50s using the dictionary method, for exam-
ple, the average output sentence length increases
from 84% of the input length to 93%.
Table 3 shows the lexical category accuracies us-
ing the dictionary, and supertagger with different ?
levels. The timeout limit is set to 5 seconds. As
the lexical category ambiguity decreases, the accu-
Length dictionary ? = 0.0001 ? = 0.075
? 5 75.65 89.42 92.64
? 10 57.74 66.00 78.54
? 20 42.44 48.89 58.23
? 40 37.48 40.32 46.00
? 80 36.50 39.01 44.26
all 36.27 38.75 43.99
Table 4: BLEU scores measured on different lengths on
development data. Timeout = 5s. 1 training iteration.
racy increases. The best lexical category accuracy
of 77% is achieved when using a supertagger with
a ? level 0.075, the level for which the least lexical
category disambiguation is required. However, com-
pared to the 93% lexical category accuracy of a CCG
parser (Clark and Curran, 2007), which also uses a ?
level of 0.075 for the majority of sentences, the ac-
curacy of our grammaticality improvement system
is much lower. The lower score reflects the lower
quality of the parse trees produced by our system.
Besides the difference in the algorithms themselves,
one important reason is the much higher complexity
of our search problem.
Table 4 shows the BLEU scores measured by dif-
ferent sizes of input. We also give some example
output sentences in Figure 3. It can be seen from
the table that the BLEU scores are higher when the
size of input is smaller. For sentences shorter than
20 words, our system generally produces reason-
ably fluent and grammatical outputs. For longer sen-
tences, the grammaticality drops. There are three
possible reasons. First, larger constituents require
more steps to construct. The model and search algo-
rithm face many more ambiguities, and error propa-
gation is more severe. Second, the search algorithm
often fails to find a goal hypothesis before timeout,
and a default output that is less grammatical than
a complete constituent is constructed. Long sen-
tences have comparatively more input words uncov-
ered in the output. Third, the upper bound is not 100,
and presumably lower for longer sentences, because
there are many ways to generate a grammatical sen-
tence given a bag of words. For example, the bag
{ cats, chase, dogs } can produce two equally fluent
and grammatical sentences.
The relatively low score for long sentences is un-
1153
(dictionary) our products There is no asbestos in now .
(? = 0.0001) in our products now There is no asbestos .
(? = 0.075) There is no asbestos in our products now .
(dictionary) No price for the new shares has been set .
(both ?) No price has been set for the new shares .
(all) Federal Data Corp. got a $ 29.4 million Air Force contract for
intelligence data handling .
(dictionary) was a nonexecutive director of Rudolph Agnew and former chairman
of Consolidated Gold Fields PLC , this British industrial
conglomerate , 55 years old . named
(? = 0.0001) old Consolidated Gold Fields PLC , was named 55 years , former
chairman of Rudolph Agnew and a nonexecutive director of this
British industrial conglomerate .
(? = 0.075) Consolidated Gold Fields PLC , 55 years old , was named former
chairman of Rudolph Agnew and a nonexecutive director of this
British industrial conglomerate .
(dictionary) McDermott International Inc. said its Babcock & Wilcox unit
completed the sale of its Bailey Controls Operations for
Finmeccanica S.p . A. to $ 295 million .
(? = 0.0001) $ 295 million McDermott International Inc. for the sale of
its Babcock & Wilcox unit said its Bailey Controls Operations
completed to Finmeccanica S.p . A. .
(? = 0.075) McDermott International Inc. said its Bailey Controls
Operations completed the sale of Finmeccanica S.p . A. for its
Babcock & Wilcox unit to $ 295 million .
Figure 3: Example outputs on development data.
likely to be such a problem in practice, because
the base system (e.g. an SMT system) is likely to
produce sentences with locally fluent subsequences.
When fluent local phrases in the input are treated as
atomic units, the effective sentence length is shorter.
All the above development experiments were per-
formed using only one training iteration. Figure 4
shows the effect of different numbers of training it-
erations. For the final test, based on the graphs in
Figure 4, we chose the training iterations to be 8, 6
and 4 for the dictionary, ? = 0.0001 and ? = 0.075
methods, respectively.
6.2 Final Accuracies
Table 5 shows the final results of our system, to-
gether with the MST-based (?Wan 2009 CLE?)
and assignment-based (?Wan 2009 AB?) systems
of Wan et al (2009). Our system outperforms the
BLEU
Wan 2009 CLE 26.8
Wan 2009 AB 33.7
This paper dictionary 40.1
This paper ? = 0.0001 43.2
This paper ? = 0.075 50.1
Table 5: Final accuracies.
dependency grammar-based systems, and using a
supertagger with small ? value produces the best
BLEU. Note that through the use of a supertagger,
we are no longer assuming that the input is a bag of
words without any order, and therefore only the dic-
tionary results are directly comparable with Wan et
al. (2009)2.
2We also follow Wan et al (2009) by assuming each word is
associated with its POS tag.
1154
 0.34
 0.36
 0.38
 0.4
 0.42
 0.44
 0.46
 0.48
 1  2  3  4  5  6  7  8
BL
EU
training iteration
0.075
0.0001
dictionary
Figure 4: The effect of training iterations.
7 Related Work
Both Wan et al (2009) and our system use approx-
imate search to solve the problem of input word or-
dering. There are three differences. First, Wan et
al. use a dependency grammar to model grammati-
cality, while we use CCG. Compared to dependency
trees, CCG has stronger category constraints on the
parse structure. Moreover, CCG allows us to reduce
the ambiguity level of the search algorithm through
the assignment of possible lexical categories to input
words, which is useful when the input has a basic
degree of fluency, as is often the case in a grammat-
icality improvement task.
Second, we use learning to optimise search in or-
der to explore a large search space. In contrast, Wan
et al break the search problem into a sequence of
sub tasks and use greedy search to connect them.
Finally, in addition to ordering, our algorithm fur-
ther allows word selection. This gives our system
the flexibility to support word insertion and deletion.
White (2004) describes a system that performs
CCG realization using best-first search. The search
process of our algorithm is similar to his work.
The problem we solve is different from realization,
which takes an input in logical form and produces
a corresponding sentence. Without constraints, the
word order ambiguities can be much larger with a
bag of words, and we use learning to guide our
search algorithm. Espinosa et al (2008) apply hy-
pertagging to logical forms to assign lexical cate-
gories for realization. White and Rajkumar (2009)
further use perceptron reranking on N-best outputs
to improve the quality.
The use of perceptron learning to improve search
has been proposed in guided learning for easy-first
search (Shen et al, 2007) and LaSO (Daume? III and
Marcu, 2005). LaSO is a general framework for
various search strategies. Our learning algorithm is
similar to LaSO with best-first inference, but the pa-
rameter updates are different. In particular, LaSO
updates parameters when all correct hypotheses are
lost, but our algorithm makes an update as soon as
the top item from the agenda is incorrect. Our algo-
rithm updates the parameters using a stronger pre-
condition, because of the large search space. Given
an incorrect hypothesis, LaSO finds the correspond-
ing gold hypothesis for perceptron update by con-
structing its correct sibling. In contrast, our algo-
rithm takes the lowest scored gold hypothesis cur-
rently in the agenda to avoid updating parameters
for hypotheses that may have not been constructed.
Our parameter update strategy is closer to the
guided learning mechanism for the easy-first algo-
rithm of Shen et al (2007), which maintains a queue
of hypotheses during search, and performs learning
to ensure that the highest scored hypothesis in the
queue is correct. However, in easy-first search, hy-
potheses from the queue are ranked by the score of
their next action, rather than the hypothesis score.
Moreover, Shen et al use aggressive learning and
regenerate the queue after each update, but we per-
form non-agressive learning, which is faster and is
more feasible for our complex search space. Similar
methods to Shen et al (2007) have also been used
in Shen and Joshi (2008) and Goldberg and Elhadad
(2010).
8 Conclusion
We proposed a grammaticality improvement system
using CCG, and evaluated it using a standard input
word ordering task. Our system gave higher BLEU
scores than the dependency-based system of Wan et
al. (2009). We showed that the complex search prob-
lem can be solved effectively using guided learning
for best-first search.
Potential improvements to our system can be
made in several areas. First, a large scale lan-
guage model can be incorporated into our model in
the search algorithm, or through reranking. Sec-
ond, a heuristic future cost (e.g. Varges and Mel-
1155
lish (2010)) can be considered for each hypothesis
so that it also considers the words that have not been
used, leading to better search. Future work also in-
cludes integration with an SMT system, where con-
tent word selection will be applicable.
Acknowledgements
We thank Graeme Blackwood, Bill Byrne, Adria` de
Gispert, Stephen Wan and the anonymous reviewers
for their discussions and suggestions. Yue Zhang
and Stephen Clark are supported by the European
Union Seventh Framework Programme (FP7-ICT-
2009-4) under grant agreement no. 247762.
References
Srinivas Bangalore, Owen Rambow, and Steve Whittaker.
2000. Evaluation metrics for generation. In Proceed-
ings of the First International Natural Language Gen-
eration Conference (INLG2000), Mitzpe, pages 1?8.
Graeme Blackwood, Adria` de Gispert, and William
Byrne. 2010. Fluency constraints for minimum bayes-
risk decoding of statistical machine translation lattices.
In Proceedings of the 23rd International Conference
on Computational Linguistics (Coling 2010), pages
71?79, Beijing, China, August. Coling 2010 Organiz-
ing Committee.
Sharon A. Caraballo and Eugene Charniak. 1998. New
figures of merit for best-first probabilistic chart pars-
ing. Comput. Linguist., 24:275?298, June.
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG and
log-linear models. Computational Linguistics,
33(4):493?552.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
EMNLP, pages 1?8, Philadelphia, USA.
Hal Daume? III and Daniel Marcu. 2005. Learning as
search optimization: approximate large margin meth-
ods for structured prediction. In ICML, pages 169?
176.
Dominic Espinosa, Michael White, and Dennis Mehay.
2008. Hypertagging: Supertagging for surface real-
ization with CCG. In Proceedings of ACL-08: HLT,
pages 183?191, Columbus, Ohio, June. Association
for Computational Linguistics.
Yoav Goldberg and Michael Elhadad. 2010. An effi-
cient algorithm for easy-first non-directional depen-
dency parsing. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Linguis-
tics, pages 742?750, Los Angeles, California, June.
Association for Computational Linguistics.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: A corpus of CCG derivations and dependency
structures extracted from the Penn Treebank. Compu-
tational Linguistics, 33(3):355?396.
Julia Hockenmaier. 2003. Parsing with generative mod-
els of predicate-argument structure. In Proceedings of
the 41st Meeting of the ACL, pages 359?366, Sapporo,
Japan.
Kevin Knight. 2007. Automatic language translation
generation help needs badly. In MT Summit XI Work-
shop on Using Corpora for NLG: Keynote Address.
Phillip Koehn. 2010. Statistical Machine Translation.
Cambridge University Press.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311?318, Philadelphia, Pennsylva-
nia, USA, July. Association for Computational Lin-
guistics.
Stefan Riezler, Tracy H. King, Ronald M. Kaplan,
Richard Crouch, John T. III Maxwell, and Mark John-
son. 2002. Parsing the Wall Street Journal using
a lexical-functional grammar and discriminative esti-
mation techniques. In Proceedings of 40th Annual
Meeting of the Association for Computational Lin-
guistics, pages 271?278, Philadelphia, Pennsylvania,
USA, July. Association for Computational Linguistics.
F. Rosenblatt. 1958. The perceptron: A probabilistic
model for information storage and organization in the
brain. Psychological Review, 65:386?408.
Libin Shen and Aravind Joshi. 2008. LTAG dependency
parsing with bidirectional incremental construction.
In Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, pages 495?
504, Honolulu, Hawaii, October. Association for Com-
putational Linguistics.
Libin Shen, Giorgio Satta, and Aravind Joshi. 2007.
Guided learning for bidirectional sequence classifica-
tion. In Proceedings of ACL, pages 760?767, Prague,
Czech Republic, June.
Mark Steedman. 2000. The Syntactic Process. The MIT
Press, Cambridge, Mass.
Sebastian Varges and Chris Mellish. 2010. Instance-
based natural language generation. Natural Language
Engineering, 16(3):309?346.
Stephen Wan, Mark Dras, Robert Dale, and Ce?cile Paris.
2009. Improving grammaticality in statistical sen-
tence generation: Introducing a dependency spanning
tree algorithm with an argument satisfaction model.
1156
In Proceedings of the 12th Conference of the Euro-
pean Chapter of the ACL (EACL 2009), pages 852?
860, Athens, Greece, March. Association for Compu-
tational Linguistics.
Michael White and Rajakrishnan Rajkumar. 2009. Per-
ceptron reranking for CCG realization. In Proceedings
of the 2009 Conference on Empirical Methods in Nat-
ural Language Processing, pages 410?419, Singapore,
August. Association for Computational Linguistics.
Michael White. 2004. Reining in CCG chart realization.
In Proc. INLG-04, pages 182?191.
1157
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1427?1432,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Detecting Compositionality of Multi-Word Expressions using Nearest
Neighbours in Vector Space Models
Douwe Kiela
University of Cambridge
Computer Laboratory
douwe.kiela@cl.cam.ac.uk
Stephen Clark
University of Cambridge
Computer Laboratory
stephen.clark@cl.cam.ac.uk
Abstract
We present a novel unsupervised approach to
detecting the compositionality of multi-word
expressions. We compute the compositional-
ity of a phrase through substituting the con-
stituent words with their ?neighbours? in a se-
mantic vector space and averaging over the
distance between the original phrase and the
substituted neighbour phrases. Several meth-
ods of obtaining neighbours are presented.
The results are compared to existing super-
vised results and achieve state-of-the-art per-
formance on a verb-object dataset of human
compositionality ratings.
1 Introduction
Multi-word expressions (MWEs) are defined as ?id-
iosyncratic interpretations that cross word bound-
aries? (Sag et al, 2002). They tend to have a
standard syntactic structure but are often semanti-
cally non-compositional; i.e. their meaning is not
fully determined by their syntactic structure and the
meanings of their constituents. A classic example
is kick the bucket, which means to die rather than to
hit a bucket with the foot. These types of expres-
sions account for a large proportion of day-to-day
language interactions (Schuler and Joshi, 2011) and
present a significant problem for natural language
processing systems (Sag et al, 2002).
This paper presents a novel unsupervised ap-
proach to detecting the compositionality of MWEs,
specifically of verb-noun collocations. The idea is
that we can recognize compositional phrases by sub-
stituting related words for constituent words in the
phrase: if the result of a substitution yields a mean-
ingful phrase, its individual constituents are likely to
contribute toward the overall meaning of the phrase.
Conversely, if a substitution yields a non-sensical
phrase, its constituents are likely to contribute less
or not at all to the overall meaning of the phrase.
For the phrase eat her hat, for example, we might
consider the following substituted phrases:
1. consume her hat
2. eat her trousers
Both phrases are semantically anomalous, implying
that eat hat is a highly non-compositional verb-noun
collocation. Following a similar procedure for eat
apple, however, would not lead to an anomaly: con-
sume apple and eat pear are perfectly meaningful,
leading us to believe that eat apple is compositional.
In the context of distributional models, this idea
can be formalised in terms of vector spaces:
the average distance between a phrase
vector and its substituted phrase vectors is
related to its compositionality.
Since we are relying on the relative distances of
phrases in semantic space, we require a method
for computing vectors for phrases. We experi-
mented with a number of composition operators
from Mitchell and Lapata (2010), in order to com-
pose constituent word vectors into phrase vectors.
The relation between phrase vectors and substituted
phrase vectors is most pronounced in the case of
1427
pointwise multiplication, which has the effect of
placing semantically anomalous phrases relatively
close together in space (since the vectors for the con-
stituent words have little in common), whereas the
semantically meaningful phrases are further apart.
This implies that compositional phrases are less sim-
ilar to their neighbours, which is to say that the
greater the average distance between a phrase vec-
tor and its substituted phrase vectors, the greater its
compositionality.
The contribution of this short focused research pa-
per is a novel approach to detecting the composition-
ality of multi-word expressions that makes full use
of the ability of semantic vector space models to cal-
culate distances between words and phrases. Using
this unsupervised approach, we achieve state-of-the-
art performance in a direct comparison with existing
supervised methods.
2 Dataset and Vectors
The verb-noun collocation dataset from Venkatapa-
thy and Joshi (2005), which consists of 765 verb-
object pairs with human compositionality ratings,
was used for evaluation. Venkatapathy & Joshi used
a support vector machine (SVM) to obtain a Spear-
man ?s correlation of 0.448. They employed a va-
riety of features ranging from frequency to LSA-
derived similarity measures and used 10% of the
dataset as training data with tenfold cross-validation.
McCarthy et al (2007) used the same dataset and ex-
panded on the original approach by adding WordNet
and distributional prototypes to the SVM, achieving
a ?s correlation of 0.454.
The distributional vectors for our experiments
were constructed from the ukWaC corpus (Baroni
et al, 2009). Vectors were obtained using a stan-
dard window method (with a window size of 5) and
the 50,000 most frequent context words as features,
with stopwords removed. We also experimented
with syntax-based co-occurrence features extracted
from a dependency-parsed version of ukWaC, but
in agreement with results obtained by Schulte im
Walde et al (2013) for predicting compositional-
ity in German, the window-based co-occurrence
method produced better results.
We tried several weighting schemes from the liter-
ature, such as t-test (Curran, 2004), positive mutual
information (Bullinaria and Levy, 2012) and the ra-
tio of the probability of the context word given the
target word1 to the context word?s overall probabil-
ity (Mitchell and Lapata, 2010). We found that a
tf-idf variant called LTU yielded the best results, de-
fined as follows (Reed et al, 2006):
wij =
(log(fij) + 1.0) log(Nnj )
0.8 + 0.2? |context word|
|avg context word|
where fij is the number of times that the target word
and context word co-occur in the same window, nj
is the context word frequency, N is the total fre-
quency and |context word| is the total number of oc-
currences of a context word. Distance is calculated
using the standard cosine measure:
dist(v1, v2) = 1?
v1 ? v2
|v1||v2|
where v1 and v2 are vectors in the semantic vector
space model.
3 Finding Neighbours and Computing
Compositionality
We experimented with two different ways of obtain-
ing neighbours for the constituent words in a phrase.
Since vector space models lend themselves naturally
to similarity computations, one way to get neigh-
bours is to take the k-most similar vectors from a
similarity matrix. This approach is straightforward,
but has some potential drawbacks: it assumes that
we have a large number of vectors to select neigh-
bours from, and becomes computationally expensive
when the number of neighbours is increased.
An alternative source for obtaining neighbours is
the lexical database WordNet (Fellbaum, 1998). We
define neighbours as siblings in the hypernym hier-
archy, so that the neighbours of a word can be found
by taking the hyponyms of its hypernyms. Word-
Net alo allows us to extract only neighbours of the
same grammatical type (yielding noun neighbours
for nouns and verb neighbours for verbs, for exam-
ple). Since not every word has the same number
of neighbours in WordNet, we use only the first k
1We use target word to refer to the word for which a vector
is being constructed.
1428
neighbours, which means that the neighbours have
to be ranked. An obvious ranking method is to use
the frequency with which each neighbour co-occurs
with the other constituent(s) of the same phrase. For
example, for all the WordNet neighbours of eat (for
all senses of eat), we count the co-occurrences with
hat in a given window size and rank them accord-
ingly. This ranking method also has the desirable
side-effect of performing some word sense disam-
biguation, at least in some cases. For example, the
highly ranked neighbours of apple for eat apple are
likely to be items of food, and not (inedible) trees
(apple is also a tree in WordNet).
In order to obtain frequency-ranked neighbours,
we used the ukWaC corpus with a window size of
5. One reason for having multiple neighbours is that
it allows us to correct for word sense disambigua-
tion errors (as mentioned above), since averaging
over results for several neighbours reduces the im-
pact of including incorrect senses. For example, the
first 20 neighbours of eat, ranked by co-occurrence
frequency with all the objects of eat in the dataset,
are:
eat use consume drink sample smoke
swallow spend break hit save afford burn
partake dine breakfast worry damage de-
plete drug
One problem with the evaluation dataset is that
it does not solely consist of verb-noun pairs: 84
phrases contain pronouns, while there are also sev-
eral examples containing words that WordNet con-
siders to be adjectives rather than nouns. This prob-
lem was mitigated by part-of-speech tagging the
dataset. As neighbours for pronouns (which are not
included in WordNet), we used the other pronouns
present in the dataset. For the remaining words,
we included the part-of-speech when looking up the
word in WordNet.
3.1 Average distance compositionality score
We considered several different ways of construct-
ing phrasal vectors. We chose not to use the com-
positional models of Baroni and Zamparelli (2010)
and Socher et al (2011) because we believe that it is
important that our methods are completely unsuper-
vised and do not require any initial learning phase.
Hence, we experimented with different ways of con-
structing phrasal vectors according to Mitchell and
Lapata (2010) and found that pointwise multiplica-
tion  worked best in our experiments. Thus, we
define the composed vector
?????
eat hat as:
??
eat
??
hat
We can now compute a compositionality score sc by
averaging the distance between the original phrase
vector and its substituted neighbour phrase vectors
via the following formula:
sc(
?????
eat hat) =
1
2k
(
k?
i=1
dist(
??
eat
??
hat,
??
eat
????????
neighbouri) +
k?
j=1
dist(
??
eat
??
hat,
????????
neighbourj 
??
hat))
We also experimented with substituting only for
the noun or the verb, and in fact found that only tak-
ing neighbours for the verb yields better results:
sc(
?????
eat hat) =
1
k
k?
j=1
dist(
??
eat
??
hat,
????????
neighbourj 
??
hat)
To illustrate the method, consider the collocations
take breath and lend money. The annotators as-
signed these phrases a compositionality score of 1
out of 6 and 6 out of 6, respectively, meaning that the
former is non-compositional and the latter is com-
positional. The distances between the first ten verb-
substituted phrases and the original phrase, together
with the average distance, are shown in Table 1 and
Table 2.
Substituting the verb in the non-compositional
phrase yields semantically anomalous vectors,
which leads to very small changes in the distance
between it and the original phrase vector. This is a
result of using pointwise multiplication, where over-
lapping components are stressed: since the vectors
for take and breath have little overlap outside of
1429
Neighbour Dist
get breath 0.049
find breath 0.051
use breath 0.050
work breath 0.060
hold breath 0.094
run breath 0.079
carry breath 0.076
look breath 0.065
play breath 0.071
buy breath 0.100
AvgDist 0.069
Table 1: Example take breath
Neighbour Dist
pay money 0.446
put money 0.432
bring money 0.405
provide money 0.442
owe money 0.559
sell money 0.404
cost money 0.482
look money 0.425
distribute money 0.544
offer money 0.428
AvgDist 0.457
Table 2: Example lend money
the idiomatic sense in take breath, its neighbour-
substituted phrases also have little overlap, result-
ing in a smaller change in distance upon substitu-
tion. Conversely, substituting the verb in the com-
positional phrase yields meaningful vectors, putting
them in locations in semantic vector space which are
sufficiently far apart to distinguish them from the
non-compositional cases.
4 Results
Results are given for the two methods of obtaining
neighbours: via frequency-ranked WordNet neigh-
bours and via vector space neighbours. The com-
positionality score was computed by using only the
verb, only the noun, or both constituent neighbours
in the substituted phrase vectors.
System ?s
Venkatapathy and Joshi (2005) 0.447
McCarthy et al (2007) 0.454
AvgDist VSM neighbours-both 0.131
AvgDist VSM neighbours-verb 0.420
AvgDist VSM neighbours-noun 0.245
AvgDist WN-ranked neighbours-both 0.165
AvgDist WN-ranked neighbours-verb 0.461
AvgDist WN-ranked neighbours-noun 0.169
Table 3: Spearman ?s results
The results are compared with the scores reported
in Venkatapathy and Joshi (2005) and McCarthy et
al. (2007), which were achieved using SVMs with a
wide variety of features. Values of 1 ? k ? 20 were
tried. If a phrase has fewer than k neighbours be-
cause not enough neighbours have been found to co-
occur with the other constituent, we use all of them.
The results for k = 20 are reported here because
that gave the best overall score. The dataset has an
inter-annotator agreement of Kendall?s ? of 0.61 and
a Spearman ?s of 0.71 and all reported differences
in values are highly significant. Table 3 gives the
results.
Note that, even though the current approach is un-
supervised (in terms of not having access to compo-
sitionality ratings during training, although it does
rely on WordNet), it outperforms SVMs that require
an ensemble of complex feature sets (some of which
are also based on WordNet).
It is interesting to observe that the state-of-the-art
performance is reached when only using the verb?s
neighbours to compute substituted phrase vectors.
One might initially expect this not to be the case,
since e.g. eat trousers, where the noun has been
substituted, does not make a lot of sense either ?
which we would expect to be informative for de-
termining compositionality. There are two possi-
ble explanations for this, which might be at play
simultaneously: since our dataset consists of verb-
object pairs, the verb constituent is always the head
word of the phrase, and the dataset contains several
so-called ?light verbs?, which have little semantic
content of their own. Head words have been found
to have a higher impact on compositionality scores
for compound nouns: Reddy et al (2011) weighted
1430
the contribution of individual constituents in such a
way that the modifier?s contribution is included but
is weighted less highly than the head?s contribution,
which led to an improvement in performance. Our
results might be improved by weighting the contri-
bution of constituent words in a similar fashion, and
by more closely examining the impact of light verbs
for the compositionality of a phrase.
5 Related Work
The past decade has seen extensive work on compu-
tational and statistical methods in detecting the com-
positionality of MWEs (Lin, 1999; Schone and Ju-
rafsky, 2001; Katz and Giesbrecht, 2006; Sporleder
and Li, 2009; Biemann and Giesbrecht, 2011).
Many of these methods rely on distributional mod-
els and vector space models (Schu?tze, 1993; Tur-
ney and Pantel, 2010; Erk, 2012). Work has been
done on different types of phrases, including work
on particle verbs (McCarthy et al, 2003; Bannard
et al, 2003), verb-noun collocations (Venkatapathy
and Joshi, 2005; McCarthy et al, 2007), adjective-
noun combinations (Vecchi et al, 2011) and noun-
noun compounds (Reddy et al, 2011), as well as on
languages other than English (Schulte im Walde et
al., 2013). Recent developments in distributional
compositional models (Widdows, 2008; Mitchell
and Lapata, 2010; Baroni and Zamparelli, 2010; Co-
ecke et al, 2010; Socher et al, 2011) have opened
up a number of possibilities for constructing vectors
for phrases, which have also been applied to com-
positionality tests (Giesbrecht, 2009; Kochmar and
Briscoe, 2013).
This paper takes that work a step further: by con-
structing phrase vectors and evaluating these vectors
on a dataset of human compositionality ratings, we
show that existing compositional models allow us to
detect compositionality of multi-word expressions
in a straightforward and intuitive manner.
6 Conclusion
We have presented a novel unsupervised approach
that can be used to detect the compositionality of
multi-word expressions. Our results show that the
underlying intuition appears to be sound: substitut-
ing neighbours may lead to meaningful or meaning-
less phrases depending on whether or not the phrase
is compositional. This can be formalized in vec-
tor space models to obtain compositionality scores
by computing the average distance to the original
phrase?s substituted neighbour phrases. In this short
focused research paper, we show that, depending on
how we obtain neighbours, we are able to achieve
a higher performance than that achieved by super-
vised methods which rely on a complex feature set
and support vector machines.
Acknowledgments
This work has been supported by EPSRC grant
EP/I037512/1. The authors would like to thank Di-
ana McCarthy for providing the dataset; and Ed
Grefenstette, Eva Maria Vecchi, Laura Rimell and
Tamara Polajnar and the anonymous reviewers for
their helpful comments.
References
Colin Bannard, Timothy Baldwin, and Alex Lascarides.
2003. A statistical approach to the semantics of verb-
particles. In Proceedings of the ACL 2003 Workshop
on Multiword expressions: analysis, acquisition and
treatment, MWE 03.
M. Baroni and R. Zamparelli. 2010. Nouns are vectors,
adjectives are matrices: Representing adjective-noun
constructions in semantic space. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP ?10, pages 1183?1193.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and
Eros Zanchetta. 2009. The wacky wide web: A
collection of very large linguistically processed web-
crawled corpora. Language Resources and Evalua-
tion, 43(3):209?226.
Chris Biemann and Eugenie Giesbrecht. 2011. Disco-
11: Proceedings of the workshop on distributional se-
mantics and compositionality.
John A. Bullinaria and Joseph P. Levy. 2012. Extracting
Semantic Representations from Word Co-occurrence
Statistics: Stop-lists, Stemming and SVD. Behavior
Research Methods, 44:890?907.
Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen Clark.
2010. Mathematical foundations for a compositional
distributional model of meaning. In J. van Bentham,
M. Moortgat, and W. Buszkowski, editors, Linguistic
Analysis (Lambek Festschrift), volume 36, pages 345?
384.
James Curran. 2004. From Distributional to Semantic
Similarity. Ph.D. thesis, University of Edinburgh.
1431
Katrin Erk. 2012. Vector space models of word meaning
and phrase meaning: A survey. Language and Lin-
guistics Compass, 6(10):635?653.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. Bradford Books.
Eugenie Giesbrecht. 2009. In search of semantic com-
positionality in vector spaces. In Sebastian Rudolph,
Frithjof Dau, and SergeiO. Kuznetsov, editors, Con-
ceptual Structures: Leveraging Semantic Technolo-
gies, volume 5662 of Lecture Notes in Computer Sci-
ence, pages 173?184. Springer Berlin Heidelberg.
Graham Katz and Eugenie Giesbrecht. 2006. Automatic
identification of non-compositional multi-word ex-
pressions using latent semantic analysis. In Proceed-
ings of the Workshop on Multiword Expressions: Iden-
tifying and Exploiting Underlying Properties, MWE
?06, pages 12?19.
Ekaterina Kochmar and Ted Briscoe. 2013. Capturing
Anomalies in the Choice of Content Words in Com-
positional Distributional Semantic Space. In Recent
Advances in Natural Language Processing.
Dekang Lin. 1999. Automatic identification of non-
compositional phrases. In Proceedings of the 37th
annual meeting of the Association for Computational
Linguistics on Computational Linguistics, ACL ?99,
pages 317?324.
Diana McCarthy, Bill Keller, and John Carroll. 2003.
Detecting a continuum of compositionality in phrasal
verbs. In Proceedings of the ACL 2003 workshop
on Multiword expressions: analysis, acquisition and
treatment - Volume 18, MWE ?03, pages 73?80.
Diana McCarthy, Sriram Venkatapathy, and Aravind
Joshi. 2007. Detecting compositionality of verb-
object combinations using selectional preferences. In
Proceedings of the 2007 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL), pages 369?379.
Jeff Mitchell and Mirella Lapata. 2010. Composition in
distributional models of semantics. Cognitive Science,
34(8):1388?1429.
Siva Reddy, Diana McCarthy, and Suresh Manandhar.
2011. An empirical study on compositionality in com-
pound nouns. In Proceedings of The 5th Interna-
tional Joint Conference on Natural Language Process-
ing 2011 (IJCNLP 2011), Thailand.
J.W. Reed, Y. Jiao, T.E. Potok, B.A. Klump, M.T. El-
more, and A.R. Hurson. 2006. TF-ICF: A new term
weighting scheme for clustering dynamic data streams.
In Machine Learning and Applications, 2006. ICMLA
?06. 5th International Conference on, pages 258?263.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann A.
Copestake, and Dan Flickinger. 2002. Multiword ex-
pressions: A Pain in the Neck for NLP. In Proceed-
ings of the Third International Conference on Com-
putational Linguistics and Intelligent Text Processing,
CICLing ?02, pages 1?15.
Patrick Schone and Daniel Jurafsky. 2001. Is
knowledge-free induction of multiword unit dictionary
headwords a solved problem? In Proceedings of
Empirical Methods in Natural Language Processing,
EMNLP ?01.
William Schuler and Aravind K. Joshi. 2011. Tree-
rewriting models of multi-word expressions. In Pro-
ceedings of the Workshop on Multiword Expressions:
from Parsing and Generation to the Real World, MWE
?11, pages 25?30.
Sabine Schulte im Walde, Stefan Mu?ller, and Stephen
Roller. 2013. Exploring Vector Space Models to
Predict the Compositionality of German Noun-Noun
Compounds. In Proceedings of the 2nd Joint Confer-
ence on Lexical and Computational Semantics, pages
255?265, Atlanta, GA.
Hinrich Schu?tze. 1993. Word space. In Advances in
Neural Information Processing Systems 5, pages 895?
902. Morgan Kaufmann.
Richard Socher, Cliff Lin, Andrew Y. Ng, and Christo-
pher D. Manning. 2011. Parsing Natural Scenes
and Natural Language with Recursive Neural Net-
works. In The 28th International Conference on Ma-
chine Learning, ICML 2011.
Caroline Sporleder and Linlin Li. 2009. 2009. unsuper-
vised recognition of literal and non-literal use of id-
iomatic expressions. In Proceedings of the 12th Con-
ference of the European Chapter of the ACL, EACL
?09.
Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning: vector space models of semantics.
J. Artif. Int. Res., 37(1):141?188, January.
Eva Maria Vecchi, Marco Baroni, and Roberto Zampar-
elli. 2011. (linear) maps of the impossible: Capturing
semantic anomalies in distributional space. In Pro-
ceedings of the Workshop on Distributional Seman-
tics and Compositionality, pages 1?9, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
Sriram Venkatapathy and Aravind K. Joshi. 2005. Mea-
suring the relative compositionality of verb-noun (v-n)
collocations by integrating features. In Proceedings of
the conference on Human Language Technology and
Empirical Methods in Natural Language Processing,
HLT ?05, pages 899?906.
Dominic Widdows. 2008. Semantic vector products:
Some initial investigations. In Second AAAI Sympo-
sium on Quantum Interaction, Oxford.
1432
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1036?1046,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Reducing Dimensions of Tensors in Type-Driven Distributional Semantics
Tamara Polajnar Luana F
?
ag
?
ar
?
as?an Stephen Clark
Computer Laboratory
University of Cambridge
Cambridge, UK
first.last@cl.cam.ac.uk
Abstract
Compositional distributional semantics is
a subfield of Computational Linguistics
which investigates methods for represent-
ing the meanings of phrases and sen-
tences. In this paper, we explore im-
plementations of a framework based on
Combinatory Categorial Grammar (CCG),
in which words with certain grammatical
types have meanings represented by multi-
linear maps (i.e. multi-dimensional arrays,
or tensors). An obstacle to full implemen-
tation of the framework is the size of these
tensors. We examine the performance of
lower dimensional approximations of tran-
sitive verb tensors on a sentence plausi-
bility/selectional preference task. We find
that the matrices perform as well as, and
sometimes even better than, full tensors,
allowing a reduction in the number of pa-
rameters needed to model the framework.
1 Introduction
An emerging subfield of computational linguis-
tics is concerned with learning compositional dis-
tributional representations of meaning (Mitchell
and Lapata, 2008; Baroni and Zamparelli, 2010;
Coecke et al., 2010; Grefenstette and Sadrzadeh,
2011; Clarke, 2012; Socher et al., 2012; Clark,
2013). The advantage of such representations lies
in their potential to combine the benefits of dis-
tributional approachs to word meaning (Sch?utze,
1998; Turney and Pantel, 2010) with the more tra-
ditional compositional methods from formal se-
mantics (Dowty et al., 1981). Distributional repre-
sentations have the properties of robustness, learn-
ability from data, ease of handling ambiguity,
and the ability to represent gradations of mean-
ing; whereas compositional models handle the un-
bounded nature of natural language, as well as
providing established accounts of logical words,
quantification, and inference.
One promising approach which attempts to
combine elements of compositional and distribu-
tional semantics is by Coecke et al. (2010). The
underlying idea is to take the type-driven approach
from formal semantics ? in particular the idea
that the meanings of complex grammatical types
should be represented as functions ? and ap-
ply it to distributional representations. Since the
mathematics of distributional semantics is pro-
vided by linear algebra, a natural set of functions
to consider is the set of linear maps. Coecke et
al. recognize that there is a natural correspon-
dence from complex grammatical types to tensors
(multi-linear maps), so that the meaning of an ad-
jective, for example, is represented by a matrix (a
2nd-order tensor)
1
and the meaning of a transitive
verb is represented by a 3rd-order tensor.
Coecke et al. use the grammar of pregroups
as the syntactic machinery to construct distribu-
tional meaning representations, since both pre-
groups and vector spaces can be seen as exam-
ples of the same abstract structure, which leads
to a particularly clean mathematical description of
the compositional process. However, the approach
applies more generally, for example to other forms
of categorial grammar, such as Combinatory Cate-
gorial Grammar (Steedman, 2000; Maillard et al.,
2014), and also to phrase-structure grammars in a
way that a formal linguist would recognize (Ba-
roni et al., 2014). Clark (2013) provides a descrip-
tion of the tensor-based framework aimed more at
computational linguists, relying only on the math-
ematics of multi-linear algebra rather than the cat-
egory theory used in Coecke et al. (2010). Sec-
tion 2 repeats some of this description.
A major open question associated with the
tensor-based semantic framework is how to learn
1
This same insight lies behind the work of Baroni and
Zamparelli (2010).
1036
the tensors representing the meanings of words
with complex types, such as verbs and adjec-
tives. The framework is essentially a composi-
tional framework, providing a recipe for how to
combine distributional representations, but leav-
ing open what the underlying vector spaces are and
how they can be acquired. One significant chal-
lenge is an engineering one: in a wide-coverage
grammar, which is able to handle naturally occur-
ring text, there will be a) a large lexicon with many
word-category pairs requiring tensor representa-
tions; and b) many higher-order tensors with large
numbers of parameters which need to be learned.
In this paper we take a first step towards learning
such representations, by learning tensors for tran-
sitive verbs.
One feature of the tensor-based framework is
that it allows the meanings of words and phrases
with different basic types, for example nouns and
sentences, to live in different vector spaces. This
means that the sentence space is task specific, and
must be defined in advance. For example, to calcu-
late sentence similarity, we would have to learn a
vector space where distances between vectors rep-
resenting the meanings of sentences reflect simi-
larity scores assigned by human annotators.
In this paper we describe an initial investi-
gation into the learning of word meanings with
complex syntactic types, together with a simple
sentence space. The space we consider is the
?plausibility space? described by Clark (2013), to-
gether with sentences of the form subject-verb-
object. This space is defined to distinguish se-
mantically plausible sentences (e.g. Animals eat
plants) from implausible ones (e.g. Animals eat
planets). Plausibility can be either represented
as a single continuous variable between 0 and 1,
or as a two-dimensional probability distribution
over the classes plausible (>) and implausible (?).
Whether we consider a one- or two-dimensional
sentence space depends on the architecture of the
logistic regression classifier that is used to learn
the verb (Section 3).
We begin with this simple plausibility sentence
space to determine if, in fact, the tensor-based rep-
resentation can be learned to a sufficiently useful
degree. Other simple sentence spaces which can
perhaps be represented using one or two variables
include a ?sentence space? for the sentiment anal-
ysis task (Socher et al., 2013), where one variable
represents positive sentiment and the other nega-
tive. We also expect that the insights gained from
research on this task can be applied to more com-
plex sentence spaces, for example a semantic simi-
larity space which will require more than two vari-
ables.
2 Syntactic Types to Tensors
The syntactic type of a transitive verb in English
is (S\NP)/NP (using notation from Steedman
(2000)), meaning that a transitive verb is a func-
tion which takes an NP argument to the right, an
NP argument to the left, and results in a sentence
S . Such categories with slashes are complex cate-
gories; S and NP are basic or atomic categories.
Interpreting such categories under the Coecke et
al. framework is straightforward. First, for each
atomic category there is a corresponding vector
space; in this case the sentence space S and the
noun space N.
2
Hence the meaning of a noun or
noun phrase, for example people, will be a vector
in the noun space:
????
people ? N. In order to obtain
the meaning of a transitive verb, each slash is re-
placed with a tensor product operator, so that the
meaning of eat, for example, is a 3rd-order tensor:
eat ? S?N?N. Just as in the syntactic case,
the meaning of a transitive verb is a function (a
multi-linear map) which takes two noun vectors as
arguments and returns a sentence vector.
Meanings combine using tensor contraction,
which can be thought of as a multi-linear gen-
eralisation of matrix multiplication (Grefenstette,
2013). Consider first the adjective-noun case, for
example black cat. The syntactic type of black
is N /N ; hence its meaning is a 2nd-order tensor
(matrix): black ? N?N. In the syntax, N /N
combines with N using the rule of forward appli-
cation (N /N N ? N ), which is an instance of
function application. Function application is also
used in the tensor-based semantics, which, for a
matrix and vector argument, corresponds to ma-
trix multiplication.
Figure 1 shows how the syntactic types com-
bine with a transitive verb, and the corresponding
tensor-based semantic types. Note that, after the
verb has combined with its object NP , the type
of the verb phrase is S\NP , with a correspond-
ing meaning tensor (matrix) in S ?N. This ma-
trix then combines with the subject vector, through
2
In practice, for example using the CCG parser of Clark
and Curran (2007), there will be additional atomic categories,
such as PP , but not many more.
1037
people eat fish
NP (S\NP)/NP NP
N S?N?N N
>
S\NP
S?N
<
S
S
Figure 1: Syntactic reduction and tensor-based se-
mantic types for a transitive verb sentence
matrix multiplication, to give a sentence vector.
In practice, using for example the wide-
coverage grammar from CCGbank (Hockenmaier
and Steedman, 2007), there will be many types
with more than 3 slashes, with corresponding
higher-order tensors. For example, a com-
mon category for a preposition is the follow-
ing: ((S\NP)\(S\NP))/NP , which would be
assigned to WITH in eat WITH a fork. (The way
to read the syntactic type is as follows: with re-
quires an NP argument to the right ? a fork in
this example ? and then a verb phrase to the
left ? eat with type S\NP ? resulting in a verb
phrase S\NP .) The corresponding meaning ten-
sor lives in the tensor space S?N?S?N?N,
i.e. a 5th-order tensor. Categories with even
more slashes are not uncommon, for example
((N /N )/(N /N ))/((N /N )/(N /N )). Clearly
learning parameters for such tensors is highly
challenging, and it is likely that lower dimensional
approximations will be required.
3 Methods
In this paper we compare five different methods
for modelling the type-driven semantic represen-
tation of subject-verb-object sentences. The ten-
sor is a function that encodes the meaning of a
verb. It takes two vectors from the K-dimensional
noun space as input, and produces a representa-
tion of the sentence in the S-dimensional sentence
space. In this paper, we consider a plausibility
space where S is either a single variable or a two-
dimensional space over two classes: plausible (>)
and implausible (?).
The first method (Tensor) follows Krishna-
murthy and Mitchell (2013) by learning a tensor as
parameters in a softmax classifier. We introduce
three related methods (2Mat, SKMat, KKMat),
all of which model the verb as a matrix or a pair of
matrices (Figure 2). Table 1 gives the number of
Tensor 2Mat SKMat KKMat DMat
V 2K
2
4K 2K K
2
K
2
? 4 8 4 0 0
Table 1: Number of parameters per method.
parameters for each method. Tensor, 2Mat, and
SKMat all have a two-dimensional S space, while
KKMat produces a scalar value. In all of these
learning-based methods the derivatives were ob-
tained via the chain rule with respect to each set
of parameters and gradient descent was performed
using the Adagrad algorithm (Duchi et al., 2011).
We also reimplement a distributional method
(DMat), which was previously used in SVO
experiments with the type-driven framework
(Grefenstette and Sadrzadeh, 2011). While the
other methods are trained as plausibility classi-
fiers, in DMat we estimate the class boundary
from cosine similarity via training data (see expla-
nation below).
Tensor If subject (n
s
) and object (n
o
) nouns are
K-dimensional vectors and the plausibility vec-
tor is S-dimensional with S = 2, we can learn
the values of the K ? K ? S tensor represent-
ing the verb as parameters (V) of a regression al-
gorithm. To represent this space as a distribution
over two classes (>,?) we apply a sigmoid func-
tion (?) to restrict the output to the [0,1] range and
the softmax activation function (g) to balance the
class probabilities. The full parameter set which
needs to be optimised for is B = {V,?}, where
? = {?
>
, ?
?
} are the softmax parameters for
the two classes. For each verb we optimise the
KL-divergence L between the training labels t
i
and classifier predictions using the following reg-
ularised objective:
O(B) =
N?
i=1
L
(
t
i
, g
(
?
(
h
V
(
n
i
s
, n
i
o
))
,?
))
+
?
2
||B||
2
(1)
where n
i
s
and n
i
o
are the subject and object of
the training instance i ? N , and h
V
(
n
i
s
, n
i
o
)
=
(n
i
s
)V(n
i
o
)
T
describes tensor contraction. The
function h
V
is described diagrammatically in Fig-
ure 2-(a), where the verb tensor parameters are
drawn as a cube with the subject and object noun
vectors as operands on either side. The output
is a two-dimensional vector which is then trans-
formed using the sigmoid and softmax functions.
1038
K

S
K
KK
S
(a) K

K S
K
S
K
2*S
x
S
S
x
(b)
K

K
S
K
K
00
0 0
00 K S
S
x
x
(c) K
 K
K
K
x
x
(d)
Figure 2: Illustrations of the h
V
function for the regression-based methods (a)-Tensor, (b)-2Mat, (c)-
SKMat, (d)-KKMat. The operation in (a) is tensor contraction, T denotes transpose, and ? denotes
matrix multiplication.
The gold-standard distribution over training labels
is defined as (1, 0) or (0, 1), depending on whether
the training instance is a positive (plausible) or
negative (implausible) example. Tensor contrac-
tion is implemented using the Matlab Tensor Tool-
box (Bader et al., 2012).
2Mat An alternative approach is to decouple
the interaction between the object and subject by
learning a pair of S ? K (S = 2) matrices (V
s
,
V
o
) for each of the input noun vectors (one ma-
trix for the subject slot of the verb and one for the
object slot). The resulting S-vectors are concate-
nated, after the subject and object nouns have been
combined with their matrices, and combined with
the softmax component to produce the output dis-
tribution. Therefore the objective function is the
same as in Equation 1, but h
V
is defined as:
h
V
(
n
i
s
, n
i
o
)
=
(
(n
i
s
)V
T
s
)
||
(
V
o
(n
i
o
)
T
)
T
where || represents vector concatenation. The in-
tention is to test whether we can learn the verb
without directly multiplying subject and object
features, n
i
s
and n
j
o
. The function h
V
is shown in
Figure 2-(b), where the verb tensor parameters are
drawn as two 2?K matrices, one of which inter-
acts with the subject and the other with the object
noun vector. The output is a four-dimensional vec-
tor whose values are then restricted to [0,1] using
the sigmoid function and then transformed into a
two-dimensional distribution over the classes us-
ing the softmax function.
SKMat A third option for generating a sentence
vector with S = 2 dimensions is to consider the
verb as an S ?K matrix. If we transform the ob-
ject vector into a K ?K matrix with the noun on
the diagonal and zeroes elsewhere, we can com-
bine the verb and object to produce a new S ?K
matrix, which is encoding the meaning of the verb
phrase. We can then complete the sentence re-
duction by multiplying the subject vector with this
verb phrase vector to produce an S-dimensional
sentence vector. Formally, we define SKMat as:
h
V
(
n
i
s
, n
i
o
)
= n
i
s
(
Vdiag(n
i
o
)
)
T
and use it in Equation 1. The function h
V
is
described in Figure 2-(c), where the verb ten-
sor parameters are drawn as a matrix, the sub-
ject as a vector, and the object as a diagonal ma-
1039
trix. The graphic demonstrates the two-step com-
bination and the intermediate S ? K verb phrase
matrix, as well as the the noun vector product
that results in a two-dimensional vector which is
then transformed using the sigmoid and softmax
functions. Whilst the tensor method captures the
interactions between all pairs of context features
(n
s
i
? n
o
j
), SKMat only captures the interactions
between matching features (n
s
i
? n
o
i
).
KKMat Given a two-class problem, such as
plausibility classification, the softmax implemen-
tation is overparameterised because the class
membership can be estimated with a single vari-
able. To produce a scalar output, we can learn the
parameters for a single K ? K matrix (V) using
standard logistic regression with the mean squared
error cost function:
O(V) = ?
1
m
[
N?
i=1
t
i
log h
V
(
n
i
s
, n
i
o
)
+ (1? t
i
) log h
V
(
n
i
s
, n
i
o
)
]
where h
V
(
n
i
s
, n
i
o
)
= (n
i
s
)V(n
i
o
)
T
and the objec-
tive is regularised: O(V) +
?
2
||V||
2
. This function
is shown in Figure 2-(d), where the verb parame-
ters are shown as a matrix, while the subject and
object are vectors. The output is a single scalar,
which is then transformed with the sigmoid func-
tion. Values over 0.5 are considered plausible.
DMat The final method produces a scalar as in
KKMat, but is distributional and based on corpus
counts rather than regression-based. Grefenstette
and Sadrzadeh (2011) introduced a corpus-based
approach for generating a K ?K matrix for each
verb from an average of Kronecker products of the
subject and object vectors from the positively la-
belled subset of the training data. The intuition is
that, for example, the matrix for eat may have a
high value for the contextual topic pair describing
animate subjects and edible objects. To determine
the plausibility of a new subject-object pair for a
particular verb, we calculate the Kronecker prod-
uct of the subject and object noun vectors for this
pair, and compare the resulting matrix with the av-
erage verb matrix using cosine similarity.
For label prediction, we calculate the similar-
ity between each of the training data pairs and the
learned average matrix. Unlike for KKmat, the
class cutoff is estimated at the break-even point
of the receiver operator characteristic (ROC) gen-
erated by comparing the training labels with this
cosine similarity value. The break-even point is
when the true positive rate is equal to the false pos-
itive rate. In practice it would be more accurate
to estimate the cutoff on a validation dataset, but
some of the verbs have so few training instances
that this was not possible.
4 Experiments
In order to examine the quality of learning we run
several experiments where we compare the differ-
ent methods. In these experiments we consider
the DMat method as the baseline. Some of the
experiments employ cross-validation, in particular
five repetitions of 2-fold cross validation (5x2cv),
which has been shown to be statistically more ro-
bust than the traditional 10-fold cross validation
(Alpaydin, 1999; Ulas? et al., 2012). The results of
5x2cv experiments can be compared using the reg-
ular paired t-test, but the specially designed 5x2cv
F-test has been proven to produce fewer statistical
errors (Ulas? et al., 2012).
The performance was evaluated using the area
under the ROC (AUC) and the F
1
measure (based
on precision and recall over the plausible class).
The AUC evaluates whether a method is ranking
positive examples above negative ones, regardless
of the class cutoff value. F
1
shows how accurately
a method assigns the correct class label. Another
way to interpret the results is to consider the AUC
as the measure of the quality of the parameters in
the verb matrix or tensor, while the F-score indi-
cates how well the softmax, the sigmoid, and the
DMat cutoff algorithm are estimating class partic-
ipation.
Ex-1. In the first experiment, we compare the
different transitive verb representations by running
5x2cv experiments on ten verbs chosen to cover a
range of concreteness and frequency values (Sec-
tion 4.2).
Ex-2. In the initial experiments we found that
some models had low performance, so we applied
the column normalisation technique, which is of-
ten used with regression learning to standardise
the numerical range of features:
~x :=
~x?min(~x)
max(~x)?min(~x)
(2)
This preserves the relative values of features be-
tween training samples, while moving the values
to the [0,1] range.
1040
Ex-3. There are varying numbers of training ex-
amples for each of the verbs, so we repeated the
5x2cv with datasets of 52 training points for each
verb, since this is the size of the smallest dataset of
the verb CENSOR. The points were randomly sam-
pled from the datasets used in the first experiment.
Finally, the four verbs with the largest datasets
were used to examine how the performance of the
methods changes as the amount of training data
increases. The 4,000 training samples were ran-
domised and half were used for testing. We sam-
pled between 10 and 1000 training triples from the
other half (Figure 4).
4.1 Noun vectors
Distributional semantic models (Turney and Pan-
tel, 2010) encode word meaning in a vector for-
mat by counting co-occurrences with other words
within a specified context window. We con-
structed the vectors from the October 2013 dump
of Wikipedia articles, which was tokenised us-
ing the Stanford NLP tools
3
, lemmatised with the
Morpha lemmatiser (Minnen et al., 2001), and
parsed with the C&C parser (Clark and Curran,
2007). In this paper we use sentence boundaries to
define context windows and the top 10,000 most
frequent lemmatised words in the whole corpus
(excluding stopwords) as context words. The raw
co-occurrence counts are re-weighted using the
standard tTest weighting scheme (Curran, 2004),
where f
w
i
c
j
is the number of times target noun w
i
occurs with context word c
j
:
tTest( ~w
i
, c
j
) =
p(w
i
, c
j
)? p(w
i
)p(c
j
)
?
p(w
i
)p(c
j
)
(3)
where p(w
i
) =
?
j
f
w
i
c
j?
k
?
l
f
w
k
c
l
, p(c
j
) =
?
i
f
w
i
c
j?
k
?
l
f
w
k
c
l
,
and p(w
i
, c
j
) =
f
w
i
c
j?
k
?
l
f
w
k
c
l
.
Using all 10,000 context words would result in
a large number of parameters for each verb ten-
sor, and so we apply singular value decomposition
(SVD) (Turney and Pantel, 2010) with 40 latent
dimensions to the target-context word matrix. We
use context selection (with N = 140) and row
normalisation as described in Polajnar and Clark
(2014) to markedly improve the performance of
SVD on smaller dimensions (K) and enable us to
train the verb tensors using very low-dimensional
3
http://nlp.stanford.edu/software/index.shtml
Verb Concreteness # of Positive Frequency
APPLY 2.5 5618 47361762
CENSOR 3 26 278525
COMB 5 164 644447
DEPOSE 2.5 118 874463
EAT 4.44 5067 26396728
IDEALIZE 1.17 99 485580
INCUBATE 3.5 82 833621
JUSTIFY 1.45 5636 10517616
REDUCE 2 26917 40336784
WIPE 4 1090 6348595
Table 2: The 10 chosen verbs together with their
concreteness scores. The number of positive SVO
examples was capped at 2000. Frequency is the
frequency of the verb in the GSN corpus.
noun vectors. Performance of the noun vectors
was measured on standard word similarity datasets
and the results were comparable to those reported
by Polajnar and Clark (2014).
4.2 Training data
In order to generate training data we made use
of two large corpora: the Google Syntactic N-
grams (GSN) (Goldberg and Orwant, 2013) and
the Wikipedia October 2013 dump. We first chose
ten transitive verbs with different concreteness
scores (Brysbaert et al., 2013) and frequencies, in
order to obtain a variety of verb types. Then the
positive (plausible) SVO examples were extracted
from the GSN corpus. More precisely, we col-
lected all distinct syntactic trigrams of the form
nsubj ROOT dobj, where the root of the phrase was
one of our target verbs. We lemmatised the words
using the NLTK
4
lemmatiser and filtered these ex-
amples to retain only the ones that contain nouns
that also occur in Wikipedia, obtaining the counts
reported in Table 2.
For every positive training example, we con-
structed a negative (implausible) one by replac-
ing both the subject and the object with a con-
founder, using a standard technique from the se-
lectional preference literature (Chambers and Ju-
rafsky, 2010). A confounder was generated by
choosing a random noun from the same frequency
bucket as the original noun.
5
Frequency buckets
of size 10 were constructed by collecting noun fre-
quency counts from the Wikipedia corpus. For ex-
4
http://nltk.org/
5
Note that the random selection of the confounder could
result in a plausible negative example by chance, but man-
ual inspection of a subset of the data suggests this happens
infrequently for those verbs which select strongly for their
arguments, but more often for those verbs that don?t.
1041
Verb Tensor DMat KKMat SKMat 2Mat
AUC
APPLY 85.68? 81.46? 88.88?? 68.02 88.92??
CENSOR 79.40 85.54 80.55 78.52 83.19
COMB 89.41 85.65 88.38 69.20?? 89.56
DEPOSE 92.70 94.44 93.12 84.47? 93.20
EAT 94.62 93.81 95.17 67.92 95.88?
IDEALIZE 69.56 75.84 72.46 61.19 70.23
INCUBATE 89.33 85.53 88.61 70.59 91.40
JUSTIFY 85.27? 88.70? 89.97? 73.56 90.10?
REDUCE 96.13 95.48 96.69? 79.32 97.21
WIPE 85.19 84.47 87.84? 64.93?? 81.29
MEAN 86.93 87.29 88.37 71.96 88.30
Tensor DMat KKMat SKMat 2Mat
F
1
79.27 64.00 81.24? 54.06 80.80?
70.66 47.93 73.52 37.86 71.07
81.15 45.02 81.38 39.67 82.36
84.60 54.77 84.79 43.79 86.15
88.91 52.45 88.83 56.22 89.95
66.53 48.28 68.39 31.03 67.43
80.30 50.84 80.90 31.99 84.55
79.73 73.71 81.10 54.09 82.52
91.24 71.24? 87.46 76.67? 92.22
78.57 47.62 80.65 39.50 78.90
80.30 55.79 81.03 46.69 81.79
Table 3: The best AUC and F
1
results for all the verbs, where ? denotes statistical significance compared
to DMat and ? denotes significance compared to Tensor according to the 5x2cv F-test with p < 0.05.
ample, for the plausible triple animal EAT plant,
we generate the implausible triple mountain EAT
product. Some verbs were well represented in the
corpus, so we used up to the top 2,000 most fre-
quent triples for training.
0
0.5
1
AUC
 
 
APPL
Y
CENS
OR COMBDEPO
SE EAT
IDEAL
IZE
INCU
BATEJUST
IFY
REDU
CE WIPE
TensorTensor*SKMatSKMat*
?0.2
0
0.2
0.4
0.6
0.8
1
1.2
F?S
core
 
 
APPLYCENS
OR COMBDEPO
SE EAT
IDEAL
IZE
INCUB
ATEJUSTI
FY
REDU
CE WIPE
TensorTensor*SKMatSKMat*
Figure 3: The effect of column normalisation (*)
on Tensor and SKMat. Top table shows AUC and
the bottom F
1
-score, while the error bars indicate
standard deviation.
5 Results
The results from Ex-1 are summarised in Ta-
ble 3. We can see that linear regression can lead
to models that are able to distinguish between
plausible and implausible SVO triples. The Ten-
sor method outperforms DMat, which was pre-
viously shown to produce reasonable verb repre-
sentations in related experiments (Grefenstette and
Sadrzadeh, 2011). 2Mat and KKMat, in turn,
outperform Tensor demonstrating that it is pos-
sible to learn lower dimensional approximations
of the tensor-based framework. 2Mat is an appro-
priate approximation for functions with two inputs
and a sentence space of any dimensionality, while
KKMat is only appropriate for a single valued
sentence space, such as the plausibility or senti-
ment space. Due to method variance and dataset
size there are very few AUC results that are sig-
nificantly better than DMat and even fewer that
outperform Tensor. All methods perform poorly
on the verb IDEALIZE, probably because it has
the lowest concreteness value and is in one of the
smallest datasets. This verb is also particularly dif-
ficult because it does not select strongly for either
its subject or object, and so some of the pseudo-
negative examples are in fact somewhat plausible
(e.g. town IDEALIZE authority or child IDEALIZE
racehorse). In general, this would indicate that
more concrete verbs are easier to learn, as they
have a clearer pattern of preferred property types,
but there is no distinct correlation.
The results of the normalisation experiments
(Ex-2) are shown in Table 4. We can see that the
SKMat method, which performed poorly in Ex-
1 notably improves with normalisation. Tensor
AUC scores also improve through normalisation,
but the F-scores decrease. The rest of the methods,
and in particular DMat are negatively affected by
column normalisation. The results from Ex-1 and
Ex-2 for SKMat and Tensor are summarised in
1042
Verb Tensor DMat KKMat SKMat 2Mat
AUC
APPLY 86.16? 48.63? 82.63?? 85.73? 85.65?
CENSOR 75.74 71.20 78.00 82.77 78.64
COMB 91.67? 62.42? 90.85? 89.79? 91.42?
DEPOSE 93.96? 54.93? 93.56? 93.87? 93.81?
EAT 95.64? 47.68? 92.92? 94.99?? 94.76?
IDEALIZE 69.64 55.98 72.20?? 76.71?? 71.85?
INCUBATE 90.97? 61.31? 89.69? 90.19? 90.05?
JUSTIFY 89.76? 54.87? 87.26?? 89.64? 89.05?
REDUCE 96.63? 59.58? 94.99?? 96.14? 96.53?
WIPE 86.82? 58.02? 84.18? 83.65? 86.02?
MEAN 87.90 57.66 86.83 88.55 87.98
Tensor DMat KKMat SKMat 2Mat
F
1
45.57 46.99 46.17 60.86 76.60?
30.43 55.16 65.19 49.59 44.22
33.37 61.05 71.20 64.56 75.96
42.73 39.71 73.07 54.51 56.54
60.42 47.42 58.80 69.05 87.44?
39.14 49.16 41.75 31.57 50.59
46.35 53.33 70.45 41.57 63.61
47.38 51.40 41.91 63.96 80.55?
51.63 54.27 69.18 69.76 90.77?
44.04 55.19 47.84 49.89 75.80
44.31 51.57 58.76 55.73 70.41
Table 4: The best AUC and F
1
results for all the verbs with normalised vectors, where ? denotes statistical
significance compared to DMat and ? denotes significance compared to Tensor according to the 5x2cv
F-test with p < 0.05.
Figure 3. This figure also shows that AUC values
have much lower variance, but that high variance
in F-score leads to results that are not statistically
significant.
When considering the size of the datasets (Ex-
3), it would seem from Table 5 that 2Mat is able to
learn from less data than DMat or Tensor. While
this may be true over a 5x2cv experiment on small
data, Figure 4 shows that this view may be overly
simplistic and that different training examples can
influence learning. Analysis of errors shows that
the baseline method mostly generates false nega-
tive errors (i.e. predicting implausible when the
gold standard label is plausible). In contrast, Ten-
sor produces almost equal numbers of false posi-
tives and false negatives, but sometimes produces
false negatives with low frequency nouns (e.g.
bourgeoisie IDEALIZE work), presumably because
there is not enough information in the noun vec-
tor to decide on the correct class. It also produces
some false positive errors when either of the nouns
is plausible (but the triple is implausible), which
would suggest results may be improved by train-
ing with data where only one noun is confounded
or by treating negative data as possibly positive
(Lee and Liu, 2003).
6 Discussion
Current methods which derive distributed repre-
sentations for phrases, for example the work of
Socher et al. (2012), typically use only matrix rep-
resentations, and also assume that words, phrases
and sentences all live in the same vector space.
The tensor-based semantic framework is more
flexible, in that it allows different spaces for dif-
ferent grammatical types, which results from it be-
Verb Tensor DMat 2Mat
APPLY 95.76 86.50 86.31
CENSOR 82.97 84.09 77.79
COMB 90.13 92.93 95.18
DEPOSE 92.41 91.27 95.61
EAT 99.64 98.25 99.58
IDEALIZE 75.03 76.68 88.98
INCUBATE 91.10 87.20 96.42
JUSTIFY 88.96 88.99 87.31
REDUCE 100.0 99.87 99.46
WIPE 97.20 91.63 96.36
MEAN 91.52 89.94 92.50
Table 5: Results show average of 5x2cv AUC on
small data (26 positive + 26 negative per verb).
None of the results are significant.
ing tied more closely to a type-driven syntactic de-
scription; however, this flexibility comes at a cost,
since there are many more paramaters to learn.
Various communities are beginning to recog-
nize the additional power that tensor representa-
tions can provide, through the capturing of interac-
tions that are difficult to represent with vectors and
matrices (see e.g. (Ranzato et al., 2010; Sutskever
et al., 2009; Van de Cruys et al., 2012)). Hierar-
chical recursive structures in language potentially
represent a large number of such interactions ? the
obvious example for this paper being the interac-
tion between a transitive verb?s subject and object
? and present a significant challenge for machine
learning.
This paper is a practical extension of the work
in Krishnamurthy and Mitchell (2013), which in-
troduced learning of CCG-based function tensors
with logistic regression on a compositional se-
mantics task, but was implemented as a proof-of-
concept with vectors of length 2 and on small,
manually created datasets based on propositional
1043
10 20 40 80 150 300 600 800 1000 2000
0.650.7
0.750.8
0.850.9
0.951
# Training Examples
AUC
apply
 
 
DMatTensor2Mat
10 20 40 80 150 300 600 800 1000 20000.7
0.75
0.8
0.85
0.9
0.95
1
# Training Examples
AUC
eat
 
 
DMatTensor2Mat
10 20 40 80 150 300 600 800 1000 2000
0.650.7
0.750.8
0.850.9
0.951
# Training Examples
AUC
justify
 
 
DMatTensor2Mat
10 20 40 80 150 300 600 800 1000 20000.8
0.85
0.9
0.95
1
# Training Examples
AUC
reduce
 
 
DMatTensor2Mat
Figure 4: Comparison of DMat, Tensor, and 2Mat methods as the number of training instances in-
creases.
logic examples. Here, we go beyond this by learn-
ing tensors using corpus data and by deriving sev-
eral different matrix representations for the verb in
the subject-verb-object (SVO) sentence.
This work can also be thought of as applying
neural network learning techniques to the clas-
sic problem of selectional preference acquisition,
since the design of the pseudo-disambiguation ex-
periments is taken from the literature on selec-
tional preferences (Clark and Weir, 2002; Cham-
bers and Jurafsky, 2010). We do not compare di-
rectly with methods from this literature, e.g. those
based on WordNet (Resnik, 1996; Clark and Weir,
2002) or topic modelling techniques (Seaghdha,
2010), since our goal in this paper is not to ex-
tend the state-of-the-art in that area, but rather to
use selectional preference acquisition as a test bed
for the tensor-based semantic framework.
7 Conclusion
In this paper we introduced three dimensionally
reduced representations of the transitive verb ten-
sor defined in the type-driven framework for com-
positional distributional semantics (Coecke et al.,
2010). In a comprehensive experiment on ten dif-
ferent verbs we find no significant difference be-
tween the full tensor representation and the re-
duced representations. The SKMat and 2Mat rep-
resentations have the lowest number of parame-
ters and offer a promising avenue of research for
more complex sentence structures and sentence
spaces. KKMat and DMat also had high scores
on some verbs, but these representations are appli-
cable only in spaces where a single-value output is
appropriate.
In experiments where we varied the amount of
training data, we found that in general more con-
crete verbs can learn from less data. Low con-
creteness verbs require particular care with dataset
design, since some of the seemingly random ex-
amples can be plausible. This problem may be
circumvented by using semi-supervised learning
techniques.
We also found that simple numerical tech-
niques, such as column normalisation, can
markedly alter the values and quality of learning.
On our data, column normalisation has a side-
effect of removing the negative values that were
introduced by the use of tTest weighting measure.
The use of the PPMI weighting scheme and non-
negative matrix factorisation (NMF) (Grefenstette
et al., 2013; Van de Cruys, 2010) could lead to a
similar effect, and should be investigated. Further
numerical techniques for improving the estimation
of the class decision boundary, and consequently
the F-score, will also constitute future work.
1044
References
Ethem Alpaydin. 1999. Combined 5x2 CV F-test
for comparing supervised classification learning al-
gorithms. Neural Computation, 11(8):1885?1892,
November.
Brett W. Bader, Tamara G. Kolda, et al. 2012. Matlab
tensor toolbox version 2.5. Available online, Jan.
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP-10), pages 1183?1193,
Cambridge, MA.
Marco Baroni, Raffaella Bernardi, and Roberto Zam-
parelli. 2014. Frege in space: A program for com-
positional distributional semantics. Linguistic Is-
sues in Language Technology, 9:5?110.
Marc Brysbaert, Amy Beth Warriner, and Victor Ku-
perman. 2013. Concreteness ratings for 40 thou-
sand generally known English word lemmas. Be-
havior research methods, pages 1?8.
Nathanael Chambers and Dan Jurafsky. 2010. Im-
proving the use of pseudo-words for evaluating se-
lectional preferences. In Proceedings of ACL 2010,
Uppsala, Sweden.
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG
and log-linear models. Computational Linguistics,
33(4):493?552.
Stephen Clark and David Weir. 2002. Class-based
probability estimation using a semantic hierarchy.
Computational Linguistics, 28(2):187?206.
Stephen Clark. 2013. Type-driven syntax and seman-
tics for composing meaning vectors. In Chris He-
unen, Mehrnoosh Sadrzadeh, and Edward Grefen-
stette, editors, Quantum Physics and Linguistics:
A Compositional, Diagrammatic Discourse, pages
359?377. Oxford University Press.
Daoud Clarke. 2012. A context-theoretic frame-
work for compositionality in distributional seman-
tics. Computational Linguistics, 38(1):41?71.
Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen
Clark. 2010. Mathematical foundations for a com-
positional distributional model of meaning. In J. van
Bentham, M. Moortgat, and W. Buszkowski, edi-
tors, Linguistic Analysis (Lambek Festschrift), vol-
ume 36, pages 345?384.
James R. Curran. 2004. From Distributional to Seman-
tic Similarity. Ph.D. thesis, University of Edinburgh.
David R. Dowty, Robert E. Wall, and Stanley Peters.
1981. Introduction to Montague Semantics. Dor-
drecht.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. J. Mach. Learn. Res.,
12:2121?2159, July.
Yoav Goldberg and Jon Orwant. 2013. A dataset of
syntactic-ngrams over time from a very large corpus
of English books. In Second Joint Conference on
Lexical and Computational Semantics, pages 241?
247, Atlanta,Georgia.
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011.
Experimental support for a categorical composi-
tional distributional model of meaning. In Proceed-
ings of the 2011 Conference on Empirical Methods
in Natural Language Processing, pages 1394?1404,
Edinburgh, Scotland, UK, July.
Edward Grefenstette, Georgiana Dinu, Yao-Zhong
Zhang, Mehrnoosh Sadrzadeh, and Marco Baroni.
2013. Multi-step regression learning for compo-
sitional distributional semantics. Proceedings of
the 10th International Conference on Computational
Semantics (IWCS 2013).
Edward Grefenstette. 2013. Category-Theoretic
Quantitative Compositional Distributional Models
of Natural Language Semantics. Ph.D. thesis, Uni-
versity of Oxford.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: a corpus of CCG derivations and dependency
structures extracted from the Penn Treebank. Com-
putational Linguistics, 33(3):355?396.
Jayant Krishnamurthy and Tom M Mitchell. 2013.
Vector space semantic parsing: A framework for
compositional vector space models. In Proceed-
ings of the 2013 ACL Workshop on Continuous Vec-
tor Space Models and their Compositionality, Sofia,
Bulgaria.
Wee Sun Lee and Bing Liu. 2003. Learning with posi-
tive and unlabeled examples using weighted logistic
regression. In Proceedings of the Twentieth Interna-
tional Conference on Machine Learning (ICML).
Jean Maillard, Stephen Clark, and Edward Grefen-
stette. 2014. A type-driven tensor-based semantics
for CCG. In Proceedings of the EACL 2014 Type
Theory and Natural Language Semantics Workshop
(TTNLS), Gothenburg, Sweden.
Guido Minnen, John Carroll, and Darren Pearce. 2001.
Applied morphological processing of English. Nat-
ural Language Engineering, 7(3):207?223.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL-08, pages 236?244, Columbus, OH.
Tamara Polajnar and Stephen Clark. 2014. Improving
distributional semantic vectors through context se-
lection and normalisation. In 14th Conference of the
European Chapter of the Association for Computa-
tional Linguistics, EACL?14, Gothenburg, Sweden.
1045
M. Ranzato, A. Krizhevsky, and G. E. Hinton. 2010.
Factored 3-way restricted boltzmann machines for
modeling natural images. In Proceedings of the
Thirteenth International Conference on Artificial In-
telligence and Statistics (AISTATS), Sardinia, Italy.
Philip Resnik. 1996. Selectional constraints: An
information-theoretic model and its computational
realization. Cognition, 61:127?159.
Hinrich Sch?utze. 1998. Automatic word sense dis-
crimination. Computational Linguistics, 24(1):97?
124.
Diarmuid O Seaghdha. 2010. Latent variable mod-
els of selectional preference. In Proceedings of ACL
2010, Uppsala, Sweden.
Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Semantic composi-
tionality through recursive matrix-vector spaces. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages 1201?
1211, Jeju, Korea.
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Chris Manning, Andrew Ng, and Chris
Potts. 2013. Recursive deep models for semantic
compositionality over a sentiment treebank. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing (EMNLP 2013), Seat-
tle, USA.
Mark Steedman. 2000. The Syntactic Process. The
MIT Press, Cambridge, MA.
I. Sutskever, R. Salakhutdinov, and J. B. Tenenbaum.
2009. Modelling relational data using bayesian clus-
tered tensor factorization. In Proceedings of Ad-
vances in Neural Information Processing Systems
(NIPS 2009), Vancouver, Canada.
Peter D. Turney and Patrick Pantel. 2010. From
frequency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37:141?188.
Ayd?n Ulas?, Olcay Taner Y?ld?z, and Ethem Alpayd?n.
2012. Cost-conscious comparison of supervised
learning algorithms over multiple data sets. Pattern
Recognition, 45(4):1772?1781, April.
Tim Van de Cruys, Laura Rimell, Thierry Poibeau, and
Anna Korhonen. 2012. Multi-way tensor factor-
ization for unsupervised lexical acquisition. In Pro-
ceedings of COLING 2012, Mumbai, India.
Tim Van de Cruys. 2010. A non-negative tensor fac-
torization model for selectional preference induc-
tion. Journal of Natural Language Engineering,
16(4):417?437.
1046
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 736?746,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Syntax-Based Word Ordering Incorporating a Large-Scale Language
Model
Yue Zhang
University of Cambridge
Computer Laboratory
yz360@cam.ac.uk
Graeme Blackwood
University of Cambridge
Engineering Department
gwb24@eng.cam.ac.uk
Stephen Clark
University of Cambridge
Computer Laboratory
sc609@cam.ac.uk
Abstract
A fundamental problem in text generation
is word ordering. Word ordering is a com-
putationally difficult problem, which can
be constrained to some extent for particu-
lar applications, for example by using syn-
chronous grammars for statistical machine
translation. There have been some recent
attempts at the unconstrained problem of
generating a sentence from a multi-set of
input words (Wan et al 2009; Zhang and
Clark, 2011). By using CCG and learn-
ing guided search, Zhang and Clark re-
ported the highest scores on this task. One
limitation of their system is the absence
of an N-gram language model, which has
been used by text generation systems to
improve fluency. We take the Zhang and
Clark system as the baseline, and incor-
porate an N-gram model by applying on-
line large-margin training. Our system sig-
nificantly improved on the baseline by 3.7
BLEU points.
1 Introduction
One fundamental problem in text generation is
word ordering, which can be abstractly formu-
lated as finding a grammatical order for a multi-
set of words. The word ordering problem can also
include word choice, where only a subset of the
input words are used to produce the output.
Word ordering is a difficult problem. Finding
the best permutation for a set of words accord-
ing to a bigram language model, for example, is
NP-hard, which can be proved by linear reduction
from the traveling salesman problem. In prac-
tice, exploring the whole search space of permu-
tations is often prevented by adding constraints.
In phrase-based machine translation (Koehn et al
2003; Koehn et al 2007), a distortion limit is
used to constrain the position of output phrases.
In syntax-based machine translation systems such
as Wu (1997) and Chiang (2007), synchronous
grammars limit the search space so that poly-
nomial time inference is feasible. In fluency
improvement (Blackwood et al 2010), parts of
translation hypotheses identified as having high
local confidence are held fixed, so that word or-
dering elsewhere is strictly local.
Some recent work attempts to address the fun-
damental word ordering task directly, using syn-
tactic models and heuristic search. Wan et al
(2009) uses a dependency grammar to solve word
ordering, and Zhang and Clark (2011) uses CCG
(Steedman, 2000) for word ordering and word
choice. The use of syntax models makes their
search problems harder than word permutation us-
ing an N -gram language model only. Both meth-
ods apply heuristic search. Zhang and Clark de-
veloped a bottom-up best-first algorithm to build
output syntax trees from input words, where
search is guided by learning for both efficiency
and accuracy. The framework is flexible in allow-
ing a large range of constraints to be added for
particular tasks.
We extend the work of Zhang and Clark (2011)
(Z&C) in two ways. First, we apply online large-
margin training to guide search. Compared to the
perceptron algorithm on ?constituent level fea-
tures? by Z&C, our training algorithm is theo-
retically more elegant (see Section 3) and con-
verges more smoothly empirically (see Section 5).
Using online large-margin training not only im-
proves the output quality, but also allows the in-
corporation of an N -gram language-model into
736
the system. N -gram models have been used as a
standard component in statistical machine trans-
lation, but have not been applied to the syntac-
tic model of Z&C. Intuitively, an N -gram model
can improve local fluency when added to a syntax
model. Our experiments show that a four-gram
model trained using the English GigaWord cor-
pus gave improvements when added to the syntax-
based baseline system.
The contributions of this paper are as follows.
First, we improve on the performance of the Z&C
system for the challenging task of the general
word ordering problem. Second, we develop a
novel method for incorporating a large-scale lan-
guage model into a syntax-based generation sys-
tem. Finally, we analyse large-margin training in
the context of learning-guided best-first search,
offering a novel solution to this computationally
hard problem.
2 The statistical model and decoding
algorithm
We take Z&C as our baseline system. Given
a multi-set of input words, the baseline system
builds a CCG derivation by choosing and ordering
words from the input set. The scoring model is
trained using CCGBank (Hockenmaier and Steed-
man, 2007), and best-first decoding is applied. We
apply the same decoding framework in this paper,
but apply an improved training process, and incor-
porate an N -gram language model into the syntax
model. In this section, we describe and discuss
the baseline statistical model and decoding frame-
work, motivating our extensions.
2.1 Combinatory Categorial Grammar
CCG, and parsing with CCG, has been described
elsewhere (Clark and Curran, 2007; Hockenmaier
and Steedman, 2002); here we provide only a
short description.
CCG (Steedman, 2000) is a lexicalized gram-
mar formalism, which associates each word in a
sentence with a lexical category. There is a small
number of basic lexical categories, such as noun
(N), noun phrase (NP), and prepositional phrase
(PP). Complex lexical categories are formed re-
cursively from basic categories and slashes, which
indicate the directions of arguments. The CCG
grammar used by our system is read off the deriva-
tions in CCGbank, following Hockenmaier and
Steedman (2002), meaning that the CCG combina-
tory rules are encoded as rule instances, together
with a number of additional rules which deal with
punctuation and type-changing. Given a sentence,
its CCG derivation can be produced by first assign-
ing a lexical category to each word, and then re-
cursively applying CCG rules bottom-up.
2.2 The decoding algorithm
In the decoding algorithm, a hypothesis is an
edge, which corresponds to a sub-tree in a CCG
derivation. Edges are built bottom-up, starting
from leaf edges, which are generated by assigning
all possible lexical categories to each input word.
Each leaf edge corresponds to an input word with
a particular lexical category. Two existing edges
can be combined if there exists a CCG rule which
combines their category labels, and if they do not
contain the same input word more times than its
total count in the input. The resulting edge is as-
signed a category label according to the combi-
natory rule, and covers the concatenated surface
strings of the two sub-edges in their order or com-
bination. New edges can also be generated by ap-
plying unary rules to a single existing edge. Start-
ing from the leaf edges, the bottom-up process is
repeated until a goal edge is found, and its surface
string is taken as the output.
This derivation-building process is reminiscent
of a bottom-up CCG parser in the edge combina-
tion mechanism. However, it is fundamentally
different from a bottom-up parser. Since, for
the generation problem, the order of two edges
in their combination is flexible, the search prob-
lem is much harder than that of a parser. With
no input order specified, no efficient dynamic-
programming algorithm is available, and less con-
textual information is available for disambigua-
tion due to the lack of an input string.
In order to combat the large search space, best-
first search is applied, where candidate hypothe-
ses are ordered by their scores, and kept in an
agenda, and a limited number of accepted hy-
potheses are recorded in a chart. Here the chart
is essentially a set of beams, each of which con-
tains the highest scored edges covering a particu-
lar number of words. Initially, all leaf edges are
generated and scored, before they are put onto the
agenda. During each step in the decoding process,
the top edge from the agenda is expanded. If it is
a goal edge, it is returned as the output, and the
737
Algorithm 1 The decoding algorithm.
a? INITAGENDA( )
c? INITCHART( )
while not TIMEOUT( ) do
new? []
e? POPBEST(a)
if GOALTEST(e) then
return e
end if
for e? ? UNARY(e, grammar) do
APPEND(new, e)
end for
for e? ? c do
if CANCOMBINE(e, e?) then
e?? BINARY(e, e?, grammar)
APPEND(new, e?)
end if
if CANCOMBINE(e?, e) then
e?? BINARY(e?, e, grammar)
APPEND(new, e?)
end if
end for
for e? ? new do
ADD(a, e?)
end for
ADD(c, e)
end while
decoding finishes. Otherwise it is extended with
unary rules, and combined with existing edges in
the chart using binary rules to produce new edges.
The resulting edges are scored and put onto the
agenda, while the original edge is put onto the
chart. The process repeats until a goal edge is
found, or a timeout limit is reached. In the latter
case, a default output is produced using existing
edges in the chart.
Pseudocode for the decoder is shown as Algo-
rithm 1. Again it is reminiscent of a best-first
parser (Caraballo and Charniak, 1998) in the use
of an agenda and a chart, but is fundamentally dif-
ferent due to the fact that there is no input order.
2.3 Statistical model and feature templates
The baseline system uses a linear model to score
hypotheses. For an edge e, its score is defined as:
f(e) = ?(e) ? ?,
where ?(e) represents the feature vector of e and
? is the parameter vector of the model.
During decoding, feature vectors are computed
incrementally. When an edge is constructed, its
score is computed from the scores of its sub-edges
and the incrementally added structure:
f(e) = ?(e) ? ?
=
(
(
?
es?e
?(es)
)
+ ?(e)
)
? ?
=
(
?
es?e
?(es) ? ?
)
+ ?(e) ? ?
=
(
?
es?e
f(es)
)
+ ?(e) ? ?
In the equation, es ? e represents a sub-edge of
e. Leaf edges do not have any sub-edges. Unary-
branching edges have one sub-edge, and binary-
branching edges have two sub-edges. The fea-
ture vector ?(e) represents the incremental struc-
ture when e is constructed over its sub-edges.
It is called the ?constituent-level feature vector?
by Z&C. For leaf edges, ?(e) includes informa-
tion about the lexical category label; for unary-
branching edges, ?(e) includes information from
the unary rule; for binary-branching edges, ?(e)
includes information from the binary rule, and ad-
ditionally the token, POS and lexical category bi-
grams and trigrams that result from the surface
string concatenation of its sub-edges. The score
f(e) is therefore the sum of f(es) (for all es ? e)
plus ?(e) ??. The feature templates we use are the
same as those in the baseline system.
An important aspect of the scoring model is that
edges with different sizes are compared with each
other during decoding. Edges with different sizes
can have different numbers of features, which can
make the training of a discriminative model more
difficult. For example, a leaf edge with one word
can be compared with an edge over the entire in-
put. One way of reducing the effect of the size dif-
ference is to include the size of the edge as part of
feature definitions, which can improve the compa-
rability of edges of different sizes by reducing the
number of features they have in common. Such
features are applied by Z&C, and we make use of
them here. Even with such features, the question
of whether edges with different sizes are linearly
separable is an empirical one.
3 Training
The efficiency of the decoding algorithm is de-
pendent on the statistical model, since the best-
738
first search is guided to a solution by the model,
and a good model will lead to a solution being
found more quickly. In the ideal situation for the
best-first decoding algorithm, the model is perfect
and the score of any gold-standard edge is higher
than the score of any non-gold-standard edge. As
a result, the top edge on the agenda is always a
gold-standard edge, and therefore all edges on the
chart are gold-standard before the gold-standard
goal edge is found. In this oracle procedure, the
minimum number of edges is expanded, and the
output is correct. The best-first decoder is perfect
in not only accuracy, but also speed. In practice
this ideal situation is rarely met, but it determines
the goal of the training algorithm: to produce the
perfect model and hence decoder.
If we take gold-standard edges as positive ex-
amples, and non-gold-standard edges as negative
examples, the goal of the training problem can be
viewed as finding a large separating margin be-
tween the scores of positive and negative exam-
ples. However, it is infeasible to generate the full
space of negative examples, which is factorial in
the size of input. Like Z&C, we apply online
learning, and generate negative examples based
on the decoding algorithm.
Our training algorithm is shown as Algo-
rithm 2. The algorithm is based on the decoder,
where an agenda is used as a priority queue of
edges to be expanded, and a set of accepted edges
is kept in a chart. Similar to the decoding algo-
rithm, the agenda is intialized using all possible
leaf edges. During each step, the top of the agenda
e is popped. If it is a gold-standard edge, it is ex-
panded in exactly the same way as the decoder,
with the newly generated edges being put onto
the agenda, and e being inserted into the chart.
If e is not a gold-standard edge, we take it as a
negative example e?, and take the lowest scored
gold-standard edge on the agenda e+ as a positive
example, in order to make an udpate to the model
parameter vector ?. Our parameter update algo-
rithm is different from the baseline perceptron al-
gorithm, as will be discussed later. After updating
the parameters, the scores of agenda edges above
and including e?, together with all chart edges,
are updated, and e? is discarded before the start
of the next processing step. By not putting any
non-gold-standard edges onto the chart, the train-
ing speed is much faster; on the other hand a wide
range of negative examples is pruned. We leave
Algorithm 2 The training algorithm.
a? INITAGENDA( )
c? INITCHART( )
while not TIMEOUT( ) do
new? []
e? POPBEST(a)
if GOLDSTANDARD(e) and GOALTEST(e)
then return e
end if
if not GOLDSTANDARD(e) then
e?? e
e+?MINGOLD(a)
UPDATEPARAMETERS(e+ , e?)
RECOMPUTESCORES(a, c)
continue
end if
for e? ? UNARY(e, grammar) do
APPEND(new, e)
end for
for e? ? c do
if CANCOMBINE(e, e?) then
e?? BINARY(e, e?, grammar)
APPEND(new, e?)
end if
if CANCOMBINE(e?, e) then
e?? BINARY(e?, e, grammar)
APPEND(new, e?)
end if
end for
for e? ? new do
ADD(a, e?)
end for
ADD(c, e)
end while
for further work possible alternative methods to
generate more negative examples during training.
Another way of viewing the training process is
that it pushes gold-standard edges towards the top
of the agenda, and crucially pushes them above
non-gold-standard edges. This is the view de-
scribed by Z&C. Given a positive example e+ and
a negative example e?, they use the perceptron
algorithm to penalize the score for ?(e?) and re-
ward the score of ?(e+), but do not update pa-
rameters for the sub-edges of e+ and e?. An argu-
ment for not penalizing the sub-edge scores for e?
is that the sub-edges must be gold-standard edges
(since the training process is constructed so that
only gold-standard edges are expanded). From
739
the perspective of correctness, it is unnecessary
to find a margin between the sub-edges of e+ and
those of e?, since both are gold-standard edges.
However, since the score of an edge not only
represents its correctness, but also affects its pri-
ority on the agenda, promoting the sub-edge of
e+ can lead to ?easier? edges being constructed
before ?harder? ones (i.e. those that are less
likely to be correct), and therefore improve the
output accuracy. This perspective has been ob-
served by other works of learning-guided-search
(Shen et al 2007; Shen and Joshi, 2008; Gold-
berg and Elhadad, 2010). Intuitively, the score
difference between easy gold-standard and harder
gold-standard edges should not be as great as the
difference between gold-standard and non-gold-
standard edges. The perceptron update cannot
provide such control of separation, because the
amount of update is fixed to 1.
As described earlier, we treat parameter update
as finding a separation between correct and incor-
rect edges, in which the global feature vectors ?,
rather than ?, are considered. Given a positive ex-
ample e+ and a negative example e?, we make a
minimum update so that the score of e+ is higher
than that of e? with some margin:
? ? argmin
??
? ????0 ?, s.t.?(e+)????(e?)?? ? 1
where ?0 and ? denote the parameter vectors be-
fore and after the udpate, respectively. The up-
date is similar to the update of online large-margin
learning algorithms such as 1-best MIRA (Cram-
mer et al 2006), and has a closed-form solution:
? ? ?0+
f(e?)? f(e+) + 1
? ?(e+)? ?(e?) ?2
(
?(e+)??(e?)
)
In this update, the global feature vectors ?(e+)
and ?(e?) are used. Unlike Z&C, the scores
of sub-edges of e+ and e? are also udpated, so
that the sub-edges of e? are less prioritized than
those of e+. We show empirically that this train-
ing algorithm significantly outperforms the per-
ceptron training of the baseline system in Sec-
tion 5. An advantage of our new training algo-
rithm is that it enables the accommodation of a
separately trained N -gram model into the system.
4 Incorporating an N-gram language
model
Since the seminal work of the IBM models
(Brown et al 1993), N -gram language models
have been used as a standard component in statis-
tical machine translation systems to control out-
put fluency. For the syntax-based generation sys-
tem, the incorporation of an N -gram language
model can potentially improve the local fluency
of output sequences. In addition, the N -gram
language model can be trained separately using
a large amount of data, while the syntax-based
model requires manual annotation for training.
The standard method for the combination of
a syntax model and an N -gram model is linear
interpolation. We incorporate fourgram, trigram
and bigram scores into our syntax model, so that
the score of an edge e becomes:
F (e) = f(e) + g(e)
= f(e) + ? ? gfour(e) + ? ? gtri(e) + ? ? gbi(e),
where f is the syntax model score, and g is the
N -gram model score. g consists of three com-
ponents, gfour, gtri and gbi, representing the log-
probabilities of fourgrams, trigrams and bigrams
from the language model, respectively. ?, ? and
? are the corresponding weights.
During decoding, F (e) is computed incremen-
tally. Again, denoting the sub-edges of e as es,
F (e) = f(e) + g(e)
=
(
?
es?e
F (es)
)
+ ?(e)? + g?(e)
Here g?(e) = ? ?g?four(e)+? ?g?tri(e)+? ?g?bi(e)
is the sum of log-probabilities of the new N -
grams resulting from the construction of e. For
leaf edges and unary-branching edges, no new N -
grams result from their construction (i.e. g? = 0).
For a binary-branching edge, new N -grams result
from the surface-string concatenation of its sub-
edges. The sum of log-probabilities of the new
fourgrams, trigrams and bigrams contribute to g?
with weights ?, ? and ?, respectively.
For training, there are at least three methods to
tune ?, ?, ? and ?. One simple method is to train
the syntax model ? independently, and select ?,
?, and ? empirically from a range of candidate
values according to development tests. We call
this method test-time interpolation. An alterna-
tive is to select ?, ? and ? first, initializing the
vector ? as all zeroes, and then run the training
algorithm for ? taking into account the N -gram
language model. In this process, g is considered
when finding a separation between positive and
740
negative examples; the training algorithm finds a
value of ? that best suits the precomputed ?, ?
and ? values, together with the N -gram language
model. We call this method g-precomputed in-
terpolation. Yet another method is to initialize ?,
?, ? and ? as all zeroes, and run the training al-
gorithm taking into account the N -gram language
model. We call this method g-free interpolation.
The incorporation of an N -gram language
model into the syntax-based generation system is
weakly analogous to N -gram model insertion for
syntax-based statistical machine translation sys-
tems, both of which apply a score from the N -
gram model component in a derivation-building
process. As discussed earlier, polynomial-time
decoding is typically feasible for syntax-based
machine translation systems without an N -gram
language model, due to constraints from the
grammar. In these cases, incorporation of N -
gram language models can significantly increase
the complexity of a dynamic-programming de-
coder (Bar-Hillel et al 1961). Efficient search
has been achieved using chart pruning (Chiang,
2007) and iterative numerical approaches to con-
strained optimization (Rush and Collins, 2011).
In contrast, the incorporation of an N -gram lan-
guage model into our decoder is more straightfor-
ward, and does not add to its asymptotic complex-
ity, due to the heuristic nature of the decoder.
5 Experiments
We use sections 2?21 of CCGBank to train our
syntax model, section 00 for development and
section 23 for the final test. Derivations from
CCGBank are transformed into inputs by turn-
ing their surface strings into multi-sets of words.
Following Z&C, we treat base noun phrases (i.e.
NPs that do not recursively contain other NPs) as
atomic units for the input. Output sequences are
compared with the original sentences to evaluate
their quality. We follow previous work and use
the BLEU metric (Papineni et al 2002) to com-
pare outputs with references.
Z&C use two methods to construct leaf edges.
The first is to assign lexical categories according
to a dictionary. There are 26.8 lexical categories
for each word on average using this method, cor-
responding to 26.8 leaf edges. The other method
is to use a pre-processing step ? a CCG supertag-
ger (Clark and Curran, 2007) ? to prune can-
didate lexical categories according to the gold-
CCGBank Sentences Tokens
training 39,604 929,552
development 1,913 45,422
GigaWord v4 Sentences Tokens
AFP 30,363,052 684,910,697
XIN 15,982,098 340,666,976
Table 1: Number of sentences and tokens by language
model source.
standard sequence, assuming that for some prob-
lems the ambiguities can be reduced (e.g. when
the input is already partly correctly ordered).
Z&C use different probability cutoff levels (the
? parameter in the supertagger) to control the
pruning. Here we focus mainly on the dictionary
method, which leaves lexical category disam-
biguation entirely to the generation system. For
comparison, we also perform experiments with
lexical category pruning. We chose ? = 0.0001,
which leaves 5.4 leaf edges per word on average.
We used the SRILM Toolkit (Stolcke, 2002)
to build a true-case 4-gram language model es-
timated over the CCGBank training and develop-
ment data and a large additional collection of flu-
ent sentences in the Agence France-Presse (AFP)
and Xinhua News Agency (XIN) subsets of the
English GigaWord Fourth Edition (Parker et al
2009), a total of over 1 billion tokens. The Gi-
gaWord data was first pre-processed to replicate
the CCGBank tokenization. The total number
of sentences and tokens in each LM component
is shown in Table 1. The language model vo-
cabulary consists of the 46,574 words that oc-
cur in the concatenation of the CCGBank train-
ing, development, and test sets. The LM proba-
bilities are estimated using modified Kneser-Ney
smoothing (Kneser and Ney, 1995) with interpo-
lation of lower n-gram orders.
5.1 Development experiments
A set of development test results without lexical
category pruning (i.e. using the full dictionary) is
shown in Table 2. We train the baseline system
and our systems under various settings for 10 iter-
ations, and measure the output BLEU scores after
each iteration. The timeout value for each sen-
tence is set to 5 seconds. The highest score (max
BLEU) and averaged score (avg. BLEU) of each
system over the 10 training iterations are shown
in the table.
741
Method max BLEU avg. BLEU
baseline 38.47 37.36
margin 41.20 39.70
margin +LM (g-precomputed) 41.50 40.84
margin +LM (? = 0, ? = 0, ? = 0) 40.83 ?
margin +LM (? = 0.08, ? = 0.016, ? = 0.004) 38.99 ?
margin +LM (? = 0.4, ? = 0.08, ? = 0.02) 36.17 ?
margin +LM (? = 0.8, ? = 0.16, ? = 0.04) 34.74 ?
Table 2: Development experiments without lexical category pruning.
The first three rows represent the baseline sys-
tem, our largin-margin training system (margin),
and our system with the N -gram model incorpo-
rated using g-precomputed interpolation. For in-
terpolation we manually chose ? = 0.8, ? = 0.16
and ? = 0.04, respectively. These values could
be optimized by development experiments with
alternative configurations, which may lead to fur-
ther improvements. Our system with large-margin
training gives higher BLEU scores than the base-
line system consistently over all iterations. The
N -gram model led to further improvements.
The last four rows in the table show results
of our system with the N -gram model added us-
ing test-time interpolation. The syntax model is
trained with the optimal number of iterations, and
different ?, ?, and ? values are used to integrate
the language model. Compared with the system
using no N -gram model (margin), test-time inter-
polation did not improve the accuracies.
The row with ?, ?, ? = 0 represents our system
with the N -gram model loaded, and the scores
gfour , gtri and gbi computed for each N -gram
during decoding, but the scores of edges are com-
puted without using N -gram probabilities. The
scoring model is the same as the syntax model
(margin), but the results are lower than the row
?margin?, because computing N -gram probabil-
ities made the system slower, exploring less hy-
potheses under the same timeout setting.1
The comparison between g-precomputed inter-
polation and test-time interpolation shows that the
system gives better scores when the syntax model
takes into consideration the N -gram model during
1More decoding time could be given to the slower N -
gram system, but we use 5 seconds as the timeout setting
for all the experiments, giving the methods with the N -gram
language model a slight disadvantage, as shown by the two
rows ?margin? and ?margin +LM (?, ?, ? = 0).
 37
 38
 39
 40
 41
 42
 43
 44
 45
 1  2  3  4  5  6  7  8  9  10
BL
EU
training iteration
baseline
margin
margin +LM
Figure 1: Development experiments with lexical cate-
gory pruning (? = 0.0001).
training. One question that arises is whether g-
free interpolation will outperform g-precomputed
interpolation. g-free interpolation offers the free-
dom of ?, ? and ? during training, and can poten-
tially reach a better combination of the parameter
values. However, the training algorithm failed to
converge with g-free interpolation. One possible
explanation is that real-valued features from the
language model made our large-margin training
harder. Another possible reason is that our train-
ing process with heavy pruning does not accom-
modate this complex model.
Figure 1 shows a set of development experi-
ments with lexical category pruning (with the su-
pertagger parameter ? = 0.0001). The scores
of the three different systems are calculated by
varying the number of training iterations. The
large-margin training system (margin) gave con-
sistently better scores than the baseline system,
and adding a language model (margin +LM) im-
proves the scores further.
Table 3 shows some manually chosen examples
for which our system gave significant improve-
ments over the baseline. For most other sentences
the improvements are not as obvious. For each
742
baseline margin margin +LM
as a nonexecutive director Pierre Vinken
, 61 years old , will join the board . 29
Nov.
61 years old , the board will join as a
nonexecutive director Nov. 29 , Pierre
Vinken .
as a nonexecutive director Pierre Vinken
, 61 years old , will join the board Nov.
29 .
Lorillard nor smokers were aware of the
Kent cigarettes of any research on the
workers who studied the researchers
of any research who studied Neither the
workers were aware of smokers on the
Kent cigarettes nor the researchers
Neither Lorillard nor any research on the
workers who studied the Kent cigarettes
were aware of smokers of the researchers
.
you But 35 years ago have to recognize
that these events took place .
recognize But you took place that these
events have to 35 years ago .
But you have to recognize that these
events took place 35 years ago .
investors to pour cash into money funds
continue in Despite yields recent declines
Despite investors , yields continue to
pour into money funds recent declines in
cash .
Despite investors , recent declines in
yields continue to pour cash into money
funds .
yielding The top money funds are cur-
rently well over 9 % .
The top money funds currently are yield-
ing well over 9 % .
The top money funds are yielding well
over 9 % currently .
where A buffet breakfast , held in the mu-
seum was food and drinks to . everyday
visitors banned
everyday visitors are banned to where
A buffet breakfast was held , food and
drinks in the museum .
A buffet breakfast , everyday visitors are
banned to where food and drinks was
held in the museum .
A Commonwealth Edison spokesman
said an administrative nightmare would
be tracking down the past 3 12 years that
the two million customers have . whose
changed
tracking A Commonwealth Edison
spokesman said that the two million cus-
tomers whose addresses have changed
down during the past 3 12 years would
be an administrative nightmare .
an administrative nightmare whose ad-
dresses would be tracking down A Com-
monwealth Edison spokesman said that
the two million customers have changed
during the past 3 12 years .
The $ 2.5 billion Byron 1 plant , Ill. , was
completed . near Rockford in 1985
The $ 2.5 billion Byron 1 plant was near
completed in Rockford , Ill. , 1985 .
The $ 2.5 billion Byron 1 plant near
Rockford , Ill. , was completed in 1985 .
will ( During its centennial year , The
Wall Street Journal report events of the
past century that stand as milestones of
American business history . )
as The Wall Street Journal ( During its
centennial year , milestones stand of
American business history that will re-
port events of the past century . )
During its centennial year events will re-
port , The Wall Street Journal that stand
as milestones of American business his-
tory ( of the past century ) .
Table 3: Some chosen examples with significant improvements (supertagger parameter ? = 0.0001).
method, the examples are chosen from the devel-
opment output with lexical category pruning, af-
ter the optimal number of training iterations, with
the timeout set to 5s. We also tried manually se-
lecting examples without lexical category prun-
ing, but the improvements were not as obvious,
partly because the overall fluency was lower for
all the three systems.
Table 4 shows a set of examples chosen ran-
domly from the development test outputs of our
system with the N -gram model. The optimal
number of training iterations is used, and a time-
out of 1 minute is used in addition to the 5s time-
out for comparison. With more time to decode
each input, the system gave a BLEU score of
44.61, higher than 41.50 with the 5s timout.
While some of the outputs we examined are
reasonably fluent, most are to some extent frag-
mentary.2 In general, the system outputs are
still far below human fluency. Some samples are
2Part of the reason for some fragmentary outputs is the
default output mechanism: partial derivations from the chart
are greedily put together when timeout occurs before a goal
hypothesis is found.
syntactically grammatical, but are semantically
anomalous. For example, person names are often
confused with company names, verbs often take
unrelated subjects and objects. The problem is
much more severe for long sentences, which have
more ambiguities. For specific tasks, extra infor-
mation (such as the source text for machine trans-
lation) can be available to reduce ambiguities.
6 Final results
The final results of our system without lexical cat-
egory pruning are shown in Table 5. Row ?W09
CLE? and ?W09 AB? show the results of the
maximum spanning tree and assignment-based al-
gorithms of Wan et al(2009); rows ?margin?
and ?margin +LM? show the results of our large-
margin training system and our system with the
N -gram model. All these results are directly com-
parable since we do not use any lexical category
pruning for this set of results. For each of our
systems, we fix the number of training iterations
according to development test scores. Consis-
tent with the development experiments, our sys-
743
timeout = 5s timeout = 1m
drooled the cars and drivers , like Fortune 500 executives . over
the race
After schoolboys drooled over the cars and drivers , the race
like Fortune 500 executives .
One big reason : thin margins . One big reason : thin margins .
You or accountants look around ... and at an eye blinks . pro-
fessional ballplayers
blinks nobody You or accountants look around ... and at an eye
. professional ballplayers
most disturbing And of it , are educators , not students , for the
wrongdoing is who .
And blamed for the wrongdoing , educators , not students who
are disturbing , much of it is most .
defeat coaching aids the purpose of which is , He and other
critics say can to . standardized tests learning progress
gauge coaching aids learning progress can and other critics say
the purpose of which is to defeat , standardized tests .
The federal government of government debt because Congress
has lifted the ceiling on U.S. savings bonds suspended sales
The federal government suspended sales of government debt
because Congress has n?t lifted the ceiling on U.S. savings
bonds .
Table 4: Some examples chosen at random from development test outputs without lexical category pruning.
System BLEU
W09 CLE 26.8
W09 AB 33.7
Z&C11 40.1
margin 42.5
margin +LM 43.8
Table 5: Test results without lexical category pruning.
System BLEU
Z&C11 43.2
margin 44.7
margin +LM 46.1
Table 6: Test results with lexical category pruning (su-
pertagger parameter ? = 0.0001).
tem outperforms the baseline methods. The acu-
racies are significantly higher when the N -gram
model is incorporated.
Table 6 compares our system with Z&C using
lexical category pruning (? = 0.0001) and a 5s
timeout for fair comparison. The results are sim-
ilar to Table 5: our large-margin training systems
outperforms the baseline by 1.5 BLEU points, and
adding the N -gram model gave a further 1.4 point
improvement. The scores could be significantly
increased by using a larger timeout, as shown in
our earlier development experiments.
7 Related Work
There is a recent line of research on text-to-
text generation, which studies the linearization of
dependency structures (Barzilay and McKeown,
2005; Filippova and Strube, 2007; Filippova and
Strube, 2009; Bohnet et al 2010; Guo et al
2011). Unlike our system, and Wan et al(2009),
input dependencies provide additional informa-
tion to these systems. Although the search space
can be constrained by the assumption of projec-
tivity, permutation of modifiers of the same head
word makes exact inference for tree lineariza-
tion intractable. The above systems typically ap-
ply approximate inference, such as beam-search.
While syntax-based features are commonly used
by these systems for linearization, Filippova and
Strube (2009) apply a trigram model to control
local fluency within constituents. A dependency-
based N-gram model has also been shown effec-
tive for the linearization task (Guo et al 2011).
The best-first inference and timeout mechanism
of our system is similar to that of White (2004), a
surface realizer from logical forms using CCG.
8 Conclusion
We studied the problem of word-ordering using
a syntactic model and allowing permutation. We
took the model of Zhang and Clark (2011) as the
baseline, and extended it with online large-margin
training and an N -gram language model. These
extentions led to improvements in the BLEU eval-
uation. Analyzing the generated sentences sug-
gests that, while highly fluent outputs can be pro-
duced for short sentences (? 10 words), the sys-
tem fluency in general is still way below human
standard. Future work remains to apply the sys-
tem as a component for specific text generation
tasks, for example machine translation.
Acknowledgements
Yue Zhang and Stephen Clark are supported by the Eu-
ropean Union Seventh Framework Programme (FP7-
ICT-2009-4) under grant agreement no. 247762.
744
References
Yehoshua Bar-Hillel, M. Perles, and E. Shamir. 1961.
On formal properties of simple phrase structure
grammars. Zeitschrift fu?r Phonetik, Sprachwis-
senschaft und Kommunikationsforschung, 14:143?
172. Reprinted in Y. Bar-Hillel. (1964). Language
and Information: Selected Essays on their Theory
and Application, Addison-Wesley 1964, 116?150.
Regina Barzilay and Kathleen McKeown. 2005. Sen-
tence fusion for multidocument news summariza-
tion. Computational Linguistics, 31(3):297?328.
Graeme Blackwood, Adria` de Gispert, and William
Byrne. 2010. Fluency constraints for minimum
Bayes-risk decoding of statistical machine trans-
lation lattices. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics
(Coling 2010), pages 71?79, Beijing, China, Au-
gust. Coling 2010 Organizing Committee.
Bernd Bohnet, Leo Wanner, Simon Mill, and Alicia
Burga. 2010. Broad coverage multilingual deep
sentence generation with a stochastic multi-level re-
alizer. In Proceedings of the 23rd International
Conference on Computational Linguistics (Coling
2010), pages 98?106, Beijing, China, August. Col-
ing 2010 Organizing Committee.
Peter F. Brown, Stephen Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathe-
matics of statistical machine translation: Parameter
estimation. Computational Linguistics, 19(2):263?
311.
Sharon A. Caraballo and Eugene Charniak. 1998.
New figures of merit for best-first probabilistic chart
parsing. Comput. Linguist., 24:275?298, June.
David Chiang. 2007. Hierarchical Phrase-
based Translation. Computational Linguistics,
33(2):201?228.
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG
and log-linear models. Computational Linguistics,
33(4):493?552.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. Online
passive-aggressive algorithms. Journal of Machine
Learning Research, 7:551?585.
Katja Filippova and Michael Strube. 2007. Gener-
ating constituent order in german clauses. In Pro-
ceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics, pages 320?
327, Prague, Czech Republic, June. Association for
Computational Linguistics.
Katja Filippova and Michael Strube. 2009. Tree lin-
earization in english: Improving language model
based approaches. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, Companion Volume:
Short Papers, pages 225?228, Boulder, Colorado,
June. Association for Computational Linguistics.
Yoav Goldberg and Michael Elhadad. 2010. An effi-
cient algorithm for easy-first non-directional depen-
dency parsing. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 742?750, Los Angeles, California,
June. Association for Computational Linguistics.
Yuqing Guo, Deirdre Hogan, and Josef van Genabith.
2011. Dcu at generation challenges 2011 surface
realisation track. In Proceedings of the Generation
Challenges Session at the 13th European Workshop
on Natural Language Generation, pages 227?229,
Nancy, France, September. Association for Compu-
tational Linguistics.
Julia Hockenmaier and Mark Steedman. 2002. Gen-
erative models for statistical parsing with Combi-
natory Categorial Grammar. In Proceedings of the
40th Meeting of the ACL, pages 335?342, Philadel-
phia, PA.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: A corpus of CCG derivations and dependency
structures extracted from the Penn Treebank. Com-
putational Linguistics, 33(3):355?396.
R. Kneser and H. Ney. 1995. Improved backing-off
for m-gram language modeling. In International
Conference on Acoustics, Speech, and Signal Pro-
cessing, 1995. ICASSP-95, volume 1, pages 181?
184.
Philip Koehn, Franz Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings
of NAACL/HLT, Edmonton, Canada, May.
Philipp Koehn, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Chris Dyer, Ondrej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical ma-
chine translation. In Proceedings of the 45th An-
nual Meeting of the Association for Computational
Linguistics Companion Volume Proceedings of the
Demo and Poster Sessions, pages 177?180, Prague,
Czech Republic, June. Association for Computa-
tional Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. Bleu: a method for auto-
matic evaluation of machine translation. In Pro-
ceedings of 40th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 311?318,
Philadelphia, Pennsylvania, USA, July. Association
for Computational Linguistics.
Robert Parker, David Graff, Junbo Kong, Ke Chen, and
Kazuaki Maeda. 2009. English Gigaword Fourth
Edition, Linguistic Data Consortium.
Alexander M. Rush and Michael Collins. 2011. Exact
decoding of syntactic translation models through la-
745
grangian relaxation. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
72?82, Portland, Oregon, USA, June. Association
for Computational Linguistics.
Libin Shen and Aravind Joshi. 2008. LTAG depen-
dency parsing with bidirectional incremental con-
struction. In Proceedings of the 2008 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 495?504, Honolulu, Hawaii, Octo-
ber. Association for Computational Linguistics.
Libin Shen, Giorgio Satta, and Aravind Joshi. 2007.
Guided learning for bidirectional sequence classi-
fication. In Proceedings of ACL, pages 760?767,
Prague, Czech Republic, June.
Mark Steedman. 2000. The Syntactic Process. The
MIT Press, Cambridge, Mass.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Proceedings of the In-
ternational Conference on Spoken Language Pro-
cessing, pages 901?904.
Stephen Wan, Mark Dras, Robert Dale, and Ce?cile
Paris. 2009. Improving grammaticality in statisti-
cal sentence generation: Introducing a dependency
spanning tree algorithm with an argument satisfac-
tion model. In Proceedings of the 12th Conference
of the European Chapter of the ACL (EACL 2009),
pages 852?860, Athens, Greece, March. Associa-
tion for Computational Linguistics.
Michael White. 2004. Reining in CCG chart realiza-
tion. In Proc. INLG-04, pages 182?191.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3).
Yue Zhang and Stephen Clark. 2011. Syntax-
based grammaticality improvement using CCG and
guided search. In Proceedings of the 2011 Confer-
ence on Empirical Methods in Natural Language
Processing, pages 1147?1157, Edinburgh, Scot-
land, UK., July. Association for Computational Lin-
guistics.
746
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 230?238,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Improving Distributional Semantic Vectors through Context Selection and
Normalisation
Tamara Polajnar
University of Cambridge
Computer Laboratory
tp366@cam.ac.uk
Stephen Clark
University of Cambridge
Computer Laboratory
sc609@cam.ac.uk
Abstract
Distributional semantic models (DSMs)
have been effective at representing seman-
tics at the word level, and research has re-
cently moved on to building distributional
representations for larger segments of text.
In this paper, we introduce novel ways of
applying context selection and normalisa-
tion to vary model sparsity and the range
of values of the DSM vectors. We show
how these methods enhance the quality of
the vectors and thus result in improved
low dimensional and composed represen-
tations. We demonstrate these effects on
standard word and phrase datasets, and on
a new definition retrieval task and dataset.
1 Introduction
Distributional semantic models (DSMs) (Turney
and Pantel, 2010; Clarke, 2012) encode word
meaning by counting co-occurrences with other
words within a context window and recording
these counts in a vector. Various IR and NLP
tasks, such as word sense disambiguation, query
expansion, and paraphrasing, take advantage of
DSMs at a word level. More recently, researchers
have been exploring methods that combine word
vectors to represent phrases (Mitchell and Lapata,
2010; Baroni and Zamparelli, 2010) and sentences
(Coecke et al., 2010; Socher et al., 2012). In this
paper, we introduce two techniques that improve
the quality of word vectors and can be easily tuned
to adapt the vectors to particular lexical and com-
positional tasks.
The quality of the word vectors is generally as-
sessed on standard datasets that consist of a list of
word pairs and a corresponding list of gold stan-
dard scores. These scores are gathered through an
annotation task and reflect the similarity between
the words as perceived by human judges (Bruni et
al., 2012). Evaluation is conducted by comparing
the word similarity predicted by the model with
the gold standard using a correlation test such as
Spearman?s ?.
While words, and perhaps some frequent
shorter phrases, can be represented by distri-
butional vectors learned through co-occurrence
statistics, infrequent phrases and novel construc-
tions are impossible to represent in that way. The
goal of compositional DSMs is to find methods of
combining word vectors, or perhaps higher-order
tensors, into a single vector that represents the
meaning of the whole segment of text. Elemen-
tary approaches to composition employ simple op-
erations, such as addition and elementwise prod-
uct, directly on the word vectors. These have been
shown to be effective for phrase similarity evalua-
tion (Mitchell and Lapata, 2010) and detection of
anomalous phrases (Kochmar and Briscoe, 2013).
The methods that will be introduced in this pa-
per can be applied to co-occurrence vectors to pro-
duce improvements on word similarity and com-
positional tasks with simple operators. We chose
to examine the use of sum, elementwise prod-
uct, and circular convolution (Jones and Mewhort,
2007), because they are often used due to their
simplicity, or as components of more complex
models (Zanzotto and Dell?Arciprete, 2011).
The first method is context selection (CS), in
which the top N highest weighted context words
per vector are selected, and the rest of the values
are discarded (by setting to zero). This technique
is similar to the way that Explicit Semantic Analy-
sis (ESA) (Gabrilovich and Markovitch, 2007) se-
lects the number of topics that represent a word,
and the word filtering approach in Gamallo and
Bordag (2011). It has the advantage of improv-
ing word representations and vector sum represen-
tations (for compositional tasks) while using vec-
tors with fewer non-zero elements. Programming
languages often have efficient strategies for stor-
230
ing these sparse vectors, leading to lower memory
usage. As an example of the resulting accuracy
improvements, when vectors with up to 10,000
non-zero elements are reduced to a maximum of
N  240 non-zero elements, the Spearman ? im-
proves from 0.61 to 0.76 on a standard word sim-
ilarity task. We also see an improvement when
used in conjunction with further, standard dimen-
sionality reduction techniques: the CS sparse vec-
tors lead to reduced-dimensional representations
that produce higher correlations with human simi-
larity judgements than the original full vectors.
The second method is a weighted l
2
-
normalisation of the vectors prior to application of
singular value decomposition (SVD) (Deerwester
et al., 1990) or compositional vector operators. It
has the effect of drastically improving SVD with
100 or fewer dimensions. For example, we find
that applying normalisation before SVD improves
correlation from ?  0.48 to ?  0.70 for 20
dimensions, on the word similarity task. This
is an essential finding as many more complex
models of compositional semantics (Coecke et al.,
2010; Baroni and Zamparelli, 2010; Andreas and
Ghahramani, 2013) work with tensor objects and
require good quality low-dimensional represen-
tations of words in order to lower computational
costs. This technique also improves the perfor-
mance of vector addition on texts of any length
and vector elementwise product on shorter texts,
on both the similarity and definitions tasks.
The definition task and dataset are an additional
contribution. We produced a new dataset of words
and their definitions, which is separated into nine
parts, each consisting of definitions of a particular
length. This allows us to examine how composi-
tional operators interact with CS and normalisa-
tion as the number of vector operations increases.
This paper is divided into three main sections.
Section 2 describes the construction of the word
vectors that underlie all of our experiments and the
two methods for adaptation of the vectors to spe-
cific tasks. In Section 3 we assess the effects of
CS and normalisation on standard word similar-
ity datasets. In Section 4 we present the compo-
sitional experiments on phrase data and our new
definitions dataset.
2 Word Vector Construction
The distributional hypothesis assumes that words
that occur within similar contexts share similar
meanings; hence semantic vector construction first
requires a defintition of context. Here we use
a window method, where the context is defined
as a particular sequence of words either side of
the target word. The vectors are then populated
through traversal of a large corpus, by recording
the number of times each of the target words co-
occurs with a context word within the window,
which gives the raw target-context co-occurrence
frequency vectors (Freq).
The rest of this section contains a description
of the particular settings used to construct the raw
word vectors and the weighting schemes (tTest,
PPMI) that we considered in our experiments.
This is followed by a detailed description of the
context selection (CS) and normalisation tech-
niques. Finally, dimensionality reduction (SVD) is
proposed as a way of combating sparsity and ran-
dom indexing (RI) as an essential step of encoding
vectors for use with the convolution operator.
Raw Vectors We used a cleaned-up corpus
of 1.7 billion lemmatised tokens (Minnen et
al., 2001) from the October, 2013 snapshot of
Wikipedia, and constructed context vectors by us-
ing sentence boundaries to provide the window.
The set of context wordsC consisted of the 10,000
most frequent words occurring in this dataset, with
the exception of stopwords from a standard stop-
word list. Therefore, a frequency vector for a tar-
get word w
i
PW is represented as ~w
i
 tf
w
i
c
j
u
j
,
where c
j
P C (|C|  10, 000), W is a set of target
words in a particular evaluation dataset, and f
w
i
c
j
is the co-occurrence frequency between the target
word, w
i
and context word, c
j
.
Vector Weighting We used the tTest and PPMI
weighting schemes, since they both performed
well on the development data. The vectors result-
ing from the application of the weighting schemes
are as follows, where the tTest and PPMI functions
give weighted values for the basis vector corre-
sponding to context word c
j
for target word w
i
:
tTestp ~w
i
, c
j
q 
ppw
i
, c
j
q  ppw
i
qppc
j
q
a
ppw
i
qppc
j
q
(1)
PPMIp ~w
i
, c
j
q  ppw
i
, c
j
q log

ppw
i
, c
j
q
ppw
i
qppc
j
q


(2)
where ppw
i
q 
?
j
f
w
i
c
j
?
k
?
l
f
w
k
c
l
, ppc
j
q 
?
i
f
w
i
c
j
?
k
?
l
f
w
k
c
l
, and
ppw
i
, c
j
q 
f
w
i
c
j
?
k
?
l
f
w
k
c
l
.
231
Original Normalised Normalised*10?1?0.5
00.5
11.5
22.5
33.5
Figure 1: The range of context weights on tTest
weighted vectors before and after normalisation.
Context Ranking and Selection The weight-
ing schemes change the importance of individ-
ual target-context raw co-occurrence counts by
considering the frequency with which each con-
text word occurs with other target words. This
is similar to term-weighting in IR and many re-
trieval functions are also used as weighting func-
tions in DSMs. In the retrieval-based model ESA
(Gabrilovich and Markovitch, 2007), only the N
highest-weighted contexts are kept as a represen-
tative set of ?topics? for a particular target word,
and the rest are set to zero. Here we use a sim-
ilar technique and, for each target word, retain
only the N -highest weighted context words, using
a word-similarity development set to choose the
N that maximises correlation across all words in
that dataset. Throughout the paper, we will refer
to this technique as context selection (CS) and use
N to indicate the maximum number of contexts
per word. Hence all word vectors have at most N
non-zero elements, effectively adjusting the spar-
sity of the vectors, which may have an effect on
the sum and elementwise product operations when
composing vectors.
Normalisation PPMI has only positive values
that span the range r0,8s, while tTest spans
r1, 1s, but generally produces values tightly con-
centrated around zero. We found that these ranges
can produce poor performance due to numerical
problems, so we corrected this through weighted
row normalisation: ~w : ?
~w
||~w||
2
. With ?  10 this
has the effect of restricting the values to r10, 10s
for tTest and r0, 10s for PPMI. Figure 1 shows the
range of values for tTest. In general we use ?  1,
but for some experiments we use ?  10 to push
the highest weights above 1, as a way of combat-
ing the numerical errors that are likely to arise due
to repeated multiplications of small numbers. This
normalisation has no effect on the ordering of con-
text weights or cosine similarity calculations be-
tween single-word vectors. We apply normalisa-
tion prior to dimensionality reduction and RI.
SVD SVD transforms vectors from their target-
context representation into a target-topic space.
The resulting space is dense, in that the vectors
no longer contain any zero elements. If M is a
|w|  |C| matrix whose rows are made of word
vectors ~w
i
, then the lower dimensional representa-
tion of those vectors is encoded in the |W |  K
matrix
?
M
K
 U
K
S
K
where SVDpM,Kq 
U
K
S
K
V
K
(Deerwester et al., 1990). We also
tried non-negative matrix factorisation (NNMF)
(Seung and Lee, 2001), but found that it did not
perform as well as SVD. We used the standard
Matlab implementation of SVD.
Random Indexing There are two ways of creat-
ing RI-based DSMs, the most popular being to ini-
tialise all target word vectors to zero and to gener-
ate a random vector for each context word. Then,
while traversing through the corpus, each time a
target word and a context word co-occur, the con-
text word vector is added to the vector represent-
ing the target word. This method allows the RI
vectors to be created in one step through a single
traversal of the corpus. The other method, follow-
ing Jones and Mewhort (2007), is to create the RI
vectors through matrix multiplication rather than
sequentially. We employ this method and assign
each context word a random vector ~e
c
j
 tr
k
u
k
where r
k
are drawn from the normal distribution
N p0,
1
D
q and | ~e
c
j
|  D  4096. The RI repre-
sentation of a target word RIp ~w
i
q  ~w
i
R is con-
structed by multiplying the word vector ~w
i
, ob-
tained as before, by the |C|  D matrix R where
each column represents the vectors ~e
c
j
. Weighting
is performed prior to random indexing.
3 Word Similarity Experiments
In this section we investigate the effects of context
selection and normalisation on the quality of word
vectors using standard word similarity datasets.
The datasets consist of word pairs and a gold stan-
dard score that indicates the human judgement of
the similarity between the words within each pair.
We calculated the similarity between word vectors
for each pair and compared our results with the
gold standard using Spearman correlation.
232
tTest PPMI Freq
Data Max ? Full ? Max ? Full ? Max ? Full ?
MENdev: 0.75 0.73 0.76 0.61 0.66 0.57
MENtest 0.76 0.73 0.76 0.61 0.66 0.56
WS353 0.70 0.63 0.70 0.41 0.57 0.41
Table 1: ValuesN learned on dev (:) also improve
performance on the test data. Max ? indicates cor-
relation at the values of N that lead to the high-
est Spearman correlation on the development data.
For each weighting scheme these are: 140 (tTest),
240 (PPMI), and 20 (Freq). Full ? indicates the
correlation when using full vectors without CS.
The cosine, Jaccard, and Lin similarity mea-
sures (Curran, 2004) were all used to ensure the
results reflect genuine effects of context selection,
and not an artefact of any particular similarity
measure. The similarity measure and value of N
were chosen, given a particular weighting scheme,
to maximise correlation on the development part
of the MEN data (Bruni et al., 2012) (MENdev).
Testing was performed on the remaining section
of MEN and the entire WS353 dataset (Finkelstein
et al., 2002). The MEN dataset consists of 3,000
word pairs rated for similarity, which is divided
into a 2,000-pair development set and a 1,000-pair
test set. WS353 consists only of 353 pairs, but has
been consistently used as a benchmark word simi-
larity dataset throughout the past decade.
Results Figure 2 shows how correlation varies
with N for the MEN development data. The
peak performance for tTest is achieved when using
around 140 top-ranked contexts per word, while
for PPMI it is at N  240, and for Freq N  20.
The dramatic drop in performance is demonstrated
when using all three similarity measures, although
Jaccard seems particularly sensitive to the nega-
tive tTest weights that are introduced when lower-
ranked contexts are added to the vectors. The re-
maining experiments only consider cosine similar-
ity. We also find that context selection improves
correlation for tTest, PPMI, and the unweighted
Freq vectors on the test data (Table 1). Moreover,
the lower the correlation from the full vectors, the
larger the improvement when using CS.
3.1 Dimensionality Reduction
Figure 3 shows the effects of dimensionality re-
duction described in the following experiments.
0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000
0.58
0.6
0.62
0.64
0.66
0.68
0.7
0.72
0.74
0.76
Maximum nonzero elements per vector
Spearm
an
 
 
ttestppmifreqmaxmaxmax
0 1000 2000 3000 4000 5000 6000 7000 8000 9000 100000.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Maximum nonzero elements per vector
Spearman
 
 
ttestppmifreqmaxmaxmax
0 1000 2000 3000 4000 5000 6000 7000 8000 9000 100000
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Maximum nonzero elements per vector
Spearman
 
 ttestppmifreqmaxmaxmax
Figure 2: Correlation decreases as more lower-
ranked context words are introduced (MENdev),
with cosine (top), Lin (bottom left), and Jaccard
(bottom right) simialrity measures.
3.1.1 SVD and CS
To check whether CS improves the correlation
through increased sparsity or whether it improves
the contextual representation of the words, we in-
vestigated the behaviour of SVD on three differ-
ent levels of vector sparsity. To construct the most
sparse vectors, we chose the best performing N
for each weighting scheme (from Table 1). Thus
sparse tTest vectors had
140
10000
 0.0140, or 1.4%,
non-zero elements. We also chose a mid-range
of N  3300 for up to 33% of non-zero ele-
ments per vector, and finally the full vectors with
N  10000.
Results In general the CS-tuned vectors lead
to better lower-dimensional representations. The
mid-range contexts in the tTest weighting scheme
seem to hold information that hinders SVD, while
the lowest-ranked negative weights appear to help
(when the mid-range contexts are present as well).
For the PPMI weighting, fewer contexts consis-
tently lead to better representations, while the un-
weighted vectors seem to mainly hold information
in the top 20 most frequent contexts for each word.
3.1.2 SVD, CS, and Normalisation
We also consider the combination of normalisation
and context selection followed by SVD.
233
0 100 200 300 400 500 600 700 8000.3
0.4
0.5
0.6
0.7
0.8
Number of dimensions (K)
Spearma
n
tTest
 
 
140N=3300N=10000norm 140norm N=3300norm N=10000all 140all N=3300all N=10000 0 100 200 300 400 500 600 700 8000.10.2
0.30.4
0.50.6
0.70.8
Number of dimensions (K)
Spearma
n
PPMI
 
 
240N=3300N=10000norm 240norm N=3300norm N=10000all 240all N=3300all N=10000 0 100 200 300 400 500 600 700 8000.20.3
0.4
0.5
0.6
0.7
Number of dimensions (K)
Spearma
n
Freq
 
 
20N=3300N=10000norm 20norm N=3300norm N=10000all 20all N=3300all N=10000
Figure 3: Vectors tuned for sparseness (blue) consistently produce equal or better dimensionality reduc-
tions (results on MENdev). The solid lines show improvement in lower dimensional representations of
SVD when dimensionality reduction is applied after normalisation.
Results Normalisation leads to more stable SVD
representations, with a large improvement for
small numbers of dimensions (K) as demonstrated
by the solid lines in Figure 3. At K  20 the
Spearman correlation increases from 0.61 to 0.71.
In addition, for tTest there is an improvement in
the mid-range vectors, and a knock-on effect for
the full vectors. As the tTest values effectively
range from 0.1 to 0.1, the mid-range values are
very small numbers closely grouped around zero.
Normalisation spreads and increases these num-
bers, perhaps making them more relevant to the
SVD algorithm. The effect is also visible for
PPMI weighting where at K  20 the correlation
increases from 0.48 to 0.70. For PPMI and Freq
we also see that, for the full and mid-range vec-
tors, the SVD representations have slightly higher
correlations than the unreduced vectors.
3.2 Random Indexing
We use random indexing primarily to produce a
vector representation for convolution (Section 4).
While this produces a lower-dimensional repre-
sentation, it may not use less memory since the re-
sulting vectors, although smaller, are fully dense.
In summary, the RI encoded vectors with di-
mensions of D  4096 lead to only slightly re-
duced correlation values compared to their unen-
coded counterparts. We find that for tTest we
get similar performance with or without CS at
any level, while for PPMI CS helps especially for
D ? 512. On Freq we find that CS with N  60
leads to higher correlation, but mid-range and full
vectors have equivalent performance. For Freq,
the correlation is equivalent to full vectors from
D  128, while for the weighted vectors 512 di-
mensions appear to be sufficient. Unlike for SVD,
normalisation slightly reduces the performance for
mid-range dimensions.
4 Compositional Experiments
We examine the performance of vectors aug-
mented by CS and normalisation in two compo-
sitional tasks. The first is an extension of the word
similarity task to phrase pairs, using the dataset
of Mitchell and Lapata (2010). Each entry in the
dataset consists of two phrases, each consisting of
two words (in various syntactic relations, such as
verb-object and adjective noun), and a gold stan-
dard score. We combine the two word vectors into
a single phrase vector using various operators de-
scribed below. We then calculate the similarity
between the phrase vectors using cosine and com-
pare the resulting scores against the gold standard
using Spearman correlation. The second task is
our new definitions task where, again, word vec-
tors from each definition are composed to form a
single vector, which can then be compared for sim-
ilarity with the target term.
We use PPMI- and tTest-weighted vectors at
three CS cutoff points: the best chosen N from
Section 3, the top third of the ranked contexts at
N  3300, and the full vectors without CS at
N  10000. This gives us a range of values to
examine, without directly tuning on this dataset.
For dimensionality reduction we consider vectors
reduced with SVD to 100 and 700 dimensions. In
some cases we exclude the results for SVD
700
be-
cause they are very close to the scores for unre-
duced vectors. We experiment with 3 values of D
from t512, 1024, 4096u for the RI vectors.
Operators To combine distributional vectors
into a single-vector sentence representation, we
use a representative set of methods from Mitchell
and Lapata (2010). In particular, we use vector
addition, elementwise (Hadamard) product, Kro-
necker product, and circular convolution (Plate,
1991; Jones and Mewhort, 2007), which are de-
234
fined as follows for two word vectors ~x, ~y:
Sum ~x  ~y  t~x
i
  ~y
i
u
i
Prod ~xd ~y  t~x
i
 ~y
i
u
i
Kron ~xb ~y  t~x
i
 ~y
j
u
ij
Conv ~xg ~y 
!
?
n
j0
p~xq
j%n
 p~yq
pijq%n
)
i
Repeated application of the Sum operation adds
contexts for each of the words that occur in a
phrase, which maintains (and mixes) any noisy
parts of the component word vectors. Our inten-
tion was that use of the CS vectors would lead
to less noisy word vectors and hence less noisy
phrase and sentence vectors. The Prod operator,
on the other hand, provides a phrase or sentence
representation consisting only of the contexts that
are common to all of the words in the sentence
(since zeros in any of the word vectors lead to
zeros in the same position in the sentence vec-
tor). This effect is particularly problematic for rare
words which may have sparse vectors, leading to
a sparse vector for the sentence.
1
We address the
sparsity problem through the use of dimensional-
ity reduction, which produces more dense vectors.
Kron, the Kronecker (or tensor) product of two
vectors, produces a matrix (second order tensor)
whose diagonal matches the result of the Prod
operation, but whose off-diagonal entries are all
the other products of elements of the two vectors.
We only apply Kron to SVD-reduced vectors, and
to compare two matrices we turn them into vec-
tors by concatenating matrix rows, and use co-
sine similarity on the resulting vectors. While in
the more complex, type-driven methods (Baroni
and Zamparelli, 2010; Coecke et al., 2010) ten-
sors represent functions, and off-diagonal entries
have a particular transformational interpretation as
part of a linear map, the significance of the off-
diagonal elements is difficult to interpret in our
setting, apart from their role as encoders of the or-
der of operands. We only examine Kron as the un-
encoded version of the Conv operator to see how
the performance is affected by the random index-
ing and the modular summation by which Conv
differs from Kron.
2
We cannot use Kron for com-
bining more than two words as the size of the re-
sulting tensor grows exponentially with the num-
1
Sparsity is a problem that may be addressable through
smoothing (Zhai and Lafferty, 2001), although we do not in-
vestigate that avenue in this paper.
2
Conv also differs from Kron in that it is commutative,
unless one of the operands is permuted. In this paper we do
not permute the operands.
Oper N=140 N=3300 N=10000
sum
ttest 0.40 (0.41) 0.40 (0.40) 0.40 (0.40)
SVD
100
0.37 (0.42) 0.35 (0.41) 0.37 (0.40)
prod
ttest 0.32 (0.32) 0.40 (0.40) 0.32 (0.32)
SVD
100
0.25 (0.23) 0.23 (0.23) 0.21 (0.23)
kron
SVD
100
0.31 (0.34) 0.34 (0.38) 0.29 (0.32)
SVD
700
0.39 (0.39) 0.37 (0.37) 0.30 (0.30)
conv
RI
512
0.10 (0.12) 0.26 (0.21) 0.25 (0.25)
RI
1024
0.22 (0.15) 0.29 (0.27) 0.25 (0.26)
RI
4096
0.16 (0.19) 0.33 (0.34) 0.28 (0.30)
Table 2: Behaviour of vector operators with tTest
vectors on ML2010 (Spearman correlation). Val-
ues for normalised vectors in parentheses.
Oper N=240 N=3300 N=10000
sum
ppmi 0.40 (0.39) 0.40 (0.39) 0.29 (0.29)
SVD
100
0.40 (0.40) 0.38 (0.40) 0.29 (0.30)
prod
ppmi 0.28 (0.28) 0.40 (0.40) 0.30 (0.30)
SVD
100
0.23 (0.17) 0.18 (0.22) 0.14 (0.12)
kron
SVD
100
0.37 (0.30) 0.36 (0.38) 0.27 (0.27)
SVD
700
0.38 (0.37) 0.37 (0.37) 0.26 (0.26)
conv
RI
512
0.09 (0.09) 0.27 (0.30) 0.25 (0.24)
RI
1024
0.08 (0.14) 0.33 (0.37) 0.25 (0.27)
RI
4096
0.18 (0.19) 0.37 (0.38) 0.27 (0.27)
Table 3: Behaviour of vector operators with PPMI
vectors on ML2010 (Spearman correlation). Val-
ues for normalised vectors in parentheses.
ber of vector operations, but we can use Conv as
an encoded alternative as it results in a vector of
the same dimension as the two operands.
4.1 Phrase Similarity
To test how CS, normalisation, and dimensional-
ity reduction affect simple compositional vector
operations we use the test portion of the phrasal
similarity dataset from Mitchell and Lapata (2010)
(ML2010). This dataset consists of pairs of two-
word phrases and a human similarity judgement
on the scale of 1-7. There are three types of
phrases: noun-noun, adjective-noun, and verb-
object. In the original paper, and some subse-
quent works, these were treated as three different
datasets; however, here we combine the datasets
into one single phrase pair dataset. This allows us
to summarise the effects of different types of vec-
tors on phrasal composition in general.
Results Our results (Tables 2 and 3) are compa-
rable to those in Mitchell and Lapata (2010) av-
eraged across the phrase-types (?  0.44), but
are achieved with much smaller vectors. We find
that with normalisation, and the optimal choice
of N , there is little difference between Prod and
Sum. Sum and Kron benefit from normalisa-
tion, especially in combination with SVD, but for
Prod it either makes no difference or reduces per-
formance. Product-based methods (Prod, Kron,
235
Conv) have a preference for context selection that
includes the mid-rank contexts (N  3300), but
not the full vector (N  10000). On tTest vec-
tors Sum is relatively stable across different CS
and SVD settings, but with PPMI weighting, there
is a preference for lower N . SVD reduces perfor-
mance for Prod, but not for Kron. Finally, Conv
gets higher correlation with higher-dimensional RI
vectors and with PPMI weights.
4.2 Definition Retrieval
In this task, which is formulated as a retrieval task,
we investigate the behaviour of different vector
operators as multiple operations are chained to-
gether. We first encode each definition into a sin-
gle vector through repeated application of one of
the operators on the distributional vectors of the
content words in the definition. Then, for each
head (defined) word, we rank all the different defi-
nition vectors in decreasing order according to in-
ner product (unnormalised cosine) similarity with
the head word?s distributional vector.
Performance is measured using precision and
Mean Reciprocal Rank (MRR). If the correct defi-
nition is ranked first, the precision (P@1) is 1, oth-
erwise 0. Since there is only one definition per
head word, the reciprocal rank (RR) is the inverse
of the rank of the correct definition. So if the cor-
rect definition is ranked fourth, for example, then
RR is
1
4
. MRR is the average of the RR across all
head words.
The difficulty of the task depends on how many
words there are in the dataset and how similar their
definitions are. In addition, if a head word oc-
curs in the definition of another word in the same
dataset, it may cause the incorrect definition to be
ranked higher than the correct one. These prob-
lems are more likely to occur with higher fre-
quency words and in a larger dataset. In order
to counter these effects, we average our results
over ten repeated random samplings of 100 word-
definition pairs. The sampling also gives us a ran-
dom baseline for P@1 of 0.01300.0106 and for
MRR 0.0576  0.0170, which can be interpreted
as there is a chance of slightly more than 1 in 100
of ranking the correct definition first, and on aver-
age the correct definition is ranked around the 20
mark.
For this task all experiments were performed
using the tTest-weighted vectors. When applying
normalisation we use ?  1 (Norm) and ?  10
DD2 DD3 DD4 DD5 DD6 DD7 DD8 DD9 DD10
346 547 594 537 409 300 216 150 287
Table 4: Number of definitions per dataset.
(Norm10). In addition, we examine the effect of
continually applying Norm after every operation
(CNorm).
Dataset We developed a new dataset (DD) con-
sisting of 3,386 definitions from the Wiktionary
BNC spoken-word frequency list.
3
Most of the
words have several definitions, but we considered
only the first definition with at least two non-
stopwords. The word-definition pairs were di-
vided into nine separate datasets according to the
number of non-stopwords in the definition. For ex-
ample, all of the definitions that have five content
words are in DD5. The exception is DD10, which
contains all the definitions of ten or more words.
Table 4 shows the number of definitions in each
dataset.
Results Figure 4 shows how the MRR varies
with different DD datasets for Sum, Prod, and
Conv. The CS, SVD, and RI settings for each op-
erator correspond to the best average settings from
Table 5. In some cases other settings had simi-
lar performance, but we chose these for illustrative
purposes. We can see that all operators have rel-
atively higher MRR on smaller datasets (DD6-9).
Compensating for that effect, we can hypothesise
that Sum has a steady performance across differ-
ent definition sizes, while the performance of both
Prod and Conv declines as the number of oper-
ations increases. Normalisation helps with Sum
throughout, with little difference in performance
between Norm and Norm10, but with a slight de-
crease when CNorm is used. On the other hand,
only CNorm improves the ranking of Prod-based
vectors. Normalisation makes no difference for RI
vectors combined with convolution and the results
in Table 5 show that, on average, Conv performs
worse than the random baseline.
In Figure 5 we can see that, although dimen-
sionality reduction leads to lower MRR, for Sum,
normalisation prior to SVD counteracts this effect,
while, for Prod, dimensionality reduction, in gen-
eral, reduces the performance.
3
http://simple.wiktionary.org/wiki/Wiktionary:BNC spoken freq
236
DD2 DD3 DD4 DD5 DD6 DD7 DD8 DD9 DD100
0.1
0.2
0.3
0.4
0.5
0.6
0.7
MRR
 
 
SumSum+NormProdProd+NormConvConv+NormDDsize/1000
Figure 4: Per-dataset breakdown of best nor-
malised and unnormalised vectors for each vector
operator. Stars indicate the dataset size from Ta-
ble 4 divided by 1000.
Sum Prod Conv
Norm No Yes No CN No Yes
CS (N ) 140 140 3300 10000 140 3300
SVD(K)/RI(D) 700 700 None None 2048 512
mean P@1 0.18 0.23 0.01 0.11 0.00 0.00
mean MRR 0.28 0.35 0.06 0.17 0.02 0.02
Table 5: Best settings for operators calculated
from the highest average MRR across all the
datasets, with and without normalisation. The
results for vectors with no normalisation or CS
are: Sum - P@1=0.1567, MRR=0.2624; Prod -
P@1=0.0147, MRR=0.0542; Conv P@1=0.0027,
MRR=0.0192.
5 Discussion
In this paper we introduced context selection and
normalisation as techniques for improving the se-
mantic vector space representations of words. We
found that, although our untuned vectors perform
better on WS353 data (?  0.63) than vectors used
by Mitchell and Lapata (2010) (?  0.42), our
best phrase composition model (Sum, ?  0.40)
produces a lower performance than an estimate of
their best model (Prod, ?  0.44).
4
This indicates
that better performance on word-similarity data
does not directly translate into better performance
on compositional tasks; however, CS and normal-
isation are both effective in increasing the qual-
ity of the composed representation (?  0.42).
Since CS and normalisation are computationally
inexpensive, they are an excellent way to improve
model quality compared to the alternative, which
4
The estimate is computed as an average across the three
phrase-type results.
DD2 DD3 DD4 DD5 DD6 DD7 DD8 DD9 DD100
0.1
0.2
0.3
0.4
0.5
0.6
0.7
MRR
 
 Sum BestSum+SVDSum+SVD+Norm10Prod BestProd+SVDProd+SVD+CNorm
Figure 5: Per-dataset breakdown of best nor-
malised and unnormalised SVD vectors for Sum
and Prod. For both operators the best CS and SVD
settings for normalised vectors were N  140,
K  700, and for unnormalised wereN  10000,
K  700.
is building several models with various context
types, in order to find which one suits the data best.
Furthermore, we show that, as the number of
vector operations increases, Sum is the most sta-
ble operator and that it benefits from sparser rep-
resentations (low N ) and normalisation. Employ-
ing both of these methods, we are able to build an
SVD-based representation that performs as well
as full-dimensional vectors which, together with
Sum, give the best results on both phrase and def-
inition tasks. In fact, normalisation and CS both
improve the SVD representations of the vectors
across different weighting schemes. This is a key
result, as many of the more complex composi-
tional methods require low dimensional represen-
tations for computational reasons.
Future work will include application of CS
and normalised lower-dimensional vectors to more
complex compositional methods, and investiga-
tions into whether these strategies apply to other
context types and other dimensionality reduction
methods such as LDA (Blei et al., 2003).
Acknowledgements
Tamara Polajnar is supported by ERC Starting
Grant DisCoTex (306920). Stephen Clark is sup-
ported by ERC Starting Grant DisCoTex (306920)
and EPSRC grant EP/I037512/1. We would like
to thank Laura Rimell for helpful discussion, and
Laura and the anonymous reviewers for helpful
comments on the paper.
237
References
Jacob Andreas and Zoubin Ghahramani. 2013. A gen-
erative model of vector space semantics. In Pro-
ceedings of the ACL 2013 Workshop on Continu-
ous Vector Space Models and their Compositional-
ity, Sofia, Bulgaria.
M. Baroni and R. Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP-10), Cambridge, MA.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet allocation. J. Mach. Learn.
Res., 3:993?1022.
Elia Bruni, Gemma Boleda, Marco Baroni, and
Nam Khanh Tran. 2012. Distributional semantics
in technicolor. In Proceedings of the 50th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), pages 136?145,
Jeju Island, Korea, July. Association for Computa-
tional Linguistics.
Daoud Clarke. 2012. A context-theoretic frame-
work for compositionality in distributional seman-
tics. Comput. Linguist., 38(1):41?71, March.
B. Coecke, M. Sadrzadeh, and S. Clark. 2010. Math-
ematical foundations for a compositional distribu-
tional model of meaning. In J. van Bentham,
M. Moortgat, and W. Buszkowski, editors, Linguis-
tic Analysis (Lambek Festschrift), volume 36, pages
345?384.
James R. Curran. 2004. From Distributional to Seman-
tic Similarity. Ph.D. thesis, University of Edinburgh.
Scott Deerwester, Susan T. Dumais, Thomas K. Lan-
dauer, George W. Furnas, and Richard Harshman.
1990. Indexing by latent semantic analysis. Journal
of the Society for Information Science, 41(6):391?
407.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2002. Placing search in context: The
concept revisited. ACM Transactions on Informa-
tion Systems, 20:116?131.
Evgeniy Gabrilovich and Shaul Markovitch. 2007.
Computing semantic relatedness using Wikipedia-
based explicit semantic analysis. In Proceedings of
the 20th international joint conference on Artifical
intelligence, IJCAI?07, pages 1606?1611, San Fran-
cisco, CA, USA. Morgan Kaufmann Publishers Inc.
Pablo Gamallo and Stefan Bordag. 2011. Is singu-
lar value decomposition useful for word similarity
extraction? Language Resources and Evaluation,
45(2):95?119.
Michael N. Jones and Douglas J. K. Mewhort. 2007.
Representing word meaning and order information
in a composite holographic lexicon. Psychological
Review, 114:1?37.
Ekaterina Kochmar and Ted Briscoe. 2013. Capturing
anomalies in the choice of content words in compo-
sitional distributional semantic space. In Proceed-
ings of the Recent Advances in Natural Language
Processing (RANLP-2013), Hissar, Bulgaria.
Guido Minnen, John Carroll, and Darren Pearce. 2001.
Applied morphological processing of English. Nat-
ural Language Engineering, 7(3):207?223.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388?1429.
T. A. Plate. 1991. Holographic reduced Repre-
sentations: Convolution algebra for compositional
distributed representations. In J. Mylopoulos and
R. Reiter, editors, Proceedings of the 12th Inter-
national Joint Conference on Artificial Intelligence,
Sydney, Australia, August 1991, pages 30?35, San
Mateo, CA. Morgan Kauffman.
D Seung and L Lee. 2001. Algorithms for non-
negative matrix factorization. Advances in neural
information processing systems, 13:556?562.
Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Semantic Composi-
tionality Through Recursive Matrix-Vector Spaces.
In Proceedings of the 2012 Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), Jeju Island, Korea.
Peter Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37:141?188.
Fabio Massimo Zanzotto and Lorenzo Dell?Arciprete.
2011. Distributed structures and distributional
meaning. In Proceedings of the Workshop on Dis-
tributional Semantics and Compositionality, DiSCo-
11, pages 10?15, Portland, Oregon. Association for
Computational Linguistics.
Chengxiang Zhai and John Lafferty. 2001. A study
of smoothing methods for language models applied
to ad hoc information retrieval. In Proceedings of
the 24th annual international ACM SIGIR confer-
ence on Research and development in information
retrieval, SIGIR ?01, pages 334?342, New York,
NY, USA. ACM.
238
Wide-Coverage Efficient Statistical Parsing
with CCG and Log-Linear Models
Stephen Clark?
University of Oxford
James R. Curran??
University of Sydney
This article describes a number of log-linear parsing models for an automatically extracted
lexicalized grammar. The models are ?full? parsing models in the sense that probabilities are
defined for complete parses, rather than for independent events derived by decomposing the parse
tree. Discriminative training is used to estimate the models, which requires incorrect parses for
each sentence in the training data as well as the correct parse. The lexicalized grammar formalism
used is Combinatory Categorial Grammar (CCG), and the grammar is automatically extracted
from CCGbank, a CCG version of the Penn Treebank. The combination of discriminative training
and an automatically extracted grammar leads to a significant memory requirement (up to
25 GB), which is satisfied using a parallel implementation of the BFGS optimization algorithm
running on a Beowulf cluster. Dynamic programming over a packed chart, in combination with
the parallel implementation, allows us to solve one of the largest-scale estimation problems in the
statistical parsing literature in under three hours.
A key component of the parsing system, for both training and testing, is a Maximum En-
tropy supertagger which assigns CCG lexical categories to words in a sentence. The super-
tagger makes the discriminative training feasible, and also leads to a highly efficient parser.
Surprisingly, given CCG?s ?spurious ambiguity,? the parsing speeds are significantly higher
than those reported for comparable parsers in the literature. We also extend the existing parsing
techniques for CCG by developing a new model and efficient parsing algorithm which exploits
all derivations, including CCG?s nonstandard derivations. This model and parsing algorithm,
when combined with normal-form constraints, give state-of-the-art accuracy for the recovery of
predicate?argument dependencies from CCGbank. The parser is also evaluated on DepBank and
compared against the RASP parser, outperforming RASP overall and on the majority of relation
types. The evaluation on DepBank raises a number of issues regarding parser evaluation.
This article provides a comprehensive blueprint for building a wide-coverage CCG parser.
We demonstrate that both accurate and highly efficient parsing is possible with CCG.
? University of Oxford Computing Laboratory, Wolfson Building, Parks Road, Oxford, OX1 3QD, UK.
E-mail: stephen.clark@comlab.ox.ac.uk.
?? School of Information Technologies, University of Sydney, NSW 2006, Australia.
E-mail: james@it.usyd.edu.au.
Submission received: 27 April 2006; revised submission received: 30 November 2006; accepted for publication:
16 March 2007.
? 2007 Association for Computational Linguistics
Computational Linguistics Volume 33, Number 4
1. Introduction
Log-linear models have been applied to a number of problems in NLP, for example,
POS tagging (Ratnaparkhi 1996; Lafferty, McCallum, and Pereira 2001), named entity
recognition (Borthwick 1999), chunking (Koeling 2000), and parsing (Johnson et al
1999). Log-linear models are also referred to as maximum entropy models and random
fields in the NLP literature. They are popular because of the ease with which complex
discriminating features can be included in the model, and have been shown to give
good performance across a range of NLP tasks.
Log-linear models have previously been applied to statistical parsing (Johnson
et al 1999; Toutanova et al 2002; Riezler et al 2002; Malouf and van Noord 2004),
but typically under the assumption that all possible parses for a sentence can be
enumerated. For manually constructed grammars, this assumption is usually sufficient
for efficient estimation and decoding. However, for wide-coverage grammars extracted
from a treebank, enumerating all parses is infeasible. In this article we apply the dy-
namic programming method of Miyao and Tsujii (2002) to a packed chart; however,
because the grammar is automatically extracted, the packed charts require a consid-
erable amount of memory: up to 25 GB. We solve this massive estimation problem by
developing a parallelized version of the estimation algorithm which runs on a Beowulf
cluster.
The lexicalized grammar formalism we use is Combinatory Categorial Grammar
(CCG; Steedman 2000). A number of statistical parsing models have recently been devel-
oped for CCG and used in parsers applied to newspaper text (Clark, Hockenmaier, and
Steedman 2002; Hockenmaier and Steedman 2002b; Hockenmaier 2003b). In this article
we extend existing parsing techniques by developing log-linear models for CCG, as well
as a new model and efficient parsing algorithm which exploits all CCG?s derivations,
including the nonstandard ones.
Estimating a log-linear model involves computing expectations of feature values.
For the conditional log-linear models used in this article, computing expectations re-
quires a sum over all derivations for each sentence in the training data. Because there
can be a massive number of derivations for some sentences, enumerating all derivations
is infeasible. To solve this problem, we have adapted the dynamic programming method
of Miyao and Tsujii (2002) to packed CCG charts. A packed chart efficiently represents all
derivations for a sentence. The dynamic programming method uses inside and outside
scores to calculate expectations, similar to the inside?outside algorithm for estimating
the parameters of a PCFG from unlabeled data (Lari and Young 1990).
Generalized Iterative Scaling (Darroch and Ratcliff 1972) is a common choice in
the NLP literature for estimating a log-linear model (e.g., Ratnaparkhi 1998; Curran
and Clark 2003). Initially we used generalized iterative scaling (GIS) for the parsing
models described here, but found that convergence was extremely slow; Sha and Pereira
(2003) present a similar finding for globally optimized log-linear models for sequences.
As an alternative to GIS, we use the limited-memory BFGS algorithm (Nocedal and
Wright 1999). As Malouf (2002) demonstrates, general purpose numerical optimization
algorithms such as BFGS can converge much faster than iterative scaling algorithms
(including Improved Iterative Scaling; Della Pietra, Della Pietra, and Lafferty 1997).
Despite the use of a packed representation, the complete set of derivations for the
sentences in the training data requires up to 25 GB of RAM for some of the models in this
article. There are a number of ways to solve this problem. Possibilities include using a
subset of the training data; repeatedly parsing the training data for each iteration of
the estimation algorithm; or reading the packed charts from disk for each iteration.
494
Clark and Curran Wide-Coverage Efficient Statistical Parsing
These methods are either too slow or sacrifice parsing performance, and so we use
a parallelized version of BFGS running on an 18-node Beowulf cluster to perform the
estimation. Even given the large number of derivations and the large feature sets in our
models, the estimation time for the best-performing model is less than three hours. This
gives us a practical framework for developing a statistical parser.
A corollary of CCG?s base-generative treatment of long-range dependencies in rel-
ative clauses and coordinate constructions is that the standard predicate?argument re-
lations can be derived via nonstandard surface derivations. The addition of ?spurious?
derivations in CCG complicates the modeling and parsing problems. In this article we
consider two solutions. The first, following Hockenmaier (2003a), is to define a model in
terms of normal-form derivations (Eisner 1996). In this approach we recover only one
derivation leading to a given set of predicate?argument dependencies and ignore the
rest.
The second approach is to define a model over the predicate?argument dependen-
cies themselves, by summing the probabilities of all derivations leading to a given set
of dependencies. We also define a new efficient parsing algorithm for such a model,
based on Goodman (1996), which maximizes the expected recall of dependencies. The
development of this model allows us to test, for the purpose of selecting the correct
predicate?argument dependencies, whether there is useful information in the additional
derivations. We also compare the performance of our best log-linear model against
existing CCG parsers, obtaining the highest results to date for the recovery of predicate?
argument dependencies from CCGbank.
A key component of the parsing system is a Maximum Entropy CCG supertagger
(Ratnaparkhi 1996; Curran and Clark 2003) which assigns lexical categories to words
in a sentence. The role of the supertagger is twofold. First, it makes discriminative
estimation feasible by limiting the number of incorrect derivations for each training
sentence; the supertagger can be thought of as supplying a number of incorrect but
plausible lexical categories for each word in the sentence. Second, it greatly increases the
efficiency of the parser, which was the original motivation for supertagging (Bangalore
and Joshi 1999). One possible criticism of CCG has been that highly efficient parsing is
not possible because of the additional ?spurious? derivations. In fact, we show that a
novel method which tightly integrates the supertagger and parser leads to parse times
significantly faster than those reported for comparable parsers in the literature.
The parser is evaluated on CCGbank (available through the Linguistic Data Con-
sortium). In order to facilitate comparisons with parsers using different formalisms, we
also evaluate on the publicly available DepBank (King et al 2003), using the Briscoe
and Carroll annotation consistent with the RASP parser (Briscoe, Carroll, and Watson
2006). The dependency annotation is designed to be as theory-neutral as possible to
allow easy comparison. However, there are still considerable difficulties associated with
a cross-formalism comparison, which we describe. Even though the CCG dependencies
are being mapped into another representation, the accuracy of the CCG parser is over
81% F-score on labeled dependencies, against an upper bound of 84.8%. The CCG parser
also outperforms RASP overall and on the majority of dependency types.
The contributions of this article are as follows. First, we explain how to estimate a
full log-linear parsing model for an automatically extracted grammar, on a scale as large
as that reported anywhere in the NLP literature. Second, the article provides a compre-
hensive blueprint for building a wide-coverage CCG parser, including theoretical and
practical aspects of the grammar, the estimation process, and decoding. Third, we inves-
tigate the difficulties associated with cross-formalism parser comparison, evaluating the
parser on DepBank. And finally, we develop new models and decoding algorithms for
495
Computational Linguistics Volume 33, Number 4
CCG, and give a convincing demonstration that, through use of a supertagger, highly
efficient parsing is possible with CCG.
2. Related Work
The first application of log-linear models to parsing is the work of Ratnaparkhi and
colleagues (Ratnaparkhi, Roukos, and Ward 1994; Ratnaparkhi 1996, 1999). Similar to
Della Pietra, Della Pietra, and Lafferty (1997), Ratnaparkhi motivates log-linear models
from the perspective of maximizing entropy, subject to certain constraints. Ratnaparkhi
models the various decisions made by a shift-reduce parser, using log-linear distri-
butions defined over features of the local context in which a decision is made. The
probabilities of each decision are multiplied together to give a score for the complete
sequence of decisions, and beam search is used to find the most probable sequence,
which corresponds to the most probable derivation.
A different approach is proposed by Abney (1997), who develops log-linear models
for attribute-value grammars, such as Head-driven Phrase Structure Grammar (HPSG).
Rather than define a model in terms of parser moves, Abney defines a model directly
over the syntactic structures licensed by the grammar. Another difference is that Abney
uses a global model, in which a single log-linear model is defined over the complete
space of attribute?value structures. Abney?s motivation for using log-linear models is
to overcome various problems in applying models based on PCFGs directly to attribute-
value grammars. A further motivation for using global models is that these do not suffer
from the label bias problem (Lafferty, McCallum, and Pereira 2001), which is a potential
problem for Ratnaparkhi?s approach.
Abney defines the following model for a syntactic analysis ?:
P(?) =
?
i ?
fi(?)
i
Z (1)
where fi(?) is a feature, or feature function, and ?i is its corresponding weight; Z is a
normalizing constant, also known as the partition function. In much work using log-
linear models in NLP, including Ratnaparkhi?s, the features of a model are indicator
functions which take the value 0 or 1. However, in Abney?s models, and in the models
used in this article, the feature functions are integer valued and count the number of
times some feature appears in a syntactic analysis.1 Abney calls the feature functions
frequency functions and, like Abney, we will not always distinguish between a feature
and its corresponding frequency function.
There are practical difficulties with Abney?s proposal, in that finding the maximum-
likelihood solution during estimation involves calculating expectations of feature val-
ues, which are sums over the complete space of possible analyses. Abney suggests a
Metropolis-Hastings sampling procedure for calculating the expectations, but does not
experiment with an implementation.
Johnson et al (1999) propose an alternative solution, which is to maximize the
conditional likelihood function. In this case the likelihood function is the product of
the conditional probabilities of the syntactic analyses in the data, each probability condi-
tioned on the respective sentence. The advantage of this method is that calculating the
conditional feature expectations only requires a sum over the syntactic analyses for the
1 In principle the features could be real-valued, but we only use integer-valued features in this article.
496
Clark and Curran Wide-Coverage Efficient Statistical Parsing
sentences in the training data. The conditional-likelihood estimator is also consistent
for the conditional distributions (Johnson et al 1999). The same solution is arrived at
by Della Pietra, Della Pietra, and Lafferty (1997) via a maximum entropy argument.
Another feature of Johnson et al?s approach is the use of a Gaussian prior term to avoid
overfitting, which involves adding a regularization term to the likelihood function; the
regularization term penalizes models whose weights get too large in absolute value.
This smoothing method for log-linear models is also proposed by Chen and Rosenfeld
(1999).
Calculating the conditional feature expectations can still be problematic if the gram-
mar licenses a large number of analyses for some sentences. This is not a problem for
Johnson et al (1999) because their grammars are hand-written and constraining enough
to allow the analyses for each sentence to be enumerated. However, for grammars with
wider coverage it is often not possible to enumerate the analyses for each sentence in
the training data. Osborne (2000) investigates training on a sample of the analyses for
each sentence, for example the top-n most probable according to some other probability
model, or simply a random sample.
The CCG grammar used in this article is automatically extracted, has wide cover-
age, and can produce an extremely large number of derivations for some sentences,
far too many to enumerate. We adapt the feature-forest method of Miyao and Tsujii
(2002), which involves using dynamic programming to efficiently calculate the feature
expectations. Geman and Johnson (2002) propose a similar method in the context of LFG
parsing; an implementation is described in Kaplan et al (2004).
Miyao and Tsujii have carried out a number of investigations similar to the work
in this article. In Miyao and Tsujii (2003b, 2003a) log-linear models are developed for
automatically extracted grammars for Lexicalized Tree Adjoining Grammar (LTAG) and
Head Driven Phrase Structure Grammar (HPSG). One of Miyao and Tsujii?s motivations
is to model predicate?argument dependencies, including long-range dependencies,
which was one of the original motivations of the wide-coverage CCG parsing project.
Miyao and Tsujii (2003a) present another log-linear model for an automatically extracted
LTAG which uses a simple unigram model of the elementary trees together with a log-
linear model of the attachments. Miyao and Tsujii (2005) address the issue of practical
estimation using an automatically extracted HPSG grammar. A simple unigram model
of lexical categories is used to limit the size of the charts for training, in a similar way to
how we use a CCG supertagger to restrict the size of the charts.
The main differences between Miyao and Tsujii?s work and ours, aside from the
different grammar formalisms, are as follows. The CCG supertagger is a key component
of our parsing system. It allows practical estimation of the log-linear models as well
as highly efficient parsing. The Maximum Entropy supertagger we use could also be
applied to Miyao and Tsujii?s grammars, although whether similar performance would
be obtained depends on the characteristics of the grammar; see subsequent sections for
more discussion of this issue in relation to LTAG. The second major difference is in our
use of a cluster and parallelized estimation algorithm. We have found that significantly
increasing the size of the parse space available for discriminative estimation, which is
possible on the cluster, improves the accuracy of the resulting parser. Another advan-
tage of parallelization, as discussed in Section 5.5, is the reduction in estimation time.
Again, our parallelization techniques could be applied to Miyao and Tsujii?s framework.
Malouf and van Noord (2004) present similar work to ours, in the context of an
HPSG grammar for Dutch. One similarity is that their parsing system uses an HMM
tagger before parsing, similar to our supertagger. One difference is that we use a
Maximum Entropy tagger which allows more flexibility in terms of the features that can
497
Computational Linguistics Volume 33, Number 4
be encoded; for example, we have found that using Penn Treebank POS tags as features
significantly improves supertagging accuracy. Another difference is that Malouf and
van Noord use the random sampling method of Osborne (2000) to allow practical
estimation, whereas we construct the complete parse forest but use the supertagger to
limit the size of the charts. Their work is also on a somewhat smaller scale, with the
Dutch Alpino treebank containing 7,100 sentences, compared with the 36,000 sentences
we use for training.
Kaplan et al (2004) present similar work to ours in the context of an LFG grammar
for English. The main difference is that the LFG grammar is hand-built, resulting in less
ambiguity than an automatically extracted grammar and thus requiring fewer resources
for model estimation. One downside of hand-built grammars is that they are typically
less robust, which Kaplan et al address by developing a ?fragment? grammar, together
with a ?skimming mode,? which increases coverage on Section 23 of the Penn Treebank
from 80% to 100%. Kaplan et al also present speed figures for their parser, comparing
with the Collins parser. Comparing parser speeds is difficult because of implementation
and accuracy differences, but their highest reported speed is around 2 sentences per
second on sentences from Section 23. The parse speeds that we present in Section 10.3
are an order of magnitude higher.
More generally, the literature on statistical parsing using linguistically motivated
grammar formalisms is large and growing. Statistical parsers have been developed
for TAG (Chiang 2000; Sarkar and Joshi 2003), LFG (Riezler et al 2002; Kaplan et al
2004; Cahill et al 2004), and HPSG (Toutanova et al 2002; Toutanova, Markova, and
Manning 2004; Miyao and Tsujii 2004; Malouf and van Noord 2004), among others.
The motivation for using these formalisms is that many NLP tasks, such as Machine
Translation, Information Extraction, and Question Answering, could benefit from the
more sophisticated linguistic analyses they provide.
The formalism most closely related to CCG from this list is TAG. TAG grammars have
been automatically extracted from the Penn Treebank, using techniques similar to those
used by Hockenmaier (Chen and Vijay-Shanker 2000; Xia, Palmer, and Joshi 2000). Also,
the supertagging idea which is central to the efficiency of the CCG parser originated with
TAG (Bangalore and Joshi 1999). Chen et al (2002) describe the results of reranking the
output of an HMM supertagger using an automatically extracted LTAG. The accuracy
for a single supertag per word is slightly over 80%. This figure is increased to over
91% when the tagger is run in n-best mode, but at a considerable cost in ambiguity,
with 8 supertags per word. Nasr and Rambow (2004) investigate the potential impact
of LTAG supertagging on parsing speed and accuracy by performing a number of
oracle experiments. They find that, with the perfect supertagger, extremely high parsing
accuracies and speeds can be obtained. Interestingly, the accuracy of LTAG supertaggers
using automatically extracted grammars is significantly below the accuracy of the CCG
supertagger. One possible way to increase the accuracy of LTAG supertagging is to use a
Maximum Entropy, rather than HMM, tagger (as discussed previously), but this is likely
to result in an improvement of only a few percentage points. Thus whether the differ-
ence in supertagging accuracy is due to the nature of the formalisms, the supertagging
methods used, or properties of the extracted grammars, is an open question.
Related work on statistical parsing with CCG will be described in Section 3.
3. Combinatory Categorial Grammar
Combinatory Categorial Grammar (CCG) (Steedman 1996, 2000) is a type-driven lex-
icalized theory of grammar based on Categorial Grammar (Wood 1993). CCG lexical
498
Clark and Curran Wide-Coverage Efficient Statistical Parsing
entries consist of a syntactic category, which defines valency and directionality, and
a semantic interpretation. In this article we are concerned with the syntactic compo-
nent; see Steedman (2000) for how a semantic interpretation can be composed dur-
ing a syntactic derivation, and also Bos et al (2004) for how semantic interpretations
can be built for newspaper text using the wide-coverage parser described in this
article.
Categories can be either basic or complex. Examples of basic categories are S (sen-
tence), N (noun), NP (noun phrase), and PP (prepositional phrase). Complex categories
are built recursively from basic categories, and indicate the type and directionality
of arguments (using slashes), and the type of the result. For example, the following
category for the transitive verb bought specifies its first argument as a noun phrase to its
right, its second argument as a noun phrase to its left, and its result as a sentence:
bought := (S\NP)/NP (2)
In the theory of CCG, basic categories are regarded as complex objects that include
syntactic features such as number, gender, and case. For the grammars in this article,
categories are augmented with some additional information, such as head information,
and also features on S categories which distinguish different types of sentence, such as
declarative, infinitival, and wh-question. This additional information will be described
in later sections.
Categories are combined in a derivation using combinatory rules. In the original
Categorial Grammar (Bar-Hillel 1953), which is context-free, there are two rules of
functional application:
X/Y Y ? X (>) (3)
Y X\Y ? X (<) (4)
where X and Y denote categories (either basic or complex). The first rule is forward
application (>) and the second rule is backward application (<). Figure 1 gives an
example derivation using these rules.
CCG extends the original Categorial Grammar by introducing a number of addi-
tional combinatory rules. The first is forward composition, which Steedman denotes
Figure 1
Example derivation using forward and backward application.
499
Computational Linguistics Volume 33, Number 4
Figure 2
Example derivation using type-raising and forward composition.
by > B (because B is the symbol used by Curry to denote function composition in
combinatory logic; Curry and Feys 1958):
X/Y Y/Z ?B X/Z (> B) (5)
Forward composition is often used in conjunction with type-raising (T), as in Figure 2.
In this case type-raising takes a subject noun phrase and turns it into a functor looking
to the right for a verb phrase; the fund is then able to combine with reached using forward
composition, giving the fund reached the category S[dcl]/NP (a declarative sentence
missing an object). It is exactly this type of constituent which the object relative pronoun
category is looking for to its right: (NP\NP)/(S[dcl]/NP).
Note that the fund reached is a perfectly reasonable constituent in CCG, having the
type S[dcl]/NP. This allows analyses for sentences such as the fund reached but investors
disagreed with the agreement, even though this construction is often described as ?non-
constituent coordination.? In this example, the fund reached and investors disagreed with
have the same type, allowing them to be coordinated, resulting in the fund reached but
investors disagreed with having the type S[dcl]/NP. Note also that it is this flexible notion
of constituency which leads to so-called spurious ambiguity, because even the simple
sentence the fund reached an agreement will have more than one derivation, with each
derivation leading to the same set of predicate?argument dependencies.
Forward composition is generalized to allow additional arguments to the right
of the Z category in (5). For example, the following combination allows analysis of
sentences such as I offered, and may give, a flower to a policeman (Steedman 2000):
may give
(S\NP)/(S\NP) ((S\NP)/PP)/NP
>B
((S\NP)/PP)/NP
This example shows how the categories for may and give combine, resulting in a cate-
gory of the same type as offered, which can then be coordinated. Steedman (2000) gives
a more precise definition of generalized forward composition.
Further combinatory rules in the theory of CCG include backward composition
(< B) and backward crossed composition (< BX):
Y\Z X\Y ?B X\Z (< B) (6)
Y/Z X\Y ?B X/Z (< BX) (7)
500
Clark and Curran Wide-Coverage Efficient Statistical Parsing
Backward composition provides an analysis for sentences involving ?argument cluster
coordination,? such as I gave a teacher an apple and a policeman a flower (Steedman 2000).
Backward crossed composition is required for heavy NP shift and coordinations such
as I shall buy today and cook tomorrow the mushrooms. In this coordination example from
Steedman (2000), backward crossed composition is used to combine the categories
for buy, (S\NP)/NP, and today, (S\NP)\(S\NP), and similarly for cook and tomorrow,
producing categories of the same type which can be coordinated. This rule is also
generalized in an analogous way to forward composition.
Finally, there is a coordination rule which conjoins categories of the same type,
producing a further category of that type. This rule can be implemented by assuming
the following category schema for a coordination term: (X\X)/X, where X can be any
category.
All of the combinatory rules described above are implemented in our parser. Other
combinatory rules, such as substitution, have been suggested in the literature to deal
with certain linguistic phenomena, but we chose not to implement them. The reason is
that adding new combinatory rules reduces the efficiency of the parser, and we felt that,
in the case of substitution, for example, the small gain in grammatical coverage was not
worth the reduction in speed. Section 9.3 discusses some of the choices we made when
implementing the grammar.
One way of dealing with the additional ambiguity in CCG is to only consider
normal-form derivations. Informally, a normal-form derivation is one which uses type-
raising and composition only when necessary. Eisner (1996) describes a technique for
eliminating spurious ambiguity entirely, by defining exactly one normal-form deriva-
tion for each semantic equivalence class of derivations. The idea is to restrict the
combination of categories produced by composition; more specifically, any constituent
which is the result of a forward composition cannot serve as the primary (left) functor in
another forward composition or forward application. Similarly, any constituent which
is the result of a backward composition cannot serve as the primary (right) functor
in another backward composition or backward application. Eisner only deals with a
grammar without type-raising, and so the constraints cannot guarantee a normal-form
derivation when applied to the grammars used in this article. However, the constraints
can still be used to significantly reduce the parsing space. Section 9.3 describes the
various normal-form constraints used in our experiments.
A recent development in the theory of CCG is the multi-modal treatment given by
Baldridge (2002) and Baldridge and Kruijff (2003), following the type-logical approaches
to Categorial Grammar (Moortgat 1997). One possible extension to the parser and
grammar described in this article is to incorporate the multi-modal approach; Baldridge
suggests that, as well as having theoretical motivation, a multi-modal approach can
improve the efficiency of CCG parsing.
3.1 Why Use CCG for Statistical Parsing?
CCG was designed to deal with the long-range dependencies inherent in certain
constructions, such as coordination and extraction, and arguably provides the most
linguistically satisfactory account of these phenomena. Long-range dependencies are
relatively common in text such as newspaper text, but are typically not recovered by
treebank parsers such as Collins (2003) and Charniak (2000). This has led to a number
of proposals for post-processing the output of the Collins and Charniak parsers, in
which trace sites are located and the antecedent of the trace determined (Johnson 2002;
Dienes and Dubey 2003; Levy and Manning 2004). An advantage of using CCG is that
501
Computational Linguistics Volume 33, Number 4
the recovery of long-range dependencies can be integrated into the parsing process in
a straightforward manner, rather than be relegated to such a post-processing phase
(Clark, Hockenmaier, and Steedman 2002; Hockenmaier 2003a; Clark, Steedman, and
Curran 2004).
Another advantage of CCG is that providing a compositional semantics for the
grammar is relatively straightforward. It has a completely transparent interface between
syntax and semantics and, because CCG is a lexicalized grammar formalism, providing a
compositional semantics simply involves adding semantic representations to the lexical
entries and interpreting the small number of combinatory rules. Bos et al (2004) show
how this can be done for the grammar and parser described in this article.
Of course some of these advantages could be obtained with other grammar for-
malisms, such as TAG, LFG, and HPSG, although CCG is especially well-suited to
analysing coordination and long-range dependencies. For example, the analysis of
?non-constituent coordination? described in the previous section is, as far as we know,
unique to CCG.
Finally, the lexicalized nature of CCG has implications for the engineering of a wide-
coverage parser. Later we show that use of a supertagger (Bangalore and Joshi 1999) prior
to parsing can produce an extremely efficient parser. The supertagger uses statistical
sequence tagging techniques to assign a small number of lexical categories to each word
in the sentence. Because there is so much syntactic information in lexical categories,
the parser is required to do less work once the lexical categories have been assigned;
hence Srinivas and Joshi, in the context of TAG, refer to supertagging as almost parsing.
The parser is able to parse 20 Wall Street Journal (WSJ) sentences per second on standard
hardware, using our best-performing model, which compares very favorably with other
parsers using linguistically motivated grammars.
A further advantage of the supertagger is that it can be used to reduce the parse
space for estimation of the log-linear parsing models. By focusing on those parses
which result from the most probable lexical category sequences, we are able to perform
effective discriminative training without considering the complete parse space, which
for most sentences is prohibitively large.
The idea of supertagging originated with LTAG; however, in contrast to the CCG
grammars used in this article, the automatically extracted LTAG grammars have, as yet,
been too large to enable effective supertagging (as discussed in the previous section). We
are not aware of any other work which has demonstrated the parsing efficiency benefits
of supertagging using an automatically extracted grammar.
3.2 Previous Work on CCG Statistical Parsing
The work in this article began as part of the Edinburgh wide-coverage CCG parsing
project (2000?2004). There has been some other work on defining stochastic categorial
grammars, but mainly in the context of grammar learning (Osborne and Briscoe 1997;
Watkinson and Manandhar 2001; Zettlemoyer and Collins 2005).
An early attempt from the Edinburgh project at wide-coverage CCG parsing is pre-
sented in Clark, Hockenmaier, and Steedman (2002). In order to deal with the problem
of the additional, nonstandard CCG derivations, a conditional model of dependency
structures is presented, based on Collins (1996), in which the dependencies are modeled
directly and derivations are not modeled at all. The conditional probability of a depen-
dency structure ?, given a sentence S, is factored into two parts. The first part is the
probability of the lexical category sequence, C, and the second part is the dependency
structure, D, giving P(?|S) = P(C|S)P(D|C, S). Intuitively, the category sequence is gen-
502
Clark and Curran Wide-Coverage Efficient Statistical Parsing
erated first, conditioned on the sentence, and then attachment decisions are made to
form the dependency links. The probability of the category sequence is estimated using
a maximum entropy model, following the supertagger described in Clark (2002). The
probabilities of the dependencies are estimated using relative frequencies, following
Collins (1996).
The model was designed to include some long-range predicate?argument depen-
dencies, as well as local dependencies. However, there are a number of problems with
the model, as the authors acknowledge. First, the model is deficient, losing probability
mass to dependency structures not generated by the grammar. Second, the relative
frequency estimation of the dependency probabilities is ad hoc, and cannot be seen
as maximum likelihood estimation, or some other principled method. Despite these
flaws, the parser based on this model was able to recover CCG predicate?argument
dependencies at around 82% overall F-score on unseen WSJ text.
Hockenmaier (2003a) and Hockenmaier and Steedman (2002b) present a generative
model of normal-form derivations, based on various techniques from the statistical
parsing literature (Charniak 1997; Goodman 1997; Collins 2003). A CCG binary deriva-
tion tree is generated top-down, with the probability of generating particular child
nodes being conditioned on some limited context from the previously generated struc-
ture. Hockenmaier?s parser uses rule instantiations read off CCGbank (see Section 3.3)
and some of these will be instances of type-raising and composition; hence the parser
can produce non-normal-form derivations. However, because the parsing model is
estimated over normal-form derivations, any non-normal-form derivations will receive
low probabilities and are unlikely to be returned as the most probable parse.
Hockenmaier (2003a) compares a number of generative models, starting with a
baseline model based on a PCFG. Various extensions to the baseline are considered:
increasing the amount of lexicalization; generating a lexical category at its maximal
projection; conditioning the probability of a rule instantiation on the grandparent node
(Johnson 1998); adding features designed to deal with coordination; and adding dis-
tance to the dependency features. Some of these extensions, such as increased lexicaliza-
tion and generating a lexical category at its maximal projection, improved performance,
whereas others, such as the coordination and distance features, reduced performance.
Hockenmaier (2003a) conjectures that the reduced performance is due to the problem of
data sparseness, which becomes particularly severe for the generative model when the
number of features is increased. The best performing model outperforms that of Clark,
Hockenmaier, and Steedman (2002), recovering CCG predicate?argument dependencies
with an overall F-score of around 84% using a similar evaluation.
Hockenmaier (2003b) presents another generative model of normal-form deriva-
tions, which is based on the dependencies in the predicate?argument structure, in-
cluding long-range dependencies, rather than the dependencies defined by the local
trees in the derivation. Hockenmaier also argues that, compared to Hockenmaier and
Steedman (2002b), the predicate?argument model is better suited to languages with
freer word order than English. The model was also designed to test whether the inclu-
sion of predicate?argument dependencies improves parsing accuracy. In fact, the results
given in Hockenmaier (2003b) are lower than previous results. However, Hockenmaier
(2003b) reports that the increased complexity of the model reduces the effectiveness of
the dynamic programming used in the parser, and hence a more aggressive beam search
is required to produce reasonable parse times. Thus the reduced accuracy could be due
to implementation difficulties rather than the model itself.
The use of conditional log-linear models in this article is designed to overcome some
of the weaknesses identified in the approach of Clark, Hockenmaier, and Steedman
503
Computational Linguistics Volume 33, Number 4
(2002), and to offer a more flexible framework for including features than the generative
models of Hockenmaier (2003a). For example, adding long-range dependency features
to the log-linear model is straightforward. We also showed in Clark and Curran (2004b)
that, in contrast with Hockenmaier (2003a), adding distance to the dependency features
in the log-linear model does improve parsing accuracy. Another feature of conditional
log-linear models is that they are trained discriminatively, by maximizing the condi-
tional probability of each gold-standard parse relative to the incorrect parses for the
sentence. Generative models, in contrast, are typically trained by maximizing the joint
probability of the ?training sentence, parse? pairs, even though the sentence does not
need to be inferred.
3.3 CCGbank
The treebank used in this article performs two roles: It provides the lexical category set
used by the supertagger, plus some unary type-changing rules and punctuation rules
used by the parser, and it is used as training data for the statistical models. The treebank
is CCGbank (Hockenmaier and Steedman 2002a; Hockenmaier 2003a), a CCG version
of the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993). Penn Treebank
conversions have also been carried out for other linguistic formalisms, including TAG
(Chen and Vijay-Shanker 2000; Xia, Palmer, and Joshi 2000), LFG (Burke et al 2004), and
HPSG (Miyao, Ninomiya, and Tsujii 2004).
CCGbank was created by converting the phrase-structure trees in the Penn Tree-
bank into CCG normal-form derivations. Some preprocessing of the phrase-structure
trees was required, in order to allow the correct CCG analyses for some constructions,
such as coordination. Hockenmaier (2003a) gives a detailed description of the procedure
used to create CCGbank. Figure 3 shows an example normal-form derivation for an (ab-
breviated) CCGbank sentence. The derivation has been inverted, so that it is represented
as a binary tree.
Sentence categories (S) in CCGbank carry features, such as [dcl] for declarative, [wq]
for wh-questions, and [for] for small clauses headed by for; see Hockenmaier (2003a) for
the complete list. S categories also carry features in verb phrases; for example, S[b]\NP
Figure 3
Example CCG derivation as a binary tree for the sentence Under new features, participants can
transfer money from the new funds.
504
Clark and Curran Wide-Coverage Efficient Statistical Parsing
is a bare-infinitive; S[to]\NP is a to-infinitive; S[pss]\NP is a past participle in passive
mode. Note that, whenever an S or S\NP category is modified, any feature on the S is
carried through to the result category; this is true in our parser also. Finally, determiners
specify that the resulting noun phrase is non-bare: NP[nb]/N, although this feature is
largely ignored by the parser described in this article.
As well as instances of the standard CCG combinatory rules?forward and back-
ward application, forward and backward composition, backward-crossed composi-
tion, type-raising, coordination of like types?CCGbank contains a number of unary
type-changing rules and rules for dealing with punctuation. The type-changing rules
typically change a verb phrase into a modifier. The following examples, taken from
Hockenmaier (2003a), demonstrate the most common rules. The bracketed expression
has the type-changing rule applied to it:
 S[pss]\NP ? NP\NP
workers [exposed to it]
 S[adj]\NP ? NP\NP
a forum [likely to bring attention to the problem]
 S[ng]\NP ? NP\NP
signboards [advertising imported cigarettes]
 S[ng]\NP ? (S\NP)\(S\NP)
became chairman [succeeding Ian Butler]
 S[dcl]/NP ? NP\NP
the millions of dollars [it generates]
Another common type-changing rule in CCGbank, which appears in Figure 3, changes
a noun category N into a noun phrase NP. Appendix A lists the unary type-changing
rules used by our parser.
There are also a number of rules in CCGbank for absorbing punctuation. For exam-
ple, Figure 3 contains a rule which takes a comma followed by a declarative sentence
and returns a declarative sentence:
, S[dcl] ? S[dcl]
There are a number of similar comma rules for other categories. There are also similar
punctuation rules for semicolons, colons, and brackets. There is also a rule schema
which treats a comma as a coordination:
, X ? X\X
Appendix A contains the complete list of punctuation rules used in the parser.
A small number of local trees in CCGbank?consisting of a parent and one or two
children?do not correspond to any of the CCG combinatory rules, or the type-changing
rules or punctuation rules. This is because some of the phrase structure subtrees in the
505
Computational Linguistics Volume 33, Number 4
Penn Treebank are difficult to convert to CCG combinatory rules, and because of noise
introduced by the Treebank conversion process.
3.4 CCG Dependency Structures
Dependency structures perform two roles in this article. First, they are used for parser
evaluation: The accuracy of a parsing model is measured using precision and recall
over CCG predicate?argument dependencies. Second, dependency structures form the
core of the dependency model: Probabilities are defined over dependency structures,
and the parsing algorithm for this model returns the highest scoring dependency
structure.
We define a CCG dependency structure as a set of CCG predicate?argument depen-
dencies. They are defined as sets, rather than multisets, because the lexical items in a
dependency are considered to be indexed by sentence position; this is important for
evaluation purposes and, for the dependency model, determining which derivations
lead to a given set of dependencies. However, there are situations where the lexical
items need to be considered independently of sentence position, for example when
defining feature functions in terms of dependencies. Such cases should be clear from the
context.
We define CCG predicate?argument relations in terms of the argument slots in CCG
lexical categories. Thus the transitive verb category, (S\NP)/NP, has two predicate?
argument relations associated with it, one corresponding to the object NP argument
and one corresponding to the subject NP argument. In order to distinguish different
argument slots, the arguments are numbered from left to right. Thus, the subject relation
for a transitive verb is represented as ?(S\NP1)/NP2, 1?.
The predicate?argument dependencies are represented as 5-tuples: ?hf , f, s, ha, l?,
where hf is the lexical item of the lexical category expressing the dependency relation,
f is the lexical category, s is the argument slot, ha is the head word of the argument, and
l encodes whether the dependency is non-local. For example, the dependency encoding
company as the object of bought (as in IBM bought the company) is represented as follows:
?bought2, (S\NP1)/NP2, 2, company4, ?? (8)
The subscripts on the lexical items indicate sentence position, and the final field (?)
indicates that the dependency is a local dependency.
Head and dependency information is represented on the lexical categories, and
dependencies are created during a derivation as argument slots are filled. Long-range
dependencies are created by passing head information from one category to another
using unification. For example, the expanded category for the control verb persuade is:
persuade := ((S[dcl]persuade\NP1)/(S[to]2\NPX))/NPX,3 (9)
The head of the infinitival complement?s subject is identified with the head of the
object, using the variable X. Unification then passes the head of the object to the subject
of the infinitival, as in standard unification-based accounts of control. In the current
implementation, the head and dependency markup depends on the category only and
not the lexical item. This gives semantically incorrect dependencies in some cases; for
506
Clark and Curran Wide-Coverage Efficient Statistical Parsing
example, the control verbs persuade and promise have the same lexical category, which
means that promise Brooks to go is assigned a structure meaning promise Brooks that Brooks
will go.
The kinds of lexical items that use the head passing mechanism are raising, aux-
iliary and control verbs, modifiers, and relative pronouns. Among the constructions
that project unbounded dependencies are relativization and right node raising. The
following relative pronoun category (for words such as who, which, and that) shows how
heads are co-indexed for object-extraction:
who := (NPX\NPX,1)/(S[dcl]2/NPX) (10)
In a sentence such as The company which IBM bought, the co-indexing will allow com-
pany to be returned as the object of bought, which is represented using the following
dependency:
?bought2, (S\NP1)/NP2, 2, company4, (NP\NP)/(S[dcl]/NP)? (11)
The final field indicates the category which mediated the long-range dependency, in this
case the object relative pronoun category.
The dependency annotation also permits complex categories as arguments. For
example, the marked up category for about (as in about 5,000 pounds) is:
(NX/NX)Y/(N/N)Y,1 (12)
If 5,000 has the category (NX/NX)5,000, the dependency relation marked on the (N/N)Y,1
argument in (12) allows the dependency between about and 5,000 to be captured.
In the current implementation every argument slot in a lexical category corresponds
to a dependency relation. This means, for example, that the parser produces subjects of
to-infinitival clauses and auxiliary verbs. In the sentence IBM may like to buy Lotus, IBM
will be returned as the subject of may, like, to, and buy. The only exception is during
evaluation, when some of these dependencies are ignored in order to be consistent with
the predicate?argument dependencies in CCGbank, and also DepBank. In future work
we may investigate removing some of these dependencies from the parsing model and
the parser output.
4. Log-Linear Parsing Models for CCG
This section describes two parsing models for CCG. The first defines the probabil-
ity of a dependency structure, and the second?the normal-form model?defines the
probability of a single derivation. In many respects, modeling single derivations is
simpler than modeling dependency structures, as the rest of the article will demonstrate.
However, there are a number of reasons for modeling dependency structures. First,
for many applications predicate?argument dependencies provide a more useful output
than derivations, and the parser evaluation is over dependencies; hence it would seem
reasonable to optimize over the dependencies rather than the derivation. Second, we
want to investigate, for the purposes of parse selection, whether there is useful infor-
mation in the nonstandard derivations. We can test this by defining the probability of
a dependency structure in terms of all the derivations leading to that structure, rather
507
Computational Linguistics Volume 33, Number 4
than emphasising a single derivation. Thus, the probability of a dependency structure,
?, given a sentence, S, is defined as follows:
P(?|S) =
?
d??(?)
P(d,?|S) (13)
where ?(?) is the set of derivations which lead to ?.
This approach is different from that of Clark, Hockenmaier, and Steedman (2002),
who define the probability of a dependency structure simply in terms of the depen-
dencies. One reason for modeling derivations (either one distinguished derivation or a
set of derivations), in addition to predicate?argument dependencies, is that derivations
may contain useful information for inferring the correct dependency structure.
For both the dependency model and the normal-form model, the probability of a
parse is defined using a log-linear form. However, the meaning of parse differs in the
two cases. For the dependency model, a parse is taken to be a ?d,?? pair, as in Equa-
tion (13). For the normal-form model, a parse is simply a (head-lexicalized) derivation.2
We define a conditional log-linear model of a parse ? ? ?, given a sentence S, as
follows:
P(?|S) = 1ZS
e? ? f (?) (14)
where ? ? f (?) =
?
i ?ifi(?). The function fi is the integer-valued frequency function of
the ith feature; ?i is the weight of the ith feature; and ZS is a normalizing constant which
ensures that P(?|S) is a probability distribution:
ZS =
?
????(S)
e? ? f (?? ) (15)
where ?(S) is the set of possible parses for S. For the normal-form model, features
are defined over single derivations, including local word?word dependencies arising
from lexicalized rule instantiations. The feature set is derived from the gold-standard
normal-form derivations in CCGbank. For the dependency model, features are defined
over dependency structures as well as derivations, and the feature set is derived from
all derivations leading to gold-standard dependency structures, including nonstandard
derivations. Section 7 describes the feature types in more detail.
4.1 Estimating the Dependency Model
For the dependency model, the training data consists of gold-standard dependency
structures, namely, sets of CCG predicate?argument dependencies, as described earlier.
We follow Riezler et al (2002) in using a discriminative estimation method by maximiz-
ing the conditional log-likelihood of the model given the data, minus a Gaussian prior
2 We could model predicate?argument dependencies together with the derivation, but we wanted to use
features from the derivation only, following Hockenmaier and Steedman (2002b).
508
Clark and Curran Wide-Coverage Efficient Statistical Parsing
term to prevent overfitting (Chen and Rosenfeld 1999; Johnson et al 1999). Thus, given
training sentences S1, . . . , Sm, gold-standard dependency structures, ?1, . . . ,?m, and the
definition of the probability of a dependency structure from Equation (13), the objective
function is:
L?(?) = L(?) ? G(?) (16)
= log
m
?
j=1
P?(?j|Sj) ?
n
?
i=1
?2i
2?2i
=
m
?
j=1
log
?
d??(?j ) e
? ? f (d,?j )
?
???(Sj ) e
? ? f (?) ?
n
?
i=1
?2i
2?2i
=
m
?
j=1
log
?
d??(?j )
e? ? f (d,?j ) ?
m
?
j=1
log
?
???(Sj )
e? ? f (?) ?
n
?
i=1
?2i
2?2i
where L(?) is the log-likelihood of model ?, G(?) is the Gaussian prior term, and n is
the number of features. We use a single smoothing parameter ?, so that ?i = ? for all
i; however, grouping the features into classes and using a different ? for each class is
worth investigating and may improve the results.
Optimization of the objective function, whether using iterative scaling or more
general numerical optimization methods, requires calculation of the gradient of the ob-
jective function at each iteration. The components of the gradient vector are as follows:
?L?(?)
??i
=
m
?
j=1
?
d??(?j )
e? ? f (d,?j )fi(d,?j)
?
d??(?j ) e
? ? f (d,?j )
(17)
?
m
?
j=1
?
???(Sj )
e? ? f (?)fi(?)
?
???(Sj ) e
? ? f (?) ?
?i
?2i
The first two terms are expectations of feature fi: the second expectation is over all
derivations for each sentence in the training data, and the first is over only the deriva-
tions leading to the gold-standard dependency structure for each sentence.
The estimation process attempts to make the expectations in Equation (17) equal
(ignoring the Gaussian prior term). Another way to think of the estimation process is
that it attempts to put as much mass as possible on the derivations leading to the gold-
standard structures (Riezler et al 2002). The Gaussian prior term prevents overfitting
by penalizing any model whose weights get too large in absolute value.
The estimation process can also be thought of in terms of the framework of
Della Pietra, Della Pietra, and Lafferty (1997), because setting the gradient in Equation
(17) to zero yields the usual maximum entropy constraints, namely that the expected
value of each feature is equal to its empirical value (again ignoring the Gaussian prior
term). However, in this case the empirical values are themselves expectations, over all
derivations leading to each gold-standard dependency structure.
509
Computational Linguistics Volume 33, Number 4
4.2 Estimating the Normal-Form Model
For the normal-form model, the training data consists of gold-standard normal-form
derivations. The objective function and gradient vector for the normal-form model are:
L?(?) = L(?) ? G(?) (18)
= log
m
?
j=1
P?(dj|Sj) ?
n
?
i=1
?2i
2?2i
?L?(?)
??i
=
m
?
j=1
fi(dj) (19)
?
m
?
j=1
?
d??(Sj )
e? ? f (d)fi(d)
?
d??(Sj ) e
? ? f (d) ?
?i
?2i
where dj is the the gold-standard normal-form derivation for sentence Sj and ?(Sj) is the
set of possible derivations for Sj. Note that ?(Sj) could contain some non-normal-form
derivations; however, because any non-normal-form derivations will be considered
incorrect, the resulting model will typically assign low probabilities to non-normal-form
derivations.
The empirical value in Equation (19) is simply a count of the number of times
the feature appears in the gold-standard normal-form derivations. The second term in
Equation (19) is an expectation over all derivations for each sentence.
4.3 The Limited-Memory BFGS Algorithm
The limited memory BFGS (L-BFGS) algorithm is a general purpose numerical optimiza-
tion algorithm (Nocedal and Wright 1999). In contrast to iterative scaling algorithms
such as GIS, which update the parameters one at a time on each iteration, L-BFGS
updates the parameters all at once on each iteration. It does this by considering the
topology of the feature space and moving in a direction which is guaranteed to increase
the value of the objective function.
The simplest way in which to consider the shape of the feature space is to move
in the direction in which the value of the objective function increases most rapidly;
this leads to the method of steepest-ascent. Hence steepest-ascent uses the first partial
derivative (the gradient) of the objective function to determine parameter updates.
L-BFGS improves on steepest-ascent by also considering the second partial derivative
(the Hessian). In fact, calculation of the Hessian can be prohibitively expensive, and so
L-BFGS estimates this derivative by observing the change in a fixed number of previous
gradients (hence the limited memory).
Malouf (2002) gives a more thorough description of numerical optimization meth-
ods applied to log-linear models. He also presents a convincing demonstration that gen-
eral purpose numerical optimization methods can greatly outperform iterative scaling
methods for many NLP tasks.3 Malouf uses standard numerical computation libraries
3 One NLP task for which we have found GIS to be especially suitable is sequence tagging, and we still use
GIS to estimate tagging models (Curran and Clark 2003).
510
Clark and Curran Wide-Coverage Efficient Statistical Parsing
as the basis of his implementation. One of our aims was to provide a self contained
estimation code base, and so we implemented our own version of the L-BFGS algorithm
as described in Nocedal and Wright (1999).
5. Efficient Estimation
The L-BFGS algorithm requires the following values at each iteration: the expected
value and the empirical expected value of each feature, for calculating the gradient;
and the value of the likelihood function. For the normal-form model, the empirical
expected values and the likelihood can be easily obtained, because these only involve
the single gold-standard derivation for each sentence. For the dependency model, the
computations of the empirical expected values and the likelihood function are more
complex, because these involve sums over just those derivations leading to the gold-
standard dependency structures. We explain how these derivations can be found in
Section 5.4. The next section explains how CCG charts can be represented in a way which
allows efficient estimation.
5.1 Packed CCG Charts as Feature Forests
The packed charts perform a number of roles. First, they compactly represent every
?derivation, dependency-structure? pair, by grouping together equivalent chart entries.
Entries are equivalent when they interact in the same manner with both the genera-
tion of subsequent parse structure and the statistical parse selection. In practice, this
means that equivalent entries have the same span; form the same structures, that is, the
remaining derivation plus dependencies, in any subsequent parsing; and generate the
same features in any subsequent parsing. Back pointers to the daughters indicate how
an individual entry was created, so that any derivation plus dependency structure can
be recovered from the chart.
The second role of the packed charts is to allow recovery of the highest scoring
derivation or dependency structure without enumerating all derivations. And finally,
packed charts are an instance of a feature forest, which Miyao and Tsujii (2002) show
can be used to efficiently estimate expected values of features, even though the expec-
tation may involve a sum over an exponential number of trees in the forest. One of the
contributions of this section is showing how Miyao and Tsujii?s feature forest approach
can be applied to a particular grammar formalism, namely CCG. As Chiang (2003) points
out, Miyao and Tsujii do not provide a way of constructing a feature forest given a
sentence, but provide the mathematical tools for estimation once the feature forest has
been constructed.
In our packed charts, entries are equivalent when they have the same category
type, identical head, and identical unfilled dependencies. The equivalence test must
account for heads and unfilled dependencies because equivalent entries form the same
dependencies in any subsequent parsing. Individual entries in the chart are obtained
by combining canonical representatives of equivalence classes, using the rules of the
grammar. Equivalence classes in the chart are sets of equivalent individual entries.
A feature forest ? is defined as a tuple ?C, D, R,?, ?? where:
 C is a set of conjunctive nodes;
 D is a set of disjunctive nodes;
511
Computational Linguistics Volume 33, Number 4
 R ? D is a set of root disjunctive nodes;
 ? : D ? 2C is a conjunctive daughter function;
 ? : C ? 2D is a disjunctive daughter function.
The interpretation of a packed chart as a feature forest is straightforward. First, only
entries which are part of a derivation spanning the whole sentence are relevant. These
entries can be found by traversing the chart top-down, starting with the entries which
span the sentence. Individual entries in a cell are the conjunctive nodes, which are either
?lexical category, word? pairs at the leaves, or have been obtained by combining two
equivalence classes (or applying a unary rule to an equivalence class). The equivalence
classes of individual entries are the disjunctive nodes. And finally, the equivalence
classes at the roots of the CCG derivations are the root disjunctive nodes.
For each feature function defined over parses (see Section 4) there is a corresponding
feature function defined over conjunctive nodes, that is, for each fi : ? ? N there is
a corresponding fi : C ? N which counts the number of times feature fi appears on a
particular conjunctive node. The value of fi for a parse is then the sum of the values of fi
for each conjunctive node in the parse.
The features used in the parsing model determine the definition of the equivalence
relation used for grouping individual entries. In our models, features are defined in
terms of individual dependencies and local rule instantiations, where a rule instan-
tiation is the local tree arising from the application of a rule in the grammar. Note
that features can be defined in terms of long-range dependencies, even though such
dependencies may involve words which are a long way apart in the sentence. Our
earlier definition of equivalence is consistent with these feature types.
As an example, consider the following composition of will with buy using the
forward composition rule:
(S[dcl]will\NP)/NP
(S[dcl]will\NP)/(S[b]\NP) (S[b]buy\NP)/NP
The equivalence class of the resulting individual entry is determined by the CCG cate-
gory plus heads, in this case (S[dcl]will\NP)/NP, plus the dependencies yet to be filled.
The dependencies are not shown, but there are two subject dependencies on the first
NP, one encoding the subject of will and one encoding the subject of buy, and there is
an object dependency on the second NP encoding the object of buy. Entries in the same
equivalence class are identical for the purposes of creating new dependencies for the
remainder of the parsing.
5.2 Feature Locality
It is possible to extend the locality of the features beyond single rule instantiations and
local dependencies. For example, the definition of equivalence given earlier allows the
incorporation of long-range dependencies as features. The equivalence test considers
unfilled dependencies which are both local and long-range; thus any individual entries
which have different long-range dependencies waiting to be filled will be in different
equivalence classes. One of the advantages of log-linear models is that it is easy to
include such features; Hockenmaier (2003b) describes the difficulties in including such
512
Clark and Curran Wide-Coverage Efficient Statistical Parsing
features in a generative model. One of the early motivations of the Edinburgh CCG
parsing project was to see if the long-range dependencies recovered by a CCG parser
could improve the accuracy of a parsing model. In fact, we have found that adding
long-range dependencies to any of the models described in this article has no impact on
accuracy. One possible explanation is that the long-range dependencies are so rare that a
much larger amount of training data would be required for these dependencies to have
an impact. Of course the fact that CCG enables recovery of long-range dependencies is
still a useful property, even if these dependencies are not currently useful as features,
because it improves the utility of the parser output.
There is considerable flexibility in defining the features for a parsing model in our
log-linear framework, as the long-range dependency example demonstrates, but the
need for dynamic programming for both estimation and decoding reduces the range of
features which can be used. Any extension to the ?locality? of the features would reduce
the effectiveness of the chart packing and any dynamic programming performed over
the chart. Two possible extensions, which we have not investigated, include defining de-
pendency features which account for all three elements of the triple in a PP-attachment
(Collins and Brooks 1995), and defining a rule feature which includes the grandparent
node (Johnson 1998). Another alternative for future work is to compare the dynamic
programming approach taken here with the beam-search approach of Collins and Roark
(2004), which allows more ?global? features.
5.3 Calculating Feature Expectations
For estimating both the normal-form model and the dependency model, the following
expectation of each feature fi, with respect to some model ?, is required:
E? fi =
?
S
1
ZS
?
???(S)
e? ? f (?)fi(?) (20)
where ?(S) is the set of all parses for sentence S, and ? is the vector of weights for ?.
This is essentially the same calculation for both models, even though for the de-
pendency model, features can be defined in terms of dependencies as well as the
derivations. Dependencies can be stored as part of the individual entries (conjunctive
nodes) at which they are created; hence all features can be defined in terms of the
individual entries which make up the derivations.
Calculating E? fi requires summing over all derivations ? which include fi for each
sentence S in the training data. The key to performing this sum efficiently is to write
the sum in terms of inside and outside scores for each conjunctive node. The inside and
outside scores can be defined recursively. If the inside score for a conjunctive node c
is denoted ?c, and the outside score denoted ?c, then the expected value of fi can be
written as follows:
E? fi =
?
S
1
ZS
?
c?CS
fi(c)?c?c (21)
where CS is the set of conjunctive nodes in the packed chart for sentence S.
513
Computational Linguistics Volume 33, Number 4
Figure 4
Example feature forest.
Figure 4 gives an example feature forest, and shows the nodes used to calculate
the inside and outside scores for conjunctive node c5. The inside score for a disjunctive
node, ?d, is the sum of the inside scores for its conjunctive node daughters:
?d =
?
c??(d)
?c (22)
The inside score for a conjunctive node, ?c, is defined in terms of the inside scores
of c?s disjunctive node daughters:
?c =
?
d??(c)
?d e? ? f (c) (23)
where ? ? f (c) =
?
i ?ifi(c). If the conjunctive node is a leaf node, the inside score is just
the exponentiation of the sum of the feature weights on that node.
The outside score for a conjunctive node, ?c, is the outside score for its disjunctive
node mother:
?c = ?d where c ? ?(d) (24)
The calculation of the outside score for a disjunctive node, ?d, is a little more
involved; it is defined as a sum over the conjunctive mother nodes, of the product
of the outside score of the mother, the inside score of the disjunctive node sister, and
the feature weights on the mother. For example, the outside score of d4 in Figure 4 is the
sum of two product terms. The first term is the product of the outside score of c5, the
inside score of d5, and the feature weights at c5; and the second term is the product of
the outside score of c2, the inside score of d3, and the feature weights at c2. The definition
is as follows; the outside score for a root disjunctive node is 1, otherwise:
?d =
?
{c|d??(c)}
?
??c
?
{d?|d???(c),d? =d}
?d? e? ? f (c)
?
? (25)
514
Clark and Curran Wide-Coverage Efficient Statistical Parsing
The normalization constant ZS is the sum of the inside scores for the root disjunctive
nodes:
ZS =
?
dr?R
?dr (26)
In order to calculate inside scores, the scores for daughter nodes need to be calcu-
lated before the scores for mother nodes (and vice versa for the outside scores). This can
easily be achieved by ordering the nodes in the bottom-up CKY parsing order.
5.4 Estimation for the Dependency Model
For the dependency model, the computations of the empirical expected values (17)
and the log-likelihood function (16) require sums over just those derivations leading
to the gold-standard dependency structure. We will refer to such derivations as correct
derivations. As far as we know, this problem of identifying derivations in a packed chart
which lead to a particular dependency structure has not been addressed before in the
NLP literature.
Figure 5 gives an algorithm for finding nodes in a packed chart which appear in
correct derivations. cdeps(c) returns the number of correct dependencies on conjunctive
node c, and returns the incorrect marker ? if there are any incorrect dependencies on
c; dmax(c) returns the maximum number of correct dependencies produced by any
sub-derivation headed by c, and returns ? if there are no sub-derivations producing
Figure 5
Algorithm for finding nodes in correct derivations.
515
Computational Linguistics Volume 33, Number 4
only correct dependencies; dmax(d) returns the same value but for disjunctive node d.
Recursive definitions of these functions are given in Figure 5; the base case occurs when
conjunctive nodes have no disjunctive daughters.
The algorithm identifies all those root nodes heading derivations which produce
just the correct dependencies, and traverses the chart top-down marking the nodes
in those derivations. The insight behind the algorithm is that, for two conjunctive
nodes in the same equivalence class, if one node heads a sub-derivation producing
more correct dependencies than the other node (and each sub-derivation only produces
correct dependencies), then the node with less correct dependencies cannot be part of a
correct derivation.
The conjunctive and disjunctive nodes appearing in correct derivations form a
new feature forest, which we call a correct forest. The correct forest forms a subset
of the complete forest (containing all derivations for the sentence). The correct and
complete forests can be used to estimate the required log-likelihood value and feature
expectations. Let E?? fi be the expected value of fi over the forest ? for model ?; then the
values in Equation (17) can be obtained by calculating E
?j
? fi for the complete forest ?j
for each sentence Sj in the training data, and also E
?j
? fi for each correct forest ?j:
?L(?)
??i
=
m
?
j=1
(E
?j
? fi ? E
?j
? fi) (27)
The log-likelihood in Equation (16) can be calculated as follows:
L(?) =
m
?
j=1
(log Z?j ? log Z?j ) (28)
where log Z? and log Z? are the normalization constants for ? and ?.
5.5 Estimation in Practice
Estimating the parsing models consists of generating packed charts for each sentence
in the training data, and then repeatedly calculating the values needed by the L-BFGS
estimation algorithm until convergence. Even though the packed charts are an efficient
representation of the derivation space, the charts for the complete training data (Sec-
tions 02?21 of CCGbank) take up a considerable amount of memory. One solution is to
only keep a small number of charts in memory at any one time, and to keep reading in
the charts on each iteration. However, given that the L-BFGS algorithm takes hundreds
of iterations to converge, this approach would be infeasibly slow.
Our solution is to keep all charts in memory by developing a parallel version of
the L-BFGS training algorithm and running it on an 18-node Beowulf cluster. As well
as solving the memory problem, another significant advantge of parallelization is the
reduction in estimation time: using 18 nodes allows our best-performing model to be
estimated in less than three hours.
We use the the Message Passing Interface (MPI) standard for the implementation
(Gropp et al 1996). The parallel implementation is a straightforward extension of the
BFGS algorithm. Each machine in the cluster deals with a subset of the training data,
516
Clark and Curran Wide-Coverage Efficient Statistical Parsing
holding the packed charts for that subset in memory. The key stages of the algorithm
are the calculations of the model expectations and the likelihood function. For a single-
process version these are calculated by summing over all the training instances in one
place. For a multi-process version, these are summed in parallel, and at the end of each
iteration the parallel sums are combined to give a master sum. Producing a master
operation across a cluster using MPI is a reduce operation. In our case, every node needs
to be holding a copy of the master sum, so we use an all reduce operation.
The MPI library handles all aspects of the parallelization, including finding the
optimal way of summing across the nodes of the Beowulf cluster (typically it is done
using a tree algorithm). In fact, the parallelization only adds around twenty lines of
code to the single-process implementation. Because of the simplicity of the parellel
communication between the nodes, parallelizing the estimation code is an example of
an embarrassingly parallel problem. One difficult aspect of the parallel implementation
is that debugging can be much harder, in which case it is often easier to test a non-MPI
version of the program first.
6. The Decoder
For the normal-form model, the Viterbi algorithm is used to find the most probable
derivation from a packed chart. For each equivalence class, we record the individual
entry at the root of the subderivation which has the highest score for the class. The
equivalence classes were defined so that any other individual entry cannot be part of the
highest scoring derivation for the sentence. The score for a subderivation d is
?
i ?ifi(d)
where fi(d) is the number of times the ith feature occurs in the subderivation. The
highest-scoring subderivations can be calculated recursively using the highest-scoring
equivalence classes that were combined to create the individual entry.
For the dependency model, the highest scoring dependency structure is required.
Clark and Curran (2003) outline an algorithm for finding the most probable dependency
structure, which keeps track of the highest scoring set of dependencies for each node in
the chart. For a set of equivalent entries in the chart (a disjunctive node), this involves
summing over all conjunctive node daughters which head sub-derivations leading to
the same set of high scoring dependencies. In practice large numbers of such conjunctive
nodes lead to very long parse times.
As an alternative to finding the most probable dependency structure, we have
developed an algorithm which maximizes the expected labeled recall over dependen-
cies. Our algorithm is based on Goodman?s (1996) labeled recall algorithm for the
phrase-structure PARSEVAL measures. As far as we know, this is the first application
of Goodman?s approach to finding highest scoring dependency structures. Watson,
Carroll, and Briscoe (2005) have also applied our algorithm to the grammatical relations
output by the RASP parser.
The dependency structure, ?max, which maximizes the expected recall is:
?max = argmax
?
?
?i
P(?i|S)|? ? ?i| (29)
where ?i ranges over the dependency structures for S. The expectation for a single de-
pendency structure ? is realized as a weighted intersection over all possible dependency
structures ?i for S. The intuition is that, if ?i is the gold standard, then the number of
dependencies recalled in ? is |? ? ?i|. Because we do not know which ?i is the gold
517
Computational Linguistics Volume 33, Number 4
standard, then we calculate the expected recall by summing the recall of ? relative to
each ?i, weighted by the probability of ?i.
The expression can be expanded further:
?max = argmax
?
?
?i
P(?i|S)
?
???
1 if ? ? ?i
= argmax
?
?
???
?
??|????
P(??|S)
= argmax
?
?
???
?
d??(?? )|????
P(d|S) (30)
The reason for this manipulation is that the expected recall score for ? is now
written in terms of a sum over the individual dependencies in ?, rather than a sum over
each dependency structure for S. The inner sum is over all derivations which contain
a particular individual dependency ?. Thus the final score for a dependency structure
? is a sum of the scores for each dependency ? in ?; and the score for a dependency ?
is the sum of the probabilities of those derivations producing ?. This latter sum can be
calculated efficiently using inside and outside scores:
?max = argmax
?
?
???
1
ZS
?
c?C
?c?c if ? ? deps(c) (31)
where ?c is the inside score and ?c is the outside score for node c; C is the set of conjunc-
tive nodes in the packed chart for sentence S and deps(c) is the set of dependencies on
conjunctive node c. The intuition behind the expected recall score is that a dependency
structure scores highly if it has dependencies produced by high probability derivations.4
The reason for rewriting the score in terms of individual dependencies is to make
use of the packed chart: The score for an individual dependency can be calculated using
dynamic programming (as explained previously), and the highest scoring dependency
structure can be found using dynamic programming also. The algorithm which finds
?max is essentially the same as the Viterbi algorithm described earlier, efficiently finding
a derivation which produces the highest scoring set of dependencies.
7. Model Features
The log-linear modeling framework allows considerable flexibility for representing the
parse space in terms of features. In this article we limit the features to those defined
over local rule instantiations and single predicate?argument dependencies. The fea-
ture sets described below differ for the dependency and normal-form models. The
4 Coordinate constructions can create multiple dependencies for a single argument slot; in this case the
score for these multiple dependencies is the average of the individual scores.
518
Clark and Curran Wide-Coverage Efficient Statistical Parsing
Table 1
Features common to the dependency and normal-form models.
Feature type Example
LexCat + Word (S/S)/NP + Before
LexCat + POS (S/S)/NP + IN
RootCat S[dcl]
RootCat + Word S[dcl] + was
RootCat + POS S[dcl] + VBD
Rule S[dcl]?NP S[dcl]\NP
Rule + Word S[dcl]?NP S[dcl]\NP + bought
Rule + POS S[dcl]?NP S[dcl]\NP + VBD
dependency model has features defined over the CCG predicate?argument dependen-
cies, whereas the dependencies for the normal-form model are defined in terms of
local rule instantiations in the derivation. Another difference is that the rule features
for the normal-form model are taken from the gold-standard normal-form deriva-
tions, whereas the dependency model contains rule features from non-normal-form
derivations.
There are a number of features defined over derivations which are common to
the dependency model and the normal-form model.5 First, there are features which
represent each ?word, lexical-category? pair in a derivation, and generalizations of these
which represent ?POS, lexical-category? pairs. Second, there are features representing
the root category of a derivation, which we also extend with the head word of the
root category; this latter feature is then generalized using the POS tag of the head (as
previously described). Third, there are features which encode rule instantiations?local
trees consisting of a parent and one or two children?in the derivation. The first set of
rule features encode the combining categories and the result category; the second set of
features extend the first by also encoding the head of the result category; and the third
set generalizes the second using POS tags. Table 1 gives an example for each of these
feature types.
The dependency model also has CCG predicate?argument dependencies as features,
defined as 5-tuples as in Section 3.4. In addition these features are generalized in three
ways using POS tags, with the word?word pair replaced with word?POS, POS?word,
and POS?POS. Table 2 gives some examples.
We extend the dependency features further by adding distance information. The
distance features encode the dependency relation and the word associated with the
lexical category (but not the argument word), plus some measure of distance between
the two dependent words. We use three distance measures which count the following:
the number of intervening words, with four possible values 0, 1, 2, or more; the number
of intervening punctuation marks, with four possible values 0, 1, 2, or more; and the
number of intervening verbs (determined by POS tag), with three possible values 0, 1,
or more. Each of these features is again generalized by replacing the word associated
with the lexical category with its POS tag.
5 Each feature has a corresponding frequency function, defined in Equation (14), which counts the number
of times the feature appears in a derivation.
519
Computational Linguistics Volume 33, Number 4
Table 2
Predicate?argument dependency features for the dependency model.
Feature type Example
Word?Word ?bought, (S\NP1)/NP2, 2, stake, (NP\NP)/(S[dcl]/NP)?
Word?POS ?bought, (S\NP1)/NP2, 2, NN, (NP\NP)/(S[dcl]/NP)?
POS?Word ?VBD, (S\NP1)/NP2, 2, stake, (NP\NP)/(S[dcl]/NP)?
POS?POS ?VBD, (S\NP1)/NP2, 2, NN, (NP\NP)/(S[dcl]/NP)?
Word + Distance(words) ?bought, (S\NP1)/NP2, 2, (NP\NP)/(S[dcl]/NP)? + 2
Word + Distance(punct) ?bought, (S\NP1)/NP2, 2, (NP\NP)/(S[dcl]/NP)? + 0
Word + Distance(verbs) ?bought, (S\NP1)/NP2, 2, (NP\NP)/(S[dcl]/NP)? + 0
POS + Distance(words) ?VBD, (S\NP1)/NP2, 2, (NP\NP)/(S[dcl]/NP)? + 2
POS + Distance(punct) ?VBD, (S\NP1)/NP2, 2, (NP\NP)/(S[dcl]/NP)? + 0
POS + Distance(verbs) ?VBD, (S\NP1)/NP2, 2, (NP\NP)/(S[dcl]/NP)? + 0
Table 3
Rule dependency features for the normal-form model.
Feature type Example
Word?Word ?company, S[dcl]?NP S[dcl]\NP, bought?
Word?POS ?company, S[dcl]?NP S[dcl]\NP, VBD?
POS?Word ?NN, S[dcl]?NP S[dcl]\NP, bought?
POS?POS ?NN, S[dcl]?NP S[dcl]\NP, VBD?
Word + Distance(words) ?bought, S[dcl]?NP S[dcl]\NP? + > 2
Word + Distance(punct) ?bought, S[dcl]?NP S[dcl]\NP? + 2
Word + Distance(verbs) ?bought, S[dcl]?NP S[dcl]\NP? + 0
POS + Distance(words) ?VBD, S[dcl]?NP S[dcl]\NP? + > 2
POS + Distance(punct) ?VBD, S[dcl]?NP S[dcl]\NP? + 2
POS + Distance(verbs) ?VBD, S[dcl]?NP S[dcl]\NP? + 0
For the normal-form model we follow Hockenmaier and Steedman (2002b) by de-
fining dependency features in terms of the local rule instantiations, by adding the heads
of the combining categories to the rule instantiation features.6 These are generalized
in three ways using POS tags, as shown in Table 3. There are also the three distance
measures which encode the distance between the two head words of the combining cat-
egories, as for the dependency model. Here the distance feature encodes the combining
categories, the result category, the head of the result category (either as a word or POS
tag), and the distance between the two head words.
For the features in the normal-form model, a frequency cutoff of two was applied;
that is, a feature had to occur at least twice in the gold-standard normal-form deriva-
tions to be included in the model. The same cutoff was applied to the features in the
dependency model, except for the rule instantiation feature types. For these features the
counting was done across all derivations licensed by the gold-standard lexical category
sequences and a frequency cutoff of 10 was applied. The larger cutoff was used because
the productivity of the grammar can lead to very large numbers of these features. We
6 We have also considered a model containing predicate?argument dependencies as well as local rule
dependencies, but adding the extra dependency feature types had no impact on the accuracy of the
normal-form model.
520
Clark and Curran Wide-Coverage Efficient Statistical Parsing
also only included those features which had a nonzero empirical count, that is, those
features which occured on at least one correct derivation. These feature types and
frequency cutoffs led to 475,537 features for the normal-form model and 632,591 features
for the dependency model.
8. The Supertagger
Parsing with lexicalized grammar formalisms such as CCG is a two-step process: first,
elementary syntactic structures?in CCG?s case lexical categories?are assigned to each
word in the sentence, and then the parser combines the structures together. The first
step can be performed by simply assigning to each word all lexical categories the word
is seen with in the training data, together with some strategy for dealing with rare
and unknown words (such as assigning the complete lexical category set; Hockenmaier
2003a). Because the number of lexical categories assigned to a word can be high, some
strategy is needed to make parsing practical; Hockenmaier, for example, uses a beam
search to discard chart entries with low scores.
In this article we take a different approach, by using a supertagger (Bangalore and
Joshi 1999) to perform step one. Clark and Curran (2004a) describe the supertagger,
which uses log-linear models to define a distribution over the lexical category set for
each local five-word context containing the target word (Ratnaparkhi 1996). The features
used in the models are the words and POS tags in the five-word window, plus the
two previously assigned lexical categories to the left. The conditional probability of a
sequence of lexical categories, given a sentence, is then defined as the product of the
individual probabilities for each category.
The most probable lexical category sequence can be found efficiently using a variant
of the Viterbi algorithm for HMM taggers. We restrict the categories which can be
assigned to a word by using a tag dictionary: for words seen at least k times in the
training data, the tagger can only assign categories which have been seen with the
word in the data. For words seen less than k times, an alternative based on the word?s
POS tag is used: The tagger can only assign categories which have been seen with the
POS tag in the data. We have found the tag dictionary to be beneficial in terms of both
efficiency and accuracy. A value of k = 20 was used in the experiments described in this
article.
The lexical category set used by the supertagger is described in Clark and Curran
(2004a) and Curran, Clark, and Vadas (2006). It includes all lexical catgeories which ap-
pear at least 10 times in Sections 02?21 of CCGbank, resulting in a set of 425 categories.
The Clark and Curran paper shows this set to have very high coverage on unseen data.
The accuracy of the supertagger on Section 00 of CCGbank is 92.6%, with a sentence
accuracy of 36.8%. Sentence accuracy is the percentage of sentences whose words are
all tagged correctly. These figures include punctuation marks, for which the lexical
category is simply the punctuation mark itself, and are obtained using gold standard
POS tags. With automatically assigned POS tags, using the POS tagger of Curran and
Clark (2003), the accuracies drop to 91.5% and 32.5%. An accuracy of 91?92% may ap-
pear reasonable given the large lexical category set; however, the low sentence accuracy
suggests that the supertagger may not be accurate enough to serve as a front-end to a
parser. Clark (2002) reports that a significant loss in coverage results if the supertagger
is used as a front-end to the parser of Hockenmaier and Steedman (2002b). In order
to increase the number of words assigned the correct category, we develop a CCG
multitagger, which is able to assign more than one category to each word.
521
Computational Linguistics Volume 33, Number 4
Table 4
Supertagger ambiguity and accuracy on Section 00.
? k CATS/WORD ACC SENT ACC ACC (POS) SENT ACC
0.075 20 1.27 97.34 67.43 96.34 60.27
0.030 20 1.43 97.92 72.87 97.05 65.50
0.010 20 1.72 98.37 77.73 97.63 70.52
0.005 20 1.98 98.52 79.25 97.86 72.24
0.001 150 3.57 99.17 87.19 98.66 80.24
The multitagger uses the following conditional probabilities:
P(yi|w1, . . . , wn) =
?
y1,...,yi?1,yi+1,...,yn
P(yi, y1, . . . , yi?1, yi+1, . . . , yn|w1, . . . , wn) (32)
Here yi is to be thought of as a constant category, whereas yj ( j = i) varies over the
possible categories for word j. In words, the probability of category yi, given the
sentence, is the sum of the probabilities of all sequences containing yi. This sum can
be calculated efficiently using a variant of the forward?backward algorithm. For each
word in the sentence, the multitagger then assigns all those categories whose probability
according to Equation (32) is within some factor, ?, of the highest probability category
for that word. In the implementation used here the forward?backward sum is limited to
those sequences allowed by the tag dictionary. For efficiency purposes, an extra pruning
strategy is also used to discard low probability sub-sequences before the forward?
backward algorithm is run. This uses a second variable-width beam of 0.1?.
Table 4 gives the per-word accuracy of the supertagger on Section 00 for various
levels of category ambiguity, together with the average number of categories per word.7
The SENT column gives the percentage of sentences whose words are all supertagged
correctly. The set of categories assigned to a word is considered correct if it contains
the correct category. The table gives results when using gold standard POS tags and, in
the final two columns, when using POS tags automatically assigned by the POS tagger
described in Curran and Clark (2003). The drop in accuracy is expected, given the
importance of POS tags as features.
The table demonstrates the significant reduction in the average number of cate-
gories that can be achieved through the use of a supertagger. To give one example,
the number of categories in the tag dictionary?s entry for the word is is 45. However,
in the sentence Mr. Vinken is chairman of Elsevier N.V., the Dutch publishing group., the
supertagger correctly assigns one category to is for all values of ?.
In our earlier work (Clark and Curran 2004a) the forward?backward algorithm was
not used to estimate the probability in Equation (32). Curran, Clark, and Vadas (2006)
investigate the improvement obtained from using the forward?backward algorithm,
and also address the drop in supertagger accuracy when using automatically assigned
POS tags. We show how to maintain some POS ambiguity through to the supertagging
phase, using a multi-POS tagger, and also how POS tag probabilities can be encoded
as real-valued features in the supertagger. The drop in supertagging accuracy when
7 The ? values used here are slightly different to the values used in earlier publications because the
pruning strategy used in the supertagger has changed slightly.
522
Clark and Curran Wide-Coverage Efficient Statistical Parsing
moving from gold to automatically assigned POS tags is reduced by roughly 50% across
the various values of ?.
9. Parsing in Practice
9.1 Combining the Supertagger and the Parser
The philosophy in earlier work which combined the supertagger and parser (Clark,
Hockenmaier, and Steedman 2002; Clark and Curran 2003) was to use an unrestrictive
setting of the supertagger, but still allow a reasonable compromise between speed and
accuracy. The idea was to give the parser the greatest possibility of finding the correct
parse, by initializing it with as many lexical categories as possible, but still retain
reasonable efficiency. However, for some sentences, the number of categories in the
chart gets extremely large with this approach, and parsing is unacceptably slow. Hence a
limit was applied to the number of categories in the chart, and a more restrictive setting
of the supertagger was reverted to if the limit was exceeded.
In this article we consider the opposite approach: Start with a very restrictive setting
of the supertagger, and only assign more categories if the parser cannot find an analysis
spanning the sentence. In this way the parser interacts much more closely with the
supertagger. In effect, the parser is using the grammar to decide if the categories pro-
vided by the supertagger are acceptable, and if not the parser requests more categories.
The advantage of this adaptive supertagging approach is that parsing speeds are much
higher, without any corresponding loss in accuracy. Section 10.3 gives results for the
speed of the parser.
9.2 Chart Parsing Algorithm
The algorithm used to build the packed charts is the CKY chart parsing algorithm
(Kasami 1965; Younger 1967) described in Steedman (2000). The CKY algorithm applies
naturally to CCG because the grammar is binary. It builds the chart bottom-up, starting
with constituents spanning a single word, incrementally increasing the span until the
whole sentence is covered. Because the constituents are built in order of span size, at any
point in the process all the sub-constituents which could be used to create a particular
new constituent must be present in the chart. Hence dynamic programming can be used
to prevent the need for backtracking during the parsing process.
9.3 Grammar Implementation
There is a trade-off between the size and coverage of the grammar and the efficiency
of the parser. One of our main goals in this work has been to develop a parser which
can provide analyses for the vast majority of linguistic constructions in CCGbank, but is
also efficient enough for large-scale NLP applications. In this section we describe some
of the decisions we made when implementing the grammar, with this trade-off in mind.
First, the lexical category set we use does not contain all the categories in Sec-
tions 02?21 of CCGbank. Applying a frequency cutoff of 10 results in a set of 425 lexical
categories. This set has excellent coverage on unseen data (Clark and Curran 2004a)
and is a manageable size for adding the head and dependency information, and also
mapping to grammatical relations for evaluation purposes (Section 11).
523
Computational Linguistics Volume 33, Number 4
Second, for the normal-form model, and also the hybrid dependency model de-
scribed in Section 10.2.1, two types of contraints on the grammar rules are used. Sec-
tion 3 described the Eisner constraints, in which any constituent which is the result of
a forward composition cannot serve as the primary (left) functor in another forward
composition or forward application; an analogous constraint applies for backward
composition. The second type of constraint only allows two categories to combine if
they have been seen to combine in the training data. Although this constraint only
permits category combinations seen in Sections 02?21 of CCGbank, we have found that
it is detrimental to neither parser accuracy nor coverage.
Neither of these constraints guarantee a normal-form derivation, but they are both
effective at reducing the size of the charts, which can greatly increase parser speed
(Clark and Curran 2004a). The constraints are also useful for training. Section 10 shows
that having a less restrictive setting on the supertagger, when creating charts for dis-
criminative training, can lead to more accurate models. However, the optimal setting
on the supertagger for training purposes can only be used when the constraints are
applied, because otherwise the memory requirements are prohibitive.
Following Steedman (2000), we place the following constraint on backward crossed
composition (for all models): The Y category in (7) cannot be an N or NP category. We
also place a similar constraint on backward composition. Both constraints reduce the
size of the charts considerably with no impact on coverage or accuracy.
Type-raising is performed by the parser for the categories NP, PP, and S[adj]\NP.
It is implemented by adding one of three fixed sets of categories to the chart whenever
an NP, PP, or S[adj]\NP is present. Appendix A gives the category sets. Each category
transformation is an instance of the following two rule schemata:
X ?T T/(T\X) (> T) (33)
X ?T T\(T/X) (< T) (34)
Appendix A lists the punctuation and type-changing rules implemented in the
parser. This is a larger grammar than we have used in previous articles (Clark and
Curran 2004b, 2004a, 2006), mainly because the improvement in the supertagger since
the earlier work means that we can now use a larger grammar but still maintain highly
efficient parsing.
10. Experiments
The statistics relating to model estimation were obtained using Sections 02?21 of CCG-
bank as training data. The results for parsing accuracy were obtained using Section
00 as development data and Section 23 as the final test data. The results for parsing
speed were obtained using Section 23. There are various hyperparameters in the parsing
system, for example the frequency cutoff for features, the ? parameter in the Gaussian
prior term, the ? values used in the supertagger, and so on. All of these were set
experimentally using Section 00 as development data.
10.1 Model Estimation
The gold standard for the normal-form model consists of the normal-form derivations
in CCGbank. For the dependency model, the gold-standard dependency structures are
524
Clark and Curran Wide-Coverage Efficient Statistical Parsing
produced by running our CCG parser over the normal-form derivations. It is essential
that the packed charts for each sentence contain the gold standard; for the normal-
form model this means that our parser must be able to produce the gold-standard
derivation from the gold-standard lexical category sequence; and for the dependency
model this means that at least one derivation in the chart must produce the gold-
standard dependency structure. Not all rule instantiations in CCGbank can be produced
by our parser, because some are not instances of combinatory rules, and others are very
rare punctuation and type-changing rules which we have not implemented. Hence it
is not possible for the parser to produce the gold standard for every sentence in Sec-
tions 02?21, for either the normal-form or the dependency model. These sentences are
not used in the training process.
For parsing the training data, we ensure that the correct category is a member of the
set assigned to each word. (We do not do this when parsing the test data.) The average
number of categories assigned to each word is determined by the ? parameter in the
supertagger. A category is assigned to a word if the category?s probability is within ? of
the highest probability category for that word. Hence the value of ? has a direct effect
on the size of the packed charts: Smaller ? values lead to larger charts.
For training purposes, the ? parameter determines how many incorrect derivations
will be used for each sentence for the discriminative training algorithm. We have found
that the ? parameter can have a large impact on the accuracy of the resulting models:
If the ? value is too large, then the training algorithm does not have enough incorrect
derivations to ?discriminate against?; if the ? value is too small, then this introduces
too many incorrect derivations into the training process, and can lead to impractical
memory requirements.
For some sentences, the packed charts can become very large. The supertagging
approach we adopt for training differs from that used for testing and follows the original
approach of Clark, Hockenmaier, and Steedman (2002): If the size of the chart exceeds
some threshold, the value of ? is increased, reducing ambiguity, and the sentence is
supertagged and parsed again. The threshold which limits the size of the charts was set
at 300,000 individual entries. (This is the threshold used for training; a higher value was
used for testing.) For a small number of long sentences the threshold is exceeded even
at the largest ? value; these sentences are not used for training.
For the normal-form model we were able to use 35,732 sentences for training (90.2%
of Sections 02?21) and for the dependency model 35,889 sentences (90.6%). Table 5
gives training statistics for the normal-form and dependency models (and a hybrid
model described in Section 10.2.1), for various sequences of ? values, when the training
algorithm is run to convergence on an 18-node cluster. The training algorithm is defined
to have converged when the percentage change in the objective function is less than
0.0001%. The ? value in Equation (16), which was determined experimentally using the
development data, was set at 1.3 for all the experiments in this article.
Table 5
Training statistics.
Model ? values CPU time (min.) Iterations RAM (GB)
Dependency 0.1 176.0 750 24.4
Normal-form 0.1 17.2 420 5.3
Normal-form 0.0045, 0.0055, 0.01, 0.05, 0.1 72.1 466 16.1
Hybrid 0.0045, 0.0055, 0.01, 0.05, 0.1 128.4 610 22.5
525
Computational Linguistics Volume 33, Number 4
The main reason that the normal-form model requires less memory and converges
faster than the dependency model is that, for the normal-form model, we applied the
two types of normal-form restriction described in Section 9.3: First, categories can only
combine if they appear together in a rule instantiation in Sections 2?21 of CCGbank;
and second, we applied the Eisner constraints described in Section 3.
We conclude this section by noting that it is only through the use of the supertagger
that we are able to perform the discriminative estimation at all; without it the memory
requirements would be prohibitive, even when using the cluster.
10.2 Parsing Accuracy
This section gives accuracy figures on the predicate?argument dependencies in CCG-
bank. Overall results are given, as well as results broken down by relation type, as
in Clark, Hockenmaier, and Steedman (2002). Because the purpose of this article is
to demonstrate the feasibility of wide-coverage parsing with CCG, we do not give an
evaluation targeted specifically at long-range dependencies; such an evaluation was
presented in Clark, Steedman, and Curran (2004).
For evaluation purposes, the threshold parameter which limits the size of the charts
was set at 1,000,000 individual entries. This value was chosen to maximize the coverage
of the parser, so that the evaluation is performed on as much of the unseen data as
possible. This was also the threshold parameter used for the speed experiments in
Section 10.3.
All of the intermediate results were obtained using Section 00 of CCGbank as
development data. The final test result, showing the performance of the best performing
model, was obtained using Section 23. Evaluation was performed by comparing the
dependency output of the parser against the predicate?argument dependencies in CCG-
bank. We report precision, recall, and F-scores for labeled and unlabeled dependencies,
and also category accuracy. The category accuracy is the percentage of words assigned
the correct lexical category by the parser (including punctuation). The labeled depen-
dency scores take into account the lexical category containing the dependency relation,
the argument slot, the word associated with the lexical category, and the argument
head word: All four must be correct to score a point. For the unlabeled scores, only
the two dependent words are considered. The F-score is the balanced harmonic mean of
precision (P) and recall (R): 2PR/(P + R). The scores are given only for those sentences
which were parsed successfully. We also give coverage values showing the percentage
of sentences which were parsed successfully.
Using the CCGbank dependencies for evaluation is a departure from our earlier
work, in which we generated our own gold standard by running the parser over the
derivations in CCGbank and outputting the dependencies. In this article we wanted to
use a gold standard which is easily accessible to other researchers. However, there are
some differences between the dependency scheme used by our parser and CCGbank.
For example, our parser outputs some coordination dependencies which are not in
CCGbank; also, because the parser currently encodes every argument slot in each lexical
category as a dependency relation, there are some relations, such as the subject of to
in a to-infinitival construction, which are not in CCGbank either. In order to provide
a fair evaluation, we ignore those dependency relations. This still leaves some minor
differences. We can measure the remaining differences as follows: Comparing the CCG-
bank dependencies in Section 00 against those generated by running our parser over
the derivations in 00 gives labeled precision and recall values of 99.80% and 99.18%,
526
Clark and Curran Wide-Coverage Efficient Statistical Parsing
respectively. Thus there are a small number of dependencies in CCGbank which the
current version of the parser can never get right.
10.2.1 Dependency Model vs. Normal-Form Model. Table 6 shows the results for the normal-
form and dependency models evaluated against the predicate?argument dependen-
cies in CCGbank. Gold standard POS tags were used; the LF(POS) column gives the
labeled F-score with automatically assigned POS tags for comparison. Decoding with
the dependency model involves finding the maximum-recall dependency structure, and
decoding with the normal-form model involves finding the most probable derivation,
as described in Section 6. The ? value refers to the setting of the supertagger used for
training and is the first in the sequence of ?s from Table 5. The ? values used during
the testing are those in Table 4 and the new, efficient supertagging strategy of taking the
highest ? value first was used.
With the same ? values used for training (? = 0.1), the results for the dependency
model are slightly higher than for the normal-form model. However, the coverage of the
normal-form model is higher (because the use of the normal-form constraints mean that
there are less sentences which exceed the chart-size threshold). One clear result from the
table is that increasing the chart size used for training, by using smaller ? values, can
significantly improve the results, in this case around 1.5% F-score for the normal-form
model.
The training of the dependency model already uses most of the RAM available on
the cluster. However, it is possible to use smaller ? values for training the dependency
model if we also apply the two types of normal-form restriction used by the normal-
form model. This hybrid model still uses the features from the dependency model;
it is still trained using dependency structures as the gold standard; and decoding is
still performed using the maximum-recall algorithm; the only difference is that the
derivations in the charts are restricted by the normal-form constraints (both for training
and testing). Table 5 gives the training statistics for this model, compared to the de-
pendency and normal-form models. The number of sentences we were able to use for
training this model was 36,345 (91.8% of Sections 02?21). The accuracy of this hybrid
dependency model is given in Table 7. These are the highest results we have obtained
to date on Section 00. We also give the results for the normal-form model from Table 6
for comparison.
Table 8 gives the results for the hybrid dependency model, broken down by relation
type, using the same relations given in Clark, Hockenmaier, and Steedman (2002).
Automatically assigned POS tags were used.
10.2.2 Final Test Results. Table 9 gives the final test results on Section 23 for the hybrid
dependency model. The coverage for these results is 99.63% (for gold-standard POS
Table 6
Results for dependency and normal-form models on Section 00.
LF SENT CAT
Model ? LP LR LF (POS) ACC UP UR UF ACC cov
Dependency 0.1 86.52 84.97 85.73 84.24 32.06 92.91 91.24 92.07 93.37 98.17
Normal-form 0.1 85.50 84.68 85.08 83.34 31.93 92.38 91.49 91.93 93.04 99.06
Normal-form 0.0045 87.17 86.30 86.73 84.74 34.99 93.21 92.28 92.74 94.05 99.06
527
Computational Linguistics Volume 33, Number 4
Table 7
Results on Section 00 with both models using normal-form constraints.
LF SENT CAT
Model ? LP LR LF (POS) ACC UP UR UF ACC cov
Normal-form 0.0045 87.17 86.30 86.73 84.74 34.99 93.21 92.28 92.74 94.05 99.06
Hybrid dependency 0.0045 88.06 86.43 87.24 85.25 35.67 93.88 92.13 93.00 94.16 99.06
Table 8
Results for the hybrid dependency model on Section 00 by dependency relation.
Lexical category Arg Slot LP % # deps LR % # deps F-score
NX/NX,1 1 nominal modifier 95.28 7,314 95.62 7,288 95.45
NPX/NX,1 1 determiner 96.57 4,078 96.03 4,101 96.30
(NPX\NPX,1)/NP2 2 np modifying prep 82.17 2,574 88.90 2,379 85.40
(NPX\NPX,1)/NP2 1 np modifying prep 81.58 2,285 85.74 2,174 83.61
((SX\NPY)\(SX,1\NPY))/NP2 2 vp modifying prep 71.94 1,169 73.32 1,147 72.63
((SX\NPY)\(SX,1\NPY))/NP2 1 vp modifying prep 70.92 1,073 71.93 1,058 71.42
(S[dcl]\NP1)/NP2 1 transitive verb 81.62 914 85.55 872 83.54
(S[dcl]\NP1)/NP2 2 transitive verb 81.57 971 86.37 917 83.90
(SX\NPY)\(SX,1\NPY) 1 adverbial modifier 86.85 745 86.73 746 86.79
PP/NP1 1 prep complement 75.06 818 70.09 876 72.49
(S[b]\NP1)/NP2 2 inf transitive verb 84.01 663 87.03 640 85.50
(S[dcl]\NPX,1)/(S[b]2\NPX) 2 auxiliary 97.70 478 97.90 477 97.80
(S[dcl]\NPX,1)/(S[b]2\NPX) 1 auxiliary 94.15 479 92.99 485 93.57
(S[b]\NP1)/NP2 1 inf transitive verb 77.82 496 73.95 522 75.83
(NPX/NX,1)\NP2 1 s genitive 96.57 379 95.56 383 96.06
(NPX/NX,1)\NP2 2 s genitive 97.35 377 98.66 372 98.00
(S[dcl]\NP1)/S[dcl]2 1 sentential comp verb 94.88 371 90.96 387 92.88
(NPX\NPX,1)/(S[dcl]2\NPX) 1 subject rel pronoun 85.77 260 81.39 274 83.52
(NPX\NPX,1)/(S[dcl]2\NPX) 2 subject rel pronoun 97.45 275 97.10 276 97.28
(NPX\NPX,1)/(S[dcl]2/NPX) 1 object rel pronoun 81.82 22 69.23 26 75.00
(NPX\NPX,1)/(S[dcl]2/NPX) 2 object rel pronoun 86.36 22 82.61 23 84.44
NP/(S[dcl]1/NP) 1 headless obj rel pron 100.00 17 100.00 17 100.00
Table 9
Results for the hybrid dependency model on Section 23.
LP LR LF SENT UP UR UF CAT ACC cov
Hybrid dependency 88.34 86.96 87.64 36.53 93.74 92.28 93.00 94.32 99.63
Hybrid dependency (POS) 86.17 84.74 85.45 32.92 92.43 90.89 91.65 92.98 99.58
Hockenmaier (2003) 84.3 84.6 84.4 ? 91.8 92.2 92.0 92.2 99.83
Hockenmaier (POS) 83.1 83.5 83.3 ? 91.1 91.5 91.3 91.5 99.83
tags), which corresponds to 2,398 of the 2,407 sentences in Section 23 receiving an
analysis. When using automatically assigned POS tags, the coverage is slightly lower:
99.58%. We used version 1.2 of CCGbank to obtain these results. Results are also given
for Hockenmaier?s parser (Hockenmaier 2003a) which used an earlier, slightly different
version of the treebank. We wanted to use the latest version to enable other researchers
to compare with our results.
528
Clark and Curran Wide-Coverage Efficient Statistical Parsing
10.3 Parse Times
The results in this section were obtained using a 3.2 GHz Intel Xeon P4. Table 10 gives
parse times for the 2,407 sentences in Section 23 of CCGbank. In order not to optimize
speed by compromising accuracy, we used the hybrid dependency model, together
with both kinds of normal-form constraints, and the maximum-recall decoder. Times
are given for both automatically assigned POS tags and gold-standard POS tags (POS).
The sents and words columns give the number of sentences, and the number of words,
parsed per second. For all of the figures reported on Section 23, unless stated otherwise,
we chose settings for the various parameters which resulted in a coverage of 99.6%. It is
possible to obtain an analysis for the remaining 0.4%, but at a significant loss in speed.
The parse times and speeds include the failed sentences, and include the time taken by
the supertagger, but not the POS tagger; however, the POS tagger is extremely efficient,
taking less than 4 seconds to supertag Section 23, most of which consists of load time
for the Maximum Entropy model.
The first row corresponds to the strategy of earlier work by starting with an un-
restrictive setting of the supertagger. The first value of ? is 0.005; if the parser cannot
find a spanning analysis, this is changed to ? = 0.001k=150, which increases the average
number of categories assigned to a word by decreasing ? and increasing the tag-
dictionary parameter. If the node limit is exceeded at ? = 0.005 (for these experiments
the node limit is set at 1,000,000), ? is changed to 0.01. If the node limit is still exceeded,
? is changed to 0.03, and finally to 0.075.
The second row corresponds to the new strategy of starting with the most restrictive
setting of the supertagger (? = 0.075), and moving through the settings if the parser
cannot find a spanning analysis. The table shows that the new strategy has a significant
impact on parsing speed, increasing it by a factor of 3 over the earlier approach (given
the parameter settings used in these experiments).
The penultimate row corresponds to using only one supertagging level with ? =
0.075; the parser ignores the sentence if it cannot get an analysis at this level. The per-
centage of sentences without an analysis is now over 6% (with automatically assigned
POS tags), but the parser is extremely fast, processing over 30 sentences per second. This
configuration of the system would be useful for obtaining data for lexical knowledge
acquisition, for example, for which large amounts of data are required. The oracle row
gives the parser speed when it is provided with only the correct lexical categories,
showing the speeds which could be achieved given the perfect supertagger.
Table 11 gives the percentage of sentences which are parsed at each supertagger
level, for both the new and old parsing strategies. The results show that, for the old
approach, most of the sentences are parsed using the least restrictive setting of the
supertagger (? = 0.005); conversely, for the new approach, most of the sentences are
Table 10
Parse times for Section 23.
Supertagging/parsing Time Sents/ Words/ Time (POS) Sents (POS)/ Words (POS)/
constraints (sec) sec sec (sec) sec sec
? = 0.005 ? . . . ? 0.075 379.4 6.3 145.9 369.0 6.5 150.0
? = 0.075 ? . . . 0.001k=150 99.9 24.1 554.3 116.6 20.6 474.7
? = 0.075 (95.3/93.4% cov) 79.7 30.2 694.6 79.8 30.1 693.5
Oracle (94.7% cov) 23.8 101.0 2,322.6
529
Computational Linguistics Volume 33, Number 4
Table 11
Supertagger ? levels used on Section 00.
0.075 FIRST 0.005 FIRST
? CATS/WORD PARSES % PARSES %
0.075 1.27 1,786 93.4 3 0.2
0.03 1.43 45 2.4 5 0.3
0.01 1.72 24 1.3 2 0.1
0.005 1.98 18 0.9 1,863 97.4
0.001k=150 3.57 22 1.2 22 1.2
FAIL 18 0.9 18 0.9
Table 12
Comparing parser speeds on Section 23 of the WSJ Penn Treebank.
Parser Time (min.)
Collins 45
Charniak 28
Sagae 11
CCG 1.9
parsed using the most restrictive setting (? = 0.075). This suggests that, in order to
increase the accuracy of the parser without losing efficiency, the accuracy of the su-
pertagger at the ? = 0.075 level needs to be improved, without increasing the number
of categories assigned on average.
A possible response to our policy of adaptive supertagging is that any statistical
parser can be made to run faster, for example by changing the beam parameter in
the Collins (2003) parser, but that any increase in speed is typically associated with a
reduction in accuracy. For the CCG parser, the accuracy did not degrade when using the
new adaptive parsing strategy. Thus the accuracy and efficiency of the parser were not
tuned separately: The configuration used to obtain the speed results was also used to
obtain the accuracy results in Sections 10.2 and 11.
To give some idea of how these parsing speeds compare with existing parsers,
Table 12 gives the parse times on Section 23 for a number of well-known parsers. Sagae
and Lavie (2005) is a classifier-based linear time parser. The times for the Sagae, Collins,
and Charniak parsers were taken from the Sagae and Lavie paper, and were obtained
using a 1.8 GHz P4, compared to a 3.2 GHz P4 for the CCG numbers. Comparing parser
speeds is especially problematic because of implementation differences and the fact
that the accuracy of the parsers is not being controlled. Thus we are not making any
strong claims about the efficiency of parsing with CCG compared to other formalisms.
However, the results in Table 12 add considerable weight to one of our main claims in
this article, namely, that highly efficient parsing is possible with CCG, and that large-
scale processing is possible with linguistically motivated grammars.
11. Cross-Formalism Comparison
An obvious question is how well the CCG parser compares with parsers using different
grammar formalisms. One question we are often asked is whether the CCG derivations
530
Clark and Curran Wide-Coverage Efficient Statistical Parsing
output by the parser could be converted to Penn Treebank?style trees to enable a com-
parison with, for example, the Collins and Charniak parsers. The difficulty is that CCG
derivations often have a different shape to the Penn Treebank analyses (coordination
being a prime example) and reversing the mapping used by Hockenmaier to create
CCGbank is a far from trivial task.
There is some existing work comparing parser performance across formalisms.
Briscoe and Carroll (2006) evaluate the RASP parser on the Parc Dependency Bank
(DepBank; King et al 2003). Cahill et al (2004) evaluate an LFG parser, which uses an
automatically extracted grammar, against DepBank. Miyao and Tsujii (2004) evaluate
their HPSG parser against PropBank (Palmer, Gildea, and Kingsbury 2005). Kaplan et al
(2004) compare the Collins parser with the Parc LFG parser by mapping Penn Treebank
parses into the dependencies of DepBank, claiming that the LFG parser is more accurate
with only a slight reduction in speed. Preiss (2003) compares the parsers of Collins
and Charniak, the grammatical relations finder of Buchholz, Veenstra, and Daelemans
(1999), and the Briscoe and Carroll (2002) parser, using the gold-standard grammatical
relations (GRs) from Carroll, Briscoe, and Sanfilippo (1998). The Penn Treebank trees of
the Collins and Charniak parsers, and the GRs of the Buchholz parser, are mapped into
the required grammatical relations, with the result that the GR finder of Buchholz is the
most accurate.
There are a number of problems with such evaluations. The first is that, when
converting the output of the Collins parser, for example, into the output of another
parser, the Collins parser is at an immediate disadvantage. This is especially true if the
alternative output is significantly different from the Penn Treebank trees and if the in-
formation required to produce the alternative output is hard to extract. One could argue
that the relative lack of grammatical information in the output of the Collins parser is a
weakness and any evaluation should measure that. However, we feel that the onus of
mapping into another formalism should ideally lie with the researchers making claims
about their own particular parser. The second difficulty is that some constructions may
be analyzed differently across formalisms, and even apparently trivial differences such
as tokenization can complicate the comparison (Crouch et al 2002).
Despite these difficulties we have attempted a cross-formalism comparison of the
CCG parser. For the gold standard we chose the version of DepBank reannotated by
Briscoe and Carroll (2006) (hereafter B&C), consisting of 700 sentences from Section 23
of the Penn Treebank. The B&C scheme is similar to the original DepBank scheme in
many respects, but overall contains less grammatical detail. Briscoe and Carroll (2006)
describe the differences between the two schemes.
We chose this resource for the following reasons: It is publicly available, allowing
other researchers to compare against our results; the GRs making up the annotation
share some similarities with the predicate?argument dependencies output by the CCG
parser; and we can directly compare our parser against a non-CCG parser, namely the
RASP parser?and because we are converting the CCG output into the format used by
RASP the CCG parser is not at an unfair advantage. There is also the SUSANNE GR gold
standard (Carroll, Briscoe, and Sanfilippo 1998), on which the B&C annotation is based,
but we chose not to use this for evaluation. This earlier GR scheme is less like the depen-
dencies output by the CCG parser, and the comparison would be complicated further by
fact that, unlike CCGbank, the SUSANNE corpus is not based on the Penn Treebank.
The GRs are described in Briscoe (2006), Briscoe and Carroll (2006), and Briscoe,
Carroll, and Watson (2006). Table 13 contains the complete list of GRs used in the
evaluation, with examples taken from Briscoe. The CCG dependencies were trans-
formed into GRs in two stages. The first stage was to create a mapping between the
531
Computational Linguistics Volume 33, Number 4
Table 13
The grammatical relations scheme used in the cross-formalism comparison.
GR Description Example Relevant GRs in example
conj coordinator Kim likes oranges, apples, (dobj likes and)
and satsumas or clementines (conj and oranges)
(conj and apples)
(conj and or)
(conj or satsumas)
(conj or clementines)
aux auxiliary Kim has been sleeping (aux sleeping has)
(aux sleeping been)
det determiner the man (det man the)
ncmod non-clausal modifier the old man in the barn slept (ncmod man old)
(ncmod man in)
(dobj in barn)
the butcher?s shop (ncmod poss shop butcher)
xmod unsaturated predicative who to talk to (xmod to who talk)
modifier (iobj talk to)
(dobj to who)
cmod saturated clausal modifier although he came, Kim left (cmod left although)
(ccomp although came)
ncsubj non-clausal subject Kim left (ncsubj left Kim )
the upset man (ncsubj upset man obj)
(passive upset)
Kim wants to go (ncsubj go Kim )
He?s going said Kim (ncsubj said Kim inv)
passive passive verb issues were filed (passive filed)
(ncsubj filed issues obj)
xsubj unsaturated predicative subject leaving matters (xsubj matters leaving )
csubj saturated clausal subject that he came matters (csubj matters came that)
dobj direct object she gave it to Kim (dobj gave it)
(iobj gave to)
(dobj to Kim)
obj2 second object she gave Kim toys (obj2 gave toys)
(dobj gave Kim)
iobj indirect object Kim flew to Paris from Geneva (iobj flew to)
(iobj flew from)
(dobj to Paris)
(dobj from Geneva)
pcomp PP which is a PP complement Kim climbed through into the attic (pcomp climbed through)
(pcomp through into)
(dobj into attic)
xcomp unsaturated VP complement Kim thought of leaving (xcomp thought of)
(xcomp of leaving)
ccomp saturated clausal complement Kim asked about him playing rugby (ccomp asked about)
(ccomp about him)
(ncsubj playing him )
(dobj playing rugby)
ta textual adjunct delimited He made the discovery: (ta colon discovery was)
by punctuation Kim was the abbot
CCG dependencies and the GRs. This involved mapping each argument slot in the 425
lexical categories in the CCG lexicon onto a GR. In the second stage, the GRs created
for a particular sentence?by applying the mapping to the parser output?were passed
through a Python script designed to correct some of the obvious remaining differences
between the CCG and GR representations.
In the process of performing the transformation we encountered a methodological
problem: Without looking at examples it was difficult to create the mapping and im-
possible to know whether the two representations were converging. Briscoe, Carroll,
532
Clark and Curran Wide-Coverage Efficient Statistical Parsing
and Watson (2006) split the 700 sentences in DepBank into a test and development set,
but the latter only consists of 140 sentences which we found was not enough to reliably
create the transformation. There are some development files in the RASP release which
provide examples of the GRs, which we used when possible, but these only cover a
subset of the CCG lexical categories.
Our solution to this problem was to convert the gold-standard dependencies from
CCGbank into GRs and use these to develop the transformation. So we did inspect the
annotation in DepBank, and compared it to the transformed CCG dependencies, but
only the gold-standard CCG dependencies. Thus the parser output was never used dur-
ing this process. We also ensured that the dependency mapping and the post-processing
are general to the GRs scheme and not specific to the test set. Table 14 gives some
examples of the dependency mapping. Because the number of sentences annotated with
GRs is so small, the only other option would have been to guess at various DepBank
analyses, which would have made the the evaluation even more biased against the CCG
parser.
One advantage of this approach is that, by comparing the transformed gold-
standard CCG dependencies with the gold-standard GRs, we can measure how close
the CCG representation is to the GRs. This provides some indication of how difficult it
is to perform the transformation, and also provides an upper bound on the accuracy of
the parser on DepBank. This method would be useful when converting the output of
the Collins parser into an alternative representation (Kaplan et al 2004): Applying the
transformation to the gold-standard Penn Treebank trees and comparing with DepBank
would provide an upper bound on the performance of the Collins parser and give some
indication of the effectiveness of the transformation.
11.1 Mapping the CCG Dependencies to GRs
Table 14 gives some examples of the mapping. In our notation, %l indicates the word
associated with the lexical category and %f is the head of the constituent filling the
Table 14
Examples of the CCG dependency to GRs mapping; $l denotes the word associated with the
lexical category and $f is the filler.
CCG lexical category arg slot GR
(S[dcl]\NP1)/NP2 1 (ncsubj %l %f )
(S[dcl]\NP1)/NP2 2 (dobj %l %f)
(S\NP)/(S\NP)1 1 (ncmod %f %l)
(NP\NP1)/NP2 1 (ncmod %f %l)
(NP\NP1)/NP2 2 (dobj %l %f)
NP[nb]/N1 1 (det %f %l)
(NP\NP1)/(S[pss]\NP)2 1 (xmod %f %l)
(NP\NP1)/(S[pss]\NP)2 2 (xcomp %l %f)
((S\NP)\(S\NP)1)/S[dcl]2 1 (cmod %f %l)
((S\NP)\(S\NP)1)/S[dcl]2 2 (ccomp %l %f)
(S[dcl]\NP1)/(S[adj]\NP)2 1 (ncsubj %l %f )
(S[dcl]\NP1)/(S[adj]\NP)2 2 (xcomp %l %f)
((S[dcl]\NP1)/NP2)/NP3 1 (ncsubj %l %f )
((S[dcl]\NP1)/NP2)/NP3 2 (obj2 %l %f)
((S[dcl]\NP1)/NP2)/NP3 3 (dobj %l %f)
(S[dcl]\NP1)/(S[b]\NP)2 2 (aux %f %l)
533
Computational Linguistics Volume 33, Number 4
argument slot. For many of the CCG dependencies, the mapping into GRs is straightfor-
ward. For example, the first two rows of Table 14 show the mapping for the transitive
verb category (S[dcl]\NP1)/NP2: Argument slot 1 is a non-clausal subject and argument
slot 2 is a direct object. In the example Kim likes juicy oranges, likes is associated with the
transitive verb category, Kim is the subject, and oranges is the head of the constituent
filling the object slot, leading to the following GRs: (ncsubj likes Kim ) and (dobj
likes oranges). The third row shows an example of a modifier: (S\NP)/(S\NP) modi-
fies a verb phrase to the right. Note that, in this example, the order of the lexical category
(%l) and filler (%f) is switched compared to the previous example to match the DepBank
annotation.
There are a number of reasons why creating the dependency transformation is
more difficult than these examples suggest. The first problem is that the mapping from
CCG dependencies to GRs is many-to-many. For example, the transitive verb category
(S[dcl]\NP)/NP applies to the copular in sentences like Imperial Corp. is the parent of
Imperial Savings & Loan. With the default annotation the relation between is and parent
would be dobj, whereas in DepBank the argument of the copular is analyzed as an
xcomp. Table 15 gives some examples of how we attempt to deal with this problem.
The constraint in the first example means that, whenever the word associated with the
transitive verb category is a form of be, the second argument is xcomp, otherwise the
default case applies (in this case dobj). There are a number of categories with similar
constraints, checking whether the word associated with the category is a form of be.
The second type of constraint, shown in the third line of the table, checks the lexical
category of the word filling the argument slot. In this example, if the lexical category of
the preposition is PP/NP, then the second argument of (S[dcl]\NP)/PP maps to iobj;
thus in The loss stems from several factors the relation between the verb and preposition
is (iobj stems from). If the lexical category of the preposition is PP/(S[ng]\NP),
then the GR is xcomp; thus in The future depends on building cooperation the relation
between the verb and preposition is (xcomp depends on). There are a number of
CCG dependencies with similar constraints, many of them covering the iobj/xcomp
distinction.
The second difficulty in creating the transformation is that not all the GRs are binary
relations, whereas the CCG dependencies are all binary. The primary example of this is
to-infinitival constructions. For example, in the sentence The company wants to wean itself
away from expensive gimmicks, the CCG parser produces two dependencies relating wants,
to and wean, whereas there is only one GR: (xcomp to wants wean). The final row of
Table 15
Examples of the many-to-many nature of the CCG dependency to GRs mapping, and a
terniary GR.
CCG lexical category Slot GR Constraint Example
(S[dcl]\NP1)/NP2 2 (xcomp %l %f) word=be The parent is Imperial
(dobj %l %f) The parent sold Imperial
(S[dcl]\NP1)/PP2 2 (iobj %l %f) cat=PP/NP The loss stems from
several factors
(xcomp %l %f) cat=PP/(S[ng]\NP) The future depends on
building cooperation
(S[dcl]\NP1)/ 2 (xcomp %f %l %k) cat=(S[to]\NP)/ wants to wean itself
(S[to]\NP)2 (S[b]\NP) away from
534
Clark and Curran Wide-Coverage Efficient Statistical Parsing
Table 15 gives an example. We implement this constraint by introducing a %k variable
into the GR template which denotes the argument of the category in the constraint
column (which, as before, is the lexical category of the word filling the argument slot). In
the example, the current category is (S[dcl]\NP1)/(S[to]\NP)2, which is associated with
wants; this combines with (S[to]\NP)/(S[b]\NP), associated with to; and the argument of
(S[to]\NP)/(S[b]\NP) is wean. The %k variable allows us to look beyond the arguments
of the current category when creating the GRs.
A further difficulty in creating the transformation is that the head passing con-
ventions differ between DepBank and CCGbank. By ?head passing? we mean the
mechanism which determines the heads of constituents and the mechanism by which
words become arguments of long-range dependencies. For example, in the sentence
The group said it would consider withholding royalty payments, the DepBank and CCGbank
annotations create a dependency between said and the following clause. However, in
DepBank the relation is between said and consider, whereas in CCGbank the relation is
between said and would. We fixed this problem by changing the head of would consider to
be consider rather than would. In practice this means changing the annotation of all the
relevant lexical categories in the markedup file.8 The majority of the categories to which
this applies are those creating aux relations.
A related difference between the two resources is that there are more subject re-
lations in CCGbank than DepBank. In the previous example, CCGbank has a subject
relation between it and consider, and also it and would, whereas DepBank only has
the relation between it and consider. In practice this means ignoring a number of the
subject dependencies output by the CCG parser, which is implemented by annotating
the relevant lexical categories plus argument slot in the markedup file with an ?ignore?
marker.
Another example where the dependencies differ in the two resources is the treat-
ment of relative pronouns. For example, in Sen. Mitchell, who had proposed the stream-
lining, the subject of proposed is Mitchell in CCGbank but who in DepBank. Again, we
implemented this change by fixing the head annotation in the lexical categories which
apply to relative pronouns.
In summary, considerable changes were required to the markedup file in order to
bring the dependency annotations of CCGbank and DepBank closer together. The major
types of changes have been described here, but not all the details.
11.2 Post-Processing of the GR Output
Despite the considerable changes made to the parser output described in the previous
section, there were still significant differences between the GRs created from the CCG
dependencies and the DepBank GRs. To obtain some idea of whether the schemes
were converging, we performed the following oracle experiment. We took the CCG
derivations from CCGbank corresponding to the sentences in DepBank, and ran the
parser over the gold-standard derivations, outputting the newly created GRs.9 Treating
the DepBank GRs as a gold standard, and comparing these with the CCGbank GRs,
8 The markedup file is the file containing the lexical categories, together with annotation which determines
dependency and head information, plus the CCG dependency to GR mapping. Appendix B shows part of
the markedup file.
9 All GRs involving a punctuation mark were removed because the RASP evaluation script can only handle
tokens which appear in the gold-standard GRs for the sentence.
535
Computational Linguistics Volume 33, Number 4
gave precision and recall scores of only 76.23% and 79.56%, respectively. Thus given the
current mapping, the perfect CCGbank parser would achieve an F-score of only 77.86%
when evaluated against DepBank.
On inspecting the output, it was clear that a number of general rules could be
applied to bring the schemes closer together, which we implemented as a Python post-
processing script. We now provide a description of some of the major changes, to give
an indication of the kinds of rules we implemented. We tried to keep the changes as
general as possible and not specific to the test set, although some rules, such as the
handling of monetary amounts, are genre-specific. We decided to include these rules
because they are trivial to implement and significantly affect the score, and we felt that,
without these changes, the CCG parser would be unfairly penalized.
The first set of changes deals with coordination. One significant difference between
DepBank and CCGbank is the treatment of coordinations as arguments. Consider the
example The president and chief executive officer said the loss stems from several factors. In
both CCGbank and DepBank there are two conj GRs arising from the coordination:
(conj and president) and (conj and officer).10 The difference arises in the subject
of said: in DepBank the subject is and: (ncsubj said and ), whereas in CCGbank there
are two subjects: (ncsubj said president ) and (ncsubj said officer ). We deal
with this problem by replacing any pairs of GRs which differ only in their arguments,
and where the arguments are coordinated items, with a single GR containing the coor-
dination term as the argument. Two arguments are coordinated if they appear in conj
relations with the same coordinating term, where ?same term? is determined by both
the word and sentence position.
Another source of conj errors is coordination terms acting as sentential modifiers,
with category S/S, often at the beginning of a sentence. These are labeled conj in
DepBank, but the GR for S/S is ncmod. So any ncmod whose modifier?s lexical category
is S/S, and whose POS tag is CC, is changed to conj.
Ampersands are also a significant problem, and occur frequently in WSJ text. For
example, the CCGbank analysis of Standard & Poor?s index assigns the lexical category
N/N to both Standard and &, treating them as modifiers of Poor, whereas DepBank treats
& as a coordinating term. We fixed this by creating conj GRs between any & and the two
words on either side; removing the modifier GR between the two words; and replacing
any GRs in which the words on either side of the & are arguments with a single GR in
which & is the argument.
The ta relation, which identifies text adjuncts delimited by punctuation (Briscoe
2006), is difficult to assign correctly to the parser output. The simple punctuation rules
used by the parser, and derived from CCGbank, do not contain enough information to
distinguish between the various cases of ta. Thus the only rule we have implemented,
which is somewhat specific to the newspaper genre, is to replace GRs of the form
(cmod say arg) with (ta quote arg say), where say can be any of say, said, or says.
This rule applies to only a small subset of the ta cases but has high enough precision to
be worthy of inclusion.
A common source of error is the distinction between iobj and ncmod, which is not
surprising given the difficulty that human annotators have in distinguishing arguments
and adjuncts. There are many cases where an argument in DepBank is an adjunct in
CCGbank, and vice versa. The only change we have made is to turn all ncmod GRs with
10 CCGbank does not contain GRs in this form, although we will continue to talk as though it does; these are
the GRs after the CCGbank dependencies have been put through the dependency to GRs mapping.
536
Clark and Curran Wide-Coverage Efficient Statistical Parsing
of as the modifier into iobj GRs (unless the ncmod is a partitive predeterminer). This was
found to have high precision and applies to a significant number of cases.
There are some dependencies in CCGbank which do not appear in DepBank. Exam-
ples include any dependencies in which a punctuation mark is one of the arguments,
and so we removed these from the output of the parser.
We have made some attempt to fill the subtype slot for some GRs. The subtype
slot specifies additional information about the GR; examples include the value obj in
a passive ncsubj, indicating that the subject is an underlying object; the value num in
ncmod, indicating a numerical quantity; and prt in ncmod to indicate a verb particle.
The passive case is identified as follows: Any lexical category which starts S[pss]\NP
indicates a passive verb, and we also mark any verbs POS tagged VBN and assigned
the lexical category N/N as passive. Both these rules have high precision, but still leave
many of the cases in DepBank unidentified. Many of those remaining are POS tagged
JJ and assigned the lexical category N/N, but this is also true of many non-passive
modifiers, so we did not attempt to extend these rules further. The numerical case is
identified using two rules: the num subtype is added if any argument in a GR is assigned
the lexical category N/N[num], and if any of the arguments in an ncmod is POS tagged
CD. prt is added to an ncmod if the modifiee has a POS tag beginning V and if the
modifier has POS tag RP.
We are not advocating that any of these post-processing rules should form part
of a parser. It would be preferable to have the required information in the treebank
from which the grammar is extracted, so that it could be integrated into the parser in a
principled way. However, in order that the parser evaluation be as fair and informative
as possible, it is important that the parser output conform as closely to the gold standard
as possible. Thus it is appropriate to use any general transformation rules, as long as
they are simple and not specific to the test set, to achieve this.
The final columns of Table 16 show the accuracy of the transformed gold-standard
CCGbank dependencies when compared with DepBank; the simple post-processing
rules have increased the F-score from 77.86% to 84.76%. However, note that this
F-score provides an upper bound on the performance of the CCG parser, and that this
score is still below the F-scores reported earlier when evaluating the parser output
against CCGbank. Section 11.4 contains more discussion of this issue.
11.3 Results
The results in Table 16 were obtained by parsing the sentences from CCGbank corre-
sponding to those in the 560-sentence test set used by Briscoe, Carroll, and Watson
(2006). We used the CCGbank sentences because these differ in some ways from the
original Penn Treebank sentences (there are no quotation marks in CCGbank, for ex-
ample) and the parser has been trained on CCGbank. Even here we experienced some
unexpected difficulties, because some of the tokenization is different between DepBank
and CCGbank (even though both resources are based on the Penn Treebank), and
there are some sentences in DepBank which have been significantly shortened (for no
apparent reason) compared to the original Penn Treebank sentences. We modified the
CCGbank sentences?and the CCGbank analyses because these were used for the oracle
experiments?to be as close to the DepBank sentences as possible. All the results were
obtained using the RASP evaluation scripts, with the results for the RASP parser taken
from Briscoe, Carroll, and Watson (2006). The results for CCGbank were obtained using
the oracle method described previously.
537
Computational Linguistics Volume 33, Number 4
Table 16
Accuracy on DepBank.
RASP CCG parser CCGbank
Relation Prec Rec F Prec Rec F Prec Rec F # GRs
dependent 79.76 77.49 78.61 84.07 82.19 83.12 88.83 84.19 86.44 10,696
aux 93.33 91.00 92.15 95.03 90.75 92.84 96.47 90.33 93.30 400
conj 72.39 72.27 72.33 79.02 75.97 77.46 83.07 80.27 81.65 595
ta 42.61 51.37 46.58 51.52 11.64 18.99 62.07 12.59 20.93 292
det 87.73 90.48 89.09 95.23 94.97 95.10 97.27 94.09 95.66 1,114
arg mod 79.18 75.47 77.28 81.46 81.76 81.61 86.75 84.19 85.45 8,295
mod 74.43 67.78 70.95 71.30 77.23 74.14 77.83 79.65 78.73 3,908
ncmod 75.72 69.94 72.72 73.36 78.96 76.05 78.88 80.64 79.75 3,550
xmod 53.21 46.63 49.70 42.67 53.93 47.64 56.54 60.67 58.54 178
cmod 45.95 30.36 36.56 51.34 57.14 54.08 64.77 69.09 66.86 168
pmod 30.77 33.33 32.00 0.00 0.00 0.00 0.00 0.00 0.00 12
arg 77.42 76.45 76.94 85.76 80.01 82.78 89.79 82.91 86.21 4,387
subj or dobj 82.36 74.51 78.24 86.08 83.08 84.56 91.01 85.29 88.06 3,127
subj 78.55 66.91 72.27 84.08 75.57 79.60 89.07 78.43 83.41 1,363
ncsubj 79.16 67.06 72.61 83.89 75.78 79.63 88.86 78.51 83.37 1,354
xsubj 33.33 28.57 30.77 0.00 0.00 0.00 50.00 28.57 36.36 7
csubj 12.50 50.00 20.00 0.00 0.00 0.00 0.00 0.00 0.00 2
comp 75.89 79.53 77.67 86.16 81.71 83.88 89.92 84.74 87.25 3,024
obj 79.49 79.42 79.46 86.30 83.08 84.66 90.42 85.52 87.90 2,328
dobj 83.63 79.08 81.29 87.01 88.44 87.71 92.11 90.32 91.21 1,764
obj2 23.08 30.00 26.09 68.42 65.00 66.67 66.67 60.00 63.16 20
iobj 70.77 76.10 73.34 83.22 65.63 73.38 83.59 69.81 76.08 544
clausal 60.98 74.40 67.02 77.67 72.47 74.98 80.35 77.54 78.92 672
xcomp 76.88 77.69 77.28 77.69 74.02 75.81 80.00 78.49 79.24 381
ccomp 46.44 69.42 55.55 77.27 70.10 73.51 80.81 76.31 78.49 291
pcomp 72.73 66.67 69.57 0.00 0.00 0.00 0.00 0.00 0.00 24
macroaverage 62.12 63.77 62.94 65.71 62.29 63.95 71.73 65.85 68.67
microaverage 77.66 74.98 76.29 81.95 80.35 81.14 86.86 82.75 84.76
The CCG parser results are based on automatically assigned POS tags, using the
Curran and Clark (2003) tagger. For the parser we used the hybrid dependency
model and the maximum recall decoder, because this obtained the highest accuracy on
CCGbank, with the same parser and supertagger parameter settings as described in
Section 10.2.11 The coverage of the parser on DepBank is 100%. The coverage of the
RASP parser is also 100%: 84% of the analyses are complete parses rooted in S and the
rest are obtained using a robustness technique based on fragmentary analyses (Briscoe
and Carroll 2006). The coverage for the oracle experiments is less than 100% (around
95%) since there are some gold-standard derivations in CCGbank which the parser is
unable to follow exactly, because the grammar rules used by the parser are a subset
of those in CCGbank. The oracle figures are based only on those sentences for which
there is a gold-standard analysis, because we wanted to measure how close the two
resources are and provide an approximate upper bound for the parser. (But, to repeat,
the accuracy figures for the parser are based on the complete test set.)
11 The results reported in Clark and Curran (2007) differ from those here because Clark and Curran used
the normal-form model and Viterbi decoder.
538
Clark and Curran Wide-Coverage Efficient Statistical Parsing
F-score is the balanced harmonic mean of precision (P) and recall (R): 2PR/(P + R).
# GRs is the number of GRs in DepBank. For a GR in the parser output to be correct,
it has to match the gold-standard GR exactly, including any subtype slots; however, it
is possible for a GR to be incorrect at one level but correct at a subsuming level. For
example, if an ncmod GR is incorrectly labeled with xmod, but is otherwise correct, it will
be correct for all levels which subsume both ncmod and xmod, for example mod. Thus the
scores at the most general level in the GR hierarchy (dependent) correspond to unlabeled
accuracy scores. The micro-averaged scores are calculated by aggregating the counts for
all the relations in the hierarchy, whereas the macro-averaged scores are the mean of the
individual scores for each relation (Briscoe, Carroll, and Watson 2006).
The results show that the performance of the CCG parser is higher than RASP
overall, and also higher on the majority of GR types. Relations on which the CCG parser
performs particularly well, relative to RASP, are conj, det, ncmod, cmod, ncsubj, dobj,
obj2, and ccomp. The relations for which the CCG parser performs poorly are some of
the less frequent relations: ta, pmod, xsubj, csubj, and pcomp; in fact pmod and pcomp are
not in the current CCG dependencies to GRs mapping. The overall F-score for the CCG
parser, 81.14%, is only 3.6 points below that for CCGbank, which provides an upper
bound for the CCG parser.
Briscoe and Carroll (2006) give a rough comparison of RASP with the Parc LFG parser
(Kaplan et al 2004) on DepBank, obtaining similar results overall, but acknowledging
that the results are not strictly comparable because of the different annotation schemes
used.
11.4 Discussion
We might expect the CCG parser to perform better than RASP on this data because RASP
is not tuned to newspaper text and uses an unlexicalized parsing model. On the other
hand the relatively low upper bound for the CCG parser on DepBank demonstrates the
considerable disadvantage of evaluating on a resource which uses a different annotation
scheme to the parser. Our feeling is that the overall F-score on DepBank understates the
accuracy of the CCG parser, because of the information lost in the translation.
One aspect of the CCGbank evaluation which is more demanding than the DepBank
evaluation is the set of labeled dependencies used. In CCGbank there are many more
labeled dependencies than GRs in DepBank, because a dependency is defined as a lexical
category-argument slot pair. In CCGbank there is a distinction between the direct object
of a transitive verb and ditransitive verb, for example, whereas in DepBank these would
both be dobj. In other words, to get a dependency correct in the CCGbank evaluation,
the lexical category?typically a subcategorization frame?has to be correct. In a final
experiment we used the GRs generated by transforming CCGbank as a gold standard,
against which we compared the GRs from the transformed parser output. The resulting
F-score of 89.60% shows the increase obtained from using gold-standard GRs generated
from CCGbank rather than the CCGbank dependencies themselves (for which the
F-score was 85.20%).
Another difference between DepBank and CCGbank is that DepBank has been man-
ually corrected, whereas CCGbank, including the test sections, has been produced semi-
automatically from the Penn Treebank. There are some constructions in CCGbank?
noun compounds being a prominent example?which are often incorrectly analyzed,
simply because the required information is not in the Penn Treebank. Thus the evalua-
tion on CCGbank overstates the accuracy of the parser, because it is tuned to produce
539
Computational Linguistics Volume 33, Number 4
the output in CCGbank, including constructions where the analysis is incorrect. A
similar comment would apply to other parsers evaluated on, and using grammars
extracted from, the Penn Treebank.
A contribution of this section has been to highlight the difficulties associated with
cross-formalism parser comparisons. Note that the difficulties are not unique to CCG,
and many would apply to any cross-formalism comparison, especially with parsers
using automatically extracted grammars. Parser evaluation has improved on the origi-
nal PARSEVAL measures (Carroll, Briscoe, and Sanfilippo 1998), but the challenge still
remains to develop a representation and evaluation suite which can be easily applied to
a wide variety of parsers and formalisms.
12. Future Work
One of the key questions currently facing researchers in statistical parsing is how to
adapt existing parsers to new domains. There is some experimental evidence showing
that, perhaps not surprisingly, the performance of parsers trained on the WSJ Penn
Treebank drops significantly when the parser is applied to domains outside of news-
paper text (Gildea 2001; Lease and Charniak 2005). The difficulty is that developing
new treebanks for each of these domains is infeasible. Developing the techniques to
extract a CCG grammar from the Penn Treebank, together with the preprocessing of the
Penn Treebank which was required, took a number of years; and developing the Penn
Treebank itself also took a number of years.
Clark, Steedman, and Curran (2004) applied the parser described in this article to
questions from the TREC Question Answering (QA) track. Because of the small number
of questions in the Penn Treebank, the performance of the parser was extremely poor?
well below that required for a working QA system. The novel idea in Clark, Steedman,
and Curran was to create new training data from questions, but to annotate at the lexical
category level only, rather than annotate with full derivations. The idea is that, because
lexical categories contain so much syntactic information, adapting just the supertagger
to the new domain, by training on the new question data, may be enough to obtain good
parsing performance. This technique assumes that annotation at the lexical category
level can be done relatively quickly, allowing rapid porting of the supertagger. We were
able to annotate approximately 1, 000 questions in around a week, which led to an
accurate supertagger and, combined with the Penn Treebank parsing model, an accurate
parser of questions.
There are ways in which this porting technique can be extended. For example, we
have developed a method for training the dependency model which requires lexical
category data only (Clark and Curran 2006). Partial dependency structures are extracted
from the lexical category sequences, and the training algorithm for the dependency
model is extended to deal with partial data. Remarkably, the accuracy of the depen-
dency model trained on data derived from lexical category sequences alone is only 1.3%
labeled F-score less than the full data model. This result demonstrates the significant
amount of syntactic information encoded in the lexical categories. Future work will look
at applying this method to biomedical text.
We have shown how using automatically assigned POS tags reduces the accuracy
of the supertagger and parser. In Curran, Clark, and Vadas (2006) we investigate using
the multi-tagging techniques developed for the supertagger at the POS tag level. The
idea is to maintain some POS tag ambiguity for later parts of the parsing process, using
the tag probabilities to decide which tags to maintain. We were able to reduce the drop
540
Clark and Curran Wide-Coverage Efficient Statistical Parsing
in supertagger accuracy by roughly one half. Future work will also look at maintaing
the POS tag ambiguity through to the parsing stage.
Currently we do not use the probabilities assigned to the lexical categories by the
supertagger as part of the parse selection process. These scores could be incorporated
as real-valued features, or as auxiliary functions, as in Johnson and Riezler (2000).
We would also like to investigate using the generative model of Hockenmaier and
Steedman (2002b) in a similar way. Using a generative model?s score as a feature in
a discriminative framework has been beneficial for reranking approaches (Collins and
Koo 2005). Because the generative model uses local features similar to those in our
log-linear models, it could be incorporated into the estimation and decoding processes
without the need for reranking.
One way of improving the accuracy of a supertagger is to use the parser to provide
large amounts of additional training data, by taking the lexical categories chosen by the
parser as gold-standard training data. If enough unlabeled data is parsed, then the large
volume can overcome the noise in the data (Steedman et al 2002; Prins and van Noord
2003). We plan to investigate this idea in the context of our own parsing system.
13. Conclusion
This article has shown how to estimate a log-linear parsing model for an automat-
ically extracted CCG grammar, on a very large scale. The techniques that we have
developed, including the use of a supertagger to limit the size of the charts and the
use of parallel estimation, could be applied to log-linear parsing models using other
grammar formalisms. Despite memory requirements of up to 25 GB we have shown
how a parallelized version of the estimation process can limit the estimation time to
under three hours, resulting in a practical framework for parser development. One of
the problems with modeling approaches which require very long estimation times is
that it is difficult to test different configurations of the system, for example different
feature sets. It may also not be possible to train or run the system on anything other
than short sentences (Taskar et al 2004).
The supertagger is a key component in our parsing system. It reduces the size of
the charts considerably compared with naive methods for assigning lexical categories,
which is crucial for practical discriminative training. The tight integration of the su-
pertagger and parser enables highly efficient as well as accurate parsing. The parser is
significantly faster than comparable parsers in the NLP literature. The supertagger we
have developed can be applied to other lexicalized grammar formalisms.
Another contribution of the article is the development of log-linear parsing models
for CCG. In particular, we have shown how to define a CCG parsing model which
exploits all derivations, including nonstandard derivations. These nonstandard deriva-
tions are an integral part of the formalism, and we have answered the question of
whether efficent estimation and parsing algorithms can be defined for models which
use these derivations. We have also defined a new parsing algorithm for CCG which
maximizes expected recall of predicate?argument dependencies. This algorithm, when
combined with normal-form constraints, gives the highest parsing accuracy to date on
CCGbank. We have also given competitive results on DepBank, outperforming a non-
CCG parser (RASP), despite the considerable difficulties involved in evaluating on a gold
standard which uses a different annotation scheme to the parser.
There has perhaps been a perception in the NLP community that parsing with
CCG is necessarily ineffficient because of CCG?s ?spurious? ambiguity. We have
541
Computational Linguistics Volume 33, Number 4
demonstrated, using state-of-the-art statistical models, that both accurate and highly
efficient parsing is practical with CCG. Linguistically motivated grammars can now be
used for large-scale NLP applications.12
Appendix A
The following rules were selected primarily on the basis of frequency of occurrence in
Sections 02?21 of CCGbank.
Type-Raising Categories for NP, PP and S[adj]\NP
S/(S\NP)
(S\NP)\((S\NP)/NP)
((S\NP)/NP)\(((S\NP)/NP)/NP)
((S\NP)/(S[to]\NP))\(((S\NP)/(S[to]\NP))/NP)
((S\NP)/PP)\(((S\NP)/PP)/NP)
((S\NP)/(S[adj]\NP))\(((S\NP)/(S[adj]\NP))/NP)
(S\NP)\((S\NP)/PP)
(S\NP)\((S\NP)/(S[adj]\NP))
Unary Type-Changing Rules
The category on the left of the rule is rewritten bottom-up as the category on the right.
N ? NP
NP ? S/(S/NP)
NP ? NP/(NP\NP)
S[dcl]\NP ? NP\NP
S[pss]\NP ? NP\NP
S[ng]\NP ? NP\NP
S[adj]\NP ? NP\NP
S[to]\NP ? NP\NP
(S[to]\NP)/NP ? NP\NP
S[dcl]/NP ? NP\NP
S[dcl] ? NP\NP
S[pss]\NP ? (S\NP)\(S\NP)
12 The parser and supertagger, including source code, are freely available via the authors? Web pages.
542
Clark and Curran Wide-Coverage Efficient Statistical Parsing
S[ng]\NP ? (S\NP)\(S\NP)
S[adj]\NP ? (S\NP)\(S\NP)
S[to]\NP ? (S\NP)\(S\NP)
S[ng]\NP ? (S\NP)/(S\NP)
S[pss]\NP ? S/S
S[ng]\NP ? S/S
S[adj]\NP ? S/S
S[to]\NP ? S/S
S[ng]\NP ? S\S
S[dcl] ? S\S
S[ng]\NP ? NP
S[to]\NP ? N\N
Punctuation Rules
A number of categories absorb a comma to the left, implementing the following schema:
, X ? X
The categories are as follows, where S[?] matches an S category with any or no feature:
N, NP, S[?], N/N, NP\NP, PP\PP, S/S, S\S, S[?]\NP, (S\NP)\(S\NP),
(S\NP)/(S\NP), ((S\NP)\(S\NP))\((S\NP)\(S\NP))
Similarly, a number of categories absorb a comma to the right, implementing the fol-
lowing schema:
X , ? X
The categories are as follows:
N, NP, PP, S[dcl], N/N, NP\NP, S/S, S\S, S[?]\NP, (S[dcl]\NP)/S,
(S[dcl]\S[dcl])\NP, (S[dcl]\NP)/NP, (S[dcl]\NP)/PP, (NP\NP)/(S[dcl]\NP),
(S\NP)\(S\NP), (S\NP)/(S\NP)
These are the categories which absorb a colon or semicolon to the left, in the same way
as for the comma:
N, NP, S[dcl], NP\NP, S[?]\NP, (S\NP)\(S\NP)
These are the categories which absorb a colon or semicolon to the right:
N, NP, PP, S[dcl], NP\NP, S/S, S[?]\NP, (S[dcl]\NP)/S[dcl], (S\NP)\(S\NP),
(S\NP)/(S\NP)
543
Computational Linguistics Volume 33, Number 4
These are the categories which absorb a period to the right:
N, NP, S[?], PP, NP\NP, S\S, S[?]\NP, S[?]\PP, (S[dcl]\S[?])\NP, (S\NP)\(S\NP)
These are the categories which absorb a round bracket to the left:
N, NP, S[dcl], NP\NP, (S\NP)\(S\NP)
These are the categories which absorb a round bracket to the right:
N, NP, S[dcl], N\N, N/N, NP\NP, S[dcl]\NP, S/S, S\S, (N/N)\(N/N),
(S\NP)\(S\NP), (S\NP)/(S\NP)
There are some binary type-changing rules involving commas, where the two categories
on the left are rewritten bottom-up as the category on the right:
, NP ? (S\NP)\(S\NP)
NP , ? S/S
S[dcl]/S[dcl] , ? S/S
S[dcl]/S[dcl] , ? (S\NP)\(S\NP)
S[dcl]/S[dcl] , ? (S\NP)/(S\NP)
S[dcl]/S[dcl] , ? S\S
S[dcl]\S[dcl] , ? S/S
Finally, there is a comma coordination rule, and a semicolon coordination rule, repre-
sented by the following two schema:
, X ? X\X
; X ? X\X
The categories which instantiate the comma schema are as follows:
N, NP, S[?], N/N, NP\NP, S[?]\NP, (S\NP)\(S\NP)
The categories which instantiate the semicolon schema are as follows:
NP, S[?], S[?]\NP
544
Clark and Curran Wide-Coverage Efficient Statistical Parsing
Other Rules
There are two rules for combining sequences of noun phrases and sequences of declar-
ative sentences:
NP NP ? NP
S[dcl] S[dcl] ? S[dcl]
Finally, there are some coordination constructions in the original Penn Treebank which
were difficult to convert into CCGbank analyses, for which the following rule is used:
conj N ? N
Appendix B
The annotation in the markedup file for some of the most frequent categories in CCG-
bank is shown in Section 11.1. The annotation provides information about heads and
dependencies, and also the mapping from CCG dependencies to the GRs in DepBank.
The first line after the unmarked lexical category gives the number of dependency
relations plus the category annotated with head and dependency information. Variables
in curly brackets indicate heads, with ? ? used to denote the word associated with the
lexical category. For example, if the word buys is assigned the transitive verb category
((S[dcl]{ }\NP{Y}?1?){ }/NP{Z}?2?){ }, then the head on the resulting S[dcl] is buys.
Co-indexing of variables allows head passing; for example, in the relative pronoun cat-
egory ((NP{Y}\NP{Y}?1?){ }/(S[dcl]{Z}?2?\NP{Y?}){Z}){ }, the head of the resulting
NP is taken from the NP which is modified to the left, and this head also becomes the
subject of the verb phrase to the right. So in the man who owns the company, the subject of
owns is man.
Numbers in angled brackets indicate dependency relations. For example, in the
nominal modifier category (N{Y}/N{Y}?1?){ }, there is one dependency between the
modifier and the modifiee. Long-range dependencies are indicated by marking head
variables with ?. The ? in the relative pronoun category indicates that when the Y vari-
able unifies with a lexical item, this creates a long-range subject dependency.
Some categories have a second head and dependency annotation, indicated with a
!. This is used to produce the DepBank GRs. For example, the relative pronoun category
has a second annotation which results in who being the subject of owns in the man who
owns the company, rather than man, because this is consistent with DepBank. The first
annotation is consistent with CCGbank.
The remaining lines in a category entry give the CCG dependencies to GRs mapping,
described in Section 11.1.
N/N
1 (N{Y}/N{Y}<1>){_}
1 ncmod _ %f %l
NP[nb]/N
1 (NP[nb]{Y}/N{Y}<1>){_}
1 det %f %l
545
Computational Linguistics Volume 33, Number 4
(NP\NP)/NP
2 ((NP{Y}\NP{Y}<1>){_}/NP{Z}<2>){_}
1 ncmod _ %f %l
2 dobj %l %f
((S\NP)\(S\NP))/NP
2 (((S[X]{Y}\NP{Z}){Y}\(S[X]{Y}<1>\NP{Z}){Y}){_}/NP{W}<2>){_}
1 ncmod _ %f %l
2 dobj %l %f
PP/NP
1 (PP{_}/NP{Y}<1>){_}
1 dobj %l %f
(S[dcl]\NP)/NP
2 ((S[dcl]{_}\NP{Y}<1>){_}/NP{Z}<2>){_}
1 ncsubj %l %f _
2 xcomp _ %l %f =be
2 dobj %l %f
(S\NP)\(S\NP)
1 ((S[X]{Y}\NP{Z}){Y}\(S[X]{Y}<1>\NP{Z}){Y}){_}
1 ncmod _ %f %l
(S[b]\NP)/NP
2 ((S[b]{_}\NP{Y}<1>){_}/NP{Z}<2>){_}
1 ncsubj %l %f _
2 xcomp _ %l %f =be
2 dobj %l %f
(S[to]\NP)/(S[b]\NP)
2 ((S[to]{_}\NP{Z}<1>){_}/(S[b]{Y}<2>\NP{Z*}){Y}){_}
1 ignore
2 ignore
(S[dcl]\NP)/(S[b]\NP)
2 ((S[dcl]{_}\NP{Y}<1>){_}/(S[b]{Z}<2>\NP{Y*}){Z}){_}
! ((S[dcl]{Z}\NP{Y}<1>){Z}/(S[b]{Z}<2>\NP{Y*}){Z}){_}
1 ignore =aux
1 ncsubj %l %f _
2 aux %f %l =aux
2 xcomp _ %l %f
(NP[nb]/N)\NP
2 ((NP[nb]{Y}/N{Y}<1>){_}\NP{Z}<2>){_}
1 ncmod poss %f %2
2 ignore
S[adj]\NP
1 (S[adj]{_}\NP{Y}<1>){_}
1 ignore
S[pss]\NP
1 (S[pss]{_}\NP{Y}<1>){_}
1 ncsubj %l %f obj
(N/N)/(N/N)
1 ((N{Y}/N{Y}){Z}/(N{Y}/N{Y}){Z}<1>){_}
546
Clark and Curran Wide-Coverage Efficient Statistical Parsing
1 ncmod _ %f %l
(S[ng]\NP)/NP
2 ((S[ng]{_}\NP{Y}<1>){_}/NP{Z}<2>){_}
1 ncsubj %l %f _
2 dobj %l %f
(S\NP)/(S\NP)
1 ((S[X]{Y}\NP{Z}){Y}/(S[X]{Y}<1>\NP{Z}){Y}){_}
1 ncmod _ %f %l
(S[dcl]\NP)/S[dcl]
2 ((S[dcl]{_}\NP{Y}<1>){_}/S[dcl]{Z}<2>){_}
1 ncsubj %l %f _
2 ccomp _ %l %f
S[dcl]\NP
1 (S[dcl]{_}\NP{Y}<1>){_}
1 ncsubj %l %f _
(S[dcl]\NP)/(S[pt]\NP)
2 ((S[dcl]{_}\NP{Y}<1>){_}/(S[pt]{Z}<2>\NP{Y*}){Z}){_}
! ((S[dcl]{Z}\NP{Y}<1>){Z}/(S[pt]{Z}<2>\NP{Y*}){Z}){_}
1 ignore =aux
1 ncsubj %l %f _
2 aux %f %l =aux
2 xcomp _ %l %f
S/S
1 (S[X]{Y}/S[X]{Y}<1>){_}
1 ncmod _ %f %l
(NP\NP)/(S[dcl]\NP)
2 ((NP{Y}\NP{Y}<1>){_}/(S[dcl]{Z}<2>\NP{Y*}){Z}){_}
! ((NP{Y}\NP{Y}<1>){_}/(S[dcl]{Z}<2>\NP{_}){Z}){_}
1 cmod %l %f %2
2 ignore
Acknowledgments
Much of this work was carried out at
the University of Edinburgh?s School of
Informatics as part of Mark Steedman?s
EPSRC grant GR/M96889. We are immensely
grateful to Mark for his guidance and advice
during that time, and for allowing us the
freedom to pursue the approach taken in
this article. We are also grateful to Julia
Hockenmaier for the use of CCGbank, the
resource which made this work possible.
Many of the ideas in this article emerged out
of discussions with Mark and Julia. Jason
Baldridge, Johan Bos, David Chiang, Claire
Grover, Frank Keller, Yuval Krymolowski,
Alex Lascarides, Mirella Lapata, Yusuke
Miyao, Miles Osborne, Stefan Riezler, and
Bonnie Webber have all given useful
feedback. Thanks to Ted Briscoe, Rebecca
Watson, and Stephen Pulman for help
with the GRs, and thanks to Rebecca for
carrying out the GR evaluation. We have
also benefited from reviewers? comments
on this article and various conference
papers related to this work, and from the
feedback of audiences at the universities of
Cambridge, Edinburgh, Johns Hopkins,
Oxford, Sheffield, and Sussex. While at
Edinburgh, James Curran was funded
by a Commonwealth scholarship and a
University of Sydney traveling scholarship.
He is currently supported by the Australian
Research Council under Discovery Project
DP0453131.
References
Abney, Steven. 1997. Stochastic attribute-
value grammars. Computational Linguistics,
23(4):597?618.
Baldridge, Jason. 2002. Lexically Specified
Derivational Control in Combinatory
Categorial Grammar. Ph.D. thesis,
Edinburgh University, Edinburgh, UK.
547
Computational Linguistics Volume 33, Number 4
Baldridge, Jason and Geert-Jan Kruijff. 2003.
Multi-modal Combinatory Categorial
Grammar. In Proceedings of the 10th Meeting
of the EACL, pages 211?218, Budapest,
Hungary.
Bangalore, Srinivas and Aravind Joshi. 1999.
Supertagging: An approach to almost parsing.
Computational Linguistics, 25(2):237?265.
Bar-Hillel, Yehoshua. 1953. A
quasi-arithmetical notation for syntactic
description. Language, 29:47?58.
Borthwick, Andrew. 1999. A Maximum
Entropy Approach to Named Entity
Recognition. Ph.D. thesis, New York
University, New York.
Bos, Johan, Stephen Clark, Mark Steedman,
James R. Curran, and Julia Hockenmaier.
2004. Wide-coverage semantic
representations from a CCG parser.
In Proceedings of COLING-04,
pages 1240?1246, Geneva, Switzerland.
Briscoe, Ted. 2006. An introduction to tag
sequence grammars and the RASP
system parser. Technical Report
UCAM-CL-TR-662, University of
Cambridge Computer Laboratory.
Briscoe, Ted and John Carroll. 2002. Robust
accurate statistical annotation of general
text. In Proceedings of the 3rd LREC
Conference, pages 1499?1504, Las Palmas,
Gran Canaria.
Briscoe, Ted and John Carroll. 2006.
Evaluating the accuracy of an
unlexicalized statistical parser on the
PARC DepBank. In Proceedings of the
Poster Session of the Joint Conference
of the International Committee on
Computational Linguistics and the
Association for Computational Linguistics
(COLING/ACL-06), Sydney, Australia.
Briscoe, Ted, John Carroll, and Rebecca
Watson. 2006. The second release of
the RASP system. In Proceedings of
the Interactive Demo Session of the
Joint Conference of the International
Committee on Computational Linguistics
and the Association for Computational
Linguistics (COLING/ACL-06), pages 77?80,
Sydney, Australia.
Buchholz, Sabine, Jorn Veenstra, and Walter
Daelemans. 1999. Cascaded grammatical
relation assignment. In Proceedings of
EMNLP/VLC-99, pages 239?246,
University of Maryland, College Park, MD.
Burke, Michael, Aoife Cahill, Ruth
O?Donovan, Josef van Genabith, and
Andy Way. 2004. Large-scale induction
and evaluation of lexical resources from
the Penn-II Treebank. In Proceedings of the
42nd Meeting of the ACL, pages 367?374,
Barcelona, Spain.
Cahill, Aoife, Michael Burke, Ruth
O?Donovan, Josef van Genabith,
and Andy Way. 2004. Long-distance
dependency resolution in automatically
acquired wide-coverage PCFG-based LFG
approximations. In Proceedings of the 42nd
Meeting of the ACL, pages 320?327,
Barcelona, Spain.
Carroll, John, Ted Briscoe, and Antonio
Sanfilippo. 1998. Parser evaluation: A
survey and a new proposal. In Proceedings
of the 1st LREC Conference, pages 447?454,
Granada, Spain.
Charniak, Eugene. 1997. Statistical parsing
with a context-free grammar and word
statistics. In Proceedings of the 14th National
Conference on Artificial Intelligence,
pages 598?603, Menlo Park, CA.
Charniak, Eugene. 2000. A
maximum-entropy-inspired parser. In
Proceedings of the 1st Meeting of the NAACL,
pages 132?139, Seattle, WA.
Chen, John, Srinivas Bangalore, Michael
Collins, and Owen Rambow. 2002.
Reranking an N-gram supertagger.
In Proceedings of the TAG+ Workshop,
pages 259?268, Venice, Italy.
Chen, John and K. Vijay-Shanker. 2000.
Automated extraction of TAGS from the
Penn Treebank. In Proceedings of IWPT
2000, Trento, Italy.
Chen, Stanley and Ronald Rosenfeld. 1999.
A Gaussian prior for smoothing maximum
entropy models. Technical Report
CMU-CS-99-108, Carnegie Mellon
University, Pittsburgh, PA.
Chiang, David. 2000. Statistical parsing
with an automatically-extracted Tree
Adjoining Grammar. In Proceedings of the
38th Meeting of the ACL, pages 456?463,
Hong Kong.
Chiang, David. 2003. Mildly context
sensitive grammars for estimating
maximum entropy models. In Proceedings
of the 8th Conference on Formal Grammar,
Vienna, Austria.
Clark, Stephen. 2002. A supertagger for
Combinatory Categorial Grammar.
In Proceedings of the TAG+ Workshop,
pages 19?24, Venice, Italy.
Clark, Stephen and James R. Curran. 2003.
Log-linear models for wide-coverage CCG
parsing. In Proceedings of the EMNLP
Conference, pages 97?104, Sapporo, Japan.
Clark, Stephen and James R. Curran.
2004a. The importance of supertagging
for wide-coverage CCG parsing. In
548
Clark and Curran Wide-Coverage Efficient Statistical Parsing
Proceedings of COLING-04, pages 282?288,
Geneva, Switzerland.
Clark, Stephen and James R. Curran. 2004b.
Parsing the WSJ using CCG and log-linear
models. In Proceedings of the 42nd Meeting of
the ACL, pages 104?111, Barcelona, Spain.
Clark, Stephen and James R. Curran. 2006.
Partial training for a lexicalized-grammar
parser. In Proceedings of the Human
Language Technology Conference and
the Annual Meeting of the North
American Chapter of the Association for
Computational Linguistics (HLT-NAACL?06),
pages 144?151, New York.
Clark, Stephen and James R. Curran. 2007.
Formalism-independent parser evaluation
with CCG and DepBank. In Proceedings
of the 45th Meeting of the ACL, Prague,
Czech Republic.
Clark, Stephen, Julia Hockenmaier,
and Mark Steedman. 2002. Building
deep dependency structures with a
wide-coverage CCG parser. In Proceedings
of the 40th Meeting of the ACL,
pages 327?334, Philadelphia, PA.
Clark, Stephen, Mark Steedman, and
James R. Curran. 2004. Object-extraction
and question-parsing using CCG. In
Proceedings of the EMNLP Conference,
pages 111?118, Barcelona, Spain.
Collins, Michael. 1996. A new statistical
parser based on bigram lexical
dependencies. In Proceedings of the
34th Meeting of the ACL, pages 184?191,
Santa Cruz, CA.
Collins, Michael. 2003. Head-driven
statistical models for natural language
parsing. Computational Linguistics,
29(4):589?637.
Collins, Michael and James Brooks. 1995.
Prepositional phrase attachment through
a backed-off model. In Proceedings of the
3rd Workshop on Very Large Corpora,
pages 27?38, Cambridge, MA.
Collins, Michael and Terry Koo. 2005.
Discriminative reranking for natural
language parsing. Computational
Linguistics, 31(1):25?69.
Collins, Michael and Brian Roark. 2004.
Incremental parsing with the perceptron
algorithm. In Proceedings of the 42nd
Meeting of the ACL, pages 111?118,
Barcelona, Spain.
Crouch, Richard, Ronald M. Kaplan,
Tracy H. King, and Stefan Riezler.
2002. A comparison of evaluation
metrics for a broad-coverage stochastic
parser. In Proceedings of the LREC Beyond
PARSEVAL workshop, pages 67?74,
Las Palmas, Spain.
Curran, James R. and Stephen Clark.
2003. Investigating GIS and smoothing
for maximum entropy taggers. In
Proceedings of the 10th Meeting of the
EACL, pages 91?98, Budapest, Hungary.
Curran, James R., Stephen Clark, and
David Vadas. 2006. Multi-tagging for
lexicalized-grammar parsing. In
Proceedings of the Joint Conference
of the International Committee on
Computational Linguistics and the
Association for Computational Linguistics
(COLING/ACL-06), pages 697?704,
Sydney, Australia.
Curry, Haskell B. and Robert Feys. 1958.
Combinatory Logic: Vol. I. Amsterdam,
The Netherlands.
Darroch, J. N. and D. Ratcliff. 1972.
Generalized iterative scaling for log-linear
models. The Annals of Mathematical
Statistics, 43(5):1470?1480.
Della Pietra, Stephen, Vincent Della Pietra,
and John Lafferty. 1997. Inducing features
of random fields. IEEE Transactions Pattern
Analysis and Machine Intelligence,
19(4):380?393.
Dienes, Peter and Amit Dubey. 2003. Deep
syntactic processing by combining shallow
methods. In Proceedings of the 41st Meeting
of the ACL, pages 431?438, Sapporo, Japan.
Eisner, Jason. 1996. Efficient normal-form
parsing for Combinatory Categorial
Grammar. In Proceedings of the 34th Meeting
of the ACL, pages 79?86, Santa Cruz, CA.
Geman, Stuart and Mark Johnson. 2002.
Dynamic programming for parsing and
estimation of stochastic unification-based
grammars. In Proceedings of the 40th
Meeting of the ACL, pages 279?286,
Philadelphia, PA.
Gildea, Daniel. 2001. Corpus variation and
parser performance. In 2001 Conference on
Empirical Methods in Natural Language
Processing (EMNLP), pages 167?172,
Pittsburgh, PA.
Goodman, Joshua. 1996. Parsing algorithms
and metrics. In Proceedings of the 34th
Meeting of the ACL, pages 177?183,
Santa Cruz, CA.
Goodman, Joshua. 1997. Probabilistic
feature grammars. In Proceedings of the
International Workshop on Parsing
Technologies, Cambridge, MA.
Gropp, W., E. Lusk, N. Doss, and
A. Skjellum. 1996. A high-performance,
portable implementation of the MPI
message passing interface standard.
Parallel Computing, 22(6):789?828.
549
Computational Linguistics Volume 33, Number 4
Hockenmaier, Julia. 2003a. Data and Models
for Statistical Parsing with Combinatory
Categorial Grammar. Ph.D. thesis,
University of Edinburgh, Edinburgh, UK.
Hockenmaier, Julia. 2003b. Parsing with
generative models of predicate-argument
structure. In Proceedings of the 41st
Meeting of the ACL, pages 359?366,
Sapporo, Japan.
Hockenmaier, Julia and Mark Steedman.
2002a. Acquiring compact lexicalized
grammars from a cleaner treebank. In
Proceedings of the Third LREC Conference,
pages 1974?1981, Las Palmas, Spain.
Hockenmaier, Julia and Mark Steedman.
2002b. Generative models for statistical
parsing with Combinatory Categorial
Grammar. In Proceedings of the
40th Meeting of the ACL, pages 335?342,
Philadelphia, PA.
Johnson, Mark. 1998. PCFG models of
linguistic tree representations.
Computational Linguistics, 24(4):613?632.
Johnson, Mark. 2002. A simple
pattern-matching algorithm for recovering
empty nodes and their antecedents. In
Proceedings of the 40th Meeting of the ACL,
pages 136?143, Philadelphia, PA.
Johnson, Mark, Stuart Geman, Stephen
Canon, Zhiyi Chi, and Stefan Riezler.
1999. Estimators for stochastic
?unification-based? grammars. In
Proceedings of the 37th Meeting of the
ACL, pages 535?541, University of
Maryland, College Park, MD.
Johnson, Mark and Stefan Riezler. 2000.
Exploiting auxiliary distributions in
stochastic unification-based grammars.
In Proceedings of the 1st Meeting of the
NAACL, Seattle, WA.
Kaplan, Ron, Stefan Riezler, Tracy H. King,
John T. Maxwell, III, Alexander
Vasserman, and Richard Crouch. 2004.
Speed and accuracy in shallow and deep
stochastic parsing. In Proceedings of the
Human Language Technology Conference and
the 4th Meeting of the North American
Chapter of the Association for Computational
Linguistics (HLT-NAACL?04), pages 97?104,
Boston, MA.
Kasami, J. 1965. An efficient recognition
and syntax analysis algorithm for
context-free languages. Technical
Report AFCRL-65-758, Air Force
Cambridge Research Laboratory,
Bedford, MA.
King, Tracy H., Richard Crouch, Stefan
Riezler, Mary Dalrymple, and Ronald M.
Kaplan. 2003. The PARC 700 Dependency
Bank. In Proceedings of the 4th International
Workshop on Linguistically Interpreted
Corpora, Budapest, Hungary.
Koeling, Rob. 2000. Chunking with
maximum entropy models. In Proceedings
of the CoNLL Workshop 2000, pages 139?141,
Lisbon, Portugal.
Lafferty, John, Andrew McCallum, and
Fernando Pereira. 2001. Conditional
random fields: Probabilistic models for
segmenting and labeling sequence data.
In Proceedings of the 18th International
Conference on Machine Learning,
pages 282?289, Williams College,
Williamstown, MA.
Lari, K. and S. J. Young. 1990. The estimation
of stochastic context-free grammars using
the inside-outside algorithm. Computer
Speech and Language, 4(1):35?56.
Lease, Matthew and Eugene Charniak.
2005. Parsing biomedical literature. In
Proceedings of the Second International Joint
Conference on Natural Language Processing
(IJCNLP-05), Jeju Island, Korea.
Levy, Roger and Christopher Manning.
2004. Deep dependencies from
context-free statistical parsers: Correcting
the surface dependency approximation.
In Proceedings of the 41st Meeting of the
ACL, pages 328?335, Barcelona, Spain.
Malouf, Robert. 2002. A comparison of
algorithms for maximum entropy
parameter estimation. In Proceedings
of the Sixth Workshop on Natural
Language Learning, pages 49?55,
Taipei, Taiwan.
Malouf, Robert and Gertjan van Noord.
2004. Wide coverage parsing with
stochastic attribute value grammars.
In Proceedings of the IJCNLP-04 Workshop:
Beyond Shallow Analyses?Formalisms and
Statistical Modeling for Deep Analyses,
Hainan Island, China.
Marcus, Mitchell, Beatrice Santorini, and
Mary Marcinkiewicz. 1993. Building a
large annotated corpus of English: The
Penn Treebank. Computational Linguistics,
19(2):313?330.
Miyao, Yusuke, Takashi Ninomiya, and
Jun?ichi Tsujii. 2004. Corpus-oriented
grammar development for acquiring a
head-driven phrase structure grammar
from the Penn Treebank. In Proceedings of
the First International Joint Conference on
Natural Language Processing (IJCNLP-04),
pages 684?693, Hainan Island, China.
Miyao, Yusuke and Jun?ichi Tsujii. 2002.
Maximum entropy estimation for
feature forests. In Proceedings of the
550
Clark and Curran Wide-Coverage Efficient Statistical Parsing
Human Language Technology Conference,
San Diego, CA.
Miyao, Yusuke and Jun?ichi Tsujii. 2003a. A
model of syntactic disambiguation based
on lexicalized grammars. In Proceedings of
the Seventh Conference on Natural Language
Learning (CoNLL), pages 1?8, Edmonton,
Canada.
Miyao, Yusuke and Jun?ichi Tsujii. 2003b.
Probabilistic modeling of argument
structures including non-local
dependencies. In Proceedings of the
Conference on Recent Advances in
Natural Language Processing (RANLP),
pages 285?291, Borovets, Bulgaria.
Miyao, Yusuke and Jun?ichi Tsujii. 2004.
Deep linguistic analysis for the accurate
identification of predicate-argument
relations. In Proceedings of COLING-2004,
pages 1392?1397, Geneva, Switzerland.
Miyao, Yusuke and Jun?ichi Tsujii. 2005.
Probabilistic disambiguation models
for wide-coverage HPSG parsing. In
Proceedings of the 43rd meeting of the ACL,
pages 83?90, University of Michigan,
Ann Arbor.
Moortgat, Michael. 1997. Categorial type
logics. In Johan van Benthem and
Alice ter Meulen, editors, Handbook of
Logic and Language. Elsevier, Amsterdam,
and The MIT Press, Cambridge, MA,
pages 93?177.
Nasr, Alexis and Owen Rambow. 2004.
Supertagging and full parsing. In
Proceedings of the TAG+7 Workshop,
Vancouver, Canada.
Nocedal, Jorge and Stephen J. Wright.
1999. Numerical Optimization. Springer,
New York, NY.
Osborne, Miles. 2000. Estimation of
stochastic attribute-value grammars
using an informative sample. In
Proceedings of the 18th International
Conference on Computational Linguistics,
pages 586?592, Saarbru?cken, Germany.
Osborne, Miles and Ted Briscoe. 1997.
Learning stochastic categorial grammars.
In Proceedings of the ACL CoNLL Workshop,
pages 80?87, Madrid, Spain.
Palmer, Martha, Dan Gildea, and Paul
Kingsbury. 2005. The Proposition Bank:
A corpus annotated with semantic roles.
Computational Linguistics, 31(1):71?105.
Preiss, Judita. 2003. Using grammatical
relations to compare parsers. In Proceedings
of the 10th Meeting of the EACL,
pages 291?298, Budapest, Hungary.
Prins, Robbert and Gertjan van Noord. 2003.
Reinforcing parser preferences through
tagging. Traitement Automatique des
Langues, 44(3):121?139.
Ratnaparkhi, Adwait. 1996. A maximum
entropy part-of-speech tagger. In
Proceedings of the EMNLP Conference,
pages 133?142, Philadelphia, PA.
Ratnaparkhi, Adwait. 1998. Maximum
Entropy Models for Natural Language
Ambiguity Resolution. Ph.D. thesis,
University of Pennsylvania.
Ratnaparkhi, Adwait. 1999. Learning to
parse natural language with maximum
entropy models. Machine Learning,
34(1?3):151?175.
Ratnaparkhi, Adwait, Salim Roukos, and
Todd Ward. 1994. A maximum entropy
model for parsing. In Proceedings of
the International Conference on Spoken
Language Processing, pages 803?806,
Yokohama, Japan.
Riezler, Stefan, Tracy H. King, Ronald M.
Kaplan, Richard Crouch, John T. Maxwell,
III, and Mark Johnson. 2002. Parsing the
Wall Street Journal using a Lexical-
Functional Grammar and discriminative
estimation techniques. In Proceedings of the
40th Meeting of the ACL, pages 271?278,
Philadelphia, PA.
Sagae, Kenji and Alon Lavie. 2005.
A classifier-based parser with linear
run-time complexity. In Proceedings of the
9th International Workshop on Parsing
Technologies, pages 125?132, Vancouver,
Canada.
Sarkar, Anoop and Aravind Joshi. 2003.
Tree-adjoining grammars and its
application to statistical parsing. In Rens
Bod, Remko Scha, and Khalil Sima?an,
editors, Data-oriented parsing. CSLI
Publications, Stanford, CA.
Sha, Fei and Fernando Pereira. 2003.
Shallow parsing with conditional random
fields. In Proceedings of the HLT/NAACL
conference, pages 213?220, Edmonton,
Canada.
Steedman, Mark. 1996. Surface Structure
and Interpretation. The MIT Press,
Cambridge, MA.
Steedman, Mark. 2000. The Syntactic Process.
The MIT Press, Cambridge, MA.
Steedman, Mark, Steven Baker, Stephen
Clark, Jeremiah Crim, Julia Hockenmaier,
Rebecca Hwa, Miles Osborne, Paul
Ruhlen, and Anoop Sarkar. 2002.
Semi-supervised training for statistical
parsing: Final report. Technical Report
CLSP WS-02, Center for Language and
Speech Processing, Johns Hopkins
University, Baltimore, MD.
551
Computational Linguistics Volume 33, Number 4
Taskar, B., D. Klein, M. Collins, D. Koller,
and C. Manning. 2004. Max-margin
parsing. In Proceedings of the EMNLP
Conference, pages 1?8, Barcelona, Spain.
Toutanova, Kristina, Christopher
Manning, Stuart Shieber, Dan Flickinger,
and Stephan Oepen. 2002. Parse
disambiguation for a rich HPSG
grammar. In Proceedings of the First
Workshop on Treebanks and Linguistic
Theories, pages 253?263, Sozopol, Bulgaria.
Toutanova, Kristina, Penka Markova, and
Christopher Manning. 2004. The leaf
projection path view of parse trees:
Exploring string kernels for HPSG
parse selection. In Proceedings of the
EMNLP conference, pages 166?173,
Barcelona, Spain.
Watkinson, Stephen and Suresh Manandhar.
2001. Acquisition of large categorial
grammar lexicons. In Proceedings of the
Conference of the Pacific Association for
Computational Linguistics (PACLING-01),
Kitakyushu, Japan.
Watson, Rebecca, John Carroll, and E. J.
Briscoe. 2005. Efficient extraction of
grammatical relations. In Proceedings
of the 9th International Workshop on
Parsing Technologies, pages 160?170,
Vancouver, Canada.
Wood, Mary McGee. 1993. Categorial
Grammars. Routledge, London.
Xia, Fei, Martha Palmer, and Aravind Joshi.
2000. A uniform method of grammar
extraction and its applications. In
Proceedings of the EMNLP Conference,
pages 53?62, Hong Kong.
Younger, D. 1967. Recognition and
parsing of context-free languages
in time n3. Information and Control,
10(2):189?208.
Zettlemoyer, Luke S. and Michael Collins.
2005. Learning to map sentences to
logical form: Structured classification
with probabilistic categorial grammars.
In Proceedings of the 21st Conference
on Uncertainty in Artificial Intelligence,
pages 658?666, Edinburgh, UK.
552
Syntactic Processing Using the Generalized
Perceptron and Beam Search
Yue Zhang?
University of Cambridge
Stephen Clark??
University of Cambridge
We study a range of syntactic processing tasks using a general statistical framework that consists
of a global linear model, trained by the generalized perceptron together with a generic beam-
search decoder. We apply the framework to word segmentation, joint segmentation and POS-
tagging, dependency parsing, and phrase-structure parsing. Both components of the framework
are conceptually and computationally very simple. The beam-search decoder only requires the
syntactic processing task to be broken into a sequence of decisions, such that, at each stage in
the process, the decoder is able to consider the top-n candidates and generate all possibilities
for the next stage. Once the decoder has been defined, it is applied to the training data, using
trivial updates according to the generalized perceptron to induce a model. This simple framework
performs surprisingly well, giving accuracy results competitive with the state-of-the-art on all
the tasks we consider.
The computational simplicity of the decoder and training algorithm leads to significantly
higher test speeds and lower training times than their main alternatives, including log-linear
and large-margin training algorithms and dynamic-programming for decoding. Moreover, the
framework offers the freedom to define arbitrary features which can make alternative training
and decoding algorithms prohibitively slow. We discuss how the general framework is applied to
each of the problems studied in this article, making comparisons with alternative learning and
decoding algorithms. We also show how the comparability of candidates considered by the beam
is an important factor in the performance. We argue that the conceptual and computational sim-
plicity of the framework, together with its language-independent nature, make it a competitive
choice for a range of syntactic processing tasks and one that should be considered for comparison
by developers of alternative approaches.
1. Introduction
In this article we study a range of syntactic processing tasks using a general framework
for structural prediction that consists of the generalized perceptron (Collins 2002) and
? University of Cambridge Computer Laboratory, William Gates Building, 15 JJ Thomson Avenue,
Cambridge, UK. E-mail: yue.zhang@cl.cam.ac.uk.
?? University of Cambridge Computer Laboratory, William Gates Building, 15 JJ Thomson Avenue,
Cambridge, UK. E-mail: stephen.clark@cl.cam.ac.uk.
Submission received: 10 November 2009; revised submission received: 12 August 2010; accepted for
publication: 20 September 2010.
? 2011 Association for Computational Linguistics
Computational Linguistics Volume 37, Number 1
beam-search. We show that the framework, which is conceptually and computationally
simple, is practically effective for structural prediction problems that can be turned into
an incremental process, allowing accuracies competitive with the state-of-the-art to be
achieved for all the problems we consider.
The framework is extremely flexible and easily adapted to each task. One advan-
tage of beam-search is that it does not impose any requirements on the structure of
the problem, for example, the optimal sub-problem property required for dynamic-
programming, and can easily accommodate non-local features. The generalized per-
ceptron is equally flexible, relying only on a decoder for each problem and using a
trivial online update procedure for each training example. An advantage of the linear
perceptron models we use is that they are global models, assigning a score to a complete
hypothesis for each problem rather than assigning scores to parts which are then com-
bined under statistical independence assumptions. Here we are following a recent line
of work applying global discriminative models to tagging and wide-coverage parsing
problems (Lafferty, McCallum, and Pereira 2001; Collins 2002; Collins and Roark 2004;
McDonald, Crammer, and Pereira 2005; Clark and Curran 2007; Carreras, Collins, and
Koo 2008; Finkel, Kleeman, and Manning 2008).
The flexibility of our framework leads to competitive accuracies for each of the tasks
we consider. For word segmentation, we show how the framework can accommodate
a word-based approach, rather than the standard and more restrictive character-based
tagging approaches. For POS-tagging, we consider joint segmentation and POS-tagging,
showing that a single beam-search decoder can be used to achieve a significant accuracy
boost over the pipeline baseline. For Chinese and English dependency parsing, we
show how both graph-based and transition-based algorithms can be implemented as
beam-search, and then combine the two approaches into a single model which out-
performs both in isolation. Finally, for Chinese phrase-structure parsing, we describe a
global model for a shift-reduce parsing algorithm, in contrast to current deterministic
approaches which use only local models at each step of the parsing process. For all these
tasks we present results competitive with the best results in the literature.
In Section 2 we describe our general framework of the generic beam-search algo-
rithm and the generalized perceptron. Then in the subsequent sections we describe
each task in turn, based on conference papers including Zhang and Clark (2007, 2008a,
2008b, 2009, 2010), presented in our single coherent framework. We give an updated
set of results, plus a number of additional experiments which probe further into the
advantages and disadvantages of our framework. For the segmentation task, we also
compare our beam-search framework with alternative decoding algorithms including
an exact dynamic-programming method, showing that the beam-search method is sig-
nificantly faster with comparable accuracy. For the joint segmentation and POS-tagging
task, we present a novel solution using the framework in this article, and show that
it gives comparable accuracies to our previous work (Zhang and Clark 2008a), while
being more than an order of magnitude faster.
In Section 7 we provide further discussion of the framework based on the studies
of the individual tasks. We present the main advantages of the framework, and give an
analysis of the main reasons for the high speeds and accuracies achieved. We also dis-
cuss how this framework can be applied to a potential new task, and show that the com-
parability of candidates in the incremental process is an important factor to consider.
In summary, we study a general framework for incremental structural prediction,
showing how the framework can be tailored to a range of syntactic processing problems
to produce results competitive with the state-of-the-art. The conceptual and compu-
tational simplicity of the framework, together with its language-independent nature,
106
Zhang and Clark Syntactic Processing
make it a competitive choice that should be considered for comparison by developers
of alternative approaches.
2. The Decoding and Training Framework
The framework we study in this article addresses the general structural prediction
problem of mapping an input structure x ? X onto an output structure y ? Y, where
X is the set of possible inputs, and Y is the set of possible outputs. For example, for
the problem of Chinese word segmentation, X is the set of raw Chinese sentences and
Y is the set of all possible segmented Chinese sentences. For the problem of English
dependency parsing, X is the set of all English sentences and Y is the set of all possible
English dependency trees.
Given an input sentence x, the output F(x) is defined as the highest scored among
the possible output structures for x:
F(x) = arg max
y?GEN(x)
Score(y) (1)
where GEN(x) denotes the set of possible outputs for an input sentence x, and Score(y)
is some real-valued function on Y.
To compute Score(y), the output structure y is mapped into a global feature vector
?(y) ? N d. Here a feature is a count of the occurrences of a certain pattern in an output
structure, extracted according to a set of feature templates, and d is the total number of
features. The term global feature vector is used by Collins (2002) to distinguish between
feature counts for whole sequences and the local feature vectors in maximum entropy
tagging models, which are boolean-valued vectors containing the indicator features for
one element in the sequence (Ratnaparkhi 1998). Having defined the feature vector,
Score(y) is computed using a linear model:
Score(y) = ?(y) ? ~w (2)
where ~w ? Rd is the parameter vector of the model, the value of which is defined by
supervised learning using the generalized perceptron.
For the general framework we study in this article, the output y is required to be
built through an incremental process. Suppose that K incremental steps are taken in
total to build y. The incremental change at the ith step (0 < i ? K) can be written as
?(y, i). For word segmentation, ?(y, i) can be an additional character added to the output;
for shift-reduce parsing, ?(y, i) can be an additional shift-reduce action. Denoting the
change to the global feature vector at the incremental step as ?(?(y, i)), the global feature
vector ?(y) can be written as ?(y) =?Ki=1 ?(?(y, i)). Hence, Score(y) can be computed
incrementally by
Score(y) =
K?
i=1
?(?(y, i)) ? ~w (3)
In the following sections, we describe the two major components of the gen-
eral framework, that is, the general beam-search algorithm for finding F(x) =
arg maxy?GEN(x) Score(y) for a given x, and the generalized perceptron for training ~w.
107
Computational Linguistics Volume 37, Number 1
2.1 Beam-Search Decoding
Given an input x, the output structure y is built incrementally. At each step, an incre-
mental sub-structure is added to the partially built output. Due to structural ambiguity,
different sub-structures can be built. Taking POS-tagging for example, the incremental
sub-structure for each processing step can be a POS-tag assigned to the next input word.
Due to structural ambiguity, different POS-tags can be assigned to a word, and the
decoding algorithm searches for the particular path of incremental steps which builds
the highest scored output.
We present a generic beam-search algorithm for our decoding framework, which
uses an agenda to keep the B-best partial outputs at each incremental step. The partially
built structures, together with useful additional information, are represented as a set of
state items. Additional information in a state item is used by the decoder to organize
the current structures or keep a record of the incremental process. For POS-tagging
it includes the remaining input words yet to be assigned POS-tags; for a shift-reduce
parser, it includes the stack structure for the shift-reduce process and the incoming
queue of unanalyzed words.
The agenda is initialized as empty, and the state item that corresponds to the initial
structure is put onto it before decoding starts. At each step during decoding, each state
item from the agenda is extended with one incremental step. When there are multiple
choices to extend one state item, multiple new state items are generated. The new state
items generated at a particular step are ranked by their scores, and the B-best are put
back onto the agenda. The process iterates until a stopping criterion is met, and the
current best item from the agenda is taken as the output.
Pseudo code for the generic beam-search algorithm is given in Figure 1, where the
variable problem represents a particular task, such as word segmentation or dependency
parsing, and the variable candidate represents a state item, which has a different defini-
tion for each task. For example, for the segmentation task, a candidate is a pair, consisting
of the partially segmented sentence and the remaining character sequence yet to be
segmented. The agenda is an ordered list, used to keep all the state items generated at
each stage, ordered by score. The variable candidates is the set of state items that can be
used to generate new state items, that is, the B-best state items from the previous stage.
B is the number of state items retained at each stage.
function BEAM-SEARCH(problem, agenda, candidates, B)
candidates ? {STARTITEM(problem)}
agenda ? CLEAR(agenda)
loop do
for each candidate in candidates
agenda ? INSERT(EXPAND(candidate, problem), agenda)
best ? TOP(agenda)
if GOALTEST(problem, best)
then return best
candidates ? TOP-B(agenda, B)
agenda ? CLEAR(agenda)
Figure 1
The generic beam-search algorithm.
108
Zhang and Clark Syntactic Processing
STARTITEM initializes the start state item according to the problem; for example,
for the segmentation task, the start state item is a pair consisting of an empty seg-
mented sentence and the complete sequence of characters waiting to be segmented.
CLEAR removes all items from the agenda. INSERT puts one or more state items onto
the agenda. EXPAND represents an incremental processing step, which takes a state
item and generates new state items from it in all possible ways; for example, for the
segmentation task, EXPAND takes the partially segmented sentence in a state item,
and extends it in all possible ways using the first character in the remaining character
sequence in the state item. TOP returns the highest scoring state item on the agenda.
GOALTEST checks whether the incremental decoding process is completed; for example,
for the segmentation task, the process is completed if the state item consists of a fully
segmented sentence and an empty remaining character sequence. TOP-B returns the B-
highest scoring state items on the agenda, which are used for the next incremental step.
State items in the agenda are ranked by their scores. Suppose that K incremental
steps are taken in total to build an output y. At the ith step (0 < i ? K), a state item in
the agenda can be written as candidatei, and we have
Score(candidatei) =
i?
n=1
?(?(candidatei, n)) ? ~w
Features for a state item can be based on both the partially built structure and the
additional information we mentioned earlier.
The score of a state item can be computed incrementally as the item is built. The
score of the start item is 0. At the ith step (0 < i ? K), a state item candidatei is generated
by extending an existing state item candidatei?1 on the agenda with ?(candidatei, i). In
this case, we have
Score(candidatei) = Score(candidatei?1) +?(?(candidatei, i)) ? ~w
Therefore, when a state item is extended, its score can be updated by adding the
incremental score of the step ?(?(candidatei, i)) ? ~w. The nature of the scoring function
means that, given appropriately defined features, it can be computed efficiently for both
the incremental decoding and training processes.
Because the correct item can fall out of the agenda during the decoding process, the
general beam-search framework is an approximate decoding algorithm. Nevertheless,
empirically this algorithm gives competitive results on all the problems in this article.
2.2 The Generalized Perceptron
The perceptron learning algorithm is a supervised training algorithm. It initializes the
parameter vector as all zeros, and updates the vector by decoding the training examples.
For each example, the output structure produced by the decoder is compared with
the correct structure. If the output is correct, no update is performed. If the output
is incorrect, the parameter vector is updated by adding the global feature vector of
the training example and subtracting the global feature vector of the decoder output.
Intuitively, the training process is effectively coercing the decoder to produce the correct
output for each training example. The algorithm can perform multiple passes over the
same training sentences. In all experiments, we decide the number of training iterations
using a set of development test data, by choosing the number that gives the highest
109
Computational Linguistics Volume 37, Number 1
Inputs: training examples (xi, yi)
Initialization: set ~w = 0
Algorithm:
for r = 1..P, i = 1..N
calculate zi = decode(xi)
if zi 6= yi
~w = ~w +?(yi) ? ?(zi)
Outputs: ~w
Figure 2
The generalized perceptron algorithm, adapted from Collins (2002).
development test accuracy as the final number in testing. Figure 2 gives the algorithm,
where N is the number of training sentences and P is the number of passes over the data.
The averaged perceptron algorithm (Collins 2002) is a standard way of reducing
overfitting on the training data. It was motivated by the voted-perceptron algorithm
(Freund and Schapire 1999) and has been shown to give improved accuracy over the
non-averaged perceptron on a number of tasks. Let N be the number of training sen-
tences, P the number of training iterations, and ~wi,r the parameter vector immediately
after the ith sentence in the rth iteration. The averaged parameter vector ~? ? Rd is
defined as
~? = 1PN
?
i=1..N,r=1..P
~wi,r
and it is used instead of ~w as the model parameters. We use the averaged perceptron for
all the tasks we consider.
We also use the early-update strategy of Collins and Roark (2004), which is a
modified version of the perceptron algorithm specifically for incremental decoding
using beam search. At any step during the decoding process to calculate zi, if all partial
candidates in the agenda are incorrect, decoding is stopped and the parameter vector is
updated according to the current best candidate in the agenda and the corresponding
gold-standard partial output. To perform early-update, the decoder needs to keep a
version of the correct partial output for each incremental step, so that the parameter
values are adjusted as soon as the beam loses track of the correct state item. The intuition
is to force the beam to keep the correct state item at every incremental step, rather than
learning only the correct overall structure. This strategy has been shown to improve the
accuracy over the original perceptron for beam-search decoding.
In summary, our general framework consists of a global linear model, which is
trained by the averaged perceptron, and a beam-search decoder. When applied to a
particular task, the structure of a state item as well as some of the functions in the
decoder need to be instantiated. In the following sections, we show how the general
framework can be applied to Chinese word segmentation, joint segmentation and POS-
tagging, Chinese and English dependency parsing, and Chinese constituent parsing.
3. Word Segmentation
Chinese word segmentation (CWS) is the problem of finding word boundaries for
Chinese sentences, which are written as continuous character sequences. Other lan-
guages, including Japanese and Thai, also have the problem of word segmentation, and
typical statistical models for CWS can also be applied to them.
110
Zhang and Clark Syntactic Processing
Word segmentation is a problem of ambiguity resolution, often requiring knowl-
edge from a variety of sources. Out-of-vocabulary (OOV) words are a major source of
ambiguity. For example, a difficult case occurs when an OOV word consists of characters
which have themselves been seen as words; here an automatic segmentor may split the
OOV word into individual single-character words. Typical examples of unseen words
include Chinese names, translated foreign names, and idioms.
The segmentation of known words can also be ambiguous. For example,
should be (here) (flour) in the sentence (flour and rice are ex-
pensive here) or (here) (inside) in the sentence (it?s cold inside
here). The ambiguity can be resolved with information about the neighboring words. In
comparison, for the sentence , possible segmentations include (the
discussion) (will) (very) (be successful) and (the discussion meeting)
(very) (be successful). The ambiguity can only be resolved with contextual infor-
mation outside the sentence. Human readers often use semantics, contextual informa-
tion about the document, and world knowledge to resolve segmentation ambiguities.
There is no fixed standard for Chinese word segmentation. Experiments have
shown that there is only about 75% agreement among native speakers regarding the
correct word segmentation (Sproat et al 1996). Also, specific NLP tasks may require
different segmentation criteria. For example, could be treated as a single
word (Bank of Beijing) for machine translation, although it is more naturally segmented
into (Beijing) (bank) for tasks such as text-to-speech synthesis. Therefore,
supervised learning with specifically defined training data has become the dominant
approach.
Following Xue (2003), the standard approach for building a statistical CWS model
is to treat CWS as a sequence labeling task. A tag is assigned to each character in the
input sentence, indicating whether the character is a single-character word or the start,
middle, or end of a multi-character word. The context for disambiguation is normally a
five-character window with the current character in the middle. We call these methods
character-based word segmentation. The advantage of character-based segmentation is
that well-known tagging approaches can be applied directly to the CWS problem.
There are various character-based models in the literature. They differ mainly in the
learning algorithm and the features used. Several discriminative learning algorithms
have been applied to the character-based systems. Examples include Xue (2003), Peng,
Feng, and McCallum (2004), and Wang et al (2006), which use maximum entropy and
conditional random field models, and Jiang et al (2008), which uses the perceptron
model. The standard feature set is that defined by Ng and Low (2004), though other
feature sets are reported to improve the accuracy (Zhao, Huang, and Li 2006). Zhao,
Huang, and Li (2006) also showed that the best accuracy for conditional random field
(CRF) models is given by using a set of six character segmentation tags, rather than
the standard set {beginning, middle, end, single} shown previously. Standard search
algorithms for sequence tagging have been applied to the decoding process, such as
the dynamic-programming algorithm and beam-search.
A disadvantage of character-based models is the use of limited contextual infor-
mation. For these methods, context is confined to the neighboring characters. Other
contextual information, in particular the surrounding words, is not included. Consider
the sentence , which can be from (among which) (foreign)
(companies), or (in China) (foreign companies) (business). Note that the
five-character window surrounding is the same in both cases, making the tagging
decision for that character difficult given the local window. The correct decision can be
made, however, by comparing the two three-word windows containing this character.
111
Computational Linguistics Volume 37, Number 1
In Zhang and Clark (2007) we proposed a word-based approach to segmentation,
which provides a direct solution to the problem. In comparison with the character-based
approach, our segmentor does not map the CWS problem into sequence labeling. By
using a global linear model, it addresses the segmentation problem directly, extracting
word-based features from the output segmented structure. Hence we call our word
segmentation model the word-based approach. In fact, word-based segmentors can be
seen as a generalization of character-based segmentors, because any character-based
features can be defined in a word-based model.
In the following sections, we describe a word-based segmentor using the general
framework of this article, which is slightly different from the original system we pro-
posed in Zhang and Clark (2007). We compare the accuracies of this segmentor and
the 2007 segmentor, and report a set of improved results for our 2007 segmentor using
a better method to optimize the number of training iterations. We then study alterna-
tive decoders to the general framework, including a Viterbi inference algorithm and a
multiple-beam search algorithm, and provide discussion on the general framework and
word-based segmentation.
3.1 Instantiating the General Framework
In this section we formulate our word-based segmentor as an instance of the general
framework of this article. Our segmentor builds a candidate segmentation incremen-
tally, one character at a time. When each character is processed, it is either combined
with the last word of the partial candidate that has been built so far, or added to the
candidate as the start of a new word. The same process repeats for each input character,
and therefore runs in linear time.
For ambiguity resolution, we use a beam-search decoding algorithm to explore the
search space. Initially containing only an empty sentence, an agenda is used to keep a
set of candidate items for each processing step. When an input character is processed,
it is combined with each candidate in the agenda in the two aforementioned ways, and
two new candidates are generated. At the end of each step, the B-best newly generated
candidates are kept in the agenda for the next processing step. When the input sentence
is exhausted, the top candidate from the agenda is taken as the output.
This decoding process can be expressed as an instance of the generic algorithm in
Figure 1. For the word segmentation problem, a state item in the algorithm is a pair
?S, Q?, where S contains part of the input that has been segmented, and Q contains
the rest of the input sentence as a queue of incoming characters. The initial state item
STARTITEM(word segmentation) contains an empty sentence, and an incoming queue of
the whole input sentence. EXPAND(candidate, word segmentation) pops the first character
from the incoming queue, and adds it to the partial segmented sentence in candidate in
two different ways to generate two new state items: It either appends the character to
the last word in the sentence or joins it as the start of a new word in the sentence. Finally,
GOALTEST(word segmentation, best) returns true if best contains a fully segmented input
sentence, and therefore an empty incoming queue, and false otherwise.
The score of a segmented sentence is computed by the global linear model in
Equation (2), where the parameter vector ~w for the model is computed by the early-
update version of the perceptron training algorithm described in Section 2.2. Our word
segmentor computes the global feature vector ?(y) incrementally according to Equa-
tion (3), where for the ith character, ?(?(y, i)) is computed using the feature templates
in Table 1, according to whether the character is appended to or separated from its
previous character.
112
Zhang and Clark Syntactic Processing
Table 1
Feature templates for the word segmentor.
Feature template When c0 is
1 w?1 separated
2 w?1w?2 separated
3 w?1, where len(w?1) = 1 separated
4 start(w?1)len(w?1) separated
5 end(w?1)len(w?1) separated
6 end(w?1)c0 separated
7 c?1c0 appended
8 begin(w?1)end(w?1) separated
9 w?1c0 separated
10 end(w?2)w?1 separated
11 start(w?1)c0 separated
12 end(w?2)end(w?1) separated
13 w?2len(w?1) separated
14 len(w?2)w?1 separated
w = word; c = character. The index of the current character is 0.
3.2 Comparisons with Zhang and Clark (2007)
Both the segmentor of this article and our segmentor of Zhang and Clark (2007) use
a global linear model trained discriminatively using the perceptron. However, when
comparing state items in the agenda, our 2007 segmentor treated full words in the
same way as partial words, scoring them using the same feature templates. This scoring
mechanism can potentially have a negative effect on the accuracy. In this article, we take
a different strategy and apply full-word feature templates only when the next input
character is separated from the word. In fact, most of the feature templates in Table 1
are related to full word information, and are applied when separating the next character.
This method thus gives a clear separation of partial word and full word information. We
also applied early-update in this article, so that the training process is closely coupled
with beam-search decoding. In Zhang and Clark (2007) we performed the standard
global discriminative learning.
3.3 Combining Word-Based and Character-Based Segmentation
As stated earlier, a character-based segmentor maps the segmentation problem into
a sequence labeling problem, where labels are assigned to each input character to
represent its segmentation. Our word-based approach does not map the segmentation
problem into a labeling task but solves it directly. In this article, we further show that the
flexibility of the word-based approach, enabled by our general framework, allows the
combination of a character-based sub-system into our word-based system. The intuition
is simple: Both perceptron learning and beam-search decoding allow arbitrary features,
and therefore features from a typical character-based system can be incorporated into
our segmentor to provide further information. Though character-based segmentors can
also leverage word-level features indirectly, the labeling nature prevents them from
direct use of word information.
We follow the convention of character-based segmentation, and define the set of
segmentation tags as {B, E, M, S}. The tags B, E, M represent the character being the
113
Computational Linguistics Volume 37, Number 1
beginning, end, and middle of a multiple-character word, respectively, and the tag S
represents the character being a single-character word.
The character-based features that we incorporate into our segmentor are shown
in Table 2, which consist of unigram, bigram, and trigram information in the three-
character window surrounding the current character, paired with the segmentation tag
of the current character. To distinguish this system from our system without combina-
tion of character-based information, we call our segmentor in Section 3.1 the pure word-
based segmentor and the segmentor that uses character-based features the combined
segmentor in our experimental sections.
3.4 Experiments
We performed two sets of experiments. In the first set of experiments, we used the
Chinese Treebank (CTB) data to study the speed/accuracy tradeoff by varying the
size of the beam. In the second set of experiments, we used training and testing sets
from the first and second international Chinese word segmentation bakeoffs (Sproat
and Emerson 2003; Emerson 2005) to compare the accuracies to other models in the
literature, including our segmentor of Zhang and Clark (2007).
F-score is used as the accuracy measure: 2pr/(p + r), where precision p is the per-
centage of words in the decoder output that are segmented correctly, and recall r is the
percentage of gold-standard output words that are correctly segmented by the decoder.
CWS systems are evaluated by two types of tests. The closed tests require that the
system is trained only with a designated training corpus. Any extra knowledge is not
allowed, including common surnames, Chinese and Arabic numbers, European letters,
lexicons, parts-of-speech, semantics, and so on. The open tests do not impose such
restrictions. Open tests measure a model?s capability to utilize extra information and
domain knowledge, which can lead to improved performance, but because this extra
information is not standardized, direct comparison between open test results is less
informative. In this article, we focus only on the closed test.
3.4.1 Speed/Accuracy Tradeoff. We split CTB5 into training, development test, and test
sets as shown in Table 3, where the development test data are used to determine the
number of training iterations, which are used to obtain the final accuracies on the test
data. We measure the accuracies on the test data with various beam-sizes, and plot
the speed/accuracy tradeoff graph in Figure 3. Each point in the figure, from right to
left, corresponds to beam size B = 1, 2, 4, 8, 16, 32, and 64, respectively. Speed is mea-
sured in the number of thousand characters per second, and accuracy is calculated using
F-score.
As the size of the beam increases, the speed of the segmentor decreases. Because
a larger part of the search is explored with an increased beam size, the accuracy of
Table 2
Feature templates of a typical character-based word segmentor.
Feature template When c0 is
1 cis0, i ? {?1, 0, 1} separated, appended
2 ci?1cis0, i ? 0, 1 separated, appended
3 c?1c0c1s0 separated, appended
c = character; s = segmentation tag. The index of the current character is 0.
114
Zhang and Clark Syntactic Processing
Table 3
Training, development, and test data for word segmentation on CTB5.
Sections Sentences Words
Training 1?270, 400?931, 1001?1151 18,085 493,892
Dev 301?325 350 6,821
Test 271?300 348 8,008
the decoder has the potential to increase. This explains the increased accuracies when
B increases from 1 to 16. However, the amount of increase drops when the beam size
increases.
3.4.2 Closed Test on the SIGHAN Bakeoffs. Four training and testing corpora were used in
the first bakeoff (Sproat and Emerson 2003), including the Academia Sinica Corpus (AS),
the Penn Chinese Treebank Corpus (CTB), the Hong Kong City University Corpus (CU),
and the Peking University Corpus (PU). However, because the testing data from the
Penn Chinese Treebank Corpus is currently unavailable to us, we excluded this corpus
from our experiments. The corpora are encoded in GB (PU, CTB) and BIG5 (AS, CU).
In order to test them consistently in our system, they are all converted to UTF8 without
loss of information.
The results are shown in Table 4. We follow the format from Peng, Feng, and
McCallum (2004), where each row represents a CWS model. The first three columns
represent tests with the AS, CU, and PU corpora, respectively. The best score in each
column is shown in bold. The last two columns represent the average accuracy of each
model over the tests it participated in (SAV), and our average over the same tests (OAV),
respectively. The first eight rows represent models from Sproat and Emerson (2003) that
participated in at least one closed test from the table, row ?Peng? represents the CRF
model from Peng, Feng, and McCallum (2004), row ?Zhang 2007? represents our model
as reported in Zhang and Clark (2007), and the last two rows represent our model in
this article, using only word-based features in Table 1 and combined features in Tables 1
plus 2, respectively.
In Zhang and Clark (2007) we fixed the number of training iterations to six for
all experiments, according to a separate set of development data. An alternative way
to decide the number of training iterations is to set apart 10% from the training data
Figure 3
Speed/accuracy tradeoff of the segmentor.
115
Computational Linguistics Volume 37, Number 1
Table 4
The accuracies of various word segmentors over the first SIGHAN bakeoff data.
AS CU PU SAV OAV
S01 93.8 90.1 95.1 93.0 95.5
S04 93.9 93.9 94.8
S05 94.2 89.4 91.8 95.9
S06 94.5 92.4 92.4 93.1 95.5
S08 90.4 93.6 92.0 94.8
S09 96.1 94.6 95.4 95.9
S10 94.7 94.7 94.8
S12 95.9 91.6 93.8 95.9
Peng 95.6 92.8 94.1 94.2 95.5
Zhang 2007 96.5 94.6 94.0 95.0 95.5
Zhang 2007* 96.9 94.6 94.1 95.2 95.5
this article pure 97.0 94.6 94.6 95.4 95.5
this article combined 96.9 94.8 94.8
The best score in each column and the best average in each row is in boldface.
*Zhang 2007 with the (Carreras, Surdeanu, and Marquez 2006) method applied (see text for
details).
as development test data, and use the rest for development training. For testing, all
training data are used for training, with the number of training iterations set to be the
number which gave the highest accuracy during the development experiments. This
method was used by Carreras, Surdeanu, and Marquez (2006) in their parsing model.
We apply it to our segmentor model in this article. Moreover, we also use this method
to decide the number of training iterations for our system of Zhang and Clark (2007),
and show the accuracies in row ?Zhang 2007*?.
For each row the best average is shown in bold. We achieved the best accuracy in all
three corpora, and better overall accuracy than all the other models using the method
of this article. Our new method to decide the number of training iterations also gave
improved accuracies compared to our 2007 model. The combination of character-based
features and our original word-based features gave slight improvement in the overall
accuracy.
Four training and testing corpora were used in the second bakeoff (Emerson 2005),
including the Academia Sinica corpus (AS), the Hong Kong City University Corpus
(CU), the Peking University Corpus (PK), and the Microsoft Research Corpus (MR).
Different encodings were provided, and the UTF8 data for all four corpora were used
in our experiments.
Following the format of Table 4, the results for this bakeoff are shown in Table 5. We
chose the three models that achieved at least one best score in the closed tests from
Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita
(2006) for comparison. Row ?Zh-a? and ?Zh-b? represent the pure sub-word CRF model
and the confidence-based combination of the CRF and rule-based models, respectively.
Again, our model achieved better overall accuracy than all the other models. The
combination of character-based features improved the accuracy slightly again.
116
Zhang and Clark Syntactic Processing
Table 5
The accuracies of various word segmentors over the second SIGHAN bakeoff data.
AS CU PK MR SAV OAV
S14 94.7 94.3 95.0 96.4 95.1 95.6
S15b 95.2 94.1 94.1 95.8 94.8 95.6
S27 94.5 94.0 95.0 96.0 94.9 95.6
Zh-a 94.7 94.6 94.5 96.4 95.1 95.6
Zh-b 95.1 95.1 95.1 97.1 95.6 95.6
Zhang 2007 94.6 95.1 94.5 97.2 95.4 95.6
Zhang 2007* 95.0 95.1 94.6 97.3 95.5 95.6
This article pure 95.1 95.2 94.4 97.3 95.5 95.6
This article combined 95.4 95.1 94.4 97.3
The best score in each column and the best average in each row is in boldface.
*Zhang 2007 with the (Carreras, Surdeanu, and Marquez 2006) method applied (see text for
details).
3.5 Alternative Decoding Algorithms
Besides the general framework of this article, there are various alternative learning and
decoding algorithms for a discriminative linear model applied to the word segmenta-
tion problem, using the same feature templates we defined. In this section, we study
two alternative decoding algorithms to the beam-search decoder, including a multiple-
beam search algorithm, which can be viewed as an alternative decoder specifically
designed for the word segmentation and joint segmentation and tagging problems, and
a dynamic-programming algorithm. Both algorithms explore a larger part of the search
space than the single beam-search algorithm, and we compare the accuracy and speed
of these algorithms within the generalized perceptron learning framework.
3.5.1 A Multiple-Beam Search Decoder. In Zhang and Clark (2008a) we proposed a
multiple-beam decoder for the problem of joint word segmentation and POS-tagging,
in which state items only contain complete words. This algorithm can be naturally
adapted for word segmentation. Compared with the single-beam decoder, it explores a
larger fraction of the search space. Moreover, the multiple-beam decoder does not have
the problem of comparing partial words with full words in a single agenda, which our
segmentor of Zhang and Clark (2007) has. We implement this decoder for segmentation
and compare its accuracies with our single-beam decoder.
Instead of a single agenda, the multiple-beam algorithm keeps an agenda for each
character in the input sentence, recording the best partial candidates ending with the
character. Like the single beam decoder, the input sentence is processed incremen-
tally. However, at each stage, partial sequence candidates are available at all previous
characters. Therefore, the decoder can examine all candidate words ending with the
current character. These possible words are combined with the relevant partial candi-
dates from the previous agendas to generate new candidates, which are then inserted
into the agenda for the current character. The output of the decoder is the top candidate
in the last agenda, representing the best segmentation for the whole sentence. The
117
Computational Linguistics Volume 37, Number 1
multiple-beam search decoder explores a larger number of candidate outputs compared
to the single-beam search. To improve the running speed, a maximum word length
record is kept to limit the length of candidate words.
Because the multiple-beam decoder can also be applied to the joint segmentation
and POS-tagging problem in Section 4, we describe this algorithm by extending the
generic beam-search algorithm in Figure 1. Three modifications are made to the original
algorithm, shown in Figure 4. First, the B-best candidates generated in each processing
step are kept in prev topBs. Here prev topBs can be seen as a list of the candidates in the
original search algorithm, with prev topBs[k] containing the current items with size k,
and prev topBs[0] containing only the start item. An extra loop is used to enumerate all
state items in prev topBs for the generation of new state items. Second, variable k is used
to represent the size of the state items to be generated, and EXPAND generates only state
items with size k, taking k as an extra parameter. Third, the B-best newly generated state
items are appended to the back of prev topBs, without removing previous state items in
prev topBs. The algorithm thereby keeps track of kB state items instead of B at the kth
processing stage, and explores a larger subset of the exponential search space.
The rest of the algorithm is the same as the original algorithm in Figure 1. To in-
stantiate this generic algorithm for word segmentation, STARTITEM(word segmentation)
consists of an empty sentence S and a queue Q containing the full input sentence;
EXPAND(candidate, k, word segmentation) generates a single new state item by popping
characters on the incoming queue from the front until the kth character (k is the index in
the input sentence rather than the queue itself), and appending them as a new word
to candidate; and GOALTEST(word segmentation, best) returns true if best consists of a
complete segmented sentence and an empty incoming queue.
As before, the linear model from Section 2 is applied directly to score state items,
and the model parameters are trained with the averaged perceptron algorithm. The
features for a state item are extracted according to the feature templates in Table 1.
3.5.2 A Dynamic-Programming Decoder. Given the feature templates that we define,
a dynamic-programming algorithm can be used to explore the whole search space
in cubic time. The idea is to reduce the search task into overlapping sub-problems.
function MULTIPLE-BEAM-SEARCH(problem, agenda, prev topBs, B)
prev topBs ? {{STARTITEM(problem)}}
agenda ? CLEAR(agenda)
k ? 0
loop do
k ? k + 1
for each candidates in prev topBs
for each candidate in candidates
agenda ? INSERT(EXPAND(candidate, k, problem), agenda)
if GOALTEST(problem, agenda)
then return TOP(agenda)
candidates ? TOP-B(agenda, B)
prev topBs ? APPEND(prev topBs, candidates)
agenda ? CLEAR(agenda)
Figure 4
The extended generic beam-search algorithm with multiple beams.
118
Zhang and Clark Syntactic Processing
Suppose that the input has n characters; one way to find the highest-scored segmenta-
tion is to first find the highest-scored segmentations with the last word being characters
b..n ? 1, where b ? 0..n ? 1, respectively, and then choose the highest-scored one from
these segmentations. In order to find the highest-scored segmentation with the last
word being characters b, ..n ? 1, the last word needs to be combined with all different
segmentations of characters 0..b ? 1 so that the highest scored can be selected. However,
because the largest-range feature templates span only over two words (see Table 1),
the highest scored among the segmentations of characters 0..b ? 1 with the last word
being characters b?..b ? 1 will also give the highest score when combined with the word
b..n ? 1. As a result, the highest-scored segmentation with the last word being charac-
ters b..n ? 1 can be found as long as the highest-scored segmentations of 0..b ? 1 with
the last word being b?..b ? 1 are found, where b? ? 0..b ? 1. With the same reasoning, the
highest-scored segmentation of characters 0..b ? 1 with the last word being b?..b ? 1 can
be found by choosing the highest-scored one among the highest-scored segmentations
of 0..b? ? 1 with the last word being b??..b? ? 1, where b?? ? 0..b? ? 1. In this way, the
search task is reduced recursively into smaller problems, where in the simplest case
the highest-scored segmentation of characters 0..e with the last word being characters
0..e(e ? 0..n ? 1) are known. And the final highest-scored segmentation can be found by
incrementally finding the highest-scored segmentations of characters 0..e(e ? 0..n ? 1)
with the last word being b..e(b ? 0..e).
The pseudo code for this algorithm is shown in Figure 5. It works by building
an n by n table chart, where n is the number of characters in the input sentence sent.
chart[b, e] records the highest scored segmentation from the beginning to character e,
with the last word starting from character b and ending at character e. chart[0, e] can be
computed directly for e = 0..n ? 1, whereas chart[b, e] needs to be built by combing the
best segmentation on sent[0, b ? 1] and sent[b, e], for b > 0. The final output is the best
among chart[b, n ? 1], with b = 0..n ? 1. The reason for recording partial segmentations
with different final words separately (leading to cubic running time) is the word bigram
feature template. Note that with a larger feature range, exact inference with dynamic-
programming can become prohibitively slow.
Inputs: raw sentence sent (length n)
Variables: an n by n table chart, where chart[b, e] stores the best scored (partial)
segmentation of the characters from the begining of the sentence to character
e, with the last word spanning over the characters from b until e;
character index b for the start of word;
character index e for the end of word;
character index p for the start of the previous word.
Initialization:
for e = 0..n ? 1:
chart[0, e] ? a single word sent[0..e]
Algorithm:
for e = 0..n ? 1:
for b = 1..e:
chart[b, e] ? the highest scored segmentation among those derived by combining
chart[p, b ? 1] with sent[b, e], for p = 0..b ? 1
Outputs: the highest scored segmentation among chart[b, n ? 1], for b = 0..n ? 1
Figure 5
A dynamic-programming algorithm for word segmentation.
119
Computational Linguistics Volume 37, Number 1
Table 6
Comparison between three different decoders for word segmentation.
Bakeoff 1 Bakeoff 2
AS CU PU AS CU PU MS Average
SB F-measure 96.9 94.6 94.1 95.0 95.1 94.6 97.3 95.4
SB sent/sec 212 145 202 358 115 177 105 188
SB char/sec 3,054 6,846 4,808 5,263 5,333 5,870 4,963 5,162
SB # features 4.0M 0.5M 1.5M 3.9M 1.8M 1.5M 2.7M 2.3M
MB F-measure 97.0 94.5 94.1 95.0 95.0 94.4 97.3 95.3
MB sent/sec 147 7 13 167 7 11 5 51
MB char/sec 2,118 331 187 2,455 325 365 236 859
MB # features 4.0M 0.6M 1.5M 3.9M 1.8M 1.5M 2.7M 2.3M
DP F-measure 97.1 94.6 94.3 95.0 95.0 94.5 97.2 95.4
DP sent/sec 131 3 6 142 4 4 2 42
DP char/sec 1,887 142 86 2,087 185 133 95 659
DP # features 4.0M 0.5M 1.5M 3.9M 1.7M 1.4M 2.7M 2.3M
3.5.3 Experiments. Table 6 shows the comparison between the single-beam (SB), multiple-
beam (MB), and dynamic-programming (DP) decoders by F-score and speed.1 Speed is
measured by the number of sentences (sent) and characters (char) per second (excluding
model loading time). We also include the size of the models in each case. The slight
difference in model size between different methods is due to different numbers of
negative features generated during training. The single-beam search algorithm achieved
significantly higher speed than both the multiple-beam and the dynamic-programming
algorithms, whereas the multiple-beam search algorithm ran slightly faster than the
dynamic-programming algorithm. Though addressing the comparability issue and
exploring a larger number of candidate output segmentations, neither multiple-beam
search nor dynamic programming gave higher accuracy than the single-beam search
algorithm overall. One of the possible reasons is that the perceptron algorithm adjusts
its parameters according to the mistakes the decoder makes: Although the single-beam
might make more mistakes than the multiple-beam given the same model, it does not
necessarily perform worse with a specifically tailored model.
4. Joint Segmentation and Part-of-Speech Tagging
Joint word segmentation and POS-tagging is the problem of solving word segmen-
tation and POS-tagging simultaneously. Traditionally, Chinese word segmentation and
POS-tagging are performed in a pipeline. The output from the word segmentor is taken
as the input for the POS-tagger. A disadvantage of pipelined segmentation and POS-
tagging is that POS-tag information, which is potentially useful for segmentation, is
not used during the segmentation step. In addition, word segmentation errors are
propagated to the POS-tagger, leading to lower quality of the overall segmented and
1 The experiments were performed using the Zhang and Clark (2007) feature set and single-beam decoder,
and our new way to decide the number of training iterations in this article. The single-beam results
correspond to ?Zhang 2007*? in Tables 4 and 5.
120
Zhang and Clark Syntactic Processing
POS-tagged output. Joint word segmentation and POS-tagging is a method that ad-
dresses these problems. In Zhang and Clark (2008a) we proposed a joint word segmen-
tor and POS-tagger using a multiple-beam decoder, and showed that it outperformed a
pipelined baseline. We recently showed that comparable accuracies can be achieved by a
single-beam decoder, which runs an order of magnitude faster (Zhang and Clark 2010).
In this section, we describe our single-beam system using our general framework, and
provide a detailed comparison with our multiple-beam and baseline systems of Zhang
and Clark (2008a).
4.1 Instantiating the General Framework
Given an input sentence, our joint segmentor and POS-tagger builds an output incre-
mentally, one character at a time. When a character is processed, it is either concatenated
with the last word in the partially built output, or taken as a new word. In the latter
case, a POS-tag is assigned to the new word. When more characters are concatenated to
a word, the POS-tag of the word remains unchanged.
For the decoding problem, an agenda is used to keep B different candidates at
each incremental step. Before decoding starts, the agenda is initialized with an empty
sentence. When a character is processed, existing candidates are removed from the
agenda and extended with the current character in all possible ways, and the B-best
newly generated candidates are put back onto the agenda. After all the input characters
have been processed, the highest-scored candidate from the agenda is taken as output.
Expressed as an instance of the generic algorithm in Figure 1, a state item is a pair
?S, Q?, with S being a segmented and tagged sentence and Q being a queue of the next in-
coming characters. STARTITEM(joint tagging) contains an empty sentence and the whole
input sentence as incoming characters; EXPAND(candidate, joint tagging) pops the first
character from the incoming queue, adds it to candidate, and assigns POS-tags in the
aforementioned way to generate a set of new state items; and GOALTEST( joint tagging,
best) returns true if best contains a complete segmented and POS-tagged output and an
empty queue.
The linear model from Section 2 is applied to score state items, differentiating
partial words from full words in the aforementioned ways, and the model parameters
are trained with the averaged perceptron. The features for a state item are extracted
incrementally according to Equation (3), where for the ith character, ?(?(y, i)) is com-
puted according to the feature templates in both Table 1, which are related to word
segmentation, and Table 7, which are related to POS-tagging. During training, the early-
update method of Collins and Roark (2004), as described in Section 2, is used. It ensures
that state items on the beam are highly probable at each incremental step, and is crucial
to the high accuracy given by a single-beam.
4.2 Pruning
We use several pruning methods from Zhang and Clark (2008a), most of which serve
to improve the accuracy by removing irrelevant candidates from the beam. First, the
system records the maximum number of characters that a word with a particular POS-
tag can have. For example, from the Chinese Treebank that we used for our experiments,
most POS are associated only with one- or two-character words. The only POS-tags that
are seen with words over ten characters long are NN (noun), NR (proper noun), and
CD (numbers). The maximum word length information is initialized as all ones, and
updated according to each training example before it is processed.
121
Computational Linguistics Volume 37, Number 1
Table 7
POS feature templates for the joint segmentor and POS-tagger.
Feature template when c0 is
1 w?1t?1 separated
2 t?1t0 separated
3 t?2t?1t0 separated
4 w?1t0 separated
5 t?2w?1 separated
6 w?1t?1end(w?2) separated
7 w?1t?1c0 separated
8 c?2c?1c0t?1, where len(w?1) = 1 separated
9 c0t0 separated
10 t?1start(w?1) separated
11 t0c0 separated or appended
12 c0t0start(w0) appended
13 ct?1end(w?1), where c ? w?1 and c 6= end(w?1) separated
14 c0t0cat(start(w0)) separated
15 ct?1cat(end(w?1)), where c ? w?1 and c 6= end(w?1) appended
16 c0t0c?1t?1 separated
17 c0t0c?1 appended
w = word; c = character; t = POS-tag. The index of the current character is 0.
Second, a tag dictionary is used to record POS-tags associated with each word.
During decoding, frequent words and words with ?closed set? tags2 are only assigned
POS-tags according to the tag dictionary, while other words are assigned every POS-
tag to make candidate outputs. Whether a word is a frequent word is decided by the
number of times it has been seen in the training process. Denoting the number of times
the most frequent word has been seen by M, a word is a frequent word if it has been
seen more than M/5, 000 + 5 times. The threshold value is taken from Zhang and Clark
(2008a), and we did not adjust it during development. Word frequencies are initialized
as zeros and updated according to each training example before it is processed; the
tag dictionary is initialized as empty and updated according to each training example
before it is processed.
Third, we make an additional record of the initial characters for words with ?closed
set? tags. During decoding, when the current character is added as the start of a new
word, ?closed set? tags are only assigned to the word if it is consistent with the record.
This type of pruning is used in addition to the tag dictionary to prune invalid partial
words, while the tag dictionary is used to prune complete words. The record for initial
character and POS is initially empty, and updated according to each training example
before it is processed.
Finally, at any decoding step, we group partial candidates that are generated by
separating the current character as the start of a new word by the signature p0p?1w?1,
and keep only the best among those having the same p0p?1w?1. The signature p0p?1w?1
is decided by the feature templates we use: it can be shown that if two candidates cand1
and cand2 generated at the same step have the same signature, and the score of cand1
is higher than the score of cand2, then at any future step, the highest scored candidate
2 ?Closed set? tags are the set of POS-tags which are only associated with a fixed set of words, according to
the Penn Chinese Treebank specifications (Xia 2000).
122
Zhang and Clark Syntactic Processing
generated from cand1 will always have a higher score than the highest scored candidate
generated from cand2.
From these four pruning methods, only the third was not used by our multiple-
beam system (Zhang and Clark 2008a). This was designed to help keep likely partial
words in the agenda and improve the accuracy, and does not give our system a speed
advantage over our multiple-beam system.
4.3 Comparison with Multiple-Beam Search (Zhang and Clark 2008a)
Our system of Zhang and Clark (2008a) was based on the perceptron and a multiple-
beam decoder. That decoder can be seen as a slower alternative of our decoder in this ar-
ticle, but one which explores a larger part of the search space. The comparison between
our joint segmentation and tagging systems of Zhang and Clark (2008a) and this article
is similar to the comparison between our segmentors in sections 3.5.1 and 3.1. In Zhang
and Clark (2008a), we argued that the straightforward implementation of the single-
beam decoder cannot give competitive accuracies to the multiple-beam decoder, and
the main difficulties for a single-beam decoder are in the representing of partial words,
and the handling of an exponentially large combined search space using one beam. In
this section, we give a description of our system of Zhang and Clark (2008a), and discuss
the reason we can achieve competitive accuracies using a single beam in this article.
4.3.1 The Multiple-Beam System of Zhang and Clark (2008a). The decoder of our multiple-
beam system can be formulated as an instance of the multiple-beam decoder described
in Section 3.5.1.
Similar to the multiple-beam search decoder for word segmentation, the decoder
compares candidates only with complete tagged words, and enables the size of the
search space to scale with the input size. A set of state items is kept for each character
to record possible segmented and POS-tagged sentences ending with the character.
Just as with the single-beam decoder, the input sentence is processed incrementally.
However, when a character is processed, the number of previously built state items is
increased from B to kB, where B is the beam-size and k is the number of characters that
have been processed. Moreover, partial candidates ending with any previous character
are available. The decoder thus generates all possible tagged words ending with the cur-
rent character, concatenating each with existing partial sentences ending immediately
before the word, and putting the resulting sentence onto the agenda. After the character
is processed, the B-best items in the agenda are kept as the corresponding state items
for the character, and the agenda is cleared for the next character. All input characters
are processed in the same way, and the final output is the best state item for the last
character.
To instantiate the generic multiple-beam algorithm in Figure 4 for joint segmenta-
tion and POS-tagging, STARTITEM( joint tagging) consists of an empty sentence S and
a queue Q containing the full input sentence; EXPAND(candidate, k, joint tagging) pops
characters on the incoming queue from the front until the kth character (k is the index
in the input sentence rather than the queue itself), appending them as a new word
to candidate, and assigning to the new word all possible POS-tags to generate a set of
new items; and GOALTEST(joint tagging, best) returns true if best consists of a complete
segmented and POS-tagged sentence and an empty incoming queue.
The linear model from Section 2 is again applied directly to score state items, and
the model parameters are trained with the averaged perceptron algorithm. The features
123
Computational Linguistics Volume 37, Number 1
for a state item are extracted according to the union of the feature templates for the
baseline segmentor and the baseline POS-tagger.
4.3.2 Discussion. An important problem that we solve for a single-beam decoder for the
global model is the handling of partial words. As we pointed out in Zhang and Clark
(2008a), it is very difficult to score partial words properly when they are compared with
full words, although such comparison is necessary for incremental decoding with a
single-beam. To allow comparisons with full words, partial words can either be treated
as full words, or handled differently.
We showed in Zhang and Clark (2008a) that a naive single-beam decoder which
treats partial words in the same way as full words failed to give a competitive ac-
curacy. An important reason for the low accuracy is over-segmentation during beam-
search. Consider the three characters (tap water). The first two characters do not
make sense when put together as a single word. Rather, when treated as two single-
character words, they can make sense in a sentence such as (please) (self) (come)
(take). Therefore, when using single-beam search to process (tap water), the
two-character word candidate is likely to have been thrown off the agenda before
the third character is considered, leading to an unrecoverable segmentation error.
This problem is even more severe for a joint segmentor and POS-tagger than for
a pure word segmentor, because the POS-tags and POS-tag bigram of and fur-
ther supports them being separated when is considered. The multiple-beam search
decoder we proposed in Zhang and Clark (2008a) can be seen as a means to ensure
that the three characters always have a chance to be considered as a single
word. It explores candidate segmentations from the beginning of the sentence until each
character, and avoids the problem of processing partial words by considering only full
words. However, because it explores a larger part of the search space than a single-beam
decoder, its time complexity is correspondingly higher.
In our single-beam system, we treat partial words differently from full words,
so that in the previous example, the decoder can take the first two characters in
(tap water) as a partial word, and keep it in the beam before the third character is
processed. One challenge is the representation of POS-tags for partial words. The POS of
a partial word is undefined without the corresponding full word information. Though
a partial word can make sense with a particular POS-tag when it is treated as a complete
word, this POS-tag is not necessarily the POS of the full word which contains the partial
word. Take the three-character sequence as an example. The first character
represents a single-character word ?below?, for which the POS can be LC or VV. The
first two characters represent a two-character word ?rain?, for which the POS can
be VV. Moreover, all three characters when put together make the word ?rainy day?, for
which the POS is NN. As discussed earlier, assigning POS tags to partial words as if they
were full words leads to low accuracy.
An obvious solution to this problem is not to assign a POS to a partial word until it
becomes a full word. However, lack of POS information for partial words makes them
less competitive compared to full words in the beam, because the scores of full words
are further supported by POS and POS n-gram information. Therefore, not assigning POS
to partial words potentially leads to over segmentation. In our experiments, this method
did not give comparable accuracies to our multiple-beam system.
We take a different approach, and assign a POS-tag to a partial word when its
first character is separated from the final character of the previous word. When more
characters are appended to a partial word, the POS is not changed. The idea is to use
the POS of a partial word as the predicted POS of the full word it will become. Possible
124
Zhang and Clark Syntactic Processing
predictions are made with the first character of the word, and the likely ones will be
kept in the beam for the next processing steps. For example, with the three characters
, we try to keep two partial words (besides full words) in the beam when the
first word is processed, with the POS being VV and NN, respectively. The first POS
predicts the two-character word , and the second the three-character word .
Now when the second character is processed, we still need to maintain the possible POS
NN in the agenda, which predicts the three-character word .
We show that the mechanism of predicting the POS at the first character gives
competitive accuracy. This mechanism can be justified theoretically. Unlike alphabetical
languages, each Chinese character represents some specific meanings. Given a character,
it is natural for a human speaker to know immediately what types of words it can start.
This allows the knowledge of possible POS-tags of words that a character can start, using
information about the character from the training data. Moreover, the POS of the previ-
ous words to the current word are also useful in deciding possible POS for the word.3
The mechanism of first-character decision of POS also boosts the efficiency, because
the enumeration of POS is unnecessary when a character is appended to the end of an
existing word. As a result, the complexity of each processing step is reduced by half
compared to a method without POS prediction.
Finally, an intuitive way to represent the status of a partial word is using a flag
explicitly, which means an early decision of the segmentation of the next incoming
character. We take a simpler alternative approach, and treat every word as a partial
word until the next incoming character is separated from the last character of this word.
Before a word is confirmed as a full word, we only apply to it features that represent its
current partial status, such as character bigrams, its starting character, its part-of-speech,
and so forth. Full word features, including the first and last characters of a word, are
applied immediately after a word is confirmed as complete.
An important component for our proposed system is the training process, which
needs to ensure that the model scores a partial word with predicted POS properly. We
apply the general framework and use the averaged perceptron for training, together
with the ?early update? mechanism.
4.4 Experiments
We performed two sets of experiments, using the Chinese Treebank 4 and 5, respectively.
In the first set of experiments, CTB4 was separated into two parts: CTB3 (420K characters
in 150K words/10, 364 sentences) was used for the final 10-fold cross validation, and
the rest (240K characters in 150K words/4, 798 sentences) was used as training and test
data for development. The second set of experiments was performed to compare with
relevant systems on CTB5 data.
The standard F-scores are used to measure both the word segmentation accuracy
and the overall segmentation and tagging accuracy, where the overall accuracy is
JF = 2pr/(p + r)
with the precision p being the percentage of correctly segmented and tagged words
in the decoder output, and the recall r being the percentage of gold-standard tagged
3 The next incoming characters are also a useful source of information for predicting the POS. However, our
system achieved competitive accuracy compared to our multiple-beam system without such character
lookahead features.
125
Computational Linguistics Volume 37, Number 1
words that are correctly identified by the decoder. For direct comparison with Ng and
Low (2004), the POS-tagging accuracy is also calculated by the percentage of correct tags
on each character.
4.4.1 Development Experiments. Our development data consists of 150K words in 4, 798
sentences. Eighty percent (80%) of the data were randomly chosen as the development
training data, and the rest were used as the development test data. Our development
tests were mainly used to decide the size of the beam, the number of training iterations,
and to observe the effect of early update.
Figure 6 shows the accuracy curves for joint segmentation and POS-tagging by the
number of training iterations, using different beam sizes. With the size of the beam
increasing from 1 to 32, the accuracies generally increase, although the amount of
increase becomes small when the size of the beam becomes 16. After the tenth iteration,
a beam size of 32 does not always give better accuracies than a beam size of 16. We
therefore chose 16 as the size of the beam for our system.
The testing times for each beam size between 1 and 32 are 7.16 sec, 11.90 sec,
18.42 sec, 27.82 sec, 46.77 sec, and 89.21 sec, respectively. The corresponding speeds
in the number of sentences per second are 111.45, 67.06, 43.32, 28.68, 17.06 and 8.95,
respectively.
Figure 6 also shows that the accuracy increases with an increased number of train-
ing iterations, but the amount of increase becomes small after the 25th iteration. We
chose 29 as the number of iterations to train our system.
The effect of early update: We compare the accuracies by early update and normal
perceptron training. In the normal perceptron training case, the system reached the best
performance at the 22nd iteration, with a segmentation F-score of 90.58% and joint
Figure 6
The influence of beam-sizes, and the convergence of the perceptron for the joint segmentor and
POS-tagger.
126
Zhang and Clark Syntactic Processing
Table 8
The accuracies of joint segmentation and POS-tagging by 10-fold cross validation.
Baseline (pipeline) Joint single-beam Joint multiple-beam
# SF JF JA SF JF JA SF JF JA
Av. 95.2 90.3 92.2 95.8 91.4 93.0 95.9 91.3 93.0
SF = segmentation F-score; JF = overall segmentation and POS-tagging F-score; JA = tagging
accuracy by character.
F-score of 83.38%. When using early update, the algorithm reached the best accuracy
at the 30th training iteration, obtaining a segmentation F-score of 91.14% and a joint
F-score of 84.06%.
4.4.2 Cross-Validation Results. Ten-fold cross validation is performed to test the accuracy
of the joint word segmentor and POS-tagger, and to make comparisons with existing
models in the literature. Following Ng and Low (2004), we partition the sentences in
CTB3, ordered by sentence ID, into 10 groups evenly. In the nth test, the nth group is
used as the testing data.
Table 8 shows the cross-validation accuracies of the pipeline baseline system and
the joint system using single and multiple beam decoders. SF, JF and JA represent
segmentation F-score, tagging F-score, and tagging accuracy, respectively. The joint
segmentor and tagger systems outperformed the baseline consistently, while the single
beam-search decoder in this article gave comparable accuracies to our multiple-beam
algorithm of Zhang and Clark (2008a).
Speed comparisons of the three systems using the same 10-fold cross-validation are
shown in Table 9. TE, ML, and SP represents the testing time (seconds), model loading
time (seconds), and speed (number of sentences per second), respectively. Speed is cal-
culated as number of test sentences divided by the test time (excluding model loading).
For the baseline system, test time and model loading time for both the segmentor and
the POS-tagger are recorded. The joint system using a single beam decoder was over
10 times faster than the multiple-beam system, and the baseline system was more than
three times as fast as the single-beam joint system. These tests were performed on a Mac
OSX platform with a 2.13GHz CPU and a gcc 4.0.1 compiler.
Table 10 shows the overall accuracies of the baseline and joint systems, and com-
pares them to two relevant models in the literature. The accuracy of each model is
Table 9
The speeds of joint word segmentation and POS-tagging by 10-fold cross validation.
Baseline (pipeline) Joint single-beam Joint multiple-beam
# TE (s+p=total) ML (s+p=total) SP TE ML SP TE ML SP
Av. 8.8+10.4=19.2 2.9+3.7=6.6 82.2 58.6 12.1 22.4 575.0 9.5 1.9
TE = testing time (seconds); ML = model loading time (seconds); SP = speed by the number of
sentences per second, excluding loading time; (s) = segmentation in baseline; (p) = POS-tagging in
baseline.
127
Computational Linguistics Volume 37, Number 1
Table 10
The comparison of overall accuracies of various joint segmentor and POS-taggers by 10-fold
cross validation using CTB.
Model SF JF JA
Baseline+ (Ng) 95.1 ? 91.7
Joint+ (Ng) 95.2 ? 91.9
Baseline+* (Shi) 95.85 91.67 ?
Joint+* (Shi) 96.05 91.86 ?
Baseline (ours) 95.20 90.33 92.17
Joint (our multiple-beam) 95.90 91.34 93.02
Joint (our single-beam) 95.84 91.37 93.01
SF = segmentation F-score; JF = joint segmentation and POS-tagging F-score; JA = tagging accuracy
by character.
+ Knowledge about special characters.
* Knowledge from semantic net outside CTB.
shown in a row, where Ng represents the models from Ng and Low (2004), which
applies a character tagging approach to perform word segmentation and POS-tagging
simultaneously, and Shi represents the models from Shi and Wang (2007), which is a
reranking system for segmentation and POS-tagging. These two models are described
in more detail in the related work section. Each accuracy measure is shown in a col-
umn, including the segmentation F-score (SF), the overall tagging F-score ( JF), and the
tagging accuracy by character ( JA). As can be seen from the table, our joint models
achieved the largest improvement over the baseline, reducing the segmentation error
by more than 14% and the overall tagging error by over 12%.
The overall tagging accuracy of our joint model was comparable to but less than
the joint model of Shi and Wang (2007). Despite the higher accuracy improvement
from the baseline, the joint system did not give higher overall accuracy. One possi-
ble reason is that Shi and Wang (2007) included knowledge about special characters
and semantic knowledge from Web corpora (which may explain the higher baseline
accuracy), whereas our system is completely data-driven. However, the comparison is
indirect because our partitions of the CTB corpus are different. Shi and Wang (2007) also
chunked the sentences before doing 10-fold cross validation, but used an uneven split.
We chose to follow Ng and Low (2004) and split the sentences evenly to facilitate further
comparison.
Compared with Ng and Low (2004), our baseline model gave slightly better accu-
racy, consistent with our previous observations about the word segmentors in Section 3.
Due to the large accuracy gain from the baseline, our joint model performed much
better.
In summary, when compared with existing joint word segmentation and POS-
tagging systems using cross-validation tests, our proposed model achieved the best
accuracy boost from the pipelined baseline, and competitive overall accuracy. Our sys-
tem based on the general framework of this article gave comparable accuracies to our
multiple-beam system in Zhang and Clark (2008a), and a speed that is over an order of
magnitude higher than the multiple-beam algorithm.
4.4.3 Test Results Using CTB5. We follow Kruengkrai et al (2009) and split the CTB5 into
training, development testing, and testing sets, as shown in Table 11. The data are used
128
Zhang and Clark Syntactic Processing
Table 11
Training, development, and test data from CTB5 for joint word segmentation and POS-tagging.
Sections Sentences Words
Training 1?270, 400?931, 1001?1151 18,085 493,892
Dev 301?325 350 6,821
Test 271?300 348 8,008
to compare the accuracies of our joint system with models in the literature, and to draw
the speed/accuracy tradeoff graph.
Kruengkrai et al (2009) made use of character type knowledge for spaces, numerals,
symbols, alphabets, and Chinese and other characters. In the previous experiments, our
system did not use any knowledge beyond the training data. To make the comparison
fairer, we included knowledge of English letters and Arabic numbers in this experiment.
During both training and decoding, English letters and Arabic numbers are segmented
using rules, treating consecutive English letters or Arabic numbers as a single word.
The results are shown in Table 12, where row N07 refers to the model of Nakagawa
and Uchimoto (2007), rows J08a and J08b refer to the models of Jiang et al (2008) and
Jiang, Mi, and Liu (2008), and row K09 refers to the models of Kruengkrai et al (2009).
Columns SF and JF refer to segmentation and joint segmentation and tagging accuracies,
respectively. Our system gave comparable accuracies to these recent works, obtaining
the best (same as the error-driven version of K09) joint F-score.
The accuracy/speed tradeoff graphs for the joint segmentor and POS-taggers, to-
gether with the baseline pipeline system, are shown in Figure 7. For each point in each
curve, the development test data were used to decide the number of training iterations,
and then the speed and accuracy were measured using test data. No character knowl-
edge is used in any system. The baseline curve was drawn with B = 16 for the baseline
segmentor, because the baseline segmentation accuracy did not improve beyond B = 16
in our experiments. Each point in this curve corresponds to a different beam size of
the baseline POS-tagger, which are 2, 4, 8, 16, 32, 64, 128, and 256, respectively, from
right to left.
When the speed is over 2.5 thousand characters per second, the baseline system
performed better than the joint single-beam and multiple-beam systems, due to the
higher segmentation accuracy brought by the fixed beam segmentor. However, as the
Table 12
Accuracy comparisons between various joint segmentors and POS-taggers on CTB5.
SF JF
K09 (error-driven) 97.87 93.67
our system 97.78 93.67
Zhang 2008 97.82 93.62
K09 (baseline) 97.79 93.60
J08a 97.85 93.41
J08b 97.74 93.37
N07 97.83 93.32
SF = segmentation F-score; JF = joint segmentation and POS-tagging F-score.
129
Computational Linguistics Volume 37, Number 1
Figure 7
The accuracy/speed tradeoff graph for the joint segmentor and POS-taggers and the two-stage
baseline.
tagger beam size further increased, the accuracy of the baseline system did not improve.
The highest F-score of the baseline (92.83%) was achieved when the beam size of the
baseline POS-tagger was 32. In fact, we have shown in Section 3.5 that the accuracy
of the baseline segmentor was similar to using a Viterbi decoder when the beam size
was 16. This was also true for the baseline POS-tagger, according to our experiments.
Therefore, though being the most accurate when the speed is high, the baseline system
reaches the highest F-score at 2.63 thousand characters per second, and cannot further
improve the accuracy on this curve.
The points in the single-beam curve correspond to beam sizes of 2, 4, 8, 16, and 32,
respectively, from right to left. When the speed is roughly between 0.5 and 2.0 thou-
sand characters per second, the single-beam system gave the best F-score. This is
because the multiple-beam system did not reach such high speeds, and the baseline
system could not produce a higher accuracy than 92.83%. The single-beam joint system
gave the highest accuracy of 93.50% when the beam size was 16 and the speed was
1.01 thousand characters per second, and the F-score dropped slightly when the beam
further increased to 32.
The points in the multiple-beam curve correspond to beam sizes of 1, 2, 4, 8, and 16,
respectively, from right to left. The multiple-beam system gave the highest F-score of
93.62% when the beam size was 16, but the speed was down to 0.06 thousand sentences
per second.
4.5 Related Work
Ng and Low (2004) mapped the joint segmentation and POS-tagging task into a
single character sequence tagging problem. Two types of tags are assigned to each
character to represent its segmentation and POS. For example, the tag b NN indicates a
character at the beginning of a noun, and the tag e VV indicates a character at the end
of a verb. Using this method, POS features are allowed to interact with segmentation.
Because tagging is restricted to characters, the search space is reduced to O((4T)n),
where 4 is the number of segmentation tags and T is the size of the tag set. Beam-search
decoding is effective with a small beam-size. However, the disadvantage of this
model is the difficulty of incorporating whole word information into POS-tagging. For
example, the standard word + POS-tag feature is not applicable.
130
Zhang and Clark Syntactic Processing
Shi and Wang (2007) introduced POS information into segmentation by reranking.
B-best segmentation outputs are passed to a separately-trained POS-tagger, and the best
output is selected using the overall POS-segmentation probability score. In this system,
the decoding for word segmentation and POS-tagging are still performed separately, and
exact inference for both is possible. However, the interaction between POS and segmen-
tation is restricted by reranking: POS information is used to improve segmentation only
for the B segmentor outputs. In comparison to the two systems described here, our joint
system does not impose any hard constraints on the interaction between segmentation
and POS information.
Jiang, Mi, and Liu (2008) proposed a reranking system that builds a pruned word
lattice instead of generating a B-best list by the segmentor. The advantage of this
reranking method compared to Shi and Wang?s (2007) method is that more ambiguity is
kept in the reranking stage. The reranking algorithm used a similar model to our joint
segmentor and POS-tagger.
Nakagawa and Uchimoto (2007) proposed a hybrid model for word segmentation
and POS tagging using an HMM-based approach. Word information is used to process
known words, and character information is used for unknown words in a similar
way to Ng and Low (2004). In comparison, our model handles character and word
information simultaneously in a single perceptron model. Recently, Kruengkrai et al
(2009) developed this hybrid model further by scoring characters and words in the same
model. Their idea is similar to our joint segmentor and POS-tagger in Zhang and Clark
(2008a).
5. Dependency Parsing
Dependency parsing is the problem of producing the syntactic structure for an input
sentence according to dependency grammar. The output structure of a dependency
parser is called a dependency graph; in this article, only dependency trees are con-
sidered. As can be seen from Figure 8, a dependency tree consists of a set of vertices
and directed arcs. Each arc represents the relationship between a pair of words in the
sentence; it points from the head word to its modifier. For example, in the arc between
the word (I) and the word (like), (like) is the head word and (I) is the
subject that modifies (like); in the arc between the word (like) and the word
(reading), (like) is the head word and (reading) is the object that modifies
(like). In a dependency tree, there is only one word that does not modify any other
word, and it is called the head word of the sentence. The other words each modify
exactly one word, and no word can modify itself.
A dependency tree is called projective if there are no crossing arcs when the
sentence is represented linearly, in word order. Though almost all languages are non-
projective to some degree, the majority of sentences in most languages are projective. In
Figure 8
An example Chinese dependency tree.
131
Computational Linguistics Volume 37, Number 1
the CoNLL shared tasks on dependency parsing (Buchholz and Marsi 2006; Nivre et al
2007) most data sets contain only 1?2% non-projective arcs, and projective dependency
parsing models can give reasonable accuracy in these tasks (Carreras, Surdeanu, and
Marquez 2006). Although non-projective dependency parsing can be solved directly by
using a different model from projective dependency parsing (McDonald et al 2005), it
can also be solved using a projective parsing model, with the help of a reversible trans-
formation procedure between non-projective and projective dependency trees (Nivre
et al 2006). In this article we focus on projective dependency parsing.
An unlabeled dependency tree is a dependency tree without dependency labels
such as Subj and Obj in Figure 8. The same techniques used by unlabeled dependency
parsers can be applied to labeled dependency parsing. For example, a shift-reduce
unlabeled dependency parser can be extended to perform labeled dependency parsing
by splitting a single reduce action into a set of reduce actions each associated with a
dependency label. Here we focus on unlabeled dependency parsing.
Graph-based (McDonald, Crammer, and Pereira 2005; Carreras, Surdeanu, and
Marquez 2006; McDonald and Pereira 2006) and transition-based (Yamada and
Matsumoto 2003; Nivre et al 2006) parsing algorithms offer two different approaches
to data-driven dependency parsing. Given an input sentence, a graph-based algorithm
finds the highest scoring parse tree from all possible outputs, scoring each complete
tree, while a transition-based algorithm builds a parse by a sequence of actions, scoring
each action individually. Although graph-based and transition-based parsers can be
differentiated in various ways, we prefer to think in terms of the features used in the
two approaches as the differentiating factor.
In Zhang and Clark (2008b) we proposed a graph-based parser and a transition-
based parser using the generalized perceptron and beam-search, showing that beam-
search is a competitive choice for both graph-based and transition-based dependency
parsing. In the same paper we used a single discriminative model to combine graph-
based and transition-based parsing, showing that the combined parser outperforms
both graph-based and transition-based parsers individually. All three parsers can be
expressed by our general framework. Here we describe the transition-based and com-
bined parsers, which share the same decoding process.
5.1 Instantiating the General Framework
Our dependency parser uses the incremental parsing process of MaltParser (Nivre
et al 2006), which is characterized by the use of a stack and four transition actions:
SHIFT, ARCRIGHT, ARCLEFT, and REDUCE. An input sentence is formed into a queue
of incoming words and processed from left to right. Initially empty, the stack is used
throughout the parsing process to store unfinished words, which are the words before
the current word that may still be linked with the current or a future word.
The SHIFT action pops the first word from the queue and pushes it onto the stack.
The ARCRIGHT action pops the first word from the queue, adds a dependency link
from the stack top to the word (i.e., the stack top becomes the parent of the word), and
pushes the word onto the stack. The ARCLEFT action adds a dependency link from the
first word on the queue to the stack top, and pops the stack. The REDUCE action pops
the stack. Among the four transition actions, SHIFT and ARCRIGHT push a word onto
the stack, and ARCLEFT and REDUCE pop the stack; SHIFT and ARCRIGHT read the next
input word, and ARCLEFT and ARCRIGHT add a link to the output.
The initial state contains an empty stack and the whole input sentence as incoming
words. The final state contains a stack holding only the head word of the sentence and
132
Zhang and Clark Syntactic Processing
an empty queue, with the input words having all been processed. The incremental pars-
ing process starts from the initial state, and builds a candidate parse tree by repeatedly
applying one transition action out of the four, until the final state is reached.
For the decoding problem, an agenda is used to find the output parse tree from
different possible candidates. Starting with an initial state item, a set of candidate
state items is used to generate new state items for each step. At each step, each existing
state item is extended by applying all applicable actions from the four, and the generated
items are put onto the agenda. After each step, the best B items are taken from the
agenda to generate new state items for the next step, and the agenda is cleared. After all
incoming words have been processed, the corresponding parse of the top item from the
agenda is taken as the output.
The decoding process is an instance of the generalized algorithm in Figure 1. A state
item is a pair ?S, Q?, where S represents the stack with the partial parse, and Q represents
the queue of incoming words. The initial state item STARTITEM(dependency parsing)
consists of an empty stack, and an incoming queue of the whole input. The function
EXPAND(candidate, dependency parsing) applies all possible actions to candidate and gen-
erates a set of new state items. GOALTEST(dependency parsing, best) returns true if best
has reached the final state, and false otherwise.
The score of a parse tree is computed by the global linear model in Equation (2),
where the parameter vector ~w for the model is computed by the averaged perceptron
training algorithm described in Section 2.2. During training of the dependency parser,
the early-update strategy of Collins and Roark (2004) is used. The intuition is to improve
learning by avoiding irrelevant information, as discussed earlier: When all the items in
the current agenda are incorrect, further parsing steps will be irrelevant because the
correct partial output no longer exists in the candidate ranking.
Both the transition-based and the combined parsers use this training and decod-
ing framework. The main difference between the two parsers is in the definition of
the feature templates. Whereas the transition-based parser uses only transition-based
features, the combined parser applies features from the graph-based parser in addition
to transition-based features. Table 13 shows the templates for transition-based features.
Individual features for a parse are extracted from each transition action that is used to
build the parse, by first instantiating the templates according to the local context, and
then pairing the instantiated template with the transition action. Shown in Figure 9, the
contextual information consists of the top of the stack (ST), the parent (STP) of ST, the
leftmost (STLC) and rightmost child (STRC) of ST, the current word (N0), the next three
Table 13
Transition-based feature templates for the dependency parser.
1 stack top STwt; STw; STt
2 current word N0wt; N0w; N0t
3 next word N1wt; N1w; N1t
4 ST and N0 STwtN0wt; STwtN0w; STwN0wt; STwtN0t; STtN0wt; STwN0w; STtN0t
5 POS bigram N0tN1t
6 POS trigrams N0tN1tN2t; STtN0tN1t; STPtSTtN0t;
STtSTLCtN0t; STtSTRCtN0t; STtN0tN0LCt
7 N0 word N0wN1tN2t; STtN0wN1t; STPtSTtN0w;
STtSTLCtN0w; STtSTRCtN0w; STtN0wN0LCt
w = word; t = POS-tag.
133
Computational Linguistics Volume 37, Number 1
Figure 9
Transition-based feature context for the dependency parser.
words from the input (N1, N2, N3), and the leftmost child of N0 (N0LC). Word and POS
information from the context are manually combined, and the combination of feature
templates is decided by development tests.
Table 14 shows the templates used to extract graph-based features from partial
parses. Templates 1?6 are taken from MSTParser (McDonald and Pereira 2006), a
second-order graph-based parser. They are defined in the context of a word, its parent
and its sibling. To give more templates, features from templates 1?5 are also conjoined
with the arc direction and distance, whereas features from template 6 are also conjoined
with the direction and distance between the child and its sibling. Here ?distance? refers
to the difference between word indexes. Templates 7?8 are two extra feature templates
that capture information about grandchildren and arity (i.e., the number of children
to the left or right). They are difficult to include in an efficient dynamic-programming
decoder, but easy to include in a beam-search decoder. During decoding, the graph-
based feature templates are instantiated at the earliest possible situation. For example,
the first-order arc and second-order sibling feature templates are instantiated when
ARCLEFT or ARCRIGHT is performed, with sibling information for the newly added
link. The arity features are added as soon as the left or right arity of a word becomes
fixed, the left arity templates being instantiated when ARCLEFT or SHIFT is performed,
with the right arity templates being instantiated when ARCLEFT or REDUCE is per-
formed. Similarly, the leftmost grandchild features are instantiated when ARCLEFT or
ARCRIGHT is performed, and the rightmost grandchild features are instantiated when
ARCLEFT or REDUCE is performed.
Table 14
Graph-based feature templates for the dependency parser.
1 Parent word (P) Pw; Pt; Pwt
2 Child word (C) Cw; Ct; Cwt
3 P and C PwtCwt; PwtCw; PwCwt; PwtCt; PtCwt; PwCw; PtCt
4 Tag Bt between P, C PtBtCt
5 Neighbor words PtPLtCtCLt; PtPLtCtCRt; PtPRtCtCLt; PtPRtCtCRt;
of P and C, left (L) PtPLtCLt; PtPLtCRt; PtPRtCLt; PtPRtCRt; PLtCtCLt; PLtCtCRt;
and right (R) PRtCtCLt; PRtCtCRt; PtCtCLt; PtCtCRt; PtPLtCt; PtPRtCt
6 sibling (S) of C CwSw; CtSt; CwSt; CtSw; PtCtSt
7 leftmost (CLC) and PtCtCLCt; PtCtCRCt
rightmost (CRC)
children of C
8 left (la) and right (ra) Ptla; Ptra; Pwtla; Pwtra
arity of P
w = word; t = POS-tag.
134
Zhang and Clark Syntactic Processing
5.1.1 The Comparability of State Items. Our dependency parsers are based on the incre-
mental shift-reduce parsing process. During decoding, state items built with the same
number of transition actions are put onto the agenda and compared with each other. To
build any full parse tree, each word in the input sentence must be put onto the stack
once, and each word except the root of the sentence must be popped off the stack
once. Because the four transition actions are either stack-pushing or stack-popping,
each full parse must be built with 2n ? 1 transition actions, where n denotes the size
of the input. Therefore, the decoding process consists of 2n ? 1 steps, and in each step
all state items in the agenda have been built using the same number of actions. Our
experiments showed that start-of-the-art accuracy can be achieved by this intuitive
method of candidate comparison.
5.2 Experiments for English
We used Penn Treebank 3 for our experiments, which was separated into the training,
development, and test sets in the same way as McDonald, Crammer, and Pereira
(2005), shown in Table 15. Bracketed sentences from the Treebank were translated
into dependency structures using the head-finding rules from Yamada and Matsumoto
(2003).
Before parsing, POS-tags are assigned to the input sentence using our baseline
POS-tagger of Zhang and Clark (2008a), which can be seen as the perceptron tagger
of Collins (2002) with beam-search. Like McDonald, Crammer, and Pereira (2005), we
evaluated the parsing accuracy by the precision of lexical heads (the percentage of input
words, excluding punctuation, that have been assigned the correct parent) and by the
percentage of complete matches, in which all words excluding punctuation have been
assigned the correct parent.
A set of development tests, including the convergence of the perceptron, can be
found in Zhang and Clark (2008a). In this article, we report only the final test accu-
racies and a set of additional speed/accuracy tradeoff results. The accuracies of our
transition-based and combined parsers on English data are shown together with other
systems in Table 16. In the table, each row represents a parsing model. Rows Yamada
2003 and MSTParser represent Yamada and Matsumoto (2003), and MSTParser with
templates 1?6 from Table 14 (McDonald and Pereira 2006), respectively. Rows Transition
and Combined represent our pure transition-based and combined parsers, respectively.
Row Huang 2010 shows the recent work of Huang and Sagae (2010), which applies
dynamic-programming packing to transition-based dependency parsing. Rows Koo
2008 and Chen 2009 represent the models of Koo, Carreras, and Collins (2008) and
Chen et al (2009), which perform semi-supervised learning by word-clustering and
self-training, respectively. Columns Word and Complete show the precision of lexical
Table 15
The training, development, and test data for English dependency parsing.
Sections Sentences Words
Training 2?21 39,832 950,028
Development 22 1,700 40,117
Test 23 2,416 56,684
135
Computational Linguistics Volume 37, Number 1
Table 16
Accuracy comparisons between various dependency parsers on English data.
Word Complete
Yamada 2003 90.3 38.4
Transition 91.4 41.8
MSTParser 91.5 42.1
Combined 92.1 45.4
Huang 2010 92.1 ?
Koo 2008 93.2 ?
Chen 2009 93.2 47.2
See text for details.
Figure 10
The accuracy/speed tradeoff graph for the transition-based and combined dependency parsers.
heads and complete matches, respectively. The combined parser achieved 92.1% per-
word accuracy, which was significantly higher than the pure transition-based parser.4
These results showed the effectiveness of utilizing both graph-based and transition-
based information in a single model. Represented by features that are not available in a
pure transition-based system, graph-based information helped the combined parser to
achieve higher accuracy.
As in the previous sections, we plot the speed/accuracy tradeoff for the transition-
based and combined parsers. For each point in each curve in Figure 10, we run the
development test to decide the number of training iterations, and read the speed and
accuracy from the final test. Each point in the transition curve corresponds to B = 1, 2, 4,
8, 16, 32, 64, and 128, respectively, and each point in the combined curve corresponds to
B = 1, 2, 4, 8, 16, 32, and 64, respectively. When the speed was above 100 sentences per
second, the pure transition-based parser outperformed the combined parser with the
same speed. However, as the size of the beam increases, the accuracy of the combined
parser increased more rapidly. The combined parser gave higher accuracies at the same
speed when the speed was below 50 sentences per second. The accuracy of the pure
4 In Zhang and Clark (2008a) we showed that the combined parser also significantly outperformed the
graph-based parser.
136
Zhang and Clark Syntactic Processing
Table 17
Training, development, and test data for Chinese dependency parsing.
Sections Sentences Words
Training 001?815 16,118 437,859
1,001?1,136
Dev 886?931 804 20,453
1,148?1,151
Test 816?885 1,915 50,319
1,137?1,147
transition parser did not increase beyond B = 64. Our experiments were performed on
a Linux platform with a 2.0GHz CPU and a gcc 4.0.1 compiler.
When B = 1, the transition-based parser was similar to MaltParser, because our
transition-based parser is built upon the arc-each transition process of MaltParser,
and uses similar feature sets. The main difference is that our transition-based parser
performs global training using the perceptron algorithm, whereas MaltParser performs
local training using a support vector machine (SVM) with a polynomial kernel. Because
global training optimizes the model for full sequences of transition actions, a small beam
can potentially have a negative impact on learning, and hence the overall performance.
5.3 Experiments for Chinese
We used the Penn Chinese Treebank 5 for our experiments. Following Duan, Zhao, and
Xu (2007), we split the corpus into training, development, and test data as shown in
Table 17. We used a set of head-finding rules to turn the bracketed sentences into depen-
dency structures, and they can be found in Zhang and Clark (2008a). Like Duan, Zhao,
and Xu (2007), we used gold-standard POS-tags for the input. The parsing accuracy
was evaluated by the percentage of non-root words that have been assigned the correct
head, the percentage of correctly identified root words, and the percentage of complete
matches, all excluding punctuation.
The accuracies are shown in Table 18. Rows Transition and Combined show our
models in the same way as for the English experiments from Section 5.2. Row Duan 2007
represents the transition-based model from Duan, Zhao, and Xu (2007), which applied
beam-search to the deterministic model from Yamada and Matsumoto (2003). Row
Table 18
Test accuracies of various dependency parsers on CTB5 data.
Non-root Root Complete
Duan 2007 84.36 73.70 32.70
Transition 84.69 76.73 32.79
Huang 2010 85.52 78.32 33.72
Combined 86.21 76.26 34.41
See text for details.
137
Computational Linguistics Volume 37, Number 1
Huang 2010 represents the model of Huang and Sagae (2010), which applies dynamic-
programming packing to transition-based parsing. The observations were similar to the
English tests. The combined parser outperformed the transition-based parsers. It gave
the best accuracy we are aware of for supervised dependency parsing using the CTB.
One last question we investigate for this article is the overall performance when the
parser is pipelined with a POS-tagger, or with the joint segmentation and POS-tagging
algorithm in Section 4, forming a complete pipeline for Chinese inputs. The results are
shown in Table 19. For these experiments, we tune the pipelined POS-tagger and the
joint segmentor and POS-tagger on the CTB5 corpus in Table 17, using the development
test data to decide the number of training iterations and reporting the final test accuracy.
The overall accuracy is calculated in F-score: Defining nc as the number of output words
that have been correctly segmented and assigned the correctly segmented head word,
no as the number of words in the output, and nr the number of words in the reference,
precision is p = nc/no and recall is r = nc/nr. When pipelined with a pure POS-tagger
using gold-standard segmentation, the pipelined system gave 93.89% POS accuracy
and 81.21% joint tagging and parsing F-score for non-root words. When combined
with the joint segmentation and POS-tagging system, the segmentation F-score, joint
segmentation and POS-tagging F-score were 95.00% and 90.17%, respectively, and the
joint segmentation and parsing F-score for non-root words (excluding punctuations)
was 75.09%, where the corresponding accuracy with gold-standard segmented and POS-
tagged input was 86.21%, as shown in Table 18.
5.4 Related Work
MSTParser (McDonald and Pereira 2006) is a graph-based parser with an exhaustive
search decoder, and MaltParser (Nivre et al 2006) is a transition-based parser with
a greedy search decoder. Representative of each method, MSTParser and MaltParser
gave comparable accuracies in the CoNLL-X shared task (Buchholz and Marsi 2006).
However, they make different types of errors, which can be seen as a reflection of their
theoretical differences (McDonald and Nivre 2007). By combining graph-based and
transition-based information, our dependency parser achieved higher accuracy than
both graph-based and transition-based baselines. The combination of information is
enabled by our general framework. Our global model does not impose any limitation
on the kind of features that can be used, and therefore allows both graph-based
and transition-based features. Moreover, beam-search frees the decoder from locality
restrictions such as the ?optimal sub-problem? requirement for dynamic-programming,
which limits the range of features in order to achieve reasonable decoding speed.
Compared to the greedy local search decoder for MaltParser, beam-search can reduce
error propagation by keeping track of a set of candidate items.
Table 19
The combined segmentation, POS-tagging, and dependency parsing F-scores using different
pipelined systems.
Seg F Tag F Dep F
Gold seg tag 100.00 100.00 86.21
Gold seg auto tag 100.00 93.89 81.21
Auto seg tag 95.00 90.17 75.09
138
Zhang and Clark Syntactic Processing
Our transition-based parser is based on the arc-eager parsing process of MaltParser
(Nivre et al 2006), although our parser is different from MaltParser in two aspects.
First, we applied beam-search in decoding, which helps to prevent error propagation
of local search. Johansson and Nugues (2007) also use beam search. Second, we use the
perceptron to train whole sequences of transition actions globally, whereas MaltParser
uses SVM to train each transition action locally. Our global training corresponds to beam-
search decoding, which searches for a globally optimal sequence of transition actions
rather than an optimal action at each step.
An existing method to combine multiple parsing algorithms is the ensemble ap-
proach (Sagae and Lavie 2006), which was reported to be useful in improving depen-
dency parsing (Hall et al 2007). A more recent approach (Nivre and McDonald 2008)
combined MSTParser and MaltParser by using the output of one parser for features
in the other in a stacking framework. Both Hall et al (2007) and Nivre and McDonald
(2008) can be seen as methods to combine separately defined models. In contrast, our
parser combines two components in a single model, in which all parameters are trained
consistently.
6. Phrase-Structure Parsing
Phrase-structure parsing is the problem of producing the syntactic structure of an
input sentence according to a phrase-structure grammar. An example phrase-structure
parse tree is shown in Figure 11. Similar to dependency parsing, dominant approaches
to phrase-structure parsing include the transition-based method (Briscoe and Carroll
1993), which builds an output parse tree by choosing a series of transition actions such as
SHIFT and REDUCE, and the graph-based method (Collins 1999; Charniak 2000), which
explores the search space of possible parse trees to find the best output according to
graph-based scores.
For English constituent parsing using the Penn Treebank, the best performing
transition-based parser lags behind the current state-of-the-art (Sagae and Lavie 2005).
However, for Chinese constituent parsing using the Chinese Treebank, Wang et al
(2006) have shown that a shift-reduce parser can give competitive accuracy scores to-
gether with high speeds by using an SVM to make a single decision at each point in the
parsing process.
In Zhang and Clark (2009) we proposed a transition-based constituent parser for
Chinese, which is based on the transition process of Wang et al (2006). Rather than
making a single decision at each processing step, our parser uses a global linear model
Figure 11
An example Chinese lexicalized phrase-structure parse tree.
139
Computational Linguistics Volume 37, Number 1
and beam-search decoding, and achieved competitive accuracy. This phrase-structure
parser can be expressed as an instance of our general framework.
6.1 Instantiating the General Framework
The incremental parsing process of our parser is based on the shift-reduce parsers of
Sagae and Lavie (2005) and Wang et al (2006), with slight modifications. The input
is assumed to be segmented and POS-tagged, and the word?POS pairs waiting to be
processed are stored in a queue. A stack holds the partial parse trees that are built during
the parsing process. The output of this process is a binarized parse tree. The four types
of action used to build a parse are:
r SHIFT, which pushes the next word-POS pair in the queue onto the stack.
r REDUCE?unary?X, which makes a new unary-branching node with
label X; the stack is popped and the popped node becomes the child of
the new node; the new node is pushed onto the stack.
r REDUCE?binary?{L/R}?X, which makes a new binary-branching node
with label X; the stack is popped twice, with the first popped node
becoming the right child of the new node and the second popped node
becoming the left child; the new node is pushed onto the stack. The left (L)
and right (R) versions of the rules indicate whether the head of the new
node is to be taken from the left or right child.
r TERMINATE, which pops the root node off the stack and ends parsing. This
action can only be applied when the input queue is empty, and the stack
contains only one item. The popped node is taken as the output.
Defining the start state as the stack being empty and the queue containing the input
sentence, and the final state as the state immediately after a TERMINATE action, the
incremental process builds a parse tree by repeatedly applying actions from the start
state until the final state is reached. Note that not all sequences of actions produce valid
binarized trees. In the deterministic parser of Wang et al (2006), the highest scoring
action predicted by the classifier may prevent a valid binary tree from being built. In
this case, Wang et al simply return a partial parse consisting of all the subtrees on
the stack. In our parser a set of restrictions is applied which guarantees a valid parse
tree. For example, two simple restrictions are that a SHIFT action can only be applied
if the queue of incoming words is non-empty, and the binary reduce actions can only
be performed if the stack contains at least two nodes. Some of the restrictions are more
complex than this; the full set can be found in Zhang and Clark (2009). Wang et al
(2006) give a detailed example showing how a segmented and POS-tagged sentence can
be incrementally processed using the shift-reduce actions to produce a binary tree. We
show this example in Figure 12.
For the decoding problem, our parser performs beam-search using an agenda to
find the output parse from possible candidates. Initially containing a start state item, a
set of state items is used to generate new state items for each processing step. At each
step, each of the state items is extended using all applicable actions, generating a set of
new state items, which are put onto the agenda. After each step, the B-best items from
the agenda are taken for the generation of new state items in the next step. The same
140
Zhang and Clark Syntactic Processing
Figure 12
An example shift-reduce parsing process.
process repeats until the highest scored item from the agenda is in the final state, and it
is taken as the final parse.
This decoding process can be seen as an instance of the generic algorithm in Fig-
ure 1. Here a state item is a pair ?S, Q?, where S represents the stack with partial parses,
and Q represents the incoming queue. The initial state item STARTITEM(constituent
parsing) is the start state item, where the stack is empty and the queue contains the
141
Computational Linguistics Volume 37, Number 1
whole input sentence, the function EXPAND(candidate, constituent parsing) extends can-
didate by using all applicable actions to generate a set of new state items, and
GOALTEST (constituent parsing, best) returns true if best reaches the final state, and false
otherwise.
The score of a parse tree is computed by the global linear model in Equation (2),
where the parameter vector ~w for the model is computed by the averaged perceptron
training algorithm described in Section 2.2. As in the training of the dependency parser,
the early-update strategy of Collins and Roark (2004) is used.
Table 20 shows the set of feature templates for the model. Individual features are
generated from these templates by first instantiating a template with particular labels,
words, and tags, and then pairing the instantiated template with a particular action.
In the table, the symbols S0, S1, S2, and S3 represent the top four nodes on the stack,
and the symbols N0, N1, N2, and N3 represent the first four words in the incoming
queue. S0L, S0R, and S0U represent the left and right child for binary branching S0,
and the single child for unary branching S0, respectively; w represents the lexical head
token for a node; and c represents the label for a node. When the corresponding node
is a terminal, c represents its POS-tag, whereas when the corresponding node is a non-
terminal, c represents its constituent label; t represents the POS-tag for a word.
The context S0, S1, S2, S3, N0, N1, N2, N3 for the feature templates is taken from Wang
et al (2006). However, Wang et al (2006) used a polynomial kernel function with an
SVM and did not manually create feature combinations. Because we used the linear per-
ceptron algorithm we manually combined Unigram features into Bigram and Trigram
features.
The Bracket row shows bracket-related features, which were inspired by Wang et al
(2006). Here brackets refer to left brackets including ??, ?fi?, and ?
? and right brackets
including ?	?, ?fl?, and ??. In the table, b represents the matching status of the last
left bracket (if any) on the stack. It takes three different values: 1 (no matching right
bracket has been pushed onto stack), 2 (a matching right bracket has been pushed
onto stack), and 3 (a matching right bracket has been pushed onto stack, but then
popped off).
Table 20
Feature templates for the phrase-structure parser.
Description Feature templates
Unigrams S0tc, S0wc, S1tc, S1wc, S2tc, S2wc, S3tc, S3wc,
N0wt, N1wt, N2wt, N3wt,
S0lwc, S0rwc, S0uwc, S1lwc, S1rwc, S1uwc,
Bigrams S0wS1w, S0wS1c, S0cS1w, S0cS1c,
S0wN0w, S0wN0t, S0cN0w, S0cN0t,
N0wN1w, N0wN1t, N0tN1w, N0tN1t
S1wN0w, S1wN0t, S1cN0w, S1cN0t,
Trigrams S0cS1cS2c, S0wS1cS2c, S0cS1wS2c, S0cS1cS2w,
S0cS1cN0t, S0wS1cN0t, S0cS1wN0t, S0cS1cN0w
Bracket S0wb, S0cb
S0wS1cb, S0cS1wb, S0cS1cb, S0wN0tb, S0cN0wb, S0cN0tb
Separator S0wp, S0wcp, S0wq, S0wcq, S1wp, S1wcp, S1wq, S1wcq
S0cS1cp, S0cS1cq
w = word; c = constituent label; t = POS-tag.
142
Zhang and Clark Syntactic Processing
Table 21
The standard split of CTB2 data for phrase-structure parsing.
Sections Sentences Words
Training 001?270 3,475 85,058
Development 301?325 355 6,821
Test 271?300 348 8,008
The Separator row shows features that include one of the separator punctuations
(i.e., ??, ??, ??, and ?ff?) between the head words of S0 and S1. These templates apply
only when the stack contains at least two nodes; p represents a separator punctuation
symbol. Each unique separator punctuation between S0 and S1 is only counted once
when generating the global feature vector. q represents the count of any separator
punctuation between S0 and S1.
6.1.1 The Comparability of State Items. Just as in our dependency parsers, the phrase-
structure parser is based on an incremental shift-reduce parsing process, and state
items built with the same number of transition actions are compared with each other
during decoding. However, due to the possibility of unary-reduce actions, the number
of actions used to build full parse trees can vary given an input sentence. This makes the
comparison of state items more difficult than for dependency parsing, because when
some state items on the agenda are completed (i.e., in the final state), others on the
agenda may still need more actions to complete. We choose to push completed state
items back onto the agenda during the decoding process, without changing them. The
decoding continues until the highest scored state item on the agenda is completed.
When decoding stops, the highest scored state item on the agenda is taken as the final
output. In this process, completed state items that do not have the highest score in the
agenda are kept unchanged and compared with incomplete state items, even though
they may have been built with different numbers of actions. Our experiments showed
that this approach gave reasonable accuracies.
6.2 Experiments
The experiments were performed using the Chinese Treebank 2 (Table 21) and Chinese
Treebank 5 data. Standard data preparation was performed before the experiments:
Empty terminal nodes were removed; any non-terminal nodes with no children were
removed; any unary X ? X nodes resulting from the previous steps were collapsed
into one X node.
For all experiments, we used the EVALB tool5 for evaluation, and used labeled recall
(LR), labeled precision (LP) and F1 score (which is the harmonic mean of LR and LP) to
measure parsing accuracy.
6.2.1 Test Results on CTB2. The following tests were performed using both gold-standard
POS-tags and POS-tags automatically assigned by a POS-tagger. We used our base-
line POS-tagger in Section 4 for automatic POS-tagging. The results of various models
5 http://nlp.cs.nyu.edu/evalb/.
143
Computational Linguistics Volume 37, Number 1
Table 22
Accuracies of various phrase-structure parsers on CTB2 with gold-standard POS-tags.
Model LR LP F1
Bikel Thesis 80.9% 84.5% 82.7%
Wang 2006 SVM 87.2% 88.3% 87.8%
Wang 2006 Stacked 88.3% 88.1% 88.2%
Our parser 89.4% 90.1% 89.8%
See text for details.
evaluated on sentences with less than 40 words and using gold-standard POS-tags are
shown in Table 22. The rows represent the model from Bikel and Chiang (2000) and
Bikel (2004), the SVM and ensemble models from Wang et al (2006), and our parser,
respectively. The accuracy of our parser is competitive using this test set.
The results of various models using automatically assigned POS-tags are shown in
Table 23. The rows in the table represent the models from Bikel and Chiang (2000), Levy
and Manning (2003), Xiong et al (2005), Bikel (2004), Chiang and Bikel (2002), the SVM
model from Wang et al (2006), the ensemble system from Wang et al (2006), and the
parser of this article, respectively. Our parser gave comparable accuracies to the SVM
and ensemble models from Wang et al (2006). However, comparison with Table 22
shows that our parser is more sensitive to POS-tagging errors than some of the other
models. One possible reason is that some of the other parsers (e.g., Bikel 2004) use the
parser model itself to resolve tagging ambiguities, whereas we rely on a POS-tagger to
accurately assign a single tag to each word. In fact, for the Chinese data, POS-tagging
accuracy is not high, with the perceptron-based tagger achieving an accuracy of only
93%. The beam-search decoding framework we use could accommodate joint parsing
and tagging, although the use of features based on the tags of incoming words com-
plicates matters somewhat, because these features rely on tags having been assigned to
all words in a pre-processing step. One possible solution would be generating multiple
POS-tags for each word during tagging, and incorporating tag information into the shift
action, so that the parser will resolve the POS-tag ambiguity. We leave this problem for
future work.
Table 23
Accuracies of various phrase-structure parsers on CTB2 with automatically assigned tags.
? 40 words ? 100 words Unlimited
LR LP F1 POS LR LP F1 POS LR LP F1 POS
Bikel 2000 76.8% 77.8% 77.3% - 73.3% 74.6% 74.0% - - - - -
Levy 2003 79.2% 78.4% 78.8% - - - - - - - - -
Xiong 2005 78.7% 80.1% 79.4% - - - - - - - - -
Bikel Thesis 78.0% 81.2% 79.6% - 74.4% 78.5% 76.4% - - - - -
Chiang 2002 78.8% 81.1% 79.9% - 75.2% 78.0% 76.6% - - - - -
W06 SVM 78.1% 81.1% 79.6% 92.5% 75.5% 78.5% 77.0% 92.2% 75.0% 78.0% 76.5% 92.1%
W06 Stacked 79.2% 81.1% 80.1% 92.5% 76.7% 78.4% 77.5% 92.2% 76.2% 78.0% 77.1% 92.1%
Our parser 80.2% 80.5% 80.4% 93.5% 76.5% 77.7% 77.1% 93.1% 76.1% 77.4% 76.7% 93.0%
See text for details.
144
Zhang and Clark Syntactic Processing
Figure 13
The accuracy/speed tradeoff graph for the phrase-structure parser.
Petrov and Klein (2007) reported LR and LP of 85.7% and 86.9% for sentences
with less than 40 words and 81.9% and 84.8% for all sentences on the CTB2 test set,
respectively. These results are significantly better than any model from Table 23.
However, we did not include their scores in the table because they used a different
training set from CTB5, which is much larger than the CTB2 training set used by all
parsers in the table. In order to make a comparison, we split the data in the same way
as Petrov and Klein and tested our parser using automatically assigned POS-tags. It
gave LR and LP of 82.0% and 80.9% for sentences with less than 40 words and 77.8%
and 77.4% for all sentences, significantly lower than Petrov and Klein. Possible reasons
include the sensitivity of our parser to POS-tag errors, and perhaps the use of a latent
variable model by Petrov and Klein.
As in the previous sections, we plot the speed/accuracy tradeoff for our phrase-
structure parser. For each point in each curve in Figure 13, we run the development
test to decide the number of training iterations, and draw the point with speed and
accuracy from the final test. Each point in the curve corresponds to B = 1, 2, 4, 8, 16,
and 32, respectively. The accuracies increased when the beam increased from 1 to 4, but
fluctuated when the beam increased beyond 4. In contrast to the development tests, the
accuracy reached its maximum when the beam size was 4 rather than 16. However,
the general trend of increased accuracy as the speed decreases can still be observed,
and the amount of increase diminishes as the speed decreases. These experiments were
performed on a Linux platform with a 2.0GHz CPU and a gcc 4.0.1 compiler.
6.2.2 Test Accuracy Using CTB5. Table 24 presents the performance of the parser on CTB5.
We adopt the data split from the previous section, as shown in Table 17. We used the
same parser configurations as Section 6.2.1.
Table 24
Accuracies of our phrase-structure parser on CTB5 using gold-standard and automatically
assigned POS-tags.
? 40 words Unlimited
LR LP F1 POS LR LP F1 POS
87.9% 87.5% 87.7% 100% 86.9% 86.7% 86.8% 100%
80.2% 79.1% 79.6% 94.1% 78.6% 78.0% 78.3% 93.9%
145
Computational Linguistics Volume 37, Number 1
As an additional evaluation we also produced dependency output from the phrase-
structure trees, using the head-finding rules, so that we can compare with dependency
parsers. We compare the dependencies read off our constituent parser using CTB5 data
with the dependency parser from Section 5, which currently gives the best dependency
parsing accuracy on CTB5. The same measures are taken and the accuracies with gold-
standard POS-tags are shown in Table 25. Our constituent parser gave higher accuracy
than the combined dependency parser. It is interesting that, though the constituent
parser uses many fewer feature templates than the dependency parser, the features do
include constituent information, which is unavailable to the dependency parser.
6.3 Related Work
Our parser is based on the shift-reduce parsing process from Sagae and Lavie (2005)
and Wang et al (2006), and therefore it can be classified as a transition-based parser
(Nivre et al 2006). An important difference between our parser and the Wang et al
(2006) parser is that our parser is based on a discriminative learning model with global
features, whereas the parser from Wang et al (2006) is based on a local classifier that
optimizes each individual choice. Instead of greedy local decoding, we used beam-
search in the decoder.
An early work that applies beam-search to constituent parsing is Ratnaparkhi
(1999). The main difference between our parser and Ratnaparkhi?s is that we use a
global discriminative model, whereas Ratnaparkhi?s parser has separate probabilities
of actions chained together in a conditional model.
Both our parser and the parser from Collins and Roark (2004) use a global discrim-
inative model and an incremental parsing process. The major difference is that Collins
and Roark, like Roark (2001), follow a top?down derivation strategy, whereas we chose
to use a shift-reduce process which has been shown to give state-of-the-art accuracies
for Chinese (Wang et al 2006). In addition, we did not include a generative baseline
model in the discriminative model, as did Collins and Roark (2004).
7. Discussion
We have demonstrated in the previous sections that accuracies competitive with the
state-of-the-art can be achieved by our general framework for Chinese word segmen-
tation, joint word segmentation and POS-tagging, Chinese and English dependency
parsing, and Chinese phrase-structure parsing. Besides these tasks, our baseline POS-
tagger in Section 4 is also implemented in the framework. When it is applied to English
POS-tagging using feature templates from Collins (2002), it gave similar accuracy to the
Table 25
Comparison of dependency accuracies between phrase-structure parsing and dependency
parsing using CTB5 data.
Non-root Root Complete
Dependency parser 86.21% 76.26% 34.41%
Constituent parser 86.95% 79.19% 36.08%
146
Zhang and Clark Syntactic Processing
dynamic-programming decoder of Collins (2002). All these experiments suggest that the
general yet efficient framework provides a competitive solution for structural prediction
problems with an incremental output-building process. In this section, we discuss the
main reasons for the effectiveness of the general framework, as well as its prerequisites,
advantages, and limitations when applied to a general task.
One of the main reasons for the high accuracies achieved by this framework is the
freedom in using arbitrary features to capture statistical patterns, including those that
lead to impractical time complexity with alternative learning and decoding frameworks
and algorithms such as CRFs and dynamic programming. This freedom was exploited
by our effort to incorporate larger sources of statistical information for the improve-
ment of accuracy. For example, our word-based segmentor extends the character-based
approach by including word information; our joint word segmentor and POS-tagger
utilizes POS information for word segmentation; our combined dependency parser
includes statistical information from two different methods in a single, consistently
trained model. The models gave state-of-the-art accuracies in these problems, demon-
strating the advantage of using flexible information covering large parts of the output
structure.
Compared to alternative discriminative training algorithms such as structural SVM
(Tsochantaridis et al 2004) and CRFs (Lafferty, McCallum, and Pereira 2001; Clark and
Curran 2007), the perceptron has a simple parameter update process, which is often
efficient in both memory usage and running time depending on the decoding algorithm.
Consider joint word segmentation and POS-tagging for example; we found in our
experiments that practical training times can be achieved by the perceptron algorithm,
but not with structural SVM. MIRA (Crammer and Singer 2003) and its simplifications
(McDonald, Crammer, and Pereira 2005; Crammer et al 2006) are also commonly used
learning algorithms to train a global linear model. They can be treated as slower but
potentially more accurate alternatives to the perceptron for the general framework. We
experimented with these for joint segmentation and tagging but did not improve upon
the perceptron.
Beam-search enables training to be performed efficiently for extremely large com-
plex search spaces, for which dynamic programming algorithms may be impractical.
For the joint word segmentation and POS-tagging problem, a dynamic-programming
decoder is prohibitively slow but a beam-search decoder runs in reasonable time.
A more important advantage of beam-search compared to dynamic-programming
is that beam-search allows arbitrary features, whereas the efficiency of a dynamic-
programming decoder is restricted by the range of features, due to its requirement
for optimal substructure. For our combined dependency parser, the feature set makes
a dynamic-programming decoder infeasibly slow. From this perspective, beam-search
is in line with other recent research on the improvement of accuracies by incorporat-
ing non-local features via approximation, such as belief propagation for dependency
parsing (Smith and Eisner 2008), integer linear programming for dependency parsing
(Martins, Smith, and Xing 2009), and forest reranking for phrase-structure parsing
(Huang 2008).
The only prerequisite of the framework is an incremental process, which consumes
the input sentence and builds the output structure using a sequence of actions. All
four problems studied in the article were first turned into an incremental process, and
then solved by applying the framework. The number of distinct actions for a problem
is dependent on the complexity of the output. For word segmentation, there are only
two actions (append or separate). For transition-based unlabeled dependency parsing,
there are four actions (shift, arc-left, arc-right, and reduce). For joint segmentation and
147
Computational Linguistics Volume 37, Number 1
POS-tagging and constituent parsing, there are many more distinct actions according
to the set of labels. For example, the number of distinct unary-reduce actions for con-
stituent parsing is equal to the number of distinct constituent labels that form unary-
branching nodes. The incremental processes for all four problems have linear time
complexity, and a larger number of distinct actions leads to a slower decoder.
One of the most important issues in using beam-search is the comparability of par-
tially built structures at each incremental step during decoding. For word segmentation
and joint segmentation and POS-tagging, we compare partially built sentences that have
the same number of characters. For word segmentation, partial words can be treated in
the same way as full words, without losing much accuracy. The same approach was
not effective when applied to joint segmentation and POS-tagging, for which partial
words must be treated differently, or avoided by alternative inference such as using
multiple-beams. For dependency parsing, we compared partial outputs that have been
built using the same number of transition actions. Because all parses for a sentence with
size n are built using exactly 2n ? 1 transition actions, this comparison is consistent
throughout the decoding process. For constituent parsing, we initially compare partial
outputs that have been built using the same number of actions. However, because
different output parse trees can contain a different number of unary-reduce actions,
some candidate outputs will be completed earlier than others. When this happens, we
choose to retain fully built outputs in the agenda while continuing the decoding process,
until the highest scored item in the agenda is a complete parse. Therefore, there are
situations when different parses in the agenda have a different number of actions. This
did not turn out to be a significant problem. We believe that the most effective way to
organize output comparison is largely an empirical question.
Finally, alternative components can be used to replace the learning or decoding
algorithms of the framework to give higher accuracies. Huang and Sagae (2010) have
recently applied dynamic-programming to dependency parsing to pack items that have
the same signature in the beam, and obtained higher accuracy than our transition-based
dependency parser in Section 5.
8. Conclusion
We explored word segmentation, joint word segmentation and POS-tagging, depen-
dency parsing, and phrase-structure parsing using a general framework of a global
linear model, trained by the perceptron algorithm and decoded with beam-search.
We have chosen to focus on Chinese; the framework itself and our algorithms for the
specific problems are language-independent, however. In Section 5 we reported results
on English dependency parsing. Despite the rather simple nature of the decoding and
training processes, the framework achieved accuracies competitive with the state-of-
the-art for all the tasks we considered, by making use of a large range of statistical
information. As further evidence of the effectiveness of our framework, we have re-
cently adapted our phrase-structure parser in Section 6 to parsing with a lexicalized
grammar formalism, Combinatory Categorial Grammar (CCG), and achieved higher
F-scores than the state-of-the-art C&C CCG parser (Clark and Curran 2007). The range of
problems that we have studied suggests that our framework is a simple yet competitive
option for structural prediction problems in general.
Our source code can be found at www.sourceforge.net/projects/zpar. It contains
the general framework, our implementations of the four tasks addressed in this article,
and other tools for syntactic processing.
148
Zhang and Clark Syntactic Processing
Acknowledgments
This work was largely carried out while
Yue Zhang was a DPhil student at the
Oxford University Computing Laboratory,
where he was supported by an ORS
Scholarship and the Clarendon Fund.
Stephen Clark was supported by EPSRC
grant EP/E035698/1.
References
Bikel, Daniel M. 2004. On the Parameter Space
of Generative Lexicalized Statistical Parsing
Models. Ph.D. thesis, University of
Pennsylvania.
Bikel, Daniel M. and David Chiang. 2000.
Two statistical parsing models applied
to the Chinese Treebank. In Proceedings of
SIGHAN Workshop, pages 1?6, Hong Kong.
Briscoe, Ted and John Carroll. 1993.
Generalized probabilistic LR parsing
of natural language (corpora) with
unification-based grammars.
Computational Linguistics, 19(1):25?59.
Buchholz, Sabine and Erwin Marsi. 2006.
aonll-X shared task on multilingual
dependency parsing. In Proceedings of
CoNLL, pages 149?164, New York, NY.
Carreras, Xavier, Michael Collins, and Terry
Koo. 2008. Tag, dynamic programming,
and the perceptron for efficient,
feature-rich parsing. In Proceedings of
CoNLL, pages 9?16, Manchester.
Carreras, Xavier, Mihai Surdeanu, and Lluis
Marquez. 2006. Projective dependency
parsing with perceptron. In Proceedings of
CoNLL, pages 181?185, New York, NY.
Charniak, Eugene. 2000. A
maximum-entropy-inspired parser. In
Proceedings of NAACL, pages 132?139,
Seattle, WA.
Chen, Wenliang, Jun?ichi Kazama, Kiyotaka
Uchimoto, and Kentaro Torisawa. 2009.
Improving dependency parsing with
subtrees from auto-parsed data. In
Proceedings of EMNLP, pages 570?579,
Singapore.
Chiang, David and Daniel M. Bikel. 2002.
Recovering latent information in
treebanks. In Proceedings of COLING,
pages 183?198, Taipei.
Clark, Stephen and James R. Curran.
2007. Wide-coverage efficient statistical
parsing with CCG and log-linear
models. Computational Linguistics,
33(4):493?552.
Collins, Michael. 1999. Head-driven Statistical
Models for Natural Language Parsing. Ph.D.
thesis, University of Pennsylvania.
Collins, Michael. 2002. Discriminative
training methods for hidden Markov
models: Theory and experiments with
perceptron algorithms. In Proceedings of
EMNLP, pages 1?8, Philadelphia, PA.
Collins, Michael and Brian Roark. 2004.
Incremental parsing with the perceptron
algorithm. In Proceedings of ACL,
pages 111?118, Barcelona.
Crammer, Koby, Ofer Dekel, Joseph Keshet,
Shai Shalev-Shwartz, and Yoram Singer.
2006. Online passive-aggressive
algorithms. Journal of Machine Learning
Research, 7:551?585.
Crammer, Koby and Yoram Singer. 2003.
Ultraconservative online algorithms for
multiclass problems. Journal of Machine
Learning Research, 3:951?991.
Duan, Xiangyu, Jun Zhao, and Bo Xu. 2007.
Probabilistic models for action-based
Chinese dependency parsing. In
Proceedings of ECML/ECPPKDD,
pages 559?566, Warsaw.
Emerson, Thomas. 2005. The second
international Chinese word segmentation
bakeoff. In Proceedings of SIGHAN
Workshop, pages 123?133, Jeju.
Finkel, Jenny Rose, Alex Kleeman, and
Christopher D. Manning. 2008. Efficient,
feature-based, conditional random field
parsing. In Proceedings of ACL/HLT,
pages 959?967, Columbus, OH.
Freund, Y. and R. Schapire. 1999. Large
margin classification using the perceptron
algorithm. In Rob Holte, editor, Machine
Learning. Kluwer, Boston, MA,
pages 277?296.
Hall, Johan, Jens Nilsson, Joakim Nivre,
Gu?lsen Eryigit, Bea?ta Megyesi, Mattias
Nilsson, and Markus Saers. 2007. Single
malt or blended? A study in multilingual
parser optimization. In Proceedings of the
CoNLL Shared Task Session of EMNLP/
CoNLL, pages 933?939, Prague.
Huang, Liang. 2008. Forest reranking:
Discriminative parsing with non-local
features. In Proceedings of ACL/HLT,
pages 586?594, Columbus, OH.
Huang, Liang and Kenji Sagae. 2010.
Dynamic programming for linear-time
incremental parsing. In Proceedings of ACL,
pages 1077?1086, Uppsala.
Jiang, Wenbin, Liang Huang, Qun Liu, and
Yajuan Lu?. 2008. A cascaded linear model
for joint Chinese word segmentation and
part-of-speech tagging. In Proceedings of
ACL/HLT, pages 897?904, Columbus, OH.
Jiang, Wenbin, Haitao Mi, and Qun Liu. 2008.
Word lattice reranking for Chinese word
149
Computational Linguistics Volume 37, Number 1
segmentation and part-of-speech tagging.
In Proceedings of COLING, pages 385?392,
Manchester.
Johansson, Richard and Pierre Nugues. 2007.
Incremental dependency parsing using
online learning. In Proceedings of the
CoNLL/EMNLP, pages 1134?1138, Prague.
Koo, Terry, Xavier Carreras, and Michael
Collins. 2008. Simple semi-supervised
dependency parsing. In Proceedings of
ACL/HLT, pages 595?603, Columbus, OH.
Kruengkrai, Canasai, Kiyotaka Uchimoto,
Jun?ichi Kazama, Yiou Wang, Kentaro
Torisawa, and Hitoshi Isahara. 2009. An
error-driven word-character hybrid model
for joint Chinese word segmentation and
POS tagging. In Proceedings of ACL/AFNLP,
pages 513?521, Suntec.
Lafferty, J., A. McCallum, and F. Pereira.
2001. Conditional random fields:
Probabilistic models for segmenting and
labeling sequence data. In Proceedings of
ICML, pages 282?289, Williamstown, MA.
Levy, Roger and Christopher D. Manning.
2003. Is it harder to parse Chinese, or the
Chinese treebank? In Proceedings of ACL,
pages 439?446, Sapporo.
Martins, Andre, Noah Smith, and Eric Xing.
2009. Concise integer linear programming
formulations for dependency parsing. In
Proceedings of ACL/AFNLP, pages 342?350,
Suntec.
McDonald, Ryan, Koby Crammer, and
Fernando Pereira. 2005. Online
large-margin training of dependency
parsers. In Proceedings of ACL, pages 91?98,
Ann Arbor, MI.
McDonald, Ryan and Joakim Nivre. 2007.
Characterizing the errors of data-driven
dependency parsing models. In Proceedings
of EMNLP/CoNLL, pages 122?131, Prague.
McDonald, Ryan and Fernando Pereira.
2006. Online learning of approximate
dependency parsing algorithms. In
Proceedings of EACL, pages 81?88, Trento.
McDonald, Ryan, Fernando Pereira,
Kiril Ribarov, and Jan Hajic. 2005.
Non-projective dependency parsing
using spanning tree algorithms. In
Proceedings of HLT/EMNLP, pages 523?530,
Vancouver.
Nakagawa, Tetsuji and Kiyotaka Uchimoto.
2007. A hybrid approach to word
segmentation and POS tagging. In
Proceedings of ACL Demo and Poster
Session, Prague.
Ng, Hwee Tou and Jin Kiat Low. 2004.
Chinese part-of-speech tagging:
One-at-a-time or all-at-once? Word-based
or character-based? In Proceedings of
EMNLP, pages 227?284, Barcelona.
Nivre, Joakim, Johan Hall, Sandra Ku?bler,
Ryan McDonald, Jens Nilsson, Sebastian
Riedel, and Deniz Yuret. 2007. The CoNLL
2007 shared task on dependency parsing.
In Proceedings of the CoNLL Shared Task
Session of EMNLP/CoNLL, pages 915?932,
Prague.
Nivre, Joakim, Johan Hall, Jens Nilsson,
Gu?ls?en Eryig?it, and Svetoslav Marinov.
2006. Labeled pseudo-projective
dependency parsing with support vector
machines. In Proceedings of CoNLL,
pages 221?225, New York, NY.
Nivre, Joakim and Ryan McDonald.
2008. Integrating graph-based and
transition-based dependency parsers. In
Proceedings of ACL/HLT, pages 950?958,
Columbus, OH.
Peng, F., F. Feng, and A. McCallum. 2004.
Chinese segmentation and new word
detection using conditional random fields.
In Proceedings of COLING, pages 562?568,
Geneva.
Petrov, Slav and Dan Klein. 2007. Improved
inference for unlexicalized parsing. In
Proceedings of HLT/NAACL, pages 404?411,
Rochester, NY.
Ratnaparkhi, Adwait. 1998. Maximum
Entropy Models for Natural Language
Ambiguity Resolution. Ph.D. thesis,
University of Pennsylvania.
Ratnaparkhi, Adwait. 1999. Learning to
parse natural language with maximum
entropy models. Machine Learning,
34(1-3):151?175.
Roark, Brian. 2001. Probabilistic top?down
parsing and language modeling.
Computational Linguistics, 27:249?276.
Sagae, Kenji and Alon Lavie. 2005. A
classifier-based parser with linear
run-time complexity. In Proceedings of
IWPT, pages 125?132, Vancouver.
Sagae, Kenji and Alon Lavie. 2006. Parser
combination by reparsing. In Proceedings
of HLT/NAACL, Companion Volume: Short
Papers, pages 129?132, New York, NY.
Shi, Yanxin and Mengqiu Wang. 2007. A
dual-layer CRF based joint decoding
method for cascade segmentation and
labelling tasks. In Proceedings of IJCAI,
pages 1707?1712, Hyderabad.
Smith, David and Jason Eisner. 2008.
Dependency parsing by belief
propagation. In Proceedings of the 2008
Conference on Empirical Methods in Natural
Language Processing, pages 145?156,
Honolulu, HI.
150
Zhang and Clark Syntactic Processing
Sproat, R., C. Shih, W. Gail, and N. Chang.
1996. A stochastic finite-state
word-segmentation algorithm for Chinese.
Computational Linguistics, 22(3):377?404.
Sproat, Richard and Thomas Emerson. 2003.
The first international Chinese word
segmentation bakeoff. In Proceedings of The
Second SIGHAN Workshop, pages 282?289,
Sapporo.
Tsochantaridis, I., T. Hofmann, T. Joachims,
and Y. Altun. 2004. Support vector
machine learning for interdependent and
structured output spaces. In Proceedings
of ICML, pages 102?114, Banff.
Wang, Xinhao, Xiaojun Lin, Dianhai Yu,
Hao Tian, and Xihong Wu. 2006. Chinese
word segmentation with maximum
entropy and n-gram language model.
In Proceedings of SIGHAN Workshop,
pages 138?141, Sydney.
Xia, Fei. 2000. The Part-of-Speech Tagging
Guidelines for the Chinese Treebank (3.0),
University of Pennsylvania.
Xiong, Deyi, Shuanglong Li, Qun Liu,
Shouxun Lin, and Yueliang Qian. 2005.
Parsing the Penn Chinese Treebank with
semantic knowledge. In Proceedings of
IJCNLP, pages 70?81, Jeju.
Xue, N. 2003. Chinese word segmentation as
character tagging. International Journal of
Computational Linguistics and Chinese
Language Processing, 8(1):29?48.
Yamada, H. and Y. Matsumoto. 2003.
Statistical dependency analysis using
support vector machines. In Proceedings
of IWPT, pages 195?206, Nancy.
Zhang, Ruiqiang, Genichiro Kikui, and
Eiichiro Sumita. 2006. Subword-based
tagging by conditional random
fields for Chinese word segmentation.
In Proceedings of the HLT/NAACL,
Companion, volume Short Papers,
pages 193?196, New York, NY.
Zhang, Yue and Stephen Clark. 2007.
Chinese segmentation with a word-based
perceptron algorithm. In Proceedings of
ACL, pages 840?847, Prague.
Zhang, Yue and Stephen Clark. 2008a. Joint
word segmentation and POS tagging using
a single perceptron. In Proceedings of
ACL/HLT, pages 888?896, Columbus, OH.
Zhang, Yue and Stephen Clark. 2008b.
A tale of two parsers: Investigating
and combining graph-based and
transition-based dependency parsing
using beam-search. In Proceedings of
EMNLP, pages 562?571, Honolulu, HI.
Zhang, Yue and Stephen Clark. 2009.
Transition-based parsing of the Chinese
Treebank using a global discriminative
model. In Proceedings of IWPT,
pages 162?171, Paris.
Zhang, Yue and Stephen Clark. 2010. A fast
decoder for joint word segmentation and
POS-tagging using a single discriminative
model. In Proceedings of EMNLP,
pages 843?852, Cambridge, MA.
Zhao, Hai, Chang-Ning Huang, and Mu Li.
2006. An improved Chinese word
segmentation system with conditional
random field. In Proceedings of SIGHAN
Workshop, pages 162?165, Sydney.
151
152
Practical Linguistic Steganography using
Contextual Synonym Substitution and a
Novel Vertex Coding Method
Ching-Yun Chang?
University of Cambridge
Stephen Clark??
University of Cambridge
Linguistic steganography is concerned with hiding information in natural language text.
One of the major transformations used in linguistic steganography is synonym substitution.
However, few existing studies have studied the practical application of this approach. In this
article we propose two improvements to the use of synonym substitution for encoding hidden
bits of information. First, we use the Google n-gram corpus for checking the applicability of a
synonym in context, and we evaluate this method using data from the SemEval lexical substitu-
tion task and human annotated data. Second, we address the problem that arises from words with
more than one sense, which creates a potential ambiguity in terms of which bits are represented
by a particular word. We develop a novel method in which words are the vertices in a graph,
synonyms are linked by edges, and the bits assigned to a word are determined by a vertex coding
algorithm. This method ensures that each word represents a unique sequence of bits, without
cutting out large numbers of synonyms, and thus maintains a reasonable embedding capacity.
1. Introduction
In order to transmit information through an open channel without detection by anyone
other than the receiver, a covert channel can be used. In information theory, a covert
channel is a parasitic communications channel that is hidden within the medium of
a legitimate communication channel (Lampson 1973). For example, steganography is a
form of covert channel in which certain properties of the cover medium are manipulated
in an unexpected, unconventional, or unforeseen way so that, with steganographic
transmission, the encrypted messages can be camouflaged in a seemly innocent medium
and sent to the receiver with less chance of being suspected and attacked. Ideally,
because the changes to the medium are so subtle, anyone not specifically looking for
a hidden message is unlikely to notice the changes (Fridrich 2009).
? 15 JJ Thomson Avenue, Cambridge CB3 0FD, UK. E-mail: Ching-Yun.Chang@cl.cam.ac.uk.
?? 15 JJ Thomson Avenue, Cambridge CB3 0FD, UK. E-mail: Stephen.Clark@cl.cam.ac.uk.
Submission received: 18 January 2013; revised version received: 3 June 2013; accepted for publication:
1 August 2013.
doi:10.1162/COLI a 00176
? 2014 Association for Computational Linguistics
Computational Linguistics Volume 40, Number 2
In this article, we aim at concealing secret information in natural language text by
manipulating cover words. The proposed steganography system replaces selected cover
words with their synonyms, which is the mechanism used to embed information. In
order to ensure the lexical substitutions in the cover text are imperceptible, the system
uses the Google n-gram corpus (Brants and Franz 2006) for checking the applicability
of a synonym in context. In addition, the system assigns codes to acceptable substitutes
using a novel vertex coding method in which words are represented as vertices in a
graph, synonyms are linked by edges, and the bits assigned to a vertex represent the
code of a particular word. Our lexical substitution-based steganography system was
previously published in Chang and Clark (2010b), in which the system was evaluated
automatically by using data from the English lexical substitution task for SemEval-
2007.1 In this article, we extend the previous work by more closely addressing the
practical application of the proposed system. We present results from a new human
evaluation of the system?s output and a simple computational steganalysis of word-
frequency statistics which is a more direct evaluation for linguistic steganography. We
also give an extended literature review which will serve as a useful introduction to
linguistic steganography for those Computational Linguistics readers not familiar with
the problem.
1.1 Steganography
The word steganography has Greek origins and means ?concealed writing.? The original
practice can be traced back to around 440 BC when the ancient Greeks hid messages
within wax tablets by writing messages on the wood before applying a wax surface
(Herodotus 1987). Another early recorded use of steganography occurred in ancient
Greece when messengers tattooed messages on their shaved heads and concealed the
messages with the hair that grew over them afterwards, a technique also used by
German spies in the early 20th century (Newman 1940). With the advent of tiny images,
in the Russo-Japanese War (1905) microscopic images were hidden in ears, nostrils, or
under fingernails (Stevens 1957); during both World Wars messages were reduced to
microdots and stuck on top of printed periods or commas in innocent cover material
such as magazines, or inserted into slits of the edges of postcards (Newman 1940;
Hoover 1946). In both World Wars invisible inks were also used extensively to write
messages under visible text (Kahn 1967). The application of special inks is still used
today in the field of currency security to write a hidden message on bank notes or other
secure documents.
Since the 1980s, with the advent of computer technologies, digital equivalents of
these camouflage techniques were invented to hide messages in digital cover me-
dia, such as images, video, and audio signals (Fridrich 2009). For example, in 2010,
the United States Department of Justice documented that more than 100 text files
were retrieved from images posted on publicly accessible Web sites.2 According to
the Steganography Analysis and Research Centre,3 there have been over 1,100 digi-
tal steganography applications identified. Most of the digital steganography systems
exploit the redundancy of the cover media and rely on the limitations of the human
auditory or visual systems. For example, a standard image steganography system uses
1 http://www.dianamccarthy.co.uk/task10index.html.
2 http://www.justice.gov/opa/documents/062810complaint2.pdf.
3 http://www.sarc-wv.com/.
404
Chang and Clark Practical Linguistic Steganography
the least-significant-bit substitution technique. Because the difference between 11111111
and 11111110 in the value for red/green/blue intensity is likely to be undetectable by
the human eye, the least-significant-bit can be used to hide information other than color,
without being perceptable by a human observer.4
Simmons (1984) formulated steganography as the ?Prisoners? Problem.? The prob-
lem describes a scenario where two prisoners named Alice and Bob are locked up in
separate cells far apart from each other and wish to hatch an escape plan. All their
communications have to pass through the warden, Willie. If Willie detects any sign
of a conspiracy, he will thwart their plan by throwing them into high-security cells
from which nobody has ever escaped; as long as Willie does not suspect anything,
the communication can be put through. So Alice and Bob must find some way for
embedding hidden information into their seemingly innocent messages. Alice and Bob
can succeed if they are able to exchange information allowing them to coordinate their
escape without arousing Willie?s suspicion. According to information-hiding terminol-
ogy (Pfitzmann 1996), a legitimate communication among the prisoners is called a cover
object, and a message with embedded hidden information is called a stego object,
where object stands for ?text,? ?image,? ?audio,? or whatever media is being used.
The algorithms that Alice uses for creating the stego object and Bob uses for decoding
the message are collectively called a stegosystem.
A stegosystem has to fulfil two fundamental requirements. The first and foremost
requirement is security. This means that the stegomedia in which the secret message
is hidden must be unsuspicious according to a human or a computer. The second
requirement is payload capacity. The payload is the size of the secret message that
the sender wishes to conceal and transport relative to the size of the cover media.
Because steganography aims at covert information transmission, it requires sufficient
embedding capacity. An ideal stegosystem would have a high level of security and
large payload capacity. However, there is a fundamental trade-off between security and
payload because any attempt to embed additional information in the cover media is
likely to increase the chance of introducing anomalies into the media, thus degrading
the security level.
A related area to steganography is digital watermarking, in which changes are
made to a cover medium in order to verify its authenticity or to show the identity of
its owners?for example, for copyright purposes (Cox et al. 2008; Shih 2008). An inter-
esting watermarking application is ?traitor tracing,? in which documents are changed
in order to embed individual watermarks. These marks can then be used to later
identify particular documents and, if necessary, to trace the source of the documents;
for example, if a set of documents?identical except for the changes used to embed
the watermarks?has been sent to a group of individuals, and one of the documents has
been leaked to a newspaper. Both steganography and watermarking use steganographic
techniques to embed information in cover media. However, steganography aims for the
imperceptibility of a secret message to an observer, whereas watermarking tries to mark
cover media with information that is robust against modifications or for the purpose of
tamperproofing. For steganography a user can have the freedom to choose the cover
medium to carry messages, whereas for watermarking the cover medium is already
decided.
4 The observer may also be a computer program, designed to detect statistical anomalies in the image
representation that may indicate the presence of hidden information.
405
Computational Linguistics Volume 40, Number 2
Figure 1
The linguistic steganography framework.
1.2 Linguistic Steganography
A key question for any stegosystem is the choice of cover medium. Given the ubiquitous
nature of natural language and the omnipresence of text, text is an obvious medium
to consider. For example, a Nazi spy in World War II sent the following message
(Kahn 1967):
Apparently neutral?s protest is thoroughly discounted and ignored. Isman hard hit. Blockade
issue affects pretext for embargo on by-products, ejecting suets and vegetable oils.
By taking the second letter from each word the following message emerges:
Pershing sails from NY June I
The advantage of this method is that the secret message appears as some normal
communication that may not arouse suspicion. However, given the current state-of-the-
art of Natural Language Processing (NLP) technology, NLP techniques are not capable of
creating meaningful and natural text from scratch and of hiding messages in it. There-
fore, most of the existing linguistic stegosystems take already-existing text as the cover
text, and linguistic properties of the text are used to modify it and hide information.
Figure 1 shows the general linguistic steganography framework. First, some secret
message, represented as a sequence of bits, is hidden in a cover text using the embedding
algorithm, resulting in the stego text.5 Next, the stego text passes the observer (human
or computer), who does not object to innocuous messages passing between the sender
and receiver, but will examine the text for any suspicious looking content. Once the
stego text reaches the receiver, the hidden message is recovered using the extracting
algorithm.
In order to embed messages, a cover text must provide information carriers that can
be modified to represent the secret. For example, a lexical substitution-based stegosys-
tem substitutes selected words (the information carriers) with their synonyms so that
the concatenation of the bitstrings represented by the synonyms is identical to the
secret. Note that an unmodifiable text cannot carry information. So far, the literature on
5 The message may have been encrypted initially also, as in the figure, but this is not important in this
article; the key point is that the hidden message is a sequence of bits.
406
Chang and Clark Practical Linguistic Steganography
linguistic steganography is small compared with other media (Bergmair 2007). One of
the likely reasons is that it is easier to make changes to images and other non-linguistic
media that are undetectable by an observer. Language has the property that even small
local changes to a text (e.g., replacing a word with a word with similar meaning) may
result in text that is anomalous at the document level, or anomalous with respect to the
state of the world. Hence finding linguistic transformations that can be applied reliably
and frequently is a challenging problem for linguistic steganography.
An additional challenge for linguistic steganography is that evaluation of linguistic
stegosystems is much more difficult than that of image, audio, or video stegosystems be-
cause such evaluation requires us to consider many controversial linguistic issues, such
as meaning, grammaticality, fluency, and style. The current state-of-the-art techniques
for automatically evaluating the fluency and grammaticality of natural language gener-
ation systems are based on techniques for evaluating the output of machine translation
systems, such as comparing the n-grams in the machine translation output with those
in a reference translation (Papineni et al. 2002; Zhang and Clark 2011). Although some
computational steganalysis systems have been developed to identify stego text from
innocent text using statistical methods, these systems can only be applied to text that
undergoes certain linguistic transformations such as translation and lexical substitution,
and they are not accurate enough for practical evaluation. Therefore, most of the current
linguistic stegosystems were evaluated by human judges (Murphy and Vogel 2007a,
2007b; Meral et al. 2007, 2009; Kim 2008, 2009; Chang and Clark 2010a, 2012a, 2012b),
where a human assessor was provided with stego text and was asked to rate or improve
the naturalness of the stego text.
1.3 Linguistic Stegosystem
Most existing stegosystems consist of three independent modules?linguistic transfor-
mation, encoder generation, and text selection?as shown in Figure 2. As explained
earlier, in order to embed messages, a cover text must provide information carriers that
can be modified to represent the secret, and the modification must be imperceptible
to an observer. This first step of modification is called linguistic transformation. For
example, suppose there is a cover sentence I like biscuits with a cup of tea, in which
biscuits can be replaced by its synonym cookies without degrading the naturalness of
the original sentence; hence biscuits can serve as an information carrier in this example
(using synonym substitution as the transformation). After the linguistic transformation
stage, there are two alternative sentences I like biscuits with a cup of tea and I like cookies
with a cup of tea. According to the linguistic transformation used in a stegosystem, we
can classify existing work into three major categories, which will be discussed in detail
Figure 2
Three modules in a linguistic stegosystem.
407
Computational Linguistics Volume 40, Number 2
in Section 2.1: lexical or phrase substitutions, syntactic transformations, and semantic
transformations.
After generating different versions of the cover text, an encoding method is used to
assign bitstrings to the alternatives, which is called encoder generation. To continue the
previous example, a simple encoding method would first sort the alternative sentences
alphabetically and then encode the first alternative, namely, the one containing biscuits
with bit 0, and the other sentence containing cookies with bit 1. The final phase is text
selection, which chooses the alternative representing the secret bitstring as the stego
text. Let us assume the secret bit is 1; therefore during the text selection phase, I like
cookies with a cup of tea is chosen as the stego sentence in order to embed the secret. If
there is no alternative associated with the secret bitstring, the secret embedding fails.
Therefore, it is important to generate sufficient alternatives as well as to efficiently
encode each option.
To recover the secret in the example, the sender and receiver must share the same
linguistic transformation and encoder generation modules. The receiver first uses the
transformation method to determine that, in the stego sentence, cookies can be replaced
by its synonym biscuits; hence, two alternative sentences are derived. Next, the receiver
uses the encoding method to assign codes to the two alternatives. Because the stego
sentence I like cookies with a cup of tea is encoded by bit 1, the receiver knows that the
secret is 1.
The convenient modularity between the linguistic transformation and encoder gen-
eration allows a transformation to be combined with different encoding algorithms,
although the transformation may put some constraints on what encoding method can be
used. For example, suppose we replace the synonym substitution-based transformation
in the example with a translation-based transformation that takes a non-English cover
sentence as input and outputs two different English translations as the alternatives.
Assume the two translations are I like biscuits with a cup of tea and I like cookies with a
cup of tea, and that the second alternative is the stego sentence. Now the receiver has
to recover the secret from this stego sentence. However, the receiver does not know the
original cover sentence, so it is unlikely that the receiver will be able to obtain the other
translations used by the sender to derive the code assigned to the stego sentence. In
this case, the translation-based transformation cannot work with block code encoding.
A possible solution is to use hash function encoding, as explained in Section 2.2.4.
1.4 Overview
In this article we focus on linguistic steganography rather than watermarking, because
we are interested in the requirement that any changes to a text must be imperceptible
to an observer, as this makes for a strong test of the NLP technology used to modify
the cover text. There are some practical security issues in the steganography application
that we have chosen to ignore or simplify in order to focus on the underlying NLP tech-
niques. For example, we assume the adversary in the espionage scenario is acting pas-
sively rather than actively. A passive warden examines all messages exchanged between
Alice and Bob but crucially does not modify any message. In other words, we have
ignored the possibility of steganographic attacks (Fridrich 2009), via an active warden
who deliberately modifies messages in order to thwart any hidden communication. In
addition, for the human evaluation of the security level of our stegosystem, we evaluate
the naturalness of generated stego sentences. In other words, we do not investigate the
document-level coherence of stego text because this requires sophisticated knowledge
of natural language semantics and pragmatics which we consider to be outside the
408
Chang and Clark Practical Linguistic Steganography
scope of this work. Therefore, it is possible that a totally natural stego sentence can
still raise suspicion if it contains words which stand out as being completely different
from the rest of the paragraph. For the computational steganalysis, we use a simple
statistical analysis proposed in Meng et al. (2010), which compares the frequency of
high-frequency words in a stego text and in its original text.
The main objective of this article is to explore the applicability of lexical substitution
to steganography. Lexical substitution is a relatively straightforward modification of
text. It replaces selected words with the same part of speech (POS) synonyms, and
does not involve operating on the sentence structure, so the modification is likely to
be grammatical. Another advantage of this transformation is that many languages are
profuse in synonyms so there is a rich source of information carriers in a cover text.
In this work we focus on hiding information in English text. However, the proposed
methods can also be applied to other languages as long as the same resources and tools
are available for the other language, such as synonym dictionaries and n-gram corpora.
There are two practical difficulties associated with hiding bits using lexical sub-
stitution. The first is that words can have more than one sense. In terms of WordNet
(Fellbaum 1998), which is the electronic dictionary we use, words can appear in more
than one synonym set (synset). This is a problem because a word may be assigned
different secret bitstrings in the different synsets, and the receiver does not know which
of the senses to use, and hence does not know which hidden bitstring to recover. Our
solution to this problem is a novel vertex coding method which ensures that words are
always assigned the same bitstring, even when they appear in different synsets.
The second problem is that many synonyms are only applicable in certain contexts.
For example, the words in the WordNet synset {bridge, span} share the meaning of ?a
structure that allows people or vehicles to cross an obstacle such as a river or canal
or railway etc.? However, bridge and span cannot be substituted for each other in the
sentence suspension bridges are typically ranked by the length of their main span, and doing so
would likely raise the suspicion of an observer due to the resulting anomaly in the text.
Our solution to this problem is to perform a contextual check that utilizes the Google
n-gram corpus (Brants and Franz 2006). We evaluate the substitution checker using the
data from the English lexical substitution task for SemEval-2007 and a human judgment
corpus created specifically for this work.
For the proposed lexical substitution checker, the higher quality the passed substi-
tutions are, the less suspicious the stego text may be. In addition, the more substitutions
that pass the check, the more information carriers the stegosystem can use. Hence the
evaluation of the proposed substitution checker can be seen as an indirect evaluation of
the proposed stegosystem. For this reason, the performance of the proposed substitu-
tion checker is evaluated in terms of precision and recall in our automatic evaluation.
Precision is the percentage of substitutions judged acceptable by the checker that are
also in the gold standard of the SemEval-2007 English lexical substitution task; recall is
the percentage of substitutions in the gold standard that are also passed by the checker.
The interpretation of the measures for a stegosystem is that a higher precision value
implies a better security level, whereas a larger recall value means a greater payload
capacity. Apart from precision and recall evaluations, we also asked human judges to
evaluate the naturalness of the substitutions that pass the proposed checker, which is a
more direct evaluation of the imperceptibility of the steganography application.
A significant contribution of this article is to advertise the linguistic steganography
problem to the NLP community. The requirement that any linguistic transformations
maintain the grammaticality and meaning of the cover text makes the problem a strong
test for existing NLP technology. In addition, the proposed substitution checker for
409
Computational Linguistics Volume 40, Number 2
certifying sentence naturalness potentially benefits not only the steganography ap-
plication, but also other NLP applications that require a measure of how natural a
word is in a particular context. Another contribution of the work is the evaluation of
the proposed stegosystems. The results suggest that it is possible to develop a prac-
tical linguistic steganography system based on lexical substitution with current NLP
techniques.
The rest of this article is organized as follows. Section 2 reviews the current state of
the art in linguistic steganography and the various linguistic transformations that have
been used in existing stegosystems. This is a substantial literature review designed to
introduce the problem to the Computational Linguistics reader. In Section 3, we describe
our lexical substitution-based stegosystem, along with the method for checking substi-
tution quality together with an empirical evaluation. In Section 4 we propose a novel
vertex coding algorithm to solve the decoding ambiguity problem and demonstrate the
proposed stegosystem using an example.
2. Background
This section reviews existing linguistic stegosystems. Under our interpretation of the
term linguistic steganography, we are only concerned with stegosystems that make
changes that are linguistic in nature, rather than operating on superficial properties of
the text, for example, the amount of white space between words (Por, Fong, and Delina
2008), font colors (Khairullah 2009), or relying on specific file formats, such as ASCII or
HTML (Bennett 2004; Shahreza 2006).
2.1 Linguistic Transformations
In the following, we describe the three linguistic transformation categories?lexical
or phrase substitutions, syntactic transformations, and semantic transformations?that
have been used in existing stegosystems to modify cover text. For each transformation,
some examples are provided to demonstrate the text manipulation.
2.1.1 Lexical and Phrase Transformations. There are a few electronic dictionaries available
that are designed to capture various lexical relationships between words and serve as
lexical reference systems (Fellbaum 1998; Schuler 2005). These can be used to perform
lexical substitution. One of the most well-known electronic dictionaries is WordNet
(Fellbaum 1998), in which English nouns, verbs, adjectives, and adverbs are categorized
into synonym sets (synsets). Words in the same synset have the same or similar meaning
Table 1
Synsets of the word marry in WordNet 3.1.
marry (verb)
gloss: take in marriage
synset: marry, get married, wed, conjoin, hook up with, get hitched with, espouse
gloss: perform a marriage ceremony
synset: marry, wed, tie, splice
410
Chang and Clark Practical Linguistic Steganography
and in principle can be substituted with each other. For example, a search result of the
word marry in WordNet 3.1 is summarized in Table 1. According to this table, we can
change the sentence The minister will marry us on Sunday to The minister will wed us on
Sunday without introducing much semantic difference because marry and wed express a
similar lexical concept in this context.
There are three main challenges when using lexical substitution as the linguistic
transformation. The first is word-category disambiguation, which marks up a word
with a particular POS based on both its definition as well as the context. For example,
fast is an adverb in the phrase hold fast to the rope, an adjective in the phrase a fast car,
and a verb in the phrase Catholics fast during Lent. Existing POS taggers have achieved
97% accuracy on the Penn Treebank (Toutanova et al. 2003; Shen, Satta, and Joshi 2007;
Spoustova? et al. 2009; S?gaard 2010) and are widely used in lexical substitution-based
stegosystems (Chapman, Davida, and Rennhard 2001; Bolshakov 2004; Taskiran, Top-
kara, and Delp 2006; Topkara, Topkara, and Atallah 2006b; Topkara et al. 2006; Chang
and Clark 2010b).
The second challenge is word-sense disambiguation, which identifies the sense of
a word in context (if the word has more than one meaning) so the correct synset can
be used. For example, according to the context, bottom means ?a cargo ship? rather
than ?the lower side of anything? in the sentence we did our overseas trade in foreign
bottoms, and therefore it can be replaced with freighter but not undersurface. The first
lexical substitution stegosystem was proposed by Winstein (1999). In order to han-
dle the fact that a word may appear in more than one synset in WordNet, Winstein
defines ?interchangeable? words as words that belong to exactly the same synsets,
and only uses these words for substitution. For example, marry and wed in Table 1
are interchangeable words because they are always synonyms even under different
meanings. Any words that are not interchangeable are discarded and not available for
carrying information. Winstein calculates that only 30% of WordNet can be used in such
a system.
The main purpose of linguistic transformations is to generate unsuspicious alterna-
tives for a cover sentence. Although replacing a word with its synonym that conveys the
same concept may preserve the meaning of the sentence, much of the time there are still
semantic and pragmatic differences among synonyms. For example, the synset {chase,
trail, tail, tag, dog, track}means ?go after with the intent to catch.? However, an awkward
sentence would be generated if we replaced chase with dog in the sentence the dogs chase
the rabbit. Hence, it is important to check the acceptability of a synonym in context.
Bolshakov (2004) used a collocation-based test to determine whether a substitution
is applicable in context. Taskiran, Topkara, and Delp (2006) attempted to use context
by prioritizing the alternatives using an n-gram language model; that is, rather than
randomly choose an option from the synset, the system relies on the language model to
select the synonym. In Section 3, we describe how our proposed lexical substitution-
based stegosystem uses the Google n-gram corpus to certify the naturalness of the
proposed substitution.
Similar to synonym substitution, text paraphrasing restates a phrase using different
words while preserving the essential meaning of the source material being paraphrased.
In other words, text paraphrasing is multi-word substitution. For example, we can para-
phrase a high percentage of by a large number of in the sentence a form of asbestos has caused a
high percentage of cancer deaths. However, text paraphrasing may have more effect on the
grammaticality of a sentence than lexical substitution. In our earlier work (Chang and
Clark 2010a) we developed a stegosystem exploiting a paraphrase dictionary (Callison-
Burch 2008) to find potential information carriers, and used the Google n-gram corpus
411
Computational Linguistics Volume 40, Number 2
Table 2
Some common syntactic transformations in English.
Transformation Original sentence Transformed sentence
Passivization The dog kissed Peter. Peter was kissed by the dog.
Topicalization I like pasta. Pasta, I like.
Clefting He won a new bike. It was a new bike that he won.
Extraposition To achieve that is impossible. It is impossible to achieve that.
Preposing I like cheese bagels. Cheese bagels are what I like.
There-construction A cat is in the garden. There is a cat in the garden.
Pronominalization I put the cake in the fridge. I put it there.
Fronting ?What!? Peter said. ?What!? said Peter.
and a combinatory categorial grammar (CCG) parser (Clark and Curran 2007) to certify
the paraphrasing grammaticality.
2.1.2 Syntactic Transformations. Syntactic transformation methods are based on the fact
that a sentence can be transformed into more than one semantically equivalent syntac-
tic structure, using transformations such as passivization, topicalization, and clefting.
Table 2 lists some of the common syntactic transformations in English.6
The first syntactic transformation method was presented by Atallah et al. (2000).
Later, Atallah et al. (2001) generated alternative sentences by adjusting the structural
properties of intermediate representations of a cover sentence. In other words, instead
of performing lexical substitution directly on the text, the modifications are performed
on the syntactic parse tree of a cover sentence. Murphy (2001), Liu, Sun, and Wu
(2005), Topkara, Topkara, and Atallah (2006a), Meral et al. (2007), Murphy and Vogel
(2007b), and Meral et al. (2009) all belong to this syntactic transformation category. After
manipulating the syntactic parse tree, the modified deep structure form is converted
into the surface structure format via language generation tools.
Aside from these systems, Wayner (1995) and Chapman and Davida (1997) pro-
posed mimicry text approaches associated with linguistic syntax. These two stegosys-
tems generate stego text from scratch instead of modifying an existing text. Wayner
(1995) proposed a method with his context-free mimic function (Wayner 1992) to gen-
erate a stego text that has statistical properties close to natural language. The context-
free mimic function uses a probabilistic grammar-based model to structure the stego
text. Because the mimicry method only puts emphasis on the syntactic structure of a
sentence, it is likely to generate nonsensical stego text which is perceptible by humans.
Chapman and Davida (1997) developed a stegosystem called NICETEXT that generates
stego sentences using style sources and context-free grammars to simulate certain as-
pects of writing style. Compared to Wayner?s mimicry method, the stego text generated
by NICETEXT is more natural in terms of the semantics, but still not at a level that
would be suitable for practical steganography.
2.1.3 Semantic Transformations. The semantic transformation is the most sophisticated
approach for linguistic steganography, and perhaps impractical given the current state-
of-the-art for NLP technology. It requires some sophisticated tools and knowledge to
6 The categories of transformations are adopted from Topkara, Taskiran, and Delp (2005).
412
Chang and Clark Practical Linguistic Steganography
Afghanistan (nation)
borders-on China, Iran, Pakistan, Tajikistan, Uzbekistan
has-currency afghani
has-member Pashtun, Tajik, Hazara, Uzbek
has-representative Mullah Mohammad Omar
Figure 3
Parts of the ontological semantics for Afghanistan.
assault?|?agent?nation??United States?
|?theme?nation??Afghanistan?
(a)
assault?|?agent?nation??United States?
|?theme?nation??Afghanistan??|?has-representative?politician??Mullah Mohammad Omar?
(b)
Figure 4
An example of the TMR tree modification taken from Atallah et al. (2002).
model natural language semantics and to evaluate equivalence between texts in order to
perform deep semantic manipulations. For example, consider the following sentences:
Bond takes revenge for Vesper?s death.
Vesper?s death is avenged by Bond.
007 takes revenge for Vesper?s death.
The idea is to define the semantic representation in such a way that the translation from
any of these sentences to their semantic representations would yield the same form.
In this manner, the meaning of the cover sentence can be expressed in another natural
language text. For this to be successful for the example, we would have to understand
the sentences in different voices, such as active and passive, and make use of some
world knowledge, such as the fact that the codename of James Bond is 007.
The work of Atallah et al. (2002) used semantic transformations and aimed to
output alternatives by modifying the text-meaning representation (TMR) tree of a
cover sentence. The modifications include pruning, grafting, or substituting the tree
structure with information available from ontological semantic resources. A linguistic
ontology is a formal knowledge representation of the world; a conceptualization of
entities, events, and their relationships in an abstract way. For example, Figure 3, taken
from Atallah et al. (2002), shows parts of the ontological semantics for Afghanistan
that are structured in a tree-like hierarchy.7 An ontology provides concepts that are
used to define propositions in TMR. The TMR of a natural language expression can
show information such as clause relationship, author attitude, and topic composition.
It is constructed through mapping lexical items and events that are referred in the
expression to their ontology concepts. Figure 4(a) shows the TMR of the sentence the
United States are attacking Afghanistan. A modification of the tree can be performed
by grafting additional semantic information of Afghanistan as shown in Figure 4(b),
yielding the alternative sentence the United States are attacking Afghanistan, which is ruled
by Mullah Mohammed Omar. Vybornova and Macq (2007) also exploited the linguistic
7 Afghanistan was ruled by Mullah Mohammed Omar at the time of Atallah et al. (2002).
413
Computational Linguistics Volume 40, Number 2
phenomenon of presupposition, with the idea that some presuppositional information
can be removed without changing the meaning of a sentence.
Another group of studies aims to use machine-translated sentences as the alterna-
tives. The main advantage of using machine-translated text is that translations are not
perfect and therefore it is hard to determine whether the anomalies are introduced by a
translation system or due to the camouflage of secret information.
The first translation-based stegosystem was proposed by Grothoff et al. (2005). In
their method, the sender uses a set of machine translation systems to generate multiple
translations for a given cover sentence. Stutsman et al. (2006) also utilized multiple
translation systems to output alternatives for a cover sentence. Because Grothoff et al.
(2005) and Stutsman et al. (2006) used multiple machine translation systems to generate
alternative translations, which leads to a stego text containing a mixture of translations
generated from different systems and each stego sentence may have different statistical
distribution of features (e.g., percentage of high-frequency words), a simple comparison
of the statistical distribution of features obtained from a normal text and from a stego
text might be able to detect the existence of the secret message (Meng et al. 2010;
Chen et al. 2011). Instead of obtaining alternative translations from multiple translation
systems, Meng et al. (2011) and Venugopal et al. (2011) used a statistical machine
translation system to generate the n-best translations for a given cover sentence. Because
translations are from one system, each of them is more similar to the rest than that
derived from another translation system.
Another of our papers (Chang and Clark 2012b) proposed a word-ordering-based
stegosystem, where the word-ordering technique can be seen as a ?monolingual trans-
lation? that translates a cover sentence into different permutations. Because not all
the sentence permutations generated by a word-ordering system are grammatical and
semantically meaningful, we developed a maximum entropy classifier to distinguish
natural word orders from awkward ones.
Another possible semantic transformation for linguistic steganography is sentence
compression (Dorr, Zajic, and Schwartz 2003; Cohn and Lapata 2008; Zhu, Bernhard,
and Gurevych 2010). Different compressed versions of the original text provide various
alternatives for a stegosystem. We developed a compression-based stegosystem that
generates alternatives for a cover sentence by removing unnecessary adjectives in noun
phrases (Chang and Clark 2012a). For example, he spent only his own money and he
spent only his money almost express the same meaning. In order to certify the deletion
grammaticality, we only accept a deletion that does not change the CCG categories of
the words in the rest of the sentence. In addition, we propose two methods to determine
whether the adjective in a noun phrase is necessary to the context. The first method
uses the Google n-gram corpus, and the second method, which performs better, trains
a support vector machine model that combines n-gram statistics, lexical association
measures, entropy-based measures, and an n-gram divergence statistic.
2.2 Encoding Methods
In the previous sections we have explained different transformation methods for gen-
erating alternatives for an input text. This procedure is seen as the linguistic transfor-
mation module in Figure 2. After deriving alternatives for a cover text, the encoder
generation module maps each alternative to a code that can be used to represent a secret
bitstring. In this section, we introduce four encoding methods that assign bitstrings
to the alternative candidates and have been used in existing stegosystems. In order
to demonstrate each encoding method, we assume the cover sentence is we finish the
414
Chang and Clark Practical Linguistic Steganography
charitable project and the transformation applied to the text consists of simply replacing
a word with its synonym. The alternatives for the cover text arise from replacing
finish with complete, and replacing project with labor, task, or undertaking. Note that, as
mentioned earlier, linguistic transformations are largely independent of the encoding
methods and therefore the encoding methods explained here are not restricted to lexical
substitutions. After encoding the alternatives, the secret can be embedded by selecting
alternatives that directly associate with the secret bitstring.
2.2.1 Block Code Method. For a set with cardinality n, the block code method assigns
m-bit binary codes from 0 to 2m ? 1 to the elements in the set, where 2m ? n. For exam-
ple, the synonym set {complete, finish} has cardinality n = 2 so 1-bit binary codes 0 and 1
are assigned to complete and finish, respectively. Because the synonym set {labor, project,
task, undertaking} has cardinality n = 4, the block code method can use either one-bit or
2-bit codes to encode the words as shown in Figure 5. When one-bit codes are used, both
labor and task represent code 0, and both project and undertaking represent code 1; when
two-bit codes are used, the four words are assigned different codes 00, 01, 10, and 11.
The advantage of using one-bit codes is that the cover word project needs to be replaced
with its synonym only 50% of the time, whereas the two-bit scheme has a 75% chance of
modifying the cover word, assuming the secret is a random bitstring. However, one-bit
codes embed less information. Hence, there is a trade-off between security and payload
capacity. It is worth noting that, in this simple scheme, each block code representation
has the same probability of being chosen, even though native speakers might have a
preference for the choice of synonyms, which would be security-relevant. For example,
if the block code method is applied to Table 1, wed, tie, and splice would have the same
probability of replacing marry in the cover sentence The minister will marry us on Sunday.
However, of these three alternatives only wed is allowed in this context; hence choosing
tie or splice may arouse suspicion in others.
2.2.2 Mixed-Radix Number Method. In a mixed-radix number system, the numerical base
differs from position to position. For example, 8 hours, 41 minutes, and 21 seconds
can be presented relative to seconds in mixed-radix notation as: 8(24)41(60)21(60), where
each digit is written above its associated base. The numerical interpretation of a mixed-
radix number an(bn )an?1(bn?1 )...a0(b0 ) is anbn?1bn?2...b0 + an?1bn?2bn?3...b0 + ...+ a1b0 +
a0, and any number can be uniquely expressed in mixed-radix form (Soderstrand et al.
1986).
Figure 6 shows the use of the mixed-radix number method with the lexical substi-
tution example which is described in Bergmair (2004). Firstly, the words in the synsets
{complete, finish} are encoded with 0 and 1 with base 2, and the words in the synset
{labor, project, task, undertaking} are encoded with 0, 1, 2, and 3 with base 4. Therefore,
the combinations of the substitutions yield the two-digit mixed-radix numbers from
0204 to 1234, which are equal to the decimal numbers 0 to 7. Assume the secret bitstring
1-bit Word 1-bit 2-bit Word
We 0 complete the charitable 0 00 labor .
1 finish 1 01 project
0 10 task
1 11 undertaking
Figure 5
An example of the block code method.
415
Computational Linguistics Volume 40, Number 2
Code Word Code Word
We 02 complete the charitable 04 labor .
12 finish 14 project
24 task
34 undertaking
Figure 6
An example of the mixed-radix number method.
to be embedded is 110, which can be seen as the binary number for six. Because we
finish the charitable task represents the mixed-radix number 1224, which is the decimal
number 6, this sentence will be the stego sentence that embeds the secret. Like the
Block code method, each mixed-radix number representation has the same probability
of being chosen, which may have security implications. To solve this issue, one can
utilize variable-length code methods described in the next section.
2.2.3 Huffman Code Method. Figure 7 demonstrates the use of variable-length codes, in the
form of the Huffman code (Huffman 1952), for encoding words in a synset. Assuming
there is a utility score for each word, then the Huffman algorithm determines a way
to produce a variable-length binary string for each word. More importantly, it does so
in such a way that an optimal encoding is created; that is, words with higher utility
have shorter codes whereas words with lower utility get longer codes. Thus, words
frequently used by native speakers are more likely to be chosen by the stegosystem
(assuming utility corresponds to frequency). In addition, when making a low-utility
choice, the tree ensures that maximal (bitrate) benefit will be derived from that choice.
The process shown in Figure 8 begins with leaf nodes each containing a word along with
its associated probability. The two nodes with the smallest probabilities are then chosen
to become the children of a new node whose probability is the sum of the probabilities
of its children. The newly created left and right branches are assigned bit 0 and 1,
respectively. Now only the newly created node is taken into consideration instead of its
children. The procedure is repeated until only one node remains, thereby constructing
the Huffman tree. To determine the binary code assigned to a particular word, we start
from the root node and gather the bits on the path to the leaf node connected to that
word. In this example we can see that project has the highest probability among words
in the same synset and is encoded with the shortest code. Thus, it is more likely to match
the secret bitstring.
One existing stegosystem that uses Huffman code encoding is proposed by Grothoff
et al. (2005). Their system takes different translations of a cover sentence as alternatives,
each of which is assigned a probability that represents the quality of the translation.
Then a Huffman tree is built for the translations based on the probabilities, where
poorer translations are at the bottom of the tree (with lower probabilities), and quality
Code Word Prob. Code Word Prob.
We 0 complete 0.77 the charitable 110 labor 0.05 .
1 finish 0.23 0 project 0.69
10 task 0.25
111 undertaking 0.01
Figure 7
An example of the Huffman code method.
416
Chang and Clark Practical Linguistic Steganography
Figure 8
The process of constructing a Huffman tree.
translations are higher in the tree. In other words, poor translations have longer codes
while better translations have shorter codes. This ensures that quality translations ap-
pear more often, and when a poorer translation (and thus potentially more perceptible
sentence) appears, it transmits a maximal number of bits.
As described in Section 2.1.2, Wayner (1995) generates stego text by exploiting a
probabilistic context-free grammar. His method creates a Huffman tree for each set of
productions that expand the same non-terminal symbol. In this way, each production
has its own Huffman code representation, as shown in Figure 9(a). Then, we begin
with a designated start-symbol S, and expand a non-terminal symbol by choosing
the production whose Huffman code representation is identical to the portion of the
Rule No. Rule Code Probability
1. S?AB 0 0.3
2. S?AC 1 0.7
3. A?I 0 0.4
4. A?You 10 0.3
5. A?He 110 0.15
6. A?She 111 0.15
7. B?lost 0 0.4
8. B?won 1 0.6
9. C?lost the D 0 0.4
10. C?won the D 1 0.6
11. D?game 0 0.4
12. D?match 10 0.3
13. D?championship 110 0.2
14. D?competition 111 0.1
(a)
Position Prefix Rule Output
?1101110 1 2. AC
1?101110 10 4. You C
110?1110 1 10. You won the D
1101?110 110 13. You won the championship
(b)
Figure 9
An example of the Wayner (1995) mimicry method.
417
Computational Linguistics Volume 40, Number 2
secret bitstring. The procedure is repeated until a grammatical message is generated.
In the embedding example given in Figure 9(b), the secret bitstring is 1101110 and a
symbol ??? is used to indicate the current bit in reading the string. At the beginning,
the prefix string of the secret message ?1101110 is ?1? which is associated with the
second production, so the start-symbol S is expanded to AC. Now, the prefix string of
the message 1?101110 becomes ?10?. The fourth production is applied, and a string
?You C? is generated. Next, we see the prefix string ?1? in the message 101?110,
and therefore, the output string turns into ?You won the D?. Finally, the end of the
secret message 1101110? is reached, and a stego sentence You won the championship is
generated. Theoretically, the block code representation or the mixed-radix technique
explained in the previous sections can be utilized in Wayner?s stegosystem.
2.2.4 Hash Function Method. For the block code method, the mixed-radix number ap-
proach, and the Huffman code representation, the encoding process is dependent on
knowing all the alternatives (e.g., the synset). Hence, in order to extract the code
assigned to the stego text during the secret recovery process, all the alternatives must be
known to the receiver as well. Note that the receiver does not need to know the original
cover text. However, not all the linguistic transformations can meet this requirement.
For example, if the sender encodes the four best machine translations of the cover
sentence using block coding and sends the translation that represents the secret bits to
the receiver, it is unlikely that the receiver can retrieve the four best machine translations
without knowing the original cover sentence. Thus, the secret recovery fails. For this
reason, Stutsman et al. (2006), Meng et al. (2011), Venugopal et al. (2011), and Chang and
Clark (2012b) used a hash function to map a translation to a code, which is independent
of the rest of the alternatives.
Venugopal et al. (2011) defined a random hashing operation that maps a translation
to a bit sequence of fixed length. Venugopal et al. stated that a good hash function
should produce a bitstring whose 0s and 1s are generated with equal probability.
Stutsman et al. (2006) proposed a hash function encoding scheme which uses the first
h bits of a translation hash bitstring as the header bits and the next b bits as the code
represented by the translation, where h is shared between the sender and the receiver,
and b is the integer represented by the header bits. For example, assume h = 2; a hash
bitstring ?1011. . . ? has header bits 10 to indicate a 10(2)-bit code is carried by this
translation, and the two-bits are 11. Among all possible translations of a cover sentence,
the one with the combination of header and information-carrying bits for the given h
representing the next b bits of the message is chosen as the stego sentence.
2.3 Stegosystem Evaluations
So far we have introduced different linguistic transformations used to produce alter-
natives for a cover text as well as some encoding methods that are used to assign a
bitstring to a candidate. The final procedure is text selection, in which an alternative
that represents the secret bits is chosen as the stego text. We can see that the quality
of a stego text mainly relies on the quality of the applied linguistic transformation,
typically requiring sophisticated NLP tools and resources to produce a realistic stego
text. However, given the current state-of-the-art, such NLP techniques cannot guarantee
the transformation?s imperceptibility. Hence it is important to evaluate a stegosystem.
A stegosystem can be evaluated from two aspects: the security level and the embed-
ding capacity. The security assessment methods used so far can be classified into two
categories: automatic evaluation and human evaluation. Topkara, Topkara, and Atallah
418
Chang and Clark Practical Linguistic Steganography
(2006a) and Topkara et al. (2006) used machine translation evaluation metrics BLEU
(Papineni et al. 2002) and NIST (Doddington 2002), automatically measuring how close
a stego sentence is to the original. Topkara, Topkara, and Atallah (2006a) admitted that
machine translation evaluation metrics are not sufficient for evaluating stegosystems;
for example, BLEU relies on word sequences in the stego sentence matching those in
the cover sentence and thus is not suitable for evaluating transformations that change
the word order significantly.
The other widely adopted evaluation method is based on human judgments. Meral
et al. (2007, 2009) and Kim (2008, 2009) asked participants to edit stego text for im-
proving intelligibility and style. The fewer edit-hits a transformed text received, the
higher the reported security level. Murphy and Vogel (2007a, 2007b) first asked subjects
to rate the acceptability (in terms of plausibility, grammaticality, and style) of the stego
sentences on a seven-point scale. Then participants were provided with the originals
and asked to judge to what extent meaning was preserved, also on a seven-point scale.
In Chang and Clark (2010a) we asked participants to judge whether a paraphrased
sentence is grammatical and whether the paraphrasing retains the meaning of the
original. In Chang and Clark (2012a) we asked participants to annotate the naturalness
of the resulting sentences after adjective deletions; and in Chang and Clark (2012b) we
asked participants to rate the naturalness of sentence permutations on a four-point scale.
For the work presented in this article, we also use human judgments to evaluate the
proposed stegosystem, as this is close to the linguistic steganography scenario where
we assume the adversary is a human acting passively.
The other aspect of the stegosystem evaluation is to calculate the amount of data
capable of being embedded in a stego text, which can be quantified in terms of bits
of hidden message per bit transmitted or per language unit (e.g., per word or per
sentence). Payload measurements can be theoretical or empirical. The theoretical pay-
load measurement only depends on an encoding method and is independent of the
quality of a stego text; the empirical measurement takes the applicability of a linguistic
transformation, namely, the security of a stego text, into consideration and measures
the payload capacity while a certain security level is achieved. Most of the payload
rates reported in existing work are based on empirical measurements.
For the lexical substitution transformation, Topkara, Taskiran, and Delp (2005) and
Topkara, Topkara, and Atallah (2006b) achieved an average embedding payload of
0.67 bits per sentence, despite the large number of synonyms in English. In Chang
and Clark (2012a) we showed that the payload upper bound of using the adjective
deletion technique is around 0.4 bits per sentence if a deletion represents a secret
bit. The payload attained by syntactic transformations was around 0.5 to 1.0 bits per
sentence. For example, both Atallah et al. (2001) and Topkara, Topkara, and Atallah
(2006a) achieved an embedding payload of 0.5 bits per sentence, and Meral et al. (2009)
reported the data embedding rate of their system as 0.81 bits per sentence. Because the
ontological semantic transformation is currently impractical, the empirical payload is
not available for this transformation type. Another semantic method (Vybornova and
Macq 2007) that aims at modifying presuppositional information in text achieved a
payload of 1 bit per sentence through the use of a secret key to indicate sentences
with or without presupposition information. Stutsman et al. (2006) showed that their
translation-based stegosystem has a payload of 0.33 bits of hidden message for every
100 bits of data transmitted.
Not only the linguistic transformation and the encoding method, but also the choice
of cover text, can affect the security level and the payload capacity of a stegosystem.
For example, if a newspaper article were chosen as the cover text, then any changes
419
Computational Linguistics Volume 40, Number 2
could be easily found in practice by comparing the stego text with the original article,
which is likely to be readily available. In addition, an anomaly introduced by a linguistic
transformation may be more noticeable in a newspaper article than in a blog article. In
terms of payload capacity, a synonym substitution?based stegosystem may find more
words that can be substituted in a storybook than in a car repair manual because there
are usually many terminologies in a manual which cannot be changed or even cannot be
found in a standard dictionary (assuming the system does not happen to have a detailed
ontology of car parts). To the best of our knowledge, there is no study on the practical
issue of using different types of cover text for the steganography application.
3. Lexical Substitution
In the following sections we introduce our linguistic stegosystem based on lexical
substitution. In the original work on linguistic steganography, Winstein (1999) proposed
an information-hiding algorithm using a block coding method to encode synonyms, so
that the selection of a word from a synset directly associates with part of the secret
bitstring. An example of Winstein?s system can be found in Figure 5. In his system, a
sender and a receiver share the same coded synonym dictionary as the secret key. To
recover the hidden message, the receiver first seeks words in the stego text that can be
found in the shared dictionary. Those words are information carriers, and therefore the
codes assigned to them are secret bitstrings. Note that the receiver does not need the
original cover text to recover the secret message.
One of the problems faced by a synonym-based stegosystem is that many words are
polysemous, having more than one sense, and this may cause ambiguities during the
secret recovery stage. In WordNet a synset contains words expressing a similar concept,
and a word may appear in more than one synset. For example, both marry and wed
appear in the two synsets in Table 1. Figure 10 shows what happens when the block
coding method is applied to the two overlapping synsets, assuming the stego sentence
received by the receiver is the minister will marry us on Sunday. Note that we only take
single word substitution into consideration in order to avoid the confusion of finding
information carriers during the secret recovering phase. For example, if the cover word
espouse is replaced by hook up with, the receiver would not know whether the secret
message is embedded in the word hook or the phrase hook up with. After deleting multi-
word synonyms, words in the two synsets are sorted alphabetically and assigned two-
bit codes. As can be seen in Figure 10, marry is encoded by two different codewords
and thus the secret bitstring cannot be reliably recovered, because the receiver does not
know the original cover word or the sense of the word.
In order to solve the problem of words appearing in more than one synonym
set, Winstein defines interchangeable words as words that are always synonyms to
each other even under different meanings (i.e., they always appear together in the
Synset 1 Synset 2
Word Code Word Code
conjoin 00 marry 00
espouse 01 splice 01
marry 10 tie 10
wed 11 wed 11
Figure 10
An example of decoding ambiguity using lexical substitution.
420
Chang and Clark Practical Linguistic Steganography
same synsets). For example, marry and wed are interchangeable words under Winstein?s
definition. The advantage in this approach is that interchangeable words always receive
the same codeword. The disadvantage is that many synonyms need to be discarded
in order to achieve this property. As mentioned previously, Winstein reported that
only 30% of words in WordNet are interchangeable words. In addition, as explained
in Section 2.1.1, many synonyms are only applicable in certain contexts. However, in
Winstein?s steganography scheme there is no method to filter out unacceptable substi-
tutions so the generated stego text may be unnatural and arouse suspicion in others.
Another synonym substitution-based stegosystem was proposed by Bolshakov
(2004), who applies transitive closure to overlapping synsets to avoid the decoding
ambiguity. Applying transitive closure leads to a merger of all the overlapping synsets
into one set which is then seen as the synset of a target word. Consider the overlapping
synsets in Figure 10 as an example. After applying transitive closure, the resulting set is
{conjoin, espouse, marry, splice, tie, wed}. The disadvantage of Bolshakov?s system is that
all words in a synonym transitive closure chain need to be considered, which can lead to
very large sets of synonyms, many of which are not synonymous with the original target
word. For this reason, Bolshakov used a collocation-based test to remove unsuitable
words after merging the synsets. Finally, the collocationally verified synonyms are
encoded using the block coding method. Note that in Bolshakov?s system it is possible
to replace an original word with a non-synonymous word if the non-synonymous
word passes the collocation-based test.
Similar to Bolshakov?s method, our approach takes words in a synonym transitive
closure chain into consideration and assigns a score to each word using the proposed
substitution checker. A score threshold is applied to eliminate low-score words; that
is, the remaining words are both in the synonym transitive closure chain as well as
acceptable to the context. More details of the proposed substitution checker will be
described later. We then construct a synonym graph that has a vertex for each remaining
word and an undirected edge for every pair of words that share the same meaning.
After constructing the synonym graph, we use a novel vertex coding method inspired
by vertex coloring to assign codes to every word in the graph.
A crucial difference from Bolshakov?s method is that in our approach the sender
only considers words that are synonymous with the cover word as alternatives, even
though the other words in the synonym graph can also fit into the context. The reason
for also including non-synonymous words during the encoding is because the receiver
does not know the cover word and, therefore, we need a method to ensure that the
receiver is encoding the same list of words, namely, the same synonym graph, as the
sender during the secret recovery. In other words, the sender and the receiver must
derive the same synonym graph so that the sender knows the cover word and the
receiver knows the stego word.
Figure 11(a) shows a synonym graph constructed from a synonym transitive closure
chain that contains six synsets: {bind, tie}, {tie, draw}, {tie, wed, splice, marry}, {marry,
wed, espouse, conjoin}, {conjoin, join}, {join, link, unite, connect}. Assume the cover word
is conjoin. In Bolshakov?s system, there is a chance of replacing conjoin with draw, which
is three steps away from the original word in the graph; in our method, however,
we only consider a cover word?s synonyms as alternatives?that is, conjoin is only
allowed to be replaced by wed, espouse, marry, or join. Note that we have not applied
the substitution check in this example.
Now let us apply the substitution check to words in the synonym transitive closure
chain, and suppose join and marry do not pass the check. Figure 11(b) shows the
two disconnected synonym graphs G1 and G2 derived from the checked pool. The two
421
Computational Linguistics Volume 40, Number 2
(a) An unchecked synonym graph
(b) Synonym graphs derived after substitution checking
(c) Another example of checked synonym graphs
Figure 11
Synonym graphs with and without the substitution check.
synonym graphs are then encoded independently. In other words, the encoding of G1
does not affect the codes assigned to the words in G2. Because conjoin is the cover word,
the system may replace conjoin with either wed or espouse, or keep the original word
depending on the encoding of G1 and the secret bits. Assume wed is chosen as the stego
word. In order to work out the embedded message, the receiver needs to construct and
encode the same graphs as those generated by the sender. The decoding process starts
from extracting the synonym transitive closure chain of wed, and then applying the
substitution checker to the pool to filter out unacceptable words. Because the remaining
words are the same as those used by the sender, the receiver can successfully extract
the secret bits after constructing and encoding the synonym graphs.
Because the proposed substitution checker measures the acceptability of a word
according to the context, the synonym graph for a target word varies depending on its
context. Let us consider another case where the cover word is still conjoin, but this time
the substitution checker determines that conjoin, espouse, and marry are not acceptable to
the context. Figure 11(c) shows the corresponding synonym graphs of the remaining
words. In this case, the applicable alternatives are either wed or join because they
are synonyms of conjoin. As mentioned previously, disconnected graphs are encoded
independently. Therefore, it is possible that both wed and join are assigned the same
codeword which does not match the secret bits. If neither of the synonyms can be used
as the stego word, the sender will keep the original word and send conjoin to the receiver.
During the decoding process, the receiver should be able to know that conjoin fails the
check and thus does not carry any message. In contrast, if wed and join are encoded
422
Chang and Clark Practical Linguistic Steganography
by different codewords, say 0 and 1, respectively, the system can choose the one that
represents the secret bit as the stego word.
3.1 Substitution Checkers
The aim of the proposed checkers is to filter out inapplicable substitutes given the orig-
inal word in context. The substitution checkers must not only work with the proposed
linguistic stegosystem, but can also be integrated into other synonym substitution-
based applications to certify the transformation quality. The following sections are
organized so that the basic substitution checker using the Google n-gram corpus (Brants
and Franz 2006) is described first. Then we introduce the ?-skew divergence measure
(Lee 1999) that can be combined with the basic n-gram method. The proposed checkers
are evaluated using data from the SemEval lexical substitution task (McCarthy and
Navigli 2007), which is independent of the steganography application. We also perform
a more direct evaluation of the imperceptibility of the steganography application by
asking human judges to evaluate the naturalness of sentences. After explaining the
linguistic transformation module in our stegosystem, we proceed with the encoder
generation module and present the vertex coding method. Finally, we use an example
to demonstrate the complete stegosystem.
3.1.1 n-Gram Count Method (NGM). The basic checking method, referred to as NGM,
utilizes the Google n-gram corpus to calculate a substitution score for a candidate word
in context based on Bergsma, Lin, and Goebel (2009). The Google n-gram corpus was
collected by Google Research for statistical language modeling, and has been used for
many tasks such as spelling correction (Carlson, Mitchell, and Fette 2008; Islam and
Inkpen 2009), multi-word expression classification (Kummerfeld and Curran 2008), and
lexical disambiguation (Bergsma, Lin, and Goebel 2009). It contains frequency counts
for n-grams from uni-grams through to 5-grams obtained from over 1 trillion word
tokens of English Web text. Only n-grams appearing more than 40 times were kept in
the corpus.
The checking method first extracts contextual bi- to 5-grams around the word to be
tested and uses the Minnen, Carroll, and Pearce (2001) tools for correcting the form of
an indefinite and a verb?s tense. For example, if the word to be tested is maverick and it is
going to replace unorthodox in the phrase the help of an unorthodox speech therapist named
Lionel, the indefinite an will be corrected as a when extracting contextual n-grams. As
another example, assume the word to be replaced is bleach in the original phrase he
might be bleaching his skin; then a verb substitute decolor will be corrected as decoloring
because the original word is in the progressive tense.
After extracting contextual bi- to 5-grams, the checking method queries the n-gram
frequency counts from the Google n-gram corpus. For each n, the total count fn is
calculated by summing up individual n-gram frequencies, for every contextual n-gram
containing the candidate word. We define a count function:
Count(w) =
5?
n=2
log(fn),
where log(0) is defined as zero. If Count(w)=0, we assume the word w is unrelated to
the context and therefore is eliminated from the synonym transitive closure chain. After
calculating Count(w) for each word in the pool, the word that has the highest count
423
Computational Linguistics Volume 40, Number 2
is called the most likely word and its count is referred as maxcount. The main purpose of
having maxcount is to score each word relative to the most likely substitute in the chain,
so even in less frequent contexts which lead to smaller frequency counts, the score of
each word can still indicate the degree of feasibility. We also need to use the most likely
word, rather than the original cover word, because the receiver does not have access to
the cover text when applying the check. The most likely word in the context may be the
original word or another word in the synonym transitive closure chain. The substitution
score is defined as:
ScoreNGM(w) =
Count(w)
maxcount
The hypothesis is that a word with a high score is more suitable for the context, and we
apply a threshold so that words having a score lower than the threshold are discarded.
Consider as an example the calculation of the substitution score for the candidate
word clever as a possible replacement for the word bright in the cover sentence he
was bright and independent and proud. First of all, various contextual n-grams are ex-
tracted from the sentence and the Google n-gram corpus is consulted to obtain their
frequency counts, as shown in Figure 12. The derived fn values can then be used to
calculate Count(clever), which is 27.5 in this example. Suppose the threshold is 0.9, and
the maxcount is 30 from the synonym transitive closure chain. The substitution score is
as follows:
ScoreNGM(clever) =
Count(clever)
maxcount =
27.5
30 = 0.92
which is greater than the threshold (0.9), and so the word clever is determined as
acceptable for this context and is kept in the pool.
One disadvantage of using n-gram statistics is that high-frequency n-grams may
dominate the substitution score, especially lower-order n-grams. For example, even is
not a good substitute for eve in the sentence on the eve of the wedding, Miranda tells
Mr. Big that marriage ruins everything, but it still has a reasonably high score of 0.74
since the bigrams the even and even of have high frequency counts compared with those
of the 4-grams and 5-grams. As a way of overcoming this problem, we consider the
n-gram frequency fn
was clever 40,726 f2 = 302,492
clever and 261,766
He was clever 1,798 f3 = 8,072
was clever and 6,188
clever and independent 86
He was clever and 343 f4 = 343
was clever and independent 0
clever and independent and 0
He was clever and independent 0 f5 = 0
was clever and independent and 0
clever and independent and proud 0
Figure 12
n-grams and their frequency counts for calculating ScoreNGM(clever).
424
Chang and Clark Practical Linguistic Steganography
C21 C22 C31 C32 C33 C41 C42 C43 C51 C52 C53
bright 0.081 0.892 0.002 0.024 0.0002 0 0 0 0 0 0
clever 0.130 0.843 0.006 0.020 0.0002 0.001 0 0 0 0 0
Figure 13
n-gram frequency distributions of bright and clever for calculating the contextual divergence.
n-gram distributional similarity between a most likely word and a candidate substitute
in context using ?-skew divergence as explained in the next section. We assume that an
acceptable substitute should have a similar n-gram distribution to the most likely word
across the various n-gram counts.
3.1.2 Contextual ?-skew Divergence. The ?-skew divergence is a non-symmetric measure
of the difference between two probability distributions P and Q. Typically, P represents
the observations, in our case the n-gram count distribution of the most likely word, and
Q represents a model, in our case the candidate?s distribution. The ?-skew divergence
measure is defined as:
S?(Q, P) = D(P???Q + (1? ?)?P)
where 0 ? ? ? 1 and D is the Kullback-Leibler divergence (Kullback 1959):
D(P?Q) =
?
v
P(v) log
P(v)
Q(v)
The ? parameter allows us to avoid the problem of zero probabilities, and in our
method we use ? = 0.99. The value of the ?-skew divergence measure is zero if the
two probability distributions are identical and increases positively as the distributions
become less similar.
We use the following example to demonstrate how to calculate the contextual
divergence between two words. Assume the most likely word is bright and a substitute
to be considered is clever. First we need to derive the n-gram frequency distributions
of both words. We divide each n-gram frequency by the total frequency to get Cni, as
shown in Figure 13, where i means the ith n-gram (e.g., C32 is the second trigram). For a
word, Cni should sum up to 1 (over all n, i). Then we can calculate the?-skew divergence
of these two distributions:
S?(clever,bright) =
?
n
?
i
Cbrightni ? log(
Cbrightni
?Ccleverni + (1? ?)C
bright
ni
) = 0.014
Similar to the NGM method, we define a score function:
ScoreDVG(w) = 1?
S?(
??w ,
??????????????
the most likely word)
maxdivergence
where ??w and
??????????????
the most likely word are the probability distributions of n-gram counts
of the target substitute and the most likely word, respectively, and maxdivergence is the
maximum divergence between the most likely word and another word in the synonym
425
Computational Linguistics Volume 40, Number 2
transitive closure chain. In this example, suppose maxdivergence is 0.15 and therefore we
can derive the contextual divergence-based substitution score:
ScoreDVG(clever) = 1?
0.014
0.15 = 0.91
The reason to calculate S?(
??w ,
????????????
the most likely word)
maxdivergence is to spread the divergence score between
0 and 1. Note that the higher the divergence S?(
??w ,
??????????????
the most likely word) is, the lower
the score ScoreDVG(w). Finally we combine the distributional similarity with the NGM
method, referred to as NGM DVG method, by modifying the score function as follows:
ScoreNGM DVG(w) = ??ScoreNGM(w) + (1? ?)?ScoreDVG(w)
where 0 ? ? ? 1. The value of ? determines the relative weights of ScoreNGM(w) and
ScoreDVG(w).
3.2 Ranking Task Evaluation
Both NGM and NGM DVG assign a score to a word according to the context and the
most likely word in the group of alternatives. In order to evaluate the performance
of the proposed scoring methods, we apply our approaches to a ranking task that
requires a system to rank a list of substitute words given an original word and its
context. The task can test whether the proposed methods are capable of assigning higher
scores to appropriate substitutes than to unacceptable ones and thus is useful for the
steganography application. The gold standard data is derived from the English lexical
substitution task for SemEval-2007 (McCarthy and Navigli 2007) and the evaluation
measure used is Generalized Average Precision (Kishida 2005). In this section we first
describe the gold standard data used in this evaluation and then provide the results. We
compare our results with three other models developed by Erk and Pado? (2010), Dinu
and Lapata (2010), and O? Se?aghdha and Korhonen (2011), all of which are designed
for measuring word meaning similarity in context. Note that our substitution checkers
do not aim at modeling word similarity, and therefore the result comparison is just
trying to show that our substitution checkers are competitive. Later, we will evaluate
the proposed checkers with the human annotated data and see whether our methods
would be practical for linguistic steganography.
3.2.1 Data. For this evaluation, we use the SemEval-2007 lexical substitution data set as
the gold standard. The original purpose of the data set was to develop systems that
can automatically find feasible substitutes given a target word in context. The human
annotation data comprises 2,010 sentences selected from the English Internet Corpus
(Sharoff 2006), and consists of 201 target words: nouns, verbs, adjectives, and adverbs,
each with ten sentences containing that word. The five annotators were asked to provide
up to three substitutes for a target word in the context of a sentence, and were permitted
to consult a dictionary or thesaurus of their choosing. After filtering out annotation
sentences where the target word is part of a proper name and for which annotators
could not think of a good substitute, the data was separated into 298 trial sentences
and 1,696 test sentences. Table 3 illustrates two examples from the gold standard, both
featuring the target word bright. The right column lists appropriate substitutes of bright
426
Chang and Clark Practical Linguistic Steganography
in each context, and the numbers in parentheses indicate the number of annotators who
provided that substitute.
To allow comparison with previous results reported on the substitution ranking
task, following Erk and Pado? (2010), Dinu and Lapata (2010) and O? Se?aghdha and
Korhonen (2011), we pool together the positive substitutes for each target word, con-
sidering all contexts, and rank the substitutes using our scoring methods. For instance,
assume in the gold standard there are only two sentences containing the target word
bright as shown in Table 3. We merge all the substitutes of bright given by the annotators
and derive a large candidate pool {intelligent, clever, colorful, brilliant, gleam, luminous}.
We expect intelligent and clever to be ranked at the top of the list for the first sentence,
with colorful, brilliant, gleam, and luminous ranked at the top for the second sentence.
3.2.2 Experiments and Results. In the SemEval-2007 lexical substitution task participants
were asked to discover possible replacements of a target word so the evaluation metrics
provided are designed to give credit for each correct guess and do not take the ordering
of the guesses into account. In contrast, in the ranking task a system is already given a
fixed pool of substitutes and is asked to recover the order of the list. Therefore, we use
the Generalized Average Precision (GAP) to evaluate the ranked lists rather than the
metrics provided in the SemEval-2007 lexical substitution task. GAP rewards correctly
ranked items with respect to their gold standard weights while the traditional average
precision is only sensitive to the relative positions of correctly and incorrectly ranked
items. Let G = ?g1, g2, ..., gm? be the list of gold substitutions with weights ?y1, y2, ..., ym?
for a target word in context. In our task, the weight is the frequency of a substitute
in the gold standard. Let S = ?s1, s2, ..., sn? be the system ranked substitute list and
?x1, x2, ..., xn? be the weights associated with them, where m ? n and xi = 0 if si is not in
the gold list and G ? S. Then
GAP(S, G) = 1?m
j=1 I(yj)y?j
n?
i=1
I(xi)x?i and x?i =
1
i
i?
k=1
xk
where I(xi) = 1 if xi is larger than zero, zero otherwise; x?i is the average gold weight of
the first i system ranked items; y?i is defined analogously.
After experimenting on the trial data, we decided a ? value of 0.6 for the NGM DVG
method. We then applied the proposed NGM and NGM DVG methods to rank pooled
substitutes for each sentence in the test data. Table 4 summarizes the performances
of our approaches, where mean GAP values are reported on the whole test data as
well as for different POSs. We can see that the NGM DVG performs better than the
NGM system on the ranking task and achieved a mean GAP of 50.8% on the whole test
Table 3
Two sentences in the SemEval-2007 lexical substitution gold standard.
Sentence Substitutes
He was bright and independent and proud. intelligent(3), clever(3)
The roses have grown out of control, wild and carefree, their
bright blooming faces turned to bathe in the early autumn sun.
colorful(2), brilliant(1),
gleam(1), luminous(1)
427
Computational Linguistics Volume 40, Number 2
Table 4
GAP values (%) of the ranking task evaluation.
System test set noun verb adj adv
NGM 49.7 48.5 44.3 53.2 64.7
NGM DVG 50.8 50.9 44.6 53.7 66.2
Dinu and Lapata (2010) 42.9 n/a n/a n/a n/a
Erk and Pado? (2010) 38.6 n/a n/a n/a n/a
O? Se?aghdha and Korhonen (2011) 49.5 50.7 45.1 48.8 55.9
data. We then compare our results with those achieved by Erk and Pado? (2010), Dinu
and Lapata (2010), and O? Se?aghdha and Korhonen (2011). Erk and Pado? developed an
exemplar-based model for capturing word meaning in context, where the meaning of a
word in context is represented by a set of exemplar sentences most similar to it. Dinu
and Lapata proposed a vector-space model that models the meaning of a word as a
probability distribution over a set of latent senses. O? Se?aghdha and Korhonen also use
probabilistic latent variable models to describe patterns of syntactic interaction. The best
mean GAP values reported by Erk and Pado?, Dinu and Lapata, and O? Se?aghdha and
Korhonen are 38.6%, 42.9%, and 49.5% on the test data, respectively.
3.3 Classification Task Evaluation
Although the ranking task evaluation gives some indication of how reliable the pro-
posed scoring methods are, for the steganography application we require a system
that can correctly distinguish acceptable substitutes from unacceptable ones. Thus, we
conduct a classification task evaluation which is more related to the steganography
application. The task requires a system to determine acceptable substitutes from a group
of candidates given the word to be replaced and its context. Those passed substitutes
can then carry different codes and be used as stego words. Similar to the previous
section, we first describe the data and then explain the experimental setup and the
evaluation results.
3.3.1 Data. We use the sentences in the gold standard of the SemEval-2007 lexical
substitution task as the cover text in our experiments so that the substitutes provided
by the annotators can be the positive data. Because we only take into consideration
the single word substitutions, multi-word substitutes are removed from the positive
data. Moreover, we use WordNet as the source of providing candidate substitutes in
our stegosystem, so if a human-provided substitute does not appear in any synsets of
its target word in WordNet, there is no chance for our stegosystem to replace the target
word with the substitute; therefore, the substitute can be eliminated. Table 5 presents
the statistics of the positive data for our experiments.
In addition to the positive data, we also need some negative data to test whether our
methods have the ability to filter out bad substitutions. We extract the negative data for
our experiments by first matching positive substitutes of a target word to all the synsets
that contain the target word in WordNet. The synset that includes the most positive
substitutes is used to represent the meaning of the target word. If there is more than
one synset containing the highest number of positives, all of those synsets are taken
428
Chang and Clark Practical Linguistic Steganography
Table 5
Statistics of experimental data.
noun verb adj adv
number of target words 59 54 57 35
number of sentences 570 527 558 349
number of positives 2,343 2,371 2,708 1,269
number of negatives 1,914 1,715 1,868 884
into consideration. We then randomly select up to six single-word synonyms other than
positive substitutes from the chosen synset(s) as negative instances of the target word.
Let us use an example to demonstrate our automatic negative data collection. In
this example, we need to generate bad substitutions for a cover word remainder in the
sentence if we divide any number by 4, we would get 1 or 2 or 3, as the remainders, given
that the annotator-provided positives are leftover and residual. Table 6 lists the synsets of
remainder found in WordNet 3.1. Because the synset {remainder, balance, residual, residue,
residuum, rest} contains one of the positives whereas the other synsets do not, this synset
is selected for our negative data collection. We assume the selected synset represents
the meaning of the original word, and those synonyms in the synset which are not
annotated as positives must have a certain degree of mismatch to the context. Therefore,
from this example, balance, residue, residuum, and rest are extracted as negatives to test
whether our checking methods can pick out bad substitutions from a set of words
sharing similar or the same meaning.
In order to examine whether the automatically collected instances are true negatives
and hence form a useful test set, a sample of automatically generated negatives was
selected for human evaluation. For each POS one sentence of each different target word
was selected, which results in roughly 13% of the collected negative data, and every neg-
ative substitute of the selected sentences was judged by the second author of this article.
As can be seen from the annotation results shown in Table 7, most of the instances are
true negatives, and only a few cases are incorrectly chosen as false negatives. Because
the main purpose of the data set is to test whether the proposed checking methods can
guard against inappropriate lexical substitutions and be integrated in the stegosystem,
it is reasonable to have a few false negatives in our experimental data. Also, it is
Table 6
Synsets of remainder in WordNet 3.1.
remainder (noun)
gloss: something left after other parts have been taken away
synset: remainder, balance, residual, residue, residuum, rest
gloss: the part of the dividend that is left over when the dividend is not evenly divisible by
the divisor
synset: remainder
gloss: the number that remains after subtraction; the number that when added to the
subtrahend gives the minuend
synset: remainder, difference
gloss: a piece of cloth that is left over after the rest has been used or sold
synset: end, remainder, remnant, oddment
429
Computational Linguistics Volume 40, Number 2
Table 7
Annotation results for negative data.
noun verb adj adv
number of true negatives 234 201 228 98
number of false negatives 9 20 28 16
more harmless to rule out a permissible substitution than to include an inappropriate
replacement for a stegosystem in terms of its security. Table 5 gives the statistics of the
automatically collected negative data for our experiments.
3.3.2 Experiments and Results. We evaluate the classification performance of the NGM
system and the NGM DVG system in terms of accuracy, precision, and recall. Accuracy
is the percentage of correct classification decisions over all acceptable and unacceptable
substitutes; precision is the percentage of system accepted substitutes being human-
provided; recall is the percentage of human-provided substitutes being accepted by the
system. Accuracy is less important for the steganography application, and the reasons
for using precision and recall were explained in Section 1.4: A higher precision value
implies a better security level, and a larger recall value means a greater payload capacity.
It is worth noting that, although there will be a decrease in recall if more false negatives
are obtained from a system, there will not be a negative effect on the value of precision.
That is, from a security perspective, rejecting an acceptable substitute does not damage
the quality of stego text. However, it will lower the payload capacity so more stego text
transmission is needed in order to send the secret message, which may raise a security
concern.
Both the NGM system and the NGM DVG system require a threshold to decide
whether a word is acceptable in context. In order to derive sensible threshold values for
each POS, five-fold cross validation was used for the experiments. For each fold, 80%
of the data is used to find the threshold value which maximizes the accuracy, and that
threshold is then applied to the remaining 20% to get the final result.
We first test whether the proposed methods would benefit from using only longer
n-grams. We compare the performance of different combinations of n-gram counts,
which are frequency counts of bi- to five-grams, tri- to five-grams, four- to five-grams,
and five-grams only. The results show that for both methods the accuracy, precision,
and recall values drop when using fewer n-grams. In other words, among the four com-
binations, the one including bigram to five-gram frequency counts performs the best
across different POS and, therefore, is adopted in the NGM system and the NGM DVG
system. Next, we try weighting different sized n-grams in the proposed methods, as
have Bergsma, Lin, and Goebel (2009) in related work. According to the preliminary
experiments we conducted and the conclusion given by Bergsma, Lin, and Goebel, such
a method does not do much better than the simple method using uniform weights for
different sized n-grams.
Table 8 gives the results for the two checking methods and the average threshold
values over the five folds. In addition, for each POS, a simple baseline is derived by al-
ways saying a substitute is acceptable. From the table we can see that both the NGM and
the NGM DVG systems have higher precision than the baseline, which performs well
in terms of embedding capacity (100% recall) but at the expense of a lower security level
(lower precision). However, in contrast to the results of the ranking task evaluation, the
430
Chang and Clark Practical Linguistic Steganography
Table 8
Performance of the NGM and NGM DVG systems on the classification task.
NGM NGM DVG Baseline
POS Acc % Pre % Rec % Thr Acc % Pre % Rec % Thr Acc, Pre % Rec %
noun 70.2 70.0 80.2 0.58 68.1 66.5 67.3 0.70 55.0 100
verb 68.1 69.7 79.5 0.56 64.8 65.7 66.7 0.70 58.0 100
adj 72.5 72.7 85.7 0.48 70.2 68.8 77.7 0.63 59.2 100
adv 73.7 76.4 80.1 0.54 68.0 66.4 75.9 0.63 58.9 100
NGM system slightly outperforms the NGM DVG system. Because imperceptibility is
an important issue for steganography, we would prefer a system with a higher precision
value. Thus we adopt the NGM method as the linguistic transformation checker in our
lexical substitution-based stegosystem.
In addition, we are interested in the effect of the threshold value on the performance
of the NGM method. Figure 14 shows the precision and recall values with respect to
different thresholds for each POS. From the charts we can clearly see the trade-off
between precision and recall. Although a higher precision can be achieved by using
a higher threshold value?for example, noun substitutions reach almost 90% precision
with threshold equal to 0.9?the large drop in recall means many applicable substitutes
are being eliminated. In other words, the trade-off between precision and recall implies
the trade-off between imperceptibility and payload capacity for linguistic steganogra-
phy. Therefore, the practical threshold setting would depend on how steganography
users want to trade off imperceptibility for payload.
So far we have presented the performance of our checking methods using two
different automatic evaluations, the ranking task and the classification task. From the
ranking task evaluation we can see that the n-gram distributional similarity does have
the ability to further eliminate some bad substitutes after applying the basic n-gram
Figure 14
The performance of the NGM method under various thresholds.
431
Computational Linguistics Volume 40, Number 2
method. However, when facing the classification task, which is more related to the
steganography application, we find that the checking method simply based on counts
from the Google n-gram corpus is hard to beat. In addition, from the results of different
order n-gram combinations we can conclude that the more information we include
in the checking method (i.e., using all counts from bi- to five-grams) the better the
performance. This is similar to the conclusion given by Bergsma, Lin, and Goebel (2009).
3.4 Human Evaluation
We want to test how reliable the proposed NGM method is if it is used in a lexical
substitution-based stegosystem to guard against inappropriate substitutions. Therefore,
apart from the automatic evaluations, we conducted a more direct evaluation of the
imperceptibility for the steganography application by asking human judges to evaluate
the naturalness of sentences. In the following sections, we explain the evaluation data
first and then describe the evaluation setup and results.
3.4.1 Data. We collected a total of 60 sentences from Robert Peston?s BBC blog.8 For
each noun, verb, adjective, and adverb in a sentence, we first group the target word?s
synset(s) in WordNet and apply the NGM method with a score threshold equal to 0.95
to eliminate bad substitutes. If more than one substitute passes the check, the one with
the lowest score is used to replace the original word. The reason for choosing the word
with the lowest score is because this makes the test more challenging. This process is
applied to a sentence where possible and results in around two changes being made per
sentence.
We also generated another version of a sentence changed by random choice of a
target word and random choice of a substitute from a target word?s synset(s) (in order
to provide a baseline comparison). The number of changes made to a sentence using
this random method is the same as that in the version generated by the NGM method.
In this way, it is fair to compare the qualities of the two modified versions because both
of them receive the same number of substitutions. Table 9 shows lexical substituted
sentences generated by our method and by the random method. We can see that our
system replaces four words (in boldface) in the original sentence so the same number
of words (in boldface) are randomly selected when applying the random method. Note
that the random method just happens to pick the word big in the original sentence which
is also replaced by our system. We refer to an original sentence as COVER, a version
generated by our method as SYSTEM, and a version modified by the random method
as RANDOM.
3.4.2 Evaluation Setup and Results. The experimental setup follows a Latin square design
(Kirk 2012) with three groups of 10 native English speakers as shown in Table 10. In
this table, each row represents a set of annotation sentences for a group of judges,
and we can see that each sentence is presented in three different conditions: COVER,
SYSTEM, and RANDOM, as shown in a column. Subjects in the same group receive
the 60 sentences under the same set of conditions, and each subject sees each sentence
only once in one of the three conditions. The annotation process is Web-based. At the
beginning of the annotation task, we describe the aim of the annotation as shown in
Figure A1 in the Appendix. Subjects are asked to rate the naturalness of each sentence
8 http://www.bbc.co.uk/news/correspondents/robertpeston/.
432
Chang and Clark Practical Linguistic Steganography
Table 9
Different versions of a cover sentence.
Version Sentence
COVER Apart from anything else, big companies have the size and muscle to derive gains
by forcing their suppliers to cut prices (as shown by the furore highlighted in
yesterday?s Telegraph over Serco?s demand - now withdrawn - for a 2.5% rebate
from its suppliers); smaller businesses lower down the food chain simply don?t
have that opportunity.
SYSTEM Apart from anything else, large companies have the size and muscle to derive gains
by pushing their suppliers to cut prices (as evidenced by the furore highlighted in
yesterday?s Telegraph over Serco?s need - now withdrawn - for a 2.5% rebate from
its suppliers); smaller businesses lower down the food chain simply don?t have that
opportunity.
RANDOM Apart from anything else, self-aggrandizing companies have the size and muscle
to derive gains by forcing their suppliers to foreshorten prices (as shown by the
furore highlighted in yesterday?s Telegraph over Serco?s demand - now with-
drawn - for a 2.5% rebate from its suppliers); smaller businesses lower down the
food chain simply don?t birth that chance.
Table 10
Latin square design with three groups of judges.
s1, s2, . . . , s20 s21, s22, . . . , s40 s41, s42, . . . , s60
Group 1 COVER SYSTEM RANDOM
Group 2 RANDOM COVER SYSTEM
Group 3 SYSTEM RANDOM COVER
on a scale from 1 to 4 with score 1 meaning Poor English and score 4 meaning Perfect
English. Each judgment score is explained followed by an example sentence. Figure A2
in the Appendix shows a screen capture of an annotation example presented to a subject.
It is worth mentioning that we do not ask judges to spot changes in a text that has been
run through our system because a spotted change does not necessarily imply that a
sentence is unnatural; a spotted change might just be the result of the word preferences
and style of an individual judge.
The annotation results show that our judges gave an average score of 3.67 out
of 4 for the original sentences; 3.33 for the sentences checked by the NGM system;
and 2.82 for the randomly changed sentences. We measure the significance level of
our annotation results using the Wilcoxon signed-rank test (Wilcoxon 1945). The test
statistic shows that the differences between the three versions (original, system changed,
and randomly changed) are highly significant (p < 0.01). The payload capacity for
this level of imperceptibility is around two information carriers per sentence and each
information carrier guarantees to represent at least one bit. These results show that our
stegosystem achieves better payload capacity than existing lexical substitution-based
stegosystems which have achieved 0.67 bits per sentence (Topkara, Taskiran, and Delp
2005; Topkara, Topkara, and Atallah 2006b). In addition, in terms of security, the results
suggest that, with the proposed checking method, the quality of stego sentences is
improved compared with the random substitution baseline.
433
Computational Linguistics Volume 40, Number 2
Table 11
The average number of words for each word frequency rank (rank 1 is the lowest frequency and
rank 15 is the highest frequency).
rank 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
cover 961.1 56.7 35.5 29.3 36.7 45.5 55.4 49.2 48.4 39.6 31.2 23.9 19.8 14.5 239.2
stego 973.8 56.2 35.2 29.2 36.3 46.0 55.7 49.2 47.9 40.3 30.6 23.6 20.6 14.7 238.7
3.5 Computational Analysis of Word-Frequency Distribution
In the previous sections we evaluated our system using human judges and now we
test the system using a simple statistical analysis that compares the number of high-
frequency words in stego text with that in cover text. We followed the methodology
used by Meng et al. (2010), who show that there are fewer high-frequency words
in translation-based stego text than in normal text. In the following sections we first
describe the evaluation data and then give the evaluation setup and results.
3.5.1 Data. We randomly collected 1,000 pieces of text from 1,000 sections of the British
National Corpus9 as the cover text, each of which is about 20k bytes. Then for each piece
of cover text, the corresponding stego text was generated using the same stegosystem
setting as that used to generate the human evaluation data described in Section 3.4.1.
This results in a total of 152,268 cover words being replaced with synonyms in the
1,000 pieces of stego text. As mentioned in Section 2.3, one of the standard payload
measurements is to calculate the number of hidden bits per bit transmitted. From this
data, we can calculate the lower bound embedding rate of our stegosystem as 0.09 bits of
hidden message per every 100 bits of data transmitted if one substitution only embeds
one bit.
3.5.2 Evaluation Setup and Results. In order to calculate word frequency counts, words in
both the cover text and the stego text are first lemmatized using the Minnen, Carroll,
and Pearce (2001) tools. In each piece of text, the obtained word frequency count is then
mapped to a rank r between 1 and 15, where rank 1 corresponds to low frequency and
15 corresponds to high frequency. The mapping is defined as:
r =
?
frequency?min
max?min
15
?
+ 1
where max and min is the maximum and minimum word frequency counts in the text,
respectively, and the rank of max is 15. Next, we calculate the number of words for each
frequency rank in each text. Finally, we calculate the average number of words for each
frequency rank for both the cover text and the stego text as shown in Table 11, where the
average number of words has been multiplied by r2 like the results reported by Meng
et al. (2010).
From Table 11 we can see the two vectors are very close, and the only apparent
difference is that in the stego text there are more low-frequency words than in the cover
text. However, unlike the conclusion derived by Meng et al. (2010), our results do not
9 http://www.natcorp.ox.ac.uk/docs/URG/.
434
Chang and Clark Practical Linguistic Steganography
show a substantial difference in the frequency of high-frequency words between the
cover text and the stego text. A possible explanation is that a translation-based stegosys-
tem may combine translations output from different machine translation systems, hence
the usage of frequent words may not be consistent, whereas our stegosystem uses the
same substitution checker for each synonym replacement so there is a certain consis-
tency achieved in stego text.
After giving both the results of human and computational evaluations in the previ-
ous sections, now we would like to show an example of what a typical stego text will
look like. The following paragraphs are generated by the proposed NGM method with
the substitution score threshold equal to 0.9, where 24 words have been substituted:
The whistleblower, who yesterday gave me the entire recording, told me that the Telegraph?s
deletion of these sections about Mr Murdoch was a commercial decision, prompted by the fact
that the Telegraph - like Mr. Cable - would rather News Corporation does not end up as 100%
owner of BskyB.
I of course set this to the Telegraph. And quite late in the day, at 19:19 last night to be
accurate, the Telegraph?s external media adviser sent me a statement attributed to an
unidentified ?spokesman for the Daily Telegraph.? The statement reads:
?It is complete nonsense to suggest that the Daily Telegraph did not publish comments
from Vince Cable on the Rupert Murdoch takeover of BSkyB for commercial reasons. It was an
editorial decision to focus this morning on Cable?s comments on the Coalition because they cost
of wider interest to our readers.?
Well, some would say that was a somewhat eccentric editorial decision for an editor, Tony
Gallagher, widely regarded as one of the sharpest in the business. I rang Mr. Gallagher to
discuss this, but he directed me to the Telegraph?s national PR spokesperson.
Also, you may have found that the Telegraph has not even put out any clear and
unequivocal statement that it was ever planning to publish Mr Cable?s remarks about Mr
Murdoch (though it has now released them, after they were set out by the BBC).
Maybe I am being a bit naive and ridiculous to think any of this matters. Maybe most of
you believe that what we do as reporters is so plain and constantly subject to commercial
interference that there is no special benefit to be gained from asking the Telegraph to explain
itself in this case.
But really that?s not been my experience in 27 years as a hack. And I still think the question
of what news organisations put into the public domain, and how they do it, matters.
Readers can consult Appendix B for those 24 changes made by the proposed NGM
method.
4. Vertex Coding Method
As described earlier, the proposed stegosystem extends the original synset by adding
words in a synonym transitive closure chain while retaining the synonymous rela-
tionships between words using a synonym graph representation. The proposed NGM
checker effectively controls the size of a synonym graph according to the context. After
constructing the graph, namely, obtaining all the good alternatives for a cover word, the
encoder generation module needs to assign codes to every word in the graph. In this
section, we explain the coding method used in our stegosystem.
435
Computational Linguistics Volume 40, Number 2
The aim of the proposed coding method is to convert an input synonym graph into
a coded graph so that each vertex, namely, word, is encoded by a particular code. The
method is inspired by the classic vertex coloring problem in graph theory (Gould 1988),
where a coloring of a graph is a labeling of the graph?s vertices with colors subject to the
condition that no two adjacent vertices share the same color. However, in our proposed
coding method, adjacent vertices are allowed to have the same code as long as each
vertex is able to handle the prefix string of any secret message. A vertex can achieve this
by either using its own code or a neighbor?s code, as long as there is a guarantee that
at least the first secret bit of the prefix can be embedded no matter which word in the
graph is the cover word.
Let us first consider coding the synonym graph of the two joint synsets from
Figure 10. Because both of the synsets have a size of four, which means the synsets
can exhaust up to a two-bit coding space, four different two-bit codewords 00, 01, 10,
and 11 are used to code the graph, as shown in Figure 15(a). As we can see, each word
(a)
(b)
(c)
Figure 15
Examples of coded synonym graphs.
436
Chang and Clark Practical Linguistic Steganography
in the graph has access to all the two-bit codewords. This means that the fundamental
requirement of the coding method is satisfied: No matter what the target word is in
the graph, any two-digit prefix of a secret message can be accommodated. In addition,
the problematic word marry receives a unique codeword no matter which synset is
considered, which means the secret recovery process will not encounter an ambiguity
because the receiver can apply the same coding method to derive identical codewords
used by the sender.
Next, let us consider coding the synonym graph in Figure 11(a). Again, four two-bit
codewords are used because the maximum synset size is four in the synsets that make
up this graph, and a coded version of the graph is shown in Figure 15(b). Note that it
is acceptable to have conjoin and join encoded by the same codeword 00 because both
of them have access to all the two-bit codewords. However, both bind and draw have
only one neighbor, which means that only two codewords can be accommodated by
these nodes, namely, bits 0 and 1. Therefore, instead of using two-bit codewords, the
most significant bits are used to code these words and the neighbor, a process we call
codeword reduction. In this example, the codewords of bind, draw, and tie are reduced
to 0, 0, and 1, respectively. After codeword reduction, the vertex draw can only access
codeword 1 so a further change is needed: The vertex?s codeword is changed to 0 in
order to accommodate either secret bit 0 or 1, a process we call codeword correction.
Note that the final coded graph, after codeword reduction and correction, satisfies
the fundamental requirement that all vertices can represent some prefix of the secret
message. Note also that some vertices can represent a longer prefix than others. For
example, if the next part of the secret message to be embedded is 11, and the target
word is splice, then tie would be chosen as the stego word, covering only the first bit of
the prefix. However, if the target word is wed, then espouse would be chosen as the stego
word, covering two-bits of the prefix. In general the secret embedding procedure will
choose to cover as many bits of the secret message as possible at each point.
99.6% of synsets in WordNet have a size of less than eight, which means that most
of the synsets cannot exhaust more than a two-bit coding space (i.e., we can only encode
at most two bits using a typical synset). Therefore, we restrict the maximum codeword
size in our coding method to two bits. The proposed method always starts coding a
graph with two-bit codewords even if the maximum synset size of a graph is less than
four, and then adjusts the assigned codewords by codeword reduction and codeword
correction.
Figure 15(c) shows a coded synonym graph where the maximum synset size is three.
We first want to make sure that wed, tie, conjoin, and join have access to all the two-bit
codewords because they all have at least three neighboring vertices. Those vertices that
have less than three neighbors are randomly assigned one of the four codewords such
that no two adjacent vertices have the same codeword. After the two-bit codeword
encoding, for a vertex that has only two neighbors, we first check whether the two
neighbors are encoded by the same codeword. If they are, which means the vertex
and its neighbors can accommodate only two codewords, then codeword reduction is
applied to the vertex and both of its neighbors. For example, both the neighbors of link
are encoded by codeword 11 so the codewords of link and its neighbors are replaced
by the most significant bits. Then codeword correction is applied to link to ensure the
access of both bit 0 and bit 1. Similarly, the codeword of unite is replaced by its most
significant bit, but unite does not need codeword correction in this case.
However, if the two neighbors have different codewords, then the vertex has access
to only three two-bit codewords and one of the two-bit codewords is missing. In this
case the codeword that has the same most significant bit as the missing one must be
437
Computational Linguistics Volume 40, Number 2
reduced by undergoing codeword reduction. For example, the two neighbors of splice
have different two-bit codewords, and the missing codeword is 10. Among splice, wed,
and tie, tie has the same significant bit from the missing codeword, so the codeword of
tie is reduced to bit 1. Note that splice can now handle any message prefix: If the first bit
is a 0, then two bits will be embedded (using either splice or wed); if the first bit is a 1
then only that one bit will be embedded (using tie). Similarly, the codeword of espouse is
changed to bit 1.
Finally, bind and draw, which have only one neighbor, are adjusted as described
for Figure 15(b). It is worth noting that, even though the maximum synset size in this
example is three, it is possible to have a word carrying a two-bit codeword in the coded
graph.
Figure 16 describes an algorithm for assigning two-bit codewords to each node
in an input synonym graph (which is run before applying the processes of codeword
correction and reduction). The node data structure has four fields: word is the label of
INPUT: a synonym graph G that has its nodes stored in Q1 and Q2
OUTPUT: a coded synonym graph Gcoded using 2-bit codewords
struct node
string word
string code = null
set of codes cannot access = {00, 01, 10, 11}
set of nodes neighbors
function AssignCode(w, code set)
IF code set is empty THEN
w.code = random({00, 01, 10, 11})
ELSE
w.code = random(code set);
END IF
delete w.code from w.cannot access
FOR every n in w.neighbors
delete w.code from n.cannot access
END FOR
Q = [Q1, Q2]
WHILE Q is not empty
w = pop(Q)
IF w.code is null THEN
AssignCode(w, w.cannot access)
END IF
FOR every n in w.neighbors
IF n.code is null THEN
IF w.cannot access is empty THEN
AssignCode(n, n.cannot access)
ELSE IF n.cannot access is empty THEN
AssignCode(n, w.cannot access)
ELSE
AssignCode(n, w.cannot access ? n.cannot access)
END IF
END IF
END FOR
END WHILE
Figure 16
Algorithm for coding a synonym graph using two-bit codewords.
438
Chang and Clark Practical Linguistic Steganography
Figure 17
Framework of the proposed lexical substitution-based stegosystem.
the node corresponding to a word in a synset; code is the codeword of the node and
is initialized to null; neighbors contains a set of neighboring nodes; and cannot access
records the two-bit codewords that cannot be accessed by the node and is initialized to
00, 01, 10, 11. Because we want a node to have access to all the two-bit codewords where
possible, nodes that have at least three neighbors are stored in a priority queue Q1. In
this group of nodes, a node having three neighbors does not allow any of its neighbors
to carry the same code so we assign codes to three-neighbor nodes and their neighbors
first. Therefore, nodes in Q1 are sorted by the number of neighbors such that nodes
having the least neighbors are at the front of the queue. The other nodes are stored in a
queue Q2 such that nodes having two neighbors are at the front of the queue.
The function AssignCode(w, code set) randomly chooses a codeword from code set,
or from the four two-bit codewords if code set is empty. Then the function removes the
chosen codeword from w.cannot access and from the cannot access sets of w?s neighbors.
The two-bit codeword assigning procedure first loops through all the nodes in Q1 and
then Q2. For each node, the procedure first checks if a codeword has already been
assigned; if not, it calls the AssignCode function. This assigns an appropriate code, as
described earlier, and modifies the cannot access field of both the node and the node?s
neighbors (since the new code is now accessible to both the node and its neighbors). It
then loops through each of the node?s neighbors, using the AssignCode function to assign
a code to each neighbor if it does not already have one. Note that the set of available
codes passed to the function depends on the cannot access sets from both the node and
the neighbor.
After all the nodes have been assigned two-bit codes using this algorithm, code-
word reduction and codeword correction as previously described are applied to revise
improper codewords.
4.1 Proposed Lexical Stegosystem
Figure 17 illustrates the framework of our lexical substitution-based stegosystem. Note
that we asume that WordNet has been pre-processed by excluding multi-word syn-
onyms and single-entry synsets. Table 12 shows the statistics of synsets used in our
Table 12
Statistics of synsets used in our stegosystem.
noun verb adjective adverb
number of synsets 16,079 4,529 6,655 964
number of words 30,933 6,495 14,151 2,025
average synset size 2.56 2.79 2.72 2.51
max synset size 25 16 21 8
439
Computational Linguistics Volume 40, Number 2
stegosystem. A possible information carrier is first found in the cover sentence. We
define a possible information carrier as a word in the cover sentence that belongs to at
least one synset in the pre-processed WordNet. Starting from the cover word?s synset,
all words in the synonym transitive closure chain are examined by the NGM method. A
synonym graph(s) is then built based on the remaining words. Next, we assign codes to
each word in the synonym graph(s). During the encoder generation procedure, if words
in the synonym graph all belong to the same synset, the block coding method is used
to encode the words; otherwise the vertex coding method is applied to the synonym
graph. Finally, according to the secret bitstring, the system selects a substitute that is
synonymous with the cover word and has as its codeword the longest potential match
with the secret bitstring.
Repeating a comment made earlier, we use the transitive closure chain of WordNet
containing the target word as a simple method to ensure that both sender and receiver
encode the same graph. It is important to note, however, that the sender only considers
the synonyms of the target word as potential substitutes; the transitive closure chain is
only used to consistently assign the codes.
For the decoding process, the receiver does not need the original text for extracting
secret data. An information carrier can be found in the stego text by referring to Word-
Net in which related synonyms are extracted. Those words in the related sets undergo
the NGM checking method, and the words passing the check form a synonym graph(s).
The synonym graph(s) are encoded by either block coding or the vertex coding scheme
depending on whether the remaining words are in the same synset. Finally, the secret
bitstring is implicit in the codeword of the information carrier and therefore can be
extracted.
We demonstrate how to embed secret bit 1 in the sentence it is a shame that we
could not reach the next stage. A possible information carrier shame is first found in the
sentence. Table 13 lists the synsets in the synonym transitive closure chain extracted
from WordNet. The score of each word calculated by the NGM method is given in
parentheses. For the purpose of demonstrating the use of vertex coding, we select a
low threshold score of 0.27. The output of the synonym graph is shown in Figure 18(a).
Because the remaining words do not belong to the same synset, the vertex coding
method is then used to encode the words. Figure 18(b) shows the coded synonym graph
in which each vertex is assigned one of the four two-bit codewords; Figure 18(c) is
the graph after applying codeword reduction and codeword correction. Although both
disgrace and pity are encoded by 1, pity is chosen to replace the cover word because it
has a higher score. Finally, the stego text is generated, it is a pity that we could not reach
Table 13
Synsets of shame in the synonym transitive closure chain with substitution scores.
cover sentence: It is a shame that we could not reach the next stage.
{pity (0.97), shame (1.0)}
{shame (1.0), disgrace (0.84), ignominy (0.24)}
{commiseration (0.28), pity (0.97), ruth (0.13), pathos (0.31)}
{compassion (0.49), pity (0.97)}
{condolence (0.27), commiseration (0.28)}
{compassion (0.49), compassionateness (0)}
{pathos (0.31), poignancy (0.31)}
{poignance (0.12), poignancy (0.31)}
440
Chang and Clark Practical Linguistic Steganography
(a) The synonym graph of the synsets in Table 13
(b) Coded synonym graph using the four 2-bit codewords
(c) Coded synonym graph after codeword reduction and code-
word correction
Figure 18
Synonym graphs generated by the proposed stegosystem.
the next stage. As explained previously, even if a cover word does not pass the NGM
check, the proposed stegosystem can still use its synonyms to embed secret bits. For
example, assume the cover sentence is it is an ignominy that we could not reach the next
stage. The same coded synonym graph as Figure 18(c) will be constructed because both
the context and the synonym transitive closure chain are the same as that in the original
example. This time, the replacement of shame represents secret bit 0, and the replacement
with disgrace represents secret bit 1. In other words, a change must be made in order to
embed a secret bit in this case.
In cryptography, Kerckhoffs?s principle (Kerckhoffs 1883) states that a method of
secretly coding and transmitting information should be secure even if everything about
the system, except the key and any private randomizer, is public knowledge. In our
steganography scheme, the secret key is the score threshold in the NGM method, and
441
Computational Linguistics Volume 40, Number 2
the private randomizer is the one that assigns codes in the AssignCode function in the
proposed vertex coding methods. The score threshold decides the size of a synonym
graph, and the randomizer controls the encoding of a synonym graph. To extract the
secret message, the enemy needs to generate the same coded synonym graphs as
constructed by the sender. Therefore, it is difficult to recover the secret bits without
knowing the score threshold and the code randomizer.
5. Conclusions and Future Work
One of the contributions of this work is to develop a novel lexical substitution-based
stegosystem using vertex coding that improves the data embedding capacity com-
pared to existing systems. The vertex coding method represents synonym substitu-
tion as a synonym graph so the relations between words can be clearly observed. In
addition, the NGM method, an automatic system for checking synonym acceptabil-
ity in context, is integrated in our stegosystem to ensure information security. The
proposed stegosystem was automatically evaluated using the gold standard from the
SemEval2007 lexical substitution task as well as a human evaluation. From the evalu-
ation results we may conclude that our substitution-based stegosystem has achieved
a reasonable level of security while reaching the payload capacity of around two bits
per sentence.
In this work, we only evaluated the lexical substitution in terms of the sentence-
level naturalness rather than meaning retention and document-level coherence. There-
fore, it would be interesting to see to what extent the proposed substitution checkers are
useful for the security of linguistic steganography at the document-level. In addition,
apart from the linguistic transformations discussed in Section 2.1, we would like to
explore more manipulations that can meet the requirements of linguistic steganography.
As mentioned in Section 2.2, there is no research on the practical issue of using different
types of cover text for the steganography application. Thus, it would be interesting
to see whether some types of cover text are better suited to linguistic steganography
than others. Another interesting question that we have not addressed is whether some
languages are easier to be modified than others, or whether some languages work better
with particular linguistic transformations than others.
Research in linguistic steganography requires experience and knowledge of both
information security and NLP. In addition, due to the complexity of language, there
have been more studies on image or video steganography than linguistic steganog-
raphy. Hence linguistic steganography is a relatively new research area, and further
efforts are needed to develop more secure and efficient systems. The novel and original
ideas provided in this article can benefit research in both computational linguistics and
information security. It is hoped that our work can form the basis for more research
devoted to linguistic steganography.
442
Chang and Clark Practical Linguistic Steganography
Appendix A. Screenshots of the Web-Based Human Annotation
Figure A1
The introduction and guidelines for the lexical substitution annotation.
Appendix B. Original Text and the Substituted Words
The following text is part of the Robert Peston?s BBC blog article titled ?Unanswered
questions about Cable?10 where the 24 words in boldface are selected by the proposed
NGM method as information carriers and are replaced with their synonyms as shown
in the example in Section 3.4.2:
The whistleblower, who yesterday gave me the full recording, told me that the Telegraph?s
omission of these sections about Mr Murdoch was a commercial decision, motivated by the
fact that the Telegraph - like Mr Cable - would rather News Corporation does not end up as
100% owner of BskyB.
I of course put this to the Telegraph. And rather late in the day, at 19:19 last night to be
precise, the Telegraph?s external media adviser sent me a statement attributed to an unnamed
?spokesman for the Daily Telegraph.? The statement says:
?It is utter nonsense to suggest that the Daily Telegraph did not publish comments from
Vince Cable on the Rupert Murdoch takeover of BSkyB for commercial reasons. It was an
editorial decision to focus this morning on Cable?s comments on the Coalition because they were
of wider interest to our readers.?
10 http://www.bbc.co.uk/blogs/thereporters/robertpeston/2010/12/unanswered questions
about cab.html.
443
Computational Linguistics Volume 40, Number 2
Well, some would say that was a slightly eccentric editorial decision for an editor, Tony
Gallagher, widely regarded as one of the sharpest in the business. I rang Mr Gallagher to discuss
this, but he directed me to the Telegraph?s internal PR spokesperson.
Also, you may have noticed that the Telegraph has not yet put out any clear and
unambiguous statement that it was ever planning to publish Mr Cable?s remarks about Mr
Murdoch (though it has now published them, after they were put out by the BBC).
Maybe I am being a bit naive and silly to think any of this matters. Maybe most of you
think that what we do as reporters is so obviously and constantly subject to commercial
interference that there is no particular benefit to be gained from asking the Telegraph to explain
itself in this case.
But actually that?s not been my experience in 27 years as a hack. And I still think the
question of what news organisations put into the public domain, and how they do it, matters.
Figure A2
A screen capture of the lexical substitution annotation.
Acknowledgments
We would like to thank Dr. Laura Rimell,
the anonymous reviewers, and all the
contributing volunteer annotators for
useful comments and their effort and
time. This work was largely carried out
while Ching-Yun Chang was a Ph.D. student
at the Cambridge University Computer
Laboratory, where she was supported by
the Studying Abroad Scholarship from the
Ministry of Education in Taiwan.
References
Atallah, Mikhail J., Craig J. McDonough,
Victor Raskin, and Sergei Nirenburg.
2000. Natural language processing for
information assurance and security: An
overview and implementations. In
Proceedings of the 2000 Workshop on New
Security Paradigms, pages 51?65,
Ballycotton.
Atallah, Mikhail J., Victor Raskin, Michael C.
Crogan, Christian Hempelmann, Florian
444
Chang and Clark Practical Linguistic Steganography
Kerschbaum, Dina Mohamed, and
Sanket Naik. 2001. Natural language
watermarking: Design, analysis, and a
proof-of-concept implementation.
In Proceedings of the 4th International
Information Hiding Workshop,
pages 185?199, Pittsburgh, PA.
Atallah, Mikhail J., Victor Raskin,
Christian F. Hempelmann, Mercan
Karahan, Umut Topkara, Katrina E.
Triezenberg, and Radu Sion. 2002. Natural
language watermarking and
tamperproofing. In Proceedings of the 5th
International Information Hiding Workshop,
pages 196?212, Noordwijkerhout.
Bennett, Krista. 2004. Linguistic
steganography: Survey, analysis, and
robustness concerns for hiding
information in text. CERIAS Technical
Report 2004-13, Purdue University,
Lafayette, IN.
Bergmair, Richard. 2004. Towards linguistic
steganography: A systematic investigation
of approaches, systems, and issues. Final
year thesis, B.Sc. (Hons.) in Computer
Studies, The University of Derby.
Bergmair, Richard. 2007. A comprehensive
bibliography of linguistic steganography.
In Proceedings of the SPIE International
Conference on Security, Steganography, and
Watermarking of Multimedia Contents,
pages W1?W6, San Jose, CA.
Bergsma, Shane, Dekang Lin, and Randy
Goebel. 2009. Web-scale n-gram models for
lexical disambiguation. In Proceedings of
the 21st International Joint Conference on
Artifical Intelligence, pages 1,507?1,512,
Pasadena, CA.
Bolshakov, Igor A. 2004. A method of
linguistic steganography based on
collocationally-verified synonym. In
Information Hiding: 6th International
Workshop, pages 180?191, Toronto.
Brants, Thorsten and Alex Franz. 2006.
Web 1T 5-gram corpus version 1.1.
Linguistic Data Consortium,
Philadelphia, PA.
Callison-Burch, Chris. 2008. Syntactic
constraints on paraphrases extracted from
parallel corpora. In Proceedings of the
EMNLP Conference, pages 196?205,
Honolulu, HI.
Carlson, Andrew, Tom M. Mitchell, and
Ian Fette. 2008. Data analysis project:
Leveraging massive textual corpora using
n-gram statistics. Technical Report
CMU-ML-08-107, School of Computer
Science, Carnegie Mellon University,
Pittsburgh, PA.
Chang, Ching-Yun and Stephen Clark.
2010a. Linguistic steganography using
automatically generated paraphrases.
In Proceedings of the Annual Meeting of
the North American Association for
Computational Linguistics, pages 591?599,
Los Angeles, CA.
Chang, Ching-Yun and Stephen Clark. 2010b.
Practical linguistic steganography using
contextual synonym substitution and
vertex color coding. In Proceedings of the
2010 Conference on Empirical Methods
in Natural Language Processing,
pages 1,194?1,203, Cambridge, MA.
Chang, Ching-Yun and Stephen Clark.
2012a. Adjective deletion for linguistic
steganography and secret sharing.
In Proceedings of the 24th International
Conference on Computational Linguistics,
pages 493?510, Mumbai.
Chang, Ching-Yun and Stephen Clark. 2012b.
The secret?s in the word order: Text-to-text
generation for linguistic steganography.
In Proceedings of the 24th International
Conference on Computational Linguistics,
pages 511?528, Mumbai.
Chapman, Mark and George I. Davida.
1997. Hiding the hidden: A software
system for concealing ciphertext as
innocuous text. In Proceedings of the First
International Conference on Information and
Communication Security, pages 335?345,
Beijing.
Chapman, Mark, George I. Davida, and
Marc Rennhard. 2001. A practical and
effective approach to large-scale
automated linguistic steganography.
In Proceedings of the 4th International
Conference on Information Security,
pages 156?165, Malaga.
Chen, Zhili, Liusheng Huang, Peng Meng,
Wei Yang, and Haibo Miao. 2011. Blind
linguistic steganalysis against translation
based steganography. In Proceedings of the
9th International Conference on Digital
Watermarking, pages 251?265, Seoul.
Clark, Stephen and James R. Curran. 2007.
Wide-coverage efficient statistical parsing
with CCG and log-linear models.
Computation Linguistics, 33(4):493?552.
Cohn, Trevor and Mirella Lapata. 2008.
Sentence compression beyond word
deletion. In Proceedings of the 22nd
International Conference on Computational
Linguistics (Coling 2008), pages 137?144,
Manchester.
Cox, Ingemar, Matthew Miller, Jeffrey Bloom,
Jessica Fridrich, and Ton Kalker. 2008.
Digital Watermarking and Steganography.
445
Computational Linguistics Volume 40, Number 2
Morgan Kaufmann Publishers Inc.,
second edition.
Dinu, Georgiana and Mirella Lapata. 2010.
Measuring distributional similarity in
context. In Proceedings of the 2010
Conference on Empirical Methods in Natural
Language Processing, pages 1,162?1,172,
Cambridge, MA.
Doddington, George. 2002. Automatic
evaluation of machine translation quality
using n-gram co-occurrence statistics.
In Proceedings of the Second International
Conference on Human Language Technology
Research, pages 138?145, San Diego, CA.
Dorr, Bonnie, David Zajic, and Richard
Schwartz. 2003. Hedge trimmer:
A parse-and-trim approach to headline
generation. In Proceedings of the
HLT-NAACL 03 on Text Summarization
Workshop - Volume 5, pages 1?8, Edmonton.
Erk, Katrin and Sebastian Pado?. 2010.
Exemplar-based models for word meaning
in context. In Proceedings of the ACL 2010
Conference Short Papers, pages 92?97,
Uppsala.
Fellbaum, Christiane. 1998. WordNet: An
Electronic Lexical Database. MIT Press,
Cambridge, MA.
Fridrich, Jessica. 2009. Steganography in
Digital Media: Principles, Algorithms, and
Applications. Cambridge University Press.
Gould, Ronald J. 1988. Graph Theory.
Benjamin/Cummings Publishing Co.,
Menlo Park, CA.
Grothoff, Christian, Krista Grothoff,
Ludmila Alkhutova, Ryan Stutsman, and
Mikhail J. Atallah. 2005. Translation-based
steganography. In Proceedings of the
2005 Information Hiding Workshop,
pages 219?233, Barcelona.
Herodotus. 1987. The History. University of
Chicago Press. Translated by David Grene.
Hoover, J. Edgar. 1946. The enemy?s
masterpiece of espionage. The Reader?s
Digest, 48:49?53. London edition.
Huffman, David A. 1952. A method for the
construction of minimum-redundancy codes.
Proceedings of the IRE, 40(9):1098?1101.
Islam, Aminul and Diana Inkpen. 2009.
Real-word spelling correction using
Google Web IT 3-grams. In EMNLP ?09:
Proceedings of the 2009 Conference on
Empirical Methods in Natural Language
Processing, pages 1,241?1,249, Singapore.
Kahn, David. 1967. The Codebreakers: The
Story of Secret Writing. Macmillan.
Kerckhoffs, Auguste. 1883. La cryptographie
militaire. Journal des Sciences Militaires,
IX:5?83.
Khairullah, M. D. 2009. A novel text
steganography system using font color of
the invisible characters in Microsoft Word
documents. In Second International
Conference on Computer and Electrical
Engineering, pages 482?484, Dubai.
Kim, Mi-Young. 2008. Natural language
watermarking for Korean using adverbial
displacement. In Multimedia and Ubiquitous
Engineering, pages 576?581, Busan.
Kim, Mi-Young. 2009. Natural language
watermarking by morpheme
segmentation. In First Asian Conference on
Intelligent Information and Database Systems,
pages 144?149, Dong Hoi.
Kirk, Roger E. 2012. Experimental Design:
Procedures for the Behavioral Sciences.
SAGE Publications, Inc, fourth edition.
Kishida, Kazuaki. 2005. Property of average
precision and its generalization: An
examination of evaluation indicator
for information retrieval experiments.
Technical Report NII-2005-014E, National
Institute of Informatics, Tokyo.
Kullback, Solomon. 1959. Information Theory
and Statistics. John Wiley and Sons,
New York.
Kummerfeld, Jonathan K. and James R.
Curran. 2008. Classification of verb particle
constructions with the Google Web 1T
Corpus. In Proceedings of the Australasian
Language Technology Association Workshop
2008, pages 55?63, Hobart.
Lampson, Butler W. 1973. A note on the
confinement problem. Communications of
the ACM, 16(10):613?615.
Lee, Lillian. 1999. Measures of distributional
similarity. In Proceedings of the 37th Annual
Meeting of the Association for Computational
Linguistics on Computational Linguistics,
pages 25?32, College Park, MD.
Liu, Yuling, Xingming Sun, and Yong Wu.
2005. A natural language watermarking
based on Chinese syntax. In Advances in
Natural Computation, volume 3612,
pages 958?961, Changsha.
McCarthy, Diana and Roberto Navigli. 2007.
SemEval-2007 task 10: English lexical
substitution task. In Proceedings of the
4th International Workshop on Semantic
Evaluations (SemEval-2007), pages 48?53,
Prague.
Meng, Peng, Liusheng Hang, Zhili Chen,
Yuchong Hu, and Wei Yang. 2010. STBS:
A statistical algorithm for steganalysis
of translation-based steganography.
In Proceedings of the 12th International
Conference, Information Hiding,
pages 208?220, Calgary.
446
Chang and Clark Practical Linguistic Steganography
Meng, Peng, Yun-Qing Shi, Liusheng Huang,
Zhili Chen, Wei Yang, and Abdelrahman
Desoky. 2011. LinL: Lost in n-best list.
In Proceedings of the 13th International
Conference, Information Hiding,
pages 329?341, Prague.
Meral, Hasan M., Emre Sevinc, Ersin Unkar,
Bulent Sankur, A. Sumru Ozsoy, and
Tunga Gungor. 2007. Syntactic tools for
text watermarking. In Proceedings of the
SPIE International Conference on Security,
Steganography, and Watermarking of
Multimedia Contents, pages X1?X12,
San Jose, CA.
Meral, Hasan Mesut, Bu?lent Sankur,
A. Sumru O?zsoy, Tunga Gu?ngo?r, and
Emre Sevinc?. 2009. Natural language
watermarking via morphosyntactic
alterations. Computer Speech and Language,
23(1):107?125.
Minnen, Guido, John Carroll, and Darren
Pearce. 2001. Applied morphological
processing of English. Natural Language
Engineering, 7:207?223.
Murphy, Brian. 2001. Syntactic information
hiding in plain text. Masters Thesis,
Trinity College Dublin.
Murphy, Brian and Carl Vogel. 2007a.
Statistically-constrained shallow text
marking: Techniques, evaluation paradigm
and results. In Proceedings of the SPIE
International Conference on Security,
Steganography, and Watermarking of
Multimedia Contents, pages Z1?Z9,
San Jose, CA.
Murphy, Brian and Carl Vogel. 2007b. The
syntax of concealment: Reliable methods
for plain text information hiding. In
Proceedings of the SPIE International
Conference on Security, Steganography, and
Watermarking of Multimedia Contents,
volume 6505, pages Y1?Y12, San Jose, CA.
Newman, Bernard. 1940. Secrets of German
Espionage. Robert Hale Ltd.
O? Se?aghdha, Diarmuid and Anna Korhonen.
2011. Probabilistic models of similarity
in syntactic context. In Proceedings of the
Conference on Empirical Methods in Natural
Language Processing, pages 1,047?1,057,
Edinburgh.
Papineni, Kishore, Salim Roukos, Todd
Ward, and Wei-Jing Zhu. 2002. BLEU:
A method for automatic evaluation of
machine translation. In Proceedings of the
40th Annual Meeting of the Association for
Computational Linguistics, pages 311?318,
Philadelphia, PA.
Pfitzmann, Birgit. 1996. Information hiding
terminology: Results of an informal
plenary meeting and additional proposals.
In Proceedings of the First International
Workshop on Information Hiding,
pages 347?350, Cambridge.
Por, Lip Y., Ang T. Fong, and B. Delina.
2008. WhiteSteg: A new scheme in
information hiding using text
steganography. WSEAS Transactions
on Computers, 7:735?745.
Schuler, Karin Kipper. 2005. Verbnet: A
Broad-coverage, Comprehensive Verb Lexicon.
Ph.D. thesis, University of Pennsylvania,
Philadelphia, PA.
Shahreza, Mohammad Shirali. 2006. A new
method for steganography in HTML files.
Advances in Computer, Information, and
Systems Sciences, and Engineering,
pages 247?252.
Sharoff, Serge. 2006. Open-source corpora:
Using the net to fish for linguistic data.
International Journal of Corpus Linguistics,
11(4):435?462.
Shen, Libin, Giorgio Satta, and Aravind
Joshi. 2007. Guided learning for
bidirectional sequence classification.
In Proceedings of the 45th Annual Meeting
of the Association of Computational
Linguistics, pages 760?767, Prague.
Shih, Frank Y. 2008. Digital Watermarking and
Steganography: Fundamentals and Techniques.
CRC Press.
Simmons, Gustavus J. 1984. The prisoners?
problem and the subliminal channel.
In Advances in Cryptology: Proceedings
of CRYPTO ?83, pages 51?67,
Santa Barbara, CA.
Soderstrand, Michael A., Kenneth W.
Jenkins, Graham A. Jullien, and Fred J.
Taylor. 1986. Residue Number System
Arithmetic: Modern Applications in
Digital Signal Processing. IEEE Press.
S?gaard, Anders. 2010. Simple
semi-supervised training of part-of-speech
taggers. In Proceedings of the ACL 2010
Conference Short Papers, pages 205?208,
Uppsala.
Spoustova?, Drahom??ra ?Johanka,? Jan Hajic?,
Jan Raab, and Miroslav Spousta. 2009.
Semi-supervised training for the averaged
perceptron POS tagger. In Proceedings of the
12th Conference of the European Chapter of the
Association for Computational Linguistics,
pages 763?771, Athens.
Stevens, Guy William Willis. 1957.
Microphotography: Photography at Extreme
Resolution. Chapman & Hall.
Stutsman, Ryan, Christian Grothoff, Mikhail
Atallah, and Krista Grothoff. 2006. Lost in
just the translation. In Proceedings of the
447
Computational Linguistics Volume 40, Number 2
2006 ACM Symposium on Applied
Computing, pages 338?345, Dijon.
Taskiran, Cuneyt M., Mercan Topkara, and
Edward J. Delp. 2006. Attacks on lexical
natural language steganography systems.
In Proceedings of the SPIE International
Conference on Security, Steganography, and
Watermarking of Multimedia Contents,
pages 97?105, San Jose, CA.
Topkara, Mercan, Giuseppe Riccardi, Dilek
Hakkani-Tu?r, and Mikhail J. Atallah.
2006. Natural language watermarking:
Challenges in building a practical system.
In Proceedings of the SPIE International
Conference on Security, Steganography, and
Watermarking of Multimedia Contents,
pages 106?117, San Jose, CA.
Topkara, Mercan, Cuneyt M. Taskiran, and
Edward J. Delp. 2005. Natural language
watermarking. In Proceedings of the SPIE
International Conference on Security,
Steganography, and Watermarking of
Multimedia Contents, volume 5681,
pages 441?452, San Jose, CA.
Topkara, Mercan, Umut Topkara, and
Mikhail J. Atallah. 2006a. Words are not
enough: Sentence level natural language
watermarking. In Proceedings of the ACM
Workshop on Content Protection and Security,
pages 37?46, Santa Barbara, CA.
Topkara, Umut, Mercan Topkara, and
Mikhail J. Atallah. 2006b. The hiding
virtues of ambiguity: Quantifiably
resilient watermarking of natural language
text through synonym substitutions.
In Proceedings of the 8th Workshop on
Multimedia and Security, pages 164?174,
Geneva.
Toutanova, Kristina, Dan Klein,
Christopher D. Manning, and Yoram
Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network.
In Proceedings of the 2003 Conference of the
North American Chapter of the Association
for Computational Linguistics on Human
Language Technology, pages 173?180,
Edmonton.
Venugopal, Ashish, Jakob Uszkoreit,
David Talbot, Franz Och, and Juri
Ganitkevitch. 2011. Watermarking the
outputs of structured prediction with an
application in statistical machine
translation. In Proceedings of the 2011
Conference on Empirical Methods in Natural
Language Processing, pages 1,363?1,372,
Edinburgh.
Vybornova, M. Olga and Benoit Macq. 2007.
A method of text watermarking using
presuppositions. In Proceedings of the
SPIE International Conference on Security,
Steganography, and Watermarking of
Multimedia Contents, pages R1?R10,
San Jose, CA.
Wayner, Peter. 1992. Mimic functions.
Cryptologia, XVI(3):193?214.
Wayner, Peter. 1995. Strong theoretical
steganography. Cryptologia,
XIX(3):285?299.
Wilcoxon, Frank. 1945. Individual
comparisons by ranking methods.
Biometrics Bulletin, 1(6):80?83.
Winstein, Keith. 1999. Tyrannosaurus
lex. Open source. Available at
http://web.mit.edu/keithw/tlex.
Zhang, Yue and Stephen Clark. 2011.
Syntax-based grammaticality
improvement using CCG and guided
search. In Proceedings of the 2011
Conference on Empirical Methods in Natural
Language Processing, pages 1,147?1,157,
Edinburgh.
Zhu, Zhemin, Delphine Bernhard, and
Iryna Gurevych. 2010. A monolingual
tree-based translation model for sentence
simplification. In Proceedings of the
23rd International Conference on
Computational Linguistics, COLING ?10,
pages 1,353?1,361, Beijing.
448
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 591?599,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Linguistic Steganography Using Automatically Generated Paraphrases
Ching-Yun Chang
University of Cambridge
Computer Laboratory
Ching-Yun.Chang@cl.cam.ac.uk
Stephen Clark
University of Cambridge
Computer Laboratory
Stephen.Clark@cl.cam.ac.uk
Abstract
This paper describes a method for checking
the acceptability of paraphrases in context.
We use the Google n-gram data and a CCG
parser to certify the paraphrasing grammati-
cality and fluency. We collect a corpus of hu-
man judgements to evaluate our system. The
ultimate goal of our work is to integrate text
paraphrasing into a Linguistic Steganography
system, by using paraphrases to hide informa-
tion in a cover text. We propose automati-
cally generated paraphrases as a new and use-
ful source of transformations for Linguistic
Steganography, and show that our method for
checking paraphrases is effective at maintain-
ing a high level of imperceptibility, which is
crucial for effective steganography.
1 Introduction
Steganography is concerned with hiding informa-
tion in some cover medium, by manipulating prop-
erties of the medium in such a way that the hidden
information is not easily detectable by an observer
(Fridrich, 2009). The covert communication is such
that the very act of communication is to be kept se-
cret from outside observers. A related area is Wa-
termarking, in which modifications are made to a
cover medium in order to identify it, for example for
the purposes of copyright. Here the changes may
be known to an observer, and the task is to make
the changes in such a way that the watermark cannot
easily be removed.
There is a large literature on image steganogra-
phy and watermarking, in which images are mod-
ified to encode a hidden message or watermark.
Image stegosystems exploit the redundancy in an
image representation together with limitations of
the human visual system. For example, a stan-
dard image stegosystem uses the least-significant-bit
(LSB) substitution technique. Since the difference
between 11111111 and 11111110 in the value for
red/green/blue intensity is likely to be undetectable
by the human eye, the LSB can be used to hide infor-
mation other than colour, without being perceptable
by a human observer.1
A key question for any steganography system is
the choice of cover medium. Given the ubiqui-
tous nature of natural languages and electronic text,
text is an obvious medium to consider. However,
the literature on Linguistic Steganography, in which
linguistic properties of a text are modified to hide
information, is small compared with other media
(Bergmair, 2007). The likely reason is that it is
easier to make changes to images and other non-
linguistic media which are undetectable by an ob-
server. Language has the property that even small
local changes to a text, e.g. replacing a word by a
word with similar meaning, may result in text which
is anomalous at the document level, or anomalous
with respect to the state of the world. Hence find-
ing linguistic transformations which can be applied
reliably and often is a challenging problem for Lin-
guistic Steganography.
In this paper we focus on steganography rather
than watermarking, since we are interested in the re-
quirement that any changes to a text be impercep-
tible to an observer. Figure 1 shows the Linguistic
Steganography framework. First, some secret mes-
sage, represented as a sequence of bits, is hidden in a
1The observer may also be a computer program, designed to
detect statistical anomalies in the image representation which
may indicate the presence of hidden information.
591
Figure 1: The Linguistic Steganography framework
cover text using the embedding algorithm, resulting
in the stego text.2 Next, the stego text passes the hu-
man observer, who is happy for innocuous messages
to pass between the sender and receiver, but will ex-
amine the text for any suspicious looking content.
Once the stego text reaches the receiver, the hidden
message is recovered using the extracting algorithm.
There is a fundamental tradeoff in all steganogra-
phy systems, and one that is especially apparent in
the Linguistic Steganography framework: the trade-
off between imperceptibility and payload. Payload
is the number of bits that can be encoded per unit
of cover medium, for example per sentence in the
linguistic case. The tradeoff arises because any at-
tempt to hide additional information in the cover
text, through the application of more linguistic trans-
formations, is likely to increase the chances of rais-
ing the suspicions of the observer, by introducing
anomalies into the text.
The key elements of a Linguistic Steganography
system are the linguistic transformation and the em-
bedding method. In this paper we focus on the lin-
guistic transformation. Section 5 describes a pos-
sible embedding method for our framework, and
for readers unfamiliar with linguistic steganography
shows how linguistic transformations can be used to
embed hidden bits in text.
Section 2 describes some of the previous transfor-
mations used in Linguistic Steganography. Note that
we are concerned with transformations which are
2The message may have been encrypted initially also, as in
the figure, but this is not important in this paper; the key point
is that the hidden message is a sequence of bits.
linguistic in nature, rather than dealing with superfi-
cial properties of the text, e.g. the amount of white
space between words (Por et al, 2008). Our pro-
posed method is based on the automatically acquired
paraphrase dictionary described in Callison-Burch
(2008), in which the application of paraphrases from
the dictionary encodes secret bits. One advantage
of the dictionary is that it has wide coverage, be-
ing automatically extracted; however, a disadvan-
tage is that it contains many paraphrases which are
either inappropriate, or only appropriate in certain
contexts. Since we require any changes to be im-
perceptible to a human observer, it is crucial to our
system that any uses of paraphrasing are grammati-
cal and retain the meaning of the original cover text.
In order to test the grammaticality and meaning
preserving nature of a paraphrase, we employ a sim-
ple technique based on checking whether the con-
texts containing the paraphrase are in the Google n-
gram corpus. This technique is based on the sim-
ple hypothesis that, if the paraphrase in context has
been used many times before on the web, then it is
an appropriate use. We test our n-gram-based sys-
tem against some human judgements of the gram-
maticality of paraphrases in context. We find that
using larger contexts leads to a high precision sys-
tem (100% when using 5-grams), but at the cost of
a reduced recall. This precision-recall tradeoff re-
flects the inherent tradeoff between imperceptibility
and payload in a Linguistic Steganography system.
We also experiment with a CCG parser (Clark and
Curran, 2007), requiring that the contexts surround-
ing the original phrase and paraphrase are assigned
592
the same CCG lexical categories by the parser. This
method increases the precision of the Google n-gram
check with a slight loss in recall.
A contribution of this paper is to advertise the Lin-
guistic Steganography problem to the ACL commu-
nity. The requirement that any linguistic transfor-
mation maintain the grammaticality and meaning of
the cover text makes the problem a strong test for
existing NLP technology.
2 Previous Work
2.1 Synonym Substitution
The simplest and most straightforward subliminal
modification of text is to substitute selected words
with their synonyms. The first lexical substitu-
tion method was proposed by Chapman and Davida
(1997). Later works, such as Atallah et al (2001a),
Bolshakov (2004), Taskiran et al (2006) and Top-
kara et al (2006b), further made use of part-of-
speech taggers and electronic dictionaries, such as
WordNet and VerbNet, to increase the robustness of
the method. Taskiran et al (2006) attempt to use
context by prioritizing the alternatives using an n-
gram language model; that is, rather than randomly
choose an option from the synonym set, the system
relies on the language model to select the synonym.
Topkara et al (2005) and Topkara et al (2006b) re-
port an average embedding capacity of 0.67 bits per
sentence for the synonym substitution method.
2.2 Syntactic Transformations
The second and the most widely used manipulations
for linguistic steganography are syntactic transfor-
mations. This method is based on the fact that a sen-
tence can be transformed into more than one seman-
tically equivalent syntactic structure, using trans-
formations such as passivization, topicalization and
clefting. The first syntactic transformation method is
presented by Atallah et al (2001a). Later, Atallah et
al. (2001b) embedded information in the tree struc-
ture of the text by adjusting the structural proper-
ties of intermediate representations of sentences. In
other words, instead of performing lexical substitu-
tion directly to the text, the secret message is embed-
ded into syntactic parse trees of the sentences. Liu
et al (2005), Meral et al (2007), Murphy (2001),
Murphy and Vogel (2007) and Topkara et al (2006a)
all belong to the syntactic transformation category.
After embedding the secret message, modified deep
structure forms are converted into the surface struc-
ture format via language generation tools. Atallah et
al. (2001b) and Topkara et al (2006a) attained the
embedding capacity of 0.5 bits per sentence with the
syntactic transformation method.
2.3 Semantic Transformations
The semantic transformation method is the most so-
phisticated approach for linguistic steganography,
and perhaps impractical given the current state-of-
the-art for NLP technology. It requires some sophis-
ticated tools and knowledge to model natural lan-
guage semantics. Atallah et al (2002) used seman-
tic transformations and embed information in text-
meaning representation (TMR) trees of the text by
either pruning, grafting or substituting the tree struc-
ture with information available from ontological se-
mantic resources. Vybornova and Macq (2007)
aimed to embed information by exploiting the lin-
guistic phenomenon of presupposition, with the idea
that some presuppositional information can be re-
moved without changing the meaning of a sentence.
3 Data Resources
3.1 Paraphrase Dictionary
The cover text used for our experiments consists of
newspaper sentences from Section 00 of the Penn
Treebank (Marcus et al, 1993). Hence we require
possible paraphrases for phrases that occur in Sec-
tion 00. The paraphrase dictionary that we use
was generated for us by Chris Callison-Burch, using
the technique described in Callison-Burch (2008),
which exploits a parallel corpus and methods devel-
oped for statistical machine translation.
Table 1 gives summary statistics of the paraphrase
dictionary and its coverage on Section 00 of the
Penn Treebank. The length of the extracted n-gram
phrases ranges from unigrams to five-grams. The
coverage figure gives the percentage of sentences
which have at least one phrase in the dictionary. The
coverage is important for us because it determines
the payload capacity of the embedding method de-
scribed in Section 5.
Table 2 lists some examples 5-gram phrases and
paraphrases from the dictionary. The format of the
593
N-gram Number of Coverage on
phrases section 00 (%)
Unigrams 5,856 99
Bigrams 13,473 96
Trigrams 6,574 65
Four-grams 1,604 40
Five-grams 295 10
Table 1: Statistics for the paraphrase dictionary
Original phrase Paraphrases
the end of this year later this year
the end of the year
year end
a number of people some of my colleagues
differences
the European peoples party
the PPE group
Table 2: Example phrases and paraphrases from the dic-
tionary
dictionary is a mapping from phrases to sets of pos-
sible paraphrases. Each paraphrase also has a prob-
ability, based on a statistical machine translation
model, but we do not use that feature here. The ex-
amples show that, while some of the paraphrases are
of a high quality, some are not. For example, dif-
ferences is unlikely to be a suitable paraphrase for
a number of people in any context. Moreover, there
are some ?phrase, paraphrase? pairs which are only
suitable in particular contexts. For example, year
end is an unsuitable paraphrase for the end of this
year in the sentence The chart compares the gold
price at the end of last year with the end of this year.
Barzilay and McKeown (2001) also note that the ap-
plicability of paraphrases is strongly influenced by
context. Section 4 describes our method for deter-
mining if a paraphrase is suitable in a given context.
3.2 Google N-gram Data
The Google n-gram data was collected by Google
Research for statistical language modelling, and has
been used for many tasks such as lexical disam-
biguation (Bergsma et al, 2009), and contains En-
glish n-grams and their observed frequency counts,
for counts of at least 40. The striking feature of
Figure 2: The web-based annotation system
the n-gram corpus is the large number of n-grams
and the size of the counts, since the counts were ex-
tracted from over 1 trillion word tokens of English
text on publicly accessible Web pages collected in
January 2006. For example, the 5-gram phrase the
part that you were has a count of 103. The com-
pressed data is around 24 GB on disk.
3.3 Paraphrase Judgement Corpus
The focus of the paper is to develop an automatic
system for checking the grammaticality and flu-
ency of paraphrases in context. In order to evaluate
the system, we collected some human judgements,
based on 70 sentences from Section 00 of the Penn
Treebank. For each sentence, we took every phrase
in the sentence which is in the dictionary, and for
each paraphrase of that phrase, replaced the phrase
with the paraphrase to create an instance. This pro-
cedure resulted in 500 cases of paraphrases in con-
text.
Each case was then evaluated by a human judge,
using a web-based annotation system that we devel-
oped. The judges were asked to judge each case on
two dimensions: a) whether the paraphrase is gram-
matical in context; and b) whether the paraphrase
retains the meaning of the original phrase given the
context. Figure 2 gives a screen shot of the annota-
tion system.
50 of the 500 cases were judged by two judges, in
order to obtain some indication of whether the gram-
maticality and meaning retention judgements are vi-
able; the rest were judged by one annotator. (The
500 instances were randomly distributed among 10
native speakers, each being given 55 instances to
judge.) For the meaning retention check, only 34 out
of the 50 cases received the same judgement. One
reason for the low agreement may be that, for 11 of
the 16 disagreement cases, we were asking annota-
594
tors to judge the meaning retention of paraphrases
which had been judged to be ungrammatical in con-
text, which may not be a meaningful task. For the
grammatical check, 42 out of the 50 cases received
the same judgement, a much higher level of agree-
ment.
Since the meaning retention judgements were un-
reliable, we used only the grammatical judgements
to evaluate our system. Hence we are interested
in evaluating whether our n-gram and parser-based
systems can determine if a paraphrase is grammat-
ical in context. Meaning retention is important for
the imperceptibility requirement, but grammatical-
ity is even more so, since ungrammatical sentences
will be easy for an observer to spot. However, we
recognise that only testing for grammaticality does
not fully test the imperceptibility properties of the
system, only part of it.
For the 8 cases which received different judge-
ments on grammaticality, the second author of this
paper made the definitive judgement, which resulted
in a test set of 308 paraphrases judged as grammat-
ical in context, and 192 paraphrases judged as un-
grammatical in context.
4 Proposed Method and Experiments
4.1 Google N-gram Method
The main idea for testing the use of paraphrases is
to check if the various contextual n-grams appear
in the Google n-gram data, or were already in the
original sentence (before paraphrasing). Let us first
define some notation to be used in describing the
method. The leftmost and rightmost <m> words in
the phrase/paraphrase are represented as <m>INLeft
and <m>INRight, respectively. Words at the left and
right side of the substituted phrase are defined as
<c>OUTLeft and <c>OUTRight, where <c> is an
integer which indicates the number of words rep-
resented. Also, we define a context window pair
W<c><n> = (WL
<c>
<n>,WR
<c>
<n>), where WL
<c>
<n> is
composed by <c>OUTLeft concatenated with <n-
c>INLeft, and WR<c><n> is composed by <n-c>INRight
concatenated with <c>OUTRight. Figure 3 gives an
example of the context window pairs W 13 and W
2
3 in
the sentence Soviets said that it is too early to say
whether that will happen where the phrase too early
to is being considered in context.
Figure 3: An example of the context window pair
INPUT: S, P, P ?, n,maxC
OUTPUT: the acceptability of paraphrase P ?
checked by (n, maxC)
FOR each context size C from 1 to maxC
GET a context window pair WCn
IF O(WCn ) is zero THEN
OUTPUT paraphrase P ? fails
END FOR
OUTPUT paraphrase P ? passes
Figure 4: Procedure for checking acceptability
We define a google-count function G(). This func-
tion takes a context window pair W<c><n> as input and
outputs a frequency count pair of W<c><n> recorded in
the Google n-gram data. If a context window cannot
be found in the Google n-gram data, the frequency
count of that window is zero. Also, we define a bi-
nary occurrence function O(). It is used to deter-
mine whether a context window pair can be passed
as acceptable. The input of this function is W<c><n>.
The function outputs one if either both WL<c><n> and
WR<c><n> already occurred in the original sentence
(before paraphrasing) or if the frequency counts out-
put by G(W<c><n>) are both greater than zero.
The two major components in our method are the
paraphrase dictionary and the Google n-gram data.
Once a phrase P in the cover sentence S is matched
with that in the paraphrase dictionary, we test the use
of its paraphrase P ? by the following method. This
method takes into account maximum C contextual
words at both sides of the target phrase, and uses
Google n-gram data as a check, where n = 2, 3, 4 or
5, and maxC = 1 to n? 1. Each pair of (n, maxC)
provides a separate check, by considering both left
and right contexts for these values.
Figure 4 describes the procedure for checking the
595
acceptability of paraphrasing phrase P with P ? in
a given sentence S, given the n-gram size and the
maximum considered context size maxC. For ex-
ample, we want to check the acceptability of the
paraphrase in context shown in Figure 3 by using
google tri-gram data (n = 3) and taking maximum
context size equal to two into consideration (maxC
= 2). The procedure starts from taking context size
C equal to one into account, namely checking the
occurrence of W 13 . If the paraphrase P
? passes the
current test, in the next iteration it will be tested by
taking one more context word into account, namely
W 23 . However, If the paraphrase P
? fails the current
(n, C) check the checking procedure will terminate
and report that the paraphrase fails. In contrast, if
the paraphrase passes all the (n, C) checks where
C = 1 to maxC, the procedure determines the para-
phrase as acceptable. What is happening is that an n-
gram window is effectively being shifted across the
paraphrase boundary to include different amounts of
context and paraphrase.
4.2 Syntactic Filter
In order to improve the grammaticality checking, we
use a parser as an addition to the basic Google n-
gram method. We use the Clark and Curran (2007)
CCG parser to analyse the sentence before and af-
ter paraphrasing. Combinatory Categorial Grammar
(CCG) is a lexicalised grammar formalism, in which
CCG lexical categories ? typically expressing sub-
categorisation information ? are assigned to each
word in a sentence. The grammatical check works
by checking if the words in the sentence outside of
the phrase and paraphrase receive the same lexical
categories before and after paraphrasing. If there is
any change in lexical category assignment to these
words then the paraphrase is judged ungrammati-
cal. Hence the grammar check is at the word, rather
than derivation, level; however, CCG lexical cate-
gories contain a large amount of syntactic informa-
tion which this method is able to exploit.
4.3 Results
The test corpus described in Section 3.3 was split
into development and test data: 100 instances for
development and 400 for testing. The development
data was used for preliminary experiments. For the
test data, 246 of the examples (61.5%) had been
Acc% P% R% F%
baseline 61.5 61.5 100.0 76.2
parser 68.3 67.4 93.9 78.4
Table 3: Grammar check using CCG parser
judged as grammatical, and 154 (38.5%) had been
judged as ungrammatical by the annotators.
The performance of the system is evaluated us-
ing accuracy, precision, recall and balanced F-
measure. Accuracy is the percentage of correct
judgements over all grammatical and ungrammati-
cal paraphrases. Precision is the percentage of para-
phrases judged grammatical by the system which are
judged grammatical by the human judges, and recall
is the percentage of paraphrases judged grammatical
by human judges which are also judged grammatical
by the system. Precision and recall are relevant in
our setting because high precision implies high im-
perceptibility, since grammatical phrases in context
are less likely to be viewed as suspicious by the ob-
server; whereas high recall maximises the payload
(given the dictionary), since high recall implies that
phrases are being paraphrased where possible (and
hence embedding as much information as possible).
An accuracy baseline is obtained by always re-
turning the majority class, in this case always judg-
ing the paraphrase grammatical, which gives an ac-
curacy of 61.5%. Table 3 gives the performance
when only the CCG parser is used for checking gram-
maticality. As far as steganography is concerned, the
precision is low, since over 30% of the paraphrases
used are ungrammatical, which is likely to raise the
suspicions of the observer.
Table 4 gives the results for the Google n-gram
method, for various n-gram and context sizes. As the
n-gram size increases ? meaning that a larger part
of the context is used ? the accuracy falls below
that of the baseline. However, from a steganogra-
phy aspect, accuracy is not useful, since the trade-
off between precision and recall is more relevant.
As expected, with larger n-grams checking the left
and right contexts, the precision increases, reaching
100% for the 5-grams. Hence, as far as grammati-
cality judgements are concerned, the imperceptibil-
ity requirement is completely satisified. However,
the large drop in recall means that the imperceptibil-
596
N-
gram
Context Accuracy Precision Recall F-measure
Size (%) (%) (%) (%)
2-
gram
1 62.0 62.1 98.0 76.0
3-
gram
1 62.5 65.1 84.2 73.4
2 67.3 72.9 74.4 73.6
4-
gram
1 58.5 71.3 54.5 61.8
2 53.2 84.7 29.3 43.5
3 51.8 89.6 24.4 38.3
5-
gram
1 54.8 85.0 32.1 46.6
2 43.5 95.5 8.5 15.7
3 41.0 100.0 4.1 7.8
4 41.0 100.0 4.1 7.8
Table 4: Performance of google n-gram method
ity is achieved at the cost of a reduced payload, since
many of the grammatical paraphrases that could be
used to embed information are being discarded.
Table 5 shows the results for the Google n-gram
method followed by the parser check; that is, if the
Google n-gram method judges the paraphrase to be
grammatical, then it is passed to the CCG parser for
an additional check. Adding the parser generally
increases the precision with a slight loss in recall.
Which settings are best to use in practice would de-
pend on how the steganography user wished to trade
off imperceptibility for payload.
5 Possible embedding method
In this section, we propose a linguistic hiding
method which can be integrated with an automatic
paraphrasing system. It needs a large paraphrase
dictionary to determine modifiable phrases and pro-
vide available paraphrases. The embedding capacity
of the proposed linguistic stegosystem relies on the
number of paraphrasable sentences in the cover text.
If every sentence in the cover text is paraphrasable,
the system can have the maximum embedding ca-
pacity equal to 1 bit per sentence which is compara-
ble to other linguistic steganography methods using
syntactic transformations and synonym substitution.
N-
gram
Context Accuracy Precision Recall F-measure
Size (%) (%) (%) (%)
2-
gram
1 68.0 67.7 91.9 78.0
3-
gram
1 67.3 70.9 79.3 74.9
2 69.5 77.7 70.7 74.0
4-
gram
1 59.5 75.6 50.4 60.5
2 53.8 88.6 28.5 43.1
3 52.0 92.2 24.0 38.1
5-
gram
1 53.8 86.8 29.3 43.8
2 43.3 95.2 8.1 15.0
3 41.0 100.0 4.1 7.8
4 41.0 100.0 4.1 7.8
Table 5: Performance of google n-gram method with the
CCG parser filter
5.1 Data Embedding Procedure
First the sentences in a cover text T are identi-
fied using a sentence segmentation algorithm, giv-
ing N sentences s1, s2,. . . , sN . The paraphrasabil-
ity of each sentence is then checked using our au-
tomatic method. If a sentence contains at least one
paraphrasable phrase, we call the sentence a para-
phrasable sentence or a non-paraphrasable sen-
tence otherwise. Let D be the maximum number of
sentence boundaries between two subsequent para-
phrasable sentences in T. Thus, for every D sen-
tences within a cover text T, there will be at least
one paraphrasable sentence. Let every unit of D sen-
tences serve as one embedding unit in which a single
secret bit can be embedded. If we want to embed
0 in an embedding unit, we transform all the para-
phrasable sentences in this embedding unit to non-
paraphrasable sentences (assuming certain proper-
ties of the dictionary; see end of this section for dis-
cussion). If we want to embed 1, we leave the em-
bedding unit without any modifications.
Figure 5 demonstrates the embedding of the se-
cret bitstring 101 in a cover text containing nine sen-
tences t1, t2,. . . , t9 defined by a sentence segmenta-
tion algorithm. First, t1, t3, t4, t7 and t9 are de-
termined as paraphrasable sentences and thus D, the
597
Figure 5: Embedding secret bits in a cover text using sen-
tence segmentation method
size of an embedding unit, is 3. Next, we segment
the cover text into three embedding units u1, u2 and
u3, each of which contains three sentences. Since
we want to embed secret bits 101 in u1, u2 and u3 re-
spectively, the embedding unit u2 should contain no
paraphrasable sentence. That is, the paraphrasable
phrase in t4 should be replaced by its paraphrase.
Finally, the stego text is output and sent along with
the private key D to the other party. A private key is
known only to the parties that exchange messages.
In order for this method to work, we require cer-
tain properties of the paraphrase dictionary. For ex-
ample, it is crucial that, once a phrase has been para-
phrased, it does not produce another phrase that can
be paraphrased. This can be achieved by simply
requiring that any paraphrase ?on the RHS? of the
dictionary does not also appear as a phrase on the
LHS. In fact, this is not so unnatural for the Callison-
Burch dictionary, which consists of phrases mapped
to sets of paraphrases, many of which only appear
on one side.
5.2 Data Extracting Procedure
For extracting the secret data, first, the stego text
T ? undergoes sentence segmentation, and N defined
sentences s?1, s
?
2,. . . , s
?
N are obtained. According
to the private key D, every D sentences are treated
as an information unit, and in each unit we check
the occurrence of paraphrasable sentences making
use of our paraphrasing method. If an information
unit contains at least one paraphrasable sentence,
this information unit implies the embedding of 1.
In contrast, if none of the sentences in the informa-
tion unit are paraphrasable, it implies the embedding
of 0. Hence, in order to recover the hidden mes-
sage, the receiver requires the sentence segmentation
algorithm, the paraphrase dictionary, the automatic
program determining grammaticality of paraphrases
in context, and the secret key D. The extraction pro-
cess essentially reverses the embedding method.
6 Conclusions
The contributions of this paper are to develop an
automatic system for checking the grammaticality
and fluency of paraphrases in context, and the pro-
posal of using paraphrases as a suitable transfor-
mation for Linguistic Steganography. An advan-
tage of our proposed method is that it is somewhat
language and domain independent, requiring only a
paraphrase dictionary and a Google n-gram corpus,
both of which are likely to be available for a range
of languages in the future.
There are various practical issues in the applica-
tion of Linguistic Steganography systems that we
have chosen to ignore. For example, we have not
discussed the choice of cover text. If a newspaper ar-
ticle were chosen as the cover text, then any changes
could be easily found in practice by comparing the
stego text with the original article, which is likely
to be readily available. Another interesting ques-
tion that we have not addressed is whether some lan-
guages are better suited to Linguistic Steganography
than others, or whether some languages are better
suited to particular linguistic transformations than
others. Finally, we have only evaluated our gram-
matical checker and not the steganography system
itself (other than giving an indication of the likely
payload). How best to evaluate the imperceptibility
of such a system we leave to future work.
Acknowledgements
We would like to thank Chris Callison-Burch for pro-
viding the paraphrase dictionary, Katja Markert, Stephen
Pulman, Laura Rimell, and the anonymous reviewers for
useful comments. Ching-Yun Chang was funded by an
Oxford University Clarendon scholarship.
598
References
Mikhail J. Atallah, Craig J. McDonough, Victor Raskin,
and Sergei Nirenburg. 2001a. Natural language pro-
cessing for information assurance and security: an
overview and implementations. In Proceedings of the
2000 workshop on New security paradigms, pages 51?
65, Ballycotton, County Cork, Ireland.
Mikhail J. Atallah, Victor Raskin, Michael C. Crogan,
Christian Hempelmann, Florian Kerschbaum, Dina
Mohamed, and Sanket Naik. 2001b. Natural lan-
guage watermarking: design, analysis, and a proof-
of-concept implementation. In Proceedings of the 4th
International Information Hiding Workshop, volume
2137, pages 185?199, Pittsburgh, Pennsylvania.
Mikhail J. Atallah, Victor Raskin, Christian F. Hempel-
mann, Mercan Karahan, Umut Topkara, Katrina E.
Triezenberg, and Radu Sion. 2002. Natural language
watermarking and tamperproofing. In Proceedings of
the 5th International Information Hiding Workshop,
pages 196?212, Noordwijkerhout, The Netherlands.
Regina Barzilay and Kathleen R. McKeown. 2001. Ex-
tracting paraphrases from a parallel corpus. In Pro-
ceedings of the 39th ACL, pages 50?57, Toulouse.
Richard Bergmair. 2007. A comprehensive bibliogra-
phy of linguistic steganography. In Proceedings of the
SPIE Conference on Security, Steganography, and Wa-
termarking of Multimedia Contents, volume 6505.
Shane Bergsma, Dekang Lin, and Randy Goebel. 2009.
Web-scale n-gram models for lexical disambiguation.
In Proceedings of the 21st International Joint Con-
ference on Artifical Intelligence, pages 1507?1512,
Pasadena, CA.
Igor A. Bolshakov. 2004. A method of linguistic
steganography based on coladdressally-verified syn-
onym. In Information Hiding: 6th International Work-
shop, volume 3200, pages 180?191, Toronto, Canada.
Chris Callison-Burch. 2008. Syntactic constraints on
paraphrases extracted from parallel corpora. In Pro-
ceedings of the EMNLP Conference, pages 196?205,
Honolulu, Hawaii.
Mark Chapman and George I. Davida. 1997. Hiding the
hidden: A software system for concealing ciphertext
as innocuous text. In Proceedings of the First Interna-
tional Conference on Information and Communication
Security, volume 1334, pages 335?345, Beijing.
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG and
log-linear models. Comp. Ling., 33(4):493?552.
Jessica Fridrich. 2009. Steganography in Digital Media:
Principles, Algorithms, and Applications. Cambridge
University Press, first edition.
Yuling Liu, Xingming Sun, and Yong Wu. 2005. A nat-
ural language watermarking based on Chinese syntax.
In Advances in Natural Computation, volume 3612,
pages 958?961, Changsha, China.
Mitchell P. Marcus, Beatrice Santorini, and Mary A.
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguistics, 19:313?330.
Hasan M. Meral, Emre Sevinc, Ersin Unkar, Bulent
Sankur, A. Sumru Ozsoy, and Tunga Gungor. 2007.
Syntactic tools for text watermarking. In Proceed-
ings of the SPIE Conference on Security, Steganogra-
phy, and Watermarking of Multimedia Contents, vol-
ume 6505, San Jose, CA.
Brian Murphy and Carl Vogel. 2007. The syntax of con-
cealment: reliable methods for plain text information
hiding. In Proceedings of the SPIE Conference on Se-
curity, Steganography, and Watermarking of Multime-
dia Contents, volume 6505, San Jose, CA.
Brian Murphy. 2001. Syntactic information hiding in
plain text. Masters Thesis. Trinity College Dublin.
Lip Y. Por, Ang T. Fong, and B. Delina. 2008.
Whitesteg: a new scheme in information hiding using
text steganography. WSEAS Transactions on Comput-
ers, 7:735?745.
Cuneyt M. Taskiran, Mercan Topkara, and Edward J.
Delp. 2006. Attacks on linguistic steganography sys-
tems using text analysis. In Proceedings of the SPIE
Conference on Security, Steganography, and Water-
marking of Multimedia Contents, volume 6072, pages
97?105, San Jose, CA.
Mercan Topkara, Cuneyt M. Taskiran, and Edward J.
Delp. 2005. Natural language watermarking.
In Proceedings of the SPIE Conference on Secu-
rity, Steganography, and Watermarking of Multimedia
Contents, volume 5681, pages 441?452, San Jose, CA.
Mercan Topkara, Umut Topkara, and Mikhail J. Atallah.
2006a. Words are not enough: sentence level natural
language watermarking. In Proceedings of the ACM
Workshop on Content Protection and Security, pages
37?46, Santa Barbara, CA.
Umut Topkara, Mercan Topkara, and Mikhail J. Atal-
lah. 2006b. The hiding virtues of ambiguity: quan-
tifiably resilient watermarking of natural language text
through synonym substitutions. In Proceedings of the
8th Workshop on Multimedia and Security, pages 164?
174, Geneva, Switzerland.
M. Olga Vybornova and Benoit Macq. 2007. A
method of text watermarking using presuppositions.
In Proceedings of the SPIE Conference on Secu-
rity, Steganography, and Watermarking of Multimedia
Contents, volume 6505, San Jose, CA.
599
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 345?355,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Faster Parsing by Supertagger Adaptation
Jonathan K. Kummerfelda Jessika Roesner b Tim Dawborna James Haggertya
James R. Currana? Stephen Clark c?
School of Information Technologiesa Department of Computer Scienceb Computer Laboratoryc
University of Sydney University of Texas at Austin University of Cambridge
NSW 2006, Australia Austin, TX, USA Cambridge CB3 0FD, UK
james@it.usyd.edu.aua? stephen.clark@cl.cam.ac.uk c?
Abstract
We propose a novel self-training method
for a parser which uses a lexicalised gram-
mar and supertagger, focusing on increas-
ing the speed of the parser rather than
its accuracy. The idea is to train the su-
pertagger on large amounts of parser out-
put, so that the supertagger can learn to
supply the supertags that the parser will
eventually choose as part of the highest-
scoring derivation. Since the supertag-
ger supplies fewer supertags overall, the
parsing speed is increased. We demon-
strate the effectiveness of the method us-
ing a CCG supertagger and parser, obtain-
ing significant speed increases on newspa-
per text with no loss in accuracy. We also
show that the method can be used to adapt
the CCG parser to new domains, obtain-
ing accuracy and speed improvements for
Wikipedia and biomedical text.
1 Introduction
In many NLP tasks and applications, e.g. distribu-
tional similarity (Curran, 2004) and question an-
swering (Dumais et al, 2002), large volumes of
text and detailed syntactic information are both
critical for high performance. To avoid a trade-
off between these two, we need to increase parsing
speed, but without losing accuracy.
Parsing with lexicalised grammar formalisms,
such as Lexicalised Tree Adjoining Grammar and
Combinatory Categorial Grammar (CCG; Steed-
man, 2000), can be made more efficient using a
supertagger. Bangalore and Joshi (1999) call su-
pertagging almost parsing because of the signifi-
cant reduction in ambiguity which occurs once the
supertags have been assigned.
In this paper, we focus on the CCG parser and
supertagger described in Clark and Curran (2007).
Since the CCG lexical category set used by the su-
pertagger is much larger than the Penn Treebank
POS tag set, the accuracy of supertagging is much
lower than POS tagging; hence the CCG supertag-
ger assigns multiple supertags1 to a word, when
the local context does not provide enough infor-
mation to decide on the correct supertag.
The supertagger feeds lexical categories to the
parser, and the two interact, sometimes using mul-
tiple passes over a sentence. If a spanning analy-
sis cannot be found by the parser, the number of
lexical categories supplied by the supertagger is
increased. The supertagger-parser interaction in-
fluences speed in two ways: first, the larger the
lexical ambiguity, the more derivations the parser
must consider; second, each further pass is as
costly as parsing a whole extra sentence.
Our goal is to increase parsing speed without
loss of accuracy. The technique we use is a form
of self-training, in which the output of the parser is
used to train the supertagger component. The ex-
isting literature on self-training reports mixed re-
sults. Clark et al (2003) were unable to improve
the accuracy of POS tagging using self-training.
In contrast, McClosky et al (2006a) report im-
proved accuracy through self-training for a two-
stage parser and re-ranker.
Here our goal is not to improve accuracy, only
to maintain it, which we achieve through an adap-
tive supertagger. The adaptive supertagger pro-
duces lexical categories that the parser would have
used in the final derivation when using the base-
line model. However, it does so with much lower
ambiguity levels, and potentially during an ear-
lier pass, which means sentences are parsed faster.
By increasing the ambiguity level of the adaptive
models to match the baseline system, we can also
slightly increase supertagging accuracy, which can
lead to higher parsing accuracy.
1We use supertag and lexical category interchangeably.
345
Using the parser to generate training data also
has the advantage that it is not a domain specific
process. Previous work has shown that parsers
typically perform poorly outside of their train-
ing domain (Gildea, 2001). Using a newspaper-
trained parser, we constructed new training sets for
Wikipedia and biomedical text. These were used
to create new supertagging models adapted to the
different domains.
The self-training method of adapting the su-
pertagger to suit the parser increased parsing speed
by more than 50% across all three domains, with-
out loss of accuracy. Using an adapted supertagger
with ambiguity levels tuned to match the baseline
system, we were also able to increase F-score on
labelled grammatical relations by 0.75%.
2 Background
Many statistical parsers use two stages: a tag-
ging stage that labels each word with its gram-
matical role, and a parsing stage that uses the tags
to form a parse tree. Lexicalised grammars typ-
ically contain a much smaller set of rules than
phrase-structure grammars, relying on tags (su-
pertags) that contain a more detailed description
of each word?s role in the sentence. This leads to
much larger tag sets, and shifts a large proportion
of the search for an optimal derivation to the tag-
ging component of the parser.
Figure 1 gives two sentences and their CCG
derivations, showing how some of the syntactic
ambiguity is transferred to the supertagging com-
ponent in a lexicalised grammar. Note that the
lexical category assigned to with is different in
each case, reflecting the fact that the prepositional
phrase attaches differently. Either we need a tag-
ging model that can resolve this ambiguity, or both
lexical categories must be supplied to the parser
which can then attempt to resolve the ambiguity
by eventually selecting between them.
2.1 Supertagging
Supertaggers typically use standard linear-time
tagging algorithms, and only consider words in the
local context when assigning a supertag. The C&C
supertagger is similar to the Ratnaparkhi (1996)
tagger, using features based on words and POS
tags in a five-word window surrounding the target
word, and defining a local probability distribution
over supertags for each word in the sentence, given
the previous two supertags. The Viterbi algorithm
I ate pizza with cutlery
NP (S\NP)/NP NP ((S\NP)\(S\NP))/NP NP
> >
S\NP (S\NP)\(S\NP)
<
S\NP
<
S
I ate pizza with anchovies
NP (S\NP)/NP NP (NP\NP)/NP NP
>
NP\NP
<
NP
>
S\NP
<
S
Figure 1: Two CCG derivations with PP ambiguity.
can be used to find the most probable supertag se-
quence. Alternatively the Forward-Backward al-
gorithm can be used to efficiently sum over all se-
quences, giving a probability distribution over su-
pertags for each word which is conditional only on
the input sentence.
Supertaggers can be made accurate enough for
wide coverage parsing using multi-tagging (Chen
et al, 1999), in which more than one supertag
can be assigned to a word; however, as more su-
pertags are supplied by the supertagger, parsing
efficiency decreases (Chen et al, 2002), demon-
strating the influence of lexical ambiguity on pars-
ing complexity (Sarkar et al, 2000).
Clark and Curran (2004) applied supertagging
to CCG, using a flexible multi-tagging approach.
The supertagger assigns to a word all lexical cate-
gories whose probabilities are within some factor,
?, of the most probable category for that word.
When the supertagger is integrated with the C&C
parser, several progressively lower ? values are
considered. If a sentence is not parsed on one
pass then the parser attempts to parse the sentence
again with a lower ? value, using a larger set of
categories from the supertagger. Since most sen-
tences are parsed at the first level (in which the av-
erage number of supertags assigned to each word
is only slightly greater than one), this provides
some of the speed benefit of single tagging, but
without loss of coverage (Clark and Curran, 2004).
Supertagging has since been effectively applied
to other formalisms, such as HPSG (Blunsom and
Baldwin, 2006; Zhang et al, 2009), and as an in-
formation source for tasks such as Statistical Ma-
chine Translation (Hassan et al, 2007). The use
of parser output for supertagger training has been
explored for LTAG by Sarkar (2007). However, the
focus of that work was on improving parser and
supertagger accuracy rather than speed.
346
Previously , watch imports were denied such duty-free treatment
S/S , N /N N (S [dcl ]\NP)/(S [pss]\NP) (S [pss]\NP)/NP NP/NP N/N N
N N (S[dcl]\NP)/NP S [pss]\NP (N /N )/(N /N )
S [adj ]\NP (S [dcl ]\NP)/(S [adj ]\NP) (S [pss]\NP)/NP N /N
(S [pt ]\NP)/NP
(S[dcl]\NP)/NP
Figure 2: An example sentence and the sets of categories assigned by the supertagger. The first category
in each column is correct and the categories used by the parser are marked in bold. The correct category
for watch is included here, for expository purposes, but in fact was not provided by the supertagger.
2.2 Semi-supervised training
Previous exploration of semi-supervised training
in NLP has focused on improving accuracy, often
for the case where only small amounts of manually
labelled training data are available. One approach
is co-training, in which two models with indepen-
dent views of the data iteratively inform each other
by labelling extra training data. Sarkar (2001) ap-
plied co-training to LTAG parsing, in which the su-
pertagger and parser provide the two views. Steed-
man et al (2003) extended the method to a variety
of parser pairs.
Another method is to use a re-ranker (Collins
and Koo, 2002) on the output of a system to gener-
ate new training data. Like co-training, this takes
advantage of a different view of the data, but the
two views are not independent as the re-ranker is
limited to the set of options produced by the sys-
tem. This method has been used effectively to
improve parsing performance on newspaper text
(McClosky et al, 2006a), as well as adapting a
Penn Treebank parser to a new domain (McClosky
et al, 2006b).
As well as using independent views of data to
generate extra training data, multiple views can be
used to provide constraints at test time. Holling-
shead and Roark (2007) improved the accuracy
of a parsing pipeline by using the output of later
stages to constrain earlier stages.
The only work we are aware of that uses self-
training to improve the efficiency of parsers is van
Noord (2009), who adopts a similar idea to the
one in this paper for improving the efficiency of
a Dutch parser based on a manually constructed
HPSG grammar.
3 Adaptive Supertagging
The purpose of the supertagger is to cut down the
search space for the parser by reducing the set of
categories that must be considered for each word.
A perfect supertagger would assign the correct cat-
egory to every word. CCG supertaggers are about
92% accurate when assigning a single lexical cate-
gory to each word (Clark and Curran, 2004). This
is not accurate enough for wide coverage parsing
and so a multi-tagging approach is used instead.
In the final derivation, the parser uses one category
from each set, and it is important to note that hav-
ing the correct category in the set does not guaran-
tee that the parser will use it.
Figure 2 gives an example sentence and the sets
of lexical categories supplied by the supertagger,
for a particular value of ?.2 The usual target of
the supertagging task is to produce the top row of
categories in Figure 2, the correct categories. We
propose a new task that instead aims for the cat-
egories the parser will use, which are marked in
bold for this case. The purpose of this new task is
to improve speed.
The reason speed will be improved is that we
can construct models that will constrain the set of
possible derivations more than the baseline model.
We can construct these models because we can
obtain much more of our target output, parser-
annotated sentences, than we could for the gold-
standard supertagging task.
The new target data will contain tagging errors,
and so supertagging accuracy measured against
the correct categories may decrease. If we ob-
tained perfect accuracy on our new task then we
would be removing all of the categories not cho-
sen by the parser. However, parsing accuracy will
not decrease since the parser will still receive the
categories it would have used, and will therefore
be able to form the same highest-scoring deriva-
tion (and hence will choose it).
To test this idea we parsed millions of sentences
2Two of the categories for such have been left out for
reasons of space, and the correct category for watch has been
included for expository reasons. The fact that the supertagger
does not supply this category is the reason that the parser does
not analyse the sentence correctly.
347
in three domains, producing new data annotated
with the categories that the parser used with the
baseline model. We constructed new supertagging
models that are adapted to suit the parser by train-
ing on the combination of these sets and the stan-
dard training corpora. We applied standard evalu-
ation metrics for speed and accuracy, and explored
the source of the changes in parsing performance.
4 Data
In this work, we consider three domains: news-
wire, Wikipedia text and biomedical text.
4.1 Training and accuracy evaluation
We have used Sections 02-21 of CCGbank (Hock-
enmaier and Steedman, 2007), the CCG version of
the Penn Treebank (Marcus et al, 1993), as train-
ing data for the newspaper domain. Sections 00
and 23 were used for development and test eval-
uation. A further 113,346,430 tokens (4,566,241
sentences) of raw data from the Wall Street Jour-
nal section of the North American News Corpus
(Graff, 1995) were parsed to produce the training
data for adaptation. This text was tokenised us-
ing the C&C tools tokeniser and parsed using our
baseline models. For the smaller training sets, sen-
tences from 1988 were used as they would be most
similar in style to the evaluation corpus. In all ex-
periments the sentences from 1989 were excluded
to ensure no overlap occurred with CCGbank.
As Wikipedia text we have used 794,024,397
tokens (51,673,069 sentences) from Wikipedia ar-
ticles. This text was processed in the same way as
the NANC data to produce parser-annotated train-
ing data. For supertagger evaluation, one thousand
sentences were manually annotated with CCG lex-
ical categories and POS tags. For parser evalua-
tion, three hundred of these sentences were man-
ually annotated with DepBank grammatical rela-
tions (King et al, 2003) in the style of Briscoe
and Carroll (2006). Both sets of annotations were
produced by manually correcting the output of the
baseline system. The annotation was performed
by Stephen Clark and Laura Rimell.
For the biomedical domain we have used sev-
eral different resources. As gold standard data for
supertagger evaluation we have used supertagged
GENIA data (Kim et al, 2003), annotated by
Rimell and Clark (2008). For parsing evalua-
tion, grammatical relations from the BioInfer cor-
pus were used (Pyysalo et al, 2007), with the
Source Sentence Length Corpus %
Range Average Variance
0-4 3.26 0.64 1.2
5-20 14.04 17.41 39.2
News 21-40 28.76 29.27 49.4
41-250 49.73 86.73 10.2
All 24.83 152.15 100.0
0-4 2.81 0.60 22.4
5-20 11.64 21.56 48.9
Wiki 21-40 28.02 28.48 24.3
41-250 49.69 77.70 4.5
All 15.33 154.57 100.0
0-4 2.98 0.75 0.9
5-20 14.54 15.14 41.3
Bio 21-40 28.49 29.34 48.0
41-250 49.17 68.34 9.8
All 24.53 139.35 100.0
Table 1: Statistics for sentences in the supertagger
training data. Sentences containing more than 250
tokens were not included in our data sets.
same post-processing process as Rimell and Clark
(2009) to convert the C&C parser output to Stan-
ford format grammatical relations (de Marneffe
et al, 2006). For adaptive training we have
used 1,900,618,859 tokens (76,739,723 sentences)
from the MEDLINE abstracts tokenised by McIn-
tosh and Curran (2008). These sentences were
POS-tagged and parsed twice, once as for the
newswire and Wikipedia data, and then again, us-
ing the bio-specific models developed by Rimell
and Clark (2009). Statistics for the sentences in
the training sets are given in Table 1.
4.2 Speed evaluation data
For speed evaluation we held out three sets of sen-
tences from each domain-specific corpus. Specif-
ically, we used 30,000, 4,000 and 2,000 unique
sentences of length 5-20, 21-40 and 41-250 tokens
respectively. Speeds on these length controlled
sets were combined to calculate an overall pars-
ing speed for the text in each domain. Note that
more than 20% of the Wikipedia sentences were
less than five words in length and the overall dis-
tribution is skewed towards shorter sentences com-
pared to the other corpora.
5 Evaluation
We used the hybrid parsing model described in
Clark and Curran (2007), and the Viterbi decoder
to find the highest-scoring derivation. The multi-
pass supertagger-parser interaction was also used.
The test data was excluded from training data
for the supertagger for all of the newswire and
Wikipedia models. For the biomedical models ten-
348
fold cross validation was used. The accuracy of
supertagging is measured by multi-tagging at the
first ? level and considering a word correct if the
correct tag is amongst any of the assigned tags.
For the biomedical parser evaluation we have
used the parsing model and grammatical relation
conversion script from Rimell and Clark (2009).
Our timing measurements are calculated in two
ways. Overall times were measured using the C&C
parser?s timers. Individual sentence measurements
were made using the Intel timing registers, since
standard methods are not accurate enough for the
short time it takes to parse a single sentence.
To check whether changes were statistically sig-
nificant we applied the test described by Chinchor
(1995). This measures the probability that two sets
of responses are drawn from the same distribution,
where a score below 0.05 is considered significant.
Models were trained on an Intel Core2Duo
3GHz with 4GB of RAM. The evaluation was per-
formed on a dual quad-core Intel Xeon 2.27GHz
with 16GB of RAM.
5.1 Tagging ambiguity optimisation
The number of lexical categories assigned to a
word by the CCG supertagger depends on the prob-
abilities calculated for each category and the ?
level being used. Each lexical category with a
probability within a factor of ? of the most prob-
able category is included. This means that the
choice of ? level determines the tagging ambigu-
ity, and so has great influence on parsing speed, ac-
curacy and coverage. Also, the tagging ambiguity
produced by a ? level will vary between models.
A more confident model will have a more peaked
distribution of category probabilities for a word,
and therefore need a smaller ? value to assign the
same number of categories.
Additionally, the C&C parser uses multiple ?
levels. The first pass over a sentence is at a high ?
level, resulting in a low tagging ambiguity. If the
categories assigned are too restrictive to enable a
spanning analysis, the system makes another pass
with a lower ? level, resulting in a higher tagging
ambiguity. A maximum of five passes are made,
with the ? levels varying from 0.075 to 0.001.
We have taken two approaches to choosing ?
levels. When the aim of an experiment is to im-
prove speed, we use the system?s default ? levels.
While this choice means a more confident model
will assign fewer tags, this simply reflects the fact
that the model is more confident. It should pro-
duce similar accuracy results, but with lower am-
biguity, which will lead to higher speed.
For accuracy optimisation experiments we tune
the ? levels to produce the same average tagging
ambiguity as the baseline model on Section 00 of
CCGbank. Accuracy depends heavily on the num-
ber of categories supplied, so the new models are
at an accuracy disadvantage if they propose fewer
categories. By matching the ambiguity of the de-
fault model, we can increase accuracy at the cost
of some of the speed improvements the new mod-
els obtain.
6 Results
We have performed four primary sets of exper-
iments to explore the ability of an adaptive su-
pertagger to improve parsing speed or accuracy. In
the first two experiments, we explore performance
on the newswire domain, which is the source of
training data for the parsing model and the base-
line supertagging model. In the second set of ex-
periments, we train on a mixture of gold standard
newswire data and parser-annotated data from the
target domain.
In both cases we perform two experiments. The
first aimed to improve speed, keeping the ? levels
the same. This should lead to an increase in speed
as the extra training data means the models are
more confident and so have lower ambiguity than
the baseline model for a given ? value. The second
experiment aimed to improve accuracy, tuning the
? levels as described in the previous section.
6.1 Newswire speed improvement
In our first experiment, we trained supertagger
models using Generalised Iterative Scaling (GIS)
(Darroch and Ratcliff, 1972), the limited mem-
ory BFGS method (BFGS) (Nocedal and Wright,
1999), the averaged perceptron (Collins, 2002),
and the margin infused relaxed algorithm (MIRA)
(Crammer and Singer, 2003). Note that these
are all alternative methods for estimating the lo-
cal log-linear probability distributions used by the
Ratnaparkhi-style tagger. We do not use global
tagging models as in Lafferty et al (2001) or
Collins (2002). The training data consisted of Sec-
tions 02?21 of CCGbank and progressively larger
quantities of parser-annotated NANC data ? from
zero to four million extra sentences. The results of
these tests are presented in Table 2.
349
Ambiguity (%) Tagging Accuracy (%) F-score Speed (sents / sec)
Data 0k 40k 400k 4m 0k 40k 400k 4m 0k 40k 400k 4m 0k 40k 400k 4m
Baseline 1.27 96.34 85.46 39.6
BFGS 1.27 1.23 1.19 1.18 96.33 96.18 95.95 95.93 85.45 85.51 85.57 85.68 39.8 49.6 71.8 60.0
GIS 1.28 1.24 1.21 1.20 96.44 96.27 96.09 96.11 85.44 85.46 85.58 85.62 37.4 44.1 51.3 54.1
MIRA 1.30 1.24 1.17 1.13 96.44 96.14 95.56 95.18 85.44 85.40 85.38 85.42 34.1 44.8 60.2 73.3
Table 2: Speed improvements on newswire, using various amounts of parser-annotated NANC data.
Sentences Av. Time Change (ms) Total Time Change (s)
Sentence length 5-20 21-40 41-250 5-20 21-40 41-250 5-20 21-40 41-250
Lower tag amb. 1166 333 281 -7.54 -71.42 -183.23 -1.1 -29 -26
Earlier pass Same tag amb. 248 38 8 -2.94 -27.08 -108.28 -0.095 -1.3 -0.44
Higher tag amb. 530 33 14 -5.84 -32.25 -44.10 -0.40 -1.3 -0.31
Lower tag amb. 19288 3120 1533 -1.13 -5.18 -38.05 -2.8 -20 -30
Same pass Same tag amb. 7285 259 35 -0.29 0.94 24.57 -0.28 0.30 0.44
Higher tag amb. 1133 101 24 -0.25 2.70 8.09 -0.037 0.34 0.099
Lower tag amb. 334 114 104 0.90 7.60 -46.34 0.039 1.1 -2.5
Later pass Same tag amb. 14 1 0 1.06 4.26 n/a 0.0019 0.0053 0.0
Higher tag amb. 2 1 1 -0.13 26.43 308.03 -3.4e-05 0.033 0.16
Table 3: Breakdown of the source of changes in speed. The test sentences are divided into nine sets
based on the change in parsing behaviour between the baseline model and a model trained using MIRA,
Sections 02-21 of CCGbank and 4,000,000 NANC sentences.
Using the default ? levels we found that the
perceptron-trained models lost accuracy, disqual-
ifying them from this test. The BFGS, GIS and
MIRA models produced mixed results, but no
statistically significant decrease in accuracy, and
as the amount of parser-annotated data was in-
creased, parsing speed increased by up to 85%.
To determine the source of the speed improve-
ment we considered the times recorded by the tim-
ing registers. In Table 3, we have aggregated these
measurements based on the change in the pass at
which the sentence is parsed, and how the tag-
ging ambiguity changes on that pass. For sen-
tences parsed on two different passes the ambigu-
ity comparison is at the earlier pass. The ?Total
Time Change? section of the table is the change in
parsing time for sentences of that type when pars-
ing ten thousand sentences from the corpus. This
takes into consideration the actual distribution of
sentence lengths in the corpus.
Several effects can be observed in these re-
sults. 72% of sentences are parsed on the same
pass, but with lower tag ambiguity (5th row in Ta-
ble 3). This provides 44% of the speed improve-
ment. Three to six times as many sentences are
parsed on an earlier pass than are parsed on a later
pass. This means the sentences parsed later have
very little effect on the overall speed. At the same
time, the average gain for sentences parsed earlier
is almost always larger than the average cost for
sentences parsed later. These effects combine to
produce a particularly large improvement for the
sentences parsed at an earlier pass. In fact, despite
making up only 7% of sentences in the set, those
parsed earlier with lower ambiguity provide 50%
of the speed improvement.
It is also interesting to note the changes for sen-
tences parsed on the same pass, with the same
ambiguity. We may expect these sentences to be
parsed in approximately the same amount of time,
and this is the case for the short set, but not for the
two larger sets, where we see an increase in pars-
ing time. This suggests that the categories being
supplied are more productive, leading to a larger
set of possible derivations.
6.2 Newswire accuracy optimised
Any decrease in tagging ambiguity will generally
lead to a decrease in accuracy. The parser uses a
more sophisticated algorithm with global knowl-
edge of the sentence and so we would expect it
to be better at choosing categories than the su-
pertagger. Unlike the supertagger it will exclude
categories that cannot be used in a derivation. In
the previous section, we saw that training the su-
pertagger on parser output allowed us to develop
models that produced the same categories, despite
lower tagging ambiguity. Since they were trained
on the categories the parser was able to use in
derivations, these models should also now be pro-
viding categories that are more likely to be useful.
This leads us to our second experiment, opti-
350
Tagging Accuracy (%) F-score Speed (sents / sec)
NANC sents 0k 40k 400k 4m 0k 40k 400k 4m 0k 40k 400k 4m
Baseline 96.34 85.46 39.6
BFGS 96.33 96.42 96.42 96.66 85.45 85.55 85.64 85.98 39.5 43.7 43.9 42.7
GIS 96.34 96.43 96.53 96.62 85.36 85.47 85.84 85.87 39.1 41.4 41.7 42.6
Perceptron 95.82 95.99 96.30 - 85.28 85.39 85.64 - 45.9 48.0 45.2 -
MIRA 96.23 96.29 96.46 96.63 85.47 85.45 85.55 85.84 37.7 41.4 41.4 42.9
Table 4: Accuracy optimisation on newswire, using various amounts of parser-annotated NANC data.
Train Corpus Ambiguity Tag. Acc. F-score Speed (sents / sec)
News Wiki Bio News Wiki Bio News Wiki Bio News Wiki Bio
Baseline 1.267 1.317 1.281 96.34 94.52 90.70 85.46 80.8 75.0 39.6 50.9 35.1
News 1.126 1.151 1.130 95.18 93.56 90.07 85.42 81.2 75.2 73.3 83.9 60.3
Wiki 1.147 1.154 1.129 95.06 93.52 90.03 84.70 81.4 75.5 62.4 73.9 58.7
Bio 1.134 1.146 1.114 94.66 93.15 89.88 84.23 80.7 75.9 66.2 90.4 59.3
Table 5: Cross-corpus speed improvement, models trained with MIRA and 4,000,000 sentences. The
highlighted values are the top speed for each evaluation set and results that are statistically indistinguish-
able from it.
mising accuracy on newswire. We used the same
models as in the previous experiment, but tuned
the ? levels as described in Section 5.1.
Comparing Tables 2 and 4 we can see the in-
fluence of ? level choice, and therefore tagging
ambiguity. When the default ? values were used
ambiguity dropped consistently as more parser-
annotated data was used, and category accuracy
dropped in the same way. Tuning the ? levels to
match ambiguity produces the opposite trend.
Interestingly, while the decrease in supertag ac-
curacy in the previous experiment did not translate
into a decrease in F-score, the increase in tag accu-
racy here does translate into an increase in F-score.
This indicates that the supertagger is adapting to
suit the parser. In the previous experiment, the
supertagger was still providing the categories the
parser would have used with the baseline supertag-
ging model, but it provided fewer other categories.
Since the parser is not a perfect supertagger these
other categories may in fact have been incorrect,
and so supertagger accuracy goes down, without
changing parsing results. Here we have allowed
the supertagger to assign extra categories, which
will only increase its accuracy.
The increase in F-score has two sources. First,
our supertagger is more accurate, and so the parser
is more likely to receive category sets that can be
combined into the correct derivation. Also, the su-
pertagger has been trained on categories that the
parser is able to use in derivations, which means
they are more productive.
As Table 6 shows, this change translates into an
improvement of up to 0.75% in F-score on Section
Model Tag. Acc. F-score Speed
(%) (%) (sents/sec)
Baseline 96.51 85.20 39.6
GIS, 4,000k NANC 96.83 85.95 42.6
BFGS, 4,000k NANC 96.91 85.90 42.7
MIRA, 4,000k NANC 96.84 85.79 42.9
Table 6: Evaluation of top models on Section 23 of
CCGbank. All changes in F-score are statistically
significant.
23 of CCGbank. All of the new models in the table
make a statistically significant improvement over
the baseline.
It is also interesting to note that the results in
Tables 2, 4 and 6, are similar for all of the train-
ing algorithms. However, the training times differ
considerably. For all four algorithms the training
time is proportional to the amount of data, but the
GIS and BFGS models trained on only CCGbank
took 4,500 and 4,200 seconds to train, while the
equivalent perceptron and MIRA models took 90
and 95 seconds to train.
6.3 Annotation method comparison
To determine whether these improvements were
dependent on the annotations being produced
by the parser we performed a set of tests with
supertagger, rather than parser, annotated data.
Three extra training sets were created by annotat-
ing newswire sentences with supertags using the
baseline supertagging model. One set used the
one-best tagger, and two were produced using the
most probable tag for each word out of the set sup-
plied by the multi-tagger, with variations in the ?
value and dictionary cutoff for the two sets.
351
Train Corpus Ambiguity Tag. Acc. F-score Speed (sents / sec)
Wiki Bio News Wiki Bio News Wiki Bio News Wiki Bio
Baseline 1.317 1.281 96.34 94.52 90.70 85.46 80.8 75.0 39.6 50.9 35.1
News 1.331 1.322 96.53 94.86 91.32 85.84 80.1 75.2 41.8 32.6 31.4
Wiki 1.293 1.251 96.28 94.79 91.08 85.02 81.7 75.8 40.4 37.2 37.2
Bio 1.287 1.195 96.15 94.28 91.03 84.95 80.6 76.1 39.2 52.9 26.2
Table 7: Cross-corpus accuracy optimisation, models trained with GIS and 400,000 sentences.
Annotation method Tag. Acc. F-score
Baseline 96.34 85.46
Parser 96.46 85.55
One-best super 95.94 85.24
Multi-tagger a 95.91 84.98
Multi-tagger b 96.00 84.99
Table 8: Comparison of annotation methods for
extra data. The multi-taggers used ? values 0.075
and 0.001, and dictionary cutoffs 20 and 150, for
taggers a and b respectively.
Corpus Speed (sents / sec)
Sent length 5-20 21-40 41-250
News 242 44.8 8.24
Wiki 224 42.0 6.10
Bio 268 41.5 6.48
Table 9: Cross-corpus speed for the baseline
model on data sets balanced on sentence length.
As Table 8 shows, in all cases the use of
supertagger-annotated data led to poorer perfor-
mance than the baseline system, while the use of
parser-annotated data led to an improvement in F-
score. The parser has access to a range of infor-
mation that the supertagger does not, producing a
different view of the data that the supertagger can
productively learn from.
6.4 Cross-domain speed improvement
When applying parsers out of domain they are typ-
ically slower and less accurate (Gildea, 2001). In
this experiment, we attempt to increase speed on
out-of-domain data. Note that for some of the re-
sults presented here it may appear that the C&C
parser does not lose speed when out of domain,
since the Wikipedia and biomedical corpora con-
tain shorter sentences on average than the news
corpus. However, by testing on balanced sets it
is clear that speed does decrease, particularly for
longer sentences, as shown in Table 9.
For our domain adaptation development ex-
periments, we considered a collection of differ-
ent models; here we only present results for the
best set of models. For speed improvement these
were MIRA models trained on 4,000,000 parser-
annotated sentences from the target domain.
As Table 5 shows, this training method pro-
duces models adapted to the new domain. In par-
ticular, note that models trained on Wikipedia or
the biomedical data produce lower F-scores3 than
the baseline on newswire. Meanwhile, on the
target domain they are adapted to, these models
achieve a higher F-score and parse sentences at
least 45% faster than the baseline.
The changes in tagging ambiguity and accuracy
also show that adaptation has occurred. In all
cases, the new models have lower tagging ambi-
guity, and lower supertag accuracy. However, on
the corpus of the extra data, the performance of
the adapted models is comparable to the baseline
model, which means the parser is probably still be
receiving the same categories that it used from the
sets provided by the baseline system.
6.5 Cross-domain accuracy optimised
The ambiguity tuning method used to improve ac-
curacy on the newspaper domain can also be ap-
plied to the models trained on other domains. In
Table 7, we have tested models trained using GIS
and 400,000 sentences of parsed target-domain
text, with ? levels tuned to match ambiguity with
the baseline.
As for the newspaper domain, we observe in-
creased supertag accuracy and F-score. Also, in
almost every case the new models perform worse
than the baseline on domains other than the one
they were trained on.
In some cases the models in Table 7 are less ac-
curate than those in Table 5. This is because as
well as optimising the ? levels we have changed
training methods. All of the training methods were
tried, but only the method with the best results in
newswire is included here, which for F-score when
trained on 400,000 sentences was GIS.
The accuracy presented so far for the biomedi-
3Note that the F-scores for Wikipedia and biomedical text
are reported to only three significant figures as only 300 and
500 sentences respectively were available for parser evalua-
tion.
352
Train Corpus F-score
Rimell and Clark (2009) 81.5
Baseline 80.7
CCGbank + Genia 81.5
+ Newswire 81.9
+ Wikipedia 82.2
+ Biomedical 81.7
+ R&C annotated Bio 82.3
Table 10: Performance comparison for models us-
ing extra gold standard biomedical data. Models
were trained with GIS and 4,000,000 extra sen-
tences, and are tested using a POS-tagger trained
on biomedical data.
cal model is considerably lower than that reported
by Rimell and Clark (2009). This is because no
gold standard biomedical training data was used
in our experiments. Table 10 shows the results of
adding Rimell and Clark?s gold standard biomedi-
cal supertag data and using their biomedical POS-
tagger. The table also shows how accuracy can be
further improved by adding our parser-annotated
data from the biomedical domain as well as the
additional gold standard data.
7 Conclusion
This work has demonstrated that an adapted su-
pertagger can improve parsing speed and accu-
racy. The purpose of the supertagger is to re-
duce the search space for the parser. By train-
ing the supertagger on parser output, we allow the
parser to reach the derivation it would have found,
sooner. This approach also enables domain adap-
tation, improving speed and accuracy outside the
original domain of the parser.
The perceptron-based algorithms used in this
work are also able to function online, modifying
the model weights after each sentence is parsed.
This could be used to construct a system that con-
tinuously adapts to the domain it is parsing.
By training on parser-annotated NANC data
we constructed models that were adapted to the
newspaper-trained parser. The fastest model
parsed sentences 1.85 times as fast and was as
accurate as the baseline system. Adaptive train-
ing is also an effective method of improving per-
formance on other domains. Models trained on
parser-annotated Wikipedia text and MEDLINE
text had improved performance on these target do-
mains, in terms of both speed and accuracy. Op-
timising for speed or accuracy can be achieved by
modifying the ? levels used by the supertagger,
which controls the lexical category ambiguity at
each level used by the parser.
The result is an accurate and efficient wide-
coverage CCG parser that can be easily adapted for
NLP applications in new domains without manu-
ally annotating data.
Acknowledgements
We thank the reviewers for their helpful feed-
back. This work was supported by Australian Re-
search Council Discovery grants DP0665973 and
DP1097291, the Capital Markets Cooperative Re-
search Centre, and a University of Sydney Merit
Scholarship. Part of the work was completed at the
Johns Hopkins University Summer Workshop and
(partially) supported by National Science Founda-
tion Grant Number IIS-0833652.
References
Srinivas Bangalore and Aravind K. Joshi. 1999. Su-
pertagging: An approach to almost parsing. Com-
putational Linguistics, 25(2):237?265.
Phil Blunsom and Timothy Baldwin. 2006. Multi-
lingual deep lexical acquisition for HPSGs via su-
pertagging. In Proceedings of the 2006 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 164?171, Sydney, Australia.
Ted Briscoe and John Carroll. 2006. Evaluating the
accuracy of an unlexicalized statistical parser on the
PARC DepBank. In Proceedings of the Poster Ses-
sion of the 21st International Conference on Com-
putational Linguistics, Sydney, Australia.
John Chen, Srinivas Bangalore, and Vijay K. Shanker.
1999. New models for improving supertag disam-
biguation. In Proceedings of the Ninth Conference
of the European Chapter of the Association for Com-
putational Linguistics, pages 188?195, Bergen, Nor-
way.
John Chen, Srinivas Bangalore, Michael Collins, and
Owen Rambow. 2002. Reranking an n-gram su-
pertagger. In Proceedings of the 6th International
Workshop on Tree Adjoining Grammars and Related
Frameworks, pages 259?268, Venice, Italy.
Nancy Chinchor. 1995. Statistical significance
of MUC-6 results. In Proceedings of the Sixth
Message Understanding Conference, pages 39?43,
Columbia, MD, USA.
Stephen Clark and James R. Curran. 2004. The impor-
tance of supertagging for wide-coverage CCG pars-
ing. In Proceedings of the 20th International Con-
ference on Computational Linguistics, pages 282?
288, Geneva, Switzerland.
353
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG
and log-linear models. Computational Linguistics,
33(4):493?552.
Stephen Clark, James R. Curran, and Miles Osborne.
2003. Bootstrapping POS-taggers using unlabelled
data. In Proceedings of the seventh Conference on
Natural Language Learning, pages 49?55, Edmon-
ton, Canada.
Michael Collins and Terry Koo. 2002. Discriminative
reranking for natural language parsing. Computa-
tional Linguistics, 31(1):25?69.
Michael Collins. 2002. Discriminative training meth-
ods for Hidden Markov Models: Theory and experi-
ments with perceptron algorithms. In Proceedings
of the 2002 Conference on Empirical Methods in
Natural Language Processing, pages 1?8, Philadel-
phia, PA, USA.
Koby Crammer and Yoram Singer. 2003. Ultracon-
servative online algorithms for multiclass problems.
Journal of Machine Learning Research, 3:951?991.
James R. Curran. 2004. From Distributional to Seman-
tic Similarity. Ph.D. thesis, University of Edinburgh.
John N. Darroch and David Ratcliff. 1972. General-
ized iterative scaling for log-linear models. The An-
nals of Mathematical Statistics, 43(5):1470?1480.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of the 5th International Conference on
Language Resources and Evaluation, pages 449?54,
Genoa, Italy.
Susan Dumais, Michele Banko, Eric Brill, Jimmy Lin,
and Andrew Ng. 2002. Web question answering: Is
more always better? In Proceedings of the 25th In-
ternational ACMSIGIR Conference on Research and
Development, Tampere, Finland.
Daniel Gildea. 2001. Corpus variation and parser per-
formance. In Proceedings of the 2001 Conference
on Empirical Methods in Natural Language Pro-
cessing, Pittsburgh, PA, USA.
David Graff. 1995. North American News Text Cor-
pus. LDC95T21. Linguistic Data Consortium.
Philadelphia, PA, USA.
Hany Hassan, Khalil Sima?an, and Andy Way. 2007.
Supertagged phrase-based statistical machine trans-
lation. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
288?295, Prague, Czech Republic.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: A corpus of CCG derivations and dependency
structures extracted from the Penn Treebank. Com-
putational Linguistics, 33(3):355?396.
Kristy Hollingshead and Brian Roark. 2007. Pipeline
iteration. In Proceedings of the 45th Meeting of the
Association for Computational Linguistics, pages
952?959, Prague, Czech Republic.
Jin-Dong Kim, Tomoko Ohta, Yuka Teteisi, and
Jun?ichi Tsujii. 2003. GENIA corpus - a seman-
tically annotated corpus for bio-textmining. Bioin-
formatics, 19(1):180?182.
Tracy H. King, Richard Crouch, Stefan Riezler, Mary
Dalrymple, and Ronald M. Kaplan. 2003. The
PARC 700 Dependency Bank. In Proceedings of
the 4th International Workshop on Linguistically In-
terpreted Corpora, Budapest, Hungary.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In Proceedings of the Eighteenth In-
ternational Conference on Machine Learning, pages
282?289, San Francisco, CA, USA.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
David McClosky, Eugene Charniak, and Mark John-
son. 2006a. Effective self-training for parsing. In
Proceedings of the Human Language Technology
Conference of the North American Chapter of the
Association for Computational Linguistics, Brook-
lyn, NY, USA.
David McClosky, Eugene Charniak, and Mark John-
son. 2006b. Reranking and self-training for parser
adaptation. In Proceedings of the 21st International
Conference on Computational Linguistics and the
44th annual meeting of the Association for Compu-
tational Linguistics, pages 337?344, Sydney, Aus-
tralia.
Tara McIntosh and James R. Curran. 2008. Weighted
mutual exclusion bootstrapping for domain inde-
pendent lexicon and template acquisition. In Pro-
ceedings of the Australasian Language Technology
Workshop, Hobart, Australia.
Jorge Nocedal and Stephen J. Wright. 1999. Numeri-
cal Optimization. Springer.
Sampo Pyysalo, Filip Ginter, Veronika Laippala, Ka-
tri Haverinen, Juho Heimonen, and Tapio Salakoski.
2007. On the unification of syntactic annotations
under the Stanford dependency scheme: a case study
on bioinfer and GENIA. In Proceedings of the ACL
workshop on biological, translational, and clinical
language processing, pages 25?32, Prague, Czech
Republic.
Adwait Ratnaparkhi. 1996. A maximum entropy part-
of-speech tagger. In Proceedings of the 1996 Con-
ference on Empirical Methods in Natural Language
Processing, pages 133?142, Philadelphia, PA, USA.
354
Laura Rimell and Stephen Clark. 2008. Adapting a
lexicalized-grammar parser to contrasting domains.
In Proceedings of the 2008 Conference on Empiri-
cal Methods in Natural Language Processing, pages
475?484, Honolulu, HI, USA.
Laura Rimell and Stephen Clark. 2009. Port-
ing a lexicalized-grammar parser to the biomedi-
cal domain. Journal of Biomedical Informatics,
42(5):852?865.
Anoop Sarkar, Fel Xia, and Aravind K. Joshi. 2000.
Some experiments on indicators of parsing com-
plexity for lexicalized grammars. In Proceedings of
the COLING Workshop on Efficiency in Large-scale
Parsing Systems, pages 37?42, Luxembourg.
Anoop Sarkar. 2001. Applying co-training methods
to statistical parsing. In Proceedings of the Second
Meeting of the North American Chapter of the As-
sociation for Computational Linguistics, pages 1?8,
Pittsburgh, PA, USA.
Anoop Sarkar. 2007. Combining supertagging and
lexicalized tree-adjoining grammar parsing. In
Srinivas Bangalore and Aravind Joshi, editors, Com-
plexity of Lexical Descriptions and its Relevance to
Natural Language Processing: A Supertagging Ap-
proach. MIT Press, Boston, MA, USA.
Mark Steedman, Miles Osborne, Anoop Sarkar,
Stephen Clark, Rebecca Hwa, Julia Hockenmaier,
Paul Ruhlen, Stephen Baker, and Jeremiah Crim.
2003. Bootstrapping statistical parsers from small
datasets. In Proceedings of the 10th Conference of
the European Chapter of the Association for Compu-
tational Linguistics, pages 331?338, Budapest, Hun-
gary.
Geertjan van Noord. 2009. Learning efficient parsing.
In Proceedings of the 12th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics, pages 817?825. Association for Com-
putational Linguistics.
Yao-zhong Zhang, Takuya Matsuzaki, and Jun?ichi
Tsujii. 2009. HPSG supertagging: A sequence la-
beling view. In Proceedings of the 11th Interna-
tional Conference on Parsing Technologies, pages
210?213, Paris, France.
355
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 683?692,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Shift-Reduce CCG Parsing
Yue Zhang
University of Cambridge
Computer Laboratory
yue.zhang@cl.cam.ac.uk
Stephen Clark
University of Cambridge
Computer Laboratory
stephen.clark@cl.cam.ac.uk
Abstract
CCGs are directly compatible with binary-
branching bottom-up parsing algorithms, in
particular CKY and shift-reduce algorithms.
While the chart-based approach has been the
dominant approach for CCG, the shift-reduce
method has been little explored. In this paper,
we develop a shift-reduce CCG parser using
a discriminative model and beam search, and
compare its strengths and weaknesses with the
chart-based C&C parser. We study different
errors made by the two parsers, and show that
the shift-reduce parser gives competitive accu-
racies compared to C&C. Considering our use
of a small beam, and given the high ambigu-
ity levels in an automatically-extracted gram-
mar and the amount of information in the CCG
lexical categories which form the shift actions,
this is a surprising result.
1 Introduction
Combinatory Categorial Grammar (CCG; Steedman
(2000)) is a lexicalised theory of grammar which has
been successfully applied to a range of problems in
NLP, including treebank creation (Hockenmaier and
Steedman, 2007), syntactic parsing (Hockenmaier,
2003; Clark and Curran, 2007), logical form con-
struction (Bos et al, 2004) and surface realization
(White and Rajkumar, 2009). From a parsing per-
spective, the C&C parser (Clark and Curran, 2007)
has been shown to be competitive with state-of-the-
art statistical parsers on a variety of test suites, in-
cluding those consisting of grammatical relations
(Clark and Curran, 2007), Penn Treebank phrase-
structure trees (Clark and Curran, 2009), and un-
bounded dependencies (Rimell et al, 2009).
The binary branching nature of CCG means that
it is naturally compatible with bottom-up parsing al-
gorithms such as shift-reduce and CKY (Ades and
Steedman, 1982; Steedman, 2000). However, the
parsing work by Clark and Curran (2007), and also
Hockenmaier (2003) and Fowler and Penn (2010),
has only considered chart-parsing. In this paper we
fill a gap in the CCG literature by developing a shift-
reduce parser for CCG.
Shift-reduce parsers have become popular for de-
pendency parsing, building on the initial work of Ya-
mada and Matsumoto (2003) and Nivre and Scholz
(2004). One advantage of shift-reduce parsers is that
the scoring model can be defined over actions, al-
lowing highly efficient parsing by using a greedy
algorithm in which the highest scoring action (or a
small number of possible actions) is taken at each
step. In addition, high accuracy can be maintained
by using a model which utilises a rich set of features
for making each local decision (Nivre et al, 2006).
Following recent work applying global discrim-
inative models to large-scale structured prediction
problems (Collins and Roark, 2004; Miyao and
Tsujii, 2005; Clark and Curran, 2007; Finkel et
al., 2008), we build our shift-reduce parser using a
global linear model, and compare it with the chart-
based C&C parser. Using standard development
and test sets from CCGbank, our shift-reduce parser
gives a labeled F-measure of 85.53%, which is com-
petitive with the 85.45% F-measure of the C&C
parser on recovery of predicate-argument dependen-
cies from CCGbank. Hence our work shows that
683
transition-based parsing can be successfully applied
to CCG, improving on earlier attempts such as Has-
san et al (2008). Detailed analysis shows that our
shift-reduce parser yields a higher precision, lower
recall and higher F-score on most of the common
CCG dependency types compared to C&C.
One advantage of the shift-reduce parser is that
it easily handles sentences for which it is difficult
to find a spanning analysis, which can happen with
CCG because the lexical categories at the leaves of a
derivation place strong contraints on the set of possi-
ble derivations, and the supertagger which provides
the lexical categories sometimes makes mistakes.
Unlike the C&C parser, the shift-reduce parser nat-
urally produces fragmentary analyses when appro-
priate (Nivre et al, 2006), and can produce sensible
local structures even when a full spanning analysis
cannot be found.1
Finally, considering this work in the wider pars-
ing context, it provides an interesting comparison
between heuristic beam search using a rich set of
features, and optimal dynamic programming search
where the feature range is restricted. We are able to
perform this comparison because the use of the CCG
supertagger means that the C&C parser is able to
build the complete chart, from which it can find the
optimal derivation, with no pruning whatsoever at
the parsing stage. In contrast, the shift-reduce parser
uses a simple beam search with a relatively small
beam. Perhaps surprisingly, given the ambiguity lev-
els in an automatically-extracted grammar, and the
amount of information in the CCG lexical categories
which form the shift actions, the shift-reduce parser
using heuristic beam search is able to outperform the
chart-based parser.
2 CCG Parsing
CCG, and the application of CCG to wide-coverage
parsing, is described in detail elsewhere (Steedman,
2000; Hockenmaier, 2003; Clark and Curran, 2007).
Here we provide only a short description.
During CCG parsing, adjacent categories are com-
bined using CCG?s combinatory rules. For example,
a verb phrase in English (S\NP ) can combine with
1See e.g. Riezler et al (2002) and Zhang et al (2007) for chart-
based parsers which can produce fragmentary analyses.
an NP to its left using function application:
NP S\NP ? S
Categories can also combine using function
composition, allowing the combination of ?may?
((S\NP)/(S\NP)) and ?like? ((S\NP)/NP) in
coordination examples such as ?John may like but
may detest Mary?:
(S\NP)/(S\NP) (S\NP)/NP ? (S\NP)/NP
In addition to binary rules, such as function appli-
cation and composition, there are also unary rules
which operate on a single category in order to
change its type. For example, forward type-raising
can change a subject NP into a complex category
looking to the right for a verb phrase:
NP ? S/(S\NP)
An example CCG derivation is given in Section 3.
The resource used for building wide-coverage
CCG parsers of English is CCGbank (Hockenmaier
and Steedman, 2007), a version of the Penn Tree-
bank in which each phrase-structure tree has been
transformed into a normal-form CCG derivation.
There are two ways to extract a grammar from this
resource. One approach is to extract a lexicon,
i.e. a mapping from words to sets of lexical cat-
egories, and then manually define the combinatory
rule schemas, such as functional application and
composition, which combine the categories together.
The derivations in the treebank are then used to pro-
vide training data for the statistical disambiguation
model. This is the method used in the C&C parser.2
The second approach is to read the complete
grammar from the derivations, by extracting combi-
natory rule instances from the local trees consisting
of a parent category and one or two child categories,
and applying only those instances during parsing.
(These rule instances also include rules to deal with
punctuation and unary type-changing rules, in addi-
tion to instances of the combinatory rule schemas.)
This is the method used by Hockenmaier (2003) and
is the method we adopt in this paper.
Fowler and Penn (2010) demonstrate that the sec-
ond extraction method results in a context-free ap-
proximation to the grammar resulting from the first
2Although the C&C default mode applies a restriction for effi-
ciency reasons in which only rule instances seen in CCGbank
can be applied, making the grammar of the second type.
684
method, which has the potential to produce a mildly-
context sensitive grammar (given the existence of
certain combinatory rules) (Weir, 1988). However,
it is important to note that the advantages of CCG, in
particular the tight relationship between syntax and
semantic interpretation, are still maintained with the
second approach, as Fowler and Penn (2010) argue.
3 The Shift-reduce CCG Parser
Given an input sentence, our parser uses a stack of
partial derivations, a queue of incoming words, and
a series of actions?derived from the rule instances
in CCGbank?to build a derivation tree. Following
Clark and Curran (2007), we assume that each input
word has been assigned a POS-tag (from the Penn
Treebank tagset) and a set of CCG lexical categories.
We use the same maximum entropy POS-tagger and
supertagger as the C&C parser. The derivation tree
can be transformed into CCG dependencies or gram-
matical relations by a post-processing step, which
essentially runs the C&C parser deterministically
over the derivation, interpreting the derivation and
generating the required output.
The configuration of the parser, at each step of
the parsing process, is shown in part (a) of Figure 1,
where the stack holds the partial derivation trees that
have been built, and the queue contains the incoming
words that have not been processed. In the figure,
S(H) represents a category S on the stack with head
word H, while Qi represents a word in the incoming
queue.
The set of action types used by the parser is as
follows: {SHIFT, COMBINE, UNARY, FINISH}.
Each action type represents a set of possible actions
available to the parser at each step in the process.
The SHIFT-X action pushes the next incoming
word onto the stack, and assigns the lexical category
X to the word (Figure 1(b)). The label X can be any
lexical category from the set assigned to the word
being shifted by the supertagger. Hence the shift ac-
tion performs lexical category disambiguation. This
is in contrast to a shift-reduce dependency parser in
which a shift action typically just pushes a word onto
the stack.
The COMBINE-X action pops the top two nodes
off the stack, and combines them into a new node,
which is pushed back on the stack. The category of
Figure 1: The parser configuration and set of actions.
the new node is X. A COMBINE action corresponds
to a combinatory rule in the CCG grammar (or one of
the additional punctuation or type-changing rules),
which is applied to the categories of the top two
nodes on the stack.
The UNARY-X action pops the top of the stack,
transforms it into a new node with category X, and
pushes the new node onto the stack. A UNARY ac-
tion corresponds to a unary type-changing or type-
raising rule in the CCG grammar, which is applied to
the category on top of the stack.
The FINISH action terminates the parsing pro-
cess; it can be applied when all input words have
been shifted onto the stack. Note that the FINISH
action can be applied when the stack contains more
than one node, in which case the parser produces
a set of partial derivation trees, each corresponding
to a node on the stack. This sometimes happens
when a full derivation tree cannot be built due to su-
pertagging errors, and provides a graceful solution
to the problem of producing high-quality fragmen-
tary parses when necessary.
685
Figure 2: An example parsing process.
Figure 2 shows the shift-reduce parsing process
for the example sentence ?IBM bought Lotus?. First
the word ?IBM? is shifted onto the stack as an NP;
then ?bought? is shifted as a transitive verb look-
ing for its object NP on the right and subject NP on
the left ((S[dcl]\NP)/NP); and then ?Lotus? is shifted
as an NP. Then ?bought? is combined with its ob-
ject ?Lotus? resulting in a verb phrase looking for its
subject on the left (S[dcl]\NP). Finally, the resulting
verb phrase is combined with its subject, resulting in
a declarative sentence (S[dcl]).
A key difference with previous work on shift-
reduce dependency (Nivre et al, 2006) and CFG
(Sagae and Lavie, 2006b) parsing is that, for CCG,
there are many more shift actions ? a shift action for
each word-lexical category pair. Given the amount
of syntactic information in the lexical categories, the
choice of correct category, from those supplied by
the supertagger, is often a difficult one, and often
a choice best left to the parsing model. The C&C
parser solves this problem by building the complete
packed chart consistent with the lexical categories
supplied by the supertagger, leaving the selection of
the lexical categories to the Viterbi algorithm. For
the shift-reduce parser the choice is also left to the
parsing model, but in contrast to C&C the correct
lexical category could be lost at any point in the
heuristic search process. Hence it is perhaps sur-
prising that we are able to achieve a high parsing ac-
curacy of 85.5%, given a relatively small beam size.
4 Decoding
Greedy local search (Yamada and Matsumoto, 2003;
Sagae and Lavie, 2005; Nivre and Scholz, 2004)
has typically been used for decoding in shift-reduce
parsers, while beam-search has recently been ap-
plied as an alternative to reduce error-propagation
(Johansson and Nugues, 2007; Zhang and Clark,
2008; Zhang and Clark, 2009; Huang et al, 2009).
Both greedy local search and beam-search have lin-
ear time complexity. We use beam-search in our
CCG parser.
To formulate the decoding algorithm, we define a
candidate item as a tuple ?S,Q,F ?, where S repre-
sents the stack with partial derivations that have been
built, Q represents the queue of incoming words that
have not been processed, and F is a boolean value
that represents whether the candidate item has been
finished. A candidate item is finished if and only if
the FINISH action has been applied to it, and no
more actions can be applied to a candidate item af-
ter it reaches the finished status. Given an input sen-
tence, we define the start item as the unfinished item
with an empty stack and the whole input sentence as
the incoming words. A derivation is built from the
start item by repeated applications of actions until
the item is finished.
To apply beam-search, an agenda is used to hold
the N -best partial (unfinished) candidate items at
each parsing step. A separate candidate output is
686
function DECODE(input, agenda, list, N ,
grammar, candidate output):
agenda.clear()
agenda.insert(GETSTARTITEM(input))
candidate output = NONE
while not agenda.empty():
list.clear()
for item in agenda:
for action in grammar.getActions(item):
item? = item.apply(action)
if item?.F == TRUE:
if candidate output == NONE or
item?.score > candidate output.score:
candidate output = item?
else:
list.append(item?)
agenda.clear()
agenda.insert(list.best(N ))
Figure 3: The decoding algorithm; N is the agenda size
used to record the current best finished item that has
been found, since candidate items can be finished at
different steps. Initially the agenda contains only the
start item, and the candidate output is set to none. At
each step during parsing, each candidate item from
the agenda is extended in all possible ways by apply-
ing one action according to the grammar, and a num-
ber of new candidate items are generated. If a newly
generated candidate is finished, it is compared with
the current candidate output. If the candidate output
is none or the score of the newly generated candi-
date is higher than the score of the candidate output,
the candidate output is replaced with the newly gen-
erated item; otherwise the newly generated item is
discarded. If the newly generated candidate is un-
finished, it is appended to a list of newly generated
partial candidates. After all candidate items from the
agenda have been processed, the agenda is cleared
and the N -best items from the list are put on the
agenda. Then the list is cleared and the parser moves
on to the next step. This process repeats until the
agenda is empty (which means that no new items
have been generated in the previous step), and the
candidate output is the final derivation. Pseudocode
for the algorithm is shown in Figure 3.
feature templates
1 S0wp, S0c, S0pc, S0wc,
S1wp, S1c, S1pc, S1wc,
S2pc, S2wc,
S3pc, S3wc,
2 Q0wp, Q1wp, Q2wp, Q3wp,
3 S0Lpc, S0Lwc, S0Rpc, S0Rwc,
S0Upc, S0Uwc,
S1Lpc, S1Lwc, S1Rpc, S1Rwc,
S1Upc, S1Uwc,
4 S0wcS1wc, S0cS1w, S0wS1c, S0cS1c,
S0wcQ0wp, S0cQ0wp, S0wcQ0p, S0cQ0p,
S1wcQ0wp, S1cQ0wp, S1wcQ0p, S1cQ0p,
5 S0wcS1cQ0p, S0cS1wcQ0p, S0cS1cQ0wp,
S0cS1cQ0p, S0pS1pQ0p,
S0wcQ0pQ1p, S0cQ0wpQ1p, S0cQ0pQ1wp,
S0cQ0pQ1p, S0pQ0pQ1p,
S0wcS1cS2c, S0cS1wcS2c, S0cS1cS2wc,
S0cS1cS2c, S0pS1pS2p,
6 S0cS0HcS0Lc, S0cS0HcS0Rc,
S1cS1HcS1Rc,
S0cS0RcQ0p, S0cS0RcQ0w,
S0cS0LcS1c, S0cS0LcS1w,
S0cS1cS1Rc, S0wS1cS1Rc.
Table 1: Feature templates.
5 Model and Training
We use a global linear model to score candidate
items, trained discriminatively with the averaged
perceptron (Collins, 2002). Features for a (finished
or partial) candidate are extracted from each ac-
tion that have been applied to build the candidate.
Following Collins and Roark (2004), we apply the
?early update? strategy to perceptron training: at any
step during decoding, if neither the candidate out-
put nor any item in the agenda is correct, decoding
is stopped and the parameters are updated using the
current highest scored item in the agenda or the can-
didate output, whichever has the higher score.
Table 1 shows the feature templates used by the
parser. The symbols S0, S1, S2 and S3 in the ta-
ble represent the top four nodes on the stack (if ex-
istent), and Q0, Q1, Q2 and Q3 represent the front
four words in the incoming queue (if existent). S0H
and S1H represent the subnodes of S0 and S1 that
have the lexical head of S0 and S1, respectively. S0L
represents the left subnode of S0, when the lexical
head is from the right subnode. S0R and S1R rep-
resent the right subnode of S0 and S1, respectively,
687
when the lexical head is from the left subnode. If S0
is built by a UNARY action, S0U represents the only
subnode of S0. The symbols w, p and c represent the
word, the POS, and the CCG category, respectively.
These rich feature templates produce a large num-
ber of features: 36 million after the first training it-
eration, compared to around 0.5 million in the C&C
parser.
6 Experiments
Our experiments were performed using CCGBank
(Hockenmaier and Steedman, 2007), which was
split into three subsets for training (Sections 02?21),
development testing (Section 00) and the final test
(Section 23). Extracted from the training data, the
CCG grammar used by our parser consists of 3070
binary rule instances and 191 unary rule instances.
We compute F-scores over labeled CCG depen-
dencies and also lexical category accuracy. CCG de-
pendencies are defined in terms of lexical categories,
by numbering each argument slot in a complex cat-
egory. For example, the first NP in a transitive verb
category is a CCG dependency relation, correspond-
ing to the subject of the verb. Clark and Curran
(2007) gives a more precise definition. We use the
generate script from the C&C tools3 to transform
derivations into CCG dependencies.
There is a mismatch between the grammar that
generate uses, which is the same grammar as the
C&C parser, and the grammar we extract from CCG-
bank, which contains more rule instances. Hence
generate is unable to produce dependencies for
some of the derivations our shift-reduce parser pro-
duces. In order to allow generate to process all
derivations from the shift-reduce parser, we repeat-
edly removed rules that the generate script can-
not handle from our grammar, until all derivations
in the development data could be dealt with. In
fact, this procedure potentially reduces the accuracy
of the shift-reduce parser, but the effect is compar-
atively small because only about 4% of the devel-
opment and test sentences contain rules that are not
handled by the generate script.
All experiments were performed using automati-
3Available at http://svn.ask.it.usyd.edu.au/trac/candc/wiki; we
used the generate and evaluate scripts, as well as the
C&C parser, for evaluation and comparison.
cally assigned POS-tags, with 10-fold cross valida-
tion used to assign POS-tags and lexical categories
to the training data. At the supertagging stage, mul-
tiple lexical categories are assigned to each word in
the input. For each word, the supertagger assigns all
lexical categories whose forward-backward proba-
bility is above ? ? max, where max is the highest
lexical category probability for the word, and ? is a
threshold parameter. To give the parser a reasonable
freedom in lexical category disambiguation, we used
a small ? value of 0.0001, which results in 3.6 lexi-
cal categories being assigned to each word on aver-
age in the training data. For training, but not testing,
we also added the correct lexical category to the list
of lexical categories for a word in cases when it was
not provided by the supertagger.
Increasing the size of the beam in the parser beam
search leads to higher accuracies but slower running
time. In our development experiments, the accu-
racy improvement became small when the beam size
reached 16, and so we set the size of the beam to 16
for the remainder of the experiments.
6.1 Development test accuracies
Table 2 shows the labeled precision (lp), recall (lr),
F-score (lf), sentence-level accuracy (lsent) and lex-
ical category accuracy (cats) of our parser and the
C&C parser on the development data. We ran the
C&C parser using the normal-form model (we re-
produced the numbers reported in Clark and Cur-
ran (2007)), and copied the results of the hybrid
model from Clark and Curran (2007), since the hy-
brid model is not part of the public release.
The accuracy of our parser is much better when
evaluated on all sentences, partly because C&C
failed on 0.94% of the data due to the failure to pro-
duce a spanning analysis. Our shift-reduce parser
does not suffer from this problem because it pro-
duces fragmentary analyses for those cases. When
evaluated on only those sentences that C&C could
analyze, our parser gave 0.29% higher F-score. Our
shift-reduce parser also gave higher accuracies on
lexical category assignment. The sentence accuracy
of our shift-reduce parser is also higher than C&C,
which confirms that our shift-reduce parser produces
reasonable sentence-level analyses, despite the pos-
sibility for fragmentary analysis.
688
lp. lr. lf. lsent. cats. evaluated on
shift-reduce 87.15% 82.95% 85.00% 33.82% 92.77% all sentences
C&C (normal-form) 85.22% 82.52% 83.85% 31.63% 92.40% all sentences
shift-reduce 87.55% 83.63% 85.54% 34.14% 93.11% 99.06% (C&C coverage)
C&C (hybrid) ? ? 85.25% ? ? 99.06% (C&C coverage)
C&C (normal-form) 85.22% 84.29% 84.76% 31.93% 92.83% 99.06% (C&C coverage)
Table 2: Accuracies on the development test data.
 60
 65
 70
 75
 80
 85
 90
 0  5  10  15  20  25  30
pr
ec
isi
on
 %
dependency length (bins of 5)
Precision comparison by dependency length
this paper
C&C
 50
 55
 60
 65
 70
 75
 80
 85
 90
 0  5  10  15  20  25  30
re
ca
ll 
%
dependency length (bins of 5)
Recall comparison by dependency length
this paper
C&C
Figure 4: P & R scores relative to dependency length.
6.2 Error comparison with C&C parser
Our shift-reduce parser and the chart-based C&C
parser offer two different solutions to the CCG pars-
ing problem. The comparison reported in this sec-
tion is similar to the comparison between the chart-
based MSTParser (McDonald et al, 2005) and shift-
reduce MaltParser (Nivre et al, 2006) for depen-
dency parsing. We follow McDonald and Nivre
(2007) and characterize the errors of the two parsers
by sentence and dependency length and dependency
type.
We measured precision, recall and F-score rel-
ative to different sentence lengths. Both parsers
performed better on shorter sentences, as expected.
Our shift-reduce parser performed consistently bet-
ter than C&C on all sentence lengths, and there
was no significant difference in the rate of perfor-
mance degradation between the parsers as the sen-
tence length increased.
Figure 4 shows the comparison of labeled preci-
sion and recall relative to the dependency length (i.e.
the number of words between the head and depen-
dent), in bins of size 5 (e.g. the point at x=5 shows
the precision or recall for dependency lengths 1 ? 5).
This experiment was performed using the normal-
form version of the C&C parser, and the evaluation
was on the sentences for which C&C gave an anal-
ysis. The number of dependencies drops when the
dependency length increases; there are 141, 180 and
124 dependencies from the gold-standard, C&C out-
put and our shift-reduce parser output, respectively,
when the dependency length is between 21 and 25,
inclusive. The numbers drop to 47, 56 and 36 when
the dependency length is between 26 and 30. The
recall of our parser drops more quickly as the de-
pendency length grows beyond 15. A likely reason
is that the recovery of longer-range dependencies re-
quires more processing steps, increasing the chance
of the correct structure being thrown off the beam.
In contrast, the precision did not drop more quickly
than C&C, and in fact is consistently higher than
C&C across all dependency lengths, which reflects
the fact that the long range dependencies our parser
managed to recover are comparatively reliable.
Table 3 shows the comparison of labeled precision
(lp), recall (lr) and F-score (lf) for the most common
CCG dependency types. The numbers for C&C are
for the hybrid model, copied from Clark and Curran
(2007). While our shift-reduce parser gave higher
precision for almost all categories, it gave higher re-
call on only half of them, but higher F-scores for all
but one dependency type.
6.3 Final results
Table 4 shows the accuracies on the test data. The
numbers for the normal-form model are evaluated
by running the publicly available parser, while those
for the hybrid dependency model are from Clark
and Curran (2007). Evaluated on all sentences, the
accuracies of our parser are much higher than the
C&C parser, since the C&C parser failed to produce
any output for 10 sentences. When evaluating both
689
category arg lp. (o) lp. (C) lr. (o) lr. (C) lf. (o) lf. (C) freq.
N/N 1 95.77% 95.28% 95.79% 95.62% 95.78% 95.45% 7288
NP/N 1 96.70% 96.57% 96.59% 96.03% 96.65% 96.30% 4101
(NP\NP)/NP 2 83.19% 82.17% 89.24% 88.90% 86.11% 85.40% 2379
(NP\NP)/NP 1 82.53% 81.58% 87.99% 85.74% 85.17% 83.61% 2174
((S\NP)\(S\NP))/NP 3 77.60% 71.94% 71.58% 73.32% 74.47% 72.63% 1147
((S\NP)\(S\NP))/NP 2 76.30% 70.92% 70.60% 71.93% 73.34% 71.42% 1058
((S[dcl]\NP)/NP 2 85.60% 81.57% 84.30% 86.37% 84.95% 83.90% 917
PP/NP 1 73.76% 75.06% 72.83% 70.09% 73.29% 72.49% 876
((S[dcl]\NP)/NP 1 85.32% 81.62% 82.00% 85.55% 83.63% 83.54% 872
((S\NP)\(S\NP)) 2 84.44% 86.85% 86.60% 86.73% 85.51% 86.79% 746
Table 3: Accuracy comparison on the most common CCG dependency types. (o) ? our parser; (C) ? C&C (hybrid)
lp. lr. lf. lsent. cats. evaluated
shift-reduce 87.43% 83.61% 85.48% 35.19% 93.12% all sentences
C&C (normal-form) 85.58% 82.85% 84.20% 32.90% 92.84% all sentences
shift-reduce 87.43% 83.71% 85.53% 35.34% 93.15% 99.58% (C&C coverage)
C&C (hybrid) 86.17% 84.74% 85.45% 32.92% 92.98% 99.58% (C&C coverage)
C&C (normal-form) 85.48% 84.60% 85.04% 33.08% 92.86% 99.58% (C&C coverage)
F&P (Petrov I-5)* 86.29% 85.73% 86.01% ? ? ? (F&P? C&C coverage; 96.65% on dev. test)
C&C hybrid* 86.46% 85.11% 85.78% ? ? ? (F&P? C&C coverage; 96.65% on dev. test)
Table 4: Comparison with C&C; final test. * ? not directly comparable.
parsers on the sentences for which C&C produces an
analysis, our parser still gave the highest accuracies.
The shift-reduce parser gave higher precision, and
lower recall, than C&C; it also gave higher sentence-
level and lexical category accuracy.
The last two rows in the table show the accuracies
of Fowler and Penn (2010) (F&P), who applied the
CFG parser of Petrov and Klein (2007) to CCG, and
the corresponding accuracies for the C&C parser on
the same test sentences. F&P can be treated as an-
other chart-based parser; their evaluation is based
on the sentences for which both their parser and
C&C produced dependencies (or more specifically
those sentences for which generate could pro-
duce dependencies), and is not directly comparable
with ours, especially considering that their test set is
smaller and potentially slightly easier.
The final comparison is parser speed. The shift-
reduce parser is linear-time (in both sentence length
and beam size), and can analyse over 10 sentences
per second on a 2GHz CPU, with a beam of 16,
which compares very well with other constituency
parsers. However, this is no faster than the chart-
based C&C parser, although speed comparisons
are difficult because of implementation differences
(C&C uses heavily engineered C++ with a focus on
efficiency).
7 Related Work
Sagae and Lavie (2006a) describes a shift-reduce
parser for the Penn Treebank parsing task which
uses best-first search to allow some ambiguity into
the parsing process. Differences with our approach
are that we use a beam, rather than best-first, search;
we use a global model rather than local models
chained together; and finally, our results surpass
the best published results on the CCG parsing task,
whereas Sagae and Lavie (2006a) matched the best
PTB results only by using a parser combination.
Matsuzaki et al (2007) describes similar work
to ours but using an automatically-extracted HPSG,
rather than CCG, grammar. They also use the gen-
eralised perceptron to train a disambiguation model.
One difference is that Matsuzaki et al (2007) use an
approximating CFG, in addition to the supertagger,
to improve the efficiency of the parser.
690
Ninomiya et al (2009) (and Ninomiya et al
(2010)) describe a greedy shift-reduce parser for
HPSG, in which a single action is chosen at each
parsing step, allowing the possibility of highly ef-
ficient parsing. Since the HPSG grammar has rela-
tively tight constraints, similar to CCG, the possibil-
ity arises that a spanning analysis cannot be found
for some sentences. Our approach to this problem
was to allow the parser to return a fragmentary anal-
ysis; Ninomiya et al (2009) adopt a different ap-
proach based on default unification.
Finally, our work is similar to the comparison of
the chart-based MSTParser (McDonald et al, 2005)
and shift-reduce MaltParser (Nivre et al, 2006) for
dependency parsing. MSTParser can perform ex-
haustive search, given certain feature restrictions,
because the complexity of the parsing task is lower
than for constituent parsing. C&C can perform ex-
haustive search because the supertagger has already
reduced the search space. We also found that ap-
proximate heuristic search for shift-reduce parsing,
utilising a rich feature space, can match the perfor-
mance of the optimal chart-based parser, as well as
similar error profiles for the two CCG parsers com-
pared to the two dependency parsers.
8 Conclusion
This is the first work to present competitive results
for CCG using a transition-based parser, filling a gap
in the CCG parsing literature. Considered in terms
of the wider parsing problem, we have shown that
state-of-the-art parsing results can be obtained using
a global discriminative model, one of the few pa-
pers to do so without using a generative baseline as a
feature. The comparison with C&C also allowed us
to compare a shift-reduce parser based on heuristic
beam search utilising a rich feature set with an opti-
mal chart-based parser whose features are restricted
by dynamic programming, with favourable results
for the shift-reduce parser.
The complementary errors made by the chart-
based and shift-reduce parsers opens the possibil-
ity of effective parser combination, following sim-
ilar work for dependency parsing.
The parser code can be downloaded at
http://www.sourceforge.net/projects/zpar,
version 0.5.
Acknowledgements
We thank the anonymous reviewers for their sugges-
tions. Yue Zhang and Stephen Clark are supported
by the European Union Seventh Framework Pro-
gramme (FP7-ICT-2009-4) under grant agreement
no. 247762.
References
A. E. Ades and M. Steedman. 1982. On the order of
words. Linguistics and Philosophy, pages 517 ? 558.
Johan Bos, Stephen Clark, Mark Steedman, James R.
Curran, and Julia Hockenmaier. 2004. Wide-coverage
semantic representations from a CCG parser. In Pro-
ceedings of COLING-04, pages 1240?1246, Geneva,
Switzerland.
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG and
log-linear models. Computational Linguistics,
33(4):493?552.
Stephen Clark and James R. Curran. 2009. Comparing
the accuracy of CCG and Penn Treebank parsers. In
Proceedings of ACL-2009 (short papers), pages 53?
56, Singapore.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceedings
of ACL, pages 111?118, Barcelona, Spain.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
EMNLP, pages 1?8, Philadelphia, USA.
Jenny Rose Finkel, Alex Kleeman, and Christopher D.
Manning. 2008. Feature-based, conditional random
field parsing. In Proceedings of the 46th Meeting of
the ACL, pages 959?967, Columbus, Ohio.
Timothy A. D. Fowler and Gerald Penn. 2010. Ac-
curate context-free parsing with Combinatory Catego-
rial Grammar. In Proceedings of ACL-2010, Uppsala,
Sweden.
H. Hassan, K. Sima?an, and A. Way. 2008. A syntactic
language model based on incremental CCG parsing.
In Proceedings of the Second IEEE Spoken Language
Technology Workshop, Goa, India.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: A corpus of CCG derivations and dependency
structures extracted from the Penn Treebank. Compu-
tational Linguistics, 33(3):355?396.
Julia Hockenmaier. 2003. Data and Models for Statis-
tical Parsing with Combinatory Categorial Grammar.
Ph.D. thesis, University of Edinburgh.
Liang Huang, Wenbin Jiang, and Qun Liu. 2009.
Bilingually-constrained (monolingual) shift-reduce
691
parsing. In Proceedings of the 2009 EMNLP Confer-
ence, pages 1222?1231, Singapore.
Richard Johansson and Pierre Nugues. 2007. Incre-
mental dependency parsing using online learning. In
Proceedings of the CoNLL/EMNLP Conference, pages
1134?1138, Prague, Czech Republic.
Takuya Matsuzaki, Yusuke Miyao, and Jun ichi Tsu-
jii. 2007. Efficient HPSG parsing with supertagging
and CFG-filtering. In Proceedings of IJCAI-07, pages
1671?1676, Hyderabad, India.
Ryan McDonald and Joakim Nivre. 2007. Characteriz-
ing the errors of data-driven dependency parsing mod-
els. In Proceedings of EMNLP/CoNLL, pages 122?
131, Prague, Czech Republic.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proceedings of the 43rd Meeting of the
ACL, pages 91?98, Michigan, Ann Arbor.
Yusuke Miyao and Jun?ichi Tsujii. 2005. Probabilistic
disambiguation models for wide-coverage HPSG pars-
ing. In Proceedings of the 43rd meeting of the ACL,
pages 83?90, University of Michigan, Ann Arbor.
Takashi Ninomiya, Takuya Matsuzaki, Nobuyuki
Shimizu, and Hiroshi Nakagawa. 2009. Deterministic
shift-reduce parsing for unification-based grammars
by using default unification. In Proceedings of
EACL-09, pages 603?611, Athens, Greece.
Takashi Ninomiya, Takuya Matsuzaki, Nobuyuki
Shimizu, and Hiroshi Nakagawa. 2010. Deter-
ministic shift-reduce parsing for unification-based
grammars. Journal of Natural Language Engineering,
DOI:10.1017/S1351324910000240.
J. Nivre and M. Scholz. 2004. Deterministic dependency
parsing of English text. In Proceedings of COLING-
04, pages 64?70, Geneva, Switzerland.
Joakim Nivre, Johan Hall, Jens Nilsson, Gu?ls?en Eryig?it,
and Svetoslav Marinov. 2006. Labeled pseudo-
projective dependency parsing with support vector ma-
chines. In Proceedings of CoNLL, pages 221?225,
New York, USA.
Slav Petrov and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In Proceedings of
HLT/NAACL, pages 404?411, Rochester, New York,
April.
Stefan Riezler, Tracy H. King, Ronald M. Kaplan,
Richard Crouch, John T. Maxwell III, and Mark John-
son. 2002. Parsing the Wall Street Journal using a
Lexical-Functional Grammar and discriminative esti-
mation techniques. In Proceedings of the 40th Meet-
ing of the ACL, pages 271?278, Philadelphia, PA.
Laura Rimell, Stephen Clark, and Mark Steedman. 2009.
Unbounded dependency recovery for parser evalua-
tion. In Proceedings of EMNLP-09, pages 813?821,
Singapore.
Kenji Sagae and Alon Lavie. 2005. A classifier-based
parser with linear run-time complexity. In Proceed-
ings of IWPT, pages 125?132, Vancouver, Canada.
Kenji Sagae and Alon Lavie. 2006a. A best-first
probabilistic shift-reduce parser. In Proceedings of
COLING/ACL poster session, pages 691?698, Sydney,
Australia, July.
Kenji Sagae and Alon Lavie. 2006b. Parser combination
by reparsing. In Proceedings of HLT/NAACL, Com-
panion Volume: Short Papers, pages 129?132, New
York, USA.
Mark Steedman. 2000. The Syntactic Process. The MIT
Press, Cambridge, Mass.
David Weir. 1988. Characterizing Mildly Context-
Sensitive Grammar Formalisms. Ph.D. thesis, Univer-
sity of Pennsylviania.
Michael White and Rajakrishnan Rajkumar. 2009. Per-
ceptron reranking for CCG realization. In Proceedings
of the 2009 Conference on Empirical Methods in Nat-
ural Language Processing, pages 410?419, Singapore.
H Yamada and Y Matsumoto. 2003. Statistical depen-
dency analysis using support vector machines. In Pro-
ceedings of IWPT, Nancy, France.
Yue Zhang and Stephen Clark. 2008. A tale of
two parsers: investigating and combining graph-based
and transition-based dependency parsing using beam-
search. In Proceedings of EMNLP-08, Hawaii, USA.
Yue Zhang and Stephen Clark. 2009. Transition-based
parsing of the Chinese Treebank using a global dis-
criminative model. In Proceedings of IWPT, Paris,
France, October.
Yi Zhang, Valia Kordoni, and Erin Fitzgerald. 2007. Par-
tial parse selection for robust deep processing. In Pro-
ceedings of the ACL 2007 Workshop on Deep Linguis-
tic Processing, Prague, Czech Republic.
692
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 47?52,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Semantic Parsing as Machine Translation
Jacob Andreas
Computer Laboratory
University of Cambridge
jda33@cam.ac.uk
Andreas Vlachos
Computer Laboratory
University of Cambridge
av308@cam.ac.uk
Stephen Clark
Computer Laboratory
University of Cambridge
sc609@cam.ac.uk
Abstract
Semantic parsing is the problem of de-
riving a structured meaning representation
from a natural language utterance. Here
we approach it as a straightforward ma-
chine translation task, and demonstrate
that standard machine translation com-
ponents can be adapted into a semantic
parser. In experiments on the multilingual
GeoQuery corpus we find that our parser
is competitive with the state of the art,
and in some cases achieves higher accu-
racy than recently proposed purpose-built
systems. These results support the use of
machine translation methods as an infor-
mative baseline in semantic parsing evalu-
ations, and suggest that research in seman-
tic parsing could benefit from advances in
machine translation.
1 Introduction
Semantic parsing (SP) is the problem of trans-
forming a natural language (NL) utterance into
a machine-interpretable meaning representation
(MR). It is well-studied in NLP, and a wide va-
riety of methods have been proposed to tackle
it, e.g. rule-based (Popescu et al, 2003), super-
vised (Zelle, 1995), unsupervised (Goldwasser et
al., 2011), and response-based (Liang et al, 2011).
At least superficially, SP is simply a machine
translation (MT) task: we transform an NL ut-
terance in one language into a statement of an-
other (un-natural) meaning representation lan-
guage (MRL). Indeed, successful semantic parsers
often resemble MT systems in several impor-
tant respects, including the use of word align-
ment models as a starting point for rule extrac-
tion (Wong and Mooney, 2006; Kwiatkowski et
al., 2010) and the use of automata such as tree
transducers (Jones et al, 2012) to encode the re-
lationship between NL and MRL.
The key difference between the two tasks is that
in SP, the target language (the MRL) has very dif-
ferent properties to an NL. In particular, MRs must
conform strictly to a particular structure so that
they are machine-interpretable. Contrast this with
ordinary MT, where varying degrees of wrongness
are tolerated by human readers (and evaluation
metrics). To avoid producing malformed MRs, al-
most all of the existing research on SP has focused
on developing models with richer structure than
those commonly used for MT.
In this work we attempt to determine how ac-
curate a semantic parser we can build by treating
SP as a pure MT task, and describe pre- and post-
processing steps which allow structure to be pre-
served in the MT process.
Our contributions are as follows: We develop
a semantic parser using off-the-shelf MT compo-
nents, exploring phrase-based as well as hierarchi-
cal models. Experiments with four languages on
the popular GeoQuery corpus (Zelle, 1995) show
that our parser is competitve with the state-of-
the-art, in some cases achieving higher accuracy
than recently introduced purpose-built semantic
parsers. Our approach also appears to require
substantially less time to train than the two best-
performing semantic parsers. These results sup-
port the use of MT methods as an informative
baseline in SP evaluations and show that research
in SP could benefit from research advances in MT.
2 MT-based semantic parsing
The input is a corpus of NL utterances paired with
MRs. In order to learn a semantic parser using
MT we linearize the MRs, learn alignments be-
tween the MRL and the NL, extract translation
rules, and learn a language model for the MRL.
We also specify a decoding procedure that will re-
turn structured MRs for an utterance during pre-
diction.
47
states bordering Texas
state(next to(state(stateid(texas))))
? STEM & LINEARIZE
state border texa
state1 next to1 state1 stateid1 texas0
? ALIGN
state border texa
state1 next to1 state1 stateid1 texas0
? EXTRACT (PHRASE)
? state , state1 ?
? state border , state1 border1 ?
? texa , state1 stateid1 texas0 ?...
? EXTRACT (HIER)
[X] ? ?state , state1?
[X] ? ?state [X] texa ,
state1 [X] state1 stateid1 texas0?...
Figure 1: Illustration of preprocessing and rule ex-
traction.
Linearization We assume that the MRL is
variable-free (that is, the meaning representation
for each utterance is tree-shaped), noting that for-
malisms with variables, like the ?-calculus, can
be mapped onto variable-free logical forms with
combinatory logics (Curry et al, 1980).
In order to learn a semantic parser using MT
we begin by converting these MRs to a form more
similar to NL. To do so, we simply take a preorder
traversal of every functional form, and label every
function with the number of arguments it takes.
After translation, recovery of the function is easy:
if the arity of every function in the MRL is known,
then every traversal uniquely specifies its corre-
sponding tree. Using an example from GeoQuery,
given an input function of the form
answer(population(city(cityid(?seattle?, ?wa?))))
we produce a ?decorated? translation input of the
form
answer1 population1 city1 cityid2 seattle0 wa0
where each subscript indicates the symbol?s arity
(constants, including strings, are treated as zero-
argument functions). Explicit argument number
labeling serves two functions. Most importantly,
it eliminates any possible ambiguity from the tree
reconstruction which takes place during decod-
ing: given any sequence of decorated MRL to-
kens, we can always reconstruct the correspond-
ing tree structure (if one exists). Arity labeling ad-
ditionally allows functions with variable numbers
of arguments (e.g. cityid, which in some training
examples is unary) to align with different natural
language strings depending on context.
Alignment Following the linearization of the
MRs, we find alignments between the MR tokens
and the NL tokens using the IBM Model 4 (Brown
et al, 1993). Once the alignment algorithm is
run in both directions (NL to MRL, MRL to NL),
we symmetrize the resulting alignments to obtain
a consensus many-to-many alignment (Och and
Ney, 2000; Koehn et al, 2005).
Rule extraction From the many-to-many align-
ment we need to extract a translation rule ta-
ble, consisting of corresponding phrases in NL
and MRL. We consider a phrase-based transla-
tion model (Koehn et al, 2003) and a hierarchi-
cal translation model (Chiang, 2005). Rules for
the phrase-based model consist of pairs of aligned
source and target sequences, while hierarchical
rules are SCFG productions containing at most
two instances of a single nonterminal symbol.
Note that both extraction algorithms can learn
rules which a traditional tree-transducer-based ap-
proach cannot?for example the right hand side
[X] river1 all0 traverse1 [X]
corresponding to the pair of disconnected tree
fragments:
[X]

traverse

river

[X]
all
(where each X indicates a gap in the rule).
Language modeling In addition to translation
rules learned from a parallel corpus, MT systems
also rely on an n-gram language model for the tar-
get language, estimated from a (typically larger)
monolingual corpus. In the case of SP, such a
monolingual corpus is rarely available, and we in-
stead use the MRs available in the training data to
learn a language model of the MRL. This informa-
tion helps guide the decoder towards well-formed
48
structures; it encodes, for example, the preferences
of predicates of the MRL for certain arguments.
Prediction Given a new NL utterance, we need
to find the n best translations (i.e. sequences
of decorated MRL tokens) that maximize the
weighted sum of the translation score (the prob-
abilities of the translations according to the rule
translation table) and the language model score, a
process usually referred to as decoding. Standard
decoding procedures for MT produce an n-best list
of all possible translations, but here we need to
restrict ourselves to translations corresponding to
well-formed MRs. In principle this could be done
by re-writing the beam search algorithm used in
decoding to immediately discard malformed MRs;
for the experiments in this paper we simply filter
the regular n-best list until we find a well-formed
MR. This filtering can be done with time linear in
the length of the example by exploiting the argu-
ment label numbers introduced during lineariza-
tion. Finally, we insert the brackets according to
the tree structure specified by the argument num-
ber labels.
3 Experimental setup
Dataset We conduct experiments on the Geo-
Query data set. The corpus consists of a set of
880 natural-language questions about U.S. geog-
raphy in four languages (English, German, Greek
and Thai), and their representations in a variable-
free MRL that can be executed against a Prolog
database interface. Initial experimentation was
done using 10 fold cross-validation on the 600-
sentence development set and the final evaluation
on a held-out test set of 280 sentences. All seman-
tic parsers for GeoQuery we compare against also
makes use of NP lists (Jones et al, 2012), which
contain MRs for every noun phrase that appears in
the NL utterances of each language. In our exper-
iments, the NP list was included by appending all
entries as extra training sentences to the end of the
training corpus of each language with 50 times the
weight of regular training examples, to ensure that
they are learned as translation rules.
Evaluation for each utterance is performed by
executing both the predicted and the gold standard
MRs against the database and obtaining their re-
spective answers. An MR is correct if it obtains
the same answer as the gold standard MR, allow-
ing for a fair comparison between systems using
different learning paradigms. Following Jones et
al. (2012) we report accuracy, i.e. the percent-
age of NL questions with correct answers, and F1,
i.e. the harmonic mean of precision (percentage of
correct answers obtained).
Implementation In all experiments, we use the
IBM Model 4 implementation from the GIZA++
toolkit (Och and Ney, 2000) for alignment, and
the phrase-based and hierarchical models imple-
mented in the Moses toolkit (Koehn et al, 2007)
for rule extraction. The best symmetrization algo-
rithm, translation and language model weights for
each language are selected using cross-validation
on the development set. In the case of English and
German, we also found that stemming (Bird et al,
2009; Porter, 1980) was hepful in reducing data
sparsity.
4 Results
We first compare the results for the two translation
rule extraction models, phrase-based and hierar-
chical (?MT-phrase? and ?MT-hier? respectively
in Table 1). We find that the hierarchical model
performs better in all languages apart from Greek,
indicating that the long-range reorderings learned
by a hierarchical translation system are useful for
this task. These benefits are most pronounced in
the case of Thai, likely due to the the language?s
comparatively different word order.
We also present results for both models with-
out using the NP lists for training in Table 2. As
expected, the performances are almost uniformly
lower, but the parser still produces correct output
for the majority of examples.
As discussed above, one important modifica-
tion of the MT paradigm which allows us to pro-
duce structured output is the addition of structure-
checking to the beam search. It is not evident,
a priori, that this search procedure is guaran-
teed to find any well-formed outputs in reasonable
time; to test the effect of this extra requirement on
en de el th
MT-phrase 75.3 68.8 70.4 53.0
MT-phrase (-NP) 63.4 65.8 64.0 39.8
MT-hier 80.5 68.9 69.1 70.4
MT-hier (-NP) 62.5 69.9 62.9 62.1
Table 2: GeoQuery accuracies with and without
NPs. Rows with (-NP) did not use the NP list.
49
English [en] German [de] Greek [el] Thai [th]
Acc. F1 Acc. F1 Acc. F1 Acc. F1
WASP 71.1 77.7 65.7 74.9 70.7 78.6 71.4 75.0
UBL 82.1 82.1 75.0 75.0 73.6 73.7 66.4 66.4
tsVB 79.3 79.3 74.6 74.6 75.4 75.4 78.2 78.2
hybrid-tree 76.8 81.0 62.1 68.5 69.3 74.6 73.6 76.7
MT-phrase 75.3 75.8 68.8 70.8 70.4 73.0 53.0 54.4
MT-hier 80.5 81.8 68.9 71.8 69.1 72.3 70.4 70.7
Table 1: Accuracy and F1 scores for the multilingual GeoQuery test set. Results for other systems as
reported by Jones et al (2012).
the speed of SP, we investigate how many MRs
the decoder needs to generate before producing
one which is well-formed. In practice, increasing
search depth in the n-best list from 1 to 50 results
in a gain of no more than a percentage point or
two, and we conclude that our filtering method is
appropriate for the task.
We also compare the MT-based semantic
parsers to several recently published ones: WASP
(Wong and Mooney, 2006), which like the hier-
archical model described here learns a SCFG to
translate between NL and MRL; tsVB (Jones et
al., 2012), which uses variational Bayesian infer-
ence to learn weights for a tree transducer; UBL
(Kwiatkowski et al, 2010), which learns a CCG
lexicon with semantic annotations; and hybrid-
tree (Lu et al, 2008), which learns a synchronous
generative model over variable-free MRs and NL
strings.
In the results shown in Table 1 we observe that
on English GeoQuery data, the hierarchical trans-
lation model achieves scores competitive with the
state of the art, and in every language one of the
MT systems achieves accuracy at least as good as
a purpose-built semantic parser.
We conclude with an informal test of training
speeds. While differences in implementation and
factors like programming language choice make
a direct comparison of times necessarily impre-
cise, we note that the MT system takes less than
three minutes to train on the GeoQuery corpus,
while the publicly-available implementations of
tsVB and UBL require roughly twenty minutes and
five hours respectively on a 2.1 GHz CPU. So
in addition to competitive performance, the MT-
based parser also appears to be considerably more
efficient at training time than other parsers in the
literature.
5 Related Work
WASP, an early automatically-learned SP system,
was strongly influenced by MT techniques. Like
the present work, it uses GIZA++ alignments as
a starting point for the rule extraction procedure,
and algorithms reminiscent of those used in syn-
tactic MT to extract rules.
tsVB also uses a piece of standard MT ma-
chinery, specifically tree transducers, which have
been profitably employed for syntax-based ma-
chine translation (Maletti, 2010). In that work,
however, the usual MT parameter-estimation tech-
nique of simply counting the number of rule oc-
currences does not improve scores, and the au-
thors instead resort to a variational inference pro-
cedure to acquire rule weights. The present work
is also the first we are aware of which uses phrase-
based rather than tree-based machine translation
techniques to learn a semantic parser. hybrid-tree
(Lu et al, 2008) similarly describes a generative
model over derivations of MRL trees.
The remaining system discussed in this paper,
UBL (Kwiatkowski et al, 2010), leverages the fact
that the MRL does not simply encode trees, but
rather ?-calculus expressions. It employs resolu-
tion procedures specific to the ?-calculus such as
splitting and unification in order to generate rule
templates. Like other systems described, it uses
GIZA alignments for initialization. Other work
which generalizes from variable-free meaning rep-
resentations to ?-calculus expressions includes the
natural language generation procedure described
by Lu and Ng (2011).
UBL, like an MT system (and unlike most of the
other systems discussed in this section), extracts
rules at multiple levels of granularity by means of
this splitting and unification procedure. hybrid-
tree similarly benefits from the introduction of
50
multi-level rules composed from smaller rules, a
process similar to the one used for creating phrase
tables in a phrase-based MT system.
6 Discussion
Our results validate the hypothesis that it is possi-
ble to adapt an ordinary MT system into a work-
ing semantic parser. In spite of the compara-
tive simplicity of the approach, it achieves scores
comparable to (and sometimes better than) many
state-of-the-art systems. For this reason, we argue
for the use of a machine translation baseline as a
point of comparison for new methods. The results
also demonstrate the usefulness of two techniques
which are crucial for successful MT, but which are
not widely used in semantic parsing. The first is
the incorporation of a language model (or com-
parable long-distance structure-scoring model) to
assign scores to predicted parses independent of
the transformation model. The second is the
use of large, composed rules (rather than rules
which trigger on only one lexical item, or on tree
portions of limited depth (Lu et al, 2008)) in
order to ?memorize? frequently-occurring large-
scale structures.
7 Conclusions
We have presented a semantic parser which uses
techniques from machine translation to learn map-
pings from natural language to variable-free mean-
ing representations. The parser performs com-
parably to several recent purpose-built semantic
parsers on the GeoQuery dataset, while training
considerably faster than state-of-the-art systems.
Our experiments demonstrate the usefulness of
several techniques which might be broadly applied
to other semantic parsers, and provides an infor-
mative basis for future work.
Acknowledgments
Jacob Andreas is supported by a Churchill Schol-
arship. Andreas Vlachos is funded by the Eu-
ropean Community?s Seventh Framework Pro-
gramme (FP7/2007-2013) under grant agree-
ment no. 270019 (SPACEBOOK project www.
spacebook-project.eu).
References
Steven Bird, Edward Loper, and Edward Klein.
2009. Natural Language Processing with Python.
O?Reilly Media, Inc.
Peter F. Brown, Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of statistical machine translation:
parameter estimation. Computational Linguistics,
19(2):263?311.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of the 43rd Annual Meeting on Association
for Computational Linguistics, pages 263?270, Ann
Arbor, Michigan.
H.B. Curry, J.R. Hindley, and J.P. Seldin. 1980. To
H.B. Curry: Essays on Combinatory Logic, Lambda
Calculus, and Formalism. Academic Press.
Dan Goldwasser, Roi Reichart, James Clarke, and Dan
Roth. 2011. Confidence driven unsupervised se-
mantic parsing. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
1486?1495, Portland, Oregon.
Bevan K. Jones, Mark Johnson, and Sharon Goldwater.
2012. Semantic parsing with bayesian tree transduc-
ers. In Proceedings of the 50th Annual Meeting of
the Association of Computational Linguistics, pages
488?496, Jeju, Korea.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Human Language Technology
Conference of the North American Chapter of the
Association for Computational Linguistics, pages
48?54, Edmonton, Canada.
Philipp Koehn, Amittai Axelrod, Alexandra Birch-
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh System Descrip-
tion for the 2005 IWSLT Speech Translation Evalu-
ation. In Proceedings of the International Workshop
on Spoken Language Translation.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondr?ej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the As-
sociation for Computational Linguistics Companion
Volume Proceedings of the Demo and Poster Ses-
sions, pages 177?180, Prague, Czech Republic.
Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-
ter, and Mark Steedman. 2010. Inducing proba-
bilistic ccg grammars from logical form with higher-
order unification. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1223?1233, Cambridge, Mas-
sachusetts.
Percy Liang, Michael Jordan, and Dan Klein. 2011.
Learning dependency-based compositional seman-
tics. In Proceedings of the 49th Annual Meeting of
51
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 590?599, Port-
land, Oregon.
Wei Lu and Hwee Tou Ng. 2011. A probabilistic
forest-to-string model for language generation from
typed lambda calculus expressions. In Proceedings
of the Conference on Empirical Methods in Natu-
ral Language Processing, EMNLP ?11, pages 1611?
1622. Association for Computational Linguistics.
Wei Lu, Hwee Tou Ng, Wee Sun Lee, and Luke Zettle-
moyer. 2008. A generative model for parsing nat-
ural language to meaning representations. In Pro-
ceedings of the 2008 Conference on Empirical Meth-
ods in Natural Language Processing, pages 783?
792, Edinburgh, UK.
Andreas Maletti. 2010. Survey: Tree transducers
in machine translation. In Proceedings of the 2nd
Workshop on Non-Classical Models for Automata
and Applications, Jena, Germany.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of the
38th Annual Meeting of the Association for Com-
putational Linguistics, pages 440?447, Hong Kong,
China.
Ana-Maria Popescu, Oren Etzioni, and Henry Kautz.
2003. Towards a theory of natural language inter-
faces to databases. In Proceedings of the 8th Inter-
national Conference on Intelligent User Interfaces,
pages 149?157, Santa Monica, CA.
M. Porter. 1980. An algorithm for suffix stripping.
Program, 14(3):130?137.
Yuk Wah Wong and Raymond Mooney. 2006. Learn-
ing for semantic parsing with statistical machine
translation. In Proceedings of the 2006 Human Lan-
guage Technology Conference of the North Amer-
ican Chapter of the Association of Computational
Linguistics, pages 439?446, New York.
John M. Zelle. 1995. Using Inductive Logic Program-
ming to Automate the Construction of Natural Lan-
guage Parsers. Ph.D. thesis, Department of Com-
puter Sciences, The University of Texas at Austin.
52
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 218?227,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Shift-Reduce CCG Parsing with a Dependency Model
Wenduan Xu
University of Cambridge
Computer Laboratory
wx217@cam.ac.uk
Stephen Clark
University of Cambridge
Computer Laboratory
sc609@cam.ac.uk
Yue Zhang
Singapore University
of Technology and Design
yue zhang@sutd.edu.sg
Abstract
This paper presents the first dependency
model for a shift-reduce CCG parser. Mod-
elling dependencies is desirable for a num-
ber of reasons, including handling the
?spurious? ambiguity of CCG; fitting well
with the theory of CCG; and optimizing
for structures which are evaluated at test
time. We develop a novel training tech-
nique using a dependency oracle, in which
all derivations are hidden. A challenge
arises from the fact that the oracle needs
to keep track of exponentially many gold-
standard derivations, which is solved by
integrating a packed parse forest with the
beam-search decoder. Standard CCGBank
tests show the model achieves up to 1.05
labeled F-score improvements over three
existing, competitive CCG parsing models.
1 Introduction
Combinatory Categorial Grammar (CCG; Steed-
man (2000)) is able to derive typed dependency
structures (Hockenmaier, 2003; Clark and Curran,
2007), providing a useful approximation to the un-
derlying predicate-argument relations of ?who did
what to whom?. To date, CCG remains the most
competitive formalism for recovering ?deep? de-
pendencies arising from many linguistic phenom-
ena such as raising, control, extraction and coordi-
nation (Rimell et al, 2009; Nivre et al, 2010).
To achieve its expressiveness, CCG exhibits
so-called ?spurious? ambiguity, permitting many
non-standard surface derivations which ease the
recovery of certain dependencies, especially those
arising from type-raising and composition. But
this raises the question of what is the most suit-
able model for CCG: should we model the deriva-
tions, the dependencies, or both? The choice for
some existing parsers (Hockenmaier, 2003; Clark
and Curran, 2007) is to model derivations directly,
restricting the gold-standard to be the normal-form
derivations (Eisner, 1996) from CCGBank (Hock-
enmaier and Steedman, 2007).
Modelling dependencies, as a proxy for the se-
mantic interpretation, fits well with the theory of
CCG, in which Steedman (2000) argues that the
derivation is merely a ?trace? of the underlying
syntactic process, and that the structure which
is built, and predicated over when applying con-
straints on grammaticality, is the semantic inter-
pretation. The early dependency model of Clark
et al (2002), in which model features were defined
over only dependency structures, was partly moti-
vated by these theoretical observations.
More generally, dependency models are desir-
able for a number of reasons. First, modelling
dependencies provides an elegant solution to the
spurious ambiguity problem (Clark and Curran,
2007). Second, obtaining training data for de-
pendencies is likely to be easier than for syn-
tactic derivations, especially for incomplete data
(Schneider et al, 2013). Clark and Curran (2006)
show how the dependency model from Clark and
Curran (2007) extends naturally to the partial-
training case, and also how to obtain dependency
data cheaply from gold-standard lexical category
sequences alone. And third, it has been argued that
dependencies are an ideal representation for parser
evaluation, especially for CCG (Briscoe and Car-
roll, 2006; Clark and Hockenmaier, 2002), and so
optimizing for dependency recovery makes sense
from an evaluation perspective.
In this paper, we fill a gap in the literature by
developing the first dependency model for a shift-
reduce CCG parser. Shift-reduce parsing applies
naturally to CCG (Zhang and Clark, 2011), and the
left-to-right, incremental nature of the decoding
fits with CCG?s cognitive claims. The discrimina-
tive model is global and trained with the structured
perceptron. The decoder is based on beam-search
218
(Zhang and Clark, 2008) with the advantage of
linear-time decoding (Goldberg et al, 2013).
A main contribution of the paper is a novel tech-
nique for training the parser using a dependency
oracle, in which all derivations are hidden. A
challenge arises from the potentially exponential
number of derivations leading to a gold-standard
dependency structure, which the oracle needs to
keep track of. Our solution is an integration of
a packed parse forest, which efficiently stores all
the derivations, with the beam-search decoder at
training time. The derivations are not explicitly
part of the data, since the forest is built from the
gold-standard dependencies. We also show how
perceptron learning with beam-search (Collins and
Roark, 2004) can be extended to handle the ad-
ditional ambiguity, by adapting the ?violation-
fixing? perceptron of Huang et al (2012).
Results on the standard CCGBank tests show
that our parser achieves absolute labeled F-score
gains of up to 0.5 over the shift-reduce parser of
Zhang and Clark (2011); and up to 1.05 and 0.64
over the normal-form and hybrid models of Clark
and Curran (2007), respectively.
2 Shift-Reduce with Beam-Search
This section describes how shift-reduce tech-
niques can be applied to CCG, following Zhang
and Clark (2011). First we describe the determin-
istic process which a parser would follow when
tracing out a single, correct derivation; then we
describe how a model of normal-form derivations
? or, more accurately, a sequence of shift-reduce
actions leading to a normal-form derivation ?
can be used with beam-search to develop a non-
deterministic parser which selects the highest scor-
ing sequence of actions. Note this section only de-
scribes a normal-form derivation model for shift-
reduce parsing. Section 3 explains how we extend
the approach to dependency models.
The shift-reduce algorithm adapted to CCG is
similar to that of shift-reduce dependency parsing
(Yamada and Matsumoto, 2003; Nivre and Mc-
Donald, 2008; Zhang and Clark, 2008; Huang and
Sagae, 2010). Following Zhang and Clark (2011),
we define each item in the parser as a pair ?s, q?,
where q is a queue of remaining input, consisting
of words and a set of possible lexical categories for
each word (with q
0
being the front word), and s is
the stack that holds subtrees s
0
, s
1
, ... (with s
0
at
the top). Subtrees on the stack are partial deriva-
step stack (s
n
, ..., s
1
, s
0
) queue (q
0
, q
1
, ..., q
m
) action
0 Mr. President visited Paris
1 N/N President visited Paris SHIFT
2 N/N N visited Paris SHIFT
3 N visited Paris REDUCE
4 NP visited Paris UNARY
5 NP (S [dcl]\NP)/NP Paris SHIFT
6 NP (S [dcl]\NP)/NP N SHIFT
7 NP (S [dcl]\NP)/NP NP UNARY
8 NP S [dcl]\NP REDUCE
9 S [dcl] REDUCE
Figure 1: Deterministic example of shift-reduce
CCG parsing (lexical categories omitted on queue).
tions that have been built as part of the shift-reduce
process. SHIFT, REDUCE and UNARY are the three
types of actions that can be applied to an item. A
SHIFT action shifts one of the lexical categories
of q
0
onto the stack. A REDUCE action combines
s
0
and s
1
according to a CCG combinatory rule,
producing a new category on the top of the stack.
A UNARY action applies either a type-raising or
type-changing rule to the stack-top category s
0
.
1
Figure 1 shows a deterministic example for the
sentence Mr. President visited Paris, giving a sin-
gle sequence of shift-reduce actions which pro-
duces a correct derivation (i.e. one producing the
correct set of dependencies). Starting with the ini-
tial item ?s, q?
0
(row 0), which has an empty stack
and a full queue, a total of nine actions are applied
to produce the complete derivation.
Applying beam-search to a statistical shift-
reduce parser is a straightforward extension to the
deterministic example. At each step, a beam is
used to store the top-k highest-scoring items, re-
sulting from expanding all items in the previous
beam. An item becomes a candidate output once it
has an empty queue, and the parser keeps track of
the highest scored candidate output and returns the
best one as the final output. Compared with greedy
local-search (Nivre and Scholz, 2004), the use of
a beam allows the parser to explore a larger search
space and delay difficult ambiguity-resolving de-
cisions by considering multiple items in parallel.
We refer to the shift-reduce model of Zhang and
Clark (2011) as the normal-form model, where
the oracle for each sentence specifies a unique se-
quence of gold-standard actions which produces
the corresponding normal-form derivation. No de-
pendency structures are involved at training and
test time, except for evaluation. In the next sec-
tion, we describe a dependency oracle which con-
siders all sequences of actions producing a gold-
standard dependency structure to be correct.
1
See Hockenmaier (2003) and Clark and Curran (2007)
for a description of CCG rules.
219
Mr. President visited Paris
N /N N (S [dcl ]\NP)/NP NP
> >
N S [dcl ]\NP
>TC
NP
<
S [dcl ]
(a)
Mr. President visited Paris
N /N N (S [dcl ]\NP)/NP NP
>
N
>TC
NP
>T
S [dcl ]/(S [dcl ]\NP)
>B
S [dcl ]/NP
>
S [dcl ]
(b)
Figure 2: Two derivations leading to the same dependency structure. TC denotes type-changing.
3 The Dependency Model
Categories in CCG are either basic (such as NP
and PP ) or complex (such as (S [dcl ]\NP)/NP ).
Each complex category in the lexicon defines one
or more predicate-argument relations, which can
be realized as a predicate-argument dependency
when the corresponding argument slot is con-
sumed. For example, the transitive verb category
above defines two relations: one for the subject
NP and one for the object NP . In this paper a
CCG predicate-argument dependency is a 4-tuple:
?h
f
, f, s, h
a
? where h
f
is the lexical item of the
lexical category expressing the relation; f is the
lexical category; s is the argument slot; and h
a
is
the head word of the argument. Since the lexical
items in a dependency are indexed by their sen-
tence positions, all dependencies for a sentence
form a set, which is referred to as a CCG depen-
dency structure. Clark and Curran (2007) contains
a detailed description of dependency structures.
Fig. 2 shows an example demonstrating spu-
rious ambiguity in relation to a CCG depen-
dency structure. In both derivations, the first
two lexical categories are combined using for-
ward application (>) and the following depen-
dency is realized: ?Mr.,N /N
1
, 1,President?. In
the normal-form derivation (a), the dependency
?visited, (S\NP
1
)/NP
2
, 2,Paris? is created by com-
bining the transitive verb category with the ob-
ject NP using forward application. One final de-
pendency, ?visited, (S\NP
1
)/NP
2
, 1,President?, is re-
alized when the root node S [dcl ] is produced
through backward application (<).
Fig. 2(b) shows a non-normal-form derivation
which uses type-raising (T) and composition (B)
(which are not required to derive the correct de-
pendency structure). In this alternative derivation,
the dependency ?visited, (S\NP
1
)/NP
2
, 1,President?
is realized using forward composition (B), and
?visited, (S\NP
1
)/NP
2
, 2,Paris? is realized when the
S [dcl ] root is produced.
The chart-based dependency model of Clark
and Curran (2007) treats all derivations as hid-
den, and defines a probabilistic model for a de-
pendency structure by summing probabilities of
all derivations leading to a particular structure.
Features are defined over both derivations and
CCG predicate-argument dependencies. We fol-
low a similar approach, but rather than define
a probabilistic model (which requires summing),
we define a linear model over sequences of shift-
reduce actions, as for the normal-form shift-reduce
model. However, the difference compared to the
normal-form model is that we do not assume a sin-
gle gold-standard sequence of actions.
Similar to Goldberg and Nivre (2012), we de-
fine an oracle which determines, for a gold-
standard dependency structure, G, what the valid
transition sequences are (i.e. those sequences cor-
responding to derivations leading to G). More
specifically, the oracle can determine, givenG and
an item ?s, q?, what the valid actions are for that
item (i.e. what actions can potentially lead to G,
starting with ?s, q? and the dependencies already
built on s). However, there can be exponentially
many valid action sequences for G, which we rep-
resent efficiently using a packed parse forest. We
show how the forest can be used, during beam-
search decoding, to determine the valid actions
for a parse item (Section 3.2). We also show, in
Section 3.3, how perceptron training with early-
update (Collins and Roark, 2004) can be used in
this setting.
3.1 The Oracle Forest
A CCG parse forest efficiently represents an
exponential number of derivations. Following
Clark and Curran (2007) (which builds on Miyao
and Tsujii (2002)), and using the same nota-
tion, we define a CCG parse forest ? as a tuple
?C,D,R, ?, ??, where C is a set of conjunctive
220
Algorithm 1 (Clark and Curran, 2007)
Input: A packed forest ?C,D,R, ?, ??, with dmax(c)
and dmax(d) already computed
1: function MAIN
2: for each d
r
? R s.t. dmax
.
(d
r
) = |G| do
3: MARK(d
r
)
4: procedure MARK(d)
5: mark d as a correct node
6: for each c ? ?(d) do
7: if dmax(c) == dmax(d) then
8: mark c as a correct node
9: for each d
?
? ?(c) do
10: if d
?
has not been visited then
11: MARK(d
?
)
nodes and D is a set of disjunctive nodes.
2
Con-
junctive nodes are individual CCG categories in ?,
and are either obtained from the lexicon, or by
combining two disjunctive nodes using a CCG rule,
or by applying a unary rule to a disjunctive node.
Disjunctive nodes are equivalence classes of con-
junctive nodes. Two conjunctive nodes are equiv-
alent iff they have the same category, head and un-
filled dependencies (i.e. they will lead to the same
derivation, and produce the same dependencies, in
any future parsing). R ? D is a set of root dis-
junctive nodes. ? : D ? 2
C
is the conjunctive
child function and ? : C ? 2
D
is the disjunctive
child function. The former returns the set of all
conjunctive nodes of a disjunctive node, and the
latter returns the disjunctive child nodes of a con-
junctive node.
The dependency model requires all the conjunc-
tive and disjunctive nodes of ? that are part of the
derivations leading to a gold-standard dependency
structure G. We refer to such derivations as cor-
rect derivations and the packed forest containing
all these derivations as the oracle forest, denoted
as ?
G
, which is a subset of ?. It is prohibitive to
enumerate all correct derivations, but it is possible
to identify, from ?, all the conjunctive and dis-
junctive nodes that are part of ?
G
. Clark and Cur-
ran (2007) gives an algorithm for doing so, which
we use here. The main intuition behind the algo-
rithm is that a gold-standard dependency structure
decomposes over derivations; thus gold-standard
dependencies realized at conjunctive nodes can be
counted when ? is built, and all nodes that are part
of ?
G
can then be marked out of ? by traversing
it top-down. A key idea in understanding the algo-
2
Under the hypergraph framework (Gallo et al, 1993;
Huang and Chiang, 2005), a conjunctive node corresponds to
a hyperedge and a disjunctive node corresponds to the head
of a hyperedge or hyperedge bundle.
rithm is that dependencies are created when dis-
junctive nodes are combined, and hence are asso-
ciated with, or ?live on?, conjunctive nodes in the
forest.
Following Clark and Curran (2007), we also
define the following three values, where the first
decomposes only over local rule productions,
while the other two decompose over derivations:
cdeps(c) =
{
? if ? ? ? deps(c), ? /? G
|deps(c)| otherwise
dmax(c) =
?
??
??
? if cdeps(c) == ?
? if dmax(d) == ? for some d ? ?(c)
?
d??(c)
dmax(d) + cdeps(c) otherwise
dmax(d) = max{dmax(c) | c ? ?(d)}
deps(c) is the set of all dependencies on con-
junctive node c, and cdeps(c) counts the number
of correct dependencies on c. dmax(c) is the max-
imum number of correct dependencies over any
sub-derivation headed by c and is calculated re-
cursively; dmax(d) returns the same value for a
disjunctive node. In all cases, a special value ?
indicates the presence of incorrect dependencies.
To obtain the oracle forest, we first pre-compute
dmax(c) and dmax(d) for all d and c in ? when ?
is built using CKY, which are then used by Algo-
rithm 1 to identify all the conjunctive and disjunc-
tive nodes in ?
G
.
3.2 The Dependency Oracle Algorithm
We observe that the canonical shift-reduce algo-
rithm (as demonstrated in Fig. 1) applied to a sin-
gle parse tree exactly resembles bottom-up post-
order traversal of that tree. As an example, con-
sider the derivation in Fig. 2a, where the corre-
sponding sequence of actions is: sh N /N , sh N ,
re N , un NP , sh (S [dcl ]\NP)/NP , sh NP ,
re S [dcl ]\NP , re S [dcl ].
3
The order of traversal
is left-child, right-child and parent. For a single
parse, the corresponding shift-reduce action se-
quence is unique, and for a given item this canoni-
cal order restricts the possible derivations that can
be formed using further actions. We now extend
this observation to the more general case of an
oracle forest, where there may be more than one
gold-standard action for a given item.
Definition 1. Given a gold-standard dependency
3
The derivation is ?upside down?, following the conven-
tion used for CCG, where the root is S [dcl ]. We use sh, re
and un to denote the three types of shift-reduce action.
221
Mr. President visited Paris
N /N N (S [dcl ]\NP)/NP NP
> >
N S[dcl]\NP
(a)
Mr. President visited Paris
N/N N (S [dcl ]\NP)/NP NP
>
S[dcl]\NP
(b)
Figure 3: Example subtrees on two stacks, with two subtrees in (a) and three in (b); roots of subtrees are
in bold.
structure G, an oracle forest ?
G
, and an item
?s, q?, we say s is a realization of G, denoted
s ' G, if |s| = 1, q is empty and the single deriva-
tion on s is correct. If |s| > 0 and the subtrees on
s can lead to a correct derivation in ?
G
using fur-
ther actions, we say s is a partial-realization of
G, denoted as s ? G. And we define s ? G for
|s| = 0.
As an example, assume that ?
G
contains only
the derivation in Fig. 2a; then a stack containing
the two subtrees in Fig. 3a is a partial-realization,
while a stack containing the three subtrees in
Fig. 3b is not. Note that each of the three sub-
trees in Fig. 3b is present in ?
G
; however, these
subtrees cannot be combined into the single cor-
rect derivation, since the correct sequence of shift-
reduce actions must first combine the lexical cat-
egories for Mr. and President before shifting the
lexical category for visited.
We denote an action as a pair (x, c), where
x ? {SHIFT, REDUCE, UNARY} and c is the root
of the subtree resulting from that action. For all
three types of actions, c also corresponds to a
unique conjunctive node in the complete forest ?;
and we use c
s
i
to denote the conjunctive node in
? corresponding to subtree s
i
on the stack. Let
?s
?
, q
?
? = ?s, q? ? (x, c) be the resulting item from
applying the action (x, c) to ?s, q?; and let the
set of all possible actions for ?s, q? be X
?s,q?
=
{(x, c) | (x, c) is applicable to ?s, q?}.
Definition 2. Given ?
G
and an item ?s, q? s.t. s ?
G, we say an applicable action (x, c) for the item
is valid iff s
?
? G or s
?
' G, where ?s
?
, q
?
? =
?s, q? ? (x, c).
Definition 3. Given ?
G
, the dependency oracle
function f
d
is defined as:
f
d
(?s, q?, (x, c),?
G
) =
{
true if s
?
? G or s
?
' G
false otherwise
where (x, c) ? X
?s,q?
and ?s
?
, q
?
? = ?s, q? ? (x, c).
The pseudocode in Algorithm 2 implements f
d
.
It determines, for a given item, whether an appli-
cable action is valid in ?
G
.
It is trivial to determine the validity of a SHIFT
action for the initial item, ?s, q?
0
, since the SHIFT
action is valid iff its category matches the gold-
standard lexical category of the first word in
the sentence. For any subsequent SHIFT action
(SHIFT, c) to be valid, the necessary condition is
c ? c
lex
0
, where c
lex
0
denotes the gold-standard
lexical category of the front word in the queue, q
0
(line 3). However, this condition is not sufficient;
a counterexample is the case where all the gold-
standard lexical categories for the sentence in Fig-
ure 2 are shifted in succession. Hence, in general,
the conditions under which an action is valid are
more complex than the trivial case above.
First, suppose there is only one correct deriva-
tion in ?
G
. A SHIFT action (SHIFT, c
lex
0
) is valid
whenever c
s
0
(the conjunctive node in ?
G
cor-
responding to the subtree s
0
on the stack) and
c
lex
0
(the conjunctive node in ?
G
corresponding
to the next gold-standard lexical category from
the queue) are both dominated by the conjunctive
node parent p of c
s
0
in ?
G
.
4
A REDUCE action
(REDUCE, c) is valid if c matches the category of
the conjunctive node parent of c
s
0
and c
s
1
in ?
G
.
A UNARY action (UNARY, c) is valid if c matches
the conjunctive node parent of c
s
0
in ?
G
. We now
generalize the case where ?
G
contains a single
correct parse to the case of an oracle forest, where
each parent p is replaced by a set of conjunctive
nodes in ?
G
.
Definition 4. The left parent set p
L
(c) of con-
junctive node c ? ?
G
is the set of all parent con-
junctive nodes of c in ?
G
, which have the disjunc-
tive node d containing c (i.e. c ? ?(d)) as a left
child.
Definition 5. The ancestor set A(c) of conjunc-
tive node c ? ?
G
is the set of all reachable ances-
tor conjunctive nodes of c in ?
G
.
Definition 6. Given an item ?s, q?, if |s| = 1 we
say s is a frontier stack.
4
Strictly speaking, the conjunctive node parent is a parent
of the disjunctive node containing the conjunctive node c
s
0
.
We will continue to use this shorthand for parents of conjunc-
tive nodes throughout the paper.
222
Algorithm 2 The Dependency Oracle Function f
d
Input: ?
G
, an item ?s, q? s.t. s ? G, (x, c) ? X
?s,q?
Let s
?
be the stack of ?s
?
, q
?
? = ?s, q? ? (x, c)
1: function MAIN(?s, q?, (x, c), ?
G
)
2: if x is SHIFT then
3: if c 6? c
lex
0
then . c not gold lexical category
4: return false
5: else if c ? c
lex
0
and |s| = 0 then . the initial item
6: return true
7: else if c ? c
lex
0
and |s| 6= 0 then
8: computeR(c
s
?
1
, c
s
?
0
)
9: returnR(c
s
?
1
, c
s
?
0
) 6= ?
10: if x is REDUCE then . s is non-frontier
11: if c ? R(c
s
1
, c
s
0
) then
12: computeR(c
s
?
1
, c
s
?
0
)
13: return true
14: else return false
15: if x is UNARY then
16: if |s| = 1 then . s is frontier
17: return c ? ?
G
18: if |s| 6= 1 and c ? ?
G
then . s is non-frontier
19: computeR(c
s
?
1
, c
s
?
0
)
20: returnR(c
s
?
1
, c
s
?
0
) 6= ?
A key to defining the dependency oracle func-
tion is the notion of a shared ancestor set. In-
tuitively, shared ancestor sets are built up through
shift actions, and contain sets of nodes which can
potentially become the results of reduce or unary
actions. A further intuition is that shared ances-
tor sets define the space of possible correct deriva-
tions, and nodes in these sets are ?ticked off? when
reduce and unary actions are applied, as a single
correct derivation is built through the shift-reduce
process (corresponding to a bottom-up post-order
traversal of the derivation). The following defi-
nition shows how the dependency oracle function
builds shared ancestor sets for each action type.
Definition 7. Let ?s, q? be an item and let
?s
?
, q
?
? = ?s, q? ? (x, c). We define the shared an-
cestor setR(c
s
?
1
, c
s
?
0
) of c
s
?
0
, after applying action
(x, c), as:
? {c
?
| c
?
? p
L
(c
s
0
) ? A(c)}, if s is frontier and x =
SHIFT
? {c
?
| c
?
? p
L
(c
s
0
) ? A(c) and there is some c
??
?
R(c
s
1
, c
s
0
) s.t. c
??
? A(c
?
)}, if s is non-frontier and
x = SHIFT
? {c
?
| c
?
? R(c
s
2
, c
s
1
) ? A(c)}, if x = REDUCE
? {c
?
| c
?
? R(c
s
1
, c
s
0
) ? A(c)}, if s is non-frontier
and x = UNARY
? R(, c
0
s
0
) = ? where c
0
s
0
is the conjunctive node cor-
responding to the gold-standard lexical category of the
first word in the sentence ( is a dummy symbol indi-
cating the bottom of stack).
The base case for Definition 7 is when the gold-
standard lexical category of the first word in the
sentence has been shifted, which creates an empty
shared ancestor set. Furthermore, the shared an-
cestor set is always empty when the stack is a fron-
tier stack.
The dependency oracle algorithm checks the va-
lidity of applicable actions. A SHIFT action is
valid if R(c
s
?
1
, c
s
?
0
) 6= ? for the resulting stack
s
?
. A valid REDUCE action consumes s
1
and
s
0
. For the new node, its shared ancestor set is
the subset of the conjunctive nodes in R(c
s
2
, c
s
1
)
which dominate the resulting conjunctive node of
a valid REDUCE action. The UNARY case for a
frontier stack is trivial: any UNARY action ap-
plicable to s in ?
G
is valid. For a non-frontier
stack, the UNARY case is similar to REDUCE ex-
cept the resulting shared ancestor set is a subset of
R(c
s
1
, c
s
0
).
We now turn to the problem of finding the
shared ancestor sets. In practice, we do not do this
by traversing ?
G
top-down from the conjunctive
nodes in p
L
(c
s
0
) on-the-fly to find each member of
R. Instead, when we build ?
G
in bottom-up topo-
logical order, we pre-compute the set of reachable
disjunctive nodes of each conjunctive node c in
?
G
as:
D(c) = ?(c) ? (?
c
?
??(d),d??(c)
(D(c
?
)))
Each D is implemented as a hash map, which
allows us to test the membership of one potential
conjunctive node in O(1) time. For example, a
conjunctive node c ? p
L
(c
s
0
) is reachable from
c
lex
0
if there is a disjunctive node d ? D(c) s.t.
c
lex
0
? ?(d). With this implementation, the com-
plexity of checking each valid SHIFT action is then
O(|p
L
(c
s
0
)|).
3.3 Training
We use the averaged perceptron (Collins, 2002)
to train a global linear model and score each ac-
tion. The normal-form model of Zhang and Clark
(2011) uses an early update mechanism (Collins
and Roark, 2004), where decoding is stopped to
update model weights whenever the single gold
action falls outside the beam. In our parser, there
can be multiple gold items in a beam. One option
would be to apply early update whenever at least
223
Algorithm 3 Dependency Model Training
Input: (y,G) and beam size k
1: w? 0; B
0
? ?; i? 0
2: B
0
.push(?s, q?
0
) . the initial item
3: cand? ? . candidate output priority queue
4: gold? ? . gold output priority queue
5: while B
i
6= ? do
6: for each ?s, q? ? B
i
do
7: if |q| = 0 then . candidate output
8: cand.push(?s, q?)
9: if s ' G then . s is a realization of G
10: gold.push(?s, q?)
11: expand ?s, q? into B
i+1
12: B
i+1
? B
i+1
[1 : k] . apply beam
13: if ?
G
6= ?, ?
G
? B
i+1
= ? and cand[0] 6' G then
14: w? w + ?(?
G
[0])? ?(B
i+1
[0]) . early update
15: return
16: i? i+ 1 . continue to next step
17: if cand[0] 6' G then . final update
18: w? w + ?(gold[0])? ?(cand[0])
one of these gold items falls outside the beam.
However, this may not be a true violation of the
gold-standard (Huang et al, 2012). Thus, we use a
relaxed version of early update, in which all gold-
standard actions must fall outside the beam before
an update is performed. This update mechanism is
provably correct under the violation-fixing frame-
work of Huang et al (2012).
Let (y,G) be a training sentence paired with its
gold-standard dependency structure and let ?
?s,q?
be the following set for an item ?s, q?:
{?s, q? ? (x, c) | f
d
(?s, q?, (x, c),?
G
) = true}
?
?s,q?
contains all correct items at step i + 1 ob-
tained by expanding ?s, q?. Let the set of all cor-
rect items at a step i+ 1 be:
5
?
G
=
?
?s,q??B
i
?
?s,q?
Algorithm 3 shows the pseudocode for training
the dependency model with early update for one
input (y,G). The score of an item ?s, q? is calcu-
lated as w ? ?(?s, q?) with respect to the current
model w, where ?(?s, q?) is the feature vector for
the item. At step i, all items are expanded and
added onto the next beam B
i+1
, and the top-k re-
tained. Early update is applied when all gold items
first fall outside the beam, and any candidate out-
put is incorrect (line 14). Since there are poten-
tially many gold items, and one gold item is re-
quired for the perceptron update, a decision needs
5
In Algorithm 3 we abuse notation by using ?
G
[0] to de-
note the highest scoring gold item in the set.
to be made regarding which gold item to update
against. We choose to reward the highest scoring
gold item, in line with the violation-fixing frame-
work; and penalize the highest scoring incorrect
item, using the standard perceptron update. A fi-
nal update is performed if no more expansions are
possible but the final output is incorrect.
4 Experiments
We implement our shift-reduce parser on top of the
core C&C code base (Clark and Curran, 2007) and
evaluate it against the shift-reduce parser of Zhang
and Clark (2011) (henceforth Z&C) and the chart-
based normal-form and hybrid models of Clark
and Curran (2007). For all experiments, we use
CCGBank with the standard split: sections 2-21
for training (39,604 sentences), section 00 for de-
velopment (1,913 sentences) and section 23 (2,407
sentences) for testing.
The way that the CCG grammar is implemented
in C&C has some implications for our parser.
First, unlike Z&C, which uses a context-free cover
(Fowler and Penn, 2010) and hence is able to use
all sentences in the training data, we are only able
to use 36,036 sentences. The reason is that the
grammar in C&C does not have complete cover-
age of CCGBank, due to the fact that e.g. not
all rules in CCGBank conform to the combinatory
rules of CCG. Second, our parser uses the unifica-
tion mechanism from C&C to output dependencies
directly, and hence does not need a separate post-
processing step to convert derivations into CCG de-
pendencies, as required by Z&C.
The feature templates of our model consist of
all of those in Z&C, except the ones which re-
quire lexical heads to come from either the left or
right child, as such features are incompatible with
the head passing mechanism used by C&C. Each
Z&C template is defined over a parse item, and
captures various aspects of the stack and queue
context. For example, one template returns the
top category on the stack plus its head word, to-
gether with the first word and its POS tag on the
queue. Another template returns the second cat-
egory on the stack, together with the POS tag of
its head word. Every Z&C feature is defined as
a pair, consisting of an instantiated context tem-
plate and a parse action. In addition, we use all
the CCG predicate-argument dependency features
from Clark and Curran (2007), which contribute to
the score of a REDUCE action when dependencies
224
LP % LR % LF % LSent. % CatAcc. % coverage %
this parser 86.29 84.09 85.18 34.40 92.75 100
Z&C 87.15 82.95 85.00 33.82 92.77 100
C&C (normal-form) 85.22 82.52 83.85 31.63 92.40 100
this parser 86.76 84.90 85.82 34.72 93.20 99.06 (C&C coverage)
Z&C 87.55 83.63 85.54 34.14 93.11 99.06 (C&C coverage)
C&C (hybrid) ? ? 85.25 ? ? 99.06 (C&C coverage)
C&C (normal-form) 85.22 84.29 84.76 31.93 92.83 99.06 (C&C coverage)
Table 1: Accuracy comparison on Section 00 (auto POS).
 60
 65
 70
 75
 80
 85
 90
 0  5  10  15  20  25  30
Pre
cisi
on %
Dependency length (bins of 5)
C&CZ&Cthis parser
(a) precision vs. dependency length
 50
 55
 60
 65
 70
 75
 80
 85
 90
 0  5  10  15  20  25  30
Rec
all %
Dependency length (bins of 5)
C&CZ&Cthis parser
(b) recall vs. dependency length
Figure 4: Labeled precision and recall relative to dependency length on the development set. C&C
normal-form model is used.
are realized. Detailed descriptions of all the tem-
plates in our model can be found in the respective
papers. We run 20 training iterations and the re-
sulting model contains 16.5M features with a non-
zero weight.
We use 10-fold cross validation for POS tagging
and supertagging the training data, and automat-
ically assigned POS tags for all experiments. A
probability cut-off value of 0.0001 for the ? pa-
rameter in the supertagger is used for both train-
ing and testing. The ? parameter determines how
many lexical categories are assigned to each word;
? = 0.0001 is a relatively small value which al-
lows in a large number of categories, compared to
the default value used in Clark and Curran (2007).
For training only, if the gold-standard lexical cat-
egory is not supplied by the supertagger for a par-
ticular word, it is added to the list of categories.
4.1 Results and Analysis
The beam size was tuned on the development set,
and a value of 128 was found to achieve a rea-
sonable balance of accuracy and speed; hence this
value was used for all experiments. Since C&C al-
ways enforces non-fragmentary output (i.e. it can
only produce spanning analyses), it fails on some
sentences in the development and test sets, and
thus we also evaluate on the reduced sets, follow-
ing Clark and Curran (2007). Our parser does not
fail on any sentences because it permits fragmen-
tary output (those cases where there is more than
one subtree left on the final stack). The results for
Z&C, and the C&C normal-form and hybrid mod-
els, are taken from Zhang and Clark (2011).
Table 1 shows the accuracies of all parsers on
the development set, in terms of labeled precision
and recall over the predicate-argument dependen-
cies in CCGBank. On both the full and reduced
sets, our parser achieves the highest F-score. In
comparison with C&C, our parser shows signif-
icant increases across all metrics, with 0.57%
and 1.06% absolute F-score improvements over
the hybrid and normal-form models, respectively.
Another major improvement over the other two
parsers is in sentence level accuracy, LSent, which
measures the number of sentences for which the
dependency structure is completely correct.
Table 1 also shows that our parser has improved
recall over Z&C at some expense of precision. To
probe this further we compare labeled precision
and recall relative to dependency length, as mea-
sured by the distance between the two words in a
dependency, grouped into bins of 5 values. Fig. 4
shows clearly that Z&C favors precision over re-
call, giving higher precision scores for almost all
dependency lengths compared to our parser. In
225
category LP % (o) LP % (z) LP % (c) LR % (o) LR % (z) LR % (c) LF % (o) LF % (z) LF % (c) freq.
N /N 95.53 95.77 95.28 95.83 95.79 95.62 95.68 95.78 95.45 7288
NP/N 96.53 96.70 96.57 97.12 96.59 96.03 96.83 96.65 96.30 4101
(NP\NP)/NP 81.64 83.19 82.17 90.63 89.24 88.90 85.90 86.11 85.40 2379
(NP\NP)/NP 81.70 82.53 81.58 88.91 87.99 85.74 85.15 85.17 83.61 2174
((S\NP)\(S\NP))/NP 77.64 77.60 71.94 72.97 71.58 73.32 75.24 74.47 72.63 1147
((S\NP)\(S\NP))/NP 75.78 76.30 70.92 71.27 70.60 71.93 73.45 73.34 71.42 1058
((S [dcl ]\NP)/NP 83.94 85.60 81.57 86.04 84.30 86.37 84.98 84.95 83.90 917
PP/NP 77.06 73.76 75.06 73.63 72.83 70.09 75.31 73.29 72.49 876
((S [dcl ]\NP)/NP 82.03 85.32 81.62 83.26 82.00 85.55 82.64 83.63 83.54 872
((S\NP)\(S\NP)) 86.42 84.44 86.85 86.19 86.60 86.73 86.31 85.51 86.79 746
Table 2: Accuracy comparison on most frequent dependency types, for our parser (o), Z&C (z) and C&C
hybrid model (c). Categories in bold indicate the argument slot in the relation.
LP % LR % LF % LSent. % CatAcc. % coverage %
our parser 87.03 85.08 86.04 35.69 93.10 100
Z&C 87.43 83.61 85.48 35.19 93.12 100
C&C (normal-form) 85.58 82.85 84.20 32.90 92.84 100
our parser 87.04 85.16 86.09 35.84 93.13 99.58 (C&C coverage)
Z&C 87.43 83.71 85.53 35.34 93.15 99.58 (C&C coverage)
C&C (hybrid) 86.17 84.74 85.45 32.92 92.98 99.58 (C&C coverage)
C&C (normal-form) 85.48 84.60 85.04 33.08 92.86 99.58 (C&C coverage)
Table 3: Accuracy comparison on section 23 (auto POS).
terms of recall (Fig. 4b), our parser outperforms
Z&C over all dependency lengths, especially for
longer dependencies (x ? 20). When compared
with C&C, the recall of the Z&C parser drops
quickly for dependency lengths over 10. While
our parser also suffers from this problem, it is
less severe and is able to achieve higher recall at
x ? 30.
Table 2 compares our parser with Z&C and the
C&C hybrid model, for the most frequent depen-
dency relations. While our parser achieved lower
precision than Z&C, it is more balanced and gives
higher recall for all of the dependency relations ex-
cept the last one, and higher F-score for over half
of them.
Table 3 presents the final test results on Section
23. Again, our parser achieves the highest scores
across all metrics (for both the full and reduced
test sets), except for precision and lexical category
assignment, where Z&C performed better.
5 Conclusion
We have presented a dependency model for a shift-
reduce CCG parser, which fully aligns CCG parsing
with the left-to-right, incremental nature of a shift-
reduce parser. Our work is in part inspired by the
dependency models of Clark and Curran (2007)
and, in the use of a dependency oracle, is close
in spirit to that of Goldberg and Nivre (2012). The
difference is that the Goldberg and Nivre parser
builds, and scores, dependency structures directly,
whereas our parser uses a unification mechanism
to create dependencies, and scores the CCG deriva-
tions, allowing great flexibility in terms of what
dependencies can be realized. Another related
work is Yu et al (2013), which introduced a sim-
ilar technique to deal with spurious ambiguity in
MT. Finally, there may be potential to integrate the
techniques of Auli and Lopez (2011), which cur-
rently represents the state-of-the-art in CCGBank
parsing, into our parser.
Acknowledgements
We thank the anonymous reviewers for their help-
ful comments. Wenduan Xu is fully supported by
the Carnegie Trust and receives additional fund-
ing from the Cambridge Trusts. Stephen Clark
is supported by ERC Starting Grant DisCoTex
(306920) and EPSRC grant EP/I037512/1. Yue
Zhang is supported by Singapore MOE Tier2 grant
T2MOE201301.
References
Michael Auli and Adam Lopez. 2011. A compari-
son of loopy belief propagation and dual decompo-
sition for integrated CCG supertagging and parsing.
In Proc. ACL 2011, pages 470?480, Portland, OR.
Ted Briscoe and John Carroll. 2006. Evaluating the
accuracy of an unlexicalized statistical parser on the
226
PARC DepBank. In Proc. of COLING/ACL, pages
41?48, Sydney, Australia.
Stephen Clark and James R. Curran. 2006. Partial
training for a lexicalized-grammar parser. In Proc.
NAACL-06, pages 144?151, New York, USA.
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG
and log-linear models. Computational Linguistics,
33(4):493?552.
Stephen Clark and Julia Hockenmaier. 2002. Evalu-
ating a wide-coverage CCG parser. In Proc. of the
LREC 2002 Beyond Parseval Workshop, pages 60?
66, Las Palmas, Spain.
Stephen Clark, Julia Hockenmaier, and Mark Steed-
man. 2002. Building deep dependency structures
with a wide-coverage CCG parser. In Proc. ACL,
pages 327?334, Philadelphia, PA.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proc. of
ACL, pages 111?118, Barcelona, Spain.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: Theory and ex-
periments with perceptron algorithms. In Proc. of
EMNLP, pages 1?8, Philadelphia, USA.
Jason Eisner. 1996. Efficient normal-form parsing for
Combinatory Categorial Grammar. In Proc. ACL,
pages 79?86, Santa Cruz, CA.
Timothy AD Fowler and Gerald Penn. 2010. Accu-
rate context-free parsing with Combinatory Catego-
rial Grammar. In Proc. ACL, pages 335?344, Upp-
sala, Sweden.
Giorgio Gallo, Giustino Longo, Stefano Pallottino,
and Sang Nguyen. 1993. Directed hypergraphs
and applications. Discrete applied mathematics,
42(2):177?201.
Yoav Goldberg and Joakim Nivre. 2012. A dynamic
oracle for arc-eager dependency parsing. In Proc.
COLING, Mumbai, India.
Yoav Goldberg, Kai Zhao, and Liang Huang. 2013.
Efficient implementation for beam search incremen-
tal parsers. In Proceedings of the Short Papers of
ACL, Sofia, Bulgaria.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: A corpus of CCG derivations and dependency
structures extracted from the Penn Treebank. Com-
putational Linguistics, 33(3):355?396.
Julia Hockenmaier. 2003. Data and Models for Sta-
tistical Parsing with Combinatory Categorial Gram-
mar. Ph.D. thesis, University of Edinburgh.
Liang Huang and David Chiang. 2005. Better k-
best parsing. In Proceedings of the Ninth Interna-
tional Workshop on Parsing Technology, pages 53?
64, Vancouver, Canada.
Liang Huang and Kenji Sagae. 2010. Dynamic pro-
gramming for linear-time incremental parsing. In
Proc. ACL, pages 1077?1086, Uppsala, Sweden.
Liang Huang, Suphan Fayong, and Yang Guo. 2012.
Structured perceptron with inexact search. In Proc.
NAACL, pages 142?151, Montreal, Canada.
Yusuke Miyao and Jun?ichi Tsujii. 2002. Maximum
entropy estimation for feature forests. In Proceed-
ings of the Human Language Technology Confer-
ence, San Diego, CA.
Joakim Nivre and Ryan McDonald. 2008. Integrat-
ing graph-based and transition-based dependency
parsers. In Proc. of ACL/HLT, pages 950?958,
Columbus, Ohio.
J. Nivre and M Scholz. 2004. Deterministic depen-
dency parsing of English text. In Proceedings of
COLING 2004, pages 64?70, Geneva, Switzerland.
Joakim Nivre, Laura Rimell, Ryan McDonald, and Car-
los Gomez-Rodriguez. 2010. Evaluation of depen-
dency parsers on unbounded dependencies. In Proc.
of COLING, Beijing, China.
Laura Rimell, Stephen Clark, and Mark Steedman.
2009. Unbounded dependency recovery for parser
evaluation. In Proc. EMNLP, pages 813?821, Edin-
burgh, UK.
Nathan Schneider, Brendan O?Connor, Naomi Saphra,
David Bamman, Manaal Faruqui, Noah A. Smith,
Chris Dyer, and Jason Baldridge. 2013. A frame-
work for (under)specifying dependency syntax with-
out overloading annotators. In Proc. of the 7th Lin-
guistic Annotation Workshop and Interoperability
with Discourse, Sofia, Bulgaria.
Mark Steedman. 2000. The Syntactic Process. The
MIT Press, Cambridge, Mass.
H Yamada and Y Matsumoto. 2003. Statistical depen-
dency analysis using support vector machines. In
Proc. of IWPT, Nancy, France.
Heng Yu, Liang Huang, Haitao Mi, and Kai Zhao.
2013. Max-violation perceptron and forced decod-
ing for scalable mt training. In Proc. EMNLP, Seat-
tle, Washington, USA.
Yue Zhang and Stephen Clark. 2008. A tale of
two parsers: investigating and combining graph-
based and transition-based dependency parsing us-
ing beam-search. In Proc. of EMNLP, Hawaii, USA.
Yue Zhang and Stephen Clark. 2011. Shift-reduce
CCG parsing. In Proc. ACL 2011, pages 683?692,
Portland, OR.
227
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 835?841,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Improving Multi-Modal Representations Using Image Dispersion:
Why Less is Sometimes More
Douwe Kiela*, Felix Hill*, Anna Korhonen and Stephen Clark
University of Cambridge
Computer Laboratory
{douwe.kiela|felix.hill|anna.korhonen|stephen.clark}@cl.cam.ac.uk
Abstract
Models that learn semantic representations
from both linguistic and perceptual in-
put outperform text-only models in many
contexts and better reflect human concept
acquisition. However, experiments sug-
gest that while the inclusion of perceptual
input improves representations of certain
concepts, it degrades the representations
of others. We propose an unsupervised
method to determine whether to include
perceptual input for a concept, and show
that it significantly improves the ability of
multi-modal models to learn and represent
word meanings. The method relies solely
on image data, and can be applied to a va-
riety of other NLP tasks.
1 Introduction
Multi-modal models that learn semantic concept
representations from both linguistic and percep-
tual input were originally motivated by parallels
with human concept acquisition, and evidence that
many concepts are grounded in the perceptual sys-
tem (Barsalou et al, 2003). Such models extract
information about the perceptible characteristics
of words from data collected in property norming
experiments (Roller and Schulte im Walde, 2013;
Silberer and Lapata, 2012) or directly from ?raw?
data sources such as images (Feng and Lapata,
2010; Bruni et al, 2012). This input is combined
with information from linguistic corpora to pro-
duce enhanced representations of concept mean-
ing. Multi-modal models outperform language-
only models on a range of tasks, including mod-
elling conceptual association and predicting com-
positionality (Bruni et al, 2012; Silberer and Lap-
ata, 2012; Roller and Schulte im Walde, 2013).
Despite these results, the advantage of multi-
modal over linguistic-only models has only been
demonstrated on concrete concepts, such as
chocolate or cheeseburger, as opposed to abstract
concepts such as such as guilt or obesity. Indeed,
experiments indicate that while the addition of
perceptual input is generally beneficial for repre-
sentations of concrete concepts (Hill et al, 2013a;
Bruni et al, 2014), it can in fact be detrimental
to representations of abstract concepts (Hill et al,
2013a). Further, while the theoretical importance
of the perceptual modalities to concrete represen-
tations is well known, evidence suggests this is not
the case for more abstract concepts (Paivio, 1990;
Hill et al, 2013b). Indeed, perhaps the most influ-
ential characterization of the abstract/concrete dis-
tinction, the Dual Coding Theory (Paivio, 1990),
posits that concrete representations are encoded
in both the linguistic and perceptual modalities
whereas abstract concepts are encoded only in the
linguistic modality.
Existing multi-modal architectures generally
extract and process all the information from their
specified sources of perceptual input. Since per-
ceptual data sources typically contain information
about both abstract and concrete concepts, such in-
formation is included for both concept types. The
potential effect of this design decision on perfor-
mance is significant because the vast majority of
meaning-bearing words in everyday language cor-
respond to abstract concepts. For instance, 72% of
word tokens in the British National Corpus (Leech
et al, 1994) were rated by contributors to the Uni-
versity of South Florida dataset (USF) (Nelson et
al., 2004) as more abstract than the noun war, a
concept that many would consider quite abstract.
In light of these considerations, we propose
a novel algorithm for approximating conceptual
concreteness. Multi-modal models in which per-
ceptual input is filtered according to our algorithm
learn higher-quality semantic representations than
previous approaches, resulting in a significant per-
formance improvement of up to 17% in captur-
835
ing the semantic similarity of concepts. Further,
our algorithm constitutes the first means of quan-
tifying conceptual concreteness that does not rely
on labor-intensive experimental studies or annota-
tors. Finally, we demonstrate the application of
this unsupervised concreteness metric to the se-
mantic classification of adjective-noun pairs, an
existing NLP task to which concreteness data has
proved valuable previously.
2 Experimental Approach
Our experiments focus on multi-modal models
that extract their perceptual input automatically
from images. Image-based models more natu-
rally mirror the process of human concept acquisi-
tion than those whose input derives from exper-
imental datasets or expert annotation. They are
also more scalable since high-quality tagged im-
ages are freely available in several web-scale im-
age datasets.
We use Google Images as our image source,
and extract the first n image results for each con-
cept word. It has been shown that images from
Google yield higher-quality representations than
comparable sources such as Flickr (Bergsma and
Goebel, 2011). Other potential sources, such as
ImageNet (Deng et al, 2009) or the ESP Game
Dataset (Von Ahn and Dabbish, 2004), either do
not contain images for abstract concepts or do not
contain sufficient images for the concepts in our
evaluation sets.
2.1 Image Dispersion-Based Filtering
Following the motivation outlined in Section 1, we
aim to distinguish visual input corresponding to
concrete concepts from visual input correspond-
ing to abstract concepts. Our algorithm is moti-
vated by the intuition that the diversity of images
returned for a particular concept depends on its
concreteness (see Figure 1). Specifically, we an-
ticipate greater congruence or similarity among a
set of images for, say, elephant than among im-
ages for happiness. By exploiting this connection,
the method approximates the concreteness of con-
cepts, and provides a basis to filter the correspond-
ing perceptual information.
Formally, we propose a measure, image disper-
sion d of a concept word w, defined as the aver-
age pairwise cosine distance between all the image
representations { ~w
1
. . . ~w
n
} in the set of images
for that concept:
Figure 1: Example images for a concrete (elephant
? little diversity, low dispersion) and an abstract
concept (happiness ? greater diversity, high dis-
persion).
Figure 2: Computation of PHOW descriptors us-
ing dense SIFT for levels l = 0 to l = 2 and the
corresponding histogram representations (Bosch
et al, 2007).
d(w) =
1
2n(n? 1)
?
i<j?n
1?
~w
i
? ~w
j
| ~w
i
|| ~w
j
|
(1)
We use an average pairwise distance-based met-
ric because this emphasizes the total variation
more than e.g. the mean distance from the cen-
troid. In all experiments we set n = 50.
Generating Visual Representations Visual
vector representations for each image were ob-
tained using the well-known bag of visual words
(BoVW) approach (Sivic and Zisserman, 2003).
BoVW obtains a vector representation for an
836
image by mapping each of its local descriptors
to a cluster histogram using a standard clustering
algorithm such as k-means.
Previous NLP-related work uses SIFT (Feng
and Lapata, 2010; Bruni et al, 2012) or SURF
(Roller and Schulte im Walde, 2013) descriptors
for identifying points of interest in an image,
quantified by 128-dimensional local descriptors.
We apply Pyramid Histogram Of visual Words
(PHOW) descriptors, which are particularly well-
suited for object categorization, a key component
of image similarity and thus dispersion (Bosch et
al., 2007). PHOW is roughly equivalent to run-
ning SIFT on a dense grid of locations at a fixed
scale and orientation and at multiple scales (see
Fig 2), but is both more efficient and more accu-
rate than regular (dense) SIFT approaches (Bosch
et al, 2007). We resize the images in our dataset
to 100x100 pixels and compute PHOW descriptors
using VLFeat (Vedaldi and Fulkerson, 2008).
The descriptors for the images were subse-
quently clustered using mini-batch k-means (Scul-
ley, 2010) with k = 50 to obtain histograms of
visual words, yielding 50-dimensional visual vec-
tors for each of the images.
Generating Linguistic Representations We
extract continuous vector representations (also of
50 dimensions) for concepts using the continu-
ous log-linear skipgram model of Mikolov et al
(2013a), trained on the 100M word British Na-
tional Corpus (Leech et al, 1994). This model
learns high quality lexical semantic representa-
tions based on the distributional properties of
words in text, and has been shown to outperform
simple distributional models on applications such
as semantic composition and analogical mapping
(Mikolov et al, 2013b).
2.2 Evaluation Gold-standards
We evaluate models by measuring the Spearman
correlation of model output with two well-known
gold-standards reflecting semantic proximity ? a
standard measure for evaluating the quality of rep-
resentations (see e.g. Agirre et al (2009)).
To test the ability of our model to capture
concept similarity, we measure correlations with
WordSim353 (Finkelstein et al, 2001), a selec-
tion of 353 concept pairs together with a similar-
ity rating provided by human annotators. Word-
Sim has been used as a benchmark for distribu-
tional semantic models in numerous studies (see
e.g. (Huang et al, 2012; Bruni et al, 2012)).
As a complementary gold-standard, we use the
University of South Florida Norms (USF) (Nelson
et al, 2004). This dataset contains scores for free
association, an experimental measure of cognitive
association, between over 40,000 concept pairs.
The USF norms have been used in many previous
studies to evaluate semantic representations (An-
drews et al, 2009; Feng and Lapata, 2010; Sil-
berer and Lapata, 2012; Roller and Schulte im
Walde, 2013). The USF evaluation set is partic-
ularly appropriate in the present context because
concepts in the dataset are also rated for concep-
tual concreteness by at least 10 human annotators.
We create a representative evaluation set of USF
pairs as follows. We randomly sample 100 con-
cepts from the upper quartile and 100 concepts
from the lower quartile of a list of all USF con-
cepts ranked by concreteness. We denote these
sets C, for concrete, and A for abstract respec-
tively. We then extract all pairs (w
1
, w
2
) in the
USF dataset such that bothw
1
andw
2
are inA?C.
This yields an evaluation set of 903 pairs, of which
304 are such that w
1
, w
2
? C and 317 are such
that w
1
, w
2
? A.
The images used in our experiments and
the evaluation gold-standards can be down-
loaded from http://www.cl.cam.ac.uk/
?
dk427/dispersion.html.
3 Improving Multi-Modal
Representations
We apply image dispersion-based filtering as fol-
lows: if both concepts in an evaluation pair have
an image dispersion below a given threshold, both
the linguistic and the visual representations are in-
cluded. If not, in accordance with the Dual Cod-
ing Theory of human concept processing (Paivio,
1990), only the linguistic representation is used.
For both datasets, we set the threshold as the
median image dispersion, although performance
could in principle be improved by adjusting this
parameter. We compare dispersion filtered rep-
resentations with linguistic, perceptual and stan-
dard multi-modal representations (concatenated
linguistic and perceptual representations). Sim-
ilarity between concept pairs is calculated using
cosine similarity.
As Figure 3 shows, dispersion-filtered multi-
modal representations significantly outperform
837
0.145
0.532
0.477
0.542
0.189
0.229
0.203
0.247
0.0
0.1
0.2
0.3
0.4
0.5
Similarity ? WordSim 353 Free association ? USF (903)Evaluation Set
Cor
rela
tion
Model Representations
Linguistic onlyImage onlyStandard multi?modalDispersion filtered
Figure 3: Performance of conventional multi-
modal (visual input included for all concepts) vs.
image dispersion-based filtering models (visual in-
put only for concepts classified as concrete) on the
two evaluation gold-standards.
standard multi-modal representations on both
evaluation datasets. We observe a 17% increase in
Spearman correlation on WordSim353 and a 22%
increase on the USF norms. Based on the corre-
lation comparison method of Steiger (1980), both
represent significant improvements (WordSim353,
t = 2.42, p < 0.05; USF, t = 1.86, p < 0.1). In
both cases, models with the dispersion-based filter
also outperform the purely linguistic model, which
is not the case for other multi-modal approaches
that evaluate on WordSim353 (e.g. Bruni et al
(2012)).
4 Concreteness and Image Dispersion
The filtering approach described thus far improves
multi-modal representations because image dis-
persion provides a means to distinguish concrete
concepts from more abstract concepts. Since re-
search has demonstrated the applicability of con-
creteness to a range of other NLP tasks (Turney et
al., 2011; Kwong, 2008), it is important to exam-
ine the connection between image dispersion and
concreteness in more detail.
4.1 Quantifying Concreteness
To evaluate the effectiveness of image dispersion
as a proxy for concreteness we evaluated our al-
gorithm on a binary classification task based on
the set of 100 concrete and 100 abstract concepts
A?C introduced in Section 2. By classifying con-
0.184
0.257
0.29
0.054
0.189
0.167
0.0
0.1
0.2
0.3
0.4
'concrete' pairs (304) 'abstract' pairs (317)Concept Type
Cor
rela
tion
Representation Modality
LinguisticVisualLinguistic+Visual
Figure 4: Visual input is valuable for representing
concepts that are classified as concrete by the im-
age dispersion algorithm, but not so for concepts
classified as abstract. All correlations are with the
USF gold-standard.
cepts with image dispersion below the median as
concrete and concepts above this threshold as ab-
stract we achieved an abstract-concrete prediction
accuracy of 81%.
While well-understood intuitively, concreteness is
not a formally defined notion. Quantities such as
the USF concreteness score depend on the sub-
jective judgement of raters and the particular an-
notation guidelines. According to the Dual Cod-
ing Theory, however, concrete concepts are pre-
cisely those with a salient perceptual representa-
tion. As illustrated in Figure 4, our binary clas-
sification conforms to this characterization. The
importance of the visual modality is significantly
greater when evaluating on pairs for which both
concepts are classified as concrete than on pairs of
two abstract concepts.
Image dispersion is also an effective predic-
tor of concreteness on samples for which the ab-
stract/concrete distinction is less clear. On a differ-
ent set of 200 concepts extracted by random sam-
pling from the USF dataset stratified by concrete-
ness rating (including concepts across the con-
creteness spectrum), we observed a high correla-
tion between abstractness and dispersion (Spear-
man ? = 0.61, p < 0.001). On this more diverse
sample, which reflects the range of concepts typi-
cally found in linguistic corpora, image dispersion
is a particularly useful diagnostic for identifying
838
Concept Image Dispersion Conc. (USF)
shirt .488 6.05
bed .495 5.91
knife .560 6.08
dress .578 6.59
car .580 6.35
ego 1.000 1.93
nonsense .999 1.90
memory .999 1.78
potential .997 1.90
know .996 2.70
Table 1: Concepts with highest and lowest image
dispersion scores in our evaluation set, and con-
creteness ratings from the USF dataset.
the very abstract or very concrete concepts. As
Table 1 illustrates, the concepts with the lowest
dispersion in this sample are, without exception,
highly concrete, and the concepts of highest dis-
persion are clearly very abstract.
It should be noted that all previous approaches
to the automatic measurement of concreteness rely
on annotator ratings, dictionaries or manually-
constructed resources. Kwong (2008) proposes
a method based on the presence of hard-coded
phrasal features in dictionary entries correspond-
ing to each concept. By contrast, S?anchez et al
(2011) present an approach based on the position
of word senses corresponding to each concept in
the WordNet ontology (Fellbaum, 1999). Turney
et al (2011) propose a method that extends a large
set of concreteness ratings similar to those in the
USF dataset. The Turney et al algorithm quanti-
fies the concreteness of concepts that lack such a
rating based on their proximity to rated concepts
in a semantic vector space. In contrast to each of
these approaches, the image dispersion approach
requires no hand-coded resources. It is therefore
more scalable, and instantly applicable to a wide
range of languages.
4.2 Classifying Adjective-Noun Pairs
Finally, we explored whether image dispersion
can be applied to specific NLP tasks as an effec-
tive proxy for concreteness. Turney et al (2011)
showed that concreteness is applicable to the clas-
sification of adjective-noun modification as either
literal or non-literal. By applying a logistic regres-
sion with noun concreteness as the predictor vari-
able, Turney et al achieved a classification accu-
racy of 79% on this task. This model relies on sig-
nificant supervision in the form of over 4,000 hu-
man lexical concreteness ratings.
1
Applying im-
age dispersion in place of concreteness in an iden-
tical classifier on the same dataset, our entirely un-
supervised approach achieves an accuracy of 63%.
This is a notable improvement on the largest-class
baseline of 55%.
5 Conclusions
We presented a novel method, image dispersion-
based filtering, that improves multi-modal repre-
sentations by approximating conceptual concrete-
ness from images and filtering model input. The
results clearly show that including more percep-
tual input in multi-modal models is not always bet-
ter. Motivated by this fact, our approach provides
an intuitive and straightforward metric to deter-
mine whether or not to include such information.
In addition to improving multi-modal represen-
tations, we have shown the applicability of the im-
age dispersion metric to several other tasks. To
our knowledge, our algorithm constitutes the first
unsupervised method for quantifying conceptual
concreteness as applied to NLP, although it does,
of course, rely on the Google Images retrieval al-
gorithm. Moreover, we presented a method to
classify adjective-noun pairs according to modi-
fication type that exploits the link between image
dispersion and concreteness. It is striking that this
apparently linguistic problem can be addressed
solely using the raw data encoded in images.
In future work, we will investigate the precise
quantity of perceptual information to be included
for best performance, as well as the optimal filter-
ing threshold. In addition, we will explore whether
the application of image data, and the interaction
between images and language, can yield improve-
ments on other tasks in semantic processing and
representation.
Acknowledgments
DK is supported by EPSRC grant EP/I037512/1.
FH is supported by St John?s College, Cambridge.
AK is supported by The Royal Society. SC is sup-
ported by ERC Starting Grant DisCoTex (306920)
and EPSRC grant EP/I037512/1. We thank the
anonymous reviewers for their helpful comments.
1
The MRC Psycholinguistics concreteness ratings (Colt-
heart, 1981) used by Turney et al (2011) are a subset of those
included in the USF dataset.
839
References
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pas?ca, and Aitor Soroa. 2009.
A study on similarity and relatedness using distribu-
tional and wordnet-based approaches. In Proceed-
ings of Human Language Technologies: The 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
NAACL ?09, pages 19?27, Boulder, Colorado.
Mark Andrews, Gabriella Vigliocco, and David Vin-
son. 2009. Integrating experiential and distribu-
tional data to learn semantic representations. Psy-
chological review, 116(3):463.
Lawrence W Barsalou, W Kyle Simmons, Aron K Bar-
bey, and Christine D Wilson. 2003. Grounding
conceptual knowledge in modality-specific systems.
Trends in cognitive sciences, 7(2):84?91.
Shane Bergsma and Randy Goebel. 2011. Using vi-
sual information to predict lexical preference. In
RANLP, pages 399?405.
Anna Bosch, Andrew Zisserman, and Xavier Munoz.
2007. Image classification using random forests and
ferns. In Proceedings of ICCV.
Elia Bruni, Gemma Boleda, Marco Baroni, and Nam-
Khanh Tran. 2012. Distributional semantics in tech-
nicolor. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics: Long Papers-Volume 1, pages 136?145. Asso-
ciation for Computational Linguistics.
Elia Bruni, Nam Khanh Tran, and Marco Baroni. 2014.
Multimodal distributional semantics. Journal of Ar-
tificial Intelligence Research, 49:1?47.
Max Coltheart. 1981. The MRC psycholinguistic
database. The Quarterly Journal of Experimental
Psychology, 33(4):497?505.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. 2009. Imagenet: A large-scale hi-
erarchical image database. In Computer Vision and
Pattern Recognition, 2009. CVPR 2009. IEEE Con-
ference on, pages 248?255. IEEE.
Christiane Fellbaum. 1999. WordNet. Wiley Online
Library.
Yansong Feng and Mirella Lapata. 2010. Visual infor-
mation in semantic representation. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 91?99. Asso-
ciation for Computational Linguistics.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2001. Placing search in context: The
concept revisited. In Proceedings of the 10th inter-
national conference on World Wide Web, pages 406?
414. ACM.
Felix Hill, Douwe Kiela, and Anna Korhonen. 2013a.
Concreteness and corpora: A theoretical and practi-
cal analysis. CMCL 2013.
Felix Hill, Anna Korhonen, and Christian Bentz.
2013b. A quantitative empirical analysis of the ab-
stract/concrete distinction. Cognitive science, 38(1).
Eric H Huang, Richard Socher, Christopher D Man-
ning, and Andrew Y Ng. 2012. Improving word
representations via global context and multiple word
prototypes. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics: Long Papers-Volume 1, pages 873?882. Asso-
ciation for Computational Linguistics.
Oi Yee Kwong. 2008. A preliminary study on the im-
pact of lexical concreteness on word sense disam-
biguation. In PACLIC, pages 235?244.
Geoffrey Leech, Roger Garside, and Michael Bryant.
1994. Claws4: the tagging of the british national
corpus. In Proceedings of the 15th conference
on Computational linguistics-Volume 1, pages 622?
628. Association for Computational Linguistics.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word repre-
sentations in vector space. In Proceedings of Inter-
national Conference of Learning Representations,
Scottsdale, Arizona, USA.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems, pages 3111?3119.
Douglas L Nelson, Cathy L McEvoy, and Thomas A
Schreiber. 2004. The University of South Florida
free association, rhyme, and word fragment norms.
Behavior Research Methods, Instruments, & Com-
puters, 36(3):402?407.
Allan Paivio. 1990. Mental representations: A dual
coding approach. Oxford University Press.
Stephen Roller and Sabine Schulte im Walde. 2013.
A multimodal LDA model integrating textual, cog-
nitive and visual modalities. In Proceedings of the
2013 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1146?1157, Seattle,
Washington, USA, October. Association for Compu-
tational Linguistics.
David S?anchez, Montserrat Batet, and David Isern.
2011. Ontology-based information content compu-
tation. Knowledge-Based Systems, 24(2):297?303.
D Sculley. 2010. Web-scale k-means clustering. In
Proceedings of the 19th international conference on
World wide web, pages 1177?1178. ACM.
Carina Silberer and Mirella Lapata. 2012. Grounded
models of semantic representation. In Proceedings
of the 2012 Joint Conference on Empirical Methods
840
in Natural Language Processing and Computational
Natural Language Learning, pages 1423?1433. As-
sociation for Computational Linguistics.
J. Sivic and A. Zisserman. 2003. Video Google: a text
retrieval approach to object matching in videos. In
Proceedings of the Ninth IEEE International Con-
ference on Computer Vision, volume 2, pages 1470?
1477, Oct.
James H Steiger. 1980. Tests for comparing ele-
ments of a correlation matrix. Psychological Bul-
letin, 87(2):245.
Peter D Turney, Yair Neuman, Dan Assaf, and Yohai
Cohen. 2011. Literal and metaphorical sense iden-
tification through concrete and abstract context. In
Proceedings of the 2011 Conference on the Empiri-
cal Methods in Natural Language Processing, pages
680?690.
A. Vedaldi and B. Fulkerson. 2008. VLFeat: An open
and portable library of computer vision algorithms.
http://www.vlfeat.org/.
Luis Von Ahn and Laura Dabbish. 2004. Labeling
images with a computer game. In Proceedings of the
SIGCHI conference on Human factors in computing
systems, pages 319?326. ACM.
841
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 268?271,
Uppsala, Sweden, 15-16 July 2010.
c?2010 Association for Computational Linguistics
Cambridge: Parser Evaluation using Textual Entailment by
Grammatical Relation Comparison
Laura Rimell and Stephen Clark
University of Cambridge
Computer Laboratory
{laura.rimell,stephen.clark}@cl.cam.ac.uk
Abstract
This paper describes the Cambridge sub-
mission to the SemEval-2010 Parser Eval-
uation using Textual Entailment (PETE)
task. We used a simple definition of en-
tailment, parsing both T and H with the
C&C parser and checking whether the core
grammatical relations (subject and object)
produced for H were a subset of those for
T. This simple system achieved the top
score for the task out of those systems sub-
mitted. We analyze the errors made by the
system and the potential role of the task in
parser evaluation.
1 Introduction
SemEval-2010 Task 12, Parser Evaluation using
Textual Entailment (PETE) (Yuret et al, 2010),
was designed as a new, formalism-independent
type of parser evaluation scheme. The task is
broadly Recognizing Textual Entailment (RTE),
but unlike typical RTE tasks, its intention is to fo-
cus on purely syntactic entailments, assuming no
background knowledge or reasoning ability. For
example, given a text (T) The man with the hat
was tired., the hypothesis (H) The man was tired.
is entailed, but The hat was tired. is not. A cor-
rect decision on whether H is entailed can be used
as a diagnostic for the parser?s analysis of (some
aspect of) T. By requiring only a binary decision
on the entailment, instead of a full syntactic anal-
ysis, a parser can be evaluated while its underlying
formalism remains a ?black box?.
Our system had two components: a parser, and
an entailment system which decided whether T en-
tails H based on the parser?s output. We distin-
guish two types of evaluation. Task evaluation,
i.e. the official task scoring, indicates whether the
entailment decisions ? made by the parser and en-
tailment system together ? tally with the gold stan-
dard dataset. Entailment system evaluation, on the
other hand, indicates whether the entailment sys-
tem is an appropriate parser evaluation tool. In
the PETE task the parser is not evaluated directly
on the dataset, since the entailment system acts
as intermediary. Therefore, for PETE to be a vi-
able parser evaluation scheme, each parser must
be coupled with an entailment system which accu-
rately reflects the parser?s analysis of the data.
2 System
We used the C&C parser (Clark and Curran,
2007), which can produce output in the form of
grammatical relations (GRs), i.e. labelled head-
dependencies. For example, (nsubj tired
man) for the example in Section 1 represents the
fact that the NP headed by man is the subject of
the predicate headed by tired. We chose to use the
Stanford Dependency GR scheme (de Marneffe et
al., 2006), but the same approach should work for
other schemes (and other parsers producing GRs).
Our entailment system was very simple, and
based on the assumption that H is a simplified ver-
sion of T (true for this task though not for RTE in
general). We parsed both T and H with the C&C
parser. Let grs(S) be the GRs the parser produces
for a sentence S. In principle, if grs(H) ? grs(T),
then we would consider H an entailment. In prac-
tice, a few refinements to this rule are necessary.
We identified three exceptional cases. First,
syntactic transformations between T and H may
change GR labels. The most common transforma-
tion in this dataset was passivization, meaning that
a direct object in T could be a passive subject in H.
Second, H could contain tokens not present in T.
Auxiliary verbs were introduced by passivization.
Pronouns such as somebody and something were
introduced into some H sentences to indicate an
NP or other phrase not targeted for evaluation. De-
terminers were sometimes introduced or changed,
e.g. prices to the prices. Expletive subjects were
also sometimes introduced.
268
Third, the parses of T and H might be incon-
sistent in an incidental way. Consider the pair I
reached into that funny little pocket that is high up
on my dress. ? The pocket is high up on some-
thing. The intended focus of the evaluation (as in-
dicated by the content word pair supplied as a sup-
plement to the gold standard development data)
is (pocket, high). As long as the parser analyzes
pocket as the subject of high, we want to avoid
penalizing it for, say, treating the PP up on X dif-
ferently in T and H.
To address these issues we used a small set of
heuristics. First, we ignored any GR in grs(H) con-
taining a token not in T. This addressed the pas-
sive auxiliaries, pronouns, determiners, and exple-
tive subjects. Second, we equated passive subjects
with direct objects. Similar rules could be defined
for other transformations, but we implemented
only this one based on the prevalence of passiviza-
tion in the development data. Third, when check-
ing whether grs(H) ? grs(T), we considered only
the core relations subject and object. The intention
was that incidental differences between the parses
of T and H would not be counted as errors. We
chose these GR types based on the nature of the en-
tailments in the development data, but the system
could easily be reconfigured to focus on other rela-
tion types. Finally, we required grs(H) ? grs(T) to
be non-empty (no vacuous positives), but did not
restrict this criterion to subjects and objects.
We used a PTB tokenizer
1
for consistency with
the parser?s training data. We used the morpha
lemmatizer (Minnen et al, 2000), which is built
into the C&C tools, to match tokens across T and
H; and we converted all tokens to lowercase. If the
parser failed to find a spanning analysis for either
T or H, the entailment decision was NO. The full
pipeline is shown in Figure 1.
3 Results
A total of 19 systems were submitted. The base-
line score for ?always YES? was 51.8% accuracy.
Our system achieved 72.4% accuracy, which was
the highest score among the submitted systems.
Table 1 shows the results for our system, as well
as SCHWA (University of Sydney), also based on
the C&C parser and the next-highest scorer (see
Section 6 for a comparison), and the median and
lowest scores. The parser found an analysis for
1
http://www.cis.upenn.edu/
?
treebank/
tokenizer.sed.
Tokenize T and H with PTB tokenizer
?
Parse T and H with C&C parser
?
Lowercase and lemmatize all tokens
?
Discard any GR in grs(H) containing a token not in T
?
YES if core(H) ? core(T) and grs(H) ? grs(T) 6= ?,
NO otherwise
Figure 1: Full pipeline for parser and entailment
system. core(S): the set of core (subject and ob-
ject) GRs in grs(S).
99.0% of T sentences and 99.7% of H sentences
in the test data.
4 Error Analysis
Table 2 shows the results for our system on the de-
velopment data (66 sentences). The parser found
an analysis for 100% of sentences and the overall
accuracy was 66.7%. In the majority of cases the
parser and entailment system worked together to
find the correct answer as expected. For example,
for Trading in AMR shares was suspended shortly
after 3 p.m. EDT Friday and didn?t resume. ?
Trading didn?t resume., the parser produced three
GRs for H (tokens are shown lemmatized and low-
ercase): (nsubj resume trading), (neg
do n?t), and (aux resume do). All of
these were also in grs(T), and the correct YES
decision was made. For Moreland sat brood-
ing for a full minute, during which I made each
of us a new drink. ? Minute is made., the
parser produced two GRs for H. One, (auxpass
make be), was ignored because the passive
auxiliary be is not in T. The second, pas-
sive subject GR(nsubjpass make minute)
was equated with a direct object (dobj make
minute). This GR was not in grs(T), so the cor-
rect NO decision was made.
In some cases a correct YES answer was
reached via arguably insufficient positive evi-
dence. For He would wake up in the middle of
the night and fret about it. ? He would wake up.,
the parser produces incorrect analyses for the VP
would wake up for both T and H. However, these
GRs are ignored since they are non-core (not sub-
ject or object), and a YES decision is based on
the single GR match (nsubj would he). This
269
Score on YES entailments Score on NO entailments Overall
System correct incorrect accuracy (%) correct incorrect accuracy (%) accuracy (%)
Cambridge 98 58 62.8 120 25 82.8 72.4
SCHWA 125 31 80.1 87 58 60.0 70.4
Median 71 85 45.5 88 57 60.7 52.8
Low 68 88 43.6 76 69 52.4 47.8
Table 1: Results on the test data.
Score on YES entailments Score on NO entailments Overall
System correct incorrect accuracy (%) correct incorrect accuracy (%) accuracy (%)
Cambridge 22 16 57.9 22 6 78.6 66.7
Table 2: Results on the development data.
Type FN FP Total
Unbounded dependency 8 1 9
Other parser error 6 2 8
Entailment system 1 3 4
Difficult entailment 1 0 1
Total 16 6 22
Table 3: Error breakdown on the development
data. FN: false negative, FP: false positive.
is not entirely a lucky guess, since the entailment
system has correctly ignored the odd analyses of
would wake up and focused on the role of he as the
subject of the sentence. However, especially since
the target content word pair was (he, wake), more
positive evidence would be desirable. Of the 22
correct YES decisions, only two were truly lucky
guesses in that the single match was a determiner;
others had at least one core match.
Table 3 shows the breakdown of errors. The
largest category was false negatives due to un-
bounded dependencies not recovered by the
parser, for example It required an energy he no
longer possessed to be satirical about his father.
? Somebody no longer possessed the energy..
Here the parser fails to recover the direct object re-
lation between possess and energy in T. It is known
that parsers have difficulty with unbounded depen-
dencies (Rimell et al, 2009, from which the un-
bounded examples in this dataset were obtained),
so this result is not surprising.
The next category was other parser errors. This
is a miscellaneous category including e.g. errors
on coordination, parenthetical elements, identify-
ing the head of a clausal subject, and one due to
the POS tagger. For example, for Then at least he
would have a place to hang his tools and some-
thing to work on. ? He would have something to
work on., the parser incorrectly coordinated tools
and something for T. As a result (dobj have
something) was in grs(H) but not grs(T), yield-
ing an incorrect NO.
Four errors were due to the entailment system
rather than the parser; these will be dicsussed in
Section 5. We also identified one sentence where
the gold standard entailment appears to rely on
extra-syntactic information, or at least informa-
tion that is difficult for a parser to recover. This
is Index-arbitrage trading is ?something we want
to watch closely,? an official at London?s Stock Ex-
change said. ?We want to watch index-arbitrage
trading. Recovering the entailment would require
resolving the reference of something, arguably the
role of a semantic rather than syntactic module.
5 Entailment System Evaluation
We now consider whether our entailment system
was an appropriate tool for evaluating the C&C
parser on the PETE dataset. It is easy to imag-
ine a poor entailment system that makes incorrect
guesses in spite of good parser output, or con-
versely one that uses additional reasoning to sup-
plement the parser?s analysis. To be an appropri-
ate parser evaluation tool, the entailment system
must decide whether the information in H is also
contained in the parse of T, without ?introducing?
or ?correcting? any errors.
Assuming our GR-based approach is valid, then
given gold-standard GRs for T and H, we expect an
appropriate entailment system to result in 100%
accuracy on the task evaluation. To perform this
oracle experiment we annotated the development
270
data with gold-standard GRs. Using our entailment
system with the gold GRs we achieved 90.9% task
accuracy. Six incorrect entailment decisions were
made, of which one was on the arguably extra-
syntactic entailment discussed in Section 4.
Three errors were due to transformations be-
tween T and H which changed the GR label or
head. For example, consider Occasionally, the
children find steamed, whole-wheat grains for ce-
real which they call ?buckshot?. ? Grains are
steamed.. In T, steamed is a prenominal adjective,
with grains as its head; while in H, it is a passive,
with grains as its subject. The entailment system
did not account for this transformation, although
in principle it could have. The other two errors
occurred because GRs involving a non-core rela-
tion or a pronoun introduced in H, both of which
our system ignored, were crucial for the correct
entailment decision.
Table 3 shows that with automatically-
generated GRs, four errors on the task evaluation
were attributable to the entailment system. Three
of these were also found in the oracle experiment.
The fourth resulted from a POS change between T
and H for There was the revolution in Tibet which
we pretended did not exist. ? The pretended did
not exist.. The crucial GR was (nsubj exist
pretended) in grs(H), but the entailment
system ignored it because the lemmatizer did
not give pretend as the lemma for pretended as a
noun. This type of error might be prevented by
answering NO if the POS of any word changes
between T and H, but the implementation is
non-trivial since word indices may also change.
There were eight POS changes in the development
data, most of which did not result in errors. We
also observed two cases where the entailment
system ?corrected? parser errors, yielding a
correct entailment decision despite the parser?s
incorrect analysis of T. When compared with a
manual analysis of whether T entailed H based
on automatically-generated GRs, the entailment
system achieved 89.4% overall accuracy.
6 Conclusion
We achieved a successful result on the PETE task
using a state-of-the-art parser and a simple entail-
ment system, which tested syntactic entailments
by comparing the GRs produced by the parser for
T and H. We also showed that our entailment sys-
tem had accuracy of approximately 90% as a tool
for evaluating the C&C parser (or potentially any
parser producing GR-style output) on the PETE
development data. This latter result is perhaps
even more important than the task score since it
suggests that PETE is worth pursuing as a viable
approach to parser evaluation.
The second-highest scoring system, SCHWA
(University of Sydney), was also based on the
C&C parser and used a similar approach (though
using CCG dependency output rather than GRs).
It achieved almost identical task accuracy to the
Cambridge system, but interestingly with higher
accuracy on YES entailments, while our system
was more accurate on NO entailments (Table 1).
We attribute this difference to the decision crite-
ria: both systems required at least one matching
relation between T and H for a YES answer; but
we additionally answered NO if any core GR in
grs(H) was not in grs(T). This difference shows
that a GR-based entailment system can be tuned to
favour precision or recall.
Finally, we note that although this was a sim-
ple entailment system with some dataset-specific
characteristics ? such as a focus on subject and
object relations rather than, say, PP-attachment ?
these aspects should be amenable to customization
or generalization for other related tasks.
Acknowledgments
The authors were supported by EPSRC grant
EP/E035698/1. We thank Matthew Honnibal for
his help in producing the gold-standard GRs.
References
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG
and log-linear models. Computational Linguistics,
33(4):493?552.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of LREC, Genoa, Italy.
Guido Minnen, John Carroll, and Darren Pearce. 2000.
Robust, applied morphological generation. In Pro-
ceedings of INLG, Mitzpe Ramon, Israel.
Laura Rimell, Stephen Clark, and Mark Steedman.
2009. Unbounded dependency recovery for parser
evaluation. In Proceedings of EMNLP, Singapore.
Deniz Yuret, Ayd?n Han, and Zehra Turgut. 2010.
Semeval-2010 task 12: Parser evaluation using tex-
tual entailments. In Proceedings of SemEval-2010,
Uppsala, Sweden.
271
Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation, pages 44?50
Manchester, August 2008
Constructing a Parser Evaluation Scheme
Laura Rimell and Stephen Clark
Oxford University Computing Laboratory
Wolfson Building, Parks Road
Oxford, OX1 3QD, United Kingdom
{laura.rimell,stephen.clark}@comlab.ox.ac.uk
Abstract
In this paper we examine the process of
developing a relational parser evaluation
scheme, identifying a number of decisions
which must be made by the designer of
such a scheme. Making the process more
modular may help the parsing community
converge on a single scheme. Examples
from the shared task at the COLING parser
evaluation workshop are used to highlight
decisions made by various developers, and
the impact these decisions have on any re-
sulting scoring mechanism. We show that
quite subtle distinctions, such as howmany
grammatical relations are used to encode a
linguistic construction, can have a signifi-
cant effect on the resulting scores.
1 Introduction
In this paper we examine the various decisions
made by designers of parser evaluation schemes
based on grammatical relations (Lin, 1995; Car-
roll et al, 1998). Following Carroll et al (1998),
we use the term grammatical relations to refer
to syntactic dependencies between heads and de-
pendents. We assume that grammatical relation
schemes are currently the best method available
for parser evaluation due to their relative inde-
pendence of any particular parser or linguistic
theory. There are several grammatical relation
schemes currently available, for example Carroll et
al. (1998), King et al (2003), and de Marneffe et
al. (2006). However, there has been little analysis
of the decisions made by the designers in creating
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
what turns out to be a complex set of dependen-
cies for naturally occurring sentences. In particu-
lar, in this paper we consider how the process can
be made more modular to help the parsing commu-
nity converge on a single scheme.
The first decision to be made by the scheme
designer is what types of linguistic constructions
should be covered by the scheme. By construction
we mean syntactic phenomena such as subject of
verb, direct object of verb, passive voice, coordina-
tion, relative clause, apposition, and so on. In this
paper we assume that the constructions of interest
have already been identified (and there does appear
to be broad agreement on this point across the ex-
isting schemes). A construction can be thought of
as a unitary linguistic object, although it is often
represented by several grammatical relations.
The second decision to be made is which words
are involved in a particular construction. This is
important because a subset of these words will
be arguments of the grammatical relations repre-
senting the construction. Again, we assume that
there is already broad agreement among the exist-
ing schemes regarding this question. One possible
point of disagreement is whether to include empty
elements in the representation, for example when
a passive verb has no overt subject, but we will not
address that issue here.
The next question, somewhat orthogonal to the
previous one, and a source of disagreement be-
tween schemes, is how informative the represen-
tation should be. By informative we mean the
amount of linguistic information represented in the
scheme. As well as relations between heads, some
schemes include one or more features, each of
which expresses information about an individual
head. These features can be the locus of richer lin-
guistic information than is represented in the de-
44
pendencies. A useful example here is tense and
mood information for verbs. This is included in the
PARC scheme, for example, but not in the Briscoe
and Carroll or Stanford schemes; PARC is in gen-
eral more informative and detailed than competing
schemes. Although features are technically dif-
ferent from relations, they form part of an overall
evaluation scheme and must be considered by the
scheme designer. We will not consider here the
question of how informative schemes should be;
we only note the importance of this question for
the resulting scoring mechanism.
The penultimate question, also a source of
disagreement among existing schemes, is which
words among all those involved in the construc-
tion should be used to represent it in the scheme.
This decision may arise when identifying syntac-
tic heads; for example, in the sentence Brown
said house prices will continue to fall, we assume
there is no disagreement about which words are
involved in the clausal complement construction
({said, house, prices, will, continue, to, fall}), but
there may be disagreement about which subset to
use to represent the construction in the grammat-
ical relations. Here, either will or continue could
be used to represent the complement of said. This
decision may also be theory-dependent to some de-
gree, for example whether to use the determiner or
the noun as the head of a noun phrase.
The final decision to make is the choice of rela-
tions and their arguments. This can also be thought
of as the choice of how the set of representative
words should be grouped into relations. For exam-
ple, in a relative clause construction, the scheme
designer must decide whether the relation between
the relative pronoun and the head noun is impor-
tant, or the relation between the relative pronoun
and the verb, between the head noun and the verb,
or some subset of these. The choice of label for
each relation will be a natural part of this decision.
An important property of the representation,
closely related to the choices made about represen-
tative words and how they are grouped into rela-
tions, is the number of relations used for a partic-
ular construction. We refer to this as the compact-
ness property. Compactness essentially boils down
to the valency of each relation and the information
encoded in the label(s) used for the relation. We
show that this property is closely related to the as-
signing of partial credit ? awarding points even
when a construction is not recovered completely
correctly ? and that it can have a significant effect
on the resulting scoring mechanism.
The dividing lines between the various ques-
tions we have described are subtle, and in partic-
ular the last two questions (which words should
represent the construction and which relations to
use, and consequently how compactly the rela-
tions are represented) have significant overlap with
one another. For example, if the auxiliary are
in the passive construction prices are affected is
chosen as one of the representative words, then
a relation type which relates are to either prices
or affected must also be chosen. For the relative
clause construction woman who likes apples and
pears, if the words and relations chosen include
a representation along the lines of relative-clause-
subject(likes, woman) and subject(likes, who), then
it is unlikely that the more compact relation
relative-clause(likes, woman, who) would also be
chosen. Despite the overlap, each question can
provide a useful perspective for the designer of an
evaluation scheme.
Decisions must be made not only about the rep-
resentations of the individual constructions, but
also about the interfaces between constructions.
For example, in the sentenceMary likes apples and
pears, the coordination structure apples and pears
serves as direct object of likes, and it must be de-
termined which word(s) are used to represent the
coordination in the direct object relation.
We will illustrate some of the consequences of
the decisions described here with detailed exam-
ples of three construction types. We focus on pas-
sive, coordination, and relative clause construc-
tions, as analysed in the PARC (King et al, 2003),
GR (Briscoe and Carroll, 2006), and Stanford (de
Marneffe et al, 2006) evaluation schemes, using
sentences from the shared task of the COLING 2008
parser evaluation workshop.
1
These three con-
structions were chosen because we believe they
provide particularly good illustrations of the var-
ious decisions and their consequences for scoring.
Furthermore, they are constructions whose repre-
sentation differs across at least two of the three
grammatical relation schemes under dicsussion,
which makes them more interesting as examples.
We believe that the principles involved, however,
1
The shared task includes a number of additional formats
besides the three grammatical relation schemes that we con-
sider here, but the representations are sufficiently different
that we don?t consider a comparison fruitful for the present
discussion.
45
apply to any linguistic construction.
We also wish to point out that at this stage we are
not recommending any particular scheme or any
answers to the questions we raise, but only sug-
gesting ways to clarify the decision points. Nor do
we intend to imply that the ideal representation of
any linguistic construction, for any particular pur-
pose, is one of the representations in an existing
scheme; we merely use the existing schemes as
concrete and familiar illustrations of the issues in-
volved.
2 The Passive Construction
The following is an extract from Sentence 9 of the
shared task:
how many things are made out of eggs
We expect general agreement that this is a pas-
sive construction, and that it should be included in
the evaluation scheme.
2
We also expect agreement
that all the words in this extract are involved in the
construction.
Potential disagreements arise when we consider
which words should represent the construction.
Things, as the head of the noun phrase which is the
underlying object of the passive, and made, as the
main verb, seem uncontroversial. We discard how
and many as modifiers of things, and the prepo-
sitional phrase out of eggs as a modifier of made;
again we consider these decisions to be straightfor-
ward. More controversial is whether to include the
auxiliary verb are. PARC, for example, does not
include it in the scheme at all, considering it an in-
herent part of the passive construction. Even if the
auxiliary verb is included in the overall scheme, it
is debatable whether this word should be consid-
ered part of the passive construction or part of a
separate verb-auxiliary construction. Stanford, for
example, uses the label auxpass for the relation be-
tween made and are, indicating that it is part of the
passive construction.
The next decision to be made is what relations
to use. We consider it uncontroversial to include
a relation between things and made, which will be
some kind of subject relation. We also want to rep-
resent the fact that made is in the passive voice,
since this is an essential part of the construction
and makes it possible to derive the underlying ob-
ject position of things. If the auxiliary are is in-
2
PARC recognises it as an interrogative as well as a passive
construction.
cluded, then there should be a verb-auxiliary rela-
tion between made and are, and perhaps a subject
relation between are and things (although none of
the schemes under consideration use the latter rela-
tion). PARC includes a variety of additional infor-
mation about the selected words in the construc-
tion, including person and number information for
the nouns, as well as tense and mood for the verbs.
Since this is not included in the other two schemes,
we ignore it here.
The relevant relations from the three schemes
under consideration are shown below.
3
PARC
passive(make, +)
subj(make, thing)
GR
(ncsubj made things obj)
(passive made)
(aux made are)
Stanford
nsubjpass(made, things)
auxpass(made, are)
PARC encodes the grammatical relations less
compactly, with one subject relation joining make
and thing, and a separate relation expressing the
fact that make is in the passive voice. Stanford
is more compact, with a single relation nsubj-
pass that expresses both verb-subject (via the argu-
ments) and passive voice (via the label). GR has an
equally compact relation since the obj marker sig-
nifies passive when found in the ncsubj relation.
GR, however, also includes an additional feature
passive, which redundantly encodes the fact that
made is in passive voice.
4
Table 1 shows how different kinds of parsing er-
rors are scored in the three schemes. First note the
differences in the ?everything correct? row, which
shows how many points are available for the con-
struction. A parser that is good at identifying pas-
sives will earn more points in GR than in PARC
and Stanford. Of course, it is always possible to
look at accuracy figures by dependency type in or-
der to understand what a parser is good at, as rec-
ommended by Briscoe and Carroll (2006), but it is
3
Schemes typically include indices on the words for iden-
tification, but we omit these from the examples unless re-
quired for disambiguation. Note also that PARC uses the
lemma rather than the inflected form for the head words.
4
Although passive is technically a feature and not a rela-
tion, as long as it is included in the evaluation the effect will
be of double scoring.
46
PARC GR Stanf
Everything correct 2 3 2
Misidentify subject 1 2 1
Misidentify verb 0 0 0
Miss passive constr 1 1 0
Miss auxiliary 2 2 1
Table 1: Scores for passive construction.
also desirable to have a single score reflecting the
overall accuracy of a parser, which means that the
construction?s overall contribution to the score is
relevant.
5
Observe also that partial credit is assigned dif-
ferently in the three schemes. If the parser recog-
nises the subject of made but misses the fact that
the construction is a passive, for example, it will
earn one out of two possible points in PARC, one
out of three in GR (if it recognizes the auxiliary),
but zero out of two in Stanford. This type of error
may seem unlikely, yet examples are readily avail-
able. In related work we have evaluated the C&C
parser of Clark and Curran (2007) on the BioIn-
fer corpus of biomedical abstracts (Pyysalo et al,
2007), which includes the following sentence:
Acanthamoeba profilin was cross-linked
to actin via a zero-length isopeptide
bond using carbodiimide.
The parser correctly identifies profilin as the sub-
ject of cross-linked, yet because it misidentifies
cross-linked as an adjectival rather than verbal
predicate, it misses the passive construction.
Finally, note an asymmetry in the partial credit
scoring: a parser that misidentifies the subject (e.g.
by selecting the wrong head), but basically gets the
construction correct, will receive partial credit in
all three schemes; misidentifying the verb, how-
ever (again, this would likely occur by selecting
the wrong head within the verb phrase) will cause
the parser to lose all points for the construction.
3 The Coordination Construction
The coordination construction is particularly inter-
esting with regard to the questions at hand, both
because there are many options for representing
the construction itself and because the interface
with other constructions is non-trivial. Here we
5
We assume that the overall score will be an F-score over
all dependencies/features in the relevant test set.
consider an extract from Sentence 1 of the shared
task:
electronic, computer and building prod-
ucts
The coordination here is of nominal modifiers,
which means that there is a decision to make about
how the coordination interfaces with the modified
noun. All the conjuncts could interact with the
noun, or there could be a single relationship, usu-
ally represented as a relationship between the con-
junction and and the noun.
Again we consider the decisions about whether
to represent coordination constructions in an eval-
uation scheme, and about which words are in-
volved in the construction, to be generally agreed
upon. The choice of words to represent the
construction in the grammatical relations is quite
straightforward: we need all three conjuncts, elec-
tronic, computer, and building, and also the con-
junction itself since this is contentful. It also seems
reasonably uncontroversial to discard the comma
(although we know of at least one parser that
outputs relations involving the comma, the C&C
parser).
The most difficult decision here is whether the
conjuncts should be related to one another or to
the conjunction (or both). Shown below is how the
three schemes represent the coordination, consid-
ering also the interface of the coordination and the
nominal modification construction.
PARC
adjunct(product, coord)
adjunct type(coord, nominal)
conj(coord, building)
conj(coord, computer)
conj(coord, electronic)
coord form(coord, and)
coord level(coord, AP)
GR
(conj and electronic)
(conj and computer)
(conj and building)
(ncmod products and)
Stanford
conj and(electronic, computer)
conj and(electronic, building)
amod(products, electronic)
amod(products, computer)
amod(products, building)
47
Table 2 shows the range of scores assigned for
correct and partially correct parses across the three
schemes. A parser that analyses the entire con-
struction correctly will earn anywhere from four
points in GR, to seven points in PARC. Therefore,
a parser that does very well (or poorly) at coordi-
nation will earn (or lose) points disproportionately
in the different schemes.
Parc GR Stanf
Everything correct 7 4 5
Misidentify
conjunction 6 0 3
Misidentify one
conjunct 6
a
3 3
b
Misidentify two
conjuncts 5
a
2 1
a
The parser might also be incorrect about the co-
ord level relation if the conjuncts are misidentified.
b
The score would be 2 if it is the first conjunct that
is misidentified.
Table 2: Scores for coordination, including
interface with nominal modification.
A parser that recognises the conjuncts correctly
but misidentifies the conjunction would lose only
one point in PARC, where the conjunction is sep-
arated out into a single coord form relation, but
would lose all four available points in GR, because
the word and itself takes part in all four GR de-
pendencies. Only two points are lost in Stanford
(and it is worth noting that there is also an ?uncol-
lapsed? variant of the Stanford scheme in which
the coordination type is not rolled into the depen-
dency label, in which case only one point would be
lost).
Note also an oddity in Stanford which means
that if the first conjunct is missed, all the dependen-
cies are compromised, because the first conjunct
enters into relations with all the others. The more
conjuncts there are in the construction, the more
points are lost for a single parsing error, which can
easily result from an error in head selection.
Another issue is how the conjuncts are repre-
sented relative to the nominal modifier construc-
tion. In PARC and GR, the conjunct and stands in
for all the conjuncts in the modifier relation. This
means that if a conjunct is missed, no extra points
are lost on the modifier relation; whereas in Stan-
ford, points are lost doubly ? on the relations in-
volving both conjunction and modification.
4 The Relative Clause Construction
For the relative clause construction, as for coordi-
nation, the choice of words used to represent the
construction is straightforward, but the choice of
relations is less so. Consider the following relative
clause construction from Sentence 2 of the shared
task:
not all those who wrote
All three schemes under consideration use the set
{those, who, wrote} to describe this construction.
6
PARC
pron form(pro
3
, those)
adjunct(pro
3
, write)
adjunct type(write, relative)
pron form(pro
4
, who)
pron type(pro
4
, relative)
pron rel(write, pro
4
)
topic rel(write, pro
4
)
GR
(cmod who those wrote)
(ncsubj wrote those )
Stanford
nsubj(wrote, those)
rel(wrote, who)
rcmod(those, wrote)
Note that PARC represents the pronouns who
and those, as it does all pronouns, at a more ab-
stract level than GR or Stanford, creating a rep-
resentation that is less compact than the others.
GR and Stanford differ in terms of compactness as
well: GR?s cmod relation contains all three words;
in fact, the ncsubj relationship might be considered
redundant from the point of view of an evaluation
scheme, since an error in ncsubj entails an error in
cmod. Stanford?s representation is less compact,
containing only binary relations, although there is
also a redundancy between nsubj and rcmod since
the two relations are mirror images of each other.
For the sake of comparison, we include here two
additional hypothetical schemes which have dif-
ferent characteristics from those of the three tar-
get schemes. In Hypothetical Scheme 1 (HS1),
there are three relations: one between the head
noun and the relative clause verb, one between the
6
PARC also encodes the fact that pro
3
is a demonstrative
pronoun, but we don?t consider this part of the relative clause
construction.
48
PARC GR Stanf HS1 HS2
Everything correct 7 2 3 3 1
Misidentify head noun 6 0 1 1 0
Misidentify verb 3 0 0 2 0
Miss relative clause construction 3 0 0 1 0
Table 3: Scores for relative clauses.
relative pronoun and the relative clause verb, and
a third which relates the relative pronoun to the
head noun. This third relation is not included in
any of the other schemes. Hypothetical Scheme 2
(HS2) involves only one relation, which includes
the same words as GR?s cmod relation; the repre-
sentation as a whole is quite compact since only
one dependency is involved and it includes all
three words.
Hypothetical Scheme 1
relative-subject(wrote, those)
subject(wrote, who)
relative-pronoun(those, who)
Hypothetical Scheme 2
relative-clause(wrote, those, who)
Table 3 shows the range of scores that can be at-
tained in the different schemes. The total possible
score varies from one for HS2, to three for Stan-
ford and HS1, and up to seven for PARC.
Observe that any of the three types of error in
Table 3 will immediately lose all points in both GR
and HS2. Since all the schemes use the same set
of words, this is due solely to the choice of rela-
tions and the compactness of the representations.
Neither GR nor HS2 allow for partial credit, even
when the parser assigns an essentially correct rel-
ative clause structure. This is a scenario which
could easily occur due to a head selection error.
For example, consider the following phrase from
the shared task GENIA (Kim et al, 2003) data set ,
Sentence 8:
. . . the RelA ( p65 ) subunit of NF-kappa
B , which activates transcription of the c-
rel gene . . .
The C&C parser correctly identifies the relative
clause structure, including the pronoun which and
the verb activates, but incorrectly identifies the
head noun as B instead of subunit.
Even between GR and HS2, which share the
characteristic of not allowing for partial credit,
there is a difference in scoring. Because GR starts
with two dependencies, there is a loss of two
points, rather than just one, for any error, which
means errors in relative clauses are weighted more
heavily in GR than in HS2.
Stanford also has a problematic redundancy,
since the nsubj and rcmod relations are mirror im-
ages of each other. It therefore duplicates the GR
characteristic of penalising the parser by at least
two points if either the head noun or the relative
clause verb is misidentified (in fact three points for
the verb).
Observe also the asymmetry between misidenti-
fying the head noun (one out of seven points lost in
PARC, two out of three lost in Stanford and HS1)
compared to misidentifying the verb (three points
lost in PARC, all three lost in Stanford, but only one
point lost in HS1). This reflects a difference be-
tween the schemes in whether the relative pronoun
enters into a relation with the subject, the verb, or
both.
5 Conclusion
In this paper we have shown how the design pro-
cess for a relational parser evaluation scheme can
be broken up into a number of decisions, and how
these decisions can significantly affect the scoring
mechanism for the scheme. Although we have fo-
cused in detail on three construction types, we be-
lieve the decisions involved are relevant to any lin-
guistic construction, although some decisions will
be more difficult than others for certain construc-
tions. A direct object construction, for example,
will normally be represented by a single relation
between a verbal head and a nominal head, and in-
deed this is so in all three schemes considered here.
This does not mean that the representation is triv-
ial, however. The choice of which heads will rep-
resent the construction is important. In addition,
Stanford distinguishes objects of prepositions from
objects of verbs, while PARC and GR collapse the
two into a single relation. Although part of speech
information can be used to distinguish the two, a
49
parser which produces PARC- or GR-style output
in this regard will lose points in Stanford without
some additional processing.
We have made no judgements about which deci-
sions are best in the evaluation scheme design pro-
cess. There are no easy answers to the questions
raised here, and it may be that different solutions
will suit different evaluation situations. We leave
these questions for the parsing community to de-
cide. This process may be aided by an empirical
study of how the decisions affect the scores given
to various parsers. For example, it might be use-
ful to know whether one parser could be made to
score significantly higher than another simply by
changing the way coordination is represented. We
leave this for future work.
References
Briscoe, Ted and John Carroll. 2006. Evaluating the
accuracy of an unlexicalized statistical parser on the
PARC DepBank. In Proceedings of the ACL-Coling
?06 Main Conf. Poster Session, pages 41?48, Syd-
ney, Austrailia.
Carroll, John, Ted Briscoe, and Antonio Sanfilippo.
1998. Parser evaluation: a survey and a new pro-
posal. In Proceedings of the 1st LREC Conference,
pages 447?454, Granada, Spain.
Clark, Stephen and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG
and log-linear models. Computational Linguistics,
33(4):493?552.
de Marneffe, Marie-Catherine, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of the 5th LREC Conference, pages
449?454, Genoa, Italy.
Kim, Jin-Dong, Tomoko Ohta, Yuka Teteisi, and
Jun?ichi Tsujii. 2003. GENIA corpus ? a seman-
tically annotated corpus for bio-textmining. Bioin-
formatics, 19:i180?i182.
King, Tracy H., Richard Crouch, Stefan Riezler, Mary
Dalrymple, and Ronald M. Kaplan. 2003. The
PARC 700 Dependency Bank. In Proceedings of the
4th International Workshop on Linguistically Inter-
preted Corpora, Budapest, Hungary.
Lin, Dekang. 1995. A dependency-based method for
evaluating broad-coverage parsers. In Proceedings
of IJCAI-95, pages 1420?1425, Montreal, Canada.
Pyysalo, Sampo, Filip Ginter, Juho Heimonen, Jari
Bj?orne, Jorma Boberg, Jouni J?arvinen, and Tapio
Salakoski. 2007. BioInfer: A corpus for information
extraction in the biomedical domain. BMC Bioinfor-
matics, 8:50.
50
Concrete Sentence Spaces for Compositional Distributional
Models of Meaning
Edward Grefenstette?, Mehrnoosh Sadrzadeh?, Stephen Clark?, Bob Coecke?, Stephen Pulman?
?Oxford University Computing Laboratory, ?University of Cambridge Computer Laboratory
firstname.lastname@comlab.ox.ac.uk, stephen.clark@cl.cam.ac.uk
Abstract
Coecke, Sadrzadeh, and Clark [3] developed a compositional model of meaning for distributional
semantics, in which each word in a sentence has a meaning vector and the distributional meaning of the
sentence is a function of the tensor products of the word vectors. Abstractly speaking, this function is the
morphism corresponding to the grammatical structure of the sentence in the category of finite dimensional
vector spaces. In this paper, we provide a concrete method for implementing this linear meaning map,
by constructing a corpus-based vector space for the type of sentence. Our construction method is based
on structured vector spaces whereby meaning vectors of all sentences, regardless of their grammatical
structure, live in the same vector space. Our proposed sentence space is the tensor product of two noun
spaces, in which the basis vectors are pairs of words each augmented with a grammatical role. This
enables us to compare meanings of sentences by simply taking the inner product of their vectors.
1 Background
Coecke, Sadrzadeh, and Clark [3] develop a mathematical framework for a compositional distributional
model of meaning, based on the intuition that syntactic analysis guides the semantic vector composition.
The setting consists of two parts: a formalism for a type-logical syntax and a formalism for vector space
semantics. Each word is assigned a grammatical type and a meaning vector in the space corresponding to
its type. The meaning of a sentence is obtained by applying the function corresponding to the grammatical
structure of the sentence to the tensor product of the meanings of the words in the sentence. Based on the
type-logic used, some words will have atomic types and some compound function types. The compound
types live in a tensor space where the vectors are weighted sums (i.e. superpositions) of the pairs of bases
from each space. Compound types are ?applied? to their arguments by taking inner products, in a similar
manner to how predicates are applied to their arguments in Montague semantics.
For the type-logic we use Lambek?s Pregroup grammars [7]. The use of pregoups is not essential, but
leads to a more elegant formalism, given its proximity to the categorical structure of vector spaces (see [3]).
A Pregroup is a partially ordered monoid where each element has a right and left cancelling element, referred
to as an adjoint. It can be seen as the algebraic counterpart of the cancellation calculus of Harris [6]. The
operational difference between a Pregroup and Lambek?s Syntactic Calculus is that, in the latter, the monoid
multiplication of the algebra (used to model juxtaposition of the types of the words) has a right and a left
adjoint, whereas in the pregroup it is the elements themselves which have adjoints. The adjoint types are
used to denote functions, e.g. that of a transitive verb with a subject and object as input and a sentence as
output. In the Pregroup setting, these function types are still denoted by adjoints, but this time the adjoints
of the elements themselves.
As an example, consider the sentence ?dogs chase cats?. We assign the type n (for noun phrase) to ?dog?
and ?cat?, and nrsnl to ?chase?, where nr and nl are the right and left adjoints of n and s is the type of a
125
(declarative) sentence. The type nrsnl expresses the fact that the verb is a predicate that takes two arguments
of type n as input, on its right and left, and outputs the type s of a sentence. The parsing of the sentence is
the following reduction:
n(nrsnl)n ? 1s1 = s
This parse is based on the cancellation of n and nr, and also nl and n; i.e. nnr ? 1 and nln ? 1 for 1
the unit of juxtaposition. The reduction expresses the fact that the juxtapositions of the types of the words
reduce to the type of a sentence.
On the semantic side, we assign the vector space N to the type n, and the tensor space N ?S?N to the
type nrsnl. Very briefly, and in order to introduce some notation, recall that the tensor space A?B has as a
basis the cartesian product of a basis of A with a basis of B. Recall also that any vector can be expressed as
a weighted sum of basis vectors; e.g. if (??v1 , . . . ,??vn) is a basis of A then any vector ??a ? A can be written as
??a =?i Ci??vi where each Ci ? R is a weighting factor. Now for (??v1 , . . . ,??vn) a basis of A and (
??
v?1 , . . . ,
??
v?n)
a basis of B, a vector ??c in the tensor space A ? B can be expressed as follows:
?
ij
Cij (??vi ?
??
v?j )
where the tensor of basis vectors ??vi ?
??
v?j stands for their pair (??vi ,
??
v?j ). In general ??c is not separable into
the tensor of two vectors, except for the case when ??c is not entangled. For non-entangled vectors we can
write ??c = ??a ???b for ??a =
?
i Ci
??vi and
??b =
?
j C ?j
??
v?j ; hence the weighting factor of ??c can be obtained
by simply multiplying the weights of its tensored counterparts, i.e. Cij = Ci ? C ?j . In the entangled case
these weights cannot be determined as such and range over all the possibilities. We take advantage of this
fact to encode meanings of verbs, and in general all words that have compound types and are interpreted as
predicates, relations, or functions. For a brief discussion see the last paragraph of this section. Finally, we
use the Dirac notation to denote the dot or inner product of two vectors ???a | ??b ? ? R defined by
?
i Ci?C ?i.
Returning to our example, for the meanings of nouns we have
???
dogs,??cats ? N , and for the meanings of
verbs we have
????
chase ? N ? S ? N , i.e. the following superposition:
?
ijk
Cijk (??ni ???sj ???nk)
Here ??ni and ??nk are basis vectors of N and ??sj is a basis vector of S. From the categorical translation method
presented in [3] and the grammatical reduction n(nrsnl)n ? s, we obtain the following linear map as the
categorical morphism corresponding to the reduction:
N ? 1s ? N : N ? (N ? S ? N)? N ? S
Using this map, the meaning of the sentence is computed as follows:
???????????
dogs chase cats = (N ? 1s ? N )
(???
dogs ?????chase ???cats
)
= (N ? 1s ? N )
?
?
???
dogs ?
?
?
?
ijk
Cijk(??ni ???sj ???nk)
?
????cats
?
?
=
?
ijk
Cijk?
???
dogs | ??ni???sj ???nk | ??cats?
The key features of this operation are, first, that the inner-products reduce dimensionality by ?consuming?
tensored vectors and by virtue of the following component function:
N : N ? N ? R :: ??a ?
??b 7? ???a | ??b ?
126
Thus the tensored word vectors
???
dogs ? ????chase ? ??cats are mapped into a sentence space S which is common
to all sentences regardless of their grammatical structure or complexity. Second, note that the tensor product???
dogs?????chase???cats does not need to be calculated, since all that is required for computation of the sentence
vector are the noun vectors and the Cijk weights for the verb. Note also that the inner product operations
are simply picking out basis vectors in the noun space, an operation that can be performed in constant
time. Hence this formalism avoids two problems faced by approaches in the vein of [9, 2], which use
the tensor product as a composition operation: first, that the sentence meaning space is high dimensional
and grammatically different sentences have representations with different dimensionalities, preventing them
from being compared directly using inner products; and second, that the space complexity of the tensored
representation grows exponentially with the length and grammatical complexity of the sentence. In constrast,
the model we propose does not require the tensored vectors being combined to be represented explicitly.
Note that we have taken the vector of the transitive verb, e.g.
????
chase, to be an entangled vector in the
tensor space N ? S ?N . But why can this not be a separable vector, in which case the meaning of the verb
would be as follows:
????
chase =
?
i
Ci??ni ?
?
j
C ?j??sj ?
?
k
C ??k??nk
The meaning of the sentence would then become ?1?2
?
j C ?j
??sj for ?1 =
?
i Ci?
???
dogs | ??ni? and ?2 =
?
k C ??k ?
??
cats | ??nk?. The problem is that this meaning only depends on the meaning of the verb and is
independent of the meanings of the subject and object, whereas the meaning from the entangled case,
i.e. ?1?2
?
ijk Cijk
??sj , depends on the meanings of subject and object as well as the verb.
2 From Truth-Theoretic to Corpus-based Meaning
The model presented above is compositional and distributional, but still abstract. To make it concrete, N and
S have to be constructed by providing a method for determining the Cijk weightings. Coecke, Sadrzadeh,
and Clark [3] show how a truth-theoretic meaning can be derived in the compositional framework. For
example, assume that N is spanned by all animals and S is the two-dimensional space spanned by ??true and???
false. We use the weighting factor to define a model-theoretic meaning for the verb as follows:
Cijk??sj =
{??
true chase(??ni ,??nk) = true ,???
false o.w.
The definition of our meaning map ensures that this value propagates to the meaning of the whole sentence.
So chase(???dogs,???cats) becomes true whenever ?dogs chase cats? is true and false otherwise. This is exactly
how meaning is computed in the model-theoretic view on semantics. One way to generalise this truth-
theoretic meaning is to assume that chase(??ni ,??nk) has degrees of truth, for instance by defining chase as a
combination of run and catch, such as:
chase = 23run+
1
3catch
Again, the meaning map ensures that these degrees propagate to the meaning of the whole sentence. For a
worked out example see [3]. But neither of these examples provide a distributional sentence meaning.
Here we take a first step towards a corpus-based distributional model, by attempting to recover a meaning
for a sentence based on the meanings of the words derived from a corpus. But crucially this meaning goes
beyond just composing the meanings of words using a vector operator, such as tensor product, summation
or multiplication [8]. Our computation of sentence meaning treats some vectors as functions and others as
127
function arguments, according to how the words in the sentence are typed, and uses the syntactic structure
as a guide to determine how the functions are applied to their arguments. The intuition behind this approach
is that syntactic analysis guides semantic vector composition.
The contribution of this paper is to introduce some concrete constructions for a compositional distri-
butional model of meaning. These constructions demonstrate how the mathematical model of [3] can be
implemented in a concrete setting which introduces a richer, not necessarily truth-theoretic, notion of natural
language semantics which is closer to the ideas underlying standard distributional models of word meaning.
We leave full evaluation to future work, in order to determine whether the following method in conjunction
with word vectors built from large corpora leads to improved results on language processing tasks, such as
computing sentence similarity and paraphrase evaluation.
Nouns and Transitive Verbs. We take N to be a structured vector space, as in [4, 5]. The bases of N are
annotated by ?properties? obtained by combining dependency relations with nouns, verbs and adjectives. For
example, basis vectors might be associated with properties such as ?arg-fluffy?, denoting the argument of
the adjective fluffy, ?subj-chase? denoting the subject of the verb chase, ?obj-buy? denoting the object of the
verb buy, and so on. We construct the vector for a noun by counting how many times in the corpus a word
has been the argument of ?fluffy?, the subject of ?chase?, the object of ?buy?, and so on.
The framework in [3] offers no guidance as to what the sentence space should consist of. Here we take
the sentence space S to be N ? N , so its bases are of the form ??sj = (??ni ,??nk). The intuition is that, for a
transitive verb, the meaning of a sentence is determined by the meaning of the verb together with its subject
and object.1 The verb vectors Cijk(??ni ,??nk) are built by counting how many times a word that is ni (e.g. has
the property of being fluffy) has been subject of the verb and a word that is nk (e.g. has the property that it?s
bought) has been its object, where the counts are moderated by the extent to which the subject and object
exemplify each property (e.g. how fluffy the subject is). To give a rough paraphrase of the intuition behind
this approach, the meaning of ?dog chases cat? is given by: the extent to which a dog is fluffy and a cat is
something that is bought (for the N ? N property pair ?arg-fluffy? and ?obj-buy?), and the extent to which
fluffy things chase things that are bought (accounting for the meaning of the verb for this particular property
pair); plus the extent to which a dog is something that runs and a cat is something that is cute (for the N ?N
pair ?subj-run? and ?arg-cute?), and the extent to which things that run chase things that are cute (accounting
for the meaning of the verb for this particular property pair); and so on for all noun property pairs.
Adjective Phrases. Adjectives are dealt with in a similar way. We give them the syntactic type nnl and
build their vectors in N ? N . The syntactic reduction nnln ? n associated with applying an adjective to a
noun gives us the map 1N ? N by which we semantically compose an adjective with a noun, as follows:
?????
red fox = (1N ? N )(
??
red ???fox) =
?
ij
Cij??ni???nj |
??
fox?
We can view the Cij counts as determining what sorts of properties the arguments of a particular adjective
typically have (e.g. arg-red, arg-colourful for the adjective ?red?).
Prepositional Phrases. We assign the type nrn to the whole prepositional phrase (when it modifies a noun),
for example to ?in the forest? in the sentence ?dogs chase cats in the forest?. The pregroup parsing is as
follows:
n(nrsnl)n(nrn) ? 1snl1n ? snln ? s1 = s
The vector space corresponding to the prepositional phrase will thus be the tensor space N ? N and the
categorification of the parse will be the composition of two morphisms: (1S?lN )?(rN?1S?1N?rN?1N ).
1Intransitive and ditransitive verbs are interpreted in an analagous fashion; see ?4.
128
The substitution specific to the prepositional phrase happens when computing the vector for ?cats in the
forest? as follows:
?????????????
cats in the forest = (rN ? 1N )
(??
cats ??????????in the forest
)
= (rN ? 1N )
(
??
cats ?
?
lw
Clw??nl ???nk
)
=
?
lw
Clw???cats | ??nl???nw
Here we set the weights Clw in a similar manner to the cases of adjective phrases and verbs with the counts
determining what sorts of properties the noun modified by the prepositional phrase has, e.g. the number of
times something that has attribute nl has been in the forest.
Adverbs. We assign the type srs to the adverb, for example to ?quickly? in the sentence ?Dogs chase cats
quickly?. The pregroup parsing is as follows:
n(nrsnl)n(srs) ? 1s1srs = ssrs ? 1s = s
Its categorification will be a composition of two morphisms (rS ? 1S) ? (rN ? 1S ? lN ? 1S ? 1S). The
substitution specific to the adverb happens after computing the meaning of the sentence without it, i.e. that
of ?Dogs chase cats?, and is as follows:
??????????????????
Dogs chase cats quickly = (rS ? 1S) ? (rN ? 1S ? lN ? 1S ? 1S)
(???
Dogs ?????chase ???cats ??????quickly
)
= (rS ? 1S)
?
?
?
ijk
Cijk?
???
dogs | ??ni???sj ???nk | ??cats? ?
?????
quickly
?
?
= (rS ? 1S)
?
?
?
ijk
Cijk?
???
dogs | ??ni???sj ???nk | ??cats? ?
?
lw
Clw??sl ???sw
?
?
=
?
lw
Clw
?
?
ijk
Cijk?
???
dogs | ??ni???sj ???nk | ??cats? | ??sl
?
??sk
The Clw weights are defined in a similar manner to the above cases, i.e. according to the properties the
adverb has, e.g. which verbs it has modified. Note that now the basis vectors ??sl and ??sw are themselves pairs
of basis vectors from the noun space, (??ni ,??nj). Hence, Clw(??ni ,??nj) can be set only for the case when l = i
and w = j; these counts determine what sorts of properties the verbs that happen quickly have (or more
specifically what properties the subjects and objects of such verbs have). By taking the whole sentence into
account in the interpretation of the adverb, we are in a better position to semantically distinguish between
the meaning of adverbs such as ?slowly? and ?quickly?, for instance in terms of the properties that the verb?s
subjects have. For example, it is possible that elephants are more likely to be the subject of a verb which is
happening slowly, e.g. run slowly, and cheetahs are more likely to be the subject of a verb which is happening
quickly.
3 Concrete Computations
In this section we first describe how to obtain the relevant counts from a parsed corpus, and then give some
similarity calculations for some example sentence pairs.
129
Let Cl be the set of grammatical relations (GRs) for sentence sl in the corpus. Define verbs(Cl) to be
the function which returns all instances of verbs in Cl, and subj (and similarly obj ) to be the function which
returns the subject of an instance Vinstance of a verb V , for a particular set of GRs for a sentence:
subj(Vinstance) =
{
noun if Vinstance is a verb with subject noun
?n o.w.
where ?n is the empty string. We express Cijk for a verb V as follows:
Cijk =
{
?
l
?
v?verbs(Cl) ?(v, V )?
??????
subj(v) | ??ni??
?????
obj(v) | ??nk? if ??sj = (??ni ,??nk)
0 o.w.
where ?(v, V ) = 1 if v = V and 0 otherwise. Thus we construct Cijk for verb V only for cases where
the subject property ni and the object property nk are paired in the basis ??sj . This is done by counting the
number of times the subject of V has property ni and the object of V has property nk, then multiplying them,
as prescribed by the inner products (which simply pick out the properties ni and nk from the noun vectors
for the subjects and objects).
The procedure for calculating the verb vectors, based on the formulation above, is as follows:
1. For each GR in a sentence, if the relation is subject and the head is a verb, then find the complementary
GR with object as a relation and the same head verb. If none, set the object to ?n.
2. Retrieve the noun vectors
?????subject,????object for the subject dependent and object dependent from previ-
ously constructed noun vectors.
3. For each (ni, nk) ? basis(N)? basis(N) compute the inner-product of ??ni with
?????subject and ??nk with????object (which involves simply picking out the relevant basis vectors from the noun vectors). Multiply
the inner-products and add this to Cijk for the verb, with j such that ??sj = (??ni ,??nk).
The procedure for other grammatical types is similar, based on the definitions of C weights for the semantics
of these types.
We now give a number of example calculations. We first manually define the distributions for nouns,
which in practice would be obtained from a corpus:
bankers cats dogs stock kittens
1. arg-fluffy 0 7 3 0 2
2. arg-ferocious 4 1 6 0 0
3. obj-buys 0 4 2 7 0
4. arg-shrewd 6 3 1 0 1
5. arg-valuable 0 1 2 8 0
We aim to make these counts match our intuitions, in that bankers are shrewd and a little ferocious but not
furry, cats are furry but not typically valuable, and so on.
We also define the distributions for the transitive verbs ?chase?, ?pursue? and ?sell?, again manually
specified according to our intuitions about how these verbs are used. Since in the formalism proposed above,
Cijk = 0 if ??sj 6= (??ni ,??nk), we can simplify the weight matrices for transitive verbs to two dimensional Cik
matrices as shown below, where Cik corresponds to the number of times the verb has a subject with attribute
ni and an object with attribute nk. For example, the matrix below encodes the fact that something ferocious
130
(i = 2) chases something fluffy (k = 1) seven times in the hypothetical corpus from which we might have
obtained these distributions.
Cchase =
?
?
?
?
?
?
1 0 0 0 0
7 1 2 3 1
0 0 0 0 0
2 0 1 0 1
1 0 0 0 0
?
?
?
?
?
?
Cpursue =
?
?
?
?
?
?
0 0 0 0 0
4 2 2 2 4
0 0 0 0 0
3 0 2 0 1
0 0 0 0 0
?
?
?
?
?
?
Csell =
?
?
?
?
?
?
0 0 0 0 0
0 0 3 0 4
0 0 0 0 0
0 0 5 0 8
0 0 1 0 1
?
?
?
?
?
?
These matrices can be used to perform sentence comparisons:
????????????dogs chase cats | ??????????????dogs pursue kittens? =
=
?
?
?
?
ijk
Cchaseijk ?
???
dogs | ??ni???sj ???nk | ??cats?
?
?
?
?
?
?
?
?
?
?
?
ijk
Cpursueijk ?
???
dogs | ??ni???sj ???nk |
?????
kittens?
?
?
?
=
?
ijk
Cchaseijk C
pursue
ijk ?
???
dogs | ??ni??
???
dogs | ??ni????nk | ??cats????nk |
?????
kittens?
The raw number obtained from the above calculation is 14844. Normalising it by the product of the length
of both sentence vectors gives the cosine value of 0.979.
Consider now the sentence comparison ????????????dogs chase cats | ???????????cats chase dogs?. The sentences in this pair
contain the same words but the different word orders give the sentences very different meanings. The raw
number calculated from this inner product is 7341, and its normalised cosine measure is 0.656, which demon-
strates the sharp drop in similarity obtained from changing sentence structure. We expect some similarity
since there is some non-trivial overlap between the properties identifying cats and those identifying dogs
(namely those salient to the act of chasing).
Our final example for transitive sentences is ????????????dogs chase cats | ????????????bankers sell stock?, as two sentences that
diverge in meaning completely. The raw number for this inner product is 6024, and its cosine measure is
0.042, demonstrating the very low semantic similarity between these two sentences.
Next we consider some examples involving adjective-noun modification. The Cij counts for an adjective
A are obtained in a similar manner to transitive or intransitive verbs:
Cij =
{
?
l
?
a?adjs(Cl) ?(a,A)?
???????
arg-of(a) | ??ni? if ??ni = ??nj
0 o.w.
where adjs(Cl) returns all instances of adjectives in Cl; ?(a,A) = 1 if a = A and 0 otherwise; and
arg-of(a) = noun if a is an adjective with argument noun, and ?n otherwise.
As before, we stipulate the Cij matrices by hand (and we eliminate all cases where i 6= j since Cij = 0
by definition in such cases):
Cfluffy = [9 3 4 2 2] Cshrewd = [0 3 1 9 1] Cvaluable = [3 0 8 1 8]
We compute vectors for ?fluffy dog? and ?shrewd banker? as follows:
???????
fluffy dog = (3 ? 9)???????arg-fluffy+ (6 ? 3)?????????arg-ferocious+ (2 ? 4)??????obj-buys+ (5 ? 2)????????arg-shrewd+ (2 ? 2)?????????arg-valuable
???????????
shrewd banker = (0 ? 0)???????arg-fluffy+ (4 ? 3)?????????arg-ferocious+ (0 ? 0)??????obj-buys+ (6 ? 9)????????arg-shrewd+ (0 ? 1)?????????arg-valuable
Vectors for
???????
fluffy cat and
??????????
valuable stock are computed similarly. We obtain the following similarity mea-
sures:
cosine(???????fluffy dog,???????????shrewd banker) = 0.389 cosine(???????fluffy cat,??????????valuable stock) = 0.184
131
These calculations carry over to sentences which contain the adjective-noun pairings compositionally and
we obtain an even lower similarity measure between sentences:
cosine(????????????????????fluffy dogs chase fluffy cats,?????????????????????????shrewd bankers sell valuable stock) = 0.016
To summarise, our example vectors provide us with the following similarity measures:
Sentence 1 Sentence 2 Degree of similarity
dogs chase cats dogs pursue kittens 0.979
dogs chase cats cats chase dogs 0.656
dogs chase cats bankers sell stock 0.042
fluffy dogs chase fluffy cats shrewd bankers sell valuable stock 0.016
4 Different Grammatical Structures
So far we have only presented the treatment of sentences with transitive verbs. For sentences with intransitive
verbs, the sentence space suffices to be just N . To compare the meaning of a transitive sentence with an
intransitive one, we embed the meaning of the latter from N into the former N ? N , by taking ???n (the
?object? of an intransitive verb) to be
?
i
??ni , i.e. the superposition of all basis vectors of N .
Following the method for the transitive verb, we calculate Cijk for an instransitive verb V and basis pair??sj = (??ni ,??nk) as follows, where l ranges over the sentences in the corpus:
?
l
?
v?verbs(Cl)
?(v, V )?
??????
subj(v) | ??ni??
?????
obj(v) | ??nk? =
?
l
?
v?verbs(Cl)
?(v, V )?
??????
subj(v) | ??ni?????n | ??nk?
and ????n | ??ni? = 1 for any basis vector ni.
We can now compare the meanings of transitive and intransitive sentences by taking the inner product of
their meanings (despite the different arities of the verbs) and then normalising it by vector length to obtain
the cosine measure. For example:
????????????dogs chase cats | ????????dogs chase? =
?
?
?
?
ijk
Cijk?
???
dogs | ??ni???sj ???nk | ???cats ?
?
?
?
?
?
?
?
?
?
?
?
ijk
C ?ijk?
???
dogs | ??ni???sj
?
?
?
=
?
ijk
CijkC ?ijk?
???
dogs | ??ni??
???
dogs | ??ni????nk | ??cats?
The raw number for the inner product is 14092 and its normalised cosine measure is 0.961, indicating high
similarity (but some difference) between a sentence with a transitive verb and one where the subject remains
the same, but the verb is used intransitively.
Comparing sentences containing nouns modified by adjectives to sentences with unmodified nouns is straight-
forward:
?????????????????????fluffy dogs chase fluffy cats | ???????????dogs chase cats? =
?
ij
Cfluffyi C
fluffy
j Cchaseij Cchaseij ?
???dogs | ??ni?2???nj |
???cats?2 = 2437005
132
From the above we obtain the following similarity measure:
cosine(????????????????????fluffy dogs chase fluffy cats,???????????dogs chase cats) = 0.971
For sentences with ditransitive verbs, the sentence space changes to N ? N ? N , on the basis of the verb
needing two objects; hence its grammatical type changes to nrsnlnl. The transitive and intransitive verbs
are embedded in this larger space in a similar manner to that described above; hence comparison of their
meanings becomes possible.
5 Ambiguous Words
The two different meanings of a word can be distinguished by the different properties that they have. These
properties are reflected in the corpus, by the different contexts in which the words appear. Consider the
following example from [4]: the verb ?catch? has two different meanings, ?grab? and ?contract?. They are
reflected in the two sentences ?catch a ball? and ?catch a disease?. The compositional feature of our meaning
computation enables us to realise the different properties of the context words via the grammatical roles they
take in the corpus. For instance, the word ?ball? occurs as argument of ?round?, and so has a high weight
for the base ?arg-round?, whereas the word ?disease? has a high weight for the base ?arg-contagious? and as
?mod-of-heart?. We extend our example corpus from previously to reflect these differences as follows:
ball disease
1. arg-fluffy 1 0
2. arg-ferocious 0 0
3. obj-buys 5 0
4. arg-shrewd 0 0
5. arg-valuable 1 0
6. arg-round 8 0
7. arg-contagious 0 7
8. mod-of-heart 0 6
In a similar way, we build a matrix for the verb ?catch? as follows:
Ccatch =
?
?
?
?
?
?
?
?
?
?
?
?
3 2 3 3 3 8 6 2
3 2 3 0 1 4 7 4
2 4 7 1 1 6 2 2
3 1 2 0 0 3 6 2
1 1 1 0 0 2 0 1
0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0
?
?
?
?
?
?
?
?
?
?
?
?
The last three rows are zero because we have assumed that the words that can take these roles are mostly
objects and hence cannot catch anything. Given these values, we compute the similarity measure between
the two sentences ?dogs catch a ball? and ?dogs catch a disease? as follows:
?????????????dogs catch a ball | ??????????????dogs catch a disease? = 0
In an idealised case like this where there is very little (or no) overlap between the properties of the objects
associated with one sense of ?catch? (e.g. a disease), and those properties of the objects associated with an-
other sense (e.g. a ball), disambiguation is perfect in that there is no similarity between the resulting phrases.
133
In practice, in richer vector spaces, we would expect even diseases and balls to share some properties. How-
ever, as long as those shared properties are not those typically held by the object of catch, and as long as the
usages of catch play to distinctive properties of diseases and balls, disambiguation will occur by the same
mechanism as the idealised case above, and we can expect low similarity measures between such sentences.
6 Related Work
Mitchell and Lapata introduce and evaluate a multiplicative model for vector composition [8]. The particular
concrete construction of this paper differs from that of [8] in that our framework subsumes truth-theoretic
as well as corpus-based meaning, and our meaning construction relies on and is guided by the grammatical
structure of the sentence. The approach of [4] is more in the spirit of ours, in that extra information about
syntax is used to compose meaning. Similar to us, they use a structured vector space to integrate lexical
information with selectional preferences. Finally, Baroni and Zamparelli model adjective-noun combinations
by treating an adjective as a function from noun space to noun space, represented using a matrix, as we do
in this paper [1].
References
[1] M. Baroni and R. Zamparelli. Nouns are vectors, adjectives are matrices: Representing adjective-noun construc-
tions in semantic space. In Conference on Empirical Methods in Natural Language Processing (EMNLP-10),
Cambridge, MA, 2010.
[2] S. Clark and S. Pulman. Combining symbolic and distributional models of meaning. In Proceedings of AAAI
Spring Symposium on Quantum Interaction. AAAI Press, 2007.
[3] B. Coecke, M. Sadrzadeh, and S. Clark. Mathematical Foundations for a Compositional Dis-
tributional Model of Meaning, volume 36. Linguistic Analysis (Lambek Festschrift), 2010.
http://arxiv.org/abs/1003.4394.
[4] K. Erk and S. Pado?. A structured vector space model for word meaning in context. In Conference on Empirical
Methods in Natural Language Processing (EMNLP-08), pages 897?906, Honolulu, Hawaii, 2008.
[5] G. Grefenstette. Use of syntactic context to produce term association lists for text retrieval. In Nicholas J. Belkin,
Peter Ingwersen, and Annelise Mark Pejtersen, editors, SIGIR, pages 89?97. ACM, 1992.
[6] Z. Harris. Mathematical Structures of Language. Interscience Publishers John Wiley and Sons, 1968.
[7] J. Lambek. From Word to Sentence. Polimetrica, 2008.
[8] J. Mitchell and M. Lapata. Vector-based models of semantic composition. In Proceedings of the 46th Annual
Meeting of the Association for Computational Linguistics, pages 236?244, Columbus, OH, 2008.
[9] P. Smolensky and G. Legendre. The Harmonic Mind: From Neural Computation to Optimality-Theoretic Grammar
Vol. I: Cognitive Architecture Vol. II: Linguistic and Philosophical Implications. MIT Press, 2005.
134
Proceedings of the 13th Meeting on the Mathematics of Language (MoL 13), pages 41?51,
Sofia, Bulgaria, August 9, 2013. c?2013 Association for Computational Linguistics
The Frobenius Anatomy of Relative Pronouns
Stephen Clark
University of Cambridge
Computer Laboratory
sc609@cam.ac.uk
Bob Coecke
University of Oxford
Dept. of Computer Science
coecke@cs.ox.ac.uk
Mehrnoosh Sadrzadeh
Queen Mary, University of London
School of Electronic
Engineering and Computer Science
mehrnoosh.sadrzadeh@eecs.qmul.ac.uk
Abstract
This paper develops a compositional
vector-based semantics of relative pro-
nouns within a categorical framework.
Frobenius algebras are used to formalise
the operations required to model the se-
mantics of relative pronouns, including
passing information between the relative
clause and the modified noun phrase, as
well as copying, combining, and discard-
ing parts of the relative clause. We de-
velop two instantiations of the abstract se-
mantics, one based on a truth-theoretic ap-
proach and one based on corpus statistics.
1 Introduction
Ordered algebraic structures and sequent calculi
have been used extensively in Computer Science
and Mathematical Logic. They have also been
used to formalise and reason about natural lan-
guage. Lambek (1958) used the ordered alge-
bra of residuated monoids to model grammatical
types, their juxtapositions and reductions. Rela-
tional words such as verbs have implicative types
and are modelled using the residuals to the monoid
multiplication. Later, Lambek (1999) simplified
these algebras in favour of pregroups. Here, there
are no binary residual operations, but each element
of the algebra has a left and a right residual.
In terms of semantics, pregroups do not natu-
rally lend themselves to a model-theoretic treat-
ment (Montague, 1974). However, pregroups are
suited to a radically different treatment of seman-
tics, namely distributional semantics (Schu?tze,
1998). Distributional semantics uses vector spaces
based on contextual co-occurrences to model the
meanings of words. Coecke et al (2010) show
how a compositional semantics can be developed
within a vector-based framework, by exploiting
the fact that vector spaces with linear maps and
pregroups both have a compact closed categor-
ical structure (Kelly and Laplaza, 1980; Preller
and Lambek, 2007). Some initial attempts at im-
plementation include Grefenstette and Sadrzadeh
(2011a) and Grefenstette and Sadrzadeh (2011b).
One problem with the distributional approach is
that it is difficult to see how the meanings of some
words ? e.g. logical words such as and, or, and
relative pronouns such as who, which, that, whose
? can be modelled contextually. Our focus in this
paper is on relative pronouns in the distributional
compositional setting.
The difficulty with pronouns is that the contexts
in which they occur do not seem to provide a suit-
able representation of their meanings: pronouns
tend to occur with a great many nouns and verbs.
Hence, if one applies the contextual co-occurrence
methods of distributional semantics to them, the
result will be a set of dense vectors which do
not discriminate between different meanings. The
current state-of-the-art in compositional distribu-
tional semantics either adopts a simple method to
obtain a vector for a sequence of words, such as
adding or mutliplying the contextual vectors of
the words (Mitchell and Lapata, 2008), or, based
on the grammatical structure, builds linear maps
for some words and applies these to the vector
representations of the other words in the string
(Baroni and Zamparelli, 2010; Grefenstette and
Sadrzadeh, 2011a). Neither of these approaches
produce vectors which provide a good representa-
tion for the meanings of relative clauses.
In the grammar-based approach, one has to as-
sign a linear map to the relative pronoun, for in-
stance a map f as follows:
??????????????
men who like Mary = f(???men,
???????
like Mary)
However, it is not clear what this map should be.
Ideally, we do not want it to depend on the fre-
quency of the co-occurrence of the relative pro-
noun with the relevant basis vectors. But both
41
of the above mentioned approaches rely heavily
on the information provided by a corpus to build
their linear maps. The work of Baroni and Zam-
parelli (2010) uses linear regression and approxi-
mates the context vectors of phrases in which the
target word has occurred, and the work of Grefen-
stette and Sadrzadeh (2011a) uses the sum of Kro-
necker products of the arguments of the target
word across the corpus.
The semantics we develop for relative pronouns
and clauses uses the general operations of a Frobe-
nius algebra over vector spaces (Coecke et al,
2008) and the structural categorical morphisms of
vector spaces. We do not rely on the co-occurrence
frequencies of the pronouns in a corpus and only
take into account the structural roles of the pro-
nouns in the meaning of the clauses. The computa-
tions of the algebra and vector spaces are depicted
using string diagrams (Joyal and Street, 1991),
which depict the interactions that occur among the
words of a sentence. In the particular case of rel-
ative clauses, they visualise the role of the rela-
tive pronoun in passing information between the
clause and the modified noun phrase, as well as
copying, combining, and even discarding parts of
the relative clause.
We develop two instantiations of the abstract se-
mantics, one based on a truth-theoretic approach,
and one based on corpus statistics, where for the
latter the categorical operations are instantiated as
matrix multiplication and vector component-wise
multiplication. As a result, we will obtain the fol-
lowing for the meaning of a subject relative clause:
??????????????
men who like Mary = ???men (love?
????
Mary)
The rest of the paper introduces the categorical
framework, including the formal definitions rel-
evant to the use of Frobenius algebras, and then
shows how these structures can be used to model
relative pronouns within the compositional vector-
based setting.
2 Compact Closed Categories and
Frobenius Algebras
This section briefly reviews compact closed cate-
gories and Frobenius algebras. For a formal pre-
sentation, see (Kelly and Laplaza, 1980; Kock,
2003; Baez and Dolan, 1995), and for an informal
introduction see Coecke and Paquette (2008).
A compact closed category has objects A,B;
morphisms f : A ? B; a monoidal tensor A? B
that has a unit I; and for each objectA two objects
Ar andAl together with the following morphisms:
A?Ar
rA?? I
?rA?? Ar ?A
Al ?A
lA?? I
?lA?? A?Al
These morphisms satisfy the following equalities,
sometimes referred to as the yanking equalities,
where 1A is the identity morphism on object A:
(1A ? 
l
A) ? (?
l
A ? 1A) = 1A
(rA ? 1A) ? (1A ? ?
r
A) = 1A
(lA ? 1A) ? (1Al ? ?
l
A) = 1Al
(1Ar ? 
r
A) ? (?
r
A ? 1Ar) = 1Ar
A pregroup is a partial order compact closed
category, which we refer to as Preg. This means
that the objects of Preg are elements of a par-
tially ordered monoid, and between any two ob-
jects p, q ? Preg there exists a morphism of type
p ? q iff p ? q. Compositions of morphisms
are obtained by transitivity and the identities by
reflexivity of the partial order. The tensor of the
category is the monoid multiplication, and the ep-
silon and eta maps are as follows:
rp = p ? p
r ? 1 ?rp = 1 ? p
r ? p
lp = p
l ? p ? 1 ?lp = 1 ? p ? p
l
Finite dimensional vector spaces and linear
maps also form a compact closed category, which
we refer to as FVect. Finite dimensional vector
spaces V,W are objects of this category; linear
maps f : V ? W are its morphisms with compo-
sition being the composition of linear maps. The
tensor product V ?W is the linear algebraic ten-
sor product, whose unit is the scalar field of vec-
tor spaces; in our case this is the field of reals R.
As opposed to the tensor product in Preg, the ten-
sor between vector spaces is symmetric; hence we
have a naturual isomorphism V ?W ?= W ? V .
As a result of the symmetry of the tensor, the two
adjoints reduce to one and we obtain the following
isomorphism:
V l ?= V r ?= V ?
where V ? is the dual of V . When the basis vectors
of the vector spaces are fixed, it is further the case
that the following isomorphism holds as well:
V ? ?= V
42
Elements of vector spaces, i.e. vectors, are rep-
resented by morphisms from the unit of tensor to
their corresponding vector space; that is??v ? V is
represented by the morphism R
??v
?? V ; by linear-
ity this morphism is uniquely defined when setting
1 7? ??v .
Given a basis {ri}i for a vector space V , the ep-
silon maps are given by the inner product extended
by linearity; i.e. we have:
l = r : V ? ? V ? R
given by:
?
ij
cij ?i ? ?j 7?
?
ij
cij??i | ?j?
Similarly, eta maps are defined as follows:
?l = ?r : R? V ? V ?
and are given by:
1 7?
?
i
ri ? ri
A Frobenius algebra in a monoidal category
(C,?, I) is a tuple (X,?, ?, ?, ?) where, for X
an object of C, the triple (X,?, ?) is an internal
comonoid; i.e. the following are coassociative and
counital morphisms of C:
?: X ? X ?X ? : X ? I
Moreover (X,?, ?) is an internal monoid; i.e. the
following are associative and unital morphisms:
? : X ?X ? X ? : I ? X
And finally the ? and ?morphisms satisfy the fol-
lowing Frobenius condition:
(?? 1X) ? (1X ??) = ? ? ? = (1X ? ?) ? (?? 1X)
Informally, the comultiplication ? decomposes
the information contained in one object into two
objects, and the multiplication ? combines the in-
formation of two objects into one.
Frobenius algebras were originally introduced
in the context of representation theorems for group
theory (Frobenius, 1903). Since then, they have
found applications in other fields of mathematics
and physics, e.g. in topological quantum field the-
ory (Kock, 2003). The above general categorical
definition is due to Carboni and Walters (1987). In
what follows, we use Frobenius algebras that char-
acterise vector space bases (Coecke et al, 2008).
In the category of finite dimensional vector
spaces and linear maps FVect, any vector space V
with a fixed basis {??vi}i has a Frobenius algebra
over it, explicitly given by:
? :: ??vi 7?
??vi ?
??vi ? ::
??vi 7? 1
? :: ??vi ?
??vj 7? ?ij
??vi ? :: 1 7?
?
i
??vi
where ?ij is the Kronecker delta.
Frobenius algebras over vector spaces with or-
thonormal bases are moreover isometric and com-
mutative. A commutative Frobenius Algebra satis-
fies the following two conditions for ? : X?Y ?
Y ?X , the symmetry morphism of (C,?, I):
? ?? = ? ? ? ? = ?
An isometric Frobenius Algebra is one that satis-
fies the following axiom:
? ?? = 1
The vector spaces of distributional models have
fixed orthonormal bases; hence they have isomet-
ric commutative Frobenius algebras over them.
The comultiplication ? of an isometric com-
mutative Frobenius Algebra over a vector space
encodes vectors of lower dimensions into vectors
of higher dimensional tensor spaces; this oper-
ation is referred to as copying. In linear alge-
braic terms, ?(??v ) ? V ? V is a diagonal matrix
whose diagonal elements are weights of ??v ? V .
The corresponding multiplication ? encodes vec-
tors of higher dimensional tensor spaces into lower
dimensional spaces; this operation is referred to
as combining. For ??w ? V ? V , we have that
?(??w ) ? V is a vector consisting only of the diag-
onal elements of ??w .
As a concrete example, take V to be a two di-
mensional space with basis {??v1 ,
??v2}; then the ba-
sis of V ?V is {??v1?
??v1 ,
??v1?
??v2 ,
??v2?
??v1 ,
??v2?
??v2}.
For a vector v = a??v1 + b
??n2 in V we have:
?(v) = ?
(
a
b
)
=
(
a 0
0 b
)
= a??v1?
??v1+b
??v2?
??v2
And for a matrix w = a??v1 ?
??v1 + b
??v1 ?
??v2 +
c??v2 ?
??v1 + d
??v2 ?
??v2 in V ? V , we have:
?(w) = ?
(
a b
c d
)
=
(
a
d
)
= a??v1 + d
??v2
43
3 String Diagrams
The framework of compact closed categories and
Frobenius algebras comes with a complete di-
agrammatic calculus that visualises derivations,
and which also simplifies the categorical and vec-
tor space computations. Morphisms are depicted
by boxes and objects by lines, representing their
identity morphisms. For instance a morphism
f : A ? B, and an object A with the identity ar-
row 1A : A? A, are depicted as follows:
f
A
B
A
The tensor products of the objects and mor-
phisms are depicted by juxtaposing their diagrams
side by side, whereas compositions of morphisms
are depicted by putting one on top of the other;
for instance the object A?B, and the morphisms
f ? g and f ? h, for f : A ? B, g : C ? D, and
h : B ? C, are depicted as follows:
f
A
B D
g
C f
A
B
h
C
A B
The  maps are depicted by cups, ? maps
by caps, and yanking by their composition and
straightening of the strings. For instance, the di-
agrams for l : Al ? A ? I , ? : I ? A ? Al and
(l ? 1A) ? (1A ? ?l) = 1A are as follows:
Al
A Al
A
Al A Al = A
The composition of the  and ? maps with other
morphisms is depicted as before, that is by juxta-
posing them one above the other. For instance the
diagrams for the compositions (1Bl ? f) ? 
l and
?l ? (1Al ? f) are as follows:
B
f
A
Bl
Al A
f
B
As for Frobenius algebras, the diagrams for the
monoid and comonoid morphisms are as follows:
(?, ?) (?, ?)
with the Frobenius condition being depicted as:
= =
The defining axioms guarantee that any picture de-
picting a Frobenius computation can be reduced to
a normal form that only depends on the number of
input and output strings of the nodes, independent
of the topology. These normal forms can be sim-
plified to so-called ?spiders?:
=
? ? ?
? ? ?
???
???
In the category FVect, apart from spaces V,W ,
which are objects of the category, we also have
vectors ??v ,??w . These are depicted by their repre-
senting morphisms and as triangles with a number
of strings emanating from them. The number of
strings of a triangle denote the tensor rank of the
vector; for instance, the diagrams for??v ? V,
??
v? ?
V ?W , and
??
v?? ? V ?W ? Z are as follows:
V W WV ZV
Application of a linear map to a vector is de-
picted using composition of their corresponding
morphisms. For instance, for f : V ? W and
??v ? V , the application f(??v ) is depicted by the
composition I
??v
?? V
f
??W .
V
f
W
44
Applications of the Frobenius maps to vectors
are depicted in a similar fashion; for instance
?(??v ? ??v ) is the composition I ? I
??v ???v
?? V ?
V
?
?? V and ?(??v ) is the composition I
??v
??
V
?
?? I , depicted as follows:
V V
V
V
4 Vector Space Interpretations
The grammatical structure of a language is en-
coded in the category Preg: objects are grammat-
ical types (assigned to words of the language) and
morphisms are grammatical reductions (encoding
the grammatical formation rules of the language).
For instance, the grammatical structure of the sen-
tence ?Men love Mary? is encoded in the assign-
ment of types n to the noun phrases ?men? and
?Mary? and nr ? s? nl to the verb ?love?, and in
the reduction map ln ? 1s ? 
r
n. The application
of this reduction map to the tensor product of the
word types in the sentence results in the type s:
(ln ? 1s ? 
r
n)(n? (n
r ? s? nl)? n)? s
To each reduction map corresponds a string dia-
gram that depicts the structure of reduction:
n nrsnl n
Men love Mary
In Coecke et al (2010) the pregroup types and
reductions are interpreted as vector spaces and lin-
ear maps, achieved via a homomorphic mapping
from Preg to FVect. Categorically speaking, this
map is a strongly monoidal functor:
F : Preg? FVect
It assigns vector spaces to the basic types as fol-
lows:
F (1) = I F (n) = N F (s) = S
and to the compound types by monoidality as fol-
lows; for x, y objects of Preg:
F (x? y) = F (x)? F (y)
Monoidal functors preserve the compact structure;
that is the following holds:
F (xl) = F (xr) = F (x)?
For instance, the interpretation of a transitive verb
is computed as follows:
F (nr ? s? nl) = F (nr)? F (s)? F (nl) =
F (n)? ? F (s)? F (n)? = N ? S ?N
This interpretation means that the meaning vector
of a transitive verb is a vector in N ? S ?N .
The pregroup reductions, i.e. the partial order
morphisms of Preg, are interpreted as linear maps:
whenever p ? q in Preg, we have a linear map
f? : F (p) ? F (q). The  and ? maps of Preg are
interpreted as the  and ? maps of FVect. For in-
stance, the pregroup reduction of a transitive verb
sentence is computed as follows:
F (rn ? 1s ? 
r
n) = F (
r
n)? F (1s)? F (
l
n) =
F (n)
? ? F (1s)? F (n)
? = N ? 1S ? N
The distributional meaning of a sentence is ob-
tained by applying the interpretation of the pre-
group reduction of the sentence to the tensor prod-
uct of the distributional meanings of the words
in the sentence. For instance, the distributional
meaning of ?Men love Mary? is as follows:
F (rn ? 1s ? 
l
n)(
???
Men?
???
love?
????
Mary)
This meaning is depictable via the following string
diagram:
N NSN N
Men love Mary
The next section applies these techniques to the
distributional interpretation of pronouns. The in-
terpretations are defined using:  maps, for appli-
cation of the semantics of one word to another; ?
maps, to pass information around by bridging in-
termediate words; and Frobenius operations, for
copying and combining the noun vectors and dis-
carding the sentence vectors.
5 Modelling Relative Pronouns
In this paper we focus on the subject and object
relative pronouns, who(m), which and that. Ex-
amples of noun phrases with subject relative pro-
nouns are ?men who love Mary?, ?dog which ate
cats?. Examples of noun phrases with object rela-
tive pronouns are ?men whom Mary loves?, ?book
45
that John read?. In the final example, ?book? is the
head noun, modified by the relative clause ?that
John read?. The intuition behind the use of Frobe-
nius algebras to model such cases is the following.
In ?book that John read?, the relative clause acts
on the noun (modifies it) via the relative pronoun,
which passes information from the clause to the
noun. The relative clause is then discarded, and
the modified noun is returned. Frobenius algebras
provide the machinery for all of these operations.
The pregroup types of the relative pronouns are
as follows:
nrnsln (subject)
nrnnllsl (object)
These types result in the following reductions:
nr s nl nn nr n sl n
Subject Rel-Pr Verb Object
nr s nlnn nr n nll sl
Object Rel-Pr Subject Verb
The meaning spaces of these pronouns are com-
puted using the mechanism described above:
F (nrnsln) = F (nr)? F (n)? F (sl)? F (n)
= N ?N ? S ?N
F (nrnnllsl) = F (nr)? F (n)? F (nll)? F (sl)
= N ?N ?N ? S
The semantic roles that these pronouns play are
reflected in their categorical vector space mean-
ings, depicted as follows:
Subj:
N N S N
Obj:
N N SN
with the following corresponding morphisms:
Subj: (1N ? ?N ? ?S ? 1N ) ? (?N ? ?N )
Obj: (1N ? ?N ? 1N ? ?S) ? (?N ? ?N )
The diagram of the meaning vector of the sub-
ject relative clause interacting with the head noun
is as follows:
N S N NN N NN S
Subject Rel-Pronoun Verb Object
The diagram for the object relative clause is:
N S NNN N NN S
Object Rel-Pronoun Subject Verb
These diagrams depict the flow of information in
a relative clause and the semantic role of its rel-
ative pronoun, which 1) passes information from
the clause to the head noun via the ? maps; 2) acts
on the noun via the ? map; 3) discards the clause
via the ? map; and 4) returns the modified noun
via 1N . The  maps pass the information of the
subject and object nouns to the verb and to the rel-
ative pronoun to be acted on. Note that there are
two different flows of information in these clauses:
the ones that come from the grammatical structure
and are depicted by maps (at the bottom of the di-
agrams), and the ones that come from the semantic
role of the pronoun and are depicted by ? maps (at
the top of the diagrams).
The normal forms of these diagrams are:
N S N NN
Subject Verb Object
N S N NN
Subject Verb Object
Symbolically, they correspond to the following
morphisms:
(?N ? ?S ? N )
(?????
Subject?
???
Verb?
????
Object
)
(N ? ?S ? ?N )
(?????
Subject?
???
Verb?
????
Object
)
The simplified normal forms will become useful in
practice when calculating vectors for such cases.
6 Vector Space Instantiations
In this section we demonstrate the effect of the
Frobenius operations using two example instan-
tiations. The first ? which is designed perhaps
46
as a theoretical example rather than a suggestion
for implementation ? is a truth-theoretic account,
similar to Coecke et al (2010) but also allow-
ing for degrees of truth. The second is based on
the concrete implementation of Grefenstette and
Sadrzadeh (2011a).
6.1 Degrees of Truth
Take N to be the vector space spanned by a set
of individuals {??n i}i that are mutually orthogo-
nal. For example, ??n 1 represents the individual
Mary, ??n 25 represents Roger the dog,
??n 10 rep-
resents John, and so on. A sum of basis vec-
tors in this space represents a common noun; e.g.
???man =
?
i
??n i, where i ranges over the basis vec-
tors denoting men. We take S to be the one dimen-
sional space spanned by the single vector
??
1 . The
unit vector spanning S represents truth value 1, the
zero vector represents truth value 0, and the inter-
mediate vectors represent degrees of truth.
A transitive verb w, which is a vector in the
space N ? S ?N , is represented as follows:
w :=
?
ij
??n i ? (?ij
??
1 )???n j
if ??n i w?s
??n j with degree ?ij , for all i, j.
Further, since S is one-dimensional with its
only basis vector being
??
1 , the transitive verb can
be represented by the following element ofN?N :
?
ij
?ij
??n i?
??n j if
??n i w?s
??n j with degree ?ij
Restricting to either ?ij = 1 or ?ij = 0 provides
a 0/1 meaning, i.e. either ??n i w?s
??n j or not.
Letting ?ij range over the interval [0, 1] enables
us to represent degrees as well as limiting cases
of truth and falsity. For example, the verb ?love?,
denoted by love, is represented by:
?
ij
?ij
??n i?
??n j if
??n i loves
??n jwith degree?ij
If we take ?ij to be 1 or 0, from the above we
obtain the following:
?
(i,j)?Rlove
??n i ?
??n j
where Rlove is the set of all pairs (i, j) such that
??n i loves
??n j .
Note that, with this definition, the sentence
space has already been discarded, and so for this
??????????????????
Subject who Verb Object :=
(?N ? N )
(?????
Subject?
???
Verb?
????
Object
)
=
(?N ? N )
?
?
?
k?K
??n k ?(
?
ij
?ij
??n i?
??n j)?
?
l?L
??n l
?
?
=
?
ij,k?K,l?L
?ij?N (
??n k ?
??n i)? N (
??n j ?
??n l)
=
?
ij,k?K,l?L
?ij?ki
??n i?jl
=
?
k?K,l?L
?kl
??n k
Figure 1: Meaning computation with a subject rel-
ative pronoun
instantiation the ? map, which is the part of the
relative pronoun interpretation designed to discard
the relative clause after it has acted on the head
noun, is not required.
For common nouns
?????
Subject =
?
k?K
??n k and
????
Object =
?
l?L
??n l, where k and l range over
the sets of basis vectors representing the respec-
tive common nouns, the truth-theoretic meaning of
a noun phrase modified by a subject relative clause
is computed as in Figure 1. The result is highly in-
tuitive, namely the sum of the subject individuals
weighted by the degree with which they have acted
on the object individuals via the verb. A similar
computation, with the difference that the ? and 
maps are swapped, provides the truth-theoretic se-
mantics of the object relative clause:
?
k?K,l?L
?kl
??n l
The calculation and final outcome is best under-
stood with an example.
Now only consider truth values 0 and 1. Con-
sider the noun phrase with object relative clause
?men whom Mary loves? and takeN to be the vec-
tor space spanned by the set of all people; then the
males form a subspace of this space, where the ba-
sis vectors of this subspace, i.e. men, are denoted
by ??ml, where l ranges over the set of men which
we denote byM . We set ?Mary? to be the individ-
ual
??
f 1, ?men? to be the common noun
?
l?M
??ml,
47
????????????????
men whom Mary loves :=
(N ? ?N )
?
?
??
f 1 ? (
?
(i,j)?Rlove
??
f i ?
??mj)?
?
l?M
??ml
?
?
=
?
l?M,(i,j)?Rlove
N (
??
f 1 ?
??
f i)? ?(
??mj ?
??ml)
=
?
l?M,(i,j)?Rlove
?1i?jl
??mj
=
?
(1,j)?Rlove|j?M
??mj
Figure 2: Meaning computation for example ob-
ject relative clause
and ?love? to be as follows:
?
(i,j)?Rlove
??
f i ?
??mj
The vector corresponding to the meaning of ?men
whom Mary loves? is computed as in Figure 2.
The result is the sum of the men basis vectors
which are also loved by Mary.
The second example involves degrees of truth.
Suppose we have two females Mary
??
f 1 and Jane??
f 2 and four men
??m1,
??m2,
??m3,
??m4. Mary loves
??m1 with degree 1/4 and
??m2 with degree 1/2; Jane
loves ??m3 with degree 1/5; and
??m4 is not loved. In
this situation, we have:
Rlove = {(1, 1), (1, 2), (2, 3)}
and the verb love is represented by:
1/4(
??
f 1?
??m1)+1/2(
??
f 1?
??m2)+1/5(
??
f 2?
??m3)
The meaning of ?men whom Mary loves? is com-
puted by substituting an ?1,j in the last line of Fig-
ure 2, resulting in the men whom Mary loves to-
gether with the degrees that she loves them:
?
(1,j)?Rlove|j?M
?1j
??mj = 1/4
??m1 + 1/2
??m2
?men whom women love? is computed as fol-
lows, where W is the set of women:
?
k?W,l?M,(i,j)?Rlove
?ijN (
??
f k ?
??
f i)? ?(
??mj ?
??ml)
=
?
k?W,l?M,(i,j)?Rlove
?ij?ki?jl
??mj
=
?
(i,j)?Rlove|i?W,j?M
?ij
??mj
= 1/4??m1 + 1/2
??m2 + 1/5
??m3
The result is the men loved by Mary or Jane to-
gether with the degrees to which they are loved.
6.2 A Concrete Instantiation
In the model of Grefenstette and Sadrzadeh
(2011a), the meaning of a verb is taken to be ?the
degree to which the verb relates properties of its
subjects to properties of its object?. Clark (2013)
provides some examples showing how this is an
intuitive defintion for a transitive verb in the cat-
egorical framework. This degree is computed by
forming the sum of the tensor products of the sub-
jects and objects of the verb across a corpus, where
w ranges over instances of the verb:
verb =
?
w
(
??
sbj?
??
obj)w
Denote the vector space of nouns by N ; the above
is a matrix in N ? N , depicted by a two-legged
triangle as follows:
N N
The verbs of this model do not have a sentence
dimension; hence no information needs to be dis-
carded when they are used in our setting, and so no
?map appears in the diagram of the relative clause.
Inserting the above diagram in the diagrams of the
normal forms results in the following for the sub-
ject relative clause (the object case is similar):
N N NN
Subject Verb Object
The abstract vectors corresponding to such dia-
grams are similar to the truth-theoretic case, with
the difference that the vectors are populated from
corpora and the scalar weights for noun vectors
48
are not necessarily 1 or 0. For subject and object
noun context vectors computed from a corpus as
follows:
?????
Subject =
?
k
Ck
??n k
????
Object =
?
l
Cl
??n l
and the verb a linear map:
Verb =
?
ij
Cij
??n i ?
??n j
computed as above, the concrete meaning of a
noun phrase modified by a subject relative clause
is as follows:
?
kijl
CkCijCl?N (
??n k ?
??n i)N (
??n j ?
??n l)
=
?
kijl
CkCijCl?ki
??n k?jl
=
?
kl
CkCklCl
??n k
Comparing this to the truth-theoretic case, we see
that the previous ?kl are now obtained from a cor-
pus and instantiated to CkCklCl. To see how the
above expression represents the meaning of the
noun phrase, decompose it into the following:
?
k
Ck
??n k 
?
kl
CklCl
??n l
Note that the second term of the above, which is
the application of the verb to the object, modifies
the subject via point-wise multiplication. A simi-
lar result arises for the object relative clause case.
As an example, suppose that N has two dimen-
sions with basis vectors ??n 1 and
??n 2, and consider
the noun phrase ?dog that bites men?. Define the
vectors of ?dog? and ?men? as follows:
??
dog = d1
??n 1+d2
??n 2
???men = m1
??n 1+m2
??n 2
and the matrix of ?bites? by:
b11??n 1???n 2+b12??n 1???n 2+b21??n 2???n 1+b22??n 2???n 2
Then the meaning of the noun phrase becomes:
?????????????
dog that bites men :=
d1b11m1??n 1 + d1b12m2??n 1 + d2b21m1??n 2
+ d2b22m2
??n 2 = (d1
??n 1 + d2
??n 2)
((b11m1 + b12m2)
??n 1 + (b21m1 + b22m2)
??n 2)
Using matrix notation, we can decompose the sec-
ond term further, from which the application of the
verb to the object becomes apparent:
(
b11 b12
b21 b22
)
?
(
m1
m2
)
Hence for the whole clause we obtain:
??
dog (bites????men)
Again this result is highly intuitive: assuming
that the basis vectors of the noun space represent
properties of nouns, the meaning of ?dog that bites
men? is a vector representing the properties of
dogs, which have been modified (via multiplica-
tion) by those properties of individuals which bite
men. Put another way, those properties of dogs
which overlap with properties of biting things get
accentuated.
7 Conclusion and Future Directions
In this paper, we have extended the compact cate-
gorical semantics of Coecke et al (2010) to anal-
yse meanings of relative clauses in English from
a vector space point of view. The resulting vec-
tor space semantics of the pronouns and clauses
is based on the Frobenius algebraic operations on
vector spaces: they reveal the internal structure, or
what we call anatomy, of the relative clauses.
The methodology pursued in this paper and the
Frobenius operations can be used to provide se-
mantics for other relative pronouns and also other
closed-class words such as prepositions and deter-
miners. In each case, the grammatical type of the
word and a detailed analysis of the role of these
words in the meaning of the phrases in which they
occur would be needed. In some cases, it may be
necessary to introduce a linear map to represent
the meaning of the word, for instance to distin-
guish the preposition on from in.
The contribution of this paper is best demon-
strated via the string diagrammatic representations
of the vector space meanings of these clauses. A
noun phrase modified by a subject relative clause,
which before this paper was depicted as follows:
N S N NN N NN S
Subject Rel-Pronoun Verb Object
will now include the internal anatomy of its rela-
tive pronoun:
49
N S N NN N NN S
Subject Rel-Pronoun Verb Object
This internal structure shows how the information
from the noun flows through the relative pronoun
to the rest of the clause and how it interacts with
the other words. We have instantiated this vector
space semantics using truth-theoretic and corpus-
based examples.
One aspect of our example spaces which means
that they work particularly well is that the sen-
tence dimension in the verb is already discarded,
which means that the ? maps are not required (as
discussed above). Another feature is that the sim-
ple nature of the models means that the ?map does
not lose any information, even though it takes the
diagonal of a matrix and hence in general throws
information away. The effect of the ? and ? maps
in more complex representations of the verb re-
mains to be studied in future work.
On the practical side, what we offer in this paper
is a method for building appropriate vector repre-
sentations for relative clauses. As a result, when
presented with a relative clause, we are able to
build a vector for it, only by relying on the vector
representations of the words in the clause and the
grammatical role of the relative pronoun. We do
not need to retrieve information from a corpus to
be able to build a vector or linear map for the rela-
tive pronoun, neither will we end up having to dis-
card the pronoun and ignore the role that it plays in
the meaning of the clause (which was perhaps the
best option available before this paper). However,
the Frobenius approach and our claim that the re-
sulting vectors are ?appropriate? requires an empir-
ical evaluation. Tasks such as the term definition
task from Kartsaklis et al (2013) (which also uses
Frobenius algebras but for a different purpose) are
an obvious place to start. More generally, the sub-
field of compositional distributional semantics is
a growing and active one (Mitchell and Lapata,
2008; Baroni and Zamparelli, 2010; Zanzotto et
al., 2010; Socher et al, 2011), for which we argue
that high-level mathematical investigations such
as this paper, and also Clarke (2008), can play a
crucial role.
Acknowledgements
We would like to thank Dimitri Kartsaklis and
Laura Rimell for helpful comments. Stephen
Clark was supported by ERC Starting Grant Dis-
CoTex (30692). Bob Coecke and Stephen Clark
are supported by EPSRC Grant EP/I037512/1.
Mehrnoosh Sadrzadeh is supported by an EPSRC
CAF EP/J002607/1.
References
J.C. Baez and J. Dolan. 1995. Higher-dimensional al-
gebra and topological quantum field theory. Journal
of Mathematical Physics, 36:6073?6105.
M. Baroni and R. Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP-10), Cambridge, MA.
A. Carboni and R. F. C. Walters. 1987. Cartesian bicat-
egories. I. J. Pure and Appied Algebra, 49:11?32.
S. Clark. 2013. Type-driven syntax and seman-
tics for composing meaning vectors. In Chris He-
unen, Mehrnoosh Sadrzadeh, and Edward Grefen-
stette, editors, Quantum Physics and Linguistics:
A Compositional, Diagrammatic Discourse, pages
359?377. Oxford University Press.
D. Clarke. 2008. Context-theoretic Semantics for Nat-
ural Language: An Algebraic Framework. Ph.D.
thesis, University of Sussex.
B. Coecke and E. Paquette. 2008. Introducing cat-
egories to the practicing physicist. In B. Coecke,
editor, New Structures for Physics, volume 813 of
Lecture Notes in Physics, pages 167?271. Springer.
B. Coecke, D. Pavlovic, and J. Vicary. 2008. A
new description of orthogonal bases. Mathematical
Structures in Computer Science, 1:269?272.
B. Coecke, M. Sadrzadeh, and S. Clark. 2010.
Mathematical foundations for a compositional dis-
tributional model of meaning. Linguistic Analysis,
36:345?384.
F. G. Frobenius. 1903. Theorie der hyperkomplexen
Gro??en. Preussische Akademie der Wissenschaften
Berlin: Sitzungsberichte der Preu?ischen Akademie
der Wissenschaften zu Berlin. Reichsdr.
E. Grefenstette and M. Sadrzadeh. 2011a. Experimen-
tal support for a categorical compositional distribu-
tional model of meaning. In Proceedings of Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 1394?1404.
E. Grefenstette and M. Sadrzadeh. 2011b. Experi-
menting with transitive verbs in a discocat. In Pro-
ceedings of the Workshop on Geometrical Models of
Natural Language Semantics (GEMS).
50
A. Joyal and R. Street. 1991. The Geometry of Tensor
Calculus, I. Advances in Mathematics, 88:55?112.
D. Kartsaklis, M. Sadrzadeh, S. Pulman, and B. Co-
ecke. 2013. Reasoning about meaning in nat-
ural language with compact closed categories and
frobenius algebras. In J. Chubb, A. Eskandar-
ian, and V. Harizanov, editors, Logic and Algebraic
Structures in Quantum Computing and Information,
Association for Symbolic Logic Lecture Notes in
Logic. Cambridge University Press.
G. M. Kelly and M. L. Laplaza. 1980. Coherence for
compact closed categories. Journal of Pure and Ap-
plied Algebra, 19:193?213.
J. Kock. 2003. Frobenius algebras and 2D topological
quantum field theories, volume 59 of London Mathe-
matical Society student texts. Cambridge University
Press.
J. Lambek. 1958. The Mathematics of Sentence Struc-
ture. American Mathematics Monthly, 65:154?170.
J. Lambek. 1999. Type Grammar Revisited Logical
Aspects of Computational Linguistics. In Logical
Aspects of Computational Linguistics, volume 1582
of Lecture Notes in Computer Science, pages 1?27.
Springer Berlin / Heidelberg.
J. Mitchell and M. Lapata. 2008. Vector-based models
of semantic composition. In Proceedings of ACL-
08, pages 236?244, Columbus, OH.
R. Montague. 1974. English as a formal language.
In R. H. Thomason, editor, Formal philosophy: Se-
lected Papers of Richard Montague, pages 189?223.
Yale University Press.
A. Preller and J. Lambek. 2007. Free compact 2-
categories. Mathematical Structures in Computer
Science, 17:309?340.
H. Schu?tze. 1998. Automatic word sense discrimina-
tion. Computational Linguistics, 24:97?123.
R. Socher, J. Pennington, E. Huang, A. Y. Ng, and
C. D. Manning. 2011. Semi-supervised recur-
sive autoencoders for predicting sentiment distribu-
tions. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, Edin-
burgh, UK.
F. M. Zanzotto, I. Korkontzelos, F. Fallucchi, and
S. Manandhar. 2010. Estimating linear models for
compositional distributional semantics. In Proceed-
ings of COLING, Beijing, China.
51
Proceedings of the EACL 2014 Workshop on Type Theory and Natural Language Semantics (TTNLS), pages 46?54,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
A Type-Driven Tensor-Based Semantics for CCG
Jean Maillard
University of Cambridge
Computer Laboratory
jm864@cam.ac.uk
Stephen Clark
University of Cambridge
Computer Laboratory
sc609@cam.ac.uk
Edward Grefenstette
University of Oxford
Department of Computer Science
edward.grefenstette@cs.ox.ac.uk
Abstract
This paper shows how the tensor-based se-
mantic framework of Coecke et al. can
be seamlessly integrated with Combina-
tory Categorial Grammar (CCG). The inte-
gration follows from the observation that
tensors are linear maps, and hence can
be manipulated using the combinators of
CCG, including type-raising and compo-
sition. Given the existence of robust,
wide-coverage CCG parsers, this opens up
the possibility of a practical, type-driven
compositional semantics based on distri-
butional representations.
1 Intoduction
In this paper we show how tensor-based distribu-
tional semantics can be seamlessly integrated with
Combinatory Categorial Grammar (CCG, Steed-
man (2000)), building on the theoretical discus-
sion in Grefenstette (2013). Tensor-based distribu-
tional semantics represents the meanings of words
with particular syntactic types as tensors whose se-
mantic type matches that of the syntactic type (Co-
ecke et al., 2010). For example, the meaning of a
transitive verb with syntactic type (S\NP)/NP is
a 3rd-order tensor from the tensor product space
N ? S ? N . The seamless integration with CCG
arises from the (somewhat trivial) observation that
tensors are linear maps ? a particular kind of
function ? and hence can be manipulated using
CCG?s combinatory rules.
Tensor-based semantics arises from the desire to
enhance distributional semantics with some com-
positional structure, in order to make distribu-
tional semantics more of a complete semantic the-
ory, and to increase its utility in NLP applica-
tions. There are a number of suggestions for how
to add compositionality to a distributional seman-
tics (Clarke, 2012; Pulman, 2013; Erk, 2012).
One approach is to assume that the meanings of
all words are represented by context vectors, and
then combine those vectors using some operation,
such as vector addition, element-wise multiplica-
tion, or tensor product (Clark and Pulman, 2007;
Mitchell and Lapata, 2008). A more sophisticated
approach, which is the subject of this paper, is to
adapt the compositional process from formal se-
mantics (Dowty et al., 1981) and attempt to build
a distributional representation in step with the syn-
tactic derivation (Coecke et al., 2010; Baroni et al.,
2013). Finally, there is a third approach using neu-
ral networks, which perhaps lies in between the
two described above (Socher et al., 2010; Socher
et al., 2012). Here compositional distributed rep-
resentations are built using matrices operating on
vectors, with all parameters learnt through a su-
pervised learning procedure intended to optimise
performance on some NLP task, such as syntac-
tic parsing or sentiment analysis. The approach
of Hermann and Blunsom (2013) conditions the
vector combination operation on the syntactic type
of the combinands, moving it a little closer to the
more formal semantics-inspired approaches.
The remainder of the Introduction gives a short
summary of distributional semantics. The rest of
the paper introduces some mathematical notation
from multi-linear algebra, including Einstein nota-
tion, and then shows how the combinatory rules of
CCG, including type-raising and composition, can
be applied directly to tensor-based semantic rep-
resentations. As well as describing a tensor-based
semantics for CCG, a further goal of this paper is to
present the compositional framework of Coecke et
al. (2010), which is based on category theory, to a
computational linguistics audience using only the
mathematics of multi-linear algebra.
1.1 Distributional Semantics
We assume a basic knowledge of distributional se-
mantics (Grefenstette, 1994; Sch?utze, 1998). Re-
46
cent inroductions to the topic include Turney and
Pantel (2010) and Clark (2014).
A potentially useful distinction for this paper,
and one not commonly made, is between distri-
butional and distributed representations. Distri-
butional representations are inherently contextual,
and rely on the frequently quoted dictum from
Firth that ?you shall know a word from the com-
pany it keeps? (Firth, 1957; Pulman, 2013). This
leads to the so-called distributional hypothesis that
words that occur in similar contexts tend to have
similar meanings, and to various proposals for
how to implement this hypothesis (Curran, 2004),
including alternative definitions of context; alter-
native weighting schemes which emphasize the
importance of some contexts over others; alterna-
tive similarity measures; and various dimension-
ality reduction schemes such as the well-known
LSA technique (Landauer and Dumais, 1997). An
interesting conceptual question is whether a sim-
ilar distributional hypothesis can be applied to
phrases and larger units: is it the case that sen-
tences, for example, have similar meanings if they
occur in similar contexts? Work which does ex-
tend the distributional hypothesis to larger units
includes Baroni and Zamparelli (2010), Clarke
(2012), and Baroni et al. (2013).
Distributed representations, on the other hand,
can be thought of simply as vectors (or possibly
higher-order tensors) of real numbers, where there
is no a priori interpretation of the basis vectors.
Neural networks can perhaps be categorised in this
way, since the resulting vector representations are
simply sequences of real numbers resulting from
the optimisation of some training criterion on a
training set (Collobert and Weston, 2008; Socher
et al., 2010). Whether these distributed represen-
tations can be given a contextual interpretation de-
pends on how they are trained.
One important point for this paper is that the
tensor-based compositional process makes no as-
sumptions about the interpretation of the tensors.
Hence in the remainder of the paper we make no
reference to how noun vectors or verb tensors,
for example, can be acquired (which, for the case
of the higher-order tensors, is a wide open re-
search question). However, in order to help the
reader who would prefer a more grounded dis-
cussion, one possibility is to obtain the noun vec-
tors using standard distributional techniques (Cur-
ran, 2004), and learn the higher-order tensors us-
ing recent techniques from ?recursive? neural net-
works (Socher et al., 2010). Another possibility
is suggested by Grefenstette et al. (2013), extend-
ing the learning technique based on linear regres-
sion from Baroni and Zamparelli (2010) in which
?gold-standard? distributional representations are
assumed to be available for some phrases and
larger units.
2 Mathematical Preliminaries
The tensor-based compositional process relies on
taking dot (or inner) products between vectors and
higher-order tensors. Dot products, and a number
of other operations on vectors and tensors, can be
conveniently written using Einstein notation (also
referred to as the Einstein summation convention).
In the rest of the paper we assume that the vector
spaces are over the field of real numbers.
2.1 Einstein Notation
The squared amplitude of a vector v ? R
n
is given
by:
|v|
2
=
n
?
i=1
v
i
v
i
Similarly, the dot product of two vectors v,w ?
R
n
is given by:
v ?w =
n
?
i=1
v
i
w
i
Denote the components of an m?n real matrix
A by A
ij
for 1 ? i ? m and 1 ? j ? n. Then
the matrix-vector product of A and v ? R
n
gives
a vector Av ? R
m
with components:
(Av)
i
=
n
?
j=1
A
ij
v
j
We can also multiply an n?mmatrixA and an
m ? o matrix B to produce an n ? o matrix AB
with components:
(AB)
ij
=
m
?
k=1
A
ik
B
kj
The previous examples are some of the most
common operations in linear algebra, and they all
involve sums over repeated indices. They can be
simplified by introducing the Einstein summation
convention: summation over the relevant range
is implied on every component index that occurs
47
twice. Pairs of indices that are summed over are
known as contracted, while the remaining indices
are known as free. Using this convention, the
above operations can be written as:
|v|
2
= v
i
v
i
v ?w = v
i
w
i
(Av)
i
= A
ij
v
j
, i.e. the contraction of v with
the second index of A
(AB)
ij
= A
ik
B
kj
, i.e. the contraction of the
second index of A with the first of B
Note how the number of free indices is always
conserved between the left- and right-hand sides in
these examples. For instance, while the last equa-
tion has two indices on the left and four on the
right, the two extra indices on the right are con-
tracted. Hence counting the number of free indices
can be a quick way of determining what type of
object is given by a certain mathematical expres-
sion in Einstein notation: no free indices means
that an operation yields a scalar number, one free
index means a vector, two a matrix, and so on.
2.2 Tensors
Linear Functionals Given a finite-dimensional
vector space R
n
over R, a linear functional is a
linear map a : R
n
? R.
Let a vector v have components v
i
in a fixed ba-
sis. Then the result of applying a linear functional
a to v can be written as:
a(v) = a
1
v
1
+? ? ?+a
n
v
n
=
(
a
1
? ? ? a
n
)
?
?
?
v
1
.
.
.
v
n
?
?
?
The numbers a
i
are the components of the lin-
ear functional, which can also be pictured as a row
vector. Since there is a one-to-one correspondence
between row and column vectors, the above equa-
tion is equivalent to:
v(a) = a
1
v
1
+? ? ?+a
n
v
n
=
(
v
1
? ? ? v
n
)
?
?
?
a
1
.
.
.
a
n
?
?
?
Using Einstein convention, the equations above
can be written as:
a(v) = v
i
a
i
= v(a)
Thus every finite-dimensional vector is a linear
functional, and vice versa. Row and column vec-
tors are examples of first-order tensors.
Definition 1 (First-order tensor). Given a vector
space V over the field R, a first-order tensor T
can be defined as:
? an element of the vector space V ,
? a linear map T : V ? R,
? a |V |-dimensional array of numbers T
i
, for
1 ? i ? |V |.
These three definitions are all equivalent. Given
a first-order tensor described using one of these
definitions, it is trivial to find the two other de-
scriptions.
Matrices An n?mmatrixA over R can be rep-
resented by a two-dimensional array of real num-
bers A
ij
, for 1 ? i ? n and 1 ? j ? m.
Via matrix-vector multiplication, the matrix A
can be seen as a linear map A : R
m
? R
n
. It
maps a vector v ? R
m
to a vector
?
?
?
A
11
? ? ? A
1m
.
.
.
.
.
.
.
.
.
A
n1
? ? ? A
nm
?
?
?
?
?
?
v
1
.
.
.
v
m
?
?
?
,
with components
A(v)
i
= A
ij
v
j
.
We can also contract a vector with the first index
of the matrix, which gives us a map A : R
n
?
R
m
. This corresponds to the operation
(
w
1
? ? ? w
n
)
?
?
?
A
11
? ? ? A
1m
.
.
.
.
.
.
.
.
.
A
n1
? ? ? A
nm
?
?
?
,
resulting in a vector with components
(w
T
A)
i
= A
ji
w
j
.
We can combine the two operations and see a
matrix as a map A : R
n
? R
m
? R, defined by:
w
T
Av =
(
w
1
? ? ? w
n
)
?
?
?
A
11
? ? ? A
1m
.
.
.
.
.
.
.
.
.
A
n1
? ? ? A
nm
?
?
?
?
?
?
v
1
.
.
.
v
m
?
?
?
In Einstein notation, this operation can be writ-
ten as
w
i
A
ij
v
j
,
48
which yields a scalar (constant) value, consistent
with the fact that all the indices are contracted.
Finally, matrices can also be characterised in
terms of Kronecker products. Given two vectors
v ? R
n
and w ? R
m
, their Kronecker product
v ?w is a matrix
v ?w =
?
?
?
v
1
w
1
? ? ? v
1
w
m
.
.
.
.
.
.
.
.
.
v
n
w
1
? ? ? v
n
w
m
?
?
?
,
with components
(v ?w)
ij
= v
i
w
j
.
It is a general result in linear algebra that any
n ? m matrix can be written as a finite sum of
Kronecker products
?
k
x
(k)
? y
(k)
of a set of
vectors x
(k)
and y
(k)
. Note that the sum over k
is written explicitly as it would not be implied by
Einstein notation: this is because the index k does
not range over vector/matrix/tensor components,
but over a set of vectors, and hence that index ap-
pears in brackets.
An n ? m matrix is an element of the tensor
space R
n
?R
m
, and it can also be seen as a linear
map A : R
n
? R
m
? R. This is because, given
a matrix B with decomposition
?
k
x
(k)
? y
(k)
,
the matrix A can act as follows:
A(B) = A
ij
?
k
x
(k)
i
y
(k)
j
=
?
k
(
x
(k)
1
? ? ? x
(k)
n
)
?
?
?
A
11
? ? ? A
1m
.
.
.
.
.
.
.
.
.
A
n1
? ? ? A
nm
?
?
?
?
?
?
y
(k)
.
.
.
y
(k)
m
?
?
?
= A
ij
B
ij
.
Again, counting the number of free indices in the
last line tells us that this operation yields a scalar.
Matrices are examples of second-order tensors.
Definition 2 (Second-order tensor). Given vector
spaces V,W over the field R, a second-order ten-
sor T can be defined as:
? an element of the vector space V ?W ,
? a |V | ? |W |-dimensional array of numbers
T
ij
, for 1 ? i ? |V | and 1 ? j ? |W |,
? a (multi-) linear map:
? T : V ?W ,
? T : W ? V ,
? T : V ?W ? R or T : V ?W ? R.
Again, these definitions are all equivalent. Most
importantly, the four types of maps given in the
definition are isomorphic. Therefore specifying
one map is enough to specify all the others.
Tensors We can generalise these definitions to
the more general concept of tensor.
Definition 3 (Tensor). Given vector spaces
V
1
, . . . , V
k
over the field R, a k
th
-order tensor T
is defined as:
? an element of the vector space V
1
? ? ? ??V
k
,
? a |V
1
| ? ? ? ? ? |V
k
|, k
th
-dimensional array of
numbers T
i
1
???i
k
, for 1 ? i
j
? |V
j
|,
? a multi-linear map T : V
1
? ? ? ? ? V
k
? R.
3 Tensor-Based CCG Semantics
In this section we show how CCG?s syntactic types
can be given tensor-based meaning spaces, and
how the combinator?s employed by CCG to com-
bine syntactic categories carry over to those mean-
ing spaces, maintaining what is often described
as CCG?s ?transparent interface? between syntax
and semantics. Here are some example syntactic
types, and the corresponding tensor spaces con-
taining the meanings of the words with those types
(using the notation syntactic type : semantic type).
We first assume that all atomic types have
meanings living in distinct vector spaces:
? noun phrases, NP : N
? sentences, S : S
The recipe for determining the meaning space
of a complex syntactic type is to replace each
atomic type with its corresponding vector space
and the slashes with tensor product operators:
? Intransitive verb, S\NP : S? N
? Transitive verb, (S\NP)/NP : S? N? N
? Ditransitive verb, ((S\NP)/NP)/NP :
S? N? N? N
? Adverbial modifier, (S\NP)\(S\NP) :
S? N? S? N
? Preposition modifying NP , (NP\NP)/NP :
N? N? N
49
Hence the meaning of an intransitive verb, for
example, is a matrix in the tensor product space
S ? N. The meaning of a transitive verb is a
?cuboid?, or 3rd-order tensor, in the tensor product
space S?N?N. In the same way that the syntac-
tic type of an intransitive verb can be thought of as
a function ? taking an NP and returning an S ?
the meaning of an intransitive verb is also a func-
tion (linear map) ? taking a vector in N and re-
turning a vector in S. Another way to think of this
function is that each element of the matrix spec-
ifies, for a pair of basis vectors (one from N and
one from S), what the result is on the S basis vec-
tor given a value on the N basis vector.
Now we describe how the combinatory rules
carry over to the meaning spaces.
3.1 Application
The function application rules of CCG are forward
(>) and backward (<) application:
X/Y Y =? X (>)
Y X\Y =? X (<)
In a traditional semantics for CCG, if function
application is applied in the syntax, then function
application applies also in the semantics (Steed-
man, 2000). This is also true of the tensor-based
semantics. For example, the meaning of a subject
NP combines with the meaning of an intransitive
verb via matrix multiplication, which is equivalent
to applying the linear map corresponding to the
matrix to the vector representing the meaning of
the NP . Applying (multi-)linear maps in (multi-
)linear algebra is equivalent to applying tensor
contraction to the combining tensors. Here is the
case for an intransitive verb:
Pat walks
NP S\NP
N S? N
Let Pat be assigned a vector P ? N and walks
be assigned a second-order tensor W ? S ? N.
Using the backward application combinator cor-
responds to feeding P , an element of N, into W ,
seen as a function N? S. In terms of tensor con-
traction, this is the following operation:
W
ij
P
j
.
Here we use the convention that the indices
maintain the same order as the syntactic type.
Therefore, in the tensor of an object of type X/Y ,
the first index corresponds to the type X and the
second to the type Y . That is why, when perform-
ing the contraction corresponding to Pat walks,
P ? N is contracted with the second index of
W ? S ? N, and not the first.
1
The first index
of W is then the only free index, telling us that the
above operation yields a first-order tensor (vector).
Since this index corresponds to S, we know that
applying backward application to Pat walks yields
a meaning vector in S.
Forward application is performed in the same
manner. Consider the following example:
Pat kisses Sandy
NP (S\NP)/NP NP
N S? N? N N
with corresponding tensors P ? N for Pat, K ?
S? N? N for kisses and Y ? N for Sandy.
The forward application deriving the type of
kisses Sandy corresponds to
K
ijk
Y
k
,
where Y is contracted with the third index of K
because we have maintained the order defined by
the type (S\NP)/NP : the third index then corre-
sponds to an argument NP coming from the right.
Counting the number of free indices in the
above expression tells us that it yields a second-
order tensor. Looking at the types corresponding
to the free indices tells us that this second-order
tensor is of type S?N, which is the semantic type
of a verb phrase (or intransitive verb), as we have
already seen in the walks example.
3.2 Composition
The forward (>
B
) and backward (<
B
) composi-
tion rules are:
X/Y Y/Z =? X/Z (>
B
)
Y \Z X\Y =? X\Z (<
B
)
Composition in the semantics also reduces to a
form of tensor contraction. Consider the following
example, in which might can combine with kiss
using forward composition:
Pat might kiss Sandy
NP (S\NP)/(S\NP) (S\NP)/NP NP
N S? N? S? N S? N? N N
1
The particular order of the indices is not important, as
long as a convention such as this one is decided upon and
consistently applied to all types (so that tensor contraction
contracts the relevant tensors from each side when a combi-
nator is used).
50
with tensors M ? S ? N ? S ? N for might and
K ? S?N?N for kiss. Combining the meanings
of might and kiss corresponds to the following op-
eration:
M
ijkl
K
klm
,
yielding a tensor in S ? N ? N, which is the
correct semantic type for a phrase with syntactic
type (S\NP)/NP . Backward composition is per-
formed analogously.
3.3 Backward-Crossed Composition
English also requires the use of backward-crossed
composition (Steedman, 2000):
X/Y Z\X =? Z/Y (<
B
?
)
In tensor terms, this is the same as forward com-
position; we just need to make sure that the con-
traction matches up the correct parts of each ten-
sor correctly. Consider the following backward-
crossed composition:
(S\NP)/NP (S\NP)\(S\NP) ?
<
B
?
(S\NP)/NP
Let the two items on the left-hand side be rep-
resented by tensors A ? S ? N ? N and B ?
S ? N ? S ? N. Then, combining them with
backward-crossed composition in tensor terms is
B
ijkl
A
klm
,
resulting in a tensor in S ? N ? N (correspond-
ing to the indices i, j and m). Note that we have
reversed the order of tensors in the contraction to
make the matching of the indices more transpar-
ent; however, tensor contraction is commutative
(since it corresponds to a sum over products) so
the order of the tensors does not affect the result.
3.4 Type-raising
The forward (>
T
) and backward (<
T
) type-
raising rules are:
X =? T/(T\X) (>
T
)
X =? T\(T/X) (<
T
)
where T is a variable ranging over categories.
Suppose we are given an item of atomic type Y ,
with corresponding vector A ? Y. If we apply
forward type-raising to it, we get a new tensor of
type A
?
? T ? T ? Y. Now suppose the item of
type Y is followed by another item of type X\Y ,
with tensor B ? X ? Y. A phrase consisting of
two words with types Y and X\Y can be parsed
in two different ways:
? Y X\Y ? X , by backward application;
? Y X\Y ?
T
X/(X\Y ) X\Y , by forward
type-raising, and X/(X\Y ) X\Y ? X , by
forward application.
Both ways of parsing this sentence yield an item
of type X , and crucially the meaning of the result-
ing item should be the same in both cases.
2
This
property of type-raising provides an avenue into
determining what the tensor representation for the
type-raised category should be, since the tensor
representations must also be the same:
A
j
B
ij
= A
?
ijk
B
jk
.
Moreover, this equation must hold for all items,
B. As a concrete example, the requirement says
that a subject NP combining with a verb phrase
S\NP must produce the same meaning for the
two alternative derivations, irrespective of the verb
phrase. This is equivalent to the requirement that
A
j
B
ij
= A
?
ijk
B
jk
, ?B ? X? Y.
So to arrive at the tensor representation, we sim-
ply have to solve the tensor equation above. We
start by renaming the dummy index j on the left-
hand side:
A
k
B
ik
= A
?
ijk
B
jk
.
We then insert a Kronecker delta (?
ij
= 1 if i = j
and 0 otherwise):
A
k
B
jk
?
ij
= A
?
ijk
B
jk
.
Since the equation holds for allB, we are left with
A
?
ijk
= ?
ij
A
k
,
which gives us a recipe for performing type-
raising in a tensor-based model. The recipe is par-
ticularly simple and elegant: it corresponds to in-
serting the vector being type-raised into the 3rd-
order tensor at all places where the first two in-
dices are equal (with the rest of the elements in
the 3rd-order tensor being zero). For example, to
type-raise a subject NP , its meaning vector in N is
placed in the 3rd-order tensor S?S?N at all places
where the indices of the two S dimensions are the
same. Visually, the 3rd-order tensor correspond-
ing to the meaning of the type-raised category is
2
This property of CCG resulting from the use of type-
raising and composition is sometimes referred to as ?spurious
ambiguity?.
51
a cubiod in which the noun vector is repeated a
number of times (once for each sentence index),
resulting in a series of ?steps? progressing diag-
onally from the bottom of the cuboid to the top
(assuming a particular orientation).
The discussion so far has been somewhat ab-
stract, so to finish this section we include some
more examples with CCG categories, and show
that the tensor contraction operation has an intu-
itive similarity with the ?cancellation law? of cat-
egorial grammar which applies in the syntax.
First consider the example of a subject NP
with meaning A, combining with a verb phrase
S\NP with meaning B, resulting in a sentence
with meaning C. In the syntax, the two NPs can-
cel. In the semantics, for each basis of the sentence
space S we perform an inner product between two
vectors in N:
C
i
= A
j
B
ij
Hence, inner products in the tensor space corre-
spond to cancellation in the syntax.
This correspondence extends to complex argu-
ments, and also to composition. Consider the sub-
ject type-raising case, in which a subject NP with
meaning A in S ? S ? N combines with a verb
phrase S\NP with meaning B, resulting in a sen-
tence with meaning C. Again we perform inner
product operations, but this time the inner product
is between two matrices:
3
C
i
= A
ijk
B
jk
Note that two matrices are ?cancelled? for each
basis vector of the sentence space (i.e. for each
index i in C
i
).
As a final example, consider the forward com-
position from earlier, in which a modal verb with
meaningA in S?N?S?N combines with a tran-
sitive verb with meaning B in S ? N ? N to give
a transitive verb with meaning C in S ? N ? N.
Again the cancellation in the syntax corresponds
to inner products between matrices, but this time
we need an inner product for each combination of
3 indices:
C
ijk
= A
ijlm
B
lmk
3
To be more precise, the two matrices can be thought of
as vectors in the tensor space S ? N and the inner product is
between these vectors. Another way to think of this opera-
tion is to ?linearize? the two matrices into vectors and then
perform the inner product on these vectors.
For each i, j, k, two matrices ? corresponding to
the l,m indices above ? are ?cancelled?.
This intuitive explanation extends to arguments
with any number of slashes. For example, a
composition where the cancelling categories are
(N /N )/(N /N ) would require inner products be-
tween 4th-order tensors in N? N? N? N.
4 Related Work
The tensor-based semantics presented in this pa-
per is effectively an extension of the Coecke et al.
(2010) framework to CCG, re-expressing in Ein-
stein notation the existing categorical CCG exten-
sion in Grefenstette (2013), which itself builds
on an earlier Lambek Grammar extension to the
framework by Coecke et al. (2013).
This work also bears some similarity to the
treatment of categorial grammars presented by Ba-
roni et al. (2013), which it effectively encompasses
by expressing the tensor contractions described by
Baroni et al. as Einstein summations. However,
this paper also covers CCG-specific operations not
discussed by Baroni et al., such as type-raising and
composition.
One difference between this paper and the orig-
inal work by Coecke et al. (2010) is that they use
pregroups as the syntactic formalism (Lambek,
2008), a context-free variant of categorial gram-
mar. In pregroups, cancellation in the syntax is
always between two atomic categories (or more
precisely, between an atomic category and its ?ad-
joint?), whereas in CCG the arguments in complex
categories can be complex categories themselves.
To what extent this difference is significant re-
mains to be seen. For example, one area where this
may have an impact is when non-linearities are
added after contractions. Since the CCG contrac-
tions with complex arguments happen ?in one go?,
whereas the corresponding pregroup cancellation
in the semantics would be a series of contractions,
many more non-linearities would be added in the
pregroup case.
Krishnamurthy and Mitchell (2013) is based on
a similar insight to this paper ? that CCG provides
combinators which can manipulate functions op-
erating over vectors. Krishnamurthy and Mitchell
consider the function application case, whereas we
have shown how the type-raising and composition
operators apply naturally in this setting also.
52
5 Conclusion
This paper provides a theoretical framework for
the development of a compositional distributional
semantics for CCG. Given the existence of ro-
bust, wide-coverage CCG parsers (Clark and Cur-
ran, 2007; Hockenmaier and Steedman, 2002),
together with various techniques for learning the
tensors, the opportunity exists for a practical im-
plementation. However, there are significant engi-
neering difficulties which need to be overcome.
Consider adapting the neural-network learning
techniques of Socher et al. (2012) to this prob-
lem.
4
In terms of the number of tensors, the lexi-
con would need to contain a tensor for every word-
category pair; this is at least an order of magnitude
more tensors then the number of matrices learnt in
existing work (Socher et al., 2012; Hermann and
Blunsom, 2013). Furthermore, the order of the
tensors is now higher. Syntactic categories such as
((N /N )/(N /N ))/((N /N )/(N /N )) are not un-
common in the wide-coverage grammar of Hock-
enmaier and Steedman (2007), which in this case
would require an 8th-order tensor. This combina-
tion of many word-category pairs and higher-order
tensors results in a huge number of parameters.
As a solution to this problem, we are investigat-
ing ways to reduce the number of parameters, for
example using tensor decomposition techniques
(Kolda and Bader, 2009). It may also be possi-
ble to reduce the size of some of the complex cat-
egories in the grammar. Many challenges remain
before a type-driven compositional distributional
semantics can be realised, similar to the work of
Bos for the model-theoretic case (Bos et al., 2004;
Bos, 2005), but in this paper we have set out the
theoretical framework for such an implementation.
Finally, we repeat a comment made earlier that
the compositional framework makes no assump-
tions about the underlying vector spaces, or how
they are to be interpreted. On the one hand, this
flexibility is welcome, since it means the frame-
work can encompass many techniques for building
word vectors (and tensors). On the other hand, it
means that a description of the framework is nec-
essarily abstract, and it leaves open the question
4
Non-linear transformations are inherent to neural net-
works, whereas the framework in this paper is entirely linear.
However, as hinted at earlier in the paper, non-linear transfor-
mations can be applied to the output of each tensor, turning
the linear networks in this paper into extensions of those in
Socher et al. (2012) (extensions in the sense that the tensors
in Socher et al. (2012) do not extend beyond matrices).
of what the meaning spaces represent. The lat-
ter question is particularly pressing in the case of
the sentence space, and providing an interpretation
of such spaces remains a challenge for the distri-
butional semantics community, as well as relating
distributional semantics to more traditional topics
in semantics such as quantification and inference.
Acknowledgments
Jean Maillard is supported by an EPSRC MPhil
studentship. Stephen Clark is supported by ERC
Starting Grant DisCoTex (306920) and EPSRC
grant EP/I037512/1. Edward Grefenstette is sup-
ported by EPSRC grant EP/I037512/1. We would
like to thank Tamara Polajnar, Laura Rimell, Nal
Kalchbrenner and Karl Moritz Hermann for useful
discussion.
References
M. Baroni and R. Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP-10), Cambridge, MA.
M. Baroni, R. Bernardi, and R. Zamparelli. 2013.
Frege in space: A program for compositional dis-
tributional semantics (to appear). Linguistic Issues
in Language Technologies.
Johan Bos, Stephen Clark, Mark Steedman, James R.
Curran, and Julia Hockenmaier. 2004. Wide-
coverage semantic representations from a CCG
parser. In Proceedings of COLING-04, pages 1240?
1246, Geneva, Switzerland.
Johan Bos. 2005. Towards wide-coverage seman-
tic interpretation. In Proceedings of the Sixth In-
ternational Workshop on Computational Semantics
(IWCS-6), pages 42?53, Tilburg, The Netherlands.
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG
and log-linear models. Computational Linguistics,
33(4):493?552.
Stephen Clark and Stephen Pulman. 2007. Combining
symbolic and distributional models of meaning. In
Proceedings of AAAI Spring Symposium on Quan-
tum Interaction, Stanford, CA. AAAI Press.
Stephen Clark. 2014. Vector space models of lexical
meaning (to appear). In Shalom Lappin and Chris
Fox, editors, Handbook of Contemporary Semantics
second edition. Wiley-Blackwell.
Daoud Clarke. 2012. A context-theoretic frame-
work for compositionality in distributional seman-
tics. Computational Linguistics, 38(1):41?71.
53
B. Coecke, M. Sadrzadeh, and S. Clark. 2010. Math-
ematical foundations for a compositional distribu-
tional model of meaning. In J. van Bentham,
M. Moortgat, and W. Buszkowski, editors, Linguis-
tic Analysis (Lambek Festschrift), volume 36, pages
345?384.
Bob Coecke, Edward Grefenstette, and Mehrnoosh
Sadrzadeh. 2013. Lambek vs. Lambek: Functorial
vector space semantics and string diagrams for Lam-
bek calculus. Annals of Pure and Applied Logic.
R. Collobert and J. Weston. 2008. A unified architec-
ture for natural language processing: Deep neural
networks with multitask learning. In International
Conference on Machine Learning, ICML, Helsinki,
Finland.
James R. Curran. 2004. From Distributional to Seman-
tic Similarity. Ph.D. thesis, University of Edinburgh.
D.R. Dowty, R.E. Wall, and S. Peters. 1981. Introduc-
tion to Montague Semantics. Dordrecht.
Katrin Erk. 2012. Vector space models of word mean-
ing and phrase meaning: a survey. Language and
Linguistics Compass, 6(10):635?653.
J. R. Firth. 1957. A synopsis of linguistic theory 1930-
1955. In Studies in Linguistic Analysis, pages 1?32.
Oxford: Philological Society.
Edward Grefenstette, Georgiana Dinu, YaoZhong
Zhang, Mehrnoosh Sadrzadeh, and Marco Baroni.
2013. Multistep regression learning for composi-
tional distributional semantics. In Proceedings of
the 10th International Conference on Computational
Semantics (IWCS-13), Potsdam, Germany.
Gregory Grefenstette. 1994. Explorations in Auto-
matic Thesaurus Discovery. Kluwer.
Edward Grefenstette. 2013. Category-Theoretic
Quantitative Compositional Distributional Models
of Natural Language Semantics. Ph.D. thesis, Uni-
versity of Oxford.
Karl Moritz Hermann and Phil Blunsom. 2013. The
role of syntax in vector space models of composi-
tional semantics. Proceedings of ACL, Sofia, Bul-
garia, August. Association for Computational Lin-
guistics.
Julia Hockenmaier and Mark Steedman. 2002. Gen-
erative models for statistical parsing with Combi-
natory Categorial Grammar. In Proceedings of the
40th Meeting of the ACL, pages 335?342, Philadel-
phia, PA.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: a corpus of CCG derivations and dependency
structures extracted from the Penn Treebank. Com-
putational Linguistics, 33(3):355?396.
T. G. Kolda and B. W. Bader. 2009. Tensor decompo-
sitions and applications. SIAM Review, 51(3):455?
500.
Jayant Krishnamurthy and Tom M. Mitchell. 2013.
Vector space semantic parsing: A framework for
compositional vector space models. In Proceed-
ings of the 2013 ACL Workshop on Continuous Vec-
tor Space Models and their Compositionality, Sofia,
Bulgaria.
Joachim Lambek. 2008. From Word to Sentence.
A Computational Algebraic Approach to Grammar.
Polimetrica.
T. K. Landauer and S. T. Dumais. 1997. A solu-
tion to Plato?s problem: the latent semantic analysis
theory of acquisition, induction and representation
of knowledge. Psychological Review, 104(2):211?
240.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL-08, pages 236?244, Columbus, OH.
Stephen Pulman. 2013. Distributional semantic mod-
els. In Sadrzadeh Heunen and Grefenstette, editors,
Compositional Methods in Physics and Linguistics.
Oxford University Press.
Hinrich Sch?utze. 1998. Automatic word sense dis-
crimination. Computational Linguistics, 24(1):97?
124.
Richard Socher, Christopher D. Manning, and An-
drew Y. Ng. 2010. Learning continuous phrase
representations and syntactic parsing with recursive
neural networks. In Proceedings of the NIPS Deep
Learning and Unsupervised Feature Learning Work-
shop, Vancouver, Canada.
Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Semantic composi-
tionality through recursive matrix-vector spaces. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages 1201?
1211, Jeju, Korea.
Mark Steedman. 2000. The Syntactic Process. The
MIT Press, Cambridge, MA.
Peter D. Turney and Patrick Pantel. 2010. From
frequency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37:141?188.
54
Proceedings of the 2nd Workshop on Continuous Vector Space Models and their Compositionality (CVSC) @ EACL 2014, pages 21?30,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
A Systematic Study of Semantic Vector Space Model Parameters
Douwe Kiela
University of Cambridge
Computer Laboratory
douwe.kiela@cl.cam.ac.uk
Stephen Clark
University of Cambridge
Computer Laboratory
sc609@cam.ac.uk
Abstract
We present a systematic study of parame-
ters used in the construction of semantic
vector space models. Evaluation is car-
ried out on a variety of similarity tasks, in-
cluding a compositionality dataset, using
several source corpora. In addition to rec-
ommendations for optimal parameters, we
present some novel findings, including a
similarity metric that outperforms the al-
ternatives on all tasks considered.
1 Introduction
Vector space models (VSMs) represent the mean-
ings of lexical items as vectors in a ?semantic
space?. The benefit of VSMs is that they can eas-
ily be manipulated using linear algebra, allowing
a degree of similarity between vectors to be com-
puted. They rely on the distributional hypothesis
(Harris, 1954): the idea that ?words that occur in
similar contexts tend to have similar meanings?
(Turney and Pantel, 2010; Erk, 2012). The con-
struction of a suitable VSM for a particular task is
highly parameterised, and there appears to be little
consensus over which parameter settings to use.
This paper presents a systematic study of the
following parameters:
? vector size;
? window size;
? window-based or dependency-based context;
? feature granularity;
? similarity metric;
? weighting scheme;
? stopwords and high frequency cut-off.
A representative set of semantic similarity
datasets has been selected from the literature, in-
cluding a phrasal similarity dataset for evaluating
compositionality. The choice of source corpus is
likely to influence the quality of the VSM, and so
we use a selection of source corpora. Hence there
are two additional ?superparameters?:
? dataset for evaluation;
? source corpus.
Previous studies have been limited to investigat-
ing only a small number of parameters, and us-
ing a limited set of source corpora and tasks for
evaluation (Curran and Moens, 2002a; Curran and
Moens, 2002b; Curran, 2004; Grefenstette, 1994;
Pado and Lapata, 2007; Sahlgren, 2006; Turney
and Pantel, 2010; Schulte im Walde et al., 2013).
Rohde et al. (2006) considered several weighting
schemes for a large variety of tasks, while Weeds
et al. (2004) did the same for similarity metrics.
Stone et al. (2008) investigated the effectiveness
of sub-spacing corpora, where a larger corpus is
queried in order to construct a smaller sub-spaced
corpus (Zelikovitz and Kogan, 2006). Blacoe and
Lapata (2012) compare several types of vector rep-
resentations for semantic composition tasks. The
most comprehensive existing studies of VSM pa-
rameters ? encompassing window sizes, feature
granularity, stopwords and dimensionality reduc-
tion ? are by Bullinaria and Levy (2007; 2012)
and Lapesa and Evert (2013).
Section 2 introduces the various parameters of
vector space model construction. We then attempt,
in Section 3, to answer some of the fundamen-
tal questions for building VSMs through a number
of experiments that consider each of the selected
parameters. In Section 4 we examine how these
findings relate to the recent development of dis-
tributional compositional semantics (Baroni et al.,
2013; Clark, 2014), where vectors for words are
combined into vectors for phrases.
2 Data and Parameters
Two datasets have dominated the literature with
respect to VSM parameters: WordSim353 (Finkel-
stein et al., 2002) and the TOEFL synonym dataset
21
Dataset Pairings Words
RG 65 48
MC 30 39
W353 353 437
MEN 3000 751
TOEFL 80 400
M&L10 324 314
Table 1: Datasets for evaluation
(Landauer and Dumais, 1997). There is a risk
that semantic similarity studies have been overfit-
ting to their idiosyncracies, so in this study we
evaluate on a variety of datasets: in addition to
WordSim353 (W353) and TOEFL, we also use
the Rubenstein & Goodenough (RG) (1965) and
Miller & Charles (MC) (1991) data, as well as
a much larger set of similarity ratings: the MEN
dataset (Bruni et al., 2012). All these datasets con-
sist of human similarity ratings for word pairings,
except TOEFL, which consists of multiple choice
questions where the task is to select the correct
synonym for a target word. In Section 4 we ex-
amine our parameters in the context of distribu-
tional compositional semantics, using the evalua-
tion dataset from Mitchell and Lapata (2010). Ta-
ble 1 gives statistics for the number of words and
word pairings in each of the datasets.
As well as using a variety of datasets, we also
consider three different corpora from which to
build the vectors, varying in size and domain.
These include the BNC (Burnard, 2007) (10
6
word types, 10
8
tokens) and the larger ukWaC
(Baroni et al., 2009) (10
7
types, 10
9
tokens).
We also include a sub-spaced Wikipedia corpus
(Stone et al., 2008): for all words in the eval-
uation datasets, we build a subcorpus by query-
ing the top 10-ranked Wikipedia documents using
the words as search terms, resulting in a corpus
with 10
6
word types and 10
7
tokens. For examin-
ing the dependency-based contexts, we include the
Google Syntactic N-gram corpus (Goldberg and
Orwant, 2013), with 10
7
types and 10
11
tokens.
2.1 Parameters
We selected the following set of parameters for in-
vestigation, all of which are fundamental to vector
space model construction
1
.
1
Another obvious parameter would be dimensionality re-
duction, which we chose not to include because it does not
represent a fundamental aspect of VSM construction: di-
mensionality reduction relies on some original non-reduced
model, and directly depends on its quality.
Vector size Each component of a vector repre-
sents a context (or perhaps more accurately a ?con-
textual element?, such as second word to the left
of the target word).
2
The number of components
varies hugely in the literature, but a typical value
is in the low thousands. Here we consider vec-
tor sizes ranging from 50,000 to 500,000, to see
whether larger vectors lead to better performance.
Context There are two main approaches to mod-
elling context: window-based and dependency-
based. For window-based methods, contexts are
determined by word co-occurrences within a win-
dow of a given size, where the window simply
spans a number of words occurring around in-
stances of a target word. For dependency-based
methods, the contexts are determined by word
co-occurrences in a particular syntactic relation
with a target word (e.g. target word dog is the
subject of run, where run subj is the context).
We consider different window sizes and compare
window-based and dependency-based methods.
Feature granularity Context words, or ?fea-
tures?, are often stemmed or lemmatised. We in-
vestigate the effect of stemming and lemmatisa-
tion, in particular to see whether the effect varies
with corpus size. We also consider more fine-
grained features in which each context word is
paired with a POS tag or a lexical category from
CCG (Steedman, 2000).
Similarity metric A variety of metrics can be
used to calculate the similarity between two vec-
tors. We consider the similarity metrics in Table 2.
Weighting Weighting schemes increase the im-
portance of contexts that are more indicative of the
meaning of the target word: the fact that cat co-
occurs with purr is much more informative than
its co-occurrence with the. Table 3 gives defini-
tions of the weighting schemes considered.
Stopwords, high frequency cut-off Function
words and stopwords are often considered too un-
informative to be suitable context words. Ignor-
ing them not only leads to a reduction in model
size and computational effort, but also to a more
informative distributional vector. Hence we fol-
lowed standard practice and did not use stopwords
as context words (using the stoplist in NLTK (Bird
et al., 2009)). The question we investigated is
2
We will use the term ?feature? or ?context? or ?context
word? to refer to contextual elements.
22
Measure Definition
Euclidean
1
1+
??
n
i=1
(u
i
?v
i
)
2
Cityblock
1
1+
?
n
i=1
|u
i
?v
i
|
Chebyshev
1
1+max
i
|u
i
?v
i
|
Cosine
u?v
|u||v|
Correlation
(u??
u
)?(v??
v
)
|u||v|
Dice
2
?
n
i=0
min(u
i
,v
i
)
?
n
i=0
u
i
+v
i
Jaccard
u?v?
n
i=0
u
i
+v
i
Jaccard2
?
n
i=0
min(u
i
,v
i
)
?
n
i=0
max(u
i
,v
i
)
Lin
?
n
i=0
u
i
+v
i
|u|+|v|
Tanimoto
u?v
|u|+|v|?u?v
Jensen-Shannon Div 1?
1
2
(D(u||
u+v
2
)+D(v||
u+v
2
))
?
2 log 2
?-skew 1?
D(u||?v+(1??)u)
?
2 log 2
Table 2: Similarity measures between vectors v
and u, where v
i
is the ith component of v
whether removing more context words, based on
a frequency cut-off, can improve performance.
3 Experiments
The parameter space is too large to analyse ex-
haustively, and so we adopted a strategy for how
to navigate through it, selecting certain parame-
ters to investigate first, which then get fixed or
?clamped? in the remaining experiments. Unless
specified otherwise, vectors are generated with the
following restrictions and transformations on fea-
tures: stopwords are removed, numbers mapped
to ?NUM?, and only strings consisting of alphanu-
meric characters are allowed. In all experiments,
the features consist of the frequency-ranked first n
words in the given source corpus.
Four of the five similarity datasets (RG, MC,
W353, MEN) contain continuous scales of sim-
ilarity ratings for word pairs; hence we follow
standard practice in using a Spearman correlation
coefficient ?
s
for evaluation. The fifth dataset
(TOEFL) is a set of multiple-choice questions,
for which an accuracy measure is appropriate.
Calculating an aggregate score over all datasets
is non-trivial, since taking the mean of correla-
tion scores leads to an under-estimation of per-
formance; hence for the aggregate score we use
the Fisher-transformed z-variable of the correla-
Scheme Definition
None w
ij
= f
ij
TF-IDF w
ij
= log(f
ij
)? log(
N
n
j
)
TF-ICF w
ij
= log(f
ij
)? log(
N
f
j
)
Okapi BM25 w
ij
=
f
ij
0.5+1.5?
f
j
f
j
j
+f
ij
log
N?n
j
+0.5
f
ij
+0.5
ATC w
ij
=
(0.5+0.5?
f
ij
max
f
) log(
N
n
j
)
?
?
N
i=1
[(0.5+0.5?
f
ij
max
f
) log(
N
n
j
)]
2
LTU w
ij
=
(log(f
ij
)+1.0) log(
N
n
j
)
0.8+0.2?f
j
?
j
f
j
MI w
ij
= log
P (t
ij
|c
j
)
P (t
ij
)P (c
j
)
PosMI max(0,MI)
T-Test w
ij
=
P (t
ij
|c
j
)?P (t
ij
)P (c
j
)
?
P (t
ij
)P (c
j
)
?
2
see (Curran, 2004, p. 83)
Lin98a w
ij
=
f
ij
?f
f
i
?f
j
Lin98b w
ij
= ?1? log
n
j
N
Gref94 w
ij
=
log f
ij
+1
logn
j
+1
Table 3: Term weighting schemes. f
ij
denotes the
target word frequency in a particular context, f
i
the total target word frequency, f
j
the total context
frequency, N the total of all frequencies, n
j
the
number of non-zero contexts. P (t
ij
|c
j
) is defined
as
f
ij
f
j
and P (t
ij
) as
f
ij
N
.
tion datasets, and take the weighted average of
its inverse over the correlation datasets and the
TOEFL accuracy score (Silver and Dunlap, 1987).
3.1 Vector size
The first parameter we investigate is vector size,
measured by the number of features. Vectors are
constructed from the BNC using a window-based
method, with a window size of 5 (2 words either
side of the target word). We experiment with vec-
tor sizes up to 0.5M features, which is close to the
total number of context words present in the en-
tire BNC according to our preprocessing scheme.
Features are added according to frequency in the
BNC, with increasingly more rare features being
added. For weighting we consider both Positive
Mutual Information and T-Test, which have been
found to work best in previous research (Bullinaria
and Levy, 2012; Curran, 2004). Similarity is com-
puted using Cosine.
23
Figure 1: Impact of vector size on performance
across different datasets
The results in Figure 1 show a clear trend: for
both weighting schemes, performance no longer
improves after around 50,000 features; in fact, for
T-test weighting, and some of the datasets, perfor-
mance initially declines with an increase in fea-
tures. Hence we conclude that continuing to add
more rare features is detrimental to performance,
and that 50,000 features or less will give good per-
formance. An added benefit of smaller vectors is
the reduction in computational cost.
3.2 Window size
Recent studies have found that the best window
size depends on the task at hand. For example,
Hill et al. (2013) found that smaller windows work
best for measuring similarity of concrete nouns,
whereas larger window sizes work better for ab-
stract nouns. Schulte im Walde et al. (2013) found
that a large window size worked best for a com-
positionality dataset of German noun-noun com-
pounds. Similar relations between window size
and performance have been found for similar ver-
sus related words, as well as for similar versus as-
sociated words (Turney and Pantel, 2010).
We experiment with window sizes of 3, 5, 7, 9
and a full sentence. (A window size of n implies
n?1
2
words either side of the target word.) We
use Positive Mutual Information weighting, Co-
sine similarity, and vectors of size 50,000 (based
on the results from Section 3.1). Figure 2 shows
the results for all the similarity datasets, with the
aggregated score at the bottom right.
Performance was evaluated on three corpora,
in order to answer three questions: Does win-
dow size affect performance? Does corpus size
interact with window size? Does corpus sub-
Figure 2: Impact of window size across three cor-
pora
spacing interact with window size? Figure 2
clearly shows the answer to all three questions is
?yes?. First, ukWaC consistently outperforms the
BNC, across all window sizes, indicating that a
larger source corpus leads to better performance.
Second, we see that the larger ukWaC performs
better with smaller window sizes compared to the
BNC, with the best ukWaC performance typically
being found with a window size of only 3. For
the BNC, it appears that a larger window is able to
offset the smaller size of corpus to some extent.
We also evaluated on a sub-spaced Wikipedia
source corpus similar to Stone et al. (2008), which
performs much better with larger window sizes
than the BNC or ukWaC. Our explanation for this
result is that sub-spacing, resulting from search-
ing for Wikipedia pages with the appropriate tar-
get terms, provides a focused, less noisy corpus in
which context words some distance from the target
word are still relevant to its meaning.
In summary, the highest score is typically
achieved with the largest source corpora and
smallest window size, with the exception of the
much smaller sub-spaced Wikipedia corpus.
3.3 Context
The notion of context plays a key role in VSMs.
Pado and Lapata (2007) present a comparison of
window-based versus dependency-based methods
and conclude that dependency-based contexts give
better results. We also compare window-based and
dependency-based models.
Dependency-parsed versions of the BNC and
ukWaC were used to construct syntactically-
informed vectors, with a single, labelled arc be-
24
Figure 3: Window versus dependency contexts
tween the target word and context word.
3
Since
this effectively provides a window size of 3, we
also use a window size of 3 for the window-based
method (which provided the best results in Sec-
tion 3.2 with the ukWaC corpus). As well as
the ukWaC and BNC source corpora, we use the
Google syntactic N-gram corpus (Goldberg and
Orwant, 2013), which is one of the largest cor-
pora to date, and which consists of syntactic n-
grams as opposed to window-based n-grams. We
use vectors of size 50,000 with Positive Mutual In-
formation weighting and Cosine similarity. Due
to its size and associated computational cost, we
used only 10,000 contexts for the vectors gener-
ated from the syntactic N-gram corpus. The re-
sults are shown in Figure 3.
In contrast to the idea that dependency-based
methods outperform window-based methods, we
find that the window-based models outperform
dependency-based models when they are con-
structed from the same corpus using the small
window size. However, Google?s syntactic N-
gram corpus does indeed outperform window-
based methods, even though smaller vectors were
used for the Google models (10,000 vs. 50,000
features). We observe large variations across
datasets, with window-based methods performing
particularly well on some, but not all. In partic-
ular, window-based methods clearly outperform
dependency-based methods on the RG dataset (for
the same source corpus), whereas the opposite
trend is observed for the TOEFL synonym dataset.
The summary is that the model built from the syn-
tactic N-grams is the overall winner, but when we
3
The Clark and Curran (2007) parser was used to provide
the dependencies.
compare both methods on the same corpus, the
window-based method on a large corpus appears
to work best (given the small window size).
3.4 Feature granularity
Stemming and lemmatisation are standard tech-
niques in NLP and IR to reduce data sparsity.
However, with large enough corpora it may be
that the loss of information through generalisa-
tion hurts performance. In fact, it may be that in-
creased granularity ? through the use of grammat-
ical tags ? can lead to improved performance. We
test these hypotheses by comparing four types of
processed context words: lemmatised, stemmed,
POS-tagged, and tagged with CCG lexical cate-
gories (which can be thought of as fine-grained
POS tags (Clark and Curran, 2007)).
4
The source
corpora are BNC and ukWaC, using a window-
based method with windows of size 5, Positive
Mutual Information weighting, vectors of size
50,000 and Cosine similarity. The results are re-
ported in Figure 4.
The ukWaC-generated vectors outperform the
BNC-generated ones on all but a single instance
for each of the granularities. Stemming yields
the best overall performance, and increasing the
granularity does not lead to better results. Even
with a very large corpus like ukWaC, stemming
yields signficantly better results than not reduc-
ing the feature granularity at all. Conversely, apart
from the results on the TOEFL synonym dataset,
increasing the feature granularity of contexts by
including POS tags or CCG categories does not
yield any improvement.
3.5 Similarity-weighting combination
There is contrasting evidence in the literature re-
garding which combination of similarity metric
and weighting scheme works best. Here we inves-
tigate this question using vectors of size 50,000,
no processing of the context features (i.e., ?nor-
mal? feature granularity), and a window-based
method with a window size of 5. Aggregated
scores across the datasets are reported in Tables
4 and 5 for the BNC and ukWaC, respectively.
There are some clear messages to be taken from
these large tables of results. First, two weighting
schemes perform better than the others: Positive
Mutual Information (PosMI) and T-Test. On the
BNC, the former yields the best results. There are
4
Using NLTK?s Porter stemmer and WordNet lemmatiser.
25
Figure 4: Feature granularity: stemmed (S), lem-
matised (L), normal (N), POS-tagged (T) and
CCG-tagged (C)
RG MC W353 MEN TOEFL
P+COS 0.74 0.64 0.50 0.66 0.76
P+COR 0.74 0.65 0.58 0.71 0.83
T+COS 0.78 0.69 0.54 0.68 0.78
T+COR 0.78 0.71 0.54 0.68 0.78
Table 6: Similarity scores on individual datasets
for positive mutual information (P) and T-test
(T) weighting, with cosine (COS) and correlation
(COR) similarity
three similarity metrics that perform particularly
well: Cosine, Correlation and the Tanimoto coef-
ficient (the latter also being similar to Cosine; see
Table 2). The Correlation similarity metric has the
most consistent performance across the different
weighting schemes, and yields the highest score
for both corpora. The most consistent weighting
scheme across the two source corpora and similar-
ity metrics appears to be PosMI.
The highest combined aggregate score is that of
PosMI with the Correlation metric, in line with
the conclusion of Bullinaria and Levy (2012) that
PosMI is the best weighting scheme
5
. However,
for the large ukWaC corpus, T-Test achieves sim-
ilarly high aggregate scores, in line with the work
of Curran (2004). When we look at these two
weighting schemes in more detail, we see that T-
Test works best for the RG and MC datasets, while
PosMI works best for the others; see Table 6. Cor-
relation is the best similarity metric in all cases.
5
In some cases, the combination of weighting scheme and
similarity metric results in a division by zero or leads to tak-
ing the logarithm of a negative number, in which cases we
report the aggregate scores as nan (not-a-number).
Figure 5: Finding the optimal ?contiguous subvec-
tor? of size 10,000
3.6 Optimal subvector
Stopwords are typically removed from vectors and
not used as features. However, Bullinaria and
Levy (2012) find that removing stopwords has no
effect on performance. A possible explanation
is that, since they are using a weighting scheme,
the weights of stopwords are low enough that
they have effectively been removed anyhow. This
raises the question: are we removing stopwords
because they contribute little towards the meaning
of the target word, or are we removing them be-
cause they have high frequency?
The experiment used ukWaC, with a window-
based method and window size of 5, normal fea-
ture granularity, Cosine similarity and a sliding
vector of size 10,000. Having a sliding vector im-
plies that we throw away up to the first 40,000 con-
texts as we slide across to the 50,000 mark (replac-
ing the higher frequency contexts with lower fre-
quency ones). In effect, we are trying to find the
cut-off point where the 10,000-component ?con-
tiguous subvector? of the target word vector is
optimal (where the features are ordered by fre-
quency). Results are given for PosMI, T-Test and
no weighting at all.
The results are shown in Figure 5. T-test outper-
forms PosMI at the higher frequency ranges (to the
left of the plots) but PosMI gives better results for
some of the datasets further to the right. For both
weighting schemes the performance decreases as
high frequency contexts are replaced with lower
frequency contexts.
A different picture emerges when no weight-
ing is used, however. Here the performance can
increase as high-frequency contexts are replaced
26
British National Corpus
COS COR DIC JC1 JC2 TAN LIN EUC CIB CHS JSD ASK
none 0.49 0.50 0.34 0.35 0.27 0.22 0.30 0.09 0.11 0.08 0.45 0.36
tfidf 0.43 0.44 0.33 0.34 0.22 0.16 0.27 0.13 0.12 0.16 0.38 0.32
tficf 0.47 0.48 0.34 0.36 0.23 0.16 0.27 0.13 0.12 0.15 0.40 0.33
okapi 0.40 0.42 0.37 0.42 0.22 0.23 0.26 0.25 0.15 0.14 0.37 0.26
atc 0.40 0.43 0.25 0.24 0.16 0.34 0.30 0.10 0.13 0.08 0.33 0.23
ltu 0.44 0.45 0.35 0.36 0.22 0.23 0.26 0.22 0.13 0.21 0.37 0.27
mi 0.58 0.61 0.31 0.56 0.29 -0.07 0.45 0.15 0.10 0.09 0.16 -0.04
posmi 0.63 0.66 0.52 0.58 0.35 -0.08 0.45 0.15 0.11 0.06 0.54 0.46
ttest 0.63 0.62 0.11 0.34 0.08 0.63 0.17 0.18 0.14 0.11 nan nan
chisquared 0.50 0.50 0.46 0.42 0.42 0.42 nan 0.06 0.07 0.08 0.57 0.52
lin98b 0.47 0.52 0.35 0.40 0.21 -0.10 0.29 0.10 0.11 nan 0.38 0.29
gref94 0.46 0.49 0.35 0.37 0.23 0.06 0.28 0.12 0.11 0.09 0.41 0.30
Table 4: Aggregated scores for combinations of weighting schemes and similarity metrics using the BNC.
The similarity metrics are Cosine (COS), Correlation (COR), Dice (DIC), Jaccard (JC1), Jaccard2 (JC2),
Tanimoto (TAN), Lin (LIN), Euclidean (EUC), CityBlock (CIB), Chebyshev (CHS), Jensen-Shannon
Divergence (JSD) and ?-skew (ASK)
ukWaC
COS COR DIC JC1 JC2 TAN LIN EUC CIB CHS JSD ASK
none 0.55 0.55 0.28 0.35 0.24 0.41 0.31 0.06 0.09 0.08 0.56 0.49
tfidf 0.45 0.47 0.26 0.30 0.20 0.28 0.22 0.14 0.12 0.16 0.37 0.27
tficf 0.45 0.49 0.27 0.33 0.20 0.29 0.24 0.13 0.11 0.09 0.37 0.28
okapi 0.37 0.42 0.33 0.37 0.18 0.27 0.26 0.26 0.17 0.12 0.34 0.20
atc 0.34 0.42 0.13 0.13 0.08 0.15 0.28 0.10 0.09 0.07 0.28 0.15
ltu 0.43 0.48 0.30 0.34 0.19 0.26 0.25 0.26 0.16 0.24 0.36 0.23
mi 0.51 0.53 0.18 0.51 0.16 0.28 0.37 0.18 0.10 0.09 0.12 nan
posmi 0.67 0.70 0.56 0.62 0.42 0.59 0.52 0.23 0.15 0.06 0.60 0.49
ttest 0.70 0.70 0.16 0.48 0.10 0.70 0.22 0.16 0.11 0.15 nan nan
chisquared 0.57 0.58 0.52 0.56 0.44 0.52 nan 0.08 0.06 0.10 0.63 0.60
lin98b 0.43 0.63 0.31 0.37 0.20 0.23 0.26 0.09 0.10 nan 0.34 0.24
gref94 0.48 0.54 0.27 0.33 0.20 0.17 0.23 0.13 0.11 0.09 0.38 0.25
Table 5: Aggregated scores for combinations of weighting schemes and similarity metrics using ukWaC
with lower-frequency ones, with optimal perfor-
mance comparable to when weighting is used.
There are some scenarios where it may be ad-
vantageous not to use weighting, for example in
an online setting where the total set of vectors is
not fixed; in situations where use of a dimension-
ality reduction technique does not directly allow
for weighting, such as random indexing (Sahlgren,
2006); as well as in settings where calculating
weights is too expensive. Where to stop the slid-
ing window varies with the datasets, however, and
so our conclusion is that the default scheme should
be weighting plus high frequency contexts.
4 Compositionality
In order to examine whether optimal parame-
ters carry over to vectors that are combined into
phrasal vectors using a composition operator, we
perform a subset of our experiments on the canoni-
cal compositionality dataset from Mitchell and La-
pata (2010), using vector addition and pointwise
multiplication (the best performing operators in
the original study).
We evaluate using two source corpora (the BNC
and ukWaC) and two window sizes (small, with
a window size of 3; and big, where the full sen-
tence is the window). In addition to the weight-
ing schemes from the previous experiment, we in-
clude Mitchell & Lapata?s own weighting scheme,
which (in our notation) is defined as w
ij
=
f
ij
?N
f
i
?f
j
.
While all weighting schemes and similarity met-
rics were tested, we report only the best perform-
ing ones: correlations below 0.5 were ommitted
for the sake of brevity. Table 7 shows the results.
We find that many of our findings continue to
hold. PosMI and T-Test are the best performing
weighting schemes, together with Mitchell & La-
pata?s own weighting scheme. We find that ad-
dition outperforms multiplication (contrary to the
original study) and that small window sizes work
best, except in the VO case. Performance across
corpora is comparable. The best performing simi-
larity metrics are Cosine and Correlation, with the
latter having a slight edge over the former.
27
BNC - Small window
AN NN VO ALL
add-posmi-cosine 0.57 0.56 0.52 0.55
add-posmi-correlation 0.66 0.60 0.53 0.60
add-ttest-cosine 0.59 0.54 0.53 0.56
add-ttest-correlation 0.60 0.54 0.53 0.56
add-mila-correlation 0.64 0.38 0.51 0.51
ukWaC - Small window
AN NN VO ALL
add-posmi-correlation 0.64 0.59 0.56 0.59
add-ttest-cosine 0.61 0.55 0.53 0.56
add-ttest-correlation 0.61 0.55 0.53 0.56
add-mila-correlation 0.64 0.48 0.57 0.56
mult-mila-correlation 0.52 0.44 0.63 0.53
BNC - Large window
AN NN VO ALL
add-posmi-correlation 0.47 0.49 0.57 0.51
add-ttest-cosine 0.50 0.53 0.60 0.54
add-ttest-correlation 0.50 0.53 0.60 0.54
add-mila-correlation 0.51 0.49 0.61 0.54
mult-posmi-correlation 0.48 0.48 0.66 0.54
mult-mila-correlation 0.53 0.51 0.67 0.57
ukWaC - Large window
AN NN VO ALL
add-posmi-correlation 0.46 0.44 0.60 0.50
add-ttest-cosine 0.46 0.46 0.59 0.50
add-ttest-correlation 0.47 0.46 0.60 0.51
add-mila-correlation 0.47 0.46 0.64 0.52
mult-posmi-correlation 0.44 0.46 0.65 0.52
mult-mila-correlation 0.56 0.49 0.70 0.58
Table 7: Selected Spearman ? scores on the
Mitchell & Lapata 2010 compositionality dataset
5 Conclusion
Our experiments were designed to investigate a
wide range of VSM parameters, using a variety
of evaluation tasks and several source corpora.
Across each of the experiments, results are com-
petitive with the state of the art. Some important
messages can be taken away from this study:
Experiment 1 Larger vectors do not always lead
to better performance. As vector size increases,
performance stabilises, and a vector size of around
50,000 appears to be optimal.
Experiment 2 The size of the window has a
clear impact on performance: a large corpus with
a small window size performs best, but high per-
formance can be achieved on a small subspaced
corpus, if the window size is large.
Experiment 3 The size of the source corpus
is more important than whether the model is
window- or dependency-based. Window-based
methods with a window size of 3 yield better re-
sults than dependency-based methods with a win-
dow of 3 (i.e. having a single arc). The Google
Syntactic N-gram corpus yields very good perfor-
mance, but it is unclear whether this is due to being
dependency-based or being very large.
Experiment 4 The granularity of the context
words has a relatively low impact on performance,
but stemming yields the best results.
Experiment 5 The optimal combination of
weighting scheme and similarity metric is Posi-
tive Mutual Information with a mean-adjusted ver-
sion of Cosine that we have called Correlation.
Another high-performing weighting scheme is T-
Test, which works better for smaller vector sizes.
The Correlation similarity metric consistently out-
performs Cosine, and we recommend its use.
Experiment 6 Use of a weighting scheme ob-
viates the need for removing high-frequency fea-
tures. Without weighting, many of the high-
frequency features should be removed. However,
if weighting is an option we recommend its use.
Compositionality The best parameters for
individual vectors generally carry over to a com-
positional similarity task where phrasal similarity
is evaluated by combining vectors into phrasal
vectors.
Furthermore, we observe that in general perfor-
mance increases as source corpus size increases,
so we recommend using a corpus such as ukWaC
over smaller corpora like the BNC. Likewise,
since the MEN dataset is the largest similarity
dataset available and mirrors our aggregate score
the best across the various experiments, we rec-
ommend evaluating on that similarity task if only
a single dataset is used for evaluation.
Obvious extensions include an analysis of the
performance of the various dimensionality reduc-
tion techniques, examining the importance of win-
dow size and feature granularity for dependency-
based methods, and further exploring the relation
between the size and frequency distribution of a
corpus together with the optimal characteristics
(such as the high-frequency cut-off point) of vec-
tors generated from that source.
Acknowledgments
This work has been supported by EPSRC grant
EP/I037512/1. We would like to thank Laura
Rimell, Tamara Polajnar and Felix Hill for help-
ful comments and suggestions.
28
References
Marco Baroni, Silvia Bernardini, Adriano Ferraresi,
and Eros Zanchetta. 2009. The WaCky Wide Web:
A collection of very large linguistically processed
Web-crawled corpora. Language Resources and
Evaluation, 43(3):209?226.
Marco Baroni, Raffaella Bernardi, and Roberto Zam-
parelli. 2013. Frege in Space: A program for com-
positional distributional semantics. Linguistic Is-
sues in Language Technologies (LiLT).
Steven Bird, Ewan Klein, and Edward Loper.
2009. Natural Language Processing with Python.
O?Reilly Media.
William Blacoe and Mirella Lapata. 2012. A Com-
parison of Vector-based Representations for Seman-
tic Composition. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 546?556, Jeju Island, Korea,
July. Association for Computational Linguistics.
Elia Bruni, Gemma Boleda, Marco Baroni, and N. K.
Tran. 2012. Distributional Semantics in Techni-
color. In Proceedings of the ACL 2012.
John A. Bullinaria and Joseph P. Levy. 2007. Ex-
tracting Semantic Representations from Word Co-
occurrence Statistics: A computational study. Be-
havior Research Methods, 39:510?526.
John A. Bullinaria and Joseph P. Levy. 2012. Ex-
tracting Semantic Representations from Word Co-
occurrence Statistics: Stop-lists, Stemming and
SVD. Behavior Research Methods, 44:890?907.
L. Burnard. 2007. Reference Guide
for the British National Corpus.
http://www.natcorp.ox.ac.uk/docs/URG/.
Stephen Clark and James R. Curran. 2007. Wide-
Coverage Efficient Statistical Parsing with CCG and
Log-Linear Models. Computational Linguistics,
33(4):493?552.
Stephen Clark. 2014. Vector Space Models of Lexical
Meaning (to appear). In Shalom Lappin and Chris
Fox, editors, Handbook of Contemporary Semantics.
Wiley-Blackwell, Oxford.
James R. Curran and Marc Moens. 2002a. Improve-
ments in Automatic Thesaurus Extraction. In Pro-
ceedings of the ACL-02 workshop on Unsupervised
lexical acquisition-Volume 9, pages 59?66. Associa-
tion for Computational Linguistics.
James R. Curran and Marc Moens. 2002b. Scaling
Context Space. In Proceedings of the 40th Annual
Meeting on Association for Computational Linguis-
tics, pages 231?238. Association for Computational
Linguistics.
James R. Curran. 2004. FromDistributional to Seman-
tic Similarity. Ph.D. thesis, University of Edinburgh.
Katrin Erk. 2012. Vector Space Models of Word
Meaning and Phrase Meaning: A Survey. Language
and Linguistics Compass, 6(10):635?653.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2002. Placing Search in Context: The
Concept Revisited. ACM Transactions on Informa-
tion Systems, 20(1):116?131.
Yoav Goldberg and Jon Orwant. 2013. A Dataset
of Syntactic-Ngrams over Time from a Very Large
Corpus of English Books. In Second Joint Con-
ference on Lexical and Computational Semantics
(*SEM), Volume 1: Proceedings of the Main Con-
ference and the Shared Task: Semantic Textual Simi-
larity, pages 241?247, Atlanta, Georgia, USA, June.
Association for Computational Linguistics.
Gregory Grefenstette. 1994. Explorations in Auto-
matic Thesaurus Discovery. Kluwer Academic Pub-
lishers, Norwell, MA, USA.
Z. Harris. 1954. Distributional Structure. Word,
10(23):146?162.
F. Hill, D. Kiela, and A. Korhonen. 2013. Con-
creteness and Corpora: A Theoretical and Practical
Analysis. In Proceedings of ACL 2013, Workshop
on Cognitive Modelling and Computational Linguis-
tics, Sofia, Bulgaria.
Thomas K. Landauer and Susan T. Dumais. 1997.
A solution to Platos problem: The latent semantic
analysis theory of acquisition, induction, and rep-
resentation of knowledge. Psychological Review,
104(2):211?240.
Gabriella Lapesa and Stefan Evert. 2013. Evaluat-
ing neighbor rank and distance measures as predic-
tors of semantic priming. In In Proceedings of the
ACL Workshop on Cognitive Modeling and Compu-
tational Linguistics (CMCL 2013), Sofia, Bulgaria.
G.A. Miller and W.G. Charles. 1991. Contextual Cor-
relates of Semantic Similarity. Language and Cog-
nitive Processes, 6(1):1?28.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in Distributional Models of Semantics. Cognitive
Science, 34(8):1388?1429.
Sebastian Pado and Mirella Lapata. 2007.
Dependency-based Construction of Semantic
Space Models. Computational Linguistics,
33(2):161?199.
Douglas L. T. Rohde, Laura M. Gonnerman, and
David C. Plaut. 2006. An Improved Model of Se-
mantic Similarity based on Lexical Co-occurence.
Communciations of the ACM, 8:627?633.
Herbert Rubenstein and John B. Goodenough. 1965.
Contextual Correlates of Synonymy. Commun.
ACM, 8(10):627?633, October.
29
Magnus Sahlgren. 2006. The Word-Space Model: Us-
ing distributional analysis to represent syntagmatic
and paradigmatic relations between words in high-
dimensional vector spaces. Ph.D. thesis, Depart-
ment of Linguistics, Stockholm University.
Sabine Schulte im Walde, Stefan M?uller, and Stephen
Roller. 2013. Exploring Vector Space Models to
Predict the Compositionality of German Noun-Noun
Compounds. In Proceedings of the 2nd Joint Con-
ference on Lexical and Computational Semantics,
pages 255?265, Atlanta, GA.
N. Clayton Silver and William P. Dunlap. 1987. Av-
eraging Correlation Coefficients: Should Fisher?s z
Transformation Be Used? Journal of Applied Psy-
chology, 72(1):146?148, February.
Mark Steedman. 2000. The Syntactic Process. MIT
Press, Cambridge, MA, USA.
Benjamin P. Stone, Simon J. Dennis, and Peter J.
Kwantes. 2008. A Systematic Comparison of Se-
mantic Models on Human Similarity Rating Data:
The Effectiveness of Subspacing. In The Proceed-
ings of the Thirtieth Conference of the Cognitive Sci-
ence Society.
Peter D. Turney and Patrick Pantel. 2010. From Fre-
quency to Meaning: vector space models of seman-
tics. J. Artif. Int. Res., 37(1):141?188, January.
Julie Weeds, David Weir, and Diana McCarthy. 2004.
Characterising Measures of Lexical Distributional
Similarity. In Proceedings of Coling 2004, pages
1015?1021, Geneva, Switzerland, Aug 23?Aug 27.
COLING.
S. Zelikovitz and M. Kogan. 2006. Using Web
Searches on Important Words to create Background
Sets for LSI Classification. In In Proceedings of
the 19th International FLAIRS Conference, pages
598?603, Menlo Park, CA. AAAI Press.
30
Proceedings of the AHA! Workshop on Information Discovery in Text, pages 1?6,
Dublin, Ireland, August 23 2014.
Application-Driven Relation Extraction with Limited Distant Supervision
Andreas Vlachos
Computer Science Department
University College London
a.vlachos@cs.ucl.ac.uk
Stephen Clark
Computer Laboratory
University of Cambridge
sc609@cam.ac.uk
Abstract
Recent approaches to relation extraction following the distant supervision paradigm have focused
on exploiting large knowledge bases, from which they extract substantial amount of supervision.
However, for many relations in real-world applications, there are few instances available to seed
the relation extraction process, and appropriate named entity recognizers which are necessary for
pre-processing do not exist. To overcome this issue, we learn entity filters jointly with relation
extraction using imitation learning. We evaluate our approach on architect names and building
completion years, using only around 30 seed instances for each relation and show that the jointly
learned entity filters improved the performance by 30 and 7 points in average precision.
1 Introduction
In this paper we focus on relation extraction in the context of a real-world application. The application
is a dialog-based city tour guide, based in Edinburgh. One of the features of the system is its pro-active
nature, offering information which may be of interest to the user. In order to be pro-active in this way,
as well as answer users? questions, the system requires a large amount of knowledge about the city. Part
of that knowledge is stored in a database, which is time-consuming and difficult to populate manually.
Hence, we have explored the use of an automatic knowledge base population technique based on distant
supervision (Craven and Kumlien, 1999; Mintz et al., 2009).
The attraction of this approach is that the only input required is a list of seed instances of the relation in
question and a corpus of sentences expressing new instances of that relation. However, existing studies
typically assume a large seed set, whereas in our application such sets are often not readily available, e.g.
Mintz et al. (2009) reported using 7K-140K seed instances per relation as input. In this paper, the two
relations that we evaluate on are architect name and completion year of buildings. These were chosen
because they are highly relevant to our application, but also somewhat non-standard compared to the
existing literature; and crucially they do not come with a readily-available set of seed instances.
Furthermore, previous approaches typically assume named entity recognition (NER) as a pre-
processing step in order to construct the training and testing instances. However, since these tools are
not tailored to the relations of interest, they introduce spurious entity matches that are harmful to per-
formance as shown by Ling and Weld (2012) and Zhang et al. (2013). These authors ameliorated this
issue by learning fine-grained entity recognizers and filters using supervised learning. The labeled data
used was extracted from the anchor text of entity mentions annotated in Wikipedia, however this is not
possible for entities not annotated in this resource.
In this work, instead of relying on labeled data to construct entity filters, we learn them jointly with the
relation extraction component. For this purpose we use the imitation learning algorithm DAGGER (Ross
et al., 2011), which can handle the dependencies between actions taken in a sequence, and use supervision
for later actions to learn how to take actions earlier in the sequence. We evaluate our approach using
around 30 seed instances per relation and show that the jointly learned entity filters result in gains of 7
and 30 points in average precision for the completion year and the architect name relations respectively.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1
relation keywords: building, architect
question answer
Advocates? Library William Playfair
Bute House Robert Adam
Dunstane House ?
Craigiehall ?
sentences
The Advocates? Library is currently located in a William Playfair-
designed building.
Bute House is unusual in Robert Adam?s design for Charlotte Square
in having a central front door.
Dunstane House in Edinburgh was built in 1852 to the design of
architect William Playfair.
The 16-room Dunstane House was originally built by the Ross family
as their private home in 1852.
Dunstane House was designed by famous architect William Playfair.
Craigiehall is a late-17th-century country house, which now serves as
the headquarters of the Second Division of the British Army.
label question candidate sentence
training instances
+ Advocates? Library William Playfair The Advocates? Library. . .
+ Bute House Robert Adam Bute House is unusual. . .
- Bute House Charlotte Square Bute House is unusual. . .
predicted instances
- Dunstane House Edinburgh Dunstane House in. . .
+ Dunstane House William Playfair Dunstane House in. . .
+ Dunstane House Ross The 16-room Dunstane. . .
+ Dunstane House William Playfair Dunstane House was. . .
- Craigiehall Second Division Craigiehall is a . . .
- Craigiehall British Army Craigiehall is a. . .
entity filter
relation extractor
question answer score
Dunstane House William Playfair 2
Ross 1
Craigiehall
WEB
DISTANT SUPERVISION
TRAIN
PREDICT
OUTPUT
Figure 1: The stages of our proposed approach applied to the architect name relation.
2 Approach overview
We will use the architect-building relation as an example to give an overview of our approach, as shown
in Figure 1. The input to the system is a list of buildings, where for some we know the architect (the
seeds), and the task is to find the architects for the remainder. One difference with the standard setup for
relation extraction using distant supervision is that we assume a list of historical buildings instead of a
tailored NER system. This is reasonable for the example, since such a list is relatively easy to acquire.
In order to create training data, queries containing words from the seeds are sent to a search engine.
Sentences from the returned pages are then processed to find examples which contain mentions of both
a building and the corresponding architect. Applying the distant supervision hypothesis, we assume that
such sentences are indeed expressing the desired relation, and these are positive examples. While such
data contains noise, it has been shown to be useful in practice (Yao et al., 2010; Hoffmann et al., 2011).
At test time the input is the name of a historical building. Now the web is searched to find example
sentences containing this name, and the classifier is applied to each sentence, returning either the name
of the architect, or none. Note that different sentences could provide evidence for different architects;
hence assuming only one architect for each building, a procedure is required to decide between the
possible answers (see Sec. 5).
3 Entity Filtering for Relation Extraction
Each relation extraction instance consists of a sentence containing a question entity (e.g. Bute House)
and a candidate answer (e.g. Robert Adam), and the task is to predict whether the answer and question
entity have the relation of interest. The standard approach is to learn a binary classifier (possibly as part
of a more complex model e.g. Hoffmann et al. (2011)) using features that describe each entity as well
as the lexico-syntactic relation between them in the sentence. These commonly include the lexicalized
dependency path from the question entity to the candidate answer, as well as the lemmas on this path. In
this setup, NER assists by filtering the instances generated to those that contain appropriate recognized
entities and by providing features for them.
However, since we do not assume NER in pre-processing, this task becomes harder in our setup,
since the candidate answers are very often inappropriate for the relation at question. A simple way
2
Algorithm 1: Learning with DAGGER
Input: training set S, loss `, CSC learner CSCL
Output: Learned policy H
N
1 CSC Examples E = ?
2 for i = 1 to N do
3 for s in S do
4 Predict y?
1:T
= H
i?1
(s)
5 for y?
t
in pi(s) do
6 Extract features ?
t
= f(s, y?
1:t?1
)
7 foreach possible action y
j
t
do
8 Predict y?
t+1:T
= H
i?1
(s; y?
1:t?1
, y
j
t
)
9 Assess c
j
t
= `(y?
1:t?1
, y
j
t
, y?
t+1:T
)
10 Add (?
t
, c
t
) to E
11 Learn H
i
= CSCL(E)
to incorporate NER-like information is to add the features that would have been used for NER to the
relation extraction features and learn a classifier as above. Such features are commonly extracted from
the candidate answer itself as well as its context. The former include the tokens of the answer, their
lemmas, whether the answer is capitalised, etc. The latter include the words and bigrams preceding
and following the answer, as well as syntactic dependencies between the words denoting the entity and
surrounding lemmas.
However, while these features are likely to be useful, they also render learning relation extraction
harder because they are not directly relevant to the task. For example, the features describing the first
training instance of Fig. 1 would include that the token Playfair is part of the candidate answer and that
the lemma design is part on the syntactic dependency path between the architect and the building, but
only the latter is crucial for the correct classification of this instance. Thus, including the NER features
about the candidate answer can be misleading, especially since they tend to be less sparse than the relation
extraction ones.
Therefore we split the prediction into two binary classification stages: the first stage predicts whether
the candidate answer is appropriate for the relation (entity filtering), and the second one whether the
sentence expresses the relation between the answer and the question entity (relation extraction). If the
prediction for the first stage is negative, then the second stage is not reached. However, we do not have
labels to train a classifier for the entity filtering stage since if an instance is negative this could be either
due to the candidate answer or to the relation expressed in the sentence. We discuss how we overcome
this issue using the algorithm DAGGER (Ross et al., 2011) next.
4 Imitation learning with DAGGER
Imitation learning algorithms such as DAGGER and SEARN (Daum?e III et al., 2009) have been applied
successfully to a variety of structured prediction tasks (Vlachos, 2012; He et al., 2013) due to their
flexibility in incorporating features. In this work we focus on the parameter-free version of DAGGER
and highlight its ability to handle missing labels in the training data. During training, DAGGER converts
the problem of learning how to predict sequences of actions into cost sensitive classification (CSC)
learning. The dependencies between the actions are learned by appropriate generation of CSC examples.
In our case, each instance is predicted by a sequence of two actions: an entity filtering action followed (if
positive) by a relation extraction action. The output is a learned policy, consisting of the binary classifiers
for entity filtering and relation extraction.
Following Alg. 1, in each iteration DAGGER generates training examples using the previous learned
policy H
i?1
to predict the instances (line 4). For each action taken, the cost for each possible action is
estimated by assuming that the action was taken; then the following actions for that instance are predicted
3
Recall-top Precision-top F-score-top Recall-all Precision-all F-score-all
Base 0.28 0.28 0.28 0.9 0.1 0.18
1stage 0.52 0.71 0.6 0.67 0.68 0.675
2stage 0.5 0.68 0.58 0.67 0.67 0.67
Base 0.0 0.0 0.0 0.62 0.002 0.004
1stage 0.15 0.26 0.19 0.23 0.17 0.2
2stage 0.26 0.65 0.37 0.3 0.55 0.39
Table 1: Test set results for the 3 systems on year completed (top) and architect name (bottom).
using H
i?1
(line 8); and the complete sequence of actions is compared against the correct output using
the loss function (line 9). Since the latter is only applied to complete sequences, it does not need to
decompose over individual actions. We define the loss to be 0 when the relation extraction stage is
correct and 1 otherwise. Therefore we do not need to know the labels for entity filtering, but we learn a
classifier for it so that the relation extraction predictions are correct. Finally, the CSC training examples
generated are added (line 10) and a new policy is learnt (line 11).
Since the losses are either 0 or 1, the CSC learning task is equivalent to ordinary classification learning.
To learn the binary classifiers for each stage we implemented the adaptive regularization of weights
(AROW) algorithm (Crammer et al., 2009) which scales to large datasets and handles sparse feature sets
by adjusting the learning rate for each feature. In the first iteration, we do not have a learned policy, thus
we assume a naive entity filter that accepts all candidate answers and a relation extractor that predicts the
correct label.
5 Experiments
The relations used for evaluation are building-architect and building-completion year, for the reasons
given in Sec. 1. For each of the 138 listed historical buildings in Wikipedia,
1
we found the correct
answers, resulting in 60 building-completion year and 68 building-architect pairs. We split the data into
two equal parts for training/development and testing. We then collected relevant web pages querying
the web as described in Sec. 2. The queries were submitted to Bing via its Search API and the top
300 results for each query were obtained. We downloaded the corresponding pages and extracted their
textual content with BoilerPipe (Kohlsch?utter et al., 2010). We then processed the texts using the Stanford
CoreNLP toolkit.
2
We tried to match the question entity with tokens in each of the sentences, allowing
for minor differences in tokenization, whitespace and capitalization. If a sentence contained the question
entity and a candidate answer, we parsed it using the Klein and Manning (2002) parser. The instances
generated were labeled using the distant supervision assumption, resulting in 974K and 4.5M labeled
instances for the completion year and the architect relation, respectively.
We ran experiments with three systems; the jointly learned entity filtering-relation extraction approach
using imitation learning (henceforth 2stage), the one-stage classification approach using the features for
both entity filtering and relation extraction (henceforth 1stage), and a baseline that for each question
entity returns all candidate answers for the relation ranked by the number of times they appeared with
the question entity and ignoring all other information (henceforth Base). Following four-fold cross-
validations experiment on the development data, we used 12 iterations for learning with DAGGER.
Each system returns a list of answers ranked according to the number of instances classified as positive
for that answer. We used two evaluation modes. The first considers only the top-ranked answer (top),
whereas the second considers all answers returned until either the correct one is found or they are ex-
hausted (all). In all we define recall as the number of correct answers over the total number of question
entities, and precision as the chance of finding the correct answer while traversing those returned.
Results by all models are reported for both relations in Table 1. A first observation is that the architect
name relation is substantially harder to extract since all models achieve worse scores than for the com-
pletion year relation. More specifically, Base achieves respectable scores in top mode in completion year
extraction, but it fails completely in architect name. This is due to the existence of many other names
1
http://en.wikipedia.org/wiki/Category:Listed_buildings_in_Edinburgh
2
http://nlp.stanford.edu/software/corenlp.shtml
4
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9
P
re
ci
si
on
-a
ll
Recall-all
1stage
2stage
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7
P
re
ci
si
on
-a
ll
Recall-all
1stage
2stage
Table 2: Test set precision-recall curves in all mode for year completed (left) and architect name (right).
that appear more frequently together with a building than that of its architect, while the completion year
is sometimes the number most frequently mentioned in the same sentence with the building. In addition,
Base achieves the maximum possible all recall by construction, since if there is a sentence containing the
correct answer for a question entity it will be returned. However this comes at a cost of low precision.
Both the machine-learned models improve upon Base substantially on both datasets, with the 2stage
model being substantially better in architect name extraction, especially in terms of precision. In comple-
tion year extraction the differences are smaller, with 1stage being slightly better. These small differences
are expected since recognizing completion years is much easier than recognizing architect names, thus
learning a separate entity filtering model for them is less likely to be useful. Nevertheless, inspecting
the weights learned by the 2stage model showed that some useful distinctions were learned, e.g. being
preceded by the word ?between? as in ?built between 1849 and 1852? renders a number less likely to be a
completion year. Finally, we examined the quality of the learned models further by generating precision-
recall curves for the all mode by adjusting the classification thresholds used by 1stage and 2stage. As
shown in the plots of Table 2, 2stage achieves higher precision than 1stage at most recall levels for both
relations, with the benefits being more pronounced in the architect name relation. Summarizing these
curves using average precision (Manning et al., 2008), the scores were 0.69 and 0.76 for the comple-
tion year, and 0.21 and 0.51 for the architect, for the 1stage and the 2stage models respectively, thus
confirming the usefulness of separating the entity filtering features from relation extraction.
6 Discussion
While all the buildings considered in our experiments have a dedicated Wikipedia page, only a few had
a sentence mentioning them together with the correct answer in that resource. Also, the architects who
were the correct answers did not always have a dedicated Wikipedia page. Even though combining
a search engine with distant supervision results in a highly imbalanced learning task, it increases the
potential coverage of our system. In this process we rely on the keywords used in the queries in order
to find pages containing the entities intended rather than synonymous ones, e.g. the keyword ?building?
helps avoid extracting sentences mentioning saints instead of churches. Nevertheless, building names
such as churches named after saints were often ambiguous resulting in false positives.
Bunescu and Mooney (2007) also used a small seed set and a search engine, but they collected sen-
tences via queries containing both the question and the answer entities, thus (unreallistically) assuming
knowledge of all the correct answers. Instead we rely on simple heuristics to identify candidate answers.
These heuristics are relation-dependent and different types of answers can be easily accommodated, e.g.
in completed year relation they are single-token numbers. Finally, the entity filters learned jointly with
relation extraction in our approach, while they perform a role similar to NER, they are learned so that
they help avoid relation extraction errors and not to replace an actual NER system.
7 Conclusions
Our application-based setting has placed novel demands on relation extraction system trained with distant
supervision, and in this paper we have shown that reasonable results can be obtained with only around
30 seed examples without requiring NER for pre-processing. Furthermore, we have demonstrated that
learning entity filters and relation extraction jointly improves performance.
5
Acknowledgements
The research reported was conducted while the first author was at the University of Cambridge and
funded by the European Community?s Seventh Framework Programme (FP7/2007-2013) under grant
agreement no. 270019 (SPACEBOOK project www.spacebook-project.eu).
References
Razvan Bunescu and Raymond Mooney. 2007. Learning to extract relations from the web using minimal su-
pervision. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages
576?583.
Koby Crammer, Alex Kulesza, and Mark Dredze. 2009. Adaptive regularization of weight vectors. In Advances
in Neural Information Processing Systems 22, pages 414?422.
Mark Craven and Johan Kumlien. 1999. Constructing biological knowledge-bases by extracting information from
text sources. In Proceedings of the 7th International Conference on Intelligent Systems for Molecular Biology,
pages 77?86.
Hal Daum?e III, John Langford, and Daniel Marcu. 2009. Search-based structured prediction. Machine Learning,
75:297?325.
He He, Hal Daum?e III, and Jason Eisner. 2013. Dynamic feature selection for dependency parsing. In Proceedings
of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1455?1464, Seattle,
October.
Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke Zettlemoyer, and Daniel S. Weld. 2011. Knowledge-based
weak supervision for information extraction of overlapping relations. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics, pages 541?550.
Dan Klein and Chris Manning. 2002. Fast exact inference with a factored model for natural language parsing. In
Advances in Neural Information Processing Systems 15, pages 3?10.
Christian Kohlsch?utter, Peter Fankhauser, and Wolfgang Nejdl. 2010. Boilerplate detection using shallow text
features. In Proceedings of the 3rd ACM International Conference on Web Search and Data Mining, pages
441?450.
Xiao Ling and Daniel S. Weld. 2012. Fine-grained entity recognition. In Proceedings of the 26th Conference on
Artificial Intelligence, pages 94?100.
Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schtze. 2008. Introduction to Information Retrieval.
Cambridge University Press, New York, NY, USA.
Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky. 2009. Distant supervision for relation extraction without
labeled data. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural Language Processing of the AFNLP, pages 1003?1011.
St?ephane Ross, Geoffrey J. Gordon, and Drew Bagnell. 2011. A reduction of imitation learning and structured
prediction to no-regret online learning. In 14th International Conference on Artificial Intelligence and Statistics,
pages 627?635.
Andreas Vlachos. 2012. An investigation of imitation learning algorithms for structured prediction. Journal of
Machine Learning Research Workshop and Conference Proceedings, Proceedings of the 10th European Work-
shop on Reinforcement Learning, 24:143?154.
Limin Yao, Sebastian Riedel, and Andrew McCallum. 2010. Collective cross-document relation extraction with-
out labelled data. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Process-
ing, pages 1013?1023.
Xingxing Zhang, Jianwen Zhang, Junyu Zeng, Jun Yan, Zheng Chen, and Zhifang Sui. 2013. Towards accurate
distant supervision for relational facts extraction. In Proceedings of the 51st Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Papers), pages 810?815, Sofia, Bulgaria, August. Association
for Computational Linguistics.
6
